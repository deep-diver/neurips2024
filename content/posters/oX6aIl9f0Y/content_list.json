[{"type": "text", "text": "Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hilal Asi Daogao Liu \u2217 Kevin Tian Apple Inc. University of Washington University of Texas at Austin hilal.asi94@gmail.com liudaogao@gmail.com kjtian@cs.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\\mathrm{th}}$ -moment bound on the Lipschitz constants of sample functions, rather than a uniform bound. We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error $\\begin{array}{r}{G_{2}\\cdot\\frac{1}{\\sqrt{n}}+G_{k}\\cdot(\\frac{\\sqrt{d}}{n\\varepsilon})^{1-\\frac{1}{k}}}\\end{array}$ under $(\\varepsilon,\\delta)$ -approximate differential privacy, up to a mild $\\mathrm{polylog}\\big(\\frac{1}{\\delta}\\big)$ factor, where $G_{2}^{2}$ and $G_{k}^{k}$ are the $2^{\\mathrm{nd}}$ and $k^{\\mathrm{th}}$ moment bounds on sample Lipschitz constants, nearly-matching a lower bound of [LR23]. We further give a suite of private algorithms in the heavy-tailed setting which improve upon our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Differentially private stochastic convex optimization (DP-SCO), where an algorithm aims to minimize a population loss given samples from a distribution, is a fundamental problem in statistics and machine learning. In this problem, given $n$ samples from a distribution $\\mathcal{P}$ over a sample space $\\boldsymbol{S}$ , our goal is to privately find an approximate minimizer $\\hat{x}\\in\\mathcal{X}\\subset\\mathbb{R}^{d}$ for the population loss ", "page_idx": 0}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x):=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)\\right],\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f(\\cdot;s)$ is a convex function for all $s\\in S$ . The quality of an algorithm is measured by the excess population loss of its output $\\hat{x}$ , that is $F_{\\mathcal{P}}(\\hat{x})-\\operatorname*{min}_{x^{\\star}\\in\\mathcal{X}}F_{\\mathcal{P}}(x^{\\star})$ . ", "page_idx": 0}, {"type": "text", "text": "Extensive research efforts have been devoted to DP-SCO, resulting in important progress over the past few years [BFTT19, FKT20, AFKT21, BGN21, ALD21, KLL21]. In an important milestone, [BFTT19] developed optimal algorithms (in terms of the excess population loss) for DP-SCO under a uniform Lipschitz assumption (i.e., where every $f(\\cdot;s)$ is assumed to have the same Lipschitz bound), and [FKT20] followed this result with efficient and optimal algorithms that run in linear time for smooth functions. DP-SCO has also been explored in other notable settings, including developing faster algorithms for non-smooth settings [AFKT21, KLL21, $\\mathrm{CJJ}^{+}23]$ , different geometries imposed on the solution space [AFKT21, BGN21, $\\mathrm{GLL}^{+}23]$ ], and different notions of privacy [ALD21]. ", "page_idx": 0}, {"type": "text", "text": "Most existing results in DP-SCO are based on the assumption that the function $f(\\cdot;s)$ is uniformly $G$ -Lipschitz for all $s\\in S$ . This assumption is convenient for private algorithm design, because it allows us to straightforwardly bound the sensitivity of iterates of private algorithms, i.e., how far a pair of iterates defined via algorithms induced by neighboring datasets drift apart. Under the uniform Lipschitz assumption, the DP-SCO problem is relatively well-understood, as optimal and efficient algorithms exist (sometimes requiring additional regularity assumptions) [BFTT19, FKT20].2 Stateof-the-art SCO algorithms satisfying $(\\varepsilon,\\delta)$ -differential privacy (Definition 1) in the uniform Lipschitz setting result in excess population loss ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nG D\\left(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log(\\frac{1}{\\delta})}}{\\varepsilon n}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $D$ is the diameter of $\\mathcal{X}$ . However, the assumption of uniformly $G$ -Lipschitz gradients is strong, and may be violated in real-life applications where the distribution in question has heavy tails (see e.g. discussion in $[\\mathrm{ACG}^{+}16]_{,}^{\\phantom{.}}$ ). As a simple motivating example, consider mean estimation, where each $\\begin{array}{r}{f(\\cdot;s)=\\frac{1}{2}\\left\\Vert\\cdot-s\\right\\Vert^{2}}\\end{array}$ , so the minimizer of $F_{\\mathcal{P}}$ is the population mean. The uniform Lipschitz requirement amounts to $\\mathcal{P}$ having a bounded support over $\\mathcal{X}$ , whereas an algorithm that can handle heavy tails only posits the weaker assumption that $\\mathcal{P}$ has bounded $k$ -th moments. However, as pointed out by [WXDX20], many real-world datasets [MM97, BDFS07, IIW15], especially those from biomedicine and finance, are usually unbounded or even heavy-tailed. As a result, existing algorithms for DP-SCO may have overly pessimistic performance bounds when $G$ is large or even unbounded, necessitating the search for new private algorithms handling heavy-tailed gradients. ", "page_idx": 1}, {"type": "text", "text": "Motivated by this weakness of existing DP-SCO analyses, several papers studied the problem of DP-SCO with heavy-tailed gradients [WXDX20, $\\mathrm{ADF^{+}}21$ , KLZ22, LR23], formally defined in Definition 4. Rather than assuming uniformly Lipschitz gradients, this line of work builds on the more realistic assumption that the norm of the gradients has bounded $k^{\\mathrm{th}}$ -moments. In particular, $[\\mathrm{ADF}^{+}21]$ studied heavy-tailed private optimization for the related empirical loss, while [WXDX20] initiated an analogous study for the population loss. More recently, [KLZ22, LR23] also proposed algorithms to solve the heavy-tailed DP-SCO problem based on clipped stochastic gradient methods. ", "page_idx": 1}, {"type": "text", "text": "Despite the significant progress made in addressing heavy-tailed DP-SCO, it remains notably less understood compared to the uniformly Lipschitz setting. As a benchmark, under a notion called $\\rho$ -concentrated differential privacy (CDP, see Definition 3), which translates to $(\\varepsilon,\\delta)$ -DP for $\\rho\\approx$ $\\varepsilon^{2}\\log^{-1}(\\frac{1}{\\delta})$ , [LR23] established that the best excess population loss achievable scales as ", "page_idx": 1}, {"type": "equation", "text": "$$\nG_{2}D\\cdot\\frac{1}{\\sqrt{n}}+G_{k}D\\cdot\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $G_{j}^{j}$ is the $j^{\\mathrm{th}}$ moment bound on the Lipschitz constant of sampled functions, see Definition 4.   \nNote that as $k\\rightarrow\\infty$ , the rate in (2) recovers the uniform Lipschitz rate in (1). ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, existing works on heavy-tailed DP-SCO assume stringent conditions on problem parameters and are suboptimal in the general case. For example, [KLZ22] requires the loss functions to be uniformly smooth with various parameter bounds in order to guarantee optimal rates, while the recent work [LR23] obtains a suboptimal rate scaling $\\begin{array}{r}{\\mathrm{~is}^{3}\\,G_{2}D\\cdot\\frac{1}{\\sqrt{n}}+G_{k}D\\cdot(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}})^{1-\\frac{2}{k}}}\\end{array}$ , which is $k$ ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Motivated by the suboptimality of existing results for heavy-tailed DP-SCO, we develop the first algorithm for this problem, which achieves the optimal rate (2) up to logarithmic factors with no additional assumptions. Along the way, we give several simple reduction-based tools for overcoming technical barriers encountered by prior works. To state our results (deferring a formal problem statement to Definition 1), we assume that for some $k\\geq2$ and all $j\\in[k]$ , we have ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\nabla f(x;s)\\|^{j}\\right]\\leq G_{j}^{j}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our results hold in several settings and are based on different reductions which allow us to apply strategies for DP-SCO from the uniform Lipschitz setting. ", "page_idx": 1}, {"type": "text", "text": "Near-optimal rates for heavy-tailed DP-SCO (Section 3). We design an algorithm for the $k$ - heavy-tailed DP-SCO problem, which satisfies $\\rho{\\mathrm{-}}{\\mathrm{CDP}}^{4}$ and attains near-optimal excess loss ", "page_idx": 2}, {"type": "equation", "text": "$$\nG_{2}D\\cdot\\sqrt{\\frac{\\log\\left(\\frac{1}{\\delta}\\right)}{n}}+G_{k}D\\cdot\\left(\\frac{\\sqrt{d}\\log\\left(\\frac{1}{\\delta}\\right)}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This matches the lower bounds recently proved by [KLZ22, LR23] for $\\rho$ -concentrated DP algorithms up to logarithmic factors, stated in (2). Standard conve\u221arsions from CDP to $(\\varepsilon,\\delta)$ -DP imply that our algorithm also obtains loss $\\begin{array}{r}{\\approx G_{2}D\\cdot\\sqrt{\\frac{1}{n}}+G_{k}D\\cdot(\\frac{\\sqrt{d\\log^{3}(1/\\delta)}}{n\\varepsilon})^{1-\\frac{1}{k}}}\\end{array}$ under this parameterization. We note that our bound (3) holds with high probability $\\geq1-\\delta$ , whereas the lower bound (2) is for an error which holds only in expectation (see Theorem 13, [LR23]). Our lossiness in (3) is due to a natural sample-splitting strategy used to boost our failure probability, and we conjecture that (3) may be optimal in the high-probability error bound regime. ", "page_idx": 2}, {"type": "text", "text": "As in [LR23], to establish our result we begin by deriving utility guarantees for a clipped stochastic gradient descent subroutine on an empirical loss, where clipping ensures privacy but induces bias, parameterized by a dataset-dependent quantity $b_{\\mathcal{D}}^{2}$ defined in (26). We give a standard analysis of this subroutine in Proposition 1, a variant of which (with slightly different parameterizations) also appeared as Lemma 27, [LR23]. However, the key technical barrier encountered by the [LR23] analysis, when converting to population risk, was bounding $\\mathbb{E}b_{\\mathcal{D}}^{2}$ over the sampled dataset, which na\u00efvely depends on the $2k^{\\mathrm{th}}$ moment of gradients. This either incurs an overhead depending on $G_{2k}$ , or in the absence of such a bound (which is not given under the problem statement), leads to the aforementioned suboptimal rate in [LR23] losing a factor of $\\Big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\Big)^{\\frac{1}{k}}$ in the utility. We give a further discussion of natural strategies and barriers towards directly bounding $\\mathbb{E}b_{\\mathcal{D}}^{2}$ in Appendix G. ", "page_idx": 2}, {"type": "text", "text": "Where we depart from the strategy of [LR23] is in the use of a new population-level localization framework we design (see Algorithm 2), inspired by similar localization techniques in prior work [FKT20] (discussed in more detail in Section 1.2). This strategy allows us to use constant-success probability bounds on the quantity $b_{\\mathcal{D}}$ (which also bound $b_{\\mathcal{D}}^{2}$ ), which are easy to achieve depending only on $G_{k}$ rather than $G_{2k}$ via Markov\u2019s inequality. This bypasses the need in [LR23] for bounding $\\mathbb{E}b_{\\mathcal{D}}^{\\breve{2}}$ . The motivation for population-level localization is that we wish to aggregate empirical solutions to multiple datasets, some of which have small $b_{\\mathcal{D}}$ , and others which do not. However, each dataset has a different empirical minimizer, so it is unclear how to argue about convergence if we apply the empirical localization. Instead, we aggregate solutions close to the population-level minimizer and share them across datasets via a simple geometric aggregation technique, showing that it suffices for a constant fraction of datasets to have this desirable property for us to carry out our population-level localization argument. We formally state our main result achieving the rate (3) as Theorem 1. ", "page_idx": 2}, {"type": "text", "text": "Interestingly, as a straightforward corollary of our new localization framework, we achieve a tight rate for high-probability stochastic convex optimization under a bounded-variance gradient estimator parameterization, perhaps the most well-studied formulation of SCO. To our knowledge, this result was only first achieved very recently by [CH24].5 However, we find it a promising proof-of-concept that our new framework directly yields the same result. For completeness, we include a derivation in Appendix E (see Theorem 5) as a demonstration of the utility of our framework. ", "page_idx": 2}, {"type": "text", "text": "Optimal rates with known Lipschitz constants (Appendix B). We next consider the known Lipschitz setting, where each sample function $f(\\cdot;s)$ arrives with a value $\\overline{{L}}_{s}$ which is an overestimate of its Lipschitz constant, such that $\\mathbb{E}\\overline{{L}}_{s}^{j}$ is bounded for all $j\\in[k]$ (see Assumption 2). As motivation, consider the problem of learning a generalized linear model (GLM), where $f(\\cdot;s)=\\sigma(\\langle\\cdot,s\\rangle)$ for a known convex activation function $\\sigma$ . Typically, the Lipschitz constant for $f(\\cdot;s)$ is simply the Lipschitz constant of $\\sigma$ times $\\lVert s\\rVert$ , which can be straightforwardly calculated. Thus, for GLMs, our known Lipschitz heavy-tailed assumption amounts to moment bounds on the distribution $\\mathcal{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Our second result, Theorem 2, shows a natural strategy obtains optimal rates in this known Lipschitz setting, eliminating logarithmic factors from Theorem 1. As mentioned previously, this result holds for the important family of GLMs. Our algorithm is based on a straightforward reduction to the uniformly Lipschitz setting: after simply iterating over the input samples, and replacing samples whose Lipschitz constant exceeds a given threshold with a new dummy sample, we show existing Lipschitz DP-SCO algorithms then obtain the optimal heavy-tailed excess population loss (2). Despite the simplicity of this result, to the best of our knowledge, it was not previously known. ", "page_idx": 3}, {"type": "text", "text": "Efficient algorithms for smooth functions (Appendices C and D). Finally, we propose algorithms with improved query efficiency for general smooth functions or smooth GLMs, with moderate smoothness bounds. Our strategy is to analyze the stability of clipped-DP-SGD in the smooth heavytailed setting, and use localization-based reductions to transform a stable algorithm into a private one [FKT20]. This results in linear-time algorithms for the smooth case with near-optimal rates. In order to prove the privacy of our smooth, heavy-tailed algorithm, we analyze a careful interplay of our clipped stochastic gradient method with the sparse vector technique (SVT) $[{\\mathrm{DNR}}^{+}09$ , DR14]. At a high level, our use of SVT comes from the fact that under clipping, smooth gradient steps no longer enjoy the type of contraction guarantees applicable in the uniform Lipschitz setting (see Fact 3), so we must take care to not clip too often. The SVT is then used to ensure privacy of our count of how many clipping operations were used. In Appendix F, we provide a simple counterexample showing that the noncontractiveness of contractive steps after applying clipping is inherent. Our general smooth heavy-tailed DP-SCO result is stated as Theorem 3. ", "page_idx": 3}, {"type": "text", "text": "We believe the use of SVT within an optimization algorithm to ensure privacy may be of independent interest, as it is one of few such instances that have appeared in the private optimization literature to our knowledge; it is inspired by a simpler application of this technique carried out in [AL24]. ", "page_idx": 3}, {"type": "text", "text": "On the other hand, we make the simple observation that for GLMs, clipping cannot make a contractive gradient step noncontractive, by taking advantage of the fact that the derivative of $f(x;s)=\\sigma(\\langle x,s\\rangle)$ is a multiple of $s$ for any $x\\in\\mathscr{X}$ (see Lemma 14). We use this observation to give a straightforward adaptation of the smooth algorithm in [FKT20] to the heavy-tailed setting, proving Theorem 4, which attains both a linear gradient query complexity and the optimal rate (2). ", "page_idx": 3}, {"type": "text", "text": "1.2 Prior work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The best-known rates for heavy-tailed DP-SCO were recently achieved by [KLZ22, LR23]. As discussed previously, their results do not provide the same optimality guarantees as our Theorem 1. The rate achieved by [LR23] is polynomially worse than the optimal loss (2) for any constant $k$ . On the other hand, the work of [KLZ22] uses a different assumption on the gradients than Assumption 1, which is arguably more nonstandard: in particular, they require that the $k^{\\mathrm{th}}$ -order central moments of each coordinate $\\nabla_{j}f(x;s)$ is bounded. Moreover, their algorithms require each sample function $f(\\cdot;s)$ to be $\\beta$ -smooth, and the final rates have a strong dependence on the condition number $\\begin{array}{r}{\\kappa=\\frac{\\beta}{\\lambda}}\\end{array}$ where $\\lambda$ is the strong convexity parameter (see Appendix C in [LR23] for additional discussion). ", "page_idx": 3}, {"type": "text", "text": "Our result in the heavy-tailed setting assuming $\\beta$ -smoothness of sample functions, Theorem 3, is most directly related to Theorem 15 of [LR23]. These two results respectively require ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta=O\\left(\\frac{G_{k}}{D}\\cdot\\varepsilon^{1.5}\\sqrt{\\frac{n}{d}}\\right)\\mathrm{~and~}\\beta=O\\left(\\frac{G_{k}}{D}\\cdot\\left(\\frac{d^{5}}{\\varepsilon n}\\right)^{\\frac{1}{18}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "omitting logarithmic factors in our bound for simplicity, to obtain near-optimal rates. These regimes are different and not generally comparable. However, we find it potentially useful that our upper bound on $\\beta$ grows as more samples are taken, whereas the [LR23] bound degrades with larger $n$ . It is worth mentioning that [LR23]\u2019s Theorem 15 shaves roughly one logarithmic factor in the error bound from our Theorem 3. On the other hand, Theorem 3 actually requires a looser condition than mentioned above (see (20)), which can improve its guarantees in a wider range of parameters. ", "page_idx": 3}, {"type": "text", "text": "Finally, we briefly contextualize our population-level localization framework in regard to previous localization schemes proposed by [FKT20]. The two localization schemes in [FKT20] (see Sections 4.1 and 5.1 of that work) both follow the same strategy of gradually improving distance bounds to a minimizer in phases. However, their implementation is qualitatively different than our Algorithm 2, preventing their direct application in our algorithm. For instance, Section 4.1 of [FKT20] does not use strong convexity and, therefore cannot take advantage of generalization bounds afforded to strongly convex losses (see discussion in [SSSS09]). On the other hand, the scheme in Section 5.1 of [FKT20] serves a different purpose than Algorithm 2, aiming to solve strongly convex optimization by reducing it to non-strongly convex optimization; our Algorithm 2, on the other hand, directly targets non-strongly convex optimization as its goal. We view our approach as complementary to these prior frameworks and are optimistic it will find further utility in applications. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "General notation. We use $[d]$ to denote the set $\\{i\\in\\mathbb{N}\\mid i\\leq d\\}$ . We use $\\mathrm{sign}(x)\\in\\{\\pm1\\}$ to denote the sign for $x\\in\\mathbb R$ , with $\\mathrm{sign}(0)=1$ . We use $\\mathcal{N}(\\bar{\\mu},\\Sigma)$ to denote the multivariate normal distribution of specified mean and covariance. We denote the all-ones and all-zeroes vectors of dimension $d$ by $\\mathbb{1}_{d}$ and $\\mathbb{O}_{d}$ . We use $\\left\\Vert\\cdot\\right\\Vert$ to denote the Euclidean $(\\ell_{2})$ norm. We use $\\mathbf{I}_{d}$ to denote the identity matrix on $\\mathbb{R}^{d}$ . We use $\\mathbb{B}(C)$ to denote the $\\ell_{2}$ ball of radius $C$ , and for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $\\mathbb{B}(x,C)$ is used to denote $\\{x^{\\prime}\\in\\mathbb{R}^{d}\\mid\\|x^{\\prime}-x\\|\\leq C\\}$ . For a set $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , we let $\\mathsf{d i a m}(\\mathcal X):=\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal X}\\|x-x^{\\prime}\\|$ , and we let $\\Pi_{X}(x)$ denote the Euclidean projection of $x$ to $\\mathcal{X}$ , i.e. $\\mathrm{argmin}_{x^{\\prime}\\in\\mathcal{X}}\\left\\|x^{\\prime}-x\\right\\|$ , which exists and is unique when $\\mathcal{X}$ is compact. We use $f_{\\mathcal{X}}$ to denote the restriction of a function $f$ to $\\mathcal{X}$ , i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\mathcal{X}}(x)=\\left\\{f(x)\\quad x\\in\\mathcal{X}\\right._{}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we use $\\Pi_{C}(x)$ as shorthand for $\\Pi_{\\mathbb{B}(C)}(x)$ , i.e. $\\Pi_{C}(X)$ denotes the clipped vector $x\\cdot\\operatorname*{min}({\\frac{C}{\\|x\\|}},1)$ . We say two datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ are neighboring if they differ in one entry, and $|\\mathcal{D}|=|\\mathcal{D}^{\\prime}|$ . We say $x\\in\\mathscr{X}$ is an $\\varepsilon$ -approximate minimizer to $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ if $f(x)-\\operatorname*{inf}_{x^{\\star}\\in{\\mathcal{X}}}f(x^{\\star})\\leq\\varepsilon$ . For two densities $\\mu,\\nu$ on the same probability space, and $\\alpha>1$ , we define the $\\alpha$ -R\u00e9nyi divergence ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\mu\\|\\nu):=\\frac{1}{\\alpha-1}\\log\\left(\\int\\left(\\frac{\\mu(\\omega)}{\\nu(\\omega)}\\right)^{\\alpha}\\mathrm{d}\\nu(\\omega)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For an event $\\mathcal{E}$ on a probability space clear from context, we let $\\mathbb{I}_{\\mathcal{E}}$ denote the 0-1 indicator of $\\mathcal{E}$ . We say $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ is $L$ -Lipschitz if $\\left|f(x)-f(x^{\\prime})\\right|\\le L\\left\\|x-x^{\\prime}\\right\\|$ for all $x,x^{\\prime}\\in\\mathcal{X}$ ; if $f$ is differentiable and convex, an equivalent characterization is $\\|\\nabla f(x)\\|\\leq L$ for all $x\\in\\mathscr{X}$ . We say $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ is $\\mu$ - strongly convex if $\\begin{array}{r}{f(\\lambda x^{\\prime}\\!+\\!(1\\!-\\!\\lambda)x)\\le\\lambda f(x^{\\prime})\\!+\\!(1\\!-\\!\\lambda)f(x)\\!-\\!\\frac{\\mu\\lambda(1\\!-\\!\\lambda)}{2}\\left\\|x-x^{\\prime}\\right\\|^{2}}\\end{array}$ for all $x,x^{\\prime}\\in\\mathcal{X}$ . We say differentiable $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ is $\\beta$ -smooth if for all $x,x^{\\prime}\\in\\mathcal{X}$ $\\ell,\\|\\bar{\\nabla}f(x)-\\nabla f(x^{\\prime})\\|\\leq\\beta\\,\\|x-x^{\\prime}\\|$ . ", "page_idx": 4}, {"type": "text", "text": "Differential privacy. We begin with a definition of standard differential privacy. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Differential privacy). Let $\\varepsilon~\\ge~0$ , $\\delta~\\in~[0,1]$ . We say a mechanism (randomized algorithm) $\\mathcal{M}:\\mathcal{S}^{n}\\rightarrow\\Omega$ satisfies $(\\varepsilon,\\delta)$ -differential privacy (alternatively, $\\mathcal{M}$ is $(\\varepsilon,\\delta){-}D P)$ if for any neighboring $\\mathcal{D},\\mathcal{D}^{\\prime}\\in\\mathcal{S}^{n}$ , and any $S\\subseteq\\Omega$ , $\\mathrm{Pr}\\left[\\mathcal{M}(\\bar{\\mathcal{D}})\\in\\dot{S}\\right]\\leq\\exp(\\varepsilon)\\,\\mathrm{Pr}\\left[\\mathcal{M}(\\mathcal{D}^{\\prime})\\in\\dot{S}\\right]+\\delta.$ . ", "page_idx": 4}, {"type": "text", "text": "More generally, for random variables $X,Y\\in\\Omega$ satisfying $\\operatorname*{Pr}[X\\in S]\\leq\\exp(\\varepsilon)\\operatorname*{Pr}[Y\\in S]+\\delta$ for all $S\\subseteq\\Omega$ , we say that $X,Y$ are $(\\varepsilon,\\delta)$ -indistinguishable. ", "page_idx": 4}, {"type": "text", "text": "Throughout the paper, other notions of differential privacy will frequently be useful for our accounting of privacy loss in our algorithms. For example, we define the following variants of DP. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (R\u00e9nyi DP). Let $\\alpha>1$ , $\\varepsilon\\ge0$ . We say a mechanism $\\mathcal{M}:\\mathcal{S}^{n}\\rightarrow\\Omega$ satisfies $(\\alpha,\\varepsilon)$ -R\u00e9nyi differential privacy $(R D P)$ if for any neighboring $\\mathcal{D},\\mathcal{D}^{\\prime}\\in S^{n}$ , $D_{\\alpha}({\\mathcal{M}}({\\mathcal{D}})\\|{\\mathcal{M}}({\\mathcal{D}}^{\\prime}))\\leq\\varepsilon$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (CDP). Let $\\rho\\,\\geq\\,0$ . We say a mechanism $\\mathcal{M}\\,:\\,\\mathcal{S}^{n}\\,\\rightarrow\\,\\Omega$ satisfies $\\rho$ -concentrated differential privacy (alternatively, $\\mathcal{M}$ satisfies $\\rho$ -CDP) if for any neighboring $\\mathcal{D},\\mathcal{D}^{\\prime}\\in\\mathcal{S}^{n}$ , and any $\\alpha\\geq1$ , $D_{\\alpha}\\bar{(}{\\mathcal{M}}({\\mathcal{D}})\\|{\\mathcal{M}}({\\mathcal{D}}^{\\prime}))\\leq\\bar{\\alpha}\\rho$ . ", "page_idx": 4}, {"type": "text", "text": "For an extended discussion of RDP and CDP and their properties, we refer the reader to [BS16, Mir17, BDRS18]. We summarize the main facts about these notions we use here. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 ([Mir17]). RDP has the following properties. ", "page_idx": 4}, {"type": "text", "text": "1. (Composition): Let $\\mathcal{M}_{1}:\\mathcal{S}^{n}\\to\\Omega$ satisfy $(\\alpha,\\varepsilon_{1})$ -RDP and $\\mathcal{M}_{2}:S^{n}\\times\\Omega\\rightarrow\\Omega^{\\prime}$ satisfy $\\left(\\alpha,\\varepsilon_{2}\\right)$ -RDP for any input in $\\Omega$ . Then the composition of $\\mathcal{M}_{2}$ and $\\mathcal{M}_{1}$ , i.e. the randomized algorithm which takes $\\mathcal{D}$ to ${\\mathcal{M}}_{2}({\\mathcal{D}},{\\mathcal{M}}_{1}({\\mathcal{D}}))$ , satisfies $(\\alpha,\\varepsilon_{1}+\\varepsilon_{2}){-}R D P.$ 2. (RDP to ${\\cal D}P$ ): If $\\mathcal{M}$ satisfies $(\\alpha,\\varepsilon)$ -RDP, it satisfies $\\begin{array}{r}{(\\varepsilon+\\frac{1}{\\alpha-1}\\log\\frac{1}{\\delta},\\delta)}\\end{array}$ -DP for all $\\delta\\in(0,1)$ . ", "page_idx": 4}, {"type": "text", "text": "3. (Gaussian mechanism): Let $f:S^{n}\\to\\mathbb{R}^{d}$ be an $L$ -sensitive randomized function for $L\\geq0$ , i.e. for any neighboring $\\mathcal{D}$ , $\\mathcal{D}^{\\prime}$ , we have $\\|f(\\ensuremath{\\mathcal{D}})-f(\\ensuremath{\\mathcal{D}}^{\\prime})\\|\\leq L.$ . Then for any $\\sigma>0$ , the mechanism which outputs $f(\\mathcal{D})+\\xi$ for $\\boldsymbol{\\xi}\\sim\\mathcal{N}(\\mathbb{O}_{d},\\sigma^{2}\\mathbf{I}_{d})$ satisfies ${\\frac{L^{2}}{2\\sigma^{2}}}{\\mathrm{-}}C D P.$ ", "page_idx": 5}, {"type": "text", "text": "Private SCO. Throughout the paper, we study the problem of private stochastic convex optimization (SCO) with heavy-tailed gradients. We first define the assumptions used in our algorithms. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 $k$ -heavy-tailed distributions). Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be a compact, convex set. Let $\\mathcal{P}$ be a distribution over a sample space $\\boldsymbol{S}$ , such that each $s\\ \\in{\\cal S}$ induces a continuously-differentiable, convex, $L_{s}$ -Lipschitz loss function $f(\\cdot;s):\\mathcal{X}\\to\\mathbb{R},^{6}$ where $L_{s}:=\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\nabla f(x;s)\\|$ is unknown. For $k\\in\\mathbb N$ satisfying $k\\geq2$ , we say $\\mathcal{P}$ satisfies the $k$ -heavy tailed assumption $i f,$ , for a sequence of monotonically nondecreasing $\\{G_{j}\\}_{j\\in[k]},$ we have $\\mathbb{E}_{s\\sim\\mathcal{P}}[L_{s}^{j}]\\le G_{j}^{j}<\\infty$ for all $j\\in[k]$ . ", "page_idx": 5}, {"type": "text", "text": "In Appendix B, we consider a variant of Assumption 1 where we have explicit access to upper bounds on the Lipschitz constants $L_{s}$ , formalized in Assumption 2. Our goal is to approximately optimize a population loss over sample functions satisfying Assumptions 1 or 2, formalized in the following. ", "page_idx": 5}, {"type": "text", "text": "Definition 4 ( $k$ -heavy-tailed private SCO). In the $k$ -heavy-tailed private SCO problem, $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is a compact, convex set with diam $(\\boldsymbol{\\mathcal{X}})=\\boldsymbol{D}$ . Further, $\\mathcal{P}$ is a distribution over a sample space $\\boldsymbol{S}$ satisfying Assumption 1. Our goal is to design an algorithm which provides an approximate minimizer in expectation to the population loss, $F_{\\mathcal{P}}(x):=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)\\right]$ , subject to satisfying differential privacy. We say such an algorithm queries $N$ sample gradients $i f$ it queries $\\nabla f(x;s)$ for $N$ different pairs $(x,s)\\in\\mathcal{X}\\times\\mathcal{S}$ . If $\\mathcal{P}$ further satisfies Assumption 2, we call the corresponding problem the known Lipschitz $k$ -heavy-tailed private SCO problem. ", "page_idx": 5}, {"type": "text", "text": "We first observe the following consequence of Assumption 1, deferring a proof to Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ satisfying Assumption 1. Then $F_{\\mathcal{P}}$ is $G_{1}$ -Lipschitz.   \nWe require the following claim which bounds the bias of clipped heavy-tailed distributions. ", "page_idx": 5}, {"type": "text", "text": "Fact 1 ([BD14], Lemma 3). Let $k>1$ and $X\\in\\mathbb{R}^{d}$ be a random vector with $\\mathsf{E}[\\|X\\|^{k}]\\leq G^{k}$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\Pi_{C}(X)-X\\right\\|\\leq\\mathbb{E}[\\|X\\|\\,\\mathbb{I}_{\\|X\\|\\geq C}]\\leq\\frac{G^{k}}{(k-1)C^{k-1}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We also use the following standard claim on geometric aggregation. ", "page_idx": 5}, {"type": "text", "text": "Fact 2 $[\\mathrm{KLL}^{+}23]$ , Claim 1). Let $S:=\\{x_{i}\\}_{i\\in[k]}\\subset\\mathbb{R}^{d}$ have the property that for (unknown) $z\\in\\mathbb{R}^{d}$ , $|\\{i\\in[k]\\mid\\|x_{\\underline{{i}}}-z\\|\\leq R\\}|\\geq0.51k$ for some $R\\geq0.$ . There is an algorithm Aggregate which runs in time $O(d k^{2})$ and outputs $x\\in S$ such that $\\lVert x-z\\rVert\\leq3R$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, given a dataset $\\mathcal{D}\\in S^{*}$ of arbitrary size, and $\\lambda\\geq0$ , we use the following shorthand to denote the regularized empirical risk minimization (ERM) objective corresponding to the dataset: ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\mathcal{D},\\lambda}(\\boldsymbol{x}):=\\frac{1}{|\\mathcal{D}|}\\sum_{\\boldsymbol{s}\\in\\mathcal{D}}f(\\boldsymbol{x};\\boldsymbol{s})+\\frac{\\lambda}{2}\\left\\|\\boldsymbol{x}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $\\lambda=0$ , we simply denote the function above by $\\begin{array}{r}{F_{\\mathcal{D}}(\\boldsymbol{x}):=\\frac{1}{|\\mathcal{D}|}\\sum_{\\boldsymbol{s}\\in\\mathcal{D}}f(\\boldsymbol{x};\\boldsymbol{s})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "3 Heavy-Tailed Private SCO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we obtain near-optimal algorithms for the problem in Definition 4 using a new population-level localization framework, combined with geometric aggregation for boosting weak subproblem solvers to succeed with high probability (Fact 2). Our algorithm\u2019s main ingredient, in Section 3.1, is a clipped DP-SGD subroutine for privately minimizing a regularized ERM subproblem, under a condition on a randomly sampled dataset holding with constant probability. Next, in Section 3.2 we show that our algorithm from Section 3.1 returns points near the minimizer of a regularized loss function over the population, using generalization arguments. Finally, we develop our population-level localization scheme in Section 3.3, and combine it with our subproblem solver to give our overall method for heavy-tailed private SCO. Several proofs and a generalization to strongly convex functions (Corollary 3) are deferred to Appendix A. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.1 Strongly convex DP-ERM solver ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We give a parameterized subroutine for minimizing a DP-ERM objective $F_{\\mathit{D},\\lambda}(x)$ associated with a dataset $\\mathcal{D}$ and a regularization parameter $\\lambda\\geq0$ (recalling the definition (5)). In this section only, for notational convenience we identify elements of $\\mathcal{D}$ with $[n]$ where $n:=|\\mathcal{D}|$ , so we will also write ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\mathscr{D},\\lambda}(x):=\\frac{1}{n}\\sum_{i\\in[n]}f_{i}(x)+\\frac{\\lambda}{2}\\left\\|x\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "i.e. we let $f_{i}(\\cdot):=f(\\cdot;s)$ where $s\\in\\mathcal{D}$ is the element identified with $i\\in[n]$ . Our subroutine is a clipped DP-SGD algorithm (Algorithm 1), which only clips the heavy-tailed portion of $\\nabla F_{\\mathcal{D},\\lambda}$ (i.e. the sample gradients), and leaves both the regularization and additive noise unchanged. The utility of Algorithm 1 is parameterized by the following function of the dataset: ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{\\mathcal{D}}:=\\operatorname*{max}_{x\\in\\mathcal{X}}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\nabla f_{i}(x)-\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x))\\right\\|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In other words, $b_{\\mathcal{D}}$ denotes the maximum bias incurred by the clipped gradient of $F_{\\mathcal{D}}$ when compared to the true gradient, over points in $\\mathcal{X}$ ; note the maximum is achieved as $\\mathcal{X}$ is compact. ", "page_idx": 6}, {"type": "text", "text": "We are now ready to state our algorithm, Clipped-DP-SGD, as Algorithm 1. ", "page_idx": 6}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\mathbf{Algorithm\\1!:Clipped-DP-SGD}(\\mathcal{D},C,\\lambda,\\{\\eta_{t}\\}_{t\\in[T]},\\sigma^{2},T,r,\\mathcal{X})$   \n1 Input: Dataset $\\mathcal{D}\\in\\mathcal{S}^{n}$ , clip threshold $C\\in\\mathbb{R}_{\\geq0}$ , regularization $\\lambda\\in\\mathbb{R}_{\\geq0}$ , step sizes $\\{\\eta_{t}\\}_{t\\in[T]}\\subset\\mathbb{R}_{\\geq0}$ , noise $\\sigma^{2}\\in\\mathbb{R}_{\\geq0}$ , iteration count $T\\in\\mathbb N$ , radius $r\\in\\mathbb{R}_{\\geq0}$ , domain ${\\mathcal{X}}\\subset\\mathbb{B}(r)$ with $\\mathcal{X}\\ni\\mathbb{0}_{d}$   \n2 $x_{0}\\gets\\mathbb{0}_{d}$   \n3 for $0\\leq t<T$ do   \n4 $\\xi_{t}\\sim\\mathcal{N}(\\mathbb{O}_{d},\\sigma^{2}\\mathbf{I}_{d})$   \n5 $\\begin{array}{r l}&{\\hat{\\hat{g}}_{t}\\gets\\frac{1}{n}\\sum_{i\\in[n]}^{\\circ}\\overleftarrow{\\Pi}_{C}^{\\prime}(\\nabla f_{i}(x_{t}))}\\\\ &{x_{t+1}\\gets\\arg\\!\\operatorname*{min}_{x\\in\\mathcal{X}_{r}}\\{\\eta_{t}\\left\\langle\\hat{g}_{t}+\\xi_{t},x\\right\\rangle+\\frac{\\eta_{t}\\lambda}{2}\\left\\|x\\right\\|^{2}+\\frac{1}{2}\\left\\|x-x_{t}\\right\\|^{2}\\}}\\end{array}$   \n6   \n7 end   \n8 Return: $\\begin{array}{r}{\\hat{x}\\gets\\frac{\\sum_{0\\leq t<T}(t+4)x_{t}}{\\sum_{0\\leq t<T}(t+4)}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "We provide the following guarantee on Clipped-DP-SGD, by modifying an analysis of [LSB12]. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Let $\\rho\\,\\geq\\,0$ , and $\\hat{x}$ be the output of Clipped-DP-SGD with $\\begin{array}{r}{\\eta_{t}\\gets\\frac{4}{\\lambda(t+1)}}\\end{array}$ for all 0 \u2264t < T, \u03c32 \u21902C22T , and $\\begin{array}{r}{T\\ge\\operatorname*{max}(n,\\frac{n^{2}\\rho}{d})}\\end{array}$ . Clipped-DP-SGD satisfies $\\rho$ -CDP, and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\mathcal{D},\\lambda}(\\lambda)-F_{\\mathcal{D},\\lambda}(x^{\\star})]\\leq\\frac{32C^{2}d}{\\lambda n^{2}\\rho}+\\frac{b_{\\mathcal{D}}^{2}}{\\lambda}+\\frac{7\\lambda r^{2}}{n},\\;w h e r e\\;x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{D},\\lambda}(x).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For ease of use of Proposition 1, we now provide a simple bound on $b_{\\mathcal{D}}$ which holds with constant probability from a dataset drawn from a distribution satisfying Assumption 1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. Let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption 1. With probability at least $\\frac{4}{5}$ , denoting $b_{\\mathcal{D}}$ as in (26), we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{\\mathscr D}\\leq\\frac{5G_{k}^{k}}{(k-1)C^{k-1}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We therefore have the following corollary of Proposition 1 and Lemma 3. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption 1, and let $x_{\\mathcal{D},\\lambda}^{\\star}:=$ a $\\begin{array}{r}{\\mathrm{rgmin}_{x\\in\\mathcal{X}}\\,F_{\\mathcal{D},\\lambda}(x),}\\end{array}$ , following (5). If we run Clipped-DP-SGD with parameters in Proposition $^{\\,l}$ and $C\\;\\leftarrow\\;G_{k}\\;\\cdot\\;(\\frac{25n^{2}\\rho}{32d})^{\\frac{1}{2k}}$ , Clipped-DP-SGD is $\\rho$ -CDP, and there is a universal constant $C_{\\mathrm{erm}}$ such that with probability $\\textstyle\\geq{\\frac{3}{5}}$ over the randomness of $\\mathcal{D}$ and Clipped-DP-SGD, $\\hat{x}$ , the output of Clipped-DP-SGD, satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{x}}-x_{\\mathcal{D},\\lambda}^{\\star}\\right\\|\\leq C_{\\mathrm{erm}}\\left(\\frac{G_{k}}{\\lambda}\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}+\\frac{r}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Clipped-DP-SGD queries at most $\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})$ sample gradients (using samples in $\\mathcal{D}$ ). ", "page_idx": 7}, {"type": "text", "text": "Proof. Condition on the conclusion of Lemma 3, which holds with probability $\\frac{4}{5}$ . Therefore, Markov\u2019s inequality shows that with probability at least $\\frac{3}{5}$ , after a union bound with Proposition 1, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\lambda}{2}\\left\\|\\hat{x}-x_{{\\cal D},\\lambda}^{\\star}\\right\\|^{2}\\le F_{{\\cal D},\\lambda}(\\hat{x})-F_{{\\cal D},\\lambda}(x_{{\\cal D},\\lambda}^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{160C^{2}d}{\\lambda n^{2}\\rho}+\\frac{125G_{k}^{2k}}{\\lambda C^{2(k-1)}}+\\frac{7\\lambda r^{2}}{n}\\le\\displaystyle\\frac{320G_{k}^{2}}{\\lambda}\\left(\\frac{d}{n^{2}\\rho}\\right)^{1-\\frac{1}{k}}+\\frac{7\\lambda r^{2}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we used strong convexity in the first inequality, and plugg\u221aed in our choice of $C$ in the last. The conclusion follows by rearranging the above display, and using ${\\sqrt{a^{2}+b^{2}}}\\leq a\\!+\\!b$ for $a,b\\in\\mathbb{R}_{\\geq0}$ . ", "page_idx": 7}, {"type": "text", "text": "3.2 Localizing regularized population loss minimizers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we use generalization arguments from the SCO literature to show how that our algorithm Clipped-DP-SGD from Section 3.1 acts as an oracle which, with constant probability, returns a point near the minimizer of a regularized population loss. We begin with a standard helper statement. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4. Let $\\lambda\\geq0$ , let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ , let $\\bar{x}\\,\\in\\,\\mathcal{X}$ where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is compact and convex, and let ", "page_idx": 7}, {"type": "equation", "text": "$$\nx_{\\lambda,\\bar{x}}^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}\\left\\lbrace F_{\\mathcal{P}}(x)+\\frac{\\lambda}{2}\\left\\|x-\\bar{x}\\right\\|^{2}\\right\\rbrace,\\ w h e r e\\ F_{\\mathcal{P}}(x):=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then $\\begin{array}{r}{\\|\\bar{x}-x_{\\lambda,\\bar{x}}^{\\star}\\|\\leq\\frac{2G_{1}}{\\lambda}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Next, we apply a result on generalization due to [LR23] to bound the expected distance between a restricted empirical regularized minimizer and the minimizer of the population variant in (7). ", "page_idx": 7}, {"type": "text", "text": "Lemma 5. Let $\\lambda\\geq0,$ , let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ , and let ${\\bar{x}}\\in{\\mathcal{X}}$ where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is compact and convex. Following notation (4), (5), let ", "page_idx": 7}, {"type": "equation", "text": "$$\ny:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}\\left\\{\\left[F_{\\mathcal{D}}\\right]_{\\mathbb{B}(\\bar{x},r)}\\left(x\\right)+\\frac{\\lambda}{2}\\left\\Vert x-\\bar{x}\\right\\Vert^{2}\\right\\},\\;\\,f o r\\;r:=\\frac{2G_{1}}{\\lambda}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and let $x_{\\lambda,{\\bar{x}}}^{\\star}$ be defined as in (7). Then with probability $\\ge0.95$ over the randomness of $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|y-x_{\\lambda,{\\bar{x}}}^{\\star}\\right\\|_{2}\\leq{\\frac{90G_{2}}{\\lambda{\\sqrt{n}}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 2. Let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ , and let $\\bar{x}\\in\\mathcal{X}$ where $\\dot{\\mathcal{X}}\\subset\\mathbb{R}^{d}$ is compact and convex. Let $\\lambda\\geq0$ and define $x_{\\lambda,{\\bar{x}}}^{\\star}$ as in (7). There is a $\\rho{-}C D P$ algorithm A which queries max(n2, nd\u03c1 ) sample gradients (using samples in $\\mathcal{D}$ ). With probability 0.55 over the randomness of A and D, A returns x\u02c6 satisfying, for a universal constant Creg-pop, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\hat{x}-x_{\\lambda,\\bar{x}}^{\\star}\\right\\|\\leq C_{\\mathrm{reg-pop}}\\left(\\frac{G_{k}}{\\lambda}\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}+\\frac{G_{2}}{\\lambda\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. Condition on the conclusion of Lemma 5 holding for our dataset, which loses 0.05 in the failure probability. Next, consider the guarantee of Corollary 1, when applied to the truncated and shifted functions, $\\tilde{f}(x;s)\\,\\gets\\,f_{\\mathbb{B}(\\bar{x},r)}(x-\\bar{x};s)$ , where $r$ is set as in Lemma 5. It shows that with probability $\\frac{3}{5}$ , $\\begin{array}{r}{\\|\\hat{x}+\\bar{x}-y\\|=O(\\frac{G_{k}}{\\lambda}(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}})^{1-\\frac{1}{k}}+\\frac{\\sqrt{\\lambda}r}{\\sqrt{n}})}\\end{array}$ , for the point $\\hat{x}$ returned by the algorithm, and $y$ the exact minimizer of the empirical loss restricted to $\\mathbb{B}(\\bar{x},r)$ . Therefore, the conclusion follows by overloading $\\hat{x}\\gets\\hat{x}+\\bar{x}$ , applying the triangle inequality with the conclusions of Corollary 1 and 2, and taking a union bound over their failure probabilities. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "3.3 Population-level localization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide a generic population-level localization scheme for stochastic convex optimization, which may be of broader interest. Our localization scheme is largely patterned off of the analogous localization methods developed by [FKT20], but directly argues about contraction to population-level regularized minimizers (as opposed to empirical minimizers), which makes it compatible with our framework in Section 3.1 and 3.2, specificially the guarantees of Corollary 2. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 2: Population-Localiz $\\mathsf{z}(x_{0},\\mathcal{P},\\lambda,I)$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "1 Input: Initial point $x_{0}\\in\\mathcal{X}$ , distribution $\\mathcal{P}$ over samples in $\\boldsymbol{S}$ , for $\\mathcal{X},\\mathcal{S}$ inducing a $k$ -heavy-tailed DP-SCO problem as in Definition 4, with a population loss $F_{\\mathcal{P}}:=\\mathbb{E}_{s\\sim s}[f(\\cdot;s)]$ , $\\lambda\\geq0$ , $I\\in{\\mathbb N}$   \n2 for $i\\in[I]$ do   \n3\u2014 $\\lambda_{i}\\leftarrow\\lambda\\cdot32^{i}$   \n4 $x_{i}\\gets$ any point satisfying $\\left\\Vert x_{i}-x_{i}^{\\star}\\right\\Vert\\leq\\frac{\\Delta4^{i}}{\\lambda_{i}},\\mathrm{~where~}x_{i}^{\\star}:=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\{F_{\\mathcal{P}}(x)+\\frac{\\lambda_{i}}{2}\\left\\Vert x-x_{i-1}\\right\\Vert^{2}\\right\\}$ (8) ", "page_idx": 8}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5 end ", "page_idx": 8}, {"type": "text", "text": "6 Return: $x_{I}$ ", "page_idx": 8}, {"type": "text", "text": "We briefly discuss the role of the hyperparameters $\\lambda,\\Delta$ in Algorithm 2 for clarity. The parameter $\\Delta$ scales with the error guarantee of our regularized ERM solver; in particular, it will be determined by the bound in Corollary 2. The parameter $\\lambda$ specifies an initial regularization amount that will later be tuned to trade off the terms in the following Proposition 2. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2. Following notation of Algorithm 2, let $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x)}\\end{array}$ . Then, ", "page_idx": 8}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\le\\frac{G_{1}\\Delta}{\\lambda8^{I}}+\\frac{\\Delta^{2}}{4\\lambda}+\\frac{\\lambda D^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In particular, choosing $\\lambda$ to optimize this bound, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\leq2D\\sqrt{\\frac{G_{1}\\Delta}{8^{I}}}+D\\Delta.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof. We denote $x_{0}^{\\star}:=x^{\\star}$ throughout the proof. First, we expand ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x_{0}^{\\star})=F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x_{I}^{\\star})+F_{\\mathcal{P}}(x_{I}^{\\star})-F_{\\mathcal{P}}(x_{0}^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x_{I}^{\\star})+\\displaystyle\\sum_{i\\in[I]}F_{\\mathcal{P}}(x_{i}^{\\star})-F_{\\mathcal{P}}(x_{i-1}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Moreover, for each $i\\in[I]$ , since $\\boldsymbol{x}_{i}^{\\star}$ minimizes $\\begin{array}{r}{F_{\\mathcal{P}}(x)+\\frac{\\lambda_{i}}{2}\\left\\Vert x-x_{i-1}\\right\\Vert^{2}}\\end{array}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x_{i}^{\\star})\\leq F_{\\mathcal{P}}(x_{i}^{\\star})+\\frac{\\lambda_{i}}{2}\\left\\|x_{i}^{\\star}-x_{i-1}\\right\\|^{2}\\leq F_{\\mathcal{P}}(x_{i-1}^{\\star})+\\frac{\\lambda_{i}}{2}\\left\\|x_{i-1}^{\\star}-x_{i-1}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining the above two displays, and using that $F_{\\mathcal{P}}$ is $G_{1}$ -Lipschitz (Lemma 2), we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\leq G_{1}\\left\\|x_{I}-x_{I}^{\\star}\\right\\|+\\displaystyle\\sum_{i\\in[I]}\\frac{\\lambda_{i}}{2}\\left\\|x_{i-1}^{\\star}-x_{i-1}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{G_{1}\\Delta}{\\lambda8^{I}}+\\sum_{i\\in[I-1]}\\displaystyle\\frac{\\Delta^{2}16^{i}}{2\\lambda_{i}}+\\displaystyle\\frac{\\lambda D^{2}}{2}\\leq\\displaystyle\\frac{G_{1}\\Delta}{\\lambda8^{I}}+\\displaystyle\\frac{\\Delta^{2}}{4\\lambda}+\\displaystyle\\frac{\\lambda D^{2}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where we used the diameter bound assumption diam $(\\mathcal{X})=D$ , as in Definition 4. ", "page_idx": 9}, {"type": "text", "text": "In particular, note that Corollary 2 shows that by using $n$ samples from $\\mathcal{P}$ and a C\u221aDP budget of $\\rho$ , with constant probability, we can satisfy the requirement (8) with $\\begin{array}{r}{\\Delta4^{i}=O(G_{k}(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}})^{1-\\frac{1}{k}}+\\frac{G_{2}}{\\sqrt{n}})}\\end{array}$ By plugging this guarantee into the aggregation subroutine in Fact 2, we have our SCO algorithm. ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\bf\\overline{{A l g o r i t h m}}}\\:3\\colon\\mathsf{A g g r e g a t e\\!-\\!E R M}(\\bar{x},\\lambda,J,\\rho,\\{s_{\\ell}\\}_{\\ell\\in[n J]},R)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "1 Input: Regularization center ${\\bar{x}}\\in{\\mathcal{X}}$ , regularization $\\lambda\\in\\mathbb{R}_{\\geq0}$ , sample split parameter $J\\in\\mathbb{N}$ , privacy parameter $\\rho\\in\\mathbb{R}_{\\geq0}$ , samples $\\{s_{\\ell}\\}_{\\ell\\in[n J]}\\subset{\\cal S}$ , distance bound $R\\in\\mathbb{R}_{\\geq0}$   \n2 for $j\\in[J]$ do   \n3 ${\\cal D}^{j}\\leftarrow\\{s_{\\ell}\\}_{(j-1)n<\\ell\\leq j n}$ for all $j\\in[J]$   \n4 $x^{j}\\gets$ result of Corollary 2 using $\\mathcal{D}^{j}$ , on loss defined by $\\bar{x},\\lambda$ with privacy parameter $\\rho$ , i.e., $x^{j}$ is a point satisfying, with probability 0.55, for a universal constant $C_{\\mathrm{reg-pop}}$ , $\\left\\|x^{j}-x_{\\lambda,\\bar{x}}^{\\star}\\right\\|\\leq C_{\\mathrm{reg-pop}}\\left(\\frac{G_{k}}{\\lambda}\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}+\\frac{G_{2}}{\\lambda\\sqrt{n}}\\right)$ ", "page_idx": 9}, {"type": "text", "text": "5 end ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "6 x \u2190Aggregate $\\left\\{x^{j}\\right\\}_{j\\in[J]},R)$ (see Fact 2) ", "page_idx": 9}, {"type": "text", "text": "7 Return: $x$ ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Theorem 1. Consider an instance of $k$ -heavy-tailed private SCO, following notation in Definition 4, let $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x),}\\end{array}$ , and let $\\rho\\geq0$ , $\\delta\\in(0,1)$ . Algorithm 2 using Algorithm 3 in Line 5 is a $\\rho$ -CDP algorithm which draws $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , queries $\\begin{array}{r}{C_{\\mathrm{sco}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})}\\end{array}$ sample gradients (using samples in $\\mathcal{D}$ ) for a universal constant $C_{\\mathrm{sco}}$ , and outputs $x\\in\\mathscr{X}$ satisfying, with probability $\\geq1-\\delta$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\le C_{\\mathrm{sco}}\\left(G_{k}D\\cdot\\left(\\frac{\\sqrt{d}\\log\\left(\\frac1\\delta\\right)}{n\\sqrt{\\rho}}\\right)^{1-\\frac1k}+G_{2}D\\cdot\\sqrt{\\frac{\\log\\left(\\frac1\\delta\\right)}n}\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we consider the DP-SCO with heavy-tailed gradients. When the $k$ -th moments of gradients are bounded, we propose the population-level localization framework and attain nearoptimal excess loss $\\begin{array}{r}{G_{2}D\\cdot\\sqrt{\\frac{\\log(\\frac{1}{\\delta})}{n}}+G_{k}D\\cdot(\\frac{\\sqrt{d}\\log(\\frac{1}{\\delta})}{n\\sqrt{\\rho}})^{1-\\frac{1}{k}}}\\end{array}$ ( d nlo\u221ag\u03c1( \u03b41 ))1\u2212k1 with probability at least 1\u2212\u03b4 and satisfy $\\rho$ -CDP. We can achieve a tight rate for high-probability SCO under a bounded-variance gradient estimator parameterization by applying the population-level localization framework. Moreover, we improve this basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models, with interesting techniques adapted to each setting. ", "page_idx": 9}, {"type": "text", "text": "It leaves many intriguing open problems in this direction. For example, can we design near-linear time algorithms for non-smooth functions? Can the population-level localization framework be applied to solve other problems? Can we establish a high-probability lower bound or eliminate the additional logarithmic term if we are only concerned with the excess bound in expectation? Can we evaluate the algorithm\u2019s performance through numerical simulations or real-world datasets? We leave these questions for future research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "$[\\mathrm{ACG}^{+}16]$ Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 23rd Annual ACM Conference on Computer and Communications Security (CCS), pages 308\u2013318, 2016.   \n$[\\mathbf{ACJ}^{+}21]$ Hilal Asi, Yair Carmon, Arun Jambulapati, Yujia Jin, and Aaron Sidford. Stochastic bias-reduced gradient methods. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, pages 10810\u2013 10822, 2021.   \n$[\\mathrm{ADF}^{+}21]$ Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient methods for convex optimization. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 383\u2013392, 2021.   \n[AFKT21] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal rates in $\\ell_{1}$ geometry. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [AL24] Hilal Asi and Daogao Liu. User-level differentially private stochastic convex optimization: Efficient algorithms with optimal rates. In International Conference on Artificial Intelligence and Statistics, 2024, volume 238 of Proceedings of Machine Learning Research, pages 4240\u20134248. PMLR, 2024.   \n[ALD21] Hilal Asi, Daniel Levy, and John Duchi. Adapting to function difficulty and growth conditions in private optimization. In Proceedings of the 34nd Annual Conference on Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 19069\u201319081, 2021. [BD14] Rina Foygel Barber and John C. Duchi. Privacy: A few definitional aspects and consequences for minimax mean-squared error. In 53rd IEEE Conference on Decision and Control, CDC 2014, pages 1365\u20131369. IEEE, 2014.   \n[BDFS07] Atanu Biswas, Sujay Datta, Jason P Fine, and Mark R Segal. Statistical advances in the biomedical science. (No Title), 2007.   \n[BDRS18] Mark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated CDP. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 74\u201386. ACM, 2018.   \n[BFTT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex optimization with optimal rates. In Proceedings of the 32nd Annual Conference on Advances in Neural Information Processing Systems (NeurIPS), pages 11282\u201311291, 2019. [BGN21] Raef Bassily, Cristobal Guzman, and Anupama Nandi. Non-euclidean differentially private stochastic convex optimization. arXiv:2103.01278 [cs.LG], 2021. [BS16] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography - 14th International Conference, TCC 2016-B, Proceedings, Part I, volume 9985 of Lecture Notes in Computer Science, pages 635\u2013658, 2016. [CH24] Yair Carmon and Oliver Hinder. The price of adaptivity in stochastic convex optimization. CoRR, abs/2402.10898, 2024. $[\\mathbf{C}\\mathbf{J}\\mathbf{J}^{+}23]$ Yair Carmon, Arun Jambulapati, Yujia Jin, Yin Tat Lee, Daogao Liu, Aaron Sidford, and Kevin Tian. Resqueing parallel and private stochastic convex optimization. In 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023, pages 2031\u20132058. IEEE, 2023.   \n[DDXZ21] Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang. From low probability to high confidence in stochastic convex optimization. J. Mach. Learn. Res., 22:49:1\u2013 49:38, 2021.   \n$[\\mathrm{DNR}^{+}09]$ Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009, pages 381\u2013390. ACM, 2009. [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3 & 4):211\u2013407, 2014. [FKT20] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proceedings of the 52nd Annual ACM on the Theory of Computing, pages 439\u2013449, 2020.   \n$[\\mathrm{GLL}^{+}23]$ Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, and Kevin Tian. Private convex optimization in general norms. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 5068\u20135089. SIAM, 2023. [HRS16] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In ICML, pages 1225\u20131234, 2016. [HS16] Daniel J. Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. J. Mach. Learn. Res., 17:18:1\u201318:40, 2016. [IIW15] Marat Ibragimov, Rustam Ibragimov, and Johan Walden. Heavy-tailed distributions and robustness in economics and finance, volume 214. Springer, 2015. [JST24] Arun Jambulapati, Aaron Sidford, and Kevin Tian. Closing the computational-query depth gap in parallel stochastic convex optimization. In The Thirty Seventh Annual Conference on Learning Theory, COLT 2024, Proceedings of Machine Learning Research. PMLR, 2024. [KLL21] Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth empirical risk minimization and stochastic convex optimization in subquadratic steps. arXiv preprint arXiv:2103.15352, 2021.   \n$[\\mathrm{KLL}^{+}23]$ Jonathan A. Kelner, Jerry Li, Allen X. Liu, Aaron Sidford, and Kevin Tian. Semirandom sparse recovery in nearly-linear time. In The Thirty Sixth Annual Conference on Learning Theory, COLT 2023, volume 195 of Proceedings of Machine Learning Research, pages 2352\u20132398. PMLR, 2023.   \n[KLZ22] Gautam Kamath, Xingtu Liu, and Huanyu Zhang. Improved rates for differentially private stochastic convex optimization with heavy-tailed data. In Proceedings of the 39th International Conference on Machine Learning (ICML), pages 10633\u201310660, 2022. [Lia24] Jiaming Liang. Variance reduction and low sample complexity in stochastic optimization via proximal point method. CoRR, abs/2402.08992, 2024. [LR23] Andrew Lowy and Meisam Razaviyayn. Private stochastic optimization with large worst-case lipschitz parameter: Optimal rates for (non-smooth) convex losses and extension to non-convex losses. In International Conference on Algorithmic Learning Theory, volume 201 of Proceedings of Machine Learning Research, pages 986\u20131054. PMLR, 2023. [LSB12] Simon Lacoste-Julien, Mark Schmidt, and Francis R. Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method. CoRR, abs/1212.2002, 2012. [Mir17] Ilya Mironov. R\u00e9nyi differential privacy. In 30th IEEE Computer Security Foundations Symposium, CSF 2017, Santa Barbara, CA, USA, August 21-25, 2017, pages 263\u2013275. IEEE Computer Society, 2017. [MM97] Benoit B Mandelbrot and Benoit B Mandelbrot. The variation of certain speculative prices. Springer, 1997. [Sch14] Rolf Schneider. Convex bodies: the Brunn\u2013Minkowski theory. Number 151. Cambridge university press, 2014. [SSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In COLT 2009 - The 22nd Conference on Learning Theory, 2009.   \n[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In Proceedings of the Twenty Second Annual Conference on Computational Learning Theory, 2009. [SZ23] Aaron Sidford and Chenyi Zhang. Quantum speedups for stochastic optimization. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[WXDX20] Di Wang, Hanshen Xiao, Srinivas Devadas, and Jinhui Xu. On differentially private stochastic convex optimization with heavy-tailed data. In Proceedings of the 37th International Conference on Machine Learning (ICML), pages 10081\u201310091, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Deferred proofs from the main body ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Deferred proofs from Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 2. Let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ satisfying Assumption 1. Then $F_{\\mathcal{P}}$ is $G_{1}$ -Lipschitz. ", "page_idx": 13}, {"type": "text", "text": "Proof. This follows from the derivation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\nabla f(x;s)\\right]\\|\\leq\\operatorname*{max}_{x\\in\\mathcal{X}}\\mathbb{E}_{s\\sim\\mathcal{P}}\\left\\|\\nabla f(x;s)\\right\\|\\leq\\mathbb{E}_{s\\sim\\mathcal{P}}\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\nabla f(x;s)\\|\\leq G_{1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Deferred proofs from Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 1. Let $\\rho\\,\\geq\\,0$ , and $\\hat{x}$ be the output of Clipped-DP-SGD with $\\begin{array}{r}{\\eta_{t}\\gets\\frac{4}{\\lambda(t+1)}}\\end{array}$ for all 0 \u2264t < T, \u03c32 \u21902C22T , and $\\begin{array}{r}{T\\ge\\operatorname*{max}(n,\\frac{n^{2}\\rho}{d})}\\end{array}$ . Clipped-DP-SGD satisfies $\\rho$ -CDP, and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\mathcal{D},\\lambda}(\\lambda)-F_{\\mathcal{D},\\lambda}(x^{\\star})]\\leq\\frac{32C^{2}d}{\\lambda n^{2}\\rho}+\\frac{b_{\\mathcal{D}}^{2}}{\\lambda}+\\frac{7\\lambda r^{2}}{n},\\;w h e r e\\;x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{D},\\lambda}(x).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. For the privacy claim, note that each call to Line 3 is a postprocessing of a $\\frac{2C}{n}$ -sensitive statistic of the dataset $\\mathcal{D}$ , since neighboring databases can only change $\\begin{array}{r}{\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x_{t}))}\\end{array}$ by $\\frac{2C}{n}$ in the $\\ell_{2}$ norm, via the triangle inequality. Therefore, applying the first and third parts of Lemma 1 shows that after $T$ iterations, the CDP of the mechanism is at most $\\begin{array}{r}{T\\cdot\\frac{2C^{2}}{n^{2}\\sigma^{2}}\\leq\\rho}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "We next prove the utility claim. For each $0\\leq t\\leq T$ , denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta_{t}:=\\mathbb{E}\\left[F_{\\mathcal{D},\\lambda}(x_{t})-F_{\\mathcal{D},\\lambda}(x^{\\star})\\right],\\;\\Phi_{t}:=\\mathbb{E}\\left[\\frac{1}{2}\\left\\|x_{t}-x^{\\star}\\right\\|^{2}\\right],\\;g_{t}:=\\nabla F_{\\mathcal{D}}(x_{t}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where all expectations are only over randomness used by the algorithm, and not the randomness in sampling $\\mathcal{D}$ . First-order optimality applied to the definition of $x_{t+1}$ implies, for all $0\\leq t<T$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\langle\\hat{g}_{t}+\\xi_{t},x_{t}-x^{\\star}\\right\\rangle+\\left\\langle\\lambda x_{t+1},x_{t+1}-x^{\\star}\\right\\rangle\\leq\\frac{1}{2\\eta_{t}}\\left(\\left\\Vert x_{t}-x^{\\star}\\right\\Vert^{2}-\\left\\Vert x_{t+1}-x^{\\star}\\right\\Vert^{2}\\right)+\\frac{\\eta_{t}}{2}\\left\\Vert\\hat{g}_{t}+\\xi_{t}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Adding $\\left\\langle g_{t}-\\hat{g}_{t}-\\xi_{t},x_{t}-x^{\\star}\\right\\rangle$ to both sides and rearranging shows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad F_{\\mathcal{D}}(x_{t})+\\frac{\\lambda}{2}\\left\\|x_{t+1}\\right\\|^{2}-F_{\\mathcal{D},\\lambda}(x^{\\star})+\\frac{\\lambda}{2}\\left\\|x_{t+1}-x^{\\star}\\right\\|^{2}}\\\\ &{\\leq\\langle g_{t},x_{t}-x^{\\star}\\rangle+\\langle\\lambda x_{t+1},x_{t+1}-x^{\\star}\\rangle}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta_{t}}\\left(\\left\\|x_{t}-x^{\\star}\\right\\|^{2}-\\left\\|x_{t+1}-x^{\\star}\\right\\|^{2}\\right)+\\frac{\\eta_{t}}{2}\\left\\|\\hat{g}_{t}+\\xi_{t}\\right\\|^{2}+\\langle g_{t}-\\hat{g}_{t}-\\xi_{t},x_{t}-x^{\\star}\\rangle}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta_{t}}\\left(\\left\\|x_{t}-x^{\\star}\\right\\|^{2}-\\left\\|x_{t+1}-x^{\\star}\\right\\|^{2}\\right)+\\eta_{t}C^{2}+\\eta_{t}\\left\\|\\xi_{t}\\right\\|^{2}+b_{\\mathcal{D}}\\left\\|x_{t}-x^{\\star}\\right\\|-\\langle\\xi_{t},x_{t}-x^{\\star}\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the first line, we used strong convexity of the function $\\frac{\\lambda}{2}\\left\\Vert x\\right\\Vert^{2}$ , and in the last line, we used $\\left\\|a+b\\right\\|^{2}\\leq2\\left\\|a\\right\\|^{2}+2\\left\\|b\\right\\|^{2}$ and the definitions of $C$ and $b_{\\mathcal{D}}$ . Next, adding ${\\frac{\\lambda}{2}}{\\left({\\left\\|x_{t}\\right\\|}^{2}-{\\left\\|x_{t+1}\\right\\|}^{2}\\right)}$ to both sides above and taking expectations over the first $t$ iterations yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta_{t}+\\lambda\\Phi_{t+1}\\leq\\frac{1}{\\eta_{t}}\\left(\\Phi_{t}-\\Phi_{t+1}\\right)+\\eta_{t}(C^{2}+\\sigma^{2}d)+\\frac{b_{D}^{2}}{\\lambda}+\\frac{\\lambda}{2}\\Phi_{t}+\\frac{\\lambda}{2}\\left(\\mathbb{E}\\left\\Vert x_{t}\\right\\Vert^{2}-\\mathbb{E}\\left\\Vert x_{t+1}\\right\\Vert^{2}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we used the Fenchel-Young inequality to bound $\\begin{array}{r}{b_{\\mathcal{D}}\\left\\|x_{t}-x^{\\star}\\right\\|\\leq\\frac{b_{\\mathcal{D}}^{2}}{\\lambda}+\\frac{\\lambda}{4}\\left\\|x_{t}-x^{\\star}\\right\\|^{2}}\\end{array}$ . Now, plugging in our step size schedule $\\begin{array}{r}{\\eta_{t}=\\frac{4}{\\lambda(t+1)}}\\end{array}$ , multiplying by $t+4$ , and rearranging shows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+4)\\Delta_{t}\\leq\\displaystyle\\frac{\\lambda(t+3)(t+4)}{4}\\Phi_{t}-\\frac{\\lambda(t+5)(t+4)}{4}\\Phi_{t+1}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{4(t+4)}{\\lambda(t+1)}\\left(\\frac{3C^{2}T d}{n^{2}\\rho}\\right)+\\frac{(t+4)b_{D}^{2}}{\\lambda}+\\frac{\\lambda(t+4)}{2}\\left(\\mathbb{E}\\left\\Vert x_{t}\\right\\Vert^{2}-\\mathbb{E}\\left\\Vert x_{t+1}\\right\\Vert^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we plugged in the choice of $\\sigma^{2}$ and $\\begin{array}{r}{T\\geq\\frac{n^{2}\\rho}{d}}\\end{array}$ , so $\\begin{array}{r}{C^{2}\\leq\\frac{\\sigma^{2}d}{2}}\\end{array}$ . Summing the above for $0\\leq t<T$ , using that all iterates and $x^{\\star}$ lie in $\\mathbb{B}(r)$ , and dividing by $\\begin{array}{r}{Z:=\\sum_{0\\le t<T}(t+4)\\ge\\frac{T^{2}}{2}}\\end{array}$ , shows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{Z}\\sum_{0\\leq t<T}(t+4)\\Delta_{t}\\leq\\frac{3\\lambda\\Phi_{0}}{Z}+\\frac{16C^{2}T^{2}d}{\\lambda Z n^{2}\\rho}+\\frac{b_{D}^{2}}{\\lambda}+\\frac{\\lambda}{2Z}\\sum_{t\\in[T]}\\mathbb{E}\\left\\|x_{t}\\right\\|^{2}}}\\\\ &{\\leq\\frac{6\\lambda r^{2}}{T^{2}}+\\frac{32C^{2}d}{\\lambda n^{2}\\rho}+\\frac{b_{D}^{2}}{\\lambda}+\\frac{\\lambda r^{2}}{T}\\leq\\frac{32C^{2}d}{\\lambda n^{2}\\rho}+\\frac{b_{D}^{2}}{\\lambda}+\\frac{7\\lambda r^{2}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The conclusion follows from convexity of $F_{D,\\lambda}$ , the definition of $\\hat{x}$ , and $T\\geq n$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. Let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ . With probability at least $\\frac{4}{5}$ , denoting $b_{\\mathcal{D}}$ as in (26), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{\\mathscr D}\\leq\\frac{5G_{k}^{k}}{(k-1)C^{k-1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For every $s\\in S$ let $\\begin{array}{r}{x^{\\star}(s):=\\operatorname*{argmax}_{x\\in\\mathcal{X}}\\|\\nabla f(x;s)-\\Pi_{C}(\\nabla f(x;s))\\|_{2}}\\end{array}$ . Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}[b_{D}]=\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\operatorname*{max}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\nabla f_{i}(x)-\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x))\\right\\|\\right]}}\\\\ &{\\leq\\frac{1}{n}\\sum_{i\\in[n]}\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\operatorname*{max}\\|\\nabla f_{i}(x)-\\Pi_{C}(\\nabla f_{i}(x))\\|\\right]}\\\\ &{=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\|\\nabla f(x^{\\star}(s);s)-\\Pi_{C}(\\nabla f(x^{\\star}(s);s))\\|\\right]\\leq\\frac{\\mathbb{E}\\left[\\|\\nabla f(x^{\\star}(s);s)\\|^{k}\\right]}{(k-1)C^{k-1}}}\\\\ &{\\leq\\frac{G_{k}^{k}}{(k-1)C^{k-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last line used independence of samples, used Fact 1 on the random vector $\\nabla f(x^{\\star}(s);s)$ , and applied Assumption 1 with the definition of $x^{\\star}(s)$ . The conclusion uses Markov\u2019s inequality. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Let $\\lambda\\geq0$ , let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ , let $\\bar{x}\\,\\in\\,\\mathcal{X}$ where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is compact and convex, and let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{\\lambda,\\bar{x}}^{\\star}:=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\lbrace F_{\\mathcal{P}}(x)+\\frac{\\lambda}{2}\\left\\|x-\\bar{x}\\right\\|^{2}\\right\\rbrace,\\ \\index{w h e r e}\\ F_{\\mathcal{P}}(x):=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then $\\begin{array}{r}{\\|\\bar{x}-x_{\\lambda,\\bar{x}}^{\\star}\\|\\leq\\frac{2G_{1}}{\\lambda}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $r:=\\|\\bar{x}-x^{\\star}\\|$ . By strong convexity and the definition of $x_{\\lambda,{\\bar{x}}}^{\\star}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\lambda r^{2}}{2}}\\leq F_{\\mathcal{P}}(\\bar{x})-F_{\\mathcal{P}}(x^{\\star})-{\\frac{\\lambda}{2}}\\left\\|x^{\\star}-\\bar{x}\\right\\|^{2}\\leq F_{\\mathcal{P}}(\\bar{x})-F_{\\mathcal{P}}(x^{\\star})\\leq G_{1}r.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, we used that $F_{\\mathcal{P}}$ is $G_{1}$ -Lipschitz (Lemma 2), and rearranging yields the conclusion. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Let $\\lambda\\geq0,$ , let $\\mathcal{D}\\sim\\mathcal{P}^{n}$ where $\\mathcal{P}$ is a distribution over $\\boldsymbol{S}$ satisfying Assumption $^{\\,l}$ , and let ${\\bar{x}}\\in{\\mathcal{X}}$ where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is compact and convex. Following notation (4), (5), let ", "page_idx": 14}, {"type": "equation", "text": "$$\ny:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}\\left\\{\\left[F_{\\mathcal{D}}\\right]_{\\mathbb{B}(\\bar{x},r)}\\left(x\\right)+\\frac{\\lambda}{2}\\left\\Vert x-\\bar{x}\\right\\Vert^{2}\\right\\},\\;\\,f o r\\;r:=\\frac{2G_{1}}{\\lambda}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and let $x_{\\lambda,{\\bar{x}}}^{\\star}$ be defined as in (7). Then with probability $\\ge0.95$ over the randomness of $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|y-x_{\\lambda,\\bar{x}}^{\\star}\\right\\|_{2}\\leq\\frac{90G_{2}}{\\lambda\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For each $f(x;s)$ , define a restricted variant ${\\tilde{f}}(x;s)\\;:=\\;\\,f_{\\mathbb{B}(\\bar{x},r)}(x;s)$ , and let $\\begin{array}{r l}{\\widetilde{F}_{\\mathcal{P}}}&{{}:=}\\end{array}$ $\\mathbb{E}_{s\\sim S}\\tilde{f}(\\cdot;s)$ . Similarly, define ${\\tilde{F}}_{\\mathcal{D}}$ to be the restricted variant of the empirical loss $F_{\\mathcal{D}}$ . Because ${\\widetilde{F}}_{\\mathcal{P}}$ is pointwise larger than $F_{\\mathcal{P}}$ and $\\boldsymbol{x}_{\\lambda,\\bar{x}}^{\\star}\\in\\mathbb{B}(\\bar{\\boldsymbol{x}},\\boldsymbol{r})$ by Lemma 4, it is clear that ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{\\lambda,\\bar{x}}^{\\star}=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\lbrace\\widetilde{F}_{\\mathcal{P}}(x)+\\frac{\\lambda}{2}\\left\\|x-\\bar{x}\\right\\|^{2}\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $y$ is the minimizer of the empirical (restricted) variant of the above display. Moreover, each of the regularized functions $\\begin{array}{r}{\\tilde{f}(x;s)+\\frac{\\lambda}{2}\\left\\|x-\\bar{x}\\right\\|^{2}}\\end{array}$ has a Lipschitz constant at most $\\lambda r=2G_{1}$ larger than its unregularized counterpart in $\\mathcal{X}\\cap\\mathbb{B}(\\bar{x},r)$ , so these functions satisfy the moment bound in Assumption 1 for $j=2$ with a bound of $2G_{2}^{2}+8G_{1}^{2}$ . Now, applying Proposition 29, [LR23] yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left(\\widetilde{F}_{\\mathcal{P}}(y)+\\frac{\\lambda}{2}\\left\\Vert y-\\bar{x}\\right\\Vert^{2}\\right)-\\left(\\widetilde{F}_{\\mathcal{P}}(x_{\\lambda,\\bar{x}}^{\\star})+\\frac{\\lambda}{2}\\left\\Vert x_{\\lambda,\\bar{x}}^{\\star}-\\bar{x}\\right\\Vert^{2}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(\\widetilde{F}_{\\mathcal{D}}(y)+\\frac{\\lambda}{2}\\left\\Vert y-\\bar{x}\\right\\Vert^{2}\\right)-\\left(\\widetilde{F}_{\\mathcal{D}}(x_{\\lambda,\\bar{x}}^{\\star})+\\frac{\\lambda}{2}\\left\\Vert x_{\\lambda,\\bar{x}}^{\\star}-\\bar{x}\\right\\Vert^{2}\\right)\\right]}\\\\ &{+\\,\\mathbb{E}\\left[\\left(\\widetilde{F}_{\\mathcal{P}}(y)+\\frac{\\lambda}{2}\\left\\Vert y-\\bar{x}\\right\\Vert^{2}\\right)-\\left(\\widetilde{F}_{\\mathcal{D}}(y)+\\frac{\\lambda}{2}\\left\\Vert y-\\bar{x}\\right\\Vert^{2}\\right)\\right]}\\\\ &{\\leq0+\\frac{4G_{2}^{2}+16G_{1}^{2}}{\\lambda n}=\\frac{4G_{2}^{2}+16G_{1}^{2}}{\\lambda n}\\leq\\frac{20G_{2}^{2}}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first equality used that $x_{\\lambda,{\\bar{x}}}^{\\star}$ is independent of sampling $\\mathcal{D}$ , and the second used $\\hat{x}$ is the empirical risk minimizer. The conclusion follows from Markov\u2019s inequality and strong convexity. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Theorem 1. Consider an instance of $k$ -heavy-tailed private SCO, following notation in Definition 4, let $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x),}\\end{array}$ , and let $\\rho\\geq0$ , $\\delta\\in(0,1)$ . Algorithm 2 using Algorithm $^3$ in Line $^{5}$ is a $\\rho$ -CDP algorithm which draws $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , queries $\\begin{array}{r}{C_{\\mathrm{sco}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})}\\end{array}$ sample gradients (using samples in $\\mathcal{D}$ ) for a universal constant $C_{\\mathrm{sco}}$ , and outputs $x\\in\\mathscr{X}$ satisfying, with probability $\\geq1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\le C_{\\mathrm{sco}}\\left(G_{k}D\\cdot\\left(\\frac{\\sqrt{d}\\log\\left(\\frac1\\delta\\right)}{n\\sqrt{\\rho}}\\right)^{1-\\frac1k}+G_{2}D\\cdot\\sqrt{\\frac{\\log\\left(\\frac1\\delta\\right)}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Throughout, we assume that $\\frac{1}{\\delta}$ is at least a large enough constant (where lossiness can be absorbed into $C_{\\mathrm{sco.}}$ ), and that $n$ is at least a sufficiently large constant multiple of $\\log{\\frac{1}{\\delta}}$ (because the entire range of $F_{\\mathcal{P}}$ is $\\le G_{2}D\\mathrm{\\large/}$ ). We first handle the case where $\\frac{1}{\\delta}$ is larger than polylog $(n)$ , deferring the case of small $\\frac{1}{\\delta}$ to the end of the proof. Let $I,J\\in\\mathbb{N}$ be chosen such that ", "page_idx": 15}, {"type": "equation", "text": "$$\nI:=\\left\\lfloor\\log_{2}\\left({\\frac{n}{J}}\\right)\\right\\rfloor,\\ J\\in\\left[400\\log\\left({\\frac{I}{\\delta}}\\right),500\\log\\left({\\frac{I}{\\delta}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is achievable with $I=O(\\log n)$ and $\\begin{array}{r}{J=O(\\log\\frac{\\log n}{\\delta})=O(\\log\\frac{1}{\\delta})}\\end{array}$ . Let $\\begin{array}{r}{m:=\\frac{n}{J}}\\end{array}$ , and assume without loss that $m$ is a power of 2, which we can guarantee by discarding $\\leq\\frac{1}{2}$ our samples, losing a constant factor in the claim. For each $i\\in[I]$ , let $\\textstyle m_{i}:={\\frac{m}{2^{i}}}$ . We subdivide $\\bar{\\mathcal{D}}$ into $J$ portions, each with $m$ samples, and subdivide each portion into $I$ parts each with $m_{i}$ samples. For $j\\,\\in\\,[J]$ and $i\\in[I]$ , we denote the samples corresponding to the $i^{\\mathrm{th}}$ part of the $j^{\\mathrm{th}}$ portion by $\\mathcal{D}_{i}^{j}$ , so ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bigcup_{i\\in[I]}\\bigcup_{j\\in[J]}\\mathcal{D}_{i}^{j}\\subseteq\\mathcal{D},\\;|\\mathcal{D}_{i}^{j}|=m_{i}\\;\\mathrm{for}\\;\\mathrm{all}\\;j\\in[J],\\;\\mathcal{D}_{i}^{j}\\cap\\mathcal{D}_{i^{\\prime}}^{j^{\\prime}}=\\emptyset\\;\\mathrm{for}\\;\\mathrm{all}\\;(i,j)\\not=(i^{\\prime},j^{\\prime}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we show how to implement Line 5 in Algorithm 2, for an iteration $\\textit{i}\\in\\ [I]$ , by calling Algorithm 3 with appropriate parameters. Let $n\\leftarrow m_{i}$ , $\\rho\\gets\\rho$ , and initialize Algorithm 3 with the dataset $\\cup_{j\\in[J]}{\\mathcal{D}}_{i}^{j}$ and $\\begin{array}{r}{R:=\\frac{\\Delta{4}^{i}}{\\lambda_{i}}}\\end{array}$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta:=3C_{\\mathrm{reg-pop}}\\,\\left(G_{k}\\cdot\\left(\\frac{\\sqrt{d}}{m\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}+\\frac{G_{2}}{\\sqrt{m}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Corollary 2, each independent run outputs $x_{i}^{j}\\in\\mathcal{X}$ satisfying, with probability 0.55, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|x_{i}^{j}-x_{i}^{\\star}\\right\\|\\leq\\frac{C_{\\mathrm{reg-pop}}}{\\lambda_{i}}\\left(G_{k}\\cdot\\left(\\frac{\\sqrt{d}}{m_{i}\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}+\\frac{G_{2}}{\\sqrt{m_{i}}}\\right)\\leq\\frac{\\Delta4^{i}}{3\\lambda_{i}}=\\frac{R}{3}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by a Chernoff bound, with probability $\\begin{array}{r}{\\geq1-\\frac{\\delta}{I}}\\end{array}$ , at least $0.51J$ of the copies satisfy the above bound, so Fact 2 yields $x_{i}$ satisfying $\\begin{array}{r}{\\|{\\boldsymbol x}_{i}-{\\boldsymbol x}_{i}^{\\star}\\|\\leq R=\\frac{\\Delta{4}^{2}}{\\lambda_{i}}}\\end{array}$ with the same probability. Union bounding over all $I$ iterations of Algorithm 2 yields the failure probability, and so we obtain the claim from Proposition 2, after plugging in $\\begin{array}{r}{n=\\bar{O}(m\\log(\\frac{1}{\\delta}))}\\end{array}$ , since the dominant term is $D\\Delta$ . The privacy proof follows from the first part of Lemma 1 since for each pair of neighboring databases, exactly one of the datasets $\\mathcal{D}_{i}^{j}$ are neighboring, and Corollary 2 guarantees privacy of the empirical risk minimization algorithm using that dataset; privacy for all other datasets used is immediate from postprocessing properties of privacy. The gradient complexity comes from aggregating all of the $I J$ calls to Corollary 2, where we recall the sample sizes decay geometrically. ", "page_idx": 16}, {"type": "text", "text": "Finally, if $\\frac{1}{\\delta}$ is smaller than polylog $(n)$ , for the $i^{\\mathrm{th}}$ iteration of Algorithm 2 we instead set $J_{i}\\ \\in$ $\\begin{array}{r}{[400\\log(\\frac{I}{\\delta_{i}})}\\end{array}$ , $500\\log(\\frac{I}{\\delta_{i}})]$ where $\\begin{array}{r}{\\delta_{i}:=\\frac{\\delta}{2^{i}}}\\end{array}$ . Then we subdivide a consecutive batch of $\\frac{n}{2^{i}}$ samples into $J_{i}$ portions, and follow the above proof. It is straightforward to check that (9) still holds with the new value of $\\begin{array}{r}{m_{i}=\\left\\lfloor\\frac{n}{2^{i}J_{i}}\\right\\rfloor}\\end{array}$ because the $4^{i}$ factor growth on the right-hand side continues to outweigh the change in $m_{i}$ . The error bound follows from Proposition 2, and the privacy proof is identical. ", "page_idx": 16}, {"type": "text", "text": "A.3 Strongly convex heavy-tailed private SCO via localization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Finally, by following the template of standard localization reductions in the literature (see e.g. Theorem 5.1, [FKT20] or Lemma 5.5, [KLL21]), Theorem 1 obtains an improved rate when all sample functions are strongly convex. For completeness, we state this result below. ", "page_idx": 16}, {"type": "text", "text": "Corollary 3. In the setting of Theorem $^{\\,l}$ , suppose $f(x;s)$ is $\\mu$ -strongly convex for all $s\\in S$ . There is an algorithm which draws $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , queries $C_{\\mathrm{sco}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})$ sample gradients (using samples in $\\mathcal{D}$ ) for a universal constant $C_{\\mathrm{sco}}$ , and outputs $x\\in\\mathscr{X}$ satisfying, with probability $\\geq1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\leq C_{\\mathrm{sco}}\\left(\\frac{G_{k}^{2}}{\\mu}\\cdot\\left(\\frac{d\\log^{3}\\left(\\frac1\\delta\\right)}{n^{2}\\rho}\\right)^{1-\\frac1\\mu}+\\frac{G_{2}^{2}}{\\mu}\\cdot\\frac{\\log\\left(\\frac1\\delta\\right)}{n}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. This is immediate from the development in Section 5.1 (and the proof of Theorem 5.1) of [FKT20], but we mention one slight difference here. Our guarantees in Theorem 1 do not scale with the initial distance bound to the function minimizer, and instead scale with the domain size, which makes it less directly compatible with the standard localization framework in [FKT20]. However, because Theorem 1 holds with high probability, we also have explicit bounds on the domain size via function error, as seen in the proof of Theorem 5.1 in [FKT20], so we can explicitly truncate our domain to have smaller domain without removing the minimizer. With this modification, the claim follows directly from Theorem 5.1 in [FKT20]. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Optimal Algorithms in the Known Lipschitz Setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Compared to the standard Lipschitz setting (i.e. the $\\infty$ -heavy-tailed private SCO problem), our algorithm in Section 3 has two downsides: it pays a polylogarithmic overhead in the utility, and it requires an extra aggregation step. In this section, assuming we are in the known Lipschitz $k$ -heavytailed setting (see the following Assumption 2, and Definition 4), we provide a simple reduction to the standard Lipschitz setting, resulting in optimal rates. ", "page_idx": 16}, {"type": "text", "text": "Assumption 2 (Known Lipschitz $k$ -heavy-tailed distributions). In the setting of Assumption 1, suppose that for each $s\\ \\in\\ S$ we know a value $\\overline{{L}}_{s}\\,\\geq\\,L_{s}$ . For $k\\in\\mathbb{N}$ satisfying $k\\,\\geq\\,2$ , we say $\\mathcal{P}$ satisfies the known Lipschitz $k$ -heavy tailed assumption $i f,$ for a sequence of monotonically nondecreasing $\\{G_{j}\\}_{j\\in[k]}$ , we have $\\mathbb{E}_{s\\sim\\mathcal{P}}[\\overline{{L}}_{s}^{j}]\\le G_{j}^{j}<\\infty$ for all $j\\in[k]$ . ", "page_idx": 16}, {"type": "text", "text": "Note that Assumption 2 clearly implies Assumption 1, but gives us additional access to Lipschitz overestimates with bounded moments. We require some additional definitions used throughout the section. First, we augment $\\boldsymbol{S}$ with a designated element $s_{0}\\not\\in{\\mathcal{S}}$ , and define ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x;s_{0})=0{\\mathrm{~for~all~}}x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We also define a truncated distribution parameterized by $C\\geq0$ , where we use $f(\\cdot;s_{0})$ in place of sample functions with large Lipschitz overestimates, following notation of Assumption 2: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf^{C}(x;s):=\\left\\{f(x;s)\\quad\\overline{{L}}_{s}\\leq C\\right.,\\;f^{C}(x;s_{0}):=f(x;s_{0}),\\;F_{\\mathcal{P}}^{C}(x;s):=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f^{C}(x;s)\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote $S_{0}:=S\\cup\\{s_{0}\\}$ , and for $\\mathcal{D}\\in\\mathcal{S}^{n}$ , the dataset $\\mathcal{D}^{C}\\in\\mathcal{S}_{0}^{n}$ replaces all $s\\in\\mathcal{D}$ satisfying $\\overline{{L}}_{s}>C$ with $s_{0}$ . We additionally provide a second reduction in the known Lipschitz heavy-tailed setting, when all sample functions are assumed to be $\\mu$ -strongly convex. Because our treatments of these cases are slightly different, we use different notation when $\\mu=0$ and $\\mu>0$ , for convenience of exposition. Fixing an arbitrary point $\\bar{x}\\in\\mathcal{X}$ , for $\\mu>0$ , instead of using the constant 0 function as in (10), we define a strongly convex alternative $f(\\cdot;s_{\\mu})$ , for a designated element $s_{\\mu}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x;s_{\\mu})={\\frac{\\mu}{2}}\\|x-{\\bar{x}}\\|^{2},{\\mathrm{~for~all~}}x\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The truncated distribution parameterized by $C\\geq\\mu D$ , is defined in a similar way: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{\\mu}^{C}(x;s):=\\left\\{f(x;s)\\quad\\overline{{L}}_{s}\\leq C\\right.,\\;f_{\\mu}^{C}(x;s_{\\mu}):=f(x;s_{\\mu}),\\;F_{\\mathcal P}^{C,\\mu}(x;s):=\\mathbb{E}_{s\\sim\\mathcal P}\\left[f_{\\mu}^{C}(x;s)\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote $S_{\\mu}:=S\\cup\\{s_{\\mu}\\}$ , and for $\\mathcal{D}\\in\\mathcal{S}^{n}$ , the dataset $\\mathcal{D}_{\\mu}^{C}\\in S_{\\mu}^{n}$ replaces every $s\\in\\mathcal{D}$ such that $\\overline{{{\\cal L}}}_{s}>C$ with $s_{\\mu}$ . Our focus on the regime $C\\geq{\\frac{\\mu D}{4}}$ is motivated by the following well-known claim. ", "page_idx": 17}, {"type": "text", "text": "Lemma 6. Let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be compact and convex satisfying diam $(\\mathcal{X})=D$ , and suppose $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ is $L$ -Lipschitz and $\\mu$ -strongly convex. Then, $L\\geq{\\frac{\\mu D}{4}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $x^{\\star}:=\\operatorname{argmin}_{x\\in\\mathcal{X}}f(x)$ . By strong convexity, for all $x\\in\\mathscr{X}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{\\mu}{2}}\\left\\|x-x^{\\star}\\right\\|^{2}\\leq f(x)-f(x^{\\star})\\leq L\\left\\|x-x^{\\star}\\right\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, choose $x$ such that $\\|x-x^{\\star}\\|\\geq\\frac{D}{2}$ . To see this is always possible, let $x,x^{\\prime}\\,\\in\\,\\mathcal{X}$ realize $\\|{\\boldsymbol{x}}-{\\boldsymbol{x}}^{\\prime}\\|=D$ ; then at least one of $x,x^{\\prime}$ must have distance D2 from x\u22c6by the triangle inequality. The conclusion follows by rearranging after using our choice of $x$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "In other words, if $C<{\\frac{\\mu D}{4}}$ then no sample function will survive the truncation in (13). Finally, we parameterize the performance of algorithms in the standard Lipschitz setting. ", "page_idx": 17}, {"type": "text", "text": "Definition 5 (Lipschitz private SCO algorithm). We say $\\boldsymbol{\\mathcal{A}}$ is an $L$ -Lipschitz private SCO algorithm if it takes input $(\\mathcal \u1e0a D \u1e0c ,\\rho,\\mathcal \u1e0a X \u1e0c )$ , where $\\mathcal{D}\\in\\mathcal{S}^{n}$ is drawn i.i.d. from $\\mathcal{P}$ , a distribution over $\\boldsymbol{S}$ where every $s\\in S$ induces $L$ -Lipschitz $f(\\cdot;s)$ over $\\mathcal{X}\\subset\\mathbb{R}^{d}$ , ${\\mathcal{A}}({\\mathcal{D}},\\rho,{\\mathcal{X}})\\in{\\mathcal{X}}$ , and $\\boldsymbol{\\mathcal{A}}$ satisfies $\\rho$ -CDP. We denote ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{E r r}(A):=\\mathbb{E}_{A}\\left[F_{\\mathcal{P}}\\left(A(\\mathcal{D},\\rho,\\boldsymbol{\\mathscr{X}})\\right)\\right]-\\operatorname*{min}_{\\boldsymbol{x}\\in\\boldsymbol{\\mathscr{X}}}F_{\\mathcal{P}}(\\boldsymbol{x}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $F_{\\mathcal{P}}(x):=\\mathbb{E}_{s\\sim\\mathcal{P}}f(x;s)$ , and denote the number of sample gradients queried by $\\boldsymbol{\\mathcal{A}}$ by $\\mathsf{N}({\\mathcal{A}})$ . Moreover, if each Lipschitz function $f(;s)$ is $\\mu$ -strongly convex over the convex domain $_{\\mathcal{X}}$ , we say $\\boldsymbol{\\mathcal{A}}$ is an $L$ -Lipschitz, $\\mu$ -strongly convex private SCO algorithm, and define $\\mathsf{E r r}({\\mathcal{A}}),\\,\\mathsf{N}({\\mathcal{A}})$ as before. ", "page_idx": 17}, {"type": "text", "text": "With this notation in place, we state our reduction. ", "page_idx": 17}, {"type": "text", "text": "We begin with a simple bound relating $F_{\\mathcal{P}}^{C},F_{\\mathcal{P}}^{C,\\mu}$ and $F_{\\mathcal{P}}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. Let $F_{\\mathcal{P}}$ be defined as in Definition $^{4}$ , where $\\mathcal{P}$ satisfies Assumption 2, and define $F_{\\mathcal{P}}^{C}$ as in (11). Then, $F_{\\mathcal{P}}-F_{\\mathcal{P}}^{C}$ is $\\frac{G_{k}^{k}}{(k\\!-\\!1)C^{k-1}}$ -Lipschitz, and $\\begin{array}{r}{F\\mathcal{P}-F_{\\mathcal{P}}^{C,\\mu}\\,i s\\,\\frac{G_{k}^{k}}{(k-1)C^{k-1}}+\\frac{4G_{k}^{k+1}}{C^{k}}}\\end{array}$ -Lipschitz. ", "page_idx": 17}, {"type": "text", "text": "Input: Dataset $\\mathcal{D}\\in\\mathcal{S}^{n}$ , clip threshold $C\\in\\mathbb{R}_{\\geq0}$ , strong convexity parameter $\\mu\\in\\mathbb{R}_{\\geq0}$ , privacy parameter $\\rho\\in\\mathbb{R}_{>0}$ , domain $\\mathcal{X}\\in\\mathbb{R}^{d}$ , $C$ -Lipschitz private SCO algorithm $\\boldsymbol{\\mathcal{A}}$ (if $\\mu=0$ ), or $C$ -Lipschitz $\\mu$ -strongly convex private SCO algorithm $\\boldsymbol{\\mathcal{A}}$ (if $\\mu>0$ ) ", "page_idx": 18}, {"type": "text", "text": "2 if $\\mu=0$ then   \n3 Return: $\\mathcal{A}(\\mathcal{D}^{C},\\rho,\\mathcal{X})$   \n4 end   \n5 else   \nReturn: ${\\mathcal{A}}({\\mathcal{D}}_{\\mu}^{C},\\rho,{\\mathcal{X}})$   \n7 end ", "page_idx": 18}, {"type": "text", "text": "Proof. For $s\\in S$ , let $\\pi(s):=s_{0}$ if $\\overline{{L}}_{s}>C$ , and otherwise let $\\pi(s):=s$ . For any $x,x^{\\prime}\\in\\mathcal{X}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}^{C}(x))-\\left(F_{\\mathcal{P}}(x^{\\prime})-F_{\\mathcal{P}}^{C}(x^{\\prime})\\right)=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)-f(x;\\pi(s))-f(x^{\\prime};s)+f(x^{\\prime};\\pi(s))\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\left(f(x;s)-f(x^{\\prime};s)\\right)\\mathbb{I}_{\\overline{{L}}_{s}>C}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\overline{{L}}_{s}\\left\\Vert x-x^{\\prime}\\right\\Vert\\mathbb{I}_{\\overline{{L}}_{s}>C}\\right]\\leq\\frac{G_{k}^{k}}{(k-1)C^{k-1}}\\left\\Vert x-x^{\\prime}\\right\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the second line, we used that $\\pi(s)=s$ unless $\\overline{{L}}_{s}>C$ , in which case $f(\\cdot;\\pi(s))=0$ uniformly.   \nThe last line used the definition of $\\overline{{L}}_{s}$ and Fact 1 with $X\\leftarrow\\overline{{L}}_{s}$ , recalling Assumption 2. ", "page_idx": 18}, {"type": "text", "text": "Next, we analyze $F_{\\mathcal{P}}^{C,\\mu}$ . Overloading $\\pi(s):=s_{\\mu}$ if $\\overline{{{\\cal L}}}_{s}>C$ , and letting $\\pi(s):=s$ otherwise, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Big(F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}^{C,\\mu}(x)\\Big)-\\Big(F_{\\mathcal{P}}(x^{\\prime})-F_{\\mathcal{P}}^{C,\\mu}(x^{\\prime})\\Big)}\\\\ &{}&{\\quad=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[f(x;s)-f(x;\\pi(s))-f(x^{\\prime};s)+f(x^{\\prime};\\pi(s))\\right]}\\\\ &{}&{\\quad=\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\left(f(x;s)-f(x^{\\prime};s)+f(x^{\\prime};s_{\\mu})-f(x;s_{\\mu})\\right)\\mathbb{I}_{\\overline{{L}}_{s}>C}\\right]}\\\\ &{}&{\\quad\\le\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[\\overline{{L}}_{s}\\left\\Vert x-x^{\\prime}\\right\\Vert\\mathbb{I}_{\\overline{{L}}_{s>C}}\\right]+\\mathbb{E}_{s\\sim\\mathcal{P}}\\left[4G_{k}\\left\\Vert x-x^{\\prime}\\right\\Vert\\mathbb{I}_{\\overline{{L}}_{s>C}}\\right]}\\\\ &{}&{\\quad\\le\\left(\\frac{G_{k}^{k}}{(k-1)C^{k-1}}+\\frac{4G_{k}^{k+1}}{C^{k}}\\right)\\left\\Vert x-x^{\\prime}\\right\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the third line, we used that $\\mu D\\leq4G_{1}\\leq4G_{k}$ by Lemma 6 and Lemma 2 to show that $f(\\cdot;s_{\\mu})$ is $4G_{k}$ -Lipschitz over $\\mathcal{X}$ . Finally, the last line used Markov\u2019s inequality to bound $\\mathbb{E}[\\mathbb{I}_{\\overline{{L}}_{s}>C}]$ . ", "page_idx": 18}, {"type": "text", "text": "Using Lemma 7, we provide a straightforward analysis of Algorithm 4. ", "page_idx": 18}, {"type": "text", "text": "Proposition 3. Consider an instance of known-Lipschitz $k$ -heavy-tailed private SCO (Definition 4), and let $\\rho\\geq0$ . If $\\boldsymbol{\\mathcal{A}}$ is a $C$ -Lipschitz private SCO algorithm (Definition 5) and $\\mu=0$ , Algorithm 4 using $\\boldsymbol{\\mathcal{A}}$ is a $\\rho$ -CDP algorithm which outputs $x\\in\\mathscr{X}$ satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq\\mathsf{E r r}(\\mathcal{A})+\\frac{G_{k}^{k}D}{(k-1)C^{k-1}},\\mathrm{\\}w h e r e\\ x^{\\star}:=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\,F_{\\mathcal{P}}(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, if $f(\\cdot;s)$ is $\\mu$ -strongly convex for all $s\\in S$ and $\\boldsymbol{\\mathcal{A}}$ is a $C$ -Lipschitz, $\\mu$ -strongly convex private SCO algorithm for $\\mu>0$ , Algorithm $^{4}$ using $\\boldsymbol{\\mathcal{A}}$ is a $\\rho$ -CDP algorithm which outputs $x\\in\\mathscr{X}$ satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{-}[F_{P}(x)-F_{P}(x^{\\star})]\\leq\\mathsf{E r r}(A)}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\cfrac{G_{k}^{k}}{(k-1)C^{k-1}}+\\cfrac{4G_{k}^{k+1}}{C^{k}}\\right)\\left(\\cfrac{2G_{k}^{k}}{\\mu(k-1)C^{k-1}}+\\cfrac{8G_{k}^{k+1}}{\\mu C^{k}}+\\sqrt{\\cfrac{2}{\\mu}\\cdot\\mathsf{E r r}(A)}\\right),}\\\\ &{\\qquad\\qquad w h e r e~x^{\\star}:=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}~F_{P}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In either case, Algorithm $^{4}$ queries $\\mathsf{N}({\\mathcal{A}})$ sample gradients (using samples in $\\mathcal{D}$ ). ", "page_idx": 18}, {"type": "text", "text": "Proof. For the first utility claim, letting $\\begin{array}{r}{x^{\\star,C}:=\\mathrm{argmin}_{x\\in\\mathcal{X}}\\,F_{\\mathcal{P}}^{C}(x)}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\right]=\\mathbb{E}\\left[F_{\\mathcal{P}}^{C}(x)-F_{\\mathcal{P}}^{C}(x^{\\star})\\right]}\\\\ &{\\phantom{\\quad\\quad}+\\mathbb{E}\\left[\\left(F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}^{C}(x)\\right)-\\left(F_{\\mathcal{P}}(x^{\\star})-F_{\\mathcal{P}}^{C}(x^{\\star})\\right)\\right]}\\\\ &{\\phantom{\\quad\\quad}\\leq\\mathbb{E}\\left[F_{\\mathcal{P}}^{C}(x)-F_{\\mathcal{P}}^{C}(x^{\\star,C})\\right]+\\frac{G_{k}^{k}}{(k-1)C^{k-1}}\\mathbb{E}\\left[\\|x-x^{\\star}\\|\\right]}\\\\ &{\\phantom{\\quad\\quad}\\leq\\mathbb{E}r(\\mathcal{A})+\\frac{G_{k}^{k}D}{(k-1)C^{k-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality used the definition of $x^{\\star,C}$ and Lemma 7, and the second used the definition of Err and dia $\\mathsf{m}(\\bar{\\boldsymbol{x}})=\\bar{\\boldsymbol{D}}$ . For the second claim, we first have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\mu}{2}\\left\\|x-x^{\\star,C}\\right\\|^{2}\\right]\\leq\\mathsf{E r r}(A)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by the definition of $\\mathsf{E r r}(A)$ and $\\mu$ -strong convexity of $F_{\\mathcal{P}}^{C,\\mu}$ , so that E $\\begin{array}{r}{\\mathsf{\\bar{\\Phi}}[\\|x-x^{\\star,C}\\|]\\leq(\\frac{2}{\\mu}\\mathsf{E r r}(A))^{1/2}}\\end{array}$ by Jensen\u2019s inequality. Moreover, we also have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mu}{2}\\left\\|x^{\\star,C}-x^{\\star}\\right\\|^{2}\\leq F_{\\mathcal{P}}(x^{\\star,C})-F_{\\mathcal{P}}(x^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(F_{\\mathcal{P}}(x^{\\star,C})-F_{\\mathcal{P}}^{C}(x^{\\star,C})\\right)-\\left(F_{\\mathcal{P}}(x^{\\star})-F_{\\mathcal{P}}^{C}(x^{\\star})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\displaystyle\\frac{G_{k}^{k}}{(k-1)C^{k-1}}+\\displaystyle\\frac{4G_{k}^{k+1}}{C^{k}}\\right)\\left\\|x^{\\star,C}-x^{\\star}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use optimality of $x^{\\star,C}$ in the second inequality, and Lemma 7 in the third. Combining, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|x-x^{\\star}\\right\\|\\leq\\frac{2}{\\mu}\\cdot\\left(\\frac{G_{k}^{k}}{(k-1)C^{k-1}}+\\frac{4G_{k}^{k+1}}{C^{k}}\\right)+\\sqrt{\\frac{2}{\\mu}\\cdot\\mathsf{E r r}(A)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and then the claim follows by substituting this bound into (14). ", "page_idx": 19}, {"type": "text", "text": "We can use any existing optimal algorithms for DP-SCO to instantiate our reduction. In particular, we can use the algorithm of [FKT20], denoted by $\\mathcal{A}_{\\mathsf{L i p}}$ , which has the following guarantees. For simplicity of exposition, we focus on the case where our functions do not possess additional regularity properties e.g. smoothness, and we also focus on the simplest $\\mathcal{A}_{\\mathsf{L i p}}$ which attains the optimal utility bound. Because of the generality of our reduction, however, improvements can be made by using more structured or faster subroutines as $\\mathcal{A}_{\\mathsf{L i p}}$ , such as the smooth DP-SCO algorithms of [FKT20] or the Lipschitz DP-SCO algorithms of e.g. [AFKT21, KLL21, $\\mathrm{CJ}\\mathrm{J}^{+}23]$ , which are more query-efficient, sometimes at the cost of logarithmic factors in the utility (in the case of $[\\mathbf{C}\\mathbf{J}\\mathbf{J}^{+}23]$ ). ", "page_idx": 19}, {"type": "text", "text": "Proposition 4. Let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ such that $f(\\cdot;s)$ is $L$ -Lipschitz and convex for all $s\\,\\in\\,S$ . There exists a constant $C_{\\mathsf{L i p}}$ such that given $\\mathcal{D}\\sim\\mathcal{S}^{n}$ , the algorithm $\\mathcal{A}_{\\mathsf{L i p}}$ is $\\rho$ -CDP and outputs $x_{\\mathrm{priv}}$ such that, for a universal constant $C_{\\mathsf{L i p}}$ , letting $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x)}\\end{array}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\mathcal{P}}(x_{\\mathrm{priv}})-F_{\\mathcal{P}}(x^{\\star})]\\le C_{\\mathsf{L i p}}\\cdot\\left(\\frac{G_{2}D}{\\sqrt{n}}+\\frac{L D\\sqrt{d}}{n\\sqrt{\\rho}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and $\\mathcal{A}_{\\mathsf{L i p}}$ queries $\\begin{array}{r}{\\leq C_{\\lfloor\\mathsf{i p}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})}\\end{array}$ sample gradients (using samples in $\\mathcal{D}$ ), where $G_{2}$ is defined as in Assumption $^{\\,l}$ . Moreover, if $f(\\bar{\\cdot};s)$ is $\\mu$ -strongly convex for all $s\\in S$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\mathcal{P}}(x_{\\mathrm{priv}})-F_{\\mathcal{P}}(x^{\\star})]\\le C_{\\mathsf{L i p}}\\cdot\\left(\\frac{G_{2}^{2}}{\\mu n}+\\frac{L^{2}d}{\\mu n^{2}\\rho}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and $\\begin{array}{r}{\\mathcal{A}_{\\sf L i p}\\,q u e r i e s\\le C_{\\sf L i p}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})}\\end{array}$ sample gradients (using samples in $\\mathcal{D}$ ). ", "page_idx": 19}, {"type": "text", "text": "Proof. This follows from developments in [FKT20], but we briefly explain any discrepancies. The $\\mu=0$ case applies Theorem 4.8 in [FKT20], where for simplicity we consider the full-batch variant ", "page_idx": 19}, {"type": "text", "text": "which does not subsample.8Moreover, Theorem 4.8 in [FKT20] is stated with a dependence on $L$ rather than $G_{2}$ on the $\\bar{n^{-1/2}}$ term, but inspecting the proof shows it only uses a second moment bound. The $\\mu>0$ case follows from Theorem 5.1 of [FKT20], using Theorem 4.8 as a subroutine. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "We are now ready to present our main result in this section, using our reduction with $\\mathcal{A}_{\\mathsf{L i p}}$ ", "page_idx": 20}, {"type": "text", "text": "Theorem 2. Consider an instance of known-Lipschitz $k$ -heavy-tailed private \u221aSCO (Definition 4), let $\\rho\\,\\geq\\,0$ , and let $x^{\\star}:=\\,\\operatorname{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x)$ . Algorithm $^{4}$ with $C\\,\\leftarrow\\,G_{k}(\\frac{n\\sqrt{\\rho}}{\\sqrt{d}})^{\\frac{1}{k}}$ using $\\mathcal{A}_{\\sf L i p}\\ u n$ Proposition is a $\\rho$ -CDP algorithm which outputs satisfying, for a universal constant $C_{\\mathrm{HT}}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq C_{\\mathrm{HT}}\\left(\\frac{G_{2}D}{\\sqrt{n}}+G_{k}D\\cdot\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "querying $\\begin{array}{r}{\\leq\\ C_{\\mathrm{HT}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})}\\end{array}$ sample gradients (using samples in $\\mathcal{D}$ ). Further, if $f(\\cdot;s)$ is $\\mu$ - strongly convex for all $s\\in\\mathcal{S}$ , Algorithm $^{4}$ with $C\\gets G_{k}(\\frac{n^{2}\\rho}{d})^{\\frac{1}{2k}}$ using $\\mathcal{A}_{\\mathsf{L i p}}$ in Proposition $^{4}$ is $a$ $\\rho$ -CDP algorithm which outputs $x\\in\\mathscr{X}$ satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(x)-F_{\\mathcal{P}}(x^{\\star})\\right]\\le C_{\\mathrm{HT}}\\left(\\frac{G_{2}^{2}}{\\mu n}+\\frac{G_{k}^{2}}{\\mu}\\cdot\\left(\\frac{d}{n^{2}\\rho}\\right)^{1-\\frac{1}{k}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$q u e r y i n g\\leq C_{\\mathrm{HT}}\\operatorname*{max}(n^{2},\\frac{n^{3}\\rho}{d})$ sample gradients (using samples in $\\mathcal{D}$ ). ", "page_idx": 20}, {"type": "text", "text": "Proof. Throughout the proof, assume without loss of generality that $d\\,\\le\\,n^{2}\\rho$ , as otherwise all stated bounds are vacuous since the additive function value range over $\\mathcal{X}$ is at most $\\begin{array}{r}{G_{1}D\\leq\\frac{4G_{1}^{2}}{\\mu}}\\end{array}$ by Lemma 6 and Lemma 2. This also implies that $C\\geq G_{k}$ in either case. ", "page_idx": 20}, {"type": "text", "text": "In the $\\mu=0$ case, Proposition 3 and the guarantees of $\\mathcal{A}_{\\mathsf{L i p}}$ in Proposition 4 imply that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F_{\\mathcal{P}}(\\boldsymbol{\\hat{x}})-F_{\\mathcal{P}}(\\boldsymbol{x^{\\star}})\\right]\\leq\\mathsf{E r r}(\\mathcal{A}_{\\mathsf{L i p}})+\\frac{G_{k}^{k}D}{C^{k-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{\\mathsf{L i p}}\\cdot\\left(\\displaystyle\\frac{G_{2}D}{\\sqrt{n}}+\\frac{C D\\sqrt{d}}{n\\sqrt{\\rho}}\\right)+\\frac{G_{k}^{k}D}{C^{k-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq(C_{\\mathsf{L i p}}+2)\\left(\\displaystyle\\frac{G_{2}D}{\\sqrt{n}}+G_{k}D\\cdot\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from our choice of $C$ . Next, we consider $\\mu>0$ . Proposition 3 and the guarantees of $\\mathcal{A}_{\\mathsf{L i p}}$ in Proposition 4 for this case imply that, assuming $C_{\\mathsf{L i p}}\\geq2$ without loss, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F_{\\mathcal{P}}(\\hat{x})-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq\\mathsf{E r r}({\\cal A}_{\\mathsf{L i p}})+\\frac{5G_{k}^{k}}{C^{k-1}}\\left(\\sqrt{\\frac{2\\mathsf{E r r}(n,d,\\rho,C,D)}{\\mu}}+\\frac{10G_{k}^{k}}{\\mu C^{k-1}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{\\mathsf{L i p}}\\cdot\\left(\\frac{G_{2}^{2}}{\\mu n}+\\frac{C^{2}d}{\\mu n^{2}\\rho}+\\frac{5G_{k}^{k}}{C^{k-1}}\\left(\\frac{G_{2}}{\\mu\\sqrt{n}}+\\frac{C\\sqrt{d}}{\\mu n\\sqrt{\\rho}}+\\frac{10G_{k}^{k}}{\\mu C^{k-1}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq(C_{\\mathsf{L i p}}+61)\\cdot\\left(\\frac{G_{2}^{2}}{\\mu n}+\\frac{G_{k}^{2}}{\\mu}\\cdot\\left(\\frac{d}{n^{2}\\rho}\\right)^{1-\\frac{1}{k}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used $C\\geq G_{k}$ to simplify bounds, and applied our choice of $C$ . ", "page_idx": 20}, {"type": "text", "text": "C Fast Algorithms for Smooth Functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we develop a linear-time algorithm for the smooth setting where we additionally assume $f(\\cdot;s)$ is $\\beta$ -smooth for all $s\\in S$ . Our algorithm attains nearly-optimal rates for a sufficiently small value of $\\beta$ , and is based on the localization framework of [FKT20]. To apply this framework, we show that a variant of clipped DP-SGD (see Algorithm 5) is stable in the heavy-tailed setting with high probability. We then ensure that stability holds for any input dataset (not necessarily sampled from a distribution $P$ ), by using the sparse vector technique [DR14] to verify that the number of clipped gradients is not too large. In Appendix C.1, we provide some standard preliminary results from the literature. We use these results in Appendix C.2, where we state our algorithm in full as Algorithm 7 and analyze it in Theorem 3, the main result of this section. ", "page_idx": 21}, {"type": "text", "text": "C.1 Helper tools ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we state a standard bound on the contractivity of smooth gradient descent iterations. ", "page_idx": 21}, {"type": "text", "text": "Fact 3 (Lemma 3.7, [HRS16]). Let $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ be $\\beta$ -smooth, and let $\\begin{array}{r}{\\eta\\le{\\frac{2}{\\beta}}}\\end{array}$ . Then for any $x,x^{\\prime}\\in\\mathcal{X}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(x-x^{\\prime})-\\eta(\\nabla f(x)-\\nabla f(x^{\\prime}))\\|\\le\\|x-x^{\\prime}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we provide a standard utility bound on a one-pass SGD algorithm using clipped gradients. ", "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r}{\\overline{{\\mathbf{Algorithm}\\,5!\\;\\mathrm{OnePass-Clipped-SGD}(\\mathcal{D},C,\\eta,T,\\mathcal{X},x_{0})}}\\end{array}$ 1 Input: Dataset $\\mathcal{D}=\\{s_{t}\\}_{t\\in[T]}\\in\\mathcal{S}^{T}$ , clip threshold $C\\in\\mathbb{R}_{\\geq0}$ , step size $\\eta\\in\\mathbb{R}_{\\geq0}$ , iteration count $T\\in\\mathbb N$ , domain $\\mathcal{X}\\subset\\mathbb{B}(x_{0},D)$ for $x_{0}\\in\\mathcal{X}$ 2 for $0\\leq t<T$ do 3 $\\begin{array}{r}{\\widehat{x_{t+1}}\\gets\\operatorname*{argmin}_{x\\in\\mathcal{X}}\\{\\eta\\left\\langle\\Pi_{C}(\\nabla f(x_{t};s_{t+1})),x\\right\\rangle+\\frac{1}{2}\\left\\lVert x-x_{t}\\right\\rVert^{2}\\}}\\end{array}$ 4 end 5 Return: $\\begin{array}{r}{\\hat{x}\\gets\\frac{1}{T}\\sum_{0\\leq t<T}x_{t}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 8. Consider an instance of $k$ -heavy-tailed private SCO, following notation in Definition $^{4}$ , and let $u\\in\\mathscr{X}$ be independent of $\\mathcal{D}$ . Assuming $\\mathcal{D}\\sim\\bar{\\mathcal{P}}^{T}$ i.i.d., Algorithm $^{5}$ outputs ${\\hat{x}}\\in{\\mathcal{X}}$ satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(\\hat{x})-F_{\\mathcal{P}}(u)\\right]\\leq\\frac{\\left\\Vert x_{0}-u\\right\\Vert^{2}}{2\\eta T}+\\frac{\\eta G_{2}^{2}}{2}+\\frac{G_{k}^{k}D}{(k-1)C^{k-1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. To simplify notation, let $g_{t}:=\\nabla f(x_{t};s_{t+1})$ for all $0\\leq t<T$ , and let $\\hat{g}_{t}:=\\mathcal{T}_{C}(g_{t})$ . Because $s_{t+1}\\sim\\mathcal{P}$ is independent of $x_{t}$ , we have that $\\mathbb{E}g_{t}=\\nabla F_{\\mathcal{P}}(x_{t})$ . Therefore, in iteration $t$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{F_{\\mathcal{P}}(x_{t})-F_{\\mathcal{P}}(u)=\\mathbb{E}\\left[\\left\\langle g_{t},x_{t}-u\\right\\rangle\\right]}\\\\ {\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\left[\\left\\langle\\hat{g}_{t},x_{t}-u\\right\\rangle+\\left\\|g_{t}-\\hat{g}_{t}\\right\\|D\\right]}\\\\ {\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\left[\\frac{1}{2}\\left\\|x_{t}-u\\right\\|^{2}-\\frac{1}{2}\\left\\|x_{t+1}-u\\right\\|^{2}+\\frac{\\eta G_{2}^{2}}{2}\\right]+\\frac{G_{k}^{k}D}{(k-1)C^{k-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where all expectations are conditional on the first $t$ iterations of the algorithm, and taken over the randomness of $s_{t+1}$ . In the third line, we used the first-order optimality condition on $x_{t+1}$ , applied Fact 1 to bound $\\mathbb{E}\\left\\lVert g_{t}-\\hat{g}_{t}\\right\\rVert$ , and used ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|{\\hat{g}}_{t}\\right\\|^{2}\\leq\\mathbb{E}\\left\\|g_{t}\\right\\|^{2}\\leq G_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing across all iterations and dividing by $T$ yields the result upon iterating expectations. ", "page_idx": 21}, {"type": "text", "text": "We also note the following straightforward generalization of Lemma 8 to the case of randomized clipping thresholds, which is used in our later development. ", "page_idx": 21}, {"type": "text", "text": "Corollary 4. For $C,{\\hat{C}}\\geq0$ and $\\boldsymbol{g}\\in\\mathbb{R}^{d}$ , define the operation ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Pi_{C,\\hat{C}}(g):=\\left\\{\\Pi_{C}(g)\\quad\\lVert g\\rVert\\geq\\hat{C}\\ .\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If Algorithm $^{5}$ is run with $\\Pi_{C}(\\nabla f(x_{t};s_{t+1}))$ replaced by $\\Pi_{C,\\hat{C}_{t}}(\\nabla f(x_{t};s_{t+1}))$ where $\\hat{C}_{t}$ is independent of $s_{t+1}$ and satisfies $\\begin{array}{r}{\\hat{C}_{t}\\ge\\frac{C}{2}}\\end{array}$ for all $0\\leq t<T$ , then following notation in Lemma 8, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(\\boldsymbol{\\hat{x}})-F_{\\mathcal{P}}(\\boldsymbol{u})\\right]\\leq\\frac{\\left\\Vert\\boldsymbol{x}_{0}-\\boldsymbol{u}\\right\\Vert^{2}}{2\\eta T}+2\\eta G_{2}^{2}+\\frac{G_{k}^{k}D}{(k-1)(\\frac{C}{2})^{k-1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For a fixed iteration $0\\leq t<T$ , the calculation (16) changes in two ways. First, in place of the variance bound (16) (which used $\\|\\hat{g}_{t}\\|\\leq\\|g_{t}\\|$ deterministically), when using the modified clipping operators we require the modified deterministic bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\hat{g}_{t}\\|\\leq2\\left\\|g_{t}\\right\\|,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which follows because $\\|\\hat{g}_{t}\\|\\neq\\|g_{t}\\|$ (which implies $C=\\|{\\hat{g}}_{t}\\|,$ only if $\\begin{array}{r}{\\|g_{t}\\|\\geq\\frac{C}{2}}\\end{array}$ . Moreover, in place of the bias bound $\\begin{array}{r}{\\mathbb{E}\\left\\|g_{t}-\\hat{g}_{t}\\right\\|\\leq\\frac{G_{k}^{k}}{(k-1)C^{k-1}}}\\end{array}$ which followed from Fact 1, we instead have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left.^{-}\\left\\Vert\\Pi_{C,\\hat{C}_{t}}(g_{t})-g_{t}\\right\\Vert=\\mathbb{E}\\left[\\left\\Vert\\frac{C}{\\Vert g_{t}\\Vert}-1\\right\\Vert\\Vert g_{t}\\Vert\\mathbb{I}_{\\Vert g_{t}\\Vert\\geq\\operatorname*{max}(\\hat{C}_{t},C)}\\right]\\leq\\mathbb{E}\\left[\\Vert g_{t}\\Vert\\mathbb{I}_{\\Vert g_{t}\\Vert\\geq\\frac{C}{2}}\\right]\\leq\\frac{G_{k}^{k}}{(k-1)(\\frac{C}{2})^{k-1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The conclusion follows by adjusting these constants appropriately in Lemma 8. ", "page_idx": 22}, {"type": "text", "text": "Next, for $R,\\tau\\geq0$ , we let $\\mathtt{B L a p}(R,\\tau)$ denote the bounded Laplace distribution with scale parameter $R$ and truncation threshold $\\tau$ be defined as the conditional distribution of $\\xi\\sim\\mathrm{Lap}(R)$ on the event $|\\xi|\\leq\\tau$ (recall that $\\operatorname{Lap}(R)$ has a density function $\\propto\\exp(-\\textstyle\\frac{1}{R}|\\xi|))$ . It is a standard calculation that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\xi\\sim\\mathrm{Lap}(R)}\\left[|\\xi|\\leq R\\log\\left(\\frac{1}{\\delta}\\right)\\right]=1-\\delta,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so that the total variation distance between $\\operatorname{Lap}(R)$ and $\\mathrm{BLap}(R,R\\log(\\frac{1}{\\delta}))$ is $\\delta$ . We hence have the following bounded generalization of the privacy given by the Laplace mechanism. ", "page_idx": 22}, {"type": "text", "text": "Lemma 9. Let $\\varepsilon,\\delta\\;\\in\\;(0,1)$ . If $S(\\mathcal{D})\\;\\in\\;\\mathbb{R}$ is a $\\Delta$ -sensitive statistic of the dataset $\\mathcal{D}$ , i.e. for neighboring datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ we have that $\\begin{array}{r}{|S(\\mathcal{D})-S(\\mathcal{D}^{\\prime})|\\leq\\Delta,}\\end{array}$ , then the bounded Laplace mechanism which outputs $S(\\mathcal{D})+\\xi$ where $\\begin{array}{r}{\\xi\\sim\\tt B L a p(\\frac{\\Delta}{\\varepsilon},\\tau)}\\end{array}$ for any $\\begin{array}{r}{\\tau\\geq\\frac{\\Delta}{\\varepsilon}\\log(\\frac{4}{\\delta})}\\end{array}$ satisfies $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 22}, {"type": "text", "text": "Proof. For notational simplicity, let $\\boldsymbol{\\mathcal{A}}$ denote the Laplace mechanism (which samples $\\begin{array}{r}{\\xi\\sim\\mathrm{Lap}(\\frac{\\Delta}{\\varepsilon})}\\end{array}$ instead of $\\begin{array}{r}{\\mathrm{BLap}(\\frac{\\Delta}{\\varepsilon},\\tau))}\\end{array}$ , let $\\overline{{\\mathcal{A}}}$ denote the bounded Laplace mechanism, and let $\\mathcal{E}\\subseteq\\mathbb{R}$ be an event in the outcome space. By standard guarantees on $(\\varepsilon,0)$ -DP of $\\boldsymbol{\\mathcal{A}}$ (e.g. Theorem 3.6, [DR14]), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\overline{{A}}(\\mathcal{D})\\in\\mathcal{E}\\right]\\leq\\operatorname*{Pr}\\left[A(\\mathcal{D})\\in\\mathcal{E}\\right]+\\frac{\\delta}{4}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp(\\varepsilon)\\operatorname*{Pr}\\left[A(\\mathcal{D}^{\\prime})\\in\\mathcal{E}\\right]+\\frac{\\delta}{4}\\leq\\exp(\\varepsilon)\\operatorname*{Pr}\\left[\\overline{{A}}(\\mathcal{D}^{\\prime})\\in\\mathcal{E}\\right]+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any neighboring datasets, where we used $\\exp(\\varepsilon)\\leq3$ and that the total variation distance between $({\\mathcal{A}}({\\mathcal{D}}),{\\overline{{{\\mathcal{A}}}}}({\\mathcal{D}}))$ and $({\\mathcal{A}}({\\mathcal{D}}^{\\prime}),{\\overline{{{\\mathcal{A}}}}}({\\mathcal{D}}^{\\prime}))$ are bounded by $\\frac{\\delta}{4}$ by (17). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "We also use the sparse vector technique (SVT) [DR14], which has been used recently in private optimization in the user-level setting [AL24]. Given an input dataset ${\\mathcal{D}}=\\{s_{i}\\}_{i\\in[n]}\\in{\\mathcal{S}}^{n}$ , SVT takes a stream of queries $q_{1},q_{2},\\dotsc,q_{T}:{\\mathcal{D}}\\rightarrow\\mathbb{R}$ in an online manner. We assume each $q_{i}$ is $\\Delta$ -sensitive, i.e. $\\lvert q_{i}(\\mathcal{D})-q_{i}(\\mathcal{D}^{\\prime})\\rvert\\leq\\Delta$ for neighboring datasets $D,{\\mathcal{D}}^{\\prime}\\in S^{n}$ . One notable difference is that our SVT algorithm will use the bounded Laplace mechanism, rather than the Laplace mechanism, but this distinction is handled similarly to Lemma 9. We provide a guarantee on this variant of SVT in Lemma 10, and pseudocode is provided as Algorithm 6. ", "page_idx": 22}, {"type": "text", "text": "Lemma 10. Let $\\delta,\\varepsilon\\in(0,1)$ and suppose ", "page_idx": 22}, {"type": "equation", "text": "$$\nR\\geq\\frac{6\\Delta}{\\varepsilon}\\sqrt{c\\log\\left(\\frac{5}{\\delta}\\right)},\\;\\tau\\geq R\\log\\left(\\frac{10T}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Algorithm $^{6}$ outputs a sequence of answers $\\{a_{i}\\in\\{\\perp,\\top\\}\\}_{i\\in[k]}.$ for some $k\\in[T].$ , and is $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 22}, {"type": "image", "img_path": "oX6aIl9f0Y/tmp/71d62bd281ced92a534864083523ceeaffe96c7e867d9879a6f1fb30035a7220.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Proof. The proof is analogous to Lemma 9. Let $\\boldsymbol{\\mathcal{A}}$ denote SVT run with Laplace noise in place of bounded Laplace noise (i.e. $\\tau=\\infty$ ), and let $\\overline{{\\mathcal{A}}}$ denote SVT run with bounded Laplace noise. We first claim that $\\boldsymbol{\\mathcal{A}}$ is $\\left(\\varepsilon,\\frac{\\delta}{5}\\right)$ -DP, which is immediate from Theorem 3.23 and Theorem 3.20 in [DR14]. ", "page_idx": 23}, {"type": "text", "text": "Next, by a union bound on all of the $\\leq2T$ random variables sampled, the total variation distance between $({\\mathcal{A}}({\\mathcal{D}}),{\\overline{{{\\mathcal{A}}}}}({\\mathcal{D}}))$ for any dataset $\\mathcal{D}$ is bounded by $\\frac{\\delta}{5}$ . Then, for neighboring datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ and some event $\\mathcal{E}$ in the outcome space, repeating the calculation (18), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\overline{{A}}(\\mathcal{D})\\in\\mathcal{E}\\right]\\leq\\operatorname*{Pr}\\left[A(\\mathcal{D})\\in\\mathcal{E}\\right]+\\frac{\\delta}{5}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\exp(\\varepsilon)\\operatorname*{Pr}\\left[A(\\mathcal{D}^{\\prime})\\in\\mathcal{E}\\right]+\\frac{\\delta}{5}+\\frac{\\delta}{5}\\leq\\exp(\\varepsilon)\\operatorname*{Pr}\\left[\\overline{{A}}(\\mathcal{D}^{\\prime})\\in\\mathcal{E}\\right]+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.2 Algorithm statement and analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present the full details of our algorithm (see Algorithm 7) and prove its corresponding guarantees, separating out the privacy analysis and utility analysis. ", "page_idx": 23}, {"type": "text", "text": "1 Input: Dataset $\\mathcal{D}\\in\\mathcal{S}^{n}$ , initial point $x_{0}\\in\\mathcal{X}$ , step size $\\eta\\in\\mathbb{R}_{>0}$ , parameters $C,c,\\omega\\in\\mathbb{R}_{>0}$ , privacy parameters $(\\varepsilon,\\delta)\\in\\mathbb{R}_{>0}^{2}$ ", "page_idx": 24}, {"type": "image", "img_path": "oX6aIl9f0Y/tmp/ca7e1571fe2b11b3bc1c18024020f9bc03ba3b565eac8180dcffa0c9bc0e25bb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "26 Return: $x_{I}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The following theorem summarizes the guarantees of Algorithm 7. ", "page_idx": 24}, {"type": "text", "text": "Theorem 3. Consider an instance of $k$ -heavy-tailed private SCO, following notation in Definition 4, and let $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x)}\\end{array}$ , and $\\varepsilon,\\delta\\in(0,1)$ . Algorithm 7 run with parameters ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\eta\\leftarrow\\operatorname*{min}\\left(\\sqrt{\\frac{4}{n}}\\cdot\\frac{D}{G_{2}},\\:\\frac{D I}{G_{k}n}\\cdot\\left(\\frac{n^{2}\\varepsilon^{2}}{14400d\\log^{2}(\\frac{15n}{\\delta})}\\right)^{\\frac{k-1}{2k}}\\right),}\\\\ {\\displaystyle C\\leftarrow2\\left(\\frac{G_{k}^{k}D I n\\varepsilon^{2}}{14400d\\eta\\log^{2}(\\frac{15n}{\\delta})}\\right)^{\\frac{1}{k+1}},\\:c\\leftarrow\\frac{240\\sqrt{d}\\log(\\frac{15n}{\\delta})}{\\varepsilon},\\:\\omega\\leftarrow\\frac{18}{\\varepsilon}\\sqrt{2c\\log\\left(\\frac{15}{\\delta}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is $(\\varepsilon,\\delta)$ -DP and outputs $x_{I}$ that satisfies, for a universal constant $C_{\\mathrm{smooth}}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(x_{k})-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq C_{\\mathrm{smooth}}\\left(\\frac{G_{2}D}{\\sqrt{n}}+G_{k}D\\cdot\\left(\\frac{\\sqrt{d\\log^{3}(\\frac{n}{\\delta})}}{n\\varepsilon}\\right)^{1-\\frac{1}{k}}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "assuming $f(\\cdot;s)$ is $\\beta$ -smooth for all $s\\in S$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\leq\\frac{\\varepsilon^{1.5}}{24000\\eta\\sqrt{d}\\log^{2}(\\frac{30n}{\\delta})}}\\\\ &{\\quad=\\Theta\\left(\\operatorname*{max}\\left(\\frac{G_{2}}{D}\\cdot\\frac{\\sqrt{n}\\varepsilon^{1.5}}{\\sqrt{d}\\log^{2}(\\frac{n}{\\delta})},\\,\\frac{G_{k}}{D}\\cdot\\frac{\\varepsilon^{1.5}n}{\\sqrt{d}\\log(n)\\log^{2}(\\frac{n}{\\delta})}\\cdot\\left(\\frac{d\\log^{2}(\\frac{n}{\\delta})}{n^{2}\\varepsilon^{2}}\\right)^{\\frac{k-1}{2k}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now proceed to prove Theorem 3. ", "page_idx": 24}, {"type": "text", "text": "Privacy proof overview. We first overview the structure of our privacy proof. Consider two neighboring datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ that differ on a single sample $s_{i,j_{0}}\\neq s_{i,j_{0}}^{\\prime}$ . The core argument used to prove privacy is controlling the total number of times when gradients are clipped, so we introduce the variable \u201ccount.\u201d Note that we have $\\|x_{i,j_{0}+1}-x_{i,j_{0}+1}^{\\prime}\\|=O(C\\eta)$ due to the clip operation. If no clip ever happened afterward, then we know $\\|x_{i,n_{i}}\\stackrel{\\sim}{-x_{i,n_{i}}^{\\prime}}\\|\\leq\\|x_{i,j_{0}+1}-x_{i,j_{0}+1}^{\\prime}\\|=O(C\\eta)$ due to our smoothness assumption (see Fact 3), which means the algorithm is private. When count is not too large, we can still bound the sensitivity between $\\|x_{i,n_{i}}-x_{i,n_{i}}^{\\prime}\\|$ by $O(C\\eta)$ . However, when the value of count is larger, there is a risk that the sensitivity of $x_{i,n_{i}}$ is not bounded as before, and hence we halt the algorithm when count exceeds some appropriate cutoff point ${\\hat{c}}_{i}$ . ", "page_idx": 25}, {"type": "text", "text": "One subtle difference between our algorithm and standard uses of SVT is that we add Laplace noise to the cutoff point $c$ to obtain a randomized cutoff ${\\hat{c}}_{i}$ . This is because the sensitivity of the count increment at the $j_{0}^{\\mathrm{th}}$ iteration of phase $i$ is bounded by one, even though $\\|\\nabla f(x_{i,j_{0}};s_{i,j_{0}})\\|-$ $\\|\\nabla f(x_{i,j_{0}}^{\\prime};s_{i,j_{0}}^{\\prime})\\|$ can be arbitrarily large. The guarantees of the bounded Laplace mechanism imply that the noise added in ${\\hat{c}}_{i}$ hence suffices to privatize count. ", "page_idx": 25}, {"type": "text", "text": "In summary, we can control the sensitivity between $\\lVert x_{i,j}-x_{i,j}^{\\prime}\\rVert$ for all $j$ due to the termination condition in Line 18 and our use of bounded Laplace noise, and hence can control the sensitivity of the query for $\\|\\nabla f(x_{i,j};s_{i,j})\\|-\\|\\nabla f(x_{i,j}^{\\prime};s_{i,j}^{\\prime})\\|$ for all $j\\neq j_{0}$ . By adding Laplace noise on the cutoff $c$ , we handle the issue of the sensitivity of the $j_{0}^{\\mathrm{th}}$ query $\\|\\nabla f(x_{i,j_{0}};s_{i,j_{0}})\\|$ being unbounded. If the algorithm succeeds and returns $x_{k}$ , we know the sensitivity $\\|x_{i,n_{i}}-x_{i,n_{i}}^{\\prime}\\|$ is $O(C\\eta_{i})$ and the privacy guarantee follows from the Gaussian mechanism. If the algorithm fails and outputs $\\perp$ , the privacy guarantee follows from the bounded Laplace noise on the cutoff point and the guarantees of SVT. ", "page_idx": 25}, {"type": "text", "text": "Privacy proof. We now provide our formal privacy analysis following this overview. To fix notation in the remainder of the privacy proof, we consider running Algorithm 7 on two neighboring datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ that differ on a single sample $s_{i,j_{0}}\\neq\\:s_{i,j_{0}}^{\\prime}$ , for some $\\overline{{i}}\\,\\in\\,[I]$ . By standard postprocessing properties of differential privacy, it suffices to argue that the $i^{\\mathrm{th}}$ phase (i.e. the run of the loop in Lines 3 to 25 corresponding to this value of $i$ ) is private, so we fix $i\\in[I]$ in the following discussion. We let $\\{x_{i,j}\\}_{j\\in[n_{i}]}$ and $\\{x_{i,j}^{\\prime}\\}_{j\\in[n_{i}]}$ be the iterates of the $i^{\\mathrm{th}}$ phase of Algorithm 7 using $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , and we let $Y_{i,j}$ and $Y_{i,j}^{\\prime}$ be the respective 0-1 indicator variables that count increases by 1 in iteration $j$ . We also let $\\mathrm{count}_{j}$ and $\\mathrm{count}_{j}^{\\prime}$ denote the values of count at the end of the $j^{\\mathrm{th}}$ iteration, and abusing notation we let ${\\hat{c}}_{i}$ , $\\hat{c}_{i}^{\\prime}$ be the values of ${\\hat{c}}_{i}$ in the $i^{\\mathrm{th}}$ phase when using $\\mathcal{D}$ or $\\mathcal{D}^{\\prime}$ respectively. Finally, we denote $\\begin{array}{r}{\\overline{{\\boldsymbol{x}}}_{i}:=\\frac{1}{n_{i}}\\sum_{j\\in[n_{i}]}\\boldsymbol{x}_{i,j}}\\end{array}$ and let ${\\overline{{x}}}_{i}^{\\prime}$ denote the average iterate using $\\mathcal{D}^{\\prime}$ similarly. ", "page_idx": 25}, {"type": "text", "text": "We first bound the sensitivity between the iterates $\\{x_{i,j}\\}_{j\\in[n_{i}]}$ and $\\{x_{i,j}^{\\prime}\\}_{j\\in[n_{i}]}$ in the following lemma, assuming $\\mathrm{count}_{j}$ and ${\\mathrm{count}}_{j^{\\prime}}$ are bounded. The proof is deferred to Appendix $\\mathrm{H}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 11. Let $t\\in[n_{i}]$ , and suppose that $192\\eta_{i}\\beta c\\le1$ and $C\\ \\geq8\\omega_{i}\\log(\\frac{30n_{i}}{\\delta})$ . $f\\mathrm{count}_{t}<\\hat{c}_{i}$ , $\\mathrm{count}_{t}^{\\prime}<\\hat{c}_{i}^{\\prime}$ , and $Y_{i,j}=Y_{i,j}^{\\prime}$ for all $j<t$ with $j\\neq j_{0}$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Vert x_{i,t}-x_{i,t}^{\\prime}\\Vert\\leq6C\\eta_{i}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using this bound on the sensitivity, we are now ready to prove privacy of the algorithm. ", "page_idx": 25}, {"type": "text", "text": "Lemma 12. Algorithm 7 is $(\\varepsilon,\\delta)$ -DP if it is run with parameters satisfying ", "page_idx": 25}, {"type": "equation", "text": "$$\nC\\geq8\\omega_{i}\\log\\left(\\frac{30n_{i}}{\\delta}\\right),\\;c\\geq\\frac{6}{\\varepsilon}\\log\\left(\\frac{12}{\\delta}\\right),\\;\\omega\\geq\\frac{18}{\\varepsilon}\\sqrt{2c\\log\\left(\\frac{15}{\\delta}\\right)},\\;192\\eta_{i}\\beta c\\leq1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Recall our assumption that $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ only differ in $s_{i,j_{0}}$ , the $j_{0}^{\\mathrm{th}}$ sample used in the $i^{\\mathrm{th}}$ phase of the algorithm. The privacy of all phases of the algorithm other than phase $i$ is immediate from postprocessing properties of DP, so it suffices to argue that phase $i$ is $(\\varepsilon,\\delta)$ -DP. Note also that the conditions of Lemma 11 are met after reparameterizing $\\delta\\gets\\frac{\\delta}{4}$ . We split our privacy argument into two cases, depending on whether the algorithm terminates on Line 18 or Line 26. ", "page_idx": 25}, {"type": "text", "text": "Termination on Line 18. We begin with the case where the algorithm outputs $\\bot$ . We introduce some simplifying notation. For iterations $S\\subseteq[n_{i}]$ , define $W_{S}:=\\{Y_{i,j}\\}_{j\\in S}$ to be the 0-1 indicator variables for whether count incremented on iterations $j\\in S$ (when run on $\\mathcal{D}$ ), and define $[W]_{S}:=$ $\\sum_{j\\in S}Y_{i,j}$ to be their sum. Similarly, define $W_{S}^{\\prime}$ and $[W^{\\prime}]_{S}$ for when the algorithm is run on $\\mathcal{D}^{\\prime}$ . Observe that the algorithm outputs $\\perp$ iff the following event occurs: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\nY_{i,j_{0}}+[W]_{[n_{i}]\\backslash\\{j_{0}\\}}\\geq\\hat{c}_{i}\\iff(Y_{i,j_{0}}-\\hat{c}_{i})+[W]_{[n_{i}]\\backslash[j_{0}]}\\geq-[W]_{[j_{0}-1]}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The right-hand side $-[W]_{[j_{0}-1]}$ is independent of whether the dataset used was $\\mathcal{D}$ or $\\mathcal{D}^{\\prime}$ , so it suffices to argue about the privacy loss of the random variables $Y_{i,j_{0}}\\textrm{--}\\hat{c}_{i}$ and $W_{[n_{i}]\\backslash[j_{0}]}$ as a function of the dataset used. First, $Y_{i,j_{0}}-c$ is clearly a 1-sensitive statistic, so Lemma 9 implies $Y_{i,j_{0}}-\\hat{c}_{i}$ is $\\left(\\frac{\\varepsilon}{3},\\frac{\\delta}{3}\\right)$ -indistinguishable as a function of the dataset used. Next, conditioning on the value of $Y_{i,j_{0}}-\\tilde{\\hat{c}}_{i}$ , the random variable $W_{[n_{i}]\\backslash[j_{0}]}$ is an instance of Algorithm 6 run with a fixed threshold $\\hat{c}_{i}\\,-\\,Y_{i,j_{0}}\\,-\\,[W]_{[j_{0}-1]}\\,\\le\\,2c$ , where we rename the output variables $\\{\\bot,\\top\\}$ to $\\{0,1\\}$ . Moreover, Lemma 11 and smoothness of each sample function implies that the sensitivity of each query $\\|\\nabla f(\\cdot;s_{i,j})\\|$ is bounded by $\\Delta:=6C\\eta_{i}\\beta$ . Therefore, Lemma 10 shows that $W_{[n_{i}]\\backslash[j_{0}]}$ is $\\left(\\frac{\\varepsilon}{3},\\frac{\\delta}{3}\\right)$ -indistinguishable, where we note that we adjusted constants appropriately in $\\omega$ and the failure probabilities everywhere. By basic composition of DP, this implies $Y_{i,j_{0}}\\,-\\,\\hat{c}_{i}\\,+\\,[W]_{[n_{i}]\\setminus[j_{0}]}$ (a postprocessing of $Y_{i,j_{0}}-\\hat{c}_{i}$ and $W_{[n_{i}]\\backslash[j_{0}]}\\mid Y_{i,j_{0}}-\\hat{c}_{i})$ is $\\textstyle({\\frac{2\\varepsilon}{3}},{\\frac{2\\delta}{3}})$ -DP, as required. ", "page_idx": 26}, {"type": "text", "text": "Termination on Line 26. Finally, we argue about the privacy when the algorithm does not terminate on Line 18. As before, the sensitivity of ${\\bar{x}}_{i}$ is bounded by $6C\\eta_{i}$ via Lemma 11 and the triangle inequality, conditioned on a $\\textstyle\\left({\\frac{2\\varepsilon}{3}},{\\frac{2\\delta}{3}}\\right)$ -indistinguishable event (i.e. the values of $Y_{i,j_{0}}\\textrm{--}\\hat{c}_{i}$ and $W_{[n_{i}]\\backslash[j_{0}]}\\quad$ $Y_{i,j_{0}}-\\hat{c}_{i})$ . Then $x_{i}$ is $\\textstyle\\left({\\frac{\\varepsilon}{3}},{\\frac{\\delta}{3}}\\right)$ -indistinguishable by standard bounds on the Gaussian mechanism (Theorem A.1, [DR14]), which completes the proof upon applying basic composition. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Utility proof. The utility proof follows the standard analysis of localized SGD algorithms and a specialized analysis of clipped SGD (Corollary 4). We first state a utility guarantee in each phase. ", "page_idx": 26}, {"type": "text", "text": "Lemma 13. Following notation in Algorithm 7, fix $i\\in[I]$ , and suppose $\\mathcal{D}\\sim\\mathcal{P}^{n}$ i.i.d. where $\\mathcal{P}$ satisfies Assumption 1. For any $x\\in\\mathscr{X}$ , i $\\begin{array}{r}{f C\\geq8\\omega_{i}\\log(\\frac{30n_{i}}{\\delta})}\\end{array}$ and $\\begin{array}{r}{\\frac{c}{4}\\geq\\operatorname*{max}(n\\cdot(\\frac{2G_{k}}{C})^{k},6\\log(n)),}\\end{array}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\mathcal{P}}(\\overline{{x}}_{i})-F_{\\mathcal{P}}(x)]\\leq\\frac{\\|x-x_{i-1}\\|^{2}}{2\\eta_{i}n_{i}}+2\\eta_{i}G_{2}^{2}+\\frac{G_{k}^{k}D}{(k-1)(\\frac{C}{2})^{k-1}}+\\frac{G_{2}D}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By Markov\u2019s inequality, $\\begin{array}{r}{\\mathbb{E}_{s\\sim\\mathcal{P}}[\\mathbb{I}_{L_{s}>\\frac{C}{2}}]\\le(\\frac{2G_{k}}{C})^{k}}\\end{array}$ , so the total number of expected samples with $\\begin{array}{r}{L_{s}>\\frac{C}{2}}\\end{array}$ is at most $\\frac{c}{4}$ . Hence by applying a Chernoff bound, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\underbrace{\\sum_{s\\in\\mathcal{D}}\\mathbb{I}_{L_{s}>\\frac{C}{2}}\\leq\\frac{c}{2}}_{:=\\mathcal{E}}\\right]\\geq1-\\frac{1}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Conditional on $\\mathcal{E}$ , the algorithm will not halt (i.e., return $\\bot$ ) and is running one-pass clipped-SGD (Algorithm 5) using the modified clipping operation defined in the precondition in Corollary 4. Then, the statement follows from Corollary 4 as follows: letting $\\mathcal{E}^{c}$ denote the complement of $\\mathcal{E}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[F_{P}(\\overline{{x}}_{i})-F_{P}(x)\\right]=\\mathbb{E}\\big[F_{P}(\\overline{{x}}_{i})-F_{P}(x)\\ |\\ \\mathcal{E}\\big]\\operatorname*{Pr}[\\mathcal{E}]+\\mathbb{E}\\big[F_{P}(\\overline{{x}}_{i})-F_{P}(x)\\ |\\ \\mathcal{E}^{c}\\big]\\operatorname*{Pr}[\\mathcal{E}^{c}]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\|x-x_{i-1}\\|^{2}}{2\\eta_{i}n_{i}}+2\\eta_{i}G_{2}^{2}+\\frac{G_{k}^{k}D}{(k-1)(\\frac{C}{2})^{k-1}}+\\mathbb{E}[F_{P}(\\overline{{x}}_{i})-F_{P}(x)\\ |\\ \\mathcal{E}^{c}]\\operatorname*{Pr}[\\mathcal{E}^{c}]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\|x-x_{i-1}\\|^{2}}{2\\eta_{i}n_{i}}+2\\eta_{i}G_{2}^{2}+\\frac{G_{k}^{k}D}{(k-1)(\\frac{C}{2})^{k-1}}+G_{2}D\\operatorname*{Pr}[\\mathcal{E}^{c}]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\|x-x_{i-1}\\|^{2}}{2\\eta_{i}n_{i}}+2\\eta_{i}G_{2}^{2}+\\frac{G_{k}^{k}D}{(k-1)(\\frac{C}{2})^{k-1}}+\\frac{G_{2}D}{n^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used that $F_{\\mathcal P}$ is $G_{1}\\leq G_{2}$ -Lipschitz by Lemma 2. ", "page_idx": 26}, {"type": "text", "text": "Combining our privacy and utility guarantees, we are ready to prove this section\u2019s main theorem. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 3. For simplicity, let ${\\bar{x}}_{0}:=x^{\\star}$ and $\\zeta_{0}:=x_{0}-x^{\\star}$ , so $\\|\\zeta_{0}\\|\\leq D$ by assumption. Also, suppose that $n$ is a power of 2, as the adjustment on Line 2 only affects $n$ (and hence the guarantees) by constant factors. The privacy claim follows immediately from Lemma 12 assuming its preconditions are met, which we verify at the end of the proof. By applying Lemma 13 in each phase $\\bar{i}\\in[I]$ to $x\\gets x_{i}$ , assuming its preconditions are met, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq\\displaystyle\\sum_{i\\in[I]}\\left(\\frac{\\mathbb{E}\\left[\\left\\Vert\\zeta_{i-1}\\right\\Vert^{2}\\right]}{2\\eta_{i}n_{i}}+2\\eta_{i}G_{2}^{2}+\\frac{G_{k}^{k}D}{(\\frac{C}{2})^{k-1}}\\right)+\\frac{G_{2}D I}{n^{2}}}&{}\\\\ {+\\,\\mathbb{E}\\left[F_{\\mathcal{P}}(x_{k})-F_{\\mathcal{P}}(\\bar{x}_{k})\\right]}&{}\\\\ {\\leq\\displaystyle\\frac{4D^{2}}{\\eta n}+\\frac{\\eta G_{2}^{2}}{2}+\\frac{G_{k}^{k}D I}{(\\frac{C}{2})^{k-1}}+\\frac{G_{2}D}{\\sqrt{n}}+G_{2}\\sigma_{I}\\sqrt{d}}&{}\\\\ {+\\displaystyle\\sum_{i\\in[I-1]}\\left(\\frac{3600C^{2}d\\eta_{i}\\log\\left(\\frac{3}{\\delta}\\right)}{n_{i}\\varepsilon^{2}}+\\frac{\\eta_{i}G_{2}^{2}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In the first inequality, we used $G_{1}\\,\\leq\\,G_{2}$ -Lipschitzness of $F_{\\mathcal{P}}$ by Lemma 2, and in the second inequality, we pulled out the $i=1$ term and adjusted indices, and bounded $I\\leq n$ and used Jensen\u2019s inequality to bound $(\\mathbb{E}\\left\\|\\zeta_{I}\\right\\|)^{2}\\,\\leq\\,\\mathbb{E}\\left\\|\\zeta_{I}\\right\\|^{2}\\,=\\,\\bar{\\sigma}_{I}^{2}d$ . Now using that $\\frac{\\eta_{i}}{n_{i}}$ and $\\eta_{i}$ are geometrically decaying sequences, we continue bounding the above display using our choice of $C$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq\\displaystyle\\frac{4D^{2}}{\\eta n}+\\eta G_{2}^{2}+\\frac{14400(\\frac{C}{2})^{2}d\\eta\\log(\\frac{3}{\\delta})}{n\\varepsilon^{2}}+\\frac{G_{k}^{k}D I}{(\\frac{C}{2})^{k-1}}+\\frac{G_{2}D}{\\sqrt{n}}+G_{2}\\sigma_{I}\\sqrt{d}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{4D^{2}}{\\eta n}+\\eta G_{2}^{2}+2(A\\eta)^{\\frac{k-1}{k+1}}\\left(G_{k}^{k}D I\\right)^{\\frac{2}{k+1}}+\\frac{G_{2}D}{\\sqrt{n}}+G_{2}\\sigma_{I}\\sqrt{d},}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{~for~}A:=\\displaystyle\\frac{14400d\\log^{2}(\\frac{15n}{\\delta})}{n\\varepsilon^{2}},\\;C=2\\left(\\frac{G_{k}^{k}D I}{A\\eta}\\right)^{\\frac{1}{k+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, plugging in our choice of ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left(\\underbrace{\\sqrt{\\frac{4}{n}}\\cdot\\frac{D}{G_{2}}}_{:=\\eta_{1}},\\underbrace{\\frac{D I}{G_{k}n}\\cdot\\left(\\frac{n}{A}\\right)^{\\frac{k-1}{2k}}}_{:=\\eta_{2}}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have the claimed utility bound upon simplifying, and using that $G_{2}\\sigma_{I}\\sqrt{d}$ is a low-order term. ", "page_idx": 27}, {"type": "text", "text": "We now verify our parameters satisfy the conditions in Lemma 12 and Lemma 13, which concludes the proof. First, it is straightforward to check that both sets of conditions are implied by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{96\\eta\\beta c}{\\sqrt{\\varepsilon}}\\log\\left(\\frac{30n}{\\delta}\\right)\\leq1,\\;c\\geq4n\\cdot\\left(\\frac{2G_{k}}{C}\\right)^{k},\\;\\mathrm{and}\\;c\\geq\\frac{26}{\\varepsilon}\\log\\left(\\frac{15n}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "given that we chose $\\begin{array}{r}{\\omega=\\frac{18}{\\varepsilon}\\sqrt{2c\\log(\\frac{15}{\\delta})}\\leq\\frac{c}{\\sqrt{\\varepsilon}}}\\end{array}$ . Indeed, $\\begin{array}{r}{C\\ge8\\omega_{i}\\log(\\frac{30n_{i}}{\\delta})\\iff2\\eta\\beta\\omega\\log(\\frac{30n}{\\delta})\\le}\\end{array}$ 1 which is subsumed by the first condition in (22). Clearly, $\\begin{array}{r}{c\\geq\\frac{26}{\\varepsilon}\\log(\\frac{15n}{\\delta})}\\end{array}$ , giving the third condition in (22). Next, a direct computation with the definition of $\\eta_{2}$ in (21) yields ", "page_idx": 27}, {"type": "equation", "text": "$$\nc=2\\sqrt{A n}=4n\\cdot\\sqrt{\\frac{A}{n}}=4n\\cdot\\left(G_{k}\\cdot\\left(\\frac{A\\eta_{2}}{G_{k}^{k}D I}\\right)^{\\frac{1}{k+1}}\\right)^{k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now because $C$ depends inversely on $\\eta\\leq\\eta_{2}$ defined in (21), the second condition in (22) holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\nc=4n\\cdot\\left(G_{k}\\cdot\\left(\\frac{A\\eta_{2}}{G_{k}^{k}D I}\\right)^{\\frac{1}{k+1}}\\right)^{k}\\geq4n\\cdot\\left(G_{k}\\cdot\\left(\\frac{A\\eta}{G_{k}^{k}D I}\\right)^{\\frac{1}{k+1}}\\right)^{k}=4n\\cdot\\left(\\frac{2G_{k}}{C}\\right)^{k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, the first condition in (22) now follows from our upper bound on $\\beta$ . ", "page_idx": 27}, {"type": "text", "text": "D Improved Smoothness Bounds for Generalized Linear Models ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we give an improved algorithm for heavy-tailed private SCO when the sample functions $f(x;s)$ are instances of a smooth generalized linear model (GLM). That is, we assume the sample space $\\mathcal{S}\\subseteq\\mathbb{R}^{d}$ , and that for a convex function $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x;s)=\\sigma\\left(\\langle s,x\\rangle\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We also assume that all $f(x;s)$ are $\\beta.$ -smooth. Observe that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\nabla f(x;s)=\\sigma^{\\prime}(\\langle s,x\\rangle)s,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so that for all $x\\in\\mathscr{X}$ , $\\nabla f(x;s)$ are all scalar multiples of the same vector $s$ . We prove that under this assumption, clipped gradient descent steps can only improve contraction, in contrast to Fact 16. ", "page_idx": 28}, {"type": "text", "text": "Lemma 14. Let $s,s^{\\prime}\\in\\mathbb{R}$ and let $\\boldsymbol{x},\\boldsymbol{x}^{\\prime},\\boldsymbol{g}\\in\\mathbb{R}^{d}$ . Assume that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|(x-s g)-(x^{\\prime}-s^{\\prime}g)\\right\\|\\le\\left\\|x-x^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then for any $C\\geq0$ , letting $t:=\\mathrm{sign}(s)\\operatorname*{min}(|s|,C)$ and $t^{\\prime}:=\\mathrm{sign}(s^{\\prime})\\operatorname*{min}(|s^{\\prime}|,C),$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|(x-t g)-(x^{\\prime}-t^{\\prime}g)\\|\\leq\\|x-x^{\\prime}\\|\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Note that the premise is impossible unless ${\\mathrm{sign}}(s-s^{\\prime})=\\operatorname{sign}(\\langle x-x^{\\prime},g\\rangle)$ . Without loss of generality, assume they are both nonnegative, else we can negate $s,s^{\\prime},g$ . In this case, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(x-x^{\\prime})-(s-s^{\\prime})g\\|\\le\\|x-x^{\\prime}\\|\\iff(s^{\\prime}-s)^{2}\\,\\|g\\|^{2}\\le2(s-s^{\\prime})\\,\\langle x-x^{\\prime},g\\rangle}\\\\ &{\\iff s-s^{\\prime}\\le\\displaystyle\\frac{2\\,\\langle x-x^{\\prime},g\\rangle}{\\|g\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, observe that $t-t^{\\prime}\\le s-s^{\\prime}$ and $\\mathrm{sign}(t-t^{\\prime})=\\mathrm{sign}(s-s^{\\prime})$ , for any value of $C\\geq0$ . Therefore, $\\begin{array}{r}{t-t^{\\prime}\\leq\\frac{2\\langle v,g\\rangle}{\\|g\\|^{2}}}\\end{array}$ as well, and we can reverse the above chain of implications. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Note that the premise of Lemma 14 is exactly an instance of Fact 3 where $\\nabla f(x)$ and $\\nabla f(x^{\\prime})$ are scalar multiples of the same direction, which is the case for GLMs by (24). Hence, Lemma 14 shows the contraction property in Fact 3 is preserved after clipping gradients (again, for GLMs). ", "page_idx": 28}, {"type": "text", "text": "We can now directly combine Lemma 8 and our contraction results, used to analyze the stability of Algorithm 5, with the iterative localization framework of [FKT20], Section 4. ", "page_idx": 28}, {"type": "text", "text": "$\\mathbf{Algorithm\\;8!:OnePass-Clipped-DP-SGD}(\\mathcal{D},n,\\mathcal{X},x_{0},\\rho)$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1 Input: Dataset ${\\mathcal{D}}=\\{s_{i}\\}_{i\\in[n]}\\in{\\mathcal{S}}^{n}$ , domain $\\mathcal{X}\\subset\\mathbb{B}(x_{0},D)$ for $x_{0}\\in\\mathcal{X}$   \n2 $I\\leftarrow\\lfloor\\log_{2}(n)\\rfloor$   \n3 n \u21902I   \n4 $\\begin{array}{r}{\\eta\\leftarrow\\operatorname*{min}(\\sqrt{\\frac{8}{n}}\\cdot\\frac{D}{G_{2}},\\frac{1}{n}\\cdot(\\frac{n^{2}\\rho}{32d})^{\\frac{k-1}{2k}}\\cdot\\frac{2^{\\frac{k+1}{2k}}D}{G_{k}}),C\\gets(\\frac{G_{k}^{k}D\\rho n}{32\\eta d})^{\\frac{1}{k+1}}}\\end{array}$   \n5 for $i\\in[I]$ do   \n6 $n_{i}\\leftarrow2^{-i}n,\\eta_{i}\\leftarrow16^{-i}\\eta,C_{i}\\leftarrow2^{i}C,\\sigma_{i}\\leftarrow2\\eta_{i}C_{i}\\cdot\\sqrt{\\frac{2}{\\rho}}$   \n7 $D_{i}\\leftarrow\\mathrm{first}\\:n_{i}$ elements of $\\mathcal{D}$ , $D\\leftarrow D\\setminus D_{i}$   \n8 $\\bar{x}_{i}\\gets$ $){\\mathsf{n e P a s s-C l i p p e d-S G D}}(\\mathscr{D}_{i},C_{i},\\eta_{i},n_{i},\\mathscr{X},x_{i-1})$   \n9 $\\boldsymbol\\xi_{i}\\sim\\mathcal{N}(\\mathbb{O}_{d},\\sigma_{i}^{2}\\mathbf{I}_{d})$   \n10 $x_{i}\\gets\\bar{x}_{i}+\\xi_{i}$   \n11 end ", "page_idx": 28}, {"type": "text", "text": "12 Return: ", "page_idx": 28}, {"type": "text", "text": "Theorem 4. Consider an instance of $k$ -heavy-tailed private SCO, following notation in Definition 4, let $\\begin{array}{r}{x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}F_{\\mathcal{P}}(x)}\\end{array}$ , and let $\\rho\\ge0$ . Further, assume that for a convex function $\\sigma$ , the sample functions $f(x;s)$ satisfy (23) for all $s\\in\\mathcal{S}\\subseteq\\mathbb{R}^{d}$ . Finally, assume $f(x;s)$ is $\\beta$ -smooth for all $s\\in S$ , where $\\begin{array}{r}{\\beta\\,\\le\\,\\operatorname*{max}(\\sqrt{\\frac{n}{2}}\\cdot\\frac{G_{2}}{D},n\\cdot\\bigl(\\frac{d}{n^{2}\\rho}\\bigr)^{\\frac{k-1}{2k}}\\cdot\\frac{G_{k}}{D})}\\end{array}$ . Algorithm 8 is a $\\rho$ -CDP algorithm which draws $\\mathcal{D}\\sim\\mathcal{P}^{n}$ , queries $n$ sample gradients (using samples in $\\mathcal{D}$ ), and outputs $x_{I}\\in\\mathcal{X}$ satisfying ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F_{\\mathcal{P}}(x_{I})-F_{\\mathcal{P}}(x^{\\star})\\right]\\leq4G_{2}D\\sqrt{\\frac{1}{n}}+26G_{k}D\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We begin with the privacy claim. Consider neighboring datasets $\\mathcal{D},\\,\\mathcal{D}^{\\prime}$ , and suppose the datasets differ on the $j^{\\mathrm{th}}$ entry such that $s_{j}\\in\\mathcal{D}_{i}$ (if the differing entry is not in $\\cup_{i\\in[I]}\\mathcal{D}_{i}$ , Algorithm 8 clearly satisfies 0-CDP). Let ${\\bar{x}}_{i}$ and ${\\bar{x}_{i}^{\\prime}}$ be the outputs of Line 8 when run with the same initialization $x_{i-1}$ , and neighboring $\\mathcal{D}_{i}$ , $\\mathcal{D}_{i}^{\\prime}$ . By the assumption on $\\beta$ , since $\\eta_{i}~\\leq~\\eta$ for all $i\\;\\in\\;[I]$ , we can apply Fact 3 and Lemma 14 (recalling the characterization (24)) to show $\\|\\bar{x}_{i}-\\bar{x}_{i}^{\\prime}\\|\\leq2\\eta_{i}C_{i}$ with probability 1. Therefore, by our choice of $\\sigma_{i}$ and the first and third parts of Lemma 1, the whole algorithm is $\\rho$ -CDP regardless of which $\\mathcal{D}_{i}$ contained the differing sample, since all other calls to OnePass-Clipped-SGD are 0-CDP as we can couple all randomness used by the calls. ", "page_idx": 29}, {"type": "text", "text": "Next, we prove the utility claim. For simplicity, let ${\\bar{x}}_{0}:=x^{\\star}$ and $\\xi_{0}:=x_{0}-x^{\\star}$ , so $\\|\\xi_{0}\\|\\le D$ by assumption. By applying Lemma 8 for all $i\\in[I]$ with $x_{0}\\leftarrow x_{i-1}$ and $u\\gets\\bar{x}_{i-1}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F_{P}(x_{I})-F_{P}(\\mathbf{x}^{*})\\right]=\\displaystyle\\sum_{i\\in[I]}\\mathbb{E}\\left[F_{P}(\\bar{x}_{i})-F_{P}(\\bar{x}_{i-1})\\right]+\\mathbb{E}\\left[F_{P}(x_{I})-F_{P}(\\bar{x}_{I})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i\\in[I]}\\left(\\mathbb{E}\\left[\\frac{\\|\\xi_{i-1}\\|^{2}}{2\\eta_{i}n_{i}}\\right]+\\frac{\\eta_{i}G_{2}^{2}}{2}+\\frac{G_{k}^{k}D}{(k-1)C_{i}^{k-1}}\\right)+G_{1}\\mathbb{E}\\left[\\|x_{I}-\\bar{x}_{I}\\|\\right]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{4D^{2}}{\\eta n}+\\sum_{i\\in[I-1]}2^{-i}\\left(\\frac{32d\\eta C^{2}}{\\rho n}+\\frac{\\eta G_{2}^{2}}{2}+\\frac{G_{k}^{k}D}{C^{k-1}}\\right)+\\sqrt{\\frac{8d}{\\rho}}G_{1}\\eta C\\cdot8^{-I}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{4D^{2}}{\\eta n}+\\frac{32d\\eta C^{2}}{\\rho n}+\\frac{\\eta G_{2}^{2}}{2}+\\frac{G_{k}^{k}D}{C^{k-1}}+24\\sqrt{\\frac{d}{\\rho}}\\cdot\\frac{G_{1}\\eta C}{n^{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second line applied Lemma 2, the third used Jensen\u2019s inequality to bound $\\mathbb{E}[\\|x_{I}-\\bar{x}_{I}\\|]^{2}\\leq$ $\\mathbb{E}[\\Vert x_{I}-\\bar{x}_{I}\\Vert^{2}]$ and our assumption $k\\geq2$ , and the last used the geometric decay of the different parameters. Finally, by plugging in our choices of $C,\\eta$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{4D^{2}}{\\eta n}+\\frac{\\eta G_{2}^{2}}{2}+\\frac{32d\\eta C^{2}}{\\rho n}+\\frac{G_{k}^{k}D}{C^{k-1}}=\\frac{4D^{2}}{\\eta n}+\\frac{\\eta G_{2}^{2}}{2}+2\\eta^{\\frac{k-1}{k+1}}\\left(G_{k}^{k}D\\right)^{\\frac{2}{k+1}}\\left(\\frac{32d}{\\rho n}\\right)^{\\frac{k-1}{k+1}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq G_{2}D\\sqrt{\\frac{8}{n}}+8G_{k}D\\left(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\right)^{1-\\frac{1}{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can also check that the final summand is a low-order term, by using $\\begin{array}{r}{\\eta\\le\\frac{1}{n}\\cdot\\left(\\frac{n^{2}\\rho}{32d}\\right)^{\\frac{k-1}{2k}}}\\end{array}$ \u00b7 $\\frac{2^{\\frac{k+1}{2k}}D}{G_{k}}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n24{\\sqrt{\\frac{d}{\\rho}}}\\cdot{\\frac{G_{1}\\eta C}{n^{3}}}\\leq{\\frac{5G_{k}D}{n^{2}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The conclusion follows by adjusting $n$ , since Algorithm 8 is run with a sample count in $[{\\textstyle{\\frac{n}{2}}},n]$ . ", "page_idx": 29}, {"type": "text", "text": "E High-probability stochastic convex optimization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, to highlight another application of our population-level localization framework, we show that it obtains improved high-probability guarantees for the following standard bounded-variance estimator parameterization of SCO in the non-private setting. ", "page_idx": 29}, {"type": "text", "text": "Definition 6 (Stochastic convex optimization). Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be compact and convex, with diam $\\left(\\mathcal{X}\\right)=$ $D$ . In the stochastic convex optimization (SCO) problem, there is a convex function $f:\\mathcal{X}\\to\\mathbb{R},$ and we have query access to a stochastic oracle $g:\\bar{\\mathcal{X}}\\rightarrow\\mathbb{R}^{d}$ satisfying, for all $x\\in\\mathscr{X}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[g(x)\\right]\\in\\partial f(x),\\;\\mathbb{E}\\left[\\left\\|g(x)\\right\\|^{2}\\right]\\le G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For a convex function $\\psi:\\mathcal{X}\\rightarrow\\mathbb{R},$ , our goal in SCO is to optimize the composite function $f+\\psi$ . ", "page_idx": 29}, {"type": "text", "text": "For instance, one can set $\\psi$ to the constant zero function to recover the non-composite variant of SCO. We include the composite variant of Definition 6 as it is a standard extension in the SCO literature, under the assumption that the function $\\psi$ is \u201csimple.\u201d The specific notion of simplicity we use is that $\\psi:\\mathcal{X}\\rightarrow\\mathbb{R}$ admits an efficient proximal oracle (Definition 7). ", "page_idx": 30}, {"type": "text", "text": "Definition 7 (Proximal oracle). Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be compact and convex. We say $\\scriptscriptstyle\\mathcal{O}$ is $a$ proximal oracle for a convex function $\\psi:\\mathcal{X}\\rightarrow\\mathbb{R}$ if for any inputs $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , $\\eta\\in\\mathbb{R}_{\\ge0}$ , $O(v)$ returns ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\left\\{\\frac{1}{2\\eta}\\left\\|x-v\\right\\|^{2}+\\psi(x)\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In Theorem 5, we give an algorithm which uses $n$ queries to each of $g$ and a proximal oracle for $\\psi$ , and achieves an error bound for $f+\\psi$ of ", "page_idx": 30}, {"type": "equation", "text": "$$\nO\\left(G D\\cdot{\\sqrt{\\frac{\\log{\\frac{1}{\\delta}}}{n}}}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability $\\geq1-\\delta$ . Similar rates are straightforward to derive using martingale concentration when the estimator $g$ is assumed to satisfy heavier tail bounds, such as a sub-Gaussian norm. To our knowledge, the rate (25) was first attained recently by [CH24], who also proved a matching lower bound. Our Theorem 5 gives an alternative route to achieving this error bound. As was the case in several recent works in the literature [HS16, DDXZ21, Lia24] who studied high-probability variants of stochastic convex optimization, our Theorem 5 is based on using geometric aggregation techniques within a proximal point method framework (in our case, using Fact 2 within Algorithm 2). However, these aforementioned prior works all assume additional smoothness bounds on the function $f$ . ", "page_idx": 30}, {"type": "text", "text": "We use the following standard result in the literature as a key subroutine. ", "page_idx": 30}, {"type": "text", "text": "Lemma 15 (Lemma 1, $[\\mathbf{ACJ}^{+}21]$ ). In the setting of Definition $6$ , assume $\\psi$ is $\\lambda$ -strongly convex, let $x^{\\star}:=\\operatorname*{argmin}_{x\\in\\mathcal{X}}f(x)+\\psi(x)$ , and let $T\\in\\mathbb N$ . There is an algorithm which queries the stochastic oracle $g$ and a proximal oracle for $\\psi$ each $T$ times, and produces $\\bar{x}$ satisfying, with probability $\\geq\\frac{4}{5}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|{\\bar{x}}-x^{\\star}\\|\\leq{\\frac{30G}{\\lambda{\\sqrt{T}}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We combine Lemma 15 with Proposition 2 to obtain the following high-probability SCO algorithm. ", "page_idx": 30}, {"type": "text", "text": "Theorem 5. Consider an instance of SCO, following notation in Definition $\\delta,$ , let $n\\,\\in\\,{\\mathbb N}_{\\!}$ , $x^{\\star}:=$ ${\\mathrm{argmin}}_{x\\in{\\mathcal{X}}}\\,f(x)+\\psi(x)$ , and $\\delta\\in(0,\\frac{1}{2})$ . There is an algorithm using $n$ queries to $g$ and a proximal oracle for $\\psi$ and outputs $x\\in\\mathscr{X}$ satisfying, for a universal constant $C_{\\mathrm{sco}}$ , with probability $\\geq1-\\delta$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(x)+\\psi(x)-f(x^{\\star})-\\psi(x^{\\star})\\leq C_{\\mathrm{sco}}\\cdot G D\\cdot{\\sqrt{\\frac{\\log{\\frac{1}{\\delta}}}{n}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Assume without loss of generality that $\\frac{1}{\\delta}$ is a sufficiently large constant (else we can adjust the constant factor $C_{\\mathrm{sco.}}$ ), and that $n$ is sufficiently larger than $\\log{\\frac{1}{\\delta}}$ (else the result holds because the range of the function is bounded by $G D$ ). We instantiate Proposition 2 with $F_{\\mathcal{P}}\\,\\leftarrow\\,f+\\psi$ , $I\\gets{\\frac{1}{2}}\\,{\\overline{{\\log_{2}n}}}$ , and in each phase $i\\in[I]$ of Algorithm 2, we let $\\textstyle n_{i}:={\\frac{n}{2^{i}}}$ . In the remainder of the proof, we describe how to implement (8) in the $i^{\\mathrm{th}}$ phase, where $F_{\\mathcal{P}}\\gets f+\\psi$ , splitting into cases. ", "page_idx": 30}, {"type": "text", "text": "If $\\frac{1}{\\delta}$ is bounded by polylog $(n)$ and $n$ is sufficiently large, suppose that $n$ is a power of 4, else we can use fewer queries and lose a constant factor in the guarantee. Then we can use a batch of $n_{i}$ consecutive queries, divided into $48\\log(\\frac{1}{\\delta_{i}})$ portions, where $\\begin{array}{r}{\\delta_{i}:=\\frac{\\delta}{2^{i}}}\\end{array}$ . We then use Lemma 15 on each portion of queries, with $f\\leftarrow f$ and $\\begin{array}{r}{\\psi\\leftarrow\\psi+\\frac{\\lambda_{i}}{2}\\left\\|\\cdot-x_{i-1}\\right\\|^{2}}\\end{array}$ ; it is straightforward to see that Definition 7 generalizes to give a proximal oracle for this new $\\psi$ . A Chernoff bound shows that at least $\\frac{3}{5}$ of the portions will return a point satisfying the bound in Lemma 15 except with probability $\\delta_{i}$ , so Fact 2 returns us a point at distance at most $\\frac{90G}{\\lambda{\\sqrt{T}}}$ from $\\boldsymbol{x}_{i}^{\\star}$ , where ", "page_idx": 30}, {"type": "equation", "text": "$$\nT=\\Omega\\left(\\frac{n_{i}}{\\log\\frac{1}{\\delta_{i}}}\\right)=\\Omega\\left(\\frac{n}{2^{i}\\left(\\log\\frac{1}{\\delta}+i\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(accounting for rounding error). Therefore, (8) holds with ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta=\\frac{C_{\\mathrm{sco}}}{2}\\cdot G\\cdot\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{n}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for sufficiently large $C_{\\mathrm{sco}}$ . Proposition 2 then implies that Algorithm 2 outputs $x$ satisfying ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(x)+\\psi(x)-f(x^{\\star})-\\psi(x^{\\star})\\leq2G D\\cdot\\sqrt{\\frac{\\Delta}{n^{1.5}}}+\\frac{C_{\\mathrm{sco}}}{2}\\cdot G D\\cdot\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{n}}\\leq C_{\\mathrm{sco}}\\cdot G D\\cdot\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{n}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we use that $G_{1}\\leq G$ by Jensen\u2019s inequality and our second moment bound in Definition 6. The failure probability follows from a union bound because we ensured that $\\sum_{i\\in[I]}\\delta_{i}\\leq\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "Finally, if $\\frac{1}{\\delta}$ is larger than polylog $(n)$ , then we let $I,J\\in\\mathbb{N}$ be chosen such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nI:=\\left\\lfloor\\log_{2}\\left({\\frac{n}{J}}\\right)\\right\\rfloor,\\ J\\geq48\\log\\left({\\frac{I}{\\delta}}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is achievable with $I=O(\\log n)$ and $\\begin{array}{r}{J=O(\\log\\frac{\\log n}{\\delta})=O(\\log\\frac{1}{\\delta})}\\end{array}$ . Let $m:=\\textstyle{\\frac{n}{J}}$ , and assume without loss that $m$ is a power of 2, which we can guarantee by discarding $\\leq\\frac{1}{2}$ our queries, losing a constant factor in the error bound. The remainder of the proof follows identically to the first part of this proof, where we union bound over $I$ phases, the $i^{\\mathrm{th}}$ mof which uses $J$ batches of $\\frac{m}{2^{i}}$ unused queries. Again we may apply Lemma 15 and Fact 2 with $\\textstyle T={\\frac{m}{2^{i}}}$ , so (8) holds with ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta=\\frac{C_{\\mathrm{sco}}}{2}\\cdot G\\cdot\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{n}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "except with probability $\\frac{\\delta}{I}$ . The conclusion then follows from Proposition 2. ", "page_idx": 31}, {"type": "text", "text": "F Non-contraction of truncated contractive steps ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we demonstrate that a natural conjecture related to the performance of clipped private gradient algorithms in the smooth setting is false. We state this below as Conjecture 1. To motivate it, suppose $v$ is the difference between a current pair of coupled iterates of a private gradient algorithm instantiated on neighboring datasets, and suppose the differing sample function has already been encountered. If we take a coupled gradient step in a sufficiently smooth function, Fact 3 shows that the step is a contraction. However, to preserve privacy in the heavy-tailed setting, it is natural to ask whether such a contractive step remains contractive after the gradients are clipped, i.e. the statement of Conjecture 1 (which gives the freedom for $C$ to be lower bounded). ", "page_idx": 31}, {"type": "text", "text": "Conjecture 1. Let $\\|\\boldsymbol{v}\\|_{2}\\leq C$ for a sufficiently large constant $C$ , and let $\\|v-(g-h)\\|\\leq\\|v\\|$ . Let $g^{\\prime}=\\Pi_{1}(g)$ and $h^{\\prime}=\\Pi_{1}(h)$ .9Then, $\\left\\|v-(g^{\\prime}-h^{\\prime})\\right\\|\\leq C$ . ", "page_idx": 31}, {"type": "text", "text": "We strongly refute Conjecture 1, by disproving it for any $C\\geq0$ . We remark that Lemma 16 does not necessarily rule out this approach to designing heavy-tailed DP-SCO algorithms in the smooth regime, but demonstrates an obstacle if additional structure of gradients is not exploited. ", "page_idx": 31}, {"type": "text", "text": "Lemma 16. Conjecture $^{\\,l}$ is false for any choice of $C\\geq0$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. We give a 2-dimensional counterexample. Let ", "page_idx": 31}, {"type": "equation", "text": "$$\nv=\\left(\\!\\!\\begin{array}{c}{{-C}}\\\\ {{0}}\\end{array}\\!\\!\\right),\\ g=\\left(\\!\\!\\begin{array}{c}{{1}}\\\\ {{0}}\\end{array}\\!\\!\\right),\\ h=\\left(\\!\\!\\begin{array}{c}{{\\frac{2C+1}{C+1}}}\\\\ {{\\frac{C\\sqrt{2C+1}}{C+1}}}\\end{array}\\!\\!\\right)=\\sqrt{2C+1}\\underbrace{\\left(\\!\\!\\begin{array}{c}{{\\frac{\\sqrt{2C+1}}{C+1}}}\\\\ {{\\frac{C}{C+1}}}\\end{array}\\!\\!\\right)}_{:=h^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Observe that ", "page_idx": 31}, {"type": "equation", "text": "$$\nv-(g-h)=\\left(\\!\\!\\begin{array}{c}{{-(C+1)+\\frac{2C+1}{C+1}}}\\\\ {{\\frac{C\\sqrt{2C+1}}{C+1}}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c}{{\\frac{-C^{2}}{C+1}}}\\\\ {{\\frac{C\\sqrt{2C+1}}{C+1}}}\\end{array}\\!\\!\\right)=C\\left(\\!\\!\\begin{array}{c}{{\\frac{-C}{C+1}}}\\\\ {{\\frac{\\sqrt{2C+1}}{C+1}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "9By scale-invariance of the claim, the assumption that the truncation threshold is $1$ is without loss of generality. ", "page_idx": 31}, {"type": "text", "text": "It is easy to verify $\\|v-(g-h)\\|=C$ at this point. Moreover, ", "page_idx": 32}, {"type": "equation", "text": "$$\nv-(g^{\\prime}-h^{\\prime})=\\left(\\!\\!\\begin{array}{c}{{-(C+1)+\\frac{\\sqrt{2C+1}}{C+1}}}\\\\ {{\\frac{C}{C+1}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $C\\geq0$ , the first coordinate of this vector is already less than $-C$ . ", "page_idx": 32}, {"type": "text", "text": "G Non-decay of empirical squared bias ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we present an obstacle towards a natural approach to improving the logarithmic terms in our algorithm in Section 3. We follow the notation of Section 3.1, i.e. for samples $\\{i\\ {\\stackrel{-}{\\equiv}}\\ s_{i}\\}_{i\\in[n]}\\sim{\\mathcal{P}}^{n}$ , we define sample functions $f_{i}\\equiv f(\\cdot;s_{i})$ , and let ", "page_idx": 32}, {"type": "equation", "text": "$$\nb_{\\mathcal{D}}:=\\operatorname*{max}_{x\\in\\mathcal{X}}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\nabla f_{i}(x)-\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x))\\right\\|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "A basic bottleneck with known approaches following SCO-to-ERM reductions is that they require a strongly convex ERM solver as a primitive, due to known barriers to generalization in SCO without strong convexity (see e.g. discussion in [SSSSS09]). This poses an issue in the heavy-tailed setting, because standard analyses of strongly convex clipped SGD (see e.g. our Proposition 1) appear to suffer a dependence on $b_{\\mathcal{D}}^{2}$ in the utility bound, which upon taking expectations requires bounding ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}b_{\\mathcal{D}}^{2}=\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\operatorname*{max}_{x\\in\\mathcal{X}}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\nabla f_{i}(x)-\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x))\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall from Lemma 3 that it is straightforward to bound $\\begin{array}{r}{\\mathbb{E}b_{\\mathcal{D}}\\leq\\frac{G_{k}^{k}}{C^{k-1}}}\\end{array}$ , due to Fact 1. Bounding $\\mathbb{E}b_{\\mathcal{D}}^{2}$ is more problematic; in [LR23], requiring this bound resulted in a dependence on $G_{2k}$ as opposed to $G_{k}$ (see the proof of Theorem 31), which we avoid (up to a polylogarithmic overhead) via our population-level localization strategy. We now present an alternative strategy to bound (27), avoiding a $G_{2k}$ dependence. Observe that, by using $(a+b+c)^{2}\\leq3(a^{2}+b^{2}+c^{\\tilde{2}})$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}b_{\\mathcal{D}}^{2}\\leq3\\underbrace{\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\underset{x\\in\\mathcal{M}}{\\operatorname*{max}}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\nabla f_{i}(x)-\\nabla F_{\\mathcal{P}}(x)\\right\\|^{2}\\right]}_{:=T_{1}}}\\\\ &{\\phantom{\\sum_{\\mathcal{D}\\sim\\mathcal{P}^{n}}}+3\\underbrace{\\underset{x\\in\\mathcal{X}}{\\operatorname*{max}}\\left\\|\\nabla F_{\\mathcal{P}}(x)-\\mathbb{E}_{s\\sim\\mathcal{P}}[\\Pi_{C}(\\nabla f(x;s))]\\right\\|^{2}}_{:=T_{2}}}\\\\ &{\\phantom{\\sum_{\\mathcal{D}\\sim\\mathcal{P}^{n}}}+3\\underbrace{\\mathbb{E}_{\\mathcal{D}\\sim\\mathcal{P}^{n}}\\left[\\underset{x\\in\\mathcal{X}}{\\operatorname*{max}}\\left\\|\\frac{1}{n}\\sum_{i\\in[n]}\\Pi_{C}(\\nabla f_{i}(x))-\\mathbb{E}_{s\\sim\\mathcal{P}}[\\Pi_{C}(\\nabla f(x;s))]\\right\\|^{2}\\right].}_{:=K_{1}\\in[n]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We focus on $T_{1}$ , as $T_{3}$ can be bounded by similar means (as truncation can only improve moment bounds), and $\\begin{array}{r}{T_{2}\\leq\\frac{G_{k}^{2k}}{C^{2(k-1)}}}\\end{array}$ C2G(kk\u22121) via Fact 1. Hence, if we can show that T1 = O( Gn2 ) under the moment bound assumption in Assumption 1, we can avoid the logarithmic factors lost by our population localization approach. We suggest the following conjecture as an abstraction of this bound. ", "page_idx": 32}, {"type": "text", "text": "Conjecture 2. Let $\\mathcal{P}$ be a distribution over $\\boldsymbol{S}$ . For each $x\\in\\mathscr{X}$ , let $\\boldsymbol{g}(\\boldsymbol{x};s)\\in\\mathbb{R}^{d}$ be a random vector, indexed by $s\\sim S$ , satisfying $\\mathbb{E}_{s\\sim\\mathcal{P}}[g(x;s)]=\\mathbb{0}_{d}$ and $\\begin{array}{r}{\\mathbb{E}_{s\\sim\\mathcal{P}}[\\operatorname*{sup}_{\\boldsymbol{x}\\in\\mathcal{X}}\\|\\boldsymbol{g}(\\boldsymbol{x};s)\\|^{2}]\\leq1}\\end{array}$ . Finally for $S\\sim\\mathcal{P}^{n}$ and $x\\in\\mathscr{X}$ , let $\\begin{array}{r}{g(x;S):=\\frac{1}{n}\\sum_{s\\in S}g(x;s)}\\end{array}$ . Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\left[\\operatorname*{sup}_{x\\in\\mathcal{X}}g(x;S)^{2}\\right]=O\\left({\\frac{1}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that the bound in Conjecture 2 exactly corresponds to $T_{1}$ in (28), after rescaling all sample gradients by $\\frac{1}{G_{2}}$ , and centering them by subtracting $\\bar{\\nabla}F_{\\mathcal P}(x)$ . Hence, if Conjecture 2 is true, it would yield the following desirable bound in (28): ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathscr{D}\\sim\\mathscr{P}^{n}}b_{\\mathscr{D}}^{2}=O\\left(\\frac{G_{2}^{2}}{n}+\\frac{G_{k}^{k}}{(k-1)C^{k-1}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Moreover, it is simple to prove a bound of $O(1)$ on the right-hand side of Conjecture 2, and as $n\\to\\infty$ it is reasonable to suppose $g(x;S)\\rightarrow\\mathbb{O}_{d}$ for all $x\\in\\mathscr{X}$ . Nonetheless, we refute Conjecture 2 in full generality with a simple 1-dimensional example. ", "page_idx": 33}, {"type": "text", "text": "Lemma 17. Conjecture 2 is false. ", "page_idx": 33}, {"type": "text", "text": "Proof. Let ${\\cal S}=[0,1]$ and let $\\mathcal{P}$ be the uniform distribution over $\\boldsymbol{S}$ . Let $\\mathcal{X}$ index a set of random $g(x;\\cdot):[0,1]\\rightarrow[0,\\bar{1}]$ which are nonzero at finitely many points.10 Then $\\mathbb{E}_{s\\sim\\mathcal{P}}g(x;s)=0$ for all $x\\in\\mathscr{X}$ , and $g(x;s)^{2}\\leq1$ for all $x\\in\\mathcal{X},s\\in\\mathcal{S}$ . However, for any $S\\in[0,1]^{n}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in{\\mathcal{X}}}g(x;S)^{2}=1.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "While Lemma 17 does not rule out the approach suggested in (28) (or other approaches) to improve the analysis of strongly convex ERM solvers in heavy-tailed settings, it presents an obstacle to applying the natural decomposition strategy in (28). To overcome Lemma 17, one must either use more structure about the index set $\\mathcal{X}$ or the iterates encountered by the algorithm, or consider a different decomposition strategy for bounding the squared empirical bias. ", "page_idx": 33}, {"type": "text", "text": "H Proof of Lemma 11 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we prove Lemma 11. We first require the following standard fact (see e.g. [Sch14]). ", "page_idx": 33}, {"type": "text", "text": "Fact 4. Let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be a convex set. Then for any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\Pi_{K}(x)-\\Pi_{K}(y)\\|\\leq\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now set up some notation. Let $\\{\\psi_{j}:\\mathcal{X}\\to\\mathcal{X}\\}_{j\\in[T]}$ and $\\{\\phi_{j}:\\mathcal{X}\\to\\mathcal{X}\\}_{j\\in[T]}$ be two sequences of operations. We say that an operation pair $(\\psi,\\phi)$ is contractive if for any two points $x,y\\in\\mathcal{X}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\psi(x)-\\phi(y)\\|\\leq\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We say an operation pair $(\\psi,\\phi)$ is $(C,\\zeta)$ -contractive if for any $x,y$ where $\\|x-y\\|\\leq C$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\psi(x)-\\phi(y)\\|\\leq\\|x-y\\|+\\zeta.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $\\psi^{j}(x)=\\psi_{j}\\circ\\psi_{j-1}\\circ...\\circ\\psi_{1}(x)$ , and define $\\phi^{j}$ similarly, for all $j\\in[T]$ . ", "page_idx": 33}, {"type": "text", "text": "We prove Lemma 11 as a consequence of the following more general result. ", "page_idx": 33}, {"type": "text", "text": "Lemma 18. Let $x_{0}=x_{0}^{\\prime}\\in\\mathcal{X}$ , and consider two sequences of operations $\\{\\psi_{j}:\\mathcal{X}\\to\\mathcal{X}\\}_{j\\in[T]}$ and $\\{\\psi_{j}^{\\prime}:\\mathcal{X}\\to\\mathcal{X}\\}_{j\\in[T]}$ satisfying the following conditions, for $\\begin{array}{r}{c:=\\left\\lfloor\\frac{C}{\\zeta}\\right\\rfloor}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "1. For at least $T-c-1$ indices $j\\in[T],\\,(\\psi_{j},\\phi_{j})$ is contractive.   \n2. At most one operation pair, $(\\psi_{k},\\psi_{k})$ , is $(\\infty,C)$ -contractive.   \n3. For at most c indices $j\\in[T],$ , $(\\psi_{j},\\phi_{j})$ is $(2C,\\zeta)$ -contractive. ", "page_idx": 33}, {"type": "text", "text": "Then for all $j\\in[T]$ , we have that $\\begin{array}{r}{\\|\\psi^{j}(x_{0})-\\phi^{j}(y_{0})\\|\\leq2C.}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Define $\\Delta_{j}:=\\|\\psi^{j}(x_{0})-\\phi^{j}(x_{0}^{\\prime})\\|$ for all $j\\in[T]$ . Let $a_{j}\\leq c$ be the total number of $(2C,\\zeta)$ - contractive operation pairs $(\\psi_{i},\\phi_{i})$ where $i\\leq j$ , and let $b_{j}$ be the 0-1 indicator variable for $k\\leq j$ . We use induction to show that $\\Delta_{j}\\le a_{j}\\zeta+b_{j}C$ . When $j=1$ , the claim holds. Now if the claim holds for $j-1$ , then $\\Delta_{j-1}\\leq a_{j-1}\\zeta+b_{j-1}C\\leq2C$ . Hence, by definition, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Delta_{j}\\le\\Delta_{j-1}+(a_{j}-a_{j-1})\\zeta+(b_{j}-b_{j-1})C=a_{j}\\zeta+b_{j}C,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which completes our induction. This also implies $\\Delta_{T}\\leq2C$ as claimed. ", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma $_{l l}$ . Throughout the following proof, note that $\\hat{c}_{i}\\leq2c$ deterministically (due to our use of $\\begin{array}{r}{\\dot{\\mathbf{B}}\\mathrm{Lap}\\big(\\frac{3}{\\varepsilon},c\\big)}\\end{array}$ noise), and under the stated parameter bounds, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{C}\\in\\left[\\frac{7C}{8},\\frac{9C}{8}\\right]\\mathrm{~and~}|\\nu_{i,j}|\\leq\\frac{C}{4}\\mathrm{~for~all~}j\\in[n_{i}].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $\\{g_{i,j}=\\Pi_{C}(\\nabla f(x_{i,j};s_{i,j}))\\}_{j\\in[n_{i}]}$ and $\\{g_{i,j}^{\\prime}=\\Pi_{C}(\\nabla f(x_{i,j}^{\\prime};s_{i,j}^{\\prime}))\\}$ be the two truncated gradient sequences in the $i^{\\mathrm{th}}$ phase corresponding to the two datasets, and let $\\{x_{i,j}\\}_{j\\in[n_{i}]}$ and $\\{x_{i,j}^{\\prime}\\}_{j\\in[n_{i}]}$ be the corresponding iterate sequences. We set the operation sequences $\\psi_{j}(x):=\\Pi_{\\mathcal{X}}\\big(x-\\eta_{i}g_{i,j}\\big)$ and $\\phi_{j}(x):=\\bar{\\Pi_{\\mathscr{X}}}(x-\\bar{\\eta}_{i}g_{i,j}^{\\prime})$ . We bound the contractivity of these operation pairs and apply Lemma 18. ", "page_idx": 34}, {"type": "text", "text": "First, note that because $\\mathrm{count}_{t}$ , $\\mathrm{count}_{t}^{\\prime}\\;<\\;\\hat{c}_{i}\\;\\leq\\;2c$ , the operation pair $(\\psi_{j},\\phi_{j})$ is an identical untruncated gradient mapping for at least $t-2c-1$ indices $j\\in[t]$ . Because we assume each sample function $f(\\cdot;s)$ is $\\beta$ -smooth, it follows that for these indices $j\\in[t]$ , the operation pair $(\\psi_{j},\\phi_{j}\\bar{)}$ is contractive, by applying Fact 3, Fact 4, and $\\eta_{i}\\beta\\leq1$ . ", "page_idx": 34}, {"type": "text", "text": "sbsyu maspstiuomn ptthiaotn t,h ae ndda tsaismetils $\\mathcal{D},\\mathcal{D}^{\\prime}$ $j_{0}^{\\mathrm{th}}$ sfaomllpolwe so tnhlay.t  tBheec aoupseer $\\left|\\left|g_{i,j_{0}}\\right|\\right|\\leq$ $\\begin{array}{l}{{{\\frac{9C}{8}}+{\\frac{C}{4}}\\,\\leq\\,{\\frac{11C}{8}}}}\\end{array}$ $\\begin{array}{r}{\\|g_{i,j_{0}}^{\\prime}\\|\\,\\leq\\,\\frac{11C}{8}}\\end{array}$ $(\\psi_{j_{0}},\\phi_{j_{0}})$ is $(\\infty,3C\\eta_{i})$ -contractive by applying the triangle inequality and Fact 4. ", "page_idx": 34}, {"type": "text", "text": "For all remaining indices $j\\in[t],\\mathrm{count}_{t}$ and $\\mathrm{count}_{t}^{\\prime}$ both incremented (under the assumption that $Y_{i,j}\\;=\\;Y_{i,j}^{\\prime}\\;$ for these indices). We claim that $(\\psi_{j},\\phi_{j})$ is $(6\\eta_{i}C,12\\eta_{i}^{2}C\\beta)$ -contractive for these iterations. To see this, we bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\psi_{j}(x_{i,j})-\\phi_{j}(x_{i,j}^{\\prime})\\right\\|\\leq\\left\\|(x_{i,j}-\\eta_{i}g_{i,j})-(x_{i,j}^{\\prime}-\\eta_{i}g_{i,j}^{\\prime})\\right\\|}&{}\\\\ {\\leq\\left\\|(x_{i,j}-\\eta_{i}\\nabla f(x_{i,j};s_{i,j}))-(x_{i,j}^{\\prime}-\\eta_{i}\\nabla f(x_{i,j}^{\\prime};s_{i,j}))\\right\\|}&{}\\\\ {+\\;\\eta_{i}\\left\\|\\nabla f(x_{i,j};s_{i,j})-\\nabla f(x_{i,j}^{\\prime};s_{i,j})\\right\\|+\\eta_{i}\\left\\|g_{i,j}-g_{i,j}^{\\prime}\\right\\|}&{}\\\\ {\\leq\\left\\|x_{i,j}-x_{i,j}^{\\prime}\\right\\|+12\\eta_{i}^{2}C\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The first line used Fact 4, the second used the triangle inequality, and the last used Fact 3, Fact 4, and the fact that $\\|\\nabla f(x_{i,j};s_{i,j})-\\nabla f(x_{i,j}^{\\prime};s_{i,j})\\|\\le6\\eta_{i}C\\bar{\\beta}$ by smoothness, when $\\|\\boldsymbol{x}_{i,j}-\\boldsymbol{x}_{i,j}^{\\prime}\\|\\le$ 6C\u03b7i. ", "page_idx": 34}, {"type": "text", "text": "Finally, it suffices to apply Lemma 18 with $C\\gets3C\\eta_{i}$ , $\\zeta\\leftarrow12\\eta_{i}^{2}C\\beta$ , and $c\\gets2c$ , which we can check meets the conditions of Lemma 18 under the stated parameter bounds. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide new algorithms with improved guarantees for heavy-tailed private SCO in several settings, which is what we claim in the abstract and introduction. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper discusses places where the main result is lossy, providing an appendix section dedicated towards improving it to be optimal. It also compares its results to another similar bound in the literature, discussing regimes where our bound is weaker. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 35}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide a full set of verifiable details for all of our theoretical results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. ", "page_idx": 36}, {"type": "text", "text": "(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. ", "page_idx": 37}, {"type": "text", "text": "(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We read the ethics guidelines and believe we meet them. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: We do not believe our paper poses a significant negative societal impact, as it is about making existing learning algorithms differentially private under less stringent distributional assumptions, which we do not foresee being used in any significant malicious cases. We do believe that our algorithms can have positive societal impacts, but do not wish to overclaim to this effect because our results are primarily theoretical. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release data or models. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]