[{"type": "text", "text": "Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quoc Tran-Dinh Trang H. Tran   \nDepartment of Statistics and Operations Research School of OR and Information Engineering   \nThe University of North Carolina at Chapel Hill Cornell University, Ithaca, NY quoctd@email.unc.edu htt27@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Lam M. Nguyen ", "page_idx": 0}, {"type": "text", "text": "IBM Research, Thomas J. Watson Research Center Yorktown Heights, NY LamNguyen.MLTD@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: nonconvex-linear and nonconvex-strongly concave settings. The first algorithm addresses the nonconvex-linear minimax model and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. It also employs a new shuffling estimator for the \u201chyper-gradient\u201d, departing from standard shuffling techniques in optimization. The second method consists of two variants: semi-shuffling and full-shuffling schemes. These variants tackle the nonconvex-strongly concave minimax setting. We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting. Numerical examples demonstrate the performance of our algorithms and compare them with two other methods. Our results show that the new methods achieve comparable performance with SGD, supporting the potential of incorporating shuffling strategies into minimax algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minimax problems arise in various applications across generative machine learning, game theory, robust optimization, online learning, and reinforcement learning (e.g., [1, 2, 3, 5, 12, 13, 17, 19, 21, 25, 35, 40]). These models often involve stochastic settings or large finite-sum objective functions. To tackle these problems, existing methods frequently adapt stochastic gradient descent (SGD) principles to develop algorithms for solving the underlying minimax problems [4, 13]. For instance, in generative adversarial networks (GANs), early algorithms employed stochastic gradient descentascent methods where two routines, each using an SGD loop, ran iteratively [13]. However, practical implementations of SGD often incorporate shuffling strategies, as seen in popular deep learning libraries like TensorFlow and PyTorch. This has motivated recent research on developing shuffling techniques specifically for optimization algorithms [4, 5, 8, 16, 26, 32, 38]. Our work builds upon this trend by developing shuffling methods for two specific classes of minimax problems. ", "page_idx": 0}, {"type": "text", "text": "Problem statement. In this paper, we study the following minimax optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{p}}\\operatorname*{max}_{u\\in\\mathbb{R}^{q}}\\Big\\{\\mathcal{L}(w,u):=f(w)+\\mathcal{H}(w,u)-h(u)\\equiv f(w)+\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{H}_{i}(w,u)-h(u)\\Big\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ is a proper, closed, and convex function, $\\mathcal{H}_{i}:\\mathbb{R}^{p}\\times\\mathbb{R}^{q}\\to\\mathbb{R}$ are smooth for all $i\\,\\in\\,[n]\\,:=\\,\\{1,2,\\cdots\\,,n\\}$ , and $h:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ is also a proper, closed, and convex function. In this paper, we will focus on two classes of problems in (1), overlapped to each other. ", "page_idx": 1}, {"type": "text", "text": "(NL) $\\mathcal{H}_{i}$ is nonconvex in $w$ and linear in $u$ as $\\mathcal{H}_{i}(w,u):=\\langle F_{i}(w),K u\\rangle$ for a given function $F_{i}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{m}$ and a matrix $K\\in\\mathbb{R}^{q\\times m}$ for all $i\\in[n]$ and $(w,u)\\in\\mathrm{dom}\\left(\\mathcal{L}\\right)$ . (NC) $\\mathcal{H}_{i}$ is nonconvex in $w$ and $\\mathcal{H}_{i}(w,\\cdot)-h(\\cdot)$ is strongly concave in $u$ for all $(\\dot{w},\\dot{u})\\in\\mathrm{dom}\\left(\\mathcal{L}\\right)$ . ", "page_idx": 1}, {"type": "text", "text": "Although (NC) looks more general than (NL), both cases can be overlapped, but one is not a special case of the other. Under these two settings, our approach will rely on a bilevel optimization approach, where the lower-level problem is to solve $\\operatorname*{max}_{u}\\mathcal{L}(w,u)$ , while the upper-level one is $\\operatorname*{min}_{w}\\mathcal{L}(w,u)$ . ", "page_idx": 1}, {"type": "text", "text": "Challenges. The setting (NL) is a special case of stochastic nonconvex-concave minimax problems because the objective term $\\mathcal{H}(w,u):=\\langle F(w),K u\\rangle$ is linear in $u$ . It is equivalent to the compositional model (CO) described below. However, if $h$ is only merely convex and not strongly convex (e.g., the indicator of a standard simplex), then $\\Phi_{0}$ in (CO) becomes nonsmooth regardless of $F$ \u2019s properties. This presents our first challenge. A natural approach to address this issue, as discussed in Section 2, is to smooth $\\Phi_{0}$ . The second challenge arises from the composition between the outer function $h^{*}$ and the finite sum $F(\\cdot)$ in (CO). Unlike standard finite-sum optimization, this composition prevents any direct use of existing techniques, requiring a novel approach for algorithmic development and analysis. The third challenge involves unbiased estimators for gradients or \u201chyper-gradients\u201d in minimax problems. Most existing methods rely on unbiased estimators for objective gradients, with limited work exploring biased estimators. While biased estimators can be used, they require variance reduction properties (see, e.g., [10]). The setting (NC) faces the same second and third challenges as the setting (NL). Additionally, when reformulating it as a minimization problem using a bilevel optimization approach (3), constructing a shuffling estimator for the \u201chyper-gradient\u201d $\\nabla\\Phi_{0}$ becomes unclear. This requires solving the lower-level maximization problem (2). Therefore, it remains an open question whether shuffling gradient-type methods can be extended to this bilevel optimization approach to address (1). In this paper, we address the following research question: ", "page_idx": 1}, {"type": "text", "text": "Can we efficiently develop shuffling gradient methods to solve (1) for both (NL) and (NC) settings? ", "page_idx": 1}, {"type": "text", "text": "Our attempt to tackle this question leads to a novel way of constructing shuffling estimators for the hyper-gradient $\\nabla\\Phi_{0}$ or its smoothed counterpart. This allows us to develop two shuffling gradientbased algorithms with rigorous theoretical guarantees on oracle complexity, matching state-of-the-art complexity results in shuffling-type algorithms for nonconvex optimization. ", "page_idx": 1}, {"type": "text", "text": "Related work. Shuffling optimization algorithms have gained significant attention in optimization and machine communities, demonstrating advantages over standard SGDs, see, e.g., [4, 5, 8, 16, 26, 32, 38]. Nevertheless, applying these techniques to minimax problems like (1) remains challenging, with limited existing literature (e.g., [3, 8, 11]). Das et al. in [8] explored a specific case of (1) without nonsmooth terms $f$ and $h$ , assuming strong monotonicity and $L$ -Lipschitz continuity of the gradient $\\nabla\\mathcal{H}:=[\\nabla_{\\boldsymbol{w}}\\mathcal{H},\\dot{-}\\nabla_{\\boldsymbol{u}}\\mathcal{H}]$ of the joint objective $\\mathcal{H}$ . Their algorithm simplifies to a shuffling variant of fixed-point iteration or a gradient descent-ascent scheme, not applicable to our settings. Cho and Yun in [3] built upon [8] by relaxing the strong monotonicity to Polyak-\u0141ojasiewicz (P\u0141) conditions. This work is perhaps the most closely related one to our algorithm, Algorithm 2, for the (NC) setting. Note that the method in [3] exploits Nash\u2019s equilibrium perspective with a simultaneous update, which is different from our alternative update. Moreover, [3] only considers the noncomposite case with $f=0$ and $h=0$ . Though we only focus on a nonconvex-strongly-concave setting (NC), our results here can be extended to the $\\mathrm{PE}$ condition as in [3]. Very recently, Konstantinos et al. in [11] introduced shuffling extragradient methods for variational inequalities, which encompass convex-concave minimax problems as a special case. However, this also falls outside the scope of our work due to the nonconvexity of (1) in $w$ . Again, all the existing works in [3, 8, 11] utilize a Nash\u2019s equilibrium perspective, while ours leverages a bilevel optimization technique. Besides, in contrast to our sampling-without-replacement approach, stochastic and randomized methods (i.e. using i.i.d. sampling strategies) have been extensively studied for minimax problems, see, e.g., [9, 14, 15, 18, 22, 23, 31, 37, 42]. A comprehensive comparison can be found, e.g., in [3]. ", "page_idx": 1}, {"type": "text", "text": "Contribution. Our main contribution can be summarized as follows. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "(a) For setting (NL), we suggest to reformulate (1) into a compositional minimization and exploit a smoothing technique to treat this reformulation. We propose a new way of constructing shuffling estimators for the \u201chyper-gradient\u201d $\\nabla\\Phi_{\\gamma}$ (cf. (10)) and establish their properties. ", "page_idx": 1}, {"type": "text", "text": "(b) We propose a novel shuffling gradient-based algorithm (cf. Algorithm 1) to approximate an $\\epsilon$ -KKT point of (1) for the setting (NL). Our method requires $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of $F_{i}$ and $\\nabla F_{i}$ under the strong convexity of $h$ , and $\\mathcal{O}(n\\epsilon^{-7/2})$ evaluations of $F_{i}$ and $\\nabla F_{i}$ without the strong convexity of $h$ , for a desired accuracy $\\epsilon>0$ . ", "page_idx": 2}, {"type": "text", "text": "(c) For setting (NC), we develop two variants of the shuffling gradient method: semi-shuffling and full-shuffling schemes $(c f.$ Algorithm 2). The semi-shuffling variant combines both gradient ascent and shuffling gradient methods to construct a new algorithm, which requires $\\bar{O}(n\\epsilon^{-3})$ evaluations of both $\\nabla_{w}\\mathcal{H}_{i}$ and $\\nabla_{u}\\mathcal{H}_{i}$ . The full-shuffling scheme allows to perform both shuffling schemes on the maximization and the minimization alternatively, requiring either $\\mathcal{O}(n\\epsilon^{-3})$ or $\\mathcal{O}(n\\epsilon^{-4})$ evaluations of $\\nabla_{u}\\mathcal{H}_{i}$ depending on our assumptions, while maintaining $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of $\\nabla_{w}\\mathcal{H}_{i}$ for a given desired accuracy $\\epsilon>0$ . ", "page_idx": 2}, {"type": "text", "text": "If a random shuffling strategy is used in our algorithms, then the oracle complexity in all the cases presented above is improved by a factor of $\\sqrt{n}$ . Our settings (NL) and (NC) of (1) are different from existing works [3, 8, 11], as we work with general nonconvexity in $w$ , and linearity or [strong] concavity in $u$ , and both $f$ and $h$ are possibly nonsmooth. Our algorithms are not reduced or similar to existing shuffling methods for optimization, but we use shuffling strategies to form estimators for the hyper-gradient $\\nabla\\Phi_{0}$ in (5). The oracle complexity in both settings (NL) and (NC) is similar to the ones in nonconvex optimization and in a special case of (1) from [3] (up to a constant factor). ", "page_idx": 2}, {"type": "text", "text": "Paper outline. The rest of this paper is organized as follows. Section 2 presents our bilevel optimization approach to (1) and recalls necessary preliminary results. Section 3 develops our shuffling algorithm to solve the setting (NL) of (1) and establishes its convergence. Section 4 proposes new shuffling methods to solve the setting (NC) and investigates their convergence. Section 5 presents numerical experiments, while technical proofs and supporting results are deferred to Supp. Docs. ", "page_idx": 2}, {"type": "text", "text": "Notations. For a function $f$ , we use $\\operatorname{dom}\\left(f\\right)$ to denote its effective domain, and $\\nabla f$ for its gradient or Jacobian. If $f$ is convex, then $\\nabla f$ denotes a subgradient, $\\partial f$ is its subdifferential, and $\\mathrm{prox}_{f}$ is its proximal operator. We use $\\mathcal{F}_{t}$ to denote $\\sigma(w_{0},w_{1},\\cdot\\cdot\\cdot\\,,w_{t})$ , a $\\sigma$ -algebra generated by random vectors $w_{0},w_{1},\\cdots,w_{t},\\mathbb{E}_{t}[\\cdot]=\\mathbb{E}[\\cdot|{\\mathcal F}_{t}]$ is a conditional expectation, and $\\mathbb{E}[\\cdot]$ is the full expectation. As usual, $O(\\cdot)$ denotes Big-O notation in the theory of algorithm complexity. ", "page_idx": 2}, {"type": "text", "text": "2 Bilevel Optimization Approach and Preliminary Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach relies on a bilevel optimization technique [9] in contrast to Nash\u2019s game viewpoint [24], which treats the maximization as a lower level and the minimization as an upper level problem. ", "page_idx": 2}, {"type": "text", "text": "2.1 Bilevel optimization approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The minimax model (1) is split into a lower-level (i.e. a follower) maximization problem of the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{0}(w)\\,:=\\,\\underset{u\\in\\mathbb{R}^{q}}{\\operatorname*{max}}\\bigl\\{\\mathcal{H}(w,u)-h(u)\\equiv\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{H}_{i}(w,u)-h(u)\\bigr\\},}\\\\ &{u_{0}^{*}(w)\\,:=\\,\\underset{u\\in\\mathbb{R}^{q}}{\\operatorname*{argmax}}\\bigl\\{\\mathcal{H}(w,u)-h(u)\\equiv\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{H}_{i}(w,u)-h(u)\\bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For $\\Phi_{0}$ defined by (2), then the upper-level (i.e. the leader) minimization problem can be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Psi_{0}^{\\star}:=\\operatorname*{min}_{w\\in\\mathbb{R}^{p}}\\Big\\{\\Psi_{0}(w):=\\Phi_{0}(w)+f(w)\\Big\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Clearly, this approach is sequential, and only works if $\\Phi_{0}$ is well-defined, i.e. (2) is globally solvable. Hence, the concavity of $\\mathcal{H}(\\bar{w},\\cdot)\\!-\\!h(\\cdot)$ w.r.t. to $u$ is crucial for this approach as stated below. However, this assumption can be relaxed to a global solvability of (2) combined with a $\\mathrm{PE}$ condition as in [3]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Basic). Problems (1) and (3) satisfy the following assumptions for all $i\\in[n]$ : ", "page_idx": 2}, {"type": "text", "text": "(a) $\\Psi_{0}^{\\star}:=\\operatorname*{inf}_{w}\\Psi_{0}(w)>-\\infty.$ .   \n(b) $\\mathcal{H}_{i}$ is differentiable w.r.t. $(w,u)\\in\\mathrm{dom}\\left(\\mathcal{L}\\right)$ and $\\mathcal{H}_{i}(w,\\cdot)$ is concave in u for any $w$ .   \n(c) Both $f:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ and $h:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ are proper, closed, and convex. ", "page_idx": 2}, {"type": "text", "text": "This assumption remains preliminary. To develop our algorithms, we will need more conditions on $\\mathcal{H}_{i}$ and possibly on $f$ and $h$ , which will be stated later. In addition, we can work with a sublevel set ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\Psi_{0}}(w_{0}):=\\{w\\in\\mathrm{dom}\\,(\\Psi_{0}):\\Psi_{0}(w)\\leq\\Psi_{0}(w_{0})\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "of $\\Psi_{0}$ for a given initial point $w_{0}$ from our methods. If $u_{0}^{*}(w)$ is uniquely well-defined for given $w\\in\\mathcal{L}_{\\Psi_{0}}(w_{0})$ , then by the well-known Danskin\u2019s theorem, $\\Phi_{0}$ is differential at $w$ and its gradient is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\Phi_{0}(w)=\\nabla_{w}\\mathcal{H}(w,u_{0}^{*}(w))=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{w}\\mathcal{H}_{i}(w,u_{0}^{*}(w)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We adopt the term \u201chyper-gradient\u201d from bilevel optimization to name $\\nabla\\Phi_{0}$ in this paper. ", "page_idx": 3}, {"type": "text", "text": "2.2 Technical assumptions and properties of $\\Phi_{0}$ for nonconvex-linear setting (NL) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "(a) Compositional minimization formulation. If $\\mathcal{H}_{i}(w,u):=\\langle F_{i}(w),K u\\rangle$ as in setting (NL), then (1) is equivalently reformulated into the following nonconvex compositional minimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{p}}\\Big\\{\\Psi_{0}(w):=f(w)+\\Phi_{0}(w)=f(w)+h^{*}\\Big(\\frac{1}{n}\\sum_{i=1}^{n}K^{T}F_{i}(w)\\Big)\\Big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h^{*}(v):=\\operatorname*{sup}_{u}\\{\\langle v,u\\rangle-h(u)\\}$ , the Fenchel conjugate of $h$ , and $\\Phi_{0}(w)=h^{*}(K^{T}F(w))$ . If $h$ is not strongly convex, then $h^{*}$ is convex but possibly nonsmooth. ", "page_idx": 3}, {"type": "text", "text": "(b) Technical assumptions. To develop our algorithms, we also need the following assumptions.   \nAssumption 2. $h$ is $\\mu_{h}$ -strongly convex with $\\mu_{h}\\geq0$ , and $\\mathrm{dom}(h)$ is bounded by $M_{h}<+\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (For $F_{i}$ ). For setting (NL) with $\\mathcal{H}_{i}(w,u):=\\left\\langle F_{i}(w),K u\\right\\rangle\\left(i\\in[n]\\right.$ ), assume that ", "page_idx": 3}, {"type": "text", "text": "(a) $F_{i}$ is continuously differentiable, and its Jacobian $\\nabla F_{i}$ is $L_{F_{i}}$ -Lipschitz continuous. (b) $F_{i}$ is also $M_{F_{i}}$ -Lipschitz continuous or equivalently, its Jacobian $\\nabla F_{i}$ is $M_{F_{i}}$ -bounded. (c) There exists a positive constant $\\sigma_{J}\\in(0,+\\infty)$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla F_{i}(w)-\\nabla F(w)\\|^{2}\\leq\\sigma_{J}^{2},\\quad\\forall w\\in\\mathrm{dom}\\,(F)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 2 allows $\\mu_{h}=0$ that also covers the non-strong convexity of $h$ . Assumption 3 is rather standard to develop gradient-based methods for solving (1). Under Assumption 3, the finite-sum $F$ is also $M_{F}$ -Lipschitz continuous and the Jacobian $\\nabla F$ of $F$ is also $L_{F}$ -Lipschitz continuous with ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{F}:=\\operatorname*{max}\\{M_{F_{i}}:i\\in[n]\\}\\quad{\\mathrm{and}}\\quad L_{F}:=\\operatorname*{max}\\{L_{F_{i}}:i\\in[n]\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Condition (6) can be relaxed to the form $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla F_{i}(w)-\\nabla F(w)\\|^{2}\\leq\\sigma_{J}^{2}+\\Theta_{J}\\|\\nabla\\Phi_{0}(w)\\|^{2}}\\end{array}$ for some $\\Theta_{J}\\ge0$ , where $\\nabla\\Phi_{0}$ is a [sub]gradiePnt of $\\Phi_{0}$ or $\\Phi_{\\gamma}$ (its smoothed approximation). Moreover, under Assumption 3, if $\\mu_{h}>0$ , then $\\nabla h^{*}$ is $L_{h^{*}}$ -Lipschitz continuous with $\\begin{array}{r}{L_{h^{*}}:=\\frac{1}{\\mu_{h}}}\\end{array}$ 1 . Thus it is possible (see [9]) to prove that $\\Phi_{0}$ is differentiable, and $\\nabla\\Phi_{0}$ is also $L_{\\Phi_{0}}$ -Lipschitz continuous with L\u03a60 := MhkKkLF + M F2 \u00b5kKk2 as a consequence of Lemma 4 when $\\gamma\\downarrow0^{+}$ in Supp. Doc. A. ", "page_idx": 3}, {"type": "text", "text": "(c) Smoothing technique for lower-level maximization problem (2). If $h$ is only merely convex (i.e. $\\mu_{h}=0$ ), then (2) may not be uniquely solvable, leading to the possible non-differentiability of $\\Phi_{0}$ . Let us define the following convex function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{0}(v):=\\operatorname*{max}_{u\\in\\mathbb{R}^{q}}\\left\\{\\langle v,K u\\rangle-h(u)\\right\\}=h^{*}(K^{T}v).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, $\\Phi_{0}$ in (2) or (CO) can be written as $\\begin{array}{r}{\\Phi_{0}(w)=\\phi_{0}(F(w))=\\phi_{0}\\left(\\frac{1}{n}\\sum_{i=1}^{n}F_{i}(w)\\right)}\\end{array}$ . Our goal is to smooth $\\phi_{0}$ if is not strongly convex, leading to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\phi_{\\gamma}(v)\\,:=\\,\\displaystyle\\operatorname*{max}_{u}\\left\\{\\langle v,K u\\rangle-h(u)-\\gamma b(u)\\right\\},}\\\\ {u_{\\gamma}^{*}(v)\\,:=\\,\\displaystyle\\arg\\!\\operatorname*{max}_{u}\\left\\{\\langle v,K u\\rangle-h(u)-\\gamma b(u)\\right\\},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma>0$ is a given smoothness parameter and $b:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ is a proper, closed, and 1-strongly convex function such that $\\mathrm{dom}(h)\\subseteq\\mathrm{{dom}}(b)$ . We also denote $D_{b}:=\\bar{\\operatorname*{sup}}\\{\\|\\nabla b(u)\\|:u\\in\\mathrm{dom}\\,(\\bar{h})\\}$ . In particular, if we choose $\\begin{array}{r}{b(u):=\\frac{1}{2}\\|u-\\bar{u}\\|^{2}}\\end{array}$ for a fixed $\\bar{u}$ , then $u_{\\gamma}^{*}(v)=\\mathrm{prox}_{h/\\gamma}(\\bar{u}-K^{T}v)$ . ", "page_idx": 3}, {"type": "text", "text": "Using $\\phi_{\\gamma}$ , problem (CO) can be approximated by its smoothed formulation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w\\in\\mathbb{R}^{p}}{\\operatorname*{min}}\\Bigl\\{\\Psi_{\\gamma}(w):=f(w)+\\Phi_{\\gamma}(w)=f(w)+\\phi_{\\gamma}(F(w))\\equiv f(w)+\\phi_{\\gamma}\\Bigl(\\frac{1}{n}\\sum_{i=1}^{n}F_{i}(w)\\Bigr)\\Bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To develop our method, one key step is to approximate the hyper-gradient of $\\Phi_{\\gamma}$ in (10), where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\Phi_{\\gamma}(\\boldsymbol{w})=\\nabla F(\\boldsymbol{w})^{T}\\nabla\\phi_{\\gamma}(F(\\boldsymbol{w}))=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla F_{i}(\\boldsymbol{w})^{T}\\nabla\\phi_{\\gamma}(F(\\boldsymbol{w})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, r\u03a6\u03b3 is L\u03a6\u03b3-Lipschitz continuous with L\u03a6\u03b3 := MhkKkLF + M \u00b5F2 hk+K\u03b3k2 (see Lemma 4). ", "page_idx": 3}, {"type": "text", "text": "2.3 Technical assumptions and properties of $\\Phi_{0}$ for the nonconvex-strongly-concave setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To develop our shuffling gradient-based algorithms for solving (1) under the nonconvex-stronglyconcave setting (NC), we impose the following assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 (For $\\mathcal{H}_{i}$ ). $\\mathcal{H}_{i}$ for all $i\\in[n]$ in (1) satisfies the following conditions: ", "page_idx": 4}, {"type": "text", "text": "(a) For any given w such that $(w,u)\\in\\mathrm{dom}\\,(\\mathcal{H}),\\,\\mathcal{H}_{i}(w,\\cdot)$ is $\\mu_{H}$ -strongly concave w.r.t. u. (b) $\\nabla\\mathcal{H}_{i}$ is $(L_{w},L_{u})$ -Lipschitz continuous, i.e. for all $(w,u),(\\hat{w},\\hat{u})\\in\\mathrm{dom}\\,(\\mathcal{H})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{H}_{i}(w,u)-\\nabla\\mathcal{H}_{i}(\\hat{w},\\hat{u})\\|^{2}\\leq L_{w}^{2}\\|w-\\hat{w}\\|^{2}+L_{u}^{2}\\|u-\\hat{u}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(c) There exist two constants $\\Theta_{w}\\ge0$ and $\\sigma_{w}\\geq0$ such that for $(w,u)\\in\\mathrm{dom}\\,(\\mathcal{H}),$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{w}\\mathcal{H}_{i}(w,u)-\\nabla_{w}\\mathcal{H}(w,u)\\|^{2}\\,\\le\\,\\Theta_{w}\\|\\nabla_{w}\\mathcal{H}(w,u)\\|^{2}+\\sigma_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "There exist two constants $\\Theta_{u}\\ge0$ and $\\sigma_{u}\\geq0$ such that for all $(w,u)\\in\\mathrm{dom}\\,(\\mathcal{H}),$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{u}\\mathcal{H}_{i}(w,u)-\\nabla_{u}\\mathcal{H}(w,u)\\|^{2}\\,\\le\\,\\Theta_{u}\\|\\nabla_{u}\\mathcal{H}(w,u)\\|^{2}+\\sigma_{u}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 4(a) makes sure that our lower-level maximization of (1) is well-defined. Assumption 4(b) and (c) are standard in shuffling gradient-type methods as often seen in nonconvex optimization [9]. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Smoothness of $\\Phi_{0}$ ). Under Assumptions 2 and 4, $u_{0}^{*}(\\cdot)$ in (2) is $\\kappa$ -Lipschitz continuous with $\\begin{array}{r}{\\kappa:=\\frac{L_{u}}{\\mu_{H}+\\mu_{h}}}\\end{array}$ . Moreover, $\\nabla\\Phi_{0}$ in (5) is $L_{\\Phi_{0}}$ -Lipschitz continuous with $L_{\\Phi_{0}}:=(1+\\kappa)L_{w}$ . ", "page_idx": 4}, {"type": "text", "text": "2.4 Approximate KKT points and approximate stationary points ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "(a) Exact and approximate KKT points and stationary points. A pair $(w^{\\star},u^{\\star})\\in\\mathrm{dom}\\left({\\mathcal{L}}\\right)$ is called a KKT (Karush-Kuhn-Tucker) point of (1) if ", "page_idx": 4}, {"type": "equation", "text": "$$\n0\\in\\nabla_{w}\\mathcal{H}(w^{\\star},u^{\\star})+\\partial f(w^{\\star})\\quad\\mathrm{and}\\quad0\\in-\\nabla_{u}\\mathcal{H}(w^{\\star},u^{\\star})+\\partial h(u^{\\star}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given a tolerance $\\epsilon>0$ , our goal is to find an $\\epsilon$ -approximate KKT point $(\\widehat{w},\\widehat{u})$ of (1) defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{w}\\in\\nabla_{w}\\mathcal{H}(\\widehat{w},\\widehat{u})+\\partial f(\\widehat{w}),\\quad r_{u}\\in-\\nabla_{u}\\mathcal{H}(\\widehat{w},\\widehat{u})+\\partial h(\\widehat{u}),\\quad\\mathrm{and}\\quad\\mathbb{E}\\big[\\|[r_{w},r_{u}]\\|^{2}\\big]\\leq\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A vector $w^{\\star}\\in\\mathrm{dom}\\left(\\Psi_{0}\\right)$ is  bsaid to be a stationa rby  bpoint of (b3) if ", "page_idx": 4}, {"type": "equation", "text": "$$\n0\\in\\nabla\\Phi_{0}(w^{\\star})+\\partial f(w^{\\star}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $f$ is possibly nonsmooth, we can define a stationary point of (3) via a gradient mapping as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{\\eta}(w):=\\eta^{-1}\\big(w-\\mathrm{prox}_{\\eta f}(w-\\eta\\nabla\\Phi_{0}(w))\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta>0$ is given. It is well-known that $\\mathcal{G}_{\\eta}(w^{\\star})=0$ iff $w^{\\star}$ is a stationary point of (3). Again, since we cannot exactly compute $w^{\\star}$ , we expect to find an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ of (3) such that $\\mathbb{E}\\big[\\|\\mathcal{G}_{\\eta}(\\widehat{w}_{T})\\|^{2}\\big]\\leq\\epsilon^{2}$ for a given tolerance $\\epsilon>0$ . ", "page_idx": 4}, {"type": "text", "text": "(b) Constructing an approximate stationary point and KKT point from algorithms. Our algorithms below generate a sequence $\\{\\widetilde{w}_{t}\\}_{t\\ge0}^{T}$ such that $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}\\big[\\|\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})\\|^{2}\\big]\\leq\\epsilon^{2}}\\end{array}$ . Hence, we construct an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ usin eg one of the following two options: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{w}_{T}:=\\widetilde{w}_{t_{*}},\\ \\mathrm{where}\\ \\left\\{\\begin{array}{l l}{t_{*}:=\\mathrm{argmin}\\{||\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})||:0\\leq t\\leq T\\},}\\\\ {t_{*}\\ \\mathrm{is~uniformly~randomly~chosen~from~}\\{0,1\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Clearly, we have $\\begin{array}{r}{\\mathbb{E}\\big[\\|\\mathcal{G}_{\\eta}(\\widehat{w}_{T})\\|^{2}\\big]\\leq\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}\\big[\\|\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})\\|^{2}\\big]\\leq\\epsilon^{2}}\\end{array}$ . We need the following result. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. (a) $I f(w^{\\star},u^{\\star})$ is a KKT point of (1), then $w^{\\star}$ is a stationary point of (3). Conversely, if $w^{\\star}$ is a stationary point of (3), then $(w^{\\star},u_{0}^{*}(w^{\\star}))$ is a KKT point of (1). ", "page_idx": 4}, {"type": "text", "text": "(b) If $\\widehat{w}_{T}$ is an $\\epsilon$ -stationary point of (3) and $\\nabla\\Phi_{0}$ is $L_{\\Phi_{0}}$ -Lipschitz continuous, then $(\\overline{{w}}_{T},\\overline{{u}}_{T})$ is an $\\hat{\\epsilon}$ -KK T bpoint of (1), where $\\overline{{w}}_{T}:=\\mathrm{prox}_{\\eta f}\\big(\\widehat{w}_{T}-\\eta\\nabla\\Phi_{0}(\\mathring{w}_{T})\\big)$ , ${\\overline{{u}}}_{T}:=u_{0}^{*}({\\overline{{w}}_{T}})$ , and $\\hat{\\epsilon}:=(1\\!+\\!\\bar{L}_{\\Phi_{0}}\\eta)\\epsilon$ . (c) If $\\widehat{w}_{T}$ is an $\\epsilon$ -stationary point of (10) ,b then $(\\overline{{w}}_{T},\\overline{{u}}_{T})$ is an $\\hat{\\epsilon}$ -KKT point of (1), where $\\overline{{w}}_{T}:=$ $\\mathrm{prox}_{\\eta f}(\\widehat{w}_{T}-\\eta\\nabla\\Phi_{\\gamma}(\\widehat{w}_{T}))$ , ${\\overline{{u}}}_{T}:=u_{\\gamma}^{*}(F({\\overline{{w}}}_{T}))$ , and $\\hat{\\epsilon}:=\\operatorname*{max}\\{(1+L_{\\Phi_{\\gamma}}\\eta)\\epsilon,\\gamma D_{b}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 allows us to construct an $\\hat{\\epsilon}$ -approximate KKT point $(\\overline{{w}}_{T},\\overline{{u}}_{T})$ of (1) from an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ of either (3) or its smoothed problem (10), where $\\hat{\\epsilon}=\\mathcal{O}(\\operatorname*{max}\\{\\epsilon,\\gamma\\})$ . ", "page_idx": 4}, {"type": "text", "text": "2.5 Technical condition to handle the possible nonsmooth term $f$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To handle the nonsmooth term $f$ of (1) in our algorithms we require one more condition as in [5]. Assumption 5. Let $\\Phi_{\\gamma}$ be defined by (10), which reduces to $\\Phi_{0}$ given by (2) $a s\\gamma\\downarrow0^{+}$ , and $\\mathcal{G}_{\\eta}$ be defined by (18). Assume that there exist two constants $\\Lambda_{0}\\geq1$ and $\\Lambda_{1}\\geq0$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\nabla\\Phi_{\\gamma}(w)\\|^{2}\\leq\\Lambda_{0}\\|\\mathcal{G}_{\\eta}(w)\\|^{2}+\\Lambda_{1},\\quad\\forall w\\in\\mathrm{dom}\\left(\\Phi_{0}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $f=0$ , then $\\mathcal{G}_{\\eta}(w)\\equiv\\nabla\\Phi_{\\gamma}(w)$ , and Assumption 5 automatically holds with $\\Lambda_{0}=1$ and $\\Lambda_{1}=0$ . If $f\\neq0$ , then it is crucial to have $\\Lambda_{0}\\geq1$ in (20). Let us consider two examples to see why? ", "page_idx": 5}, {"type": "text", "text": "(i) If $f$ is $M_{f}$ -Lipschitz continuous (e.g., $\\ell_{1}$ -norm), then (20) also holds with $\\Lambda_{0}:=1+\\nu>1$ and $\\begin{array}{r}{\\Lambda_{1}:=\\frac{1+\\nu}{\\nu}M_{f}}\\end{array}$ for a given $\\nu>0$ . (ii ) If $f\\,=\\,\\delta_{\\mathcal{W}}\\,$ , the indicator of a nonempty, closed, convex, and bounded set $\\mathcal{W}$ , then Assumption 5 also holds by the same reason as in Example (i) (see Supp. Doc. A). ", "page_idx": 5}, {"type": "text", "text": "3 Shuffling Gradient Method for Nonconvex-Linear Minimax Problems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first propose a new construction using shuffling techniques to approximate the true gradient $\\nabla\\Phi_{\\gamma}$ in (11) for any $\\gamma\\geq0$ . Next, we propose our algorithm and analyze its convergence. ", "page_idx": 5}, {"type": "text", "text": "3.1 The shuffling gradient estimators for $\\nabla\\Phi_{\\gamma}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Challenges. To evaluate $\\nabla\\Phi_{\\gamma}(w)$ in (11), we need to evaluate both $\\nabla F(w)$ and $F(w)$ at each $w$ . However, in SGD or shuffling gradient methods, we want to approximate both quantities at each iteration. Note that this gradient can be written in a finite-sum $\\begin{array}{r}{\\frac{\\vec{1}}{n}\\overset{*}{\\sum}_{i=1}^{n}\\nabla F_{i}(w)^{T}\\overset{*}{\\nabla}\\phi_{\\gamma}(F(w))}\\end{array}$ (see (11)), but every summand requires $\\nabla\\phi_{\\gamma}(F(w))$ , which involves the full evaluation of $F$ . ", "page_idx": 5}, {"type": "text", "text": "Our estimators. Let $F_{\\pi^{(t)}(i)}(w_{i-1}^{(t)})$ and $\\nabla F_{\\hat{\\pi}^{(t)}(i)}(w_{i-1}^{(t)})$ be the function value and the Jacobian component evaluated at $w_{i-1}^{(t)}$ respectively for $i\\in[n]$ , where $\\pi^{(t)}=(\\pi^{(t)}(1),\\pi^{(t)}(2),\\cdots\\,,\\pi^{(t)}(n))$ and $\\hat{\\pi}^{(t)}=\\big(\\hat{\\pi}^{(t)}(1),\\hat{\\pi}^{(t)}(2),\\cdot\\cdot\\cdot\\,,\\hat{\\pi}^{(t)}(n)\\big)$ are two permutations of $[n]:=\\{1,2,\\cdots\\,,n\\}$ . We want to use these quantities to approximate the function value $F(w_{0}^{(t)})$ and its Jacobian $\\nabla F(w_{0}^{(t)})$ of $F$ at $w_{0}^{(t)}$ , respectively, where $w_{0}^{(t)}$ the iterate vector at the beginning of each epoch $t$ . ", "page_idx": 5}, {"type": "text", "text": "For function value $F(w_{0}^{(t)})$ , we suggest the following approximation at each inner iteration $i\\in[n]$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\bf O p t i o n\\;1!}\\quad}&{{}\\quad F_{i}^{(t)}:=\\frac{1}{n}\\left[\\sum_{j=1}^{i}F_{\\pi^{(t)}(j)}(w_{j-1}^{(t)})+\\sum_{j=i+1}^{n}F_{\\pi^{(t)}(j)}(w_{0}^{(t)})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Alternative to (21), for all $i\\in[n]$ , we can simply choose another option: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{i}^{(t)}\\,:=\\,\\frac{1}{n}\\sum_{j=1}^{n}F_{j}\\bigl(w_{0}^{(t)}\\bigr)=\\frac{1}{n}\\sum_{j=1}^{n}F_{\\pi^{(t)}(j)}\\bigl(w_{0}^{(t)}\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For Jacobian $\\nabla F(w_{0}^{(t)})$ , we suggest to use the following standard shuffling estimator for all $i\\in[n]$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla F_{i}^{(t)}:=\\nabla F_{\\hat{\\pi}^{(t)}(i)}(w_{i-1}^{(t)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For $F_{i}^{(t)}$ from (21) (or (22)) and for $\\nabla F_{i}^{(t)}$ from (23), we form an approximation of $\\nabla\\Phi_{\\gamma}(w_{0}^{(t)})$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde\\nabla\\Phi_{\\gamma}(w_{i-1}^{(t)}):=(\\nabla F_{i}^{(t)})^{T}\\nabla\\phi_{\\gamma}(F_{i}^{(t)})\\equiv(\\nabla F_{i}^{(t)})^{T}K u_{\\gamma}^{*}(F_{i}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Discussion. The estimator ${\\cal F}_{i}^{(t)}$ for $F$ requires $n-i$ more function evaluations $F_{\\pi^{(t)}(j)}(w_{0}^{(t)})$ at each epoch $t$ . The first option (21) for $F$ uses $2n$ function evaluations $F_{i}$ , while the second one in (22) only needs $n$ function evaluations at each epoch $t\\geq0$ . However, (21) uses the most updated information up to the inner iteration $i$ compared to (22), which is expected to perform better. The Jacobian estimator $\\nabla F_{i}^{(t)}$ is standard and only uses one sample or a mini-batch at each iteration $i$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 The shuffling gradient-type algorithm for nonconvex-linear setting (NL) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We propose Algorithm 1, a shuffling gradient-type method, to approximate a stationary point of (10). ", "page_idx": 5}, {"type": "text", "text": "Discussion. First, the cost per epoch of Algorithm 1 consists of either $2n$ or $n$ function evaluations $F_{i}$ , and $n$ Jacobian evaluations $\\nabla F_{i}$ . Compare to standard shuffling gradient-type methods, e.g., in [8], Algorithm 1 has either $n$ more evaluations of $F_{i}$ or the same cost. Second, when implementing ", "page_idx": 5}, {"type": "text", "text": "1: Initialization: Choose an initial point $\\widetilde{w}_{0}\\in\\mathrm{dom}\\left(\\Phi_{0}\\right)$ and a smoothness parameter $\\gamma>0$ .   \n2: for $t=1,2,\\cdots,T$ do   \n3: Set $w_{0}^{(t)}:=\\widetilde w_{t-1}$ ;   \n4: Generate tw oe permutations $\\pi^{(t)}$ and $\\hat{\\pi}^{(t)}$ of $[n]$ (identically or randomly and independently)   \n5: for $i=1,\\cdots,n$ do   \n6: Evaluate ${\\cal F}_{i}^{(t)}$ by either (21) or (22) using $\\pi^{(t)}$ , and $\\nabla F_{i}^{(t)}$ by (23) using $\\hat{\\pi}^{(t)}$ .   \n7: Solve (9) to get $u_{\\gamma}^{*}(F_{i}^{(t)})$ and form $\\widetilde{\\nabla}\\Phi_{\\gamma}(\\boldsymbol{w}_{i-1}^{(t)}):=(\\nabla F_{i}^{(t)})^{T}K\\boldsymbol{u}_{\\gamma}^{*}(F_{i}^{(t)})$ .   \n8: Update wi(t) $\\begin{array}{r}{\\boldsymbol{w}_{i}^{(t)}:=\\boldsymbol{w}_{i-1}^{(t)}-\\frac{\\eta_{t}}{n}\\widetilde{\\nabla}\\Phi_{\\gamma}(\\boldsymbol{w}_{i-1}^{(t)})}\\end{array}$   \n9: end for   \n10: Compute $\\widetilde{w}_{t}:=\\mathrm{prox}_{\\eta_{t}f}(w_{n}^{(t)})$ ;   \n11: end for ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1, we do not need to evaluate the full Jacobian $\\nabla F_{i}^{(t)}$ , but rather the product of matrix $(\\nabla F_{i}^{(t)})^{T}$ and vector $\\nabla\\Phi_{\\gamma}(\\boldsymbol{F}_{i}^{(t)})$ as $\\widetilde{\\nabla}\\Phi_{\\gamma}(\\boldsymbol{w}_{i-1}^{(t)}):=(\\nabla F_{i}^{(t)})^{T}\\nabla\\Phi_{\\gamma}(F_{i}^{(t)})$ . Evaluating this matrixvector multiplication is much more e feficient than evaluating the full Jacobian $\\nabla F_{i}^{(t)}$ and $\\nabla\\Phi_{\\gamma}(\\boldsymbol{F}_{i}^{(t)})$ individually. Third, thanks to Assumption 5, the proximal step $\\widetilde{w}_{t}:=\\mathrm{prox}_{\\eta_{t}f}(w_{n}^{(t)})$ is only required at the end of each epoch $t$ . This significantly reduces the comp uetational cost if $\\operatorname{prox}_{\\eta_{t}f}$ is expensive. ", "page_idx": 6}, {"type": "text", "text": "3.3 Convergence Analysis of Algorithm 1 for Nonconvex-Linear Setting $(\\mathbf{NL})$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now, we are ready to state the convergence result of Algorithm 1 in a short version: Theorem 1. The full version of this theorem is Theorem 6, which can be found in Supp. Doc. B. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Suppose that Assumptions 1, 2, 3, and $^{5}$ holds for the setting (NL) of (1) and $\\epsilon>0$ is $a$ sufficiently small tolerance. Let $\\{\\widetilde{w}_{t}\\}$ be generated by Algorithm $^{\\,l}$ after $\\dot{T}=\\mathcal{O}(\\epsilon^{-3})$ epochs using arbitrarily permutations $\\pi^{(t)}$ and $\\hat{\\pi}^{(t)}$ and a learning rate $\\eta_{t}=\\eta:={\\mathcal{O}}(\\epsilon)$ (see Theorem $^{6}$ in Supp. Doc. B for the exact formulas of $T$ and $\\eta$ ). Then, we have $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\|\\mathcal{G}_{\\eta_{t}}(\\widetilde{w}_{t})\\|^{2}\\leq\\epsilon^{2}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Alternatively, if $\\{\\widetilde{w}_{t}\\}$ is generated by Algorithm $^{\\,l}$ after $T:=\\mathcal{O}(n^{-1/2}\\epsilon^{-3})$ epochs using two random and independent  peermutations $\\pi^{(t)}$ and $\\hat{\\pi}^{(t)}$ and a learning rate $\\eta_{t}=\\eta:=\\mathcal{O}(n^{1/2}\\epsilon)$ (see Theorem $6$ in Supp. Doc. B for the exact formulas). Then, we have $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}[\\|\\mathcal{G}_{\\eta_{t}}(\\widetilde{w}_{t})\\|^{2}]\\leq\\epsilon^{2}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Our first goal is to approximate a stationary point $w^{\\star}$ of (CO) as $\\mathbb{E}[\\|\\mathcal{G}_{\\eta}(\\widehat{w})\\|^{2}]\\leq\\epsilon^{2}$ , while Algorithm 1 only provides an $\\epsilon$ -stationary of (10). For a proper choice of $\\gamma$ , it is al sbo an $\\epsilon_{}$ -stationary point of (3). ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Let $\\widehat{w}_{T}$ defined by (19) be generated from $\\{\\widetilde{w}_{t}\\}$ of Algorithm 1. Under the conditions of Theorem 1 and  abny permutations $\\pi^{(t)}$ and $\\hat{\\pi}^{(t)}$ , the foll oewing statements hold. ", "page_idx": 6}, {"type": "text", "text": "(a) If h is $\\mu_{h}$ -strongly convex with $\\mu_{h}>0$ , then we can set $\\gamma=0$ , and Algorithm 1 requires $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of $F_{i}$ and $\\nabla F_{i}$ to achieve an $\\epsilon$ -stationary $\\widehat{w}_{T}$ of (3).   \n(b) If $h$ is only convex (i.e. $\\mu_{h}\\,=\\,0_{,}$ ), then we can set $\\gamma:=\\mathcal{O}(\\epsilon)$ , and Algorithm 1 needs $\\mathcal{O}(n\\epsilon^{-7/2})$ evaluations of $F_{i}$ and $\\nabla F_{i}$ to achieve an $\\epsilon$ -stationary $\\widehat{w}_{T}$ of (3). ", "page_idx": 6}, {"type": "text", "text": "$I f,$ in addition, $\\pi^{(t)}$ and $\\hat{\\pi}^{(t)}$ are sampled uniformly at random without replac ebment and independently, and $\\Lambda_{1}=\\mathcal{O}(n^{-1})$ , then the numbers of evaluations of $F_{i}$ and $\\nabla F_{i}$ are reduced by a factor of $\\sqrt{n}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Shuffling Method for Nonconvex-Strongly Concave Minimax Problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we develop shuffling gradient-based methods to solve (1) under the nonconvexstrongly concave setting (NC). Since this setting does not cover the nonconvex-linear setting (NL) in Section 3 as a special case, we need to treat it separately using different ideas and proof techniques. ", "page_idx": 6}, {"type": "text", "text": "4.1 The construction of algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Unlike the linear case with $\\mathcal{H}_{i}(w,u)=\\langle F_{i}(w),K u\\rangle$ in Section 3, we cannot generally compute the solution $u_{0}^{*}(\\widetilde{w}_{t-1})$ in (2) exactly for a given $\\widetilde w_{t-1}$ . We can only approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ by some $\\widetilde{u}_{t}$ . This leads to  eanother level of inexactness in a ne approximate \u201chyper-gradient\u201d $\\widetilde{\\nabla}\\Phi_{0}(w_{i-1}^{(t)})$ defined  bey ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde\\nabla\\Phi_{0}(w_{i-1}^{(t)}):=\\nabla_{w}\\mathcal{H}_{\\hat{\\pi}^{(t)}(i)}(w_{i-1}^{(t)},\\widetilde{u}_{t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "There are different options to approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ . We propose two options below, but other choices are possible, including accelerated gradie net ascent methods and stochastic algorithms [6, 20]. ", "page_idx": 7}, {"type": "text", "text": "$\\left({\\mathrm{a}}_{1}\\right)$ Gradient ascent scheme for the lower-level problem. We apply a standard gradient ascent scheme to update $\\widetilde{u}_{t}$ : Starting from $s=0$ with $u_{0}^{(t)}:=\\widetilde u_{t-1}$ , at each epoch $s=1,\\cdots\\,,S$ , we update ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{u}_{s}^{(t)}:=\\operatorname{prox}_{\\widehat{\\eta}_{t}h}\\big(\\widehat{u}_{s-1}^{(t)}+\\frac{\\widehat{\\eta}_{t}}{n}\\sum_{i=1}^{n}\\nabla_{u}\\mathcal{H}_{i}\\big(\\widetilde{w}_{t-1},\\widehat{u}_{s-1}^{(t)}\\big)\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for a given learning rate $\\hat{\\eta}_{t}>0$ . Then, we finally output $\\widetilde{u}_{t}:=\\widehat{u}_{S}^{(t)}$ to approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ . ", "page_idx": 7}, {"type": "text", "text": "To make our method more flexible, we allow to perfor me eith erb only one iteration (i.e .e $S=1$ ) or multiple iterations (i.e. $S>1$ ) of (26). Each iteration $s$ requires $n$ evaluations of $\\nabla_{u}\\mathcal{H}_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "$\\left(\\mathrm{a_{2}}\\right)$ Shuffling gradient ascent scheme for the lower-level problem. We can also construct $\\widetilde{u}_{t}$ by a shuffling gradient ascent scheme. Again, we allow to run either only one epoch (i.e. $S=1$ ) or multiple epochs (i.e. $S>1$ ) of the shuffling algorithm to update $\\widetilde{u}_{t}$ , leading to the following scheme: Starting from $s:=1$ with $\\widehat{u}_{0}^{(t)}:=\\widetilde{u}_{t-1}$ , at each epoch $s=1,2,\\cdots\\,,S;$ , having $\\widehat{u}_{s-1}^{(t)}$ , we generate $a$ permutation $\\pi^{(s,t)}$ of $[n]$ a bnd run  ae shuffling gradient ascent scheme as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{0}^{(s,t)}:=\\widehat{u}_{s-1}^{(t)},}\\\\ {F o r\\,i=1,2,\\cdots\\,,n,\\,u p d a t e}\\\\ {\\qquad u_{i}^{(s,t)}:=u_{i-1}^{(s,t)}+\\frac{\\widehat{\\eta}_{t}}{n}\\nabla_{u}\\mathcal{H}_{\\pi^{(s,t)}(i)}\\big(\\widetilde{w}_{t-1},u_{i-1}^{(s,t)}\\big),}\\\\ {\\widehat{u}_{s}^{(t)}:=\\mathrm{prox}_{\\widehat{\\eta}_{t}h}\\big(u_{n}^{(s,t)}\\big).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "At the end of the $S$ -th epoch, we output $\\widetilde{u}_{t}:=\\widehat{u}_{S}^{(t)}$ as an approximation to $u_{0}^{*}(\\widetilde{w}_{t-1})$ . Here, we use the same learning rate $\\hat{\\eta}_{t}$ for all epochs $s\\in[S]$ . Each epoch $s$ requires $n$ evalu aetions of $\\nabla_{u}\\mathcal{H}_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "(b) Shuffling gradient descent scheme for the upper-level minimization problem. Having $\\widetilde{u}_{t}$ from either (26) or (27), we run a shuffling gradient descent epoch to update $\\widetilde{w}_{t}$ from $\\widetilde w_{t-1}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{w_{0}^{(t)}:=\\widetilde{w}_{t-1},}\\\\ {\\mathrm{For~}i=1,2,\\cdots,n,\\mathrm{update}}\\\\ {w_{i}^{(t)}:=w_{i-1}^{(t)}-\\frac{\\eta_{t}}{n}\\widetilde{\\nabla}\\Phi_{0}(w_{i-1}^{(t)})\\equiv w_{i-1}^{(t)}-\\frac{\\eta_{t}}{n}\\nabla_{w}\\mathcal{H}_{\\hat{\\pi}^{(t)}(i)}(w_{i-1}^{(t)},\\widetilde{u}_{t}),}\\\\ {\\widetilde{w}_{t}:=\\mathrm{prox}_{\\eta_{t}f}(w_{n}^{(t)}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "These two steps (26) (or (27)) in $u$ and (28) in $w$ are implemented alternatively for $t=1,\\cdot\\cdot\\cdot,T$ . ", "page_idx": 7}, {"type": "text", "text": "(c) The full algorithm. Combining both steps (26) (or (27)) and (28), we can present an alternating shuffling proximal gradient algorithm for solving (1) as in Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 (Alternating Shuffling Proximal Gradient Algorithm for Solving (1) under setting (NC)) ", "page_idx": 7}, {"type": "text", "text": "1: Initialization: Choose an initial point $(\\widetilde{w}_{0},\\widetilde{u}_{0})\\in\\mathrm{dom}\\left(\\mathcal{L}\\right)$ .   \n2: for $t=1,2,\\cdots,T$ do   \n3: Compute $\\widetilde{u}_{t}$ using either (26) or (27).   \n4: Set w0 $w_{0}^{(t)}:=\\widetilde w_{t-1}$ and generate a permutation $\\hat{\\pi}^{(t)}$ of $[n]$ .   \n5: for $i=1,\\cdots,n$ do   \n6: Evaluate $\\widetilde\\nabla\\Phi_{0}(w_{i-1}^{(t)}):=\\nabla_{w}\\mathcal{H}_{\\hat{\\pi}^{(t)}(i)}(w_{i-1}^{(t)},\\widetilde{u}_{t}).$   \n7: Update $\\begin{array}{r}{\\boldsymbol{w}_{i}^{(t)}:=\\boldsymbol{w}_{i-1}^{(t)}-\\frac{\\eta_{t}}{n}\\widetilde{\\nabla}\\Phi_{0}(\\boldsymbol{w}_{i-1}^{(t)})}\\end{array}$ .   \n8: end for   \n9: Compute $\\widetilde{w}_{t}:=\\mathrm{prox}_{\\eta_{t}f}(w_{n}^{(t)})$ .   \n10: end for ", "page_idx": 7}, {"type": "text", "text": "", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Discussion. Algorithm 2 has a similar form as Algorithm 1, where $u_{0}^{*}(\\widetilde{w}_{t-1})$ is approximated by $\\widetilde{u}_{t}$ . In Algorithm 1, $u_{0}^{*}(\\widetilde{w}_{t-1})$ is approximated by $u_{\\gamma}^{*}(F_{i}^{(t)})$ . Moreover, Alg oerithm 1 solves the smooth eed problem (10) of (3) , ewhile Algorithm 2 directly solves (3). Depending on the choice of method to approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ , we obtain different variants of Algorithm 2. We have proposed two variants: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Semi-shuffling variant: We use (26) for computing $\\widetilde{u}_{t}$ to approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ . \u2022 Full-shuffling variant: We use (27) for computing $\\widetilde{u}_{t}$ to approximate $u_{0}^{*}(\\widetilde{w}_{t-1})$ . ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Note that Algorithm 2 works in an alternative manner, where it  eapproximates $u_{0}^{*}(\\widetilde{w}_{t-1})$ up to a certain accuracy before updating $\\widetilde{w}_{t}$ . This alternating update is very natural and has be een widely applied to solve minimax optimizati oen as well as bilevel optimization problems, see, e.g., [1, 9, 13]. ", "page_idx": 8}, {"type": "text", "text": "4.2 Convergence analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now, we state the convergence of both variants of Algorithm 2: semi-shuffling and full-shuffling variants. The full proof of the following theorems can be found in Supp. Doc. C. ", "page_idx": 8}, {"type": "text", "text": "(a) Convergence of the semi-shuffling variant. Our first result is as follows.   \nTheorem 2. Suppose that Assumptions 1, 2, 4, and $^{5}$ hold for (1), and $\\mathcal{G}_{\\eta}$ is defined by (18). ", "page_idx": 8}, {"type": "text", "text": "Let $\\{(\\widetilde{w}_{t},\\widetilde{u}_{t})\\}$ be generated by Algorithm 2 using the gradient ascent scheme (26) with $\\eta:=\\mathcal{O}(\\epsilon)$ explici tely  geiven in Theorem 8 of Supp. Doc. C, $\\begin{array}{r}{\\hat{\\eta}\\in(0,\\frac{2}{L_{u}+\\mu_{h}}]}\\end{array}$ , $\\begin{array}{r}{S:=\\mathcal{O}\\big(\\frac{1}{\\hat{\\eta}}\\big(\\mu_{h}\\!+\\!\\frac{4L_{u}\\mu_{H}}{L_{u}+\\mu_{H}}\\big)^{-1}\\big)=\\mathcal{O}(1),}\\end{array}$ and $T:=\\mathcal{O}(\\epsilon^{-3})$ explicitly given in Theorem 8. Then, we have $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\|\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})\\|^{2}\\leq\\epsilon^{2}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Consequently, Algorithm 2 requires $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of both $\\nabla_{w}\\mathcal{H}_{i}$ and $\\nabla_{u}\\mathcal{H}_{i}$ to achieve an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ of (3) computed by (19). ", "page_idx": 8}, {"type": "text", "text": "Note that Theorem 2 holds for both $S>1$ and $S=1$ (i.e. we perform only one iteration of (26)). ", "page_idx": 8}, {"type": "text", "text": "(b) Convergence of the full-shuffling variant \u2013 The case $S>1$ with multiple epochs. We state our results for two separated cases: only $\\mathcal{H}_{i}$ is $\\mu_{H}$ -strongly convex, and only $h$ is $\\mu_{h}$ -strongly convex. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Strong convexity of $\\mathcal{H}_{i}$ ). Suppose that Assumptions 1, 2, 4, and $^{5}$ hold, and $\\mathcal{H}_{i}$ is $\\mu_{H}$ -strongly concave with $\\mu_{H}>0$ for $i\\in[n]$ , but $h$ is only merely convex. ", "page_idx": 8}, {"type": "text", "text": "Let $\\{(\\widetilde{w}_{t},\\widetilde{u}_{t})\\}$ be generated by Algorithm 2 using $S$ epochs of the shuffling routine (27) and fixed learni neg  reates $\\eta_{t}\\;=\\;\\eta\\;:=\\;\\mathcal{O}(\\epsilon)$ as given in Theorem 8 of Supp. Doc. $C$ for a given $\\epsilon\\mathrm{~>~0~}$ , $\\hat{\\eta}_{t}:=\\hat{\\eta}=\\mathcal{O}(\\epsilon).$ , $\\begin{array}{r}{S:=\\left\\lfloor\\frac{\\ln(7/2)}{\\mu_{H}\\hat{\\eta}}\\right\\rfloor}\\end{array}$ , and $T:=\\mathcal{O}(\\epsilon^{-3})$ . Then, we have $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\|\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})\\|^{2}\\leq\\epsilon^{2}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Consequently, Algorithm 2 requires $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of $\\nabla_{w}\\mathcal{H}_{i}$ and $\\mathcal{O}(n\\epsilon^{-4})$ evaluations of $\\nabla_{u}\\mathcal{H}_{i}$ to achieve an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ of (3) computed by (19). ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Strong convexity of $h$ ). S ubppose that Assumptions 1, 2, 4, and 5 hold for (1), and $h$ is $\\mu_{h}$ tings as in Theorem 3 $\\mu_{h}>0,$ th $\\mathcal{H}_{i}$ , the conclusions of $i\\in[n]$ m 3 still hold. $\\begin{array}{r}{S:=\\left\\lfloor\\frac{\\ln(7/2)}{\\mu_{h}\\hat{\\eta}}\\right\\rfloor}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "(c) Convergence of the full-shuffling variant \u2013 The case $S=1$ with one epoch. Both Theorems 3 and 4 require $\\mathcal{O}(n\\epsilon^{-4})$ evaluations of $\\nabla_{u}\\mathcal{H}_{i}$ . To improve this complexity, we need two additional assumptions but can perform only one epoch of (27), i.e. $S=1$ . ", "page_idx": 8}, {"type": "text", "text": "Assumption 6. Let $\\hat{\\mathcal{G}}_{\\eta}(u)\\,:=\\,\\eta^{-1}(u-\\mathrm{prox}_{\\eta h}(u+\\eta\\nabla_{u}\\mathcal{H}(w,u)))$ be the gradient mapping of $\\psi(w,\\cdot):=-\\mathcal{H}(w,\\cdot)+h(\\cdot)$ . Assume that there exist $\\hat{\\Lambda}_{0}\\ge1$ and $\\hat{\\Lambda}_{1}\\geq0$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{u}\\mathcal{H}(w,u)\\|^{2}\\leq\\hat{\\Lambda}_{0}\\|\\hat{\\mathcal{G}}_{\\eta}(u)\\|^{2}+\\hat{\\Lambda}_{1},\\quad\\forall(w,u)\\in\\mathrm{dom}\\left(\\mathcal{L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Clearly, if $h=0$ , then $\\hat{\\mathcal{G}}_{\\boldsymbol{\\eta}}(\\boldsymbol{u})=-\\nabla_{\\boldsymbol{u}}\\mathcal{H}(\\boldsymbol{w},\\boldsymbol{u})$ and (20) automatically holds for $\\hat{\\Lambda}_{0}=1$ and $\\hat{\\Lambda}_{1}=0$ . Assumption 6 is similar to Assumption 5, and it is required to handle the prox operator of $h$ in (27). Assumption 7. For $f$ in (1), there exists $L_{f}\\geq0$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(y)\\leq f(x)+\\langle f^{\\prime}(x),y-x\\rangle+\\frac{L_{f}}{2}\\|y-x\\|^{2},\\quad\\forall x,y\\in\\mathrm{dom}\\left(f\\right),\\,f^{\\prime}(x)\\in\\partial f(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Clearly, if $f$ is $L_{f}$ -smooth, then (30) holds. If $f$ is also convex, then (30) implies that $f$ is $L_{f}$ -smooth.   \nUnder these additional assumptions, we have the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Suppose that Assumptions 1, 2, 4, 5, 6, and 7 hold and $\\mathcal{G}_{\\eta}$ is defined by (18). ", "page_idx": 8}, {"type": "text", "text": "Let $\\{(\\widetilde{w}_{t},\\widetilde{u}_{t})\\}$ be generated by Algorithm 2 using one epoch $[S=1$ ) of the shuffling routine (27), and fi xeed  leearning rates $\\eta_{t}=\\eta:=\\mathcal{O}(\\epsilon)$ as in Theorem $^{\\,g}$ of Supp. Doc. $C$ for a given $\\epsilon>0$ , $\\hat{\\eta}_{t}:=$ $\\begin{array}{r}{\\hat{\\eta}=30\\kappa^{2}\\eta,}\\end{array}$ , and $T:=\\mathcal{O}(\\epsilon^{-3})$ , where $\\begin{array}{r}{\\kappa:=\\frac{L_{u}}{\\mu_{H}+\\mu_{h}}}\\end{array}$ . Then, we have $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\|\\mathcal{G}_{\\eta}(\\widetilde{w}_{t})\\|^{2}\\leq\\epsilon^{2}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Consequently, Algorithm 2 requires $\\mathcal{O}(n\\epsilon^{-3})$ evaluations of both $\\nabla_{w}\\mathcal{H}_{i}$ and of $\\nabla_{u}\\mathcal{H}_{i}$ to achieve an $\\epsilon$ -stationary point $\\widehat{w}_{T}$ of (3) computed by (19). ", "page_idx": 9}, {"type": "text", "text": "Similar to Algorit hbm 1, if $\\pi^{(s,t)}$ and $\\hat{\\pi}^{(t)}$ are generated randomly and independently, $\\Lambda_{1}=\\mathcal{O}(1/n)$ , and $\\hat{\\Lambda}_{1}=\\mathcal{O}(1/n)$ , then our complexity stated above can be improved by a factor of $\\sqrt{n}$ . Nevertheless, we omit this analysis. Finally, we can combine each Theorem 2, 3, 4 or 5 and Lemma 2 to construct an $\\hat{\\epsilon}_{}$ -KKT point of (1). Theorem 5 has a better complexity than Theorems 3 and 4, but requires stronger assumptions. Algorithm 2 is also different from the one in [3] both in terms of algorithmic form and the underlying problem to be solved, while achieving the same oracle complexity. ", "page_idx": 9}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We perform some experiments to illustrate Algorithm 1 and compare it with two existing and related algorithms. Further details and additional experiments can be found in Supp. Doc. D. ", "page_idx": 9}, {"type": "text", "text": "We consider the following regularized stochastic minimax problem studied, e.g., in [9, 33]: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{p}}\\Big\\{\\operatorname*{max}_{1\\leq j\\leq m}\\{\\frac{1}{n}\\sum_{i=1}^{n}F_{i,j}(w)\\}+\\frac{\\lambda}{2}\\|w\\|^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $F_{i,j}:\\mathbb{R}^{p}\\times\\Omega\\rightarrow\\mathbb{R}_{+}$ can be viewed as the loss of the $j$ -th model for data point $i\\in[n]$ . If we define $\\phi_{0}(v):=\\operatorname*{max}_{1\\le j\\le m}\\{v_{j}\\}$ and $\\begin{array}{r}{f(w):=\\frac{\\lambda}{2}\\lVert w\\rVert^{2}}\\end{array}$ , then (31) can be reformulated into (3). Since $v_{j}~\\geq0$ , we have $\\phi_{0}(v)\\,:=\\,\\mathrm{max}_{1\\leq j\\leq m}\\{v_{j}\\}\\,\\,\\bar{=}\\,\\,\\|v\\|_{\\infty}\\,=\\,\\mathrm{max}_{\\|u\\|_{1}\\leq1}\\langle v,u\\rangle$ , which is nonsmooth. Thus we can smooth $\\phi_{0}$ as $\\phi_{\\gamma}(v):=\\operatorname*{max}_{\\lVert u\\rVert_{1}\\leq1}\\{\\langle v,u\\rangle-(\\gamma/2)\\lVert u\\rVert^{2}\\}$ using $\\begin{array}{r}{b(u):=\\frac{1}{2}\\lVert u\\rVert^{2}}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "Here, we apply our problem (31) to solve a model selection problem in binary classification with nonnegative nonconvex losses, see, e.g., [41]. Each function $F_{i,j}$ belongs to 4 different nonconvex losses $(m\\,=\\,4)$ ): $F_{i,1}(w,\\xi):=1-\\operatorname{tanh}(b_{i}\\langle a_{i},w\\rangle)$ , $F_{i,2}(w,\\xi)\\,:=\\,\\log(1+\\exp(-b_{i}\\langle a_{i},w\\rangle))\\,-$ $\\log(1+\\exp(-b_{i}\\langle a_{i},w\\rangle-1))$ ), $F_{i,3}(w,\\xi):=(1-1/(\\exp(-b_{i}\\langle a_{i},w\\rangle)+1))^{2}$ , and $F_{i,4}(w,\\xi):=$ $\\log(1+\\exp(-b_{i}\\langle a_{i},w\\rangle))$ (see [41] for more details), where $\\left(a_{i},b_{i}\\right)$ represents data samples. ", "page_idx": 9}, {"type": "text", "text": "We implement 4 algorithms: our SGM with 2 options, SGD from [10], and Prox-Linear from [11]. We test these algorithms on two datasets from LIBSVM [6]. We set $\\lambda:=10^{-4}$ and update the smooothing parameter \u03b3t as \u03b3t := 2(t+11)1/3 . The learning rate \u2318for all algorithms is finely tuned from $\\{100,50,10,5,1,0.5,0.1,0.05,0.01,0.001,0.0001\\}$ , and the results are shown in Figure 1 for w8a and rcv1 datasets using $k_{b}=32$ blocks. The details of this experiment is given in Supp. Doc. D. ", "page_idx": 9}, {"type": "image", "img_path": "lfY0SUT3m9/tmp/98ef2a4f9cb81fd8e3a5543e0df7da6102a6ef55b1c4ba70f90d20110e47b600.jpg", "img_caption": ["Figure 1: The performance of 4 algorithms for solving (31) on two datasets after 200 epochs. ", "As shown in Figure 1, the two variants of our SGM have a comparable performance with SGD and Prox-Linear, providing supportive evidence for using shuffling strategies in minimax algorithms. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work explores a bilevel optimization approach to address two prevalent classes of nonconvexconcave minimax problems. These problems find numerous applications in practice, including robust learning and generative AIs. Motivated by the widespread use of shuffling strategies in implementing gradient-based methods within the machine learning community, we develop novel shuffling-based algorithms for solving these problems under standard assumptions. The first algorithm uses a nonstandard shuffling strategy and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization. The second algorithm is also new, flexible, and offers a promising possibility for further exploration. Our results are expected to provide theoretical justification for incorporating shuffling strategies into minimax optimization algorithms, especially in nonconvex settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partly supported by the National Science Foundation (NSF): NSF-RTG grant No. NSF DMS-2134107 and the Office of Naval Research (ONR), grant No. N00014-23-1-2588. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pages 214\u2013223, 2017.   \n[2] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.   \n[3] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust optimization. Princeton University Press, 2009.   \n[4] A. Beznosikov, E. Gorbunov, H. Berard, and N. Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. In International Conference on Artificial Intelligence and Statistics, pages 172\u2013235. PMLR, 2023.   \n[5] K. Bhatia and K. Sridharan. Online learning with dynamics: A minimax perspective. Advances in Neural Information Processing Systems, 33:15020\u201315030, 2020.   \n[6] C.-C. Chang and C.-J. Lin. LIBSVM: A library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 2011.   \n[7] H. Cho and C. Yun. SGDA with shuffling: faster convergence for nonconvex-P\u0141 minimax optimization. The 11th International Conference on Learning Representations, pp. 1\u201310, 2022.   \n[8] A. Das, B. Sch\u00f6lkopf, and M. Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization. Advances in Neural Information Processing Systems, 35:6749\u20136762, 2022.   \n[9] S. Dempe. Foundations of Bilevel Programming. Springer Science & Business Media, 2002.   \n[10] D. Driggs, J. Liang, and C.-B. Sch\u00f6nlieb. On biased stochastic gradient estimation. Journal of Machine Learning Research, vol. 23, no. 24, pp. 1\u201343, 2022.   \n[11] K. Emmanouilidis, R. Vidal, and N. Loizou. Stochastic extragradient with random reshuffling: Improved convergence for variational inequalities. In International Conference on Artificial Intelligence and Statistics, pages 3682\u20133690. PMLR, 2024.   \n[12] G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective on generative adversarial networks. International Conference on Learning Representations, pp. 1\u201310, 2019.   \n[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.   \n[14] E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysis and improved rates. In International Conference on Artificial Intelligence and Statistics, pages 7865\u20137901. PMLR, 2022.   \n[15] E. Y. Hamedani, A. Jalilzadeh, N. S. Aybat, and U. V. Shanbhag. Iteration complexity of randomized primal-dual methods for convex-concave saddle point problems. arXiv preprint arXiv:1806.04118, 2018.   \n[16] J. Z. HaoChen and S. Sra. Random shuffling beats SGD after finite epochs. International Conference on Machine Learning, pp. 2624\u20132633, 2019.   \n[17] E. Ho, A. Rajagopalan, A. Skvortsov, S. Arulampalam, and M. Piraveenan. Game theory in defence applications: A review. Sensors, 22(3):1032, 2022.   \n[18] Y. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. Advances in Neural Information Processing Systems, 33:16223\u201316234, 2020.   \n[19] Abdul Jabbar, Xi Li, and Bourahla Omar. A survey on generative adversarial networks: Variants, applications, and training. ACM Computing Surveys (CSUR), 54(8):1\u201349, 2021.   \n[20] G. Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer, 2020.   \n[21] F. Lin, X. Fang, and Z. Gao. Distributionally robust optimization: A review on theory and applications. Numerical Algebra, Control & Optimization, 12(1):159, 2022.   \n[22] N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. Advances in Neural Information Processing Systems, 34:19095\u201319108, 2021.   \n[23] L. Luo, H. Ye, and T. Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. Advances in Neural Information Processing Systems, vol. 33, pp. 20566\u201320577, 2020.   \n[24] Z. Luo, J. Pang, and D. Ralph. Mathematical Programs with Equilibrium Constraints. Cambridge University Press, Cambridge, 1996.   \n[25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[26] Q. Meng, W. Chen, Y. Wang, Z.-M. Ma, and T.-Y. Liu. Convergence analysis of distributed stochastic gradient descent with shuffling. Neurocomputing, 337:46\u201357, 2019.   \n[27] K. Mishchenko, A. Khaled, and P. Richt\u00e1rik. Random reshuffling: Simple analysis with vast improvements. Advances in Neural Information Processing Systems, 33:17309\u201317320, 2020.   \n[28] K. Mishchenko, A. Khaled, and P. Richt\u00e1rik. Proximal and federated random reshuffling. In International Conference on Machine Learning, pages 15718\u201315749. PMLR, 2022.   \n[29] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization. Kluwer Academic Publishers, 2004.   \n[30] L. M. Nguyen, Q. Tran-Dinh, D. T. Phan, P. H. Nguyen, and M. van Dijk. A unified convergence analysis for shuffling-type gradient methods. Journal of Machine Learning Research, 22(207):1\u2013 44, 2021.   \n[31] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances in Neural Information Processing Systems, pages 1416\u20131424, 2016.   \n[32] I. Safran and O. Shamir. How good is SGD with random shuffling? Conference on Learning Theory, pp. 3250\u20133284, 2020.   \n[33] A. Shapiro and A. Kleywegt. Minimax analysis of stochastic problems. Optim. Methods Softw., 17(3):523\u2013542, 2002.   \n[34] Q. Tran-Dinh, D. Liu, and L. M. Nguyen. Hybrid variance-reduced SGD algorithms for nonconvex-concave minimax problems. The 34th Conference on Neural Information Processing Systems (NeurIPs 2020), 2020.   \n[35] J. Wang, T. Zhang, S. Liu, P.-Y. Chen, J. Xu, M. Fardad, and B. Li. Adversarial attack generation empowered by min-max optimization. Advances in Neural Information Processing Systems, 34:16020\u201316033, 2021.   \n[36] M. Wang, E. Fang, and L. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Math. Program., 161(1-2):419\u2013449, 2017.   \n[37] J. Yang, N. Kiyavash, and N. He. Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621, 2020.   \n[38] B. Ying, K. Yuan, and A. H. Sayed. Convergence of variance-reduced stochastic learning under random reshuffling. arXiv preprint arXiv:1708.01383, 2(3):6, 2017.   \n[39] J. Zhang and L. Xiao. Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization. Mathematical Programming, pp. 1\u201343, 2022.   \n[40] K. Zhang, Z. Yang, and T. Ba\u00b8sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321\u2013384, 2021.   \n[41] L. Zhao, M. Mammadov, and J. Yearwood. From convex to nonconvex: a loss function analysis for binary classification. In IEEE International Conference on Data Mining Workshops (ICDMW), pages 1281\u20131288. IEEE, 2010.   \n[42] R. Zhao. Optimal stochastic algorithms for convex-concave saddle-point problems. arXiv preprint arXiv:1903.01687, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Our claims made in the abstract reflects our contribution stated in the introduction, see the \u201cContribution\" paragraph in the introduction section. Our contribution consists of two algorithms, Algorithm 1 and Algorithm 2, and their theoretical convergence guarantees stated in the subsequent theorems. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: This paper has limitation as it only focuses on two classes of minimax problems defined in (1). Yes, we only consider two classes of minimax problems: nonconvex-linear (NL) and convex-strongly concave (NC), covered by our assumption, Assumptions 1 to 5. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We state all required assumptions in Assumptions 1 to 5. Our theoretical results stated in each theorem also refers to these assumptions when required. Our full proofs are given in Supp. Docs. due to space limit, and we believe that our technical proofs are correct. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \nTheorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide the details of our experiments, including mathematical models, the detailed implementation of algorithms, the choice of parameters, and datasets. We also upload the code with examples to run and verify. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "The answer NA means that the paper does not include experiments.   \nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 14}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our data is available online from LIBSVM. The code is implemented in Python. The code for all experiments is also provided with instruction. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Supp. Doc. D provides all the details of our experiments, including how to select parameters, and how to report our results. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not have such a result to report. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our experiments were run on a MacBook Pro. 2.8GHz Quad-Core Intel Core I7, 16Gb Memory specified at the beginning of Supp. Doc. D. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our data is publicly available online from LIBSVM. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: We do not yet know if our paper has an immediate broader impact. However, since our problems and our algorithms are sufficiently general, we hope they will create broader impacts. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not have our own real data or specific model that has a high risk for misuse. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our code is open-source and will be made available online under a standard public license. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: It does not have new asset. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: It does not relate to crowdsourcing experiments and research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: It does not require any approval. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]