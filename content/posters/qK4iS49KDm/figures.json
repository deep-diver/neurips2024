[{"figure_path": "qK4iS49KDm/figures/figures_1_1.jpg", "caption": "Figure 1: We train a ReLU NN (3.1) with N = 1024 neurons using SGD (squared loss) with step size \u03b7 = 1/d to learn a single-index target f*(x) = H3((x, \u03b8)); heatmaps are values averaged over 10 runs. (a) online SGD with batch size B = 8; (b) GD on the same batch of size n for T = 214 steps. We only report weak recovery (i.e., overlap between parameters w and target \u03b8, averaged across neurons) for online SGD since the test error does not drop.", "description": "This figure shows the results of training a two-layer ReLU neural network with 1024 neurons using two different approaches: online SGD with a batch size of 8 and GD on the same batch of size n for 2\u00b9\u2074 steps. The target function is f*(x) = H3((x, \u03b8)), where H3 is the third Hermite polynomial.  The heatmaps display the weak recovery (overlap between learned parameters w and the target direction \u03b8) for online SGD and the generalization error for GD, averaged over 10 runs. The results highlight a significant difference in performance between the two approaches, with online SGD failing to achieve low test error even with a large number of samples, while GD with batch reuse achieves low generalization error with n ~ d samples.", "section": "1 Introduction"}]