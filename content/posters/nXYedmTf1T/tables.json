[{"figure_path": "nXYedmTf1T/tables/tables_4_1.jpg", "caption": "Table 1: The performance of CSR on LLaVA-1.5 across all benchmarks is presented. Most baseline results, except those for self-rewarding, are sourced from Zhou et al. [10].", "description": "This table presents a comparison of the performance of the Calibrated Self-Rewarding (CSR) method on the LLaVA-1.5 model against various baselines across multiple benchmarks.  The benchmarks cover a range of tasks, including comprehensive benchmarks assessing various aspects of multimodal reasoning, general VQA (Visual Question Answering) tasks, and hallucination benchmarks evaluating the model's tendency to generate factually incorrect or hallucinatory outputs.  The baselines represent different approaches to improving modality alignment in vision-language models, including methods leveraging human feedback, reinforcement learning, and other self-rewarding techniques. The table allows for a comprehensive comparison of the effectiveness of CSR in improving the overall performance and reducing hallucinations of LLaVA-1.5 compared to existing state-of-the-art methods.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_5_1.jpg", "caption": "Table 1: The performance of CSR on LLaVA-1.5 across all benchmarks is presented. Most baseline results, except those for self-rewarding, are sourced from Zhou et al. [10].", "description": "This table presents a comparison of the performance of the Calibrated Self-Rewarding (CSR) method on the LLaVA-1.5 model across various benchmarks.  It shows the results for CSR and several baseline methods, including methods using human feedback, additional models, and other self-rewarding approaches.  The benchmarks cover several aspects of vision-language model performance, including comprehensive benchmarks (measuring overall capabilities), general VQA (visual question answering), and hallucination benchmarks (assessing the tendency to generate inaccurate or nonsensical responses).  The table allows for a quantitative assessment of how CSR improves upon existing methods in terms of accuracy and reducing hallucination.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_5_2.jpg", "caption": "Table 2: Ablation study of vision-text reward score.", "description": "This table presents the ablation study results, comparing the performance of the proposed CSR method against three variants: the baseline (Base) without CSR, CSR using only the self-generated instruction-following score (Only RT), and CSR using only the image-response relevance score (Only R1).  The results show the effectiveness of integrating both scores in CSR for improved model performance, particularly highlighting the contribution of the image-response relevance score.", "section": "3.1 Step-Level Reward Modeling and Calibration"}, {"figure_path": "nXYedmTf1T/tables/tables_18_1.jpg", "caption": "Table 1: The performance of CSR on LLaVA-1.5 across all benchmarks is presented. Most baseline results, except those for self-rewarding, are sourced from Zhou et al. [10].", "description": "This table presents a comparison of the performance of the Calibrated Self-Rewarding (CSR) method on the LLaVA-1.5 model across various benchmarks.  It compares CSR to several baseline methods, including those using human feedback, preference optimization, and other self-rewarding techniques. The benchmarks cover comprehensive evaluations, general Visual Question Answering (VQA) tasks, and hallucination metrics. The table shows the improvements achieved by CSR in terms of various performance metrics across all benchmark types.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_18_2.jpg", "caption": "Table 1: The performance of CSR on LLaVA-1.5 across all benchmarks is presented. Most baseline results, except those for self-rewarding, are sourced from Zhou et al. [10].", "description": "This table presents a comprehensive comparison of the performance of the Calibrated Self-Rewarding (CSR) model against various baseline methods across a range of benchmarks.  These benchmarks assess different aspects of vision-language model capabilities, encompassing comprehensive evaluations, general visual question answering (VQA), and hallucination detection. The results demonstrate the improvement achieved by CSR over existing approaches in terms of accuracy and reducing hallucinatory responses.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_19_1.jpg", "caption": "Table 7: The performance of CSR online iteration with LLaVA-1.5 as the backbone on hallucination benchmarks.", "description": "This table presents the results of the Calibrated Self-Rewarding (CSR) approach on hallucination benchmarks, using LLaVA-1.5 as the base model.  It shows the performance metrics (POPEacc, POPEf1, CHAIRS, CHAIR1, and Avg Length) across multiple iterations of CSR. The results demonstrate the iterative improvement in reducing hallucinations over several iterations of the CSR process.  Each row represents a different stage (iteration) of training.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_19_2.jpg", "caption": "Table 8: The performance of CSR online iteration with Vila 7B as the backbone.", "description": "This table presents the performance of the Calibrated Self-Rewarding (CSR) approach, specifically after three iterations, using the Vila 7B model.  It shows the model's scores across various benchmarks, categorized into Comprehensive Benchmark, General VQA, and Hallucination Benchmark.  Each category includes multiple metrics, providing a comprehensive evaluation of the model's performance after undergoing the iterative CSR process. The table allows for a comparison of the model's performance before (baseline) and after the CSR iterations, showcasing the impact of the method.", "section": "4.2 Results"}, {"figure_path": "nXYedmTf1T/tables/tables_19_3.jpg", "caption": "Table 9: Performance comparison of CSR on LLaVA-1.5 7B with different \u03bb values on various benchmarks.", "description": "This table presents the results of an ablation study on the hyperparameter \u03bb used in the Calibrated Self-Rewarding (CSR) approach.  The study varies the value of \u03bb (which balances the self-generated instruction-following score and image-response relevance score) and evaluates the impact on various LVLMs benchmarks (MMEP, MMEC, SEED, LLAVAW, MMB, MM-Vet, SQA, VisWiz, GQA, POPE, CHAIRS, CHAIR1). The table shows how different values of \u03bb affect the performance on each benchmark, illustrating the importance of calibrating the reward score to improve modality alignment in LVLMs.", "section": "4.3 Analysis"}, {"figure_path": "nXYedmTf1T/tables/tables_19_4.jpg", "caption": "Table 10: Reward score and average performance score across multiple iterations of CSR on LLaVA-1.5 7B.", "description": "This table shows the reward scores for chosen and rejected responses, as well as the average performance score across five iterations of the Calibrated Self-Rewarding (CSR) method. The data demonstrates the iterative improvement in model performance as the CSR process continues.", "section": "4.2 Results"}]