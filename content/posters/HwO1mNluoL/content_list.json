[{"type": "text", "text": "Mitigating Biases in Blackbox Feature Extractors for Image Classification Tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Abhipsa Basu Saswat Subhajyoti Mallick \u2217 R. Venkatesh Babu ", "page_idx": 0}, {"type": "text", "text": "Vision and AI Lab, Indian Institute of Science, Bangalore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In image classification, it is common to utilize a pretrained model to extract meaningful features of the input images, and then to train a classifier on top of it to make predictions for any downstream task. Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task. Further, very often they may be blackbox, either due to scale, or because of unavailability of model weights or architecture. Thus, during a downstream task, we cannot debias such models by updating the weights of the feature encoder, as only the classifier can be finetuned. In this regard, we investigate the suitability of some existing debiasing techniques and thereby motivate the need for more focused research towards this problem setting. Furthermore, we propose a simple method consisting of a clustering-based adaptive margin loss with a blackbox feature encoder, with no knowledge of the bias attribute. Our experiments demonstrate the effectiveness of our method across multiple benchmarks. The code is publicly available at https: //github.com/abhipsabasu/blackbox_bias_mitigation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning models are to known to inherit harmful stereotypical biases with respect to the different genders, races, cultures, from the datasets they are trained on [1, 2, 3]. For example, a model trained on a gender-biased dataset with images of people having blond and non-blond hair may wrongly learn a correlation between the label \u2018blond hair\u2019 and the gender of the person in the image. Thus the model fails to classify images belonging to minority groups (in this case blond males and non-blond females). These biases can affect the performance of AI systems handling job recruitment, e-commerce, health care, face detection and recognition [4, 5, 6]. Models can also learn spurious correlations between irrelevant training features and the target labels, instead of focusing on the relevant ones [7, 8]. Several works focus on mitigating such biases from trained models in a variety of tasks [9, 10, 11]. ", "page_idx": 0}, {"type": "text", "text": "In recent times, large-scale models, pretrained on enormous amounts of data, are being used by machine learning practitioners as feature encoders for numerous downstream applications, as their features are shown to be semantically rich [12, 13]. However, do these powerful pretrained features themselves exude the harmful stereotypical biases that are known to affect traditional deep learning systems? A previous work [14] sheds light on these questions (when the pretrained model can be fully finetuned on the downstream dataset) \u2013 the downstream models finetuned on top of pretrained models can inherit their biases and such biases can be mitigated simply by manipulating the finetuning data. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate a more constrained yet practical problem setting. With billion-parameter models gaining impetus in today\u2019s world, it may not be feasible to finetune/retrain such models due to scarcity of resources or unavailability of model weights for privacy concerns [13, 12, 15, 16, 17, 18]. ", "page_idx": 0}, {"type": "image", "img_path": "HwO1mNluoL/tmp/7597b8efe80cb50b007fd1d7cbf6b2f9d5a03d08fb20ca395a1045ee033959e7.jpg", "img_caption": ["Figure 1: Summary of our setup. A frozen pretrained encoder is used as a feature extractor for a downstream task. An adapter is attached on top of the encoder that learns the bias in the downstream dataset. Finally, the bias is mitigated with the help of an adaptive margin loss, leading to unbiased predictions. No knowledge of the bias attribute is assumed apriori. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Hence, for downstream classification tasks, simply the classifier layer on top of the pretrained features is trained on the corresponding dataset, keeping the rest of the network frozen. We ask, to what extent can such systems be debiased, given that the pretrained model weights are unavailable? ", "page_idx": 1}, {"type": "text", "text": "For the scope of this paper, we assume that the downstream dataset consists of multiple groups (e.g. blond males, non-blond males, blond females, non-blond females) due to the presence of a certain bias attribute (e.g. gender). We find that the effect of the bias in the downstream model depends on how well the pretrained feature encodes the target attribute as compared to the bias attribute. The system remains unbiased if the pretrained features are highly aware of the target attribute (even when the bias correlation in the downstream dataset is high). However, if the pretrained features instead predominantly encode the bias attribute, the downstream system becomes biased. A simple solution to the problem is to group-balance, or reduce the bias-correlation of the finetuning dataset, similar to the suggestion of Wang et al. [14]. However, as manipulating the bias correlation of a dataset is not straightforward, we advocate for a specific strategy to debias the system in the given scenario. ", "page_idx": 1}, {"type": "text", "text": "To simplify the bias-mitigation task, we insert a trainable adapter module between the pretrained feature extractor and the classifier. With such a setting, after evaluating existing methods, we find that a large number of these methods do not yield the expected performance gains compared to the ERM-trained model, indicating the challenges in the problem setting as well as the necessity of designing specific debiasing strategies. To aid the mitigation process, we forcefully amplify the bias in the downstream model into the adapter (following previous works [19, 20]), and then investigate a number of simple debiasing techniques utilizing this biased adapter. Finally we propose a method involving clustering-based adaptive margin loss which first clusters the biased feature space, and accordingly sets the margin value for a given sample such that it is inversely proportional to the frequency of the sample\u2019s ground truth class in the cluster it belongs to. The problem setting is depicted in Fig. 1. We summarize our key contributions in this paper below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We highlight a practical yet under-explored problem setting on how harmful biases creep into a model when it uses pretrained but blackbox feature extractors to obtain features. \u2022 We explore the scenarios in which biases can propagate from the pretrained features to the downstream tasks and demonstrate the necessity and feasibility of debiasing strategies in such cases. \u2022 To debias, we first amplify the bias learnt by the model from the downstream data and propose a simple mitigation strategy by utilizing the identified biases and applying an adaptive margin loss. Extensive experiments show that the proposed method is effective across multiple benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Known biases: Mitigation of biases with the knowledge of the bias attributes, as well as their labels, is a widely explored problem [21, 22, 23, 24, 25, 26, 27, 28]. Roh et al. [29] propose a sampling algorithm by formulating a combinatorial optimization problem for unbiased sample selection. While Sagawa et al. [23] minimize worst-case training loss over the groups formed using the available bias labels and the target class labels, Zhang et al. [30] disentangle the bias and target representations and debias the network using a mutual information estimator. Some methods adopt a semi-supervised approach, where bias attributes are annotated for only a few data samples [20, 31]. Another body of work assumes knowledge of the bias attribute but not its specific labels [32, 33, 34, 35]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Unknown biases: Recent works consider the more practical scenario of unavailability of the knowledge of the bias attribute as well as its labels [36, 37, 38, 39, 40, 19, 41, 42, 43]. This area encompasses a wide variety of works, a subset of which is described here. JTT [19] debiases models using misclassifications from an ERM-trained model. Some approaches like BPA and GEORGE employ clustering to identify the biases in the dataset [44, 45]. Certain works employ the Generalized Cross-Entropy (GCE) loss [46, 47] to amplify the biases learnt by the network and then debias it. While most of these works, like LfF [20, 37, 48], employ two branches in the network, with one branch over-learning the bias and the other debiasing it, Ahn et al. [40] utilize the GCE loss to learn the bias, and then find the per-sample gradient of the trained model to obtain a balanced dataset. DebiAN [38] identifies multiple biases by having a discoverer model which optimizes an equal opportunity violation loss. Correct-n-Contrast [49] identifies training samples having the same class labels but dissimilar bias features via Empirical Risk Minimization (ERM) training and then applies contrastive loss\u2013similar to Contrastive Adapter [50]\u2013to bring the target features of these same-class samples closer. Qraitem et al [51] propose a sampling method to reduce dataset biases. Kim et al. [52] use a committee of auxiliary classifiers to identify the biases in the network, assigning large weights to the identified bias-confilcting samples during the training of the main classifier. Jeon et al. [53] show how bias in a CNN network is more pronounced at the top layers, and hence, leveraging features from the lower layers can help the model to exploit less biased representations. Kirichenko et al. [54] observe that retraining only the last layer is enough for having an unbiased model. However, they require bias annotations in the $2^{n d}$ stage of training. This is avoided by LaBonte et al. [55] by constructing a reweighting dataset using model disagreements for the second stage of training. ", "page_idx": 2}, {"type": "text", "text": "Biases in pretrained models: Utilizing pretrained feature extractors for downsteam tasks is a common trend in recent times. Such encoders may carry biases that crept in from the datasets used for training them \u2013 many recent works focus on quantifying the fairness of such pretrained models [56]. Srinivasan et al. [57] study biases in multimodal vision-language systems. Goyal et al. [58] shows how pretrained models trained with self-supervision exhibit lesser biases than those trained with supervision. Recent works show that biases of pretrained models can be reduced by manipulating the fine-tuning dataset in both Computer Vision and NLP [14, 59]. Salman et al. [60] show that bias transfer happens from pre-trained models to the downstream tasks, even when the target data is unbiased. Our problem setting on the other hand consumes features from a blackbox pretrained encoder and aims to mitigate the biases arising from that encoder. This is challenging, especially when the bias annotations are not available. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement and Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries and Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This work focuses on the task of image classification. Let $\\mathcal{X}=\\{x_{1},x_{2},...\\,x_{N}\\}$ be a set of training images of size $N$ , and $\\{y_{1},y_{2},\\dots,y_{N}\\}$ be the corresponding labels, where each $y_{i}\\in\\mathcal{C}$ . Each data point $(x_{i},y_{i})$ is associated with a hidden spurious attribute $a_{i}\\in\\mathcal{A}$ , and consequently a group $g_{i}\\in\\mathcal{G}$ , where $\\mathcal{G}=\\mathcal{C}\\times\\mathcal{A}$ . We assume that each group $g\\in{\\mathcal{G}}$ is present in the training data. In an unbiased dataset, number of training samples in each $g$ remains approximately equal. However, models tend to learn spurious correlations when there is an imbalance in these numbers. We refer to samples favoured most by the ERM trained models as bias-aligned, and the rest as bias-conflicting [20, 38]. The goal is to train a model to optimize a mapping function $f:\\mathcal{X}\\rightarrow\\mathbb{R}^{|\\mathcal{C}|}$ . In ERM training, we optimize the Cross-Entropy (CE) loss as defined below for a sample $(x,y)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{T}=\\sum_{j=1}^{|\\mathcal{C}|}-p_{j}\\log\\hat{p_{j}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $[p_{1},p_{2},\\dotsc,p_{|{\\mathcal{C}}|}]$ is a one-hot vector representing $y$ . The corresponding predicted probability vector for the same sample is given by $[\\hat{p_{1}},\\hat{p_{1}},\\dotsc,\\dotsc,\\hat{p_{|}}]$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Biases in Pretrained Features ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Extracting features from popular pretrained encoders for downstream applications is a common norm, as being trained on large amounts of data equips these models to perform well on a variety of tasks. We freeze these models to avoid backpropagating into their architectures that are generally large-scale [12] and often only accessible through API calls [15, 18, 16, 17]. In this subsection, we choose the WaterBirds dataset [23] to analyse the effect of the pretrained model on the downstream performance (see Section 4 for details). We observe two different scenarios here: ", "page_idx": 2}, {"type": "table", "img_path": "HwO1mNluoL/tmp/d0a40b1787a78a571af2962ef0725b235792fb73d0b6422cf09432f245a1fe69.jpg", "table_caption": ["Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Scenario 1: Pretrained features are target-aware. Mehta et al. [61] find that if one chooses a proper pretrained encoder for their specific downstream task, ERM training on a linear classifier attached to the pretrained embedding is enough to obtain unbiased predictions. For instance, using a ViT-H 14 encoder pretrained on the SWAG dataset [62] followed by end-to-end ImageNet [63] finetuning can achieve state-of-the-art results on the Waterbirds dataset. We find that even if all the bias-conflicting samples are removed from the training data, the worst group accuracies remain sufficiently high for the test set (see Table 1). Thus, the pretrained model is highly aware of the downstream target attribute, and no debiasing is required. However, a single pretrained model is hardly a panacea for bias mitigation. For instance, the worst group accuracy for the CelebA [64] dataset\u2019s Blond Hair Classification is merely $\\mathbf{6.71\\%}$ ! ", "page_idx": 3}, {"type": "text", "text": "Scenario 2: Pretrained features are bias-aware. Contrary to Scenario $^{\\,l}$ , when we use an ImageNetpretrained ResNet-18 as the pretrained encoder, we notice that the worst group accuracies are considerably low for the original Waterbirds dataset itself (see Table 1). An even further drop is seen when the bias-conflicting samples are removed. This shows that this feature encoder exhibits the biases present in the downstream dataset. One simple mitigation strategy is to reduce the biascorrelation in the training set. Group-balancing the Waterbirds dataset leads to considerably uniform accuracies across all groups, irrespective of the underlying pretrained model, as shown in Table 1. This may not always be simple \u2013 firstly, the bias attribute may not be known apriori to a practitioner, and secondly, group-balancing or reducing the bias in the downstream dataset may be expensive. Further analyses on these lines are presented in Appendix subsection A.1. Thus, for such systems, an explicit mitigation strategy is required so that the model predictions are unbiased. ", "page_idx": 3}, {"type": "text", "text": "3.3 Bias mitigation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To mitigate biases in this problem setting, we consider an image classification model that consists of 3 primary components (see Fig. 1): a) A pretrained feature extractor $m$ to extract the image features $f^{\\mathrm{{locked}}}\\,{\\overset{}{=}}\\,m(x)$ ( $m$ is blackbox, i.e. frozen) , b) an adapter consisting of a multi-layer perceptron model $h$ comprising of a single non-linear hidden layer, projecting $f^{\\mathrm{locked}}$ to a new latent space defined as $\\hat{f}=h(f^{\\mathrm{locked}})$ , c) a classifier $C$ , attached to $\\hat{f}$ to obtain the final predictions $\\hat{y}=C(\\hat{f})$ . ", "page_idx": 3}, {"type": "text", "text": "Performance of existing methods. A large body of work exists in the domain of bias mitigation. However, most of these works assume a fully trainable feature encoder. We pick a few representative ones like DebiaN [38], BPA [45], GEORGE [44], LfF [20], JTT [19] and Contrastive Adapter (CoAda) [50] (see Section 2 for details) and explore their efficacy in this setup. We choose three popular benchmarks: Waterbirds, CelebA, and ColorMNIST-0.995 (descriptions available in Sec 4) and use a frozen ResNet-18 feature encoder pretrained on the ImageNet dataset for this experiment. We have a number of key observations (see Table 2): ", "page_idx": 3}, {"type": "text", "text": "\u2022 LfF performs well on Waterbirds and ColorMNIST-0.995, however, it drops by $14\\%$ on CelebA. Similar trends are seen for other methods as well. This shows that existing methods exhibit inconsistencies in this constrained problem setting. \u2022 Co-Ada, which was designed to improve the zero-shot performance of foundation models (without backpropagating into the underlying model), works consistently well across all datasets. However, Contrastive Adapter is computationally expensive (Appendix Table 17). ", "page_idx": 3}, {"type": "text", "text": "Table 2: Performance of existing methods on the proposed problem setting. We observe that for three different benchmarks, performance of existing methods is either close to that of the ERM model (measured by $\\Delta_{\\mathrm{ERM}},$ ), or not consistently high. Co-Ada [50] is one exception among compared methods, having the highest worst-group accuracies for all three benchmarks. ", "page_idx": 4}, {"type": "image", "img_path": "HwO1mNluoL/tmp/3254c83b0f95ff9d4348fae88056a05df75451cd65d01be0ba402b14fcb7246b.jpg", "img_caption": ["Figure 2: Effect of increasing weight decay. (a) For CelebA, as $\\lambda$ increases, the worst group (Blond Male (M-B)) accuracy reduces, though the accuracies for Blond Female (F-B) and Non-blond Males (M-NB) still remain high. (b) For Waterbirds, we see a fall in both Land-Waterbird (L-WB) and Water-Landbird (W-LB) accuracies with increasing $\\lambda$ , though the scores for Land-Landbird (L-LB) and Water-Waterbird (W-WB) remain high. (c) For ColorMNIST, while the bias-confilcting accuracies reduce as expected, the training accuracy dips beyond $\\lambda=0.1$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The above observations serve as motivating principles behind the proposed method for this problem setting that is computationally efficient and effective across benchmark datasets. ", "page_idx": 4}, {"type": "text", "text": "3.4 Our approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we present our approach. Inspired by previous works [20, 19, 38], we first amplify the biases in the system. Following this, we discuss a few alternatives for mitigation that eventually lead upto our final method. ", "page_idx": 4}, {"type": "text", "text": "Bias-amplified Training. We propose learning features $\\hat{f}$ first using an adapter (i.e. an MLP layer) on top of the pretrained features $f^{\\mathrm{locked}}$ through ERM training, and then identifying the biases from these learnt features. The goal is to reduce the worst-group accuracies of the training set while maintaining the overall training performance \u2013 thereby amplifying the bias learnt by the model. As indicated in previous works [65, 23], we find that increasing the weight decay $\\lambda$ reduces worst-group performance of the training set considerably. We next show the effect of changing $\\lambda$ on 3 popular datasets: Waterbirds, CelebA and ColorMNIST-0.995. In Fig. 2(a), we observe that with increasing weight decay, the worst group accuracies reduce for CelebA\u2019s Blond Hair classification [64] (i.e. for Blond Males), and also dips for Non-blond Females, while the other two groups\u2019 performances remain high. This points to the amplification of gender bias, i.e., the model tends to predict Blond Hair for female images and Non-blond Hair for male images. A similar phenomenon is noticed in the case of Waterbirds [23], as shown in Fig. 2(b). Although the same pattern is exhibited in the case of ColorMNIST-0.995 in Fig. 2(c), an interesting observation also emerges. For $\\lambda=0.1$ , training accuracy drops off quite steeply. So, with the goal of reducing the performance on bias conflicting samples, the model may end up not learning anything meaningful at all (as also seen in Fig. 2(a)). This hints at a tradeoff, where we fix $\\lambda$ to a high value while ensuring the training accuracy does not fall drastically. We select the model based on the training accuracy as higher training accuracy ensures optimal learning of the bias-aligned data points even when there is less learning of the bias-confilcting samples. To avoid all predictions from collapsing to a single class, we sample equally from each class for each batch of the training set. We call the obtained bias-amplified model $B$ , and utilize its knowledge to design a few alternative mitigation strategies. For debiasing, we use the same architecture as that of the biased model $B$ , and denote it by $D$ . ", "page_idx": 4}, {"type": "table", "img_path": "HwO1mNluoL/tmp/688aad663254f0019285c0dece8681adf53b4a60db1b9c3feee00c0aea9fc39d.jpg", "table_caption": ["Table 3: Comparison of different alternatives. We compare three methods \u2013 loss-weighted CE loss (LW), cluster-weighted CE loss (CW), and cluster-based margin loss (CM). While LW leads to improved scores compared to an ERM-trained model, CW further improves upon it for CelebA and ColorMNIST-0.995. Finally, the proposed CM outperforms the above two methods by a large margin. All results are with respect to an ImageNet-pretrained ResNet-18 model. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Before delving into the rest of our method, we discuss a caveat here. An attentive reader might enquire as to whether the above technique of bias amplification will always work. We reiterate that we consider the bias in the downstream dataset to align with that in the feature encoder. An example to the contrary has been discussed in Scenario 1 (subsection 3.2). This presents an interesting future direction where one might attempt to debias a feature encoder which does not capture the downstream dataset bias. ", "page_idx": 5}, {"type": "text", "text": "Technique 1: Loss-weighted (LW) Cross-Entropy Loss. We use a weighted CE loss to train $\\begin{array}{r}{D{:}\\;{L_{L W}^{D}}=-{L_{T}^{B}}\\sum_{j=1}^{|\\mathcal{C}|}{\\bar{p}_{j}}\\log{\\hat{p}_{j}}}\\end{array}$ , where $L_{T}^{B}$ is the CE loss computed from $B$ for the sample with respect to its ground truth label. The intuition is that $B$ being biased, it would upweight the CE loss for the bias-conflicting sample in the mitigation stage. On implementing this for the Waterbirds, CelebA and ColorMNIST-0.995, we find that even though the mitigation performance increases as compared to the ERM-trained model, the improvement is not satisfactory (see Table 3). ", "page_idx": 5}, {"type": "text", "text": "Technique 2: Cluster-weighted (CW) Cross-Entropy Loss. The adapter features $\\hat{f}$ in the biased model $B$ are expected to encode the bias in the downstream dataset. Hence, we explore a cluster-based weighting scheme for bias mitigation by clustering the bias-amplified feature space $\\hat{f}$ in $B$ . We define $K$ as the number of clusters obtained and $\\bar{m}_{c}^{z}$ as the proportion of class $c$ within the cluster $z$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{m}_{c}^{z}=\\frac{n_{c}^{z}+\\epsilon}{\\sum_{c^{\\prime}\\in c}n_{c^{\\prime}}^{z}+\\epsilon}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $n_{c}^{z}$ is the number of times samples of class $c$ have occurred in cluster $z$ . For any sample in the training data, we find the cluster it belongs to (say $z$ ), and then weight the CE loss for training $D$ in the following way: $\\begin{array}{r}{L_{L W}^{D}=-\\sum_{j=1}^{|\\mathcal{C}|}(\\bar{1}-\\bar{m}_{j}^{z})\\bar{p_{j}}\\log\\hat{p}_{j}}\\end{array}$ . Intuitively, the loss is upweighted if the sample belongs to a minority class in its cluster, otherwise it is downweighted. From Table 3, we find that it is a significant improvement over LW. Infact, from Table 2, we find that it performs on par with Co-Ada [50] for Waterbirds and ColorMNIST-0.995. ", "page_idx": 5}, {"type": "text", "text": "3.5 Cluster-based Adaptive Margin Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Technique 2, we see that clustering-based weighting methods can be effective, where we upweight samples belonging to the minority classes in a cluster, whereas we downweight the others proportionally to increase the importance of the sparse samples. We further ask, inside each cluster, can we make the features of the individual classes more discriminative? We conjecture that in such a case, it will be easier for the debiasing model $D$ to distinguish among the samples from the different groups in the dataset (assuming that the clusters are an approximate representation of the true groups in the dataset). To this end, margin losses are an efficient class of loss functions that can reduce intra-class distance and increase inter-class distance in the feature space. Enforcing such margin constraints on hyperspherical feature spaces has been shown to be beneficial in case of deep face recognition [6, 5, 66], long-tailed learning [67], few-shot learning [68] and the language-bias problem [69, 70] in Visual Question Answering (VQA). Especially, inspired by the works of long-tailed learning and VQA, we make the margins adaptive to ensure discriminative features among the frequently and infrequently occurring classes within a cluster. To achieve this, we utilize the weights used in the cluster-weighted (CW) CE loss (defined above) as the margin values. We begin by defining the following: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Normalized CE loss: First, we reformulate the $C E$ loss as a cosine loss [5, 6], by $L2$ -normalizing the classifier weight vectors $\\mathbf{W_{k}}\\in C$ (recall that $C$ is the classifier) for each class $c_{k}\\in\\mathcal{C}$ $k=$ $1,2,\\ldots|{\\mathcal{C}}|)$ , and the trainable feature $\\hat{f}$ . We define $\\begin{array}{r}{\\hat{\\mathbf{W}}_{\\mathbf{k}}\\,=\\,\\frac{\\mathbf{W}_{\\mathbf{k}}}{||\\mathbf{W}_{\\mathbf{k}}||}}\\end{array}$ $\\begin{array}{r}{\\hat{\\bf f}_{\\bf n o r m}=s\\frac{\\hat{\\bf f}}{\\|\\hat{\\bf f}\\|}}\\end{array}$ , where $s$ is a scaling parameter. The bias term is set to 0 for simplicity. Let $\\theta_{k}$ be the angle between $\\hat{\\bf f}$ and $\\mathbf{W_{k}}$ . Therefore, the logit for each class $c_{k}$ becomes: $\\begin{array}{r}{\\hat{y}_{k}=\\hat{\\mathbf{W_{k}}}^{\\top}\\hat{\\mathbf{f}}_{\\mathbf{norm}}=\\|\\hat{\\mathbf{W}}_{\\mathbf{k}}\\|\\|\\hat{\\mathbf{f}}_{\\mathbf{norm}}\\|\\cos\\theta_{k}=}\\end{array}$ $s~\\cos\\theta_{k}$ . The features $\\hat{\\mathbf{f}}_{\\mathbf{norm}}$ are thus distributed on a hypersphere with a radius $s$ . This makes the normalized $C E$ loss for a single sample as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{N S}=\\sum_{k=1}^{|\\mathcal{C}|}-p_{k}\\log\\frac{\\exp(s\\ \\cos\\theta_{k})}{\\sum_{j=1}^{|\\mathcal{C}|}\\exp(s\\ \\cos\\theta_{j})}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Adaptive Margin Loss. Inspired by the ArcFace loss [6] used in face recognition, we define the adaptive margin loss here. The loss adds a margin penalty to the angle between the features $\\hat{\\bf f}$ , and the classifier weights $\\mathbf{W_{k}}$ for the $k^{t h}$ class. Since the margin is placed on the angle, it maps exactly to the \"geodesic\" distance on the hypersphere [6], leading to highly discriminative features. For face recognition, it suffices to have a constant value for the margin penalty (0.5 for ArcFace). Since we want discriminative features for the frequently and infrequently occurring classes in a cluster, we ensure that the training samples assigned to a cluster $z$ and belonging to the majority class $y$ in that cluster are allowed a smaller margin than those belonging to the minority class. We assign the loss weights from CW as the margin values (i.e. eq. 2). Specifically, for a data sample belonging to class $c$ with cluster id $z$ , we denote its margin as $m_{c}^{z}=(1-\\bar{m}_{c}^{z})$ (see Fig. 3 for an overview of the system). Finally, we define the angular adaptive margin loss for each sample belonging to the tuple $t=(c,z)$ by combining eq.s 3 and 2: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\cal L}_{\\mathrm{Margin}}^{t}=\\sum_{k=1}^{|C|}-p_{k}\\log\\frac{\\exp(s\\;\\cos(\\theta_{k}+m_{c_{k}}^{z}))}{\\sum_{j=1}^{|C|}\\exp(s\\;\\cos(\\theta_{j}+m_{c_{j}}^{z}))}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Gaussian Randomization of the margins: While we estimate the margins for the margin loss by clustering the features from the biased model $B$ , the obtained clusters can capture noisy signals, leading to erronous results. Alluding to our previous example of a dataset of people with blond and non-blond hair with gender bias (Section 1), since most of the females have blond hair, all such females should be given lesser margin values, whereas blond-haired males should be given higher values. But clustering leads to noisy grouping of samples in the dataset, whereby a certain group may include less blond females and more blond males. Therefore, for those female blond samples, comparatively large margin penalty will be assigned, while the blond male samples will get a low margin penalty. Also, the margins being high for sparse classes and low for the frequent ones in a cluster, they may over-learn the former, ignoring the latter. In face recognition, Boutros et al. [71] suggest that in typical margin losses, setting constant margins can limit the generalizability and discriminative power of a model and advise the introduction of stochasticity in the margin values to boost the same. This stochasticity can help us smooth out the effect of the noises in clustering on one hand and make the model more generalized to all groups. To enforce this, we use a randomized version of $\\bar{m}_{c}^{z}$ , called $\\Bar{r}_{c}^{z}$ , where $\\bar{r}_{c}^{z}\\sim\\mathcal{N}(\\bar{m}_{c}^{z},\\sigma)$ . Recall that $\\bar{m}_{c}^{z}$ denotes the proportion of samples belonging to class $c$ and cluster $z$ . $\\mathcal{N}$ is the Gaussian distribution, and $\\sigma$ is the standard deviation (a hyperparameter). This impedes the model from overcorrecting with respect to the rare classes in a group, thus increasing its generalizability while also compensating for the errors in margin values due to the noisy cluster labels. Finally, we obtain the randomized margin $r_{c}^{z}=1-\\bar{r}_{c}^{z}$ for each $c\\in{\\mathcal{C}}$ . We then replace $m_{c}^{z}$ with $r_{c}^{z}$ in eq. 4. Our overall method is summarized in Fig. 3. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments and Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Details. We evaluate our method on multiple benchmarks. Waterbirds [23] is a dataset of birds, labeled as waterbird if the bird is a seabird, and landbird otherwise. A spurious correlation exists between these labels and the background\u2013land or water. The dataset has 4795 training samples. The ColorMNIST dataset (CMNIST) [72] is generated from MNIST [73], where each digit is ", "page_idx": 6}, {"type": "image", "img_path": "HwO1mNluoL/tmp/b67a7e7621133539420b0b051d25b2c4a6ad6846156d3d6d6b8d2cb8b53ff539.jpg", "img_caption": ["Figure 3: Overview of our method. There are 3 steps: a) Bias-amplified training of the model through Cross-Entropy Loss with high weight decay, b) Clustering the biased features $\\hat{f}$ , c) Mitigating the biases by using the resultant clusters to calculate the margins and the corresponding loss, leading to decent performance on the bias-conflicting data points. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Performance comparison of our method (CM). Worst and Average Group Accuracies for Waterbirds & CelebA (Blond Hair), Bias-Conflicting (Bi-Co), Bias-Aligned (Bi-Al) and Average Accuracies for CMNIST-0.9. Highest accuracies are marked in bold, the $2^{n d}$ highest ones are underlined. All scores for our method are averaged over 3 seeds. ", "page_idx": 7}, {"type": "table", "img_path": "HwO1mNluoL/tmp/7f0280efb53b371574e3e15cb443de6b172691030c32bfd745aa71a277d86ede.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "associated with one color in its background most of the time (see Fig. 3). It has 50000 training and 10000 test images. We evaluate our margin loss method on two variants of CMNIST: in CMNIST-0.9, each digit is associated with one color $90\\%$ of the time and other colors only $10\\%$ of the time (moderate bias). In CMNIST-0.995, each digit is associated with a single color $99.5\\%$ of the time (severe bias). The real-world CelebA dataset [64] consists of 202, 599 face images of celebrities along with 40 attributes. We choose Blond Hair as the target attribute, as it is known to suffer from severe gender bias [28, 38, 45]. For evaluating models on CelebA and Waterbirds, we find the accuracy of each group $g=(c,a)$ in the test set [20, 45], and report the worst of all the groups in $\\mathcal{G}$ and their average. For CMNIST, we report the overall bias-aligned (i.e. images of digits with their maximally associated colors) and bias-confilcting (i.e. images of digits with other colors) accuracies, along with the average of all the groups [38]. We also show the results for two more real-life datasets (BAR [20] and UTKFace [74]) in Appendix Table 12. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details. We evaluate our method on multiple pretrained encoders: ImageNetpretrained ResNet-18 [75] and ViT-Base [76] encoders, and CLIP ResNet-50 image encoder [12] (pretrained on other datasets). All our implementations use a 1-hidden layer of $M$ neurons followed by a non-linearity in the adapter, and the classifier is a linear layer. For clustering, we use the KMeans algorithm. We discuss the hyperparameters and their analyses in Appendix subsection A.3. We assume the availability of a small group-balanced (but unannotated) validation set and calculate the overall accuracy over this dataset for model selection during the bias-mitigation phase. ", "page_idx": 7}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our baseline against Contrastive Adapter [50], given that it performs the best among all other methods across different benchmarks (see Table 2), and the ERM model. The results are presented in Table 4, where we compare our method against the ERM method and Co-Ada. For comparison with other competing methods, see Appendix Table 8. We also present the results for the ViT-H 14 encoder in Appendix Table 13. ", "page_idx": 7}, {"type": "table", "img_path": "HwO1mNluoL/tmp/c4d6a586b38056329c79ee3e0fbcf7ecbdfcecd3eb39f8c552c1f88e1838487a.jpg", "table_caption": ["Table 6: Ablations of the proposed method: Here we show the roles of the different components of our model using ResNet-18 as the pretrained backbone. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "For the Waterbirds dataset, our method far outperforms Contrastive Adapter for ResNet-18 and ViT-B (by $12.72\\%$ and $11.21\\%$ respectively), though for CLIP the worst-group accuracy is slightly lesser (by $2.67\\%$ ). In case of CelebA, for ResNet-18 and ViT-Base, the margin loss outperforms Contrastive Adapter (by $3.24\\%$ and $1.41\\%$ ). For the CLIP encoder, we observe the two methods to perform similarly. The CMNIST-0.9 dataset is affected by a moderate degree of the background color bias, whereas for CMNIST-0.995 the bias strength is severe. In case of both datasets, our method outperforms Contrastive Adapter for all backbones by a large margin, especially for CMNIST-0.9 (see results for CMNIST-0.995 in Table 5). Compared to Contrastive Adapter, our method has another advantage: it is time-efficient, as discussed in the Appendix Table 17. Our method is effective even when finetuning to the encoder weights is possible (see Appendix subsection A.7). ", "page_idx": 8}, {"type": "text", "text": "5 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we discuss the important components of our approach and the margin loss strategy. We first show what happens when the margin penalty is constant as the ArcFace loss [6] itself. Then, we show the role played by the Gaussian randomization of the group-based margins (read subsection 3.5 and Appendix subsection ??). Finally, we show what happens when we estimate the different bias-groups in the training data by clustering the blackbox features instead of the bias-amplified adapter layer. All evaluations are performed on Waterbirds, CelebA (Blond Hair classification) and CMNIST-0.9 with ResNet-18 as the backbone. The results are shown in Table 6. ", "page_idx": 8}, {"type": "text", "text": "Constant Margin. We present the results obtained by keeping the margin value constant at 0.5 as suggested by ArcFace [6] (no Gaussian randomization is applied here). This study en", "page_idx": 8}, {"type": "table", "img_path": "HwO1mNluoL/tmp/42e3fbf0bb2d38316a6eeefa30762f8e3ea1864b1fee69620163da82a46b5703.jpg", "table_caption": ["Table 5: Performance comparison of our method for CMNIST-0.995. We report Bias-Conflicting (Bi-Co), Bias-Aligned (Bi-Al) and Average Accuracies. Highest accuracies are marked in bold, the $2^{n d}$ highest ones are underlined. All scores for our method are averaged over 3 seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ables us to judge the utility of the adaptive nature of the margins. While for Waterbirds and Blond Hair, the scores are better than those of ERM, there\u2019s a sharp drop in both worst and average group accuracies compared to our method. The trends are similar in CMNIST-0.9 as well. Hence, we conclude that while margin losses improve model performance, a constant margin is not enough. ", "page_idx": 8}, {"type": "text", "text": "Without Randomization. We remove the Gaussian randomization of the margins, but here the margins are adaptive (as per eq. 2). For Waterbirds, we see that both the worst and average group accuracies reduce compared to the final scores. Worst group accuracy drops for CelebA as well, though the average group score remains similar. In CMNIST, the bias-confilcting accuracies slightly improve at the cost of a small drop in bias-aligned performance. Thus, overall we find that randomizing the margins for each sample is helpful. ", "page_idx": 8}, {"type": "text", "text": "Clustering from $f^{\\mathbf{locked}}$ . Our model identifies biases by clustering the adapter features $\\hat{f}$ . However, the pretrained features $f_{\\mathrm{locked}}$ can be useful as well, if the pretrained model and the downstream dataset share similar biases. Hence, we cluster the pretrained features $f^{\\mathrm{locked}}$ instead of $\\hat{f}$ to obtain the margin penalties. The rest of the training pipeline remains the same, as described in subsection 3.3. For all 3 datasets, we see a reduction in the worst and average group accuracies, however, the scores are still close to that of the proposed approach. Since clustering the pretrained features does not require an extra stage of ERM training, therefore in presence of time-constraints, $f^{\\mathrm{locked}}$ can be used as a proxy for $\\hat{f}$ , and subsequently, relatively decent model predictions can be obtained. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our method is specifically targeted towards cases where the bias in the downstream dataset is already encoded in the pretrained model. We believe that detecting if this assumption holds apriori is highly challenging in the absence of the bias labels. We put forward a few suggestions to identify the scenarios that fti this assumption: a) Obtain bias annotations for the small validation set. If the worst group accuracy of the validation set does not reduce substantially with increasing weight decay, it indicates that the features have stronger signals of the target class than that of the bias, making it harder to capture the bias. b) If the bias annotations of the validation set cannot be obtained due to privacy concerns, the overall validation accuracy can indicate strength of the bias. For example, the difference between the validation and training accuracy is $25\\%$ for an ERM trained method for the Waterbirds dataset on the ResNet-18 backbone. The higher this difference, the more the indication that the model is overftiting to more and more samples. Such overftiting can indicate that the model is learning the bias in the dataset, thus not generalizing on the bias-conflicting samples. While we have mentioned the situation when the pretrained model is aware of the target attribute in Section 3, another potential use case might be when the pre-training data distribution may be different from that of the downstream dataset. We prescribe finetuning the pretrained models to amplify and mitigate the biases in such a case. A more practical approach would be to choose another model, as also suggested by Zhang et al. [50]. ", "page_idx": 9}, {"type": "text", "text": "One limitation of our method is that it currently relies on the existence of biased groups in the training set (i.e., the bias labels are expected to be categorical) as it uses clustering to identify the biases. Furthermore, since our approach (along with most competing methods) relies on amplifying the bias first and then mitigating the same, it involves a risk wherein if the mitigation module fails, the bias in the system may be exacerbated. With these limitations in mind, we hope this work initiates a much required discussion in this direction leading to more sophisticated and targeted solutions in the near future. ", "page_idx": 9}, {"type": "text", "text": "Scalability of the proposed approach to large datasets and models. Since our method involves clustering the features, if the dataset is large-scale, one can randomly sample a small percentage of it to do the clustering. For example, on clustering only $10\\%$ randomly sampled images from the CelebA dataset, we find that the worst group accuracy is $81.11\\%$ , whereas the average group accuracy is $85.6\\%$ on the ResNet-18 backbone. The scores become $80.55\\%$ and $85.6\\%$ respectively when the clustering is performed only on $1\\%$ of the images (with full clustering, the scores are $81.61\\%$ and $86.04\\%$ respectively). On the other hand, for very large models, the requirement is to be able to load the model into a GPU memory. Our method adds negligible overhead owing to the addition of only an adapter and a classifier layer. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we explored the effect of using pretrained but frozen feature extractors on downstream applications with biased datasets and found the need for specific bias mitigation strategies in cases where the biases in the downstream dataset align with the pretrained encoders. Such mitigation is challenging as the encoder is blackbox. While we found many of the existing works to be inadequate, we proposed a simple method where we first amplified the biases present in the downstream dataset and then employed a clustering-based margin loss to mitigate the same. Our experiments showed this method to be an efficient and effective technique across multiple benchmarks. Lastly, it is our hope that this work will initiate a much-required discussion among the scientific community on encouraging similar works in this highly practical problem setting. We share further studies, detailed hyperparameter analyses and experiments of the proposed method in the appendix. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is funded by the Qualcomm Innovation Fellowship. Abhipsa Basu is supported by the PMRF Fellowhsip. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jialu Wang, Yang Liu, and Xin Eric Wang. Are gender-neutral queries really gender-neutral? mitigating gender bias in image search. arXiv preprint arXiv:2109.05433, 2021.   \n[2] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. arXiv preprint arXiv:1707.09457, 2017. [3] Dana\u00eb Metaxa, Michelle A Gan, Su Goh, Jeff Hancock, and James A Landay. An image of society: Gender and racial representation and impact in image search results for occupations. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1):1\u201323, 2021.   \n[4] Erik Hjelm\u00e5s and Boon Kee Low. Face detection: A survey. Computer vision and image understanding, 83(3):236\u2013274, 2001.   \n[5] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5265\u20135274, 2018. [6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019. [7] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[8] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don\u2019t just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4971\u20134980, 2018. [9] Schrasing Tong and Lalana Kagal. Investigating bias in image classification using model explanations. arXiv preprint arXiv:2012.05463, 2020.   \n[10] Vikram V Ramaswamy, Sunnie SY Kim, and Olga Russakovsky. Fair attribute classification through latent space de-biasing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9301\u20139310, 2021.   \n[11] Jean-R\u00e9my Conti, Nathan Noiry, Stephan Clemencon, Vincent Despiegel, and St\u00e9phane Gentric. Mitigating gender bias in face recognition using the von mises-fisher mixture model. In International Conference on Machine Learning, pages 4344\u20134369. PMLR, 2022.   \n[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021.   \n[13] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15180\u201315190, 2023.   \n[14] Angelina Wang and Olga Russakovsky. Overwriting pretrained bias with finetuning data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3957\u20133968, 2023. ", "page_idx": 10}, {"type": "text", "text": "[15] Google. Google CloudVision API. https://cloud.google.com/vision/, 2004. ", "page_idx": 11}, {"type": "text", "text": "[16] Amazon Web services. Amazon Rekognition API. https://aws.amazon.com/ rekognition/.   \n[17] Microsoft. Azure AI Vision. https://azure.microsoft.com/en-us/products/ ai-services/ai-vision.   \n[18] IBM. IBM Watson Visual Recognition. https://developer.ibm.com/components/ watson-visual-recognition/.   \n[19] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pages 6781\u20136792. PMLR, 2021.   \n[20] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020.   \n[21] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training deep neural networks with biased data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9012\u20139020, 2019.   \n[22] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9572\u20139581, 2019.   \n[23] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \n[24] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[25] Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshuffilng data for improved generalization in visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1417\u20131427, 2021.   \n[26] Enzo Tartaglione, Carlo Alberto Barbano, and Marco Grangetto. End: Entangling and disentangling deep representations for bias correction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13508\u201313517, 2021.   \n[27] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8919\u20138928, 2020.   \n[28] Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, and Kui Ren. Fairness-aware adversarial perturbation towards bias mitigation for deployed deep models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10379\u201310388, June 2022.   \n[29] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Sample selection for fair and robust training. Advances in Neural Information Processing Systems, 34:815\u2013827, 2021.   \n[30] Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, and Jiebo Luo. Learning bias-invariant representation by cross-sample mutual information minimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15002\u201315012, 2021.   \n[31] Sangwon Jung, Sanghyuk Chun, and Taesup Moon. Learning fair classifiers with partially annotated group labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10348\u201310357, 2022.   \n[32] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. Advances in neural information processing systems, 32, 2019.   \n[33] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. arXiv preprint arXiv:1909.03683, 2019.   \n[34] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.   \n[35] Haohan Wang, Zexue He, Zachary C Lipton, and Eric P Xing. Learning robust representations by projecting superficial statistics out. arXiv preprint arXiv:1903.06256, 2019.   \n[36] Elliot Creager, J\u00f6rn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, pages 2189\u20132200. PMLR, 2021.   \n[37] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debiased representation via disentangled feature augmentation. Advances in Neural Information Processing Systems, 34:25123\u201325133, 2021.   \n[38] Zhiheng Li, Anthony Hoogs, and Chenliang Xu. Discover and Mitigate Unknown Biases with Debiasing Alternate Networks. In The European Conference on Computer Vision (ECCV), 2022.   \n[39] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. Advances in neural information processing systems, 33:728\u2013740, 2020.   \n[40] Sumyeong Ahn, Seongyoon Kim, and Se-Young Yun. Mitigating dataset bias by using persample gradient. In The Eleventh International Conference on Learning Representations, 2023.   \n[41] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves crossdomain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 124\u2013140. Springer, 2020.   \n[42] Luke Darlow, Stanis\u0142aw Jastrze\u02dbbski, and Amos Storkey. Latent adversarial debiasing: Mitigating collider bias in deep neural networks. arXiv preprint arXiv:2011.11486, 2020.   \n[43] Youngkyu Hong and Eunho Yang. Unbiased classification through bias-contrastive and biasbalanced learning. Advances in Neural Information Processing Systems, 34:26449\u201326461, 2021.   \n[44] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\u00e9. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural Information Processing Systems, 33:19339\u201319352, 2020.   \n[45] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Unsupervised learning of debiased representations with pseudo-attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16742\u201316751, 2022.   \n[46] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems, 31, 2018.   \n[47] Sheng Liu, Xu Zhang, Nitesh Sekhar, Yue Wu, Prateek Singhal, and Carlos Fernandez-Granda. Avoiding spurious correlations via logit correction, 2023.   \n[48] Sheng Liu, Xu Zhang, Nitesh Sekhar, Yue Wu, Prateek Singhal, and Carlos Fernandez-Granda. Avoiding spurious correlations via logit correction. arXiv preprint arXiv:2212.01433, 2022.   \n[49] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher R\u00e9. Correct-n-contrast: A contrastive approach for improving robustness to spurious correlations. arXiv preprint arXiv:2203.01517, 2022.   \n[50] Michael Zhang and Christopher R\u00e9. Contrastive adapters for foundation model group robustness. Advances in Neural Information Processing Systems, 35:21682\u201321697, 2022.   \n[51] Maan Qraitem, Kate Saenko, and Bryan A Plummer. Bias mimicking: A simple sampling approach for bias mitigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20311\u201320320, 2023.   \n[52] Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. Learning debiased classifier with biased committee. arXiv preprint arXiv:2206.10843, 2022.   \n[53] Myeongho Jeon, Daekyung Kim, Woochul Lee, Myungjoo Kang, and Joonseok Lee. A conservative approach for unbiased learning on unknown biases. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16752\u201316760, 2022.   \n[54] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.   \n[55] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar. Towards last-layer retraining for group robustness with fewer annotations. arXiv preprint arXiv:2309.08534, 2023.   \n[56] Priya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. Fairness indicators for systematic assessments of visual feature extractors. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 70\u201388, 2022.   \n[57] Tejas Srinivasan and Yonatan Bisk. Worst of both worlds: Biases compound in pre-trained vision-and-language models. arXiv preprint arXiv:2104.08666, 2021.   \n[58] Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.   \n[59] Ryan Steed, Swetasudha Panda, Ari Kobren, and Michael Wick. Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3524\u20133542, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[60] Hadi Salman, Saachi Jain, Andrew Ilyas, Logan Engstrom, Eric Wong, and Aleksander Madry. When does bias transfer in transfer learning? arXiv preprint arXiv:2207.02842, 2022.   \n[61] Raghav Mehta, V\u00edtor Albiero, Li Chen, Ivan Evtimov, Tamar Glaser, Zhiheng Li, and Tal Hassner. You only need a good embeddings extractor to fix spurious correlations. arXiv preprint arXiv:2212.06254, 2022.   \n[62] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll\u00e1r, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 804\u2013814, 2022.   \n[63] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[64] CelebA dataset. http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.   \n[65] Sriram Yenamandra, Pratik Ramesh, Viraj Prabhu, and Judy Hoffman. Facts: First amplify correlations and then slice to discover bias. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4794\u20134804, 2023.   \n[66] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212\u2013220, 2017.   \n[67] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. Advances in neural information processing systems, 32, 2019.   \n[68] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot learning with adaptive margin loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12576\u201312584, 2020.   \n[69] Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Feng Ji, Ji Zhang, and Alberto Del Bimbo. Adavqa: Overcoming language priors with adapted margin cosine loss. arXiv preprint arXiv:2105.01993, 2021.   \n[70] Abhipsa Basu, Sravanti Addepalli, and R Venkatesh Babu. Rmlvqa: A margin loss approach for visual question answering with language biases. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11671\u201311680, 2023.   \n[71] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1578\u20131587, 2022.   \n[72] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Machine Learning (ICML), 2020.   \n[73] MNIST dataset. http://yann.lecun.com/exdb/mnist/.   \n[74] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5810\u20135818, 2017.   \n[75] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[76] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Biases in Pretrained Features - Further Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the main paper, we looked at two scenarios on how the absence or presence of biases in the pretrained feature encoder affects the downstream model\u2019s performance on finetuning data. However, for the analysed cases, we did not look into the specific relation between the pretraining data and the downstream data. In the appendix, we perform a controlled and detailed analysis to answer 4 questions in this regard: ", "page_idx": 14}, {"type": "text", "text": "Q1: What happens if the pretraining data is heavily biased and so is the downstream dataset? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To answer this question, we pretrain a ResNet-18 model with the CelebA dataset to make it heavily gender-biased by performing gender prediction itself as the pretraining task [14]. During the downstream step, we choose to predict the Blond Hair attribute of the CelebA dataset, which is known to be heavily correlated with gender. We class-balance the finetuning dataset to have equal number of blonds and non-blonds, so as to avoid the downstream predictions from collapsing into a single class. Finally, we also sample from the class-balanced downstream data to ensure that the total number of data points in the downstream data is approximately $1/4^{\\mathrm{th}}$ of the pretraining data. ", "page_idx": 14}, {"type": "text", "text": "Q2: What happens if the pretraining dataset is heavily biased but the downstream dataset is less biased? ", "page_idx": 14}, {"type": "text", "text": "Table 7: Dependence of downstream model performance on pretraining data. When the pretraining model has a high amount of bias, the downstream data performance is heavily affected if the latter itself is biased. On the other hand, the performance is more stable if i) the pretraining data has high bias and the finetuning data has low bias, ii) the pretraining data has low bias and the finetuning data has high bias. The downstream predictions get biased even if the finetuning data has biases which are not common with those in the pretraining data, but the performance drops. ", "page_idx": 15}, {"type": "table", "img_path": "HwO1mNluoL/tmp/f409dcbfd3eb6d2bba701d819bf9cf59625e258fc008cd448a15a400a176119e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For pretraining, we use gender prediction again as mentioned in the above case, but for the downstream dataset, we group-balance the CelebA dataset in terms of Blond Hair and gender, keeping the total number of downstream samples same as described above. ", "page_idx": 15}, {"type": "text", "text": "Q3: What happens if the pretraining data is less biased but the downstream dataset is heavily biased? ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the downstream dataset, we choose the same one as mentioned in case of Q1. For the pretraining task, instead of predicting gender, we predict the \u2018Smiling\u2019 attribute, as we find it to be much less biased in terms of gender. ", "page_idx": 15}, {"type": "text", "text": "Q4: What happens if the pretraining data is highly biased and so is the downstream dataset, but the two biases are unrelated? ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To simulate this scenario, we pretrain the ResNet-18 encoder by predicting the gender attribute again in the CelebA dataset as mentioned in the above cases, and for the downstream task, we choose a class-balanced version of Waterbirds, which has a completely different bias. ", "page_idx": 15}, {"type": "text", "text": "Findings. Note that Q1, Q2 and Q3 are related. From our experiments, we observe the following. When both the pretrained encoder and the downstream dataset are heavily biased with a common attribute (Q1), the worst group performance on the downstream dataset is the least of all the 3 cases. On the other hand, for Q2 (i.e. when the pretrained model is heavily biased but the downstream dataset is not), the worst group performance improves considerably. This corroborates with our findings in the main paper (Sec 3) that group-balancing the downstream task can improve model performance. The best performance is seen for Q3, where the pretraining model is less biased, but the downstream dataset is still highly biased \u2013 here we find that the worst group performance is infact higher than the previous case (Q2). This shows that the lesser the bias in the pretraining data, the more unbiased downstream application is going to be. ", "page_idx": 15}, {"type": "text", "text": "When the bias in the pretrained model and the downstream task are unrelated, we find that the model is indeed affected by the bias in the downstream data, with a drastic drop in overall performance. This indicates that if the pretraining and downstream data distributions do not match, one should choose a different feature encoder to achieve better performance on the downstream task. All the above findings are summarized in Table 7. Note that the overall performance of the downstream task is considerably lower in comparison to using an Imagenet pretrained encoder for all above cases. This is likely because the pretraining task involves a binary prediction of an attribute that is different from the downstream target attribute \u2013 leading to overfitting of the pretrained features to the former. ", "page_idx": 15}, {"type": "text", "text": "A.2 Comparison of the Margin Loss-based method with other Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main paper, we discuss the performance of our proposed baseline and compare it with CoAda [50] across various benchmarks and pretrained encoders. In this section, we describe the performance of the other methods in Table 8. We find that for the ResNet-18 encoder, among all methods, LfF [20] performs well on Waterbirds and both the versions of CMNIST, but the worst group accuracies for the same method is poor for CelebA. On the other hand, while BPA has high worst group accuracy for CelebA, its performance deteriorates for Waterbirds and both the CMNIST versions. This inconsistency is present for other methods as well, for every encoder. ", "page_idx": 15}, {"type": "table", "img_path": "HwO1mNluoL/tmp/00d2798d20d3b06bc8615bef6bc6e80914ad6cd8e783a4e2d21a7f394250be9a.jpg", "table_caption": ["Table 8: Performance comparison of our method with all other methods. Worst and Average Group Accuracies for Waterbirds & CelebA (Blond Hair), Bias-Conflicting (Bi-Co), Bias-Aligned (Bi-Al) and Average Accuracies for CMNIST-0.9 and CMNIST-0.995. Highest accuracies are marked in bold, while the $2^{n d}$ highest ones are underlined. All scores with respect to the proposed approach are averaged over 3 seeds. Co-Ada: Contrastive Adapter. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Hyperparameter Details and Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As mentioned in the main paper, for model selection, we use the training accuracy in the biasamplification stage, whereas for the mitigation stage, we use the overall validation accuracy. Hyperparameter tuning happens via the validation accuracy of the model checkpoint saved during the mitigation stage. The bias-amplification stage has the following hyperparameters: LR (learning rate), BS (batch size), $\\lambda$ (weight decay) and number of epochs. In the clustering stage, the number of clusters $K$ is a hyperparameter. Finally, in the mitigation stage, we have two hyperparameters specific to the margin loss: the scaling parameter $s$ (i.e. the radius of the hypersphere on which the features are projected) and the standard deviation for the Gaussian randomization, $\\sigma$ . We next define the range of each hyperparamter on which we evaluate our model and the other methods. For learning rate LR, we explore in the range $0.0001,0.0005,\\cdots\\,,0.05$ in step sizes of 5 and 2 respectively. Similarly, for batch size BS we evaluate on the range $\\{64,128,256,512\\}$ . We explore higher values of weight decay $\\lambda\\in\\{1,0.1,0.05,0.01\\}$ for the bias-amplication stage and for the mitigation stage, we search $\\lambda$ in the range $10^{-6},\\cdot\\cdot\\cdot\\,,10^{-2}$ in step sizes of 10 and also consider $\\lambda=0$ . Number of epochs for training is kept in the range $\\lbrace50,100\\rbrace$ . For the clustering stage, we choose $K\\,\\in\\,\\{2,4,6,8\\}$ for Waterbirds and CelebA, and explore $K=10,20,\\cdot\\cdot\\cdot,60$ with a step size of 10 for the CMNIST variants. In the mitigation stage, we select $s\\in\\{4,8,12,16\\}$ and $\\sigma\\in\\{0,0.05,0.1,0.15,0.2\\}$ . We show the selected hyperparameters in the bias-amplification stage in Tables 9 and 10, and for the mitigation procedure, the chosen hyperparameters are present in Table 11. For number of neurons $M$ in the MLP layer, we fix the value to be 128. For CelebA experiments, we use SGD as the optimizer, whereas for Waterbirds, and both the variants of CMNIST, we use Adam. ", "page_idx": 16}, {"type": "text", "text": "Finally, we show the changes in scores by varying the number of clusters $K$ , the standard deviation $\\sigma$ for the Gaussian randomization of margins and the hypersphere radius $s$ for CelebA and Waterbirds (with ResNet-18 as the feature encoder) in Fig. 4. We find that performance remains consistent for $K\\geq4$ for both datasets. Similarly, similar scores are seen with $\\sigma>0.1$ . The best values for $s$ are found to be 8 and 12. We also vary the weight decay $\\lambda$ in the bias amplification stage and show its effect on the final model performance. While for CelebA, the effect of changing $\\lambda$ is minimal, Waterbirds can be seen to be more sensitive towards higher values. ", "page_idx": 16}, {"type": "text", "text": "A.4 Performance on Other Datasets and Architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we describe two real-world datasets: UTKFace [74] and BAR [20] and compare our model against the ERM model and the Contrastive Adapter [50]. ", "page_idx": 16}, {"type": "image", "img_path": "HwO1mNluoL/tmp/b9e7dc5f86f718967b3270cd0963fb37bacc98fd5ce0aacb23dae3fd1ebafc48.jpg", "img_caption": ["Figure 4: Hyperparameter analysis. We vary the number of clusters $K$ , the $\\sigma$ for the Gaussian randomization of margins and the hypersphere radius $s$ and show model performance for Waterbirds and CelebA. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "HwO1mNluoL/tmp/5c35fa6ee94cdf4da5f7a85f857947995f2e4c8a260e46a9c8a13df45de4f999.jpg", "img_caption": ["Figure 5: Effect of Weight Decay on Downstream Model Performance. We vary the weight decay $\\lambda$ and show the final model performance for Waterbirds and CelebA. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4.1 UTKFace ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the main paper, we evaluate on Waterbirds, CelebA (for Blond Hair classification) and CMNIST0.9, CMNIST-0.995. Here, we evaluate on UTKFace [74], which is a dataset of faces, with gender, race and age annotations. Specifically, we predict race, with gender as the bias attribute. The dataset is stored in 3 parts with 10137, 10719 and 3252 images respectively; we use the first part as the training set, second part as test, and third part as the validation set. Like all other experiments, we manipulate the validation set to contain group-balanced data. There are 5 races - White, Black, Asian, Indian and Other, and two perceived genders - females and males. Out of the 10 resultant groups, we report the worst group\u2019s accuracy as well as the average group score. We observe that our method far outperforms the ERM scores (as well as the competing Contrastive Adapter) for the ResNet-18 image encoder, thus further demonstrating the effectiveness of our method. The results are summarized in Table 12. ", "page_idx": 17}, {"type": "text", "text": "A.4.2 BAR ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "BAR (Biased Action Recognition) [20] is an image dataset with actions as the class labels and places as the spurious attribute. Note that the test set only consists of bias-conflicting samples, hence, we ", "page_idx": 17}, {"type": "table", "img_path": "HwO1mNluoL/tmp/50ebb8fcbd675d4a9342f4b95897d01bca37674dba648becd0db7cb66251815b.jpg", "table_caption": ["Table 9: Hyperparameters for the Bias-amplification stage for Waterbirds and CelebA for all the pretrained models shown in the main paper. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "HwO1mNluoL/tmp/ff28dc2cf51b538648eaf4411954f9296f8ca44d5d63cbded7f22536f608d058.jpg", "table_caption": ["Table 10: Hyperparameters for the Bias-amplification stage for CMNIST-0.9 and CMNIST-0.995 for all the pretrained models shown in the main paper. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "HwO1mNluoL/tmp/2a2a4dffda2edc56f20bc6c121c5eff95cbb1e3cbf4e2febde93eb785c29a9a7.jpg", "table_caption": ["Table 11: Hyperparameters for Mitigation stage of our approach for Waterbirds, CelebA, CMNIST0.9 and CMNIST-0.995. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "HwO1mNluoL/tmp/d75bd3f740d9034daa5691bb3533503f4c16aced721f417996475b34300a76fb.jpg", "table_caption": ["Table 12: Performance of our method on the UTKFace dataset and the BAR dataset. In all cases, we find our proposed approach outperforming the worst group accuracies of the ERM method and Contrastive Adapter. The underlying pretrained encoder is the ResNet-18 model. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "report its test accuracies following previous work [38]. As with UTKFace, we find that our method outperforms both the ERM method and the Contrastive Adapter (see Table 12). ", "page_idx": 18}, {"type": "text", "text": "A.4.3 ViT-H 14 as the Pretrained Encoder ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In a previous work [61], it is shown that spurious correlations can be mitigated simply by using a stronger pretrained encoder. Further, the authors show that the strength of mitigation depends on the underlying pretraining data. They observe that using a ViT-H 14 encoder pretrained on the SWAG dataset [62] followed by end-to-end ImageNet [63] finetuning can achieve state-of-the-art results on the Waterbirds dataset. Likewise, to test whether the same holds for other datasets too, we use this same encoder and train for CelebA and CMNIST-0.995 using the ERM method. We observe that while indeed the worst group score for Waterbirds is considerably high, for CelebA and CMNIST-0.995, the performance is still weak. This leads us to the important observation that bias mitigation strategies are indeed required for all pretrained encoders, as even though they might perform well for some biased datasets, they may not be free of all biases that can affect model predictions. We compare the ERM scores with that of our margin loss approach and Contrastive Adapter, and find that in Waterbirds, though the ERM scores outperform the mitigation methods marginally, for the other two datasets, the mitigation scores considerably improve. Upon comparing our method with Contrastive Adapter, we find that the former outperforms the latter in CMNIST-0.995 and Waterbirds. The results are shown in Table 13. ", "page_idx": 18}, {"type": "text", "text": "Table 13: Performance on ViT-H 14 (pretrained on SWAG and finetuned on ImageNet). In this table we show the performance of ERM, Contrastive Adapter and our margin loss-based method for Waterbirds, CelebA and CMNIST-0.995. We observe that while the ERM scores are high for Waterbirds, the same does not hold for CelebA and CMNIST-0.995 \u2013 showing that a single feature encoder, however powerful, may not mitigate all biases. Thus we advocate strongly for bias mitigation strategies that can debias models in presence of untrainable feature encoders. ", "page_idx": 19}, {"type": "table", "img_path": "HwO1mNluoL/tmp/cf04932200187b114f953570e888eedfb80979edcd8b41e3769cc6db655d2f97.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "HwO1mNluoL/tmp/a3f9b4e88f60db7b2d87eb2265954693b0c39a7bc9e975e0299e2662ec0a0c13.jpg", "table_caption": ["Table 14: Ablations of our method: Here we further show the roles of the different components of our margin loss-based approach using ResNet-18 as the pretrained backbone. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.5 Ablations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this subsection, we further discuss the importance of the adaptive margin components of our approach and the margin loss strategy. Specifically, we present the results when the randomization is applied to constant margin values. As with Table 6 in the main paper, all evaluations are performed on Waterbirds, CelebA (Blond Hair classification) and CMNIST-0.9 with ResNet-18 as the backbone. The results are shown in Table 14. ", "page_idx": 19}, {"type": "text", "text": "Observations. Here we show the effects of randomizing a constant margin value to further understand the role of the adaptive margins. We perform this experiment on 3 constant margins \u2013 0.0, 1.0 and 0.5. We observe that for all three datasets, both worst and average accuracies decrease drastically. This shows the effectiveness of the adaptive margins, i.e., randomizing the margins do not help unless their underlying values represent the frequency of a training sample\u2019s class label in its assigned cluster. ", "page_idx": 19}, {"type": "text", "text": "We next show further ablations of our method. As described in the main paper, in the biasamplification stage, we set the weight decay $\\lambda$ to a high value (weight decay is the weight used for $L2$ regularization) to ensure amplification of the bias in the model. Instead of the $L2$ loss, we investigate what happens when we instead use $L1$ and $L1+L2$ regularization during this stage. The performances for all these different regularizations appear similar. We also show what happens when we do not use any of these regularizations \u2013 the scores drop drastically for Waterbirds. Moreover, in many of the previous works like JTT [19], misclassifications in the ERM stage are used to identify the biases. When we use the same in our method to calculate the margins (instead of clustering), we find that clustering works better than using misclassifications for bias-identification. Further, we find that using the Generalized Cross-Entropy loss [20] with $L2$ regularization can have similar effects as that of our original approach, though the latter outperforms the former for both the datasets. We also investigate what happens when we replace the ArcFace loss [6] with another margin loss called CosFace [5]. Here we only change the loss formulation, but calculate the margins in the same way as that our method as described in the main paper. We call this version of our method $\\mathrm{CM+CosFace}.$ and find that it is indeed effective, however the original margin loss still outperforms the former on both Waterbirds and CelebA. The results are summarized in Table 15. ", "page_idx": 19}, {"type": "text", "text": "A.6 Comparison with Other Methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.6.1 Last Layer Retraining ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In DFR [54], Kirichenko et al. show that simply retraining the last layer of a biased model can achieve considerably unbiased predictions. However, they utilize a group-balanced reweighting dataset (specifically from the validation set) for this retraining. The difference of this setting with ours is that ours does not assume a feature encoder which is already trained on the biased dataset; rather we assume a blackbox feature encoder which does not interact with the downstream training data. We summarize the results in Table 16. While we see that the performance of Waterbirds and CelebA are comparable to our method, DFR has superior performance in CMNIST-0.9. However, it is to be remembered here that DFR not only uses part of the validation data for training, but also this excess data is group-annotated. This gives additional advantage to DFR, leading to improved scores in some datasets. ", "page_idx": 19}, {"type": "table", "img_path": "HwO1mNluoL/tmp/2bcf15f7f06a5874ed91c21e38383faae35f62ea535f2c1921ceed7b62275e74.jpg", "table_caption": ["Table 15: In this table we summarize the roles of the different regularization techniques as well as misclassifications as an alternate for clustering on the ResNet-18 pretrained encoder. We find that using L1 regularization can be a good (even better) proxy for weight decay. We also show what happens when we use the GCE loss [20] along with high weight decay for the bias amplification stage. Finally, we evaluate the changes in performance when we use an alternate margin loss to mitigate the bias in the network. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "HwO1mNluoL/tmp/49803777a7cdc096fa5e5cba989eef6e3cb5096319f24bc1ac8652ca51ea25ce.jpg", "table_caption": ["Table 16: In this table we show the performance comparison of DFR with the proposed approach for the ResNet-18 backbone, for Waterbirds, CelebA and CMNIST-0.9 "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "A.6.2 Contrastive Adapter ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 17: Difference in training time (in mins) for Contrastive Adapter and the proposed approach for different datasets trained on the ResNet-18 backbone. We find that while our method is trained in considerably less amount of time, Contrastive Adapter requires enormous time, rendering it inefficient for time-constrained settings. ", "page_idx": 20}, {"type": "table", "img_path": "HwO1mNluoL/tmp/3aa9d657047da67aa78679ad1567098a9d451f080167f768076dfb67ede63221.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "While the problem statement described by Zhang et al. [50] is similar to ours, there are subtle differences. E.g., Contrastive Adapter is meant to boost the group robustness of foundation models. It uses the zero-shot capabilities of the existing foundation models (as a result, utilizes their text modalities) to find positive and negative samples with respect to an anchor sample and apply a contrastive loss for bias mitigation purposes. On the other hand, our aim is more generic: to detect and mitigate biases in case the feature encoder is untrainable (only accessible through API calls) and does not rely on any other modality. That is why, when we compare our method against other methods, we freeze the feature encoder for all of them and investigate how these methods perform when no backpropagation is allowed into the frozen backbone. Moreover, since ResNet-18 and ViT-Base do not have the text encoder, we cluster the backbone features as a proxy for the zero-shot accuracies of Contrastive Adapter using the KMeans algorithm (suggested by the authors in their code submission). Since KMeans is an unsupervised algorithm, the cluster label might not always correspond to the correct class index. Simple heuristics are employed to properly label each obtained cluster. In the binary classification setting (WaterBirds and CelebA), the cluster that has a higher number of correct predictions is assigned the label 0 (since WaterBirds has a higher number of landbirds (class 0) and CelebA has a higher number of Non-blonde samples (class 0)). This heuristic fails when the number of classes exceeds 2 (e.g. ColorMNIST), and in such cases, we use consensus to handle the prediction. For each cluster, we count the number of predictions for each class (digit), and the cluster label is the class with the largest number. This associates the most probable label with each cluster in the KMeans model \u2013 i.e., we take the ground truth class (digit) of each sample, find the majority class per cluster and assign that digit as the predicted class. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Impact of number of positives and negatives. Contrastive Adapter requires the creation of a similarity matrix, which consists of positive and negative samples for every sample in the dataset. Typically, a reasonable number of positive and negative samples are necessary for the method (contrastive learning) to function properly. The caveat is that when dealing with multi-class classification problems, gathering a sufficient number of negative samples takes a toll on the system memory, and a server of 128GiB can support only about 128 positives and negatives before overflowing. In addition to this, even if a sufficiently large server is procured to be able to support a higher number of samples, the training time grows significantly, which is a further drawback on the total runtime as well. ", "page_idx": 21}, {"type": "table", "img_path": "HwO1mNluoL/tmp/bf3305bea40f0d5942447ce9cb29e65de82f4d49752c323c6ca60f455ea9d56b.jpg", "table_caption": ["Table 18: Effect of full finetuning. Upon finetuning the encoders fully, our method achieves decent performance compared to the existing methods across two pretrained encoders. The numbers of all competing methods are taken from previous works for ResNet-18 [45] and ResNet-50 [19]. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We show the differences in training times of the proposed method and Contrastive Adapter in Table 17. It is to be noted that we perform all our experiments on a single Nvidia-RTX A5000 GPU. In a time-constrained setting, this renders Contrastive Adapter inefficient, whereas our method, even with considerable less training time, outperforms Contrastive Adapter in most cases. ", "page_idx": 21}, {"type": "text", "text": "A.7 Full finetuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we discuss the effect of full finetuning on the proposed loss, i.e., when the loss is allowed to backpropagate into the pretrained encoder. We compare our method with LfF [20], BPA [45] and JTT [19]. We report the results for LfF and BPA on the ResNet-18 encoder as per the results shown by Seo et al [45], whereas those for JTT (along with LfF) are with respect to the ResNet-50 model [19]. From Table 18, we note that for ResNet-18, our method outperforms BPA and LfF. For ResNet-50, our method surpasses that of JTT by $3\\%$ in the worst group accuracy. In comparison, for our proposed setting, our method outperforms JTT by a huge margin $(\\mathbf{30.45\\%})$ ! This study shows how our method performs satisfactorily in the setting of full finetuning, in addition to decent and consistent performance on the problem setting proposed in the paper. ", "page_idx": 21}, {"type": "text", "text": "A.8 Social Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since our work aims to create unbiased models in a unique problem setting, we believe that it promotes a positive social impact. Pretrained models are not free of biases [14], and our method is an attempt to generate unbiased predictions without having to finetune these pretrained encoders \u2013 we believe that this is a step towards ensuring that future deep learning models are free of the prejudices already present in the society towards marginalized groups, and are fair to all. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All claims are reflected in Sections 3 and 4 and Appendix Section A.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have a thorough discussion and limitations section in Section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No theoretical results have been presented in the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Along with the method discussed in Section 3, the implementation and dataset details are shared in Section 4. The code is available at https://github.com/ abhipsabasu/blackbox_bias_mitigation. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All datasets are open-source. Code is available at https://github.com/ abhipsabasu/blackbox_bias_mitigation. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All implementation details are present in Section 4 and hyperparameter details are in Appendix Section A.3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All experiments have been done over 3 seeds, and Tables 4 and 5 have the corresponding standard deviations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Such details are present in the attached code, and Appendix Section A.6.2. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Broader impacts are discussed in Appendix subsection A.8. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The does not pose any such risk. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All datasets used have been cited. Only open-source methods have been replicated and all of them have been cited. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No new assets/datasets have been proposed in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No crowdsourcing is involved in this work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No crowdsourcing is involved in this work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]