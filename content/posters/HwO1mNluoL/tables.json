[{"figure_path": "HwO1mNluoL/tables/tables_3_1.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table presents the performance of the Waterbirds dataset using two different pretrained encoders (ViT-H and ResNet-18) under three different training data scenarios: the original dataset, the dataset with bias-conflicting samples removed, and the group-balanced dataset.  The results show the worst-group and average-group accuracy for each encoder and dataset variation, highlighting how pretrained models and data manipulation influence downstream performance.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_5_1.jpg", "caption": "Table 3: Comparison of different alternatives. We compare three methods \u2013 loss-weighted CE loss (LW), cluster-weighted CE loss (CW), and cluster-based margin loss (CM). While LW leads to improved scores compared to an ERM-trained model, CW further improves upon it for CelebA and ColorMNIST-0.995. Finally, the proposed CM outperforms the above two methods by a large margin. All results are with respect to an ImageNet-pretrained ResNet-18 model.", "description": "This table compares the performance of three different bias mitigation techniques: loss-weighted cross-entropy (LW), cluster-weighted cross-entropy (CW), and the proposed cluster-based adaptive margin loss (CM).  The results are shown for three benchmark datasets: Waterbirds, CelebA, and ColorMNIST-0.995.  The table highlights that CM significantly outperforms LW and CW, demonstrating the effectiveness of the proposed method in mitigating bias in a black-box setting.", "section": "3.5 Cluster-based Adaptive Margin Loss"}, {"figure_path": "HwO1mNluoL/tables/tables_7_1.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table presents the performance of the Waterbirds dataset using two different pre-trained encoders (ViT-H and ResNet-18) under three different training scenarios. The first uses the original dataset, the second removes bias-conflicting samples, and the third uses a group-balanced dataset. The results are presented in terms of worst-group and average-group accuracy for each scenario and encoder. This helps in evaluating the impact of pre-trained encoders on downstream task performance and the effectiveness of bias mitigation strategies.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_8_1.jpg", "caption": "Table 6: Ablations of the proposed method: Here we show the roles of the different components of our model using ResNet-18 as the pretrained backbone.", "description": "This table presents an ablation study on the proposed bias mitigation method, using a ResNet-18 pretrained backbone.  It systematically removes components of the method to assess their individual contributions to performance.  The components evaluated include: the adaptive margin loss, Gaussian randomization of margins, and the use of clustered features from the original pretrained model versus features from a bias-amplified adapter.  The table shows the worst-group and average-group accuracies on the Waterbirds, CelebA, and CMNIST-0.9 datasets for each ablation.", "section": "3.3 Bias mitigation"}, {"figure_path": "HwO1mNluoL/tables/tables_8_2.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table presents the performance of the Waterbirds dataset using two different pretrained encoders (ViT-H and ResNet-18) under three different training data scenarios: the original data, data with bias-conflicting samples removed, and group-balanced data. The results show the worst-group and average-group accuracy for each encoder and data scenario, highlighting the impact of pretrained encoders and data bias on model performance.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_15_1.jpg", "caption": "Table 7: Dependence of downstream model performance on pretraining data. When the pretraining model has a high amount of bias, the downstream data performance is heavily affected if the latter itself is biased. On the other hand, the performance is more stable if i) the pretraining data has high bias and the finetuning data has low bias, ii) the pretraining data has low bias and the finetuning data has high bias. The downstream predictions get biased even if the finetuning data has biases which are not common with those in the pretraining data, but the performance drops.", "description": "This table shows how the performance of a downstream model is affected by the bias in both the pretraining and finetuning data. The results indicate that when both datasets have high bias, the performance is poor, especially for the worst-performing group. However, if either the pretraining or finetuning data has low bias, the performance is more stable, suggesting that mitigating bias in either the pretraining or finetuning data can improve the overall performance of the downstream model.  The table also shows that even when the finetuning data has a different type of bias than the pretraining data, the performance is still affected, suggesting that the model may not be able to fully correct for any biases introduced in the pretraining phase.", "section": "A.1 Biases in Pretrained Features - Further Analysis"}, {"figure_path": "HwO1mNluoL/tables/tables_16_1.jpg", "caption": "Table 2: Performance of existing methods on the proposed problem setting. We observe that for three different benchmarks, performance of existing methods is either close to that of the ERM model (measured by AERM), or not consistently high. Co-Ada [50] is one exception among compared methods, having the highest worst-group accuracies for all three benchmarks.", "description": "This table compares the performance of several existing bias mitigation methods against the Empirical Risk Minimization (ERM) model and the Contrastive Adapter (Co-Ada) method on three benchmark datasets (Waterbirds, CelebA, and ColorMNIST-0.995).  The results show that most of the existing methods either achieve performance similar to ERM or lack consistent high performance across the datasets.  The Contrastive Adapter method, however, stands out with consistently high worst-group accuracies.", "section": "3.3 Bias mitigation"}, {"figure_path": "HwO1mNluoL/tables/tables_17_1.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table shows the performance of the Waterbirds dataset using two different pretrained encoders (ViT-H and ResNet-18).  It compares performance on three versions of the training data: the original data, the data with bias-conflicting samples removed, and group-balanced data.  The test set remains consistent across all comparisons. The results illustrate how different pretrained encoders and data preprocessing techniques affect the model's ability to avoid biases.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_18_1.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table compares the performance of different pretrained encoders (ViT-H and ResNet-18) on the Waterbirds dataset under various training data conditions.  It shows the impact of removing bias-conflicting samples and group-balancing on the model's ability to generalize and avoid bias amplification.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_18_2.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table presents the performance of a waterbird image classification model using two different pretrained encoders (ViT-H and ResNet-18) and three variations of the training dataset. The first uses the original dataset; the second removes bias-conflicting samples; and the third is group-balanced. The table compares the worst group accuracy, average group accuracy, and overall accuracy for each scenario, demonstrating the effect of pretrained encoders and dataset bias on model performance.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_18_3.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table compares the performance of a model trained on the Waterbirds dataset using two different pretrained encoders (ViT-H and ResNet-18). It shows the results for three different training data variations: the original data, the data with bias-conflicting samples removed, and the group-balanced data.  The table highlights how the choice of pretrained encoder and the preprocessing of the training data affect the model's performance, particularly focusing on the performance for the worst-performing group.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_19_1.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table compares the performance of a model trained on the Waterbirds dataset using two different pre-trained encoders (ViT-H and ResNet-18).  The performance is evaluated across three different versions of the training dataset: the original, a version with all bias-conflicting samples removed, and a group-balanced version. The goal is to demonstrate how pretrained encoders and data manipulation affect downstream task performance.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_19_2.jpg", "caption": "Table 14: Ablations of our method: Here we further show the roles of the different components of our margin loss-based approach using ResNet-18 as the pretrained backbone.", "description": "This table presents ablation studies on the proposed method, specifically investigating the impact of different components on performance. It analyzes the effects of using a constant margin instead of an adaptive margin, removing the Gaussian randomization of margins, and using pretrained features for clustering instead of the bias-amplified adapter features. The results are presented for three benchmark datasets: Waterbirds, CelebA, and ColorMNIST-0.9, comparing the worst-group and average-group accuracies, along with bias-conflicting and bias-aligned accuracies for ColorMNIST-0.9.", "section": "3.4 Our approach"}, {"figure_path": "HwO1mNluoL/tables/tables_20_1.jpg", "caption": "Table 3: Comparison of different alternatives. We compare three methods \u2013 loss-weighted CE loss (LW), cluster-weighted CE loss (CW), and cluster-based margin loss (CM). While LW leads to improved scores compared to an ERM-trained model, CW further improves upon it for CelebA and ColorMNIST-0.995. Finally, the proposed CM outperforms the above two methods by a large margin. All results are with respect to an ImageNet-pretrained ResNet-18 model.", "description": "This table compares three different bias mitigation techniques: loss-weighted cross-entropy loss, cluster-weighted cross-entropy loss, and the proposed cluster-based adaptive margin loss.  The results show the worst-group and average-group accuracies on three benchmark datasets (Waterbirds, CelebA, and ColorMNIST-0.995), demonstrating the effectiveness of the proposed method.", "section": "3.3 Bias mitigation"}, {"figure_path": "HwO1mNluoL/tables/tables_20_2.jpg", "caption": "Table 1: Performance of Waterbirds on different pretrained encoders. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.", "description": "This table presents the performance of a model trained on the Waterbirds dataset using two different pretrained encoders (ViT-H and ResNet-18). The performance is evaluated under three different training data scenarios: the original data, the data with bias-conflicting samples removed, and the group-balanced data. The table shows the worst-group accuracy, average-group accuracy for each encoder and data scenario.", "section": "3.2 Biases in Pretrained Features"}, {"figure_path": "HwO1mNluoL/tables/tables_20_3.jpg", "caption": "Table 2: Performance of existing methods on the proposed problem setting. We observe that for three different benchmarks, performance of existing methods is either close to that of the ERM model (measured by AERM), or not consistently high. Co-Ada [50] is one exception among compared methods, having the highest worst-group accuracies for all three benchmarks.", "description": "This table compares the performance of several existing bias mitigation methods against the proposed method on three benchmark datasets (Waterbirds, CelebA, and ColorMNIST). The results show that most existing methods perform similarly to or worse than a standard ERM (Empirical Risk Minimization) model, except for the Contrastive Adapter method, which achieves significantly higher worst-group accuracies. This highlights the challenges of bias mitigation in the specific setting of using a frozen pretrained feature extractor.", "section": "3.3 Bias mitigation"}, {"figure_path": "HwO1mNluoL/tables/tables_21_1.jpg", "caption": "Table 2: Performance of existing methods on the proposed problem setting. We observe that for three different benchmarks, performance of existing methods is either close to that of the ERM model (measured by AERM), or not consistently high. Co-Ada [50] is one exception among compared methods, having the highest worst-group accuracies for all three benchmarks.", "description": "This table presents the performance comparison of several existing bias mitigation methods on three benchmark datasets (Waterbirds, CelebA, and ColorMNIST-0.995) under a specific setting where a pretrained black-box feature extractor is used.  The results show that the existing methods do not provide significant improvements over the standard Empirical Risk Minimization (ERM) method, except for the Contrastive Adapter (Co-Ada).  This highlights the challenge of bias mitigation when the feature extractor is not finetunable.", "section": "3.3 Bias mitigation"}]