[{"type": "text", "text": "Carrot and Stick: Eliciting Comparison Data and Beyond ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiling Chen Shi Feng Fang-Yi Yu Harvard University Harvard University George Mason University yiling@seas.harvard.edu shifeng@fas.harvard.edu fangyiyu@gmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment [11]. Our design leverages on the strong stochastic transitivity for comparison data [60, 13] to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism. ", "page_idx": 0}, {"type": "text", "text": "We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents\u2019 private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the past two decades, researchers have been embracing the challenge of eliciting private information from individuals when there is no ground truth available to evaluate the quality of elicited contributions, and have made amazing progress. Many mechanisms, collectively called peer prediction [42], have been developed to incentivize individuals to strictly truthfully report their information at a Bayesian Nash equilibrium (BNE), by artful design of payment functions that only depend on reports from individuals. Moreover, in multi-task peer prediction mechanisms, the truthful BNE gives each individual the highest expected payoff among all BNEs (i.e. it\u2019s a strongly truthful BNE). [11, 57, 30] ", "page_idx": 0}, {"type": "text", "text": "However, all prior multi-task peer prediction mechanisms require tasks being ex-ante identical, and hence individuals\u2019 private information is independently and identically distributed (iid) for each task. Multi-task peer prediction leverages this structure of information to succeed at truthful elicitation. But what if such structure of information doesn\u2019t hold for an information elicitation problem? ", "page_idx": 0}, {"type": "text", "text": "One notable application is to elicit pair-wise comparisons of multiple alternatives, such as preferences for consumer products [53], translation [34], peer grading [55], and relevance of language model outputs [9, 10]. Such pair-wise comparison data are crucial for estimating a ranking of the alternatives and for devising reward functions for reinforcement learning. Comparison tasks for different pairs are clearly not ex-ante identical \u2014 answers to the tasks demonstrate a certain degree of transitivity (e.g. if $a$ is preferred to $a^{\\prime}$ and $a^{\\prime}$ is preferred to $a^{\\prime\\prime}$ , then it\u2019s more likely that $a$ is preferred to $a^{\\prime\\prime}$ ), rendering existing peer prediction mechanisms not applicable. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we design a peer prediction mechanism for eliciting comparison data. We model individuals\u2019 private information of pair-wise comparisons as Bayesian strongly stochastically transitive (Bayesian SST), which takes many widely used models (e.g. Thurstone [58], Bradley-TerryLuce [4, 38], and Mallows [39]) as special cases. Our mechanism uses a simple bonus-penalty payment [11] (hence carrot and stick) that takes three reports as inputs and admits a strongly truthful symmetric BNE. The key insight that we develop is a condition of information structure that we call uniform dominance. When uniform dominance is satisfied, the bonus-penalty payment is the only type of payment that induces a strictly truthful BNE. Information of individuals, $i,j$ , and $k$ , satisfies uniform dominance if, conditioned on any realization of agent \ud835\udc56\u2019s information, the probability for $j$ \u2019s information to agree with $i$ \u2019s is higher than the probability for $k$ \u2019s information to agree with $i\\,\\mathbf{\\dot{s}}$ . Bayesian SST allows us to group three pairwise comparisons, $(a,a^{\\prime})$ , $(a^{\\prime\\prime},a^{\\prime})$ and $(a^{\\prime\\prime},a)$ , together such that private information about these pairs satisfies uniform dominance. After identifying uniform dominance as a central structure for incentivizing truthful elicitation, we further generalize the bonus-penalty payment to truthfully elicit private information over social networks that demonstrate homophily (i.e. friends tend to have similar opinions than non-friends) [40], and our mechanism can be integrated with common survey techniques such as snowball sampling [24]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. Our work is a leap forward for designing mechanisms for complex information elicitation settings where ground truth verification is not available. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We are the first to design mechanisms to truthfully elicit pairwise comparison data under Bayesian SST and networked data under Ising models. In our mechanisms, truthful reporting forms a BNE and yields a strictly higher payoff than any symmetric non-permutation equilibrium. \u2022 We identify a key structure of information, uniform dominance, as a lever such that the simple bonus-penalty payment is the unique payment inducing a strictly truthful BNE. This identification may offer a path for developing truthful elicitation mechanisms for other settings in the future. \u2022 We use Grifftihs\u2019 inequality and Weitz\u2019s self-avoiding walk [65] to prove the uniform dominance property in the Ising model. The resulting correlation bounds may be of independent interest. \u2022 We test our mechanisms on real-world data (sushi preference dataset [26, 27] and Last.fm dataset [8]). Even though these datasets do not perfectly satisfy our theoretical assumptions, our mechanisms still provide a stronger incentive for truthful reporting compared to misreporting. ", "page_idx": 1}, {"type": "text", "text": "Related work. Information elicitation has two settings according to whether verification is possible.   \nOur paper focuses on elicitation without verification. ", "page_idx": 1}, {"type": "text", "text": "For information elicitation without verification, Miller et al. [41] introduce the first mechanism for single task signal elicitation that has truth-telling as a strict Bayesian Nash equilibrium but requires full knowledge of the common prior. Bayesian truth serum (BTS) [47] is the first strongly truthful peer prediction mechanism, but requires complicated reports from agents (their private signal and predictions on other\u2019s reports). A series of works [48, 49, 67, 66, 68, 3, 51, 31] relax certain assumptions of BTS but still require complicated reports from agents. Dasgupta and Ghosh [11] introduces the multi-task setting where agents are assigned batch iid tasks and only report their signals. Several works extend this to multiple-choice questions [29, 33, 56, 11], predictions [37], or even continuous value [50], and investigate the limitation and robustness [52, 6, 70, 17, 70]. Another related line of work is co-training and federated learning, which wants to elicit models [32, 36], or samples [64] when multiple iid data or feature of data are available. For more related works, see Faltings [16]. ", "page_idx": 1}, {"type": "text", "text": "One popular line of work considers information elicitation when verification is possible. Spotchecking requires direct verification of the agent\u2019s report [21]. Recent work on comparison data elicitation [19] utilizes spot-checking concepts and focuses on incentivizing effort. Another form of verification involves using additional samples to evaluate how the agent\u2019s reports improve model performance [1, 28]. Additionally, the verification may have a general relation to the agent\u2019s signal, e.g., proper scoring rules [23, 46, 35, 20]. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We discuss our model for eliciting comparison data in this section and defer the extensions to Section 5. Given a collection of items $\\mathcal{A}$ and a set of strategic agents $_{\\cal N}$ . Agents privately observe ", "page_idx": 1}, {"type": "text", "text": "noisy comparisons between pairs of items. Our goal is to design mechanisms to truthfully elicit agents\u2019 private information. We will first introduce the information structure of agents\u2019 private information of pairwise comparisons in Section 2.1 and then define the information elicitation problem in Section 2.2. ", "page_idx": 2}, {"type": "text", "text": "2.1 Bayesian SST Models for Comparison Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce Bayesian Strong Stochastic Transitivity (Bayesian SST) models to capture the structure of agents\u2019 private information for comparison data. ", "page_idx": 2}, {"type": "text", "text": "Given the set of items $\\mathcal{A}$ , the underlying unknown state about the items is $\\theta\\,\\in\\,\\Theta$ . $\\theta$ can be the vector of quality scores for the items (Example 2.2) or a reference ranking (Example 2.4). $\\theta$ is drawn according to a common prior $P_{\\Theta}$ : $\\theta\\sim P_{\\Theta}$ . Any realized $\\theta$ has an associated stochastic comparison function $T_{\\theta}:\\mathcal{R}^{2}\\,\\rightarrow\\,\\{-1,1\\}$ . For comparisons of two items $a$ and $a^{\\prime}$ , $T_{\\theta}(a,a^{\\prime})$ and $T_{\\theta}(a^{\\prime},a)$ stochastically take value 1 or $^{-1}$ , with $\\mathrm{Pr}[T_{\\theta}(a,a^{\\prime})=1]=1-\\mathrm{Pr}[T_{\\theta}(a^{\\prime},a)=-1]$ . For any $\\theta,T_{\\theta}$ is strongly stochastically transitive as defined below. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 ([60, 13]). A stochastic comparison function, $T\\ :\\ \\mathcal{R}^{2}\\ \\rightarrow\\ \\{-1,1\\}$ , is strongly stochastically transitive (SST) if for all $a,a^{\\prime},a^{\\prime\\prime}\\;\\;\\in\\;\\;{\\mathcal{A}}$ with $\\operatorname*{Pr}[T(a,a^{\\prime})\\ =\\ 1]\\ >\\ 1/2$ and $\\operatorname*{Pr}[T(a^{\\prime},a^{\\prime\\prime})=1]>1/2$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ensuremath{\\operatorname*{Pr}}[T(a,a^{\\prime\\prime})=1]>\\ensuremath{\\operatorname*{max}}\\{\\ensuremath{\\operatorname*{Pr}}[T(a,a^{\\prime})=1],\\ensuremath{\\operatorname*{Pr}}[T(a^{\\prime},a^{\\prime\\prime})=1]\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, a comparison function is SST when for any three items $a,a^{\\prime},a^{\\prime\\prime}$ , if $a$ is more favorable than $a^{\\prime}$ and $a^{\\prime}$ is more favorable than $a^{\\prime\\prime}$ , then $a$ is even more favorable than $a^{\\prime\\prime}$ . The concept of SST is a well-established property of comparisons in social science and psychology [18]. ", "page_idx": 2}, {"type": "text", "text": "Each agent $i\\in\\ N$ has the knowledge of $(T_{\\theta})_{\\theta\\in\\Theta}$ and $P_{\\Theta}$ . When asked to compare a pair of items $(a,a^{\\prime})$ , the agent observes an independent draw according to the stochastic comparison function: $S_{i}\\;=\\;T_{\\theta}(a,a^{\\prime})$ , where realization $s_{i}~=~1$ represent item $a$ is preferred over item $a^{\\prime}$ by agent $i$ . We assume items are a priori similar but ex-post distinct so that for all $a,a^{\\prime}\\ \\in\\ \\mathcal{A}$ , $\\vec{\\mathbb{E}}[T_{\\theta}\\overline{{(}}a,a^{\\prime})]=\\vec{\\mathbb{E}}[\\mathbb{E}[T_{\\theta}(a,a^{\\prime})\\mid\\theta]]=\\mathbf{\\bar{(}}$ and $\\mathbb{E}[T_{\\theta}(a,a^{\\prime})\\mid\\theta]\\neq0$ for all $\\theta$ . ", "page_idx": 2}, {"type": "text", "text": "Examples of Bayesian SST models. Bayesian SST models are a general family of models that take many classical parametric ranking models, including Bradley-Terry Luce [4, 38], Thurstone (Case V) [58], and Mallows $\\eta$ -model [39], as special cases. ", "page_idx": 2}, {"type": "text", "text": "Example 2.2 (Bradley-Terry-Luce, Thurstone model, and more [59]). Let $\\theta\\in\\mathbb{R}^{\\mathcal{A}}=\\Theta$ where each coordinate is independently and identically sampled from a fixed non-atomic distribution $\\nu$ on $\\mathbb{R}$ , and each item $a$ have a scalar quality $\\theta_{a}\\in\\mathbb{R}$ . Let $F:\\mathbb{R}\\to[0,1]$ be any strictly increasing function such that $F(t)=1-F(-t)$ for all $t\\in\\mathbb{R}$ . Conditional on a fixed $\\theta$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1]=F\\left(\\theta_{a}-\\theta_{a^{\\prime}}\\right){\\mathrm{~for~all~}}a,a^{\\prime}\\in{\\mathcal{A}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This model recovers the Thurstone model [58] by setting $F(t)=\\Phi(t)$ where $\\Phi$ is the Gaussian CDF, and the Bradley-Terry-Luce model [4] by setting $\\begin{array}{r}{F(t)=\\frac{e^{t}}{1+e^{t}}}\\end{array}$ , the sigmoid function. Moreover, this model also contains any additive random utility model [2] where $T(a,a^{\\prime})=1$ if $\\theta_{a}+Z>\\theta_{a^{\\prime}}+Z^{\\prime}$ with iid noise $Z$ and $Z^{\\prime}$ , because we can set $F$ to be the CDF of the difference of two iid noise. ", "page_idx": 2}, {"type": "text", "text": "Proposition 2.3. For any strictly increasing $F$ and non-atomic $\\nu$ on ${\\mathbb R},$ the parametric model in Example 2.2 is a Bayesian SST model. ", "page_idx": 2}, {"type": "text", "text": "Example 2.4 (Mallows $\\eta$ -model [39]). Let $\\Theta$ be the set of rankings on $\\mathcal{A}$ and $\\eta>0$ be a dispersion parameter. Given a reference ranking $\\theta\\in\\Theta$ , the Mallows $\\eta$ -model generate a ranking $\\phi\\in\\Theta$ with probability $\\mathrm{Pr}(\\phi)\\propto\\exp(-\\eta d(\\theta,\\phi))$ where $d(\\theta,\\phi)=\\big|\\{(a,a^{\\prime})\\in\\mathcal{R}^{2}:\\theta(a)<\\theta(a^{\\prime})$ and $\\phi(a)>\\phi(\\dot{a}^{\\prime})\\}\\vert$ is Kendall\u2019s tau distance, and $\\theta(a)$ is the rank of item $a$ . Therefore, to generate comparisons, we first sample a uniform $\\theta$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1]=\\sum_{\\phi:\\phi(a)>\\phi(a^{\\prime})}\\operatorname*{Pr}(\\phi),{\\mathrm{~for~all~}}a,a^{\\prime}\\in{\\mathcal{A}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proposition 2.5. For any $\\eta\\,>\\,0$ , Mallows $\\eta$ -model in Example 2.4 with uniform distribution on reference ranking is an Bayesian SST model. ", "page_idx": 2}, {"type": "text", "text": "The proofs for propositions 2.3 and 2.5 are closely related to strong stochastic transitivity [54, 7], but are provided in the appendix for completeness. ", "page_idx": 2}, {"type": "text", "text": "2.2 Peer Prediction Mechanism Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To truthfully elicit comparison data from agents, a peer prediction mechanism creates a game between the agents outlined below: First, we choose an assignment $\\mathcal{E}=\\{e_{i}\\,=\\,(a_{u_{i}},a_{\\nu_{i}})\\,:\\,\\bar{i^{\\,}}\\in N\\}$ where agent $i\\in N$ gets a pair of items $e_{i}=\\left(a_{u_{i}},a_{\\nu_{i}}\\right)\\in\\mathcal{A}^{2}$ to compare. Then each agent $i\\in N$ privately observes the realization of the comparison (signal) $s_{i}\\in\\{-1,1\\}$ , which is an independent realization of $T_{\\theta}\\big(a_{u_{i}},a_{\\nu_{i}}\\big)$ , and reports $\\hat{s}_{i}\\in\\{-1,1\\}$ potentially different from her signal. We use $S_{i}=S(a_{u_{i}},a_{\\nu_{i}})$ to denote the random variable of agent $i$ \u2019s signal, where the randomness of $S(\\cdot,\\cdot)$ comes from both $\\theta$ and $T_{\\theta}$ . Let S represent the random vector of all agents\u2019 signals, $\\mathbf{s}=(s_{i})_{i\\in N}$ be all agents\u2019 realized private signals and $\\widehat{\\mathbf{s}}=(\\widehat{s}_{i})_{i\\in{N}}$ be all agents\u2019 reports. Finally, a peer prediction mechanism $(M_{i})_{i\\in N}$ takes all agents\u2019 reports $\\hat{\\bf s}$ and pays agent $i$ with $M_{i}(\\hat{\\mathbf{s}})\\in\\mathbb{R}$ . ", "page_idx": 3}, {"type": "text", "text": "Each agent $i$ \u2019s strategy is a random function from her signal to a report $\\sigma_{i}\\;:\\;s_{i}\\;\\mapsto\\;{\\hat{s}}_{i}$ , and the randomness of their strategies is independent of each other\u2019s and all signals. With slight abuse of notation, we write $\\sigma_{i}(s_{i},\\bar{s_{i}})=\\operatorname*{Pr}[\\hat{S_{i}}=\\hat{s}_{i}\\mid S_{i}=s_{i}]$ as the conditional probability of reporting $\\hat{s}_{i}$ given private signal $s_{i}$ . A strategy profile $\\sigma$ is a collection of all agent\u2019s strategies. All agents are rational and risk-neutral, so they want to maximize their expected payments. Thus, given prior $P_{\\Theta}$ , randomness of $T_{\\theta}$ and a strategy profile $\\sigma$ , agent $i$ wants to maximize her ex-ante payment denoted as $\\mathbb{E}_{\\pmb{\\sigma},\\,\\theta,T_{\\theta}}[M_{i}(\\hat{\\mathbf{S}})]$ where $\\hat{\\mathbf{S}}$ is the random vector of all agents\u2019 report that depends on the signals $\\mathbf{S}$ and strategy profile $\\sigma$ . ", "page_idx": 3}, {"type": "text", "text": "We introduce three families of strategies, truth-telling, permutation, and uninformed strategy proflies, which are central to understanding effective peer prediction mechanisms. ", "page_idx": 3}, {"type": "text", "text": "\u2022 A strategy $\\sigma_{i}$ is truthful (or truth-telling) if it is a deterministic identity map, ${\\sigma_{i}}(s_{i})\\,=\\,s_{i}$ . A strategy profile is truthful if all agents\u2019 strategies are truthful. \u2022 A permutation strategy proflie is where agents simultaneously relabel their signals and then report the relabeled ones. A permutation strategy is indistinguishable from truth-telling unless the peer prediction mechanism has additional knowledge about the prior signal distribution.[33] \u2022 Finally, a strategy is uninformed if it has the same report distribution across all signals, and it is informed otherwise. Common examples include consistently reporting all signals as a constant value, such as 1 or $^{-1}$ , or using a random report regardless of the signal. Uninformed strategies are undesirable as the reports bear no relationship to the private signals. ", "page_idx": 3}, {"type": "text", "text": "A strategy proflie is symmetric if all agents use the same strategy. For example, both truth-telling and permutation strategy profiles are symmetric. ", "page_idx": 3}, {"type": "text", "text": "We now introduce goals for a peer prediction mechanism that favors truth-telling more than other strategies. First, we want the truth-telling (strategy profile) to be a strict Bayesian Nash equilibrium (BNE) so that any agent\u2019s payment would strictly decrease if she unilaterally changes to any nontruthful strategy. Moreover, there may be multiple equilibria, and a desirable mechanism should ensure that truth-telling is better than all other equilibria. In this paper, we aim for symmetrically strongly truthful mechanisms defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.6. A peer prediction mechanism is symmetrically strongly truthful if truth-telling is a BNE, and each agent\u2019s expected payment in truth-telling is no less than the payment in any other symmetric equilibrium with equality for the equilibrium with a permutation strategy profile.1 ", "page_idx": 3}, {"type": "text", "text": "3 Bonus-penalty Payment Mechanism for Comparison Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now propose a bonus-penalty payment mechanism for eliciting comparison data. The mechanism makes use of a bonus-penalty payment function, which can be seen as an agreement payment and introduced by Dasgupta and Ghosh [11] in a different context (see discussion in appendix A). Formally, for any $\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k}\\in\\{-1,1\\}$ , the bonus-penalty payment function is ", "page_idx": 3}, {"type": "equation", "text": "$$\nU^{B P P}(\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k})=\\hat{s}_{i}\\hat{s}_{j}-\\hat{s}_{i}\\hat{s}_{k}=2\\left(\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{j}]-\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{k}]\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which rewards when the first input agrees with the second but punishes when it agrees with the third. ", "page_idx": 3}, {"type": "text", "text": "Mechanism 1 uses the bonus-penalty payment eq. (1) for each agent $i$ by carefully choosing agent $j$ and $k$ such that agent $j$ \u2019s signal is more likely to agree with agent $i$ \u2019s than agent $k$ \u2019s signal is. The crux of finding such pair of agents is to show that if agent $i$ prefers item $a$ over $a^{\\prime}$ , she would expect that others will prefer any third item $a^{\\prime\\prime}$ over $a^{\\prime}$ , and prefer $a$ over $a^{\\prime\\prime}$ . Thus, if agent $j$ has pair $(a^{\\prime\\prime},a^{\\prime})$ and agent $k$ has pair $(a^{\\prime\\prime},a)$ , then agent $j$ \u2019s signal is more likely to take the same value as $i\\,\\mathbf{\\dot{s}}$ than agent $k$ \u2019s signal is. This is the main idea behind the proof of Theorem 3.1, where we establish the symmetrically strongly truthfulness of Mechanism 1. To ensure the existence of such pairs are assigned, we require the assignment $\\varepsilon$ to be admissible where for all $(a,a^{\\prime})\\in\\mathcal{E}$ , there exists $a^{\\prime\\prime}\\in\\mathcal{A}$ so that $(a^{\\prime\\prime},a^{\\prime})$ and $(a^{\\prime\\prime},a)\\in\\mathcal{E}$ . ", "page_idx": 4}, {"type": "text", "text": "Mechanism 1: BPP mechanism for comparison data ", "text_level": 1, "page_idx": 4}, {"type": "table", "img_path": "ofjTu2ktxO/tmp/fa74a1872d57dd2f9f0a5ccb789ee1c2d2c5f2e0dca05a3a959bb52a0e8c720e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Given a collection of items $\\mathcal{A}$ and a set of agents $_{\\cal N}$ with $|{\\mathcal{A}}|,|N|\\;\\geq\\;3,$ , for any admissible assignment matrix $\\varepsilon$ and Bayesian SST model with $(T_{\\theta})_{\\theta\\in\\Theta}$ and $P_{\\Theta}$ , the BPP mechanism for comparison (Mechanism $^{\\,I}$ ) is symmetrically strongly truthful. ", "page_idx": 4}, {"type": "text", "text": "We defer the proof of theorem 3.1 to section 4. The admissible condition imposes little overhead on downstream learning problems, including rank recovery [25] and identification of the top $k$ items [15]. Specifically, the size of assignment $\\varepsilon$ is the number of comparisons and corresponds to the sample complexity for these learning problems. If a learning algorithm requires a set of pairs to compare $\\mathcal{E}^{M L}$ , we c\ud835\udc40a\ud835\udc3fn construct an admissible superset $\\varepsilon$ that introduces a constant factor overhead and can recover $\\mathcal{E}^{M L}\\subseteq\\mathcal{E}$ .2 ", "page_idx": 4}, {"type": "text", "text": "We remark that the bonus-penalty payment function eq. (2) can be seen as a boolean function for transitivity [45]; see remark 3.2 for a formal statement. Hence, theorem 3.1 implies that agents\u2019 manipulations can only decrease the probability of transitivity among their reports. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. Note that a deterministic comparison function $t:\\mathcal{A}\\!\\times\\!\\mathcal{A}\\!\\rightarrow\\{-1,1\\}$ satisfies transitivity on three items $a,a^{\\prime},a^{\\prime\\prime}\\ \\in\\ \\mathcal{R}$ if and only if $t(a,a^{\\prime}),t(a^{\\prime},a^{\\prime\\prime}),t(a^{\\prime\\prime},a)$ are not all equal, that is $N A E(t(a,a^{\\prime}),t(a^{\\prime},a^{\\prime\\prime}),t(a^{\\prime\\prime},a))=1$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\nN A E(w_{1},w_{2},w_{3})=\\frac{3}{4}-\\frac{1}{4}w_{1}w_{2}-\\frac{1}{4}w_{1}w_{3}-\\frac{1}{4}w_{2}w_{3}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The agent\u2019s random noisy comparisons may or may not satisfy transitivity. The probability of transitivity is the probability that they do. ", "page_idx": 4}, {"type": "text", "text": "We can show that the bonus-penalty payment in eq. (2) is equivalent to the above transitivity test when agents are truth-telling. Formally, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{{\\displaystyle N A E(S(a,a^{\\prime}),S(a^{\\prime},a^{\\prime\\prime}),S(a^{\\prime\\prime},a))}}&{{}}&{{}}\\\\ {{\\displaystyle=\\frac{3}{4}-\\frac{1}{4}\\left(S(a,a^{\\prime})S(a^{\\prime},a^{\\prime\\prime})+S(a,a^{\\prime})S(a^{\\prime\\prime},a)+S(a^{\\prime},a^{\\prime\\prime})S(a^{\\prime\\prime},a)\\right)}}&{{}}&{{}}\\\\ {{\\displaystyle=\\frac{1}{4}\\left(S(a,a^{\\prime})S(a^{\\prime\\prime},a^{\\prime})-S(a,a^{\\prime})S(a^{\\prime\\prime},a)\\right)+\\frac{3}{4}+\\frac{1}{4}S(a^{\\prime\\prime},a^{\\prime})S(a^{\\prime\\prime},a)}}&{{}}&{{\\displaystyle(S(a^{\\prime},a^{\\prime\\prime})=-S(a^{\\prime\\prime},a^{\\prime}))}}\\\\ {{\\displaystyle=\\frac{1}{4}\\left(s_{i}s_{j}-s_{i}s_{k}\\right)+\\frac{3}{4}+\\frac{1}{4}s_{j}s_{k}=\\frac{1}{4}M_{i}({\\bf s})+\\frac{3}{4}+\\frac{1}{4}s_{j}s_{k}}}&{{}}&{{\\mathrm{(truth-telling)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, a $\\begin{array}{r}{\\underset{s_{i}}{\\mathrm{g}\\,\\operatorname*{max}}_{\\hat{s}_{i}}\\mathbb{E}[N A E(\\hat{s}_{i},-S_{j},S_{k})|S_{i}=s_{i}]=\\arg\\operatorname*{max}_{\\hat{s}_{i}}\\mathbb{E}\\left[\\frac{1}{4}M_{i}(\\hat{s}_{i},S_{j},S_{k})+\\frac{3}{4}+\\frac{1}{4}S_{j}S_{k}|S_{i}=s_{i}\\right]=}\\end{array}$ arg $\\mathrm{max}_{\\hat{s_{i}}}\\,\\mathbb{E}[M_{i}(\\hat{s}_{i},\\dot{S}_{j},S_{k})|S_{i}=s_{i}]$ . ", "page_idx": 4}, {"type": "text", "text": "4 Proof of Theorem 3.1: from Bayesian SST Model to Uniform Dominance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To prove theorem 3.1, we formalize the idea that agent $j$ \u2019s signal is more likely to agree with agent $i$ \u2019s than agent $k$ \u2019s is as what we call uniform dominance in definition 4.1. We\u2019ll show that any Bayesian SST model satisfies this property. Then, we\u2019ll prove that BPP mechanism is symmetrically strongly truthful when agents\u2019 private signals satisfy uniform dominance. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1. Given a random vector $(S_{i},S_{j},S_{k})\\in\\{-1,1\\}^{3}$ with joint distributio $1\\,P,S_{j}$ uniformly dominates $S_{k}$ for $S_{i}$ if $\\operatorname*{Pr}[S_{j}=1\\mid S_{i}=1]>\\operatorname*{Pr}[S_{k}=1\\mid S_{i}=1]$ and $\\operatorname*{Pr}[S_{j}=-1\\mid\\operatorname{\\dot{\\boldsymbol{S}}}_{i}=-1]>$ $\\operatorname*{Pr}[S_{k}=-1\\mid S_{i}=-1]$ . We call such an ordered tuple $\\langle S_{i},S_{j},S_{k}\\rangle$ a uniformly dominant tuple.3 ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2 shows how to identify uniformly dominant tuples under Bayesian SST models. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2. Under any Bayesian SST model, for any agent \ud835\udc56and items $a$ , $a^{\\prime}$ and $a^{\\prime\\prime}$ , agent $j$ \u2019s signal $S_{j}=S(a^{\\prime\\prime},a^{\\prime})$ uniformly dominates agent $k$ \u2019s signal $S_{k}=S(a^{\\prime\\prime},a)$ for signal $S_{i}=S(a,a^{\\prime})$ . ", "page_idx": 5}, {"type": "text", "text": "In other words, under any Bayesian SST model, the distribution of $S(a,a^{\\prime}),S(a^{\\prime\\prime},a^{\\prime}),S(a^{\\prime\\prime},a)$ satisfies uniform dominance for any $a,a^{\\prime},a^{\\prime\\prime}$ . In the rest of this section, we can view $(S_{i},S_{j},S_{k})$ as an abstract random vector with some joint distribution $P$ . ", "page_idx": 5}, {"type": "text", "text": "We now establish some implications of uniform dominance on the bonus-penalty payment. Lemma 4.3 shows that truth-telling is the best response if other signals are reported truthfully. Lemma 4.4 states that the expected payment is zero if everyone uses uninformed strategies (random functions independent of input). Lemma 4.5 characterizes the best response under symmetric strategy proflies (the same random function on each coordinate). ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.3 (Truthfulness). Given a uniformly dominant tuple $\\langle S_{i},S_{j},S_{k}\\rangle$ with distribution $P$ , for all $s_{i}\\,\\in\\,\\{-1,1\\}$ , $\\begin{array}{r}{s_{i}\\,=\\,\\arg\\operatorname*{max}_{\\hat{s}_{i}\\in\\{-1,1\\}}\\mathbb{E}_{P}\\,\\left[U^{B P P}(\\hat{s}_{i},S_{j},S_{k})\\mid S_{i}=s_{i}\\right]}\\end{array}$ and $\\mathbb{E}_{P}\\left[U^{B P P}(S_{i},S_{j},S_{k})\\right]>$ 0. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4. Given a uniformly dominant tuple $\\langle S_{i},S_{j},S_{k}\\rangle$ , with joint distribution $P$ if agent $j$ and $k$ both use an uninformed strategy $\\sigma$ so that $\\hat{S}_{j}=\\sigma(S_{j})$ and $\\hat{S}_{k}=\\sigma(S_{k}),$ , for all $s_{i}$ and $\\hat{s}_{i}$ in $\\{-1,1\\}$ , $\\mathbb{E}_{\\sigma,P}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})\\ |\\ S_{i}=s_{i}\\right]=0.$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.5. Given a uniformly dominant tuple $\\langle S_{i},S_{j},S_{k}\\rangle$ with distribution $P$ , for any strategy $\\sigma$ and $s_{i}\\,\\in\\,\\{-1,1\\}$ when agent $j$ and $k$ both use $\\sigma$ , arg $\\begin{array}{r}{\\operatorname*{max}_{\\hat{s}_{i}\\in\\{-1,1\\}}\\mathbb{E}_{\\sigma,P}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})\\mid S_{i}=s_{i}\\right]=}\\end{array}$ arg $\\operatorname*{max}_{\\hat{s}_{i}\\in\\{-1,1\\}}\\left\\{\\sigma(s_{i},\\hat{s}_{i})-\\sigma(-s_{i},\\hat{s}_{i})\\right\\}$ . ", "page_idx": 5}, {"type": "text", "text": "We\u2019d like to highlight that lemmas 4.3 to 4.5 as well as the proof of theorem 3.1 below hold for any uniformly dominant tuple $\\langle S_{i},S_{j},S_{k}\\rangle$ , not necessarily derived from the Bayesian SST model. This offers a path to generalize our mechanism for comparison data to other settings. ", "page_idx": 5}, {"type": "text", "text": "Proof of theorem 3.1. By lemma 4.2, for any agent $i$ , the associated agent $j$ \u2019s signal $S_{j}=S(a^{\\prime\\prime},a^{\\prime})$ uniformly dominates the associated $k$ \u2019s signal $S_{k}=S(a^{\\prime\\prime},a)$ for signal $S_{i}=S(a,a^{\\prime})$ . By lemma 4.3, if agent $j$ and $k$ are truthful, agent $i$ \u2019s best response is truthful reporting, so truth-telling is a BNE. ", "page_idx": 5}, {"type": "text", "text": "Now we show that all other symmetric equilibria are permutation or uninformed equilibria. For any symmetric equilibrium $\\pmb{\\sigma}=(\\sigma_{\\iota})_{\\iota\\in\\mathcal{N}}$ so that everyone uses the same strategy $\\sigma_{\\iota}=\\sigma$ for all $\\iota\\in{\\cal N}$ . If $\\sigma$ is not deterministic so that $\\sigma(s,s),\\sigma(s,-s)>0$ for some $s\\in\\{-1,1\\}$ , agent $i$ must be indifferent between reporting $s$ and $-s$ when getting $S_{i}\\,=\\,s$ . $\\sigma(s,s)-\\sigma(-s,s)\\,=\\,\\sigma(s,-s)\\,-\\,\\sigma(-s,-s)$ by lemma 4.5. This means $\\sigma(s,s)\\,=\\,\\sigma(-s,s)$ and $\\sigma(s,-s)\\,=\\,\\sigma(-s,-s)$ , and $\\sigma$ is an uninformed strategy. If the strategy is deterministic, there are two cases. If $\\sigma(s)=\\sigma(-s)$ , the strategy is also uninformed. If $\\sigma(s)\\neq\\sigma(-s)$ , $\\sigma$ is either truth-telling $s\\mapsto s$ or flipping $s\\mapsto-s$ for all $s$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, by lemma 4.4, any uninformed equilibrium\u2019s expectation is zero. Additionally, because eq. (1) is invariant when all inputs are flipped, the truth-telling and flipping/permutation equilibria has the same expected payment which is positive by lemma 4.3. \u25a1 ", "page_idx": 5}, {"type": "text", "text": "5 Generalization of Bonus-penalty Payment Mechanisms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now leverage the key idea of uniform dominance to design peer prediction mechanisms for networked data in section 5.1. In section 5.2, we summarize our design approach as a general scheme that first identifies uniform dominance structures and then engages the bonus-penalty payment. We prove the uniqueness of bonus-penalty payment: it is the only payment function, up to some positive affine transformation, that induces truth-telling as a strict BNE for all uniform dominant tuples. ", "page_idx": 6}, {"type": "text", "text": "5.1 Bonus-penalty Payment Mechanisms for Networked Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Uniform dominance implies agent $i$ \u2019s signal is more likely to agree with agent $j$ \u2019s than with agent $k$ \u2019s. Social networks are another natural domain exhibiting this property, as homophily [40] suggests that agents\u2019 opinions or signals in a social network are more likely to agree with their friends than with non-friends. Leveraging this insight, we use a bonus-penalty payment scheme to elicit binary networked data. ", "page_idx": 6}, {"type": "text", "text": "Mechanism 2: BPP mechanism for networked data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Let $(V,E)$ be a graph of agents in $V,\\hat{\\mathbf{s}}\\in\\{-1,1\\}^{V}$ from all agent\u2019s reports. for agent $i\\in V$ do Find agents $j$ (friend) and $k$ (non-friend) so that $(i,j)\\in E$ but $(i,k)\\not\\in E$ , and pay agent $i$ $M_{i}(\\hat{\\bf s})=U^{B P P}(\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k})=\\hat{s}_{i}\\hat{s}_{j}-\\hat{s}_{i}\\hat{s}_{k}.$ (3) ", "page_idx": 6}, {"type": "text", "text": "Below, we provide a theoretical guarantee for our mechanism under a popular graphical model for social network data, Ising model [14, 43], which captures the correlation between agents and their friends. Formally, an Ising model consists of an undirected graph $(V,E)$ and correlation parameter $\\beta_{i,j}\\geq0$ for each edge $(i,j)\\in E$ . Each agent is a node in the graph, ${N=V}$ , and has a binary private signal (1 or $^{-1}$ ) jointly distributed as the following: For all $\\mathbf{s}=(s_{i})_{i\\in V}\\in\\{-1,1\\}^{V}$ , $\\operatorname*{Pr}_{\\beta}[\\mathbf{S}=\\mathbf{s}]\\propto\\exp(H(\\mathbf{s}))$ where the energy function is $\\begin{array}{r}{H(\\mathbf{s})=\\sum_{(i,j)\\in E}\\beta_{i,j}s_{i}s_{j}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1. If agents\u2019 signals are sampled from an Ising model on undirected graph $(V,E)$ with correlation parameters $\\beta$ , Mechanism 2 is symmetrically strongly truthful, when $\\begin{array}{r}{\\frac{2\\beta}{d}>\\ln{\\frac{e^{2(d+1)}\\overline{{\\beta}}_{+1}}{e^{2\\overline{{\\beta}}}+e^{2d\\overline{{\\beta}}}}}}\\end{array}$ where $\\underline{{\\beta}}=\\operatorname*{min}_{(i,j)\\in E}\\beta_{i,j},\\,\\overline{{\\beta}}=\\operatorname*{max}_{(i,j)\\in E}\\beta_{i,j},$ , and $d$ is the maximal degree of graph $(V,E)$ . ", "page_idx": 6}, {"type": "text", "text": "Mechanism 2 does not require knowledge about parameters of the Ising model, but only the connection of the network $(V,E)$ . Social network platforms, which already possess this knowledge, can easily integrate our mechanism when conducting surveys. Additionally, snowball sampling [24], which relies on participants referring their friends, is also naturally compatible with our mechanism. ", "page_idx": 6}, {"type": "text", "text": "The complete proof of theorem 5.1 is quite technical and is deferred to the appendix, where we also explain why the bound between $_\\beta$ and $d$ is necessary. Below, we provide a sketch of the proof. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch for theorem 5.1. As discussed in section 4, we only need to show that for any agent $i$ , for all agent $j$ with $(i,j)\\in E$ and $k$ with $(i,k)\\notin E$ , $j$ \u2019s signal uniformly dominates $k$ \u2019s signal for $i$ \u2019s signal. Because the energy function $H(\\mathbf{s})$ above remains invariant when the signs are flipped, $\\operatorname*{Pr}[S_{i}=1]=\\operatorname*{Pr}[S_{j}=1]=\\operatorname*{Pr}[S_{k}=1]=1/2$ , it is sufficient to prove that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[S_{i}=1\\mid S_{j}=1]>\\operatorname*{Pr}[S_{i}=1\\mid S_{k}=1].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We then prove a lower bound for the left-hand side and an upper bound for the right-hand side separately. For the left-hand side, we use the Griffiths\u2019 inequality [44] to show that the minimum value of ${\\dot{\\operatorname*{Pr}}}[S_{i}=1\\mid S_{j}=1]$ happens when $j$ is the only friend of $i$ . For the right-hand side, we use Weitz\u2019s self-avoiding walk [65] and reduce any graph with maximum degree $d$ into a $d$ -ary tree. \u25a1 ", "page_idx": 6}, {"type": "text", "text": "5.2 General Design Scheme and Uniqueness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The design of BPP mechanisms for comparison data and networked data has suggested a general design scheme for other elicitation settings. That is, if one can identify a uniformly dominant tuple for ", "page_idx": 6}, {"type": "text", "text": "Mechanism 3: General design scheme using BPP ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "ofjTu2ktxO/tmp/9497003fc9c9414ef92d2d14bca363aa86befcb40483bf6eb05d7b90d8a8572e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "each agent, adopting the bonus-penalty payment gives a symmetrically strongly truthful mechanism.   \nWe further show that the bonus-penalty payment is in some sense unique. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. If for each agent \ud835\udc56the associated agent $j$ \u2019s signal uniformly dominates \ud835\udc58\u2019s signal for \ud835\udc56\u2019s signal, the above scheme is symmetrically strongly truthful. ", "page_idx": 7}, {"type": "text", "text": "When an agent $i$ has multiple pairs of $(j_{1},k_{1}),\\ldots,(j_{\\ell},k_{\\ell})$ so that $j_{l}$ \u2019s signal uniformly dominates $k_{l}$ \u2019s for $i$ \u2019s for each $l=1,\\dots,\\ell$ , we may pay agent $i$ the average of bonus-penalty payment on all pairs $\\begin{array}{r}{M_{i}(\\hat{\\bf s})=\\frac{1}{\\ell}\\sum_{l=1}^{\\ell}U^{B P P}(\\hat{s}_{i},\\hat{s}_{j_{l}},\\hat{s}_{k_{l}})}\\end{array}$ . This average maintains our symmetrically strongly truthful guarantee while potentially reducing the variance in payments. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 shows that bonus-penalty payment is a sufficient condition for designing good elicitation mechanisms for information structures with uniform dominance. We now prove it is also a necessary condition: any payment that induces truth-telling as a strict BNE under all uniformly dominant tuples must be an affine transformation of the bonus-penalty payment. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3 (Uniqueness). A payment $U:\\{-1,1\\}^{3}\\to\\mathbb{R}$ satisfies that, for all uniformly dominant tuples $\\langle S_{i},S_{j},S_{k}\\rangle$ , $\\begin{array}{r}{s_{i}=\\arg\\operatorname*{max}_{\\hat{s}_{i}\\in\\{-1,1\\}}\\mathbb{E}\\left[U(\\hat{s}_{i},S_{j},S_{k})\\mid S_{i}=s_{i}\\right]}\\end{array}$ , if and only if there exist $\\lambda>0$ and $\\mu:\\{-1,1\\}^{2}\\to\\mathbb{R}$ so that ", "page_idx": 7}, {"type": "equation", "text": "$$\nU(s_{i},s_{j},s_{k})=\\lambda U^{B P P}(s_{i},s_{j},s_{k})+\\mu(s_{j},s_{k})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where choice of $\\mu$ does not affect the set of equilibria. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present experiments on real-world data to evaluate our models and mechanisms. We hope to cast insights on two questions empirically. Does our mechanism provide better rewards when all agents report truthfully than when all agents report randomly? Does our mechanism incentivize truth-telling for each agent if all other agents are truthful? We evaluate Mechanism 1 and 2 by comparing three settings, truth-telling, uninformed, and unilateral deviation, using empirical cumulative distribution functions (ECDF) on agents\u2019 payments. Each point on ECDF denotes the fraction of agents who get paid less than a particular value. ", "page_idx": 7}, {"type": "text", "text": "For both comparison and networked datasets (figs. 1 and 2), we find our mechanisms provide better payments to agents under truthful settings than the other two settings. The ECDF under truth-telling lies below the other two ECDFs, which is known as first-order stochastic dominance. This implies that the truth-telling strategy results in higher average, quantiles (e.g., first quartile, median, and third quartile), and a greater expectation of any monotone function on the empirical distribution than the other two settings. We provide additional ", "page_idx": 7}, {"type": "text", "text": "6.1 SUSHI Preference Dataset for Comparison Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider preference data for a collection of 10 sushi items (item set A) [26, 27], and focus on a group of 249 agents. Each agent provides a complete ranking of all 10 types of sushi in the dataset. These agents are female, aged thirty to forty-nine, who took more than three hundred seconds to rank the items and mostly lived in Kanto and Shizuoka until age fifteen. We restrict the set of agents to avoid significant violations of transitivity across different agents and to better align with our model assumptions. In the appendix, we will present the experimental results for other groups of users and further test whether the dataset satisfies transitivity. ", "page_idx": 7}, {"type": "text", "text": "For the first question, we use Mechanism 1 to compute each agent\u2019s payment under the truth-telling or uninformed strategy profile. For each agent $i$ , we 1) randomly sample three items $a,a^{\\prime},a^{\\prime\\prime}$ and two agents $j,k,2)$ derive agent $i^{*}$ \u2019s comparison on the first two items $(a,a^{\\prime})$ from her ranking, (and similarly for agent $j$ \u2019s comparison on $(a^{\\prime},a^{\\prime\\prime})$ , and agent $k$ \u2019s comparison on $(a,a^{\\prime\\prime}))$ , 3) compute bonus-penalty payment on these three comparisons, 4) repeat the above procedure 100 times and pay agent $i$ with the average of those 100 trials. For the uninformed strategy setting, we replace every agent\u2019s comparisons with uniform random bits and compute the payment. The left column of fig. 1 presents the ECDF of payments for the agents in both settings. The figure shows that in the uninformed random strategy setting only about $50\\%$ of the agents receive positive payments, while in the original dataset (truthful strategy setting) over $75\\%$ of the users receive positive payments. The right column of fig. 1 tests the second question if the agent has the incentive to deviate when every other agent is truthful. The truth-telling curve is identical to the left column of fig. 1. For unilateral deviation, each agent gets the above bonus-penalty payment when her comparisons are replaced by uniform random bits. We plot the ECDFs of payments for both settings in the right column of fig. 1. The figure shows that the ECDF of the unilateral deviation payments is above the ECDF of human users\u2019 payments, indicating that our mechanism pays more to the truth-telling agents. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/93f9f755a835e362dd41232c364889aeb9765d6de6c1f69b303294eca79daa67.jpg", "img_caption": ["Figure 1: SUSHI preference dataset "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Last.fm Dataset for Networked Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We test our BPP mechanism on the Last.fm dataset from Cantador et al. [8]. This dataset consists of 1892 agents on Last.fm, forming a social network with 12704 edges and an average degree of 6.71. It records agents\u2019 top fifty favorite artists whom they have listened to the most. We note that, in the dataset, listener fractions for all artists are much smaller than non-listener fractions. This bias differs from our Ising model in section 5.1 where every agent has the same chance to get both signals. Thus, the result can be seen as a stress test for our mechanism even when the data deviate from the assumption of our theoretical results. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 focuses on the most popular artist in the dataset, Lady Gaga, who has a listener fraction of $32.3\\%$ . The results for additional artists are presented in the supplementary material. The left column of fig. 2 tests the first question. Each agent has a binary signal about whether or not she listens to a particular artist (Lady Gaga in this section). For the truth-telling setting, everyone reports her signal truthfully and gets payment by the bonus-penalty payment (formally defined in section 5.1). For the uninformed setting, everyone gets the bonus-penalty payment when all reports are iid according to the prior (0.322 for Lady Gaga). When everyone is truthful, more than $76\\%$ of agents get positive payments and have an average payment of 0.37 for Lady Gaga, while when agents report randomly, only half get positive payments, and have a near zero average payment. These results suggest that agents got more incentive to choose the truth-telling equilibrium than the uninformed equilibrium. The right column of fig. 2 tests the second question. The truth-telling curve is identical to the left column of fig. 2. For the unilateral deviation setting, each agent gets the bonus-penalty payment when she reports listener/non-listener uniformly at random. The unilateral deviation\u2019s payment is worse than the payments for truth-telling, decreasing from 0.37 to near zero for Lady Gaga. ", "page_idx": 8}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/1e2be26e97b099783d14ac15878614cc5b8d5c01872b2976a1db63b007db0234.jpg", "img_caption": ["Figure 2: Last.fm dataset for Lady Gaga "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce a symmetrically strongly truthful peer prediction mechanism for eliciting comparison data without verification and extend it to eliciting networked data under Ising models. Our mechanisms are evaluated using real-world data. A key insight from our work is the identification of a structure we term \u201cuniform dominance,\u201d which suggests a path for designing mechanisms in more complex elicitation settings. For example, in time-series data, adjacent points tend to be more related than distant ones, and in contextual settings, feedback from similar contexts is typically more related than from different contexts. ", "page_idx": 9}, {"type": "text", "text": "A central assumption in this study is that agents are a priori similar. Hence, noisy comparisons of item pairs are independent of the assigned agent\u2019s identity. This assumption is reasonable for items with widely agreed-upon rankings, such as quality assessments of large language model (LLM) outputs. However, it may break down in settings where preferences are highly polarized, such as political opinions or social choice problems4. Despite this, our additional experiments in appendix F, which relax the selection rule used in obtaining fig. 1, show that the mechanism remains robust even when some dissimilarities among agents exist. ", "page_idx": 9}, {"type": "text", "text": "Agents in our model are assumed to focus solely on maximizing their payments, without accounting for efforts or external incentives such as minimizing others\u2019 rewards or intentionally distorting rankings. While our mechanism may be extended to handle binary effort as suggested in previous work [11, 57], accommodating more than two effort levels would require additional assumptions [69]. Moreover, one may hope to incorporate the designer\u2019s utility, by factoring in downstream learning problems along with elicitation payments. This would necessitate a significant overhaul of the existing learning framework. ", "page_idx": 9}, {"type": "text", "text": "Our mechanisms achieve a symmetric, strongly truthful equilibrium. This does not rule out the existence of non-symmetric equilibria with potentially higher utility. However, such equilibria would require complex coordination among agents, making them less likely to arise naturally. ", "page_idx": 9}, {"type": "text", "text": "From a technical standpoint, our approach involves several assumptions that can be generalized or relaxed. Our Bayesian SST model, which relies on strong stochastic transitivity, serves as a non-parametric extension of several widely used parametric ranking models. In appendix C.2, we present both positive and negative results regarding weaker notions of transitivity (e.g., [5]). While we assume admissible assignments, this can be relaxed to random assignments with full support. Additionally, limited liability can be ensured in our mechanism. For example, adding a constant of 1 to the payment function in eq. (2) ensures that the payment is either 2 or 0. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by the National Science Foundation under grant no. IIS2147187. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jacob D Abernethy and Rafael Frongillo. A collaborative mechanism for crowdsourcing prediction problems. Advances in neural information processing systems, 24, 2011.   \n[2] Hossein Azari, David Parks, and Lirong Xia. Random utility theory for social choice. Advances in Neural Information Processing Systems, 25, 2012.   \n[3] Aur\u00e9lien Baillon. Bayesian markets to elicit private information. Proceedings of the National Academy of Sciences, 114(30):7958\u20137962, 2017.   \n[4] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n[5] Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201908, page 268\u2013276, USA, 2008. Society for Industrial and Applied Mathematics.   \n[6] Noah Burrell and Grant Schoenebeck. Measurement integrity in peer prediction: A peer assessment case study. In Proceedings of the 24th ACM Conference on Economics and Computation, pages 369\u2013389, 2023.   \n[7] R\u00f3bert Busa-Fekete, Eyke H\u00fcllermeier, and Bal\u00e1zs Sz\u00f6r\u00e9nyi. Preference-based rank elicitation using statistical models: The case of mallows. In International conference on machine learning, pages 1071\u20131079. PMLR, 2014.   \n[8] Iv\u00e1n Cantador, Peter Brusilovsky, and Tsvi Kuflik. Second workshop on information heterogeneity and fusion in recommender systems. In Proceedings of the fifth ACM conference on Recommender systems, pages 387\u2013388, 2011.   \n[9] Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, and Enhong Chen. Towards personalized evaluation of large language models with an anonymous crowd-sourcing platform. In Companion Proceedings of the ACM on Web Conference 2024, pages 1035\u20131038, 2024.   \n[10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.   \n[11] Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous proficiency. In Proceedings of the 22nd international conference on World Wide Web, pages 319\u2013330, 2013.   \n[12] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Concentration of multilinear functions of the ising model with applications to network data. Advances in Neural Information Processing Systems, 30, 2017.   \n[13] Donald Davidson and Jacob Marschak. Experimental tests of a stochastic decision theory. Measurement: Definitions and theories, 17(2), 1959.   \n[14] Glenn Ellison. Learning, local interaction, and coordination. Econometrica: Journal of the Econometric Society, pages 1047\u20131071, 1993.   \n[15] Brian Eriksson. Learning to top-k search using pairwise comparisons. In Artificial Intelligence and Statistics, pages 265\u2013273. PMLR, 2013.   \n[16] Boi Faltings. Game-theoretic mechanisms for eliciting accurate information. In IJCAI, 2022.   \n[17] Shi Feng, Fang-Yi Yu, and Yiling Chen. Peer prediction for learning agents. Advances in Neural Information Processing Systems, 35:17276\u201317286, 2022.   \n[18] Peter C Fishburn. Binary choice probabilities: on the varieties of stochastic transitivity. Journal of Mathematical psychology, 10(4):327\u2013352, 1973.   \n[19] Kiriaki Frangias, Andrew Lin, Ellen Vitercik, and Manolis Zampetakis. Algorithmic contract design for crowdsourced ranking. arXiv preprint arXiv:2310.09974, 2023.   \n[20] Rafael Frongillo and Ian A Kash. Vector-valued property elicitation. In Conference on Learning Theory, pages 710\u2013727. PMLR, 2015.   \n[21] Xi Alice Gao, James R Wright, and Kevin Leyton-Brown. Incentivizing evaluation with peer prediction and limited access to ground truth. Artificial Intelligence, 275:618\u2013638, 2019.   \n[22] Hans-Otto Georgii. Gibbs measures and phase transitions. Walter de Gruyter GmbH & Co. KG, Berlin, 2011.   \n[23] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007.   \n[24] Leo A Goodman. Snowball sampling. The annals of mathematical statistics, pages 148\u2013170, 1961.   \n[25] David R Hunter. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):384\u2013406, 2004.   \n[26] Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order responses. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 583\u2013588, 2003.   \n[27] Toshihiro Kamishima and Shotaro Akaho. Efficient clustering for orders. In Sixth IEEE International Conference on Data Mining-Workshops (ICDMW\u201906), pages 274\u2013278. IEEE, 2006.   \n[28] Sai Praneeth Karimireddy, Wenshuo Guo, and Michael I Jordan. Mechanisms that incentivize data sharing in federated learning. arXiv preprint arXiv:2207.04557, 2022.   \n[29] Yuqing Kong. Dominantly truthful multi-task peer prediction with a constant number of tasks. In Proceedings of the fourteenth annual acm-siam symposium on discrete algorithms, pages 2398\u20132411. SIAM, 2020.   \n[30] Yuqing Kong and Grant Schoenebeck. A framework for designing information elicitation mechanisms that reward truth-telling. CoRR, abs/1605.01021, 2016. URL http://arxiv. org/abs/1605.01021.   \n[31] Yuqing Kong and Grant Schoenebeck. Equilibrium selection in information elicitation without verification via information monotonicity. In 9th Innovations in Theoretical Computer Science Conference (ITCS 2018). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2018.   \n[32] Yuqing Kong and Grant Schoenebeck. Water from two rocks: Maximizing the mutual information. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 177\u2013194, 2018.   \n[33] Yuqing Kong and Grant Schoenebeck. An information theoretic framework for designing information elicitation mechanisms that reward truth-telling. ACM Transactions on Economics and Computation (TEAC), 7(1):1\u201333, 2019.   \n[34] Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. arXiv preprint arXiv:1805.10627, 2018.   \n[35] Nicolas S Lambert, David M Pennock, and Yoav Shoham. Eliciting properties of probability distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129\u2013138, 2008.   \n[36] Yang Liu, Rixing Lou, and Jiaheng Wei. Auditing for federated learning: A model elicitation approach. In Proceedings of the Fifth International Conference on Distributed Artificial Intelligence, DAI \u201923, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400708480. doi: 10.1145/3627676.3627683. URL https://doi.org/10.1145/ 3627676.3627683.   \n[37] Yang Liu, Juntao Wang, and Yiling Chen. Surrogate scoring rules. ACM Transactions on Economics and Computation, 10(3):1\u201336, 2023.   \n[38] R Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005.   \n[39] C. L. Mallows. Non-null ranking models. i. Biometrika, 44(1/2):114\u2013130, 1957. ISSN 00063444. URL http://www.jstor.org/stable/2333244.   \n[40] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415\u2013444, 2001.   \n[41] N. Miller, P. Resnick, and R. Zeckhauser. Eliciting informative feedback: The peer-prediction method. Management Science, pages 1359\u20131373, 2005.   \n[42] Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-prediction method. Management Science, 51(9):1359\u20131373, 2005.   \n[43] Andrea Montanari and Amin Saberi. The spread of innovations in social networks. Proceedings of the National Academy of Sciences, 107(47):20196\u201320201, 2010.   \n[44] Marc M\u00e9zard and Andrea Montanari. Information, Physics, and Computation. Oxford University Press, 01 2009. ISBN 9780198570837. doi: 10.1093/acprof:oso/9780198570837.001.0001. URL https://doi.org/10.1093/acprof:oso/9780198570837.001.0001.   \n[45] Ryan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.   \n[46] Kent Harold Osband. Providing Incentives for Better Cost Forecasting (Prediction, Uncertainty Elicitation). University of California, Berkeley, 1985.   \n[47] Drazen Prelec. A bayesian truth serum for subjective data. science, 306(5695):462\u2013466, 2004.   \n[48] Goran Radanovic and Boi Faltings. A robust bayesian truth serum for non-binary signals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages 833\u2013839, 2013.   \n[49] Goran Radanovic and Boi Faltings. Incentives for truthful information elicitation of continuous signals. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 770\u2013776, 2014.   \n[50] Grant Schoenebeck and Fang-Yi Yu. Learning and strongly truthful multi-task peer prediction: A variational approach. In 12th Innovations in Theoretical Computer Science Conference (ITCS 2021). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2021.   \n[51] Grant Schoenebeck and Fang-Yi Yu. Two strongly truthful mechanisms for three heterogeneous agents answering one question. ACM Transactions on Economics and Computation, 10(4):1\u201326, 2023.   \n[52] Grant Schoenebeck, Fang-Yi Yu, and Yichi Zhang. Information elicitation from rowdy crowds. In Proceedings of the Web Conference 2021, pages 3974\u20133986, 2021.   \n[53] S\u00f6ren W Scholz, Martin Meissner, and Reinhold Decker. Measuring consumer preferences for complex products: A compositional approach basedonpaired comparisons. Journal of Marketing Research, 47(4):685\u2013698, 2010.   \n[54] Nihar Shah, Sivaraman Balakrishnan, Aditya Guntuboyina, and Martin Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. In International Conference on Machine Learning, pages 11\u201320. PMLR, 2016.   \n[55] Nihar B Shah, Joseph K Bradley, Abhay Parekh, Martin Wainwright, and Kannan Ramchandran. A case for ordinal peer-evaluation in moocs. In NIPS workshop on data driven education, volume 15, page 67, 2013.   \n[56] Victor Shnayder, Arpit Agarwal, Rafael Frongillo, and David C. Parkes. Informed truthfulness in multi-task peer prediction. In Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916, pages 179\u2013196, New York, NY, USA, 2016. ACM. ISBN 978-1-4503- 3936-0.   \n[57] Victor Shnayder, Arpit Agarwal, Rafael M. Frongillo, and David C. Parkes. Informed truthfulness in multi-task peer prediction. CoRR, abs/1603.03151, 2016. URL http: //arxiv.org/abs/1603.03151.   \n[58] Louis L Thurstone. A law of comparative judgment. In Scaling, pages 81\u201392. Routledge, 2017.   \n[59] Amos Tversky and J Edward Russo. Substitutability and similarity in binary choices. Journal of Mathematical psychology, 6(1):1\u201312, 1969.   \n[60] Stephan Vail. A stochastic model for utilities. Unpublished manuscript, 1953.   \n[61] Luis Von Ahn and Laura Dabbish. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319\u2013326, 2004.   \n[62] Luis von Ahn and Laura Dabbish. Designing games with a purpose. Commun. ACM, 51(8): 58\u201367, aug 2008. ISSN 0001-0782. doi: 10.1145/1378704.1378719. URL https://doi. org/10.1145/1378704.1378719.   \n[63] Bo Waggoner and Yiling Chen. Output agreement mechanisms and common knowledge. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 2, pages 220\u2013226, 2014.   \n[64] Jiaheng Wei, Zuyue Fu, Yang Liu, Xingyu Li, Zhuoran Yang, and Zhaoran Wang. Sample elicitation. In International Conference on Artificial Intelligence and Statistics, pages 2692\u2013 2700. PMLR, 2021.   \n[65] Dror Weitz. Counting independent sets up to the tree threshold. In Proceedings of the thirtyeighth annual ACM symposium on Theory of computing, pages 140\u2013149, 2006.   \n[66] Jens Witkowski and David C. Parkes. A Robust Bayesian Truth Serum for Small Populations. In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012), 2011.   \n[67] Jens Witkowski and David C. Parkes. Peer prediction without a common prior. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC 2012, Valencia, Spain, June 4-8, 2012, pages 964\u2013981. ACM, 2012.   \n[68] Peter Zhang and Yiling Chen. Elicitability and knowledge-free elicitation with peer prediction. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 245\u2013252. International Foundation for Autonomous Agents and Multiagent Systems, 2014.   \n[69] Yichi Zhang and Grant Schoenebeck. High-effort crowds: Limited liability via tournaments. In Proceedings of the ACM Web Conference 2023, WWW \u201923, page 3467\u20133477, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/ 3543507.3583334. URL https://doi.org/10.1145/3543507.3583334.   \n[70] Shuran Zheng, Fang-Yi Yu, and Yiling Chen. The limits of multi-task peer prediction. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 907\u2013926, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Further discussion on BPP payment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we discuss the connection of bonus-penalty payment and existing peer prediction mechanisms. First, if we substitute the third input with a uniformly random bit, denoted as $\\hat{s}_{k}=Z\\sim_{u}$ $\\{-1,1\\}$ , the bonus-penalty payment simplifies to the agreement mechanism [62, 61, 63], one of the most basic peer prediction mechanisms, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[U^{B P P}(\\hat{s}_{i},\\hat{s}_{j},Z)\\right]=\\hat{s}_{i}\\hat{s}_{j}=2\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{j}]-1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "However, the agreement mechanism is not symmetrically strongly truthful, as all agents always reporting 1 and $^{-1}$ can result in higher payments than truth-telling. ", "page_idx": 14}, {"type": "text", "text": "The bonus-penalty payment eq. (1) is originally proposed by [11, 57] for the multi-task setting. Our BPP mechanism in Mechanism 3 can be seen as a generalization of multi-task setting. In the multi-task setting, agents works on multiple tasks and for each task the private signals are jointly identically and independently (iid) sampled from a fixed distribution and the each agent\u2019s strategy also are iid. Take two agents (Isabel and Julia) and two tasks as an example: Isabel has a private signal $(s_{i}^{1},s_{i}^{2})$ and reports $(\\hat{s}_{i}^{1},\\hat{s}_{i}^{2})$ and Julia has $(s_{j}^{1},s_{j}^{2})$ and reports $(\\hat{s}_{j}^{1},\\hat{\\hat{s}_{j}^{2}})$ where $(s_{i}^{l},s_{j}^{l})$ are iid from random vector $(S_{i},S_{j})$ . Isabel and Julia decide their reports on each task using random function $\\sigma_{i},\\sigma_{j}:\\{-1,1\\}\\mapsto\\{-1,1\\}$ respectively. Dasgupta and Ghosh [11] use the following payments for Isabel ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{1}[\\hat{s}_{i}^{1}=\\hat{s}_{j}^{1}]-\\mathbf{1}[\\hat{s}_{i}^{1}=\\hat{s}_{j}^{2}]=\\frac{1}{2}U^{B P P}\\left(\\hat{s}_{i}^{1},\\hat{s}_{j}^{1},\\hat{s}_{j}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The payment is a special case of Mechanism 3 by taking the second input as $\\hat{s}_{j}^{1}$ and the third input as $\\hat{s}_{j}^{2}$ . Additionally, $S_{j}^{1}$ uniform dominates $S_{j}^{2}$ for $S_{i}^{1}$ if and only if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[S_{j}=1\\mid S_{i}=1]>\\operatorname*{Pr}[S_{j}=1],{\\mathrm{~and~}}\\operatorname*{Pr}[S_{j}=-1\\mid S_{i}=-1]>\\operatorname*{Pr}[S_{j}=-1]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is called categorical signal distributions [57]. ", "page_idx": 14}, {"type": "text", "text": "Finally, similar to Shnayder et al. [57], we may extend to non-binary signal setting by extending the payment to ", "page_idx": 14}, {"type": "equation", "text": "$$\nU^{B P P}(\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k})=2\\left(\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{j}]-\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{k}]\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the definition of uniform dominance to the following. ", "page_idx": 14}, {"type": "text", "text": "Definition A.1. Given a random vector $(S_{i},S_{j},S_{k})\\in\\Omega^{3}$ on a discrete domain, we say $S_{j}$ uniformly dominates $S_{k}$ for $S_{i}$ if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}[S_{j}=s\\ |\\ S_{i}=s]-\\operatorname*{Pr}[S_{k}=s\\ |\\ S_{i}=s]>0\\ \\mathrm{an}}\\\\ &{\\operatorname*{Pr}[S_{j}=s^{\\prime}\\ |\\ S_{i}=s]-\\operatorname*{Pr}[S_{k}=s^{\\prime}\\ |\\ S_{i}=s]<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $s,s^{\\prime}\\in\\Omega$ with $s\\neq s^{\\prime}$ ", "page_idx": 14}, {"type": "text", "text": "However, the guarantee for truth-telling (informed truthfulness) is weaker than the binary setting. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.2. Given any discrete domain $\\Omega,$ , if for each agent \ud835\udc56the associated agent $j$ \u2019s signal uniformly dominates $k\\,\\gamma_{s}$ signal for \ud835\udc56\u2019s signal (definition A.1), Mechanism $3\\,\\mathrm{\\dot{s}}$ scheme is symmetrically informed truthful so that ", "page_idx": 14}, {"type": "text", "text": "1. truth-telling is a strict equilibrium, and ", "page_idx": 14}, {"type": "text", "text": "2. each agent\u2019s expected payment in truth-telling is no less than the payment in any other symmetric equilibria and strictly better than any uninformed equilibrium\u2019s. ", "page_idx": 14}, {"type": "text", "text": "Proof. First truth-telling is a strict equilibrium, because if $S_{i}=s$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\hat{s}}{\\arg\\operatorname*{max}}\\,\\mathbb{E}\\left[U^{B P P}(\\hat{s},S_{j},S_{k})\\mid S_{i}=s\\right]}\\\\ &{=\\underset{\\hat{s}}{\\arg\\operatorname*{max}}\\,\\mathrm{Pr}[S_{j}=\\hat{s}\\mid S_{i}=s]-\\mathrm{Pr}[S_{k}=\\hat{s}\\mid S_{i}=s]}\\\\ &{=\\!s}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Additionally, because $\\operatorname*{Pr}[S_{j}=s\\ |\\ S_{i}=s]-\\operatorname*{Pr}[S_{k}=s\\ |\\ S_{i}=s]>\\operatorname*{Pr}[S_{j}=s^{\\prime}\\ |\\ S_{i}=s]-\\operatorname*{Pr}[S_{k}=s]$ $s^{\\prime}\\mid S_{i}\\,=\\,s]$ for all $s^{\\prime}\\neq s$ , summing over all possible $s^{\\prime}\\in\\Omega$ on both sides gets $\\operatorname*{Pr}[S_{j}\\,=\\,s\\mid S_{i}\\,=$ $s\\mathbf{\\mathrm{\\rfloor}}-\\operatorname*{Pr}[S_{k}=s\\mid S_{i}=s]>0$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[U^{B P P}(S_{i},S_{j},S_{k})\\right]>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any informed equilibrium, by a direct computation $\\mathbb{E}\\left[U^{B P P}(\\hat{S}_{i},\\hat{S}_{j},\\hat{S}_{k})\\right]=0.$ . ", "page_idx": 15}, {"type": "text", "text": "Finally, we show that the truth-telling has the maximum expected payment for each agents. When all agent use a strategy $\\sigma:\\Omega\\to\\Omega$ , agent $i$ \u2019s expected payment is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s_{i},\\hat{s}_{i}\\in\\Omega}\\mathrm{Pr}[S_{i}=s_{i}]\\sigma(s_{i},\\hat{s}_{i})\\mathbb{E}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{=2\\displaystyle\\sum_{s_{i},\\hat{s}_{i}\\in\\Omega}\\mathrm{Pr}[S_{i}=s_{i}]\\sigma(s_{i},\\hat{s}_{i})\\sum_{s\\in\\Omega}(\\mathrm{Pr}[S_{j}=s\\mid S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s\\mid S_{i}=s_{i}])\\sigma(s,\\hat{s}_{i})}\\\\ &{=2\\displaystyle\\sum_{s_{i}\\in\\Omega}\\mathrm{Pr}[S_{i}=s_{i}]\\sum_{\\hat{s}_{i},s\\in\\Omega}\\sigma(s_{i},\\hat{s}_{i})\\sigma(s,\\hat{s}_{i})(\\mathrm{Pr}[S_{j}=s\\mid S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s\\mid S_{i}=s_{i}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\begin{array}{r l r}{f_{s_{i}}(s)}&{{}:=}&{\\sum_{\\hat{s}_{i}\\in\\Omega}\\sigma(s_{i},\\hat{s}_{i})\\sigma(s,\\hat{s}_{i})}\\end{array}$ which is between 0 and 1, because $f_{s_{i}}(s)$ \u2264 $\\begin{array}{r}{\\sum_{\\hat{s}_{i}\\in\\Omega}\\sigma(s_{i},\\hat{s}_{i})\\sum_{\\hat{s}_{i}\\in\\Omega}\\!\\dot{\\sigma}(s,\\hat{s}_{i})=1}\\end{array}$ . Then the expectation becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{s_{i},\\hat{s_{i}}\\in\\Omega}\\operatorname*{Pr}[S_{i}=s_{i}]\\sigma(s_{i},\\hat{s_{i}})\\mathbb{E}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{=2\\displaystyle\\sum_{s_{i}\\in\\Omega}\\operatorname*{Pr}[S_{i}=s_{i}]\\sum_{s\\in\\Omega}\\left(\\operatorname*{Pr}[S_{j}=s\\mid S_{i}=s_{i}]-\\operatorname*{Pr}[S_{k}=s\\mid S_{i}=s_{i}]\\right)f_{s_{i}}(s)}\\\\ &{\\leq2\\displaystyle\\sum_{s_{i}\\in\\Omega}\\operatorname*{Pr}[S_{i}=s_{i}]\\left(\\operatorname*{Pr}[S_{j}=s_{i}\\mid S_{i}=s_{i}]-\\operatorname*{Pr}[S_{k}=s_{i}\\mid S_{i}=s_{i}]\\right)}\\\\ &{=\\mathbb{E}\\left[U^{B P P}(S_{i},S_{j},S_{k})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inequality holds because $f_{s_{i}}\\in[0,1]$ and definition A.1. Therefore, we complete the proof. \u25a1 ", "page_idx": 15}, {"type": "text", "text": "B Proofs in Section 2: Bayesian SST model and other models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proofs of propositions 2.3 and 2.5 are standard, and variations can be found in related literature.   \nWe include proofs here for completeness. ", "page_idx": 15}, {"type": "text", "text": "Proof of proposition 2.3. First given $\\theta\\ \\in\\ \\mathbb{R}^{\\mathcal{R}}$ , for all distinct $a,a^{\\prime},a^{\\prime\\prime}\\ \\ \\in\\ {\\mathcal{A}}$ , $\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})\\;=$ 1], $\\operatorname*{Pr}[T_{\\theta}(a^{\\prime},a^{\\prime\\prime})\\,=\\,1]\\,>\\,1/2$ implies that $\\theta_{a}-\\theta_{a^{\\prime}}\\,>\\,0$ and $\\theta_{a^{\\prime}}\\,-\\,\\theta_{a^{\\prime\\prime}}\\,>\\,0$ becuase $F$ is strictly increasing and $F(0)=1/2$ . Because $\\theta_{a}-\\theta_{a^{\\prime\\prime}}=\\theta_{a}-\\theta_{a^{\\prime}}+\\theta_{a^{\\prime}}-\\theta_{a^{\\prime\\prime}}>\\operatorname*{max}(\\theta_{a}-\\theta_{a^{\\prime}},\\theta_{a^{\\prime}}-\\theta_{a^{\\prime\\prime}}),$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime\\prime})=1]=\\!F(\\theta_{a}-\\theta_{a^{\\prime\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\quad>\\operatorname*{max}{F(\\theta_{a}-\\theta_{a^{\\prime}})},F(\\theta_{a^{\\prime}}-\\theta_{a^{\\prime\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\operatorname*{max}{\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1]},\\operatorname*{Pr}[T_{\\theta}(a^{\\prime},a^{\\prime\\prime})=1]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and thus $T_{\\theta}$ is strongly stochastically transitive for all $\\theta$ with distinct coordinates which happens surely as $\\nu$ is non-atomic. Finally, since the distribution on $\\theta$ is exchangeable on each coordinate, E [E $[T_{\\theta}(a,a^{\\prime})]]=0$ for all $a,a^{\\prime}$ . \u25a1 ", "page_idx": 15}, {"type": "text", "text": "Proof of proposition 2.5. First given $\\theta\\in\\Theta$ , for all distinct $a,a^{\\prime}\\in\\mathcal{A}$ , if the rank of $a$ is higher than $a^{\\prime}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1]=h_{\\eta}(\\theta(a^{\\prime})-\\theta(a)+1)-h_{\\eta}(\\theta(a^{\\prime})-\\theta(a))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{h_{\\eta}(x)=\\frac{x}{1-\\exp\\left(-\\eta x\\right)}}\\end{array}$ by Busa-Fekete et al. [7]. ", "page_idx": 15}, {"type": "text", "text": "Claim B.1. For any $\\eta>0$ and $x\\in\\mathbb{Z}_{>0}$ , the difference $h_{\\eta}(x+1)-h_{\\eta}(x)$ is increasing and larger than 1/2 where \u210e\ud835\udf02(\ud835\udc65) = 1\u2212exp(\u2212\ud835\udf02\ud835\udc65) . ", "page_idx": 15}, {"type": "text", "text": "By claim B.1, $\\mathrm{Pr}[T_{\\theta}(a,a^{\\prime})\\,=\\,1],\\mathrm{Pr}[T_{\\theta}(a^{\\prime},a^{\\prime\\prime})\\,=\\,1]\\,>\\,1/2$ implies that $\\theta(a^{\\prime})\\,-\\,\\theta(a)\\,>\\,0$ and $\\theta(a^{\\prime\\prime})-\\theta(a^{\\prime})>0$ . Thus, $\\theta(a^{\\prime\\prime})-\\theta(a)>\\mathrm{max}(\\theta(a^{\\prime\\prime})-\\theta(a^{\\prime}),\\theta(a^{\\prime\\prime})-\\theta(a^{\\prime}))$ , and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime\\prime})=1]=h(\\theta(a^{\\prime\\prime})-\\theta(a)+1)-h(\\theta(a^{\\prime\\prime})-\\theta(a))}\\\\ &{>\\operatorname*{max}h(\\theta(a^{\\prime\\prime})-\\theta(a^{\\prime})+1)-h(\\theta(a^{\\prime\\prime})-\\theta(a^{\\prime})),h(\\theta(a^{\\prime})-\\theta(a)+1)-h(\\theta(a^{\\prime})-\\theta(a))}\\\\ &{=\\operatorname*{max}\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1],\\operatorname*{Pr}[T_{\\theta}(a^{\\prime},a^{\\prime\\prime})=1]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality is due to claim B.1. Therefore, $T_{\\theta}$ is strongly stochastically transitive for all $\\theta$ . Finally, E [E $ [T_{\\theta}(a,a^{\\prime})]]=0$ for all $a,a^{\\prime}$ since $\\theta$ is an uniform distribution on rankings. \u25a1 ", "page_idx": 16}, {"type": "text", "text": "Proof of claim B.1. We first prove that the function \u210e\ud835\udf02(\ud835\udc65) = 1\u2212exp\ud835\udc65(\u2212\ud835\udf02\ud835\udc65) is increasing and strictly convex on $x\\,\\geq\\,0$ . Because $\\begin{array}{r}{h_{\\eta}(x)\\,=\\,\\frac{1}{\\eta}h_{1}(\\eta x)}\\end{array}$ , for all $\\eta,x$ , it is sufficient to consider $\\eta=1$ . First, $\\begin{array}{r}{h_{1}^{\\prime}(x)\\;=\\;\\frac{1-(x+1)e^{-x}}{(1-e^{-x})^{2}}\\;>\\;0}\\end{array}$ , so $h_{1}$ is increasing. Second, as $\\begin{array}{r}{h_{1}^{\\prime\\prime}(x)\\;=\\;\\frac{e^{-x}\\left((x-2)+(x+2)e^{-x}\\right)}{(1-e^{-x})^{3}}}\\end{array}$ , to show $h_{1}^{\\prime\\prime}(x)\\,>\\,0$ for all $x\\,>\\,0$ , it is sufficient to show that $g(x)\\,=\\,(x-2)+(x+2)e^{-x}\\,>\\,0$ . Because $g(0)=0$ and $g^{\\prime}(x)=1-(x+1)e^{-x}>0,g(x)$ $g(x)>0$ for all $x>0$ . Therefore, $h_{1}$ is strictly convex. On the other hand, $h_{\\eta}(x{+}2){-}h_{\\eta}(x{+}1)>h_{\\eta}(x{+}1){-}h_{\\eta}(x)$ for all $x$ by convexity, and $h_{\\eta}(2){-}h_{\\eta}(1)=$ $\\begin{array}{r}{\\frac{1}{1+e^{-\\eta}}>\\frac{1}{2}}\\end{array}$ which completes the proof. \u25a1 ", "page_idx": 16}, {"type": "text", "text": "C Proofs in Section 3 and 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Uniform dominance from Bayesian SST ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of lemma 4.2. With a prior similar assumption for Bayesian SST model, we only need to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=1]>\\mathrm{Pr}[S(a^{\\prime\\prime},a)=1\\mid S(a,a^{\\prime})=1],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the other case $\\mathrm{Pr}[S(a^{\\prime\\prime},a^{\\prime})=-1\\mid S(a,a^{\\prime})=-1]>\\mathrm{Pr}[S(a^{\\prime\\prime},a)=-1\\mid S(a,a^{\\prime})=-1]$ follows by symmetry. To prove eq. (5), we can rewrite the conditional probability in expectations of $T_{\\theta}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{P}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=1\\mid}&\\\\ &{=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\mathbb{P}[\\Gamma_{\\theta}(a^{\\prime\\prime},a^{\\prime})=1,T_{e}(a,a^{\\prime})=1\\mid\\theta]d P_{\\theta}}\\\\ &{=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Claim C.1. For any strongly stochastically transitive $T_{\\theta}$ on $\\mathcal{A}$ , and distinct $a,a^{\\prime},a^{\\prime\\prime}\\in\\mathcal{A}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[T_{\\theta}(a,a^{\\prime})\\mid\\theta\\right]\\mathbb{E}\\left[T_{\\theta}(a^{\\prime\\prime},a^{\\prime})\\mid\\theta\\right]>\\mathbb{E}\\left[T_{\\theta}(a,a^{\\prime})\\mid\\theta\\right]\\mathbb{E}\\left[T_{\\theta}(a^{\\prime\\prime},a)\\mid\\theta\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With claim C.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{Pr}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=1]=\\frac{1}{2}\\int\\mathbb{E}\\left[T_{\\theta}(a^{\\prime\\prime},a^{\\prime})\\mid\\theta\\right]\\mathbb{E}\\left[T_{\\theta}(a,a^{\\prime})\\mid\\theta\\right]+1d P_{\\Theta}}}\\\\ {{\\displaystyle>\\frac{1}{2}\\int\\mathbb{E}\\left[T_{\\theta}(a^{\\prime\\prime},a)\\mid\\theta\\right]\\mathbb{E}\\left[T_{\\theta}(a,a^{\\prime})\\mid\\theta\\right]+1d P_{\\Theta}=\\operatorname*{Pr}[S(a^{\\prime\\prime},a)=1\\mid S(a,a^{\\prime})=1].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof of eq. (5), and thus the uniform dominance. ", "page_idx": 16}, {"type": "text", "text": "Proof of claim C.1. We let $Q(\\alpha,\\alpha^{\\prime}):=\\mathbb{E}\\left[T_{\\theta}(\\alpha,\\alpha^{\\prime})\\mid\\theta\\right]=2\\operatorname*{Pr}[T_{\\theta}(\\alpha,\\alpha^{\\prime})=1\\mid\\theta]-1$ for all $\\alpha,\\alpha^{\\prime}$ .   \nNote that $Q(\\alpha,{\\alpha^{\\prime}})>0$ if and only if $\\operatorname*{Pr}[T_{\\theta}(\\alpha,\\alpha^{\\prime})=1\\mid\\theta]>1/2$ and $\\begin{array}{r}{Q(\\alpha,\\alpha^{\\prime})=-Q(\\alpha^{\\prime},\\alpha)}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "By symmetry, let $\\mathcal{Q}(a,a^{\\prime})>0$ . It is sufficient to show that ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ(a^{\\prime\\prime},a^{\\prime})>Q(a^{\\prime\\prime},a).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $Q(a^{\\prime},a^{\\prime\\prime})\\;>\\;0$ , by definition $2.1\\ Q(a,a^{\\prime\\prime})\\;>\\;Q(a^{\\prime},a^{\\prime\\prime})\\;>\\;0$ so $Q(a^{\\prime\\prime},a^{\\prime})\\;>\\;Q(a^{\\prime\\prime},a)$ . Now consider $\\mathcal{Q}(a^{\\prime},a^{\\prime\\prime})\\,<\\,0$ . If $Q(a^{\\prime\\prime},a)\\,<\\,0$ , $\\ensuremath{\\mathcal{Q}}(a^{\\prime\\prime},a^{\\prime})\\,>\\,0\\,>\\,Q(a^{\\prime\\prime},a)$ . If $Q(a^{\\prime\\prime},a)\\,>\\,0$ , we have $Q(a^{\\prime\\prime},a)>0,Q(a,a^{\\prime})>0$ , and thus $\\ensuremath{\\mathcal{Q}}(a^{\\prime\\prime},a^{\\prime})>\\ensuremath{\\mathcal{Q}}(a^{\\prime\\prime},a)$ by definition 2.1 \u25a1 ", "page_idx": 17}, {"type": "text", "text": "C.2 Uniform dominance and weak notions of stochastic transitivity ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "There are weaker forms of stochastic transitivity, raising the question of whether they are sufficient for uniform dominance as in lemma 4.2. We show that general weak stochastic transitivity is not sufficient. Additionally, we show that although the noisy sorting model from [5] is only weakly stochastically transitive but does not satisfy definition 2.1, it exhibits uniform dominance. ", "page_idx": 17}, {"type": "text", "text": "Definition C.2 ([13]). A stochastic comparison function, $T:{\\mathcal{A}}^{2}\\rightarrow\\{-1,1\\}$ , is weakly stochastically transitive if for all $a,a^{\\prime},a^{\\prime\\prime}\\in\\mathcal{A}$ with $\\operatorname*{Pr}[T(a,a^{\\prime})=1]>1/2$ and $\\operatorname*{Pr}[T(a^{\\prime},a^{\\prime\\prime})=1]>1/2$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T(a,a^{\\prime\\prime})=1]>1/2.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Compared to definition 2.1, the weak stochastic transitivity only require the item $a$ is favorable than $a^{\\prime\\prime}$ . Below we provide a simple weakly stochastically transitive example with a prior similar property that does not satisfy the uniform dominance in eq. (5). ", "page_idx": 17}, {"type": "text", "text": "Example C.3. Consider the set of three items and $\\Theta$ consists of all ranking on $\\mathcal{A}$ with uniform prior where $\\theta$ maps each items to its value. Given $\\theta\\in\\Theta$ so that if $\\theta(a)>\\theta(a^{\\prime})^{\\mathsf{\\bar{\\Phi}}}>\\theta(a^{\\prime\\prime})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[T_{\\theta}(a,a^{\\prime})=1]=\\mathrm{Pr}[T_{\\theta}(a^{\\prime},a^{\\prime\\prime})=1]=0.9\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the model is weakly stochastically transitive, because an item with a larger value is more favorable and the weak stochastic transitivity is reduced to transitivity on the values. However, the model is not strongly stochastically transitive, because $\\mathrm{Pr}[T_{\\theta}(a,a^{\\prime\\prime})\\stackrel{.}{=}1]\\,=\\,0.6\\,<\\,\\mathrm{max}\\{\\mathrm{Pr}[(T(a,a^{\\prime})\\,=\\,$ 1], $\\operatorname*{Pr}[(T(a^{\\prime},a^{\\prime\\prime})=1]]\\}=0.9$ . Finally, as the rank $\\theta$ has a uniform prior, the model satisfies a prior similar assumption. ", "page_idx": 17}, {"type": "text", "text": "To conclude the example, we show that eq. (5) does not hold for the above model. By direct computation over all six possible ranking $\\theta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathrm{Pr}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=1]}\\\\ &{=\\\\!\\!\\frac{1}{2}\\displaystyle\\int\\mathbb{E}\\left[T_{\\theta}(a^{\\prime\\prime},a^{\\prime})\\mid\\theta\\right]\\mathbb{E}\\left[T_{\\theta}(a,a^{\\prime})\\mid\\theta\\right]+1d P_{\\Theta}}\\\\ &{=\\!\\!\\frac{1}{2}\\left(1-\\frac{64}{6}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "but $\\begin{array}{r}{\\operatorname*{Pr}[S(a^{\\prime\\prime},a)=1\\mid S(a,a^{\\prime})=1]=\\frac{1}{2}\\left(1+\\frac{64}{6}\\right)\\!.}\\end{array}$ . Therefore, we have $\\mathrm{Pr}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=$ $1]<\\operatorname*{Pr}[S(a^{\\prime\\prime},a)=1\\mid S(a,a^{\\prime})=1]$ , and show that eq. (5) does not hold. ", "page_idx": 17}, {"type": "text", "text": "Though the above example shows that weak stochastic transitivity is not sufficient.5 Below we show a popular weakly stochastically transitive model in Braverman and Mossel [5] has uniform dominance as in lemma 4.2. ", "page_idx": 17}, {"type": "text", "text": "Example C.4. Let $\\Theta$ be the set of rankings on $\\mathcal{A}$ and $\\eta\\,>\\,0$ be a parameter. Given a uniformly distributed reference ranking $\\theta\\in\\Theta$ , the noise ranking model [5] ensures that for all $\\theta(a)>\\theta(a^{\\prime})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[T_{\\theta}(a,a^{\\prime})=1]=\\frac{1}{2}+\\eta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the above model does not satisfy the strict inequality in definition 2.1, but by direct computation, $\\begin{array}{r}{\\operatorname*{Pr}[S(a^{\\prime\\prime},a^{\\prime})=1\\mid S(a,a^{\\prime})=\\Bar{1}]=\\frac{1}{2}\\left(1+\\frac{4\\gamma^{2}}{3}\\right)}\\end{array}$ and $\\mathrm{Pr}[S(a^{\\prime\\prime},a)=1\\mid S(a,a^{\\prime})=1]=$ $\\textstyle{\\frac{1}{2}}\\left(1-{\\frac{4\\gamma^{2}}{3}}\\right)$ , which satisfies lemma 4.2. ", "page_idx": 17}, {"type": "text", "text": "C.3 Symmetrically strongly truthful from uniform dominance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of lemma 4.3. Suppose $S_{i}=1$ . Because $\\operatorname*{Pr}[S_{j}\\,=\\,1|S_{i}\\,=\\,1]\\,>\\,\\operatorname*{Pr}[S_{k}\\,=\\,1|S_{i}\\,=\\,1]$ , $\\mathrm{{Pr}}[S_{j}=$ $-1|S_{i}=1|<\\operatorname*{Pr}[S_{k}=-1|S_{i}=1]$ . Therefore, arg $\\begin{array}{r l}{\\operatorname*{mix}_{\\hat{s}\\in\\{-1,1\\}}\\operatorname*{Pr}[S_{j}=\\hat{s}|S_{i}=1]-\\operatorname*{Pr}[S_{k}=\\hat{s}_{i}|\\bar{S}_{i}=}&{{}}\\end{array}$ $1]=1$ . Identical argument holds for the case of $S_{i}=-1$ which completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Additionally, the expected payment of truth-telling is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{\\displaystyle3\\left[U^{B P P}(S_{i},S_{j},S_{k})\\right]=\\sum_{a}\\mathrm{Pr}[S_{i}=s_{i}]\\displaystyle\\sum_{s_{j},s_{k}}\\mathrm{Pr}[S_{j}=s_{j},S_{k}=s_{k}\\ |\\ S_{i}=s_{i}]U^{B P P}(s_{i},s_{j},s_{k})}}\\\\ {{\\displaystyle=2\\sum_{a}\\mathrm{Pr}[S_{i}=s_{i}]\\displaystyle\\sum_{s_{j},s_{k}}\\mathrm{Pr}[S_{j}=s_{j},S_{k}=s_{k}\\ |\\ S_{i}=s_{i}]({\\bf1}[s_{i}=s_{k}]-{\\bf1}[s_{i}=s_{k}])}}\\\\ {{\\displaystyle=2\\sum_{a}\\mathrm{Pr}[S_{i}=s_{i}]\\left(\\mathrm{Pr}[S_{j}=s_{i}\\ |\\ S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s_{i}\\ |\\ S_{i}=s_{i}]\\right)}}\\\\ {{\\displaystyle>0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last inequality holds due to definition 4.1. ", "page_idx": 18}, {"type": "text", "text": "Proof of lemma 4.4. As $\\sigma$ is uninformed, let $\\mu(s)\\,=\\,\\sigma(s,s)\\,=\\,\\sigma(-s,s)$ and $\\mu(-s)\\,=\\,\\sigma(s,-s)\\,=$ $\\sigma(-s,-s)$ for all $s$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n3\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})\\mid S_{i}=s_{i}\\right]=\\sum_{\\hat{s}_{j},\\hat{s}_{k}}\\mu(\\hat{s}_{j})\\mu(\\hat{s}_{k})U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})=\\sum_{\\hat{s}_{j},\\hat{s}_{k}}\\mu(\\hat{s}_{j})\\mu(\\hat{s}_{k})(\\hat{s}_{i}\\hat{s}_{j}-\\hat{s}_{i}\\hat{s}_{k})=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first equality holds as the reports are independent of signals. ", "page_idx": 18}, {"type": "text", "text": "Proof of lemma 4.5. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P,\\sigma}\\left[U^{B P P}(\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{=\\displaystyle\\sum_{s_{j},s_{k},\\hat{s}_{j},\\hat{s}_{k}}\\mathrm{Rr}[S_{j}=s_{j},S_{k}=s_{k}\\mid S_{i}=s_{i}]\\sigma(s_{j},\\hat{s}_{j})\\sigma(s_{k},\\hat{s}_{k})U^{B P P}(\\hat{s}_{i},\\hat{s}_{j},\\hat{s}_{k})}\\\\ &{=2\\displaystyle\\sum_{s_{j},s_{k},\\hat{s}_{j},\\hat{s}_{k}}\\mathrm{Rr}[S_{j}=s_{j},S_{k}=s_{k}\\mid S_{i}=s_{i}]\\sigma(s_{j},\\hat{s}_{j})\\sigma(s_{k},\\hat{s}_{k})\\left(\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{j}]-\\mathbf{1}[\\hat{s}_{i}=\\hat{s}_{k}]\\right)}\\\\ &{\\phantom{=}\\displaystyle(\\mathrm{br}\\mathcal{S}_{j}\\times\\mathrm{br}[S_{j}=s_{j}\\mid S_{i}=s_{i}]\\sigma(s_{j},\\hat{s}_{j}){\\mathbb{I}\\left[\\hat{s}_{i}=\\hat{s}_{j}\\right]}-2\\displaystyle\\sum_{s_{k},\\hat{s}_{k}}\\mathrm{Pr}[S_{k}=s_{k}]\\;S_{i}=s_{i}]\\sigma(s_{k},\\hat{s}_{k}){\\mathbb{I}\\left[\\hat{s}_{i}=\\hat{s}_{j}\\right]}}\\\\ &{=2\\displaystyle\\sum_{s_{j},s_{j}}(\\mathrm{Pr}[S_{j}=s\\mid S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s\\mid S_{i}=s_{i}])\\sigma(s_{j},\\hat{s}_{k}){\\mathbb{I}\\left[\\hat{s}_{i}=\\hat{s}_{j}\\right]}\\mathrm{~(renaming~dammy~vac~}}\\\\ &{=2\\displaystyle\\sum_{s_{j}=\\hat{s}}(\\mathrm{Pr}[S_{j}=s\\mid S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s\\mid S_{i}=s_{i}])\\sigma(s_{k},\\hat{s}_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\delta\\,=\\,\\operatorname*{Pr}\\!\\left[S_{j}\\,=\\,s_{i}\\ \\big\\mid\\,S_{i}\\,=\\,s_{i}\\right]\\,-\\,\\operatorname*{Pr}\\!\\left[S_{k}\\,=\\,s_{i}\\ \\big\\mid\\,S_{i}\\,=\\,s_{i}\\right]\\,>\\,0$ , because $S_{j}$ uniformly dominates $S_{k}$ for $S_{i}$ . Additionally, $\\operatorname*{Pr}[S_{j}\\,=\\,-s_{i}\\;\\mid\\;S_{i}\\,=\\,s_{i}]\\,-\\operatorname*{Pr}[S_{k}\\,=\\,-s_{i}\\;\\mid\\,S_{i}\\,=\\,s_{i}]$ ] = 1 \u2212Pr[\ud835\udc46\ud835\udc57 = \ud835\udc60\ud835\udc56 | \ud835\udc46\ud835\udc56 = $s_{i}]-1+\\operatorname*{Pr}[S_{k}=s_{i}\\mid S_{i}={\\bar{s_{i}}}]=-\\delta$ . We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{}&{\\;\\;\\;\\mathbb{B}_{P,\\sigma}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})~|~S_{i}=s_{i}\\right]}\\\\ &{=2\\displaystyle\\sum_{s}(\\mathrm{Pr}[S_{j}=s~|~S_{i}=s_{i}]-\\mathrm{Pr}[S_{k}=s~|~S_{i}=s_{i}])\\sigma(s,\\hat{s}_{i})}\\\\ &{=2\\delta\\left(\\sigma(s_{i},\\hat{s}_{i})-\\sigma(-a,\\hat{s}_{i})\\right),}\\\\ &{\\;\\;\\;-1,1\\right)\\mathbb{B}_{P,\\sigma}\\left[U^{B P P}(\\hat{s}_{i},\\hat{S}_{j},\\hat{S}_{k})~|~S_{i}=s_{i}\\right]\\;=\\;\\arg\\operatorname*{max}_{\\hat{s}_{i}\\in\\{-1,1\\}}\\left\\{\\sigma(s_{i},\\hat{s}_{i})-\\sigma(-s_{i},\\hat{s}_{i})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so arg max\ud835\udc60\u02c6\ud835\udc56\u2208 which completes the proof. \u25a1 ", "page_idx": 18}, {"type": "text", "text": "D Proofs in Section 5.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before diving into the proof, we introduce some notations. We further introduce Ising models with bias parameter $\\alpha\\in\\mathbb{R}_{\\geq0}^{V^{}}$ R\ud835\udc49\u22650 in addition to \ud835\udf37where ", "page_idx": 18}, {"type": "equation", "text": "$$\nH(\\mathbf{s})=\\sum_{i,j\\in V}\\beta_{i,j}s_{i}s_{j}+\\sum_{i\\in V}\\alpha_{i}s_{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\operatorname*{Pr}_{\\alpha,\\beta}[\\mathbf{S}=\\mathbf{s}]\\propto\\exp(H(\\mathbf{s}))$ , for all configuration s. Given $i\\in V$ , let the expectation and ratio be ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{i}(\\alpha,\\beta)=\\mathbb{E}_{\\alpha,\\beta}\\left[S_{i}\\right]=\\operatorname*{Pr}_{\\alpha,\\beta}\\left[S_{i}=1\\right]-\\operatorname*{Pr}_{\\alpha,\\beta}\\left[S_{i}=-1\\right]\\;\\mathrm{and}\\;\\rho_{i}(\\alpha,\\beta)=\\frac{\\operatorname*{Pr}_{\\alpha,\\beta}\\left[S_{i}=1\\right]}{\\operatorname*{Pr}_{\\alpha,\\beta}\\left[S_{i}=-1\\right]}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "respectively which are monotone to each other. We will omit $\\alpha,\\beta$ when clear. Given a subset $U\\subseteq V$ , $\\mathbf{s}_{U}\\,\\in\\,\\{-1,\\dot{1}\\}^{U}$ is a configuration over the nodes in $U$ , and $\\mathbf{s}_{U}=1$ if $x_{\\iota}=1$ for all $\\iota\\in U$ . We write $\\mathrm{Pr}[\\cdot|{\\bf S}_{U}={\\bf s}_{U}]$ , $\\nu_{i|\\mathbf{S}_{U}=\\mathbf{s}_{U}}$ , and $\\rho_{i}|\\mathbf{S}_{U}{=}\\mathbf{s}_{U}$ for the conditional probability, expectation and ratio when the configuration in $U$ is fixed as specified by $\\mathbf{s}_{U}$ . ", "page_idx": 19}, {"type": "text", "text": "A lower bound for LHS Informally, we want to lower bound the correlation between adjacent $i$ and $j$ (friends). Note that as we remove edges (setting coordinates of $_\\beta$ to zeros), the correlation should decrease, and the smallest correlation between neighboring nodes $i$ and $j$ happens when $E=\\{(i,j)\\}$ . Lemma D.2 formalizes this idea using the following monotone inequality [44, Theorem 17.2]. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.1 (Griffiths\u2019 inequality). For any $i\\in\\textit{V}$ , $\\nu_{i}(\\alpha,\\beta)\\,=\\,\\mathbb{E}_{\\alpha,\\beta}\\left[S_{i}\\right]$ is non-negative and non-decreasing in all $\\beta_{j,k}\\geq0$ and $\\alpha_{j}\\geq0$ with $j,k\\in V$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma D.2. Given $V$ and $i,j\\in V$ , for all $\\alpha,\\beta$ , and $\\beta^{\\prime}$ , i $f\\beta_{e}^{\\prime}=\\beta_{e}$ when $e\\,=\\,(i,j)$ and $\\beta_{e}^{\\prime}=0$ otherwise, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{i|S_{j}=1}(\\alpha,\\beta)\\geq\\nu_{i|S_{j}=1}(\\alpha,\\beta^{\\prime})\\,a n d\\,\\rho_{i|S_{j}=1}(\\alpha,\\beta)\\geq\\rho_{i|S_{j}=1}(\\alpha,\\beta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, note that we can write the conditional expectation $\\mathbb{E}_{\\alpha,\\beta}\\left[S_{i}\\mid S_{j}=1\\right]$ as marginal expectation. Formally, consider $\\alpha^{\\eta}$ so that $\\alpha_{\\iota}^{\\eta}\\ =\\ \\alpha_{\\iota}$ if $\\iota\\ \\neq\\ j$ and $\\alpha_{j}^{\\eta}~=~\\alpha_{j}\\,+\\eta$ . Because $\\eta\\ \\to\\ \\alpha^{\\eta}$ is non-decreasing, $\\eta~\\rightarrow~\\nu_{i}(\\alpha^{\\eta},\\beta)$ is non-decreasing by theorem D.1. In addition, $\\operatorname*{Pr}_{\\alpha^{\\eta},\\beta}[S_{i}\\mid S_{j}=s]=\\operatorname*{Pr}_{\\alpha,\\beta}[S_{i}\\mid S_{j}=s]$ for all $s$ , and $\\mathrm{Pr}_{\\alpha^{\\eta},\\beta}[S_{j}=-1]=O(e^{-2\\eta})$ , so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{i|S_{j}=1}(\\alpha,\\beta)=\\mathbb{E}_{\\alpha,\\beta}[S_{i}\\mid S_{j}=1]=\\operatorname*{lim}_{\\eta\\to+\\infty}\\nu_{i}(\\alpha^{\\eta},\\beta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{i|S_{j}=1}(\\alpha,\\beta^{\\prime})=\\mathbb{E}_{\\alpha,\\beta^{\\prime}}[S_{i}\\mid S_{j}=1]=\\operatorname*{lim}_{\\eta\\to+\\infty}\\nu_{i}(\\alpha^{\\eta},\\beta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, consider $\\beta^{\\lambda}$ so that $\\beta_{e}^{\\lambda}=\\beta_{e}$ if $e\\neq(i,j)$ and $\\beta_{i,j}^{\\lambda}=\\beta_{i,j}+\\lambda$ . By theorem D.1, $\\nu_{i}(\\alpha^{\\eta},\\beta^{\\lambda})$ is non-decreasing in $\\lambda$ for all $\\eta$ . Because $\\boldsymbol{\\beta}^{0}=\\boldsymbol{\\beta}^{\\prime}$ and $\\beta^{1}=\\beta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{i|S_{j}=1}(\\alpha,\\beta^{\\prime})=\\operatorname*{lim}_{\\eta\\rightarrow+\\infty}\\nu_{i}(\\alpha^{\\eta},\\beta^{\\prime})\\leq\\operatorname*{lim}_{\\eta\\rightarrow+\\infty}\\nu_{i}(\\alpha^{\\eta},\\beta)=\\nu_{i|S_{j}=1}(\\alpha,\\beta)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because \ud835\udf0c\ud835\udc56= $\\begin{array}{r}{\\rho_{i}=\\frac{1+\\nu_{i}}{1-\\nu_{i}}}\\end{array}$ is monotone in $\\nu_{i}$ , the second part follows. ", "page_idx": 19}, {"type": "text", "text": "Given $\\beta^{\\prime}$ defined in lemma D.2, by some direct computation with $\\alpha=0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho_{i|S_{j}=1}(\\alpha,\\beta)\\geq\\rho_{i|S_{j}=1}(\\alpha,\\beta^{\\prime})=e^{2\\alpha_{i}+2\\beta_{i,j}}=e^{2\\beta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "An upper bound for RHS Now, we need to upper bound the correlation between non-adjacent $i$ and $k$ (non-friends). We will use Weitz\u2019s self-avoiding walks reduction [65] to upper bound the correlation on general graph $G$ by the correlation on trees. ", "page_idx": 19}, {"type": "text", "text": "Given a general graph $G$ , and an arbitrary node $i$ , we can construct the Self Avoiding Walk Tree of $G$ rooted at $i$ , denoted $T_{S A W}(G,i)$ , so that $\\operatorname*{Pr}[S_{i}=1\\mid\\mathbf{S}_{U}=\\mathbf{s}_{U}]$ is the same in $G$ as in the tree. We outline the construction. $T_{S A W}(G,i)$ enumerates all self-avoiding walks in $G$ starting at $i$ which terminates when it revisits a previous node (closes a cycle). Then, $T_{S A W}(G,i)$ introduces a leaf with a certain boundary condition. The self-avoiding walk never revisits a node immediately, so there all the leaves with fixed boundary conditions are at least three hops away from node $i$ . Note that if $G$ has maximum degree $d,T_{S A W}$ is a $d$ -ary tree. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.3 ([65]). For any $\\alpha,\\,\\beta,$ , node $i\\in V$ , and configuration $\\mathbf{s}_{U}$ on $U\\subset V$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\alpha,\\beta}\\left[S_{i}=1\\mid\\mathbf{S}_{U}=\\mathbf{s}_{U}\\right]=\\operatorname*{Pr}_{T_{S A W}\\left(G,i\\right)}\\left[S_{i}=1\\mid\\mathbf{S}_{U}=\\mathbf{s}_{U}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First, with the above theorem, we have $\\nu_{i|S_{k}=1}(\\alpha,\\beta)=\\mathbb{E}_{\\alpha,\\beta}[S_{i}\\mid S_{k}=1]=\\mathbb{E}_{T_{S A W}(G,i)}[S_{i}\\mid S_{k}=1]$ . By the monotone property in theorem D.1, setting all two-hop neighbors $U$ in $T_{S A W}(G,i)$ to 1 (recalled that any boundary conditions for $T_{S A W}(G,i)$ being at least three hops away) increases the conditional expectation, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{T_{S A W}(G,i)}\\left[S_{i}\\mid S_{k}=1\\right]\\le\\mathbb{E}_{T_{S A W}(G,i)}\\left[S_{i}\\mid S_{U}=1,S_{k}=1\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $T$ be the tree by truncating $T_{S A W}(G,i)$ at level 2. By the Markov property of Ising models, the expectation is equal to the expectation on $T$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\alpha,\\beta}\\left[S_{i}\\mid S_{k}=1\\right]\\le\\mathbb{E}_{T_{S A W}\\left(G,i\\right)}\\left[S_{i}\\mid\\mathbf{S}_{U}=1\\right]=\\mathbb{E}_{T}\\left[S_{i}\\mid\\mathbf{S}_{U}=1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we can recursively compute the probability ratio $\\rho_{i}$ (and thus expectation $\\nu_{i}$ ) on trees. Specifically, given a rooted tree $T^{\\prime}$ , we define $\\rho_{T^{\\prime}}$ as the ratio of probabilities for the root to be $+1$ and $^{-1}$ respectively, and $\\rho_{T^{\\prime}}|\\mathbf{S}_{U}{=}\\mathbf{s}_{U}$ for the ratio of conditional probabilities. As stated in the following lemma, it is well known (see, for example, [22]) that the ratio of each node can be computed recursively over the children\u2019s ratio. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.4. Given a tree $T$ rooted at \ud835\udc56with parameter $(\\alpha,\\beta)$ and boundary condition $\\mathbf{s}_{U}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{T|\\mathbf{S}_{U}=\\mathbf{s}_{U}}=e^{2\\alpha_{i}}\\prod_{l=1}^{d}\\frac{\\rho_{T_{l}|\\mathbf{S}_{U}=\\mathbf{s}_{U}}e^{2\\beta_{i,j_{l}}}+1}{e^{2\\beta_{i,j_{l}}}+\\rho_{T_{l}|\\mathbf{S}_{U}=\\mathbf{s}_{U}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $j_{1},\\ldots,j_{d}$ are children of \ud835\udc56and $T_{l}$ is the subtree rooted at $j_{l}$ for all \ud835\udc59. ", "page_idx": 20}, {"type": "text", "text": "By the monotone property in theorem D.1, the maximum of right-hand side of eq. (7) happens when $T$ is a complete $d$ -ary tree with $\\beta=\\overline{{\\beta}}$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{i|S_{k}=1}(\\alpha,\\beta)\\leq\\left(\\frac{e^{2(d+1)\\overline{{\\beta}}}+1}{e^{2\\overline{{\\beta}}}+e^{2d\\overline{{\\beta}}}}\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, with eqs. (6) and (8), we have \ud835\udf0c\ud835\udc56|\ud835\udc46\ud835\udc57=1(\ud835\udf36, \ud835\udf37) \u2265\ud835\udc522\ud835\udefd \u2265  \ud835\udc52\ud835\udc5222(\ud835\udefd\ud835\udc51++1\ud835\udc52)2\ud835\udefd\ud835\udc51+\ud835\udefd1 which implies eq. (4). ", "page_idx": 20}, {"type": "text", "text": "Remark D.5. Note that for any graph $G$ there exists small enough ${\\overline{{\\beta}}},{\\underline{{\\beta}}}$ so that the condition in theorem 5.1 is satisfied, because the inequality become equality when $\\overline{{\\beta}}=\\underline{{\\beta}}=0$ , and $\\begin{array}{r}{\\frac{\\partial}{\\partial\\beta}\\frac{2\\beta}{d}>0=}\\end{array}$ \ud835\udf15\ud835\udf15\ud835\udefdln \ud835\udc52\ud835\udc5222(\ud835\udefd\ud835\udc51++1\ud835\udc52)2\ud835\udefd\ud835\udc51+\ud835\udefd1. ", "page_idx": 20}, {"type": "text", "text": "The bound between $_\\beta$ and $d$ is necessary as shown in fig. 3. On the other hand, by the Markov property of the Ising model, the majority of all neighbor\u2019s signals is a sufficient statistic, and we can show the majority of all neighbor\u2019s signals are uniformly dominant to a non-neighbor\u2019s signal. Therefore, we can get a symmetrically strongly truthful mechanism by replacing $j$ \u2019s reports with the majority of reports from $i$ \u2019s neighbors. ", "page_idx": 20}, {"type": "text", "text": "E Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The sufficient condition is done by lemma 4.3, because ", "page_idx": 20}, {"type": "equation", "text": "$(\\lambda>0)$ ", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\delta_{i}\\in\\{-1,1\\}}{\\mathrm{arg\\,max}}\\,\\mathbb{E}\\left[\\lambda U^{B P P}(\\hat{s}_{i},S_{j},S_{k})+\\mu(S_{j},S_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{=\\underset{\\delta_{i}\\in\\{-1,1\\}}{\\mathrm{arg\\,max}}\\,\\mathbb{E}\\left[\\lambda U^{B P P}(\\hat{s}_{i},S_{j},S_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{\\quad\\hat{s}_{i}\\!\\in\\!\\{-1,1\\}}\\\\ &{=\\underset{\\delta_{i}\\in\\{-1,1\\}}{\\mathrm{arg\\,max}}\\,\\mathbb{E}\\left[U^{B P P}(\\hat{s}_{i},S_{j},S_{k})\\mid S_{i}=s_{i}\\right]}\\\\ &{\\quad\\hat{s}_{i}\\!\\in\\!\\{-1,1\\}}\\\\ &{=\\!s_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the necessary, given $U$ , define $D(s_{j},s_{k})\\;=\\;{\\textstyle\\frac{1}{2}}\\left(U(1,s_{j},s_{k})-U(-1,s_{j},s_{k})\\right)$ and $\\mu(s_{j},s_{k})\\;=\\;$ $\\begin{array}{r l}{\\lefteqn{\\frac{1}{2}(U(1,s_{j},s_{k})+U(-1,s_{j},s_{k}))}\\quad}&{{}}\\end{array}$ for all $s_{j}$ and $s_{k}$ in $\\{-1,1\\}$ . Hence ", "page_idx": 20}, {"type": "equation", "text": "$$\nU(s_{i},s_{j},s_{k})=s_{i}\\cdot D(s_{j},s_{k})+\\mu(s_{j},s_{k}),\\forall s_{i},s_{j},s_{k}\\in\\{-1,1\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/09b9bad528609ddfedbf2a8906e4f34508dfbe9068ee1165ae8157296d6f31e7.jpg", "img_caption": ["Figure 3: As fixing any $\\beta,\\overline{\\beta}$ , we can construct a simple graph with $V\\,=\\,\\bigl\\{\\nu_{0},\\dots,\\nu_{n-1}\\bigr\\}$ and $E\\,=$ $\\{(\\nu_{0},\\nu_{l}),(\\nu_{l},\\nu_{n-1}):l={\\overline{{1}}},\\ldots,n-2\\}$ where agent $\\nu_{0}$ and $\\nu_{n-1}$ are not connected but share $n-2$ common friends. We can show that the correlation between $S_{0}$ and $S_{n-1}$ converge to 1 as the number of common friends $d$ increases, while the correlation between $S_{0}$ and $S_{1}$ is bounded away from 1. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Given a joint distribution satisfying definition 4.1, we let $p^{s_{i}}(s_{j},s_{k})=\\operatorname*{Pr}[S_{j}=s_{j},S_{k}=s_{k}\\mid S_{i}=s_{i}]$ and additionally write $p^{s_{i}}=\\left[p^{s_{i}}(1,1)\\begin{array}{c c}{p^{s_{i}}(1,-1)}\\\\ {p^{s_{i}}(-1,1)}\\end{array}\\right].$ . Then definition 4.1 ensures that ", "page_idx": 21}, {"type": "equation", "text": "$$\np^{1}(1,-1)>p^{1}(-1,1)\\mathrm{~and~}p^{-1}(1,-1)<p^{-1}(-1,1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because $U$ is truthful for all uniformly dominant tuples, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0\\prec\\mathbb{B}\\left[U(1,S_{j},S_{k})\\mid S_{i}=1\\right]-\\mathbb{B}\\left[U(-1,S_{j},S_{k})\\mid S_{i}=1\\right]=2\\sum_{s_{j},s_{k}}D(s_{j},s_{k})p^{1}(s_{i},s_{j})}\\\\ {\\displaystyle0\\succ\\mathbb{B}\\left[U(1,S_{j},S_{k})\\mid S_{i}=-1\\right]-\\mathbb{B}\\left[U(-1,S_{j},S_{k})\\mid S_{i}=-1\\right]=2\\sum_{s_{j},s_{k}}D(s_{j},s_{k})p^{-1}(s_{i},s_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Suppose the following are true ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{D(1,-1)=-D(-1,1)>0}}\\\\ {{D(1,1)=D(-1,-1)=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\lambda=D(1,-1)>0.$ . By eqs. (11) and (12), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(s_{i},s_{j},s_{k})=\\!s_{i}\\cdot D(s_{j},s_{k})+\\mu(s_{j},s_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\!\\lambda\\cdot s_{i}(s_{j}-s_{k})+\\mu(s_{j},s_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof. Thus, we will construct three joint distributions satisfying definition 4.1 to prove eqs. (11) and (12). ", "page_idx": 21}, {"type": "text", "text": "The first joint distribution $p_{1}^{s_{i}}(s_{j},s_{k})$ with $0<\\delta\\leq1/2$ ", "page_idx": 21}, {"type": "equation", "text": "$$\np^{1}=\\left[\\!\\!\\begin{array}{c c}{0}&{1/2+\\delta}\\\\ {1/2-\\delta}&{0}\\end{array}\\!\\!\\right]\\mathrm{~and~}p^{-1}=\\left[\\!\\!\\begin{array}{c c}{0}&{1/2-\\delta}\\\\ {1/2+\\delta}&{0}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then eq. (10) on the first distribution reduces to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle1<D(1,-1)p_{1}^{1}(1,-1)+D(-1,1)p_{1}^{1}(-1,1)=\\frac{1}{2}(D(1,-1)+D(-1,1))+\\delta(D(1,-1)-D(-1,1))}}\\\\ {{\\displaystyle1>D(1,-1)p_{1}^{-1}(1,-1)+D(-1,1)p_{1}^{-1}(-1,1)=\\frac{1}{2}(D(1,-1)+D(-1,1))-\\delta(D(1,-1)-D(-1,1))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As we take $\\delta$ to zero, we prove $D(1,-1)=-D(-1,1)$ . Then plugging in with nonzero $\\delta$ , we have $D(1,-1)>0$ and complete the proof of eq. (11). ", "page_idx": 21}, {"type": "text", "text": "The second joint distribution $p_{2}^{s_{i}}(s_{j},s_{k})$ with $0\\leq\\epsilon\\leq1$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\np^{1}=\\left[\\!\\!\\begin{array}{c c}{1-\\epsilon}&{\\frac{3}{4}\\epsilon}\\\\ {\\frac{\\epsilon}{4}}&{0}\\end{array}\\!\\!\\right]\\mathrm{~and~}p^{-1}=\\left[\\!\\!\\begin{array}{c c}{1-\\epsilon}&{\\frac{\\epsilon}{4}}\\\\ {\\frac{3\\epsilon}{4}}&{0}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With eq. (11), eq. (10) reduces to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{0<(1-\\epsilon)D(1,1)+\\displaystyle\\frac{\\epsilon}{4}(D(1,-1)-D(-1,1))}}\\\\ {{0>(1-\\epsilon)D(1,1)-\\displaystyle\\frac{\\epsilon}{4}(D(1,-1)-D(-1,1)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By taking $\\epsilon$ to zero, we prove $D(1,1)=0$ . We can prove $D(-1,-1)=0$ using the similar trick and complete the proof of eq. (12). ", "page_idx": 22}, {"type": "text", "text": "F Additional empirical results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Comparison data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we test if the dataset satisfy transitivity property. We denote the proportion of rankings such that item $a$ is higher than item $a^{\\prime}$ in the dataset by $p_{a>a^{\\prime}}$ . If $p_{a>a^{\\prime}}>1/2$ , $p_{a^{\\prime}>a^{\\prime\\prime}}>1/2$ , and $p_{a>a^{\\prime\\prime}}>1/2$ , we say the triple of items $\\{a,a^{\\prime},a^{\\prime\\prime}\\}$ empirically satisfies transitivity. If $p_{a>a^{\\prime}}>1/2$ , $p_{a^{\\prime}>a^{\\prime\\prime}}>1/2$ , and $p_{a>a^{\\prime\\prime}}>\\operatorname*{max}\\{p_{a>a^{\\prime}},p_{a^{\\prime}>a^{\\prime\\prime}}\\}$ , we say the triple of items $\\{a,a^{\\prime},a^{\\prime\\prime}\\}$ empirically satisfies strong transitivity. We first test the transitivity of the SUSHI subdataset selected in section 6.1. We find that $100\\%$ of the item triples empirically satisfy transitivity, and $69.17\\%$ of the item triples empirically satisfy strong transitivity. This suggests that our transitivity assumption for the comparison data is mostly aligned. ", "page_idx": 22}, {"type": "text", "text": "Moreover, we conducted an experiment on the entire SUSHI dataset without any selection criteria and demonstrated the results in fig. 4. Observe that the ECDF of payments from original human users also dominates the payments under the uninformed strategy and the unilateral deviating strategy. This is consistent with our experimental results in section 6.1. However, there are two minor difference. First the separation of truth-telling from the other two is slightly less prominent than fig. 1 with the selection criteria. This may be due to a slightly lower degree of transitivity across agents with different backgrounds. In particular, we found the average value of $p_{a>a^{\\prime\\prime}}-\\operatorname*{max}\\{p_{a>a^{\\prime}},p_{a^{\\prime}>a^{\\prime\\prime}}\\}$ is 0.0559 without the selection criteria which is less than 0.0604 with the selection criteria in fig. 1. Second, the fraction of agents receiving positive payments is slightly higher than in fig. 1 (0.785 and 0.763 respectively). This aligned with or empirical (strong) transitively which are 1 and 0.7667 compared to the above 1 and 0.69117. Furthermore, we also conducted experiments on other groups of users by changing the selection criteria. Those interested can refer to fig. 5, fig. 6 and table 1 for the results, which further verify the effectiveness of our mechanism. ", "page_idx": 22}, {"type": "table", "img_path": "ofjTu2ktxO/tmp/ea3ae52a0a782906e21a0cf31d9e08481a5c33c132b5efb2b58addcd571ee05e.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of truth-telling utility in appendix F.1. "], "page_idx": 22}, {"type": "text", "text": "F.2 Networked data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Alongside fig. 2, Figure 7 and table 2 present empirical results for the top five popular artists in the dataset, excluding Lady Gaga, who are Britney Spears, Rihanna, The Beatles, and Katy Perry. All these settings show similar results. However, the Beatles\u2019 data is less conclusive as the payment distribution under the uninformed strategy profile is close to the truth-telling. This observation is also documented in Daskalakis et al. [12] which notes that the Ising model performs much better for rock artists than for pop artists. The authors conjecture that this may be due to the highly divisive popularity of pop artists like Lady Gaga and Britney Spears, whose listeners may form dense cliques within the graph. ", "page_idx": 22}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/12dad6e65a7b5d96c89ffbba263c31302abeab4138d2922d14888c9c147f0744.jpg", "img_caption": ["Figure 4: ECDF comparisons on all users without any selection. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Note that there is a buck of agent with a payment of around 0.5 under the truth-telling. This is because many non-listeners have no listener friends, and payment is $1-[(1-p)-p]=2p$ is twice the popularity $p\\approx0.25$ . Moreover, the jump is most minor for the Beatles, and indicates less agreement between non-listeners. Additionally, by the definition of bonus-penalty payment, we can see the payment of deviation is the minus of the truthful payment, so that the ECDF is symmetric around (0, 0.5). ", "page_idx": 23}, {"type": "table", "img_path": "ofjTu2ktxO/tmp/dea9bc4e1ae1dbb1c4b541e777785b0ffa9e8d5e6c50be591ba1c457b4a2e3ac.jpg", "table_caption": [], "table_footnote": ["Table 2: Summary of truth-telling utility in appendix F.2. "], "page_idx": 23}, {"type": "text", "text": "Figure 8 further shows the scatter plot of average payment and fraction of agents with positive payments across the top fifty popular artists where all settings have more than $60\\%$ percent of agents get positive payment. However, for less popular artists, the performance of our mechanism declines. This is expected, as we cannot provide effective incentives when only one agent listens to an artist. ", "page_idx": 23}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/eea3bdc6a423fa11455e1c6036dfc8a540c72fa05f28f10c1cc4d94162a07767.jpg", "img_caption": ["Figure 5: In each of the rows, we present the ECDF comparisons after changing the selection criteria for the user group as follows: from female to male, from ages 30\u201349 to ages 5\u201329, from ages 30\u201349 to ages $^{50+}$ , respectively. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/ce3693c8edd56972448e1cd44cff41282f509365c7496c0ddef5e1844e838399.jpg", "img_caption": ["Figure 6: In each of the rows, we present the ECDF comparisons after changing the location criteria for the user group as follows: from mostly living in Kanto or Shizuoka to Tohoku until age 15, and from mostly living in Kanto or Shizuoka to Hokuriku until age 15, respectively. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/c8355029c7962b85ce312c771b1fb0c7648571964a5c7d4353929d8b3d9d8cbc.jpg", "img_caption": ["Figure 7: Last.fm dataset for other top five popular artists excluding Lady Gaga. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "ofjTu2ktxO/tmp/fbbf907c5b3e448afd1305749333c20d73a02b9eb59a8bef60b86536c36e3539.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: Average payment and fraction of positive payment under the truth-telling across top fifty popular artists. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: Our claims accurately reflect the contributions and scope of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In section 7, we discuss potential future research directions, which are the limitations of our current work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We present all the assumptions. The complete proofs are provided in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The code is uploaded in the supplementary material. All the information required to reproduce the experimental results is provided. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The code is uploaded in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 30}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Most of these details are explained in section 6 and in the provided code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We believe the error bars are not relevant to our empirical metrics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We believe the computer resources are not relevant to our main contributions. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The research conducted in our paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work contributes to the theory of information elicitation. We discussed the applicability and limitations for elicitation settings. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We believe this paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The datasets used in this paper are mentioned with URLs and the licenses and terms of use are properly respected. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]