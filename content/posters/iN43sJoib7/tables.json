[{"figure_path": "iN43sJoib7/tables/tables_3_1.jpg", "caption": "Table 1: Effect of self-attention in PatchTST on forecasting performance (MSE) on ETTm1.", "description": "This table presents the mean squared error (MSE) results of the PatchTST model on the ETTm1 dataset, comparing the performance with and without self-attention for different forecasting horizons (96, 192, 336, and 720).  It demonstrates the impact of removing self-attention on forecasting accuracy. ", "section": "3 Revisiting Self-Attention in Time Series Forecasting"}, {"figure_path": "iN43sJoib7/tables/tables_5_1.jpg", "caption": "Table 2: Time complexity of Transformer-based models to calculate attention outputs. Time refers to the inference time obtained by averaging 10 runs under L = 96 and T = 720 on Electricity.", "description": "This table compares the time complexity of various Transformer-based models for calculating attention outputs.  The time complexity is expressed using Big O notation, and the actual inference time (in milliseconds) is reported for an experiment with input sequence length L=96 and forecasting horizon T=720 on the Electricity dataset.  The models included are Transformer, Informer, Autoformer, FEDformer, Pyraformer, Crossformer, PatchTST, and the authors' proposed CATS model. The table highlights the significant efficiency gains achieved by the CATS model compared to other Transformer models.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_5_2.jpg", "caption": "Table 3: Effect of parameter sharing across horizons on the number of parameters for different forecasting horizons on ETTh1.", "description": "This table presents the number of parameters used in a model with and without parameter sharing across different forecasting horizons. The results are shown for the ETTh1 dataset.  The table highlights how parameter sharing significantly reduces the number of parameters, particularly as the forecasting horizon increases.", "section": "4 Proposed Methodology"}, {"figure_path": "iN43sJoib7/tables/tables_6_1.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined. Full results are provided in Appendix.", "description": "This table presents the results of multivariate long-term time series forecasting experiments using various models, including the proposed CATS model.  The results are evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics for different forecasting horizons (96, 192, 336, and 720) across four datasets (ETT, Traffic, Electricity, and Weather).  The best performing model for each metric and horizon is highlighted in bold, and the second best is underlined.  A more complete table with additional results is available in the appendix.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_7_1.jpg", "caption": "Table 5: Comparison of models with the number of parameters, GPU memory consumption, and MSE across different input sequence lengths on ETTm1. Full results with more diverse input lengths are provided in Appendix.", "description": "This table compares the performance of different time series forecasting models (PatchTST, TimeMixer, DLinear, and CATS) across various input sequence lengths (336, 720, 1440, and 2880) on the ETTm1 dataset.  For each model and input length, the table shows the number of parameters (in millions), the GPU memory usage (in GB), and the mean squared error (MSE).  The results highlight the impact of input sequence length on model complexity and performance. The full results with more diverse input lengths are available in the appendix.", "section": "5.2 Efficient and Robust Forecasting for Long Input Sequences"}, {"figure_path": "iN43sJoib7/tables/tables_8_1.jpg", "caption": "Table 6: Averaged univariate short-term forecasting results in the M4 dataset. The best results are in bold and the second best are underlined. Full results are presented in Appendix.", "description": "This table presents the average results of various models on the M4 dataset for short-term time series forecasting.  The metrics used are SMAPE, MASE, and OWA. The best performing model for each metric is highlighted in bold, with the second-best underlined. Complete results can be found in the Appendix. This provides a comparison of CATS performance against several other state-of-the-art models in a commonly used benchmark for short-term time series prediction.", "section": "5.3 Short-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_8_2.jpg", "caption": "Table 7: Performance comparison on models with three attention layers. We replace one or more cross-attentions (CA) with self-attentions (SA) in our model. In total, there are three cross-attentions in all settngs and 'Zero SA' stands for our model. The best results are in bold.", "description": "This table presents the results of an ablation study comparing the performance of the proposed model (CATS) with different numbers of self-attention layers.  The 'Zero SA' row represents the original CATS model, which uses only cross-attention. The other rows show the performance when one or two of the cross-attention layers are replaced with self-attention layers.  The table shows that the model performs best when only cross-attention is used (Zero SA).", "section": "5.4 Replacement of Cross-Attention with Self-Attention"}, {"figure_path": "iN43sJoib7/tables/tables_13_1.jpg", "caption": "Table 8: Details of 13 real-world datasets.", "description": "This table presents the details of thirteen real-world datasets used in the paper's experiments on long-term time series forecasting.  For each dataset, it lists the dimension (number of variables), frequency of data points (e.g., hourly, daily), total number of timesteps, the type of information contained, and the forecasting horizons used in the evaluations.", "section": "A.1 Datasets"}, {"figure_path": "iN43sJoib7/tables/tables_13_2.jpg", "caption": "Table 9: Experimental settings used in Table 4 and Table 11.", "description": "This table details the hyperparameter settings used for the experiments presented in Tables 4 and 11 of the paper.  It specifies the number of layers, embedding size, whether query sharing was used, the input sequence length, batch size, number of epochs, and learning rate for each of the seven datasets used in the long-term forecasting experiments: Weather, Electricity, Traffic, ETTh1, ETTh2, ETTm1, and ETTm2. The settings were largely kept consistent to ensure fair comparison and reproducibility.", "section": "A.2 Hyperparameter Settings"}, {"figure_path": "iN43sJoib7/tables/tables_14_1.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined.", "description": "This table presents a comparison of the proposed CATS model's performance against other state-of-the-art time series forecasting models on various datasets.  The results are reported using two common metrics: Mean Squared Error (MSE) and Mean Absolute Error (MAE). The table shows the results for multiple forecasting horizons and highlights the superiority of the CATS model in many cases.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_14_2.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined. Full results are provided in Appendix.", "description": "This table presents a comparison of the model's performance (measured by MSE and MAE) on four different datasets (ETT, Traffic, Electricity, and Weather) across four different forecasting horizons (96, 192, 336, and 720).  The results show that the proposed CATS model consistently achieves the lowest MSE and MAE, indicating superior performance compared to other state-of-the-art models for long-term forecasting tasks. The full results, including additional metrics, are available in the Appendix.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_14_3.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined. Full results are provided in Appendix.", "description": "This table presents the mean squared error (MSE) and mean absolute error (MAE) for various long-term time series forecasting models on different datasets, including Electricity, Traffic, ETT (ETTh1, ETTh2, ETTm1, ETTm2) and Weather. The models compared include CATS (the proposed model), TimeMixer, PatchTST, Timesnet, Crossformer, MICN, FiLM, DLinear, Autoformer, and Informer. The table shows the performance of each model across different forecasting horizons (96, 192, 336, and 720) for each dataset.  The best performance for each metric and horizon are highlighted in bold, and the second-best are underlined. The full results including additional metrics are given in the Appendix.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_15_1.jpg", "caption": "Table 13: Experimental settings with an input sequence length of 512.", "description": "This table details the hyperparameters used in the experiments with an input sequence length of 512.  It lists the specific settings for each of the seven datasets used in the paper's long-term forecasting experiments (Weather, Electricity, Traffic, ETTh1, ETTh2, ETTm1, and ETTm2), including the number of layers, embedding size, query sharing, batch size, number of epochs, and learning rate.  The variations in these settings reflect the diverse characteristics of the different datasets.", "section": "B.1 Performance with Longer Input Sequences"}, {"figure_path": "iN43sJoib7/tables/tables_15_2.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined.", "description": "This table presents a comparison of the proposed CATS model's performance against several state-of-the-art long-term forecasting models across various datasets and forecasting horizons.  The metrics used for comparison are Mean Squared Error (MSE) and Mean Absolute Error (MAE).  The best performing model for each metric and dataset is highlighted in bold, while the second-best is underlined. This provides a comprehensive overview of CATS' performance relative to existing models in a long-term forecasting context.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_16_1.jpg", "caption": "Table 5: Comparison of models with the number of parameters, GPU memory consumption, and MSE across different input sequence lengths on ETTm1. Full results with more diverse input lengths are provided in Appendix.", "description": "This table compares the performance of PatchTST, TimeMixer, DLinear, and CATS models across various input sequence lengths (96, 192, 336, 512, 720, 1440, 2880) on the ETTm1 dataset.  For each model and sequence length, the number of parameters (in millions), GPU memory consumption (in MB), and Mean Squared Error (MSE) are reported.  The table demonstrates how the model performance and resource requirements vary with the length of the input sequence, highlighting the efficiency of CATS in terms of parameter count and memory usage while maintaining competitive MSE values.", "section": "5.2 Efficient and Robust Forecasting for Long Input Sequences"}, {"figure_path": "iN43sJoib7/tables/tables_16_2.jpg", "caption": "Table 5: Comparison of models with the number of parameters, GPU memory consumption, and MSE across different input sequence lengths on ETTm1. Full results with more diverse input lengths are provided in Appendix.", "description": "This table compares the performance of different time series forecasting models (PatchTST, TimeMixer, DLinear, and CATS) across various input sequence lengths (96, 192, 336, 512, 720, 1440, and 2880) on the ETTm1 dataset.  It shows the number of parameters, GPU memory consumption, and mean squared error (MSE) for each model and input length. The goal is to demonstrate the efficiency and robustness of the proposed CATS model compared to existing models, particularly when dealing with longer input sequences.", "section": "5.2 Efficient and Robust Forecasting for Long Input Sequences"}, {"figure_path": "iN43sJoib7/tables/tables_18_1.jpg", "caption": "Table 11: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined.", "description": "This table presents a comparison of the proposed CATS model with several state-of-the-art long-term time series forecasting models across multiple datasets and forecast horizons.  The models were evaluated using a consistent set of hyperparameters to ensure fair comparison.  The table shows Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each model and dataset, highlighting CATS' superior performance, particularly for longer horizons.", "section": "5.1 Long-term Time Series Forecasting Results"}, {"figure_path": "iN43sJoib7/tables/tables_18_2.jpg", "caption": "Table 4: Multivariate long-term forecasting results with recent forecasting models and ours for unified hyperparameter settings. The best results are in bold and the second best are underlined. Full results are provided in Appendix.", "description": "This table presents a comparison of the model's performance in multivariate long-term forecasting.  It shows the mean squared error (MSE) and mean absolute error (MAE) for various models (including the proposed CATS model) across different datasets and forecasting horizons. The best and second-best results for each metric are highlighted.", "section": "5.1 Long-term Time Series Forecasting Results"}]