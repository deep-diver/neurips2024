[{"type": "text", "text": "Diffusion Twigs with Loop Guidance for Conditional Graph Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Giangiacomo Mercatali \u2020 \u2217 HES-SO Gen\u00e8ve University of Manchester giangiacomo.mercatali@hesge.ch ", "page_idx": 0}, {"type": "text", "text": "Yogesh Verma \u2020 Aalto University yogesh.verma@aalto.fi ", "page_idx": 0}, {"type": "text", "text": "Vikas Garg YaiYai Ltd & Aalto University vgarg@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Andre Freitas   \nIdiap Research Institute   \nUniversity of Manchester   \nNBC, CRUK Manchester Institute   \nandre.freitas@idiap.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a novel score-based diffusion framework named Twigs that incorporates multiple co-evolving flows for enriching conditional generation tasks. Specifically, a central or trunk diffusion process is associated with a primary variable (e.g., graph structure), and additional offshoot or stem processes are dedicated to dependent variables (e.g., graph properties or labels). A new strategy, which we call loop guidance, effectively orchestrates the flow of information between the trunk and the stem processes during sampling. This approach allows us to uncover intricate interactions and dependencies, and unlock new generative capabilities. We provide extensive experiments to demonstrate strong performance gains of the proposed method over contemporary baselines in the context of conditional graph generation, underscoring the potential of Twigs in challenging generative tasks such as inverse molecular design and molecular optimization. Code is available at https://github.com/Aalto-QuML/Diffusion_twigs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conditional graph generation is a fundamental problem in scientific domains such as de novo drug design [21, 43, 74] and material design [39]. However, searching for new molecules with desired physicochemical properties poses significant challenges to traditional brute-force methods due to the vast combinatorial spaces [64]. With the advent of neural networks [44], deep generative models have emerged as a powerful tool for learning informative conditional representations of molecules, facilitating the development of in silico methods for chemical design [16, 31, 61, 73]. ", "page_idx": 0}, {"type": "text", "text": "Score-based diffusion generative models (SGMs) and denoising probabilistic diffusion models (DDPMs) [24, 67] have recently emerged as powerful techniques for training deep networks on graphstructured data, with applications spanning molecular design [37, 53, 36, 81], molecular docking [6], molecular dynamics simulations [78], protein folding [79], and backbone modeling [70]. Notably, diffusion models exhibit superior capabilities for conditional graph generation, excelling in both discrete [26, 75, 49] and continuous [3, 28, 45, 11] settings. The training of the mentioned conditional diffusion models is achieved by two types of diffusion guidance algorithms: classifier-based guidance [8], which involves training a separate property predictor model alongside the diffusion model; and classifier-free guidance [23], which integrates scores from both unconditional and conditional diffusion models. While these guidance techniques have been found to be effective, the algorithm design is not tailored to encompass the intricate hierarchical or multi-resolution elements inherent in conditional generation. Consequently, it is plausible that this inadequacy may contribute to suboptimal representations, particularly notable in tasks such as conditional graph generation. The recent success of hierarchical diffusion flows in various domains, such as modeling interactions between node and edge features [37], multi-resolution modeling [25], decision-making [47], and conditional image generation [4, 71] underscores the need to integrate hierarchical information beyond the capabilities of classifier-based and classifier-free guidance. ", "page_idx": 0}, {"type": "image", "img_path": "fvOCJAAYLx/tmp/1b9eed05e8113d46d69d32a0d18bec39b3c417454a58d10278575ebe7a22752b.jpg", "img_caption": ["Figure 1: Overview of the proposed method (Twigs). We define two types of diffusion processes: (1) multiple Stem processes $(s_{\\phi_{i}})$ , which unravel the interactions between graph structure and single properties, and (2) the Trunk process, which orchestrates the combination of the graph structure score from $s_{\\theta}$ with the stem process contributions from $s_{\\phi_{i}}$ . During the forward process, the structure ${\\bf y}_{s}$ and the properties $\\{\\mathbf{y}_{i}\\}_{k}$ co-evolve toward noise. In each step of the reverse process, the structure is first denoised and subsequently used to denoise the properties (indicated by the green-dashed line). Such de-noised properties are then utilized, in turn, to further denoise the structure (red line), in a process that resembles a guidance loop. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We assert that conditional diffusion models for structured spaces, such as graphs, could be enhanced with hierarchical conditional processes. Specifically, rather than treating heterogeneous structural and label information uniformly within the hierarchy, we advocate for the co-evolution of multiple processes with distinct roles (asymmetric). These roles encompass a primary process governing the structural evolution alongside multiple secondary processes responsible for driving conditional content. We aim to propose an alternative to existing conditional graph diffusion techniques (outlined in Table 1) by bestowing the models with finer control over two key aspects: 1) the evolution of structural graph components, including nodes and edges, and 2) the co-adaptation of the graph structure in conjunction with one or more associated properties. ", "page_idx": 1}, {"type": "text", "text": "Towards this objective, we present a novel diffusion framework for conditional generation named Twigs, drawing analogies from the trunk and offshoots of a tree. Concretely, we establish a central trunk process governing a primary variable, which interacts with several stem processes, each associated with a secondary variable. In contrast with classifier-free and classifier-based methodologies, a novel conditional mechanism, termed loop guidance, orchestrates information exchange between the trunk and the stem processes (refer to Figure 1). Our methodology facilitates the acquisition of flexible representations, capitalizing on the disentanglement of intricate interactions and dependencies. We formalize our framework by drawing upon the theory of denoising score matching [67] and leveraging tools derived from stochastic differential equations (SDEs) [1]. The effectiveness of Twigs is substantiated through compelling empirical validation across various conventional constrained generation tasks, utilizing both molecular and generic graph datasets. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In summary, this paper makes the following key contributions: ", "page_idx": 1}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/d50eb4da61ace3bc40fb940a686045dee3d494a8a975264d331e783eebf1ef93.jpg", "table_caption": ["Table 1: Comparison of related methodologies. Twigs is the first method that enables a seamless orchestration of multiple asymmetric property-oriented hierarchical diffusion processes via SDEs. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "\u2022 (Conceptual and methodological) The introduction of a new score-based, end-to-end trainable, non-autoregressive generative model Twigs designed for acquiring conditional representations. Our approach enables precise guidance of multiple property-conditioned diffusion processes. ", "page_idx": 2}, {"type": "text", "text": "\u2022 (Technical) We present a robust mathematical framework, including a novel strategy called loop guidance, that employs tools from Stochastic Differential Equations (SDEs) to derive both the forward diffusion process and its corresponding reverse SDE for conditional generation. This framework is designed to seamlessly integrate additional contexts as conditioning information. ", "page_idx": 2}, {"type": "text", "text": "\u2022 (Empirical) We showcase the versatility of the proposed diffusion mechanism (Twigs) through extensive empirical evidence across various challenging conditional graph generation tasks, consistently surpassing contemporary baselines. ", "page_idx": 2}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Table 1 we provide an overview of the similarities and differences between Twigs and related methods. We refer the reader to Appendix E for additional related work. ", "page_idx": 2}, {"type": "text", "text": "Diffusion guidance is typically applied to regulate the diffusion process for conditional generation. Previous approaches that perform class-conditional generation are divided into classifier-based [8], and classifier-free guidance [23]. While some works model diffusion with multiple flows [5, 37, 46], they treat nodes and edges in a symmetric way; i.e., they associate multiple flows for nodes and edges that have equivalent contributions (in other words, these flows have the same roles). We instead abstract graph properties as secondary processes that branch from, and interact with, the main process that pertains to the graph structure. In addition, while other guidance methods are related [18, 40, 52], they do not leverage multiple diffusion flows. To our knowledge, the proposed method is the first to incorporate multiple diffusion flows in a hierarchical fashion for conditional generation. We formalize in Table 2 how Twigs differs, mathematically, from classifier-free and classifier-based methods. ", "page_idx": 2}, {"type": "text", "text": "Conditional Diffusion for Graphs Recent advancements in generative modeling have prominently featured score-based techniques (SGM), utilizing diffusion or stochastic differential equations (SDEs) [19, 32, 35, 37, 48], including for graph generation [3, 5, 13, 14, 15, 18, 26, 40, 45, 46, 52, 72, 75, 82]. Guidance methods have been adopted in conditional molecule generation settings. The works from Hoogeboom et al. [26], Huang et al. [28, 28], Xu et al. [82] are classifier-free approaches, while Bao et al. [3], Vignac et al. [75], Lee et al. [45] focus on classifier-based methods. Diverging from these approaches, we explicitly model the dynamic interaction between primary variables (e.g., graph structure) and dependent variables (e.g., graph properties) using dedicated diffusion processes to achieve more expressive representations and improve performance for conditional generation. ", "page_idx": 2}, {"type": "text", "text": "3 Diffusion Twigs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Method overview We extend score-based techniques [67] for training conditional diffusion models over graphs. Differently from current guidance methods, as summarised in Table 2, we leverage a finer control over the structure and graph properties to diffuse multiple hierarchical processes, toward achieving a more robust representation. Our method, Twigs, defines a trunk process over the primary ", "page_idx": 2}, {"type": "text", "text": "Table 2: Twigs comparison to Classifier-based [8] and Classifier-free [23] guidance, applied for conditional generation in Diffusion models. Here ${\\bf y}_{s}$ represents the graph structure, $\\{\\mathbf{y}_{i}\\}_{k}$ represent the $k$ -properties of graph. The $f_{\\phi}$ function is the classifier, $\\epsilon_{\\theta}$ and $s_{\\theta,\\phi}$ are learnable score models. ", "page_idx": 3}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/43119ad4824a8ac6443c9dc46a280b4b29bb5edf866c8bd49def1164267d0ebd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "variable (graph structure) ${\\bf y}_{s}$ , and a stem process over each dependent variable $\\mathbf{y}_{i}\\in\\mathbb{R}$ (e.g., graph property). We achieve the desired flexibility with a variable ${\\bf y}_{s}$ that encompasses both node features and the adjacency matrix as well as the coordinates. The details of the dimensions of ${\\bf y}_{s}$ are given in Section B.1 for the 3D case, and in Section B.2 for the 2D case. ", "page_idx": 3}, {"type": "text", "text": "Forward process We define multiple forward processes within a hierarchy that co-evolves data and properties into noise. The trunk forward process for the graph structure ${\\bf y}_{s}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\bf y}_{s}={\\bf f}_{s}({\\bf y}_{s,t},t)\\mathrm{d}t+g_{s}(t)\\mathrm{d}{\\bf w}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{f}_{s}$ and $g_{s}$ are corresponding diffusion and drift functions, and dw is the Wiener noise. The stem forward process over the $k$ dependent variables $\\mathbf{y}=\\left\\{\\mathbf{y}_{1},\\dots,\\mathbf{y}_{k}\\right\\}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{y}(t)=\\left(\\begin{array}{c}{\\mathrm{d}\\mathbf{y}_{1}(t)}\\\\ {\\vdots}\\\\ {\\mathrm{d}\\mathbf{y}_{k}(t)}\\end{array}\\right)=\\left(\\mathbf{f}_{p}(\\mathbf{y}_{1,t},\\mathbf{y}_{s,t},t)\\mathrm{d}t+g_{p}(t)\\mathrm{d}\\mathbf{w}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{f}_{p}$ and $g_{p}$ denote the diffusion and drift functions, respectively, for the $k$ stem processes. Collectively, along with the trunk forward process, they constitute Twigs. These operations introduce random Gaussian noise, iteratively, to the data toward a prior (typically Gaussian) distribution. ", "page_idx": 3}, {"type": "text", "text": "Reverse Process The Twigs reverse process starts from the prior distribution (Gaussian noise) towards the data distribution. A key difference with Song et al. [67] is that here our variable $\\mathbf{y}_{t}$ comprises both structure and properties, leading to the following modification of the overall diffusion process: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd\\mathbf{y}_{t}=[f(\\mathbf{y}_{t},t)-g_{t}^{2}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{t})]d t+g_{t}d\\Bar{\\mathbf{w}}\\qquad\\mathrm{where}\\quad\\mathbf{y}_{t}=\\left\\{\\mathbf{y}_{s,t},\\left\\{\\mathbf{y}_{i,t}\\right\\}_{i=1}^{k}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We derive Equation (3) in Section A.1. The joint distribution over the trunk and stem processes is assumed to factorize as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t}(\\mathbf{y}_{s,t},\\mathbf{y}_{1,t},...,\\mathbf{y}_{k,t})=p_{t}(\\mathbf{y}_{s,t})\\prod_{i=1}^{k}\\;p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In turn, the score function simplifies as in Equation (5), leading to the decomposition in Equation (6). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{s,t},\\mathbf{y}_{1,t},\\ldots,\\mathbf{y}_{k,t})=\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{s,t})+\\sum_{i=1}^{k}\\;\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{y}_{t}=[\\mathbf{f}(\\mathbf{y}_{t},t)-g_{t}^{2}(\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{s,t})+\\sum_{i=1}^{k}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t}))]\\mathrm{d}t+g_{t}\\mathrm{d}\\bar{\\mathbf{w}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Conditional modeling We expand our proposed approach to enable conditional generation with an external context $\\mathbf{y}_{C}=\\{\\mathbf{y}_{c}\\ |\\ c\\in C\\}$ , where $C\\subseteq\\{1,\\ldots,k\\}$ . The context can be represented as a scalar or vector, describing a particular value associated with a data-dependent variable. For example, in case of molecules, it could represent one or more of the $k$ properties such as the Synthetic Accessibility (SA) score or the Quantitative Estimate of Drug likeness (QED). This extension modifies the joint distribution for the score function in Equation (5). ", "page_idx": 3}, {"type": "text", "text": "Reverse SDE under conditioning context ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The reverse SDE for $\\mathbf{y}_{t}=\\{\\mathbf{y}_{s,t},\\{\\mathbf{y}_{i,t}\\}_{k}\\}$ give an external conditioning context $\\mathbf y_{C}$ is shown below (details in Appendix A.2). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{y}_{t}\\!=\\![\\mathbf{f}(\\mathbf{y}_{t},t)\\!-\\!g_{t}^{2}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{t},\\mathbf{y}_{C})]\\mathrm{d}t+g_{t}\\mathrm{d}\\bar{\\mathbf{w}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We resort to the following factorization of the distribution, conditioned on the context $\\mathbf{y}_{C}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t}(\\mathbf{y}_{s,t},\\{\\mathbf{y}_{i,t}\\}_{k},\\mathbf{y}_{C})=\\prod_{i}^{k}p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t},\\mathbf{y}_{C})p_{t}(\\mathbf{y}_{s,t},\\mathbf{y}_{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As a result, the factorization of the score function $\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{s,t},\\left\\{\\mathbf{y}_{i,t}\\right\\}_{k},\\mathbf{y}_{C})$ amounts to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{s,t},\\mathbf{y}_{C})\\!+\\!\\!\\sum_{i\\;\\notin\\;C}^{k}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t})\\!+\\!\\!\\sum_{c}^{C}\\sum_{i}^{k}\\delta_{i=c}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t},\\mathbf{y}_{c})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above-factorized score function parameterizes our reverse diffusion process, thus offering a novel approach to integrate external contextual information into conditional generation. ", "page_idx": 4}, {"type": "text", "text": "Training We propose to train Twigs by incorporating the factorization from Equation (8) within a score-matching objective function [30, 67]. Algorithm 1 shows the training procedure to learn two types of time-dependent score-based models: ${\\boldsymbol{s}}_{\\theta,t}$ , which approximates the trunk variable, and ${\\pmb{s}}_{\\phi_{i},t}$ which approximates the coupling between the stem variable and the trunk variable. The objective function for optimizing the score networks $s_{\\theta},s_{\\phi_{i}}$ , is given as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\theta,\\phi_{i}}\\mathbb{E}_{t}\\left\\{\\lambda_{\\mathbf{y}_{t}}(t)\\mathbb{E}_{\\mathbf{y}_{0}}\\mathbb{E}_{\\mathbf{y}_{t}|\\mathbf{y}_{0}}\\big\\lVert s_{\\theta,t}\\big(\\mathbf{y}_{s,t},\\mathbf{y}_{c}\\big)+\\!\\!\\sum_{i}^{k}s_{\\phi_{i},t}\\big(\\mathbf{y}_{i,t},\\mathbf{y}_{s,t},\\mathbf{y}_{c}\\big)\\!-\\!\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\big(\\mathbf{y}_{t},\\mathbf{y}_{C}\\big)\\big\\rVert_{2}^{2}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathbf{y}_{0}}=\\mathbb{E}_{\\mathbf{y}_{s,0},\\mathbf{y}_{i,0}}$ and $\\mathbb{E}_{\\mathbf{y}_{t}}=\\mathbb{E}_{\\mathbf{y}_{s,t},\\mathbf{y}_{i,t}}$ . It is worth noting that the influence introduced by the variable $s_{\\phi_{i}}$ provides the directions for the diffusion model to converge into distributions with the desired properties. Such property-oriented knowledge operates in conjunction with the structural information provided by $s_{\\theta}$ , resulting in a novel form of guidance that is orchestrated by a branching diffusion process, named Loop guidance. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Training Twigs Algorithm 2 Generating with Twigs Input: Dataset $\\mathcal{D}$ , iterations $n_{\\mathrm{iter}}$ , batch size $B$ , Input:Score-based models ${s_{\\theta,t}},\\{s_{\\phi_{i},t}\\}_{i=1}^{K}$ , number of batches $n_{B}$ , $K$ properties to consider Time step schedule $\\{t\\}_{t=T}^{0}$ , Langevin MCMC Initialize parameters $s_{\\theta,t},\\stackrel{\\cdot}{\\{s_{\\phi_{i},t}\\}}_{i=1}^{K}$ for Score step size $\\alpha$ , External context $\\mathbf{\\nabla}y_{C}$ Networks for $k=1,\\ldots,n_{\\mathrm{iter}}\\,\\mathbf{do}$ for $b=1,\\dots,n_{B}$ do $\\begin{array}{r l}&{y_{s_{T}},\\{y_{i,T}\\}_{i=1}^{K}\\sim\\mathcal{N}(0,I)}\\\\ &{\\mathbf{\\mathrm{tor}}\\,t=T,\\ldots,0\\,\\mathbf{do}}\\\\ &{\\quad s_{\\theta,t}\\leftarrow s_{\\theta,t}(y_{s_{t}},\\{y_{i,t}\\}_{i=1}^{K},y_{C})}\\\\ &{\\quad\\{s_{\\phi_{i},t}^{\\phantom{}}\\}_{i=1}^{K}\\leftarrow\\{s_{\\phi_{i,t}}(y_{s_{t}},y_{i,t},y_{C})\\}_{i=1}^{K}}\\\\ &{\\quad\\mathbf{y}_{s_{t}}\\leftarrow\\mathbf{y}_{s_{t}}+\\frac{\\alpha}{2}s_{\\theta,t}+\\sqrt{\\alpha z_{s}};z_{s}\\sim\\mathcal{N}(0,I)}\\\\ &{\\quad\\mathbf{y}_{i_{t}}\\leftarrow\\mathbf{y}_{i_{t}}+\\frac{\\alpha}{2}s_{\\phi_{i},t}+\\sqrt{\\alpha}z_{i};z_{i}\\sim\\mathcal{N}(0,I)}\\end{array}$ $t\\sim\\mathcal{U}(0,1]$ $\\mathcal{D}_{b}=\\left\\{(\\pmb{{y}}_{s,l},\\{\\pmb{y}_{i,l}\\}_{i=1}^{K})_{l=1}^{B},\\pmb{y}_{C}\\right\\}\\sim\\mathcal{D}$ $\\mathcal{L}_{b}\\gets\\mathrm{Eq}$ . 9 end for $\\begin{array}{r}{\\theta,\\{\\phi_{i}\\}_{i=i}^{K}\\leftarrow\\mathsf{o p t i m}(\\frac{1}{n_{B}}\\sum_{b=1}^{n_{B}}\\mathcal{L}_{b})}\\end{array}$ end for end for ", "page_idx": 4}, {"type": "text", "text": "Sampling Given a trained conditional Twigs model, our generative process begins by sampling an external context or conditioning value $\\mathbf{y}_{C}$ , which can also be supplied externally. We then simulate the reverse diffusion process, similar to the one described in Equation 8, but with a modified score function to generate the data. The proposed algorithm for generating new data samples with Twigs is given in Algorithm 2 and involves a loop of updates between processes: the stem score network $s_{\\phi_{i}}$ evolves the property $\\mathbf{y}_{i}$ , integrating information from the structure ${\\bf y}_{s}$ , and subsequently, the updated property information from $s_{\\phi_{i}}$ is integrated into the main process by the score network $s_{\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conduct a set of comprehensive experiments to demonstrate that Twigs improves over contemporary conditional generation methods. Benchmarks include: molecule generation conditioned over single $(\\S\\,4.1)$ , and multiple $(\\S\\ 4.2)$ properties on QM9, as well as molecule optimization on ZINC250K $(\\S\\,4.3)$ , and network-graph generation conditioned on desired properties $(\\S\\,4.4)$ . ", "page_idx": 4}, {"type": "text", "text": "Figure 2: First row: Samples by Twigs for 3D molecules conditioned on single properties on QM9.   \nSecond row: KDE and KL divergence results between target and predicted properties. ", "page_idx": 5}, {"type": "image", "img_path": "fvOCJAAYLx/tmp/525b6671b59ba04f9378562b0c24c30e10f8d286cc9ef812bc67520b116ae272.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/7927eaeb80697c95497901bc398f6d2a2c43a36c246de2df3373bcb709d51fc3.jpg", "table_caption": ["Table 3: MAE\u2193results on single target quantum property for the QM9 dataset. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/806608a8ce8cda07c84fbfe1efc4be02dc63c0dc6ef50c905db1b46fe24f7985.jpg", "table_caption": ["Table 4: Novelty, atom & molecule stability for QM9 single property. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Single Quantum properties on QM9 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setup. We evaluate the effectiveness of Twigs for generating molecules with a single desired quantum property, sourced from the QM9 dataset [58], specifically, we consider $C_{v}$ , $\\mu,\\,\\alpha,\\,\\Delta\\epsilon$ , \u03f5LUMO and $\\epsilon_{\\mathrm{{HOMO}}}$ . To ensure consistency and comparability with the baselines, which include JODO [28], EDM [26], EEGSDE [3], GeoLDM [82], TEDMol [49], EquiFM [68], we adhere to the identical dataset preprocessing, training/test data partitions, and evaluation metrics outlined by Huang et al. [28]. Regarding parameterization of Twigs, we follow the attention architecture defined in Section B.1 with a single stem process. ", "page_idx": 5}, {"type": "text", "text": "Results. In Table 3, we report the Mean Absolute Error (MAE) results, and in Table 4, the Novelty, Atom stability and Molecule stability. Our method outperforms all the evaluated baselines across the specified properties. In Figure 2, the bottom row provides a Kernel Density Estimation (KDE) visualization which shows that Twigs achieves a more accurate distribution for the property values when compared with JODO, while the top row shows some 3D molecule samples by our model. ", "page_idx": 5}, {"type": "image", "img_path": "fvOCJAAYLx/tmp/00165a08343dd86d3893ec4a9c7a88e62393e77de2755916df44997f4617aa5c.jpg", "img_caption": ["Figure 3: Samples of multiple-property conditional molecules by Twigs ( $C_{v}$ and $\\mu$ ) for QM9. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/c9eff6c0013e892d11529f59c502156897782b2aa573aad12646799263379f85.jpg", "table_caption": ["Table 5: MAE $\\left(\\downarrow\\right)$ for conditional generation on QM9 with multiple properties. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Multiple Quantum properties on QM9 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. This experiment evaluates the capability to combine multiple desired properties in the generated molecule. Specifically we follow Huang et al. [29] and consider all possible combinations of properties involving $\\mu$ : $(C_{v},\\mu)$ , $(\\Delta\\epsilon,\\mu)$ , $(\\alpha,\\mu)$ . Since we model two properties, we test our Twigs with two stem networks within the attention architecture described in Section B.1. We benchmark against several contemporary baselines, including EDM [26], EEGSDE [3] and JODO [28]. ", "page_idx": 6}, {"type": "text", "text": "Results. In Table 5, we present the Mean Absolute Error (MAE) results obtained from the property predictors introduced by Huang et al. [28] for the various property pairs under consideration. The superior performance of Twigs across all baselines reinforces the findings from the single property experiment (Section 4.1), emphasizing the benefits of learning multiple hierarchical stem processes. ", "page_idx": 6}, {"type": "image", "img_path": "fvOCJAAYLx/tmp/c54501fa28758d3089b4aea395dc3f6bebbeb4ba2d9192ce909928c6942e2d98.jpg", "img_caption": ["Figure 4: Molecules generated by Twigs from ZINC250k conditioned on fa7 (top), parp1 (bottom). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Molecule optimization on ZINC250K ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. The goal is to generate molecules from the ZINC250K dataset that exhibit optimal binding affinity, drug-likeness, and synthesizability for the following five target proteins: parp1, fa7, 5ht1b, braf, jak2. We adhere to the evaluation protocol established by Lee et al. [45], which involves generating 3000 molecules and assessing them using two metrics that constrain the desired properties, including docking score (DS), drug-likeness (QED), and synthetic accessibility (SA). ", "page_idx": 6}, {"type": "text", "text": "The first metric, Novel hit ratio $(\\%)$ , represents the fraction of unique hit molecules that have a maximum Tanimoto similarity of less than 0.4 with the training molecules. Hit molecules are defined as those meeting the criteria: $\\mathrm{DS}<$ (the median DS of the known active molecules), $\\mathrm{QED}>0.5$ , and $S\\mathbf{A}<5$ . The second metric, Novel top $5\\%$ docking score, is the average DS of the top $5\\%$ unique molecules that satisfy QED $>0.5$ and $S\\mathbf{A}<5$ , with a maximum similarity of less than 0.4 to the training molecules. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We consider REINVENT [55]: a reinforcement learning (RL) model that utilizes a prior sequence model, MORLD [33]: a RL model that uses QED and SA scores as intermediate rewards and docking scores as final rewards, HierVAE [34]: a VAE-based model that utilizes hierarchical molecular representation and active learning, GDSS [37]: a score-based diffusion model that evolves nodes and edge information with a system of SDEs, MOOD [45]: a score-based diffusion model based on GDSS that trains an additional property predictor to improve conditional generation. For MOOD we consider the version without the out-of-distribution (OOD) control, to have a fair comparison with our method. For Twigs we follow the GCN-based architecture described in Section B.2, with multiple stem processes (one for each target protein). ", "page_idx": 7}, {"type": "text", "text": "Results. In Table 6 we report the results for top $5\\%$ docking scores. We observe that Twigs achieves the highest score across all properties, excluding braf, where it achieves the second-best score after MOOD. In Table 7 we report the results for Novel hit ratio. The outcomes confirm that our model is improving the performance substantially over all the considered properties, except for braf, on which Twigs is the second-best performing model after MOOD. In Figure 4, we provide some samples of the molecules obtained by Twigs with the respective QED, SA, and docking score. Additionally, in Table 13 we report the MAE values for generating molecules with a desired target protein property, and in Table 14 we compare the inference cost of Twigs against MOOD. ", "page_idx": 7}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/b9bd696631143b2dee7a3fa3243f1ab86f1b5606c1142ed09b58227910f85db3.jpg", "table_caption": ["Table 6: Novel top $5\\%$ docking score on ZINC250K. Best is boldfaced, second-best is in gray . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/734bcd1034ab37b1c809390dbbee3ba966fd604a11195ca2c075f42029ef2121.jpg", "table_caption": ["Table 7: Novel hit ratio $(\\uparrow)$ results on ZINC250K. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Generation of Network graphs with desired properties ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. We follow the data processing delineated by Jo et al. [37] and provide results for the Community-small [60] and Enzymes datasets [62]. To test the capabilities to generate conditional graphs, we extract four properties via the NetworkX library [20], including density, clustering, assortativity, and transitivity. Considering a graph $G$ with $n$ nodes and $m$ edges, we have: (1) Density: $\\begin{array}{r}{d=\\frac{2m}{n(n-1)}}\\end{array}$ , (2) Clustering coefficient: the average $\\begin{array}{r}{C=\\frac{1}{n}\\sum_{v\\in G}c_{v}}\\end{array}$ . (3) Assortativity: measures the similarity of connections in the graph with respect to the node degree. (4) Transitivity: the fraction of all possible triangles present in $G$ . Possible triangles are identified by the number of \"triads\" (two edges with a shared vertex). The transitivity is $\\bar{T}=3\\frac{\\#t r i a n g l e s}{\\#t r i a d s}$ ", "page_idx": 7}, {"type": "text", "text": "Baselines. In terms of baselines, we first consider two versions of MOOD [45] (two OOD coefficients), and we train the property predictors using the codes from the authors. Our second baseline is GDSS [37], which we modify to be equipped with a classifier-free guidance scheme. We also consider the version of GDSS based on transformers, which leverages the graph-multi-head attention [2]. Finally, we consider Digress [75], which is a classifier-based guidance diffusion model based on attention mechanisms. We parameterize our Twigs model with our GCN architecture described in Section B.2, with a single stem process. ", "page_idx": 7}, {"type": "image", "img_path": "fvOCJAAYLx/tmp/953fd1ad84dcb1c0eef28b6af91a4c6c1d1ba8b021167494e61581822be4d145.jpg", "img_caption": ["Figure 5: Visualization of Community-small and Enzymes datasets. First and second rows: samples generated by Twigs. Third and fourth rows: KDE plots and corresponding KL divergence values. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/0469314529ca555a98f1938b66f8e92b16e30a870fd44aa645138e6a432b2f8e.jpg", "table_caption": ["Table 8: MAE $\\left(\\downarrow\\right)$ values on Community-small and Enzymes, conditioned on single properties. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results. Table 8 reports the MAE average of three runs, demonstrating that Twigs consistently outperforms the considered baselines on all cases across the two datasets. MOOD is the second-best performing model in the majority of the cases. We further strengthen the MAE results by providing in Figure 5 (bottom) the KDE plots of the property distributions of the graph generated by Twigs and MOOD. The Figure demonstrates that Twigs can achieve a higher fidelity to the data, which is also confirmed by the lower KL divergence values. Figure 5 (top) depicts some random graph samples generated by Twigs. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation study on multiple properties ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setup. Assuming conditional independence among the properties $\\alpha$ , \u03f5HOMO, \u03f5LUMO, $\\Delta\\epsilon$ , $\\mu$ , and $C_{v}$ given the molecular graph can simplify the modeling process. This assumption leverages the fact that the molecular graph captures the essential structural dependencies, allowing us to treat the properties as independent for computational efficiency and ease of interpretation, even if slight interdependencies exist. ", "page_idx": 8}, {"type": "text", "text": "Results. Here we show that such modeling assumption can work practically. Table 9 reports the MAE on molecular graphs for QM9 on three properties, showing that our method consistently achieves lower error on all the properties. Table 10 shows that on generic graphs Twigs can achieve lower MAE on all the considered cases, in the cases of two and three properties. ", "page_idx": 8}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/5561c52e634ed8893b696f309ac96aae735165bbc5fbae6217178ed246fd39f6.jpg", "table_caption": ["Table 9: MAE values over three properties for QM9. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/a110f3f05d7b1dafa7ef77abc68096837cd361a3c8a1d8d486db54d28d7ef7e0.jpg", "table_caption": ["Table 10: MAE results for two and three properties on community small. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.6 Training time ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Table 11 we study the impact of multiple diffusion flows on the community-small and Enzymes datasets. Specifically, we report the average time for the overall training for Twigs with one and three secondary diffusion flows. We observe that our models encounter a small overhead compared to GDSS and Digress, however, we believe it is a good tradeoff because it achieves a lower MAE. ", "page_idx": 9}, {"type": "text", "text": "Table 11: Overall training time for 5,000 epochs (hours and minutes) for Twigs with different secondary diffusion flows, GDSS, and Digress on the Community-small and Enzymes datasets. ", "page_idx": 9}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/d6c5f6ec9a12b29c5844ee829c89fbfd4a40bf2efe5bef3e7f20dae84151fa07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Broader Implications, and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a novel approach to model conditional information within generative models tailored for graph data. Twigs incorporates the novel mechanism of loop guidance to control the overall generative process by first bifurcating the diffusion flow into multiple stem processes and then re-integrating them into the trunk process, resembling a loop. Our experimental results showcase the performance gains of Twigs when compared to current state-of-the-art baselines across various conditional graph generation tasks. ", "page_idx": 9}, {"type": "text", "text": "Conditional generation is fast emerging as one of the most exciting avenues within machine learning and would benefit from techniques beyond classifier-based and classifier-free schemes, making our method applicable to settings beyond this work. Indeed, while the current work has focused on graph settings, Twigs might find use in other domains (e.g., image, text, and audio). However, whether Twigs is effective in such settings needs to be investigated in future works. ", "page_idx": 9}, {"type": "text", "text": "Training multiple properties (stem processes) might require training additional parameters, incurring additional computation and training time. Our ablation study on training time due to multiple processes (Section 4.6) suggests that Twigs could provide a good tradeoff (lower MAE compared to some prominent existing methods at the expense of small additional computational overhead). ", "page_idx": 9}, {"type": "text", "text": "Finally, assuming factorization of the distribution over stem processes conditioned on the trunk process might not always be realistic. Our experiments in Section 4.5 suggest that Twigs might still be able to achieve a strong performance when considering multiple properties. In case some prior knowledge is available about some properties that violate this assumption, we could, in principle, adapt Twigs by grouping them into a single stem process while factorizing with the remaining ones. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "YV and VG acknowledge support from the Research Council of Finland for the \u201cHuman-steered next-generation machine learning for reviving drug design\u201d project (grant decision 342077). VG also acknowledges Jane and Aatos Erkko Foundation (grant 7001703) for \u201cBiodesign: Use of artificial intelligence in enzyme design for synthetic biology\u201d. GM acknowledges support from the Engineering and Physical Sciences Research Council (EPSRC) and the BBC under iCASE. AF is partially funded by the CRUK National Biomarker Centre, by the Manchester Experimental Cancer Medicine Centre and the NIHR Manchester Biomedical Research Centre. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. ISSN 0304-4149. doi: https://doi.org/10.1016/ 0304-4149(82)90051-5. URL https://www.sciencedirect.com/science/article/ pii/0304414982900515. [2] Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot$ JHcqXGaqiGn. [3] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided SDE for inverse molecular design. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\cdot$ r0otLtOwYW. [4] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023. [5] Xiaohui Chen, Jiaxing He, Xu Han, and Liping Liu. Efficient and degree-guided graph generation via discrete diffusion modeling. In International Conference on Machine Learning, pages 4585\u20134610. PMLR, 2023.   \n[6] Gabriele Corso, Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776, 2022. [7] Alex O Davies, Nirav S Ajmeri, et al. Hierarchical gnns for large graph generation. arXiv preprint arXiv:2306.11412, 2023. [8] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021.   \n[9] Anh-Dung Dinh, Daochang Liu, and Chang Xu. Rethinking conditional diffusion sampling with progressive guidance. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=gThGBHhqcU.   \n[10] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International Conference on Machine Learning, pages 8489\u20138510. PMLR, 2023.   \n[11] Alexandru Dumitrescu, Dani Korpela, Markus Heinonen, Yogesh Verma, Valerii Iakovlev, Vikas Garg, and Harri L\u00e4hdesm\u00e4ki. Field-based molecule generation. arXiv preprint arXiv:2402.15864, 2024.   \n[12] Peter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K Gilson, and Rose Yu. Limo: Latent inceptionism for targeted molecule generation. arXiv preprint arXiv:2206.09010, 2022.   \n[13] Niklas Gebauer, Michael Gastegger, and Kristof Sch\u00fctt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. Advances in neural information processing systems, 32, 2019.   \n[14] Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert M\u00fcller, and Kristof T Sch\u00fctt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):973, 2022.   \n[15] Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, and Tie-Yan Liu. De novo molecular generation via connection-aware motif mining. In International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=Q_Jexl8-qDi.   \n[16] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[17] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. Advances in Neural Information Processing Systems, 35:14715\u201314728, 2022.   \n[18] Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien LafranceVanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew G Wilson. Protein design with guided discrete diffusion. Advances in neural information processing systems, 36, 2023.   \n[19] Florentin Guth, Simon Coste, Valentin De Bortoli, and Stephane Mallat. Wavelet score-based generative modeling, 2022.   \n[20] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.   \n[21] Philip J Hajduk and Jonathan Greer. A decade of fragment-based drug design: strategic advances and lessons learned. Nature reviews Drug discovery, 6(3):211\u2013219, 2007.   \n[22] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, et al. Manifold preserving guided diffusion. arXiv preprint arXiv:2311.16424, 2023.   \n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022.   \n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020.   \n[25] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):2249\u20132281, 2022.   \n[26] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867\u20138887. PMLR, 2022.   \n[27] Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, and Weifeng Lv. Graphgdp: Generative diffusion processes for permutation invariant graph generation. In 2022 IEEE International Conference on Data Mining (ICDM), pages 201\u2013210. IEEE, 2022.   \n[28] Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Learning joint 2d & 3d diffusion models for complete molecule generation. arXiv preprint arXiv:2305.12347, 2023.   \n[29] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. 3dlinker: an e (3) equivariant variational autoencoder for molecular linker design. arXiv preprint arXiv:2205.07309, 2022.   \n[30] Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[31] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-based protein design. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf.   \n[32] John Ingraham, Max Baranov, Zak Costello, Vincent Frappier, Ahmed Ismail, Shan Tie, Wujie Wang, Vincent Xue, Fritz Obermeyer, Andrew Beam, et al. Illuminating protein space with a programmable generative model. BioRxiv, pages 2022\u201312, 2022.   \n[33] Woosung Jeon and Dongsup Kim. Autonomous molecule generation using reinforcement learning and docking to develop potential novel inhibitors. Scientific reports, 10(1):22104, 2020.   \n[34] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International conference on machine learning, pages 4839\u2013 4848. PMLR, 2020.   \n[35] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models. In Lecture Notes in Computer Science, pages 274\u2013289. Springer Nature Switzerland, 2022. doi: 10.1007/978-3-031-20050-2_17. URL https://doi.org/10.1007% 2F978-3-031-20050-2_17.   \n[36] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. Advances in Neural Information Processing Systems, 35:24240\u201324253, 2022.   \n[37] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pages 10362\u201310383. PMLR, 2022.   \n[38] Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. Graph generation with destination-driven diffusion mixture. arXiv preprint arXiv:2302.03596, 2023.   \n[39] Kisuk Kang, Ying Shirley Meng, Julien Breger, Clare P Grey, and Gerbrand Ceder. Electrodes with high power and high capacity for rechargeable lithium batteries. Science, 311(5763): 977\u2013980, 2006.   \n[40] Leo Klarner, Tim GJ Rudner, Garrett M Morris, Charlotte M Deane, and Yee Whye Teh. Context-guided diffusion for out-of-distribution molecular and protein design. arXiv preprint arXiv:2407.11942, 2024.   \n[41] Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B Aditya Prakash, and Chao Zhang. Autoregressive diffusion model for graph generation. In International Conference on Machine Learning, pages 17391\u201317408. PMLR, 2023.   \n[42] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling. Advances in Neural Information Processing Systems, 35: 2550\u20132563, 2022.   \n[43] Najwa Laabid, Severi Rissanen, Markus Heinonen, Arno Solin, and Vikas Garg. Alignment is key for applying diffusion models to retrosynthesis. arXiv preprint arXiv:2405.17656, 2024.   \n[44] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[45] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-of-distribution generation. Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[46] Mufei Li, Eleonora Kreac\u02c7ic\u00b4, Vamsi K Potluru, and Pan Li. Graphmaker: Can diffusion models generate large attributed graphs? arXiv preprint arXiv:2310.13833, 2023.   \n[47] Wenhao Li, Xiangfeng Wang, Bo Jin, and Hongyuan Zha. Hierarchical diffusion for offline decision making. In International Conference on Machine Learning, pages 20035\u201320064. PMLR, 2023.   \n[48] Meng Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji. Graphebm: Molecular graph generation with energy-based models. arXiv preprint arXiv:2102.00546, 2021.   \n[49] Yanchen Luo, Sihang Li, Zhiyuan Liu, Jiancan Wu, Zhengyi Yang, Xiangnan He, Xiang Wang, and Qi Tian. Text-guided diffusion model for 3d molecule generation, 2024. URL https://openreview.net/forum?id $\\cdot$ FdUloEgBSE.   \n[50] Lukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Micha\u0142 Warcho\u0142. Mol-cyclegan: a generative model for molecular optimization. Journal of Cheminformatics, 12(1):1\u201318, 2020.   \n[51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.   \n[52] Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024.   \n[53] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134484. PMLR, 2020.   \n[54] Juhwan Noh, Dae-Woong Jeong, Kiyoung Kim, Sehui Han, Moontae Lee, Honglak Lee, and Yousung Jung. Path-aware and structure-preserving generation of synthetically accessible molecules. In International Conference on Machine Learning, pages 16952\u201316968. PMLR, 2022.   \n[55] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):1\u201314, 2017.   \n[56] Yidong Ouyang, Liyan Xie, and Guang Cheng. Improving adversarial robustness through the contrastive-guided diffusion process. In International Conference on Machine Learning, pages 26699\u201326723. PMLR, 2023.   \n[57] Bo Qiang, Yuxuan Song, Minkai Xu, Jingjing Gong, Bowen Gao, Hao Zhou, Wei-Ying Ma, and Yanyan Lan. Coarse-to-fine: a hierarchical diffusion model for molecule generation in 3d. In International Conference on Machine Learning, pages 28277\u201328299. PMLR, 2023.   \n[58] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.   \n[59] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00f3, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.   \n[60] Ida Schomburg, Antje Chang, Christian Ebeling, Marion Gremse, Christian Heldt, Gregor Huhn, and Dietmar Schomburg. Brenda, the enzyme database: updates and major new developments. Nucleic acids research, 32(suppl_1):D431\u2013D433, 2004.   \n[61] Daniel Schwalbe-Koda and Rafael G\u00f3mez-Bombarelli. Generative models for automatic chemical design. Machine Learning Meets Quantum Physics, pages 445\u2013467, 2020.   \n[62] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[63] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International conference on machine learning, pages 9558\u20139568. PMLR, 2021.   \n[64] Gregory Sliwoski, Sandeepkumar Kothiwale, Jens Meiler, and Edward W Lowe. Computational methods in drug discovery. Pharmacological reviews, 66(1):334\u2013395, 2014.   \n[65] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9_gsMA8MRKQ.   \n[66] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483\u201332498. PMLR, 2023.   \n[67] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[68] Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=hHUZ5V9XFu.   \n[69] Brian L Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint arXiv:2206.04119, 2022.   \n[70] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ 6TxBxqNME1Y.   \n[71] Alex M Tseng, Tommaso Biancalani, Max Shen, and Gabriele Scalia. Hierarchically branched diffusion models for efficient and interpretable multi-class conditional generation. arXiv preprint arXiv:2212.10777, 2022.   \n[72] Alex M Tseng, Nathaniel Diamant, Tommaso Biancalani, and Gabriele Scalia. Graphguide: interpretable and controllable conditional graph generation with discrete bernoulli diffusion. arXiv preprint arXiv:2302.03790, 2023.   \n[73] Yogesh Verma, Samuel Kaski, Markus Heinonen, and Vikas Garg. Modular flows: Differential molecular generation. In Advances in Neural Information Processing Systems, volume 35, pages 12409\u201312421. Curran Associates, Inc., 2022.   \n[74] Yogesh Verma, Markus Heinonen, and Vikas Garg. Abode: Ab initio antibody design using conjoined odes. In International Conference on Machine Learning, pages 35037\u201335050. PMLR, 2023.   \n[75] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=UaAD-Nu86WX.   \n[76] Wujie Wang, Minkai Xu, Chen Cai, Benjamin Kurt Miller, Tess Smidt, Yusu Wang, Jian Tang, and Rafael G\u00f3mez-Bombarelli. Generative coarse-graining of molecular conformations. arXiv preprint arXiv:2201.12176, 2022.   \n[77] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. arXiv preprint arXiv:2208.11126, 2022.   \n[78] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model for molecular dynamics simulations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5321\u20135329, 2023.   \n[79] Kevin Eric Wu, Kevin K Yang, Rianne van den Berg, James Zou, Alex Xijie Lu, and Ava P Amini. Protein structure generation via folding diffusion, 2023. URL https://openreview. net/forum?id $\\cdot$ Nkd7AS2USRd.   \n[80] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and Qiang Liu. Diffusion-based molecule generation with informative prior bridges. Advances in Neural Information Processing Systems, 35:36533\u201336545, 2022.   \n[81] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= PzcvxEMzvQC.   \n[82] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592\u201338610. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Derivation of the reverse SDE ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a Stochastic Differential Equation (SDE) of the form, ", "page_idx": 15}, {"type": "equation", "text": "$$\nd\\boldsymbol{x}=f(\\boldsymbol{x}_{t},t)\\mathrm{d}t+g(\\boldsymbol{x}_{t},t)\\mathrm{d}\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f(\\cdot)$ and $g(\\cdot)$ are diffusion, drift function and dw is the weiner noise. The evolution of the distribution of $x_{t}$ is governed by the Kolmogorov Forward Equation (KFE) as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}p\\left(x_{t}\\right)=-\\partial_{x_{t}}\\left[f\\left(x_{t}\\right)p\\left(x_{t}\\right)\\right]+\\frac{1}{2}\\partial_{x_{t}}^{2}\\left[g^{2}\\left(x_{t}\\right)p\\left(x_{t}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Kolmogrov Forward/Backward Equation (KFE/KBE). Essentially KFE describes the evolution of a probability distribution $p(\\boldsymbol{x}_{t})$ forward in time. The reverse-time SDE can be derived by solving the Kolmogorov Backward Equation (K.B.E) as derived in Anderson [1]. It can be defined for $t_{1}\\geq t_{0}$ as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\partial_{t}p\\left(x_{t_{1}}\\mid x_{t_{0}}\\right)=f\\left(x_{t_{0}}\\right)\\partial_{x_{t_{0}}}p\\left(x_{t_{1}}\\mid x_{t_{0}}\\right)+{\\frac{1}{2}}g^{2}\\left(x_{t_{0}}\\right)\\partial_{x_{t_{0}}}^{2}p\\left(x_{t_{1}}\\mid x_{t_{0}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{t_{0}}$ and $x_{t_{1}}$ are distributions at the respective time steps. Specifically, it models how the distribution dynamics at a later point $t_{1}$ in time changes as we change $t_{0}$ at an earlier time. ", "page_idx": 15}, {"type": "text", "text": "In our case, we consider the diffusion over structure $\\mathbf{y}_{s}$ and properties $\\left\\{\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{k}\\right\\}$ . The KFE of the system $\\mathbf{y}=\\{\\mathbf{y}_{s},\\mathbf{y}_{1},\\dots,\\mathbf{y}_{k}\\}$ is given by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}p\\left(\\mathbf{y}_{t}\\right)=-\\partial_{\\mathbf{y}_{t}}\\left[f\\left(\\mathbf{y}_{t}\\right)p\\left(\\mathbf{y}_{t}\\right)\\right]+\\frac{1}{2}\\partial_{\\mathbf{y}_{t}}^{2}\\left[g^{2}\\left(\\mathbf{y}_{t}\\right)p\\left(\\mathbf{y}_{t}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Independence Factorization. We can factorize $p\\left(\\mathbf{y}_{t}\\right)$ based on our assumption that the properties $\\left\\{\\mathbf{y}_{1,t},\\dotsc,\\mathbf{y}_{k,t}\\right\\}$ are independent conditioned on the structure $\\mathbf{y}_{s,t}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{p(\\mathbf{y}_{t})=p(\\mathbf{y}_{s,t},\\mathbf{y}_{1,t},\\dots,\\mathbf{y}_{k,t})}\\\\ {=p(\\mathbf{y}_{s,t})p(\\mathbf{y}_{1,t},\\dots,\\mathbf{y}_{k,t}\\mid\\mathbf{y}_{s,t})}\\\\ {={p(\\mathbf{y}_{s,t})}\\displaystyle\\prod_{i}^{k}p(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Leveraging this factorization, we can define a system of SDEs with KFEs for each variable, leading us to the SDE system defined in Eq. 1 and Eq. 2. ", "page_idx": 15}, {"type": "text", "text": "Reverse SDE: In the reverse case, we aim to denoise the full vector $\\mathbf{y}=\\{\\mathbf{y}_{s},\\mathbf{y}_{1},\\dots,\\mathbf{y}_{k}\\}$ where ${\\bf y}_{s}$ denotes the diffusion over structure and $\\{\\mathbf{y}_{1},\\hdots,\\mathbf{y}_{k}\\}$ over the $k$ properties via reverse SDE. Expressing in the form of Eq. 12, we note that for $t_{1}\\geq t_{0}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\partial_{t}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)=f\\left(\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)+{\\frac{1}{2}}g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}^{2}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Anderson [1] defines a joint distribution over the time-ordered variables ${\\bf y}_{t_{1}}$ and $\\mathbf{y}_{t_{0}}$ to derive the reverse SDE. We utilize their analysis and define a joint distribution ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right):=p\\left(\\mathbf{y}_{s,t_{1}},\\mathbf{y}_{1,t_{1}},...,\\mathbf{y}_{k,t_{1}},\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},...,\\mathbf{y}_{k,t_{0}}\\right)}\\\\ &{\\qquad\\qquad\\qquad=p\\left(\\mathbf{y}_{s,t_{1}},\\mathbf{y}_{1,t_{1}},...,\\mathbf{y}_{k,t_{1}}\\mid\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},...,\\mathbf{y}_{k,t_{0}}\\right)p\\left(\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},...,\\mathbf{y}_{k,t_{0}}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We denote $p(\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},\\ldots{},\\mathbf{y}_{k,t_{0}})$ by $p(\\mathbf{y}_{t_{0}})$ , and note that it can be decomposed similarly as in Eq. 14. Taking the time derivative of Eq. 16, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\partial_{t}p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)=-\\partial_{t}p\\left(\\mathbf{y}_{s,t_{1}},\\mathbf{y}_{1,t_{1}},...,\\mathbf{y}_{k,t_{1}}~\\middle|~\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},...,\\mathbf{y}_{k,t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\partial_{t}p(\\mathbf{y}_{t_{0}})p\\left(\\mathbf{y}_{s,t_{1}},\\mathbf{y}_{1,t_{1}},...,\\mathbf{y}_{k,t_{1}}~\\middle|~\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},...,\\mathbf{y}_{k,t_{0}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Comparison with KFE/KBE. We observe that $\\partial_{t}p\\left(\\mathbf{y}_{s,t_{1}},\\mathbf{y}_{1,t_{1}},\\ldots,\\mathbf{y}_{k,t_{1}}\\mid\\mathbf{y}_{s,t_{0}},\\mathbf{y}_{1,t_{0}},\\ldots,\\mathbf{y}_{k,t_{0}}\\right)$ corresponds to the KBE in Eq. 15 and $\\partial_{t}p(\\mathbf{y}_{t_{0}})$ to the KFE in Eq. 13. Denoting ", "page_idx": 15}, {"type": "text", "text": "$\\left\\{{\\bf y}_{s,t_{1}},{\\bf y}_{1,t_{1}},\\ldots,{\\bf y}_{k_{t}1}\\right\\}$ by ${\\bf y}_{t_{1}}$ , we immediately get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{-\\,\\partial_{t}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)p\\!\\left(\\mathbf{y}_{t_{0}}\\right)-\\partial_{t}p\\!\\left(\\mathbf{y}_{t_{0}}\\right)\\!p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)}}\\\\ {{=\\left(f\\left(\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)+\\frac{1}{2}g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}^{2}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)\\right)p\\!\\left(\\mathbf{y}_{t_{0}}\\right)}}\\\\ {{+\\,p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)\\left(\\partial_{\\mathbf{y}_{t_{0}}}\\left[f\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)\\right]-\\frac{1}{2}\\partial_{\\mathbf{y}_{t_{0}}}^{2}\\left[g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)\\right]\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The derivatives can be handled, by following standard differentiation rules as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{\\mathbf{y}_{t_{0}}}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)=\\partial_{\\mathbf{y}_{t_{0}}}\\left[\\frac{p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)}{p\\left(\\mathbf{y}_{t_{0}}\\right)}\\right]}\\\\ &{\\qquad\\qquad=\\frac{\\partial_{\\mathbf{y}_{t_{0}}}p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)}{p\\left(\\mathbf{y}_{t_{0}}\\right)}-\\frac{p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)\\,\\partial_{\\mathbf{y}_{t_{0}}}p\\left(\\mathbf{y}_{t_{0}}\\right)}{p^{2}\\left(\\mathbf{y}_{t_{0}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Evaluating the derivative of the products in the forward Kolmogorov equation and substituting the derivatives accordingly we obtain, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-\\partial_{t}p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)=\\partial_{\\mathbf{y}_{t_{0}}}\\left[f\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}},\\mathbf{y}_{t_{1}}\\right)\\right]+\\frac{1}{2}g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}^{2}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)p(\\mathbf{y}_{t_{0}})}\\\\ &{}&{\\;\\;-\\,\\frac{1}{2}p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}^{2}\\left[g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)p(\\mathbf{y}_{t_{0}})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Matching the terms of the second-order derivatives with the expansion of the derivative and doing some algebraic manipulations, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\partial_{t}p\\left(\\mathbf{y}_{t_{0}},\\mathbf{y}_{t_{0}}\\right)=\\partial_{\\mathbf{y}_{t_{0}}}\\left[f\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}},\\mathbf{y}_{t_{1}}\\right)\\right]+\\frac{1}{2}\\partial_{\\mathbf{y}_{t_{0}}}^{2}\\left[p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)\\right]}\\\\ &{\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,-\\,\\partial_{\\mathbf{y}_{t_{0}}}\\left[p\\left(\\mathbf{y}_{t_{1}}\\mid\\mathbf{y}_{t_{0}}\\right)\\partial_{\\mathbf{y}_{t_{0}}}\\left[g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)\\right]\\right]\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle-\\partial_{t}p\\left({\\bf y}_{t_{1}},{\\bf y}_{t_{0}}\\right)=-\\,\\partial_{{\\bf y}_{t_{0}}}\\left[p\\left({\\bf y}_{t_{1}},{\\bf y}_{t_{0}}\\right)\\left(-f\\left({\\bf y}_{t_{0}}\\right)+\\frac{1}{p\\left({\\bf y}_{t_{0}}\\right)}\\partial_{{\\bf y}_{t_{0}}}\\left(g^{2}\\left({\\bf y}_{t_{0}}\\right)p\\left({\\bf y}_{t_{0}}\\right)\\right)\\right)\\right]+}}\\\\ {{\\displaystyle\\frac{1}{2}\\partial_{{\\bf y}_{t_{0}}}^{2}\\left[p\\left({\\bf y}_{t_{1}},{\\bf y}_{t_{0}}\\right)g^{2}\\left({\\bf y}_{t_{0}}\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comparison with KFE. The above result is in the form of a Kolmogorov forward equation with the joint probability distribution $p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)$ . The time-ordering is $t_{1}>t_{0}$ and the term $-\\partial_{t}p\\left(\\mathbf{y}_{t_{1}},\\mathbf{y}_{t_{0}}\\right)$ describes the change of probability distribution as we move backward in time. We can marginalize over $t_{1}$ , using the Leibniz rule, to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\partial_{t}p\\left(\\mathbf{y}_{t_{0}}\\right)=-\\partial_{\\mathbf{y}_{t_{0}}}\\left[p\\left(\\mathbf{y}_{t_{0}}\\right)\\left(-f\\left(\\mathbf{y}_{t_{0}}\\right)+{\\frac{1}{p\\left(\\mathbf{y}_{t_{0}}\\right)}}\\partial_{\\mathbf{y}_{t_{0}}}\\left(g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)\\right)\\right)\\right]+{\\frac{1}{2}}\\partial_{\\mathbf{y}_{t_{0}}}^{2}\\left[p\\left(\\mathbf{y}_{t_{0}}\\right)g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)\\right]=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This finally gives a stochastic differential equation analogous to the Fokker-Planck/forward Kolmogorov equation that can be solved backward in time: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\mathbf{y}_{t_{0}}=\\left(-f(\\mathbf{y}_{t_{0}},t)+{\\frac{1}{p\\left(\\mathbf{y}_{t_{0}}\\right)}}\\partial_{\\mathbf{y}_{t_{0}}}\\left(g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)p\\left(\\mathbf{y}_{t_{0}}\\right)\\right)\\right){\\mathrm{d}}t+g\\left(\\mathbf{y}_{t_{0}}\\right){\\mathrm{d}}\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We keep $g^{2}\\left(\\mathbf{y}_{t_{0}}\\right)$ independent of $\\mathbf{y}_{t_{0}}$ . Applying the log-derivative trick, the SDE simplifies to ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\mathbf{y}_{t_{0}}=(f(\\mathbf{y}_{t_{0}},t)-g_{t_{0}}^{2}\\nabla_{\\mathbf{y}_{t_{0}}}\\log p(\\mathbf{y}_{t_{0}}))\\mathrm{d}t+g_{t_{0}}\\mathrm{d}\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Conditional score factorization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We extend our method to incorporate an external context or conditional information for conditional generation, similar to classifier-based [8] and classifier-free [23] guidance. Following similar notation, the reverse SDE [67], given an external context $\\mathbf y_{C}$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{y}_{t}=[\\mathbf{f}(\\mathbf{y}_{t},t)-g_{t}^{2}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}(\\mathbf{y}_{t},\\mathbf{y}_{C})]\\mathrm{d}t+g_{t}\\mathrm{d}\\bar{\\mathbf{w}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here $\\mathbf y_{t}\\,=\\,\\{\\mathbf y_{s,t},\\mathbf y_{1,t},\\dots,\\mathbf y_{k,t}\\}$ , and $\\mathbf{y}_{C}\\,=\\,\\{\\mathbf{y}_{c}\\,\\mid\\,c\\in C\\}$ is an external context or conditioning variable. This external context can be a scalar or vector describing a property value of the primary variable like QED or plogp in the case of molecules or image labels in the case of images. The $\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\big(\\mathbf{y}_{t},\\mathbf{y}_{C}\\big)$ term pertains to the score function which guides the process (see table 2 for comparison with both classifier-based and classifier-free guidance). Under our condition independence assumption, the score function factorizes as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle p_{t}\\bigl(\\mathbf{y}_{s,t},\\mathbf{y}_{1,t},\\ldots,\\mathbf{y}_{k,t},\\mathbf{y}_{C}\\bigr)=\\prod_{i}^{k}p_{t}\\bigl(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t},\\mathbf{y}_{c}\\bigr)p_{t}\\bigl(\\mathbf{y}_{s,t},\\mathbf{y}_{C}\\bigr)}\\\\ {\\displaystyle\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\bigl(\\mathbf{y}_{s,t},\\mathbf{y}_{1,t},\\ldots,\\mathbf{y}_{k,t},\\mathbf{y}_{C}\\bigr)=\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\bigl(\\mathbf{y}_{s,t},\\mathbf{y}_{C}\\bigr)+\\sum_{i\\textit{c}}^{k}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\bigl(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t}\\bigr)}\\\\ {\\displaystyle+\\sum_{c}^{C}\\sum_{i}^{k}\\delta_{i=c}\\nabla_{\\mathbf{y}_{t}}\\log p_{t}\\bigl(\\mathbf{y}_{i,t}\\mid\\mathbf{y}_{s,t},\\mathbf{y}_{c}\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Parameterizations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we describe two instances of Twigs based on architecture choices: Attention networks, and graph convolution networks (GCNs). Twigs with attention is used in 4.1 and 4.2, while Twigs with GCNs is used in 4.3 and 4.4. ", "page_idx": 17}, {"type": "text", "text": "B.1 Twigs with graph attention ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We denote the variable ${\\bf y}_{s}$ as a 3D graph $G=(A,x,h)$ , with node coordinates $\\pmb{x}=(\\pmb{x}^{1},\\dots,\\pmb{x}^{N})\\in$ $\\mathbb{R}^{N\\times3}$ , node features $\\boldsymbol{h}\\,=\\,\\bigl(h^{1},\\ldots,h^{N}\\bigr)\\,\\in\\,\\mathring{\\mathbb{R}}^{N\\times d1}$ , and edge information $\\pmb{A}\\in\\mathbb{R}^{N\\times N\\times d2}$ . The variable $\\mathbf{C}\\in\\mathbb{R}$ denotes the conditional information, which is obtained by adding the noise level $\\log(\\alpha_{t}^{2}/\\sigma_{t}^{2})$ , the perturbed property $\\mathbf{y}_{i}\\sim\\mathcal{N}(0,I)\\in\\mathbb{R}$ , and the fixed property $\\mathbf{y}_{C}\\in\\mathbb{R}$ . The context $\\mathbf{C}$ is combined with ${\\bf y}_{s}$ by multilayer perceptions (MLP), after projecting $(h,A,x)$ respectively into $\\mathbf{H},\\mathbf{E},\\mathbf{P}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{AdaLN}=(1\\mathrm{+}\\mathrm{MLP}(\\mathbf{C}))\\mathrm{\\cdotLN}(\\mathbf{H})\\mathrm{+MLP}(\\mathbf{C})}\\\\ &{\\mathbf{M}^{l}=\\mathrm{MHA}(\\mathrm{AdaLN}(\\mathbf{H},\\mathbf{C}),\\mathrm{AdaLN}(\\mathbf{E},\\mathbf{C}),\\mathbf{P})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where MHA is the multi-head attention, and AdaLN is Adaptive LayerNorm (LN) function. Subsequently, we leverage the Scale function $\\operatorname{Scale}(\\mathrm{h},\\mathbf{C})=\\mathrm{MLP}(\\mathbf{C})\\cdot\\mathrm{h}$ , and the Feed Forward Network (FFN) to obtain the Diffusion Graph Transformer (DGT) block, as defined in [28], which is described by Eq (31)(32). DGT first computes the intermediate representations for the $l$ -th layer as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}^{l}=\\mathrm{MHA}(\\mathrm{AdaLN}(\\mathbf{H}^{l},\\mathbf{C}),\\mathrm{AdaLN}(\\mathbf{E}^{l},\\mathbf{C}),\\mathbf{P}^{l})}\\\\ &{\\mathbf{\\hat{H}}=\\mathrm{Scale}(\\mathbf{M}^{l},\\mathbf{C})+\\mathbf{H}^{l}}\\\\ &{\\mathbf{\\hat{E}}=\\mathrm{Scale}(\\mathbf{M}_{i}^{l}+\\mathbf{M}_{j}^{l},\\mathbf{C})+\\mathbf{E}^{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then computes the $l+1$ layer as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}^{l+1}\\!=\\!\\operatorname{Scale}(\\mathrm{FFN}(\\mathrm{AdaLN}(\\mathrm{Scale}(\\hat{\\mathbf{E}},C)),C)\\!+\\!\\hat{\\mathbf{E}}}\\\\ &{\\mathbf{H}^{l+1}=\\operatorname{Scale}(\\mathrm{FFN}(\\mathrm{AdaLN}(\\hat{\\mathbf{H}},\\mathbf{C})),\\mathbf{C})+\\hat{\\mathbf{H}}}\\\\ &{\\mathbf{P}_{i}^{l+1}=\\sum_{i\\neq j}\\frac{\\mathbf{P}_{i}^{l}-\\mathbf{P}_{j}^{l}}{||\\mathbf{P}_{i}^{l}-\\mathbf{P}_{j}^{l}||^{2}}\\mathrm{tanh}(\\mathrm{MLP}(\\mathbf{E}_{i,j}^{l+1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The Twigs trunk process $s_{\\theta}$ is parameterized as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{\\theta}=\\mathrm{DGT}(\\mathbf{y}_{s},\\mathbf{y}_{i},\\mathbf{y}_{C})+\\sum_{i}\\mathrm{PDGT}_{i}(\\mathbf{y}_{s},\\mathbf{y}_{i},\\mathbf{y}_{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathrm{PDGT}_{i}$ resembles the stem process networks $s_{\\phi_{i}}$ , which is obtained by pooling to a onedimensional variable by an MLP operation, over the output of the DGT block. To optimize Eq (9), DGT minimizes the denoising score matching objective from [28] for node, edge and position information $(h,A,x)$ , while $\\mathrm{PDGT}_{i}$ for the perturbed property $\\mathbf{y}_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "B.2 Twigs with graph convolutions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the case of 2D graphs with N nodes we consider the variable ys = (X, A) \u2208RN\u00d7F \u00d7 RN\u00d7N, where $F$ is the dimension of the node features, $\\pmb{X}\\,\\in\\,\\mathbb{R}^{N\\times F}$ are node features, $A\\,\\in\\,\\mathbb{R}^{N\\times N}$ is weighted adjacency matrix. We define the perturbed property $\\mathbf{y}_{i}\\in\\mathbb{R}$ and the (fixed) property $\\mathbf{y}_{C}\\in\\mathbb{R}$ . The stem process network $s_{\\phi_{i}}$ is given as: ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{\\phi_{i}}\\!=\\!\\mathrm{MLP}_{i}(\\mathbf{GNN}(\\mathrm{P}_{i},\\mathbf{A}));\\quad\\mathrm{P}_{i}\\!=\\!(\\boldsymbol{X}\\|\\mathbf{v}_{i},\\lVert\\mathbf{v}_{C})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{v}_{i}$ and $\\mathbf{v}_{C}$ are vectors obtained by repeating $N$ times the perturbed property $\\mathbf{y}_{i}$ and the fixed property $\\mathbf{y}_{C}$ respectively, and concatenating them into the node features matrix $\\mathbf{\\deltaX}$ . The Twigs trunk process $s_{\\theta}$ is obtained by combining the contributions from the properties $\\mathbf{y}_{i}$ derived by the stem processes $s_{\\phi_{i}}$ and the structure ${\\bf y}_{s}$ , as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{\\theta}=s_{\\theta}x\\left(\\mathbf{\\boldsymbol{X}},\\mathbf{\\boldsymbol{A}},\\mathbf{\\mathbf{y}}_{C}\\right)+\\sum_{i}s_{\\phi_{i}}(\\mathbf{\\boldsymbol{X}},\\mathbf{\\mathbf{y}}_{i},\\mathbf{\\mathbf{y}}_{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $S_{\\theta}x$ is a conditional node feature score network: $s_{\\theta^{X}}=\\mathrm{MLP}(\\mathbf{GNN}(X\\parallel\\mathbf{y}_{C},{\\cal A}))$ . Finally, following [37], $\\pmb{A}$ is co-evolved together with the node features, by the adjacency score model $\\pmb{s}_{\\theta}^{A}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{\\theta^{A}}=\\mathrm{MLP}\\left(\\left[\\left\\{\\mathrm{GMH}\\left(H_{i},A_{t}^{p}\\right)\\right\\}_{i=0,p=1}^{K,P}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where GMH is graph multi-head attention [2], which employs higher-order adjacency matrices $A_{t}^{p}$ , and $K$ denotes the number of GMH layers. The optimization for the Twigs objective function (9), is obtained by minimizing the denoising score matching for $A,X,\\operatorname{P}_{i}$ . ", "page_idx": 18}, {"type": "text", "text": "The GMH block employs higher-order adjacency matrices $A_{t}^{p}$ to represent the long-range dependencies and is provided as: s\u03b8A(Gt) = MLP  {GMH (Hi, Atp )}iK=,0P,p=1  . ", "page_idx": 18}, {"type": "text", "text": "C Additional experimental results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 QM9 dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Further details for generation conditioned on quantum properties from Section 4.1. ", "page_idx": 18}, {"type": "text", "text": "Molecular quality. Additional results for molecular stability in 2D and Fr\u00e9chet ChemNet Distance (FCD) for 2D and 3D are given in Table 12. ", "page_idx": 18}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/99ee1bdc4ce9f5efb11e2e87849853a1a2c3817cb627a3572f454a630adddb09.jpg", "table_caption": ["Table 12: Molecule quality results "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 ZINC250K dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Conditional generation. The evaluation is performed by measuring the MAE of the pre-trained predictors released from [45], which given a molecule $G_{t}$ are trained to predict ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Obj}=\\widehat{\\mathrm{DS}}(\\pmb{G}_{t})\\times\\mathrm{QED}(\\pmb{G}_{t})\\times\\widehat{\\mathrm{SA}}(\\pmb{G}_{t})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\widehat{\\sf D S}$ is the normalized docking score (DS) of the considered target protein, QED is the druglikeness, and $\\widehat{\\sf S A}$ is the normalized synthetic accessibility (SA). ", "page_idx": 18}, {"type": "text", "text": "In terms of baselines, we consider the MOOD model [45], which leverages a classifier-based guidance scheme, and we also implement a diffusion guidance version of GDSS [37] based on the classifier-free scheme. Our Twigs method is parameterized by the architecture described in B.2, with a single stem process. The models are conditioned on the function in Equation (37). ", "page_idx": 18}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/5b6410c0f76b8a9f9e6c56f9cf60fbd0c8c8a72c0fbf2ae1551e85440453b6e3.jpg", "table_caption": ["Table 13: MAE for ZINC250K conditioned on single properties. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Results. In Table 13, we report the mean MAE values over multiple runs computed from the generated molecules using the pre-trained classifiers from [45]. We can observe that the Twigs consistently achieves a lower error, demonstrating an improved control over generating molecules with the desired target proteins. ", "page_idx": 19}, {"type": "text", "text": "Runtime. We have incorporated the runtime for molecule generation at inference time for a largescale dataset (ZINC250K) as for Section 4.3, in Table 14. A comparison with MOOD [45] indicates that our model incurs a certain overhead, as anticipated. However, it demonstrates improved alignment when generating conditional molecules. ", "page_idx": 19}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/a5c1c5d2d91c913737750b3d68212e95d2e8333ca864c7d9ab9970442318bbfc.jpg", "table_caption": ["Table 14: Runtime for inference on molecule generation. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Experimental details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Computational resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All experiments are performed with GPUs, Nvidia A100 or v100. ", "page_idx": 19}, {"type": "text", "text": "D.2 Models details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We follow the data splits from Huang et al. [28] for 4.1, 4.2, the ones from Lee et al. [45] for 4.3, and the data splits from Jo et al. [37] for 4.4. We use Adam optimizers on all experiments. ", "page_idx": 19}, {"type": "text", "text": "For Sections 4.1 and 4.2 we follow the same hyperparameters from Huang et al. [28]. For Section 4.3 we follow the hyperparameters from Lee et al. [45], for the MOOD baseline, we explore OOD coefficients between 0.01 and 0.09. For Section 4.4 we follow the hyperparameters from Jo et al. [37]. ", "page_idx": 19}, {"type": "text", "text": "E Additional Related Works ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section extends the discussion presented in Section 2 by exploring additional related works in the field. In Table 15 we summarise related methods including score-sdes, hierarchical models (not necessarily conditional), and hierarchical conditional models. ", "page_idx": 19}, {"type": "text", "text": "Conditional molecular diffusion. Guidance techniques have also been adopted in conditional molecule generation settings: in the context of classifier-free approaches, Hoogeboom et al. [26] proposes an equivariant approach based on DDPM for 3D molecules; Huang et al. [28] explores attention mechanisms within SGM models; and Xu et al. [82] investigates DDPMs in latent space settings. ", "page_idx": 19}, {"type": "text", "text": "In terms of classifier-based guidance, Bao et al. [3] incorporate energy guidance into a diffusion model by leveraging a stochastic differential equation; Vignac et al. [75] provide a DDPM coupled with a classifier over quantum molecular properties; and Lee et al. [45] operate over a pre-trained SGM and train an additional predictor for fine-tuning the desired protein target properties. ", "page_idx": 19}, {"type": "table", "img_path": "fvOCJAAYLx/tmp/aa362750efbcf01ae5493f74d498d526888a13fb8f988222e8713a5686ce0ecd.jpg", "table_caption": ["Table 15: Comparison with related works. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Guidance methods. Recent works utilize multiple diffusion processes: cascaded diffusion [25], provides a flow for each resolution, and GDSS [37] has a joint system of diffusion processes one for nodes and the other for edge features, but it does not cover mechanisms for conditional generation. Tseng et al. [71] define a hierarchy of branching points within a single diffusion flow. ", "page_idx": 20}, {"type": "text", "text": "Other Diffusion methods for Graphs. Other works related to ours focus on hierarchical diffusion processes [7], diffusion applied to protein backbones [69], geometry-based models [59, 82], and autoregressive models [41]. In the realm of stochastic differential equation (SDE)-based approaches, the literature includes bridge methods [38], permutation invariance [27], torsional modeling [36], and docking [6]. Additionally, [63] introduces the ConfGF approach, estimating gradient fields of atomic coordinates, while [80] proposes a method steering the training of diffusion-based generative models using physical and statistical prior information. ", "page_idx": 20}, {"type": "text", "text": "Autoencoder-Based graph models. This category includes works employing autoencoders, such as retrieval-based models [77, 12], scaffold modeling [50], link design [29], and coarse-grain modeling [76]. Notably, [54] proposes a reaction-embedded and structure-conditioned variational autoencoder, while [42] defines the concept of principal subgraphs, relevant to informative patterns within molecules. ", "page_idx": 20}, {"type": "text", "text": "Conditional Diffusion. In the realm of diffusion generative models, several noteworthy approaches have been developed to enhance their performance and versatility. Du et al. [10] introduce an energy-based parameterization of diffusion models, allowing the integration of novel compositional operators and Metropolis-corrected samplers. Building on this, He et al. [22] contribute a training-free conditional generation framework, leveraging pretrained diffusion models focusing on the manifold hypothesis to refine guided diffusion steps and introduce a shortcut algorithm. Meanwhile, Meng et al. [51] employ a stochastic differential equation (SDE) in synthesizing realistic images, iterating through denoising steps guided by a pretrained diffusion model. ", "page_idx": 20}, {"type": "text", "text": "In a different vein, Song et al. [66] propose guiding denoising diffusion models with general differentiable loss functions in a plug-and-play manner, facilitating controllable generation without additional training. Addressing the challenge of inferring high-dimensional data within the context of diffusion models, Graikos et al. [17] present a model consisting of a prior and an auxiliary differentiable constraint. Dinh et al. [9] tackle diversity and adversarial effects in classifier guidance for diffusion generative models by allowing relevant classes\u2019 gradients to contribute to shared information construction during noisy early sampling steps. Furthermore, Song et al. [65] put forth a method for estimating conditional scores without additional training. Lastly, Ouyang et al. [56] propose the Contrastive-Guided Diffusion Process (Contrastive-DP), integrating contrastive loss to guide the diffusion model in data generation. These diverse contributions collectively advance the field by addressing various challenges and expanding the capabilities of diffusion generative models. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We demonstrate with theoretical results and a comprehensive set of experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Limitations provided in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Proofs provided in Section A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 4 we provide details to reproduce the results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "The implementation details are in appendix to run the experiments. The used datasets are public and can be accessed with the reference paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Described in D.2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report mean and standard deviation in our experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Described in D.1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Provided in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Does not apply for our paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The resources that we used are cited, the source code we used is released on open licenses. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve Crowdsourcing. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve IRB. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]