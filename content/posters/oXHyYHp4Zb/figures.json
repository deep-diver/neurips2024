[{"figure_path": "oXHyYHp4Zb/figures/figures_1_1.jpg", "caption": "Figure 1: SparseLLM decomposes the global pruning of LLMs into manageable subproblems by leveraging the chain of modules and auxiliary variables while maintaining dependencies.", "description": "This figure compares three different pruning methods: global pruning, local pruning, and SparseLLM. Global pruning attempts to prune the entire model at once, which is computationally expensive and impractical for large language models. Local pruning, on the other hand, prunes each layer independently, which can lead to suboptimal performance. SparseLLM addresses the limitations of both global and local pruning by decomposing the global pruning objective into multiple subproblems. Each subproblem can be solved independently, and the results are then combined to achieve a globally optimal solution. The use of auxiliary variables helps to maintain the dependencies between different layers and ensures that the pruning process is efficient and effective.", "section": "1 Introduction"}, {"figure_path": "oXHyYHp4Zb/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of SparseLLM on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., \u2248) allow SparseLLM to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are analytically solvable and enjoy fast convergence.", "description": "This figure illustrates how SparseLLM decomposes the global pruning problem of LLMs into smaller, manageable subproblems.  It shows the architecture for both OPT and LLaMA models, highlighting the use of auxiliary variables and soft constraints (\u2248) to maintain dependencies between subproblems while allowing for efficient, parallel optimization.  The analytical solvability of these subproblems contributes to the fast convergence of the SparseLLM algorithm.", "section": "4 SparseLLM: Towards global pruning for LLMs"}, {"figure_path": "oXHyYHp4Zb/figures/figures_8_1.jpg", "caption": "Figure 3: Fast convergence of SparseLLM. Training loss per epoch for pruning layer 3 of OPT-125m at 80% sparsity (Left) and layer 6 of LlaMA-2 13b at 70% sparsity (Right).", "description": "This figure demonstrates the rapid convergence of the SparseLLM algorithm during training.  The left panel shows the training loss for pruning layer 3 of the OPT-125m model at 80% sparsity, while the right panel shows the training loss for pruning layer 6 of the LlaMA-2 13b model at 70% sparsity. In both cases, the training loss decreases significantly within the first few epochs, illustrating the algorithm's efficiency in achieving a global optimal solution quickly.", "section": "5 Experiments"}, {"figure_path": "oXHyYHp4Zb/figures/figures_13_1.jpg", "caption": "Figure 2: Illustration of SparseLLM on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., \u2248) allow SparseLLM to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are analytically solvable and enjoy fast convergence.", "description": "This figure illustrates how SparseLLM, a novel global pruning framework, handles the decomposition of the global pruning problem into smaller, more manageable subproblems. It showcases how the framework operates on both OPT and LLAMA architectures by leveraging auxiliary variables and soft constraints (represented by the \u2248 symbol) to maintain dependencies between subproblems. The use of auxiliary variables allows for an efficient optimization process, resulting in faster convergence and analytically solvable subproblems. This decomposition is crucial for handling the computational challenges posed by large language models (LLMs) and allows for global pruning without the excessive memory requirements of traditional global pruning methods.", "section": "4 SparseLLM: Towards global pruning for LLMs"}, {"figure_path": "oXHyYHp4Zb/figures/figures_14_1.jpg", "caption": "Figure 4: Illustration of SparseLLM pruning method compared to conventional global pruning and local pruning. We consider a two-layer neural network as an abstraction for simplicity. Global pruning (left) is memory prohibitive due to poor scalability. Local pruning (mid) considers pruning each layer independently, while inevitably sacrificing performance due to the ignorance of global supervision. Our adaptive global pruning (right) achieves global pruning with low memory cost by leveraging auxiliary variables and soft constraints.", "description": "This figure compares three different pruning methods: global pruning, local pruning, and the proposed SparseLLM method. Global pruning is shown to be memory-prohibitive due to its need to consider all layers simultaneously. Local pruning, while memory efficient, sacrifices performance by ignoring global relationships between layers.  SparseLLM addresses these issues by employing auxiliary variables and soft constraints to decompose the global pruning problem into more manageable subproblems, allowing for both resource efficiency and the preservation of global optimality.", "section": "4 SparseLLM: Towards global pruning for LLMs"}, {"figure_path": "oXHyYHp4Zb/figures/figures_15_1.jpg", "caption": "Figure 2: Illustration of SparseLLM on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., \u2248) allow SparseLLM to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are analytically solvable and enjoy fast convergence.", "description": "This figure illustrates how SparseLLM, a novel global pruning framework, decomposes the global pruning problem into smaller, manageable subproblems for both OPT and LLAMA architectures.  It uses auxiliary variables and soft constraints (represented by \u2248) to maintain dependencies between these subproblems, allowing for efficient optimization.  The method is designed to be analytically solvable, leading to faster convergence compared to traditional global pruning approaches which suffer from scalability issues and suboptimal performance.", "section": "4 SparseLLM: Towards global pruning for LLMs"}, {"figure_path": "oXHyYHp4Zb/figures/figures_15_2.jpg", "caption": "Figure 2: Illustration of SparseLLM on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., \u2248) allow SparseLLM to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are analytically solvable and enjoy fast convergence.", "description": "This figure illustrates the SparseLLM framework applied to both OPT and LLAMA models.  It highlights how auxiliary variables and soft constraints are used to break down the global pruning problem into smaller, more manageable subproblems. This decomposition allows for efficient optimization, maintaining the dependencies between the subproblems.  The figure emphasizes that the resulting subproblems have analytically solvable solutions, leading to faster convergence during the optimization process.", "section": "4 SparseLLM: Towards global pruning for LLMs"}]