{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models", "publication_date": "2023-07-09", "reason": "This paper introduces LLaMA 2, a large language model that is central to the paper's experiments and analysis."}, {"fullname_first_author": "Jason Wei", "paper_title": "Emergent Abilities of Large Language Models", "publication_date": "2022-06-07", "reason": "This paper is referenced to highlight the transformative impact of LLMs on natural language processing, a key theme of the paper."}, {"fullname_first_author": "Elias Frantar", "paper_title": "Massive Language Models Can Be Accurately Pruned in One-Shot", "publication_date": "2023-01-01", "reason": "This paper introduces SparseGPT, a highly relevant local pruning method for LLMs that serves as a baseline for comparison in the current work."}, {"fullname_first_author": "Mingjie Sun", "paper_title": "A Simple and Effective Pruning Approach for Large Language Models", "publication_date": "2023-06-26", "reason": "This paper introduces Wanda, another significant local pruning technique for LLMs used as a comparison baseline in the study."}, {"fullname_first_author": "Yann LeCun", "paper_title": "Optimal Brain Damage", "publication_date": "1989-01-01", "reason": "This foundational paper introduces the concept of pruning in neural networks, providing historical context and theoretical underpinnings for the current research on LLM pruning."}]}