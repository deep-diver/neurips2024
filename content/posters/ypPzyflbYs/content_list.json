[{"type": "text", "text": "Neural Concept Binder ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wolfgang Stammer1,2,\u2217 Antonia W\u00fcst1,\u2217 David Steinmann1,2,\u2217 Kristian Kersting1,2,3,4 ", "page_idx": 0}, {"type": "text", "text": "1Computer Science Department, TU Darmstadt; 2Hessian Center for AI (hessian.AI);   \n3German Research Center for AI (DFKI); 4Centre for Cognitive Science, TU Darmstadt ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct. Achieving this in an unsupervised manner requires human users to understand the model\u2019s learned concepts and, if necessary, revise incorrect ones. To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as \u201cconcept-slot encodings\u201d. NCB employs two types of binding: \u201csoft binding\u201d, which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent \u201chard binding\u201d, achieved through hierarchical clustering and retrieval-based inference. This enables obtaining expressive, discrete representations from unlabeled images. Moreover, the structured nature of NCB\u2019s concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks. We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset. Code and data at: project page. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "An essential aspect of visual reasoning is obtaining a proper conceptual understanding of the world by learning visual concepts and processing these into a suitable representation (cf. Fig. 1). The majority of current machine learning (ML) approaches that focus on visual concept-based processing utilize forms of supervised [34, 68, 32, 84], weakly-supervised [41, 69, 49, 63, 8, 82] or text-guided [29] learning of concepts. These approaches all require some form of additional (prior) knowledge about the relevant domain. An attractive alternative, though much more challenging, is to learn concepts in an unsupervised fashion. This comes with several challenges: (i) learning an expressive concept representation without concept supervision is intrinsically difficult [40], and (ii) there is no guarantee that learned concepts align with general domain knowledge [36, 85, 9] and (iii) can therefore be utilized for complex downstream tasks. Moreover, (iv) to trust that the learned concept representations are reliable for high stakes scenarios [15], it is necessary to make the model\u2019s concept representations human-inspectable and -revisable [68, 69, 31] (cf. Fig. 1 (left)). ", "page_idx": 0}, {"type": "text", "text": "These challenges raise questions about the nature of the unsupervised learned concept representations. Continuous encodings [60, 59, 77, 79] are easier to learn and more expressive. However, they are difficult to interpret and suffer from problems related to poor generalization [81] and information leakage [47, 50]. On the other hand, discrete encodings [69, 79, 26, 4] are hard to learn [44, 72, 9], but are easier to understand and thus align, e.g., to a task at hand. ", "page_idx": 0}, {"type": "image", "img_path": "ypPzyflbYs/tmp/60d0b38aa11f6f031883f1febb4c73129695dbfaa3edca13d116bb8ac438b177.jpg", "img_caption": ["Figure 1: Unsupervised learning of concepts for visual reasoning. (left) Models that learn concepts from unlabeled data require inspectable and revisable concept representations. (right) Concepts obtained from the Neural Concept Binder (NCB) can be utilized both in (interpretable) neural and symbolic computations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This work proposes the Neural Concept Binder (NCB) framework to learn expressive, yet inspectable and revisable, concepts from unlabeled data. NCB combines continuous encodings, obtained via block-slot-based soft-binding, with discrete concept representations, derived through retrieval-based hard-binding. NCB\u2019s soft binding leverages the object-factor disentanglement capabilities of the recent SysBinder mechanism [65]. Subsequently, NCB\u2019s hard binding mechanism utilizes HDBSCAN [12, 13] to cluster the continuous block-slot encodings, distilling a structured corpus of discrete concepts from these clusters. This corpus enables the retrieval of discrete concept representations during inference by matching the continuous encoding with the closest entries in the corpus. Thus, to address the challenges of unsupervised concept learning, NCB integrates the strengths of both continuous and discrete concept representations. Moreover, NCB enables straightforward concept inspection and facilitates easy revision procedures, allowing alignment of the learned concepts with prior knowledge. In our evaluations, we demonstrate that NCB\u2019s discrete concept-slot encodings retain the expressiveness of their continuous counterparts. Moreover, they can be seamlessly integrated into downstream applications via symbolic and interpretable neural computations (cf. Fig. 1 (right)). In this context, we introduce our novel CLEVR-Sudoku dataset, which presents a challenging visual puzzle that requires both perception and reasoning capabilities (cf. Fig. 4). ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are the following: (i) we introduce the Neural Concept Binder framework (NCB) for unsupervised concept learning, (ii) we show the possibilities to integrate NCB with symbolic and subsymbolic modules in challenging downstream tasks, achieving performance on par with supervised trained models, (iii) we highlight the possibilities of easy concept inspection and revision via NCB, and (iv) we introduce the novel CLEVR-Sudoku dataset, which combines challenging visual perception and symbolic reasoning. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Unsupervised visual concept learning focuses on obtaining concept-level representations from unlabeled images [25]. Some works have tackled this only for specific domains, such as extracting \u201cteachable\u201d concepts for chess [62] or learning manipulation concepts from videos of task demonstrations [39]. Others rely on object-level concept guidance through initial image segmentations [27] or \u201cnatural supervision\u201d [49]. In contrast, Vedantam et al. [77] and W\u00fcst et al. [81] focus on learning higher-level relational concepts, i.e., assuming that basic-level concepts have already been provided. Several approaches learn concepts from the training signal of an image classification task [78, 1, 14, 38], often focusing on image-region-based concepts [22]. More recently, several works have explored leveraging the knowledge stored in large pretrained models, such as combining large language models with CLIP embeddings [82, 52] or using weakly-supervised queries to a vision-language model [8]. These approaches still rely on some form of supervision, whether through text, class labels, or prompts. In contrast, this work focuses on learning unsupervised concepts at both the object and factor levels, ensuring that these concepts remain inherently inspectable and revisable. ", "page_idx": 1}, {"type": "text", "text": "The motivation for inherently inspectable and revisable concept representations is to allow human stakeholders to investigate and potentially revise a model\u2019s internal concepts. Most research in this area focuses on post-hoc approaches that distill concept knowledge from pretrained models [83, 21, 57, ", "page_idx": 1}, {"type": "image", "img_path": "ypPzyflbYs/tmp/14bcc965aa8312fc8a085bed6dceecad391e86feb651ecb7fb67559b9ad121bd.jpg", "img_caption": ["Neural Concept Binder (NCB) ", "Figure 2: The Neural Concept Binder (NCB) combines continuous, block-slot encodings via slotattention based image processing with discrete, concept-slot encodings via retrieval-based inference. The structured retrieval corpus (distilled from the block-slot encodings) allows for easy concept inspection and revision by human stakeholders. Moreover, the resulting concept-slot encodings can be easily integrated into complex downstream tasks. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "18, 23]. In contrast, Lage and Doshi-Velez [35] explore learning inspectable concept representations through human feedback, focusing on tabular data and higher-level concepts. Similarly, Stammer et al. [69] develop inherently inspectable visual concepts using weak supervision and a prototype-based binding mechanism. However, no existing work addresses the development of inherently inspectable and revisable concept representations in the context of unsupervised visual learning. ", "page_idx": 2}, {"type": "text", "text": "The properties of discrete vs. continuous encodings are a vibrant research topic that is highly relevant to learning suitable concept representations. Continuous encodings allow for easier and more flexible optimization and information binding [43, 64, 65, 7]. However, discrete representations are considered essential for understanding AI models [31], mitigating shortcut learning [68, 3], and solving complex visual reasoning tasks [26, 66]. Despite their advantages, learning discrete representations through neural modules remains a challenging problem [44, 24, 20, 74]. While some works have focused on categorical-distribution-based discretization [4, 28, 46], others have explored retrieval-based discretization of continuous encodings using various forms of inherent \u201ccodebooks\u201d [73, 71]. Only a few studies have explicitly addressed how to bind semantic visual information to specific discrete representations [69]. Whereas previous works typically emphasize one of the two representation types, we see great potential in the recent trend of explicitly integrating both discrete and continuous representations [17, 32, 84, 51]. ", "page_idx": 2}, {"type": "text", "text": "3 Neural Concept Binder (NCB): Extracting Hard from Soft Concepts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we refer to a concept as \"the label of a set of things that have something in common\" [2]. This definition can be applied on different scales of a visual scene: on an image level (e.g., an image of a park), an object level (e.g., a tree vs. a bird) or an object-factor level (e.g., the color of a bird). Our proposed Neural Concept Binder (NCB) framework tackles the challenge of learning inspectable and revisable object-factor level concepts from unlabeled images by combining two key elements: (i) continuous representations via SysBinder\u2019s block-slot-attention [65, 43] with (ii) discrete representations via retrieval-based inference. Fig. 2 provides an overview of NCB\u2019s inference, training, concept inspection, and revision processes. Let us formally introduce these processes. ", "page_idx": 2}, {"type": "text", "text": "Overall, we consider a set of unlabeled images $X:=(x_{1},\\cdot\\cdot\\cdot\\,,x_{N})\\in\\mathbb{R}^{N\\times D}$ with $x_{i}\\in\\mathbb{R}^{D}$ , $N\\in\\mathbb{N}$ and $D\\in\\mathbb{N}$ (for simplicity, we drop the image index notation in the following). Briefly, given an image, x, NCB infers latent block-slot encodings, $\\mathbf{Z}$ , and performs a retrieval-based discretization step on ${\\bf Z}$ to infer concept-slot encodings, c. These express the concepts of the objects in the image, i.e., object-factor level concepts. We begin by introducing the inference procedure of NCB. We hereby assume that NCB\u2019s components have already been trained and will introduce details of the training procedure subsequently. ", "page_idx": 2}, {"type": "text", "text": "3.1 Inferring Concept-Slot Representations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Obtaining Continuous Block-Slot-Encodings. Consider an image $x\\in X$ . The first component of NCB, the soft binder, is based on the systematic binding mechanism [65] and is represented by a block-slot encoder (cf. Fig. 2 (i)), $g_{\\theta}:\\dot{\\boldsymbol{x}}\\rightarrow\\boldsymbol{z}\\in\\mathbb{R}^{N_{S}\\times\\breve{N}_{B}\\times D_{B}}$ , where $g$ is parameterized by $\\theta$ (for simplicity, this notation is omitted in the following). The soft binder transforms an input image into a latent, continuous block-slot representation, where $N_{S}$ represents the number of slots, $N_{B}$ the number of blocks per slot, and $D_{B}$ the dimension of each block. The soft binder employs two key types of binding mechanisms: spatial and factor binding. Spatial binding ensures spatial modularity across the entire scene and is achieved through slot attention [43], allowing each object in the image to be represented in a specific slot, $z_{i}$ . Factor binding, introduced by Singh et al. [65], ensures that different object factors (e.g., attributes like color) are encoded in separate blocks of a slot, i.e., $z_{i}^{j}$ These two binding mechanisms work together to perform object- and factor-based image processing. We refer to Suppl. A.1 for additional details on both systematic (factor) binding and slot attention. Overall, the resulting block-slot encodings represent continuous, object-centric representations of the input image, with objects encoded in slots and object factors encoded within the blocks of those slots. ", "page_idx": 3}, {"type": "text", "text": "Obtaining Discrete Concept-Slot-Encodings. The role of NCB\u2019s second processing component, the hard binder, is to transform the continuous block-slot encodings into expressive, yet discrete concept-slot encodings. Specifically, the hard binder is represented by a retrieval encoder, $f$ (cf. Fig. 2 (v)), which processes the block-slot encodings, $z,$ , into a set of discrete concept-slot encodings, $c$ . In detail, $f$ defines a function $f_{\\mathcal{R}}\\,:\\,z\\,\\rightarrow\\,c\\,\\in\\,\\bar{\\mathbb{N}}^{N_{S}\\times N_{B}}$ , parameterized by a retrieval corpus $\\mathcal{R}$ (cf. Fig. 2 (iv)). This retrieval corpus consists of a tuple of sets $\\mathcal{R}:=[\\mathcal{R}^{\\bar{1}},\\ldots,\\mathcal{R}^{N_{B}}].$ , where each set $\\mathcal{R}^{j}:=\\{({\\sf e n c}_{l}^{j},v_{l}):l\\in\\{1,...,|\\mathcal{R}^{j}|\\}\\}$ contains tuples of block encodings, $\\mathsf{e n c}_{l}^{j}\\in\\mathbb{R}^{D_{B}}$ , and corresponding discrete values, $v_{l}\\in\\{1,\\cdots,N_{C}\\}$ . Importantly, $\\mathtt{e n c}_{l}^{j}$ is a representative block encoding of a specific concept cluster, determined during NCB\u2019s training phase (cf. Fig. 2 (iii), detailed below). $v_{l}$ serves as the symbol identifier for the concept cluster associated with $\\mathtt{e n c}_{l}^{j}$ . Each block can contain up to $N_{C}\\in\\mathbb{N}$ different concepts. To infer the concept symbol for a sample\u2019s block-slot encoding, NCB compares $z_{i}^{j}$ with the encodings in the corresponding block\u2019s retrieval corpus, $\\mathcal{R}^{j}$ , and selects the most fitting concept. Specifically, given a distance metric $d(\\cdot,\\cdot)$ and the block-slot encoding $z_{i}^{j}$ , the selection function $s_{\\mathcal{R}}:z_{i}^{j}\\to l\\in\\mathbb{N}$ (Fig. 2 (v)) finds the index $l$ of the closest encoding in the retrieval corpus: $s_{\\mathcal{R}}(z_{i}^{j})=\\mathrm{argmin}_{l}\\,d(\\mathsf{e n c}_{l}^{j},z_{i}^{j})$ such that $({\\sf e n c}_{l}^{j},v_{l})\\in\\mathcal{R}^{j}$ . This results in the concept representation for slot $i$ and block $j$ , denoted as $c_{i}^{j}:=v_{s_{\\mathcal{R}}(z_{i}^{j})}$ . For slot $i$ , the full concept representation is denoted as $c_{i}:=[c_{i}^{1},\\ldots,c_{i}^{N_{B}}]$ and the final concept-slot encoding as $c:=f_{\\mathcal{R}}(z)=\\bar{[c}_{1},\\ldots,c_{N_{S}}\\right]$ . We refer to Suppl. A.2 for details on an alternative top- $.k$ selection function. We further note that NCB\u2019s flexibility, in principle, allows also to utilize the continuous encodings of its soft binder (Fig. 2 dashed arrow) in case a downstream task requires it. Let us now move on to NCB\u2019s training procedure. ", "page_idx": 3}, {"type": "text", "text": "3.2 Unsupervised Concept Learning via NCB ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The training procedure of the Neural Concept Binder is separated into two subsequent steps where we provide an overview here and details in Suppl. A.3. We formally describe these steps using the pseudocode in Alg. 1. The first step consists of optimizing the encoder, $g$ , to provide object-factorised blockslot encodings. It is optimized for unsupervised image reconstruction based on the decoder model, $g_{\\theta^{\\prime}}^{\\prime}:z\\to\\tilde{x}\\overset{\\subset}{\\in}\\mathbb{R}^{D}$ (Fig. 2 (ii)) and utilizing a mean squared error loss: $L=L_{\\mathrm{MSE}}(x,g^{\\prime}(g(x)))$ . The goal of NCB\u2019s second training step is to obtain the retrieval corpus, $\\mathcal{R}$ . This procedure is based on obtaining an optimal clustering of block encodings via an unsupervised clustering model, $h$ , and distilling the resulting information from $h$ into explicit representations in the retrieval corpus. For each block $j$ a clustering model $h_{\\phi^{j}}$ (Fig. 2 (iii)) is fti to identify a potentially overparameterised set of clusters within a set of block encodings (based on an unsupervised criterion, e.g., a density-based score [53]), resulting in $N_{C}\\in\\mathbb{N}$ clusters. Next, for each cluster, $v\\in\\{1,\\cdots\\,,N_{C}\\}$ , representative block encodings, $\\mathsf{e n c}^{j}$ , are extracted from $h$ . Such an encoding represents either an averaged prototype or instance-based exemplar encoding. The corresponding tuples $(\\mathtt{e n c}^{j},v)$ are explicitly stored in the retrieval corpus $\\mathcal{R}^{j}$ (Fig. 2 (iv)) where we use the index $l$ to identify specific encodings in $\\mathcal{R}^{j}$ , leading to $\\mathcal{R}^{j}:=\\{(\\mathbf{enc}_{l}^{j},v_{l}):l\\in\\{1,...,|\\mathcal{R}^{j}|\\}\\}$ . Thus, $\\mathtt{e n c}_{l}^{j}$ represents one block encoding of $\\mathcal{R}^{j}$ that has been assigned to cluster $v_{l}$ . Finally, $\\mathcal{R}=[\\mathcal{R}^{1},\\cdot\\cdot\\cdot\\,,\\mathcal{R}^{N_{B}}]$ represents the final retrieval corpus, i.e., the set of corpora for each block. Through this training procedure, NCB learns to unsupervisedly categorize the object-factor information from the latent encoding space of the soft binder and stores this information in a structured, symbolic, and accessible way in the hard binder\u2019s retrieval corpus. We refer to the resulting clusters of each block as NCB\u2019s concepts and denote concepts with a capital letter for the block and a natural number for the category id, e.g., $A3$ . We note that in practice, it is further possible to finetune the block-slot encoder, $g$ , through supervision from the hard binder (cf. gray arrow in Fig. 2), e.g., once initial categories have been identified, and can be achieved via a standard supervised approach. Ultimately, this allows for dynamically finetuning NCB\u2019s concept representations. Let us now introduce how human stakeholders can inspect and revise NCB\u2019s learned concepts. ", "page_idx": 3}, {"type": "image", "img_path": "ypPzyflbYs/tmp/0c964f788e1ca7a8d18bc1b6bd9749032b49162e730b3ef6f2f559ac3d708347.jpg", "img_caption": ["Figure 3: NCB\u2019s concept space is inherently inspectable. A human stakeholder can easily inspect the concept space by asking a diverse set of questions. For example, NCB answers interventional questions (iii) via generating images with selectively modified concepts. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Inspecting and Revising NCB\u2019s Concepts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspection. NCB inherently enables: (i) implicit, (ii) comparative, (iii) interventional and (iv) similarity-based inspection (cf. Fig. 3). Where the first three aim at investigating NCB\u2019s explicit, symbolic concept space (stored in $\\mathcal{R}$ ), the last one aims at investigating its latent, continuous concept space (stored in $\\theta$ ). (i) Implicit inspection queries the model to provide a set of examples for a specific concept. Essentially, this answers the question \"What are examples of this concept?\". NCB answers this question in two ways: by providing samples from the retrieval corpus corresponding to exemplars of the concept or by identifying additional data samples belonging to the concept at hand. (ii) Comparative inspection, on the other hand, allows comparing two specifically different concepts, e.g., \"Why does this object depict concept $H5$ and not concept $H l?^{\\prime\\prime}$ . NCB hereby provides examples for both concepts for the user to compare and potentially identify dissimilar properties. Ultimately, this form of inspection allows to answer questions of the form \"Why not ...?\" and represents a valuable tool for in-depth and targeted concept inspection. (iii) Interventional inspection allows to answer questions such as \"What if this object would have concept $H l\\operatorname{?}$ To answer this question, NCB utilizes its decoder $g^{\\prime}$ . Specifically, by swapping the block $z_{i}^{j}$ of a data sample\u2019s block-slot encoding with that of a representative sample, $(\\mathbf{enc}_{l}^{j},v_{l})\\in\\mathcal{R}^{j}$ , NCB can provide an interventional image reconstruction, from which the effect of the swapped concept can be observed. Ultimately, this form of inspection allows to answer important questions of the form \"What if ...?\". Finally, (iv) Similarity inspection allows inspecting NCB\u2019s continuous encoding space on a more global level (in comparison to the more symbolic, sample-based inspection above), e.g., \"What are similar concepts to this concept?\". Specifically, NCB\u2019s distance metric $d$ directly provides information about the similarity between concepts in the continuous representation. Inspecting the block-slot encoding space thus allows to identify a suboptimal soft binding, e.g., when block encodings are similar according to $g$ but not according to the human stakeholder. Overall, these inspection mechanisms allow a human stakeholder to ask a diverse set of questions concerning a model\u2019s learned concepts (cf. Fig. 15, Fig. 16 and Fig. 17 for additional examples of the inspection types). ", "page_idx": 4}, {"type": "text", "text": "Revise. Let us now describe how a human stakeholder can revise NCB\u2019s concept space. Below, we provide details on the three main actions for symbolic revision (i.e., revision on the representations in $\\mathcal{R}$ ): (i) merging, (ii) deleting, or (iii) adding information. These actions can be performed on a single encoding or on a concept level and essentially represent a form of \"reorganization\" of information stored in $\\mathcal{R}$ . Furthermore, we provide details on how to (iv) revise the continuous latent space, which essentially requires finetuning of $g$ \u2019s parameters. (i) Merge Concepts: In the case that $\\mathcal{R}$ contains multiple concepts that, according to additional knowledge (e.g., from a human or other model), represent a joint underlying concept (e.g., two concepts for purple in Fig. 3 (right)) it is easy to update the model\u2019s internal representations by replacing the concept symbols of one concept with those of the second concept. Specifically, for block $j$ if concept $m$ should be merged with concept $b$ where $m,b\\in\\{1,\\cdots,N_{C}\\}$ , then for all corpus tuples, $(\\mathbf{enc}_{l}^{j},v_{l})\\in\\mathcal{R}^{j}$ , we replace $v_{l}$ with $b$ if $v_{l}=m$ . (ii) Delete Encodings or Concepts: If $\\mathcal{R}^{j}$ contains an encoding, $\\mathtt{e n c}_{l}^{j}$ , for a specific concept, $m$ , that does not match the other encodings of that concept (e.g., a misplaced exemplar) this encoding can simply be deleted from the corpus. Accordingly, if an entire concept, $m$ , is identified as suboptimal, one can simply delete all corresponding encodings of that concept. I.e., for all corpus tuples, $(\\mathbf{enc}_{l}^{j},v_{l})\\in\\mathcal{R}^{j}$ , we remove the tuple if $v_{l}=m$ . (iii) Add Encodings or Concepts: If a specific concept is not sufficiently well captured via the existing encodings in $\\mathcal{R}^{j}$ , one can simply add a new encoding, $\\mathtt{e\\hat{n}c}_{l+1}^{j}$ , for the concept, $m$ , to the corpus. This leads to an additional entry iHn etrheeb yc, oornpeu sg, $(\\widehat{\\mathbf e}\\hat{\\mathbf n}\\mathbf c_{l+1}^{j},m)$ .n cAocdcionrgdsi nofg loyb, jiet citss  talhsaot  rpeopsrseisbelnet  ttoh aat dndo evnelc coodinncgesp tf aorn da na dednst itrhee sceo tnoc ethpte. scoofrtp buisn adse $(\\widehat{\\mathbf{enc}}_{l+1}^{j},b)$ uwbiotpht $b=N_{C}+1$ .a n(idv f) aRcteovri-slee vtehl eb l(oCcokn-stliontu eonucso) dLinagtse,n itt  iSsp faucrteh: eLr apsotlsys,i bifl et htoe integrate revisory feedback on the soft binder\u2019s continuous latent space. This can be achieved via additional finetuning of the soft binder\u2019s parameters, $\\ \\theta,\\,e.g.$ , via standard forms of weak supervision [42, 69] or interactive learning [68, 61]. ", "page_idx": 4}, {"type": "table", "img_path": "ypPzyflbYs/tmp/34146473b9fd010f7be8924b331757ed5eb6205cce96c2a7c2d660320ba8ebae.jpg", "table_caption": ["Table 1: Comparison of different approaches for concept learning. Hereby, we differentiate based on the following categories: whether a method (1) is learned in an unsupervised fashion, (2) provides object-level concepts (i.e., can explicitly process multiple objects), (3) provides factor-level concepts (e.g., the color green), (4) provides continuous concept encodings, (5) provides discrete concept encodings, (6) provides inherently inspectable and (7) revisable concept representations. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In summary, our novel Neural Concept Binder framework fulfills several important desiderata for concept learning (cf. Tab. 1). Specifically, NCB learns concepts in an unsupervised fashion that are structured on both an object and factor-level. Furthermore, next to standard continuous encodings, NCB also provides discrete concept representations, which are crucial for interpretability and integration into symbolic computations. Lastly, NCB\u2019s concept space is inspectable and revisable, essential for unsupervised learned concept representations. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Evaluations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our evaluations, we investigate the potential of NCB\u2019s soft and hard binding mechanisms in unsupervised concept learning and its integration into downstream tasks. Notably, NCB encompasses concept processing between both of its components (soft binder and hard binder) whereby the direction \"soft binder $\\leftarrow$ hard binder\" (cf. Fig. 2) represents a standard approach (i.e., supervised learning of the soft binder\u2019s encoding space via symbolic concept labels, e.g., [34, 68]). Therefore, we focus our evaluations on NCB\u2019s more novel processing direction, \"soft binder $\\rightarrow$ hard binder\". We aim to answer the following research questions: (Q1) Does NCB provide expressive and distinct encodings? (Q2) Can NCB be combined with symbolic methods to solve complex downstream tasks? (Q3) Can NCB\u2019s learned concepts be revised to improve suboptimal behaviour? (Q4) Can NCB be combined with subsymbolic methods to transparently solve complex downstream tasks? ", "page_idx": 5}, {"type": "text", "text": "Data. We focus our evaluations on different variations of the popular CLEVR dataset. Specifically, we investigate (Q1 & Q3) in the context of the CLEVR [30] and CLEVR-Easy [65] datasets. For investigating the integration of NCB into symbolic modules (Q2), we utilize our novel CLEVRSudoku puzzles introduced in the following. Finally, to evaluate the integration of NCB into subsymbolic modules (Q4), we evaluate on confounded and non-confounded variants of the CLEVRHans3 dataset [68]. We provide further details on these datasets in the supplements (cf. Suppl. C). ", "page_idx": 6}, {"type": "text", "text": "CLEVR-Sudoku. To investigate the potential of integrating NCB\u2019s discrete concept representations into symbolic downstream tasks, we introduce the novel CLEVR-Sudoku dataset. This dataset presents a challenging visual puzzle that requires both visual object perception and reasoning capabilities. Each sample in the dataset (cf. Fig. 4 for an example puzzle) consists of a Sudoku puzzle (partially filled) with CLEVR-based images [30] and additional example images depicting the mapping of relevant object properties to digits. Specifically, each digit in the Sudoku is replaced by an image of an object. All objects representing the same digit share a set of common properties, e.g., in Fig. 4, all objects replacing \"1\"s are yellow spheres. ", "page_idx": 6}, {"type": "text", "text": "We introduce two variants of CLEVR-Sudoku: Sudoku CLEVR-Easy and Sudoku CLEVR. In the first variant, shape and color are distinguishing properties for the digits. In Sudoku CLEVR, additional object attributes \u2014 size and material \u2014 are relevant for the digit identification. Moreover, up to 10 example images are provided per digit mapping; the fewer examples provided, the more difficult it becomes to learn the mapping. The initial state and digit-attribute mappings vary across samples. One specific intricacy of CLEVR-Sudoku is that the puzzle can only be solved if all subcell images are correctly mapped to their corresponding digits. Even a single mistake can render the Sudoku unsolvable. Thus, compared to standard Sudoku puzzles, which primarily require deductive reasoning, solving CLEVR-Sudoku also demands complex object recognition and the ability to map visual concept perceptions to the task concepts (i.e., the 9 digits of Sudoku). For further details, we refer to Suppl. B. ", "page_idx": 6}, {"type": "image", "img_path": "ypPzyflbYs/tmp/6b90d9d9e749dcca6f4b254807954601348f749e8f333b81f0bab8a331b40805.jpg", "img_caption": ["Figure 4: Example from CLEVR-Sudoku. Each digit is represented by CLEVR objects with the same attribute combination. The objective is to solve the Sudoku only based on the initial grid of CLEVR images and the digit mapping of candidate examples. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Models. For our evaluations, we instantiate Neural Concept Binder based on the SysBinder model [65] for the soft binder encoder, $g$ , and HDBSCAN [12, 13] for the clustering model, $h$ . Further details about the instantiation can be found in Suppl. A.4. In the context of Q1, we compare NCB\u2019s results to four variations of the SysBinder model [65], as well as the recent Neural Language of Thought Model (NLOTM) [80]. We refer to the original SysBinder configuration as SysBinder (cont.), which provides continuous block-slot encodings. In SysBinder, SysBinder\u2019s continuous encodings are discretized at inference time via an argmin operation over its internal codebooks. SysBinder (hard) is trained from the beginning to produce discrete encodings using a low codebook softmax temperature. SysBinder (step) is trained with a step-wise decrease in temperature (cf. Suppl. D for details). For evaluations on CLEVR-Sudoku (Q2 and Q3), we first infer NCB\u2019s discrete concept-slot encodings from the puzzle\u2019s candidate examples. These encodings, along with their corresponding digit labels, are then passed to a symbolic classifier, which is trained to predict digits from the encodings. The classifier subsequently infers the digits for each subcell in the puzzle\u2019s initial state. These predictions are used by a constraint propagation and search-based algorithm [55, 10] to solve the puzzle (cf. Suppl. E.2 for details). We refer to the combination of the symbolic classifier and constraint solver as the solver. We compare the solver\u2019s performance when provided with ground-truth (GT) object-property labels (GT concepts), encodings from a supervised slot attention encoder [43] (SA (supervised)), and the discrete encodings from SysBinder (denoted as SysBinder (unsupervised)). For classification evaluations (Q4), we evaluate a configuration in which a set transformer classifier [37] is provided with NCB\u2019s concept encodings $(N C B+N N)$ to make final class predictions (cf. Suppl. E.4). We compare this to $S A+N N$ , where a supervised slot attention encoder [43] provides object-property predictions. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We evaluate all models based on their accuracies on held-out test splits, each with 3 seeded runs. We provide average accuracies and standard deviations over these. When assessing the expressiveness of NCB\u2019s concept-slot encodings (Q1), we evaluate the accuracy for object-property ", "page_idx": 6}, {"type": "table", "img_path": "ypPzyflbYs/tmp/c65b1d3d1c2d45ae9f877b26742e821550c005a662f37c74776ac9276861f00b.jpg", "table_caption": ["Table 2: NCB\u2019s concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best $(^{\\leftarrow}\\bullet^{\\bullet})$ and runner-up $(^{\\bullet\\bullet}\\circ^{\\bullet\\bullet})$ results are bold. "], "table_footnote": ["prediction. When evaluating the performance of the downstream tasks, we provide the percentage of solved CLEVR-Sudokus (Q2) and the classification accuracy on the test set of CLEVR-Hans3 (Q4). "], "page_idx": 7}, {"type": "text", "text": "4.1 Evaluations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Discrete, yet expressive representations (Q1). First, we investigate how much valuable information NCB\u2019s discrete concept-slot encodings contain, despite NCB\u2019s inherent information bottleneck. To assess this, we train a classifier on NCB\u2019s encodings to predict corresponding object-property labels, e.g., the color green (cf. Suppl. E.1 for details). In Tab. 2, we present the results for the CLEVR-Easy and CLEVR datasets, using classification training sets with 2000, 200, 50, or 20 encodings. Focusing first on the results for $N=2000$ , we observe that, as expected, the continuous representation of the original SysBinder model contains more information compared to all discrete encodings. Remarkably, however, NCB\u2019s discrete concept representations are nearly on par with the continuous encodings. This is particularly notable given NCB\u2019s immense information bottleneck1. Additionally, we observe that NCB\u2019s encodings significantly outperform all other forms of discrete representations. Shifting focus to the results when the classifier is trained on data subsets, we observe a substantial degradation in performance when using encodings from any of the discrete baselines or the continuous encodings. In stark contrast, when classifying based on NCB\u2019s encodings, the accuracy remains nearly constant, even with just $1\\%$ of the initial training samples. We provide additional ablations on the effect of concept encoding types and NCB\u2019s selection function in Suppl. F.1, as well as an ablation analysis on the effect of suboptimal behavior from NCB\u2019s individual components in Suppl. F.2. Further analysis of NCB\u2019s concept space can be found in Suppl. F.3, along with qualitative examples of learned concepts in Suppl. G. Overall, our results demonstrate the expressiveness of NCB\u2019s concept encodings despite their significant information bottleneck. Furthermore, our results suggest that NCB\u2019s encodings are easier to generalize compared to the baselines. Thus, we answer Q1 affirmatively. ", "page_idx": 7}, {"type": "text", "text": "Utilizing unsupervised concepts for solving visual Sudoku (Q2). In our following evaluations, we investigate the potential of NCB\u2019s representations for solving complex reasoning tasks through their integration into symbolic computations. These evaluations are based on our novel CLEVRSudoku dataset. The percentage of solved puzzles for CLEVR-Sudoku is reported in Fig. 5. It is important to note that the solver can only solve a puzzle if each image in the initial state has been classified correctly, meaning the results in Fig. 5 represent \"all-or-nothing\" outcomes. Focusing on the results to the left of the dashed lines, we observe that the symbolic module solves every puzzle with ground-truth (GT) concepts, even when only one example image is provided. Interestingly, performance drops significantly when using encodings from SA (supervised). This highlights the difficulty of the CLEVR-Sudoku puzzles: minor errors in digit prediction can lead to major failures in solving the puzzle. When comparing the performance of encodings from the two unsupervised models, we observe that NCB\u2019s concept encodings perform quite well. E.g., they enable solving approximately $50\\%$ of the puzzles for the 10-example configurations, compared to approximately $61\\%$ for SA (supervised). In contrast, when using SysBinder\u2019s encodings, the solver fails across all Sudoku variations. This demonstrates the effectiveness of NCB\u2019s binding mechanisms over those of the SysBinder approach alone. We refer to Suppl. F.4 for further discussions and quantitative digit classification results (Fig. 10). Overall, our evaluations highlight the potential of NCB\u2019s unsupervised concept encodings for solving complex symbolic downstream tasks. We therefore answer Q2 affirmatively. ", "page_idx": 7}, {"type": "image", "img_path": "ypPzyflbYs/tmp/5e08bd5593c72a653fc51573ca5b0c11fb7e8d681221578c45c6d7335ebdf46b.jpg", "img_caption": ["Figure 5: NCB\u2019s unsupervised concepts allow solving symbolic puzzles. Accuracy of solved Sudokus via different discrete concept encodings on Sudoku CLEVR-Easy and Sudoku CLEVR (left sides). Additional revision on NCB\u2019s concepts leads to improved performances (right sides). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ypPzyflbYs/tmp/60e1ced2950f26c2d61089ddf75555cb5a0b4a043b06e4db34ba0b4048a99a10.jpg", "table_caption": ["Table 3: NCB\u2019s unsupervised concept representations facilitate interpretable neural computations. Explanations of a NN classifier trained on the unsupervised concepts of NCB. Via NCB\u2019s inherent inspection procedures a human stakeholder can identify which concepts the classifier focuses on to make its predictions and thus interpret the NN\u2019s underlying decision rule. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Easily revising NCB\u2019s concepts (Q3). In our next evaluations, we illustrate the potential of NCB\u2019s revision procedures. Since revising the continuous latent space of NCB\u2019s soft binder is analogous to existing approaches (e.g., [61, 58, 69]), we focus on the novel, NCB-specific forms of symbolic revision, i.e., revisions within the hard binder\u2019s concept space. We demonstrate two forms of symbolic revision (removing and merging concept information) using feedback from two sources: a pretrained vision-language model (here via GPT-4 [56]) and simulated human feedback. In both cases, we ask the revisory agent to identify which concepts in each block should be removed or merged based on exemplar images of each concept, i.e., implicit concept inspection (cf. Suppl. E.3 for details). In Fig. 5, we show CLEVR-Sudoku performance when NCB\u2019s retrieval corpus is updated by different revisory agents (i.e., NCB revised (GPT-4) and NCB revised (human)). Interestingly, while GPT-4\u2019s revisions improve performance in settings with few examples, they have a negative impact when more digit examples are present. This is due to GPT-4\u2019s suboptimal consistency in object descriptions, leading to the removal or merging of too much concept information. This highlights the potential issue of \"ill-informed\" feedback (cf. Suppl. F.5). In contrast, human revisions provide a substantial boost in Sudoku performance, particularly in puzzle configurations with fewer candidate examples. Moreover, using NCB\u2019s similarity inspection mechanism (cf. Sec. 3.3), a human stakeholder can easily identify models that suffer from suboptimal soft binding processing. In such cases, these models can be excluded from further downstream evaluations (cf. NCB revised (human)\\*) and refined by finetuning $g$ \u2019s parameters (e.g., via approaches from [61, 58, 69]). In Suppl. F.6, we further explore concept revision by adding new information. Overall, our results demonstrate the potential and ease of revising NCB\u2019s concept space, allowing us to answer Q3 positively. ", "page_idx": 8}, {"type": "text", "text": "Utilizing unsupervised concepts for understanding neural computations (Q4). In our final evaluations, we investigate whether NCB\u2019s discrete concept encodings can make subsymbolic computations more transparent. We focus on the task of image classification using concept-bottleneck-like approaches [68, 34] on variations of the benchmark CLEVR-Hans3 dataset [68]. While the concept encodings in $N C B+N N$ are trained unsupervised, they perform on par with the supervised approach of [68] (cf. Suppl. F.7). More importantly, integrating NCB\u2019s inherently inspectable concept representations into neural computations leads to more transparent decision processes. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We illustrate this in Tab. 3, where we provide classlevel explanations of the classifier in $N C B\\,+\\,N N$ (cf. Suppl. E.4 for details). Using NCB\u2019s inspection mechanisms, human stakeholders can easily identify the classifier\u2019s internal decision rules for a class (e.g., \"a large gray object\"). This is a critical feature for deploying trustworthy AI models in real-world scenarios. The key result is that this transparency is achieved even with unsupervised concept encodings. In Fig. 6, we further investigate whether a NCBbased neural classifier can be revised to mitigate confounders in the CLEVR-Hans3 dataset (cf. Suppl. E.4 and Suppl. F.8 for details). The confounding factor in the training set is the color gray, and we present the non-confounded test set accuracy in Fig. 6. We observe that standard loss-based feedback via explanatory interactive learning (XIL) [68] on the NN classifier\u2019s explanations ( $^+$ XIL on NN) significantly reduces the effect of the confounder. Alternatively, by simply zeroing the activations of the undesired concept gray $\\left.+\\mathit{X}\\right/\\!L\\right$ on concepts), we achieve even better confounding mitigation results without the typical issues of joint optimization. Our results highlight the potential of integrating NCB\u2019s unsupervised concept representations for eliciting transparent and trustworthy subsymbolic computations. We thus answer Q4 affirmatively. ", "page_idx": 9}, {"type": "image", "img_path": "ypPzyflbYs/tmp/3cd05ced207f508a66460449f1391d23ef2eb8694bcc8c50ebea9f866d631f1f.jpg", "img_caption": ["Figure 6: NCB\u2019s unsupervised concept representations facilitate shortcut mitigation. Test accuracy for classification via NN predictor when trained on confounded images. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. NCB largely beneftis from high-quality initial block-slot encodings. If these encodings are suboptimal, the resulting concept-slot encodings also degrade in quality. An important next step to handle more complex visual inputs, such as video data, is the integration of recent approaches (e.g., [16, 19]). Additionally, due to NCB\u2019s unsupervised training nature, further alignment of NCB\u2019s concepts is inevitable for effective deployment in downstream tasks [9]. Further, to build trust in NCB\u2019s concept knowledge, human inspection is essential. Lastly, revisions are a critical aspect of NCB. However, they rely on humans to provide accurate feedback; a malicious user could manipulate NCB\u2019s concepts. Fortunately, by inspecting the concept space, it is possible to track and mitigate such manipulation effectively. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce the Neural Concept Binder framework for learning visual object-factor concepts in an unsupervised manner. Our evaluations suggest that NCB\u2019s specific binding mechanisms facilitate the learning of expressive yet discrete concept representations. Furthermore, our results highlight the potential of integrating NCB\u2019s inherently inspectable and revisable concept-slot encodings into both symbolic and neural modules. Promising directions for future research include exploring the beneftis of NCB\u2019s representations in continual learning settings [11], high-level concept learning [81], and probabilistic logic programming approaches [66, 67], as well as investigating connections to object-centric causal representation learning [48]. Lastly, incorporating downstream learning signals may be valuable (if present) for improving the quality of NCB\u2019s initial concept encodings, e.g., through classification [5, 6] or differentiable clustering [76]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Gautam Singh for help with SysBinder and Cyprien Dzialo for preliminary results and insights. This work was supported by the Priority Program (SPP) 2422 in the subproject \u201cOptimization of active surface design of high-speed progressive tools using machine and deep learning algorithms\u201c funded by the German Research Foundation (DFG), the \u201dML2MT\u201d project from the Volkswagen Stiftung and the \u201dThe Adaptive Mind\u201d project from the Hessian Ministry of Science and Arts (HMWK). It has further beneftied from the HMWK projects \u201dThe Third Wave of Artificial Intelligence - 3AI\u201d, and Hessian.AI, as well as the Hessian research priority program LOEWE within the project WhiteBox, and the EU-funded \u201cTANGO\u201d project (EU Horizon 2023, GA No 57100431). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with selfexplaining neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 7786\u20137795, 2018.   \n[2] E James Archer. The psychological nature of concepts. In Analyses of concept learning, pages 37\u201349. Elsevier, 1966. [3] Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, and Kenji Kawaguchi. Unsupervised concept discovery mitigates spurious correlations. In International Conference on Machine Learning (ICML). OpenReview.net, 2024.   \n[4] Masataro Asai and Alex Fukunaga. Classical planning in deep latent space: Bridging the subsymbolic-symbolic boundary. In Conference on Artificial Intelligence (AAAI), pages 6094\u2013 6101. AAAI Press, 2018. [5] Yaniv Aspis, Krysia Broda, Jorge Lobo, and Alessandra Russo. Embed2sym - scalable neurosymbolic reasoning via clustered embeddings. In International Conference on Principles of Knowledge Representation and Reasoning (KR), 2022. [6] Yaniv Aspis, Mohammad Albinhassan, Jorge Lobo, and Alessandra Russo. Embed2rule scalable neuro-symbolic learning via latent space weak-labelling. In Neural-Symbolic Learning and Reasoning (NeSy), volume 14979 of Lecture Notes in Computer Science, pages 195\u2013218. Springer, 2024.   \n[7] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Mateo Espinosa Zarlenga, Lucie Charlotte Magister, Alberto Tonda, Pietro Lio, Fr\u00e9d\u00e9ric Precioso, Mateja Jamnik, and Giuseppe Marra. Interpretable neural-symbolic concept reasoning. In International Conference on Machine Learning (ICML), 2023.   \n[8] Adrita Barua, Cara Widmer, and Pascal Hitzler. Concept induction using llms: a user experiment for assessment. CoRR, abs/2404.11875, 2024.   \n[9] Aaron Bembenek and Toby Murray. Symbol correctness in deep neural networks containing symbolic layers. CoRR, abs/2402.03663, 2024.   \n[10] Christian Bessiere. Constraint propagation. In Foundations of Artificial Intelligence, volume 2, pages 29\u201383. Elsevier, 2006.   \n[11] Florian Peter Busch, Roshni Kamath, Rupert Mitchell, Wolfgang Stammer, Kristian Kersting, and Martin Mundt. Where is the truth? the risk of getting confounded in a continual world. CoRR, abs/2402.06434, 2024.   \n[12] Ricardo J. G. B. Campello, Davoud Moulavi, and J\u00f6rg Sander. Density-based clustering based on hierarchical density estimates. In Advances in Knowledge Discovery and Data Mining (PAKDD), 2013.   \n[13] Ricardo J. G. B. Campello, Davoud Moulavi, Arthur Zimek, and J\u00f6rg Sander. Hierarchical density estimates for data clustering, visualization, and outlier detection. ACM Transactions on Knowledge Discovery from Data, 10(1):5:1\u20135:51, 2015.   \n[14] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan Su. This looks like that: Deep learning for interpretable image recognition. In Advances in Neural Information Processing Systems (NeurIPS), pages 8928\u20138939, 2019.   \n[15] Alex J. DeGrave, Joseph D. Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects shortcuts over signal. Nature Machine Intelligence, 3(7):610\u2013619, 2021.   \n[16] Quentin Delfosse, Wolfgang Stammer, Thomas Rothenbacher, Dwarak Vittal, and Kristian Kersting. Boosting object representation learning via motion and object continuity. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (PKDD / ECML), 2023.   \n[17] Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, and Sepp Hochreiter. Symbolicai: A framework for logic-based approaches combining generative models and solvers. CoRR, abs/2402.00854, 2024.   \n[18] Maximilian Dreyer, Reduan Achtibat, Wojciech Samek, and Sebastian Lapuschkin. Understanding the (extra-)ordinary: Validating deep model decisions with prototypical concept-based explanations. CoRR, abs/2311.16681, 2023.   \n[19] Gamaleldin F. Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C. Mozer, and Thomas Kipf. Savi $^{++}$ : Towards end-to-end object-centric learning from real-world videos. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[20] Jerome Feldman. The neural binding problem (s). Cognitive neurodynamics, 7:1\u201311, 2013.   \n[21] Asma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff, and Rosalind W. Picard. DISSECT: disentangled simultaneous explanations via concept traversals. In International Conference on Learning Representations (ICLR), 2022.   \n[22] Amirata Ghorbani, James Wexler, James Y. Zou, and Been Kim. Towards automatic conceptbased explanations. In Advances in Neural Information Processing Systems (NeurIPS), pages 9273\u20139282, 2019.   \n[23] Shantanu Ghosh, Ke Yu, Forough Arabshahi, and Kayhan Batmanghelich. Dividing and conquering a blackbox to a mixture of interpretable models: Route, interpret, repeat. In International Conference on Machine Learning (ICML), 2023.   \n[24] Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. CoRR, abs/2012.05208, 2020.   \n[25] Avani Gupta and P. J. Narayanan. A survey on concept-based approaches for model improvement. CoRR, abs/2403.14566, 2024.   \n[26] Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas Rahimi. A neurovector-symbolic architecture for solving raven\u2019s progressive matrices. Nature Machine Intelligence, 5(4):363\u2013375, 2023.   \n[27] Haiyang Huang, Zhi Chen, and Cynthia Rudin. Segdiscover: Visual concept discovery via unsupervised semantic segmentation. CoRR, abs/2204.10926, 2022.   \n[28] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations (ICLR), 2017.   \n[29] Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, and Philip Teare. An image is worth multiple words: Learning object level concepts using multi-concept prompt learning. CoRR, abs/2310.12274, 2023.   \n[30] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[31] Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, and Lin Guan. Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable AI systems. In Conference on Artificial Intelligence (AAAI), 2022.   \n[32] Eunji Kim, Dahuin Jung, Sangha Park, Siwon Kim, and Sungroh Yoon. Probabilistic concept bottleneck models. In International Conference on Machine Learning (ICML), 2023.   \n[33] Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307\u2013392, 2019.   \n[34] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning (ICML), 2020.   \n[35] Isaac Lage and Finale Doshi-Velez. Learning interpretable concept-based models with human feedback. CoRR, abs/2012.02898, 2020.   \n[36] Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. The dangers of post-hoc interpretability: Unjustified counterfactual explanations. In International Joint Conference on Artificial Intelligence (IJCAI), 2019.   \n[37] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning (ICML), 2019.   \n[38] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In Conference on Artificial Intelligence (AAAI), 2018.   \n[39] Ruizhe Liu, Qian Luo, and Yanchao Yang. Infocon: Concept discovery with generative and discriminative informativeness. 2024.   \n[40] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In International Conference on Machine Learning (ICML), 2019.   \n[41] Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International conference on machine learning (ICML), 2020.   \n[42] Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Disentangling factors of variations using few labels. In International Conference on Learning Representations (ICLR), 2020.   \n[43] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[44] Luca Salvatore Lorello and Marco Lippi. The challenge of learning symbolic representations. In International Workshop on Neural-Symbolic Learning and Reasoning, 2023.   \n[45] James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Berkeley Symposium on Mathematical Statistics and Probability, 1967.   \n[46] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations (ICLR), 2017.   \n[47] Anita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi-Velez, and Weiwei Pan. Promises and pitfalls of black-box concept learning models. CoRR, abs/2106.13314, 2021.   \n[48] Amin Mansouri, Jason S. Hartford, Yan Zhang, and Yoshua Bengio. Object centric architectures enable efficient causal representation learning. In International Conference on Learning Representations (ICLR). OpenReview.net, 2024.   \n[49] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations (ICLR), 2019.   \n[50] Emanuele Marconato, Andrea Passerini, and Stefano Teso. Glancenets: Interpretable, leak-proof concept-based models. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[51] Eleonora Misino, Giuseppe Marra, and Emanuele Sansone. VAEL: bridging variational autoencoders and probabilistic logic programming. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[52] Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, and Soheil Feizi. Text-to-concept (and back) via cross-model alignment. In International Conference on Machine Learning (ICML), 2023.   \n[53] Davoud Moulavi, Pablo A. Jaskowiak, Ricardo J. G. B. Campello, Arthur Zimek, and J\u00f6rg Sander. Density-based clustering validation. In International Conference on Data Mining, 2014.   \n[54] Frank Nielsen. Hierarchical clustering. Introduction to HPC with MPI for Data Science, pages 195\u2013211, 2016.   \n[55] Peter Norvig. Solving every sudoku puzzle. URL http://norvig. com/sudoku. html, 2006.   \n[56] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.   \n[57] Bo Pan, Zhenke Liu, Yifei Zhang, and Liang Zhao. Surrocbm: Concept bottleneck surrogate models for generative post-hoc explanation. CoRR, abs/2310.07698, 2023.   \n[58] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models by constraining their explanations. In International Joint Conference on Artificial Intelligence (IJCAI), 2017.   \n[59] Adam Santoro, Felix Hill, David G. T. Barrett, Ari S. Morcos, and Timothy P. Lillicrap. Measuring abstract reasoning in neural networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4477\u20134486. PMLR, 2018.   \n[60] Yoshihide Sawada and Keigo Nakamura. Concept bottleneck model with additional unsupervised concepts. IEEE Access, 10:41758\u201341765, 2022.   \n[61] Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. Making deep neural networks right for the right scientific reasons by interacting with their explanations. Nature Machine Intelligence, 2(8):476\u2013486, 2020.   \n[62] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the human-ai knowledge gap: Concept discovery and transfer in alphazero. CoRR, abs/2310.16410, 2023.   \n[63] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentanglement with guarantees. In International Conference on Learning Representations (ICLR), 2020.   \n[64] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-E learns to compose. In International Conference on Learning Representations (ICLR), 2022.   \n[65] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural systematic binder. In International Conference on Learning Representations (ICLR), 2023.   \n[66] Arseny Skryagin, Wolfgang Stammer, Daniel Ochs, Devendra Singh Dhami, and Kristian Kersting. Neural-probabilistic answer set programming. In International Conference on Principles of Knowledge Representation and Reasoning (KR), 2022.   \n[67] Arseny Skryagin, Daniel Ochs, Devendra Singh Dhami, and Kristian Kersting. Scalable neuralprobabilistic answer set programming. Journal of Artificial Intelligence Research, 78:579\u2013617, 2023.   \n[68] Wolfgang Stammer, Patrick Schramowski, and Kristian Kersting. Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[69] Wolfgang Stammer, Marius Memmel, Patrick Schramowski, and Kristian Kersting. Interactive disentanglement: Learning concepts by interacting with their prototype representations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[70] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning (ICML), 2017.   \n[71] Alex Tamkin, Mohammad Taufeeque, and Noah D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks. CoRR, abs/2310.17230, 2023.   \n[72] Sever Topan, David Rolnick, and Xujie Si. Techniques for symbol grounding with satnet. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[73] Frederik Tr\u00e4uble, Anirudh Goyal, Nasim Rahaman, Michael Curtis Mozer, Kenji Kawaguchi, Yoshua Bengio, and Bernhard Sch\u00f6lkopf. Discrete key-value bottleneck. In International Conference on Machine Learning (ICML), 2023.   \n[74] Anne Treisman. Solutions to the binding problem: progress through controversy and convergence. Neuron, 24(1):105\u2013125, 1999.   \n[75] A\u00e4ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[76] Georgios Vardakas and Aristidis Likas. Neural clustering based on implicit maximum likelihood. Neural Computing and Applications, 35(29):21511\u201321524, 2023.   \n[77] Ramakrishna Vedantam, Arthur Szlam, Maximilian Nickel, Ari Morcos, and Brenden M. Lake. CURI: A benchmark for productive concept learning under uncertainty. In International Conference on Machine Learning (ICML), 2021.   \n[78] Bowen Wang, Liangzhi Li, Yuta Nakashima, and Hajime Nagahara. Learning bottleneck concepts in image classification. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[79] Taylor Whittington Webb, Ishan Sinha, and Jonathan D. Cohen. Emergent symbols through binding in external memory. In International Conference on Learning Representations (ICLR), 2021.   \n[80] Yi-Fu Wu, Minseung Lee, and Sungjin Ahn. Neural language of thought models. In International Conference on Learning Representations (ICLR). OpenReview.net, 2024.   \n[81] Antonia W\u00fcst, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, and Kristian Kersting. Pix2code: Learning to compose neural visual concepts as programs. Uncertainty in Artificial Intelligence (UAI), 2024.   \n[82] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[83] Chih-Kuan Yeh, Been Kim, Sercan \u00d6mer Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[84] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Fr\u00e9d\u00e9ric Precioso, Stefano Melacci, Adrian Weller, Pietro Li\u00f3, and Mateja Jamnik. Concept embedding models: Beyond the accuracyexplainability trade-off. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[85] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the following, we provide details on Neural Concept Binder, experimental evaluations as well as additional evaluations. ", "page_idx": 15}, {"type": "text", "text": "Impact Statement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our work provides a new framework for unsupervised concept learning for visual reasoning. It improves the reliability of the unsupervised concept learning by explicitly including both inspection and revision of the concept space in the framework. NCB thus makes an important step towards more reliable and transparent AI, by providing an interpretable symbolic concept representation. This representation can be utilized within reliable and proven symbolic methods, or to improve transparency of neural modules. However, as the concepts are learned unsupervised, one has to keep in mind that they are not necessarily aligned with human knowledge, and might require inspections to achieve this. As NCB features a concept revision via human feedback, it is also necessary to consider that these revisions could have negative effects. A user with malicious intents could modify the memory and thus make the concept space incorrect. The fact that the learned representation of NCB is explicitly inspectable can, however, prove to be helpful in limiting such malicious interventions. ", "page_idx": 15}, {"type": "text", "text": "A Details on Neural Concept Binder ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Details on Systematic Binding and Slot Attention ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The binding mechanism (SysBinder) of Singh et al. [65] allows images to be encoded into continuous block-slot representations and relies on the recently introduced slot attention mechanism [43]. In slot attention, so-called slots, $s\\in R^{N_{S}\\times N_{B}D_{B}}$ (each slot has dimension $N_{B}D_{B}.$ ), compete for attending to parts of the input via a softmax-based attention. These slot encodings are iteratively updated and allow to capture distinct objects or image components. The result is an attention matrix $\\dot{A}\\in R^{N_{S}\\times D}$ for an input $x\\,\\in\\,R^{D}$ . Each entry $A_{i}$ corresponds to the attention weight of slot $i$ for the input $x$ . Based on the attention matrix, the input is processed to read-out each object by multiplying $A$ with the input resulting in a matrix $U\\in\\dot{R}^{N_{S}\\times\\dot{N_{B}}D_{B}}$ . ", "page_idx": 15}, {"type": "text", "text": "SysBinder now performs an additional factor binding on the vectors $u_{i}$ of $U$ . The goal of this factor binding mechanism is to find a distribution over a codebook memory for each block in $u_{i}$ , i.e., $u_{i}^{j}$ . This codebook memory (one for each block), $M^{j}\\in R^{K\\times D_{B}}$ , consists of a set of $K$ learnable codebook vectors. Specifically, for each block $j$ an RNN consisting of a GRU and MLP component iteratively updates the $j$ -th block of slot $s_{i},s_{i}^{j}$ , based on $u_{i}^{j}$ and previous $s_{i}^{j}$ . Finally, a soft information bottleneck is applied where each block $s_{i}^{j}$ performs dot-product attention over the codebook memory leading to the final block-slot representation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{s}_{i}^{j}=\\left[\\operatorname{softmax}_{K}\\left(\\frac{\\mathbf{s}_{i}^{j}\\cdot(\\mathbf{M}^{j})^{T}}{\\sqrt{D_{B}}}\\right)\\right]\\cdot\\mathbf{M}^{j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This process is iteratively refined together with the refinement processes of slot attention. Overall, the encodings of SysBinder represent each object in an image by a slot with $N_{B}$ blocks where each block represents a factor of the object like shape or color. ", "page_idx": 15}, {"type": "text", "text": "Note that in the main text, the final $s_{i}^{j}$ is denoted as $z_{i}^{j}$ ", "page_idx": 15}, {"type": "text", "text": "A.2 Selection Function ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the default setting, NCB selects that encoding from the retrieval corpus with the minimal distance to infer a corresponding concept representation. We further explore a top- $k$ approach for the selection function $s$ with $k>1$ . In this case, $s$ selects the values $v_{l}$ , for the $k\\in\\mathbb N$ closest encodings in the retrieval corpus and the resulting $c_{i}^{j}$ is obtained via majority vote over these values. Additionally, via this selection approach the probability of $c_{i}^{j}$ based on the occurrence distribution over the top- $k$ values $v_{l}$ can be estimated. We provide ablations regarding this in our evaluations in Suppl. F.1. ", "page_idx": 15}, {"type": "table", "img_path": "ypPzyflbYs/tmp/e7628e95455a894224459b299802321f7feec746bcbcf379515b9000c39e915b.jpg", "table_caption": ["Algorithm 1 Training NCB: Given a set of images, $X$ , a block-slot encoder, $g_{\\theta}$ , an unsupervised clustering model $h_{\\phi}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Details on Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The first step (cf. L.1 in Alg. 1) optimizes the encoder $g$ to provide object-factorised block-slot encodings. It is optimized for unsupervised image reconstruction based on the decoder model, $g_{\\theta^{\\prime}}^{\\prime}:z\\bar{\\to}\\ \\tilde{x}\\in\\mathbb{R}^{D}$ (cf. Fig. 2) and a mean squared error loss: $L=L_{\\mathrm{MSE}}(x,g^{\\prime}(g(x)))$ . In practice, additional losses have been shown to be beneficial for further improving the obtained block-slot encodings [65, 64]. ", "page_idx": 16}, {"type": "text", "text": "The goal of NCB\u2019s second training step is to obtain the retrieval corpus, $\\mathcal{R}$ . This procedure is based on obtaining an optimal clustering of block encodings via an unsupervised clustering model $h$ and distilling the resulting information from $h$ into explicit representations in the retrieval corpus. This step is divided into several substeps (cf. L.2-6 in Alg. 1). It starts with gathering a set of block-slot encodings $Z=g_{\\hat{\\theta}}(X)$ . As $Z$ can include slots which do not encode objects but, e.g., the background, we first select the \"object-slot\" encodings from $Z$ . This step results in $\\bar{Z}\\subseteq Z$ and consists of a heuristic selection based on the corresponding slot attention masks (described in the following section). ", "page_idx": 16}, {"type": "text", "text": "For each block $j$ we next perform the following steps: (i) a clustering model, $h_{\\phi^{j}}$ (cf. Fig. 2), is fti to find a set of clusters within $\\bar{Z}^{j}$ thereby identifying $N_{C}\\in\\mathbb{N}$ meaningful clusters. The learning of this optimal clustering is based on an unsupervised criteria, e.g., density based scores [53]. Ideally, this leads to that objects that share similar block encodings are clustered together in the corresponding latent block space, whereas objects that possess very different block encodings are associated with distant clusters. This resulting clustering is stored in $h$ \u2019s internal representation which we denote as $\\phi^{j}$ (e.g., the merge tree in a hierarchical clustering method [12, 13, 54]. Importantly, $h_{\\phi^{j}}$ is optimized individually for each block. (ii) In the distill step representative block encodings of each cluster, $\\mathsf{e n c}^{j}$ , are extracted from $h$ \u2019s internal representation, $\\phi^{j}$ . Hereby, every $\\mathsf{e n c}^{j}$ can represent either an averaged prototype or instance-based exemplar encoding of a cluster. This is performed for every identified cluster, $v\\in\\{1,\\cdots\\,,N_{C}\\}$ and is based on $\\bar{Z}^{j}$ and $\\phi^{j}$ . As a result, the tuples $(\\mathtt{e n c}^{j},v)$ are explicitly stored in the retrieval corpus $\\mathcal{R}^{j}$ . The final retrieval corpus consists of the set of individual corpora for each block, $R=[R^{1},\\dot{\\cdot}\\cdot\\cdot\\,,R^{N_{B}}]$ . ", "page_idx": 16}, {"type": "text", "text": "We note that in practice, it is further possible to finetune the block-slot encoder, $g$ , through supervision from the hard binder, e.g., once initial categories have been identified and can be achieved via a standard supervised approach. Ultimately, this allows for dynamically finetuning NCB\u2019s concept representations. ", "page_idx": 16}, {"type": "text", "text": "Heuristic object-slot selection. In the following we describe the process of identifying the slot which contains an object. This is based on heuristically selecting slot ids based on their corresponding slot attention values. Importantly, this approach can select object-slot ids without additional supervision, e.g., via (GT) object segmentation masks. ", "page_idx": 16}, {"type": "text", "text": "In principle, our object-slot selection approach finds the slots which contain slot attention values above a predefined threshold, $\\delta\\in(0,1]$ . However, selecting such a threshold can be cumbersome in practice. In our evaluations we therefore select only a single slot per image, i.e., that slot which contains the maximum slot attention value over all slots. Essentially, this sets the maximum number of selected slots per image to 1 and in images that contain one objects represent no loss of object relevant information. In preliminary evaluations we observed that the consensus between object-slot selection based on GT object segmentation masks (matching object segmentation masks with slot attention masks) and our maximum-based selection heuristic is $99.45\\%$ over 2000 single object images. ", "page_idx": 16}, {"type": "text", "text": "A.4 Instantiating Neural Concept Binder ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We instantiate NCB\u2019s soft binder via the SysBinder approach of Singh et al. [65] which has been shown to provide valuable, object-factor disentangled representations. Thus, the soft binder was trained as in the original setup and with the published hyperparameters. Furthermore we instantiate the clustering model, $h$ , via the powerful HDBSCAN method [12, 13, 54] (based on the popular HDBSCAN library2). Hereby, $h$ \u2019s internal representation, $\\phi$ , consists of the learned hierarchical merge tree. In practice we found it beneficial to perform a grid search over $h$ \u2019s hyperparameters based on the unsupervised density-based cluster validity score [53]. The searched parameters are the minimal cluster size (the minimum number of samples in a group for that group to be considered a cluster) and minimal sample number (the number of samples in a neighborhood for a point to be considered as a core point) each over the values [5, 10, 15, 20, 25, 30, 50, 80, 100]. Moreover, we utilize the excess of mass algorithm and allow for single clusters. We performed the training of the retrieval corpus, i.e., fitting $h$ , on a dataset of images containing single objects for simplifying the subsequent concept inspection mechanisms of our evaluations. However, this can easily be extended to multiple object images by utilising the soft binder\u2019s slot attention masks to identify relevant objects in an image. Finally, we instantiate the retrieval corpus as a set of dictionaries and, unless stated otherwise, we utilise a retrieval corpus which contains one prototype and a set of exemplar encodings per concept. Furthermore, $s_{\\mathcal{R}}$ represents the argmin selection function and we utilize the euclidean distance as $d(\\cdot,\\cdot)$ . It is important to note that $h$ does not make any assumptions about the number of clusters, $N_{C}$ . Thus, although $h$ fits a clustering to best fit the block-slot encodings of a block, it can potentially provide an overparameterized clustering, e.g. by representing one underlying factor such as \u201cgray\u201d with several clusters. This highlights the importance of task-alignment, e.g., for symbolic downstream tasks, and concept inspection for general concept alignment. We refer to our code for more details3, where trained model checkpoints and corresponding parameter logs are available. ", "page_idx": 17}, {"type": "text", "text": "A.5 Computational Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The resources used for training NCB were: CPU: AMD EPYC 7742 64- Core Processor, RAM: 2064 GB, GPU: NVIDIA A100-SXM4-40GB GPU with $40\\:\\mathrm{GB}$ of RAM. Hereby, training the SysBinder model [65] is the computational bottleneck of NCB where we utilised two GPUs per SysBinder run. Training for 500 epochs took ${\\approx}108$ GPU hours. The fitting of $h$ (including the grid search over hyperparameters) was performed on the CPU and finished within a few hours. ", "page_idx": 17}, {"type": "text", "text": "B Details on CLEVR-Sudoku ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "ypPzyflbYs/tmp/53dcbb8f5737ee741110fd60e67de8128ae043b156f2d24766f80630844c548b.jpg", "img_caption": ["Figure 7: Examples of Sudoku CLEVR for different K values. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "CLEVR-Sudoku provides Sudokus based on the datasets CLEVR and CLEVR-Easy. Classic Sudokus have a $9\\mathrm{x9}$ grid which is fliled with digits from 1 to 9. In CLEVR-Sudoku these digits are replaced by images of objects. Hereby, a digit corresponds to a specific attribute combination, e.g., \"yellow\" and \"sphere\". Consequently, digits of the Sudoku are replaced by images of objects with these attribute combinations. These images each contain one object. To indicate, which attributes correspond to which digit, candidate examples of the digits are provided. The number of these examples is a flexible parameter, in our evaluations we used $N\\in\\{1,3,5,10\\}$ . Further, the number of images provided in the Sudoku grid is flexible as well. In our main evaluations we only considered CLEVR-Sudokus with $K=30$ , meaning that 51 of the 81 Sudoku cells are filled and 30 are left to complete. For additional investigation we considered values for $K\\in\\{10,50\\}$ as well. Examples of those Sudokus for Sudoku CLEVR are shown in Fig. 7. The dataset has a number of 1000 samples for Sudoku CLEVR-Easy and Sudoku CLEVR respectively for each value of $K$ . Each sample has a different puzzle and a distinct set of images, no image is used twice for one puzzle4. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "CLEVR. Briefly, a CLEVR [30] image contains multiple 3D geometric objects placed in an illuminated background scene. Hereby, the objects can possess one of three forms, one of 8 colors, one of two sizes, one of two materials and a random position within the scene. ", "page_idx": 18}, {"type": "text", "text": "CLEVR-Easy. CLEVR-Easy [65] images are similar to CLEVR images, except that in CLEVR-Easy the size and material is fixed over all objects, i.e., all objects are large and metallic. ", "page_idx": 18}, {"type": "text", "text": "CLEVR-Hans3. The CLEVR-Hans3 [68] represents a classification dataset that contains images with CLEVR objects where the image class is determined based on the attribute combination of several objects (e.g., an image belongs to class 1 if it contains a large, gray cube and a large cylinder). Furthermore, we utilize a confounded and non-confounded version of CLEVR-Hans3. In the confounded case (i.e., the original dataset) the train and validation set contains spurious correlations among object attributes (e.g., all large cubes are gray in class 1) that are not present in the test set (e.g., large cubes of class 1 take any color). In our evaluations investigating only neural-based classification we utilize the original validation split as the held-out test split and select a subset from the original training split as validation set. Thus, the non-confounded version corresponds to a standard classification setup in which the data distribution is identical over all three data splits. Lastly we provide evaluations on a single object version of CLEVR-Hans3 (class 1: a large, gray cube; class 2: a small metal cube; class 3: a large, blue sphere; cf. Tab. 3) and the original, multi-object version. ", "page_idx": 18}, {"type": "text", "text": "D Baseline Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We note upfront, that all SysBind configurations below were trained for as many epochs as NCB, followed by an additional finetuning for 2 epochs on the same dataset that was used to distill NCB\u2019s retrieval corpus. ", "page_idx": 18}, {"type": "text", "text": "SysBind (cont.). This denotes the original SysBinder configuration which was trained as in [65] and provides continuous block-slot encodings. We refer to the original work for hyperparameter details. ", "page_idx": 18}, {"type": "text", "text": "SysBind. This denotes a SysBinder configuration that was trained as in [65]. However, at inference time we perform discretisation via an argmin operation over the attention values to each block\u2019s prototype codebook. ", "page_idx": 18}, {"type": "text", "text": "SysBind (hard). This denotes a configuration in which the SysBinder model was trained via a codebook attention softmax temperature of $1e-4$ , resulting in a learned discrete representation. ", "page_idx": 18}, {"type": "text", "text": "SysBind (step). SysBinder (step) is trained by step-wise decreasing this temperature his denotes a configuration in which the SysBinder model was trained via a step-wise decreasing codebook attention softmax temperature (with a decrease by a factor of 0.5 every 50 epochs, starting from 1.). ", "page_idx": 18}, {"type": "text", "text": "NLOTM. NLOTM [80] builds on the principles of SysBinder and incorporates a Semantic VectorQuantized (SVQ) Variational Autoencoder along with the Autoregressive LoT Prior (ALP). The SVQ component facilitates discrete semantic decomposition of a scene by learning hierarchical, composable factors that correspond closely to objects and their attributes in visual scenes. We refer to the original work for details. ", "page_idx": 18}, {"type": "text", "text": "Supervised Concept Learner. This corresponds to a slot attention encoder [43] that was trained for set prediction (i.e., in a supervised fashion) to predict the object-properties for every object in a CLEVR image. We refer to Locatello et al. [43] and Stammer et al. [68] for details. ", "page_idx": 18}, {"type": "text", "text": "E Details on Experimental Setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Classifying object-properties from concept encodings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For our evaluations in the context of (Q1) we utilise a decision tree as classification model that is trained on a set of concept encodings to predict corresponding object properties, e.g., sphere, cube or cylinder. Importantly, we train a separate classifier for each property category, e.g., the categories shape, color, material and size in the case of CLEVR, and average accuracies over these. The classifiers parameters correspond to the default parameters of the sklearn library5. ", "page_idx": 19}, {"type": "text", "text": "E.2 CLEVR-Sudoku evaluations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For our CLEVR-Sudoku evaluations we use a solver that combines a symbolic classifier with a constraint propagation based algorithm. To solve CLEVR-Sudokus, it is at first required to detect the underlying mapping from the object attribute combinations to the digits via the provided candidate examples. For this, we require a symbolic classifier to learn this mapping, which in the case of our evaluations is achieved via a decision tree classifier. For each evaluated model the concept encodings of the candidate example images of a CLEVR-Sudoku are retrieved and provided as input to the classifier. Hereby, the corresponding digits are the labels to be predicted. With the predictions of the trained classifier the concept encodings of the images in the Sudoku grid are classified to get a symbolic representation of the Sudoku, i.e., map the images in the cells to their corresponding digits. Based on this numerical representation of the puzzle, we use an algorithm from [55] that uses a combination of constraint propagation [10] and search. The algorithm keeps track of all possible values for each cell. Within each step, the Sudoku constraints are used to eliminate all invalid digits from the possibilities. Then the search of the algorithm select a digit for a non-filled cell. Based on this digit, the possibilities are updated for all other cells. When there is a constraint violation, the search-tree is traversed backwards and other possible digits for non-filled cells are explored. This process is repeated until the Sudoku is solved (in case the initial state inferred from the objects was correct) or until there is no possible solution left (meaning that the initial state was incorrectly inferred from the objects). The implementation of the algorithm is based on the code from6. Finally, to avoid errors due to random seeding of the classifier, for each puzzle we fti 10 independent classifiers (each with different seeds) to predict the corresponding mapping. For the results in our evaluations we average the performance over these 10 classifier seeds. ", "page_idx": 19}, {"type": "text", "text": "Lastly, the evaluations in the context of (Q2) are based on the trained (NCB) models of (Q1). ", "page_idx": 19}, {"type": "text", "text": "E.3 Obtaining Revisory Feedback ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We note that the evaluations in the context of (Q3) are based on the trained NCB models of (Q1). ", "page_idx": 19}, {"type": "text", "text": "Revisory feedback for downstream Sudoku task. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To revise its discrete concepts, NCB offers the possibility to delete or merge clusters in the blocks. In the case of merging, the prototypes and exemplars of the clusters to be merged get aggregated so that they all map to the same concept symbol. For deletion there are several processing cases, depending on how many categories are in the block and how many are supposed to be deleted: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Case 1: if all clusters from a block should be deleted (or if there is only one concept in the block, which should be deleted), we map all samples to the same concept. This results in the block containing no information (we keep the block to avoid issues with the dimensions of the concept representation).   \n\u2022 Case 2: all clusters but one are to be deleted. In this case we still want to distinguish between the presumably \"informative\" cluster and the uninformative other clusters. Therefore we map all the blocks to be deleted to one cluster id instead of deleting them completely.   \n\u2022 Case 3: at least two clusters should not be deleted. In this case, we completely remove the encodings of the to-be-removed clusters. The cluster id for these clusters no longer exists in the retrieval corpus. ", "page_idx": 19}, {"type": "text", "text": "Feedback via GPT-4. We systematically prompt GPT-4 [56] for receiving revisory feedback. We provide example prompts in Listing 1. First, we ask GPT-4 to name relevant object properties for a set of example images, e.g., \"shapes: [cube, cylinder], color: [red, blue]\". Based on these provided property lists we ask GPT-4 to provide a descriptive list of each exemplar object\u2019s image for each concept of each block, e.g., \"{Exemplar1: [cube, red], Exemplar2: [cube, blue], ... }\". Based on these descriptions we identify whether all exemplar objects of one concept share a common subproperty, e.g., \"cube\". If there is no common subproperty, the concept should be removed from the retrieval corpus. In a second step we evaluate whether all exemplar objects from two separate concepts share a common subproperty. In this case we decide to merge the concepts based on GPT-4\u2019s analysis. We finally integrate GPT-4\u2019s feedback into NCB\u2019s retrieval corpus via the procedures described above. ", "page_idx": 20}, {"type": "text", "text": "Feedback via simulated humans. To simulate feedback by a human user, we utilise a decision tree (DT) classifier to classify attributes of objects based on NCB\u2019s discrete concepts (similar to Q1). For this, we transform the concept-slot encodings into multi-hot encodings. We then extract the importance of the concepts from the trained DT classifier. Based on this we select \"unimportant\" concepts to be deleted based on the procedures describe above. Note that in this setting we do not query for feedback considering the merging of concepts. ", "page_idx": 20}, {"type": "text", "text": "E.4 Neural Classification ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We note that the evaluations in the context of (Q4) are based on the trained NCB models of (Q1). ", "page_idx": 20}, {"type": "text", "text": "Neural classifier. In the context of the classification evaluations (Q4) we utilize the setup of Stammer et al. [68]. Specifically, a set transformer [37] is trained to classify images from the CLEVR-Hans3 dataset given encodings that are, in turn, obtained from either NCB or a supervised trained slot attention encoder [43] (SA). In the case of utilizing NCB\u2019s encodings we transform the concept-slot encodings into multi-hot encodings to match those of the SA-based setup. We refer to Stammer et al. [68] and our code for additional details concerning this setup. ", "page_idx": 20}, {"type": "text", "text": "Obtaining explanations from the neural classifier. We provide the explanations in Tab. 3 for the single object version of Fig. 14. To obtain these explanations for the neural classifier we utilize the approach of Stammer et al. [68] which is based on the integrated gradients explanation method [70]. This estimates the importance value of each input element (in this case input concept encodings) for a classifiers final decision. We remove negative importance values and normalise the importance values as in [68]. We then sum over the importance values corresponding to images of a class, normalise the values per block and binarize these aggregated and normalised importance values via the threshold of 0.25 (i.e., importance values above 0.25 are set to 1, otherwise 0). This provides us with a binary vector indicating which concepts are considered important per block. We illustrate these investigations via explanations from one model. ", "page_idx": 20}, {"type": "text", "text": "Explanatory interactive learning (XIL). Explanatory interactive learning $^{+}$ XIL on NN) is used to mitigate the confounder in the CLEVR-Hans dataset. Hereby, (simulated) human feedback on the explanation of the neural classifier is used to retrain the classifier via the loss based approach of Stammer et al. [68]. The feedback annotations mark which of NCB\u2019s concepts should not be used for the NN\u2019s classification decision. This is integrated into the NN by training the model to provide (integrated gradients-based) explanations that do not focus on these concepts. We refer to Stammer et al. [68] for details. The second form of interactive learning $\\left.+X I L\\right.$ on NCB concepts) is directly applied on the NCB\u2019s concept representation. Specifically, concepts from NCB that encode information concerning the irrelevant, confounding factors are simply set to zero, corresponding to not being inferred for the object in the image. E.g., if the NCB infers concepts concerning the color \u201cgray\u201d to be present in an object and the underlying confounder is the color \u201cgray\u201d the corresponding concept activations of the NCB\u2019s prediction are set to zero, i.e., no gray. Then the neural classifier is retrained on the new concept representations. Next to a better performance, the advantage of this approach is that it does not require the more costly loss-based XIL training loop. We illustrate these investigations via interactions on one model. ", "page_idx": 20}, {"type": "table", "img_path": "ypPzyflbYs/tmp/5ffe3551a9b5cb3cdafbe6aab16e33ee239e0941d1284eaac1c98c7ccf916aff.jpg", "table_caption": ["Table 4: Ablation of NCB\u2019s selection components for classifying attributes from concept representations. Best results are in bold. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "ypPzyflbYs/tmp/2b6436ad1d8fce3286555fb128754f804dac689c7c30d019d8305210cffa1e40.jpg", "table_caption": ["Table 5: Ablation: Classifying attributes from concept representations with sub-optimal NCB components. The left column serves as a reference and represents the configurations used in the main evaluations, i.e., where the soft binder was trained for 600 epochs and the clustering model represented the HDBSCAN approach that was optimized via a grid-search over its corresponding hyperparameters. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Additional Quantitative Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Encoding Expressivity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In our evaluations in Tab. 2 it appears that training for discrete encodings via SysBinder (hard) leads to no learning effect of the model altogether. In contrast training step-wise via SysBinder (step) provides better results, even slightly above the encodings of SysBinder (i.e., training for continuous representations and then discretising via argmin). Lastly, we observe that NCB\u2019s encodings lead to much lower performance variance compared to all baselines. Particularly SysBinder (step)\u2019s high variances, hint towards issues with local optima. ", "page_idx": 21}, {"type": "text", "text": "We further provide ablations in the context of (Q1) on different component choices of NCB in Tab. 4. Specifically, we investigate the effect of a top- ${\\cdot k}$ selection function as well as the influence of using only prototype encodings in the retrieval corpus (NCB (P)) versus using prototype and exemplar encodings (NCB $\\mathrm{(P+E)}_{\\cdot}$ ). Unless noted otherwise, the NCB configurations in Tab. 4 utilize the argmin selection function. We note that when using prototypes, the average encoding of all elements in a cluster is formed, resulting in one prototype encoding per cluster in $R^{j}$ . In the second variant, we extend the prototypes with exemplars for each cluster. Exemplars are representative encodings for this cluster added to the corpus, resulting in a larger corpus, which potentially provides an improved structure of the encoding space. Indeed, we observe that NCB provides the best performances via the argmin selection function and utilizing both prototype and exemplar encodings. This was the setting used in all evaluations of the main paper. ", "page_idx": 21}, {"type": "text", "text": "F.2 Ablation Analysis of Suboptimal NCB Components ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lastly, in the context of (Q1) we further refer to ablations in Tab. 5 on the specific implementation choices of the NCB instantiation of our evaluations. We hereby investigate the effect of sub-optimal soft and hard binder components on a classifier\u2019s ability to identify object attributes from NCB\u2019s concept encodings. Specifically, we investigate (i) the effect when the soft binder, i.e., SysBinder encoder, was trained for fewer epochs, resulting in less disentangled continuous representations, and (ii) when the HDBSCAN model of the hard binder was not optimized via a parameter grid search or replaced with a more rudimentary clustering model, i.e., a k-means clustering approach [45]. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "In the leftmost column of Tab. 5, we provide the performances of the NCB configuration of our main evaluations as a reference. As a reminder, hereby, NCB\u2019s soft binder was finetuned for 500 epochs, and its hard binder component contains a clustering model based on the HDBSCAN approach that was furthermore optimized via a grid search over its corresponding hyperparameters. Focusing on the next two columns right of the baseline, we observe that when the soft binder component is trained for fewer epochs than the baseline NCB we indeed observe a decrease in classification performance. Notably, however, we still observe higher performances in comparison to the discrete SysBinder configurations (cf. Suppl. F.1), but also when compared to SysBinder\u2019s continuous configuration (for $N=20$ ). Focusing next on the rightmost column of Tab. 5 where NCB\u2019s clustering model was replaced with the more rudimentary $\\mathbf{k}$ -means clustering approach, we observe a strong decrease in classifier performance. This is particularly true in the small data regime $N=50$ and $N=20$ ). Surprisingly, focusing on the second to the rightmost column, we observe that when we select the default hyperparameter values of the HDBSCAN package (rather than performing a grid-search over these), the classifier reaches slightly improved performances than via the baseline NCB configuration (particularly for $N=20$ ). Thus, in this particular case, the default values seem practical. However, this cannot be guaranteed in all future cases, and we still recommend performing a form of grid search if no prior knowledge can be provided upfront on an optimal parameter set. We postulate that the specific density-based cluster validity score used for selecting the optimal cluster parameters has been sub-optimal and leave investigating other, more optimal selection criteria for future work. ", "page_idx": 22}, {"type": "text", "text": "Overall, our ablation investigations indeed indicate that we obtain less expressive concept encodings via NCB with less powerful sub-components. However, we also observe a certain amount of robustness of our NCB instantiation towards sub-optimal components. ", "page_idx": 22}, {"type": "text", "text": "F.3 Analysis of Learned Concept Space ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We here provide a brief analysis of NCB\u2019s learned concept space. These evaluations were performed on the models that were trained in the context of (Q1). Specifically, in Fig. 8, we provide the number of obtained concepts over all blocks (averaged over the 3 initialization seeds) both for CLEVR-Easy and CLEVR. We observe a much larger number of concepts overall for the CLEVR dataset but also a much larger variance in the number of concepts. This is largely due to that in CLEVR-Easy $N_{B}\\,=\\,8$ whereas in CLEVR $N_{B}\\,=\\,16$ . Thus, the models are able to learn a more overparameterized concept space in the case of CLEVR. Further, in Fig. 9, we present the distribution of the number of concepts per block over all $3\\ \\mathrm{NCB}$ runs, both for CLEVR-Easy and for CLEVR. We observe that while most blocks contain maximally 20 concepts for CLEVR-Easy and 50 for CLEVR, there are several block outliers which contain a much greater set of concepts. ", "page_idx": 22}, {"type": "text", "text": "These represent cases in which the initial blockslot encoding space was uninformative to begin with and, therefore, difficult to find some form of useful clustering via $h$ . Where some of these blocks only contained irrelevant information in general, some blocks encoded positional information, which represents a continuous variable to begin with and is thus unlikely to be well represented via a clustering. ", "page_idx": 22}, {"type": "image", "img_path": "ypPzyflbYs/tmp/2f380b5912013584842083aec63f642c8de264ccf439e46ef38730744db02bc2.jpg", "img_caption": ["Figure 8: Average number of concepts (over all blocks) in NCB\u2019s retreival corpus. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.4 CLEVR-Sudoku Evaluations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our evaluations on (Q2) we observe that, interestingly, for Sudoku CLEVR the supervised object classifier shows better results than for CLEVR-Easy. This seems counter-intuitive, however, in CLEVR-Easy-Sudoku digit labels are mapped to combinations of attributes that only stem from two categories, shape and color (in contrast to four categories in CLEVR-Sudoku) thus making it more likely to obtain recurring attributes over several digits (e.g., digits 3, 4 and 5 of Fig. 4 all depict green objects). Thus, if an error occurs in the digit classification due to errors concerning one attribute the effect of this error will have a larger effect. Moreover in the case of CLEVR-Easy, we observe that in comparison to the supervised model, whose property misprediction errors can lead to large issues in the downstream module, NCB\u2019s unsupervised and somewhat overparameterised concept space appears to dampen this issue, thus leading to a higher number of solved puzzles, e.g., for 3, 5 or 10 examples. ", "page_idx": 22}, {"type": "image", "img_path": "ypPzyflbYs/tmp/1afcaa6135c1b4f856209bc20ed56eff8c89d0a8aea04454085bd16d346b93c3.jpg", "img_caption": ["Figure 9: The distribution of number of obtained concepts per block both for CLEVR-Easy and CLEVR. These values are computed over all seeds. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "ypPzyflbYs/tmp/9eaddbca75c3eb5094f2cc9439d2a88b0ff25f25a70514ab5ee12034f68c20ad.jpg", "img_caption": ["Sudoku CLEVR-Easy ", "Figure 10: Error ratios $(\\%)$ of the digit classification in CLEVR-Sudoku based on different symbolic concept encodings. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "In Fig. 10 we report the errors in predicting the underlying digits of the CLEVR-Sudokus. We observe that the errors of SysBinder (unsupervised) are drastically higher than the errors of the other methods. These high classification errors further explain this method\u2019s low performances, i.e., did not allow to solve any Sudoku. It can further be seen that for one example per digit the digit classification errors are much higher. This is reasonable as hereby the difficulty for the classifier is also higher. However, with an increasing number of examples the classifier\u2019s errors decrease. The relations between the errors in the digit prediction and the overall performance in CLEVR-Sudoku are similar which is sensible since the error is decisive for the number of solved puzzles. ", "page_idx": 23}, {"type": "image", "img_path": "ypPzyflbYs/tmp/9c5d0b8cb4e3cea8b145484a1b59e09f2034b646304e9422a94622a0990cdaf5.jpg", "img_caption": ["Sudoku CLEVR-Easy with 5 Examples "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "", "img_caption": ["Figure 11: Solved Sudokus $(\\%)$ of Sudoku CLEVR-Easy and Sudoku CLEVR with different values for K (empty cells). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "We further evaluate the influence of the number of missing images per Sudoku. For this we consider Sudokus with $K\\in\\{10,30,50\\}$ . The results on these variations with 5 candidate example images are reported in Fig. 11. We see that the more empty cells there are in a Sudoku\u2019s initial state (higher $K_{\\star}$ ), the more Sudokus are solved. This is due to the lower probability of misclassifying an image inside the Sudoku cells, as there are less images to classify. This pattern is observable for all of the different concept encodings we compared. ", "page_idx": 24}, {"type": "text", "text": "F.5 Revision Statistics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide statistics of the number of resulting removal requests per agent in Fig. 12. For the revision of CLEVREasy concepts we can see that GPT-4 detects only a few concepts to delete while via simulated human revision more concepts get deleted. In our initial evaluations (cf. Fig. 4) we had observed that human revision leads to ", "page_idx": 24}, {"type": "image", "img_path": "ypPzyflbYs/tmp/390bc18b6029706cc50407f9da1b764d051159112f343e6508d21675a4deba5c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 12: Average number of cluster deletions over all blocks via GPT-4 and simulated human user revision. ", "page_idx": 24}, {"type": "text", "text": "substantial improvements while GPT-4\u2019s revision even reduces performances slightly. For CLEVRSudoku in Fig. 12, we specifically observe that the overall number of deletions via GPT-4 is significantly higher. Interestingly, GPT-4 detects on average more blocks to delete here but also has a higher variance over the 3 different NCB runs. We hypothesize that this very \u201cconservative\u201d revision leads to the removal of concepts that actually contain valuable concept information, thus leading to less expressive concept encodings overall. Ultimately, this is due to mistakes in GPT-4\u2019s analysis of provided images (cf. Suppl. E.3). ", "page_idx": 24}, {"type": "text", "text": "F.6 Dynamically Discretising Continuous Factors via Symbolic Revision ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In our second set of evaluations in the context of (Q3) we investigate the third form of symbolic revision as introduced in Sec. 3.3: adding concept information to the hard binder\u2019s retrieval corpus. Hereby, we focus on the task of learning a novel concept that had only been stored implicitly in the soft binder\u2019s representations, but not explicitly in the hard binder\u2019s representations. Specifically, we focus on positional concepts of CLEVR objects where the underlying GT position is represented via continuous values. Overall, it is debatable whether one, in principle, should or even can represent such a continuous underlying feature via a discrete concept representation. ", "page_idx": 25}, {"type": "text", "text": "In this set of evaluations we investigate a setting in which it is necessary to identify coarse categorisations of an object\u2019s position, e.g., whether the object is placed in the left or right half of an image. We hereby simulate a human stakeholder that, having identified the block $j$ that generally encodes position information, revises the corresponding concept encodings. This revision is performed in two ways: (i) by iterating over all of the block\u2019s concepts and merging concepts into left and right concepts or (ii) by replacing all information in $\\mathcal{R}$ with encodings from a selected set of positive example images for the two relevant positions. Fig. 13 presents the results of training a classifier to predict the attributes \u201cleft\u201d and \u201cright\u201d from NCB\u2019s encodings (we here focus only on one seeded run for illustrations) with different types of revision. We observe that both allow to easily retrieve relevant information from NCB\u2019s newly revised concept space. These results illustrate the important ability to easily adapt the hard binder\u2019s concept representations by dynamically re-reading out the information of the soft binder\u2019s representations in a use-case based manner. The results further illustrate the effect of adding prior knowledge to NCB\u2019s concept representations, thereby potentially reducing the amount of inspection effort required on the stakeholder\u2019s side, e.g., in comparison to the merge revision. ", "page_idx": 25}, {"type": "image", "img_path": "ypPzyflbYs/tmp/6224448465091ca4018bf2a12df1cb9b4f964a86d3a97c135c957138bdecd12f.jpg", "img_caption": ["Figure 13: Test accuracy $(\\%)$ for classifying objects as placed left or right in a scene. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "F.7 Classifying CLEVR-Hans3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In our final evaluations (Q1) we highlight the advantage of NCB\u2019s concept encodings when combined with subsymbolic (i.e., neural) modules for making their decisions transparent. ", "page_idx": 25}, {"type": "text", "text": "Specifically, while a discrete concept representation is technically not required for neural modules, it has a key advantage: a discrete and inspectable representation allows for transparent downstream computations. We highlight this property in the context of image classification on variations of the benchmark CLEVRHans3 dataset [68]. For these evaluations we revert to training a set transformer [37] (denoted as NN in the following) for classifying images when provided the unsupervised concept encodings of NCB as image representations. We denote this configuration as NCB $+\\,N N$ and compare it to a configuration in which the set transformer is provided concept encodings from a supervised slot attention encoder, denoted as $S A+$ NN. In Fig. 14 we obseve that NCB\u2019s concepts perform on par with those learned supervisedly, each reaching held-out test accuracies higher than $\\mathrm{\\dot{95}\\%}$ . ", "page_idx": 25}, {"type": "image", "img_path": "ypPzyflbYs/tmp/8d8c032f0342b040c6289d38ba3ee45dce87dd783adae0b9975cbb062cf1fe16.jpg", "img_caption": ["Figure 14: Test accuracy $(\\%)$ for classifying CLEVR-Hans3 images with a neural classifier that is provided concept representations of NCB and of a supervised trained slot attention encoder. We differentiate here between class rules based on one object and multiple objects. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F.8 Confounding Evaluations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For the confounding mitigation evaluations in the context of (Q4) we train the $N C B+N N$ configuration on the confounded version of CLEVR-Hans3, where we hereby focus on the single object class rules similar to those in Fig. 14. In this case all large cubes of class one images posses the color gray at training time, but arbitrary colors at test time. We observed accuracies of $N C B+N N$ on the confounded validation set of $99.22\\%$ against the non-confounded test set $79.29\\%$ . This very high validation accuracy versus a significantly reduced test accuracy indicates that the classifier is strongly influenced by the datasets underlying confounding factor. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "G Qualitative Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Fig. 15 further exemplifies the inspection types of Sec. 3.3. Fig. 16 and Fig. 17 represent qualitative inspection results of NCB\u2019s learned concepts. We specifically present implicit inspection via exemplars of concepts from two blocks from NCB when trained on CLEVR-Easy. One can observe that block 2 (Fig. 16) appears to encode shape concepts, however contains one ambiguous concept. We further observe that Fig. 17 appears to encode color concepts, whereby it contains one ambiguous concept (concept 8) and two concepts that appear to both encode the color purple (concept 9 and 10) which could potentially be merged. ", "page_idx": 26}, {"type": "image", "img_path": "ypPzyflbYs/tmp/a038ec292662daa545ffa088968b7fdcc83404977febe180957e9dd95d842d0b.jpg", "img_caption": ["Figure 16: Concepts of Block 2 for NCB with CLEVR-Easy. We here provide implicit inspection examples (i.e., via exemplars of each concept). We observe that block 2 appears to encode shape information (concept 1-3) and contains one ambiguous concept (concept 4). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "ypPzyflbYs/tmp/271ae2a2f0611b37714b4c9588fe1c23ded9d307f75295ad4d9df7d99e8426f4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 17: Concepts of Block 8 for NCB with CLEVR-Easy. We here provide implicit inspection examples (i.e., via exemplars of each concept). We observe that block 8 appears to be encoding color information, contains one ambiguous concept (concept 8) and two concepts that appear to both encode the color purple (concept 9 and 10). ", "page_idx": 28}, {"type": "table", "img_path": "ypPzyflbYs/tmp/7c653d3c5e13d21a38bbe0d5f9bbbc6081f320dd2555b3b97a6e7294780f5545.jpg", "table_caption": ["Table 6: Percentage of solved CLEVR-Sudokus for different number of example images. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ypPzyflbYs/tmp/1b0aff4a4121e3dad9ec3c441b6108633a4742bbd4541f9c0e95b8d0d91ce4a0.jpg", "table_caption": ["Table 7: Error ratios on digit classification of CLEVR-Sudokus for different number of example images. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "H Numerical Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In our evaluations we presented the results on CLEVR-Sudoku in the form of bar plots. We refer to Tab. 6, Tab. 7 and Tab. 8 for the numerical values for the different variations of the dataset. ", "page_idx": 29}, {"type": "table", "img_path": "ypPzyflbYs/tmp/f132d2b1d8d7af8ade796714f6e2a27ad7b6bbb1cc51f902cf3cf7a5c151a1df.jpg", "table_caption": ["Table 8: Percentage of solved CLEVR-Sudokus for different values of K with 5 example images. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "P r o p er t y L i s t Prompt : ", "page_idx": 31}, {"type": "text", "text": "You are provided s i x images . An image c o n t a i n s subimages . Each subimage d e p i c t s one o b j e c t . Each o b j e c t r e p r e s e n t s a r e f l e c t i v e geometric s o l i d t h a t i s placed in a n e u t r a l gray background scene with a l i g h t source . Furthermore , each o b j e c t has m u l t i p l e p r o p e r t i e s ,   \ne . g . , color , shape , size , m a t e r i a l .   \nEach p r o p e r t y can be subdivided i n t o s e v e r a l sub \u2212p r o p e r t i e s , e . g . , brown i s a sub \u2212p r o p e r t y of the p r o p e r t y c o l o r . ", "page_idx": 31}, {"type": "text", "text": "Pl ease provide a l i s t of obect p r o p e r t i e s and s u b p r o p e r t i e s t h a t are d e p i c t e d in a l l images . Ignore the background and the object \u2019 s luminance and r e f l e c t i v i t y . Use the f o l l o w i n g answer t e m p l a t e : ", "page_idx": 31}, {"type": "text", "text": "p r o p e r t y : [ sub \u2212property , sub \u2212property , . . . ] p r o p e r t y : [ sub \u2212property , sub \u2212property , . } ", "page_idx": 31}, {"type": "text", "text": "D e s c r i p t i o n Prompt : ", "page_idx": 31}, {"type": "text", "text": "You are provided an image . The image c o n t a i n s a t most 25 subimages . Each subimage d e p i c t s one o b j e c t . Each o b j e c t r e p r e s e n t s a r e f l e c t i v e geometric s o l i d t h a t i s placed in a n e u t r a l gray background scene with a l i g h t source . Furthermore , each o b j e c t has m u l t i p l e p r o p e r t i e s , e . g . , c o l o r . Each p r o p e r t y can be subdivided i n t o s e v e r a l sub \u2212p r o p e r t i e s , e . g . , green i s a sub \u2212p r o p e r t y of the p r o p e r t y c o l o r . The p o s s i b l e p r o p e r t i e s and sub \u2212p r o p e r t i e s are the f o l l o w i n g : ", "page_idx": 31}, {"type": "text", "text": "INSERT_PREVIOUSLY_OBTAINED_PROPERTY_LIST", "page_idx": 31}, {"type": "text", "text": "Focusing only on t h e s e p r o p e r t i e s , p l e a s e perform the f o l l o w i n g t a s k s . F i r s t , f o r every o b j e c t in the image p l e a s e l i s t the sub \u2212p r o p e r t i e s from the given l i s t s t h a t the o b j e c t d e p i c t s . Only name the sub \u2212p r o p e r t i e s t h a t are given . Please use the f o l l o w i n g format : ", "page_idx": 31}, {"type": "text", "text": "{   \nObject1 : [ sub \u2212property , . . . ] ,   \nObject2 : [ sub \u2212property , . . . ] , } ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We demonstrate empirically that NCB generates expressive encodings comparable to supervised learned concepts, among others on the novel CLEVR-Sudoku dataset. Additionally we highlight the inspectability and revisablility of the learned concept space. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper contains an explicit limitation section, discussing potential limitations of this work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Experimental setup and training details are provided in the appendix. Additionally, the setup is available in the provided code, together with the CLEVR-Sudoku dataset. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Access to the code repository is provided. For the CLEVR-Sudoku dataset, the generation flies are provided in the code and the full dataflies will be made public upon acceptance. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The experimental details are provided in the appendix and the provided code. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper provides results over multiple seeds. In all experiments, average and standard deviation are reported. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide details about the used computational resources in the appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The relevant parts of the code of ethics are discussed in the impact statement and the remainder of the checklist. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper contains an explicit impact statement, discussing potential societal impacts. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific clusters), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve pretrained models of any kind. The released dataset is not scraped from the internet and does not require safeguards. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The authors of existing models and datasets used within the paper are cited and their licenses respected. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The released code and the new dataset are both documented, including training and license information. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]