[{"figure_path": "ypPzyflbYs/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of different approaches for concept learning. Hereby, we differentiate based on the following categories: whether a method (1) is learned in an unsupervised fashion, (2) provides object-level concepts (i.e., can explicitly process multiple objects), (3) provides factor-level concepts (e.g., the color green), (4) provides continuous concept encodings, (5) provides discrete concept encodings, (6) provides inherently inspectable and (7) revisable concept representations.", "description": "This table compares several methods for concept learning across seven criteria: whether the learning is unsupervised, whether object-level and factor-level concepts are considered, whether continuous or discrete concept encodings are used, and whether the resulting concept representations are inherently inspectable and revisable.  It highlights the unique capabilities of the Neural Concept Binder (NCB) in contrast to existing methods.", "section": "3 Neural Concept Binder (NCB): Extracting Hard from Soft Concepts"}, {"figure_path": "ypPzyflbYs/tables/tables_7_1.jpg", "caption": "Table 2: NCB's concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best (\"\") and runner-up (\"\u25cb\") results are bold.", "description": "This table presents the results of classifying object properties using different continuous and discrete concept encodings.  The experiment varied the amount of training data (N Train) provided to a classifier for four different encodings: SysBinder (continuous), SysBinder, SysBinder (hard), SysBinder (step), and NLOTM. The Neural Concept Binder (NCB) results are also shown.  The best and second-best performing models for each training data size are highlighted in bold. This demonstrates the expressiveness of NCB's encodings even with limited training data.", "section": "4.1 Evaluations"}, {"figure_path": "ypPzyflbYs/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of different approaches for concept learning. Hereby, we differentiate based on the following categories: whether a method (1) is learned in an unsupervised fashion, (2) provides object-level concepts (i.e., can explicitly process multiple objects), (3) provides factor-level concepts (e.g., the color green), (4) provides continuous concept encodings, (5) provides discrete concept encodings, (6) provides inherently inspectable and (7) revisable concept representations.", "description": "This table compares several concept learning approaches across seven criteria: unsupervised learning capability, ability to handle object-level and factor-level concepts, use of continuous vs. discrete encodings, and the inspectability and revisability of the learned concepts.  It highlights the Neural Concept Binder's (NCB) advantages in offering all seven features, unlike other methods.", "section": "3 Neural Concept Binder (NCB): Extracting Hard from Soft Concepts"}, {"figure_path": "ypPzyflbYs/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of different approaches for concept learning. Hereby, we differentiate based on the following categories: whether a method (1) is learned in an unsupervised fashion, (2) provides object-level concepts (i.e., can explicitly process multiple objects), (3) provides factor-level concepts (e.g., the color green), (4) provides continuous concept encodings, (5) provides discrete concept encodings, (6) provides inherently inspectable and (7) revisable concept representations.", "description": "This table compares several existing concept learning methods along seven criteria: unsupervised learning capability, ability to handle object-level and factor-level concepts, use of continuous or discrete concept encodings, and the inspectability and revisability of learned concepts.  It highlights the unique strengths of the Neural Concept Binder (NCB) method introduced in the paper by showing that it satisfies all seven criteria, unlike other approaches.", "section": "3 Neural Concept Binder (NCB): Extracting Hard from Soft Concepts"}, {"figure_path": "ypPzyflbYs/tables/tables_21_1.jpg", "caption": "Table 2: NCB's concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best (\"\") and runner-up (\"o\") results are bold.", "description": "This table presents the results of classifying object properties using different continuous and discrete concept encodings.  The experiment varies the number of training samples provided to the classifier (N_Train = 2000, 200, 50, 20) and compares the performance of  NCB with several baselines: SysBinder (continuous), SysBinder, SysBinder (hard), SysBinder (step), and NLOTM.  The accuracy of the classification is shown for both the CLEVR-Easy and CLEVR datasets. The best and second-best performing methods for each condition are highlighted.", "section": "4 Experimental Evaluations"}, {"figure_path": "ypPzyflbYs/tables/tables_21_2.jpg", "caption": "Table 5: Ablation: Classifying attributes from concept representations with sub-optimal NCB components. The left column serves as a reference and represents the configurations used in the main evaluations, i.e., where the soft binder was trained for 600 epochs and the clustering model represented the HDBSCAN approach that was optimized via a grid-search over its corresponding hyperparameters.", "description": "This table presents an ablation study on the Neural Concept Binder (NCB) by varying the training epochs of the soft binder, removing the hyperparameter grid search optimization for HDBSCAN clustering, and replacing HDBSCAN with k-means clustering. The impact of these changes on the accuracy of classifying object attributes using NCB's concept representations is evaluated on the CLEVR dataset with various training set sizes. The leftmost column shows the baseline NCB performance, while the remaining columns illustrate the effects of the changes.", "section": "F Additional Quantitative Results"}, {"figure_path": "ypPzyflbYs/tables/tables_29_1.jpg", "caption": "Table 2: NCB's concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best (\"\") and runner-up (\"o\") results are bold.", "description": "This table presents the results of a classification experiment using different continuous and discrete concept encodings.  The goal is to show that NCB's discrete encodings are expressive, even when limited training data is available.  The table compares NCB's performance against several other methods (SysBinder (cont.), SysBinder, SysBinder (hard), SysBinder (step), and NLOTM) across different training set sizes (N=2000, 200, 50, 20) for both CLEVR-Easy and CLEVR datasets.  The best and second-best performing methods for each condition are highlighted.", "section": "4 Experimental Evaluations"}, {"figure_path": "ypPzyflbYs/tables/tables_29_2.jpg", "caption": "Table 2: NCB\u2019s concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best (\"\") and runner-up (\"o\") results are bold.", "description": "This table presents the results of a classification experiment designed to evaluate the expressiveness of NCB's concept encodings, even under an information bottleneck. Different continuous and discrete encoding methods are compared, including SysBinder (continuous), SysBinder (hard), SysBinder (step), NLOTM, and NCB. The classifier was trained with varying amounts of training data (2000, 200, 50, and 20 encodings) for both the CLEVR-Easy and CLEVR datasets. The accuracy of object property classification demonstrates that NCB's discrete encodings are highly expressive even with limited training data and outperform other discrete methods.", "section": "4 Experimental Evaluations"}, {"figure_path": "ypPzyflbYs/tables/tables_30_1.jpg", "caption": "Table 2: NCB's concept encodings are expressive despite information bottleneck. Classifying object properties from different continuous and discrete encodings. The classifier is provided with different amounts of training sample encodings. The best (\"\") and runner-up (\"o\") results are bold.", "description": "This table presents the results of classifying object properties using different continuous and discrete concept encodings.  The experiment varied the amount of training data (number of encodings) used for the classifier. The performance of NCB's concept encodings is compared against several baselines, including variations of the SysBinder model and the NLOTM model. The table shows that NCB achieves high accuracy even with limited training data, demonstrating the expressiveness of its encodings despite the information bottleneck.", "section": "4.1 Evaluations"}]