[{"type": "text", "text": "Aligning LLM Agents by Learning Latent Preference from User Edits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ge $\\mathbf{Gao^{26}}*$ Alexey Taymanov\u2662\u2217 Eduardo Salinas\u2662 Paul Mineiro\u2662 Dipendra Misra\u2662 Department of Computer Science, Cornell University\u2663 Microsoft Research New York\u2662 ggao@cs.cornell.edu {ataymano, edus, pmineiro, dimisra}@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study interactive learning of LLM-based language agents based on user edits made to the agent\u2019s output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent\u2019s alignment with the user\u2019s preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user\u2019s latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the $k$ -closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments \u2013 summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost over the base agent. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Language agents based on large language models (LLMs) have been developed for a variety of applications (Dohmke, 2022; Brynjolfsson et al., 2023), following recent breakthroughs in improving LLMs (Achiam et al., 2023; Ouyang et al., 2022b; Team et al., 2023). However, despite their impressive zero-shot performance, LLMs still need to align to a given user and task (Mysore et al., 2023; Li et al., 2023). In many applications, a natural feedback for LLM-based agents is user edits, where a user queries the agent and edits the agent\u2019s response before their own final use. In contrast, typical feedback used for fine-tuning, such as the comparison-based preference feedback in RLHF, is explicitly collected by providing annotators with model responses and asking them to rank (Ziegler et al., 2019; Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022a, inter alia), making such feedback an expensive choice for improving alignment. Motivated by this observation, we focus on interactive learning of LLM-based language agents using user edits as feedback. ", "page_idx": 0}, {"type": "image", "img_path": "DlYNGpCuwa/tmp/323975e10e408a6481388c9905b35e65f9e7a72be050f83e6da2936b5670d617.jpg", "img_caption": ["Figure 1: Illustration of interactive learning from user edits. Color coding in edits is for visualization only \u2013 our agent takes the plain revised text as feedback. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Consider the scenario in Figure 1 where a user interacts with an LLM-based writing assistant (agent) to complete their task. The interaction starts with the user (and the world) providing a context to the agent. This context may include a query prompt provided by the user, along with additional information provided by the world, such as the content on the screen, current time, and the user\u2019s calendar information. The agent generates a textual response given the context. ", "page_idx": 1}, {"type": "text", "text": "In the beginning, the agent\u2019s response may not be optimal for the user, as it is not personalized to this user\u2019s individual needs and preference. As most users are not familiar with prompt engineering, and LLMs are often able to generate a reasonable response for the task, therefore, users may find it the most convenient to simply edit this response when it is not ideal, rather than trying different prompts to get new responses. The example in Figure 1 illustrates that the user directly edits the summary generated by the agent to satisfy their preference for bullet point format. It takes time and effort for the user to make edits which can be measured using metrics such as the edit distance between the agent\u2019s response and the user edits. Our goal is to minimize the cumulative user edit cost over time using feedback from user edits. Notably, there is no distinction between training and testing in our setting as every natural use of the agent yields an edit feedback for learning. ", "page_idx": 1}, {"type": "text", "text": "We conjecture that user edits are driven by user\u2019s hidden preference which can be described in natural language. These preference descriptions are different from the notion of comparison-based preference used in RLHF. In this paper, we use the word preference to mean preference descriptions. For instance, preference of the user in Figure 1 can be described as bullet points. In practice, user preference can be compound, such as preferring bullet point, informal, with emojis at the same time, and also context-dependent, e.g., informal tone when writing an email to a family member, and formal tone when writing to a colleague. In more complex settings, user preference can evolve with time (nonstationary), or depend on information unavailable in the context (partially observed). Further, users may not be fully aware of all their preferences, or may fail to express these preferences in their query prompt. These considerations imply that user preference is latent to the language agent. If the agent could learn the latent preference correctly, it can significantly improve its performance by generating satisfactory responses. Furthermore, preference learned by the agent can be shown to the user to enhance interpretability, and can even be modified by the user to improve correctness. Motivated by this, we propose a learning framework, PRELUDE (PREference Learning from User\u2019s Direct Edits), where we seek to learn a user preference description for a given context using the history of user edits. ", "page_idx": 1}, {"type": "text", "text": "In a typical real-world scenario such as writing assistants, one has to potentially update the LLM-based agent for every user. Efficient approaches, therefore, must scale with the number of users. This makes approaches that fine-tune LLM parameters expensive to scale. Furthermore, LLMs typically undergo rigorous evaluation on a variety of safety tests before being released, and fine-tuning them can result in loosing the safety guarantees offered by these tests. For example, fine-tuning GPT-4 for millions of users can quickly turn very expensive. Approaches such as adding LORA and Adapter layers and only updating them, or using federated learning, can reduce the expense to some extent, but the loss of safety guarantees remains a concern. In this work, we focus on leveraging a frozen, black-box LLM, and instead learning a prompt policy that can infer user preference description for a given context, and then use it to directly drive the response generation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We introduce a simple yet effective algorithm CIPHER that implements the PRELUDE framework. CIPHER infers user preference for every context in the history with the aid of an LLM. In the future, given a context, it retrieves inferred preferences of similar contexts from the history and uses them to generate a response. CIPHER is computationally efficient and only slightly increases the LLM query cost compared to the base agent. ", "page_idx": 2}, {"type": "text", "text": "We introduce two interactive environments that evaluate the agent\u2019s ability to summarize documents and compose emails from a given notes. These tasks are inspired by writing assistant applications.For both tasks, we simulate a GPT-4 user that can generate edits based on a pre-designed latent preference that can vary based on the context. We evaluate CIPHER against several baselines and show that it achieves the lowest user edit cost. Additionally, CIPHER results in a lower LLM query cost than other retrieval-based baselines. Finally, we analyze preferences learned by our agents, and find that they show significant similarity to the ground truth latent preferences in our setup. ", "page_idx": 2}, {"type": "text", "text": "2 Interactive Learning from User Edits and the PRELUDE Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first describe LLM agents and the general learning framework from user edits and then discuss our PRELUDE framework and associated learning challenges. ", "page_idx": 2}, {"type": "text", "text": "LLM and Language Agents. We assume access to a language agent that internally relies on an LLM. We make no assumption on the agent except that it can take as input a piece of context which can include both texts and images and an additional prompt (which can be in-context learning examples or learned preferences) and generates a text response. The language agent may simply perform greedy decoding of the LLM given the input or may perform complex planning to generate a response. ", "page_idx": 2}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/86758cc2594ee5596b0d185d92eb793511b293fe38753cad2c2c8689ada8d42f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Interactive Learning from User Edits. In an application such as a writing assistant, a user interacts with the language agent over $T$ rounds. Protocol 1 shows such learning protocol. In the $t^{t h}$ round, the user and the world provide a context $x_{t}\\in\\mathscr{X}$ where $\\mathcal{X}$ is the space of all possible contexts. This context will include the user prompt in text, along with additional information provided by the user or the world, and may include multimodal data as well such as images. Given the context $x_{t}$ , the language agent generates a response $y_{t}\\in\\mathcal{V}$ in text, where $\\boldsymbol{\\wp}$ is the space of all texts. The user edits the response $y_{t}$ to $y_{t}^{\\prime}$ . If the user does not perform any edits, we treat this as setting $y_{t}^{\\prime}=y_{t}$ . The agent receives a cost of $\\boldsymbol{c}_{t}=\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})$ for this round, which measures the user\u2019s efforts on making edits. The goal of the agent is to minimize the sum of costs across all rounds $\\textstyle\\sum_{t=1}^{T}c_{t}$ . In our experiments, we use $\\Delta_{\\mathrm{edit}}$ as Levenshtein edit distance (Levenshtein, 1965) in the  token space which computes the minimum number of token insertion, deletion, and substitution necessary to convert $y_{t}$ to $y_{t}^{\\prime}$ . In general, a higher edit distance implies that the user has made more edits and spent more efforts. ", "page_idx": 2}, {"type": "text", "text": "PRELUDE Framework. We describe our PRELUDE framework in Protocol 2 which is a specialization of Protocol 1. In PRELUDE, in the $t^{t h}$ round, the agent infers the preference of the user as $f_{t}$ , and uses it to generate a response. We assume that in this round and for the given context $x_{t}$ , the user has a latent preference $f_{t}^{\\star}$ that drives the user to perform all edits. Furthermore, we assume that if the agent was able to infer this latent preference $\\langle f_{t}=f_{t}^{\\star})$ , then it will lead to minimal possible edits.2 To remove the dependence on performance due to the choice of the base LLM agent, we compare with an oracle agent that has access to $f_{t}^{\\star}$ at the start of each round. We assume that the LLM remains frozen across all methods in this work. ", "page_idx": 2}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/54e9f602d96f04b7dfa5f057f2a54093ec8513abefab8171526fbbe50c0083ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Challenges of Learning User Preference. Learning user preference from edits is challenging. In practice, user preference are multifaceted and complex. Furthermore, user\u2019s preference can also significantly vary based on the context. The feedback in the form of user edits emerges naturally but is inherently implicit, lacking direct expressions of the actual preference and carrying subtleties that may lead to diverse interpretations. The combination of preference variability and the implicit nature of feedback poses considerable challenges for agents in accurately learning and integrating these preferences. ", "page_idx": 3}, {"type": "text", "text": "3 Learning User Preference through Retrieval and Aggregation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our method, CIPHER (Consolidates Induced Preferences based on Historical Edits with Retrieval), that learns user preference based on user edits. ", "page_idx": 3}, {"type": "text", "text": "Algorithm $\\mathbf{1}\\,\\,\\mathrm{CIPHER}(\\phi,k,\\delta)$ . A context representation function $\\phi\\,:\\,\\boldsymbol{\\mathcal{X}}\\,\\rightarrow\\,\\mathbb{R}^{d}$ , the retrieval   \nhyperparameter $k$ , and tolerance hyperparameter $\\delta\\geq0$ . We initialize history $\\mathcal{D}=\\emptyset$ .   \n1: for $t=1,2,\\cdot\\cdot\\cdot,T\\,\\mathbf{c}$ do   \n2: User (and the world) presents a context $x_{t}$   \n3: Retrieve the top- $k$ examples $\\{\\phi(x_{z_{i}}),\\tilde{f}_{z_{i}}\\}_{i=1}^{k}$ in $\\mathcal{D}$ with maximum cosine similarity to $\\phi(\\boldsymbol{x}_{t})$   \n4: If $k>1$ , then query the LLM to aggregate these preferences $\\{\\tilde{f}_{z_{i}}\\}_{i=1}^{k}$ into $f_{t}$ , else $f_{t}=\\tilde{f}_{z_{1}}$   \n5: Agent generates a text response $y_{t}$ based on $x_{t}$ and $f_{t}$   \n6: User edits the response to $y_{t}^{\\prime}$ using their latent preference $f_{t}^{\\star}$   \n7: Agent incurs a cost $c_{t}=\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})$   \n8: if $c_{t}\\leq\\delta$ then   \n9: $\\tilde{f_{t}}=f_{t}$   \n10: else   \n11: Query the LLM to generate a preference $\\tilde{f}_{t}$ that best explains user edits in $(y_{t},y_{t}^{\\prime})$   \n12: $\\begin{array}{r l}&{\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{(\\phi(x_{t}),\\tilde{f}_{t})\\}}\\\\ &{\\mathrm{urn}\\sum_{t=1}^{T}c_{t}}\\end{array}$   \n13: Return tT=1 ct ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 shows CIPHER which implements the PRELUDE framework. CIPHER maintains a preference history $\\mathcal{D}_{t}=\\{(x_{\\ell},\\tilde{f}_{\\ell})\\}_{\\ell=1}^{t-1}$ of past contexts $x_{\\ell}$ along with a preference $\\tilde{f_{\\ell}}$ inferred by the agent. CIPHER assumes access to a context representation function $\\phi:\\mathcal{X}\\to\\mathbb{R}^{d}$ that can map a context to a vector representation. For a given round $t$ with context $x_{t}$ , the agent first retrieves the $k$ -closest contexts from the interaction history $\\mathcal{D}_{t}$ . We use cosine similarity for computing proximity, although other metrics such as Euclidean distance, or Hamming distance when $\\phi$ outputs a binary vector, can be used. Given the retrieved contexts and their inferred preferences $\\{(x_{z_{i}},\\tilde{f}_{z_{i}})\\}_{i=1}^{k}$ , we query the underlying LLM to summarize the inferred preferences $\\{\\tilde{f}_{z_{i}}\\}_{i=1}^{k}$ into a single preference $f_{t}$ . In the beginning, when $t\\leq k$ , we retrieve all the past $t$ contexts. In particular, for $t=1$ we have $f_{1}$ as an empty string as the agent has no prior knowledge of this user\u2019s preference.3 ", "page_idx": 3}, {"type": "text", "text": "The agent uses the inferred preference $f_{t}$ to generate the response. This is done by concatenating the context $x_{t}$ with an agent prompt such as \u201cThis user has a preference of $<f_{t}>$ which must be used when generating the response\u201d, where $<f_{t}>$ indicates where we insert the inferred preference $f_{t}$ . We list the actual template used in our experiments in Table 7 in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Given the user edits $y_{t}^{\\prime}$ , if the user edits are minimal, i.e., $\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})\\le\\delta$ for a hyperparameter $\\delta$ , then we set the inferred preference for this round as ${\\tilde{f}}_{t}=f_{t}$ as using $f_{t}$ for generating a response resulted in minimal edits. However, if $\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})\\,>\\,\\delta$ , then we query the LLM a third time to generate the inferred preference $\\tilde{f}_{t}$ that explains why the user edited $y_{t}$ to $y_{t}^{\\prime}$ . We call this the Latent Preference Induction (LPI) step. In both cases, we append $(x_{t},f_{t})$ to the preference history. ", "page_idx": 3}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/5e4a161304f9b91b53b618ff456dd21969f9f3bacaeaa36dce1d4a7dbd115e25.jpg", "table_caption": ["Table 1: Latent user preference design, specific to the document source. "], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/3a9c364a8f9cf09e8c0c324f534fb1764604e1fca9941a1a7eb83691d9d60f78.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Note that we cannot query the LLM for the inferred preference in the first case where the user edit cost $c_{t}$ is small, i.e., $c_{t}\\leq\\delta$ . In this case, querying the LLM to infer the preference to explain the edits in $y_{t}^{\\prime}$ given $y_{t}$ , will result in the LLM outputting that the agent has no preference which is incorrect. ", "page_idx": 4}, {"type": "text", "text": "Computational Cost of CIPHER. In a given round, CIPHER adds a maximum of 3 LLM calls on top of the cost of calling the underlying inference algorithm of the agent in line 5. CIPHER further reduces the memory storage by only storing the representation of contexts in the preference string instead of the input itself. Finally, CIPHER only adds a small prompt to the context $x_{t}$ , before calling the agent\u2019s inference algorithm. This only slightly increases the length of the prompt, thereby, reducing the query cost associated with LLMs that scales with the number of input tokens. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first introduce two interactive tasks for learning from user edits, and then describe our results. ", "page_idx": 4}, {"type": "text", "text": "4.1 Two Interactive Writing Assistant Environments for Learning from User Edits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Task. We introduce two tasks inspired by the use of LLMs as writing assistants (Mysore et al., 2023; Shen et al., 2023; Wang et al., 2023). In the first task, we evaluate the agent\u2019s ability to summarize a given document. In the second task, we evaluate the agent\u2019s ability to compose an email given notes. For both tasks, we use documents from several existing sources listed in Table 1. These sources represent a diverse category of documents that a writing assistant would typically encounter (see Table 4 in Appendix for examples). In any given round, the user is provided a context that is a document from one of the sources for the given task. Importantly, the agent is unaware of the source of the given document which as we discuss later, will determine the user preference. For both tasks, we run an experiment for $T=200$ rounds. We sample an equal number of documents from each source and mix them to remove any temporal correlation in document sources. ", "page_idx": 4}, {"type": "text", "text": "Two-Stage GPT-4 Simulated User. We simulate a user that can edit a given response. We define a set of latent user preferences for the user that vary based on the document source. Table 1 lists the preference for every source. This captures the context-dependent nature of user preferences as the document source influences the type of context. For example, the Personal problem document source contains documents pertaining to discussions with a friend, and a user may have a different preference when writing an email to a friend compared to writing an email to a colleague. We assume that our user is aware of the document source $d_{t}$ of a given context $x_{t}$ . This implies, that we can express the true user preference for $x_{t}$ as $f_{t}^{\\star}=F(d_{t})$ where $F$ maps a given document source to the user preference. Recall that the agent is never provided the document source of any context. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We model our user using GPT-4 with a two-stage approach. Given an agent response $y_{t}$ and the context $x_{t}$ , we first query GPT-4 to check if $y_{t}$ satisfies the preference in $f_{t}^{\\star}$ . If the answer is yes, then the user preforms no edits and returns $y_{t}^{\\prime}=y_{t}$ . If the answer is no, then we use GPT-4 to generate the edited response $y_{t}^{\\prime}$ given $y_{t}$ and $f_{t}^{\\star}$ . We found that our two-stage GPT-4 user can generate high-quality edits, consistent with observations in prior work that LLM-written feedback is high-quality and useful to learn from (Bai et al., 2022; Saunders et al., 2022). We adopted a two-stage process since using GPT-4 to directly edit the response $y_{t}$ always resulted in edits even when the response satisfied the preference $f_{t}^{\\star}$ . We provide GPT-4 user prompt template and user edit examples in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metric. We propose three metrics for evaluating agents learning from user edits. Our main metric is the cumulative user edit cost $\\begin{array}{r}{\\sum_{t=1}^{T}\\Delta_{\\mathrm{edit}}(y_{t},\\bar{y}_{t}^{\\prime})}\\end{array}$ over $T$ rounds where $\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})$ is the Levenshtein edit distance between a gent response $y_{t}$ and user edits $y_{t}^{\\prime}$ computed in the token space using Tiktoken tokenizer. For methods that learn an interpretable preference, we additionally evaluate the quality of the inferred user preference $f_{t}$ . We do so by evaluating if $f_{t}$ is closer to the true preference $\\overline{{f_{t}^{\\star}}}=F(d_{t})$ , where $d_{t}$ is the document source of context in round $t$ ,r cg tBoE pRreTfSecroerne ,h ewr hdeorceu BmEenRtT sSocuorrcee .( ZFhoarnmga\\*l ley,t  awl.e,  c2o0m20p)u ties $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{1}\\{d_{t}=}\\end{array}$ $\\operatorname*{max}_{d\\in S}$ $(f_{t},F(\\dot{d}))\\}$   \nmetric and $\\boldsymbol{S}$ is the set of all document sources. Finally, we report the total number of input and output BPE tokens to the LLM across all rounds. This measures the expense associated with using LLM, used by popular LLM providers to charge their customers. ", "page_idx": 5}, {"type": "text", "text": "4.2 Details of CIPHER and Comparison Systems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use GPT-4 as our base LLM for CIPHER and all baselines. We do not perform fine-tuning of the GPT-4 and do not add any additional parameters to the model. We use a prompt-based GPT-4 agent for all methods that uses a single prompt with greedy decoding to generate the response. Our main method CIPHER and the baselines, can be extended to more complex language agents that perform multiple steps of reasoning on top of the base LLM before generating a response. ", "page_idx": 5}, {"type": "text", "text": "CIPHER Details. We use a simple agent that uses GPT-4 with a prompt template to generate the response $y_{t}$ given the context $x_{t}$ and preference $f_{t}$ . We list templates in Table 7 in Appendix B. We experiment with MPNET (Song et al., 2020) and BERT (Devlin et al., 2019) as our two context representation functions $\\phi$ , and use cosine similarity for retrieval. We experiment with two different values of the number of retrieved examples $k\\in\\{1,5\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Baselines. We evaluate CIPHER against baselines that either perform no learning, or learn contextagnostic preferences, or directly use past edits to generate a response: ", "page_idx": 5}, {"type": "text", "text": "1. No learning: The agent performs no learning based on interaction with the user.   \n2. Explore-then-exploit ( $E$ -then- $e$ ) $L P I$ : This baseline is based on the classic explore-then-exploit strategy in interactive learning (Garivier et al., 2016). The agent first generates responses for the first $T_{e}$ rounds without performing any learning (exploration stage). It then infers a single user preference $\\tilde{f}_{e}$ using the user edits in the first $T_{e}$ rounds by applying the LPI step (Algorithm 1, line 11), which is used to generate responses for remaining rounds (exploitation step).   \n3. Continual $L P I$ : This baseline is similar to $E$ -then-e LPI except that it never stops exploring and avoids overfitting to the first $T_{e}$ rounds. In any given round $t$ , it uses the data of all past edits $\\{(y_{\\ell},y_{\\ell}^{\\prime})\\}_{\\ell=1}^{t-1}$ to learn a preference $f_{t}$ by performing the LPI step. It then generates a response using this preference. Similar to -then-e , this approach learn context-agnostic preferences.   \n4. ICL-edit: This is a standard retrieval-based in-context learning (ICL) baseline (Brown et al., 2020). In a given round $t$ , the agent first retrieves the closest $k$ examples $\\{(y_{z_{\\ell}},y_{z_{\\ell}}^{\\prime})\\}_{\\ell=1}^{k}$ to the given context $x_{t}$ using the representation function $\\phi$ . These examples are provided in an ICL prompt and use to generate the response $y_{t}$ . This approach does not learn preferences but unlike E-then-e LPI and Continual LPI it can perform context-dependent learning. ", "page_idx": 5}, {"type": "text", "text": "5. CoT-edit: This is a standard retrieval-based chain-of-thought (CoT) baseline (Wei et al., 2022). This baseline is similar to ICL-edit except the prompt for generation requires the agent to infer a user preference $f_{t}$ based on retrieved $k$ examples, and generate an output according to $f_{t}$ .4 ", "page_idx": 6}, {"type": "text", "text": "Oracle Method. We also evaluate an oracle approach which uses the true user preference in each round to generate the response. This provides an upper bound on performance and helps to evaluate if our setup is well-designed, i.e., whether learning the true user preference indeed leads to low edit costs. ", "page_idx": 6}, {"type": "text", "text": "4.3 Main Result and Discussion. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Main Results. Table 2 reports the performance of all methods on the two tasks on three metrics. We report the mean and standard deviation across 3 different random seeds.5 ", "page_idx": 6}, {"type": "text", "text": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy. $\\mu_{\\sigma}$ denotes the mean $\\mu$ and standard deviation $\\sigma$ across 3 runs over different seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is $\\cdot10^{5}$ ). We use $-k$ in method names to denote that we use $k$ retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best. ", "page_idx": 6}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/73a7f594d146b2a9224747d1b874e8db4781909086a129304eee99cfcdb0ba05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Discussion of Main Result. We observe that not performing learning results in a high edit cost, whereas using the oracle preferences achieves a significantly smaller edit cost. This shows that our environments are sound and well-conditioned. E-then-e LPI and Continual LPI learn context-agnostic preferences which cannot capture the context-dependent preferences in the environments and end up doing poorly. For the summarization task, they end up with a higher edit distance than even performing no learning. One possible explanation is that using context-agnostic preferences can push the model to specialize to a given preference much more than the base model, resulting in more edits when that preference is incorrect. We see this in preference accuracy, which is low for both of these baselines, and lower for the summarization task than the email writing task where they outperform no learning baselines. Further, Continual LPI has a higher expense cost due to constantly querying the LLM to infer the user preference. ", "page_idx": 6}, {"type": "text", "text": "ICL-edit baselines perform significantly better on the summarization task. However, using a list of user edits in the prompt results in a higher token expense cost, as the responses and their edits can be significantly long in practice. Further, the ICL-edit baselines provide no interpretable explanation for their response or for explaining user behavior. Although $C o T.$ -edit baselines provide an interpretable preference, they still result in relatively high expense and low classification accuracy. ", "page_idx": 6}, {"type": "text", "text": "CIPHER achieves the smallest edit distance cost reducing edits by $31\\%$ in the summarization task and $73\\%$ in the email writing task. We observe that retrieving $k=5$ preferences and aggregating them achieves lower edit distance, however, the choice of ideal representation $\\phi$ seems task-dependent. ", "page_idx": 6}, {"type": "text", "text": "Figure 2: Learning curves of different methods based on cumulative cost over time (average across 3 seeds). In the legend, - $-k$ means with top $k$ retrieved examples, $-B$ for BERT, and $-M$ for MPNET. ", "page_idx": 7}, {"type": "image", "img_path": "DlYNGpCuwa/tmp/b7861c6cef95fca013f15240dbc73b208860288dd8f6c43b494184d126794b60.jpg", "img_caption": ["Figure 3: Percentage of zero-cost examples of CIPHER over time, binned per 20 rounds to show the trend (average across 3 seeds). In the legend, $-k$ means with top $k$ retrieved examples, $-B$ for BERT, and - $-M$ for MPNET. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "DlYNGpCuwa/tmp/5824e556137edaa18c634f021131fbe6caed52dd9417da592f25fd58109ae958.jpg", "img_caption": ["Further, CIPHER achieves the highest preference accuracy showing that CIPHER can learn preferences that correlate more with the ground truth preference than preferences of other document sources. Note that the performance of a random preference classifier is only $20\\%$ for summarization and $25\\%$ for email writing. Further, CIPHER achieves a smaller cost than ICL-edit and Continual LPI baselines, as it doesn\u2019t use long user edits in the prompt for generating a response. In summary, CIPHER provides a cheap, more effective, and interpretable method than our baselines. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Learning Curves. We plot mean cumulative user edit costs over rounds in Figure 2. The cumulative user edit costs in Figure 2 show that the angle of the learning curves decreases for CIPHER after an initial number of rounds, showing that learning helps decrease the rate at which user edits are accumulated. In contrast, the angle of the learning curve for the no-learning baseline remains unchanged. ", "page_idx": 7}, {"type": "text", "text": "Evaluating Fraction of Non-Edited Responses. Recall that the first stage of our GPT-4 user checks if the agent response satisfies the latent user preference $f^{\\star}$ . If it does, then no edits are performed, otherwise, the user edits the response. We plot the percentage of examples with zero edit cost per 20 rounds bin in Figure 3. We notice a small increase in the number of examples with zero edit cost. This indicates that gains come not just by increasing the number of examples that avoid getting edited in stage 1 of our user but more generally across examples. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Analysis of Learned Preferences. We evaluate the quality of preferences learned by CIPHER on the harder summarization task. Table 3 lists 3 learned preferences per document source for CIPHER-5-MPNET which are randomly sampled at the beginning, middle, and end of the interaction history. We see that overall the agent can learn a reasonable description of the latent preference. For example, it can learn bullet points preference for Wikipedia articles, and second person narrative for Reddit posts, and $Q A$ style for Movie reviews. CIPHER can pick some preferences fairly early such as bullet points for Wikipedia and emojis for Paper abstract, whereas some are learned only later such as Structured $Q\\&A$ for Movie reviews. This shows using CIPHER can quickly learn useful preferences, but further interaction continues to help.6 ", "page_idx": 7}, {"type": "text", "text": "Table 3: Examples of learned preferences on summarization task with CIPHER-5-MPNET, grouped based on the document source and corresponding latent preference. We randomly sample 3 examples per type at the beginning, middle, and end of the interaction history. ", "page_idx": 8}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/274076426866bee7a0bcda9ba88be884aa8f850d490561c34cf4a71292624d7b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Human Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct two types of evaluation with human users to further understand the performance of our methods on summarization. We focus on our best-performing method CIPHER-5-MPNET.7 ", "page_idx": 8}, {"type": "text", "text": "Win Rate Evaluation. We conduct win rate evaluation where evaluators are given a pair of text and choose which one has higher quality. We compare the output of CIPHER-5-MPNET against the output of the best-performing baseline ICL-edit-5-MPNET, and against the generation of the oracle method. Each evaluation covers 15 text pairs, with three random samples from each scenario in the last 50 rounds of interaction. We conduct these CIPHER vs. ICL and CIPHER vs. Oracle evaluations with 7 human evaluators recruited through our personal network. For each text pair, we consider the output receiving the majority vote as a win. We find that the win rate of CIPHER-5-MPNET against ICL-edit-5-MPNET is $73.3\\%$ . This confirms that our method outperforms the best-performing baseline for human users. In CIPHER vs. Oracle evaluation, the win rate of CIPHER-5-MPNET is $23.7\\%$ , which reflects the performance gap we reported in previous sections. ", "page_idx": 8}, {"type": "text", "text": "Edits by Human Users. We study the edit feedback from human users to the generation of CIPHER5-MPNET and the oracle method. We instruct human users to edit the output based on the given latent preference, and to leave no edits when the output aligns with the given preference. We mix 20 outputs from CIPHER-5-MPNET and the oracle method so that human users cannot tell the source of each output. The total edit distance, averaged across 3 human users, is 211 for CIPHER, and 98 for the oracle method. The averaged percentage of zero-edit examples is $60\\%$ for CIPHER and $76.7\\%$ for oracle. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We study aligning LLM-based agents using user edits that arise naturally in applications such as writing assistants. We introduce the PRELUDE framework that seeks to learn the latent user preferences that drive these edits, and uses them to generate a response. We propose a practical algorithm CIPHER that implements PRELUDEand outperforms baselines on two interactive tasks with a GPT-4 simulated user. Evaluating CIPHER with human-in-the-loop as well as developing algorithms that can fine-tune LLMs using user edit where fine-tuning is feasible, are interesting future work directions. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Gao was a research intern in MSR NYC, and later was partially supported by NSF project #1901030. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. We thank MSR NYC research community, Jonathan D. Chang, Daniel D. Lee, Claire Cardie, and Sasha Rush for helpful discussions and support. We also thank St\u00e9phane Aroca-Ouellette, Kyunghyun Cho, and Columbia NLP community for their valuable feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 9}, {"type": "text", "text": "Sweta Agrawal and Marine Carpuat. An imitation learning curriculum for text editing with nonautoregressive models. ArXiv, abs/2203.09486, 2022. ", "page_idx": 9}, {"type": "text", "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. ", "page_idx": 9}, {"type": "text", "text": "Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov. Correcting diverse factual errors in abstractive summarization via post-editing and language model infilling. ArXiv, abs/2210.12378, 2022. ", "page_idx": 9}, {"type": "text", "text": "Nitsan Bar. Papertweet. https://github.com/bnitsan/PaperTweet/, 2022. ", "page_idx": 9}, {"type": "text", "text": "Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge, and Dipanjan Das. Learning to split and rephrase from wikipedia edit history. ArXiv, abs/1808.09468, 2018. ", "page_idx": 9}, {"type": "text", "text": "Shaked Brody, Uri Alon, and Eran Yahav. A structural model for contextual code changes. Proceedings of the ACM on Programming Languages, 4:1 \u2013 28, 2020. ", "page_idx": 9}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. ", "page_idx": 9}, {"type": "text", "text": "Erik Brynjolfsson, Danielle Li, and Lindsey R Raymond. Generative ai at work. Technical report, National Bureau of Economic Research, 2023. ", "page_idx": 9}, {"type": "text", "text": "Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. Factual error correction for abstractive summarization models. ArXiv, abs/2010.08712, 2020. ", "page_idx": 9}, {"type": "text", "text": "Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun. Learning to generate better than your llm. arXiv preprint arXiv:2306.11816, 2023. ", "page_idx": 9}, {"type": "text", "text": "Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-No\u00ebl Pouchet, Denys Poshyvanyk, and Monperrus Martin. Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Transactions on Software Engineering, 47:1943\u20131959, 2018. ", "page_idx": 9}, {"type": "text", "text": "Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language feedback. arXiv preprint arXiv:2312.06853, 2023. ", "page_idx": 9}, {"type": "text", "text": "Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye Jin, Liner Yang, Yange Fan, Hanghang Fan, and Erhong Yang. Leveraging prefix transfer for multi-intent text revision. Annual Meeting of the Association for Computational Linguistics, 2023.   \nColin B. Clement, Matthew Bierbaum, Kevin P. O\u2019Keeffe, and Alexander A. Alemi. On the use of arxiv as a dataset, 2019.   \nMike D\u2019Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, and Doug Downey. Aries: A corpus of scientific paper edits made in response to peer reviews. ArXiv, abs/2306.12587, 2023.   \nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. North American Chapter of the Association for Computational Linguistics, 2019.   \nThomas Dohmke. Github copilot is generally available to all developers. https://github.blog/ 2022-06-21-github-copilot-is-generally-available-to-all-developers/, 2022. Accessed: April-20-2024.   \nWanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa Lopez, and Dongyeop Kang. Understanding iterative revision from human-written text. ArXiv, abs/2203.03802, 2022.   \nFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. ArXiv, abs/2010.12826, 2020.   \nPatrick Fernandes, Aman Madaan, Emmy Liu, Ant\u00f3nio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jos\u00e9 G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, and Andr\u00e9 F. T. Martins. Bridging the gap: A survey on integrating (human) feedback for natural language generation. ArXiv, abs/2305.00955, 2023.   \nWikimedia Foundation. Wikimedia downloads. https://dumps.wikimedia.org, 2022.   \nAur\u00e9lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. Advances in Neural Information Processing Systems, 29, 2016.   \nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024.   \nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437\u2013450, 2017.   \nBraden Hancock, Antoine Bordes, Pierre-Emmanuel Mazar\u00e9, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! Annual Meeting of the Association for Computational Linguistics, 2019.   \nXinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. Argument mining for understanding peer reviews. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), June 2019.   \nDaniel James Kershaw and R. Koeling. Elsevier oa cc-by corpus. ArXiv, abs/2008.00774, 2020. doi: https://doi.org/10.48550/arXiv.2008.00774. URL https://elsevier.digitalcommonsdata. com/datasets/zm33cdndxs.   \nZae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. Improving iterative text revision by learning where to edit from other revision tasks. ArXiv, abs/2212.01350, 2022. ", "page_idx": 10}, {"type": "text", "text": "Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq R. Joty, Caiming Xiong, and Chien-Sheng Wu. Swipe: A dataset for document-level simplification of wikipedia pages. Annual Meeting of the Association for Computational Linguistics, 2023. ", "page_idx": 11}, {"type": "text", "text": "Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L.C. Guo, Md. Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S. Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, and Pao Siangliulue. A design space for intelligent and interactive writing assistants. Conference on Human Factors in Computing Systems, abs/2403.14117, 2024.   \nVladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics. Doklady, 10:707\u2013710, 1965.   \nCheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. Automatic prompt rewriting for personalized text generation. arXiv preprint arXiv:2310.00152, 2023.   \nJiwei Li, Alexander H. Miller, Sumit Chopra, Marc\u2019Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. ArXiv, abs/1611.09823, 2016.   \nZichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung, and Siva Reddy. Using interactive feedback to improve the accuracy and explainability of question answering systems post-deployment. ArXiv, abs/2204.03025, 2022.   \nRuibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu, and Soroush Vosoughi. Second thoughts are best: Learning to re-align with human values from text edits. ArXiv, abs/2301.00355, 2023.   \nYixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker, Dragomir R. Radev, and Ahmed Hassan Awadallah. On improving summarization factual consistency from natural language feedback. Annual Meeting of the Association for Computational Linguistics, 2022.   \nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, June 2011.   \nChaitanya Malaviya, Subin Lee, Dan Roth, and Mark Yatskar. Pachinko: Patching interpretable qa models through natural language feedback. ArXiv, abs/2311.09558, 2023.   \nJonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido. Felix: Flexible text editing through tagging and insertion. ArXiv, abs/2003.10687, 2020.   \nEdison Marrese-Taylor, Machel Reid, and Yutaka Matsuo. Variational inference for learning representations of natural language edits. ArXiv, abs/2004.09143, 2020.   \nEdison Marrese-Taylor, Machel Reid, and Alfredo Solano. Edit aware representation learning via levenshtein prediction. The Fourth Workshop on Insights from Negative Results in NLP, 2023.   \nDipendra Misra, Aldo Pacchiano, and Robert E Schapire. Provable interactive learning with hindsight instruction feedback. arXiv preprint arXiv:2404.09123, 2024.   \nMasato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya Mizumoto, Jun Suzuki, and Kentaro Inui. Towards automated document revision: Grammatical error correction, fluency edits, and beyond. ArXiv, abs/2205.11484, 2022.   \nSheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers. arXiv preprint arXiv:2311.09180, 2023.   \nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, 2021.   \nKhanh X Nguyen, Dipendra Misra, Robert Schapire, Miroslav Dud\u00edk, and Patrick Shafto. Interactive learning from activity description. International Conference on Machine Learning, pp. 8096\u20138108, 2021.   \nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, 2022a.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u2013 27744, 2022b.   \nDominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov, and Iryna Gurevych. Learning from free-text human feedback - collect new datasets or extend existing ones? ArXiv, abs/2310.15758, 2023.   \nDheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay Kumar Jauhar, Diyi Yang, and Eduard H. Hovy. One document, many revisions: A dataset for classification and description of edit intents. International Conference on Language Resources and Evaluation, 2022.   \nMachel Reid and Graham Neubig. Learning to model editing processes. Conference on Empirical Methods in Natural Language Processing, 2022.   \nMachel Reid, Vincent J. Hellendoorn, and Graham Neubig. Diffuser: Diffusion via edit-based reconstruction. International Conference on Learning Representations, 2023.   \nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Ouyang Long, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. ArXiv, abs/2206.05802, 2022.   \nJ\u2019er\u2019emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. ArXiv, abs/2303.16755, 2023.   \nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointergenerator networks. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), July 2017.   \nZejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. Beyond summarization: Designing ai support for real-world expository writing tasks. arXiv preprint arXiv:2304.02623, 2023.   \nWeiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu. When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. ArXiv, abs/2210.15893, 2022.   \nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. ArXiv, abs/2004.09297, 2020.   \nFelix Stahlberg and Shankar Kumar. Seq2edits: Sequence transduction using span-level edit operations. Conference on Empirical Methods in Natural Language Processing, 2020.   \nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. $A r X i\\nu$ , abs/2009.01325, 2020.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nSitong Wang, Lydia B Chilton, and Jeffrey V Nickerson. Writing with generative ai: Multi-modal and multi-dimensional tools for journalists. The Second Workshop on Intelligent and Interactive Writing Assistants at ACM CHI, 2023.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.   \nJason Weston. Dialog-based language learning. ArXiv, abs/1604.06045, 2016.   \nJing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston. Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback. Annual Meeting of the Association for Computational Linguistics, 2022.   \nZiyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham Neubig. Learning structural edits via incremental tree transformations. ArXiv, abs/2101.12087, 2021.   \nPengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Learning to represent edits. ArXiv, abs/1810.13337, 2018.   \nJiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milo\u0161 Gligori\u00b4c. Coditt5: Pretraining for source code and natural language editing. Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022.   \nTianyi Zhang\\*, Varsha Kishore\\*, Felix $\\mathrm{{Wu^{*}}}$ , Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. International Conference on Learning Representations, 2020.   \nDaniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, 2019. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We describe related work in this area grouped by main themes in this work. ", "page_idx": 14}, {"type": "text", "text": "Learning from Feedback. Besides pair-wise comparison feedback from annotators used in Reinforcement Learning from Human Feedback (RLHF) research (Ziegler et al., 2019; Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022a, inter alia), prior work has also studied free-form text feedback provided by annotators (Fernandes et al., 2023), such as on the task of dialog (Weston, 2016; Li et al., 2016; Hancock et al., 2019; Xu et al., 2022; Petrak et al., 2023), question answering (Li et al., 2022; Malaviya et al., 2023), summarization (Saunders et al., 2022), and general decision making (Cheng et al., 2023). This feedback, tailored to each example, is often utilized to rank candidate outputs, thereby improving task performance. Some work studies learning from text feedback to generate outputs directly (Scheurer et al., 2023; Bai et al., 2022; Shi et al., 2022), by generating multiple refinements of the original output based on the feedback and fine-tuning the original model to maximize the likelihood of the best refinement. In grounded settings such as instruction-based navigation, one line of work has also used hindsight feedback that explicitly provides a text instruction for the generated trajectory, to train policies (Nguyen et al., 2021; Misra et al., 2024). Moving beyond the conventional focus on text feedback that explicitly articulates human intent, we investigate feedback in the form of direct edits on the original model output. Such revisions by users occur naturally during model deployment in practice. Additionally, we examine the learning of user preferences through historical interactions, aiming to surpass the constraints of example-specific feedback. ", "page_idx": 14}, {"type": "text", "text": "Language Agents and Personalization. LLMs have enabled the development of language agents for a variety of tasks from writing assistants (Lee et al., 2024), coding assistants (Dohmke, 2022), and customer service assistants (Brynjolfsson et al., 2023). Since these LLM-based assistants are often used by individuals, a natural question has arisen on how to personalize these agents for each user. Straightforward approaches for fine-tuning LLMs includes supervised learning, online DPO (Guo et al., 2024), learning-to-search (Chang et al., 2023), and reinforcement learning (Ouyang et al., 2022b). These approaches can be directly applied to our setting. For example, one can use $(y_{t},y_{t}^{\\prime})$ in Protocol 1 as the preference data where $y_{t}^{\\prime}$ is preferred over $y_{t}$ , or use $y_{t}^{\\prime}$ as the ground truth for supervised learning. However, fine-tuning is expensive and hard to scale with the number of users. Therefore, a line of work has explored improving the alignment of frozen LLMs by prompt engineering, such as learning a personalized retrieval model (Mysore et al., 2023), learning a prompt policy given a reward function (Deng et al., 2022), or more generally, learning to rewrite the entire prompt (Li et al., 2023). We focus on learning a prompt policy by learning from user edits, and specifically, using them to extract textural descriptions of user preference. ", "page_idx": 14}, {"type": "text", "text": "Edits and Revisions. Many prior work on editing model output focuses on error correction, such as fixing source code (Yin et al., 2018; Chen et al., 2018; Reid et al., 2023) and improving the factual consistency of model summaries (Cao et al., 2020; Liu et al., 2022; Balachandran et al., 2022). A line of work has explored understanding human edits based on edit history of Wikipedia (Botha et al., 2018; Faltings et al., 2020; Rajagopal et al., 2022; Reid & Neubig, 2022; Laban et al., 2023), or revisions of academic writings (Mita et al., 2022; Du et al., 2022; D\u2019Arcy et al., 2023). Prior work explores predicting text revisions with edit intents (Brody et al., 2020; Kim et al., 2022; Chong et al., 2023), and modeling edits with various approaches, including latent vectors (Guu et al., 2017; Marrese-Taylor et al., 2020, 2023), structured trees (Yao et al., 2021), discrete diffusion process (Reid et al., 2023), or a series of singular edit operations (Stahlberg & Kumar, 2020; Mallinson et al., 2020; Agrawal & Carpuat, 2022; Zhang et al., 2022; Liu et al., 2023). However, these methodologies predominantly target generic improvements in model performance, overlooking the intricacies of individual user satisfaction and preference. Our research takes a distinct direction, focusing on understanding edits across a variety of examples to study user-level preferences, with a practical goal of aligning the agent to individual preferences. ", "page_idx": 14}, {"type": "text", "text": "B Additional Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Dataset Examples. We list links to dataset sources for our user-provided context in Table 4. ", "page_idx": 15}, {"type": "text", "text": "GPT-4 User\u2019s Edits We list examples of OUR GPT-4 user\u2019s edits with different latent preference on summarization in Table 5. ", "page_idx": 15}, {"type": "text", "text": "GPT-4 User Templates. Prompt templates used by our GPT-4 user are provided in Table 6. ", "page_idx": 15}, {"type": "text", "text": "Baseline Hyperparameters. For E-then-e LPI and Continual LPI we set $T_{e}=5$ . For ICL-edit baselines, we experimented with different values of $k$ , and report our best results with $k=5$ . ", "page_idx": 15}, {"type": "text", "text": "CIPHER Templates. Prompt templates used by CIPHER are provided in Table 7. ", "page_idx": 15}, {"type": "text", "text": "ICL-edit Templates. Prompt templates used by ICL-edit baseline are provided in Table 8. ", "page_idx": 15}, {"type": "text", "text": "CoT-edit Templates. Prompt templates used by CoT-edit baseline are provided in Table 9. ", "page_idx": 15}, {"type": "text", "text": "C Additional Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Evaluating Normalized Edit Cost. The cumulative user edit cost measures the total effort of the user but is susceptible to outlier examples, as the edit distance for a given round is potentially unbounded. Therefore, we also compute a normalized edit distance $\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})/|y_{t}|$ by dividing the edit distance by $\\operatorname*{max}\\{|y_{t}|,|y_{t}^{\\prime}|\\}$ , i.e. the max length of the agent output or user revised text. As Levenshtein distance $\\Delta_{\\mathrm{edit}}(y_{t},y_{t}^{\\prime})$ is upper bounded by $\\operatorname*{max}\\lbrace|\\bar{y_{t}}|,|y_{t}^{\\prime}|\\rbrace$ , therefore, the normalized cost is at most 1. Figure 4 reports normalized cost over rounds for the top 3 methods. We notice that for all variants of CIPHER for the summarization task, and for CIPHER-5-M for the email writing task, the normalized cost decreases notably as training progresses indicating learning. As the cost is normalized by the response length, even a small decrease can lead to a significant reduction in the number of tokens edited. ", "page_idx": 15}, {"type": "text", "text": "Detailed Expense. We list a detailed computational expense of different methods in Table 10. ", "page_idx": 15}, {"type": "text", "text": "Failure Case Analysis. CIPHER notably reduces the edit cost and learns useful preference, however, significant gaps to the oracle method remain, especially in the summarization task. We manually analyze failure cases on summarization task with the best performing method CIPHER-5- MPNET. Table 11 in the Appendix reports the summary and example of our findings, categorized as preference inference from output-revision pair, consolidation of inferred preferences, and retrieval. In brief, the most common type of failure is on the preference inference step given the agent output and user revision. For example, the agent often misses the exact keyword for brief or short sentences, and sometimes struggles with inferring the second-person narrative aspect. ", "page_idx": 15}, {"type": "text", "text": "Retrieval Accuracy. We calculate retrieval accuracy for CIPHER as the fraction of all retrieved contexts that are of the same document type as the currently given context across all seeds and time steps. We report the results in Table 12. We find that the retrieval accuracy is higher on the summarization task than on email writing. and using MPNET typically performs better than using Bert to encode context. ", "page_idx": 15}, {"type": "text", "text": "Survey Details. We did a small survey with several participants recruited from our personal network. The instructions for the two tasks are as follows: ", "page_idx": 15}, {"type": "text", "text": "1. Task 1 instruction: \u201cYou\u2019re asked to compare 2 pieces of writing in terms of satisfaction towards certain preference. There are 15 pairs to compare in total.\". For a specific example, we ask \u201cAssume that the writing style you prefer is <preference>. (e.g., you want to quickly get main opinions from a movie review) Which piece of writing below do you like better\". We replace <preference> with the given preference. ", "page_idx": 15}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/c7345680a33a7ffe9fdd6e8c2ce4976c30c72f51593a88904049afbe9118a2ae.jpg", "table_caption": ["Table 4: Link to each source dataset, from which we randomly sample examples as the user-provided context in our tasks. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Normalized cost of CIPHER over time, binned per 20 rounds to show the trend (average across 3 seeds). In the legend, $-k$ means with top $k$ retrieved examples, - $-\\boldsymbol{B}$ for BERT, and $-M$ for MPNET. ", "page_idx": 16}, {"type": "image", "img_path": "DlYNGpCuwa/tmp/2f881e9671fb2c4093d688aa412ff0bacb2d63cca930a799ce5d87b263c4868c.jpg", "img_caption": ["2. Task 2 instruction: \u2018\u2018This study is a simulation of how you use AI writing assistants. There are 20 pieces of text that you need to review. You will be given an assumption of the preferred writing style, and you can edit the piece of writing if it doesn\u2019t satisfy the given preference. If the writing aligns with the specified style, please leave no edits! Please treat each writing as an independent piece, even though some writings are based on the same article. For example, when reviewing the writing and making edits, do not bring in the knowledge you learned from another piece of writing. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Broader Impact Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our work provides a way to develop language agents that learn from user edits. Our work is part of a broader effort to release language-based agents and consequently shares all risks associated with deploying language agents such as hallucination or bias innate in these models. We advocate for caution and thorough testing in releasing language agents. Further, we strongly emphasize seeking the permission of users before releasing language agents that learn from their edits. Specially, users should have the option to take back their consent at any time. ", "page_idx": 16}, {"type": "text", "text": "Table 5: Examples of user edits with different latent preference on summarization. The agent output column shows the agent generation without conditioning on any preference. The latent preference for the user edits is in bold and parenthesis. We truncate some examples due to limited space. ", "page_idx": 17}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/f8110965fb680c179f8b6647b3f27d1e7b1263b2294c98891f0a2c0c85f759d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The article discusses the challenge of detecting and classifying stuttered speech, which affects many people but has been subject to limited research. The authors present a new model that differs from most existing methods, which typically use language models for stutter identification. Instead, their model is based on acoustic features, which allows it to identify various kinds of stuttering disfluencies without relying on speech recognition technology. The proposed model combines a deep residual network with bidirectional long short-term memory (LSTM) layers to classify different stutter types. It has been shown to significantly outperform current leading methods, reducing the average miss rate to $10.03\\%$ , which is a $27\\%$ improvement over the state-of-the-art. ", "page_idx": 18}, {"type": "text", "text": "(tweet style, simple English, inquisitive, skillful foreshadowing, with emojis)   \nEver struggled with stuttering or know someone who does? $\\Bumpeq$ Research on detecting stutters hasn\u2019t been huge, but guess what? A new study is changing the game! Instead of just looking at words, this smart model listens to how things are said. $\\textcircled{<}$ It\u2019s like a detective for sounds, using cool tech called deep learning and memory networks to spot different stutter types. $\\frac{48}{200}$ And it\u2019s crushing it, beating old methods by $27\\%$ ! Missed stutters? Down to just $10\\%!$ $7$ This could be a big deal for speech therapy! $35$ #Stuttering # SpeechTech # Innovation ", "page_idx": 18}, {"type": "text", "text": "The article discusses a movie that begins promisingly but later becomes confusing due to a poorly depicted portrayal of ancient black magic rituals. The male lead delivers a strong performance despite his character\u2019s unfavorable outcome in the climax. The movie is compared to a mix of \"Boomerang\" and \"Extremities.\" ", "page_idx": 18}, {"type": "text", "text": "(question answering style, direct, concise)   \nWhat is the general sentiment of the article about the movie?   \nThe article suggests that the movie starts off well but eventually becomes muddled. The depiction of ancient black magic rituals is criticized for being confusing.   \nHow does the article describe the male lead\u2019s performance?   \nThe male lead\u2019s performance is described as strong, even though his character does not have a favorable outcome in the climax.   \nWhat flims is the movie compared to in the article? The movie is compared to \"\"Boomerang\"\" and \"\"Extremities\"\" in the article. ", "page_idx": 18}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/5f1004bdd43b5bf9f3221bde12cb4b248a2e2a979095232901029f84a0871cda.jpg", "table_caption": ["Table 6: Prompt templates for the AI user. The first step is to prompt the user for yes/no answer regarding satisfaction. If the answer is no, the second step is to ask the user edit the agent output according to the latent preference. If the answer is yes, the agent output receives 0 edits. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/1c36a4c85f5fbb504ab8719389d13117ad95dcd58ce80d491701e10f6daad633.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/30a011091ceffee2f6042de4753e1acc12069a9654ce608985f75f79f1c775ec.jpg", "table_caption": ["Table 8: Prompt templates for the ICL-edit baseline. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/36e8659d59515579a20b6adfd260ffc5b66c4775a07baa05f021df81ec4c0af7.jpg", "table_caption": ["Table 9: Prompt templates for the CoT-edit baseline. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/ac762a9e1010eb5c9db3849d1696a2f97bd7785083ca7e68051d9bbf682b60e6.jpg", "table_caption": ["Table 10: Expense of different methods: number of BPE tokens in terms of input, output and total. Each number is the average across 3 runs (unit is $\\cdot10^{5}$ ). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/882b7b6142edce416d040b0f4a2bb0ac80b6ac5a4e505939d8406a71ed999db4.jpg", "table_caption": ["Table 11: Summary of failure cases on summarization task with CIPHER-5-MPNET. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 12: We report retrieval accuracy as the percentage of total retrieved document representations across all time steps and seeds that are of the same document source type as the context document for which they were retrieved. We use 3 seeds. We retrieve 600 examples for $k=1$ and 2970 examples for $k=5$ . ", "page_idx": 21}, {"type": "table", "img_path": "DlYNGpCuwa/tmp/eaaa8d6d0d022bce95f03120f0cc12305d0714ae07383fb6b09f316c5df2f7ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Discussed in intro and experiment section. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide a thorough failure case analysis in Appendix. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: NA. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide extensive details in the main paper and Appendix and will follow up with releasing code and linking it from the main paper. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We use publicly available data in our paper which we cite in the paper. We will release a code in the future and link it to the paper. We believe we provide enough details in the paper to reproduce all results in the paper including hyperparameters, prompt templates, etc. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide hyperparameter details in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: NA. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: NA. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \nJustification: NA. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have added a broader impact statement in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer:[NA] Justification: We do not release models or data with this paper. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite all data sources and models that we use. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: We will release the interactive environments in the future on Github and provide a detailed README. ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not provide a new dataset in this paper, but we do use common existing datasets that are available on Huggingface. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide survey details in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: NA. ", "page_idx": 23}]