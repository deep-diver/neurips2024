{"references": [{"fullname_first_author": "Y. LeCun", "paper_title": "Efficient backprop", "publication_date": "2002", "reason": "This paper is foundational for the efficient training of neural networks, a key aspect of the LoRA finetuning method discussed in the paper."}, {"fullname_first_author": "D. P. Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014", "reason": "The Adam optimizer is widely used in deep learning, including for LoRA finetuning, making this a highly relevant reference."}, {"fullname_first_author": "K. He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016", "reason": "ResNet architectures are fundamental to many large language models and the paper details their important properties for image recognition which provides critical context to the paper."}, {"fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019", "reason": "BERT is a highly influential pretrained language model and its pre-training methodology provides context for the LoRA finetuning discussed in the paper."}, {"fullname_first_author": "E. J. Hu", "paper_title": "LORA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021", "reason": "This paper introduces LoRA, the core subject of the current research paper, making it the most directly relevant and important reference."}]}