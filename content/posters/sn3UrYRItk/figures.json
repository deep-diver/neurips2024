[{"figure_path": "sn3UrYRItk/figures/figures_1_1.jpg", "caption": "Figure 1: Summary of our contributions in this paper: a description of the difference between the finetuning dynamics when LoRA weights A and B are initialized with Init[A] or Init[B].", "description": "This figure summarizes the key findings of the paper regarding the impact of initialization on LoRA finetuning. It compares two initialization schemes, Init[A] and Init[B], showing their effects on optimal learning rate, finetuning dynamics, and overall performance. Init[A], which initializes B to zero and A to random, generally leads to better performance due to more efficient feature learning, although with some instability.  Init[B] provides stability but suffers from suboptimal feature learning.  The figure visually represents the differences in the learning processes and the resulting performance.", "section": "1 Introduction"}, {"figure_path": "sn3UrYRItk/figures/figures_7_1.jpg", "caption": "Figure 3: Evolution of the norms of the  Z<sub>A</sub>, Z<sub>B</sub> features, averaged over training data. We compute the average  Z<sub>A</sub> def N<sup>-1</sup> \u03a3||Z<sub>A</sub>(x<sub>i</sub>)|| (and same for Z<sub>B</sub>), where the x<sub>i</sub>'s are the training data. The dynamics are shown for widths n = 128 and n = 8192, two seeds, and for both Init[A] and Init[B]. Train loss and the (optimal) learning rate are shown on top of each plot. We observe that the magnitude of Z<sub>A</sub> is significantly higher with Init[A] compared to Init[B] at large width (n = 8192). Interestingly, the train loss is smaller with Init[A], as compared to Init[B]. Results with other seeds and widths are shown in Appendix B.", "description": "This figure shows the evolution of the norms of the LoRA features Z<sub>A</sub> and Z<sub>B</sub> during training, averaged over the training data points.  It compares two different initialization schemes (Init[A] and Init[B]) for two different network widths (n = 128 and n = 8192). The plots illustrate that the magnitude of Z<sub>A</sub> is substantially larger with Init[A] than with Init[B], especially for the larger network width.  Despite this, the training loss is lower with Init[A], suggesting a tradeoff between feature magnitude and training loss. The figure also includes the optimal learning rates used for each initialization and network width.", "section": "3.7 Toy Model"}, {"figure_path": "sn3UrYRItk/figures/figures_7_2.jpg", "caption": "Figure 2: Optimal Learning rate for the fine-tuning of synthetic model Equation (4) with Init [A] and Init [B] as initialization. The optimal LRs are shown as a function of width n. Theoretical lines n\u207b\u00b9 and n\u207b\u00b9/\u00b2 are shown as well (constants C\u2081, C\u2082 are chosen to provide suitable trend visualization). As model width n grows, the optimal learning rate with Init [A] becomes larger than the optimal learning rate with Init [B]. This is in agreement with the theoretical results.", "description": "This figure shows the optimal learning rate for a synthetic model as a function of model width, comparing two different initialization schemes (Init[A] and Init[B]).  The plot shows that as the model width increases, the optimal learning rate for Init[A] becomes significantly larger than for Init[B].  Theoretical lines representing the scaling laws of n\u207b\u00b9 and n\u207b\u00b9/\u00b2 are included for comparison, demonstrating agreement with the theoretical analysis presented in the paper.", "section": "3.7 Toy Model"}, {"figure_path": "sn3UrYRItk/figures/figures_8_1.jpg", "caption": "Figure 3: Evolution of the norms of the ZA, ZB features, averaged over training data. We compute the average  ZA def N-11|ZA(xi) || (and same for ZB), where the xi's are the training data. The dynamics are shown for widths n = 128 and n = 8192, two seeds, and for both Init [A] and Init [B]. Train loss and the (optimal) learning rate are shown on top of each plot. We observe that the magnitude of ZA is significantly higher with Init [A] compared to Init [B] at large width (n = 8192). Interestingly, the train loss is smaller with Init [A], as compared to Init [B]. Results with other seeds and widths are shown in Appendix B.", "description": "This figure shows the evolution of the norms of the LoRA features ZA and ZB during training, averaged across training data points.  It compares two different initialization schemes (Init[A] and Init[B]) at different network widths (n=128 and n=8192). The results demonstrate that the magnitude of ZA is substantially larger with Init[A] than with Init[B], especially at larger widths.  Despite this, the training loss is lower with Init[A], suggesting a trade-off between feature magnitude and training loss.", "section": "3.7 Toy Model"}, {"figure_path": "sn3UrYRItk/figures/figures_9_1.jpg", "caption": "Figure 3: Evolution of the norms of the ZA, ZB features, averaged over training data. We compute the average ZA def N-1\u03a3i||ZA(xi)|| (and same for ZB), where the xi's are the training data. The dynamics are shown for widths n = 128 and n = 8192, two seeds, and for both Init [A] and Init [B]. Train loss and the (optimal) learning rate are shown on top of each plot. We observe that the magnitude of ZA is significantly higher with Init [A] compared to Init [B] at large width (n = 8192). Interestingly, the train loss is smaller with Init [A], as compared to Init [B]. Results with other seeds and widths are shown in Appendix B.", "description": "This figure displays the evolution of the norms of ZA and ZB features during training, averaged across training datapoints.  It compares two initialization schemes, Init [A] and Init [B], at different network widths (n=128 and n=8192) and random seeds. The plots reveal that the magnitude of ZA is considerably larger with Init [A] than with Init [B], especially at larger network widths.  This difference is interesting because the training loss is lower with Init [A].", "section": "3.7 Toy Model"}, {"figure_path": "sn3UrYRItk/figures/figures_22_1.jpg", "caption": "Figure 3: Evolution of the norms of the ZA, ZB features, averaged over training data. We compute the average ZA def N-1 \u03a3i ||ZA(xi)|| (and same for ZB), where the xi's are the training data. The dynamics are shown for widths n = 128 and n = 8192, two seeds, and for both Init [A] and Init [B]. Train loss and the (optimal) learning rate are shown on top of each plot. We observe that the magnitude of ZA is significantly higher with Init [A] compared to Init [B] at large width (n = 8192). Interestingly, the train loss is smaller with Init [A], as compared to Init [B]. Results with other seeds and widths are shown in Appendix B.", "description": "This figure shows the evolution of the norms of the LoRA features ZA and ZB during training, averaged across training data points.  It compares two initialization schemes, Init [A] and Init [B], across different network widths (n = 128 and n = 8192) and random seeds. The plots illustrate how the magnitude of ZA is considerably larger with Init [A] than with Init [B], especially at a larger width (n=8192).  Despite this, the training loss is lower for Init [A], suggesting that while Init [A] exhibits higher internal instability, it achieves better performance.", "section": "3.7 Toy Model"}]