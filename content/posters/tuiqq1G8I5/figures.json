[{"figure_path": "tuiqq1G8I5/figures/figures_8_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework.  It shows how identifying discriminative components (neurons or filters) is used for two model editing tasks: structured pruning and class-wise unlearning.  For structured pruning, the goal is to identify non-discriminative components and remove them, reducing model size with minimal accuracy loss. For class-wise unlearning, the goal is to identify components responsible for a specific class and remove them, effectively \"forgetting\" that class.  The diagram highlights the use of lower bounds to identify these components, which is a core contribution of the paper.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_20_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework for model editing.  It shows how the algorithm identifies discriminative components (neurons, filters, etc.) within a neural network model that are responsible for making predictions for specific classes or the overall model accuracy. This information is then used for two model editing tasks:\n\n1. **Classwise Unlearning:** Components most important to a particular class are identified and pruned to selectively forget that class, effectively reducing the model's accuracy for that class while maintaining accuracy for others.\n2. **Structured Pruning:** Components important to *no* class (non-discriminative components) are identified and removed, improving model efficiency by reducing size and computation cost with minimal impact on overall accuracy.\nThe diagram highlights the steps involved: identifying discriminative components, using those components for classwise unlearning or structured pruning and finally showing the pruned/modified model.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_22_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework. It shows how discriminative components are identified using lower bounds on the TV distance, which are then used for either classwise unlearning or structured pruning.  For classwise unlearning (unlearning a specific class), the components that are most discriminative for that class are pruned or masked.  For structured pruning (improving efficiency overall), components that are not discriminative for any classes are removed. The goal of both is to edit the model while minimizing accuracy loss on the remaining classes.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_22_2.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework for both model unlearning and structured pruning.  It shows how identifying discriminative components (those crucial for class-wise predictions) allows for targeted editing of the model. In model unlearning, components discriminative for a specific class are pruned to make the model \"forget\" that class. In structured pruning, nondiscriminative components (those not contributing significantly to any class prediction) are identified and pruned to reduce model size without substantial loss of accuracy. The process involves leveraging lower bounds on the Total Variation (TV) distance to identify these components.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_23_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework's approach to model editing.  It highlights the process of identifying discriminative components (e.g., neurons, filters) within a neural network that are crucial for class-wise predictions. These components are then either pruned or masked for structured pruning, where the aim is to reduce model size with minimal accuracy loss across all classes, or for classwise unlearning, where the goal is to selectively remove components responsible for the predictions of a specific class.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_26_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework.  It shows how the method identifies discriminative components (e.g., filters in a convolutional neural network) that contribute significantly to class-wise predictions. These components are then used for two main model editing tasks:\n\n1. **Classwise Unlearning:**  Components highly discriminative for a specific class are pruned or masked, effectively making the model 'forget' that class. \n2. **Structured Pruning:** Components that are *not* discriminative for *any* class are removed to achieve model sparsity and efficiency. The process of identifying these discriminative components involves analyzing the distributional properties of feature maps generated by each component.  Lower bounds on the Total Variation (TV) distance between class-conditional feature distributions are used to quantify the discriminative ability of each component.", "section": "1 Introduction"}, {"figure_path": "tuiqq1G8I5/figures/figures_27_1.jpg", "caption": "Figure 1: Identifying Discriminative Components for Model Unlearning and Structured Pruning", "description": "This figure illustrates the DISCEDIT framework, which identifies discriminative components in neural networks for model editing tasks such as structured pruning and class-wise unlearning.  It shows how identifying these components through lower bounds on the total variation distance allows for the selective pruning or masking of components to achieve the desired effects of either reduced model size (structured pruning) or selective forgetting of specific class information (class-wise unlearning).", "section": "1 Introduction"}]