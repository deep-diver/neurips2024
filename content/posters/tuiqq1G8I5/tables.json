[{"figure_path": "tuiqq1G8I5/tables/tables_8_1.jpg", "caption": "Table 1: A summary of results of class unlearning using DISCEDIT-U. We take the average over the Forget and Remain accuracies (FA and RA) after applying DISCEDIT-U to each class. Note that FA=accuracy drop on forget class, RA=accuracy drop on remain set. Values are averaged over 10 trials. NoFT refers to using DISCEDIT-U without fine-tuning, and with pruning only 5.4% of weights for VGG16, 1.8% of weights for ResNet56, 1.8% of weights for ResNet20, whereas FT refers to using DISCEDIT-U with 1 epoch of fine-tuning, and with pruning ratios of 18.4%, 22%, 16.6%, and 10.2% for VGG16, ResNet56, ResNet20, and our ViT respectively.", "description": "This table summarizes the results of class unlearning experiments using the DISCEDIT-U algorithm.  It shows the accuracy drop (FA) on the target class to be forgotten and the accuracy drop (RA) on the remaining classes.  The results are presented both with and without fine-tuning (NoFT and FT, respectively) after applying the algorithm to different models (VGG16, ResNet56, ResNet20, ViT) trained on CIFAR-10 and CIFAR-100 datasets.  Baselines from the literature are also included for comparison.", "section": "6 Empirical Evaluations"}, {"figure_path": "tuiqq1G8I5/tables/tables_9_1.jpg", "caption": "Table 2: DISCEDIT-SP performance on CIFAR10 and ImageNet models with fine-tuning. TVSPrune refers to [39], and CHIP refers to [52]. 'Sparsity' refers to parametric sparsity.", "description": "This table presents the results of structured pruning experiments using DISCEDIT-SP on CIFAR10 and ImageNet datasets.  It compares the accuracy drop after pruning (with and without fine-tuning) using DISCEDIT-SP against three baselines: TVSPrune [39], CHIP [52], and L1 pruning. The table shows the sparsity achieved (percentage of parameters removed) and the resulting accuracy drop for different models and sparsity levels. Positive accuracy changes indicate improved accuracy after pruning.", "section": "6.2 Structured Pruning with DISCEDIT-SP with Fine-Tuning"}, {"figure_path": "tuiqq1G8I5/tables/tables_19_1.jpg", "caption": "Table 3: Comparison of complexities of computing different witness functions. (P) refers to pruning (used in DISCEDIT-SP) and (U) refers to unlearning (used in DISCEDIT-U).", "description": "This table compares the computational cost and storage requirements for different witness functions used in the DISCEDIT-SP and DISCEDIT-U algorithms.  It breaks down the costs for pruning (P) and unlearning (U) tasks, considering the number of filters (L), classes (C), and feature dimensions (n). The table shows that the computational cost and storage increase with the complexity of the witness function.", "section": "Additional Results: Computational Complexity of rij scores with Different Witness functions"}, {"figure_path": "tuiqq1G8I5/tables/tables_19_2.jpg", "caption": "Table 1: A summary of results of class unlearning using DISCEDIT-U. We take the average over the Forget and Remain accuracies (FA and RA) after applying DISCEDIT-U to each class. Note that FA=accuracy drop on forget class, RA=accuracy drop on remain set. Values are averaged over 10 trials. NoFT refers to using DISCEDIT-U without fine-tuning, and with pruning only 5.4% of weights for VGG16, 1.8% of weights for ResNet56, 1.8% of weights for ResNet20, whereas FT refers to using DISCEDIT-U with 1 epoch of fine-tuning, and with pruning ratios of 18.4%, 22%, 16.6%, and 10.2% for VGG16, ResNet56, ResNet20, and our ViT respectively.", "description": "This table summarizes the results of class unlearning experiments using the DISCEDIT-U algorithm.  It compares the accuracy drop on the forgotten class (FA) and the accuracy drop on the remaining classes (RA) under different conditions.  The conditions include using DISCEDIT-U without fine-tuning (NoFT) and with one epoch of fine-tuning (FT), along with different pruning ratios for each model architecture (VGG16, ResNet56, ResNet20, ViT).  The results are averages over ten trials.", "section": "Empirical Evaluations"}, {"figure_path": "tuiqq1G8I5/tables/tables_24_1.jpg", "caption": "Table 4: Ablations for models trained on CIFAR10. We consider 10 different models, and average the accuracy after pruning/fine-tuning over them.", "description": "This table presents ablation studies on 10 different instances of models trained on CIFAR10 to demonstrate the robustness of DISCEDIT-SP across various model architectures.  The results show the sparsity achieved, the best and worst accuracy drops observed across all models, and the mean accuracy drop with standard deviation across the models. The table indicates that DISCEDIT-SP consistently performs well across different model types after pruning and fine-tuning.", "section": "E.4.1 CIFAR10 Experiments"}, {"figure_path": "tuiqq1G8I5/tables/tables_24_2.jpg", "caption": "Table 5: Ablations for models trained on CIFAR100. We consider 10 different models, and average the accuracy of our pruning algorithm over them.", "description": "This table shows the results of ablation experiments conducted on CIFAR100.  The goal was to evaluate the robustness of DISCEDIT-SP across multiple model instantiations. Ten different models were used. The table reports the sparsity achieved, the best and worst accuracy drops observed across the different runs and the mean accuracy drop and its standard deviation.", "section": "E.4.2 CIFAR100 Experiments"}, {"figure_path": "tuiqq1G8I5/tables/tables_24_3.jpg", "caption": "Table 6: Ablations for models trained on Imagenet. We consider 3 different models, and average the accuracy drop.", "description": "This table presents the results of ablation studies performed on ResNet50 models trained on the ImageNet dataset.  Three different model instantiations were used, and the table shows the average, best, and worst accuracy drops after applying the DISCEDIT-SP algorithm.  This demonstrates the robustness of the algorithm across different model initializations.", "section": "E.4.3 Imagenet Experiments"}, {"figure_path": "tuiqq1G8I5/tables/tables_25_1.jpg", "caption": "Table 7: Pruning results with DISCEDIT-SP without fine-tuning.", "description": "This table presents the accuracy drop after applying structured pruning using DISCEDIT-SP without fine-tuning. It compares the accuracy drop of DISCEDIT-SP with three other methods: TVSPrune [39], CHIP [52], and L1-based pruning, across different models (VGG16, VGG19, ResNet50) and datasets (CIFAR10, ImageNet) with varying sparsity levels.  The results demonstrate the effectiveness of DISCEDIT-SP in achieving high sparsity while maintaining accuracy.", "section": "E.4.4 Pruning without Fine-tuning"}, {"figure_path": "tuiqq1G8I5/tables/tables_25_2.jpg", "caption": "Table 8: Pruning results with DISCEDIT-SP on CIFAR10 models in the high-sparsity regime", "description": "This table presents the results of applying DISCEDIT-SP for structured pruning on CIFAR10 models with high sparsity (over 80% of parameters pruned). It compares the accuracy drop (with and without fine-tuning) of DISCEDIT-SP against other methods such as CHIP and TVSPrune.  The results demonstrate that even with extremely high sparsity, DISCEDIT-SP maintains competitive accuracy compared to the other algorithms.", "section": "E.5 Pruning in the High Sparsity Regime"}, {"figure_path": "tuiqq1G8I5/tables/tables_27_1.jpg", "caption": "Table 1: A summary of results of class unlearning using DISCEDIT-U. We take the average over the Forget and Remain accuracies (FA and RA) after applying DISCEDIT-U to each class. Note that FA=accuracy drop on forget class, RA=accuracy drop on remain set. Values are averaged over 10 trials. NoFT refers to using DISCEDIT-U without fine-tuning, and with pruning only 5.4% of weights for VGG16, 1.8% of weights for ResNet56, 1.8% of weights for ResNet20, whereas FT refers to using DISCEDIT-U with 1 epoch of fine-tuning, and with pruning ratios of 18.4%, 22%, 16.6%, and 10.2% for VGG16, ResNet56, ResNet20, and our ViT respectively.", "description": "This table summarizes the results of class unlearning experiments using the DISCEDIT-U algorithm.  It compares the accuracy drop on the forgotten class (FA) and the remaining classes (RA) under different conditions: no fine-tuning (NoFT) and with fine-tuning (FT). Baselines from prior work [23] are also included for comparison. The table highlights that DISCEDIT-U achieves significant accuracy reduction on the target class while maintaining high accuracy on the rest, even without fine-tuning.  Different model architectures (VGG16, ResNet56, ResNet20, ViT) and datasets (CIFAR-10 and CIFAR-100) are used to demonstrate the robustness of the method.", "section": "6 Empirical Evaluations"}, {"figure_path": "tuiqq1G8I5/tables/tables_27_2.jpg", "caption": "Table 10: Test accuracy for CIFAR datasets.", "description": "This table shows the test accuracy achieved by a custom Vision Transformer (ViT) model on the CIFAR10 and CIFAR100 datasets.  The ViT model is a custom model trained by the authors and used in several experiments in the paper. The table provides baseline accuracies against which the results of various model editing experiments can be compared.", "section": "6 Empirical Evaluations"}, {"figure_path": "tuiqq1G8I5/tables/tables_28_1.jpg", "caption": "Table 11: Test accuracy for different datasets and models.", "description": "This table presents the test accuracy achieved by different models (VGG16, ResNet56, ResNet20, ViT) on three different datasets (CIFAR10, CIFAR100, and ImageNet).  The results show the baseline performance of the models used in the subsequent experiments for structured pruning and class unlearning.", "section": "Empirical Evaluations"}, {"figure_path": "tuiqq1G8I5/tables/tables_28_2.jpg", "caption": "Table 1: A summary of results of class unlearning using DISCEDIT-U. We take the average over the Forget and Remain accuracies (FA and RA) after applying DISCEDIT-U to each class. Note that FA=accuracy drop on forget class, RA=accuracy drop on remain set. Values are averaged over 10 trials. NoFT refers to using DISCEDIT-U without fine-tuning, and with pruning only 5.4% of weights for VGG16, 1.8% of weights for ResNet56, 1.8% of weights for ResNet20, whereas FT refers to using DISCEDIT-U with 1 epoch of fine-tuning, and with pruning ratios of 18.4%, 22%, 16.6%, and 10.2% for VGG16, ResNet56, ResNet20, and our ViT respectively.", "description": "This table presents the results of class unlearning experiments using DISCEDIT-U.  It shows the average accuracy drop (FA) on the forgotten class and the average accuracy drop (RA) on the remaining classes. Results are given for models with and without fine-tuning, comparing the performance of DISCEDIT-U against existing methods (RA (IU) [23]).  The table also shows the percentage of weights pruned in each model for both fine-tuned and non-fine-tuned scenarios.", "section": "6 Empirical Evaluations"}]