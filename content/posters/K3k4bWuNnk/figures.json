[{"figure_path": "K3k4bWuNnk/figures/figures_2_1.jpg", "caption": "Figure 2: Steps of our method. (a) The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. (b) Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize V. (c, d) Attention scores are extracted from this network for each layer, and used to sample, in (e), a sparse directed graph, which becomes the attention graph for the final network (f). This network, with a much larger feature dimension, does not normalize V.", "description": "This figure illustrates the two-stage training process of the Spexphormer model. The first stage trains a small-width network (Attention Score Estimator Network) to estimate pairwise attention scores.  This network uses a combination of graph edges, self-loops, and expander graph edges for its attention mechanism.  The attention scores are then used to create a sparse attention graph for a second, larger network.  This sparse graph is used to train the final, wider network. The figure highlights the differences in the attention mechanism between the two networks, particularly the normalization of V in the first network.", "section": "4 Method"}, {"figure_path": "K3k4bWuNnk/figures/figures_5_1.jpg", "caption": "Figure 2: Steps of our method. (a) The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. (b) Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize V. (c, d) Attention scores are extracted from this network for each layer, and used to sample, in (e), a sparse directed graph, which becomes the attention graph for the final network (f). This network, with a much larger feature dimension, does not normalize V.", "description": "This figure illustrates the two-phase training process of the Spexphormer model.  The first phase uses a low-width network with a specific attention mechanism (combining graph edges, expander graphs, and self-loops) to estimate pairwise attention scores. These scores are then used in a second phase to create a sparse attention graph for training a higher-width network. The figure visually depicts each step of the process, showing the attention mechanisms, the sparsification process, and the final sparse graph used for training.", "section": "4 Method"}, {"figure_path": "K3k4bWuNnk/figures/figures_7_1.jpg", "caption": "Figure 3: Energy distance between the attention scores of various networks to a network of width 64. \"Uniform\" refers to the baseline placing uniform scores on each neighbor, while \"random\" refers to the baseline with uniformly distributed logits. The remaining bars refer to networks trained on the appropriately labeled width.", "description": "This figure presents the results of an experiment to evaluate how well smaller networks can estimate attention scores of larger networks. The energy distance, a measure of the dissimilarity between two probability distributions, was calculated between the attention score distributions of networks with different widths (4, 8, 16, 32, 64) and a reference network with width 64. Two baseline distributions, uniform and random, were also compared.  The results, shown separately for Actor and Photo datasets with and without expander graphs, indicate that smaller networks yield reasonably accurate estimates of attention scores.", "section": "Experimental Results"}, {"figure_path": "K3k4bWuNnk/figures/figures_8_1.jpg", "caption": "Figure 2: Steps of our method. (a) The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. (b) Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize V. (c, d) Attention scores are extracted from this network for each layer, and used to sample, in (e), a sparse directed graph, which becomes the attention graph for the final network (f). This network, with a much larger feature dimension, does not normalize V.", "description": "This figure illustrates the two-phase training process of the Spexphormer model.  The first phase uses a small-width network (Attention Score Estimator Network) with a sparse attention mechanism to estimate pairwise attention score patterns.  This network combines graph edges, self-loops, and expander graph edges to learn which neighbor nodes are most informative for each node. The learned attention scores are then used to sparsify the graph for the second phase, creating a sparse graph used to train a larger network (High-width Network). The final network, trained on this sparser graph, achieves better efficiency than the original Exphormer network.", "section": "4 Method"}, {"figure_path": "K3k4bWuNnk/figures/figures_15_1.jpg", "caption": "Figure 5: The memory, run-time trade-off for ogbn-proteins and ogbn-arxiv datasets. It is worth mentioning that the experiments with different batch sizes yield similar results for test accuracy/AUC. Memory and time can be traded in our approach.", "description": "This figure shows the relationship between memory usage, time per epoch, and batch size for training the model on two large graph datasets: ogbn-proteins and ogbn-arxiv.  The plots demonstrate a trade-off: increasing the batch size reduces the time per epoch but increases memory consumption. Importantly, the test accuracy/AUC remains relatively consistent across different batch sizes, highlighting the flexibility of the proposed approach to balance computational resources and performance.", "section": "C.1 Time-Memory Trade-off"}]