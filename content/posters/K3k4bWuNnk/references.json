{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to the work in this paper."}, {"fullname_first_author": "Zaheer, M.", "paper_title": "Big bird: Transformers for longer sequences", "publication_date": "2020-12-01", "reason": "This paper is cited as an example of sparsifying the attention mechanism in sequence models, which is relevant to the paper's focus on sparse graph transformers."}, {"fullname_first_author": "Shirzad, H.", "paper_title": "Exphormer: Sparse transformers for graphs", "publication_date": "2023-07-01", "reason": "This is the authors' previous work, which directly builds upon and is improved by the current paper."}, {"fullname_first_author": "Wu, Q.", "paper_title": "Nodeformer: A scalable graph structure learning transformer for node classification", "publication_date": "2022-12-01", "reason": "This paper is cited for its conceptual similarity in addressing scalability issues in graph Transformers."}, {"fullname_first_author": "Kreuzer, D.", "paper_title": "Rethinking graph transformers with spectral attention", "publication_date": "2021-06-01", "reason": "This paper is cited as an example of work that has approached the problem of scalable graph Transformers using different methods."}]}