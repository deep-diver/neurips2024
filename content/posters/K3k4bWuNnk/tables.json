[{"figure_path": "K3k4bWuNnk/tables/tables_9_1.jpg", "caption": "Table 3: Ablation studies on two homophilic and two heterophilic datasets. Metrics: accuracy for Photo and Computer, ROC-AUC (\u00d7100) for Tolokers and Minesweeper. For the initial network, we report the result for the network used for training the Spexphormer and thus, there is no confidence interval for them.", "description": "This table presents the ablation study results for Spexphormer on four datasets: two homophilic (Photo and Computer) and two heterophilic (Minesweeper and Tolokers). The metrics used are accuracy for the homophilic datasets and ROC-AUC for the heterophilic datasets. The table compares the performance of the full Spexphormer model against several variants: Spexphormer-uniform (using uniform sampling instead of attention-based sampling), Spexphormer-max (selecting only the top-k edges instead of sampling), Spexphormer without temperature annealing, and Spexphormer without layer normalization. The \"Initial Network\" row shows the performance of the small, initial network used for attention score estimation. The results demonstrate the impact of different design choices in the Spexphormer architecture.", "section": "Ablation Studies"}, {"figure_path": "K3k4bWuNnk/tables/tables_13_1.jpg", "caption": "Table 1: Comparison of our model with other GNNs on five homophilic and three heterophilic datasets. The reported metric is ROC-AUC (\u00d7100) for the Minesweeper and Tolokers datasets and accuracy for all others. The average edge ratio is the degree of the nodes in Spexphormer over the average degree of the nodes in the initial attention pattern.", "description": "This table compares the performance of Spexphormer against other Graph Neural Networks (GNNs) on eight datasets.  Five datasets exhibit homophily (similar nodes tend to cluster together), while three datasets show heterophily (nodes with dissimilar features tend to be close).  The metrics used are ROC-AUC (for Minesweeper and Tolokers) and accuracy (for the others).  The table also shows the average edge ratio, which represents the sparsity of the Spexphormer's attention mechanism relative to the original graph.", "section": "5 Experimental Results"}, {"figure_path": "K3k4bWuNnk/tables/tables_14_1.jpg", "caption": "Table 5: Dataset statistics. The reported number of edges is the number of directed edges, which will be twice the number of actual edges for the undirected graphs.", "description": "This table presents the statistics of eleven graph datasets used in the paper's experiments. For each dataset, it provides the number of nodes, number of edges, average node degree, number of node features, number of classes, and the evaluation metric used (Accuracy or AUC).  The table helps to understand the scale and characteristics of the datasets, which are crucial for interpreting the experimental results. The note clarifies that edge counts represent directed edges, which are double the number of undirected edges for undirected graphs.", "section": "B Dataset Descriptions"}, {"figure_path": "K3k4bWuNnk/tables/tables_15_1.jpg", "caption": "Table 6: Hyperparameters used for training the networks for homophilous datasets.", "description": "This table shows the hyperparameters used to train both the Attention Score Estimator Network and the final Spexphormer Network on five homophilic datasets: ogbn-arxiv, Computer, Photo, CS, and Physics.  For each dataset, it specifies the number of layers (L), the width of the small network (ds), the number of training epochs, the learning rate, the width of the larger network (d<sub>l</sub>), the number of sampled edges per layer (deg<sub>l</sub>), the number of attention heads, the learning rate for the larger network, the number of training epochs for the larger network, and the dropout rate. The hyperparameters are tuned for each dataset individually to achieve optimal performance.", "section": "C.2 Hyperparameters"}, {"figure_path": "K3k4bWuNnk/tables/tables_15_2.jpg", "caption": "Table 7: Hyperparameters used for training the networks for heterophilic datasets.", "description": "This table shows the hyperparameters used for training both the attention score estimator network and the final Spexphormer network on three heterophilic datasets: Actor, Minesweeper, and Tolokers.  It lists the number of layers (L), the width of the estimator network (ds), the number of training epochs, the learning rate, the width of the final network (d<sub>l</sub>), the number of sampled neighbors per layer (deg<sub>l</sub>), the number of attention heads, the learning rate of the final network, the number of training epochs for the final network, and the dropout rate.  These hyperparameters were tuned separately for each dataset to optimize performance.", "section": "5 Experimental Results"}, {"figure_path": "K3k4bWuNnk/tables/tables_16_1.jpg", "caption": "Table 8: Hyperparameters used for training the networks for the large graphs datasets.", "description": "This table shows the hyperparameters used for training the attention score estimator network and the final Spexphormer network for three large graph datasets: ogbn-proteins, Amazon2M, and Pokec.  The hyperparameters include the number of layers (L), the width of the smaller network (ds), the expander degree, the number of epochs, the learning rate, the width of the larger network (di), the number of edges per layer (dege), the number of heads, the learning rate, the number of epochs, the dropout rate, the batch size, and the GPU memory used.", "section": "5 Experimental Results"}]