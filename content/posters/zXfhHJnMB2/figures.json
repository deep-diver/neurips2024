[{"figure_path": "zXfhHJnMB2/figures/figures_8_1.jpg", "caption": "Figure 1: Conditional mean (top only) and 90% confidence interval for NCP, NFs and CCP. Top: Laplace distribution; Bottom: Cauchy distribution.", "description": "This figure compares the performance of three methods \u2013 NCP, Normalizing Flows (NF), and Conditional Conformal Prediction (CCP) \u2013 in estimating the conditional mean and 90% confidence intervals for two different distributions: Laplace (top) and Cauchy (bottom).  For each distribution, the figure shows the true conditional mean (orange dashed line), the estimated conditional mean from each method (solid lines), the true 90% confidence interval (blue dashed lines), and the 90% confidence interval estimated by each method (blue solid lines). The plots demonstrate the effectiveness of NCP in producing accurate estimates and reliable confidence intervals, particularly when compared to NF and CCP which show instability and less accurate estimation in certain areas..", "section": "Experiments"}, {"figure_path": "zXfhHJnMB2/figures/figures_8_2.jpg", "caption": "Figure 2: Protein folding dynamics. Pairwise Euclidean distances between Chignolin atoms exhibit increased variance during folded metastable states (between 87-88\u00b5s and around 89.5\u00b5s). Ground truth is depicted in blue, predicted mean in orange, and the grey lines indicate the estimated 10% lower and upper quantiles.", "description": "This figure shows the results of applying the NCP method to predict the pairwise Euclidean distances between atoms in a protein (Chignolin) during a folding simulation.  The x-axis represents time (in microseconds), and the y-axis represents the pairwise distance. The blue dots show the actual data points from the simulation.  The orange line shows the conditional expectation of the distance predicted by the NCP model at each time point, and the grey lines indicate the 10% lower and upper quantiles. The figure highlights that the NCP model successfully predicts not only the mean distance but also its uncertainty.", "section": "Experiments"}, {"figure_path": "zXfhHJnMB2/figures/figures_9_1.jpg", "caption": "Figure 3: High-dimensional synthetic experiment. We consider two models for Y|X with d = 100. Left: Y|X ~ N(\u03b8(X), sin(\u03b8(X)/2). Right: Y \u2208 {1, 2, 3, 4, 5} admits discrete distribution depending on \u03b8(X): Y|X ~ P\u2081 if \u03b8(X) \u2208 [0, \u03c0/2), P\u2082 if \u03b8(X) \u2208 [\u03c0/2, \u03c0), P\u2083 if \u03b8(X) \u2208 [\u03c0, 3\u03c0/2), P\u2084 if \u03b8(X) \u2208 [3\u03c0/2, 2\u03c0). We take P\u2081 = (1/5, 1/5, 1/5, 1/5, 1/5), P\u2082 = (1/2, 1/2, 0, 0, 0), P\u2083 = (0, 0, 1, 0, 0), P\u2084 = (0, 0, 0, 1/2, 1/2).", "description": "This figure shows the results of a high-dimensional synthetic experiment to evaluate the performance of the NCP model in estimating conditional distributions. Two models for Y|X were considered with dimension d=100: a Gaussian model and a discrete model. The Gaussian model's parameters depend on the angle \u03b8(X), and its distribution is normal with mean \u03b8(X) and standard deviation sin(\u03b8(X)/2).  The discrete model's distribution depends on the range of \u03b8(X), assigning different probability distributions (P\u2081, P\u2082, P\u2083, P\u2084) to different ranges of \u03b8(X). The results shown in the figure's plots illustrates the performance of NCP under these different model conditions.", "section": "Experiments"}, {"figure_path": "zXfhHJnMB2/figures/figures_13_1.jpg", "caption": "Figure 4: Learning dynamic for the Laplace experiment in Section 6. Training Validation", "description": "This figure shows the training and validation loss curves for a Laplace experiment described in Section 6 of the paper.  The x-axis represents the number of epochs (iterations) during training, and the y-axis represents the value of the loss function.  The plot illustrates how the loss decreases over time for both the training and validation datasets, indicating the model's learning progress.  The relatively close proximity of the training and validation loss curves suggests that the model is not overfitting significantly.", "section": "A.2 Learning dynamics with NCP"}, {"figure_path": "zXfhHJnMB2/figures/figures_28_1.jpg", "caption": "Figure 5: Performances for CDE on synthetic datasets w.r.t sample size n. Performance metric is Kolmogorov-Smirnov (KS) distance to truth.", "description": "This figure displays the performance of different conditional density estimation (CDE) methods across six synthetic datasets with varying sample sizes (n).  The x-axis represents the sample size (n), and the y-axis represents the Kolmogorov-Smirnov (KS) distance, which measures the difference between the estimated and true cumulative distribution functions (CDFs).  Lower KS distance indicates better performance. The figure shows that the performance of most methods improves with increasing sample size.  The NCP methods (NCP, NCP-C, and NCP-W) generally show competitive or better performance compared to other methods.", "section": "Experiments"}, {"figure_path": "zXfhHJnMB2/figures/figures_29_1.jpg", "caption": "Figure 6: Estimated conditional PDFs (left) and CDFs (right) for each synthetic dataset for 3 different conditioning points. Dotted lines represent the true distributions, while solid lines represent the estimates from NCP. The average KS distance over 5 repetitions is also reported on the right plots.", "description": "This figure compares the estimated and true probability density functions (PDFs) and cumulative distribution functions (CDFs) for six synthetic datasets.  Three different conditioning points are shown for each dataset. The left column displays the PDFs, and the right column displays the CDFs.  Dotted lines represent the true distributions, and solid lines show the estimates generated by the Neural Conditional Probability (NCP) method. The Kolmogorov-Smirnov (KS) statistic, a measure of the distance between the estimated and true CDFs, is provided for each conditioning point, quantifying the accuracy of the NCP estimations.", "section": "Experiments"}, {"figure_path": "zXfhHJnMB2/figures/figures_31_1.jpg", "caption": "Figure 7: Left: we observe only \u2248 20% increase in compute time going from d = 102 to d = 103. Right: average KS distance to the truth and standard deviation over 10 repetitions.", "description": "This figure shows the scalability of NCP with respect to increasing dimensionality. The left panel shows that the computation time only increases by about 20% when the dimension increases from 100 to 1000. The right panel shows the average Kolmogorov-Smirnov (KS) distance between the estimated and true conditional CDFs for different dimensions, demonstrating that NCP maintains strong statistical performance even in high-dimensional settings.", "section": "High-dimensional synthetic experiment"}]