[{"figure_path": "Z4R2rkPgBy/figures/figures_1_1.jpg", "caption": "Figure 1: Independent VAEs (Figure 1a) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 1b) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 1c) enhances independent VAEs with a data-dependent prior, h(z | X), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.", "description": "This figure compares three different approaches for handling multiple modalities in a variational autoencoder (VAE).  (a) shows independent VAEs, where each modality is processed separately.  (b) illustrates an aggregated VAE that attempts to combine information from all modalities into a single joint representation, but this can negatively impact reconstruction quality. (c) presents the proposed MMVM VAE, which uses a data-dependent prior to softly guide the individual modality representations towards a shared aggregate posterior, aiming to balance individual information preservation and shared representation learning.", "section": "1 Introduction"}, {"figure_path": "Z4R2rkPgBy/figures/figures_5_1.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure compares the performance of different VAE models on three benchmark datasets (translated PolyMNIST, bimodal CelebA, and CUB) in terms of reconstruction error, classification accuracy, and sample coherence.  The MMVM model consistently outperforms other methods, showing improvements in either classification accuracy or sample coherence while maintaining similar or better reconstruction error.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_6_1.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure compares the performance of different VAE models (independent, AVG, MoE, PoE, MoPoE, MMVAE+, and MMVM) on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results are presented as scatter plots, with the x-axis representing reconstruction error and the y-axis representing either latent representation classification accuracy or coherence of generated samples. Each point represents the average performance over multiple random seeds and a specific beta value.  The ideal model would have both low reconstruction error and high classification accuracy/coherence, indicated by a position in the top right corner of each plot.  The MMVM model consistently shows superior performance compared to the other methods.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_6_2.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure displays the results of three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB, comparing different VAE methods (independent, AVG, MoE, PoE, MoPoE, MMVAE+, and MMVM).  The results are plotted as average precision against reconstruction error. Higher average precision and lower reconstruction error indicate better performance. The MMVM method shows superior performance compared to others, showing higher average precision or lower reconstruction error in both latent representation and coherence of samples.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_7_1.jpg", "caption": "Figure 4: Latent neural representation during a memory experiment. Each model's performance is evaluated based on its own optimal \u03b2 value (0.00001, 0.01, 0.00001, 0.001 for independent, AVG, MoPoE, and MMVM respectively) in terms of the unimodal latent representation classification accuracy according to Figure 3a. Our method can distinguish the odor stimuli in the latent space with a clear separation of odors similar to MoPoE VAE (4 different colors). Conversely, unimodal and AVG models failed to combine multi-views as the odor separation only occurred within single views.", "description": "This figure compares the latent neural representations learned by four different VAE models (independent, AVG, MoPoE, and MMVM) during a memory experiment involving odor stimuli.  The models' performance is assessed based on their ability to classify odor stimuli within the learned latent space, using a classification accuracy metric as described in Figure 3a. The MMVM model achieves a clear separation of the four odors in the latent space, demonstrating its effectiveness at capturing and distinguishing odor information. In contrast, both the independent and AVG models fail to integrate information from multiple views (rats), resulting in odor separation confined to individual views. The MoPoE model demonstrates a similar performance to MMVM in terms of odor separation.", "section": "5.2 Hippocampal Neural Activities"}, {"figure_path": "Z4R2rkPgBy/figures/figures_15_1.jpg", "caption": "Figure 5: We compare the achieved values of the proposed objective \\(\\mathcal{E}\\) to the vanilla Autoencoder's negative mean squared error (MSE). Lowering the \\(\\beta\\) value of the regularizer \\(R\\) in the objective (see Section 4) approximates the negative MSE bound provided by the vanilla AE. This proves empirically that the negative MSE of the vanilla AE indeed upper bounds the proposed objective \\(\\mathcal{E}\\).", "description": "The figure shows a plot of the objective function \\(\\mathcal{E}\\) against the logarithm of the hyperparameter \\(\\beta\\).  It demonstrates that the negative mean squared error (MSE) of a standard autoencoder provides an upper bound for the proposed MMVM VAE objective function.  As \\(\\beta\\) decreases, the objective function approaches the MSE bound, suggesting a connection between the two and validating the theoretical findings of the paper.", "section": "A MMVM VAE"}, {"figure_path": "Z4R2rkPgBy/figures/figures_17_1.jpg", "caption": "Figure 6: PolyMNIST (translated, scale=75%): every column is a multimodal tuple X, and every row shows samples of a single modality x<sub>m</sub>. We see the random translation between samples by looking at images from a single row or column.", "description": "This figure shows examples from the translated PolyMNIST dataset. Each column represents a multimodal data point with multiple modalities (images of the same digit with different backgrounds). Each row displays samples from a single modality, illustrating the random translation of the digit within each image.", "section": "B.4 PolyMNIST"}, {"figure_path": "Z4R2rkPgBy/figures/figures_19_1.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure presents the results of experiments conducted on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results compare various VAE methods, including the proposed MMVM VAE, across two key metrics: latent representation quality and coherence of generated samples.  The plots show a trade-off between reconstruction error and classification accuracy.  The MMVM VAE consistently demonstrates superior performance, achieving higher classification accuracy or lower reconstruction error than existing methods in most scenarios.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_19_2.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure shows the results of three benchmark datasets (translated PolyMNIST, bimodal CelebA, and CUB) comparing different VAE methods (independent, AVG, MoE, PoE, MoPoE, MMVAE+, and MMVM) based on their latent representation classification accuracy and sample coherence against reconstruction error.  The MMVM method shows improvements compared to other methods, achieving either higher classification/coherence with similar reconstruction error or similar classification/coherence with lower reconstruction error.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_19_3.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure presents the results of experiments on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results are presented as scatter plots, showing the trade-off between reconstruction error and either classification accuracy (latent representation) or sample coherence.  Each point represents the average of multiple runs with different random seeds and a specific \u03b2 value.  The MMVM method generally achieves superior performance compared to alternative methods.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_19_4.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure displays the results of experiments on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results are shown in terms of reconstruction error (lower is better) and classification accuracy or sample coherence (higher is better). The MMVM method consistently outperforms other methods, demonstrating its ability to improve latent representation learning in multimodal VAEs.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_20_1.jpg", "caption": "Figure 9: Qualitative results for the conditional generation task for the set of unimodal VAEs.", "description": "This figure shows the qualitative results of the conditional generation task performed on the unimodal VAEs.  The results visualize the conditional generation of one modality given another modality. Each subfigure represents the conditional generation of a specific modality (m0, m1, m2) given another specific modality (m0, m1, m2).  The images are arranged in a grid to compare the generated samples across different modalities.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_20_2.jpg", "caption": "Figure 1: Independent VAEs (Figure 1a) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 1b) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 1c) enhances independent VAEs with a data-dependent prior, h(z | X), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.", "description": "This figure compares three different approaches to handling multiple modalities in a Variational Autoencoder (VAE).  (a) shows independent VAEs, where each modality is processed separately, lacking information sharing. (b) shows an aggregated VAE, attempting to combine modalities into a single shared representation, but potentially losing detail or leading to poor reconstruction. (c) presents the proposed MMVM VAE, which uses a data-dependent prior to allow for soft information sharing between modalities, aiming to retain individual modality detail while improving the shared representation.", "section": "1 Introduction"}, {"figure_path": "Z4R2rkPgBy/figures/figures_21_1.jpg", "caption": "Figure 9: Qualitative results for the conditional generation task for the set of unimodal VAEs.", "description": "This figure shows the qualitative results of applying unimodal VAEs to the conditional generation task.  The results are presented in a grid format, where each cell shows a generated sample. The rows represent the input modality (m0), while the columns represent the generated modality (m0, m1, m2). This visualization helps in assessing the quality and coherence of the generated samples in each modality. Each modality is treated independently, therefore showing the limitations of this approach.", "section": "Experiments"}, {"figure_path": "Z4R2rkPgBy/figures/figures_21_2.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure compares the performance of different VAE models on three benchmark datasets (translated PolyMNIST, bimodal CelebA, and CUB).  The results are presented as scatter plots, where the x-axis represents the reconstruction error and the y-axis represents either classification accuracy (latent representation) or sample coherence.  The MMVM model generally outperforms other methods, achieving better classification performance or lower reconstruction error for a given level of coherence or accuracy. The plot indicates that the ideal VAE model would have both low reconstruction error and high classification performance.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_21_3.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure displays the performance of various models (Independent, AVG, MoE, PoE, MoPoE, MMVAE+, and MMVM) on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results are shown in terms of reconstruction error and classification accuracy (for latent representation learning) and coherence (for generated sample quality). The ideal model would minimize reconstruction error and maximize classification accuracy/coherence.  The MMVM model consistently achieves superior results in at least one of these metrics across all three datasets. The multiple points for each method represent results for different hyperparameter (\u03b2) values. ", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_21_4.jpg", "caption": "Figure 1: Independent VAEs (Figure 1a) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 1b) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 1c) enhances independent VAEs with a data-dependent prior, h(z | X), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.", "description": "This figure compares three different approaches for handling multiple modalities in a Variational Autoencoder (VAE): independent VAEs, aggregated VAEs, and the proposed MMVM VAE.  Independent VAEs process each modality separately, lacking information sharing. Aggregated VAEs combine modality information into a joint representation, potentially sacrificing individual modality detail.  The MMVM VAE offers a compromise, softly integrating information across modalities via a data-dependent prior while preserving individual modality reconstructions.", "section": "1 Introduction"}, {"figure_path": "Z4R2rkPgBy/figures/figures_23_1.jpg", "caption": "Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 2a to 2c) or coherence of generated samples (Coh, Figures 2d to 2f), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \u03b2 value (see Section 5.1).", "description": "This figure presents the results of the MMVM VAE and five comparative methods on three benchmark datasets: translated PolyMNIST, bimodal CelebA, and CUB.  The results are shown using scatter plots, where each point represents the average performance across multiple trials with a specific value of the hyperparameter \u03b2. The x-axis of each plot represents the reconstruction error, while the y-axis displays either the classification accuracy of a learned latent representation or the coherence of generated samples (depending on the subplot).  An ideal model would show a low reconstruction error and high values for both classification accuracy and coherence, residing in the top-right corner of each plot. The figure demonstrates the superior performance of the MMVM VAE in multiple scenarios by achieving either better classification or coherence scores at the same level of reconstruction error, or achieving similar classification and coherence scores with lower reconstruction error compared to the other methods.", "section": "5.1 Benchmark Datasets"}, {"figure_path": "Z4R2rkPgBy/figures/figures_24_1.jpg", "caption": "Figure 1: Independent VAEs (Figure 1a) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 1b) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 1c) enhances independent VAEs with a data-dependent prior, h(z | X), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.", "description": "This figure compares three different approaches to handling multimodal data with VAEs.  (a) shows independent VAEs, where each modality is processed separately.  (b) illustrates an aggregated VAE, combining modality information into a single joint posterior, but potentially leading to poorer reconstruction quality. (c) presents the proposed MMVM VAE, which uses a data-dependent prior to softly guide each modality's latent representation towards a shared representation, combining the benefits of independent processing with information sharing.", "section": "1 Introduction"}, {"figure_path": "Z4R2rkPgBy/figures/figures_24_2.jpg", "caption": "Figure 1: Independent VAEs (Figure 1a) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 1b) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 1c) enhances independent VAEs with a data-dependent prior, h(z | X), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.", "description": "This figure compares three different approaches to handling multiple modalities in a variational autoencoder (VAE): independent VAEs, aggregated VAEs, and the proposed MMVM VAE.  Independent VAEs process each modality separately, lacking information sharing. Aggregated VAEs combine modality information into a joint posterior, potentially leading to poor reconstruction. The MMVM VAE introduces a soft constraint, allowing for information sharing while maintaining modality-specific reconstructions.", "section": "1 Introduction"}, {"figure_path": "Z4R2rkPgBy/figures/figures_25_1.jpg", "caption": "Figure 17: MIMIC-CXR experiment dataset: every column is a bimodal tuple X, the top row shows samples of the frontal modality xf, and the bottom row shows samples of the lateral modality x1. The first two tuples are linked to No Findings, indicative of healthy conditions. Tuples three and four are labeled with Consolidation disease. The tuple five is labeled with Atelectasis disease. We can observe that tuples three and four share the same frontal image, but they differ due to having distinct lateral images.", "description": "This figure shows examples from the MIMIC-CXR dataset used in the paper. Each column represents a single study with paired frontal and lateral X-ray images.  The caption highlights that the first two examples are labeled as \"No Findings\", indicating healthy patients, while subsequent examples illustrate patients with \"Consolidation\" and \"Atelectasis\" conditions. The figure emphasizes that although some frontal views may be the same across multiple studies, the paired lateral views are distinct.", "section": "B.7 MIMIC-CXR"}, {"figure_path": "Z4R2rkPgBy/figures/figures_27_1.jpg", "caption": "Figure 18: Latent representation classification for the MIMIC-CXR dataset. The mean AUROC over all labels and averaged over three seeds is reported.", "description": "The figure shows the performance of latent representation classification for the MIMIC-CXR dataset.  The mean AUROC (area under the receiver operating characteristic curve) is calculated across all labels and averaged over three different seeds.  This provides an aggregate measure of classification performance, taking into account variation across different random initializations of the models.  The x-axis represents the reconstruction error, indicating a trade-off between reconstruction quality and classification accuracy.", "section": "5.3 MIMIC-CXR"}, {"figure_path": "Z4R2rkPgBy/figures/figures_28_1.jpg", "caption": "Figure 4: Latent neural representation during a memory experiment. Each model's performance is evaluated based on its own optimal \u03b2 value (0.00001, 0.01, 0.00001, 0.001 for independent, AVG, MoPoE, and MMVM respectively) in terms of the unimodal latent representation classification accuracy according to Figure 3a. Our method can distinguish the odor stimuli in the latent space with a clear separation of odors similar to MoPoE VAE (4 different colors). Conversely, unimodal and AVG models failed to combine multi-views as the odor separation only occurred within single views.", "description": "The figure displays the latent neural representations learned by four different VAE models (independent, AVG, MoPoE, and MMVM) during a memory experiment involving odor stimuli. The performance of each model is assessed based on its unimodal latent representation classification accuracy.  The MMVM model effectively separates the odor stimuli in the latent space, demonstrating its ability to integrate information from multiple views (rats). In contrast, the independent and AVG models fail to integrate multi-view information, resulting in less distinct odor separation.", "section": "5.2 Hippocampal Neural Activities"}, {"figure_path": "Z4R2rkPgBy/figures/figures_28_2.jpg", "caption": "Figure 4: Latent neural representation during a memory experiment. Each model's performance is evaluated based on its own optimal \u03b2 value (0.00001, 0.01, 0.00001, 0.001 for independent, AVG, MoPoE, and MMVM respectively) in terms of the unimodal latent representation classification accuracy according to Figure 3a. Our method can distinguish the odor stimuli in the latent space with a clear separation of odors similar to MoPoE VAE (4 different colors). Conversely, unimodal and AVG models failed to combine multi-views as the odor separation only occurred within single views.", "description": "This figure shows the 2D latent representations generated by four different VAE models (independent, AVG, MoPoE, and MMVM) for a hippocampal neural activity memory experiment.  Each point represents a 100ms sub-window of neural activity data, colored by odor stimulus. The MMVM model shows clear separation of odors, indicating better representation learning compared to other models, which struggle to separate odors effectively across modalities.", "section": "5.2 Hippocampal Neural Activities"}]