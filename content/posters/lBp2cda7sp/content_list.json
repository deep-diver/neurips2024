[{"type": "text", "text": "RMLR: Extending Multinomial Logistic Regression into General Geometries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziheng Chen1, Yue Song2,\u2217 Rui Wang3, Xiao-Jun $\\mathbf{W}\\mathbf{u}^{3}$ , Nicu Sebe1 1 University of Trento, 2 Caltech, 3 Jiangnan University ziheng_ch@163.com, yuesong@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Riemannian neural networks, which extend deep learning techniques to Riemannian spaces, have gained significant attention in machine learning. To better classify the manifold-valued features, researchers have started extending Euclidean multinomial logistic regression (MLR) into Riemannian manifolds. However, existing approaches suffer from limited applicability due to their strong reliance on specific geometric properties. This paper proposes a framework for designing Riemannian MLR over general geometries, referred to as RMLR. Our framework only requires minimal geometric properties, thus exhibiting broad applicability and enabling its use with a wide range of geometries. Specifically, we showcase our framework on the Symmetric Positive Definite (SPD) manifold and special orthogonal group $\\mathrm{SO}(n)$ , i.e.,the set of rotation matrices in $\\mathbb{R}^{n}$ . On the SPD manifold, we develop five families of SPD MLRs under five types of power-deformed metrics. On $\\mathrm{SO}(n)$ , we propose Lie MLR based on the popular bi-invariant metric. Extensive experiments on different Riemannian backbone networks validate the effectiveness of our framework. The code is available at https://github.com/GitZH-Chen/RMLR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, significant advancements have been achieved in Deep Neural Networks (DNNs), enabling them to effectively analyze complex patterns from various types of data, including images, videos, and speech [29, 38, 27, 66]. However, most existing models have primarily assumed the underlying data with a Euclidean structure. Recently, a growing body of research has emerged, recognizing that the latent spaces of many applications exhibit non-Euclidean geometries, such as Riemannian geometries [9]. Various frequently-encountered manifolds in machine learning have posed interesting challenges and opportunities, including special orthogonal groups $\\mathrm{SO}(n)$ [67, 31], symmetric positive definite (SPD) [30, 10, 42, 73, 18, 19], Gaussian [14, 47], Grassmannian [32, 72] spherical [56], and hyperbolic manifolds [23]. These manifolds share an important Riemannian property \u2014 their Riemannian operators, including geodesics, exponential & logarithmic maps, and parallel transportation, often possess closed-form expressions. Leveraging these Riemannian operators, researchers have successfully generalized different types of DNNs into manifolds, dubbed Riemannian neural networks. ", "page_idx": 0}, {"type": "text", "text": "Although Riemannian networks demonstrated success in many applications, most approaches still rely on Euclidean spaces for classification, such as tangent spaces [30, 31, 10, 47, 69, 71, 48, 49, 37, 70, 15], ambient Euclidean spaces [68, 57, 58], or coordinate systems [12]. However, these strategies distort the intrinsic geometry of the manifold, undermining the effectiveness of Riemannian networks. Researchers have recently started directly developing Riemannian Multinomial Logistic Regression (RMLR) on manifolds. Inspired by the idea of hyperplane margin [39], Ganea et al. [23] developed a hyperbolic MLR in the Poincar\u00e9 ball for Hyperbolic Neural Networks (HNNs). Motivated by HNNs, Nguyen and Yang [50] developed three kinds of gyro SPD MLRs based on three distinct gyro structures of the SPD manifold. In parallel, Chen et al. [16] proposed a framework for building SPD MLRs induced by the flat metrics on the SPD manifold. Nguyen et al. [51] proposed gyro MLRs for the Symmetric Positive Semi-definite (SPSD) manifold based on the product of gyro spaces. However, these classifiers often rely on specific Riemannian properties, limiting their generalizability to other geometries. For instance, the hyperbolic MLR [23] relies on the generalized law of sine, while the gyro MLRs [50, 51] rely on the gyro structures. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper presents a framework of RMLR over general geometries. In contrast to previous works, our framework only requires the explicit expression of the Riemannian logarithm, which is the minimal requirement in extending the Euclidean MLR into manifolds. Since this property is satisfied by many commonly encountered manifolds in machine learning, our framework can be broadly applied to various types of manifolds. Empirically, we showcase our framework on the SPD manifold and rotation matrices. On the SPD manifold, we systematically propose SPD MLRs under five families of power-deformed metrics. We also present a complete theoretical discussion on the geometric properties of these metrics. In the Lie group of $\\mathrm{SO}(\\bar{n})$ , we propose Lie MLR based on the widely used bi-invariant metric to build the Lie MLR. Our work is the first to extend the Euclidean MLR into Lie groups. Besides, our framework incorporates several previous Riemannian MLRs, including gyro SPD MLRs in [50], SPD MLRs in [16], and gyro SPSD MLRs in [51]. ", "page_idx": 1}, {"type": "text", "text": "Our SPD MLRs are validated on four SPD backbone networks, including SPDNet [30] on the radar and human action recognition tasks and TSMNet [37] on the electroencephalography (EEG) classification tasks for the Riemannian feedforward network, RResNet [36] on the human action recognition task for the Riemannian residual network, and SPDGCN [76] on the node classification for the Riemannian graph neural network. Our Lie MLR is validated on the classic LieNet [31] backbone for the human action recognition task. Compared with previous non-intrinsic classifiers, our MLRs achieve consistent performance gains. Especially, our SPD MLRs outperform the previous classifiers by $14.23\\%$ on SPDNet and $13.72\\%$ on RResNet for human action recognition, and $4.46\\%$ on TSMNet for EEG inter-subject classification. Furthermore, our Lie MLR can improve both the training stability and performance. In summary, our main theoretical contributions are the following: (a) We develop a general framework for designing Riemannian MLR over general geometries, incorporating several previous Riemannian MLRs on different geometries. (b) We systematically propose 5 families of SPD MLRs based on different geometries of the SPD manifold. (c) We propose a novel Lie MLR for deep neural networks on $\\mathrm{SO}(\\bar{n})$ . ", "page_idx": 1}, {"type": "text", "text": "Main theoretical results: We solve the Riemannian margin distance to the hyperplane in Thm. 3.2 and present our RMLR framework in Thm. 3.3. As shown in Tab. 1, our RMLR incorporates several existing MLRs on different geometries. Thm. 4.2 showcases our RMLR on the SPD manifold under five families of metrics summarized in Tab. 2. To remedy the numerical instability of BWM geometry on the SPD manifold, we also propose a backpropagation-friendly solver for the SPD MLR under BWM in App. F.2.2. Thm. 5.2 proposes the Lie MLR for the Lie group $\\mathrm{SO}(n)$ . Due to the page limits, we put all the proofs in App. H. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section provides a brief review of the basic geometries of SPD manifolds and special orthogonal groups. Detailed review and notations are left in Apps. B and B.1. ", "page_idx": 1}, {"type": "text", "text": "SPD manifolds: The set of $n\\times n$ symmetric positive definite (SPD) matrices is an open submanifold of the Euclidean space $S^{n}$ of symmetric matrices, referred to as the SPD manifold $S_{++}^{n}$ [3]. There are five kinds of popular Riemannian metrics on $S_{++}^{n}$ : Affine-Invariant Metric (AIM) [52], LogEuclidean Metric (LEM) [3], Power-Euclidean Metrics (PEM) [22], Log-Cholesky Metric (LCM) [41], and Bures-Wasserstein Metric (BWM) [5]. Note that, when power equals 1, the PEM is reduced to the Euclidean Metric (EM). Thanwerdas and Pennec [63] generalized AIM, LEM, and EM into two-parameters families of $\\mathrm{O}(n)$ -invariant metrics, $i.e.,(\\alpha,\\beta)$ -AIM, $(\\alpha,\\beta)$ -LEM, and $(\\alpha,\\beta)$ -EM, with $\\operatorname*{min}(\\alpha,\\alpha+n\\beta)>0.$ . We denote the metric tensor of $(\\alpha,\\beta)$ -AIM, $(\\alpha,\\beta)$ -LEM, $(\\alpha,\\beta)$ -EM, LCM, and BWM as $g^{(\\alpha,\\beta)}$ -AIM, $g^{(\\alpha,\\beta)}$ -LEM, $g^{(\\alpha,\\beta)}$ -EM, $g^{\\mathrm{LCM}}$ , and $g^{\\mathrm{BWM}}$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "Rotation matrices: The special orthogonal group $\\mathrm{SO}(n)$ is the set of $n\\times n$ orthogonal matrices with unit determinant, the elements of which are also referred to as rotation matrices. As shown in [25], $\\mathrm{SO}(n)$ forms a Lie group. We adopt the widely used bi-invariant Riemannian metric [8]. ", "page_idx": 1}, {"type": "text", "text": "3 Riemannian multinomial logistic regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by [39], Ganea et al. [23], Nguyen and Yang [50], Chen et al. [16], Nguyen et al. [51] extended the Euclidean MLR into hyperbolic, SPD, and SPSD manifolds. However, these classifiers rely on specific Riemannian properties, such as the generalized law of sines, gyro structures, and flat metrics, which limits their generality. In this section, we first revisit several existing MLRs and then propose our Riemannian classifiers with minimal geometric requirements. ", "page_idx": 2}, {"type": "text", "text": "3.1 Revisiting existing multinomial logistic regressions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given $C$ classes, the Euclidean MLR computes the multinomial probability of each class: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall k\\in\\{1,\\ldots,C\\},\\quad p(y=k\\mid x)\\propto\\exp\\left(\\left\\langle a_{k},x\\right\\rangle-b_{k}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $b_{k}\\in\\mathbb{R}$ , and $x,a_{k}\\in\\mathbb{R}^{n}\\backslash\\{\\mathbf{0}\\}$ . As shown in [23], the Euclidean MLR can be reformulated by the margin distance to the hyperplane: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p(y=k\\mid x)\\propto\\exp\\left(\\mathrm{sign}(\\langle a_{k},x-p_{k}\\rangle)\\|a_{k}\\|d(x,H_{a_{k},p_{k}})\\right),}}\\\\ {{H_{a_{k},p_{k}}=\\{x\\in{\\mathbb{R}}^{n}:\\langle a_{k},x-p_{k}\\rangle=0\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\langle a_{k},p_{k}\\rangle=b_{k}$ , and $H_{a_{k},p_{k}}$ is a hyperplane. ", "page_idx": 2}, {"type": "text", "text": "Eqs. (2) and (3) can be naturally extended into manifolds $\\mathcal{M}$ by Riemannian operators: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S)\\propto\\exp\\left(\\mathrm{sign}(\\langle\\tilde{A}_{k},\\mathrm{Log}_{P_{k}}(S)\\rangle_{P_{k}})\\|\\tilde{A}_{k}\\|_{P_{k}}\\tilde{d}(S,\\tilde{H}_{\\tilde{A}_{k},P_{k}})\\right),}\\\\ &{\\qquad\\tilde{H}_{\\tilde{A}_{k},P_{k}}=\\{S\\in\\mathcal{M}:g_{P_{k}}(\\mathrm{Log}_{P_{k}}\\,S,\\tilde{A}_{k})=0\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $P_{k}\\in\\mathcal{M},\\tilde{A}_{k}\\in T_{P_{k}}\\mathcal{M}\\backslash\\{\\mathbf{0}\\},g_{P_{k}}$ is the Riemannian metric at $P_{k}$ , and $\\mathrm{Log}_{P_{k}}$ is the Riemannian logarithm at $P_{k}$ . The margin distance is defined as an infimum: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{d}(S,\\tilde{H}_{\\tilde{A}_{k},P_{k}}))=\\operatorname*{inf}_{Q\\in\\tilde{H}_{\\tilde{A}_{k},P_{k}}}d(S,Q).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The MLRs proposed in [39, 23, 50, 16] can be viewed as different implementations of Eq. (4)- Eq. (6). To calculate the MLR in Eq. (4), one has to compute the associated Riemannian metrics, logarithmic maps, and margin distance. The associated Riemannian metrics and logarithmic maps often have closed-form expressions on the frequently-encounter manifolds in machine learning. However, the computation of the margin distance can be challenging. On the Poincar\u00e9 ball of hyperbolic manifolds, the generalized law of sines simplifies the calculation of Eq. (6) [23]. However, the generalized law of sines is not universally guaranteed on other manifolds. Additionally, Chen et al. [16] developed a closed-form solution of margin distance on the SPD manifold under any metric pulled back from Euclidean spaces. For curved manifolds, solving Eq. (6) would become a non-convex optimization problem. To address this challenge, Nguyen and Yang [50] defined gyro structures on the SPD manifold and proposed a pseudo-gyrodistance to calculate the margin distance. Similarly, Nguyen et al. [51] proposed a pseudo-gyrodistance on the SPSD manifold based on the gyro product space. However, gyro structures do not necessarily exist in general geometries. In summary, the aforementioned methods often rely on specific properties of their associated Riemannian metrics, which usually do not generalize to general geometries. ", "page_idx": 2}, {"type": "text", "text": "3.2 Riemannian multinomial logistic regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recalling Eqs. (4) and (5), the least requirement of extending Euclidean MLR into manifolds is the well-definedness of $\\mathrm{Log}_{P_{k}}(S)$ for each $k$ . In this subsection, we will develop Riemannian MLR, which depends solely on the Riemannian logarithm, without additional requirements, such as gyro structures and generalized law of sines. In the following, we always assume the well-definedness of the Riemannian logarithm. We start by reformulating the Euclidean margin distance to the hyperplane from a trigonometry perspective and then present our Riemannian MLR. ", "page_idx": 2}, {"type": "text", "text": "As we discussed before, obtaining the margin distance of Eq. (6) could be challenging. Inspired by [50], we resort to the perspective of trigonometry to reinterpret Euclidean margin distance. In Euclidean space, the margin distance is equivalent to ", "page_idx": 2}, {"type": "equation", "text": "$$\nd(x,H_{a,p}))=\\sin(\\angle x p y^{*})d(x,p),\\quad\\mathrm{with~}y^{*}=\\operatorname*{arg\\,max}_{y\\in H_{a,p}\\setminus\\{p\\}}(\\cos\\angle x p y).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We extend Eq. (7) to manifolds by the Riemannian trigonometry and geodesic distance, the counterparts of Euclidean trigonometry and distance. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Riemannian Margin Distance). Let $\\tilde{H}_{\\tilde{A},P}$ be a Riemannian hyperplane defined in Eq. (5), and $S\\in{\\mathcal{M}}$ . The Riemannian margin distance from $S$ to $\\tilde{H}_{\\tilde{A},P}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(S,\\tilde{H}_{\\tilde{A},P})=\\sin(\\angle S P Y^{*})d(S,P),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $d(S,P)$ is the geodesic distance, and $Y^{*}=\\operatorname{argmax}(\\cos\\angle S P Y)$ with $Y\\in\\tilde{H}_{\\tilde{A},P}\\backslash\\{P\\}$ . The initial velocities of geodesics define cos $\\angle S P Y$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos\\angle S P Y={\\frac{\\langle\\mathrm{Log}_{P}\\,Y,\\mathrm{Log}_{P}\\,S\\rangle_{P}}{\\|\\,\\mathrm{Log}_{P}\\,Y\\|_{P},\\|\\,\\mathrm{Log}_{P}\\,S\\|_{P}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle_{P}$ is the Riemannian metric at $P$ , and $\\|\\cdot\\|_{P}$ is the associated norm. ", "page_idx": 3}, {"type": "text", "text": "The Riemannian margin distance in Def. 3.1 has a closed-form expression. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2. [\u2193] The Riemannian margin distance defined in Def. 3.1 is given as ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(S,\\tilde{H}_{\\tilde{A},P})=\\frac{|\\langle\\mathrm{Log}_{P}\\,S,\\tilde{A}\\rangle_{P}|}{\\|\\tilde{A}\\|_{P}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Putting the Eq. (10) into Eq. (4), we can a closed-form expression for Riemannian MLR. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3 (RMLR). [\u2193] Given a Riemannian manifold $\\{\\mathcal{M},g\\}$ , the Riemannian MLR induced by $g$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left(\\langle\\mathrm{Log}_{P_{k}}\\,S,\\tilde{A}_{k}\\rangle_{P_{k}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{k}\\in\\mathcal{M},\\,\\tilde{A}_{k}\\in T_{P_{k}}\\mathcal{M}\\backslash\\{\\mathbf{0}\\}$ , and Log is the Riemannian logarithm. ", "page_idx": 3}, {"type": "text", "text": "$\\tilde{A}_{k}$ in Eq. (11) can not be directly viewed as a Euclidean parameter, as $\\tilde{A}_{k}\\in T_{P_{k}}\\mathcal{M}$ depends on $P_{k}$ and $P_{k}$ varies during the training. However, the tangent vector $\\tilde{A}_{k}$ can be generated from a tangent space at a fixed point. Several tricks can be used, such as Riemannian parallel transportation [21], vector transportation [1], the differential of Lie group or gyrogroup translation [64, 65]. Following previous work [23, 16, 50], we focus on parallel transportation and Lie group translation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{A}_{k}=\\Gamma_{Q\\rightarrow P_{k}}A_{k},}\\\\ &{\\tilde{A}_{k}=L_{P_{k}\\odot Q_{\\odot}^{-1}*,Q}A_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Q\\in{\\mathcal{M}}$ is a fixed point, $A_{k}\\,\\in\\,T_{Q}\\mathcal{M}\\backslash\\{0\\}$ , $\\Gamma$ is the parallel transportation along geodesic connecting $Q$ and $P_{k}$ , and $L_{P_{k}\\odot Q_{\\odot}^{-1}*,Q}$ denotes the differential map at $Q$ of left translation $L_{P_{k}\\odot Q_{\\odot}^{-1}}$ with $P_{k}\\odot Q_{\\odot}^{-1}$ denoting Lie group product and inverse. In this way, $A_{k}$ lies in a fixed tangent space and, therefore, can be optimized by a Euclidean optimizer. ", "page_idx": 3}, {"type": "text", "text": "Remark 3.4. We make the following remarks w.r.t. our Riemannian MLR. ", "page_idx": 3}, {"type": "text", "text": "(a). The reformulation of Eq. (7) in gyro MLR [50, 51] and ours are different. Gyro MLR adopts gyro trigonometry and gyro distance to reformulate Eq. (7), while our method directly uses Riemannian trigonometry and geodesic distance. ", "page_idx": 3}, {"type": "text", "text": "(b). Compared with the MLRs on hyperbolic, SPD, or SPSD manifolds in [23, 50, 16, 51], our framework enjoys broader applicability, as our framework only requires the Riemannian logarithm. This property is commonly satisfied by most manifolds encountered in machine learning, such as the five metrics on SPD manifolds mentioned in Sec. 2, the invariant metric on $\\mathrm{SO}(n)$ [8], and hyperbolic & spherical manifolds [11, 56]. Besides, several existing MLRs on different geometries are special cases of our Riemannian MLR, which are detailed in Tab. 1. ", "page_idx": 3}, {"type": "text", "text": "(c). The well-definedness of the Riemannian logarithm is a much weaker requirement compared to the existence of the gyro structure. The gyro structure not only requires the Riemannian logarithm but also implicitly requires geodesic completeness [50, Eqs. (1-2)]. For instance, on SPD manifolds, EM and BWM [63] are incomplete, undermining the well-definedness of gyro operations. ", "page_idx": 3}, {"type": "text", "text": "4 SPD multinomial logistic regressions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section showcases our RMLR framework on the SPD manifold. We first systematically discuss the power-deformed geometries of SPD manifolds. Based on these metrics, we will develop five families of deformed SPD MLRs. ", "page_idx": 3}, {"type": "table", "img_path": "lBp2cda7sp/tmp/2b07d32c20ed33777b88d4b096b032d87231f5707f07d2cd668c12ce38753686.jpg", "table_caption": ["Table 1: Several MLRs on different geometries are special cases of our MLR. "], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "lBp2cda7sp/tmp/736846e34a8ae6ad1b3da0836f236cf8aa4ebe184e5fe55917754cfe54943ea0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Illustration on the deformation (left) and Venn diagram (right) of metrics on SPD manifolds, where IEM, SREM, and $\\frac{1}{4}$ PAM denotes Inverse Euclidean Metric, Square Root Euclidean Metric, and Polar Affine Metric scaled by $^{1}\\!/\\!4$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Deformed geometries of SPD manifolds ", "text_level": 1, "page_idx": 4}, {"type": "table", "img_path": "lBp2cda7sp/tmp/bc19a723f25dc4d4d1cc4fbc58f9a06421042e31c2a59e12b147413a2fc3394a.jpg", "table_caption": ["Table 2: Properties of deformed metrics on SPD manifolds $\\mathbf{\\nabla}\\theta\\neq0$ and $\\operatorname*{min}(\\alpha,\\alpha+n\\beta)>0)$ . "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "As discussed in Sec. 2, there are five popular Riemannian metrics on SPD manifolds. These metrics can be all extended into power-deformed metrics. For a metric $g$ on $S_{++}^{n}$ , the power-deformed metric is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{g}_{P}\\left(V,W\\right)=\\frac1{\\theta^{2}}g_{P^{\\theta}}\\left((\\phi_{\\theta})_{*,P}(V),(\\phi_{\\theta})_{*,P}(W)\\right),\\forall P\\in S_{++}^{n},V,W\\in T_{P}S_{++}^{n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{\\theta}(P)=P^{\\theta}$ is the matrix power, and $(\\phi_{\\theta})_{*,P}$ is the differential map. The deformed metric $\\tilde{g}$ can interpolate between a LEM-like metric $\\left.\\theta\\right.\\rightarrow0$ ) and $g$ $\\mathbf{\\nabla}\\theta=1\\mathbf{\\Omega}$ ) [61]. Previous work has extended $(\\alpha,\\beta)$ -AIM, $(\\alpha,\\beta)$ -LEM, LCM, and BWM into power-deformed metrics and $(\\alpha,\\beta)$ -LEM is proven to be invariant under the power deformation [17]. We denote these metrics as $(\\theta,\\alpha,\\beta)$ -AIM [59], $(\\alpha,\\beta)$ -LEM [17], $2\\theta$ -BWM [61], and $\\theta$ -LCM [17], respectively. The deformation of these metrics is discussed in App. E.1. We further define the power-deformed metric of $(\\alpha,\\beta)$ -EM by Eq. (14), denoted as $(\\theta,\\alpha,\\beta)$ -EM. We have the following for the deformation of $(\\theta,\\alpha,\\beta)$ -EM. ", "page_idx": 4}, {"type": "text", "text": "So far, all five popular Riemannian metrics on SPD manifolds have been generalized into powerdeformed families of metrics. We summarize their associated properties in Tab. 2 and present their theoretical relation in Fig. 1. We leave technical details in App. E.2. ", "page_idx": 4}, {"type": "text", "text": "4.2 Five families of SPD multinomial logistic regressions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This subsection presents five families of specific SPD MLRs by our general framework in Thm. 3.3 and metrics discussed in Sec. 4.1. We focus on generating $\\tilde{A_{k}}$ by parallel transportation from the identity matrix, except for $2\\theta$ -BWM. Since the parallel transportation under $2\\theta$ -BWM would undermine numerical stability (please refer to App. F.2.1 for more details), we resort to a newly ", "page_idx": 4}, {"type": "image", "img_path": "lBp2cda7sp/tmp/8019545c8b709893aa4af2f4aa61f3b1c5d1e64d9cf5003a3e2c3b18ceaa0cb5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Conceptual illustration of SPD hyperplanes induced by five families of Riemannian metrics.   \nThe black dots denote the boundary of S2++. ", "page_idx": 5}, {"type": "text", "text": "developed Lie group operation [62]: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{1}\\odot S_{2}=L_{1}S_{2}L_{1}^{T},\\forall S_{1},S_{2}\\in\\mathcal{S}_{++}^{n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{1}=\\mathrm{Chol}(S_{1})$ is the Cholesky decomposition. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (SPD MLRs). [\u2193] By abuse of notation, we omit the subscripts $k$ of $A_{k}$ and $P_{k}$ . Given SPD feature $S$ , the SPD MLRs, $p(y=k\\mid S\\in S_{++}^{n})$ , are proportional to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\alpha,\\beta)\\!\\cdot\\!L E M\\cdot\\exp\\left[\\langle\\log(S)-\\log(P),A\\rangle^{(\\alpha,\\beta)}\\right],}\\\\ &{(\\theta,\\alpha,\\beta)\\!\\cdot\\!A I M\\cdot\\exp\\left[\\frac{1}{\\theta}\\langle\\log(P^{-\\frac{\\theta}{2}}S^{\\theta}P^{-\\frac{\\theta}{2}}),A\\rangle^{(\\alpha,\\beta)}\\right],}\\\\ &{(\\theta,\\alpha,\\beta)\\!\\cdot\\!E M\\cdot\\exp\\left[\\frac{1}{\\theta}\\langle S^{\\theta}-P^{\\theta},A\\rangle^{(\\alpha,\\beta)}\\right],}\\\\ &{\\qquad\\theta\\!\\cdot\\!L C M\\cdot\\exp\\left[\\frac{1}{\\theta}\\langle\\vert\\tilde{K}\\vert-\\vert\\tilde{L}\\vert+\\Big[\\mathrm{Dlog}(\\mathbb{D}(\\tilde{K}))-\\mathrm{Dlog}(\\mathbb{D}(\\tilde{L}))\\Big],\\vert A\\vert+\\frac{1}{2}\\mathbb{D}(A)\\rangle\\right],}\\\\ &{\\!\\!\\!\\!2\\theta\\!\\cdot\\!B W M\\cdot\\exp\\left[\\frac{1}{4\\theta}\\langle(P^{2\\theta}S^{2\\theta})^{\\frac{1}{2}}+(S^{2\\theta}P^{2\\theta})^{\\frac{1}{2}}-2P^{2\\theta},\\mathcal{L}_{P^{2\\theta}}(\\bar{L}A\\bar{L}^{\\top})\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A\\in T_{I}S_{++}^{n}\\backslash\\{0\\}$ is a symmetric matrix, $\\log(\\cdot)$ is the matrix logarithm, ${\\mathcal{L}}_{P}(V)$ is the solution to the matrix linear system $\\dot{\\mathcal{L}_{P}}[V]P+P\\mathcal{L}_{P}[V]\\stackrel{\\cdot}{=}\\dot{V}$ , known as the Lyapunov operator, $\\mathrm{Dlog}(\\cdot)$ is the diagonal element-wise logarithm, $\\lfloor\\cdot\\rfloor$ is the strictly lower part of a square matrix, and $\\mathbb{D}(\\cdot)$ is $a$ diagonal matrix with diagonal elements of a square matrix. Besides, $\\log_{*,P}$ is the differential maps at $P$ , $\\tilde{K}=\\operatorname{Chol}(S^{\\theta})$ , $\\tilde{L}=\\mathrm{Chol}(P^{\\theta})$ , and $\\bar{L}=\\mathrm{Chol}(P^{2\\theta})$ . ", "page_idx": 5}, {"type": "text", "text": "The Lyapunov operator in Eq. (20) requires the eigendecomposition. However, the backpropagation of eigendecomposition involves $^1\\!/\\!(\\sigma_{i}\\!-\\!\\sigma_{j})$ [34], undermining the numerical stability. Therefore, we propose a numerically stable backpropagation for the Lyapunov operator, detailed in App. F.2.2. ", "page_idx": 5}, {"type": "text", "text": "As $2\\times2$ SPD matrices can be embedded into $\\mathbb{R}^{3}$ as an open cone [74], we illustrate SPD hyperplanes induced by five families of metrics in Fig. 2. ", "page_idx": 5}, {"type": "image", "img_path": "lBp2cda7sp/tmp/b0330d0baf6505c511aad2a28e54ac62fca9cc2132bc06a8ba67e7555f50f1b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Remark 4.3. Our SPD MLRs extend the existing SPD MLRs [50, 16]. The pseudo-gyrodistance to a SPD hyperplane in [50, Thms. 2.23- 2.25] is incorporated by our Thm. 3.2, while the flat SPD MLRs under $(\\alpha,\\beta)$ -LEM and $\\theta$ -LCM in [16, Cor. 4.1] are special cases of our Thm. 4.2. Furthermore, our approach extends the scope of prior work as neither [16] nor [50] explored SPD MLRs based on $(\\theta,\\alpha,\\beta)$ -EM and $2\\theta$ -BWM. The gyro operations in [50, Eq. (1)] implicitly requires geodesic completeness, whereas $(\\theta,\\alpha,\\beta)$ -EM ", "page_idx": 5}, {"type": "text", "text": "Figure 3: Conceptual illustration of a Lie hyperplane. Each pair of antipodal black dots corresponds to a rotation matrix with an Euler angle of $\\pi$ , while the green dots denote a Lie hyperplane. ", "page_idx": 5}, {"type": "text", "text": "and $2\\theta$ -BWM are incomplete. As neither $(\\theta,\\alpha,\\beta)$ -EM nor $2\\theta$ -BWM belong to pullback Euclidean metrics, the framework presented in [16] cannot be applied to these metrics. To the best of our knowledge, our work is the first to apply PEM and BWM to establish Riemannian neural networks, opening up new possibilities for utilizing these metrics in machine learning applications. Besides, neither Nguyen and Yang [50] nor Chen et al. [16] explore the deformed metrics for building SPD MLRs. ", "page_idx": 5}, {"type": "text", "text": "Table 3: Comparison of SPDNet with LogEig against SPD MLRs on the Radar dataset. ", "page_idx": 6}, {"type": "table", "img_path": "lBp2cda7sp/tmp/0f7da1a6310d4f0c72cf7b90b1048ab7e3d06f4b91ae7ea2eaa185f1088e31e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "lBp2cda7sp/tmp/362f5fac73d33315b838308fe0ac98da6e87ca8ca7b029c53b83138eea4d34e0.jpg", "table_caption": ["Table 4: Comparison of SPDNet with LogEig against SPD MLRs on the HDM05 dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "lBp2cda7sp/tmp/7899f14adedb2c37afa0e4b05990246c2f9dd523455191f831e56fdc9a256a34.jpg", "table_caption": ["Table 5: Inter-session experiments of TSMNet with different MLRs on the Hinss2021 dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 6: Inter-subject experiments of TSMNet with different MLRs on the Hinss2021 dataset. ", "page_idx": 6}, {"type": "table", "img_path": "lBp2cda7sp/tmp/cc612ca3b9abe201bfdd79fcbf6b5fa945b12addacab2034c3b2c98d05518e87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Lie multinomial logistic regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section introduces our Lie MLR on $\\mathrm{SO}(n)$ based on the general RMLR framework in Thm. 3.3.   \nThe Riemannian metric on $\\mathrm{SO}(n)$ is assumed to be the invariant metric in Tab. 13. ", "page_idx": 6}, {"type": "text", "text": "The two ways to generate $\\tilde{A}_{k}$ in RMLR, i.e.,Eqs. (12) and (13), are equivalent on $\\mathrm{SO}(n)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1. [\u2193] ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Gamma_{Q\\rightarrow P}=L_{P Q^{-1}*,Q},\\forall P,Q\\in\\mathrm{SO}(n).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar with SPD MLRs, we set $Q=I$ . The Lie MLR on $\\mathrm{SO}(n)$ is presented in the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2. [\u2193] The Lie MLR on $\\mathrm{SO}(n)$ is given as ", "page_idx": 6}, {"type": "equation", "text": "$$\np(y=k\\mid R\\in\\operatorname{SO}(n))\\propto\\langle\\log(P_{k}^{\\top}S),A_{k}\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $P_{k}\\in\\mathrm{SO}(n)$ and $A_{k}\\in\\mathfrak{s o}(n)$ . ", "page_idx": 6}, {"type": "text", "text": "We refer to the Riemannian hyperplanes (Eq. (5)) on $\\mathrm{SO}(n)$ as Lie hyperplanes. As SO(3) is homeomorphic to 3-dimensional real projective space $\\mathbb{R P}^{3}$ [26], Fig. 3 illustrates Lie hyperplanes in the closed ball in $\\mathbb{R}^{3}$ of radius $\\pi$ . ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first validate our SPD MLRs on four SPD neural networks: SPDNet [30] and TSMNet [37] for Riemannian feedforward networks, RResNet [36] for Riemannian residual networks, and SPDGCN [76] for Riemannian graph neural networks. Then, we proceed with experiments of our Lie MLR under the classic LieNet architecture [31]. The classifier in all the above networks is the LogEig MLR (matrix logarithm $+\\,\\mathrm{FC}+$ softmax), a Euclidean MLR on the tangent space at the identity matrix. We substitute the original non-intrinsic LogEig MLR in each baseline model with our RMLRs. Notably, the gyro SPD MLRs [50] are special cases of our SPD MLRs under the standard AIM, LEM, and LCM $((\\theta,\\alpha,\\beta)=(1,1,0))$ ), while flat SPD MLRs [16] are incorporated by our SPD MLRs under $(\\alpha,\\beta)$ -LEM and $\\theta$ -LCM. More implementation details are presented in App. G. ", "page_idx": 6}, {"type": "text", "text": "6.1 Experiments on the proposed SPD MLRs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following, we abbreviate SPD MLR-metric as metric. For instance, $(\\theta,\\alpha,\\beta)$ -AIM denotes the baseline endowed with the SPD MLR induced by $(\\theta,\\alpha,\\beta)$ -AIM and (1,1,0) as the value of $(\\theta,\\alpha,\\beta)$ . ", "page_idx": 6}, {"type": "text", "text": "6.1.1 Experiments on the Riemannian feedforward network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate our SPD MLRs for Riemannian feedforward networks under the SPDNet and TSMNet backbones. Following [30, 10], on SPDNet, we use the Radar dataset [10] for radar recognition and the HDM05 dataset [44] for human action recognition. TSMNet [37] is one of the stateof-the-art methods for the EEG classification task. Following [37], we use the Hinss2021 [28] dataset. For each family of SPD MLRs, we report the SPD MLR induced from the standard metric $(\\theta=1,\\alpha=1,\\beta=0)$ , and the one induced from the deformed metric with best $(\\theta,\\alpha,\\beta)$ . Besides, if the standard SPD MLR is already saturated, we only report the results of the standard one. Under each metric, We highlight the results in bold of our SPD MLR under the best hyperparameters. We visualize the results in App. G.1.6. ", "page_idx": 7}, {"type": "text", "text": "Radar: In line with [10], we evaluate our classifiers under two network architectures: 2-Block and 5-Block configurations. The 10-fold results (mean\u00b1std) are presented in Tab. 3. Note that the SPD MLR induced by standard AIM is saturated. Generally speaking, our SPD MLRs achieve superior performance against the vanilla LogEig MLR. Moreover, for most families of metrics, the associated SPD MLRs with proper $(\\theta,\\alpha,\\beta)$ outperform the standard SPD MLR, demonstrating the effectiveness of our parameterization. Besides, among all SPD MLRs, the ones induced by $(\\alpha,\\beta)$ -LEM achieve the best performance. ", "page_idx": 7}, {"type": "text", "text": "HDM05: Following [30], three architectures are adopted: 1-Block, 2-Block and 3-Block configurations. The 10-fold results (mean\u00b1std) are presented in Tab. 4. Note that the standard SPD MLRs under AIM, LEM, and BWM are already saturated on this dataset. As the Radar dataset, similar observations can be made on this dataset. Our SPD MLRs can bring consistent performance gain for SPDNet, and properly selected hyperparameters can bring further improvement. Particularly, among all the SPD MLRs, the ones based on the $2\\theta$ -BWM and $(\\theta,\\alpha,\\beta)$ -EM achieve the best performance. Compared to the vanilla LogEig MLR, the highest performance improvement is $14.23\\%$ , highlighting our approach\u2019s effectiveness. Notably, since $2\\theta$ -BWM and $(\\theta,\\alpha,\\beta)$ -EM are geodesically incomplete and not pulled back from a Euclidean space, the SPD MLR under these two metrics can not be derived by the framework of gyro or flat MLR. This contrast confirms the applicability of our theoretical framework to a broader range of geometries. ", "page_idx": 7}, {"type": "text", "text": "Hinss2021: The results (mean\u00b1std) of leaving $5\\%$ out cross-validation are reported in Tabs. 5 and 6. Once again, our intrinsic classifiers demonstrate improved performance compared to the LogEig MLR in both inter-session and inter-subject scenarios. Besides, the SPD MLRs based on $\\theta$ -LCM achieve the best performance, outperforming the vanilla classifier by $2.6\\%$ for inter-session and by $4.46\\%$ for inter-subject. This finding highlights the versatility of our framework. ", "page_idx": 7}, {"type": "text", "text": "Table 7: Comparison of LogEig against SPD MLRs under the RResNet architecture. ", "page_idx": 7}, {"type": "table", "img_path": "lBp2cda7sp/tmp/d271d43fb6ba5db9bc5322f762471edc9e9d973dc96ab51691864e461180badc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.1.2 Experiments on the Riemannian residual network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following [36], we use the HDM05 and NTU60 [55] datasets on the RResNet backbone. For the hyperparameter $(\\theta,\\alpha,\\beta)$ in our SPD MLRs, we borrow the best ones from Tab. 4. Tab. 7 reports the 10-fold and 5-fold results on the HDM05 and NTU datasets. The SPD MLRs still consistently outperform the vanilla LogEig MLR. Besides, similar to the SPD MLRs under the SPDNet backbone for action recognition (Tab. 4), the SPD MLR based on $\\theta$ -LCM, $2\\theta$ -BWM, or $(\\theta,\\alpha,\\beta)$ -EM outperforms the vanilla LogEig MLR by a large margin. Especially, the highest performance improvement is $13.72\\%$ and ${\\bf{8.4\\%}}$ on these two datasets. ", "page_idx": 7}, {"type": "text", "text": "6.1.3 Experiments on the Riemannian graph network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use SPDGCN [76] as the backbone network for the Riemannian graph network. Following [76], we use the Disease [2], Cora [54], and Pubmed [46] datasets for node classification. The 10-fold average and maximum results of the vanilla LogEig MLR against our SPD MLR with best $(\\theta,\\alpha,\\beta)$ are reported in Tab. 8. Similar to the previous results, our SPD MLRs outperform the LogEig MLR. Besides, the SPD MLR based on $(\\alpha,\\beta)$ -LEM generally achieves the best performance for SPDGCN. ", "page_idx": 7}, {"type": "text", "text": "6.1.4 Ablations of SPD MLRs on direct classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For a more straightforward comparison, we compare LogEig against our SPD MLRs for direct classification. We adopt the Radar, HDM05, and Hinss2021 datasets. We follow the preprocessing of ", "page_idx": 7}, {"type": "table", "img_path": "lBp2cda7sp/tmp/716e4d11b31c910bb9ff21f18dd3d292a77e6482c1b67b1db929c648cbb52fdb.jpg", "table_caption": ["Table 8: Comparison of LogEig against SPD MLRs under the SPDGCN architecture. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lBp2cda7sp/tmp/6b47a885247d38591e5f6b5d30170350540126c0802a00f3aa327520a74ffc4b.jpg", "table_caption": ["Table 9: Comparison of LogEig against SPD MLRs for direct classification. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "SPDNet and TSMNet to model features into the SPD manifold and directly use LogEig or our SPD MLRs for classification. The average results are presented in Tab. 9. The hyperparameters $(\\theta,\\alpha,\\beta)$ are borrowed from Tabs. 3 to 6. Our SPD MLRs consistently outperform the vanilla LogEig MLR. Particularly on the HDM05 dataset, the highest performance improvement by our SPD MLRs is $18.34\\%$ , surpassing the non-intrinsic LogEig MLR by a large margin. Ablations on model efficiency are also discussed in App. G.1.5. ", "page_idx": 8}, {"type": "text", "text": "6.2 Experiments on the proposed Lie MLR ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "lBp2cda7sp/tmp/d5ea84b10d6b0dabbec4ae255f012d527e619a6d2c786d993f4276f374347096.jpg", "table_caption": ["Table 10: Results of LogEig MLR against Lie MLR under the LieNet architecture. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We apply our Lie MLR into the classic $\\mathrm{SO}(n)$ network, i.e.,LieNet [31], where features are on the Lie group of $\\mathrm{SO(3)}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathrm{SO(3)}$ . Following LieNet [31], we use G3D [6] and HDM05 [44] datasets. We also extend the Riemannian optimization package geoopt [4] into SO(3), allowing for Riemannian optimization. We find that Riemannian SGD performs best for LieNet. Tab. 10 presents the 10-fold average results of LieNet with or without Lie MLR. Note that on the HDM05 datasets, the LieNet might fail to converge, fluctuating between the validation accuracy of $70\\%\\textrm{-}75\\%$ . Therefore, we select 10-fold best performance out of 20-fold experiments. It can be observed that our Lie MLR can improve the performance of LieNet. Besides, our Lie MLR can also improve the training stability. On the HDM05 dataset, LieNet fails to converge in 8 out of 20 folds. However, when endowed with our Lie MLR, LieNet+LieMLR only encounters convergence failures in 2 folds. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper presents a novel and versatile framework for designing RMLR for general geometries, with a specific focus on SPD manifolds and $\\mathrm{SO}(n)$ . On the SPD manifold, we systematically explore five families of Riemannian metrics and utilize them to construct five families of deformed SPD MLRs. On $\\mathrm{SO}(n)$ , we develop the Lie MLR for classifying rotation matrices. Extensive experiments demonstrate the superiority of our intrinsic classifiers. We expect that our work could present a promising direction for designing intrinsic classifiers on diverse geometries. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by the MUR PNRR project FAIR (PE00000013) funded by the NextGenerationEU, the EU Horizon project ELIAS (No. 101120237), a donation from Cisco, the National Natural Science Foundation of China (62306127), the Natural Science Foundation of Jiangsu Province (BK20231040), and the Fundamental Research Funds for the Central Universities (JUSRP124015). The authors also gratefully acknowledge the financial support from the China Scholarship Council (CSC). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2008.   \n[2] Roy M Anderson and Robert M May. Infectious diseases of humans: dynamics and control. Oxford University Press, 1991.   \n[3] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Fast and simple computations on tensors with Log-Euclidean metrics. PhD thesis, INRIA, 2005.   \n[4] Gary B\u00e9cigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. arXiv preprint arXiv:1810.00760, 2018.   \n[5] Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the Bures-Wasserstein distance between positive definite matrices. Expositiones Mathematicae, 37(2):165\u2013191, 2019.   \n[6] Victoria Bloom, Dimitrios Makris, and Vasileios Argyriou. G3D: A gaming action dataset and real time action recognition evaluation framework. In 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 7\u201312. IEEE, 2012.   \n[7] Silvere Bonnabel, Anne Collard, and Rodolphe Sepulchre. Rank-preserving geometric means of positive semi-definite matrices. Linear Algebra and its Applications, 438(8):3202\u20133216, 2013.   \n[8] Nicolas Boumal and P-A Absil. A discrete regression method on manifolds and its application to data on SO(n). IFAC Proceedings Volumes, 44(1):2284\u20132289, 2011.   \n[9] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34 (4):18\u201342, 2017.   \n[10] Daniel Brooks, Olivier Schwander, Fr\u00e9d\u00e9ric Barbaresco, Jean-Yves Schneider, and Matthieu Cord. Riemannian batch normalization for SPD neural networks. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[11] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry. Flavors of geometry, 31(59-115):2, 1997.   \n[12] Rudrasis Chakraborty, Chun-Hao Yang, Xingjian Zhen, Monami Banerjee, Derek Archer, David Vaillancourt, Vikas Singh, and Baba Vemuri. A statistical recurrent model on the manifold of symmetric positive definite matrices. Advances in Neural Information Processing Systems, 31, 2018.   \n[13] Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domainspecific batch normalization for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7354\u20137362, 2019.   \n[14] Ziheng Chen, Tianyang Xu, Xiao-Jun Wu, Rui Wang, and Josef Kittler. Hybrid Riemannian graph-embedding metric learning for image set classification. IEEE Transactions on Big Data, 2021.   \n[15] Ziheng Chen, Tianyang Xu, Xiao-Jun Wu, Rui Wang, Zhiwu Huang, and Josef Kittler. Riemannian local mechanism for SPD neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7104\u20137112, 2023.   \n[16] Ziheng Chen, Yue Song, Gaowen Liu, Ramana Rao Kompella, Xiaojun Wu, and Nicu Sebe. Riemannian multinomial logistics regression for SPD neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024.   \n[17] Ziheng Chen, Yue Song, Yunmei Liu, and Nicu Sebe. A Lie group approach to Riemannian batch normalization. In The Twelfth International Conference on Learning Representations, 2024.   \n[18] Ziheng Chen, Yue Song, Xiao-Jun Wu, Gaowen Liu, and Nicu Sebe. Understanding matrix function normalizations in covariance pooling through the lens of Riemannian geometry. arXiv preprint arXiv:2407.10484, 2024.   \n[19] Ziheng Chen, Yue Song, Xiao-Jun Wu, and Nicu Sebe. Product geometries on Cholesky manifolds with applications to SPD manifolds. arXiv preprint arXiv:2407.02607, 2024.   \n[20] Ziheng Chen, Yue Song, Tianyang Xu, Zhiwu Huang, Xiao-Jun Wu, and Nicu Sebe. Adaptive Log-Euclidean metrics for SPD matrix learning. IEEE Transactions on Image Processing, 2024.   \n[21] Manfredo Perdigao Do Carmo and J Flaherty Francis. Riemannian Geometry, volume 6. Springer, 1992.   \n[22] Ian L Dryden, Xavier Pennec, and Jean-Marc Peyrat. Power Euclidean metrics for covariance matrices with application to diffusion tensor imaging. arXiv preprint arXiv:1009.3045, 2010.   \n[23] Octavian Ganea, Gary B\u00e9cigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \n[24] Alexandre Gramfort. MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience, 7, 2013.   \n[25] Brian C Hall and Brian C Hall. Lie groups, Lie algebras, and representations. Springer, 2013.   \n[26] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong Li. Rotation averaging. International Journal of Computer Vision, 103:267\u2013305, 2013.   \n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[28] Marcel F. Hinss, Ludovic Darmet, Bertille Somon, Emilie Jahanpour, Fabien Lotte, Simon Ladouce, and Rapha\u00eblle N. Roy. An EEG dataset for cross-session mental workload estimation: Passive BCI competition of the Neuroergonomics Conference 2021, 2021.   \n[29] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9 (8):1735\u20131780, 1997.   \n[30] Zhiwu Huang and Luc Van Gool. A Riemannian network for SPD matrix learning. In Thirty-first AAAI Conference on Artificial Intelligence, 2017.   \n[31] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning on Lie groups for skeleton-based action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6099\u20136108, 2017.   \n[32] Zhiwu Huang, Jiqing Wu, and Luc Van Gool. Building deep networks on Grassmann manifolds. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[33] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation for deep networks with structured layers. In Proceedings of the IEEE International Conference on Computer Vision, pages 2965\u20132973, 2015.   \n[34] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Training deep networks with structured layers by matrix backpropagation. arXiv preprint arXiv:1509.07838, 2015.   \n[35] Vinay Jayaram and Alexandre Barachant. MOABB: trustworthy algorithm benchmarking for BCIs. Journal of Neural Engineering, 15(6):066011, 2018.   \n[36] Isay Katsman, Eric Ming Chen, Sidhanth Holalkere, Anna Asch, Aaron Lou, Ser-Nam Lim, and Christopher De Sa. Riemannian residual neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[37] Reinmar Kobler, Jun-ichiro Hirayama, Qibin Zhao, and Motoaki Kawanabe. SPD domainspecific batch normalization to crack interpretable unsupervised domain adaptation in EEG. Advances in Neural Information Processing Systems, 35:6219\u20136235, 2022.   \n[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25, 2012.   \n[39] Guy Lebanon and John Lafferty. Hyperplane margin classifiers on the multinomial manifold. In Proceedings of the Twenty-first International Conference on Machine Learning, page 66, 2004.   \n[40] John M Lee. Introduction to smooth manifolds. Springer, 2013.   \n[41] Zhenhua Lin. Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. SIAM Journal on Matrix Analysis and Applications, 40(4):1353\u20131370, 2019.   \n[42] Federico L\u00f3pez, Beatrice Pozzetti, Steve Trettel, Michael Strube, and Anna Wienhard. Vectorvalued distance and Gyrocalculus on the space of symmetric positive definite matrices. Advances in Neural Information Processing Systems, 34:18350\u201318366, 2021.   \n[43] H\u00e0 Quang Minh. Alpha Procrustes metrics between positive definite operators: a unifying formulation for the Bures-Wasserstein and Log-Euclidean/Log-Hilbert-Schmidt metrics. Linear Algebra and its Applications, 636:25\u201368, 2022.   \n[44] Meinard M\u00fcller, Tido R\u00f6der, Michael Clausen, Bernhard Eberhardt, Bj\u00f6rn Kr\u00fcger, and Andreas Weber. Documentation mocap database HDM05. Technical report, Universit\u00e4t Bonn, 2007.   \n[45] Richard M Murray, Zexiang Li, and S Shankar Sastry. A mathematical introduction to robotic manipulation. CRC press, 2017.   \n[46] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In 10th International Workshop on Mining and Learning with Graphs, volume 8, page 1, 2012.   \n[47] Xuan Son Nguyen. Geomnet: A neural network based on Riemannian geometries of SPD matrix space and Cholesky space for 3D skeleton-based interaction recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 13379\u201313389, 2021.   \n[48] Xuan Son Nguyen. The Gyro-structure of some matrix manifolds. In Advances in Neural Information Processing Systems, volume 35, pages 26618\u201326630, 2022.   \n[49] Xuan Son Nguyen. A Gyrovector space approach for symmetric positive semi-definite matrix learning. In Proceedings of the European Conference on Computer Vision, pages 52\u201368, 2022.   \n[50] Xuan Son Nguyen and Shuo Yang. Building neural networks on matrix manifolds: A Gyrovector space approach. arXiv preprint arXiv:2305.04560, 2023.   \n[51] Xuan Son Nguyen, Shuo Yang, and Aymeric Histace. Matrix manifold neural networks $^{++}$ . In The Twelfth International Conference on Learning Representations, 2024.   \n[52] Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor computing. International Journal of Computer Vision, 66(1):41\u201366, 2006.   \n[53] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3D deep learning with PyTorch3D. arXiv preprint arXiv:2007.08501, 2020.   \n[54] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[55] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. NTU $\\mathrm{RGB+}$ D: A large scale dataset for 3D human activity analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1010\u20131019, 2016.   \n[56] Ondrej Skopek, Octavian-Eugen Ganea, and Gary B\u00e9cigneul. Mixed-curvature variational autoencoders. arXiv preprint arXiv:1911.08411, 2019.   \n[57] Yue Song, Nicu Sebe, and Wei Wang. Why approximate matrix square root outperforms accurate SVD in global covariance pooling? In Proceedings of the IEEE International Conference on Computer Vision, pages 1115\u20131123, 2021.   \n[58] Yue Song, Nicu Sebe, and Wei Wang. On the eigenvalues of global covariance pooling for finegrained visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3554\u20133566, 2022.   \n[59] Yann Thanwerdas and Xavier Pennec. Is affine-invariance well defined on SPD matrices? a principled continuum of metrics. In Geometric Science of Information: 4th International Conference, GSI 2019, Toulouse, France, August 27\u201329, 2019, Proceedings 4, pages 502\u2013510. Springer, 2019.   \n[60] Yann Thanwerdas and Xavier Pennec. Exploration of balanced metrics on symmetric positive definite matrices. In Geometric Science of Information: 4th International Conference, GSI 2019, Toulouse, France, August 27\u201329, 2019, Proceedings 4, pages 484\u2013493. Springer, 2019.   \n[61] Yann Thanwerdas and Xavier Pennec. The geometry of mixed-Euclidean metrics on symmetric positive definite matrices. Differential Geometry and its Applications, 81:101867, 2022.   \n[62] Yann Thanwerdas and Xavier Pennec. Theoretically and computationally convenient geometries on full-rank correlation matrices. SIAM Journal on Matrix Analysis and Applications, 43(4): 1851\u20131872, 2022.   \n[63] Yann Thanwerdas and Xavier Pennec. O (n)-invariant Riemannian metrics on SPD matrices. Linear Algebra and its Applications, 661:163\u2013201, 2023.   \n[64] Loring W.. Tu. An introduction to manifolds. Springer, 2011.   \n[65] Abraham A Ungar. Analytic hyperbolic geometry: Mathematical foundations and applications. World Scientific, 2005.   \n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[67] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa. Human action recognition by representing 3D skeletons as points in a Lie group. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 588\u2013595, 2014.   \n[68] Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang, and Peihua Li. Deep CNNs meet global covariance pooling: Better representation and generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(8):2582\u20132597, 2020.   \n[69] Rui Wang, Xiao-Jun Wu, and Josef Kittler. SymNet: A simple symmetric positive definite manifold deep learning method for image set classification. IEEE Transactions on Neural Networks and Learning Systems, 33(5):2208\u20132222, 2021.   \n[70] Rui Wang, Xiao-Jun Wu, Ziheng Chen, Tianyang Xu, and Josef Kittler. DreamNet: A deep Riemannian manifold network for SPD matrix learning. In Proceedings of the Asian Conference on Computer Vision, pages 3241\u20133257, 2022.   \n[71] Rui Wang, Xiao-Jun Wu, Ziheng Chen, Tianyang Xu, and Josef Kittler. Learning a discriminative SPD manifold neural network for image set classification. Neural networks, 151:94\u2013110, 2022.   \n[72] Rui Wang, Chen Hu, Ziheng Chen, Xiao-Jun Wu, and Xiaoning Song. A Grassmannian manifold self-attention network for signal classification. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 5099\u20135107, 2024.   \n[73] Rui Wang, Xiao-Jun Wu, Ziheng Chen, Cong Hu, and Josef Kittler. SPD manifold deep metric learning for image set classification. IEEE Transactions on Neural Networks and Learning Systems, 2024.   \n[74] Or Yair, Mirela Ben-Chen, and Ronen Talmon. Parallel transport on the cone manifold of SPD matrices for domain adaptation. IEEE Transactions on Signal Processing, 67(7):1797\u20131811, 2019.   \n[75] Hongwei Yong, Jianqiang Huang, Deyu Meng, Xiansheng Hua, and Lei Zhang. Momentum batch normalization for deep learning with small batch size. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XII 16, pages 224\u2013240. Springer, 2020.   \n[76] Wei Zhao, Federico Lopez, J Maxwell Riestenberg, Michael Strube, Diaaeldin Taha, and Steve Trettel. Modeling graphs beyond hyperbolic: Graph neural networks in symmetric positive definite matrices. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 122\u2013139. Springer, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitations and future avenues 17 ", "page_idx": 14}, {"type": "text", "text": "B Preliminaries 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Notations 17   \nB.2 Brief review of Riemannian geometry 17   \nB.3 Basic geometries of SPD manifolds 18   \nB.4 Basic geometry of rotation matrices 18 ", "page_idx": 14}, {"type": "text", "text": "C RMLR as a natural extension of the Euclidean MLR 19 ", "page_idx": 14}, {"type": "text", "text": "D Gyro SPSD MLR as special cases of our RMLR 19 ", "page_idx": 14}, {"type": "text", "text": "E Theories on the deformed metrics 21 ", "page_idx": 14}, {"type": "text", "text": "E.1 Limiting cases of the deformed metrics . 21   \nE.2 Proof of the properties of the deformed metrics (Tab. 2) 22 ", "page_idx": 14}, {"type": "text", "text": "F Computational details on the SPD MLR under power-deformed BWM 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Matrix square roots in the SPD MLR under power-deformed BWM 23   \nF.2 Numerical stability of the SPD MLR under power-deformed BWM 23   \nF.2.1 Instability of parallel transportation under power-deformed BWM 23   \nF.2.2 Numerically stable methods for the SPD MLR under power-deformed BWM 23 ", "page_idx": 14}, {"type": "text", "text": "G Implementation details and additional experiments 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Additional details and experiments on the SPD MLRs 24   \nG.1.1 Basic layers in SPDNet and TSMNet 24   \nG.1.2 Datasets and preprocessing 25   \nG.1.3 Implementation details 25   \nG.1.4 Hyper-parameters . 26   \nG.1.5 Model efficiency 26   \nG.1.6 Visualization 27   \nG.2 Additional details and experiments on the Lie MLR 27   \nG.2.1 Basic layers in LieNet 27   \nG.2.2 Datasets and preprocessing 28   \nG.2.3 Implementation details 28   \nG.3 Hardware 28   \nH.1 Proof of Thm. 3.2 28   \nH.2 Proof of Thm. 3.3 29   \nH.3 Proof of Prop. 4.1 29   \nH.4 Proof of Thm. 4.2 29   \nH.5 Proof of Lem. 5.1 32   \nH.6 Proof of Thm. 5.2 32 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Limitations and future avenues ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Limitation: Recalling our RMLR in Eq. (11), our RMLR might be over-parameterized. In our RMLR, each class would require a Riemannian parameter $P_{k}$ and Euclidean parameter $A_{k}$ . Consequently, as the number of classes grows, the classification layer would become burdened with excessive parameters. We will address this problem in future work. ", "page_idx": 16}, {"type": "text", "text": "Future work: We highlight the advantage of our approach compared to existing methods that our framework only requires the Riemannian logarithm, which is commonly satisfied by various manifolds encountered in machine learning. Therefore, as a future avenue, our framework offers various possibilities for designing intrinsic classifiers for neural networks on other manifolds. ", "page_idx": 16}, {"type": "text", "text": "B Preliminaries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Notations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We briefly summarize the notations in Tab. 11 for better clarity. ", "page_idx": 16}, {"type": "text", "text": "Table 11: Summary of notations. ", "page_idx": 16}, {"type": "table", "img_path": "lBp2cda7sp/tmp/9973335dfcf6ce2bae1deec26c40ca6473f2d5edfb4371b23138d237ba9f7a49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Brief review of Riemannian geometry ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Intuitively, manifolds are locally Euclidean spaces. Differentials are the generalization of derivatives in classic calculus. For more details on smooth manifolds, please refer to [64, 40]. Riemannian manifolds are the manifolds endowed with Riemannian metrics, which can be intuitively viewed as point-wise inner products. ", "page_idx": 16}, {"type": "text", "text": "Definition B.1 (Riemannian Manifolds). A Riemannian metric on $\\mathcal{M}$ is a smooth symmetric covariant 2-tensor field on $\\mathcal{M}$ , which is positive definite at every point. A Riemannian manifold is a pair $\\{\\mathcal{M},g\\}$ , where $\\mathcal{M}$ is a smooth manifold and $g$ is a Riemannian metric. ", "page_idx": 16}, {"type": "text", "text": "W.l.o.g., we abbreviate $\\{\\mathcal{M},g\\}$ as $\\mathcal{M}$ . The Riemannian metric $g$ induces various Riemannian operators, including the geodesic, exponential, and logarithmic maps, and parallel transportation. These operators correspond to straight lines, vector addition, vector subtraction, and parallel displacement in Euclidean spaces, respectively [52, Tabel 1]. A plethora of discussions on Riemannian geometry can be found in [21]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "When a manifold $\\mathcal{M}$ is endowed with a smooth operation, it is referred to as a Lie group. ", "page_idx": 17}, {"type": "text", "text": "Definition B.2 (Lie Groups). A manifold is a Lie group, if it forms a group with a group operation $\\odot$ such that $m(x,y)\\mapsto x\\odot y$ and $i(x)\\mapsto x_{\\odot}^{-1}$ are both smooth, where $x_{\\odot}^{-1}$ is the group inverse of $x$ . ", "page_idx": 17}, {"type": "text", "text": "Lastly, we review the definition of pullback metric, a common technique in Riemannian geometry.   \nThis idea is a natural generalization of bijection from set theory. ", "page_idx": 17}, {"type": "text", "text": "Definition B.3 (Pullback Metrics). Suppose $\\mathcal{M},\\mathcal{N}$ are smooth manifolds, $g$ is a Riemannian metric on $\\mathcal{N}$ , and $f:\\mathcal{M}\\rightarrow\\mathcal{N}$ is smooth. Then the pullback of $g$ by $f$ is defined point-wisely, ", "page_idx": 17}, {"type": "equation", "text": "$$\n(f^{*}g)_{p}(V_{1},V_{2})=g_{f(p)}(f_{*,p}(V_{1}),f_{*,p}(V_{2})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p\\in\\mathcal{M},f_{*,p}(\\cdot)$ is the differential map of $f$ at $p$ , and $V_{i}\\in T_{p}\\mathcal{M}$ . If $f^{\\ast}g$ is positive definite, it is a Riemannian metric on $\\mathcal{M}$ , which is called the pullback metric defined by $f$ . ", "page_idx": 17}, {"type": "table", "img_path": "lBp2cda7sp/tmp/b01286657230884838ed2b0301e2a5a2813fdd6288ef384842f644c59bd76ede.jpg", "table_caption": ["Table 12: The associated Riemannian operators and properties of five basic metrics on SPD manifolds. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Basic geometries of SPD manifolds ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $S_{++}^{n}$ be the set of $n\\times n$ symmetric positive definite (SPD) matrices. As shown in [3], $S_{++}^{n}$ is an open submanifold of the Euclidean space $S^{n}$ of symmetric matrices. There are five kinds of popular Riemannian metrics on $S_{++}^{n}$ : Affine-Invariant Metric (AIM) [52], Log-Euclidean Metric (LEM) [3], Power-Euclidean Metrics (PEM) [22], Log-Cholesky Metric (LCM) [41], and Bures-Wasserstein Metric (BWM) [5]. Note that, when power equals 1, the PEM is reduced to the Euclidean Metric (EM). The standard LEM, AIM, and EM have been generalized into parametrized families of metrics. We define $\\mathbf{S}\\mathbf{T}=\\{(\\alpha,\\beta)\\in\\mathbb{R}^{2}\\,\\,|\\,\\operatorname*{min}(\\alpha,\\alpha+n\\beta)\\stackrel{*}{>}0\\}$ , and denote the $\\mathrm{O}(n)$ -invariant Euclidean metric on $S^{n}$ [63] as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle V,W\\rangle^{(\\alpha,\\beta)}=\\alpha\\langle V,W\\rangle+\\beta\\operatorname{tr}(V)\\operatorname{tr}(W),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(\\alpha,\\beta)\\in\\mathbf{ST}$ , and $\\langle\\cdot,\\cdot\\rangle$ is the Frobenius inner product. By $\\mathrm{O}(n)$ -invariant Euclidean metric on $S^{n}$ , Thanwerdas and Pennec [63] generalized AIM, LEM, and EM into two-parameters families of $\\mathrm{O}(n)$ -invariant metrics, i.e., $(\\alpha,\\beta)$ -AIM, $(\\alpha,\\beta)$ -LEM, and $(\\alpha,\\beta)$ -EM, with $(\\alpha,\\beta)\\in\\mathbf{ST}$ . We denote the metric tensor of $(\\alpha,\\beta)$ -AIM, $(\\alpha,\\beta)$ -LEM, $(\\alpha,\\beta)$ -EM, LCM, and BWM as $g^{(\\alpha,\\beta)}$ -AIM, $g^{(\\alpha,\\beta)-\\mathrm{LEM}}$ , $g^{(\\alpha,\\beta)}$ -EM, $g^{\\mathrm{LCM}}$ , and $g^{\\mathrm{BWM}}$ , respectively. ", "page_idx": 17}, {"type": "text", "text": "For any SPD points $P,Q\\in S_{++}^{n}$ and tangent vectors $V,W\\in T_{P}S_{++}^{n}$ , we follow the notations in Tab. 11 and further denote $\\tilde{V}=\\mathrm{Chol}_{\\ast,P}(V)$ , $\\tilde{W}=\\mathrm{Chol}_{*,P}(W)$ , $L=\\operatorname{Chol}P$ , and $K=\\mathrm{Chol}\\,Q$ . For parallel transportation under the BWM, we only present the case where $P,Q$ are commuting matrices, $i.e.,P\\overset{\\star}{=}U\\Sigma U^{\\top}$ and $Q=U\\Delta U^{\\top}$ . We summarize the associated Riemannian operators and properties in Tab. 12. Although there also exist other metrics on SPD manifolds [60, 61, 63], their lack of closed-form Riemannian operators makes them problematic to be applied in machine learning. ", "page_idx": 17}, {"type": "text", "text": "B.4 Basic geometry of rotation matrices ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "lBp2cda7sp/tmp/6ab182db2eac82cb5d38538726942abb0f53b3d0719896a001435a106362de71.jpg", "table_caption": ["Table 13: The associated Riemannian operators on Rotation matrices. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We denote R \u2208SO(n), A1, A2 \u2208TRSO(n), U \u2208Rn\u00d7n, skew(A) = A\u22122A\u22a4, and $\\mathcal{Q}(\\cdot)$ as the function return an orthogonal matrix by QR decomposition. There are two equivalent representations for the tangent vector on $\\mathrm{SO}(n)$ . In this paper, we use the Lie algebra representation, $i.e.\\bar{J}_{R}\\mathrm{SO}(n)\\cong$ ${\\mathfrak{s o}}(n)$ as the set of skew-symmetric matrices. We summarize all the necessary Riemannian ingredients for $\\mathrm{SO}(n)$ in Tab. 13. ", "page_idx": 18}, {"type": "text", "text": "For the specific case of $R\\in\\mathrm{SO}(3)$ , $R$ can be represented by the Euler angle and axis [26, Sec. 3.2]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\theta({\\cal R})=\\operatorname{arccos}\\left(\\frac{\\mathrm{tr}({\\cal R})-1}{2}\\right),}}\\\\ {{\\displaystyle\\omega\\left({\\cal R}\\right)=\\frac{1}{2\\sin\\left(\\theta\\left({\\cal R}\\right)\\right)}\\left(\\begin{array}{l}{{\\displaystyle R(3,2)-R(2,3)}}\\\\ {{\\displaystyle R(1,3)-R(3,1)}}\\\\ {{\\displaystyle R(2,1)-R(1,2)}}\\end{array}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Besides, the matrix logarithm on SO(3) can be calculated without decomposition [45, Ex. A.14]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(R)=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{~if~}\\theta(R)=0}\\\\ {\\frac{\\theta(R)}{2\\sin(\\theta(R))}\\left(R-R^{T}\\right),}&{\\mathrm{~otherwise~}}\\end{array}\\!\\!\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\theta$ is the Euler angle. Obviously, the matrix logarithm is related to the Euler angle and axis when $\\theta\\neq0$ . ", "page_idx": 18}, {"type": "text", "text": "C RMLR as a natural extension of the Euclidean MLR ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition C.1. When $\\mathcal{M}=\\mathbb{R}^{n}$ is the standard Euclidean space, the RMLR defined in Thm. 3.3 becomes the Euclidean MLR in Eq. (1). ", "page_idx": 18}, {"type": "text", "text": "Proof. On the standard Euclidean space $\\mathbb{R}^{n}$ , $\\operatorname{Log}_{y}x=x-y,\\forall x,y\\in\\mathbb{R}^{n}$ . Besides, the differential maps of left translation and parallel transportation are the identity maps. Therefore, given $x,p_{k}\\in\\mathbb{R}^{n}$ and $a_{k}\\in\\mathbb{R}^{n}/\\{0\\}\\cong T_{0}\\mathbb{R}^{n}/\\{0\\}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid x\\in\\mathbb{R}^{n})\\propto\\exp(\\langle\\mathrm{Log}_{p_{k}}\\,x,a_{k}\\rangle_{p_{k}}),}\\\\ &{\\qquad\\qquad\\qquad\\propto\\exp(\\langle x-p_{k},a_{k}\\rangle),}\\\\ &{\\qquad\\qquad\\quad\\propto\\exp(\\langle x,a_{k}\\rangle-b_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{k}=\\langle x,p_{k}\\rangle$ . ", "page_idx": 18}, {"type": "text", "text": "D Gyro SPSD MLR as special cases of our RMLR ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Gyro SPSD MLR [51] is derived by the product of the Grassmannian and SPD gyro spaces. This section will show that the gyro SPSD MLR is the special case of our RMLR on the product geometry of the SPSD manifold. We first review some necessary results about gyro SPSD MLR and then show the equivalence. ", "page_idx": 18}, {"type": "text", "text": "Following the notations in [51], we denote the Grassmannian with canonical metric under the projector and ONB perspective as $\\operatorname{Gr}(p,n)$ and ${\\widetilde{\\operatorname{Gr}}}(p,n)$ , respectively. The space of $n\\times n$ SPSD matrices with a fixed rank $p$ , denoted as $S_{n,p}^{+}$ , fo rms an SPSD manifold [7]. As shown in [7, 51], the SPSD manifold is a product space, $i.e.,\\stackrel{\\cdot}{S_{n,p}^{+}}\\cong\\widetilde{\\mathrm{Gr}}(p,n)\\times S_{++}^{p}$ . In other words, every $P\\in S_{n,p}^{+}$ can be decomposed as $P=U_{P}S_{P}U_{P}^{\\top}$ with $U_{p}\\in{\\widetilde{\\mathrm{Gr}}}(p,n)$ and $S_{P}\\in S_{++}^{p}$ . We further denote $S_{++}^{p,g}$ as the SPD manifold with metric $g$ , where $g$ could be AIM, LEM, and LCM. As shown in [51], the gyro space in Sn+,p can be defined by the product of gyro spaces of ${\\widetilde{\\operatorname{Gr}}}(p,n)$ and $S_{++}^{p,g}$ . By this product structure, Nguyen et al. [51] proposed the SPSD Pseudo-gyrodistance to a hyperplane. ", "page_idx": 18}, {"type": "text", "text": "iDn esftirnuitcitounr e Ds.p1a. c(e p laaren edse [f5in1e])d  aLset ${\\cal P},{\\cal W}\\in\\widetilde{\\mathrm{Gr}}(p,n)\\times{\\cal S}_{++}^{n,g}$ . Then hypergyroplanes $\\widetilde{\\mathrm{Gr}}(p,n)\\times S_{++}^{n,g}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nH_{W,P}^{p s d,g}=\\left\\{Q\\in\\widetilde{\\mathrm{Gr}}(p,n)\\times S_{++}^{n,g}:\\langle\\Theta_{p s d,g}P\\,\\oplus_{p s d,g}Q,W\\rangle^{p s d,g}=0\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\oplus_{p s d,g}$ and $\\langle,\\rangle^{p s d,g}$ are gyro addition and gyro inner product, which are defined in [51]. ", "page_idx": 18}, {"type": "text", "text": "Theorem D.2. (SPSD Pseudo-gyrodistance [51]) Let $W\\,=\\,\\left(U_{W},S_{W}\\right),P\\,=\\,(U_{P},S_{P})\\,,X\\,=$ $(U_{X},S_{X})\\in\\widetilde{\\mathrm{Gr}}(p,n)\\times S_{++}^{n,g}$ ,  farnodm $\\mathcal{H}_{A,P}^{p s d,g}$ iysp geirvgeynr obpylane in structure space $\\widetilde{\\mathrm{Gr}}(p,n)\\times S_{++}^{n,g}$ $X$ $\\mathcal{H}_{A,P}^{p s d,g}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\iota}\\left(X,H_{W,P}^{p s d,g}\\right)=\\frac{\\left|\\lambda\\left<\\left(\\widetilde{\\Theta}_{g r}U_{P}\\widetilde{\\Phi}_{g r}U_{X}\\right)\\left(\\widetilde{\\Theta}_{g r}U_{P}\\widetilde{\\Phi}_{g r}U_{X}\\right)^{T},U_{W}U_{W}^{T}\\right>^{g r}+\\left<\\Theta_{g}S_{P}\\,\\Phi_{g}\\,S_{X},S_{W}\\right>^{g}\\right|}{\\sqrt{\\lambda\\left(\\left\\|U_{W}U_{W}^{T}\\right\\|^{g r}\\right)^{2}+\\left(\\left\\|S_{W}\\right\\|^{g}\\right)^{2}}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lVert.\\rVert^{g r}$ and $\\lVert.\\rVert^{g}$ are the gyro norms on the Grassmann and SPD [51], and $\\langle,\\rangle^{g r}$ and $\\langle,\\rangle^{g}$ are gyro inner products [51]. $\\widetilde{\\Phi}_{g r}$ and $\\oplus_{g}$ are gyro additions on ${\\widetilde{\\operatorname{Gr}}}(p,n)$ and $S_{++}^{p,g}$ . ", "page_idx": 19}, {"type": "text", "text": "Denoting $g^{g r}$ as the canonical metric on ${\\widetilde{\\operatorname{Gr}}}(p,n)$ and $g$ as AIM, LEM, or LCM, we can prove that Thm. D.2 is the special case of our Thm.  3.2. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.3. Under the product metric $g^{p s d,g}=\\lambda g^{g r}\\times g,$ , the Riemannian hyperplane in Eq. (5) on the SPSD manifold equals the SPSD hypergyroplane in Def. D.1. Similarly, the Riemannian margin distance in Thm. 3.2 on the SPSD manifold equals SPSD Pseudo-gyrodistance in Thm. D.2. ", "page_idx": 19}, {"type": "text", "text": "Proof. Following the notations in Def. D.1 and Thm. D.2, we further denote $P\\,=\\,U_{P}S_{P}U_{P}^{\\top}$ , $Q=U_{Q}S_{Q}U_{Q}^{\\top}$ , $W=U_{W}S_{W}U_{W}^{\\top}$ , and $X=U_{X}S_{X}U_{X}^{\\top}$ with $U_{P},U_{Q},U_{W},U_{X}\\,\\in\\,\\widetilde{\\mathrm{Gr}}(p,n)$ and $S_{P},S_{Q},S_{W},S_{X}\\,\\in\\,S_{++}^{p,g}$ . $I_{p}$ is the $p\\times p$ identity matrix. $\\widetilde{I}_{p,n}\\,=\\,(I_{p},0)^{\\top}$ is the gyro identity on $\\widetilde{\\mathrm{Gr}}(p,n).\\,\\,\\,\\widetilde{\\Gamma}^{g r},\\,\\widetilde{\\mathrm{Log}}^{g r}$ , and $\\left\\langle,\\right\\rangle_{U_{p}}^{g r}$ are Riemannian parallel transport along a geodesic, logarithm and Riemannian metric at $U_{p}$ on ${\\widetilde{\\mathrm{Gr}}}(p,n).\\ \\Gamma^{g},\\,\\mathrm{Log}^{g}$ , and $\\langle,\\rangle_{S_{p}}^{g}$ are Riemannian parallel transport along a geodesic, logarithm and  Riemannian metric at $S_{p}$ on $S_{++}^{p,g}$ . $\\Gamma^{p s d,g}$ , $\\mathrm{Log}^{p s d,g}$ , and $\\langle,\\rangle_{X}^{p s d,g}$ are Riemannian parallel transport along a geodesic, logarithm and Riemannian metric at $X$ on $S_{n,p}^{+}\\cong\\widetilde{\\mathrm{Gr}}(p,n)\\stackrel{\\cdot}{\\times}S_{++}^{p}$ . ", "page_idx": 19}, {"type": "text", "text": "First, we show that the SPSD hypergyroplane equals our Riemannian hyperplane in Eq. (5). We have the following ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\Theta_{p s d,g}P\\oplus_{p s d,g}Q,W\\rangle^{p s d,g}}\\\\ &{\\stackrel{(1)}{=}\\lambda\\langle\\Theta_{g r}U_{P}\\oplus_{g r}U_{Q},U_{W}\\rangle^{g r}+\\langle\\Theta_{g}S_{P}\\oplus_{g}S_{Q},S_{W}\\rangle^{g}}\\\\ &{\\stackrel{(2)}{=}\\lambda\\langle\\mathrm{Log}_{U_{P}}^{g r}U_{Q},A_{U_{W}}\\rangle_{U_{P}}^{g r}+\\langle\\mathrm{Log}_{S_{P}}^{g r}\\,S_{Q},A_{S_{W}}\\rangle_{S_{P}}^{g}}\\\\ &{\\stackrel{(3)}{=}\\langle\\mathrm{Log}_{P}^{p s d,g}Q,\\tilde{A}\\rangle_{P}^{p s d,g}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\tilde{A}_{U_{W}}}&{{}=}&{\\widetilde{\\Gamma}_{\\widetilde{I}_{p,n}\\rightarrow U_{P}}^{g r}\\left(\\widetilde{\\mathrm{Log}_{\\widetilde{I}_{p,n}}}(U_{W})\\right)}\\end{array}$ , $\\begin{array}{r l r}{\\tilde{A}_{S_{W}}}&{{}=}&{\\Gamma_{I_{p}\\rightarrow S_{P}}^{g}\\left(\\mathrm{Log}_{I_{p}}^{g}(S_{W})\\right)}\\end{array}$ and $\\begin{array}{r l}{\\tilde{A}}&{{}=}\\end{array}$ $(\\tilde{A}_{U_{W}},\\tilde{A}_{S_{W}})\\in T_{\\underline{{{P}}}}S_{n,p}^{+}\\cong T_{U_{P}}\\widetilde{\\mathrm{Gr}}(p,P)\\otimes T_{S_{P}}S_{++}^{p,g}$ with $\\otimes$ as the Cartesian product. The above derivation comes from the follo wing. ", "page_idx": 19}, {"type": "text", "text": "(1) The definition of gyro addition, gyro inverse, and gyro inner product on the SPSD manifold [51, Sec. 3.3].   \n(2) The proof of [51, Prop. 3.2] indicates that similar results also hold on the Grassmannian. Combining Prop. 3.2 and its counterparts in the Grassmannian, one can obtain the equation. ", "page_idx": 19}, {"type": "text", "text": "(3) The Riemannian product geometry. ", "page_idx": 19}, {"type": "text", "text": "By the product geometry of the SPSD manifold, we can immediately get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{A}=(\\tilde{A}_{U_{W}},\\tilde{A}_{S_{W}})=\\Gamma_{I_{p,n}\\rightarrow P}^{p s d,g}\\left(\\mathrm{Log}_{I_{p,n}}^{p s d,g}\\left(W\\right)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $I_{p,n}=\\widetilde{I}_{p,n}\\widetilde{I}_{p,n}^{\\top}$ is the gyro identity on the SPSD manifold. ", "page_idx": 19}, {"type": "text", "text": "Next, we show the equivalence between SPSD pseudo-gyrodistance and our Riemannian margin distance: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{d}\\left(X,R_{\\mathrm{t},n}^{\\nu\\mu\\alpha}\\right)\\overset{\\mathrm{t}=}\\left(\\underset{\\left\\|\\begin{array}{l}{0}\\\\ {1}\\\\ {0}\\end{array}\\right\\|}\\frac{\\left\\{\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\left(\\mathcal{S}_{n}\\right)P_{\\alpha}\\left(\\Omega_{n}\\right)\\right\\}\\bigcap_{\\alpha}^{\\mu\\alpha\\beta}\\right|}{\\left\\{\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\left(\\mathcal{S}_{n}\\right)\\right\\}^{\\alpha\\beta-1}}\\right.}\\\\ {\\overset{\\mathrm{c}}{=}\\left.\\frac{\\left\\|\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\mathcal{S}_{n}\\right\\|_{\\alpha}^{\\alpha\\beta}\\biggr|}{\\left\\|\\mathcal{W}\\right\\|^{\\alpha\\beta}}}\\\\ {\\overset{\\mathrm{c}}{=}\\left.\\frac{\\left\\|\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\mathcal{S}_{n}\\right\\|_{\\alpha}^{\\alpha\\beta}\\biggr|}{\\left\\|\\log_{n}^{\\beta\\alpha}\\right\\|_{\\alpha}^{\\beta\\beta-1}}\\right\\}}\\\\ {\\overset{\\mathrm{c}}{=}\\frac{\\left\\|\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\left(\\mathcal{S}_{n}\\right)\\right\\|_{\\alpha}^{\\alpha\\beta}\\alpha_{1}}{\\left\\|\\Gamma_{t}^{\\alpha\\beta}-\\Gamma_{t}^{\\alpha\\beta}\\right\\|_{\\alpha}^{\\alpha\\beta}}\\left\\|\\right\\}_{\\gamma}^{\\alpha\\beta-1}}\\\\ {\\overset{\\mathrm{c}}{=}\\frac{\\left\\|\\bigcup_{\\alpha\\in\\mathbb{Z}^{n}}\\left(\\mathcal{S}_{n}\\right)\\right\\|_{\\alpha}^{\\alpha\\beta}\\alpha_{1}}{\\left\\|\\Gamma_{t}^{\\alpha\\beta}-\\Gamma_{t}^{\\alpha\\beta}\\right\\|_{\\alpha}^{\\alpha\\beta}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(1) The definition of gyro addition, gyro inverse, gyro inner product, and gyro norm on the SPSD manifold.   \n(2) Eq. (33).   \n(3) The definition of SPSD gyro norm [51].   \n(4) Riemannian parallel transportation maintains the norm of the tangent vector [21, Def. 3.1] (5) Eq. (34) ", "page_idx": 20}, {"type": "text", "text": "(6) Thm. 3.2 ", "page_idx": 20}, {"type": "text", "text": "Remark D.4. We make the following remark w.r.t. gyro and our MLR on the SPSD manifold. ", "page_idx": 20}, {"type": "text", "text": "1. Eq. (34) indicates that when generating $\\tilde{A}$ in our RMLR by parallel transporting a tangent vector $A\\in T_{I_{p,n}}S_{n,p}^{+}$ , $\\tilde{A}$ is the initial velocity of $W$ in Eq. (32).   \n2. Putting pseudo-gyrodistance and Riemannian margin distance into Eq. (4), one can get gyro MLR and our Riemannian MLR. Therefore, Thm. D.3 indicates the equivalence of the gyro MLR with our RMLR on the SPSD.   \n3. As $g$ are required to induce gyro structures, the metric $g$ in gyro SPSD MLR is confined within AIM, LEM, and LCM. However, our SPSD MLR can be the product space of the Grassmannian and SPD manifold under other metrics, such as BWM and PEM, as our framework does not require gyro structures. ", "page_idx": 20}, {"type": "text", "text": "E Theories on the deformed metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Limiting cases of the deformed metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Thanwerdas and Pennec [59] generalized $(\\alpha,\\beta)$ -AIM into three-parameters families of metrics by power deformation, $i.e.,(\\theta,\\alpha,\\beta)$ -AIM. The family of $(\\theta,\\alpha,\\beta)$ -AIM comprises $(\\alpha,\\beta)$ -AIM for $\\theta=1$ and approaches $(\\alpha,\\beta)$ -LEM with $\\theta\\rightarrow0$ [59]. ", "page_idx": 20}, {"type": "text", "text": "Chen et al. [17] extended LCM and $(\\alpha,\\beta)$ -LEM into power-deformed metrics, denoted as $(\\theta,\\alpha,\\beta)$ -LEM and $\\theta$ -LCM. The authors show that $(\\theta,\\alpha,\\beta)$ -LEM is equal to $(\\alpha,\\beta)$ -LEM, and $\\theta$ -LCM interpolates between $\\tilde{g}$ -LEM $\\;\\theta\\rightarrow0\\;$ ) and LCM $\\theta=1$ ), with $\\tilde{g}$ -LEM defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle V_{1},V_{2}\\rangle_{P}=\\frac{1}{2}\\langle\\widetilde{V_{1}},\\widetilde{V_{2}}\\rangle-\\frac{1}{4}\\langle\\mathbb{D}(\\widetilde{V_{1}}),\\mathbb{D}(\\widetilde{V_{2}})\\rangle,\\forall V_{i}\\in T_{P}\\mathcal{S}_{++}^{n},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\widetilde{V}_{i}\\,=\\,\\log_{*,P}(V_{i})$ with $\\log_{*,P}$ as the differential map of matrix logarithm, and $\\mathbb{D}(V_{i})$ is a diagonal matrix consisting of the diagonal elements of $V_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "Thanwerdas and Pennec [61] identified the Alpha-Procrustes metric [43] with power-deformed BWM, denote as $2\\theta$ -BWM. Similarly, $2\\theta$ -BWM becomes BWM with $\\theta=0.5$ [61]. We further show the limiting case of $2\\theta$ -BWM under $\\theta\\rightarrow0$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition E.1. $2\\theta$ -BWM tends to be $({\\textstyle\\frac{1}{4}},0)$ -LEM with $\\theta\\rightarrow0$ . ", "page_idx": 21}, {"type": "text", "text": "Before starting the proof, we first recall a well-known property of deformed metrics [61]. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.2. Let $\\scriptstyle{\\frac{1}{\\theta^{2}}}\\phi_{\\theta}^{*}g$ be the deformed metric on SPD manifolds pulled back from $g$ by the matrix power $\\phi_{\\theta}$ and scaled by $\\displaystyle\\frac{1}{\\theta^{2}}$ . Then when $\\theta$ tends to $O_{3}$ , for all $P\\in S_{++}^{n}$ and all $V\\in T_{P}S_{++}^{n}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\frac{1}{\\theta^{2}}\\phi_{\\theta}^{*}g)_{P}(V,V)\\to g_{I}(\\log_{*,P}(V),\\log_{*,P}(V)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we present our proof for the limiting cases of deformed metrics. ", "page_idx": 21}, {"type": "text", "text": "Proof of Prop. E.1. First, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\ng_{I}^{\\mathrm{BWM}}(V,V)=\\frac{1}{4}\\langle V,V\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lem. E.2, we have the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{g_{P}^{2\\theta\\mathrm{{-}B W M}}(V,V)\\xrightarrow{\\theta\\rightarrow0}g_{I}^{\\mathrm{BWM}}\\left(\\mathrm{log}_{\\ast,P}(V),\\mathrm{log}_{\\ast,P}(V)\\right)}}\\\\ &{=\\frac{1}{4}\\langle\\mathrm{log}_{\\ast,P}(V),\\mathrm{log}_{\\ast,P}(V)\\rangle}\\\\ &{=g_{P}^{(\\frac{1}{4},0)\\mathrm{-}\\mathrm{LEM}}\\left(V,V\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E.2 Proof of the properties of the deformed metrics (Tab. 2) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we prove the properties presented in Tab. 2. We first present a useful lemma and then present our detailed proof. This lemma will be useful in the proof of our SPD MLRs as well. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.3. Supposing a Riemannian manifold $\\{\\mathcal{M},g\\}$ and a positive real scalar $a\\,>\\,0$ , the scaling metric ag over $\\mathcal{M}$ shares the same Riemannian logarithmic & exponential maps and parallel transportation with $g$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since the Christoffel symbols of $a g$ are identical to those of $g$ , the geodesics and parallel transportation under both $a g$ and $g$ remain unchanged. The equivalence of geodesics implies that the Riemannian exponential maps are the same for $a g$ and $g$ . As the inverse of the Riemannian exponential maps, the Riemannian logarithm maps under $a g$ and $g$ are also identical. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "According to Lem. E.3, the geodesic completeness is independent of the scaling factor $a>0$ . By the definition of ${\\mathrm{O}}(n).$ -, left-, right-, and bi-invariance, these invariant properties are also independent of the scaling factor $a>0$ . Without loss of generality, we will omit the scaling factor in the following proof. ", "page_idx": 21}, {"type": "text", "text": "Proof. Firstly, we prove $\\mathrm{O}(n)$ -invariance of $(\\theta,\\alpha,\\beta)$ -LEM, $(\\theta,\\alpha,\\beta)$ -EM, $(\\theta,\\alpha,\\beta)$ -AIM, and $2\\theta$ - BWM. Since the differential of $\\phi_{\\theta}$ is $\\mathrm{O}(n)$ -equivariant, and $(\\alpha,\\beta)$ -LEM, $(\\alpha,\\beta)$ -EM, $(\\alpha,\\beta)$ -AIM, and BWM are $\\mathrm{O}(n)$ -invariant [63], $\\mathrm{O}(n)$ -invariance are thus acquired. ", "page_idx": 21}, {"type": "text", "text": "Next, we focus on geodesic completeness. It can be easily proven that Riemannian isometries preserve geodesic completeness. On the other hand, $(\\alpha,\\beta)$ -LEM, $(\\alpha,\\beta)$ -AIM, and LCM are geodesically complete [63, 41]. As a direct corollary, geodesic completeness can be obtained since $\\phi_{\\theta}$ is a Riemannian isometry. ", "page_idx": 21}, {"type": "text", "text": "Finally, we deal with Lie group invariance. Similarly, it can be readily proved that Lie group invariance is preserved under isometries. LCM, LEM, and $(\\alpha,\\beta)$ -AIM are Lie group bi-invariant [41], bi-invariant [3], and left-invariant [62]. As an isometric pullback metric from the standard LEM [63], $(\\alpha,\\beta)$ -LEM is, therefore, Lie group bi-invariant. As pullback metrics, $(\\theta,\\alpha,\\beta)$ -LEM, $(\\theta,\\alpha,\\beta)$ -AIM, and $\\theta$ -LCM are therefore bi-invariant, left-invariant, and bi-invariant, respectively. ", "page_idx": 21}, {"type": "text", "text": "F Computational details on the SPD MLR under power-deformed BWM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Matrix square roots in the SPD MLR under power-deformed BWM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the case of MLRs induced by $2\\theta$ -BWM, computing square roots like $(B A)^{\\frac{1}{2}}$ and $(A B)^{\\frac{1}{2}}$ with $B,A\\in S_{++}^{n}$ poses a challenge. Eigendecomposition cannot be directly applied since $B A$ and $A B$ are no longer symmetric, let alone positive definitity. Instead, we use the following formulas to compute these square roots [43]: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(B A)^{\\frac{1}{2}}=B^{\\frac{1}{2}}(B^{\\frac{1}{2}}A B^{\\frac{1}{2}})^{\\frac{1}{2}}B^{-\\frac{1}{2}}\\,\\,\\mathrm{and}\\,\\,(A B)^{\\frac{1}{2}}=[(B A)^{\\frac{1}{2}}]^{\\top},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the involved square roots can be computed using eigendecomposition or singular value decomposition (SVD). ", "page_idx": 22}, {"type": "text", "text": "F.2 Numerical stability of the SPD MLR under power-deformed BWM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us first explain why we abandon parallel transportation on the SPD MLR derived from $2\\theta$ -BWM.   \nThen, we propose our numerically stable methods for computing the SPD MLR based on $2\\theta$ -BWM. ", "page_idx": 22}, {"type": "text", "text": "F.2.1 Instability of parallel transportation under power-deformed BWM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As discussed in Thm. 3.3, there are two ways to generate A\u02dc in SPD MLR: parallel transportation and Lie group translation. However, parallel transportation under $2\\theta$ -BWM could cause numerical problems. W.l.o.g., we focus on the standard BWM as $2\\theta$ -BWM is isometric to the BWM. ", "page_idx": 22}, {"type": "text", "text": "Although the general solution of parallel transportation under BWM is the solution of an ODE, for the case of parallel transportation starting from the identity matrix, we have a closed-form expression [63]: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Gamma_{I\\rightarrow P}(V)=U\\left[\\sqrt{\\frac{\\sigma_{i}+\\sigma_{j}}{2}}\\left[U^{\\top}V U\\right]_{i j}\\right]U^{\\top},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\boldsymbol{P}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\intercal}$ is the eigendecomposition of $P\\,\\in\\,S_{++}^{n}$ . There would be no problem in the forward computation of Eq. (41). However, during backpropagation (BP), Eq. (41) would require the BP of eigendecomposition, involving the calculation of $^1\\!/(\\sigma_{i}\\!-\\!\\sigma_{j})$ [33, Prop. 2]. When $\\sigma_{i}$ is close to $\\sigma_{j}$ , the BP of eigendecomposition could be problematic. ", "page_idx": 22}, {"type": "text", "text": "F.2.2 Numerically stable methods for the SPD MLR under power-deformed BWM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To bypass the instability of parallel transportation under BWM, we use Lie group left translation to generate $\\tilde{A}$ in MLRs induced from $2\\theta$ -BWM. However, there is another problem that could cause instability. The computation of the Riemannian metric of $2\\theta$ -BWM requires solving the Lyapunov operator, $i.e.,\\mathcal{L}_{P}[V]\\bar{P}+P\\mathcal{L}_{P}[V]=V$ . Under the case of symmetric matrices, the Lyapunov operator can be obtained by eigendecomposition: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P}[V]=U\\left[\\frac{V_{i j}^{\\prime}}{\\sigma_{i}+\\sigma_{j}}\\right]_{i,j}U^{\\top},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $V\\in{\\cal S}^{n}$ , $U V^{\\prime}U^{\\top}=V$ , and $P=U\\Sigma U^{\\top}$ is the eigendecomposition of $P\\in S_{++}^{n}$ . Similar with Eq. (41), the BP of Eq. (42) requires $1\\big/(\\sigma_{i}\\!-\\!\\sigma_{j})$ , undermining the numerical stability. ", "page_idx": 22}, {"type": "text", "text": "To remedy this problem, we proposed the following formula to stably compute the BP of Eq. (42). ", "page_idx": 22}, {"type": "text", "text": "Proposition F.1. For all $P\\in S_{++}^{n}$ and all $V\\in S^{n}$ , we denote the Lyapunov equation as ", "page_idx": 22}, {"type": "equation", "text": "$$\nX P+P X=V,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $X=\\mathcal{L}_{P}[V]$ . Given the gradient $\\frac{\\partial L}{\\partial X}$ of loss $L$ w.r.t. $X$ , then the BP of the Lyapunov operator can be computed by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial L}{\\partial V}=\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}],}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial P}=-X\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}]-\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}]X,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathcal{L}_{P}[\\cdot]$ can be computed by Eq. (42). ", "page_idx": 22}, {"type": "text", "text": "Proof. Differentiating both sides of Eq. (43), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\mathrm{d}\\,X P+X\\,\\mathrm{d}\\,P+\\mathrm{d}\\,P X+P\\,\\mathrm{d}\\,X=\\mathrm{d}\\,V,}\\\\ &{\\Longrightarrow\\,\\mathrm{d}\\,X P+P\\,\\mathrm{d}\\,X=\\mathrm{d}\\,V-X\\,\\mathrm{d}\\,P-\\mathrm{d}\\,P X,}\\\\ &{\\Longrightarrow\\,\\mathrm{d}\\,X=\\mathcal{L}_{P}[\\mathrm{d}\\,V-X\\,\\mathrm{d}\\,P-\\mathrm{d}\\,P X].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Besides, easy computations show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{P}[V]:W=V:\\mathcal{L}_{P}[W],\\forall W,V\\in\\mathcal{S}^{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\cdot:\\cdot$ denotes the standard Frobenius inner product. ", "page_idx": 23}, {"type": "text", "text": "Then we have the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial L}{\\partial X}:\\mathrm{d}X=\\frac{\\partial L}{\\partial X}:\\mathcal{L}_{P}[\\mathrm{d}\\,V-X\\,\\mathrm{d}\\,P-\\mathrm{d}\\,P X],}\\\\ {\\displaystyle\\Longrightarrow\\frac{\\partial L}{\\partial X}:\\mathrm{d}\\,X=\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}]:\\mathrm{d}\\,V+\\left(-X\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}]-\\mathcal{L}_{P}[\\frac{\\partial L}{\\partial X}]X\\right):\\mathrm{d}\\,P.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark F.2. Eq. (42) needs to be computed in the Lyapunov operator\u2019s forward and backward process. Therefore, in the forward process, we can save the intermediate matrices $U$ and $K$ with $\\begin{array}{r}{\\bar{K}_{i,j}=\\left[\\frac{1}{\\sigma_{i}+\\sigma_{j}}\\right]_{i,j}}\\end{array}$ , and then use them to compute the backward process efficiently. ", "page_idx": 23}, {"type": "text", "text": "G Implementation details and additional experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section offers additional details on the experiments of SPD and Lie MLRs. ", "page_idx": 23}, {"type": "text", "text": "G.1 Additional details and experiments on the SPD MLRs G.1.1 Basic layers in SPDNet and TSMNet ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "SPDNet [30] is the most classic SPD neural network. SPDNet mimics the conventional densely connected feedforward network, consisting of three basic building blocks: ", "page_idx": 23}, {"type": "equation", "text": "$$\nS^{k}=U^{k-1}\\operatorname*{max}(\\Sigma^{k-1},\\epsilon I_{n})U^{k-1\\top},{\\mathrm{~with~}}S^{k-1}=U^{k-1}\\Sigma^{k-1}U^{k-1\\top},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\operatorname*{max}()$ is element-wise maximization. BiMap and ReEig mimic transformation and nonlinear activation, while LogEig maps SPD matrices into the tangent space at the identity matrix for classification. ", "page_idx": 23}, {"type": "text", "text": "SPDNetBN [10] further proposed Riemannian batch normalization based on AIM: ", "page_idx": 23}, {"type": "text", "text": "SPD domain-specific momentum batch normalization (SPDDSMBN) [37] is an improved version of SPDNetBN. Apart from controlling the mean, it can also control variance. The key operation in SPDDSMBN of controlling mean and variance is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Gamma_{I\\to G}\\circ\\Gamma_{\\mathfrak{G}\\to I}(S_{i})^{\\frac{\\nu}{\\bar{\\nu}+\\varepsilon}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathfrak{G}$ and $\\bar{v}$ are Riemannian mean and variance. Inspired by [75], during the training stage, SPDDSMBN generates running means and running variances for training and testing with distinct momentum parameters. Besides, it sets $\\mathfrak{G}$ and $\\bar{v}$ as the running mean and running variance w.r.t. training for training and the ones w.r.t. testing for testing. SPDDSMBN also applies domain-specific techniques [13], keeping multiple parallel BN layers and distributing observations according to the associated domains. To crack cross-domain knowledge, $v$ is uniformly learned across all domains, and $G$ is set to be the identity matrix. TSMNet [37] adopted SPDDSMBN to solve domain adaptation in EEG classification. ", "page_idx": 23}, {"type": "text", "text": "In the above models, the Euclidean MLR in the co-domain of matrix logarithm (matrix logarithm $^+$ $\\mathrm{FC}+$ softmax) is used for classification. Following the terminology in [16], we call this classifier as LogEig MLR. The LogEig MLR is the Euclidean classifier in the tangent space at the identity, which might distort the innate geometry of the SPD manifold. ", "page_idx": 23}, {"type": "text", "text": "G.1.2 Datasets and preprocessing ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Radar2: This dataset [10] consists of 3,000 synthetic radar signals. Following the protocol in [10], each signal is split into windows of length 20, resulting in 3,000 SPD covariance matrices of $20\\times20$ equally distributed in 3 classes. ", "page_idx": 24}, {"type": "text", "text": "$\\mathbf{H}\\mathbf{D}\\mathbf{M}\\mathbf{0}\\mathbf{5}^{3}$ : This dataset [44] contains 2,273 skeleton-based motion capture sequences executed by various actors. Each frame consists of 3D coordinates of 31 joints of the subjects, and each sequence can be, therefore, modeled by a $93\\times93$ covariance matrix. Following the protocol in [10], we trim the dataset down to 2086 sequences scattered throughout 117 classes by removing some under-represented classes. ", "page_idx": 24}, {"type": "text", "text": "$\\mathbf{Hinss2021^{4}}$ : This dataset [28] is a recent competition dataset consisting of EEG signals for mental workload estimation. The dataset is used for two types of experiments: inter-session and inter-subject, which are modeled as domain adaptation problems. Recently, geometry-aware methods have shown promising performance in EEG classification [74, 37]. We choose the SOTA method, TSMNet [37], as our baseline model on this dataset. We follow the Python implementation5 [37] to carry out preprocessing. In detail, the python package MOABB [35] and MNE [24] are used to preprocess the datasets. The applied steps include resampling the EEG signals to $250/256\\ \\mathrm{Hz}$ , applying temporal filters to extract oscillatory EEG activity in the 4 to $36\\,\\mathrm{Hz}$ range, extracting short segments $\\mathrm{(\\leq3s)}$ associated with a class label, and finally obtaining $40\\times40$ SPD covariance matrices. ", "page_idx": 24}, {"type": "text", "text": "Disease [2]: It represents a disease propagation tree, simulating the SIR disease transmission model [2], with each node representing either an infection or a non-infection state. ", "page_idx": 24}, {"type": "text", "text": "Cora [54]: It is a citation network where nodes represent scientific papers in the area of machine learning, edges are citations between them, and node labels are academic (sub)areas. ", "page_idx": 24}, {"type": "text", "text": "Pubmed [46]: This is a standard benchmark describing citation networks where nodes represent scientific papers in the area of medicine, edges are citations between them, and node labels are academic (sub)areas. ", "page_idx": 24}, {"type": "text", "text": "For the Disease, Cora and Pubmed datasets, we follow [76] to model features into $S_{++}^{3}$ ", "page_idx": 24}, {"type": "text", "text": "G.1.3 Implementation details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "SPDNet [30] and TSMNet [37]: We follow the official Pytorch code of SPDNetBN6 and TSMNet7 to implement our experiments. To evaluate the performance of our intrinsic classifiers, we substitute the LogEig MLR in SPDNet and TSMNet with our SPD MLRs. We implement our SPD MLRs induced from five parameterized metrics. On the Radar and HDM05 datasets, the learning rate is $1e^{-2}$ , and the batch size is 30. On the Hinss2021 dataset, following [37], the learning rate is $1e^{-3}$ with a $1e^{-4}$ weight decay, and batch size is 50. The maximum training epoch is 200, 200, and 50, respectively. We use the standard-cross entropy loss as the training objective and optimize the parameters with the Riemannian AMSGrad optimizer [4]. ", "page_idx": 24}, {"type": "text", "text": "RResNet [36]: We focus on the AIM-based RResNet, and use the official code8 and suggested network settings to implement the experiments on the RResNet. We conduct 10-fold and 5-fold experiments on the HDM05 and NTU datasets. Since RResNet is developed based on SPDNet, we use the same learning settings with the SPDNet for the action recognition task, and borrow the best $(\\theta,\\alpha,\\beta)$ from Tab. 4 for our SPD MLRs under the RResNet backbone. ", "page_idx": 24}, {"type": "text", "text": "SPDGCN [76]: We use the official code9 and the suggested network settings in [76]. Note that the SPDGCN with SPD MLR remains the same network settings as the vanilla SPDGCN. Tab. 14 presents the hyperparameters $(\\theta,\\alpha,\\beta)$ on different datasets. ", "page_idx": 24}, {"type": "table", "img_path": "lBp2cda7sp/tmp/0dbf511279f46d0e664485f2422ac7964d33b88d190fbe7af44255b768f9c2ed.jpg", "table_caption": ["Table 14: $(\\theta,\\alpha,\\beta)$ of SPD MLRs on the SPDGCN backbone. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Network Architectures: We denote the network architecture as $[d_{0},d_{1},\\cdots\\,,d_{L}]$ , where the dimension of the parameter in the $i$ -th BiMap layer (App. G.1.1) is $d_{i}\\times d_{i-1}$ . For SPDNet, we also validate our SPD MLRs under different network architectures on the Radar and HDM05 datasets. The network architectures on the Radar dataset are [20, 16, 8] for the 2-block configuration and [20, 16, 14, 12, 10, 8] for the 5-block configuration, while on the HDM05 dataset, the network architectures are [93, 30] for 1-block, [93. 70, 30] for 2-block, and [93, 70, 50, 30] for 3-block. For TSMNet, the 1-block architecture is [40,20]. ", "page_idx": 25}, {"type": "text", "text": "Scoring Metrics: In line with the previous work [10, 37, 76, 36], we use balanced accuracy, the average recall across classes, as the scoring metric for the Hinss2021 dataset, and accuracy for other datasets. On the Hinss2021 dataset, models are fit and evaluated with a randomized leave $5\\%$ of the sessions (inter-session) or subjects (inter-subject) out cross-validation (CV) scheme. On other datasets, K-fold experiments are carried out with randomized initialization and split, ", "page_idx": 25}, {"type": "text", "text": "G.1.4 Hyper-parameters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We implement the SPD MLRs induced by not only five standard metrics, i.e.,LEM, AIM, EM, LCM, and BWM, but also five families of parameterized metrics. Therefore, in our SPD MLRs, we have a maximum of three hyper-parameters, $i.e.,\\theta,\\alpha,\\beta$ , where $(\\alpha,\\beta)$ are associated with $\\mathrm{O}(n)$ -invariance and $\\theta$ controls deformation. For $(\\alpha,\\beta)$ in $(\\theta,\\alpha,\\beta)$ -LEM, $(\\theta,\\alpha,\\beta)$ -AIM, and $(\\theta,\\alpha,\\beta)$ -EM, recalling Eq. (24), $\\alpha$ is a scaling factors, while $\\beta$ measures the relative significance of traces. As scaling is less important [59], we set $\\alpha\\,=\\,1$ . As for the value of $\\beta$ , we select it from a predefined set: $\\{1,1/n,1/\\bar{n}^{2},0,-1/n+\\epsilon,-1/n^{2}\\}$ , where $n$ is the dimension of input SPD matrices in SPD MLRs. The purpose of including $\\epsilon\\in\\mathbb{R}_{+}$ is to ensure $\\mathrm{O}(n)$ -invariance $\\mathbf{\\Pi}^{\\prime}(\\alpha,\\beta)\\in\\mathbf{ST})$ ). These chosen values for $\\beta$ allow for amplifying, neutralizing, or suppressing the trace components, depending on the characteristics of the datasets. For the deformation factor $\\theta$ , we roughly select its value around its deformation boundary, i.e.,[0.25,1.5] for $(\\theta,\\alpha,\\beta)$ -AIM, [0.5,1.5] for $\\theta$ -LCM, [0.25,1.5] and $(\\theta,\\alpha,\\beta)$ -EM, [0.25,0.75] for $2\\theta$ -BWM. The details values are listed in Tab. 15. ", "page_idx": 25}, {"type": "table", "img_path": "lBp2cda7sp/tmp/095581264c43c05c529e77888bf3784bed315abaa2ae17a22276d7a9d98b64a8.jpg", "table_caption": ["Table 15: Candidate values for hyper-parameters in SPD MLRs "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.1.5 Model efficiency ", "text_level": 1, "page_idx": 25}, {"type": "table", "img_path": "lBp2cda7sp/tmp/ab09024507e668a038911ec0c2997e7cbea94fb3b5c5dd26323a1d338eadf351.jpg", "table_caption": ["Table 16: Training efficiency (s/epoch). "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "We adopt the deepest architectures, namely [20, 16, 14, 12, 10, 8] for the Radar dataset, [93, 70, 50, 30] for the HDM05 dataset, and [40, 20] for the Hinss2021 dataset. For simplicity, we focus on the SPD MLRs induced by standard metrics, i.e.,AIM, EM, LEM, BWM, and LCM. The average training time (in seconds) per epoch is reported in Tab. 16. Generally, when the number of classes is small (e.g.,3 in the Radar and Hinss2021 datasets), our SPD MLRs only bring minor additional training time compared to the baseline LogEig MLR. However, when dealing with a larger number of classes (e.g.,117 classes in the HDM05 dataset), there could be some inefficiency caused by our SPD MLRs. This is because each class requires an SPD parameter, and each parameter might require matrix decomposition in the forward or optimization processes during training. Nonetheless, the SPD MLRs induced by EM or LCM generally achieve comparable efficiency with the vanilla LogEig MLR. This is due to the fast computation of their Riemannian operators, making them efficient choices for tasks with a larger number of classes. This result highlights the flexibility of our framework and its applicability to various scenarios. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "lBp2cda7sp/tmp/55714e1fb7955746d351a651c94b7ef3b868a25e4046a415fc0b8aa6e1cffa7f.jpg", "img_caption": ["Figure 4: Visualization of 10-fold average accuracy of SPDNet with different SPD MLRs on the Radar and HDM05 datasets. The error bar denotes the standard deviation. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.1.6 Visualization ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We visualize the 10-fold average results of SPDNet with different classifiers on the Radar and HDM05 datasets. We focus on the deepest architectures, i.e.,. [20,16,14,12,10,8] for the Radar dataset, and [93,70,50,30] for the HDM05 dataset. Note that we only report the SPD MLR with the best hyper-parameters $(\\theta,\\alpha,\\beta)$ . The figures are presented in Fig. 4. All the results are sourced from Tabs. 3 and 4. ", "page_idx": 26}, {"type": "text", "text": "G.2 Additional details and experiments on the Lie MLR G.2.1 Basic layers in LieNet ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "LieNet [31] is the most classic neural network on rotation matrices. The latent space of LieNet is the Lie group $\\mathrm{SO^{N}(3)}=\\mathrm{SO(3)}\\times\\mathrm{SO(3)}\\cdots\\times\\mathrm{SO(3)}$ , $i.e.,R=(R_{1},\\cdot\\cdot\\cdot,R_{N})\\in\\mathrm{SO}^{N}(3)$ . The group and manifold structures on $\\mathrm{SO}^{N}(3)$ are defined by product spaces. For instance, $R^{1}\\odot R^{2}=$ $\\left(R_{1}^{1}\\bar{R}_{1}^{2},\\cdot\\cdot\\cdot,R_{N}^{1}R_{N}^{2}\\right)$ . There are three basic layers in LieNet: ", "page_idx": 26}, {"type": "text", "text": "where $\\Theta(\\cdot)$ is the Euler angle, and $(n_{i},m_{i})$ are two indexes. The RotMap and RotPooling layers mimic the convolution and pooling layers, while the LogMap layer map rotation matrices into tangent space for classification. In the official Matlab implementation, the LogMap layer is implemented as the Euler axis-angle representation. The classification is performed by Euler axis-angle $+\\,\\mathrm{FC}+$ Softmax. As the axis-angle is an equivalent representation of matrix logarithm, we call this classifier as LogEig MLR as well. This classifier is, therefore, also non-intrinsic. ", "page_idx": 26}, {"type": "text", "text": "In LieNet, each rotation feature has a shape of [num, frame, 3, 3], where num and frame denote spatial and temporal dimensions. The RotPooling layer is performed either along spatial or temporal dimensions, while the RotMap layer is performed along spatial dimensions, $i.\\bar{e}.,\\bar{W}^{k}$ with a size of [num, 3, 3]. ", "page_idx": 26}, {"type": "text", "text": "G.2.2 Datasets and preprocessing ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For a fair comparison, we follow LieNet to use G3D [6] and HDM05 datasets to validate our Lie MLR. ", "page_idx": 27}, {"type": "text", "text": "G3D[6]: This dataset consists of 663 sequences of 20 different gaming actions. Each sequence is recorded by 3D locations of 20 joints (i.e., 19 bones). ", "page_idx": 27}, {"type": "text", "text": "HDM05: We trim it down by removing some under-represented sequences, resulting in 2,326 sequences scattered throughout 122 classes. Following [30], we use the code of [67] to represent each skeleton sequence as a point on the Lie group $\\mathrm{SO}^{\\bar{N}\\times T}(3)$ , where $N$ and $T$ denote spatial and temporal dimensions. As preprocessed in [30], we set $T$ as 100 and 16 for each sequence on the G3D and HDM05 datasets, respectively. ", "page_idx": 27}, {"type": "text", "text": "G.2.3 Implementation details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "LieNet: Note that the official code of LieNet10 is developed by Matlab. We follow the open-sourced Pytorch code11 to implement our experiments. To reproduce LieNet more faithfully, we made the following modifications to this Pytorch code. We re-code the LogMap and RotPooling layers to make them consistent with the official Matlab implementation. In addition, we also extend the existing Riemannian optimization package geoopt [4] into SO(3) to allow for Riemannian version of SGD, ADAM, and AMSGrad on SO(3), which is missing in the current package. However, we find that SGD is the best optimizer for LieNet. Therefore, we use SGD as our optimizer during the experiments. ", "page_idx": 27}, {"type": "text", "text": "Lie MLR: We use our Lie MLR to replace the axis-angle classifier in LieNet and call the resulting network LieNet+LieMLR. To alleviate the computational burden, we set each $P_{k}$ as the dimension of [num, 3, 3], where num is the spacial dimension of the input of the Lie MLR layer. In other words, $P_{k}$ is shared in the temporal dimension. We adopt Pytroch3D [53] to calculate the matrix logarithm. Due to the instabilities of pytorch3d.transforms.so3_log_map, we use pytorch3d.transforms.matrix_to_axis_angle first to calculate the quaternion axis and angle, and then convert this representation into matrix logarithm12. ", "page_idx": 27}, {"type": "text", "text": "Training Details: Following [31], we focus on the 3Blocks and 2Blocks architecture for the G3D and HDM05 datasets, which are the suggested architectures for these two datasets. The learning rate is $1e^{-2}$ on both datasets, and we further set weight decay as $1e^{-5}$ on the G3D dataset. For LieNet and LieNet+LieMLR, we use torch.nn.utils.clip_grad_norm_ for gradient clipping with a clipping factor of 5. The clipping is imposed to the dimensionality reduction weight in the final FC linear on LieNet, or, accordingly, $A=\\{A_{1},\\cdot\\cdot\\cdot,A_{k}\\}$ in the Lie MLR layer on LieNet+LieMLR. ", "page_idx": 27}, {"type": "text", "text": "Scoring Metrics: For the G3D dataset, following LieNet [31], we adopt a 10-fold cross-subject test setting, where half the subjects are used for training and the other half are employed for testing. For the HDM05 dataset, following [31], we randomly select half of the sequences for training and the rest for testing. Due the instabilities of LieNet, we conduct 20-fold experiments and select the best 10 folds to evaluate the performance. ", "page_idx": 27}, {"type": "text", "text": "G.3 Hardware ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "All experiments use an Intel Core i9-7960X CPU with 32GB RAM and an NVIDIA GeForce RTX 2080 Ti GPU. ", "page_idx": 27}, {"type": "text", "text": "H Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Proof of Thm. 3.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Thm. 3.2. Let us first solve $Y^{*}$ in Eq. (8), which is the solution to the following constrained optimization problem: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{Y}\\left(\\frac{\\langle\\mathrm{Log}_{P}\\,Y,\\mathrm{Log}_{P}\\,S\\rangle_{P}}{\\|\\,\\mathrm{Log}_{P}\\,Y\\|_{P},\\|\\,\\mathrm{Log}_{P}\\,S\\|_{P}}\\right)\\quad\\mathrm{s.t.}\\langle\\mathrm{Log}_{P}\\,S,\\tilde{A}\\rangle_{P}=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "10https://github.com/zhiwu-huang/LieNet   \n11https://github.com/hjf1997/LieNet   \n12https://github.com/facebookresearch/pytorch3d/issues/188 ", "page_idx": 27}, {"type": "text", "text": "Note that Eq. (61) is well-defined due to the existence of the Riemannian logarithm. Although, Eq. (61) is normally non-convex, Eq. (61) and Eq. (8) can be reduced to a Euclidean problem: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\tilde{Y}}{\\operatorname*{max}}\\frac{\\langle\\tilde{Y},\\tilde{S}\\rangle_{P}}{\\|\\tilde{Y}\\|_{P}\\|\\tilde{S}\\|_{P}}\\quad\\mathrm{s.t.}\\langle\\tilde{Y},\\tilde{A}\\rangle_{P}=0,}\\\\ {d(S,\\tilde{H}_{\\tilde{A},P})=\\sin(\\angle S P Y^{*})\\|\\tilde{S}\\|_{P},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\tilde{Y}=\\mathrm{Log}_{P}\\,Y$ and $\\tilde{S}=\\mathrm{Log}_{P}\\,S$ . ", "page_idx": 28}, {"type": "text", "text": "Let us first discuss Eq. (62). Denote the solution of Eq. (62) as ${\\tilde{Y}}^{*}$ . Note that ${\\tilde{Y}}^{*}$ is not necessarily unique. Note that $\\mathrm{Exp}_{P}$ is only well-defined locally. More precisely, $\\mathrm{Exp}_{P}$ is well-defined in an open ball $\\mathrm{B}_{\\epsilon}(0)$ centered at $0\\in T_{P}\\mathcal{M}$ . Therefore, ${\\tilde{Y}}^{*}$ might not be in $\\mathrm{B}_{\\epsilon}(0)$ . In this case, we can scale ${\\tilde{Y}}^{*}$ into $\\mathrm{B}_{\\epsilon}(0)$ , and the scaled ${\\tilde{Y}}^{*}$ is still the maximizer of Eq. (62). Therefore, w.l.o.g., we assume $\\tilde{Y}^{*}\\in\\mathrm{B}_{\\epsilon}(0)$ . ", "page_idx": 28}, {"type": "text", "text": "Putting ${\\tilde{Y}}^{*}$ into Eq. (63), Eq. (63) is reduced to the distance to the hyperplane $\\langle\\tilde{Y},\\tilde{A}\\rangle_{P}=0$ in the Euclidean space $\\{T_{P}\\mathcal{M},\\langle\\cdot,\\cdot\\rangle_{P}\\}$ , which has a closed-form solution: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(S,\\tilde{H}_{\\tilde{A},P})=\\frac{\\vert\\langle\\tilde{S},\\tilde{A}\\rangle_{P}\\vert}{\\Vert\\tilde{A}\\Vert_{P}},}\\\\ &{\\qquad\\qquad=\\frac{\\vert\\langle\\mathrm{Log}_{P}\\,S,\\tilde{A}\\rangle_{P}\\vert}{\\Vert\\tilde{A}\\Vert_{P}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "H.2 Proof of Thm. 3.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof for Thm. 3.3 . Putting the margin distance (Eq. (10)) into Eq. (4), we have the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S)\\propto\\exp\\left(\\mathrm{sign}(\\langle\\tilde{A}_{k},\\mathrm{Log}_{P_{k}}(S)\\rangle_{P_{k}})\\|\\tilde{A}_{k}\\|_{P_{k}}d(S,\\tilde{H}_{\\tilde{A}_{k},P_{k}})\\right),}\\\\ &{\\qquad\\qquad=\\exp\\left(\\mathrm{sign}(\\langle\\tilde{A}_{k},\\mathrm{Log}_{P_{k}}(S)\\rangle_{P_{k}})\\|\\tilde{A}_{k}\\|_{P_{k}}\\frac{\\left\\vert\\left\\langle\\mathrm{Log}_{P_{k}}(S),\\tilde{A}_{k}\\right\\rangle_{P_{k}}\\right\\vert}{\\left\\|\\tilde{A}_{k}\\right\\|_{P_{k}}}\\right),}\\\\ &{\\qquad=\\exp\\left(\\langle\\mathrm{Log}_{P_{k}}\\,S,\\tilde{A}_{k}\\rangle_{P_{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "H.3 Proof of Prop. 4.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof for Prop. 4.1 . The Riemannian metric $(\\alpha,\\beta)$ -EM at $I$ is ", "page_idx": 28}, {"type": "equation", "text": "$$\ng_{I}^{(\\alpha,\\beta)\\cdot\\mathrm{EM}}(V,V)=\\langle V,V\\rangle^{(\\alpha,\\beta)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lem. E.2, we have the following ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{P}^{(\\theta,\\alpha,\\beta)\\cdot\\mathtt{E M}}(V,V)\\xrightarrow{\\theta\\rightarrow0}g_{I}^{(\\alpha,\\beta)\\cdot\\mathtt{E M}}\\big(\\mathrm{log}_{\\ast,P}(V),\\mathrm{log}_{\\ast,P}(V)\\big)}\\\\ &{\\hphantom{=\\langle}=\\langle\\mathrm{log}_{\\ast,P}(V),\\mathrm{log}_{\\ast,P}(V)\\rangle^{(\\alpha,\\beta)}}\\\\ &{\\hphantom{=\\langle}=g_{P}^{(\\alpha,\\beta)\\cdot\\mathrm{LEM}}\\,(V,V)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "H.4 Proof of Thm. 4.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As the five families of metrics presented in Thm. 4.2 are pullback metrics, we first present a general result regarding Riemannian MLRs under pullback metrics. ", "page_idx": 28}, {"type": "text", "text": "Lemma H.1 (Riemannian MLRs under Pullback Metrics). Supposing $\\{\\mathcal{N},g\\}$ is a Riemannian manifold and $\\dot{\\phi}:\\mathcal{M}\\to\\mathcal{N}$ is a diffeomorphism between manifolds, the Riemannian MLR by parallel transportation $(E q.\\,(11)+E q.$ . (12)) on $\\mathcal{M}$ under $\\tilde{g}=\\phi^{\\ast}g$ can be obtained by $g$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left[\\langle\\mathrm{Log}_{P_{k}}S,\\tilde{\\Gamma}_{Q\\to P_{k}}A_{k}\\rangle_{P_{k}}\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\exp\\left[\\langle\\mathrm{Log}_{\\phi(P_{k})}\\,\\phi(S),\\tilde{A}_{k}\\rangle_{\\phi(P_{k})}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\tilde{A}_{k}\\,=\\,\\Gamma_{\\phi(Q)\\to\\phi(P_{k})}\\phi_{*,Q}(A_{k})$ with $A_{k}\\;\\in\\;T_{Q}{\\mathcal{M}}$ ,L\u02dcog, $\\tilde{\\Gamma}$ are Riemannian logarithm and parallel transportation under $\\tilde{g}$ , and Log, $\\Gamma$ are the counterparts under $g$ . ", "page_idx": 29}, {"type": "text", "text": "Furthermore, if N has a Lie group operation $\\odot_{i}$ , $\\mathcal{M}$ could be endowed with a Lie group structure $\\tilde{\\odot}$ by $f$ . The Riemannian MLR by left translation $(E q.\\,(11)+E q.$ . (13)) on $\\mathcal{M}$ under $\\tilde{g}$ and $\\tilde{\\odot}$ can be calculated by $g$ and $\\odot$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left[\\langle\\mathrm{Log}_{P_{k}}S,\\tilde{L}_{\\tilde{R}_{k}*,Q}A_{k}\\rangle_{P_{k}}\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\exp\\left[\\langle\\mathrm{Log}_{\\phi(P_{k})}\\,\\phi(S),\\tilde{A}_{k}\\rangle_{\\phi(P_{k})}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\tilde{A}_{k}=L_{R_{k}*,\\phi(Q)}\\circ\\phi_{*,Q}(A_{k}),\\,\\tilde{R}_{k}=P_{k}\\tilde{\\odot}Q_{\\tilde{\\odot}}^{-1}$ , $R_{k}=\\phi(P)\\odot\\phi(Q)_{\\odot}^{-1}$ , and $\\tilde{L}_{P_{k}\\tilde{\\odot}Q_{\\tilde{\\odot}}^{-1}}$ is the left translation under $\\tilde{\\odot}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof for Lem. H.1. Before starting, we should point out that since $\\phi$ is a diffeomorphism, $\\tilde{\\odot}$ and $\\tilde{g}$ are indeed well defined, and $\\{\\mathcal{M},\\tilde{g}\\}$ forms a Riemannian manifold and $\\{\\mathcal{M},\\tilde{\\odot}\\}$ forms a Lie group. We denote $\\phi_{*}^{-1}$ as the differential of $\\phi^{-1}$ . We first focus on the Riemannian MLR by parallel transportation: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})}\\\\ &{\\propto\\exp(\\tilde{g}_{P_{k}}(\\mathrm{L}\\tilde{\\mathrm{og}}_{P_{k}}S,\\tilde{\\mathrm{r}}_{Q\\to P_{k}}A_{k}))}\\\\ &{=\\exp\\left[g_{\\phi(P_{k})}\\left(\\phi_{*,P_{k}}\\circ\\phi_{*,\\phi(P_{k})}^{-1}\\operatorname{Log}_{\\phi(P_{k})}\\phi(S),\\phi_{*,P_{k}}\\circ\\phi_{*,\\phi(P_{k})}^{-1}\\Gamma_{\\phi(Q)\\to\\phi(P_{k})}\\phi_{*,Q}(A_{k})\\right)\\right]}\\\\ &{=\\exp\\left[g_{\\phi(P_{k})}(\\mathrm{Log}_{\\phi(P_{k})}\\,\\phi(S),\\Gamma_{\\phi(Q)\\to\\phi(P_{k})}\\phi_{*,Q}(A_{k}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the case of the Riemannian MLR by left translation, we first note that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{L}_{\\tilde{R}_{k}}=\\phi^{-1}\\circ L_{\\phi(P_{k})\\odot\\phi(Q)_{\\odot}^{-1}}\\circ\\phi.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, the associated differential is: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{L}_{\\tilde{R}_{k}*}=\\phi_{*}^{-1}\\circ L_{\\phi(P_{k})\\odot\\phi(Q)_{\\odot}^{-1}*}\\circ\\phi_{*}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Putting Eq. (75) in Eq. (71), we can obtain the results. ", "page_idx": 29}, {"type": "text", "text": "Now, we apply Lem. H.1 to derive the expressions of our SPD MLRs presented in Thm. 4.2. For our cases of SPD MLRs, we set $Q=I$ . For simplicity, we will omit the subscript $k$ for $P_{k}$ and $A_{k}$ . We will first derive the expressions of SPD MLRs under $(\\theta,\\alpha,\\beta)$ -LEM, $\\theta$ -LCM, $(\\theta,\\alpha,\\beta)$ -EM, and $(\\theta,\\alpha,\\beta)$ -AIM, as they are sourced from Eq. (70). Then we will proceed to present the expression of MLR under $2\\theta$ -BWM, which is sourced from Eq. (72). According to Lem. E.3, the scaled metric $a g$ shares the same Riemannian operators as $g$ . We will use this fact throughout the following proof. ", "page_idx": 29}, {"type": "text", "text": "Proof of Thm. 4.2. For simplicity, we abbreviate $\\phi_{\\theta}$ as $\\phi$ during the proof. Note that for $2\\theta$ -BWM, $\\phi$ should be understood as $\\phi_{2\\theta}$ . We first show $\\phi(I)$ and the differential map $\\phi_{*,I}$ , which will be frequently required in the following proof: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\phi(I)=I,}}\\\\ {{\\phi_{*,I}(A)=\\theta A,\\forall A\\in T_{I}S_{++}^{n}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denoting $\\phi:\\{S_{++}^{n},\\tilde{g}\\}\\,\\to\\,\\{S_{++}^{n},g\\}$ , then the SPD MLR under $\\tilde{g}$ by parallel transportation with $Q=I$ is ", "page_idx": 29}, {"type": "equation", "text": "$$\np(y=k\\mid S\\in\\mathcal{M})=\\exp\\left[g_{\\phi(P)}(\\mathrm{Log}_{\\phi(P)}\\,\\phi(S),\\Gamma_{I\\to\\phi(P)}\\theta A)\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we begin to prove the five SPD MLRs one by one. ", "page_idx": 30}, {"type": "text", "text": "$(\\alpha,\\beta)$ -LEM: As shown by Chen et al. [20], the standard LEM is the pullback metric from the Euclidean space $S^{n}$ . Similarly, $(\\alpha,\\beta)$ -LEM is also a pullback metric: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\{S_{++}^{n},g^{(\\alpha,\\beta)\\cdot\\mathrm{LEM}}\\}\\xrightarrow{\\log}\\{S^{n},g^{(\\alpha,\\beta)}\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Eq. (70), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})=\\exp\\left[\\langle\\log(S)-\\log(P),\\log_{*,I}(A)\\rangle^{(\\alpha,\\beta)}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\exp\\left[\\langle\\log(S)-\\log(P),A\\rangle^{(\\alpha,\\beta)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "$\\theta$ -LCM: Simple computations show that $\\theta$ -LCM is the scaled pullback metric of standard Euclidean metric in the Euclidean space of lower triangular matrices $\\textstyle{\\mathcal{L}}^{n}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\{S_{++}^{n},\\theta^{2}g^{\\theta\\cdot\\mathrm{LCM}}\\}\\xrightarrow{\\phi}\\{S_{++}^{n},g^{\\mathrm{LCM}}\\}\\xrightarrow{\\mathrm{Chol}}\\{\\mathcal{L}_{+}^{n},g^{\\mathrm{CM}}\\}\\xrightarrow{\\mathrm{Dlog}}\\{S^{n},g^{\\mathrm{E}}\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $g^{\\mathrm{E}}$ is the standard Frobenius inner product, and $g^{\\mathrm{CM}}$ is the Cholesky metric on the Cholesky space $\\mathcal{L}_{+}^{n}$ [41]. Denoting $\\zeta=\\mathrm{Dlog}\\circ\\mathrm{Chol}\\,\\circ\\phi$ , then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\zeta_{*,I}(A)=\\theta\\left(\\lfloor A\\rfloor+{\\frac{1}{2}}\\mathbb{D}(A)\\right),\\forall A\\in T_{I}S_{++}^{n}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar with the case of $(\\theta,\\alpha,\\beta)$ -LEM, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left[\\frac{1}{\\theta^{2}}\\langle\\zeta(S)-\\zeta(P),\\zeta_{*,I}A\\rangle\\right],}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\exp\\left[\\frac{1}{\\theta}\\langle\\lfloor\\tilde{K}\\rfloor-\\lfloor\\tilde{L}\\rfloor+\\left[\\mathrm{Dlog}(\\mathbb{D}(\\tilde{K}))-\\mathrm{Dlog}(\\mathbb{D}(\\tilde{L}))\\right],\\lfloor A\\rfloor+\\frac{1}{2}\\mathbb{D}(A)\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\tilde{K}=\\operatorname{Chol}(S^{\\theta})$ , $\\tilde{L}=\\mathrm{Chol}(P^{\\theta}),\\mathbb{D}(\\tilde{K})$ is a diagonal matrix with diagonal elements from $\\tilde{K}$ , and $\\lfloor\\tilde{K}\\rfloor$ is a strictly lower triangular matrix from $\\tilde{K}$ . ", "page_idx": 30}, {"type": "text", "text": "$(\\theta,\\alpha,\\beta)$ -EM: Let $\\begin{array}{r}{\\eta=\\frac{1}{|\\theta|}\\phi}\\end{array}$ . Simple computation shows that $(\\theta,\\alpha,\\beta)$ -EM is the pullback metric of $(\\alpha,\\beta)$ -EM: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\{S_{++}^{n},g^{(\\theta,\\alpha,\\beta)\\cdot\\mathrm{EM}}\\}\\xrightarrow{\\eta}\\{S_{++}^{n},g^{(\\alpha,\\beta)\\cdot\\mathrm{EM}}\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Besides, we have the following for $\\eta$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\eta_{*,I}(A)=\\operatorname{sgn}\\theta A,\\forall A\\in T_{I}S_{++}^{n}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "According to Eq. (70), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{p(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left[\\langle\\eta(S)-\\eta(P),\\mathrm{sgn}(\\theta)A\\rangle\\right],}\\\\ &{}&{=\\exp\\left[\\frac{1}{\\theta}\\langle S^{\\theta}-P^{\\theta},A\\rangle^{(\\alpha,\\beta)}\\right].\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "$(\\theta,\\alpha,\\beta)$ -AIM: Putting $g^{(\\alpha,\\beta)-\\mathrm{AIM}}$ into Eq. (78), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y=k\\mid S\\in\\mathcal{M})\\propto\\exp\\left[\\frac{1}{\\theta^{2}}g_{\\phi(P)}^{(\\alpha,\\beta)\\cdot\\mathrm{AIM}}(P^{\\frac{\\theta}{2}}\\log(P^{-\\frac{\\theta}{2}}S^{\\theta}P^{-\\frac{\\theta}{2}})P^{\\frac{\\theta}{2}},P^{\\frac{\\theta}{2}}\\theta A P^{\\frac{\\theta}{2}})\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\exp\\left[\\frac{1}{\\theta}\\langle\\log(P^{-\\frac{\\theta}{2}}S^{\\theta}P^{-\\frac{\\theta}{2}}),A\\rangle^{(\\alpha,\\beta)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "$2\\theta$ -BWM: We first simplify Eq. (72) under the cases of SPD manifolds and then proceed to focus on the case of $g=g^{\\mathrm{BWM}}$ . Denote $\\phi:\\{S_{++}^{n},\\tilde{g},\\tilde{\\odot}\\}\\rightarrow\\{S_{++}^{n},g,\\odot\\}$ , where the Lie group operation $\\odot$ [62] is defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\nS_{1}\\odot S_{2}=L_{1}S_{2}L_{1}^{T},\\forall S_{1},S_{2}\\in S_{++}^{n},\\ \\mathrm{with}\\ L_{1}=\\mathrm{Chol}(S_{1}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that $I$ is the identity element of $\\{S_{++}^{n},\\odot\\}$ , and for any $S\\in S_{++}^{n}$ , the differential map of the left translation $L_{S}$ under $\\odot$ is ", "page_idx": 31}, {"type": "equation", "text": "$$\nL_{S*,Q}(V)=L V L^{\\top},\\forall Q\\in{\\cal S}_{++}^{n},\\forall V\\in T_{Q}{\\cal S}_{++}^{n},\\ \\mathrm{with}\\ L=\\mathrm{Chol}(S).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the induced Lie group $\\{S_{++}^{n},\\Tilde{\\odot}\\}$ , the left translation $\\tilde{L}_{P\\tilde{\\odot}I_{\\tilde{\\odot}}^{-1}}$ under $\\tilde{\\odot}$ is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{L}_{P\\tilde{\\odot}I_{\\tilde{\\odot}}^{-1}}=\\phi^{-1}\\circ L_{\\phi(P)\\odot\\phi(I)_{\\odot}^{-1}}\\circ\\phi,}\\\\ &{\\qquad\\qquad=\\phi^{-1}\\circ L_{P^{2\\theta}}\\circ\\phi.\\quad(\\phi(P)\\odot\\phi(I)_{\\odot}^{-1}=P^{2\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The associated differential at $I$ is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{L}_{P\\tilde{\\varsigma}I_{\\tilde{\\varsigma}}^{-1},\\tau}(A)=\\phi_{*,\\phi(P)}^{-1}\\circ L_{P^{2\\theta}*,\\phi(I)}\\circ\\phi_{*,I}(A),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\theta\\phi_{*,\\phi(P)}^{-1}(\\bar{L}A\\bar{L}^{\\top}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\bar{L}=\\mathrm{Chol}(P^{2\\theta})$ . Then the SPD MLRs under $\\tilde{g}$ and $\\tilde{\\odot}$ by left translation is ", "page_idx": 31}, {"type": "equation", "text": "$$\np(y=k\\mid S\\in\\mathcal{M})=\\exp\\left[2\\theta g_{\\phi(P)}\\left(\\mathrm{Log}_{\\phi(P)}\\,\\phi(S),\\bar{L}A\\bar{L}^{\\top}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Setting $g=g^{\\mathrm{BWM}}$ (We omit the scaling factor.), we obtain the SPD MLR under $2\\theta$ -BWM: ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{p(y=k\\mid S\\in{\\mathcal{M}})=\\exp\\left[2\\theta\\cdot{\\frac{1}{4\\theta^{2}}}g_{\\phi(P)}^{\\mathbf{BWM}}\\left(\\mathrm{Log}_{\\phi(P)}^{\\mathbf{BWM}}\\phi(S),{\\bar{L}}A{\\bar{L}}^{\\top}\\right)\\right],}\\\\ &{\\qquad\\qquad\\qquad=\\exp\\left[{\\frac{1}{4\\theta}}\\langle(P^{2\\theta}S^{2\\theta})^{\\frac{1}{2}}+(S^{2\\theta}P^{2\\theta})^{\\frac{1}{2}}-2P^{2\\theta},{\\mathcal{L}}_{P^{2\\theta}}({\\bar{L}}A{\\bar{L}}^{\\top})\\rangle\\right].}\\end{array}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "H.5 Proof of Lem. 5.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lem. 5.1. During this proof, we use the ambient representation of tangent vectors. We only need to prove the following: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Gamma_{Q\\rightarrow P}=L_{P Q^{-1}*,Q},\\forall P,Q\\in\\mathrm{SO}(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Given rotation matrices $P,Q$ and a tangent vector $H\\in T_{Q}\\mathrm{SO}(n)$ , the parallel transportation [8, Tab. 1] is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Gamma_{Q\\rightarrow P}(H)=P Q^{\\top}H=P Q^{-1}H.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the other hand, given a curve $c(t)$ over $\\mathrm{SO}(n)$ , satisfying $c(0)=Q$ and $c^{\\prime}(0)=H$ , the differential of the left translation $L_{P Q^{-1}}$ at $Q$ is ", "page_idx": 31}, {"type": "equation", "text": "$$\nL_{P Q^{-1}*,Q}(H)=\\left.\\frac{d P Q^{-1}c(t)}{d t}\\right|_{t=0}=P Q^{-1}H,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 31}, {"type": "text", "text": "H.6 Proof of Thm. 5.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Thm. 5.2. Lem. 5.1 indicates we can use either parallel transportation or group translation. Putting the associated expressions from Tab. 13 into Eq. $(11)+\\mathrm{Eq}$ . (12), one can directly obtain the results. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our abstract and introduction (Sec. 1) accurately reflect the paper\u2019s theoretical and empirical contributions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We discuss the limitations specifically in App. A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Assumptions are clearly claimed in each theorem, and all the proofs are presented in App. H. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Implementation details are discussed in App. G. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All datasets (Apps. G.1.2 and G.2.2) are publicly available. The code will be released after the review. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: In App. G, we present the experimental details for reproducing the results. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Mean, STD, and max of K-fold results are presented in Sec. 6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Hardware is mentioned in App. G.3. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] Justification: No ethic issue. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 36}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Original papers and datasets have been cited. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Code will be released after the review. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]