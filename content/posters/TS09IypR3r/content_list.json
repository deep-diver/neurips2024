[{"type": "text", "text": "MetaCURL: Non-stationary Concave Utility Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bianca Marin Moreno Margaux Br\u00e9g\u00e8re Pierre Gaillard Nadia Oudjane Inria\u2217 Sorbonne Universit\u00e9\u2021 Inria\u2217 EDF R&D\u2020 EDF R&D\u2020 EDF R&D\u2020 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We explore online learning in episodic Markov decision processes on non-stationary environments (changing losses and probability transitions). Our focus is on the Concave Utility Reinforcement Learning problem (CURL), an extension of classical RL for handling convex performance criteria in state-action distributions induced by agent policies. While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations. Despite recent solutions to classical CURL, none address non-stationary MDPs. This paper introduces MetaCURL, the first CURL algorithm for non-stationary MDPs. It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework. The key hurdle is partial information due to MDP uncertainty. Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), the algorithm achieves optimal dynamic regret without prior knowledge of MDP changes. Unlike approaches for RL, MetaCURL handles adversarial losses. We believe our approach for managing non-stationarity with experts can be of interest to the RL community. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the task of learning in an episodic Markov decision process (MDP) with a finite state space $\\mathcal{X}$ , a finite action space $\\boldsymbol{\\mathcal{A}}$ , episodes of length $N$ , and a probability transition kernel $p:=(p_{n})_{n\\in[N]}$ such that for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , $p_{n}(\\cdot|x,a)\\in S_{\\mathcal{X}}$ . For any finite set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , we denote by $\\boldsymbol{S_{B}}$ the simplex induced by this set, and by $|\\beta|$ its cardinality. For all $d\\in\\mathbb{N}$ we let $[d]:=\\{1,\\ldots,d\\}$ . At each time step $n$ , an agent in state $x_{n}$ chooses an action $a_{n}\\sim\\pi_{n}(\\cdot|x_{n})$ by means of a policy, and moves to the next state $x_{n+1}\\sim p_{n+1}(\\cdot|x_{n},a_{n})$ , inducing a state-action distribution sequence $\\mu^{\\pi,p}:=(\\mu_{n}^{\\pi,p})_{n\\in[N]}$ , where $\\mu_{n}^{\\pi,p}\\in S_{\\mathcal{X}\\times\\mathcal{A}}$ for all $n\\in[N]$ . ", "page_idx": 0}, {"type": "text", "text": "In many applications of learning in episodic MDPs, an agent aims at finding an optimal policy $\\pi$ maximizing/minimizing a concave/convex function $F$ of its state-action distribution, known as the Concave Utility Reinforcement Learning (CURL) problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in(S_{A})^{\\chi\\times N}}F(\\mu^{\\pi,p}).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "CURL extends reinforcement learning (RL) from linear to convex losses. Many machine learning problems can be written in the CURL setting, including: RL, where for a loss function $\\ell$ , $F(\\mu^{\\pi,p})=$ $\\overline{{\\langle\\ell,\\mu^{\\pi,p}\\rangle}}$ ; pure RL exploration [28], where $\\bar{F}(\\mu^{\\pi,p})=\\bar{\\langle\\mu^{\\pi,p},\\log(\\mu^{\\pi,p})\\rangle}$ ; imitation learning [26, 35] and apprenticeship learning [55, 1], where $F(\\mu^{\\pi,p})=D_{g}(\\mu^{\\pi,p},\\mu^{*})$ , with $D_{g}$ representing a Bregman divergence induced by a function $g$ and $\\mu^{*}$ being a behavior to be imitated; certain instances of meanfield control [7], where $F(\\mu^{\\pi,p})=\\langle\\ell(\\mu^{\\pi,p}),\\mu^{\\pi,p}\\rangle$ ; mean-field games with potential rewards [34]; among others. The CURL problem alters the additive structure inherent in standard RL, invalidating the classical Bellman equations, requiring the development of new algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Most of existing works on CURL focus on stationary environments [28, 57, 58, 5, 56, 25, 12, 11], where both the objective function $F$ and the probability transition kernel $p$ remain the same across episodes. However, in practical scenarios, environments are rarely stationary. The work of [39] is the first to address online CURL with objective functions that can change arbitrarily between episodes, also known as adversarial losses [19]. However, their work assumes stationary probability kernels and presents results in terms of static regret (performance comparable to an optimal policy). In non-stationary scenarios, it is more relevant to minimize dynamic regret\u2014the gap between the learner\u2019s total loss and that of any policy sequence (see Eq. (5) for formal definition). In this work we address this problem by introducing the first algorithm for CURL handling adversarial objective functions and non-stationary probability transitions, achieving near-optimal dynamic regret. ", "page_idx": 1}, {"type": "text", "text": "High-level idea. Our approach, called MetaCURL, draws inspiration from the online learning literature. In online learning [9], non-stationarity is often managed by running multiple black-box algorithm instances from various starting points and dynamically selecting the best performer using an \"expert\" algorithm. This strategy has demonstrated effectiveness in settings with complete information [29, 59, 47, 33]. With MetaCURL, we extend this concept to decision-making in MDPs. Unlike classical online learning, the main challenge faced is uncertainty. We assume that the probability transition kernel in each episode has a known deterministic structure but is affected by an external noise with unknown distribution, placing us in a setting with only partial information (see Section 2 for more details). The learner is then unable to observe the agent\u2019s loss under policies other than the one played. ", "page_idx": 1}, {"type": "text", "text": "MetaCURL is a general algorithm that can be applied with any black-box algorithm with low dynamic regret in near-stationary environments. CURL approaches suitable as black-boxes rely on parametric algorithms that would require prior knowledge of the MDP changes to tune their learning rate. MetaCURL also addresses this challenge by simultaneously running multiple learning rates and weighting them in\u221a direct proportion to their empirical performance. MetaCURL achieves optimal regret of order $\\tilde{O}\\big(\\sqrt{\\Delta^{\\pi^{*}}T}+\\operatorname*{min}{\\{\\sqrt{\\Delta_{\\infty}^{p}T},\\;T^{2/3}(\\Delta^{p})^{\\bar{1}/3}\\}}\\big)$ , where $\\Delta_{\\infty}^{p}$ and $\\Delta^{p}$ represent the frequency and magnitude of changes of the probability transition kernel respectively, and $\\Delta^{\\pi^{*}}$ is the magnitude of changes of the policy sequence we compare ourselves with in dynamic regret (see Eqs. (6) and (7) for formal definitions). MetaCURL does not require previous knowledge of the degree of non-stationarity of the environment, and can handle adversarial losses. To ensure completeness, we show that Greedy MD-CURL from [39] fulfills the requirements to serve as a black-box algorithm. This is the first dynamic regret analysis for a CURL approach. ", "page_idx": 1}, {"type": "text", "text": "Comparisons. Without literature on non-stationary CURL, we review non-stationary RL approaches. Most methods [24, 13, 45, 17, 20, 40, 21] typically rely on prior knowledge of the MDP\u2019s nonstationarity degree, while MetaCURL does not. Let $\\dot{\\Delta}_{\\infty}^{l}$ and $\\Delta^{l}$ represent the frequency and magnitude of change in the RL loss function, respectively1. Recently, [54] achieved a regret of $\\tilde{O}\\big(\\operatorname*{min}{\\{\\sqrt{(\\Delta_{\\infty}^{p}+\\Delta_{\\infty}^{l})T},\\;T^{2/3}(\\Delta^{p}+\\Delta^{l})^{1/3}\\}}\\big)$ , a near-optimal result as demonstrated by [40], without requiring prior knowledge of the environment\u2019s variation. However, this regret bound is tied to changes in loss functions, making it ineffective against adversarial losses. In contrast, rather than depending on the magnitude of variation of the loss function, MetaCURL\u2019s bound depends on the magnitude of variation of the policy sequence we use for comparison in dynamic regret. This allows it to handle adversarial losses, and to compare against policies with a more favorable bias-variance trade-off, which may not align w\u221aith the optimal policies for each loss. In addition, we improve this dependency by paying it as $\\sqrt{\\Delta\\pi^{*}T}$ instead of $(\\Delta^{\\pi^{*}})^{1/3}T^{2/3}$ . We summarize comparisons in Table 1. ", "page_idx": 1}, {"type": "table", "img_path": "TS09IypR3r/tmp/41fc95e3418aa03dc4c72c28cfab1c27bb2e051efd173704c9010f0279d363ce.jpg", "table_caption": ["Table 1: Comparisons of our results with the state-of-the-art in non-stationary RL. Here, $\\Delta_{\\infty}^{p}$ , $\\Delta^{p}$ and $\\Delta^{\\pi^{*}}$ are defined in (6) and (7\u221a); and $\\Delta_{\\infty}^{l}$ and $\\Delta^{l}$ measure the RL loss function variations1. We introduce $D_{T}(\\Delta_{\\infty},\\Delta):=\\operatorname*{min}\\{\\sqrt{\\Delta_{\\infty}T},\\ T^{2/3}\\Delta^{1/3}\\}.$ . "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Other related works. The studies by [43, 42] examine the difference between optimizing the objective over infinite trials and the expectation of the objective over a single trial, challenging the traditional CURL formulation in Eq. (1). Here, we retain the classic formulation to align with existing CURL research. Other works on RL with nonlinear objective functions are [46, 16] focusing on rewards over trajectories rather than individual states. In addition to non-stationarity, there is a series of works on RL with adversarial losses but stationary probability transitions, with results only on static regret [48, 30, 18, 50, 32, 14]. Another line of research is known as corruption-robust RL. It differs from non-stationary MDPs in that it assumes a ground-truth MDP and measures adversary malice by the degree of ground-truth corruption [31, 38, 10, 60, 53]. ", "page_idx": 2}, {"type": "text", "text": "Contributions. We resume our main contributions below: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce MetaCURL, the first algorithm for non-stationary CURL. Under the framewor\u221ak described in Se\u221action 2, MetaCURL achieves the optimal dynamic regret bound of order $\\tilde{O}\\big(\\sqrt{\\Delta^{\\pi^{*}}T}+\\operatorname*{min}\\{\\sqrt{\\Delta_{\\infty}^{p}T},T^{2/3}(\\Delta^{p})^{1/3}\\}\\big)$ , without requiring previous knowledge of the degree of non-stationarity of the MDP. MetaCURL handles full adversarial losses and improves the dependency of the regret on the total variation of policies. MetaCURL is the first adaptation of Learning with Expert Advice (LEA) to deal with uncertainty in non-stationary MDPs. \u2022 We also establish the first dynamic regret upper bound for an online CURL algorithm in a nearly stationary environment, which can serve as a black-box routine for MetaCURL. ", "page_idx": 2}, {"type": "text", "text": "Notations. Let $\\|\\cdot\\|_{1}$ be the $L_{1}$ norm, and for all $v:=(v_{n})_{n\\in[N]}$ , such that $v_{n}\\in\\mathbb{R}^{\\mathcal{X}\\times\\mathcal{A}}$ we define $\\begin{array}{r}{\\|v\\|_{\\infty,1}:=\\operatorname*{sup}_{1\\leq n\\leq N}\\|v_{n}\\|_{1}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "2 General framework: non-stationary CURL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When an agent plays a policy $\\pi:=(\\pi_{n})_{n\\in[N]}$ in an episodic MDP with probability transition $p$ it induces a state-action distribution sequence (also called the occupancy-measure [61]), which we denote by $\\mu^{\\pi,p}:=\\,(\\mu_{n}^{\\pi,p})_{n\\in[N]}$ , with $\\mu_{n}^{\\pi,p}\\,\\in\\,{\\cal S}_{\\mathcal{X}\\times\\mathcal{A}}$ . It can be calculated recursively for all $(x,a)\\in\\mathcal{X}$ and $n\\in[N]$ by taking $\\bar{\\mu}_{0}^{\\pi,p}(x,a)=\\mu_{0}(x,a)$ fixed, and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{n}^{\\pi,p}(x,a)=\\sum_{(x^{\\prime},a^{\\prime})\\in\\mathcal{X}\\times A}\\mu_{n-1}^{\\pi,p}(x^{\\prime},a^{\\prime})p_{n}(x|x^{\\prime},a^{\\prime})\\pi_{n}(a|x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Offilne CURL. The classic CURL optimization problem in Eq. (1) considers minimizing a function $F:(S_{\\mathcal{X}\\times\\mathcal{A}})^{N}\\rightarrow\\mathbb{R}$ , here defined as $\\begin{array}{r}{F(\\mu):=\\sum_{n=1}^{N}f_{n}(\\mu_{n})}\\end{array}$ with $f_{n}$ a convex function over $\\mu_{n}$ with values in $[0,1]$ , across all policies that induce $\\mu^{\\pi,p}$ . Note that $F$ is not convex on the policy $\\pi$ . To convexify the problem, we define the set of state-action distributions satisfying the Bellman flow of a MDP with transition kernel $p$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\mu_{0}}^{p}:=\\biggl\\{\\mu\\big|\\,\\sum_{a^{\\prime}\\in\\mathcal{A}}\\mu_{n}(x^{\\prime},a^{\\prime})=\\sum_{x\\in\\mathcal{X},a\\in\\mathcal{A}}p_{n}(x^{\\prime}|x,a)\\mu_{n-1}(x,a)\\ ,\\forall x^{\\prime}\\in\\mathcal{X},\\forall n\\in[N]\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For any $\\mu\\in\\mathcal{M}_{\\mu_{0}}^{p}$ , there exists a strategy $\\pi$ such that $\\boldsymbol{\\mu}^{\\pi,p}=\\boldsymbol{\\mu}$ . It suffices to take $\\pi_{n}(a|x)\\propto\\mu_{n}(x,a)$ when the normalization factor is non-zero, and arbitrarily defined otherwise. There is thus an equivalence between the CURL problem (optimization on policies) and a convex optimization problem on state-action distributions satisfying the Bellman flow: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in(S_{\\mathcal A})^{\\chi}\\times\\,N}F(\\mu^{\\pi,p})\\equiv\\operatorname*{min}_{\\mu\\in\\mathcal{M}_{\\mu_{0}}^{p}}F(\\mu).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Online CURL. In this paper we consider the online CURL problem in a non-stationary setting. We assume a finite-horizon scenario with $T$ episodes. An oblivious adversary generates a sequence of changing objective functions $(F^{t})_{t\\in[T]}$ , with $F^{t}$ being fully communicated to the learner only at the end of episode $t$ . We assume $F^{t}$ is $L_{F}$ -Lipschitz with respect to the $\\|\\cdot\\|_{\\infty,1}$ norm for all $t$ . The probability transition kernel is also allowed to evolve over time and is denoted by $p^{t}$ at episode $t$ . The learner\u2019s objective is then to calculate a sequence of strategies $(\\pi^{t})_{t\\in[T]}$ minimizing a total lporsosb $\\begin{array}{r}{L_{T}:=\\sum_{t=1}^{T}F^{t}(\\mu^{\\pi^{t},p^{t}})}\\end{array}$ .w hTiol em deeaasluirneg t hwei tlhe aardnevre\u2019rss apreiraflo rombjaencctie,v ew feu unscet itohnes $F^{t}$ oann do fc dhyannagimnigc $p^{t}$ regret (the difference between the learner\u2019s total loss and that of any policy sequence $(\\pi^{t,*})_{t\\in[T]})$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big):=\\sum_{t\\in[T]}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t,*},p^{t}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Non-stationarity measures. We consider the following two non-stationary measures $\\Delta_{\\infty}^{p}$ and $\\Delta^{p}$ on the probability transition kernels that respectively measure abrupt and smooth variations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{\\infty}^{p}:=1+\\sum_{t=1}^{T-1}\\mathbb{1}_{\\{p^{t}\\neq p^{t+1}\\}},\\quad\\Delta^{p}:=1+\\sum_{t=1}^{T-1}\\Delta_{t}^{p},\\quad\\Delta_{t}^{p}:=\\operatorname*{max}_{n,x,a}\\|p_{n}^{t}(\\cdot|x,a)-p_{n}^{t+1}(\\cdot|x,a)\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Regarding dynamic regret, we define for any sequence of policies $(\\pi^{t,*})_{t\\in[T]}$ , its non-stationarity measure as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta^{\\pi^{*}}:=1+\\sum_{t=1}^{T-1}\\Delta_{t}^{\\pi^{*}},\\qquad\\Delta_{t}^{\\pi^{*}}:=\\operatorname*{max}_{n\\in[N],x\\in\\mathcal{X}}\\|\\pi_{n}^{t,*}(\\cdot|x)-\\pi_{n}^{t+1,*}(\\cdot|x)\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, for any interval $I\\subseteq[T]$ , we write $\\begin{array}{r}{\\Delta_{I}^{p}:=\\sum_{t\\in I}\\Delta_{t}^{p}}\\end{array}$ and $\\begin{array}{r}{\\Delta_{I}^{\\pi^{*}}:=\\sum_{t\\in I}\\Delta_{t}^{\\pi^{*}}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Dynamic\u2019s hypothesis. For each episode $t$ , let $(x_{0}^{t},a_{0}^{t})\\sim\\mu_{0}(\\cdot)$ , and for all time steps $n\\in[N]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{n+1}^{t}=g_{n}(x_{n}^{t},a_{n}^{t},\\epsilon_{n}^{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g_{n}$ represents the deterministic part of the dynamics, and $(\\epsilon_{n}^{t})_{n\\in[N]}$ is a sequence of independent external noises such that $\\epsilon_{n}^{t}\\sim h_{n}^{t}(\\cdot)$ , where $h_{n}^{t}$ is any centered distribution. Note that these dynamics imply that the probability transition kernel can be written as $p_{n+1}^{t}(x^{\\prime}|x,a)=\\mathbb{P}\\big(g_{n}(x,a,\\epsilon_{n}^{t})=x^{\\prime}\\big)$ . Different variants of this problem can be considered, depending on the prior information available about the dynamics in Eq. (8). In this article we consider the case where $g_{n}$ is fixed and known by the learner, but $h_{n}^{t}$ is unknown and can change (hence the source of uncertainty and non-stationarity of the transitions). To the best of our knowledge, there are no black-box algorithms in the literature that achieve sublinear regret for online CURL with adversarial losses without relying on model assumptions. In using RL methods to CURL, we believe model-optimistic approaches like UCRL (Upper Confidence RL [4]) could be adapted. However, these methods are computationally expensive, as they require solving an additional optimization problem in every episode. The black-box algorithm for CURL we consider from [39] provides closed-form solutions, which is more computationally efficient, but requires the same dynamic assumption as in Eq. (8). Another class of RL methods is policy optimization (PO), which directly optimizes the policy and often yields closed-form solutions, leading to faster performance. Recent theoretical work [37] has shown that PO methods can achieve near-optimal regret without model assumptions. However, these methods rely on RL\u2019s Bellman equations, which do not apply to CURL due to its non-linear nature. We believe that the MetaCURL analysis could potentially be extended to the case where $g_{n}$ is unknown but belongs to a parametric family. We leave this extension for future work. ", "page_idx": 3}, {"type": "text", "text": "This particular dynamic is also motivated by many real-world applications: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Controlling a fleet of drones in a known environment, subject to external influences due to weather conditions or human interventions.   \n\u2022 Addressing data center power management aiming to cut energy expenses while maintaining service quality. Workload fluctuations cause dynamic job queue transitions, and volatile electricity prices lead to varying operational costs. The probabilities of task processing by each server are predetermined, but the probabilities of task arrival are uncertain [6].   \n\u2022 As renewable energy use increases and energy demand grows, balancing production and consumption becomes harder. Certain devices, like electric vehicle batteries and water heaters, can serve as flexible energy storage options. However, this requires electric grids to establish policies regulating when these devices turn on or off to match a desired consumption profile. These profiles can fluctuate daily due to changes in energy production levels. Despite knowing the devices\u2019 physical dynamics, household consumption habits remain unpredictable and constantly changing [51, 41]. ", "page_idx": 3}, {"type": "text", "text": "Outline. In this paper, we propose a novel approach to handle non-stationarity in MDPs, being the first to propose a solution to CURL within this context. We begin in Section 3 by discussing the idea behind our algorithm\u2019s construction and the key challenges within our framework. Section 4 introduces MetaCURL, while Section 5 presents the main results of our regret analysis. The proofs\u2019 specifics are provided in the appendix. ", "page_idx": 4}, {"type": "text", "text": "3 Main idea ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A hypothetical learner who achieves optimal regret. Let $m>1$ . Assume a hypothetical learner that could compute a sequence of restart times $1=t_{1}<...<t_{m+1}=T+1$ , where for each $i\\in[m]$ we let $\\bar{\\mathcal{T}}_{i}:=[t_{i},\\bar{t_{i+1}}-1]$ , such that (9) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\mathcal{T}_{i}}^{p}\\leq\\Delta^{p}/m.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consider any algorithm that, when computing $(\\pi^{t})_{t\\in I}$ with learning rate $\\lambda$ for any interval $I\\subseteq[T]$ , attains a dynamic regret relative to any sequence of policies $(\\pi^{t,*})_{t\\in I}$ upper bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{I}\\left(\\left(\\pi^{t,*}\\right)_{t\\in I}\\right)\\leq c_{1}\\lambda|I|+\\lambda^{-1}(c_{2}\\Delta_{I}^{\\pi^{*}}+c_{3})+|I|\\Delta_{I}^{p},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(c_{j})_{j\\in[3]}$ are constants that may depend on the MDP parameters, and on the interval size only in logarithmic terms. This kind of regret bound holds for Greedy MD-CURL from [39] as we show in Appendix $\\mathrm{G}$ . Suppose the hypothetical learner could also access $\\Delta^{\\pi^{*}}$ to calculate the optimal learning rate. Hence, playing such an algorithm for all horizon $T$ with the optimal learning rate, the learner would have a dynamic regret upper bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)\\leq2\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+T\\Delta^{p}m^{-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Optimizing over $m$ , the learner would obtain the optimal regret of order $\\tilde{O}\\big(\\sqrt{\\Delta^{\\pi^{*}}T}+(\\Delta^{p})^{1/3}T^{2/3}\\big)$ . In the case where the MDP \u221ais piece-wi\u221ase stationary, if the learner takes $\\mathcal{T}_{i}$ such that $\\Delta_{\\mathcal{T}_{i}}^{p}=0$ , it obtains a regret of order $O(\\sqrt{\\Delta^{\\pi*}T}+\\sqrt{\\Delta_{\\infty}^{p}T})$ , where $\\Delta_{\\infty}^{p}$ is the number of times the probability transitions of the MDP change over $[T]$ . ", "page_idx": 4}, {"type": "text", "text": "A meta algorithm to learn restart times. In reality, the restart times of Eq. (9), and the optimal learning rate, are unknown to the learner. Hence, we propose to build a meta aggregation algorithm to learn both. Let $\\mathcal{E}$ represent a parametric black-box algorithm with dynamic regret as in Eq. (10). We introduce a meta algorithm $\\mathcal{M}$ that, takes as input a finite set of learning rates $\\Lambda$ , and at each episode $t$ , initializes $|\\Lambda|$ instances of $\\mathcal{E}$ , denoted as $\\mathcal{E}^{t,\\lambda}$ for each $\\lambda\\in\\Lambda$ . Each $\\overline{{\\mathcal{E}^{t,\\lambda}}}$ operates independently within the interval $[t,T]$ . At time $t$ , $\\mathcal{M}$ combines the decisions from the active runs $\\{\\mathcal{E}^{s,\\lambda}\\}_{s\\leq t,\\lambda\\in\\Lambda}$ by weighted average. The idea is that at time $t$ , some of the outputs of $\\{\\mathcal{E}^{s,\\lambda}\\}_{s\\leq t,\\lambda\\in\\Lambda}$ are not based on data prior to $t^{\\prime}<t$ , so if the environment changes at time $t^{\\prime}$ , these outputs can be given a greater weight by the meta algorithm, enabling it to adapt more quickly to the change. At the same time, we expect a larger weight will be given to the empirically best learning rate. Let $\\mathcal{M}(\\mathcal{E},\\Lambda)$ be the complete algorithm. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. The meta-algorithm increases the computational complexity of the parametric blackbox algorithm by a factor of $T\\times|\\Lambda|,$ , as it requires updating $t\\times|\\Lambda|$ instances at each episode t. By strategically designing intervals to run the black-box algorithms, previous works on online learning have reduced computational complexity to $O(\\log(T))$ [15, 29, 27]. Extending our analysis to these intervals is straightforward, but it would complicate the presentation of the paper. Thus, we decided to present our results using the naive choice of intervals. Also, in Section $^{5}$ , we show that a learning rate grid with $|\\Lambda|=\\log(\\bar{T})$ is sufficient to obtain the optimal regret. ", "page_idx": 4}, {"type": "text", "text": "Regret decomposition. Denote by $\\pi^{t,s,\\lambda}$ the policy output from $\\mathcal{E}^{s,\\lambda}$ at episode $t$ , for learning rate $\\lambda$ , for all $s\\leq t$ , and by $\\pi^{t}$ the policy output by the meta algorithm $\\mathcal{M}(\\mathcal{E},\\Lambda)$ to be played by the learner. The regret of $\\mathcal{M}(\\mathcal{E},\\Lambda)$ can be decomposed as the sum of the regret suffered by the meta algorithm aggregation scheme, $\\mathcal{M}$ , and the regret from the black-box algorithm, $\\mathcal{E}$ , played with any learning rate $\\lambda\\,\\in\\,\\Lambda$ . The dynamic regret, defined in Eq. (5), can be decomposed, for any set of intervals ${\\cal Z}_{i}=[t_{i},t_{i+1}-1]$ , with $1=t_{1}<...<t_{m+1}=T+1$ , and for any learning rate $\\lambda\\in\\Lambda$ , as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Im_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)=\\sum_{i=1}^{m}\\sum_{t\\in\\cal Z_{i}}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})+\\sum_{i=1}^{m}\\sum_{t\\in\\cal Z_{i}}F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})-F^{t}(\\mu^{\\pi^{t,*},p^{t}})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n:=R_{[T]}^{\\mathrm{meta}}+R_{[T]}^{\\mathrm{black-box}}\\left((\\pi^{t,*})_{t\\in[T]}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The black-box regret on $\\mathcal{T}_{i}$ is exactly the standard regret for $T\\,=\\,|{\\mathcal{T}}_{i}|$ with a learning rate of $\\lambda$ . Hence, in order to prove low dynamic regret for $\\bar{\\mathcal{M}}(\\bar{\\mathcal{E}},\\Lambda)$ we have to: show that $\\mathcal{M}$ incurs a low dynamic regret in each interval ${\\mathcal{T}}_{i}$ ; find a black-box algorithm $\\mathcal{E}$ for CURL that has dynamic regret as in Eq. (10), and build a learning rate grid $\\Lambda$ allowing us to perform nearly as well as the optimal learning rate. ", "page_idx": 5}, {"type": "text", "text": "4 MetaCURL Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We call our meta-algorithm $\\mathcal{M}$ MetaCURL. It is based on sleeping experts, is parameter-free, and achieves optimal regret. Its construction is described below. ", "page_idx": 5}, {"type": "text", "text": "4.1 Learning with expert advice ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "General setting. In Learning with Expert Advice (LEA), a learner makes sequential online predictions $u^{1},\\ldots,u^{T}$ in a decision space $\\boldsymbol{\\mathcal{U}}$ , over a series of $T$ episodes, with the help of $K$ experts [22, 36, 9]. For each round $t$ , each expert $k$ makes a prediction $u^{t,k}$ , and the learner combines the experts\u2019 predictions by computing a vector $v^{t}:=(v^{\\bar{t},1},\\ldots,v^{t,K})\\in S_{K}$ , and predicting the convex combination of experts\u2019 prediction ut := kK=1 vt, . The environment then reveals a convex loss function $\\ell^{t}:\\mathcal{U}\\to\\mathbb{R}$ . Each expert suffers a loss $\\ell^{t,k}:=\\ell^{t}(u^{t,k})$ , and the learner suffers a loss $\\hat{\\ell}^{t}:=\\ell^{t}(u^{t})$ . The learner\u2019s objective is to keep the cumulative regret with respect to each expert as low as possible. For each expert $k$ , this quantity is defined as $\\begin{array}{r}{\\mathrm{Reg}_{[T]}(k):=\\sum_{t=1}^{T}\\hat{\\ell}^{t}-\\ell^{t,k}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Sleeping experts. In our case, each black-box algorithm is an expert that does not produce solutions outside its active interval. This problem can be reduced to the sleeping expert problem [8, 23], where experts are not required to provide solutions at every time step. Let $I^{t,\\tilde{k}}\\in\\dot{\\{0,1\\}}$ define a signal equal to 1 if expert $k$ is active at episode $t$ and 0 otherwise. The algorithm knows $(I^{t,k})_{k\\in[K]}$ and assigns a zero weight to sleeping experts $'I^{t,k}=0$ implies $\\boldsymbol{v}^{t,k}=0_{.}$ ). We would like to have a guarantee with respect to expert $k\\in[K]$ but only when it is active. Hence, we now aim to bound a cumulative regret that depends on the signal $\\begin{array}{r}{I^{t,k}\\colon\\mathrm{Reg}_{[T]}^{\\mathrm{sleep}}(k):=\\sum_{t=1}^{T}I^{t,k}(\\hat{\\ell}^{t}-\\ell^{t,k})}\\end{array}$ . There is a generic reduction from the sleeping expert framework to the general LEA setting [3, 2] (see Appendix A.1). ", "page_idx": 5}, {"type": "text", "text": "4.2 Meta-aggregation scheme ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In every episode $t$ , for every learning rate $\\lambda\\\\\\in\\Lambda$ and $s\\,\\leq\\,t$ , an instance $\\mathcal{E}^{s,\\lambda}$ of the black-box algorithm acts as an expert computing a policy $\\pi^{t,s,\\lambda}$ . The meta algorithm aims to aggregate these predictions using a sleeping expert approach based on the expert\u2019s losses. However, within CURL\u2019s framework, the meta algorithm faces two challenges: ", "page_idx": 5}, {"type": "text", "text": "Uncertainty. At the episode\u2019s end, the learner has full information about the objective function $F^{t}$ . If the learner also knew $\\bar{p}^{t}$ , they could recursively calculate the corresponding state-action distribution $\\mu^{\\pi^{t,s,\\lambda},p^{t}}$ using Eq. (2) and observe the actual loss of each expert, denoted as $F^{t}(\\mu^{\\pi^{t,s,\\lambda}},\\!p^{t})$ . However, given that $p^{t}$ is unknown to the learner, the true loss remains unobservable. Consequently, the metaalgorithm needs to create an estimator $\\Hat{p}^{t}$ for $p^{t}$ and utilize it to estimate the losses. We propose a method to compute an estimator $\\hat{p}^{t}$ in Subsection 4.3. ", "page_idx": 5}, {"type": "text", "text": "Convexity. As discussed in Section 2, the objective functions $F^{t}$ are not convex over the space of polices. However, CURL is equivalent to a convex problem over the state-action distributions satisfying the Bellman\u2019s flow as shown in Eq. (4). Therefore, instead of aggregating policies, the meta algorithm aggregates the associated state-action distributions using the probability estimator $\\Hat{p}^{t}$ and the recursive scheme at Eq. (2). We detail MetaCURL in Alg. 1 when employed with the Exponentially Weighted Average forecaster (EWA) as the sleeping expert subroutine (we detail EWA in Appendix A.2). ", "page_idx": 5}, {"type": "text", "text": "4.3 Building an estimator of $p^{t}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As discussed earlier, applying the learning with experts framework requires estimating the loss of non-played expert policies, which depends on estimating the non-stationary transition probabilities $\\hat{p}^{t}$ . Standard RL techniques for bounding the $L_{1}$ norm between the empirical estimator $\\overline{{p}}^{t}$ and the true ", "page_idx": 5}, {"type": "text", "text": "1: Input: number of episodes $T$ , finite set of learning rates $\\Lambda$ , black-box algo. $\\mathcal{E}$ , EWA learning rate $\\eta=\\sqrt{8\\log(T)T}$ ", "page_idx": 6}, {"type": "text", "text": "2: Initialization: $\\begin{array}{r}{\\hat{p}_{n}^{1}(\\cdot|x,a):=\\frac{1}{|\\mathcal{X}|}}\\end{array}$ for all $n\\in[N],(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$   \n3: for $t=1,\\dots,T$ do   \n4: Start $|\\Lambda|$ new instances of $\\mathcal{E}$ denoted by $\\mathcal{E}^{t,\\lambda}$ for all $\\lambda\\in\\Lambda$ , assign each of them a new weight $\\begin{array}{r}{v^{t,t,\\lambda}=\\frac{1}{|\\Lambda|t}}\\end{array}$ , and normalize weight vectors $v^{t,s,\\lambda}$ for $s\\in[t{-}1]$ such that $v^{t}:=(v^{t,s,\\lambda})_{s\\leq t,\\lambda\\in\\Lambda}$ is a probability vector in Rt\u00d7\u039b   \n5: For $s\\leq t$ and $\\lambda\\in\\Lambda$ , $\\mathcal{E}^{s,\\lambda}$ outputs $\\pi^{t,s,\\lambda}$   \n6: Compute recursively $\\mu^{\\pi^{t,s,\\lambda},\\hat{p}^{t}}$ using Eq. (2) for all $s\\leq t$ and $\\lambda\\in\\Lambda$   \n7: Aggregate the state-action distributions: $\\textstyle\\mu^{t}:=\\sum_{s=1}^{t}\\sum_{\\lambda\\in\\Lambda}\\mu^{\\pi^{t,s,\\lambda},\\hat{p}^{t}}v^{t,s,\\lambda}$   \n8: Retrieve $\\pi^{t}$ from $\\mu^{t}$ : for all $n$ , $(x,a)$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{n}^{t}(a|x)=\\left\\{\\begin{array}{r l}&{\\frac{\\mu_{n}^{t}(x,a)}{\\sum_{a^{\\prime}\\in A}\\mu_{n}(x,a^{\\prime})},\\quad\\mathrm{~if~}\\mu_{n}^{t}(x,a)\\neq0}\\\\ &{\\frac{1}{|A|},\\quad\\mathrm{~if~}\\mu_{n}^{t}(x,a)=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "9: Learner plays $\\pi^{t}$ : Agent starts at $(x_{0}^{t},a_{0}^{t})\\sim\\mu_{0}(\\cdot)$ ", "page_idx": 6}, {"type": "text", "text": "10: for ${n=1,\\ldots,N}$ do   \n11: Environment draws new state $x_{n}^{t}\\sim p_{n}^{t}(\\cdot|x_{n-1}^{t},a_{n-1}^{t})$   \n1123:: LAegaernnte cr hoobosseersv easn  aagcetinot\u2019ns $a_{n}^{t}\\sim\\pi_{n}^{t}(\\cdot|x_{n}^{t})$ $\\varepsilon_{n}^{t}$ ", "page_idx": 6}, {"type": "text", "text": "14: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "15: Objective function $F^{t}$ is exposed   \n16: Compute experts\u2019 losses $\\ell^{t,s,\\lambda}:=F_{\\;.}^{t}(\\mu^{\\pi^{t,s,\\lambda},\\hat{p}^{t}})$ , for all $s\\leq t$ and $\\lambda\\in\\Lambda$   \n17: Compute the new weight vector $v^{t+1}$ : for all $s\\leq t$ and $\\lambda\\in\\Lambda$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nv^{t+1,s,\\lambda}=\\frac{v^{t,s,\\lambda}\\exp{(-\\eta\\ell^{t,s,\\lambda})}}{\\sum_{s^{\\prime}=1}^{t}\\sum_{\\lambda^{\\prime}\\in\\Lambda}v^{t,s^{\\prime},\\lambda^{\\prime}}\\exp{(-\\eta\\ell^{t,s^{\\prime},\\lambda^{\\prime}})}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "18: Use agent\u2019s external noise trajectory $(\\varepsilon_{n}^{t})_{n\\in[N]}$ to compute $\\hat{p}^{t+1}$ as in Subsection 4.3 ", "page_idx": 6}, {"type": "text", "text": "19: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "dynamics $p^{t}$ [44, 49] are not applicable here due to non-stationarity. To address this, we introduce a second layer of sleeping experts for each $(n,x,a)\\in[N]\\times\\mathcal{X}\\times\\mathcal{A}$ , where each expert provides an empirical estimate of $p^{t}$ based on different intervals. We then propose a new loss function in Eq. (12) and conduct a novel regret analysis in Prop. 5.2 to achieve the optimal regret rate. ", "page_idx": 6}, {"type": "text", "text": "In each episode $t$ , the learner calculates independent samples $x_{n,x,a}^{t}\\sim p_{n}^{t}(\\cdot|x,a)$ utilizing the external noise sequence $(\\varepsilon_{n}^{t})_{n\\in[N]}$ observed (just let xtn,x,a = gn\u22121(x, a, \u03b5tn\u22121), see Eq. (8)). Each expert outputs an empirical estimator of $p_{n}^{t}(\\cdot|x,a)$ using samples across different intervals. We assume $T$ experts, with expert $s$ active in interval $[s,T]$ . Expert $s$ at episode $t>s$ outputs: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{p}_{n}^{t,s}(x^{\\prime}|x,a)=\\frac{N_{n,x,a}^{s:t-1}(x^{\\prime})}{(t-s)},\\quad\\mathrm{with}~~N_{n,x,a}^{s:t-1}(x^{\\prime}):=\\sum_{q=s}^{t-1}\\mathbb{1}_{\\left\\{x_{n,x,a}^{q}=x^{\\prime}\\right\\}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We let $\\hat{p}_{n}^{t}(\\cdot|x,a)$ be the result of employing sleeping EWA with experts $\\hat{p}_{n}^{t,s}(\\cdot|x,a)$ , for $s~<t$ . Typically, in density estimation with EWA, a logarithmic $\\mathrm{loss-log(\\cdot)}$ is used. However, in this case $-\\log(\\cdot)$ can be unbounded, so we opt here for a smoothed logarithmic loss, given by, for all $q\\in S_{\\mathcal{X}}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell^{t}(q):=\\sum_{x\\in\\mathcal{X}}-\\log\\left(q(x)+\\frac{1}{|\\mathcal{X}|}\\right)\\mathbb{1}_{\\left\\{\\tilde{x}_{n,x,a}^{t}=x\\right\\}},\\mathrm{~where~}\\tilde{x}_{n,x,a}^{t}\\sim\\left(p_{n}^{t}(\\cdot|x,a)+\\frac{1}{|\\mathcal{X}|}\\right)/2.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The definition of this non-standard loss is further clarified during the regret analysis in Section 5. This loss function is 1-exp concave (see Lemma 4 of [52]), hence the cumulative regret of EWA with respect to each expert $s\\ \\in\\ [T]$ , for all episodes $\\tau\\ \\in\\ [s,T]$ , satisfies $\\begin{array}{r}{\\mathsf{R e g}_{[s,\\tau]}^{\\mathrm{sleep}}(s)=\\sum_{t=s}^{\\tau}\\ell^{t}(\\hat{p}_{n}^{t^{\\star}}(\\cdot|x,a))-\\ell^{t}(\\hat{p}_{n}^{t,\\mathrm{s}}(\\cdot|,x,a))\\overset{\\star}{\\leq}\\log(T)}\\end{array}$ (for more information on the regret bounds of EWA with exp-concave losses, see Appendix A.2). We describe the complete online scheme to compute $\\hat{p}^{t}$ in Alg. 3 at Appendix B. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Regret analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section presents the main result concerning MetaCURL\u2019s regret analysis. Subsection 5.1 shows an upper bound for $R^{\\mathrm{meta}}$ when MetaCURL is played with EWA and $\\Hat{p}^{t}$ is computed as in Subsection. 4.3. Subsection 5.2 introduces a learning rate grid for MetaCURL when the black-box algorithm meets the dynamic regret criteria in Eq. (10), providing an upper bound for $R^{\\mathrm{black-box}}$ . Given the dynamic regret decomposition of Eq. (11), we see that the combination of these results leads to our main result, the full proof of which can be found in appendix (F) : ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Main result). Let $\\delta\\,\\in\\,(0,1)$ . Playing MetaCURL, with a parametric black-box algorithm $\\mathcal{E}$ with dynamic regret as in Eq. (10), with a learning rate grid $\\Lambda\\;:=\\;\\{2^{-j}|j\\;=$ $0,1,2,\\ldots,\\lceil\\log_{2}(T)/2\\rceil\\Bigl\\}$ , and with EWA as the sleeping expert subroutine, we obtain, with probability at least $1-2\\delta$ , for any sequence of policies $(\\pi^{t,*})_{t\\in[T]}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)\\leq\\tilde{O}\\Big(\\sqrt{\\Delta^{\\pi^{*}}T}+\\operatorname*{min}\\big\\{\\sqrt{T\\Delta_{\\infty}^{p}},\\;T^{2/3}(\\Delta^{p})^{1/3}\\big\\}\\Big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5.1 Meta-algorithm analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given the uncertainty in the probability transition, the meta regret term can be decomposed as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{[T]}^{\\mathrm{meta}}=\\underbrace{\\sum_{t=1}^{T}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})}_{{\\displaystyle R_{[T]}^{\\hat{p}}(\\pi^{t})(\\hat{p}^{t}\\mathrm{\\tiny~eimation})}}}}\\\\ {{\\displaystyle+\\underbrace{\\sum_{i=1}^{m}\\sum_{t\\in{\\cal T}_{i}}F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})-F^{t}(\\mu^{\\pi^{t},t_{i},\\lambda,\\hat{p}^{t}})}_{{\\displaystyle\\mathrm{slecping~LEA~regret}}}+\\underbrace{\\sum_{i=1}^{m}\\sum_{t\\in{\\cal T}_{i}}F^{t}(\\mu^{\\pi^{t},t_{i},\\lambda,\\hat{p}^{t}})-F^{t}(\\mu^{\\pi^{t},t_{i},\\lambda,p^{t}})}_{{\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in{\\cal T}_{i}}R_{[Z_{i}}^{\\hat{p}}(\\pi^{t},t^{i},\\lambda)\\,(\\hat{p}^{t}\\mathrm{\\tiny~eimation})})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Sleeping LEA regret. Referring to Thm. A.1 in Appendix A, using sleeping EWA as the sleeping expert subroutine of MetaCURL, with signals $I^{t,s^{-}}\\!=1$ for active experts $(s\\leq t)$ , experts\u2019 convex losses $\\ell^{t,s,\\lambda}:=F^{t}(\\mu^{\\pi^{t^{\\lambda,s,\\lambda}},\\hat{p}^{t}})$ , and learner loss $\\hat{\\ell}^{t}:=F^{t}(\\boldsymbol{\\mu}^{\\pi^{t},\\hat{p}^{t}})$ , yields, for any $m\\in[T]$ and for any set of intervals ${\\cal Z}_{i}=[t_{i},t_{i+1}-1]$ , with $1=t_{1}<...<t_{m+1}=T+1$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{m}\\sum_{t\\in T_{i}}F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})-F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},\\hat{p}^{t}})=\\sum_{i=1}^{m}\\mathrm{Reg}_{T_{i}}^{\\mathrm{sleep}}(t_{i})}}\\\\ &{}&{\\leq\\displaystyle\\sum_{i=1}^{m}\\sqrt{\\frac{|{\\mathcal D}_{i}|}{2}\\log(T|\\Lambda|)}\\leq\\sqrt{\\frac{m T}{2}\\log(T|\\Lambda|)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$\\Hat{p}^{t}$ Estimation regret. In a scenario without uncertainty in the MDP\u2019s probability transitions, the meta-algorithm\u2019s regret would simply be bounded by Eq. (14), the sleeping expert regret used as a subroutine. However, given the presence of uncertainty, the main challenge in analyzing the meta-regret comes from the regret terms associated with the estimator $\\hat{p}^{t}$ . We outline this analysis in Prop. 5.2. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.2. Let $\\delta\\in(0,1)$ , $\\begin{array}{r}{C:=\\sqrt{\\frac{1}{2}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|}T}{\\delta}\\right)}}\\end{array}$ , and $L_{F}$ be the Lipschitz constant of $F^{t}$ , with respect to the norm $\\|\\cdot\\|_{\\infty,1;}$ , for all $t\\in[T]$ . With a probability of at least $1-\\delta$ , MetaCURL obtains ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{[T]}^{\\hat{p}}(\\pi^{t}):=\\sum_{t=1}^{T}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})\\leq2L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|\\mathcal{A}|}C^{2/3}\\log(T)^{1/3}T^{2/3}(\\Delta^{p})^{1/3}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For any $m\\in[T]$ and for any set of intervals $\\mathcal{T}_{i}=[t_{i},t_{i+1}-1],$ , with $1=t_{1}<...<t_{m+1}=T+1$ , the same bound is valid for $\\begin{array}{r}{\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}R_{\\mathbb{Z}_{i}}^{\\hat{p}}(\\pi^{t,t_{i},\\lambda})}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. The proof idea is based mainly on the formulation of $\\hat{p}^{t}$ described in Subsection 4.3. We start by using the convexity of $F^{t}$ to linearize the expression, then we apply Holder\u2019s inequality and exploit the $L_{F}$ -Lipschitz property of $F^{t}$ to establish an upper bound based on the $L_{1}$ norm difference of the state-action distributions induced by the true probability transition and the estimator. Using Lemma C.5 in Appendix C, we then obtain that ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{[T]}^{\\hat{p}}\\leq L_{F}\\sum_{t=1}^{T}\\sum_{n=1}^{N}\\sum_{j=1}^{n}\\sum_{x,a}\\mu_{j-1}^{\\pi^{t},p^{t}}(x,a)\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To use the results from Subsection 4.3, we first regularize $p^{t}$ and $\\Hat{p}^{t}$ , for each $(n,x,a)$ , by averaging each with the uniform distribution over $\\mathcal{X}$ , that we denote by $p_{0}:=1/|\\mathcal{X}|$ . As both probabilities are now lower bounded, we can employ Pinsker\u2019s inequality to convert the $L_{1}$ norm into a KL divergence. The sum over $t\\in[T]$ of the KL divergence can then be decomposed as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{t=1}^{T}\\mathrm{KL}\\Big(\\frac{p_{j}^{t}(\\cdot|x,a)+p_{0}}{2}\\Big|\\frac{\\hat{p}_{j}^{t}(\\cdot|x,a)+p_{0}}{2}\\Big)=\\sum_{i=1}^{m}\\sum_{t\\in T_{i}}\\mathrm{KL}\\Big(\\frac{p_{j}^{t}(\\cdot|x,a)+p_{0}}{2}\\Big|\\frac{\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)+p_{0}}{2}\\Big)}}\\\\ {{\\displaystyle\\qquad+\\sum_{i=1}^{m}\\sum_{t\\in T_{i}}\\mathbb{E}_{\\tilde{x}_{j,x,a}^{t}}\\big[\\log\\big(\\hat{p}_{j}^{t,t_{i}}(\\tilde{x}_{j,x,a}^{t}|x,a)+p_{0}\\big)-\\log\\big(\\hat{p}_{j}^{t}(\\tilde{x}_{j,x,a}^{t}|x,a)+p_{0}\\big)\\big],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)$ is the empirical estimate of $p_{i}^{t}(\\cdot|x,a)$ calculated with the observed data from $t_{i}$ to $t-1$ , and the expectation is over $\\tilde{x}_{j,x,a}^{t}\\sim(p_{j}^{t}(\\cdot|\\underline{{{x}}},a)+p_{0})/2$ . The second term is the cumulative regret of computing $\\hat{p}^{t}$ using EWA with loss as in Eq. (12), and is bounded by $m\\log(T)$ . We finish and give more details of the proof in Appendix D. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Prop. 5.2 together with Eq. (14) yields the main result of this subsection: ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.3 (Meta regret bound). With the same assumptions as Prop. 5.2, for any $m\\in[T]$ , with probability at least $1-2\\delta$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{[T]}^{m e t a}\\leq4L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|\\mathcal{A}|}C^{2/3}\\log(T)^{1/3}T^{2/3}(\\Delta^{p})^{1/3}+\\sqrt{\\frac{m T}{2}\\log(T|\\Lambda|)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5.2 Black-box algorithm analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Assuming $\\mathcal{E}$ is a parametric black-box algorithm with dynamic regret satisfying Eq. (10) for any laecahrineivneg  oruart ef $\\lambda>0$ ,u nwde  oonn $\\bar{R}_{[T]}^{\\mathrm{black-box}}$ .o address the selection of the $\\lambda s$ grid and optimize across $\\lambda$ to ", "page_idx": 8}, {"type": "text", "text": "Learning rate grid. The dynamic regret of Eq. (10) implies that any two $\\lambda$ that are a constant factor of each other will guarantee the same upper-bound up to essentially the same constant factor. We therefore choose an exponentially spaced grid ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda:=\\big\\{2^{-j}|j=0,1,2,\\ldots,\\lceil\\log_{2}(T)/2\\rceil\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The meta-algorithm aggregation scheme guarantees that the learner performs as well as the best empirical learning rate. We thus obtain a bound on $R_{[T]}^{\\mathrm{black-box}}$ , with its proof in Appendix E: ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.4 (Black-box regret bound). Assume MetaCURL is played with a black-box algorithm satisfying dynamic regret as in Eq. (10), with learning rate grid as in Eq. (15). Hence, for any sequence of policies $\\mathring{(\\pi^{t,*})}_{t\\in[T]}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{[T]}^{b l a c k\\cdot b o x}\\bigl((\\pi^{t,*})_{t\\in[T]}\\bigr)\\leq N\\biggl(\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\biggr)+c_{1}\\sqrt{T}+3\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+\\frac{T\\Delta^{p}}{m}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Greedy MD-CURL. Greedy MD-CURL, developed by [39], is a computationally efficient policyoptimization algorithm known for achieving sublinear static regret in online CURL with adversarial objective functions within a stationary MDP. In Thm. G.3 of Appendix G, we extend this analysis showing that Greedy MD-CURL also achieves dynamic regret as in Eq. (10). To our knowledge, this is the first dynamic regret result for a CURL algorithm. Hence, Greedy MD-CURL can be used as a black-box for MetaCURL. We detail Greedy MD-CURL in Alg. 4 in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion, discussion, and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present MetaCURL, the first algorithm for dealing with non-stationarity in CURL, a setting covering many problems in the literature that modifies the standard linear RL configuration, making typical RL techniques difficult to use. We also employ a novel approach to deal with nonstationarity in MDPs using the learning with expert advice framework from the online learning literature. The main difficulty in analyzing this method arises from uncertainty about probability transitions. We overcome this problem by employing a second expert scheme, and show that MetaCURL achieves near-optimal regret. ", "page_idx": 9}, {"type": "text", "text": "Compared to the RL literature, our approach is more efficient, deals with adversarial losses, and has a better regret dependency concerning the varying losses, but to do so, we need to simplify the assumptions about the dynamics (all uncertainty comes only from the external noise, that is independent of the agent\u2019s state-action). There seems to be a trade-off in RL: all algorithms dealing with both non-stationarity and full exploration use UCRL-type approaches, and are thus computationally expensive. We thus leave a question for future work: How can we effectively manage non-stationarity and adversarial losses using efficient algorithms, all while addressing full exploration? ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2004. [2] D. Adamskiy, W. M. Koolen, A. Chernov, and V. Vovk. A closer look at adaptive regret. Journal of Machine Learning Research, 17(23):1\u201321, 2016. [3] D. Adamskiy, M. K. K. Warmuth, and W. M. Koolen. Putting bayes to sleep. In Advances in Neural Information Processing Systems (NeurIPS), volume 25, 2012.   \n[4] P. Auer, T. Jaksch, and R. Ortner. Near-optimal regret bounds for reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 21, 2008. [5] A. Barakat, I. Fatkhullin, and N. He. Reinforcement learning with general utilities: simpler variance reduction and large state-action space. In International Conference on Machine Learning (ICML), pages 1753\u20131800, 2023. [6] M. Bayati. Power management policy for heterogeneous data center based on histogram and discrete-time mdp. Electronic Notes in Theoretical Computer Science, 337:5\u201322, 2018. Proceedings of the Ninth International Workshop on the Practical Application of Stochastic Modelling (PASM).   \n[7] A. Bensoussan, P. Yam, and J. Frehse. Mean Field Games and Mean Field Type Control Theory. SpringerBriefs in Mathematics. Springer, 2013.   \n[8] A. Blum. Empirical support for winnow and weighted-majority based algorithms: results on a calendar scheduling domain. In A. Prieditis and S. Russell, editors, Machine Learning Proceedings 1995, pages 64\u201372, 1995.   \n[9] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.   \n[10] Y. Chen, S. S. Du, and K. Jamieson. Improved corruption robust algorithms for episodic reinforcement learning. In M. Meila and T. Zhang, editors, International Conference on Machine Learning (ICML), volume 139, pages 1561\u20131570, 2021.   \n[11] W. C. Cheung. Exploration-exploitation trade-off in reinforcement learning on online markov decision processes with global concave rewards. ArXiv, abs/1905.06466, 2019.   \n[12] W. C. Cheung. Regret minimization for reinforcement learning with vectorial feedback and complex objectives. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.   \n[13] W. C. Cheung, D. Simchi-Levi, and R. Zhu. Reinforcement learning for non-stationary Markov decision processes: The blessing of (More) optimism. In International Conference on Machine Learning (ICML), volume 119, pages 1843\u20131854, 2020.   \n[14] A. Cohen, Y. Efroni, Y. Mansour, and A. Rosenberg. Minimax regret for stochastic shortest path. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 28350\u201328361, 2021.   \n[15] A. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML), volume 37, pages 1405\u20131411, 2015.   \n[16] R. De Santi, M. Prajapat, and A. Krause. Global reinforcement learning $:$ Beyond linear and convex rewards via submodular semi-gradient methods. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 10235\u201310266, 2024.   \n[17] O. D. Domingues, P. M\u2019enard, M. Pirotta, E. Kaufmann, and M. Valko. A kernel-based approach to non-stationary reinforcement learning in metric spaces. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 130, pages 3538\u20133546, 2021.   \n[18] Y. Efroni, L. Shani, A. Rosenberg, and S. Mannor. Optimistic policy optimization with bandit feedback. In International Conference on Machine Learning (ICML), volume 119, pages 8604\u20138613, 2020.   \n[19] E. Even-Dar, S. M. Kakade, and Y. Mansour. Online markov decision processes. Mathematics of Operations Research, 34(3):726\u2013736, 2009.   \n[20] Y. Fei, Z. Yang, Z. Wang, and Q. Xie. Dynamic regret of policy optimization in non-stationary environments. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 6743\u20136754, 2020.   \n[21] S. Feng, M. Yin, R. Huang, Y.-X. Wang, J. Yang, and Y. Liang. Non-stationary reinforcement learning under general function approximation. In International Conference on Machine Learning (ICML), volume 202, pages 9976\u201310007, 2023.   \n[22] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997.   \n[23] Y. Freund, R. E. Schapire, Y. Singer, and M. K. Warmuth. Using and combining predictors that specialize. In Annual ACM Symposium on Theory of Computing (STOC), page 334\u2013343, 1997.   \n[24] P. Gajane, R. Ortner, and P. Auer. A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions. ArXiv, abs/1805.10066, 2018.   \n[25] M. Geist, J. P\u00e9rolat, M. Lauri\u00e8re, R. Elie, S. Perrin, O. Bachem, R. Munos, and O. Pietquin. Concave utility reinforcement learning: The mean-field game viewpoint. In International Conference on Autonomous Agents and Multiagent Systems, page 489\u2013497, 2022.   \n[26] S. K. S. Ghasemipour, R. Zemel, and S. Gu. A divergence minimization perspective on imitation learning methods. In Proceedings of the Conference on Robot Learning, volume 100, pages 1259\u20131277, 2020.   \n[27] A. Gy\u00f6rgy, T. Linder, and G. Lugosi. Efficient tracking of large classes of experts. In 2012 IEEE International Symposium on Information Theory Proceedings, pages 885\u2013889, 2012.   \n[28] E. Hazan, S. Kakade, K. Singh, and A. Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning (ICML), volume 97, pages 2681\u20132691, 2019.   \n[29] E. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. Electronic Colloquium on Computational Complexity (ECCC), 14, 01 2007.   \n[30] C. Jin, T. Jin, H. Luo, S. Sra, and T. Yu. Learning adversarial Markov decision processes with bandit feedback and unknown transition. In International Conference on Machine Learning (ICML), volume 119, pages 4860\u20134869, 2020.   \n[31] T. Jin, J. Liu, C. Rouyer, W. Chang, C.-Y. Wei, and H. Luo. No-regret online reinforcement learning with adversarial losses and transitions. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 38520\u201338585, 2023.   \n[32] T. Jin and H. Luo. Simultaneously learning stochastic and adversarial episodic mdps with known transition. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 16557\u201316566, 2020.   \n[33] K.-S. Jun, F. Orabona, S. Wright, and R. Willett. Improved Strongly Adaptive Online Learning using Coin Betting. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 54, pages 943\u2013951, 2017.   \n[34] P. Lavigne and L. Pfeiffer. Generalized conditional gradient and learning in potential mean field games. Applied Mathematics & Optimization, 88(3):89, Oct 2023.   \n[35] J. W. Lavington, S. Vaswani, and M. Schmidt. Improved policy optimization for online imitation learning. In Proceedings of The 1st Conference on Lifelong Learning Agents, volume 199, pages 1146\u20131173, 2022.   \n[36] N. Littlestone and M. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212\u2013261, 1994.   \n[37] H. Luo, C.-Y. Wei, and C.-W. Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 22931\u201322942, 2021.   \n[38] T. Lykouris, M. Simchowitz, A. Slivkins, and W. Sun. Corruption-robust exploration in episodic reinforcement learning. In Conference on Learning Theory (COLT), volume 134, pages 3242\u2013 3245, 2021.   \n[39] B. M Moreno, M. Bregere, P. Gaillard, and N. Oudjane. Efficient model-based concave utility reinforcement learning through greedy mirror descent. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 238, pages 2206\u20132214, 2024.   \n[40] W. Mao, K. Zhang, R. Zhu, D. Simchi-Levi, and T. Basar. Near-optimal model-free reinforcement learning in non-stationary episodic mdps. In International Conference on Machine Learning (ICML), volume 139, pages 7447\u20137458, 2021.   \n[41] B. Marin Moreno, M. Br\u00e9g\u00e8re, P. Gaillard, and N. Oudjane. (Online) Convex Optimization for Demand-Side Management: Application to Thermostatically Controlled Loads, Jan. 2023.   \n[42] M. Mutti, R. De Santi, P. De Bartolomeis, and M. Restelli. Challenging common assumptions in convex reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 4489\u20134502, 2022.   \n[43] M. Mutti, R. D. Santi, P. D. Bartolomeis, and M. Restelli. Convex reinforcement learning in finite trials. Journal of Machine Learning Research, 24(250):1\u201342, 2023.   \n[44] G. Neu, A. Gyorgy, and C. Szepesvari. The adversarial stochastic shortest path problem with unknown transition probabilities. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 805\u2013813. PMLR, 2012.   \n[45] R. Ortner, P. Gajane, and P. Auer. Variational regret bounds for reinforcement learning. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115, pages 81\u201390, 2020.   \n[46] M. Prajapat, M. Mutny, M. Zeilinger, and A. Krause. Submodular reinforcement learning. In International Conference on Learning Representations (ICLR), 2024.   \n[47] A. Raj, P. Gaillard, and C. Saad. Non-stationary online regression, 2020.   \n[48] A. Rosenberg and Y. Mansour. Online convex optimization in adversarial Markov decision processes. In International Conference on Machine Learning (ICML), volume 97, pages 5478\u20135486, 2019.   \n[49] A. Rosenberg and Y. Mansour. Online convex optimization in adversarial Markov decision processes. In International Conference on Machine Learning (ICML), pages 5478\u20135486, 2019.   \n[50] A. Rosenberg and Y. Mansour. Stochastic shortest path with adversarially changing costs. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2936\u20132942, 2021.   \n[51] A. S\u00e9guret, C. Wan, and C. Alasseur. A mean field control approach for smart charging with aggregate power demand constraints. In 2021 IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe), pages 01\u201305, 2021.   \n[52] D. van der Hoeven, N. Zhivotovskiy, and N. Cesa-Bianchi. High-probability risk bounds via sequential predictors, 2023.   \n[53] C.-Y. Wei, C. Dann, and J. Zimmert. A model selection approach for corruption robust reinforcement learning. In International Conference on Algorithmic Learning Theory (ALT), volume 167, pages 1043\u20131096, 2022.   \n[54] C.-Y. Wei and H. Luo. Non-stationary reinforcement learning without prior knowledge: an optimal black-box approach. In Conference on Learning Theory (COLT), volume 134, pages 4300\u20134354, 2021.   \n[55] T. Zahavy, A. Cohen, H. Kaplan, and Y. Mansour. Apprenticeship learning via frank-wolfe. In AAAI Conference on Artificial Intelligence, 2019.   \n[56] T. Zahavy, B. O' Donoghue, G. Desjardins, and S. Singh. Reward is enough for convex mdps. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 25746\u201325759, 2021.   \n[57] J. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and M. Wang. Variational policy gradient method for reinforcement learning with general utilities. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 4572\u20134583, 2020.   \n[58] J. Zhang, C. Ni, z. Yu, C. Szepesvari, and M. Wang. On the convergence and sample efficiency of variance-reduced policy gradient method. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 2228\u20132240, 2021.   \n[59] L. Zhang, T. Yang, rong jin, and Z.-H. Zhou. Dynamic regret of strongly adaptive methods. In International Conference on Machine Learning (ICML), volume 80, pages 5882\u20135891, 2018.   \n[60] X. Zhang, Y. Chen, X. Zhu, and W. Sun. Robust policy gradient against strong data corruption. In International Conference on Machine Learning (ICML), volume 139, pages 12391\u201312401, 2021.   \n[61] A. Zimin and G. Neu. Online learning in episodic markovian decision processes by relative entropy policy search. In Advances in Neural Information Processing Systems (NeurIPS), volume 26, pages 1583\u20131591, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Learning with expert advice ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we take a closer look at the Learning with Expert Advice (LEA) framework. We start by presenting, in Subsection A.1, a general reduction of the sleeping experts framework to the standard framework. Thus, any LEA algorithm can be used as a sub-routine for MetaCURL. In Section 5 of the main paper, we show a regret bound for MetaCURL using the Exponentially Weighted Average Forecaster (EWA) algorithm [9], also known as Hedge. In Subsection A.2 we present the main results of playing EWA with convex and exp-concave losses. ", "page_idx": 14}, {"type": "text", "text": "Setting. We recall the general setting of learning with expert advice (LEA) as presented in the main paper: a learner makes sequential online predictions $u^{1},\\bar{\\ldots},u^{T}$ in a decision space $\\boldsymbol{\\mathcal{U}}$ , over a series of $T$ episodes, with the help of $K$ experts. For each round $t$ , each expert $k$ makes a prediction $u^{t,k}$ , and the learner combines the experts\u2019 predictions by computing a vector $v^{t}:=(v^{t,1},\\Bar{\\ldots},v^{t,K})\\in S_{K}$ , and predicting $\\begin{array}{r}{u^{t}:=\\sum_{k=1}^{K}v^{t,k}u^{t,k}}\\end{array}$ . The environment then reveals a convex loss function $\\ell^{t}:\\mathcal{U}\\to\\mathbb{R}$ Each expert suffers a loss $\\ell^{t,k}:=\\ell^{t}(u^{t,k})$ , and the learner suffers a loss $\\hat{\\ell}^{t}:=\\ell^{t}(u^{t})$ . The learner\u2019s objective is to keep the cumulative regret with respect to each expert as low as possible. For each expert $k$ , this quantity is defined as $\\begin{array}{r}{\\bar{\\mathrm{Reg}}_{[T]}(k):=\\bar{\\sum_{t=1}^{T}}\\hat{\\ell}^{t}-\\ell^{t,k}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Sleeping experts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The sleeping expert problem [8, 23] is a LEA framework where experts are not required to provide solutions at every time step. Let $\\bar{I}^{t,k}\\,\\in\\,\\{0,1\\}$ define a binary signal that equals 1 if expert $k$ is active at episode $t$ and 0 otherwise. The algorithm knows $(I^{t,k})_{k\\in[K]}$ and assigns a zero weight to sleeping experts. We would like to have a guarantee with respect to expert $k\\,\\in\\,[K]$ but only when it is active. Hence, we now aim to bound a cumulative regret that depends on the signal $\\begin{array}{r}{I^{t,k}\\colon\\mathrm{Reg}_{[T]}^{\\mathrm{sleep}}(k):=\\sum_{t=1}^{T}I^{t,k}(\\hat{\\ell}^{t}-\\ell^{t,k})}\\end{array}$ . We present a generic reduction from the sleeping expert framework to the standard LEA framework [3, 2]: ", "page_idx": 14}, {"type": "text", "text": "Let, for all episodes $t\\in[T]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{u}^{t}:=\\frac{\\sum_{k=1}^{K}I^{t,k}v^{t,k}u^{t,k}}{\\sum_{k=1}^{K}I^{t,k}v^{t,k}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We play a standard LEA algorithm with modified outputs where, at episode $t$ , expert $k$ outputs ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{u}^{t,k}:=\\left\\{\\!\\!\\begin{array}{l l}{u^{t,k},\\quad\\mathrm{~if~}k\\mathrm{~is~active~at~episode~}t}\\\\ {\\hat{u}^{t},\\quad}&{\\mathrm{if~not}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A standard LEA algorithm gives an upper bound on the regret ${\\mathrm{Reg}}_{T}(k)$ with respect to each expert $k$ . Using that $\\textstyle\\sum_{k=1}^{K}{\\tilde{u}}^{t,k}v^{t,k}={\\hat{u}}^{t}$ , we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Reg}_{[T]}(k):=\\sum_{t=1}^{T}\\ell^{t}\\bigg(\\displaystyle\\sum_{k=1}^{K}\\tilde{u}^{t,k}v^{t,k}\\bigg)-\\ell^{t}(\\tilde{u}^{t,k})}}\\\\ &{}&{\\;\\;\\;\\;\\;\\;\\;\\;=\\displaystyle\\sum_{t=1}^{T}\\ell^{t}(\\hat{u}^{t})-\\ell^{t}(\\tilde{u}^{t,k})}\\\\ &{}&{\\;\\;\\;\\;\\;\\;\\;\\;=\\displaystyle\\sum_{t=1}^{T}I^{t,k}\\big(\\ell^{t}(\\hat{u}^{t})-\\ell^{t}(u^{t,k})\\big)}\\\\ &{}&{\\;\\;\\;\\;\\;\\;=:\\mathrm{Reg}_{[T]}^{\\mathrm{sleop}}(k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, the cumulative regret with respect to each expert during the times it is active is upper bounded by the standard regret of playing a LEA algorithm with the modified outputs. ", "page_idx": 14}, {"type": "text", "text": "A.2 Exponentially Weighted Average forecaster ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The exponentially weighted average forecaster (EWA), also called Hedge, is a LEA algorithm that chooses a weight that decreases exponentially fast with past errors. We present EWA in Alg. 2. ", "page_idx": 14}, {"type": "text", "text": "Input: $[K]:=\\{1,\\ldots,K\\}$ a finite set of experts, $v^{0}$ a prior over $[K]$ , a learning rate $\\eta>0$   \nfor $t\\in\\{1,\\ldots,T\\}$ do Observe loss function $\\ell^{t}$ , compute the loss suffered by each expert $\\ell^{t,k}:=\\ell^{t}(u^{t,k})$ and suffer   \nloss $\\begin{array}{r}{\\hat{\\ell}^{t}:=\\ell\\Big(\\sum_{k=1}^{K}v^{t,k}u^{t,k}\\Big)}\\end{array}$ Update for all $k\\in[K]$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nv^{t+1,k}=\\frac{v^{t,k}\\exp{(-\\eta\\ell^{t,k})}}{\\sum_{k^{\\prime}=1}^{K}v^{t,k^{\\prime}}\\exp{(-\\eta\\ell^{t,k^{\\prime}})}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "end for ", "page_idx": 15}, {"type": "text", "text": "We recall two results of playing EWA with convex losses, and with exp-concave losses, used in the main paper: ", "page_idx": 15}, {"type": "text", "text": "Theorem A.1 (EWA with convex losses: Corollary 2.2 from [9]). If the $\\ell^{t}$ losses are convex and take value in $[0,1]$ , then the regret of the learner playing EWA with any $\\eta>0$ satisfies, for any $k\\in[K]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nR e g_{[T]}(k)\\leq\\frac{\\log(K)}{\\eta}+\\frac{T\\eta}{8}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, with $\\eta=\\sqrt{8\\log(K)/T}$ , the upper bound becomes $\\sqrt{(T/2)\\log(K)}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem A.2 (EWA with exp-concave losses: Thm. 3.2 from [9]). If the $\\ell^{t}$ losses are $\\eta$ -exp concave, then the regret of the learner playing EWA (with the same value of $\\eta_{,}$ ) satisfies, for any $k\\in[K]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nR e g_{[T]}(k)\\leq\\frac{\\log(K)}{\\eta}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Algorithm for the online estimation of the probability kernel $(\\hat{p}^{t}$ estimator) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Online estimation of the probability kernel $\\hat{p}^{t}$ estimator)   \n1: for $t\\in\\{1,\\ldots,T\\}$ do   \n2: Get agent\u2019s external noise trajectory $(\\varepsilon_{n}^{t})_{n\\in[N]}$ from MetaCURL   \n3: for $(n,x,a)\\in[N]\\times\\mathcal{X}\\times\\mathcal{A}$ do   \n4: Compute $x_{n,x,a}^{t\\phantom{}}=g_{n-1}(x,a,\\varepsilon_{n-1}^{t})$   \n5: Update the empirical estimations for $s<t$ and $x^{\\prime}\\in\\mathcal{X}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}_{n}^{t,s}(x^{\\prime}|x,a)=\\frac{\\mathbb{1}_{\\{x_{n,x,a}^{t}=x^{\\prime}\\}}}{t-s}+\\frac{(t-1-s)}{t-s}\\hat{p}_{n}^{t-1,s}(x^{\\prime}|x,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "6: Initialize a new estimator, $\\hat{p}_{n}^{t,t}(x^{\\prime}|x,a)\\,=\\,\\mathbb{1}_{\\{x_{n,x,a}^{t}=x^{\\prime}\\}}$ for all $x^{\\prime}\\,\\in\\,\\chi$ , assign it a new $\\textstyle v_{n,x,a}^{t,t}={\\frac{1}{t}}$ ,i sa na dp rnoobrambailliitzye  vwecetiogrh it nv ors $v_{n,x,a}^{t,s}$ for $s\\,\\in\\,[t-1]$ such that $v_{n,x,a}^{t}:=(v_{n,x,a}^{t,s})_{s\\leq t,\\lambda\\in\\Lambda}$ $\\mathbb{R}^{t}$ ", "page_idx": 16}, {"type": "text", "text": "7: Simulate a sample $\\tilde{x}_{n,x,a}^{t}$ from distribution $\\textstyle\\left(p_{n}^{t}(\\cdot|x,a)+{\\frac{1}{|x|}}\\right)/2$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{x}_{n,x,a}^{t}=\\left\\{\\begin{array}{l l}{\\;\\;x_{n,x,a}^{t},\\mathrm{~with~probability~}1/2,}\\\\ {\\;\\;\\mathrm{Uniformly~over~}\\mathcal{X},\\mathrm{~with~probability~}1/2,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and use it to build the loss function ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell^{t}(q):=\\sum_{x\\in\\mathcal{X}}-\\log\\Big(q(x)+\\frac{1}{|\\mathcal{X}|}\\Big)\\mathbb{1}_{\\{\\tilde{x}_{n,x,a}^{t}=x\\}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "8: Update weights using EWA with loss $\\ell^{t}$ : for all $s\\leq t$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nv_{n,x,a}^{t+1,s}=\\frac{\\hat{v}_{n,x,a}^{t,s}\\exp\\big(-\\ell^{t}(\\hat{p}_{n}^{t,s}(\\cdot|x,a))\\big)}{\\sum_{s^{\\prime}=1}^{t}\\hat{v}_{n,x,a}^{t,s^{\\prime}}\\exp\\big(-\\ell^{t}(\\hat{p}_{n}^{t,s^{\\prime}}(\\cdot|x,a))\\big)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "9: Compute $\\begin{array}{r}{\\hat{p}_{n}^{t+1}(\\cdot|x,a)=\\sum_{s=1}^{t}v_{n,x,a}^{t+1,s}\\hat{p}_{n}^{t,s}(\\cdot|x,a)}\\end{array}$   \n10: end for   \n11: Issue $\\hat{p}^{t+1}$ to MetaCURL (line 18 of Alg. 1)   \n12: end for ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C Auxiliary lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We start with some auxiliary results. For $t\\in I:=[t_{s}+1,t_{e}]\\subseteq[T]$ , we define the average probability distribution for all $n$ and $(x,a)$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{p}}^{t}(x^{\\prime}|x,a)=\\frac{1}{t-t_{s}}\\sum_{s=t_{s}}^{t-1}p_{n}^{s}(x^{\\prime}|x,a).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.1. Let $\\hat{p}^{t,t_{s}}$ be the empirical probability transition kernel computed with data from episodes $[t_{s},t-1]$ . For any $\\delta\\in(0,1)$ , with probability $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Vert\\hat{p}_{n}^{t,t_{s}}(\\cdot|x,a)-\\overline{{p}}_{n}^{t}(\\cdot|x,a)\\Vert_{1}\\leq\\sqrt{\\frac{1}{2(t-t_{s})}\\log\\left(\\frac{N|\\mathcal{X}||\\mathcal{A}|2^{|\\mathcal{X}|}T}{\\delta}\\right)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "simultaneously for all $n\\in[N],\\,(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\,t_{s}\\in[T-1],$ , and $t\\in[t_{s}+1,T]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. For a fixed $n\\in[N],(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , and $\\theta\\in\\{-1,1\\}^{|\\mathcal{X}|}$ , we define for all $s\\in I$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nY_{n,x,a,\\theta}^{s}:=\\sum_{x^{\\prime}\\in\\mathcal{X}}\\theta(x^{\\prime})\\mathbb{1}_{\\{g_{n}(x,a,\\varepsilon_{n}^{s})=x^{\\prime}\\}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "a Bernoulli random variable with mean value given by $\\begin{array}{r}{\\sum_{x^{\\prime}\\in\\mathcal{X}}\\theta(x^{\\prime})p_{n}^{s}(x^{\\prime}|x,a)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "The sequence of random variables given by $\\left(Y_{n,x,a,\\theta}^{s}\\right)_{s\\in I}$ is independent, as we assume that the external noises observed at each episode are all independent. Hence, by Hoeffding\u2019s inequality we get that, for all $\\xi>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\sum_{s=t_{s}}^{t-1}Y_{n,x,a,\\theta}^{s}-\\mathbb{E}\\Big[\\sum_{s=t_{s}}^{t-1}Y_{n,x,a,\\theta}^{s}\\Big]\\geq\\xi\\bigg)\\leq\\exp\\Big(\\frac{-2\\xi^{2}}{t-t_{s}}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\bigg(\\displaystyle\\sum_{x^{\\prime}\\in\\mathcal X}\\theta(x^{\\prime})\\big(\\hat{p}_{n}^{t,t_{s}}(x^{\\prime}|x,a)-\\bar{p}_{n}^{t}(x^{\\prime}|x,a)\\big)\\geq\\xi\\bigg)}\\\\ &{\\qquad=\\mathbb P\\bigg(\\displaystyle\\frac1{t-t_{s}}\\bigg[\\displaystyle\\sum_{s=t_{s}}^{t-1}\\sum_{x^{\\prime}\\in\\mathcal X}\\theta(x^{\\prime})\\mathbb1_{\\{g_{n}(x,a,\\xi_{n}^{s})=x^{\\prime}\\}}-\\displaystyle\\sum_{s=t_{s}}^{t-1}\\theta(x^{\\prime})p_{n}^{s}(x^{\\prime}|x,a)\\bigg]\\geq\\xi\\bigg)}\\\\ &{\\qquad=\\mathbb P\\bigg(\\displaystyle\\frac1{t-t_{s}}\\bigg(\\displaystyle\\sum_{s=t_{s}}^{t-1}Y_{n,x,a,\\theta}^{s}-\\mathbb E\\Big[\\displaystyle\\sum_{s=t_{s}}^{t-1}Y_{n,x,a,\\theta}^{s}\\Big]\\bigg)\\geq\\xi\\bigg)}\\\\ &{\\qquad\\leq\\exp\\big(-2\\xi^{2}(t-t_{s})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By applying an union bound on all $n\\in[N]$ , $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , and $\\theta\\in\\{-1,1\\}^{|\\mathcal{X}|}$ and noting that, for any two probability distributions $p,q\\in\\Delta_{\\mathcal{X}}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|p-q\\|_{1}=\\operatorname*{max}_{\\theta\\in\\{-1,1\\}^{|x|}}\\sum_{x\\in\\mathcal{X}}\\theta(x)(p(x)-q(x)),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we arrive at the final result. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.2. Let $t\\in I:=[t_{s}+1,t_{e}]\\subseteq[T]$ . For all $n\\in[N]$ , and $(x,a)\\in\\mathcal{X}\\times\\mathcal{A},$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|p_{n}^{t}(\\cdot|x,a)-\\overline{{p}}_{n}^{t}(\\cdot|x,a)\\|_{1}\\leq\\sum_{j=t_{s}}^{t-1}\\Delta_{j}^{p},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For $t\\in I$ , and for all $n$ and $(x,a)$ we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert p_{n}^{t}(\\cdot\\vert x,a)-\\bar{p}_{n}^{t}(\\cdot\\vert x,a)\\Vert_{1}=\\underset{x^{\\prime}\\in\\mathcal{X}}{\\sum}\\left\\vert p_{n}^{t}(x^{\\prime}\\vert x,a)-\\frac{1}{t-t_{s}}\\underset{\\leq}{\\sum}p_{n}^{t-1}(x^{\\prime}\\vert x,a)\\right\\vert}\\\\ &{\\underset{x^{\\prime}\\in\\mathcal{X}}{\\sum}\\frac{1}{t-t}\\underset{s=t_{s}}{\\sum}\\left\\vert\\underset{p_{n}=t_{s}}{\\sum}^{t-1}(p_{n}^{t}(x^{\\prime}\\vert x,a)-p_{n}^{s}(x^{\\prime}\\vert x,a))\\right\\vert}\\\\ &{\\leq\\frac{1}{t-t_{s}}\\underset{s=t_{s}}{\\sum}\\underset{\\leq}{\\sum}\\Vert p_{n}^{t}(\\cdot\\vert x,a)-p_{n}^{j-1}(\\cdot\\vert x,a)\\Vert_{1}}\\\\ &{=\\frac{1}{t-t_{s}}\\underset{j=t_{s}}{\\sum}(j-t_{s})\\Vert p_{n}^{j}(\\cdot\\vert x,a)-p^{j-1}(\\cdot\\vert x,a)\\Vert_{1}}\\\\ &{\\leq\\underset{j=t_{s}}{\\sum}\\frac{1}{t-t_{s}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where recall that we define $\\begin{array}{r}{\\Delta_{j}^{p}:=\\operatorname*{max}_{n,s,a}\\|p_{n}^{j+1}(\\cdot|x,a)-p_{n}^{j}(\\cdot|x,a)\\|_{1}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma C.3. Let $\\hat{p}^{t,t_{s}}$ be the empirical probability transition kernel computed with data from episodes $[t_{s},t-1]$ . For any $\\delta\\in(0,1)$ , with probability $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|p_{n}^{t}\\big(\\cdot|x,a\\big)-\\hat{p}_{n}^{t,t_{s}}(\\cdot|x,a)\\|_{1}\\leq\\sqrt{\\frac{1}{2\\big(t-t_{s}\\big)}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|}T}{\\delta}\\right)}+\\sum_{j=t_{s}}^{t-1}\\Delta_{j}^{p},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "simultaneously for all $n\\in[N],\\,(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\,t_{s}\\in[T-1]$ , and $t\\in[t_{s}+1,T]$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma C.4 (A version of the inverse of Pinsker\u2019s inequality). Let $p^{\\prime},q^{\\prime}$ be any distributions over $\\mathcal{S}_{\\mathcal{X}}$ . Define ", "page_idx": 18}, {"type": "equation", "text": "$$\np:=\\frac{p^{\\prime}+\\frac{1}{|\\mathcal{X}|}}{2},\\quad a n d\\quad q:=\\frac{q^{\\prime}+\\frac{1}{|\\mathcal{X}|}}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\nK L(p\\mid q)\\leq2|\\mathcal{X}|\\|p-q\\|_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. First, note that $q$ is lower bounded by $\\frac{1}{2|\\mathcal{X}|}$ , hence $\\mathsf{K L}(p\\mid q)$ is well defined. Also, by convexity of the simplex, $p,q\\in S_{\\mathcal{X}}$ , therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p\\,|\\,q)=\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}p(x)\\log\\left(\\frac{p(x)}{q(x)}\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}p(x)\\log\\left(1+\\left(\\frac{p(x)}{q(x)}-1\\right)\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}p(x)\\left(\\frac{p(x)}{q(x)}-1\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}\\frac{p(x)}{q(x)}\\left(p(x)-q(x)\\right)+\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}\\left(q(x)-p(x)\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{X}}\\frac{(p(x)-q(x))^{2}}{q(x)}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{\\sum_{\\varepsilon\\in\\mathcal{X}}q(x)}{\\operatorname*{min}_{\\varepsilon\\in\\mathcal{X}}q(x)}\\|p-q\\|_{1}^{2}}\\\\ &{\\qquad\\leq2|\\mathcal{X}|\\|p-q\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma C.5. For any strategy $\\pi\\in(S_{\\mathcal{A}})^{\\mathcal{X}\\times N}$ , for any two probability kernels $p=(p_{n})_{n\\in[N]}$ and $q=(q_{n})_{n\\in[N]}$ such that $p_{n},q_{n}:\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{X}\\to[0,1]$ , and for all $n\\in[N]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mu_{n}^{\\pi,p}-\\mu_{n}^{\\pi,q}\\|_{1}\\leq\\sum_{i=0}^{n-1}\\sum_{x,a}\\mu_{i}^{\\pi,p}(x,a)\\|p_{i+1}(\\cdot|x,a)-q_{i+1}(\\cdot|x,a)\\|_{1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. From the definition of a state-action distribution sequence induced by a policy $\\pi$ in a MDP with probability kernel $p$ in Eq. (2), we have that for all $(x,{\\bar{a}})\\in{\\mathcal{X}}\\times{\\mathcal{A}}$ and $n\\in[N]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{n}^{\\pi,p}(x,a)=\\sum_{x^{\\prime},a^{\\prime}}\\mu_{n-1}^{\\pi,p}(x^{\\prime},a^{\\prime})p_{n}(x|x^{\\prime},a^{\\prime})\\pi_{n}(a|x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mu_{n}^{\\tau,p}-\\mu_{n}^{\\tau,q}\\|_{1}=\\sum_{\\ell,n}\\mu_{n}^{\\tau,p}(x,a)-\\mu_{n}^{\\tau,q}(x,a)\\|}}\\\\ &{=\\sum_{\\ell\\leq0}\\sum_{\\ell^{\\prime},\\ell^{\\prime}}\\left|\\mu_{n}^{\\tau,p}(x^{\\prime},a^{\\prime})p_{n}(x|^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\tau,q}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})\\right|\\pi_{n}(a|x)}\\\\ &{=\\sum_{\\ell\\leq0}\\sum_{\\ell^{\\prime},\\ell^{\\prime}}\\Big|\\mu_{n}^{\\tau,p}(x^{\\prime},a^{\\prime})p_{n}(x|^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\tau,q}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})\\Big|}\\\\ &{=\\sum_{\\ell^{\\prime},\\ell^{\\prime},\\ell^{\\prime}}\\Big|\\mu_{n}^{\\tau,p}(x^{\\prime},a^{\\prime})p_{n}(x|^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\tau,p}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})}\\\\ &{=\\displaystyle\\sum_{x^{\\prime}\\leq t,x^{\\prime}\\leq t^{\\prime}}\\Big|\\mu_{n-1}^{\\tau,p}(x^{\\prime},a^{\\prime})p_{n}(x|^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\tau,p}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})}\\\\ &{\\qquad+\\mu_{n-1}^{\\tau,p}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\tau,q}(x^{\\prime},a^{\\prime})q_{n}(x|^{\\prime},a^{\\prime})\\Big|}\\\\ &{\\leq\\displaystyle\\sum_{\\ell^{\\prime},n}\\mu_{n-1}^{\\tau,p}(x^{\\prime},a^{\\prime})\\|p_{n}(x|^{\\prime},a^{\\prime})-q_{n}(\\cdot|x^{\\prime},a^{\\prime})\\|_{1}+\\displaystyle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since for $n=0$ , $\\lVert\\boldsymbol{\\mu}_{0}^{\\pi,p}-\\boldsymbol{\\mu}_{0}^{\\pi,q}\\rVert_{1}=0$ , by induction we get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Vert\\mu_{n}^{\\pi,p}-\\mu_{n}^{\\pi,q}\\Vert_{1}\\leq\\sum_{i=0}^{n-1}\\sum_{x^{\\prime},a^{\\prime}}\\mu_{i}^{\\pi,p}(x^{\\prime},a^{\\prime})\\Vert p_{i+1}(\\cdot|x^{\\prime},a^{\\prime})-q_{i+1}(\\cdot|x^{\\prime},a^{\\prime})\\Vert_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma C.6. For any pair of strategies $\\pi$ $\\mathsf{r},\\pi^{\\prime}\\in(\\Delta_{A})^{\\mathcal{X}\\times N},$ , for any probability kernel $p=(p_{n})_{n\\in[N]}$ such that $p_{n}:\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{X}\\to[0,1],$ , and for all $n\\in[N]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mu_{n}^{\\pi,p}-\\mu_{n}^{\\pi^{\\prime},p}\\|_{1}\\leq\\sum_{i=1}^{n}\\sum_{x\\in\\mathcal{X}}\\rho_{i}^{\\pi,p}(x)\\|\\pi_{i}(\\cdot|x)-\\pi_{i}^{\\prime}(\\cdot|x)\\|_{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho_{i}^{\\pi,p}(x):=\\sum_{a\\in\\mathcal{A}}\\mu_{i}^{\\pi,p}(x,a)}\\end{array}$ for all $x\\in\\mathscr{X}$ and $i\\in[N]$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Using the recursive relation from Eq. (2) of a state-action distribution induced by a policy $\\pi$ in a MDP with probability transition $p$ we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\mu_{n}^{\\pi,p}-\\mu_{n}^{\\pi^{\\prime},p}\\|_{1}=\\sum_{x,a}\\big|\\mu_{n}^{\\pi,p}(x,a)-\\mu_{n}^{\\pi^{\\prime},p}(x,a)\\big|}}\\\\ &{\\leq\\sum_{x,a}\\sum_{x^{\\prime},a^{\\prime}}\\big|\\mu_{n-1}^{\\pi,p}(x^{\\prime},a^{\\prime})\\pi_{n}(a|x)-\\mu_{n-1}^{\\pi^{\\prime},p}(x^{\\prime},a^{\\prime})\\pi_{n}^{\\prime}(a|x)\\big|p_{n}(x|x^{\\prime},a^{\\prime})}\\\\ &{\\leq\\sum_{x,a}\\sum_{x^{\\prime},a^{\\prime}}\\mu_{n-1}^{\\pi,p}(x^{\\prime},a^{\\prime})p_{n}(x|x^{\\prime},a^{\\prime})\\big|\\pi_{n}(a|x)-\\pi_{n}^{\\prime}(a|x)\\big|}\\\\ &{\\qquad+\\displaystyle\\sum_{x,a}\\sum_{x^{\\prime},a^{\\prime}}\\pi_{n}^{\\prime}(a|x)p_{n}(x|x^{\\prime},a^{\\prime})\\big|\\mu_{n-1}^{\\pi,p}(x^{\\prime},a^{\\prime})-\\mu_{n-1}^{\\pi^{\\prime},p}(x^{\\prime},a^{\\prime})\\big|}\\\\ &{=\\displaystyle\\sum_{x}\\rho_{n}^{\\pi,p}(x)\\|\\pi_{n}(\\cdot|x)-\\pi_{n}^{\\prime}(\\cdot|x)\\|_{1}+\\|\\mu_{n-1}^{\\pi,p}-\\mu_{n-1}^{\\pi^{\\prime},p}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\mu_{0}$ is fixed for each state-action distribution sequence, by induction we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mu_{n}^{\\pi,p}-\\mu_{n}^{\\pi^{\\prime},p}\\|_{1}\\leq\\sum_{i=1}^{n}\\sum_{x}\\rho_{i}^{\\pi,q}(x)\\|\\pi_{i}^{t}(\\cdot|x)-\\pi_{i}^{t-1}(\\cdot|x)\\|_{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "completing the proof. ", "page_idx": 19}, {"type": "text", "text": "D Proof of Prop. 5.2: $R_{[T]}^{\\hat{p}}$ regret analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Here, we set an upper bound on the term $R_{[T]}^{\\hat{p}}$ where we pay for errors in estimating $p^{t}$ by $\\hat{p}^{t}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{[T]}^{\\hat{p}}:=\\displaystyle\\sum_{t=1}^{T}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla F^{t}(\\mu^{\\pi^{t},p^{t}}),\\mu^{\\pi^{t},p^{t}}-\\mu^{\\pi^{t},\\hat{p}^{t}}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq L_{F}\\displaystyle\\sum_{t=1}^{T}\\sum_{n=1}^{N}\\|\\mu_{n}^{\\pi^{t},p^{t}}-\\mu_{n}^{\\pi^{t},\\hat{p}^{t}}\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq L_{F}\\displaystyle\\sum_{t=1}^{T}\\sum_{n=1}^{N}\\sum_{j=1}^{n}\\mu_{j-1}^{\\pi^{t},p^{t}}(x,a)\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To obtain the first inequality, we use the convexity of $F^{t}$ for all $t\\,\\in\\,[T]$ , then we use Holder\u2019s inequality and the fact that $F^{t}$ is $L_{F}$ -Lipschitz, and for the last inequality we use Lemma C.5. ", "page_idx": 20}, {"type": "text", "text": "The difficulty in analyzing the $L_{1}$ difference between $p^{t}$ and $\\hat{p}^{t}$ arises from the non-stationarity of $p^{t}$ . To overcome this we want to use the scheme presented in Subsection 4.3. By Cauchy-Schwartz, we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{[T]}^{\\hat{p}}\\leq L_{F}\\sqrt{\\underbrace{\\sum_{t=1}^{T}\\sum_{n=1}^{N}\\sum_{j=1}^{n}\\sum_{x,a}(\\mu_{j-1}^{\\pi^{t},p^{t}}(x,a))^{2}}_{\\leq T N^{2}}\\underbrace{\\sum_{t=1}^{T}\\sum_{n=1}^{N}\\sum_{j=1}^{n}\\sum_{x,a}\\Vert p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\Vert_{1}^{2}}_{(*)}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Analysis of the $L_{1}$ norm. We start by analysing the sum over $t\\in[T]$ of the $L_{1}$ norm in term $(*)$ . For each $n\\in[N],j\\in[n]$ and $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}^{2}=4\\sum_{t=1}^{T}\\left\\|\\frac{p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|x|}}{2}-\\left(\\frac{\\hat{p}_{j}^{t}(\\cdot|x,a)+\\frac{1}{|x|}}{2}\\right)\\right\\|_{1}^{2}}}\\\\ &{}&{\\leq8\\sum_{t=1}^{T}\\mathrm{KL}\\left(\\frac{p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|x|}}{2}\\bigg|\\frac{\\hat{p}_{j}^{t}(\\cdot|x,a)+\\frac{1}{|x|}}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we apply Pinsker\u2019s inequality. ", "page_idx": 20}, {"type": "text", "text": "Consider a sequence of episodes $1=t_{1}<t_{2}<.\\ldots<t_{m+1}=T+1$ , with $\\mathcal{T}_{i}:=[t_{i},t_{i+1}-1]$ , such that $\\Delta_{\\mathcal{T}_{i.}}^{p}\\leq\\bar{\\Delta^{p}}/m$ for all $i\\in[m]$ . Decomposing the KL sum over $t\\in[T]$ as a sum on the intervals ${\\mathcal{Z}}_{i}$ , we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\mathrm{KL}\\left(\\frac{p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|X|}}{2}\\bigg|\\frac{\\hat{p}_{j}^{t}(\\cdot|x,a)+\\frac{1}{|X|}}{2}\\right)=\\sum_{i=1}^{m}\\sum_{t\\in\\mathcal{T}_{i}}\\mathrm{KL}\\left(\\frac{p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|X|}}{2}\\bigg|\\frac{\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)+\\frac{1}{|X|}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n+\\sum_{i=1}^{m}\\sum_{t\\in\\mathcal{T}_{i}}\\mathbb{E}_{\\tilde{x}_{j,x,a}^{t}}\\left[\\log\\left(\\frac{\\hat{p}_{j}^{t,t_{i}}(\\tilde{x}_{j,x,a}^{t}|x,a)+\\frac{1}{|\\mathcal{X}|}}{2}\\right)-\\log\\left(\\frac{\\hat{p}_{j}^{t}(\\tilde{x}_{j,x,a}^{t}|x,a)+\\frac{1}{|\\mathcal{X}|}}{2}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the expectation of the second term is with respect to $\\tilde{x}_{j,x,a}^{t}\\sim\\left(p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|\\mathcal{X}|}\\right)/2.$ ", "page_idx": 20}, {"type": "text", "text": "We analyse each term separately: ", "page_idx": 20}, {"type": "text", "text": "First, note that $(i i)$ is the expectation over $\\tilde{x}_{j,x,a}^{t}$ of the cum ulative regret of sleeping EWA on interval ${\\mathcal{Z}}_{i}$ with respect to the expert $t_{i}$ using the loss function defined in Eq. (12). This term is upper bounded by $\\log(T)$ (see Subection 4.3 of the main paper). From it we deduce that ", "page_idx": 20}, {"type": "equation", "text": "$$\n(i i)\\leq\\sum_{i=1}^{m}\\log(T)=m\\log(T).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Regarding term $(i)$ , we start by using the inverse of Pinsker\u2019s inequality presented in Lemma C.4, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(i)\\leq\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}2|\\mathcal{X}|\\left\\|\\frac{p_{j}^{t}(\\cdot|x,a)+\\frac{1}{|\\mathcal{X}|}}{2}-\\left(\\frac{\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)+\\frac{1}{|\\mathcal{X}|}}{2}\\right)\\right\\|_{1}^{2}}\\\\ {\\displaystyle=\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\frac{|\\mathcal{X}|}{2}\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)\\|_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To simplify notations, from now on we let $\\begin{array}{r}{C:=\\sqrt{\\frac{1}{2}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|}T}{\\delta}\\right)}}\\end{array}$ . Applying Lemma C.3, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\frac{|\\mathcal{X}|}{2}\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t,t_{i}}(\\cdot|x,a)\\|_{1}^{2}}}\\\\ &{\\leq\\frac{|\\mathcal{X}|}{2}\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\bigg[\\frac{C^{2}}{t-t_{i}+1}+\\frac{2C}{\\sqrt{t-t_{i}+1}}\\sum_{k=t_{i}}^{t-1}\\Delta_{k}^{p}+\\bigg(\\displaystyle\\sum_{k=t_{i}}^{t-1}\\Delta_{k}^{p}\\bigg)^{2}\\bigg]}\\\\ &{\\leq\\frac{|\\mathcal{X}|}{2}\\sum_{i=1}^{m}\\big[C^{2}\\log(|Z_{i}|)+2C\\sqrt{|\\mathcal{Z}_{i}|}\\Delta_{T_{i}}^{p}+|Z_{i}|(\\Delta_{T_{i}}^{p})^{2}\\big]}\\\\ &{\\leq\\frac{|\\mathcal{X}|}{2}\\bigg[C^{2}m\\log(T)+2C\\sqrt{T}\\frac{\\Delta^{p}}{\\sqrt{m}}+T\\frac{(\\Delta^{p})^{2}}{m^{2}}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Joining the upper bounds of $(i)$ and $(i i)$ we conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}^{2}\\leq8\\big((i)+(i i)\\big)}}\\\\ &{}&{\\leq8\\Big((C^{2}\\frac{|\\mathcal{X}|}{2}+1)m\\log(T)+2\\frac{|\\mathcal{X}|}{2}C\\sqrt{T}\\frac{\\Delta^{p}}{\\sqrt{m}}+T\\frac{|\\mathcal{X}|}{2}\\frac{(\\Delta^{p})^{2}}{m^{2}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, for $\\begin{array}{r}{m=\\left(\\frac{2T\\Delta^{p}}{C^{2}\\log(T)}\\right)^{1/3}}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}^{2}\\leq12|\\mathcal{X}|C^{4/3}\\log(T)^{2/3}T^{1/3}(\\Delta^{p})^{2/3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Back to the analysis of $R_{[T]}^{\\hat{p}}$ . Using the inequality in Eq. (17) to bound the $L_{1}$ norm of $(*)$ on Eq. (16), we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{[T]}^{\\hat{p}}\\leq L_{F}N\\sqrt{T}\\sqrt{\\displaystyle\\sum_{n=1}^{N}\\sum_{j=1}^{n-1}\\sum_{x,a}12|\\mathcal{X}|C^{4/3}\\log(T)^{2/3}T^{1/3}(\\Delta^{p})^{2/3}}}\\\\ &{\\qquad\\leq2L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|\\mathcal{A}|}C^{2/3}\\log(T)^{1/3}T^{2/3}(\\Delta^{p})^{1/3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "Note that for $\\begin{array}{r}{\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}R_{\\mathbb{Z}_{i}}^{\\hat{p}}(\\pi^{t,t_{i},\\lambda})}\\end{array}$ (the third term of the meta-regret decomposition of Eq. (13)), following the  same procedure as above, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}F^{t}(\\mu^{\\pi^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}})-F^{t}(\\mu^{\\pi^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}})}\\\\ &{\\quad\\le\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\langle\\nabla F^{t}(\\mu^{\\pi^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}}),\\mu^{\\pi^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}}-\\mu^{\\pi^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}}\\rangle}\\\\ &{\\quad\\le L_{F}\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\|\\mu_{n}^{\\pi_{t}^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}}-\\mu_{n}^{\\pi_{t}^{t,t_{i}},\\hat{\\boldsymbol{p}}^{t}}\\|_{1}}\\\\ &{\\quad\\le L_{F}\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}\\sum_{n=1}^{N}\\sum_{x,a}\\mu_{j-1}^{\\pi_{t}^{t,t_{i}},\\boldsymbol{p}^{t}}(x,a)\\|\\boldsymbol{p}_{j}^{t}(\\cdot|x,a)-\\hat{\\boldsymbol{p}}_{j}^{t}(\\cdot|x,a)\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the second term is independent of $\\pi^{t,t_{i}}$ , the analysis is the same as before and we obtain the same upper bound as for $R_{[T]}^{\\hat{p}}\\bar{(}\\pi^{t})$ . ", "page_idx": 22}, {"type": "text", "text": "E Proof of Prop. 5.4: $R_{[T]}^{\\mathbf{black-box}}$ regret analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Assume a Black-box algorithm satisfying the dynamic regret bound of Eq. (10), i.e., for any interval $I\\subseteq[T]$ , with respect to any sequence of policies $(\\pi^{t,*})_{t\\in I}$ , and for any learning rate $\\lambda$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{I}\\big((\\pi^{t,*})_{t\\in I}\\big)\\leq c_{1}\\lambda|I|+\\frac{c_{2}\\Delta_{I}^{\\pi^{*}}+c_{3}}{\\lambda}+|I|\\Delta_{I}^{p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consider any sequence of episodes $1\\,=\\,t_{1}\\,<\\,t_{2}\\,<\\,\\ldots\\,<\\,t_{m+1}\\,=\\,T+1$ , forming intervals $\\bar{\\mathcal{Z}_{i}}:=[t_{i},t_{i+1}-\\bar{1}]$ for all $i\\in[m]$ . We can decompose the black-box regret over $[T]$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{[T]}^{\\mathrm{black-box}}\\left((\\pi^{t,*})_{t\\in[T]}\\right)=\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})-F^{t}(\\mu^{\\pi^{t,*},p^{t}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i=1}^{m}c_{1}\\lambda|\\mathbb{Z}_{i}|+\\displaystyle\\frac{c_{2}\\Delta_{T_{i}}^{*}+c_{3}}{\\lambda}+|\\mathbb{Z}_{i}|\\Delta_{I}^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq c_{1}\\lambda T+\\displaystyle\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{\\lambda}+\\displaystyle\\frac{T\\Delta^{p}}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In principle, we would like to select the optimal $\\lambda$ that optimizes this dynamic regret. However, as $\\Delta^{\\bar{\\pi^{*}}}$ may be unknown in advance, this is not possible. We show here that running MetaCURL with the learning rate set $\\Lambda:=\\{2^{-j}|j=0,1,2,\\dot{\\cdot}\\cdot.\\cdot,\\lceil[\\log_{2}(T)/2]\\rceil$ ensures that the optimal empirical learning rate is close to the true optimal one by a factor of 2 and that the learner always plays as well as the optimal empirical learning rate. ", "page_idx": 22}, {"type": "text", "text": "Denote by $\\lambda^{*}$ the optimal learning rate and \u03bb\u02c6\u2217the empirical optimal learning rate in $\\Lambda$ . Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\sqrt{\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}T}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We consider three different cases: ", "page_idx": 22}, {"type": "text", "text": "If $\\lambda^{*}\\geq1;$ : this implies that $\\begin{array}{r}{\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}T}\\geq1}\\end{array}$ . Therefore, we have that $\\begin{array}{r}{T\\leq\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}}\\end{array}$ . As we assume $f_{n}^{t}\\in[0,1]$ for all time steps $n\\in[N]$ and episodes $t\\in[T]$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{[T]}^{\\mathrm{black-box}}\\bigl((\\pi^{t,*})_{t\\in[T]}\\bigr)\\leq N T+\\frac{T\\Delta^{p}}{m}\\leq N\\biggl(\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\biggr)+\\frac{T\\Delta^{p}}{m}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $\\lambda^{*}\\leq1/\\sqrt{T}$ : this implies that $\\begin{array}{r}{\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\leq1}\\end{array}$ . Therefore, taking $\\hat{\\lambda}^{*}=1/\\sqrt{T}\\in\\Lambda$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{[T]}^{\\mathrm{black-box}}\\bigl((\\pi^{t,*})_{t\\in[T]}\\bigr)\\leq\\lambda c_{1}T+\\frac{c_{1}}{\\lambda}+\\frac{T\\Delta^{p}}{m}\\leq c_{1}\\sqrt{T}+\\frac{T\\Delta^{p}}{m}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\lambda^{*}~\\in~[1/\\sqrt{T},1]$ : in this case, given the construction of $\\Lambda$ , there is a $\\hat{\\lambda}^{*}\\ \\in\\ \\Lambda$ such that $\\lambda^{*}\\leq\\hat{\\lambda}^{*}\\\\\\overset{\\cdot}{\\leq}2\\lambda^{*}$ . Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{[T]}^{\\mathrm{black-box}}\\left(\\left(\\pi^{t,*}\\right)_{t\\in[T]}\\right)\\leq3\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+\\frac{T\\Delta^{p}}{m}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by taking $\\lambda=\\hat{\\lambda}^{*}$ in the analysis, we can ensure that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{[T]}^{\\mathrm{black-box}}\\left((\\pi^{t,*})_{t\\in[T]}\\right)=\\displaystyle\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})-F^{t}(\\mu^{\\pi^{t,*},p^{t}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq N\\Bigg(\\displaystyle\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\Bigg)+c_{1}\\sqrt{T}+3\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+\\displaystyle\\frac{T\\Delta^{p}}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "F Proof of Thm. 5.1: Main result ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Joining the results from the meta-regret bound and the black-box regret bound, we get the main result of the paper: ", "page_idx": 23}, {"type": "text", "text": "Theorem (Main result). Let $\\delta\\in(0,1)$ . Playing MetaCURL, with black-box algorithm with dynamic regret as in Eq. (10), with a learning rate grid $\\Lambda:=\\left\\{2^{-j}|j=0,1,2,\\ldots,\\lceil1/2\\log_{2}(T)\\rceil\\right\\}$ , and with $E$ WA as the sleeping expert subroutine, we obtain, with probability at least $1-2\\delta$ , for any sequence of policies $(\\pi^{t,*})_{t\\in[T]}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)\\leq\\tilde{O}\\Big(\\sqrt{\\Delta^{\\pi^{*}}T}+\\operatorname*{min}\\big\\{\\sqrt{T\\Delta_{\\infty}^{p}},\\;T^{2/3}(\\Delta^{p})^{1/3}\\big\\}\\Big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Define a sequence of episodes $1=t_{1}<t_{2}<...<t_{m+1}=T+1.$ , with $\\mathcal{T}_{i}:=[t_{i},t_{i+1}-1]$ , such that $\\Delta_{\\mathcal{T}_{i}}^{p}\\leq\\bar{\\Delta^{p}}/m$ for all $i\\in[m]$ . ", "page_idx": 23}, {"type": "text", "text": "The dynamic regret of $\\mathcal{M}(\\mathcal{E},\\Lambda)$ with respect to any sequence of policies $(\\pi^{t,*})_{t\\in[T]}$ , and any $\\lambda\\in\\Lambda$ , can be decomposed as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{1}_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)=\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})+\\sum_{i=1}^{m}\\sum_{t\\in\\mathbb{Z}_{i}}F^{t}(\\mu^{\\pi^{t,t_{i},\\lambda},p^{t}})-F^{t}(\\mu^{\\pi^{t,*},p^{t}})\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n:=R_{[T]}^{\\mathrm{meta}}+R_{[T]}^{\\mathrm{black-box}}\\left((\\pi^{t,*})_{t\\in[T]}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Prop. 5.3, we have that with probability at least $1-2\\delta$ , and $\\begin{array}{r}{C:=\\sqrt{\\frac{1}{2}\\log{\\left(\\frac{N|\\mathcal{X}||\\mathcal{A}|2^{|\\mathcal{X}|}T}{\\delta}\\right)}},}\\end{array}$ $R_{[T]}^{\\mathrm{meta}}\\leq4L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|\\mathcal{A}|}C^{2/3}\\log(T)^{1/3}T^{2/3}(\\Delta^{p})^{1/3}+\\sqrt{\\frac{m T}{2}\\log(T|\\Lambda|)}.$ ", "page_idx": 23}, {"type": "text", "text": "In addition, for $\\Lambda:=\\left\\{2^{-j}|j=0,1,2,\\ldots,\\lceil1/2\\log_{2}(T)\\rceil\\right\\}$ , and $\\lambda$ equal the best empirical learning rate in $\\Lambda$ , Prop. 5.4 yields that, if the black-box algorithm has dynamic regret as in Eq. (10) for any interval in $T$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{[T]}^{\\mathrm{black-box}}\\left((\\pi^{t,*})_{t\\in[T]}\\right)\\leq N\\left(\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\right)+c_{1}\\sqrt{T}+3\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+\\frac{T\\Delta^{p}}{m}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{R_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)\\leq4L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|\\mathcal{A}|}C^{2/3}\\log(T)^{1/3}T^{2/3}(\\Delta^{p})^{1/3}+\\sqrt{\\frac{m T}{2}\\log(T\\log_{2}(T))}}\\\\ &{}&{\\qquad\\qquad\\qquad+\\ N\\bigg(\\displaystyle\\frac{c_{2}\\Delta^{\\pi^{*}}+c_{3}}{c_{1}}\\bigg)+c_{1}\\sqrt{T}+3\\sqrt{c_{1}(c_{2}\\Delta^{\\pi^{*}}+c_{3}m)T}+\\displaystyle\\frac{T\\Delta^{p}}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, for $\\begin{array}{r}{m=\\left(\\frac{2\\sqrt{T}\\Delta^{p}}{\\gamma}\\right)^{2/3}}\\end{array}$ , with log(T lo2g2(T ))+ 3\u221ac1c3, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{[T]}\\big((\\pi^{t,*})_{t\\in[T]}\\big)\\leq T^{2/3}\\Delta^{1/3}\\big(4L_{F}N^{2}|\\mathcal{X}|\\sqrt{3|A|}C^{2/3}\\log(T)^{1/3}+2\\gamma^{2/3}\\big)}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}+N\\bigg(\\frac{c_{2}\\Delta^{\\pi^{*}}}{c_{1}}+c_{3}\\bigg)+c_{1}\\sqrt{T}+3\\sqrt{c_{1}c_{2}\\Delta^{\\pi^{*}}T}}\\\\ &{\\phantom{x x x x x x}=\\tilde{O}\\Big(\\sqrt{\\Delta^{\\pi^{*}}T}+\\operatorname*{min}\\big\\{\\sqrt{T\\Delta_{\\infty}^{p}},\\;T^{2/3}(\\Delta^{p})^{1/3}\\big\\}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "G Greedy MD-CURL dynamic regret analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we introduce Greedy MD-CURL developed by [39], a computationally efficient policyoptimization algorithm known for achieving sublinear static regret in online CURL with adversarial objective functions within a stationary MDP. We begin by detailing Greedy MD-CURL as presented in [39] in Alg. 4. We then provide a new analysis upper bounding the dynamic regret of Greedy MD-CURL in a quasi-stationary interval valid for any learning rate $\\lambda$ . Hence, Greedy MD-CURL can be used as a black-box for MetaCURL. This is the first dynamic regret analysis for a CURL approach. ", "page_idx": 24}, {"type": "text", "text": "Let $\\mathcal{M}_{\\mu_{0}}^{p,*}$ denote the subset of $\\mathcal{M}_{\\mu_{0}}^{p}$ where the corresponding policies $\\pi$ are such that $\\pi_{n}(a|x)\\neq0$ for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , $n\\in[N]$ . For any probability transition $p,\\Gamma:\\mathcal{M}_{\\mu_{0}}^{p}\\times\\mathcal{M}_{\\mu_{0}}^{p,*}\\to\\mathbb{R}$ such that, for all $\\mu\\in\\mathcal{M}_{\\mu_{0}}^{p}$ with its associated policy $\\pi$ , and $\\mu^{\\prime}\\in\\mathcal{M}_{\\mu_{0}}^{p,*}$ with its associated policy $\\pi^{\\prime}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Gamma(\\mu^{\\pi},\\mu^{\\pi^{\\prime}}):=\\sum_{n=1}^{N}\\mathbb{E}_{(x,a)\\sim\\mu_{n}^{\\pi}(\\cdot)}\\bigg[\\log\\bigg(\\frac{\\pi_{n}(a|x)}{\\pi_{n}^{\\prime}(a|x)}\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This divergence $\\Gamma$ is a Bregman divergence (see Proposition 4.3 of [39]). Problem (20) implemented with this Bregman divergence $\\Gamma$ has a closed form solution, as showed in [39]. ", "page_idx": 24}, {"type": "text", "text": "Algorithm 4 Greedy MD-CURL ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1: Input: number of episodes $T$ , initial sequence of policies $\\overline{{\\pi^{1}\\in(S_{\\mathcal{A}})^{\\mathcal{X}\\times N}}}$ , initial state-action distribution $\\mu_{0}$ , learning rate $\\lambda>0$ , sequence of parameters $(\\alpha_{t})_{t\\in[T]}$ . ", "page_idx": 25}, {"type": "text", "text": "2: Initialization: $\\begin{array}{r}{\\forall(x,a),\\hat{p}^{1}(\\cdot|x,a)=\\frac{1}{|\\mathcal{X}|}}\\end{array}$ and $\\mu^{1}=\\tilde{\\mu}^{1}:=\\mu^{\\pi^{1},\\hat{p}^{1}}$ ", "page_idx": 25}, {"type": "text", "text": "3: for $t=1,\\dots,T$ do   \n4: Agent starts at $(x_{0}^{t},a_{0}^{t})\\sim\\mu_{0}(\\cdot)$   \n5: for ${n=1,\\ldots,N}$ do   \n6: Environment draws new state $x_{n}^{t}\\sim p_{n}(\\cdot|x_{n-1}^{t},a_{n-1}^{t})$   \n7: Learner observes agent\u2019s external noise $\\varepsilon_{n}^{t}$   \n8: Agent chooses an action $a_{n}^{t}\\sim\\pi_{n}^{t}(\\cdot|x_{n}^{t})$   \n9: end for   \n0: Update probability kernel estimate for all $(x,a)$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{p}_{n}^{t+1}(\\cdot|x,a):=\\frac{1}{t}\\delta_{g_{n}(x,a,\\epsilon_{n}^{t})}+\\frac{t-1}{t}\\hat{p}_{n}^{t}(\\cdot|x,a)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "12: Compute policy for the next episode: ", "page_idx": 25}, {"type": "text", "text": "13: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu^{t+1}\\in\\underset{\\mu\\in\\mathcal{M}_{\\mu_{0}}^{\\hat{\\mu}^{t+1}}}{\\arg\\operatorname*{min}}\\left\\{\\lambda\\langle\\nabla F^{t}(\\mu^{t}),\\mu\\rangle+\\Gamma(\\mu,\\tilde{\\mu}^{t})\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "14: for all $n\\in[N],(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{n}^{t+1}(a|x)=\\frac{\\mu_{n}^{t+1}(x,a)}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\mu_{n}^{t+1}(x,a^{\\prime})}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "15: Compute $\\tilde{\\pi}^{t+1}:=(1-\\alpha_{t+1})\\pi^{t+1}+\\alpha_{t+1}/|A|$   \n16: Compute $\\tilde{\\mu}^{t+1}:=\\mu^{\\tilde{\\pi}^{t+1},\\hat{p}^{t+1}}$ as in Eq. (2) ", "page_idx": 25}, {"type": "text", "text": "18: return (\u03c0t)t\u2208[T ] ", "page_idx": 25}, {"type": "text", "text": "G.1 Dynamic regret analysis of Greedy MD-CURL ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let us assume we analyze our regret in an interval $I:=[t_{s},t_{e}]\\subseteq[T]$ . We denote by $R_{I}$ the dynamic regret of an instance of Greedy MD-CURL started at episode $t_{s}$ until the end of interval $I$ at episode $t_{e}$ . We denote by $\\pi^{t}$ the policy produced by this instance of Greedy MD-CURL at episode $t\\in I$ , $p^{t}$ the true probability transition kernel, and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{p}_{n}^{t}(x^{\\prime}|x,a)=\\frac{1}{t-t_{s}}\\sum_{s=t_{s}}^{t-1}\\mathbb{1}_{\\left\\{g_{n}(x,a,\\varepsilon_{n}^{s})=x^{\\prime}\\right\\}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the empirical estimate of the probability kernel at episode $t$ , with data from the beginning of the interval $I$ . ", "page_idx": 25}, {"type": "text", "text": "We define and decompose the dynamic regret $R_{I}$ with respect to any sequence of policies $(\\pi^{t,*})_{t\\in I}$ into three terms as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{R_{I}\\big((\\pi^{t,*})_{t\\in I}\\big):=\\displaystyle\\sum_{t\\in I}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{*},t},p^{t})=\\displaystyle\\sum_{t\\in I}F^{t}(\\mu^{\\pi^{t},p^{t}})-F^{t}(\\mu^{\\pi^{t},{p^{t}}})}}\\\\ &{}&{\\qquad+\\underbrace{\\displaystyle\\sum_{t\\in I}F^{t}(\\mu^{\\pi^{t},{p^{t}}})-F^{t}(\\mu^{\\pi^{*},t},{\\hat{p}^{t}})}_{R_{I}^{\\mathrm{phy}}\\big((\\pi^{t,*})_{t\\in I}\\big)}+\\underbrace{\\displaystyle\\sum_{t\\in I}F^{t}(\\mu^{\\pi^{t},*},{\\hat{p}^{t}})-F^{t}(\\mu^{\\pi^{t},*},p^{t})}_{R_{I}^{\\mathrm{shp}}(\\pi^{t,*})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The terms $R_{I}^{\\mathrm{MDP}}(\\pi^{t})$ and $R_{I}^{\\mathrm{MDP}}(\\pi^{t,*})$ pay for our lack of knowledge of the true MDP, forcing us to use its empirical estimate. The term $R_{I}^{\\mathrm{policy}}$ corresponds to the loss incurred in calculating the policy ", "page_idx": 25}, {"type": "text", "text": "by solving the optimization problem given in Eq. (20). Below, we present the first analysis of the dynamic regret for a CURL algorithm. We consider each term separately. ", "page_idx": 26}, {"type": "text", "text": "G.1.1 $R_{I}^{\\mathbf{MDP}}$ analysis", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Section 2 we assume that the deterministic part of the dynamics, given by $g_{n}$ in equation (8) for each time step $n$ , is known in advance. The source of uncertainty and non-stationarity in the MDP comes only from the external noise dynamics, that is independent of the agent\u2019s state-action pair. Therefore, we do not need to explore in this setting, so the analysis of the two terms RIMDP(\u03c0t) and $R_{I}^{\\mathrm{MDP}}(\\pi^{t,*})$ are the same. ", "page_idx": 26}, {"type": "text", "text": "Proposition G.1. With probability at least $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{I}^{M D P}(\\pi^{t})\\leq L_{F}N^{2}\\sqrt{\\frac{1}{2}\\log\\left(\\frac{N|\\mathcal{X}||\\mathcal{A}|2^{|\\mathcal{X}|}T}{\\delta}\\right)}\\sqrt{|I|}+|I|\\Delta_{I}^{p},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all intervals $I\\in[T]$ . The same result is valid for $R_{I}^{M D P}(\\pi^{t,*})$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. We start by using the convexity of $F^{t}$ , Holder\u2019s inequality, that $F^{t}$ is $L_{F}$ -Lipschitz, and Lemma C.5 to obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{I}^{\\mathrm{MDP}}(\\pi^{t})\\leq L_{F}\\sum_{t=t_{s}}^{t_{e}}\\sum_{n=1}^{N}\\sum_{j=1}^{n}\\sum_{x,a}\\mu_{j-1}^{\\pi^{i,t},p^{t}}(x,a)\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying Lemmas C.1 and C.2, we have that for any $\\delta\\in(0,1)$ , with probability $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|p_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}\\leq\\|p_{j}^{t}(\\cdot|x,a)-\\overline{{p}}_{j}^{t}(\\cdot|x,a)\\|_{1}+\\|\\overline{{p}}_{j}^{t}(\\cdot|x,a)-\\hat{p}_{j}^{t}(\\cdot|x,a)\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\displaystyle\\frac{1}{2(t-t_{s})}\\log\\left(\\frac{N|\\mathcal{X}||\\mathcal{A}|2^{|\\mathcal{X}|}T}{\\delta}\\right)}+\\displaystyle\\sum_{k=t_{s}}^{t-1}\\Delta_{k}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using this to continue the upper bound of Eq. (22), we conclude our proof: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{I}^{\\mathrm{MDP}}(\\pi^{t})\\leq L_{F}N^{2}\\displaystyle\\sum_{t=t_{s}}^{t_{e}}\\left(\\sqrt{\\frac{1}{2(t-t_{s})}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|}T}{\\delta}\\right)}+\\displaystyle\\sum_{k=t_{s}}^{t-1}\\Delta_{k}^{p}\\right)}\\\\ &{\\qquad\\qquad\\leq L_{F}N^{2}\\sqrt{\\frac{1}{2}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|}T}{\\delta}\\right)}\\sqrt{|I|}+|I|\\Delta_{I}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "G.1.2 RIpolicya nalysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition G.2. Let b be a constant defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\nb:=8N^{2}+2N^{2}\\log(|{\\cal A}|)\\log(|I|)\\big(1+\\log(|I|)\\big)+2N\\log(|I|)+N\\log(|{\\cal A}|).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, Greedy MD-CURL obtains, for any sequence of policies $(\\pi^{t,*})_{t\\in I}$ , and for any learning rate $\\lambda>0$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{I}^{p o l i c y}\\big(({\\pi}^{t,*})_{t\\in I}\\big)\\leq\\lambda L_{F}^{2}|I|+\\frac{N^{2}}{\\lambda}{\\Delta_{I}^{\\pi}}^{*}+b\\frac{1}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We adapt the proof of Prop. 5.7 of [39] that upper bounds the static regret incurred when solving the optimization Problem (20), for a proof that upper bounds the dynamic regret. The main difference is that, in the case of static regret, we compare ourselves to the same policy throughout the interval, whereas in the case of dynamic regret, at each episode we compare ourselves to a different policy given by $\\pi^{t,*}$ . Consequently, the analysis remains the same as in [39] for all terms that do not depend on $\\pi^{t,\\dot{*}}$ , but requires a new analysis in terms that do depend on it. ", "page_idx": 26}, {"type": "text", "text": "To simplify notation, we take $\\ell^{t}:=\\nabla F^{t}(\\mu^{\\pi^{t},\\hat{p}^{t}})$ and $\\boldsymbol{\\mu}^{t}:=\\boldsymbol{\\mu}^{\\pi^{t},\\hat{p}^{t}}$ . We can use the same reasoning as in appendix $D.5$ of [39] to show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{S}_{I}^{\\mathrm{policy}}\\leq\\underbrace{\\frac{1}{\\lambda}\\sum_{t\\in I}\\left[\\lambda\\langle\\ell^{t},\\mu^{t}-\\mu^{t+1}\\rangle-\\Gamma(\\mu^{t+1},\\tilde{\\mu}^{t})\\right]}_{t}+\\underbrace{\\frac{1}{\\lambda}\\sum_{t\\in I}\\left[\\Gamma(\\mu^{\\pi^{t,*},\\tilde{\\rho}^{t+1}},\\tilde{\\mu}^{t})-\\Gamma(\\mu^{\\pi^{t,*},\\tilde{\\rho}^{t+1}},\\mu^{t+1})\\right]}_{t\\in I}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since the $A$ term does not depend on $\\pi^{t,*}$ , its analysis follows directly from [39], and is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nA\\leq\\lambda L_{F}^{2}|I|+\\frac{1}{2\\lambda}\\sum_{t\\in I}\\bigg(\\frac{2N}{t-t_{s}}+2N\\alpha_{t}\\bigg)^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $L_{F}$ is the Lipschitz constant of $F^{t}$ and $\\alpha^{t}$ is an input parameter of Greedy MD-CURL. ", "page_idx": 27}, {"type": "text", "text": "We then proceed to analyze term $B$ . Again, following the procedure of appendix $D.5$ of [39], we obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B=\\displaystyle\\sum_{t=1}^{T}\\Gamma(\\mu^{\\pi^{t,*},\\hat{p}^{t+1}},\\tilde{\\mu}^{t})-\\Gamma(\\mu^{\\pi^{t-1,*},\\hat{p}^{t}},\\tilde{\\mu}^{t})+\\sum_{t=1}^{T}\\Gamma(\\mu^{\\pi^{t-1,*},\\hat{p}^{t}},\\tilde{\\mu}^{t})-\\Gamma(\\mu^{\\pi^{t-1,*},\\hat{p}^{t}},\\mu^{t})}\\\\ &{\\hfill+\\underbrace{\\sum_{t=1}^{T}\\Gamma(\\mu^{\\pi^{t-1,*},\\hat{p}^{t}},\\mu^{t})-\\Gamma(\\mu^{\\pi^{t,*},\\hat{p}^{t+1}},\\mu^{t+1})}_{(i i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\psi:(S_{\\mathcal{X}\\times\\mathcal{A}})^{N}\\rightarrow\\mathbb{R}$ denote the function inducing the Bregman divergence $\\Gamma$ of Eq. (19). [39] further shows that: ", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{\\star\\ (i)\\leq-\\psi(\\mu^{\\pi^{t_{i}-1,*},\\hat{p}^{t_{i}}})+N\\sum_{t\\in I}\\log\\Big(\\frac{|\\mathcal{A}|}{\\alpha_{t}}\\Big)\\|\\mu^{\\pi^{t-1,*},\\hat{p}^{t}}-\\mu^{\\pi^{t,*},\\hat{p}^{t+1}}\\|_{\\infty,1}}\\end{array}$ \u2022 $\\begin{array}{r}{(i i)\\le2N\\sum_{t\\in I}\\alpha_{t}}\\end{array}$ , and this upper bound is found independently of $\\pi^{t,*}$ $\\bullet\\,\\,(i i i)\\leq\\Gamma(\\mu^{\\pi^{t-1,*},\\hat{p}^{t}},\\mu^{t}).$ ", "page_idx": 27}, {"type": "text", "text": "Lemma $D.6$ of [39] shows that, ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\psi(\\mu^{{\\pi^{t_{i}-1,*}},{\\hat{p}^{t_{i}}}})+\\Gamma(\\mu^{{\\pi^{t_{i}-1,*}},{\\hat{p}^{t_{i}}}},\\mu^{t_{i}})\\leq N\\log(|{\\cal A}|).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Only term $(i)$ , which involves $\\|\\boldsymbol{\\mu}^{\\pi^{*,t-1},\\hat{p}^{t}}-\\boldsymbol{\\mu}^{\\pi^{*,t},\\hat{p}^{t+1}}\\|_{\\infty,1}$ , depends on the sequence $(\\pi^{t,*})_{t\\in[T]}$ , requiring then a new analysis. For this purpose, we rely on the following two results: ", "page_idx": 27}, {"type": "text", "text": "\u2022 From Lemma 5.6 of [39], we have that, for all strategies $\\pi$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mu^{\\pi,\\hat{p}^{t}}-\\mu^{\\pi,\\hat{p}^{t+1}}\\|_{\\infty,1}\\leq\\frac{2N}{t-t_{s}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 From auxiliary Lemma C.6 proved in Appendix $\\mathbf{C}$ we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|{\\mu_{n}^{\\pi^{*,t-1},\\hat{p}^{t+1}}-\\mu_{n}^{\\pi^{t,*},\\hat{p}^{t+1}}}\\|_{1}\\leq\\sum_{i=1}^{n}\\sum_{x\\in\\mathcal{X}}\\rho_{i}^{\\pi^{t-1,*}}(x)\\|\\pi_{i}^{t,*}(\\cdot|x)-\\pi_{i}^{t-1,*}\\|_{1}\\leq N\\Delta_{t}^{\\pi^{*}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, using the triangular inequality and the two results above, we obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mu^{\\pi^{*,t-1,\\widehat{p}^{t}}}-\\mu^{\\pi^{t,*},\\widehat{p}^{t+1}}\\|_{\\infty,1}\\leq\\|\\mu^{\\pi^{t-1,*},\\widehat{p}^{t}}-\\mu^{\\pi^{t-1,*},\\widehat{p}^{t+1}}\\|_{\\infty,1}+\\|\\mu^{\\pi^{t-1,*},\\widehat{p}^{t+1}}-\\mu^{\\pi^{t,*},\\widehat{p}^{t+1}}\\|_{\\infty,1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2N}{t}+N\\Delta_{t}^{\\pi^{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, the bound on term $B$ is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nB\\leq\\frac{1}{\\lambda}\\Biggl[N\\sum_{t\\in I}\\log\\left(\\frac{|\\mathcal{A}|}{\\alpha_{t}}\\right)\\left(\\frac{2N}{t-t_{s}}+N\\Delta_{t}^{\\pi^{*}}\\right)+2N\\sum_{t\\in I}\\alpha_{t}+N\\log(|\\mathcal{A}|)\\Biggr].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Final step: joining all results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Joining the upper bounds on term $A$ from Eq. (23) and on term $B$ from Eq. (24), we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{R_{I}^{\\mathrm{policy}}\\left(\\left(\\pi^{t,*}\\right)_{t\\in I}\\right)\\leq A+B}\\\\ {\\leq\\lambda L_{F}^{2}|I|+\\displaystyle\\frac{1}{2\\lambda}\\sum_{t\\in I}\\left(\\frac{2N}{t-t_{s}}+2N\\alpha_{t}\\right)^{2}}\\\\ {\\quad\\qquad\\qquad+\\displaystyle\\frac{1}{\\lambda}\\Bigg[N\\sum_{t\\in I}\\log\\left(\\frac{|A|}{\\alpha_{t}}\\right)\\left(\\frac{2N}{t-t_{s}}+N\\Delta_{t}^{\\pi^{*}}\\right)+2N\\sum_{t\\in I}\\alpha_{t}+N\\log(|A|)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If we take the learning rate as $\\alpha_{t}=1/t$ , then, for all $\\lambda>0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{p o l i e y}\\displaystyle((\\pi^{t,*})t\\in I)\\leq\\lambda L_{F}^{2}|I|+\\frac{N^{2}}{\\lambda}\\Delta_{I}^{\\pi^{*}}}\\\\ &{\\phantom{\\leq\\lambda}+\\displaystyle\\frac{1}{\\lambda}\\Bigg[8N^{2}+2N^{2}\\log(|A|)\\log(|I|)\\big(1+\\log(|I|)\\big)+2N\\log(|I|)+N\\log(|A|)\\Bigg]}\\\\ &{\\phantom{=\\ }=\\lambda L_{F}^{2}|I|+\\frac{N^{2}\\Delta_{I}^{\\pi^{*}}+b}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nb:=8N^{2}+2N^{2}\\log(|A|)\\log(|I|)\\big(1+\\log(|I|)\\big)+2N\\log(|I|)+N\\log(|A|).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "G.2 Final Greedy MD-CURL regret analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Replacing the bounds of Prop. G.1 and G.2 in Eq. (21) yields the final upper bound of Greedy MDCURL\u2019s dynamic regret for any interval $I\\subseteq T$ with respect to any sequence of policies $(\\pi^{t,*})_{t\\in I}$ : ", "page_idx": 28}, {"type": "text", "text": "Theorem G.3 (Dynamic regret of Greedy MD-CURL). Let b be a constant defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\nb:=8N^{2}+2N^{2}\\log(|{\\cal A}|)\\log(|I|)\\big(1+\\log(|I|)\\big)+2N\\log(|I|)+N\\log(|{\\cal A}|).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $\\delta\\in(0,1)$ . With probability at least $1-2\\delta$ , for any interval $I\\subseteq[T]$ , for any sequence of policies $(\\pi^{t,*})_{t\\in I}$ , for any learning rate $\\lambda>0$ , and for $\\alpha_{t}:=1/t$ , Greedy MD-CURL obtains ", "page_idx": 28}, {"type": "equation", "text": "$$\nR_{I}\\Big((\\pi^{t,*})_{t\\in I}\\Big)\\leq\\lambda L_{F}^{2}|I|+\\frac{N^{2}\\Delta_{I}^{\\pi^{*}}+b}{\\lambda}+2F N^{2}\\sqrt{\\frac{1}{2}\\log\\left(\\frac{N|\\mathcal{X}||A|2^{|\\mathcal{X}|T}}{\\delta}\\right)}\\sqrt{|I|}+2|I|\\Delta_{I}^{p}+2|I|\\Delta_{I}^{p}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, Greedy MD-CURL meets the requisite dynamic regret bound from Eq. (10) to serve as a black-box algorithm for MetaCURL achieving optimal dynamic regret. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We clearly provide the contributions of our work and our settings in the Introduction. We formally detail our hypothesis in Section 2. We present the new algorithm in Section 4. All regret results related to the new algorithm claimed in the abstract and introduction are stated in Section 5 and proved in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In Section 2, we formally detail our assumptions and discuss the strong assumption made about probability transitions, justifying it as a means of providing lowcomplexity methods. We also discuss real-world applications that satisfy these assumptions. In Section 6, we discuss the aspirational goal of addressing this limitation as a future work. In Remark 3.1, we discuss the complexity of the algorithm. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All the main results are in the paper, and all proofs are carefully stated in the Appendix. All auxiliary results used to obtain the main results are also included in the Appendix. A proof sketch is provided for the main results in the main paper. All results are properly referenced. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This is a theoretical paper, with no experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 30}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: This is a theoretical paper, with no experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper, with no experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper, with no experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a theoretical paper, with no experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work conform with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The results presented in this paper are largely theoretical. The framework provided in this paper is very general and could be applied to any reinforcement learning or concave utility reinforcement learning problem in a tabular MDP. Therefore, as with any reinforcement learning algorithm, it is possible that the algorithms developed from the ideas presented in this paper could be applied in contexts that have negative societal impacts, or in contexts where the reward function has a negative negative societal impacts. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not use existing assets. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]