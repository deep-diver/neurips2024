[{"figure_path": "HSRs6yyuUK/figures/figures_4_1.jpg", "caption": "Figure 1: Eigenvalue distributions of the first linear layer's weight matrices in the encoder of 1-st view.", "description": "This figure shows the eigenvalue distributions of the first linear layer's weight matrices in the encoder of the first view at the 100th and 1200th epochs for both DCCA and NR-DCCA.  It visually demonstrates the hypothesis that model collapse in DCCA is due to the low-rank nature of the DNN weight matrices.  The plots show that at the 100th epoch, the eigenvalues decay relatively slowly for both methods. However, by the 1200th epoch, DCCA exhibits a much faster decay in eigenvalues compared to NR-DCCA, indicating increased redundancy in the weight matrices of DCCA and suggesting a link between this redundancy and model collapse.  The plots are normalized and also include the y=1/x curve as a reference.", "section": "Model Collapse of DCCA"}, {"figure_path": "HSRs6yyuUK/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of NR-DCCA. We take the CUB dataset as an example: similar to DCCA, the k-th view Xk is transformed using fk to obtain new representation fk(Xk) and then maximize the correlation between new representations. Additionally, for the k-th view, we incorporate the proposed NR loss to regularize fk.", "description": "This figure illustrates the architecture of NR-DCCA, a novel method proposed in the paper to prevent model collapse in deep canonical correlation analysis (DCCA).  The figure shows how the NR-DCCA model takes multi-view data as input, processes it using neural networks (fk), and incorporates a novel noise regularization (NR) loss to prevent model collapse. The NR loss is calculated by comparing the correlation between the original data and noise with the correlation between the transformed data and transformed noise. By minimizing this loss, the model is encouraged to maintain correlations that are robust to random noise.  The diagram highlights the key components including multi-view data input, feature extraction using GoogleNet and Doc2Vec, correlation calculation, noise regularization and loss functions (CCA loss and NR loss), and the resulting unified representation. The CUB dataset is used as a specific example.", "section": "5 DCCA with Noise Regularization (NR-DCCA)"}, {"figure_path": "HSRs6yyuUK/figures/figures_7_1.jpg", "caption": "Figure 3: Construction of a synthetic dataset. This example consists of 2 views and n objects, and the common rate is 0%.", "description": "This figure illustrates the process of constructing synthetic datasets used to evaluate multi-view representation learning (MVRL) methods.  It starts with a 'God Embedding' (G), a high-dimensional representation of the object.  This embedding is then partially selected and transformed using non-linear functions (\u03a61 and \u03a62), and noise is added, to create the two views (X1 and X2) for the synthetic dataset.  The common rate (shown here as 0%) determines the amount of shared information between the two views.  Separately, downstream tasks (T1...Tj) are generated using a separate transformation (\u03c8j) of the full God Embedding.  This setup allows for controlled generation of datasets with varying levels of shared and complementary information between views.", "section": "6.1 Construction of synthetic datasets (RQ1)"}, {"figure_path": "HSRs6yyuUK/figures/figures_8_1.jpg", "caption": "Figure 1: Eigenvalue distributions of the first linear layer's weight matrices in the encoder of 1-st view.", "description": "The figure displays eigenvalue distributions of the first linear layer's weight matrices of the encoder for the first view across different training epochs (100th and 1200th epochs) for both DCCA and NR-DCCA methods. The plots reveal how eigenvalues decay over time. In the 100th epoch, both DCCA and NR-DCCA show a gradual decay. However, by the 1200th epoch, DCCA displays a significantly faster decay compared to NR-DCCA, suggesting increased redundancy in DCCA's weight matrices.", "section": "4 Model Collapse of DCCA"}, {"figure_path": "HSRs6yyuUK/figures/figures_9_1.jpg", "caption": "Figure 5: Performance of different methods in real-world datasets. Each column represents the performance on a specific dataset. The number of views in the dataset is denoted in the parentheses next to the dataset name.", "description": "This figure shows the performance of different methods (LINEAR CCA, DCCA, DCCAE, DCCA_PRIVATE, and NR-DCCA) on three real-world datasets: PolyMnist, CUB, and Caltech101.  Each dataset is tested with a varying number of views (indicated in parentheses in the dataset name). The graphs illustrate the F1 score over epochs, demonstrating the stability and performance of each method on the different datasets.  NR-DCCA shows consistently high F1 scores across all datasets, indicating robustness and superior performance compared to the other methods.", "section": "6.3 Consistent Performance on Real-world Datasets (RQ3)"}, {"figure_path": "HSRs6yyuUK/figures/figures_22_1.jpg", "caption": "Figure 6: The effects of hyper-parameter r of DCCA in the CUB dataset.", "description": "The figure shows the effects of different ridge regularization parameters (r) on the performance of DCCA on the CUB dataset.  Three different metrics are plotted against the ridge parameter values:\n\n1. **F1_performance**: The F1 score, a measure of a model's accuracy in a classification task.\n2. **Corr_in_features**: Correlation between features within the data itself.\n3. **Corr_with_noise**: Correlation between the features and random noise added to the data.\n\nThe plot reveals how varying the ridge parameter impacts these three aspects of model performance.  It illustrates the effect of ridge regularization on DCCA, demonstrating its impact on accuracy and the correlation of the data both internally and with noise.", "section": "A.5 Hyper-parameter r in Ridge Regularization"}, {"figure_path": "HSRs6yyuUK/figures/figures_22_2.jpg", "caption": "Figure 7: The effects of hyper-parameter \u03b1 of NR-DCCA in the CUB dataset.", "description": "The figure shows the performance of NR-DCCA on the CUB dataset with different values of hyperparameter \u03b1. It demonstrates that using a too small value of \u03b1 leads to model collapse while a too large value of \u03b1 causes slow convergence.  The optimal \u03b1 is the smallest value that prevents model collapse, while maintaining high performance. ", "section": "A.5 Hyper-parameter Settings"}, {"figure_path": "HSRs6yyuUK/figures/figures_23_1.jpg", "caption": "Figure 1: Eigenvalue distributions of the first linear layer's weight matrices in the encoder of 1-st view.", "description": "This figure shows the eigenvalue distributions of the first linear layer's weight matrices in the encoder of the first view for both DCCA and NR-DCCA at the 100th and 1200th epochs.  The plots illustrate how the eigenvalues decay over time, representing the redundancy in the weight matrices.  At the 100th epoch, both DCCA and NR-DCCA show a relatively slow decay. However, by the 1200th epoch, the eigenvalues in DCCA decay much faster than in NR-DCCA, indicating increased redundancy and supporting the hypothesis that model collapse in DCCA is linked to low-rank weight matrices.  The y=1/x line serves as a reference for comparison.", "section": "4 Model Collapse of DCCA"}, {"figure_path": "HSRs6yyuUK/figures/figures_24_1.jpg", "caption": "Figure 9: Visualization of the learned representations with t-SNE in the CUB dataset.", "description": "This figure visualizes the learned representations of different multi-view representation learning (MVRL) methods on the CUB dataset using t-SNE.  Each point represents a data point, and the color indicates the class label. The visualization helps to understand how well each method separates the data points into distinct clusters according to their class labels and the level of dispersion within each cluster.  Ideally, points of the same class should cluster together tightly but not overlap significantly with other clusters, demonstrating the preservation of distinctive features of the data and the ability to learn a meaningful representation of the data.", "section": "A.9 Visualization of Learned Representations"}, {"figure_path": "HSRs6yyuUK/figures/figures_25_1.jpg", "caption": "Figure 4: (a) Mean and standard deviation of the (D)CCA-based method performance across synthetic datasets in different training epochs. (b) The mean correlation between noise and real data after transformation varies with epochs. (c) Average NESum across all weights within the trained encoders. (d,e) The mean of reconstruction and denoising loss on the test set.", "description": "This figure visualizes the performance of different methods across various training epochs on synthetic datasets.  Subfigure (a) shows the mean and standard deviation of R2 performance for each method across different epochs. Subfigure (b) displays the correlation between noise and real data after transformation. Subfigure (c) presents the average NESum (Normalized Eigenvalue Sum) across all weights in the trained encoders. Subfigures (d) and (e) show the mean reconstruction and denoising loss on the test set, respectively, as a function of the training epoch.  The results demonstrate the stability and performance of NR-DCCA compared to other methods in preventing model collapse.", "section": "Model Collapse of DCCA"}, {"figure_path": "HSRs6yyuUK/figures/figures_26_1.jpg", "caption": "Figure 11: Performance of DGCCA-based methods in real-world datasets. Each column represents the performance on a specific dataset. The number of views in the dataset is denoted in the parentheses next to the dataset name.", "description": "The figure shows the performance of various DGCCA methods on three real-world datasets: PolyMnist, CUB, and Caltech101.  For PolyMnist, the number of views varies from 2 to 5. The x-axis represents the training epoch, while the y-axis represents the F1 score. The figure helps to visualize the stability and performance of different methods across various datasets and view numbers.  The lines for each method demonstrate performance trends over time.", "section": "6.3 Consistent Performance on Real-world Datasets (RQ3)"}]