[{"figure_path": "HSRs6yyuUK/tables/tables_22_1.jpg", "caption": "Table 1: Effect of Noise on DCCA and NR-DCCA", "description": "This table shows the performance comparison of DCCA and NR-DCCA with Gaussian noise and uniform noise at different epochs (100, 800, 1200).  The results demonstrate that the NR-DCCA approach effectively suppresses model collapse, as evident by significantly higher R2 performance values compared to standard DCCA, especially at later training stages (800 and 1200 epochs). The performance is consistent whether using Gaussian or uniform noise.", "section": "A.6 Effects of the Distribution of Noise"}, {"figure_path": "HSRs6yyuUK/tables/tables_23_1.jpg", "caption": "Table 2: Performance comparison of DCCA and NR-DCCA across different network depths.", "description": "This table presents the performance comparison of DCCA and NR-DCCA models trained with different numbers of hidden layers (1, 2, and 3). The results (R2 values) are reported for three different epochs (100, 800, and 1200) to illustrate the performance change during training.  It shows how the model performance of DCCA declines with increasing depth, demonstrating model collapse, while the NR-DCCA method maintains relatively stable performance.", "section": "A.7 Effects of depths of Encoders"}, {"figure_path": "HSRs6yyuUK/tables/tables_27_1.jpg", "caption": "Table 3: Performance in synthetic datasets.", "description": "This table presents the performance of various multi-view representation learning (MVRL) methods across synthetic datasets with varying common rates (0%, 20%, 40%, 60%, 80%, 100%).  The results represent the mean and standard deviation of the R2 score across 50 downstream regression tasks.  The table allows for comparison of different MVRL approaches in controlled settings.", "section": "A.11 Additional Experimental Results"}, {"figure_path": "HSRs6yyuUK/tables/tables_27_2.jpg", "caption": "Table 4: Performance in real-world datasets", "description": "This table presents the performance of various multi-view representation learning (MVRL) methods on three real-world datasets: PolyMnist, CUB, and Caltech101.  For each dataset, the F1 score (for classification tasks) is reported for different methods.  The number of views used for each dataset is indicated in parentheses. The table allows for a comparison of the performance of different methods across various datasets and different numbers of views, highlighting the effectiveness and robustness of the proposed NR-DCCA method.", "section": "A.11 Additional Experimental Results"}, {"figure_path": "HSRs6yyuUK/tables/tables_27_3.jpg", "caption": "Table 5: Comparisons of computational complexity against baselines", "description": "This table compares the computational complexity of different DCCA-based methods.  The complexity is analyzed based on the number of views (K), number of samples (N), feature dimensions (D), number of hidden layers (L), number of neurons per layer (H), and the dimensionality of the unified representation (M). The table breaks down the complexity for each step involved in the different methods, such as noise generation, encoding and decoding using MLPs, reconstruction loss calculation, correlation maximization, and noise regularization.  The complexities are expressed in Big O notation.", "section": "A.12 Complexity Analysis"}]