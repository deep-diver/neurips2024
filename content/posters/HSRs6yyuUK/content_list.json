[{"type": "text", "text": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junlin He   \nThe Hong Kong Polytechnic University Hong Kong SAR, China   \njunlinspeed.he@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "Jinxiao Du The Hong Kong Polytechnic University Hong Kong SAR, China jinxiao.du@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "Susu Xu   \nJohns Hopkins University   \nMaryland, USA   \nsxu83@jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Wei Ma\u2217 The Hong Kong Polytechnic University Hong Kong SAR, China wei.w.ma@polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data. Deep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-theart performance. However, with extensive experiments, we observe the issue of model collapse, i.e., the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the Correlation Invariant Property is the key to preventing model collapse, and our noise regularization forces the neural network to possess such a property. A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA. Our code will be released at https://github.com/Umaruchain/NRDCCA.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, multi-view representation learning (MVRL) has emerged as a core technology for learning from multi-source data and providing readily useful representations to downstream tasks (Sun et al. 2023, Yan et al. 2021), and it has achieved tremendous success in various applications, such as video surveillance (Guo et al. 2015, Feichtenhofer et al. 2016, Deepak, K. et al. 2021), medical diagnosis (Wei et al. 2019, Xu et al. 2020) and social media (Srivastava & Salakhutdinov 2012, Karpathy & Fei-Fei 2015, Mao et al. 2014, Fan et al. 2020). Specifically, multi-source data can be collected from the same object, and each data source can be regarded as one view of the object. For instance, an object can be described simultaneously through texts, videos, and audio, which contain both common and complementary information of the object (Yan et al. 2021, Zhang, Liu & Fu 2019, Hwang et al. 2021, Geng et al. 2021), and the MVRL aims to learn a unified representation of the object from the multi-view data. ", "page_idx": 0}, {"type": "text", "text": "The key challenge of MVRL is to learn the intricate relationships of different views. The Canonical Correlation Analysis (CCA), which is one of the early and representative methods for MVRL, transforms all the views into a unified space by maximizing their correlations (Hotelling 1992, Horst 1961, Hardoon et al. 2004, Lahat et al. 2015, Yan et al. 2023, Sun et al. 2023). Through correlation maximization, CCA can identify the common information between different views and extract them to form the representation of the object. On top of CCA, Linear CCA, and DCCA maximize the correlation defined by CCA through gradient descent, while the former uses an affine transformation and the latter uses Deep Neural Networks (DNNs). (Andrew et al. 2013). Indeed, there are quite a few variants of DCCA, such as DGCCA (Benton et al. 2017), DCCAE (Wang et al. 2015), DVCCA (Wang et al. 2016), DTCCA (Wong et al. 2021) and DCCA GHA (Chapman et al. 2022). ", "page_idx": 1}, {"type": "text", "text": "However, extensive experimentation reveals that DCCA-based methods typically excel during the initial stages of training but suffer a significant decline in performance as training progresses. This phenomenon is defined as model collapse within the context of DCCA. Notably, our definition is grounded in the performance of the learned representations on downstream tasks. Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (Andrew et al. 2013, De Bie & De Moor 2003). Nevertheless, they did not further explore whether merely guaranteeing that the full-rank representations can guarantee that the weight matrices are full-rank. ", "page_idx": 1}, {"type": "text", "text": "Though early stopping could be adopted to prevent model collapse (Prechelt 1998, Yao et al. 2007), it remains challenging when to stop. The model collapse issue of DCCA-based methods prevents the adoption in large models, and currently, many applications still use simple concatenation to combine different views (Yan et al. 2021, Zheng et al. 2020, Nie et al. 2017). Therefore, how to develop a DCCA-based MVRL method free of model collapse remains an interesting and open question. ", "page_idx": 1}, {"type": "text", "text": "In this work, we demonstrate that both representations and weight matrices of Linear CCA are full-rank whereas DCCA only guarantees that representations are full-rank but not for the weight matrices. Considering that Linear CCA does not show the model collapse while DCCA does, we conjecture that the root cause of the model collapse in DCCA is that the weight matrices in DNNs tend to be low-rank. A wealth of research supports this assertion, both theoretically and empirically, demonstrating that over-parameterized DNNs are predisposed to discovering low-rank solutions (Jing et al. 2021, Saxe et al. 2019, Soudry et al. 2018, Dwibedi et al. 2021). If the weight matrices in DNNs tend to be low-rank, it means that the weight matrices are highly self-related and redundant, which limits the expressiveness of DNNs and thus affects the quality of representations. ", "page_idx": 1}, {"type": "text", "text": "Therefore, this paper develops NR-DCCA, a DCCA-based method equipped with a generalized noise regularization (NR) approach. The NR approach ensures that the correlation with random data is invariant before and after the transformation, which we define as the Correlation Invariant Property (CIP). It is also verified that the NR approach can be applied to other DCCA-based methods. Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate the consistent outperformance and stability of the developed NR-DCCA method. ", "page_idx": 1}, {"type": "text", "text": "From a theoretical perspective, we derive the equivalent conditions between the full-rank property and CIP of the weight matrix. By forcing DNNs to possess CIP and thus mimicking the behavior of Linear CCA, we introduce random data to constrain the weight matrices in DNNs and expect to avoid them being redundant and thus prevent model collapse. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are four-fold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 The model collapse issue in DCCA-based methods for MVRL is identified, demonstrated, and explained.   \n\u2022 A simple yet effective noise regularization approach is proposed and NR-DCCA is developed to prevent model collapse. Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate the consistent outperformance and stability of the developed NR-DCCA.   \n\u2022 Rigorous proofs are provided to demonstrate that CIP is the equal condition of the full-rank weight matrix, which justifies the developed NR approach from a theoretical perspective.   \n\u2022 A novel framework is proposed to construct synthetic data with different common and complementary information for comprehensively evaluating MVRL methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Multi-view representation learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "MVRL aims to uncover relationships among multi-view data in an unsupervised manner, thereby obtaining semantically rich representations that can be utilized for various downstream tasks (Sun et al. 2023, Yan et al. 2021). Several works have been proposed to deal with MVRL from different aspects. DMF-MVC (Zhao et al. 2017) utilizes deep matrix factorization to extract a shared representation from multiple views. MDcR (Zhang, Fu, Hu, Zhu & Cao 2016) maps each view to a lower-dimensional space and applies kernel matching to enforce dependencies across the views. CPM-Nets (Zhang, Han, Fu, Zhou, Hu et al. 2019) formalizes the concept of partial MVRL and many works have been proposed for such issue (Zhang et al. 2020, Tao et al. 2019, Li et al. 2022, Yin & Sun 2021). $\\mathrm{\\AE^{2}}$ -Nets (Zhang, Liu & Fu 2019) utilizes a two-level autoencoder framework to obtain a comprehensive representation of multi-view data. DUA-Nets (Geng et al. 2021) takes a generative modeling perspective and dynamically estimates the weights for different views. MVTCAE (Hwang et al. 2021) explores MVRL from an information-theoretic perspective, which can capture the shared and view-specific factors of variation by maximizing or minimizing specific total correlation. Our work focuses on CCA as a simple, classic, and theoretically sound approach as it can still achieve state-of-the-art performance consistently. ", "page_idx": 2}, {"type": "text", "text": "2.2 CCA and its variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Canonical Correlation Analysis (CCA) projects the multi-view data into a unified space by maximizing their correlations (Hotelling 1992, Horst 1961, Hardoon et al. 2004, Lahat et al. 2015, Yan et al. 2023, Sun et al. 2023). It has been widely applied in various scenarios that involve multiview data, including dimension reduction (Zhang, Zhang, Pan & Zhang 2016, Sun, Ceran & Ye 2010, Avron et al. 2013), classification (Kim et al. 2007, Sun, Ji & Ye 2010), and clustering (Fern et al. 2005, Chang & Lin 2011). To further enhance the nonlinear transformability of CCA, Kernel CCA (KCCA) uses kernel methods, while Deep CCA (DCCA) employs DNNs. Since DNNs is parametric and can take advantage of large amounts of data for training, numerous DCCA-based methods have been proposed. Benton et al. (2017) utilizes DNNs to optimize the objective of Generalized CCA, to reveal connections between multiple views more effectively. To better preserve view-specific information, Wang et al. (2015) introduces the reconstruction errors of autoencoders to DCCA. Going a step further, Wang et al. (2016) proposes Variational CCA and utilizes dropout and private autoencoders to project common and view-specific information into two distinct spaces. Furthermore, many studies are exploring efficient methods for computing the correlations between multi-view data when dealing with more than two views such as MCCA, GCCA, and TCCA (Horst 1961, Nielsen 2002, Kettenring 1971, Hwang et al. 2021). Some research focuses on improving the efficiency of computing CCA by avoiding the need for singular value decomposition (SVD) (Chang et al. 2018, Chapman et al. 2022). However, the model collapse issue of DCCA-based methods has not been explored and addressed. ", "page_idx": 2}, {"type": "text", "text": "2.3 Noise Regularization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Noise regularization is a pluggable approach to regularize the neural networks during training (Bishop 1995, An 1996, Sietsma & Dow 1991, Gong et al. 2020). In supervised tasks, Sietsma & Dow (1991) might be the first to propose that, by adding noise to the train data, the model will generalize well on new unseen data. Moreover, Bishop (1995), Gong et al. (2020) analyze the mechanism of the noise regularization, and He et al. (2019), Gong et al. (2020) indicate that noise regularization can also be used for adversarial training to improve the generalization of the network. In unsupervised tasks, Poole et al. (2014) systematically explores the role of noise injection at different layers in autoencoders, and distinct positions of noise perform specific regularization tasks. However, how to make use of noise regularization for DCCA-based methods, especially for preventing model collapse, has not been studied. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will explain the objectives of the MVRL and then introduce Linear CCA and DCCA as representatives of the CCA-based methods and DCCA-based methods, respectively. Lastly, the model collapse issue in DCCA is demonstrated. ", "page_idx": 3}, {"type": "text", "text": "3.1 Settings for MVRL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose the set of datasets from $K$ different sources that describe the same object is represented by $X$ , and we define $X\\,=\\,\\{X_{1},\\cdot\\cdot\\cdot\\,,X_{k},\\cdot\\cdot\\cdot\\,,X_{K}\\},X_{k}\\,\\in\\,\\mathbb{R}^{d_{k}\\times n}$ , where $x_{k}$ represents the $k$ -th view ( $k$ -th data source), $n$ is the sample size, and $d_{k}$ represents the feature dimension for the $k$ -th view. And we use $X_{k}^{\\prime}$ to denote the transpose of $X_{k}$ . We take the Caltech101 dataset as an example and the training set has 6400 images. One image has been fed to three different feature extractors producing three features: a 1984-d HOG feature, a 512-d GIST feature, and a 928-d SIFT feature. Then for this dataset, we have $X_{1}\\in\\mathbb{R}^{1984\\times6400}$ , $X_{2}\\in\\mathbb{R}^{512\\times6400}$ , $X_{3}\\in\\mathbb{R}^{928\\times6400}$ . ", "page_idx": 3}, {"type": "text", "text": "The objective of MVRL is to learn a transformation function $\\Psi$ that projects the multi-view data $X$ to a unified representation $Z\\in\\mathbb{R}^{m\\times n}$ , where $m$ represents the dimension of the representation space, as shown below: ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ=\\Psi(X)=\\Psi(X_{1},\\cdot\\cdot\\cdot\\,,X_{k},\\cdot\\cdot\\cdot\\,,X_{K}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After applying $\\Psi$ for representation learning, we expect that the performance of using $Z$ would be better than directly using $X$ for various downstream tasks. ", "page_idx": 3}, {"type": "text", "text": "3.2 Canonical Correlation Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Among various MVRL methods, CCA projects the multi-view data into a common space by maximizing their correlations. We first define the correlation between the two views as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Corr}(W_{1}X_{1},W_{2}X_{2})=\\operatorname{tr}((\\Sigma_{11}^{-1/2}\\Sigma_{12}\\Sigma_{22}^{-1/2})^{\\prime}\\Sigma_{11}^{-1/2}\\Sigma_{12}\\Sigma_{22}^{-1/2})^{1/2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where tr denotes the matrix trace, $\\Sigma_{11},\\,\\Sigma_{22}$ represent the self-covariance matrices of the projected views, and $\\Sigma_{12}$ is the cross-covariance matrix between the projected views (D\u2019Agostini 1994, Andrew et al. 2013). The correlation between the two projected views can be regarded as the sum of all singular values of the normalized cross-covariance (Hotelling 1992, Anderson et al. 1958). ", "page_idx": 3}, {"type": "text", "text": "For multiple views, their correlation is defined as the summation of all the pairwise correlations (Nielsen 2002, Kettenring 1971), which is shown as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Corr}(W_{1}X_{1},\\cdot\\cdot\\cdot\\cdot,W_{k}X_{k},\\cdot\\cdot\\cdot\\cdot,W_{K}X_{K})=\\sum_{k<j}\\operatorname{Corr}(W_{k}X_{k},W_{j}X_{j}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Essentially, Linear CCA searches for the linear transformation matrices $\\{W_{k}\\}_{k}$ that maximize correlation among all the views. Mathematically, it can be represented as follows (Wang et al. 2015): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lbrace W_{k}^{*}\\rbrace_{k}=\\arg\\operatorname*{max}_{\\lbrace W_{k}\\rbrace_{k}}\\mathrm{Corr}(W_{1}X_{1},\\cdot\\cdot\\cdot\\,,W_{k}X_{k},\\cdot\\cdot\\cdot\\,,W_{K}X_{K}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Once $W_{k}^{*}$ is obtained by backpropagation, the multi-view data are projected into a unified space. Lastly, all projected data are concatenated to obtain $Z\\,=\\,\\bigl[W_{1}^{*}X_{1};\\cdot\\cdot\\cdot;W_{k}^{*}X_{k};\\cdot\\cdot\\cdot;W_{K}^{*}X_{K}\\bigr]$ for downstream tasks. ", "page_idx": 3}, {"type": "text", "text": "As an extension of linear CCA, DCCA employs neural networks to capture the nonlinear relationship among multi-view data. The only difference between DCCA and Linear CCA is that the linear transformation matrix $W_{k}$ is replaced by multi-layer perceptrons (MLP). Specifically, each $W_{k}$ is replaced by a neural network $f_{k}$ , which can be viewed as a nonlinear transformation. Similar to Linear CCA, the goal of DCCA is to solve the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{f_{k}^{*}\\}_{k}=\\arg\\operatorname*{max}_{\\{f_{k}\\}_{k}}\\mathsf{C o r r}\\left(f_{1}(X_{1}),\\cdot\\cdot\\cdot\\cdot,f_{k}(X_{k}),\\cdot\\cdot\\cdot\\cdot,f_{K}(X_{K})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The parameters in Linear CCA and DCCA are both updated through backpropagation (Andrew et al. 2013, Wang et al. 2015). Again, the unified representation is obtained by $Z\\ =$ $[f_{1}^{*}(X_{1});\\cdot\\cdot\\cdot~;f_{k}^{*}(X_{k});\\cdot\\cdot\\cdot~;f_{K}^{*}(X_{K})]$ for downstream tasks. ", "page_idx": 3}, {"type": "text", "text": "4 Model Collapse of DCCA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite exhibiting promising performance, DCCA shows a significant decline in performance as the training proceeds. We define this decline-in-performance phenomenon as the model collapse of DCCA. ", "page_idx": 4}, {"type": "text", "text": "Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (Andrew et al. 2013, De Bie & De Moor 2003). However, we further demonstrate that both representations and weight matrices of Linear CCA are full-rank whereas DCCA only guarantees that representations are full-rank but not for the weight matrices. Given that Linear CCA has only a single layer of linear transformation $W_{k}$ and the representations $W_{k}X_{k}$ are constrained to be full-rank by the loss function, $W_{k}$ in Linear CCA is full-rank (referred to Lemma 4 and assume that $W_{k}$ is a square matrix and $X_{k}$ is full-rank). As for DCCA, we consider a simple case when $f_{k}(X_{k})=R e l u(W_{k}X_{k})$ , and $f_{k}$ is a single-layer network and uses an element-wise Relu activation function. Only the representations $R e l\\bar{u}(W_{k}X_{k})$ are constrained to be full-rank, and hence we cannot guarantee that $W_{k}X_{k}$ is full-rank. For example, when $R e l u(W_{k}X_{k})\\,=\\,{\\binom{1,\\,0}{0,\\,1}}$ , it is clear that this is a matrix of rank 2, but in fact $W_{k}X_{k}$ can be $\\left(\\begin{array}{c c}{{1,}}&{{-1}}\\\\ {{-1,}}&{{1}}\\end{array}\\right)$ , and this is not full-rank. This reveals that the neural network $f_{k}$ is overfitted on $X_{k}$ , i.e., making representations $R e l u(W_{k}X_{k})$ to be full-rank with the constraint of its loss function, rather than $W_{k}$ itself being full-rank (verified in Appendix A.5.1). ", "page_idx": 4}, {"type": "text", "text": "Thus, we hypothesize that model collapse in DCCA arises primarily due to the low-rank nature of the DNN weight matrices. To investigate this, we analyze the eigenvalue distributions of the first linear layer\u2019s weight matrices in both DCCA and NR-DCCA across various training epochs on synthetic datasets. Figure 1 illustrates that during the initial training phase (100th epoch), the eigenvalues decay slowly for both DCCA and NR-DCCA. However, by the 1200th epoch, DCCA exhibits a markedly faster decay in eigenvalues compared to NR-DCCA. This observation suggests a synchronization between model collapse in DCCA and increased redundancy of the weight matrices. For more details on the experimental setup and results, please refer to Section 6.2. ", "page_idx": 4}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/1cb8a5d3e9efb317ce761dc2196d6d901ac659aa48a60e87165dece4981ef39f.jpg", "img_caption": ["Figure 1: Eigenvalue distributions of the first linear layer\u2019s weight matrices in the encoder of 1-st view. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "5 DCCA with Noise Regularization (NR-DCCA) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Method ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the discussions in previous sections, we present NR-DCCA, which makes use of the noise regularization approach to prevent model collapse in DCCA. Indeed, the developed noise regularization approach can be applied to variants of DCCA methods, such as Deep Generalized CCA (DGCCA) (Benton et al. 2017). An overview of the NR-DCCA framework is presented in Figure 2. ", "page_idx": 5}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/0bf84257b81c335fce132ea5adf5e20c060e557703be29794b55b134706c02e4.jpg", "img_caption": ["Figure 2: Illustration of NR-DCCA. We take the CUB dataset as an example: similar to DCCA, the $k$ -th view $X_{k}$ is transformed using $f_{k}$ to obtain new representation $f_{k}(\\bar{X}_{k})$ and then maximize the correlation between new representations. Additionally, for the $k$ -th view, we incorporate the proposed NR loss to regularize $f_{k}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The key idea in NR-DCCA is to generate a set of i.i.d Gaussian white noise, denoted as $A\\,=$ $\\{A_{1},\\therefore\\cdot\\cdot,A_{k},\\cdot\\cdot\\cdot,A_{K}\\},A_{k}\\,\\in\\,\\mathbb{R}^{d_{k}^{-}\\times n}$ , with the same shape as the multi-view data $X_{k}$ . In Linear CCA, the correlation with noise is invariant to the linear transformation $W_{k}$ : $\\operatorname{Corr}(X_{k},A_{k})=$ $\\operatorname{Corr}(W_{k}X_{k},W_{k}A_{k})$ (rigorous proof provided in Theorem 1). However, for DCCA, $\\operatorname{Corr}(X_{k},A_{k})$ might not equal $\\operatorname{Corr}(f_{k}(X_{k}),f_{k}(A_{k}))$ because the powerful neural networks $f_{k}$ have overfitted to the maximization program in DCCA and the weight matrices have been highly self-related. Therefore, we enforce the DCCA to mimic the behavior of Linear CCA by adding an NR loss $\\zeta_{k}=|C o r r(f_{k}(X_{k}),f_{k}(A_{k}))-C o r r(X_{k},A_{k})|$ , and hence the formulation of NR-DCCA is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{f_{k}^{*}\\}_{k}=\\arg\\operatorname*{max}_{\\{f_{k}\\}_{k}}\\mathsf{C o r r}\\left(f_{1}(X_{1}),\\cdot\\cdot\\cdot\\right.,f_{K}(X_{K}))-\\alpha\\sum_{k=1}^{K}\\zeta_{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ is the hyper-parameter weighing the NR loss. NR-DCCA can be trained through backpropagation with the randomly generated $A$ in each epoch, and the unified representation is obtained directly using $\\{f_{k}^{*}\\}_{k}$ in the same manner as DCCA. ", "page_idx": 5}, {"type": "text", "text": "5.2 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide the rationale for why the developed noise regularization can help to prevent the weight matrices from being low-rank and thus model collapse. Moreover, we prove the effect of full-rank weight matrices on the representations, which provides a tool to empirically verify the full-rank property of weight matrices by the quality of representations. ", "page_idx": 5}, {"type": "text", "text": "Utilizing a new Moore-Penrose Inverse (MPI)-based (Petersen et al. 2008) form of Corr in CCA, we discover that the full-rank property of $W_{k}$ is equal to CIP: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Correlation Invariant Property (CIP) of $W_{k}$ ) Given $W_{k}$ is a square matrix for any $k$ and $\\eta_{k}\\,=\\,|C o r r(W_{k}X_{k},W_{k}A_{k})-C o r r(X_{k},A_{k})|,$ , we have $\\eta_{k}\\,=\\,0$ (i.e. CIP) $\\longleftrightarrow\\ \\ W_{k}$ is full-rank. ", "page_idx": 5}, {"type": "text", "text": "Similarly, we say $f_{k}$ possess CIP if $\\zeta_{k}=0$ . Under Linear CCA, it is redundant to introduce the NR approach and force $W_{k}$ to possess CIP, since forcing $W_{k}X_{k}$ to be full-rank is sufficient to ensure that $W_{k}$ is full-rank. However, in DCCA, $f_{k}$ is overfitted on $X_{k}$ , i.e., making representations $f_{k}(X_{k})$ to be full-rank, rather than weight matrices in $f_{k}$ being full-rank. By forcing $f_{k}$ to possess CIP and thus mimicking the behavior of Linear CCA, the NR approach constrains the weight matrices to be full-rank and less redundant and thus prevents model collapse. ", "page_idx": 6}, {"type": "text", "text": "Next, we show that full-rank weight matrices (i.e., CIP) can greatly affect the quality of representations. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Effects of CIP on the obtained representations) For any $k_{i}$ , if $W_{k}$ is a square matrix and CIP holds for $W_{k}$ (i.e. $W_{k}$ is full-rank), $W_{k}X_{k}$ holds that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P_{k}}\\|P_{k}W_{k}X_{k}-X_{k}\\|_{F}=0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q_{k}}\\|Q_{k}W_{k}(X_{k}+A_{k})-W_{k}X_{k}\\|_{F}\\leq\\sqrt{n}\\|W_{k}A_{k}\\|_{F},E(\\|W_{K}A_{k}\\|_{F}^{2})=\\|W_{k}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\epsilon$ is a small positive threshold. $P_{k}$ and $Q_{k}$ are searched weight matrices of $k$ -th view to recover the input and discard noise, respectively. And we refer $\\|P_{k}W_{k}X_{k}-X_{k}\\|_{F}$ and $\\|Q_{k}W_{k}(X_{k}+A_{k})-\\bar{W}_{k}X_{k}\\|_{F}$ as reconstruction loss and denosing loss. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 suggests that the obtained representation is of low reconstruction loss and denoising loss. Low reconstruction loss suggests that the representations can be linearly reconstructed to the inputs. This implies that $W_{k}$ preserves distinct and essential features of the input data, which is a desirable property to avoid model collapse since it ensures that the model captures and retains the whole modality of data (Zhang, Liu & $:\\mathrm{Fu}\\;2019$ , Tschannen et al. 2018, Tian & Zhang 2022). Low denoising loss implies that the model\u2019s representation is robust to noise, which means that small perturbations in the input do not lead to significant changes in the output. This condition can be seen as a form of regularization that prevents overfitting the noise in the data (Zhou & Paffenroth 2017, Yan et al. 2023, Staerman et al. 2023). Additionally, the theorem also suggests that the rank of weight matrices is a good indicator to assess the quality of representations, which coincides with existing literature (Kornblith et al. 2019, Raghu et al. 2021, Garrido et al. 2023, Nguyen et al. 2020, Agrawal et al. 2022). ", "page_idx": 6}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct extensive experiments on both synthetic and real-world datasets to answer the following research questions: ", "page_idx": 6}, {"type": "text", "text": "\u2022 RQ1: How can we construct synthetic datasets to evaluate the MVRL methods comprehensively? \u2022 RQ2: Does NR-DCCA avoid model collapse across all synthetic MVRL datasets? \u2022 RQ3: Does NR-DCCA perform consistently in real-world datasets? ", "page_idx": 6}, {"type": "text", "text": "We follow the protocol described in Hwang et al. (2021) for evaluating the MVRL methods. For each dataset, we construct a training dataset and a test dataset. The encoders of all MVRL methods are trained on the training dataset. Subsequently, we encode the test dataset to obtain the representation, which will be evaluated in downstream tasks. We employ Ridge Regression (Hoerl & Kennard 1970) for the regression task and use $R2$ as the evaluation metric. For the classification task, we use a Support Vector Classifier (SVC) (Chang & Lin 2011) and report the average F1 scores. All tasks are evaluated using 5-fold cross-validation, and the reported results correspond to the average values of the respective metrics. ", "page_idx": 6}, {"type": "text", "text": "For a fair comparison, we use the same architectures of MLPs for all D(G)CCA methods. To be specific, for the synthetic dataset, which is simple, we employ only one hidden layer with a dimension of 256. For the real-world dataset, we use MLPs with three hidden layers, and the dimension of the middle hidden layer is 1024. We further demonstrate that increasing the depth of MLPs further accelerates the mod collapse of DCCA, while NR-DCCA maintains a stable performance in Appendix A.7. ", "page_idx": 6}, {"type": "text", "text": "Baseline methods include CONCAT, PRCCA (Tuzhilina et al. 2023), KCCA (Akaho 2006), Linear CCA Wang et al. (2015),Linear GCCA,DCCA (Andrew et al. 2013),DCCA EY, DCCA GHA (Chapman et al. 2022), DGCCA (Benton et al. 2017), DCCAE/DGCCAE (Wang et al. 2015), DCCA PRIVATE/DGCCA PRIVATE (Wang et al. 2016), and MVTCAE (Hwang et al. 2021). ", "page_idx": 7}, {"type": "text", "text": "It is important to note that our proposed NR approach requires the noise matrix employed to be full-rank, which is compatible with several common continuous noise distributions. In our primary experiments, we utilize Gaussian white noise. Additionally, as demonstrated in Appendix A.6, uniformly distributed noise is also effective in our NR approach. ", "page_idx": 7}, {"type": "text", "text": "Details of the experiment settings including datasets and baselines are presented in Appendix A.3. Hyper-parameter settings, including ridge regularization of DCCA, $\\alpha$ of NR, are discussed in Appendix A.5. We also analyze the computational complexity of different DCCA-based methods in Appendix A.12 and the learned representations are visualized in Appendix A.9. In the main paper, we mainly compare Linear CCA, DCCA-based methods, and NR-DCCA while other MVRL methods are discussed in Appendix A.11. The results related to DGCCA and are similar and presented in Appendix A.10. ", "page_idx": 7}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/a1b3184365ccbf6f8bc4104af1739da615ad29875a96bf22e77b2d771fdd8e8b.jpg", "img_caption": ["Figure 3: Construction of a synthetic dataset. This example consists of 2 views and $n$ objects, and the common rate is $0\\%$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.1 Construction of synthetic datasets (RQ1) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We construct synthetic datasets to assess the performance of MVRL methods, and the framework is illustrated in Figure 3. We believe that the multi-view data describes the same object, which is represented by a high-dimensional embedding $G^{d\\times n}$ , where $d$ is the feature dimension and $n$ is the size of the data, and we call it God Embedding. Each view of data is regarded as a nonlinear transformation of part (or all) of $G$ . For example, we choose $K\\,=\\,2,d\\,=\\,100$ , and then $X_{1}=\\phi_{1}(G[0:50+\\mathrm{{CR}/2,:]),X_{2}=\\phi_{x}(G[50-\\mathrm{{CR}/2:100],:})$ , where $\\phi_{1}$ and $\\phi_{2}$ are non-linear transformations, and $C R$ is referred to as common rate. The common rate is defined as follows: ", "page_idx": 7}, {"type": "text", "text": "Definition 1 (Common Rate) For two view data $X\\,=\\,\\{X_{1},X_{2}\\}.$ , common rate is defined as the percentage overlap of the features in $X_{1}$ and $X_{2}$ that originate from $G$ . ", "page_idx": 7}, {"type": "text", "text": "One can see that the common rate ranges from $0\\%$ to $100\\%$ . The larger the value, the greater the correlation between the two views, and a value of 0 indicates that the two views do not share any common dimensions in $G$ . Additionally, we construct the downstream tasks by directly transforming the God Embedding $G$ . Each task $T_{j}=\\psi_{j}(G)$ , where $\\psi_{j}$ is a transformation, and $T_{j}$ represents the $j$ -th task. By setting different $G$ , common rates, $\\phi_{k}$ , and $\\psi_{j}$ , we can create various synthetic datasets to evaluate the MVRL methods. Finally, $X_{k}$ are observable to the MVRL methods for learning the representation, and the learned representation will be used to classify/regress $T_{j}$ to examine the performance of each method. Detailed implementation is given in Appendix A.4. ", "page_idx": 7}, {"type": "text", "text": "6.2 Performance on Synthetic Datasets (RQ2) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We generate the synthetic datasets with different common rates, and the proposed NR-DCCA and other baseline methods are compared. As shown in Figure 4, one can see that the DCCA-based methods (e.g. DCCA, DCCAE, DCCA PRIVATE) will encounter model collapse during training, and the variance of accuracy also increases. Linear CCA demonstrates stable performance, while the best accuracy is not as good as DCCA-based methods. Our proposed NR-DCCA achieves stateof-the-art performance as well as training stability to prevent model collapse. The results at the final epoch for all common rates are also presented in Table 3 in Appendix A.11. ", "page_idx": 7}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/bfccacd0a0ec8f2ad821e0d2ca7d63b1ca055b90c174ed451d265bdeb8ef4b1c.jpg", "img_caption": ["Figure 4: (a) Mean and standard deviation of the (D)CCA-based method performance across synthetic datasets in different training epochs. (b) The mean correlation between noise and real data after transformation varies with epochs. (c) Average NESum across all weights within the trained encoders. (d,e) The mean of reconstruction and denoising loss on the test set. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Considering that we believe that the low-rank property (i.e. highly self-related and redundant) of the weight matrices is the root cause of the model collapse, we utilize NESum to measure the correlation among filters in the weight matrices ( defined in A.8). Higher NESum represents lower redundancy in weight matrices. As shown in (b) of Figure 4, our findings demonstrate that the NR approach effectively reduces filter redundancy, thereby preventing the emergence of low-rank weight matrices and thus averting model collapse. ", "page_idx": 8}, {"type": "text", "text": "Moreover, according to our analysis, the correlation should be invariant if neural networks have CIP. Therefore, after training DCCA, DCCAE, and NR-DCCA, we utilize the trained encoders to project the corresponding view data and randomly generated Gaussian white noise and then compute their correlation, as shown in (c) of Figure 4. It can be observed that, except for our method (NR-DCCA), as training progresses, other methods increase the correlation between unrelated data. It should be noted that this phenomenon always occurs under any common rates. ", "page_idx": 9}, {"type": "text", "text": "Given that the full-rank weight matrix not only produces features that are linearly reconstructed but also discriminates noise in the inputs, we also present the mean value pf Reconstruction and Denoising Loss across different common rates in (d) of Figure 4. Notably, NR-DCCA achieves a markedly lower loss, comparable to that observed with Linear CCA, whereas alternative DCCAbased approaches generally lose the above properties. ", "page_idx": 9}, {"type": "text", "text": "6.3 Consistent Performance on Real-world Datasets (RQ3) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We further conduct experiments on three real-world datasets: PolyMnist (Sutter et al. 2021), CUB (Wah et al. 2011), Caltech (Deng et al. 2018). Additionally, we use a different number of views in PolyMnist. The results are presented in Figure 5, and the performance of the final epoch in the figure is presented in Table 3 in the Appendix A.11. Generally, the proposed NR-DCCA demonstrates a competitive and stable performance. Different from the synthetic data, the DCCA-based methods exhibit varying degrees of collapse on various datasets, which might be due to the complex nature of the real-world views. ", "page_idx": 9}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/8d6ee1745b40a13c05d38c663dc4d1ea511a6d9646799a1e497909e441ce0254.jpg", "img_caption": ["Figure 5: Performance of different methods in real-world datasets. Each column represents the performance on a specific dataset. The number of views in the dataset is denoted in the parentheses next to the dataset name. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a novel noise regularization approach for DCCA in the context of MVRL, and it can prevent model collapse during the training, which is an issue observed and analyzed in this paper for the first time. Specifically, we theoretically analyze the CIP in Linear CCA and demonstrate that it is the key to preventing model collapse. To this end, we develop a novel NR approach to equip DCCA with such a property (NR-DCCA). Additionally, synthetic datasets with different common rates are generated and tested, which provide a benchmark for fair and comprehensive comparisons of different MVRL methods. The NR-DCCA developed in the paper inherits the merits of both Linear CCA and DCCA to achieve stable and consistent outperformance in both synthetic and real-world datasets. More importantly, the proposed noise regularization approach can also be generalized to other DCCA-based methods (e.g., DGCCA). ", "page_idx": 9}, {"type": "text", "text": "In future studies, we wish to explore the potential of noise regularization in other representation learning tasks, such as contrastive learning and generative models. It is also interesting to further investigate the difference between our developed NR and other neural network regularization approaches, such as orthogonality regularization (Bansal et al. 2018, Huang et al. 2020) and weight decay (Loshchilov & Hutter 2017, Zhang et al. 2018, Krogh & Hertz 1991). Our ultimate goal is to make the developed noise regularization a pluggable and useful module for neural network regularization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work described in this paper was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU/25209221 and PolyU/15206322), and grants from the Otto Poon Charitable Foundation Smart Cities Research Institute (SCRI) at the Hong Kong Polytechnic University (Project No. P0043552). The contents of this article reflect the views of the authors, who are responsible for the facts and accuracy of the information presented herein. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agrawal, K. K., Mondal, A. K., Ghosh, A. & Richards, B. (2022), \u2018a-req: Assessing representation quality in self-supervised learning by measuring eigenspectrum decay\u2019, Advances in Neural Information Processing Systems 35, 17626\u201317638. ", "page_idx": 10}, {"type": "text", "text": "Akaho, S. (2006), \u2018A kernel method for canonical correlation analysis\u2019, arXiv preprint cs/0609071 . ", "page_idx": 10}, {"type": "text", "text": "An, G. (1996), \u2018The effects of adding noise during backpropagation training on a generalization performance\u2019, Neural computation 8(3), 643\u2013674. ", "page_idx": 10}, {"type": "text", "text": "Anderson, T. W., Anderson, T. W., Anderson, T. W. & Anderson, T. W. (1958), An introduction to multivariate statistical analysis, Vol. 2, Wiley New York. ", "page_idx": 10}, {"type": "text", "text": "Andrew, G., Arora, R., Bilmes, J. & Livescu, K. (2013), Deep canonical correlation analysis, in \u2018International conference on machine learning\u2019, PMLR, pp. 1247\u20131255. ", "page_idx": 10}, {"type": "text", "text": "Avron, H., Boutsidis, C., Toledo, S. & Zouzias, A. (2013), Efficient dimensionality reduction for canonical correlation analysis, in \u2018International conference on machine learning\u2019, PMLR, pp. 347\u2013355. ", "page_idx": 10}, {"type": "text", "text": "Bansal, N., Chen, X. & Wang, Z. (2018), \u2018Can we gain more from orthogonality regularizations in training deep networks?\u2019, Advances in Neural Information Processing Systems 31. ", "page_idx": 10}, {"type": "text", "text": "Belitskii, G. et al. (2013), Matrix norms and their applications, Vol. 36, Birkha\u00a8user. ", "page_idx": 10}, {"type": "text", "text": "Benton, A., Khayrallah, H., Gujral, B., Reisinger, D. A., Zhang, S. & Arora, R. (2017), \u2018Deep generalized canonical correlation analysis\u2019, arXiv preprint arXiv:1702.02519 . ", "page_idx": 10}, {"type": "text", "text": "Bishop, C. M. (1995), \u2018Training with noise is equivalent to tikhonov regularization\u2019, Neural Computation 7(1), 108\u2013116. ", "page_idx": 10}, {"type": "text", "text": "Chang, C.-C. & Lin, C.-J. (2011), \u2018Libsvm: a library for support vector machines\u2019, ACM transactions on intelligent systems and technology (TIST) 2(3), 1\u201327. ", "page_idx": 10}, {"type": "text", "text": "Chang, X., Xiang, T. & Hospedales, T. M. (2018), Scalable and effective deep cca via soft decorrelation, in \u2018Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\u2019, pp. 1488\u20131497. ", "page_idx": 10}, {"type": "text", "text": "Chapman, J., Aguila, A. L. & Wells, L. (2022), \u2018A generalized eigengame with extensions to multiview representation learning\u2019, arXiv preprint arXiv:2211.11323 . ", "page_idx": 10}, {"type": "text", "text": "Deepak, K., Srivathsan, G., Roshan, S. & Chandrakala, S. (2021), \u2018Deep multi-view representation learning for video anomaly detection using spatiotemporal autoencoders\u2019, Circuits Systems and Signal Processing 40(2).   \nDeng, C., Chen, Z., Liu, X., Gao, X. & Tao, D. (2018), \u2018Triplet-based deep hashing network for cross-modal retrieval\u2019, IEEE Transactions on Image Processing 27(8), 3893\u20133903.   \nDwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. (2021), With a little help from my friends: Nearest-neighbor contrastive learning of visual representations, in \u2018Proceedings of the IEEE/CVF International Conference on Computer Vision\u2019, pp. 9588\u20139597.   \nFan, W., Ma, Y., Xu, H., Liu, X., Wang, J., Li, Q. & Tang, J. (2020), Deep adversarial canonical correlation analysis, in \u2018Proceedings of the 2020 SIAM International Conference on Data Mining\u2019, SIAM, pp. 352\u2013360.   \nFeichtenhofer, C., Pinz, A. & Zisserman, A. (2016), Convolutional two-stream network fusion for video action recognition, in \u2018Proceedings of the IEEE conference on computer vision and pattern recognition\u2019, pp. 1933\u20131941.   \nFern, X. Z., Brodley, C. E. & Friedl, M. A. (2005), Correlation clustering for learning mixtures of canonical correlation models, in \u2018Proceedings of the 2005 SIAM International Conference on Data Mining\u2019, SIAM, pp. 439\u2013448.   \nGarrido, Q., Balestriero, R., Najman, L. & Lecun, Y. (2023), Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 10929\u201310974.   \nGeng, Y., Han, Z., Zhang, C. & Hu, Q. (2021), Uncertainty-aware multi-view representation learning, in \u2018Proceedings of the AAAI Conference on Artificial Intelligence\u2019, Vol. 35, pp. 7545\u20137553.   \nGong, C., Ren, T., Ye, M. & Liu, Q. (2020), \u2018Maxup: A simple way to improve generalization of neural network training\u2019, arXiv preprint arXiv:2002.09024 .   \nGuo, H., Wang, J., Xu, M., Zha, Z.-J. & Lu, H. (2015), Learning multi-view deep features for small object retrieval in surveillance scenarios, in \u2018Proceedings of the 23rd ACM international conference on Multimedia\u2019, pp. 859\u2013862.   \nHardoon, D. R., Szedmak, S. & Shawe-Taylor, J. (2004), \u2018Canonical correlation analysis: An overview with application to learning methods\u2019, Neural computation 16(12), 2639\u20132664.   \nHe, Z., Rakin, A. S. & Fan, D. (2019), Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack, in \u2018Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\u2019, pp. 588\u2013597.   \nHoerl, A. E. & Kennard, R. W. (1970), \u2018Ridge regression: applications to nonorthogonal problems\u2019, Technometrics 12(1), 69\u201382.   \nHorst, P. (1961), \u2018Generalized canonical correlations and their applications to experimental data\u2019, Journal of Clinical Psychology 17(4), 331\u2013347.   \nHotelling, H. (1992), \u2018Relations between two sets of variates\u2019, Breakthroughs in statistics: methodology and distribution pp. 162\u2013190.   \nHuang, L., Liu, L., Zhu, F., Wan, D., Yuan, Z., Li, B. & Shao, L. (2020), Controllable orthogonalization in training dnns, in \u2018Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\u2019, pp. 6429\u20136438.   \nHwang, H., Kim, G.-H., Hong, S. & Kim, K.-E. (2021), \u2018Multi-view representation learning via total correlation objective\u2019, Advances in Neural Information Processing Systems 34, 12194\u201312207.   \nJing, L., Vincent, P., LeCun, Y. & Tian, Y. (2021), \u2018Understanding dimensional collapse in contrastive self-supervised learning\u2019, arXiv preprint arXiv:2110.09348 .   \nKarpathy, A. & Fei-Fei, L. (2015), Deep visual-semantic alignments for generating image descriptions, in \u2018Proceedings of the IEEE conference on computer vision and pattern recognition\u2019, pp. 3128\u20133137.   \nKettenring, J. R. (1971), \u2018Canonical analysis of several sets of variables\u2019, Biometrika 58(3), 433\u2013 451.   \nKim, T.-K., Wong, S.-F. & Cipolla, R. (2007), Tensor canonical correlation analysis for action classification, in \u20182007 IEEE Conference on Computer Vision and Pattern Recognition\u2019, IEEE, pp. 1\u20138.   \nKornblith, S., Norouzi, M., Lee, H. & Hinton, G. (2019), Similarity of neural network representations revisited, in \u2018International conference on machine learning\u2019, PMLR, pp. 3519\u20133529.   \nKrogh, A. & Hertz, J. (1991), \u2018A simple weight decay can improve generalization\u2019, Advances in neural information processing systems 4.   \nLahat, D., Adali, T. & Jutten, C. (2015), \u2018Multimodal data fusion: an overview of methods, challenges, and prospects\u2019, Proceedings of the IEEE 103(9), 1449\u20131477.   \nLe, Q. & Mikolov, T. (2014), Distributed representations of sentences and documents, in \u2018International conference on machine learning\u2019, PMLR, pp. 1188\u20131196.   \nLi, Z., Tang, C., Zheng, X., Liu, X., Zhang, W. & Zhu, E. (2022), \u2018High-order correlation preserved incomplete multi-view subspace clustering\u2019, IEEE Transactions on Image Processing 31, 2067\u2013 2080.   \nLoshchilov, I. & Hutter, F. (2017), \u2018Decoupled weight decay regularization\u2019, arXiv preprint arXiv:1711.05101 .   \nMao, J., Xu, W., Yang, Y., Wang, J., Huang, Z. & Yuille, A. (2014), \u2018Deep captioning with multimodal recurrent neural networks (m-rnn)\u2019, arXiv preprint arXiv:1412.6632 .   \nMorcos, A. S., Barrett, D. G., Rabinowitz, N. C. & Botvinick, M. (2018), \u2018On the importance of single directions for generalization\u2019, arXiv preprint arXiv:1803.06959 .   \nNguyen, T., Raghu, M. & Kornblith, S. (2020), \u2018Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth\u2019, arXiv preprint arXiv:2010.15327 .   \nNie, F., Li, J., Li, X. et al. (2017), Self-weighted multiview clustering with multiple graphs., in \u2018IJCAI\u2019, pp. 2564\u20132570.   \nNielsen, A. A. (2002), \u2018Multiset canonical correlations analysis and multispectral, truly multitemporal remote sensing data\u2019, IEEE transactions on image processing 11(3), 293\u2013305.   \nPetersen, K. B., Pedersen, M. S. et al. (2008), \u2018The matrix cookbook\u2019, Technical University of Denmark 7(15), 510.   \nPoole, B., Sohl-Dickstein, J. & Ganguli, S. (2014), \u2018Analyzing noise in autoencoders and deep networks\u2019, arXiv preprint arXiv:1406.1831 .   \nPrechelt, L. (1998), \u2018Automatic early stopping using cross validation: quantifying the criteria\u2019, Neural networks 11(4), 761\u2013767.   \nRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C. & Dosovitskiy, A. (2021), \u2018Do vision transformers see like convolutional neural networks?\u2019, Advances in neural information processing systems 34, 12116\u201312128.   \nSaxe, A. M., McClelland, J. L. & Ganguli, S. (2019), \u2018A mathematical theory of semantic development in deep neural networks\u2019, Proceedings of the National Academy of Sciences 116(23), 11537\u2013 11546.   \nSietsma, J. & Dow, R. J. (1991), \u2018Creating artificial neural networks that generalize\u2019, Neural networks 4(1), 67\u201379.   \nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. & Srebro, N. (2018), \u2018The implicit bias of gradient descent on separable data\u2019, Journal of Machine Learning Research 19(70), 1\u201357.   \nSrivastava, N. & Salakhutdinov, R. R. (2012), \u2018Multimodal learning with deep boltzmann machines\u2019, Advances in neural information processing systems 25.   \nStaerman, G., Adjakossa, E., Mozharovskyi, P., Hofer, V., Sen Gupta, J. & Cle\u00b4menc\u00b8on, S. (2023), \u2018Functional anomaly detection: a benchmark study\u2019, International Journal of Data Science and Analytics 16(1), 101\u2013117.   \nSun, J., Xiu, X., Luo, Z. & Liu, W. (2023), \u2018Learning high-order multi-view representation by new tensor canonical correlation analysis\u2019, IEEE Transactions on Circuits and Systems for Video Technology .   \nSun, L., Ceran, B. & Ye, J. (2010), A scalable two-stage approach for a class of dimensionality reduction techniques, in \u2018Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining\u2019, pp. 313\u2013322.   \nSun, L., Ji, S. & Ye, J. (2010), \u2018Canonical correlation analysis for multilabel classification: A leastsquares formulation, extensions, and analysis\u2019, IEEE Transactions on Pattern Analysis and Machine Intelligence 33(1), 194\u2013200.   \nSutter, T. M., Daunhawer, I. & Vogt, J. E. (2021), \u2018Generalized multimodal elbo\u2019, arXiv preprint arXiv:2105.02470 .   \nTao, H., Hou, C., Yi, D., Zhu, J. & Hu, D. (2019), \u2018Joint embedding learning and low-rank approximation: A framework for incomplete multiview learning\u2019, IEEE transactions on cybernetics 51(3), 1690\u20131703.   \nTian, Y. & Zhang, Y. (2022), \u2018A comprehensive survey on regularization strategies in machine learning\u2019, Information Fusion 80, 146\u2013166.   \nTschannen, M., Bachem, O. & Lucic, M. (2018), \u2018Recent advances in autoencoder-based representation learning\u2019, arXiv preprint arXiv:1812.05069 .   \nTuzhilina, E., Tozzi, L. & Hastie, T. (2023), \u2018Canonical correlation analysis in high dimensions with structured regularization\u2019, Statistical modelling 23(3), 203\u2013227.   \nWah, C., Branson, S., Welinder, P., Perona, P. & Belongie, S. (2011), \u2018The caltech-ucsd birds-200- 2011 dataset\u2019, california institute of technology .   \nWang, W., Arora, R., Livescu, K. & Bilmes, J. (2015), On deep multi-view representation learning, in \u2018International conference on machine learning\u2019, PMLR, pp. 1083\u20131092.   \nWang, W., Yan, X., Lee, H. & Livescu, K. (2016), \u2018Deep variational canonical correlation analysis\u2019, arXiv preprint arXiv:1610.03454 .   \nWang, Z., Xiang, C., Zou, W. & Xu, C. (2020), \u2018Mma regularization: Decorrelating weights of neural networks by maximizing the minimal angles\u2019, Advances in Neural Information Processing Systems 33, 19099\u201319110.   \nWei, J., Xia, Y. & Zhang, Y. (2019), \u2018M3net: A multi-model, multi-size, and multi-view deep neural network for brain magnetic resonance image segmentation\u2019, Pattern Recognition 91, 366\u2013378.   \nWong, H. S., Wang, L., Chan, R. & Zeng, T. (2021), \u2018Deep tensor cca for multi-view learning\u2019, IEEE Transactions on Big Data 8(6), 1664\u20131677.   \nXu, J., Zheng, H., Wang, J., Li, D. & Fang, X. (2020), \u2018Recognition of eeg signal motor imagery intention based on deep multi-view feature learning\u2019, Sensors 20(12), 3496.   \nYan, H., Cheng, L., Ye, Q., Yu, D.-J. & Qi, Y. (2023), \u2018Robust generalized canonical correlation analysis\u2019, Applied Intelligence pp. 1\u201316.   \nYan, X., Hu, S., Mao, Y., Ye, Y. & Yu, H. (2021), \u2018Deep multi-view learning methods: A review\u2019, Neurocomputing 448, 106\u2013129.   \nYang, M., Li, Y., Huang, Z., Liu, Z., Hu, P. & Peng, X. (2021), Partially view-aligned representation learning with noise-robust contrastive loss, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 1134\u20131143.   \nYao, Y., Rosasco, L. & Caponnetto, A. (2007), \u2018On early stopping in gradient descent learning\u2019, Constructive Approximation 26, 289\u2013315.   \nYin, J. & Sun, S. (2021), \u2018Incomplete multi-view clustering with reconstructed views\u2019, IEEE Transactions on Knowledge and Data Engineering .   \nZhang, C., Cui, Y., Han, Z., Zhou, J. T., Fu, H. & Hu, Q. (2020), \u2018Deep partial multi-view learning\u2019, IEEE transactions on pattern analysis and machine intelligence 44(5), 2402\u20132415.   \nZhang, C., Fu, H., Hu, Q., Zhu, P. & Cao, X. (2016), \u2018Flexible multi-view dimensionality coreduction\u2019, IEEE Transactions on Image Processing 26(2), 648\u2013659.   \nZhang, C., Han, Z., Fu, H., Zhou, J. T., Hu, Q. et al. (2019), \u2018Cpm-nets: Cross partial multi-view networks\u2019, Advances in Neural Information Processing Systems 32.   \nZhang, C., Liu, Y. & Fu, H. (2019), Ae2-nets: Autoencoder in autoencoder networks, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 2577\u20132585.   \nZhang, G., Wang, C., Xu, B. & Grosse, R. (2018), \u2018Three mechanisms of weight decay regularization\u2019, arXiv preprint arXiv:1810.12281 .   \nZhang, H., Wang, S., Ioannidis, V. N., Adeshina, S., Zhang, J., Qin, X., Faloutsos, C., Zheng, D., Karypis, G. & Yu, P. S. (2023), \u2018Orthoreg: Improving graph-regularized mlps via orthogonality regularization\u2019, arXiv preprint arXiv:2302.00109 .   \nZhang, Y., Zhang, J., Pan, Z. & Zhang, D. (2016), \u2018Multi-view dimensionality reduction via canonical random correlation analysis\u2019, Frontiers of Computer Science 10, 856\u2013869.   \nZhao, H., Ding, Z. & Fu, Y. (2017), Multi-view clustering via deep matrix factorization, in \u2018Proceedings of the AAAI conference on artificial intelligence\u2019, Vol. 31.   \nZheng, Q., Zhu, J., Li, Z., Pang, S., Wang, J. & Li, Y. (2020), \u2018Feature concatenation multi-view subspace clustering\u2019, Neurocomputing 379, 89\u2013102.   \nZhou, C. & Paffenroth, R. C. (2017), Anomaly detection with robust deep autoencoders, in \u2018Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining\u2019, pp. 665\u2013674.   \nZhu, X., Zhou, W. & Li, H. (2018), Improving deep neural network sparsity through decorrelation regularization., in \u2018Ijcai\u2019, pp. 3264\u20133270. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1.1 Preparations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove Theorem 1, we need first to prove the following Lemmas. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 Given a specific matrix $B$ and a zero-centered $C$ with respect to rows, the product BC is also zero-centered with respect to rows. ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 1: Let $B_{i,j}$ and $C_{i,j}$ denote the $(i,j)$ -th entry of $B$ and $C$ , respectively. Then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(B C)_{i,j}=\\sum_{r=1}B_{i,r}C_{r,j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since each row of $C$ has a mean of $0$ , we have $\\textstyle\\sum_{j=1}^{n}C_{r,j}=0,\\forall r$ . For the mean value of $i$ -th row of $B C$ , we can write: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}(B C)_{i,j}=\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}D_{i,r}C_{r,j}}&{}\\\\ {\\displaystyle}&{=\\displaystyle\\frac{1}{n}\\sum_{r=1}^{n}D_{i,r}C_{r,j}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{r=1}^{n}B_{i,r}\\left(\\sum_{j=1}^{n}C_{r,j}\\right)}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{r=1}^{n}B_{i,r}\\cdot0}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The Moore-Penrose Inverse (MPI) (Petersen et al. 2008) will be used for analysis, and the MPI is defined as follows: ", "page_idx": 15}, {"type": "text", "text": "Definition 2 Given a specific matrix $Y$ , its Moore-Penrose Inverse (MPI) is denoted as $Y^{+}$ . $Y^{+}$   \nsatisfies: $Y Y^{+}Y=Y$ , $Y^{+}Y Y^{+}=Y^{+}$ , $Y Y^{+}$ is symmetric, and $Y^{+}Y$ is symmetric. ", "page_idx": 15}, {"type": "text", "text": "The MPI $Y^{+}$ is unique and always exists for any $Y$ . Furthermore, when matrix $Y$ is invertible, its inverse matrix $Y^{-}$ is exactly $Y^{+}$ . Using the definition of MPI, we can rewrite the formulation of CCA. In particular, $\\mathrm{Corr}(\\cdot,\\cdot)$ can be derived by replacing the inverse with MPI. Using $\\operatorname{Corr}(X_{k},A_{k})$ as an example, the following Lemma holds: ", "page_idx": 15}, {"type": "text", "text": "Lemma 2 (MPI-based CCA) For the $k$ -th view data $X_{k}$ and the Gaussian white noise $A_{k}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nC o r r(X_{k},A_{k})=\\frac{1}{(n-1)^{2}}t r(A_{k}^{+}A_{k}X_{k}^{+}X_{k})^{1/2},\\forall k.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 2: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{C o r}(X_{k},A_{k})=\\mathrm{tr}((\\Sigma_{1}^{-1/2}\\Sigma_{12}\\Sigma_{2}^{-1/2})\\Sigma_{1}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2})^{1/2}}\\\\ &{=\\mathrm{tr}(\\Sigma_{2}^{-1/2}\\Sigma_{12}^{-1}\\Sigma_{1}^{-1/2}\\Sigma_{1}^{-1/2}\\Sigma_{1}\\Sigma_{2}^{-1/2})^{1/2}}\\\\ &{=\\mathrm{tr}(\\Sigma_{2}^{-1/2}\\Sigma_{2}^{-1/2}\\Sigma_{1}^{-1/2}\\Sigma_{1}^{-1/2}\\Sigma_{1})^{1/2}}\\\\ &{=\\mathrm{tr}(\\Sigma_{2}^{-1}\\Sigma_{1}^{-1}\\Sigma_{1})^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((A_{k}A_{k}^{*})^{-1}(X_{k}A_{k}^{*})^{\\prime}(X_{k}X_{k}^{*})^{-1}(X_{k}A_{k}^{*}))^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((A_{k}X_{k}^{*})^{-1}(A_{k}X_{k}^{*})(X_{k}X_{k}^{*})^{-1}(X_{k}A_{k}^{*}))^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((A_{k}A_{k}^{*})^{+}(A_{k}X_{k}^{*})(X_{k}X_{k}^{*})^{+}(X_{k}A_{k}^{*}))^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A_{k}^{*}(A_{k}A_{k}^{*})^{+}+A_{k}X_{k}^{*}(X_{k}X_{k}^{*})^{+}X_{k})^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A_{k}^{*}(A_{k}A_{k}^{*})^{+}A_{k}X_{k}^{*}(X_{k}^{*}) \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first row is based on the definition of Corr, the second row is because the trace is invariant under cyclic permutation, the fifth row is to replace matrix inverse by MPI and the ninth row is due to $\\dot{Y}^{+}=\\bar{Y}^{\\prime}(Y Y^{+})$ (Petersen et al. 2008). ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 Given a specific matrix $Y$ and its MPI $Y^{+}$ , let $R a n k(Y)$ and $R a n k(Y^{+}Y)$ be the ranks of $Y$ and $Y^{+}Y$ , respectively. It is true that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R a n k(Y)=R a n k(Y^{+}Y)}\\\\ {R a n k(Y^{+}Y)=t r(Y^{+}Y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3: ", "page_idx": 16}, {"type": "text", "text": "Firstly, the column space of $Y^{+}Y$ is a subspace of the column space of $Y$ . Therefore, $\\operatorname{Rank}(Y^{+}Y)\\,\\leq\\,\\operatorname{Rank}(\\operatorname{\\bar{\\calY}})$ . On the other hand, according to the definition of MPI (Petersen et al. 2008), we know that $\\dot{Y}\\overset{\\cdot}{=}Y(Y^{+}Y)$ . Since the rank of a product of matrices is at most the minimum of the ranks of the individual matrices, we have $\\operatorname{Rank}(Y)\\,\\leq\\,\\operatorname{Rank}(Y^{+}Y)$ . Combining the two inequalities, we have $\\operatorname{Rank}(Y)\\,=\\,\\operatorname{Rank}(Y^{+}Y)$ . Furthermore, since $(Y^{+}Y)(Y^{+}Y)=\\bar{Y}^{+}Y$ (it holds that $Y^{+}\\,=\\,Y^{+}Y Y^{+}$ according to the definition of MPI (Petersen et al. 2008)), $Y^{+}Y$ is an idempotent and symmetric matrix, and thus its eigenvalues must be 0 or 1. So the sum of its eigenvalues is exactly its rank. Considering matrix trace is the sum of eigenvalues of matrices, we have $\\operatorname{Rank}(Y^{+}Y)=\\operatorname{\\mathbf{tr}}(Y^{+}Y)$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 $R a n k(W_{k}X_{k})\\,<\\,R a n k(X_{k})$ , when $W_{k}$ is not $a$ full-rank matrix and $X_{k}$ is a full-rank matrix. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 4: Since the rank of a product of matrices is at most the minimum of the ranks of the individual matrices, we have $\\operatorname{Rank}(W_{k}X_{k})\\,\\leq\\,m i n(\\operatorname{Rank}(W_{k}),\\operatorname{Rank}(X_{k}))$ . Considering $X_{k}$ is full-rank, $\\operatorname{Rank}(X_{k})\\,=\\,m i n(d_{k},n)$ and then $\\operatorname{Rank}(W_{k}X_{k})\\,\\leq\\,m i n(\\operatorname{Rank}(W_{k}),\\operatorname{Rank}(X_{k}))\\,=$ $m i n(\\operatorname{Rank}(W_{k}),m i n(d_{k},n))$ . Since $W_{k}$ is not full-rank, we have $\\mathrm{Rank}(W_{k})<d_{k}$ . In conclusion, $\\operatorname{Rank}(W_{k}X_{k})<m i n(d_{k},m i n(d_{k},n))$ and then $\\operatorname{Rank}(W_{k}X_{k})<d_{k}\\leq\\operatorname{Rank}(X_{k})$ . ", "page_idx": 16}, {"type": "text", "text": "A.1.2 Main proofs of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We prove the two directions of Theorem 1 in the following two Lemmas. First, we prove CIP holds if $W_{k}$ is a square and full-rank matrix. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 For any $k$ , if $W_{k}$ is a square and full-rank matrix, the correlation between $X_{k}$ and $A_{k}$ remains unchanged before and after the transformation by $W_{k}$ (i.e. CIP holds for $W_{k}$ ). Mathematically, we have $C o r r(X_{k},A_{k})=C o r r(W_{k}X_{k},W_{k}A_{k})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5: ", "page_idx": 16}, {"type": "text", "text": "Firstly, we have the $k$ -th view data $X_{k}$ to be full-rank, as we can always delete the redundant data, and the random noise $A_{k}$ is full-rank as each column is generated independently. Without loss of generality, we assume that all the datasets $X_{k}$ are zero-centered with respect to row (Hotelling 1992), which implies that $W_{k}A_{k}$ and $W_{k}X_{k}$ are both zero-centered matrices 1. When computing the covariance matrix, there is no need for an additional subtraction of the mean of row, which simplifies our subsequent derivations. And $W_{k}$ is always full-rank since Linear CCA seeks full-rank $W_{k}X_{k}$ . Then by utilizing Lemma 2, we derive that the correlation between $X_{k}$ and $A_{k}$ remains unchanged before and after the transformation: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{2orr}(W_{k}X_{k},W_{k}A_{k})=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}A_{k})^{+}W_{k}A_{k}(W_{k}X_{k})^{+}W_{k}X_{k})^{1/2}}&{}&\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}A_{k})^{+}(W_{k}A_{k}A_{k}^{+})^{+}W_{k}A_{k}(W_{k}^{+}W_{k}X_{k})^{+}(W_{k}X_{k}^{+})^{+}W_{k}X_{k})^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}A_{k})^{+}W_{k}^{+}W_{k}A_{k}(W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})^{1/2}}\\\\ &{=\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A_{k}^{+}A_{k}X_{k}^{+}X_{k})^{1/2}}\\\\ &{=\\mathrm{Corr}(X_{k},A_{k})}&{...}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first row is based on Lemma 2, the second row is because given two matrices $B$ and $C$ , $(B C)^{+}\\:=\\:(B^{+}B C)^{+}(B C^{+}C)^{+}$ always holds (Petersen et al. 2008), and the third row utilizes the properties of full-rank and square matrix $W_{k}\\colon W_{k}^{+}=W_{k}^{-}$ , which means ${\\cal W}_{k}^{+}{\\cal W}_{k}={\\cal W}_{k}{\\cal W}_{k}^{+}=$ $I_{d_{k}}$ (Petersen et al. 2008). ", "page_idx": 17}, {"type": "text", "text": "Then we prove that $W_{k}$ is a full-rank matrix if CIP holds and $W_{k}$ is square. ", "page_idx": 17}, {"type": "text", "text": "Lemma 6 For any $k$ , if $C o r r(X_{k},A_{k})\\,=\\,C o r r(W_{k}X_{k},W_{k}A_{k})$ and $W_{k}$ is a square matrix, then $W_{k}$ must be a full-rank matrix. ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 6: ", "page_idx": 17}, {"type": "text", "text": "This Lemma is equivalent to its contra-positive proposition: if $W_{k}$ is not a full-rank matrix, there exists random noise data $A_{k}$ such that $\\eta_{k}\\,=\\,|C o r r(W_{k}X_{k},W_{k}(A_{k}))-C o r r(X_{k},A_{k})|$ is not 0. And we find that when $W_{k}$ is not full-rank, there exists $A_{k}=X_{k}$ such that $\\eta_{k}\\neq0$ . We have the following derivation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\lambda=|C o r r(W_{k}X_{k},W_{k}A_{k})-C o r r(X_{k},A_{k})|}}\\\\ {\\displaystyle{\\phantom{=}=\\left|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}A_{k})^{+}(W_{k}A_{k})(W_{k}X_{k})^{+}(W X_{k}))^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A^{+}A X^{+}X)^{1/2}\\right|}}\\\\ {\\displaystyle{\\phantom{=}=\\left|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}A_{k})^{+}(W_{k}A_{k}A_{k}^{+})^{+}W_{k}A_{k}(W_{k}^{+}W_{k}X_{k})^{+}(W_{k}X_{k}X_{k}^{+})^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A_{k}^{+}A_{k}X_{k}^{+}X_{k})^{1/2}\\right|}}\\\\ {\\displaystyle{\\phantom{=}=\\left|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}A_{k})^{+}W_{k}^{+}W_{k}A_{k}(W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(A_{k}^{+}A X_{k}^{+}X_{k})^{1/2}\\right|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first row is the definition of NR loss with respect to $W_{k}$ , the second row is based on the new form of CCA, the third row is because given two specific matrices $B$ and $C$ , it holds the equality $(B C)^{+}=(B^{+}B C)^{+}(B C^{+}C)^{+}$ (Petersen et al. 2008), and the fourth row utilizes the properties of full-rank matrix: for full-rank matrices $X_{k}$ and $A_{k}$ , whose sample size is larger than dimension size, they fulfill: $X_{k}{X_{k}}^{+}=I_{d_{k}},A_{k}{A_{k}}^{+}=I_{d_{k}}$ (given a specific full-rank matrix $Y$ , if its number of rows is smaller than that of cols, it holds that $\\bar{Y}^{+}=Y^{\\prime}(\\bar{Y}Y^{\\prime})^{-}$ , which means that $Y Y^{+}=I$ ) (Petersen et al. 2008). ", "page_idx": 17}, {"type": "text", "text": "Let us analyze the case when $A_{k}=X_{k}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\eta_{k}=\\left|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k}(W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(X_{k}^{+}X_{k}X_{k}^{+}X_{k})^{1/2}\\right|}\\\\ {\\displaystyle\\quad=\\left|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(X_{k}^{+}X_{k})^{1/2}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first row is to replace $A_{k}$ with $X_{k}$ , the second row is because $X_{k}^{+}X_{k}X_{k}^{+}~=~X_{k}^{+}$ and $(W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k}(W_{k}^{+}W_{k}X_{k})^{+}=W_{k}^{+}W_{k}X_{k}$ , which are based on the definition of MPI that given a specific matrix $Y$ , $Y^{+}Y Y^{+}=Y^{+}$ (Petersen et al. 2008). ", "page_idx": 17}, {"type": "text", "text": "As a result, we can know that when the random noise data $A_{k}$ is exactly $X_{k}$ and $W_{k}$ is not full-rank, $\\eta_{k}$ can not be zero: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\eta_{k}=\\bigg|\\frac{1}{(n-1)^{2}}\\mathrm{tr}((W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{tr}(X_{k}^{+}X_{k})^{1/2}\\bigg|}\\\\ {\\quad=\\bigg|\\frac{1}{(n-1)^{2}}\\mathrm{Rank}(W_{k}^{+}W_{k}X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{Rank}(X)^{1/2}\\bigg|}\\\\ {\\quad\\neq\\bigg|\\frac{1}{(n-1)^{2}}\\mathrm{Rank}(X_{k})^{1/2}-\\frac{1}{(n-1)^{2}}\\mathrm{Rank}(X_{k})^{1/2}\\bigg|}\\\\ {\\quad\\neq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first row is due to Equation 14, the second row is based on Lemma 3 that $\\mathrm{tr}((W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k})=\\mathrm{\\bar{Rank}}(W_{k}^{+}W_{k}X_{k})$ and $\\operatorname{tr}(X_{k}^{+}X_{k})=\\operatorname{Rank}(X)$ , and the third row is because of Lemma 4. ", "page_idx": 18}, {"type": "text", "text": "Finally, if $\\eta_{k}$ is always constrained to 0 for any $A_{k}$ , then $W_{k}$ must be a full-rank matrix. ", "page_idx": 18}, {"type": "text", "text": "Combining Lemma 5 and 6, we complete the proof. ", "page_idx": 18}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For linear regression problem : $R^{*}\\,=\\,\\arg\\operatorname*{min}_{R}\\,\\|R B\\,-\\,C\\|_{F}$ , where $R\\ \\in\\ \\mathbb{R}^{o u t\\_d i m\\times i n p u t\\_d i m}$ is the weight matrix, and $B\\,\\in\\,\\mathbb{R}^{i n p u t\\-d i m\\times n},C\\,\\in\\,\\mathbb{R}^{o u t p u t\\-d i m\\times n}$ are the input and target matrix $(i n p u t.d i m<n)$ ), respectively. $R^{*}$ has a closed-form solution: $R^{*}=C B^{\\prime}(B B^{\\prime})^{-}$ and therefore, it holds that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}\\|R B-C\\|_{F}=\\|R^{*}B-C\\|_{F}}&{}\\\\ {\\triangleq\\|C B^{\\prime}(B B^{\\prime})^{-}B-C\\|_{F}}&{}\\\\ {\\triangleq\\|C B^{\\prime}(B B^{\\prime})^{+}B-C\\|_{F}}&{}\\\\ {\\triangleq\\|C B^{+}B-C\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Equation $a$ is the use of the closed-form solution and Equation $b$ is to replace the matrix inverse by MPI. Equation $c$ is because $Y^{\\prime}(Y Y^{\\prime})^{+}=Y^{+}$ (Petersen et al. 2008). ", "page_idx": 18}, {"type": "text", "text": "Given $k$ -th view, when $W_{k}$ is square possesses CIP, $W_{k}$ is full-rank. Using the above equation, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}\\|P_{k}W_{k}X_{k}-X_{k}\\|_{F}\\triangleq\\|X_{k}(W_{k}X_{k})^{+}(W_{k}X_{k})-X_{k}\\|_{F}}&{}\\\\ {\\triangleq\\|X_{k}(W_{k}^{+}W_{k}X_{k})^{+}(W_{k}X_{k}X_{k}^{+})^{+}(W_{k}X_{k})-X_{k}\\|_{F}}&{}\\\\ {\\overset{\\mathrm{c}}{=}\\|X_{k}(W_{k}^{+}W_{k}X_{k})^{+}W_{k}^{+}W_{k}X_{k}-X_{k}\\|_{F}}&{}\\\\ {\\overset{\\mathrm{d}}{=}\\|X_{k}X_{k}^{+}X_{k}-X_{k}\\|_{F}}&{}\\\\ {=0}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Equation $a$ is due to Equation 16 (Petersen et al. 2008). Equation $b$ holds because given two matrices $B$ and $C$ , $(B C)^{+}\\,=\\,\\bar{(}B^{+}B C)^{+}(B C^{+}C)^{+}$ always holds and Equation $c$ is because for full-rank matrix $X_{k}\\,\\in\\,\\mathbb{R}^{d_{k}\\times n}(d_{k}\\,<\\,n)$ , $X_{k}X_{k}^{+}\\,=\\,I_{d_{k}}$ . Equation $c$ utilizes the properties of full-rank and square matrix $W_{k}$ : $W_{k}^{+}\\,=\\,W_{k}^{-}$ , which means $W_{k}^{+}W_{k}\\,=\\,W_{k}W_{k}^{+}\\,=\\,I_{d_{k}}$ (Petersen et al. 2008). Equation $d$ is based on the definition of MPI: given a specific matrix $Y$ and its $Y^{+}$ , it holds that $Y{\\bar{Y}}^{+}Y=Y$ . We show the first property in Theorem 2. ", "page_idx": 18}, {"type": "text", "text": "As for the second property: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}\\|Q_{i}W_{k}(X_{k}+A_{k})-W_{k}X_{4}\\|_{F}}&{\\leq\\|W_{k}X_{k}(W_{k}(X_{k}+A_{k}))^{*}(W_{k}(X_{k}+A_{k}))-W_{k}X_{1}\\|_{F}}\\\\ &{\\quad\\leq\\operatorname*{lim}_{\\bar{W}_{k}\\bar{W}_{k}}\\left(W_{k}^{*}W_{k}(X_{k}+A_{k})\\right)^{*}(W_{k}(X_{k}{A}_{k})(X_{k}+A_{k}))^{+}(W_{k}(X_{k}+A_{k}))-W_{K}X_{4}\\|_{F}}\\\\ &{\\quad\\leq\\|W_{k}X_{k}(W_{k}^{*}W_{k}(X_{k}+A_{k}))^{*}W_{k}^{*}\\bar{W}_{k}(X_{k}+A_{k})-W_{K}X_{1}\\|_{F}}\\\\ &{\\quad\\leq\\|W_{k}X_{k}(X_{k}+A_{k})^{*}(X_{k}+A_{k})-W_{K}X_{1}\\|_{F}}\\\\ &{\\quad\\overset{,}{\\quad\\leq}\\|W_{k}(X_{k}+A_{k})(X_{k}+A_{k})^{*}(X_{k}+A_{k})-W_{k}X_{4}(X_{k}+A_{k})^{*}(X_{k}+A_{k})-W_{K}X_{4}\\|_{F}}\\\\ &{\\quad\\overset{,}{\\quad\\leq}\\|W_{k}(X_{k}+A_{k})-W_{k}A_{k}(X_{k}+A_{k})^{*}(X_{k}+A_{k})-W_{K}X_{1}\\|_{F}}\\\\ &{\\quad\\leq\\|W_{k}X_{k}(W_{k}-(X_{k}+A_{k})^{*}(X_{k}+A_{k})^{*}(X_{k}+A_{k})-W_{K}X_{1}\\|_{F}}\\\\ &{\\quad\\overset{,}{\\quad\\leq}\\|W_{k}A_{k}-W_{k}A_{k}(X_{k}+A_{k})^{*}(X_{k}+A_{k})\\|_{F}}\\\\ &{\\quad\\overset{,}{\\quad\\leq}\\|W_{k}A_{k}[r_{k}-(X_{k}+A_{k})^{*}(X_{k}+A_\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Equation $a$ is because of Equation 16. Equation $b$ holds because given two matrices $B$ and $C$ , $(\\dot{B}C)^{+}=(B^{+}B C)^{+}(B C^{+}\\dot{C})^{+}$ always holds and Equation $c$ is because we assume $X_{k}+A_{k}$ is a full-rank matrix. Equation $e$ utilizes the properties of full-rank and square matrix $W_{k}$ : $W_{k}^{+}W_{k}=$ $W_{k}W_{k}^{+}=I_{d_{k}}$ . Equation $g$ is based on the definition of MPI: $(X_{k}+A_{k})(X_{k}+A_{k})^{+}(X_{k}+A_{k})=$ $\\left(X_{k}+A_{k}\\right)$ . Equation $j$ holds because given two specific matrices $B$ and $C$ , $\\|B C\\|_{F}\\,\\le\\,\\|B\\|_{F}\\,*$ $\\|C\\|_{F}$ (Belitskii et al. 2013). Equation $k$ and $l$ is because given a specific matrix $B$ , $I-B^{+}B$ is an idempotent matrix and $\\|I-B^{+}B\\|_{F}=\\sqrt{\\mathrm{tr}((I-B^{+}B)^{\\prime}(I-B^{+}B))}=\\sqrt{\\mathrm{tr}(I-B^{+}B)}$ . ", "page_idx": 19}, {"type": "text", "text": "Now, we use $(W_{k}A_{k})_{(}i,j)$ to donate the $(i,j)$ -th entry of $W_{k}A_{k}$ and the expected value of the square of the Frobenius norm of $W_{k}A_{k}$ is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|W A_{k}\\|_{F}^{2}\\right]=\\mathbb{E}\\left[\\sum_{i}\\sum_{j}\\left[(W_{k}A_{k})_{i,j}\\right]^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Expanding the product $(W_{k}A_{k})_{i,j}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n(W_{k}A_{k})_{i,j}=\\sum_{r}(W_{k})_{i,r}(A_{k})_{r,j}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substituting back into the expectation, we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert W A_{k}\\Vert_{F}^{2}\\right]=\\mathbb{E}\\left[\\sum_{i}\\sum_{j}\\left[\\sum_{r}(W_{k})_{i,r}(A_{k})_{r,j}\\right]^{2}\\right]=\\sum_{i}\\sum_{j}\\mathbb{E}\\left[\\left[\\sum_{r}(W_{k})_{i,r}(A_{k})_{r,j}\\right]^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the elements of $A_{k}$ are i.i.d. with zero mean and unit variance, the expectation of their squares is 1, and the cross terms vanish due to the zero mean. Therefore, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i}\\sum_{j}\\mathbb{E}\\left[\\left[\\sum_{r}(W_{k})_{i,r}(A_{k})_{r,j}\\right]^{2}\\right]=\\sum_{i}\\sum_{r}(W_{k})_{i r}^{2}=\\|W_{k}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we have shown that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lVert W_{k}A_{k}\\rVert_{F}^{2}\\right]=\\lVert W_{k}\\rVert_{F}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "A.3 Details of Datasets and Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Synthetic datasets: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All the datasets used in the paper are either provided or open datasets. Detailed proofs of all the Theorems in the main paper can be found in the Appendix. Both source codes and appendix can be downloaded from the supplementary material. ", "page_idx": 19}, {"type": "text", "text": "We make 6 groups of multi-view data originating from the same $G~\\in~\\mathbb{R}^{d\\times n}$ (we set $n\\ =$ $4000,d\\ =\\ \\Bar{100})$ . Each group consists of tuples with 2 views (2000 tuples for training and 2000 tuples for testing) and a distinct common rate. Common rates of these sets are from $\\{0\\%,20\\bar{\\%}$ , $40\\%$ , $60\\%$ , $\\bar{8}0\\%,100\\%\\}$ and there are 50 downstream regression tasks. We report the mean and standard deviation of R2 score across all the tasks. ", "page_idx": 20}, {"type": "text", "text": "Real-world datasets: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "PolyMnist (Sutter et al. 2021): A dataset consists of tuples with 5 different MNIST images (60, 000 tuples for training and 10, 000 tuples for testing). Each image within a tuple possesses distinct backgrounds and writing styles, yet they share the same digit label. The background of each view is randomly cropped from an image and is not used in other views. Thus, the digit identity represents the common information, while the background and writing style serve as view-specific factors. The downstream task is the digit classification task. CUB (Wah et al. 2011): A dataset consists of tuples with deep visual features (1024-d) extracted by GOOGLENET and text features (300-d) obtained through DOC2VEC (Le & Mikolov 2014) (480 tuples for training and 600 tuples for testing). This MVRL task utilizes the first 10 categories of birds in the original dataset and the downstream task is the bird classification task. Caltech (Deng et al. 2018): A dataset consists of tuples with traditional visual features extracted from images that belong to 101 object categories, including an additional background category (6400 tuples for training and 9144 tuples for testing). Following Yang et al. (2021), three features are used as views: a 1, 984-d HOG feature, a 512-d GIST feature, and a 928-d SIFT feature. ", "page_idx": 20}, {"type": "text", "text": "Baselines: All of our experiments are conducted with fixed random seeds and all the performance of downstream tasks is the average value of a 5-fold cross-validation. We use one single 3090 GPU. The CCA-zoo package is adopted as the implementation of various CCA/GCCA-based methods, and the original implementation of MVTCAE is employed. Both baselines and our developed NRDCCA/NR-DGCCA are implemented in the same PyTorch environment (see requirements.txt in the source codes). ", "page_idx": 20}, {"type": "text", "text": "Direct method: ", "page_idx": 20}, {"type": "text", "text": "CONCAT straightforwardly concatenates original features from different views. ", "page_idx": 20}, {"type": "text", "text": "CCA methods: ", "page_idx": 20}, {"type": "text", "text": "\u2022 PRCCA Tuzhilina et al. (2023) preserves the internal data structure by grouping highdimensional data features while applying an l2 penalty to CCA,   \n\u2022 Linear CCA (Wang et al. 2015) employs individual linear layers to project multi-view data and then maximizes their correlation defined in (Hotelling 1992).   \n\u2022 Linear GCCA uses linear layers to maximize the correlation of multi-view data defined in (Benton et al. 2017). ", "page_idx": 20}, {"type": "text", "text": "Kernel CCA Methods: ", "page_idx": 20}, {"type": "text", "text": "\u2022 KCCA (Akaho 2006) employs CCA methods through positive-definite kernels. ", "page_idx": 20}, {"type": "text", "text": "DCCA-based methods: ", "page_idx": 20}, {"type": "text", "text": "\u2022 DCCA (Andrew et al. 2013) employs neural networks to individually project multiple sets of views, obtaining new representations that maximize the correlation between each pair of views.   \n\u2022 DGCCA (Benton et al. 2017) constructs a shared representation and maximizes the correlation between each view and the shared representation.   \n\u2022 DCCA EY (Chapman et al. 2022) optimizes the objective of CCA via a sample-based EigenGame.   \n\u2022 DCCA GHA (Chapman et al. 2022) solves the objective of CCA by a sample-based generalized Hebbian algorithm.   \n\u2022 DCCAE/DGCCAE (Wang et al. 2015) introduces reconstruction objectives to DCCA, which simultaneously optimize the canonical correlation between the learned representations and the reconstruction errors of the autoencoders. ", "page_idx": 20}, {"type": "text", "text": "\u2022 DCCA PRIVATE/DGCCA PRIVATE (Wang et al. 2016) incorporates dropout and private autoencoders, thus preserving both shared and view-specific information. ", "page_idx": 21}, {"type": "text", "text": "Information theory-based methods: ", "page_idx": 21}, {"type": "text", "text": "\u2022 MVTCAE (Hwang et al. 2021) maximizes the reduction in Total Correlation to capture both shared and view-specific factors of variations. ", "page_idx": 21}, {"type": "text", "text": "All CCA-based methods leverage the implementation of CCA-Zoo (Chapman & Wang 2021). To ensure fairness, we use the official implementation of MVTCAE while replacing the strong CNN backbone with MLP. ", "page_idx": 21}, {"type": "text", "text": "A.4 Implementation Details of Synthetic Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We draw $n$ random samples with dim $d$ from a Gaussian distribution as $G\\,\\in\\,\\mathbb{R}^{d\\times n}$ to represent complete representations of $n$ objects. We define the non-linear transformation $\\phi_{k}$ as the addition of noise to the data, followed by passing it through a randomly generated MLP. To generate the data for the $k$ -th view, we select specific feature dimensions from $G$ based on a given common rate 1 and then apply $\\phi_{k}$ to those selected dimensions. And we define $\\psi_{j}$ as a linear layer, and task $T_{j}$ is generated by directly passing G through $\\psi_{j}$ . ", "page_idx": 21}, {"type": "text", "text": "A.5 Hyper-parameter Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To ensure a fair comparison, we tune the hyper-parameters of all baselines within the ranges suggested in the original papers, including hyper-parameter $r$ of ridge regularization, except for the following fixed settings: ", "page_idx": 21}, {"type": "text", "text": "The embedding size for the real-world datasets is set as 200, while the size for synthetic datasets is set as 100. Batch size is min(2000, full-size). The same MLP architectures are used for D(G)CCAbased methods. The hyper-parameter $r$ of ridge regularization is set as 0 in our NR-DCCA and NR-DGCCA. And the best $r$ for Linear (G)CCA and D(G)CCA-based methods is tuned on the validation data (synthetic datasets and PolyMnist: 1e-3, CUB and Caltech $101:0$ ). ", "page_idx": 21}, {"type": "text", "text": "In the synthetic datasets, Linear CCA and Linear GCCA use a minimum learning rate of $1e-4$ , DCCA, DGCCA, DCCAE, and DGCCAE methods utilize a bigger learning rate of $5e\\mathrm{~-~}3$ . DCCA PRIVATE/DGCCA PRIVATE employ a slightly higher learning rate of $1e-2$ . In contrast, our proposed methods, NR-DCCA/NR-DGCCA, utilize the maximum learning rate of $1.5e\\mathrm{~-~}2$ . And the regularization weight $\\alpha$ is set as 200. ", "page_idx": 21}, {"type": "text", "text": "In the real-world datasets, the learning rates for all deep methods are set to $1e\\!-\\!4$ while that of Linear CCA and Linear GCCA are $1e-5$ . To expedite the computation of $\\operatorname{Corr}(X_{k},A_{k})$ , in the real-world datasets, we simply employ $X_{k}[$ : outdim, $:]$ and $A_{k}[$ : outdim, : $\\vdots]$ to compute of ${\\mathrm{Corr}}(X_{k},A_{k})$ . The optimal $\\alpha$ values of NR-DCCA for the CUB, PolyMnist, and Caltech datasets are found to be 1.5, 2, and 10, respectively. ", "page_idx": 21}, {"type": "text", "text": "A.5.1 Hyper-parameter $r$ in Ridge Regularization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we discuss the effects of hyper-parameter $r$ in ridge regularization. Ridge regularization is commonly used across almost all (D)CCA methods, which improves numerical stability. It works by adding an identity matrix $I$ to the estimated covariance matrix. However, ridge regularization mainly regularizes the features, rather than the transformation (i.e., $W_{k}$ in Linear CCA and $f_{k}$ in DCCA) and it cannot prevent the weight matrices in DNNs from being low-rank or redundant. To further support our arguments, we provide the experimental results with different ridge parameters on a real-world dataset CUB as shown in Figure 6. One can see that the ridge regularization even damages the performance of DCCA and also leads to an increase in the internal correlation within the feature and the correlation between the feature and noise. In our NR-DCCA, we set the ridge parameter to zero. We conjecture the reason is that the large ridge parameter could make the neural network even \u201clazier\u201d to actively project the data into a better feature space, as the full-rank property of features and covariance matrix are already guaranteed. ", "page_idx": 21}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/4b8c6fe63fdccf1f9bd833bf142eede105417c4ea8ba416c01d8f06c0f3998a6.jpg", "img_caption": ["Figure 6: The effects of hyper-parameter $r$ of DCCA in the CUB dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.5.2 Hyper-parameter $\\alpha$ of NR-DCCA ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The choice of the hyper-parameter $\\alpha$ is essential in NR-DCCA. Different from the conventional hyper-parameter tuning procedures, the determination of $\\alpha$ is simpler, as we can search for the smallest $\\alpha$ that can prevent the model collapse, and the model collapse can be directly observed on the validation data. Specifically, we increase the $\\alpha$ adaptively until the model collapse issue is tackled, i.e., the correlation with noise will not increase or the performance of DCCA will not drop with increasing training epochs, then the optimal $\\alpha$ is found. To further illustrate the influence of $\\alpha$ in NR-DCCA, we present performance curves of NR-DCCA in CUB under different $\\alpha$ . As shown in Figure 7, if $\\alpha$ is too large, the convergence of the training becomes slow; if $\\alpha$ is too small, model collapse remains. Additionally, one can see the NR-DCCA outperforms DCCA robustly with a wide range of $\\alpha$ . ", "page_idx": 22}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/e6b961969c7c2baa40680c0a6f9c25618a6688b962bcdd22cfdb7d189cd12a73.jpg", "img_caption": ["Figure 7: The effects of hyper-parameter $\\alpha$ of NR-DCCA in the CUB dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.6 Effects of the Distribution of Noise ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From our theoretical analysis, the most important feature of noise in NR is that the sampled noise matrix is a full-rank matrix. Therefore, continuous distributions such as the uniform distribution can also be applied to NR, which demonstrates the robustness of the proposed NR method. We compare NR-DCCA with different noise distributions on synthetic datasets, and both noises are effective in suppressing model collapse as shown in Table 1. ", "page_idx": 22}, {"type": "table", "img_path": "HSRs6yyuUK/tmp/5bcc5dacd479e77eae4c8184c82644e4cc1f3fef30b99e7d13c163242f77eaaf.jpg", "table_caption": ["Table 1: Effect of Noise on DCCA and NR-DCCA "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.7 Effects of depths of Encoders ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we test the effects of depths of encoders (i.e. MLPs) on model collapse and NR. Specifically, we increase the depth of MLPs to observe the variation in the performance of DCCA and NR-DCCA on synthetic datasets. As shown in Table 2, The increase in network depth results in a faster decline in DCCA performance, while NR-DCCA still maintains a stable performance. ", "page_idx": 22}, {"type": "table", "img_path": "HSRs6yyuUK/tmp/8c04cca2daf2aeadeca4aa1dfa30d86142f3c481643ed88f5f2ff1ad4e4ea360.jpg", "table_caption": ["Table 2: Performance comparison of DCCA and NR-DCCA across different network depths. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.8 Effects of NR Loss on Filter Redundancy ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Extensive research has established a significant correlation between the redundancy of neurons or filters and the compromised generalization capabilities of neural networks, indicating a propensity for overfitting (Wang et al. 2020, Morcos et al. 2018, Zhu et al. 2018). Considering the fully connected layer with 1024 units in a MLP network as a paradigm, the initial layer\u2019s weights, denoted by $W\\in\\dot{\\mathbb{R}}^{1024\\times3\\times28\\times28}$ , can be interpreted as 1024 discriminative filters. These filters operate on images with 3 channels, each of size $28\\times28$ , with every filter representing a vector in a $3\\times28\\times28$ dimensional space. Subsequently, a similarity matrix $S$ is constructed, wherein each element $S_{i j}$ quantifies the cosine similarity between the $i^{t h}$ and $j^{t h}$ filters, with higher values indicating greater redundancy. To further assess filter redundancy in $W$ , we employ NESum, a metric designed for evaluating redundancy and whiten degrees of features (Zhang et al. 2023). ", "page_idx": 23}, {"type": "text", "text": "Definition 3 (NESum of Weight) Given a weight matrix $W\\,\\in\\,\\mathbb{R}^{o u t p u t\\times i n p u t}$ with an accompanying output-wise similarity matrix $S\\,\\in\\,\\mathbb{R}^{o u t p u t\\times o u t p u t}$ and eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{o u t p u t}$ sorted in descending order, the normalized eigenvalue sum is defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nN E S u m(W)=\\frac{1}{o u t p u t}\\sum_{i=1}^{o u t p u t}\\frac{\\lambda_{i}}{\\lambda_{1}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Figure 8, we present the evolution of the average NESum across all weights within the trained encoders. Notably, we observe a sustained increase in NESum exclusively in NR-DCCA throughout prolonged training epochs. This phenomenon underscores the efficacy of the loss of NR in reducing filter redundancy, crucially preventing low-rank solutions. ", "page_idx": 23}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/5c7925bf8367236b515068066d6162db3b48a2aada8df7f70f2f4456ddfbe8a0.jpg", "img_caption": ["Figure 8: Average NESum across all weights within the trained encoders. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.9 Visualization of Learned Representations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To further demonstrate the effectiveness of our method, we employ 2D-tSNE visualization to depict the learned representations of the CUB dataset (test set) under different methods. Each data point is colored based on its corresponding class, as illustrated in Figure 9. There are a total of 10 categories, with 60 data points in each category. A reasonable distribution of learned representations entails that data points belonging to the same class are grouped in the same cluster, which is distinguishable from clusters representing other classes. Additionally, within each cluster, the data points should exhibit an appropriate level of dispersion, indicating that the data points within the same class can be differentiated rather than collapsing into a single point. This dispersion is indicative of the preservation of as many distinctive features of the data as possible. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "From Figure. 9, we can observe that CCA, DCCA / DGCCA have all confused the data from different categories. Specifically, CCA completely scatters the data points as it cannot handle nonlinear relationships. By incorporating autoencoders, DCCAE / DGCCAE and DCCA PRIVATE / DGCCA PRIVATE have partially separated the data; however, they have not fully separated the green and orange categories. NR-DCCA / NR-DGCCA is the only method that successfully separates all categories. ", "page_idx": 24}, {"type": "text", "text": "It is worth noting that our approach not only separates the data into different clusters but also maintains dispersion within each cluster. Unlike DCCA PRIVATE / DGCCA PRIVATE, where the data points within a cluster form a strip-like distribution, our method ensures that the data points within each cluster remain appropriately scattered. ", "page_idx": 24}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/f3cbc2fd5266ceee24ef42ef5ff13043fbb451638a2f41907c060981442ca63a.jpg", "img_caption": ["Figure 9: Visualization of the learned representations with t-SNE in the CUB dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "This section presents the experimental results for DGCCA and NR-DGCCA, which supplement the results of GCCA and NR-DCCA presented in the main paper. In general, DGCCA is a variant of DCCA, and hence the proposed noise regularization approach can also be applied. We repeat the experiments in Figures 4,and 5, and hence we have the results for DGCCA in Figure 10, and 11. One can see that the proposed noise regularization approach can also help DGCCA prevent model collapse, proving its generalizability. ", "page_idx": 25}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/a8c832f2d6f71818405f89b93347d07ad61f3836745d8aeed704349a2d151027.jpg", "img_caption": ["A.10 DGCCA and NR-DGCCA ", "Figure 10: (a) Mean and standard deviation of the (D)GCCA-based method performance across synthetic datasets in different training epochs.(b) The mean correlation between noise and real data after transformation varies with epochs. (c) Average NESum across all weights within the trained encoders. (d,e) The mean of Reconstruction and Denoising Loss on the test set. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "HSRs6yyuUK/tmp/675bf928f2ba336a60dd6f96d5ddd3d7158ac877c9bf1f3b3f614461a502649e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: Performance of DGCCA-based methods in real-world datasets. Each column represents the performance on a specific dataset. The number of views in the dataset is denoted in the parentheses next to the dataset name. ", "page_idx": 26}, {"type": "text", "text": "A.11 Additional Experimental Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Table 3 and 4 present the model performance of various MVRL methods in synthetic and real-world datasets, and both tables correspond to the final epoch of the results presented in Figure 4and 5. It should be noted that the values in Table 3 represent the mean and standard deviation of the methods across different tasks, indicating their performance and variability. ", "page_idx": 27}, {"type": "table", "img_path": "HSRs6yyuUK/tmp/0f66afdc8fffa7dac71aa799180fa08a50ab7d59b2c271d9de2ec21a1e1b1c47.jpg", "table_caption": ["Table 3: Performance in synthetic datasets. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "HSRs6yyuUK/tmp/aa268ca682c58ba76850ffd0a6e9d86293fc528d92c8c0cd5d9663a679ecaad6.jpg", "table_caption": ["Table 4: Performance in real-world datasets "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "A.12 Complexity Analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we compare the computational complexity of different DCCA-based methods. Assuming that we have data from $K$ views, with each view containing $N$ samples and $D$ feature dimensions, then we have the computational complexity of each method in Table 5. ", "page_idx": 27}, {"type": "table", "img_path": "HSRs6yyuUK/tmp/a7a44dc412427c5ae1495f5ce90502d7ec98395886c40e7c7641665cf0a00671.jpg", "table_caption": ["Table 5: Comparisons of computational complexity against baselines "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 Complexity of MLP: We will use neural networks with the same MLP structure, consisting of $L$ hidden layers, each with $H$ neurons. Therefore, the computational complexity of one pass of the data through the neural networks can be expressed as $O(N*(D*H+D*M+$ $\\dot{L}\\ast H^{2})$ ). To simplify, we use $O(N*L*H^{2})$ . ", "page_idx": 27}, {"type": "text", "text": "\u2022 Complexity of Corr: During the process of calculating $C o r$ among $K$ views, three main computations are involved. The calculation complexity of the covariance is $O(N*(M*$ $K)^{2}$ . Second, the complexity of the inverse matrix and the eigenvalues are $O((M*K)^{3}$ . As a result, the computational complexity of calculating Cor can be considered as $O((M*$ $K)^{3}$ ).   \n\u2022 Complexity of reconstruction loss: The reconstruction loss, also known as the mean squared error (MSE) loss, has a complexity of $O(N*D)$ . ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In the abstract, we explain that we have found that model collapse occurs in DCCA, and we propose the NR approach to solve this phenomenon, and the NR approach generalizes well in DGCCA. In the introduction, we list our 4 contributions in detail. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our approach requires the introduction of additional computational overhead, which is discussed in Appendix. Our theory needs to assume that 1) feature matrice is fullrank and 2) the input and output dimensions are the same. The first assumption is fairly common because the DCCA itself requires a large batchsize. The second assumption is the same as the theoretical assumption that Linear CCA gets a full-rank weight matrix, and our experiments verify that our NR approach is still effective even if the dimensions are different. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 29}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We declare our theorem in the main paper and provide a complete proof in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All of our experimental codes including how to divide the generated data, training and testing are provided in the supplementary materials. And we\u2019ve fixed random seeds. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include instructions in the appendix, including how to download and divide the data set, how to execute training and test scripts, and how to visualize the results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have explained data division and hyperparameters in detail in both Appendix and code, and discussed the impact of hyperparameters on results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We randomly generated dozens of downstream tasks and reported the mean and standard deviation of their performance. And we used a 5-fold cross-validation to report the average of the 5-fold performance. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We compared the time cost of different methods in Appendix, and explained that our experiments were all completed on a single 3090 GPU. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The data sets we use are all public data sets, and we strictly follow previous studies, which will not have a bad impact on the society. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Multi-view data is very common in society, and our method can more stably represent multi-view data, so that it can be applied to a wide range of tasks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We found the shortcomings of the original DCCA and tried to improve it so that it would not be applied to the wrong places. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We referenced the framework we used and the data set we used. In the code, we listed the URL of each data set and the code environment we needed. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]