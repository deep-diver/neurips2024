[{"type": "text", "text": "Language Models Encode Collaborative Signals in Recommendation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent studies empirically indicate that language models (LMs) encode rich world   \n2 knowledge beyond mere semantics, attracting significant attention across various   \n3 fields. However, in the recommendation domain, it remains uncertain whether   \n4 LMs implicitly encode user preference information. Contrary to the prevailing   \n5 understanding that LMs and traditional recommender models learn two distinct rep  \n6 resentation spaces due to a huge gap in language and behavior modeling objectives,   \n7 this work rethinks such understanding and explores extracting a recommendation   \n8 space directly from the language representation space. Surprisingly, our findings   \n9 demonstrate that item representations, when linearly mapped from advanced LM   \n10 representations, yield superior recommendation performance. This outcome sug  \n11 gests a homomorphic relationship between the language representation space and   \n12 an effective recommendation space, implying that collaborative signals may indeed   \n13 be encoded within advanced LMs. Motivated by these findings, we propose a   \n14 simple yet effective collaborative filtering (CF) model named AlphaRec, which   \n15 utilizes language representations of item textual metadata (e.g., titles) instead of tra  \n16 ditional ID-based embeddings. Specifically, AlphaRec is comprised of three main   \n17 components: a multilayer perceptron (MLP), graph convolution, and contrastive   \n18 learning (CL) loss function, making it extremely easy to implement and train. Our   \n19 empirical results show that AlphaRec outperforms leading ID-based CF models   \n20 on multiple datasets, marking the first instance of such a recommender with text   \n21 embeddings achieving this level of performance. Moreover, AlphaRec introduces   \n22 a new text-based CF paradigm with several desirable advantages: being easy to   \n23 implement, lightweight, rapid convergence, superior zero-shot recommendation   \n24 abilities in new domains, and being aware of user intention. ", "page_idx": 0}, {"type": "text", "text": "25 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "26 Language models (LMs) have achieved great success across various domains [3\u20137], prompting a   \n27 critical question about the knowledge encoded within their representation spaces. Recent studies   \n28 empirically find that LMs extend beyond semantic understanding to encode comprehensive world   \n29 knowledge about various domains, including game states [8], lexical attributes [9], and even concepts   \n30 of space and time [10] through language modeling. However, in the domain of recommendation   \n31 where the integration of LMs is attracting widespread interest [11\u201315], it remains unclear whether   \n32 LMs inherently encode relevant information on user preferences and behaviors. One possible reason   \n33 is the significant difference between the objectives of language modeling for LMs and user behavior   \n34 modeling for recommenders [16\u201319].   \n35 Currently, one prevailing understanding holds that general LMs and traditional recommenders   \n36 encode two distinct representation spaces: the language space and the recommendation space   \n37 (i.e., user and item representation space), each offering potential enhancements to the other for   \n38 recommendation tasks [17, 20]. On the one hand, when using LMs as recommenders, aligning the   \n39 language space with the recommendation space could significantly improve the performance of   \n40 LM-based recommendation [14, 21\u201323]. Various alignment strategies are proposed, including fine  \n41 tuning LMs with recommendation data [15, 16, 24\u201326], incorporating embeddings from traditional   \n42 recommenders as a new modality of LMs [17, 20, 27], and extending the vocabulary of LMs with item   \n43 tokens [18, 19, 28\u201331]. On the other hand, when using LMs as the enhancer, traditional recommenders   \n44 greatly benefti from from leveraging text representations [32\u201345], semantic and reasoning information   \n45 [46\u201349], and generated user behaviors [50, 51]. Despite these efforts, explicit explorations of the   \n46 relationship between language and recommendation spaces remain largely unexplored.   \n47 In this work, we rethink the prevailing understanding and explore whether LMs inherently encode   \n48 user preferences through language modeling. Specifically, we test the possibility of directly deriving a   \n49 recommendation space from the language representation space, assessing whether the representations   \n50 of item textual metadata (e.g., titles) obtained from LMs can independently achieve satisfactory   \n51 recommendation performance. Positive results would imply that user behavioral patterns, such as   \n52 collaborative signals (i.e., user preference similarities between items) [52, 53], may be implicitly   \n53 encoded by LMs. To test this hypothesis, we employ linear mapping to project the language   \n54 representations of item titles into a recommendation space (see Figure 1a). Our observations include:   \n55 \u2022 Surprisingly, this simple linear mapping yields high-quality item representations, which achieve   \n56 exceptional recommendation performance (see Figure 1b and experimental results in Section 2).   \n57 \u2022 The clustering of items is generally preserved from the language space to the recommendation   \n58 space (see Figure 1c). For example, movies with the theme of superheroes and monsters are   \n59 gathering in both language and recommendation spaces.   \n60 \u2022 Interestingly, the linear mapping effectively reveals preference similarities that may be implicit   \n61 or even obscure in the language space. For instance, while certain movies, such as those of   \n62 homosexual movies (illustrated in Figure 1c), show dispersed representations in the language space,   \n63 their projections through linear mapping tend to cluster together, reflecting their genres affliiation.   \n64 These findings indicate a homomorphic relationship between the language representation space of   \n65 LMs and an effective item representation space for recommendation. Motivated by this insight, we   \n66 propose a new text-based recommendation paradigm for general collaborative filtering (CF), which   \n67 utilizes the pre-trained language representations of item titles as the item input and the average   \n68 historical interactions\u2019 representations as the user input. Different from traditional ID-based CF   \n69 models [54, 55, 52] that heavily rely on trainable user and item IDs, this paradigm solely uses   \n70 pre-trained LM embeddings and completely abandons ID-based embeddings. In this paper, to fully   \n71 explore the potential of advanced language representations, we adopt a simple model architecture   \n72 consisting of a two-layer MLP with graph convolution, and the popular contrastive loss, InfoNCE   \n73 [56\u201358], as the objective function. This model is named AlphaRec for its originality and a series of   \n74 good properties.   \n75 Benefiting from paradigm shifts from ID-based embeddings to language representations, AlphaRec   \n76 presents three desirable advantages. First, AlphaRec is notable for its simplicity, lightweight, rapid   \n77 convergence, and exceptional recommendation performance (see Section 4.1). We empirically   \n78 demonstrate that, for the first time, such a simple model with embeddings from pre-trained LMs can   \n79 outperform leading CF models on multiple datasets. This finding strongly supports the possibility   \n80 for developing language-representation-based recommender systems. Second, AlphaRec exhibits   \n81 a strong zero-shot recommendation capability across untrained domains (see Section 4.2). By   \n82 co-training on three Amazon datasets (Books, Movies & TV, and Video Games) [1], AlphaRec   \n83 can achieve performance comparable to the fully-trained LightGCN on entirely different platforms   \n84 (MovieLens-1M [59] and BookCrossing [60]), and even exceed LightGCN in a completely new   \n85 domain (Amazon Industrial), without additional training on these target datasets. This capability   \n86 underscores AlphaRec\u2019s potential to develop more general recommenders. Third, AlphaRec is user  \n87 friendly, offering a new research paradigm that enhances recommendation by leveraging language  \n88 based user feedback (see Section 4.3). Endowed with its inherent semantic comprehension of   \n89 language representations, AlphaRec can refine recommendations based on user intentions expressed   \n90 in natural language, enabling traditional CF recommenders to evolve into intention-aware systems   \n91 through a straightforward paradigm shift. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/1c25978b2edcddce5bbf03da4f41812bfb80623ec0e5633a7d325a3360d73ffe.jpg", "img_caption": ["(b) Performance comparison (c) The t-SNE representations of movies and user intention in two spaces. Figure 1: Linearly mapping item titles in language representation space into recommendation space yields superior recommendation performance on Movies & TV [1] dataset. (1a) The framework of linear mapping. (1b) The recommendation performance comparison between leading CF recommenders and linear mapping. (1c) The t-SNE [2] visualizations of movie representations, with colored lines linking identical movies or user intention across language space (left) and linearly projected recommendation space (right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 2 Uncovering Collaborative Signals in LMs via Linear Mapping ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 In this section, we aim to explore whether LMs implicitly encode collaborative signals in their   \n94 representation spaces. We first formulate the personalized item recommendation task, then detail the   \n95 linear mapping and its empirical findings. Empirical evidence indicates a homomorphic relationship   \n96 between the representation spaces of advanced LMs and effective recommendation spaces.   \n97 Task formulation. Personalized item recommendation with implicit feedback aims to select items   \n98 $i\\in\\mathcal{Z}$ that best match user $u$ \u2019s preferences based on binary interaction data $\\mathbf{Y}=[y_{u i}]$ , where $y_{u i}=1$   \n99 $y_{u i}=0)$ ) indicates user $u\\in\\mathcal{U}$ has (has not) interacted with item $i$ [58]. The primary objective of   \n100 recommendation is to model the user-item interaction matrix $\\mathbf{Y}$ using a scoring function $\\hat{y}:\\mathcal{U}\\times\\mathcal{T}\\xrightarrow{}$   \n101 $\\mathbb{R}$ , where $\\hat{y}_{u i}$ measures $u$ \u2019s preference for $i$ . The scoring function $\\hat{y}_{u i}=s\\circ\\phi_{\\theta}\\big(\\mathbf{x}_{u},\\mathbf{x}_{i}\\big)$ comprises   \n102 three key components: pre-existing features $\\mathbf{x}_{u}$ and $\\mathbf{x}_{i}$ for user $u$ and item $i$ , a representation learning   \n103 module $\\bar{\\phi}_{\\theta}(\\cdot,\\bar{\\cdot})$ parametrized by $\\theta$ , and a similarity function $s(\\cdot,\\cdot)$ . The representation learning   \n104 module $\\phi_{\\theta}$ transfers $u$ and $i$ into representations $\\mathbf{e}_{u}$ and $\\mathbf{e}_{i}$ for similarity matching $s(\\mathbf{e}_{u},\\mathbf{e}_{i})$ , and the   \n105 Top- $K$ highest scoring items are recommended to $u$ .   \n106 Different recommenders employ various pre-existing features $\\mathbf{x}_{u},\\mathbf{x}_{i}$ and representation learning   \n107 architecture $\\phi_{\\theta}(\\cdot,\\cdot)$ . Traditional ID-based recommenders use one-hot vectors as pre-existing features   \n108 $\\mathbf{x}_{u},\\mathbf{x}_{i}$ . The choice of ID-based representation learning architecture $\\phi_{\\theta}$ can vary widely, including   \n109 ID-based embedding matrix [54], multilayer perception [61], graph neural network [52, 62], and   \n110 variational autoencoder [63]. The commonly used similarity function is cosine similarity [64, 57]   \n111 $\\begin{array}{r}{s(\\mathbf{e}_{u},\\mathbf{e}_{i})={\\frac{\\mathbf{e}_{u}{\\mathrm{\\tiny~\\top}}\\mathbf{e}_{i}}{\\left\\|\\mathbf{e}_{u}\\right\\|\\cdot\\left\\|\\mathbf{e}_{i}\\right\\|}}}\\end{array}$ , which we adopt in this paper.   \n112 Linear mapping. Building on the extensive knowledge encoded by LMs, we explore utilizing LMs   \n113 as feature extractors, leveraging the language representations of item titles as initial item feature $\\mathbf{x}_{i}$ .   \n114 For initial user feature $\\mathbf{x}_{u}$ , we use the average of the title representations of historically interacted   \n115 items, defined as $\\begin{array}{r}{\\mathbf{x}_{u}\\,=\\,\\frac{1}{|\\mathcal{N}_{u}|}\\sum_{i\\in\\mathcal{N}_{u}}\\mathbf{x}_{i}}\\end{array}$ , where $\\mathcal{N}_{u}$ is the set of items user $u$ has interacted with.   \n116 Detailed procedures for obtaining these language-based features are provided in Appendix B.2.   \n117 We select a trainable linear mapping matrix $W$ as the representation learning module $\\phi_{\\theta}$ , setting   \n118 $\\mathbf{e}_{u}=W\\mathbf{x}_{u}$ and $\\mathbf{e}_{i}=W\\mathbf{x}_{i}$ . To learn the linear mapping $W$ , we adopt the InfoNCE loss [56] as the   \n119 objective function, which has demonstrated state-of-the-art performance in both ID-based [65, 66]   \n120 and LM-enhanced collaborative filtering (CF) recommendations [47] (refer to Equation (4) for the   \n121 formula). The overall framework of the linear mapping process is illustrated in Figure 1a. We directly   \n122 use linearly mapped representations $\\mathbf{e}_{u}$ and $\\mathbf{e}_{i}$ to calculate the user-item similarity $s(\\mathbf{e}_{u},\\mathbf{e}_{i})$ for   \n123 recommendation. High performance on the test set would suggest that collaborative signals (i.e., user   \n124 preference similarities between items) have been implicitly encoded in the language representation   \n125 space [67, 10].   \n126 Empirical findings. We compare the recommendation performance of the linear mapping method   \n127 with three classical CF baselines, matrix factorization (MF) [54, 68], MultVAE [63], and LightGCN   \n128 [55] (see more details about baselines in Appendix C.2.1). We report three widely used metrics Hit   \n129 Ratio $(\\operatorname{HR}\\!\\odot K)$ , Recall $@K$ , Normalized Discounted Cumulative Gain $(\\mathbf{NDCG}@K)$ ) to evaluate   \n130 the effectiveness of linear mapping, with $K$ set by default at 20. We evaluate a wide range of LMs,   \n131 including BERT-style models [4, 5], decoder-only language models [6, 69], and LM-based text   \n132 embedding models [70, 71] (see Appendix B.1 for details about used LMs).   \n133 Table 1 reports the recommendation performance yielded by the linear mapping on three Amazon   \n134 datasets [1], comparing with classic CF baselines. We observe that the performance of most advanced   \n135 text embedding models (e.g., text-embeddings-3-large [70] and SFR-Embedding-Mistral [71]) exceed   \n136 LightGCN on all datasets. We further empirically prove that these improvements do not merely   \n137 come from the better feature encoding ability (refer to Appendix B.3). These findings indicate   \n138 the homomorphic relationship between the language representation space of advanced LMs and an   \n139 effective item representation space for recommendation. Moreover, with the advances in LMs, the   \n140 performance of item representation linearly mapped from LMs exhibits a rising trend, gradually   \n141 surpassing traditional ID-based CF models. Representations from early BERT-style models (e.g.,   \n142 BERT [4] and RoBERTa [5]) only show weaker or equal capabilities compared with MF, while the   \n143 performance of decoder-only LMs (e.g., Llama-7B [6] ) start to match MultVAE and LightGCN. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/aa3b078af6e03013a7353d460d927347dd3c8eaad0f4218b7e0ee40421dcd10f.jpg", "table_caption": ["Table 1: The recommendation performance of linear mapping comparing with classical CF baselines. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "144 3 AlphaRec ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "145 This finding of space homomorphic relationship sheds light on building advanced CF models purely   \n146 based on LM representations without introducing ID-based embeddings. To be specific, we try to   \n147 incorporate only three simple components (i.e., nonlinear projection [61], graph convolution [55]   \n148 and contrastive learning (CL) objectives [56]), to develop a simple yet effective CF model called   \n149 AlphaRec. It is important to highlight that our approach is centered on exploring the potential of   \n150 LM representations for CF by integrating essential components from leading CF models, rather than   \n151 deliberately inventing new CF mechanisms. We present the model structure of AlphaRec in Section   \n152 3.1, and compare AlphaRec with two popular recommendation paradigms in Section 3.2. ", "page_idx": 3}, {"type": "text", "text": "153 3.1 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "154 We present how AlphaRec is designed and trained. Generally, the representation learning architecture   \n155 $\\phi_{\\theta}(\\cdot,\\cdot)$ of AlphaRec is simple, which only contains a two-layer MLP and the basic graph convolution   \n156 operation, with language representations as the input features $\\mathbf{x}_{u},\\mathbf{x}_{i}$ . The cosine similarity is used as   \n157 the similarity function $s(\\cdot,\\cdot)$ , and the contrastive loss InfoNCE [56, 57] is adopted for optimization.   \n158 For simplicity, we consistently adopt text-embeddings-3-large [70] as the language representation   \n159 model, for its excellent language understanding and representation capabilities.   \n160 Nonlinear projection. In AlphaRec, we substitute the linear mapping matrix delineated in Section 2   \n161 with a nonlinear MLP. This conversion from linear to nonlinear is non-trivial, for the paradigm shift   \n162 from ID-based embeddings to LM representations, since nonlinear transformation helps in excavating   \n163 more comprehensive collaborative signals from the LM representation space with rich semantics (see   \n164 discussions about this in Appendix C.2.3) [61]. Specifically, we project the language representation   \n165 $\\mathbf{x}_{i}$ of the item title to an item space for recommendation with the two-layer MLP, and obtain user   \n166 representations as the average of historical items: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{i}^{(0)}=W_{2}\\operatorname{LeakyReLU}\\left(W_{1}\\mathbf{x}_{i}+b_{1}\\right)+b_{2},\\quad\\mathbf{e}_{u}^{(0)}=\\frac{1}{|\\mathcal{N}_{u}|}\\sum_{i\\in\\mathcal{N}_{u}}\\mathbf{e}_{i}^{(0)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 Graph convolution. Graph neural networks (GNNs) have shown superior effectiveness for recom  \n168 mendation [52, 55], owing to the natural user-item graph structure in recommender systems [72].   \n169 In AlphaRec, we employ a minimal graph convolution operation [55] to capture more complicated   \n170 collaborative signals from high-order connectivity [55, 73, 74, 72] as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{u}^{(k+1)}=\\sum_{i\\in{\\mathcal{N}}_{u}}\\frac{1}{\\sqrt{|{\\mathcal{N}}_{u}|}\\sqrt{|{\\mathcal{N}}_{i}|}}\\mathbf{e}_{i}^{(k)},\\quad\\mathbf{e}_{i}^{(k+1)}=\\sum_{u\\in{\\mathcal{N}}_{i}}\\frac{1}{\\sqrt{|{\\mathcal{N}}_{i}|}\\sqrt{|{\\mathcal{N}}_{u}|}}\\mathbf{e}_{u}^{(k)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 The information of connected neighbors is aggregated with a symmetric normalization term   \n172 $\\frac{1}{\\sqrt{|\\mathcal{N}_{u}|}\\sqrt{|\\mathcal{N}_{i}|}}$ . Here $\\mathcal{N}_{u}\\ (\\mathcal{N}_{i})$ denotes the historical item (user) set that user $u$ (item $i$ ) has inter  \n173 acted with. The features ${\\bf e}_{u}^{(0)}$ and ${\\bf e}_{i}^{(0)}$ projected from the MLP are used as the input of the first layer.   \n174 After propagating for $K$ layers, the final representation of a user (item) is obtained as the average of   \n175 features from each layer: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{u}=\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbf{e}_{u}^{(k)},\\quad\\mathbf{e}_{i}=\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbf{e}_{i}^{(k)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 Contrastive learning objective. The introduction of contrasting learning is another key element for   \n177 the success of leading CF models. Recent research suggests that the contrast learning objective, rather   \n178 than data augmentation, plays a more significant role in improving recommendation performance   \n179 [66, 75, 65]. Therefore, we simply use the contrast learning object InfoNCE [56] as the loss function   \n180 without any additional data augmentation on the graph [76, 57]. With cosine similarity as the   \n181 similarity function $\\begin{array}{r}{s(\\mathbf{e}_{u},\\mathbf{e}_{i})={\\frac{\\mathbf{e}_{u}{\\mathrm{\\tiny~\\top}}\\mathbf{e}_{i}}{\\left\\|\\mathbf{e}_{u}\\right\\|\\cdot\\left\\|\\mathbf{e}_{i}\\right\\|}}}\\end{array}$ , the InfoNCE loss [56, 76, 77] is written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\sum_{(u,i)\\in\\mathcal{O}^{+}}\\log\\frac{\\exp\\left(s(u,i)/\\tau\\right)}{\\exp\\left(s(u,i)/\\tau\\right)+\\sum_{j\\in\\mathcal{S}_{u}}\\exp\\left(s(u,j)/\\tau\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 Here, $\\tau$ is a hyperparameter called temperature [78], $\\mathcal{O}^{+}=\\{(u,i)|y_{u i}=1\\}$ denoting the observed   \n183 interactions between users $\\boldsymbol{\\mathcal{U}}$ and items $\\mathcal{T}$ . And $\\textstyle S_{u}$ is a randomly sampled subset of negative items   \n184 that user $u$ does not adopt. ", "page_idx": 4}, {"type": "text", "text": "185 3.2 Discussion of Recommendation Paradigms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "186 We compare the language-representation-based AlphaRec with two popular recommendation   \n187 paradigms in Table 2 (see more discussion about related works in Appendix A).   \n188 ID-based recommendation (ID-Rec) [52, 54]. In the traditional ID-based recommendation paradigm,   \n189 users and items are represented by ID-based learnable embeddings derived from a large number of   \n190 user interactions. While ID-Rec exhibits excellent recommendation capabilities with low training and   \n191 inference costs [62, 76], it also has two significant drawbacks. Firstly, these ID-based embeddings   \n192 learned in specific domains are difficult to transfer to new domains without overlapping users   \n193 and items [37], thereby hindering zero-shot recommendation capabilities. Additionally, there is a   \n194 substantial gap between ID-Rec and natural languages [34], which makes ID-based recommenders   \n195 hard to incorporate language-based user intentions and further refine recommendations accordingly.   \n196 LM-based recommendation (LM-Rec) [15, 16, 24]. Beneftiting from the extensive world knowledge   \n197 and powerful reasoning capabilities of LMs [7, 79], the LM-based recommendation paradigm has   \n198 gained widespread attention [11, 13]. LM-Rec tends to convert user interaction history into text   \n199 prompts as input for LMs, utilizing pre-trained or fine-tuned LMs in a text generation pattern to   \n200 recommend items. LM-Rec demonstrates zero-shot and few-shot abilities and can easily understand   \n201 language-based user intentions. However, LM-Rec faces significant challenges. Firstly, the LM-based   \n202 model architecture leads to huge training and inference costs, with real-world deployment difficulties.   \n203 Additionally, limited by the text generation paradigm, LM-based models tend to perform candidate   \n204 selection [17] or generate a single next item [24]. It remains difficult for LM-Rec to comprehensively   \n205 rank the entire item corpus or recommend multiple items that align with user interests.   \n206 Language-representation-based recommendation. We argue that AlphaRec follows a new CF   \n207 paradigm, which we term the language-representation-based paradigm. This paradigm replaces   \n208 the ID-based embeddings in ID-Rec with representations from pre-trained LMs, employing feature   \n209 encoders to map LM representations directly into the recommendation space. Few early studies lie in   \n210 this paradigm, including using BERT-style LMs to learn universal sequence representations [37, 44],   \n211 or adopting the same model architecture as ID-Rec with simple input features replacement [34, 35].   \n212 These early explorations, which are mostly based on BERT-style LMs, are usually only applicable in   \n213 certain specific scenarios, such as the transductive setting with the help of ID-based embeddings [37].   \n214 This phenomenon is consistent with our previous findings in Section 2, indicating that BERT-style   \n215 LMs may fail to effectively encode collaborative signals. We point out that AlphaRec is the first   \n216 recommender in the language-representation-based paradigm to surpass the traditional ID-based   \n217 paradigm on multiple tasks, faithfully demonstrating the effectiveness and potential of this paradigm. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/6b76d18a927bc0a6d33707234252dd888366d8ecc10de24106cf7e2e0941172f.jpg", "table_caption": ["Table 2: Comparison of recommendation paradigms "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "218 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "219 In this section, we aim to explore the effectiveness of AlphaRec. Specifically, we are trying to answer   \n220 the following research questions: ", "page_idx": 5}, {"type": "text", "text": "221 \u2022 RQ1: How does AlphaRec perform compared with leading ID-based CF methods? ", "page_idx": 5}, {"type": "text", "text": "222 \u2022 RQ2: Can AlphaRec learn general item representations, and achieve good zero-shot recommenda  \n223 tion performance on entirely new datasets?   \n224 \u2022 RQ3: Can AlphaRec capture user intention described in natural language and adjust the recom  \n225 mendation results accordingly? ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "226 4.1 General Recommendation Performance (RQ1) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "227 Motivation. We aim to explore whether the language-representation-based recommendation paradigm   \n228 can outperform the ID-Rec paradigm. An excellent performance of AlphaRec would shed light on   \n229 the research line of building representation-based recommenders in the future.   \n230 Baselines. We only consider ID-based baselines in this section. We ignore LM-based methods due to   \n231 two practical difficulties: the huge inference cost on datasets with millions of interactions and the   \n232 task limitation of candidate selection or next item prediction. In addition to classic baselines (i.e., MF,   \n233 MultVAE, and LightGCN) introduced in section 2, we consider two categories of leading ID-based   \n234 CF baselines: CL-based CF methods: SGL [80], BC Loss [76], XSimGCL [66] and LM-enhanced   \n235 methods: KAR [48], RLMRec [47]. See more details about baselines in Appendix C.2.1.   \n236 Results. Table 3 presents the performance of AlphaRec compared with leading CF baselines. The   \n237 best-performing methods are bold, while the second-best methods are underlined. Figure 2a and   \n238 Figure 2b report the training efficiency and ablation results. We observe that:   \n239 \u2022 AlphaRec consistently outperforms leading CF baselines by a large margin across all metrics   \n240 on all datasets. AlphaRec shows an improvement ranging from $6.79\\%$ to $9.75\\%$ on Recall $@20$   \n241 compared to the best baseline RLMRec [47]. We further conduct the ablation study to explore the   \n242 reason for its success (see more ablation results in Appendix C.2.2). As shown in Figure 2b, each   \n243 component in AlphaRec contributes positively. Specifically, the performance degradation caused by   \n244 replacing the MLP with a linear weight matrix (w/o MLP) indicates that nonlinear transformations   \n245 can further extract the implicit collaborative signals encoded in the LM representation space.   \n246 Moreover, the performance drop from replacing InfoNCE loss [57] with BPR loss [68] (w/o CL)   \n247 and removing the graph convolution (w/o GCN) suggests that explicitly modeling the collaborative   \n248 relationships through the loss function and model architecture can further enhance recommendation   \n249 performance. These findings suggest that, by carefully designing the model to extract collaborative   \n250 signals, the language-representation-based paradigm can surpass the ID-Rec paradigm.   \n251 \u2022 The incorporation of semantic LM representations into traditional ID-based CF methods can   \n252 lead to significant performance improvements. We note that two LM-enhanced CF methods,   \n253 KAR and RLMRec, both show improvements over CL-based CF methods. Nevertheless, the com  \n254 bination of ID-based embeddings and LM representations in these methods does not yield higher   \n255 results than purely language-representation-based AlphaRec. We attribute this phenomenon to the   \n256 fact that the performance contribution of these methods mainly comes from the LM representations,   \n257 which is consistent with the previous findings [34, 44].   \n258 \u2022 AlphaRec exhibits fast convergence speed. We find that the convergence speed of AlphaRec is   \n259 comparable with, or even surpasses, CL-based methods with data augmentation (e.g., SGL [80]   \n260 and XSimGCL [66]). Meanwhile, methods based solely on graph convolution (LightGCN [55]) or   \n261 CL objective (BC Loss [76]) show relatively slow convergence speed, indicating that introducing   \n262 these modules may not lead to convergence speed improvement. Therefore, we attribute the fast   \n263 convergence speed of AlphaRec to the homomorphic relationship between the LM representation   \n264 space and a good recommendation space, so only minor adjustments to the LM representations are   \n265 needed for recommendation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/20f60c5f0bdffc3ee2c17c7cc6ad55f04fd01c879c54ca137a4790692d2ae372.jpg", "img_caption": ["Figure 2: (2a) The bar charts show the number of epochs needed for each model to converge. AlphaRec tends to exhibit an extremely fast convergence speed. (2b) The effect of each component in AlphaRec on Books dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "266 4.2 Zero-shot Recommendation Performance on Entirely New Datasets (RQ2) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "267 Motivation. We aim to explore whether AlphaRec has learned general item representations [37],   \n268 which enables it to perform well on entirely new datasets without any user and item overlap.   \n269 Task and datasets. In zero-shot recommendation [38], there is not any item or user overlap between   \n270 the training set and test set [38, 33], which is different from the research line of cross-domain   \n271 recommendation in ID-Rec [81]. We jointly train AlphaRec on three source datasets (i.e., Books,   \n272 Movies & TV, and Video Games), while testing it on three completely new target datasets (i.e.,   \n273 Movielens-1M [59], Book Crossing [60], and Industrial [1]) without further training on these new   \n274 datasets. (see more details about how we train AlphaRec on multiple datasets in Appendix C.3.1).   \n275 Baselines. Due to the lack of zero-shot recommenders in the field of general recommendation, we   \n276 slightly modify two zero-shot methods in the sequential recommendation [82], ZESRec [37] and   \n277 UniSRec [37], as baselines. We also incorporate two strategy-based CF methods, Random and Pop   \n278 (see more details about these baselines in Appendix C.3.2).   \n279 Results. Table 4 presents the zero-shot recommendation performance comparison on entirely new   \n280 datasets. The best-performing methods are bold and starred, while the second-best methods are   \n281 underlined. We observe that:   \n282 \u2022 AlphaRec demonstrates strong zero-shot recommendation capabilities, comparable to or even   \n283 surpassing the fully trained LightGCN. On datasets from completely different platforms (e.g.,   \n284 MovieLens-1M and Book Crossing), AlphaRec is comparable with the fully trained LightGCN.   \n285 On the same Amazon platform dataset, Industrial, AlphaRec even surpasses LightGCN, which we   \n286 attribute to the possibility that AlphaRec implicitly learns unique user behavioral patterns on the   \n287 Amazon platform [1]. Conversely, ZESRec and UniSRec exhibit a marked performance decrement   \n288 compared with AlphaRec. We attribute this phenomenon to two aspects. On the one hand, BERT  \n289 style LMs [4, 5] used in these works may not have effectively encoded collaborative signals, which   \n290 is consistent with our findings in Section 2. On the other hand, components designed for the   \n291 next item prediction task in sequential recommendation [83] may not be suitable for capturing the   \n292 general preferences of users in CF scenarios.   \n293 \u2022 The zero-shot recommendation capability of AlphaRec generally benefits from an increased   \n294 amount of training data, without harming the performance on source datasets. As illustrated   \n295 in Figure 8, the zero-shot performance of AlphaRec, when trained on a mixed dataset, is generally   \n296 superior to training on one single dataset [37]. Additionally, we also note that training data with   \n297 themes similar to the target domain contributes more to the zero-shot performance. For instance, the   \n298 zero-shot capability on MovieLens-1M may primarily stem from Movies & TV. Furthermore, we   \n299 discover that AlphaRec, when trained jointly on multiple datasets, hardly experiences a performance   \n300 decline on each source dataset. These findings further point to the general recommendation   \n301 capability of a single pre-trained AlphaRec across multiple datasets. The above findings also offer   \n302 a potential research path to achieve general recommendation capabilities, by incorporating more   \n303 training data with more themes. See more details about these results in Appendix C.3.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/f4b989eaf37dd0dda09c5ad02ab6fb3c1596c3c644048b24cf3c9ce0eb6aa569.jpg", "table_caption": ["Table 4: The zero-shot recommendation performance comparison on entirely new datasets. The improvement achieved by AlphaRec is significant ( $\\boldsymbol{p}$ -value $<<0.05$ ). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "304 4.3 User Intention Capture Performance (RQ3) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "305 Motivation. We aim to investigate whether a straightforward paradigm shift enables pre-trained   \n306 AlphaRec to perceive text-based user intentions and refine recommendations.   \n307 Task and datasets. We test the user intention capture ability of AlphaRec on MovieLens-1M and   \n308 Video Games. In the test set, only one target item remains for each user [84], with one intention   \n309 query generated by ChatGPT [85, 40] (see the details about how to generate and check these intention   \n310 queries in Appendix C.4.1). In the training stage, we follow the same procedure as illustrated in   \n311 Section 2 to train AlphaRec. In the inference stage, we obtain the LM representation ${\\bf e}_{u}^{I}$ ntention   \n312 for each user intention query and combine it with the original user representation to get a new user   \n313 representation as $\\tilde{\\mathbf{e}}_{u}^{(0)}=\\overline{{(1-\\alpha)\\mathbf{e}_{u}^{(0)}+\\alpha\\mathbf{e}_{u}^{I n t e n t i o n}}}$ \u03b1eIuntention[84]. This new user representation is sent into the   \n314 freezed AlphaRec for recommendation. We report a relatively small $K=5$ for all metrics to better   \n315 reflect the intention capture accuracy.   \n316 User intention capture results. Table 5 represents the user intention capture experiment results,   \n317 compared with the baseline TEM [86]. Clearly, the introduction of user intention (w Intention)   \n318 significantly refines the recommendations of the pre-trained AlphaRec (w/o Intention). Moreover,   \n319 AlphaRec outperforms the baseline model TEM by a large margin, even without additional training   \n320 on search tasks. We further conduct a case study on MovieLens-1M to demonstrate how AlphaRec   \n321 captures the user (see more case study results in Appendix C.4.3). As shown in Figure 3a, AlphaRec   \n322 accurately captures the hidden user intention for \u201cGodfather\u201d, while keeping most of the recommen  \n323 dation results unchanged. This indicates that AlphaRec captures the user intention and historical   \n324 interests simultaneously.   \n325 Effect of the intention strength $\\alpha$ . By controlling the value of $\\alpha$ , AlphaRec can provide better   \n326 recommendation results, with a balance between user historical interests and user intent capture.   \n327 Figure 3b depicts the effect of $\\alpha$ . Initially, as $\\alpha$ increases, the recommendation performance rises   \n328 accordingly, indicating that incorporating user intention enables AlphaRec to provide better rec  \n329 ommendation results. However, as the $\\alpha$ approaches 1, the recommendation performance starts to   \n330 decrease, which suggests that the user historical interests learned by AlphaRec also play a vital role.   \n331 The similar effect of $\\alpha$ on Video Games is discussed in Appendix C.4.4. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/b21434364dfc375593b947f800d9de0ed7eb5cf0d833e6be3f24fb7703050bec.jpg", "img_caption": ["Figure 3: User intention capture experiments on MovieLens-1M. (3a) AlphaRec refines the recommendations according to language-based user intention. (3b) The effect of user intention strength $\\alpha$ . ", "(a) Case study of user intention capture "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "332 5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "333 There are several limitations not addressed in this paper. On the one hand, although we have demon  \n334 strated the excellence of AlphaRec for multiple tasks on various offilne datasets, the effectiveness of   \n335 online employment remains unclear. On the other hand, although we have successfully explored the   \n336 potential of language-representation-based recommenders by incorporating essential components in   \n337 leading CF models, we do not elaboratively focus on designing new components for CF models. ", "page_idx": 8}, {"type": "text", "text": "338 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "339 In this paper, we explored what knowledge about recommendations has been encoded in the LM   \n340 representation space. Specifically, we found that the advanced LMs representation space exhibits   \n341 a homomorphic relationship with an effective recommendation space. Based on this finding, we   \n342 developed a simple yet effective CF model called AlphaRec, which exhibits good recommendation   \n343 performance with zero-shot recommendation and user intent capture ability. We pointed out that   \n344 AlphaRec follows a new recommendation paradigm, language-representation-based recommendation,   \n345 which uses language representations from LMs to represent users and items and completely abandons   \n346 ID-based embeddings. We believed that AlphaRec is an important stepping stone towards building   \n347 general recommenders in the future.1 ", "page_idx": 8}, {"type": "text", "text": "348 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "349 [1] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. Justifying recommendations using distantly  \n350 labeled reviews and fine-grained aspects. In EMNLP, 2019.   \n351 [2] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine   \n352 learning research, 9(11), 2008.   \n353 [3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,   \n354 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n355 [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of   \n356 deep bidirectional transformers for language understanding. In ACL, 2019.   \n357 [5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,   \n358 Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT   \n359 pretraining approach. CoRR, abs/1907.11692, 2019.   \n360 [6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n361 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas   \n362 Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,   \n363 Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony   \n364 Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian   \n365 Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut   \n366 Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mi  \n367 haylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi   \n368 Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,   \n369 Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,   \n370 Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,   \n371 Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open   \n372 foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.   \n373 [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla   \n374 Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar  \n375 wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,   \n376 Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,   \n377 Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan  \n378 dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.   \n379 In NeurIPS, 2020.   \n380 [8] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda B. Vi\u00e9gas, Hanspeter Pfister, and   \n381 Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a   \n382 synthetic task. In ICLR, 2023.   \n383 [9] Ivan Vulic, Edoardo Maria Ponti, Robert Litschko, Goran Glavas, and Anna Korhonen. Probing   \n384 pretrained language models for lexical semantics. In EMNLP, 2020.   \n385 [10] Wes Gurnee and Max Tegmark. Language models represent space and time. CoRR,   \n386 abs/2310.02207, 2023.   \n387 [11] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang,   \n388 and Qing Li. Recommender systems in the era of large language models (llms). CoRR,   \n389 abs/2307.02046, 2023.   \n390 [12] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. Large language models for generative   \n391 recommendation: A survey and visionary discussions. CoRR, abs/2309.01157, 2023.   \n392 [13] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin,   \n393 Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. A survey on large language   \n394 models for recommendation. CoRR, abs/2305.19860, 2023.   \n395 [14] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu,   \n396 Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. How can recommender systems   \n397 benefit from large language models: A survey. CoRR, abs/2306.05817, 2023.   \n398 [15] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Rec  \n399 ommendation as instruction following: A large language model empowered recommendation   \n400 approach. CoRR, abs/2305.07001, 2023.   \n401 [16] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: An   \n402 effective and efficient tuning framework to align large language model with recommendation.   \n403 In RecSys, 2023.   \n404 [17] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, and Xiang Wang. Llara:   \n405 Aligning large language models with sequential recommenders. CoRR, abs/2312.02445, 2023.   \n406 [18] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language   \n407 model for recommender systems. CoRR, abs/2311.01343, 2023.   \n408 [19] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and   \n409 Ji-Rong Wen. Adapting large language models by integrating collaborative semantics for   \n410 recommendation. CoRR, abs/2311.09049, 2023.   \n411 [20] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. Collm:   \n412 Integrating collaborative embeddings into large language models for recommendation. CoRR,   \n413 abs/2310.19488, 2023.   \n414 [21] Arpita Vats, Vinija Jain, Rahul Raja, and Aman Chadha. Exploring the impact of large   \n415 language models on recommender systems: An extensive review. CoRR, abs/2402.18590,   \n416 2024.   \n417 [22] Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, and Julian J. McAuley. Founda  \n418 tion models for recommender systems: A survey and new perspectives. CoRR, abs/2402.11143,   \n419 2024.   \n420 [23] Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao, and   \n421 Ji-Rong Wen. Prompting large language models for recommender systems: A comprehensive   \n422 framework and empirical analysis. CoRR, abs/2401.04997, 2024.   \n423 [24] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation   \n424 as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm   \n425 (P5). In RecSys, 2022.   \n426 [25] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative   \n427 pretrained language models are open-ended recommender systems. CoRR, abs/2205.08084,   \n428 2022.   \n429 [26] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming   \n430 Tang, Yong Yu, and Weinan Zhang. Rella: Retrieval-enhanced large language models for   \n431 lifelong sequential behavior comprehension in recommendation. CoRR, abs/2308.11131, 2023.   \n432 [27] Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang   \n433 Wang, and Xiangnan He. Large language model can interpret latent space of sequential   \n434 recommender. CoRR, abs/2310.20487, 2023.   \n435 [28] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu,   \n436 Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi,   \n437 and Mahesh Sathiamoorthy. Recommender systems with generative retrieval. In Alice Oh,   \n438 Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,   \n439 NeurIPS, 2023.   \n440 [29] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. ONCE: boosting content-based   \n441 recommendation with both open- and closed-source large language models. In Luz Angelica   \n442 Caudillo-Mata, Silvio Lattanzi, Andr\u00e9s Mu\u00f1oz Medina, Leman Akoglu, Aristides Gionis, and   \n443 Sergei Vassilvitskii, editors, WSDM, 2024.   \n444 [30] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong,   \n445 Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. Actions speak louder than words: Trillion  \n446 parameter sequential transducers for generative recommendations. CoRR, abs/2402.17152,   \n447 2024.   \n448 [31] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. How to index item ids for   \n449 recommendation foundation models. In Qingyao Ai, Yiqin Liu, Alistair Moffat, Xuanjing   \n450 Huang, Tetsuya Sakai, and Justin Zobel, editors, SIGIR-AP, 2023.   \n451 [32] Guanghu Yuan, Fajie Yuan, Yudong Li, Beibei Kong, Shujie Li, Lei Chen, Min Yang, Chenyun   \n452 Yu, Bo Hu, Zang Li, Yu Xu, and Xiaohu Qie. Tenrec: A large-scale multipurpose benchmark   \n453 dataset for recommender systems. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle   \n454 Belgrave, K. Cho, and A. Oh, editors, NeurIPS, 2022.   \n455 [33] Jiaqi Zhang, Yu Cheng, Yongxin Ni, Yunzhu Pan, Zheng Yuan, Junchen Fu, Youhua Li,   \n456 Jie Wang, and Fajie Yuan. Ninerec: A benchmark dataset suite for evaluating transferable   \n457 recommendation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n458 [34] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin   \n459 Ni. Where to go next for recommender systems? ID- vs. modality-based recommender models   \n460 revisited. In SIGIR, 2023.   \n461 [35] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. Exploring the   \n462 upper limits of text-based collaborative flitering using large language models: Discoveries and   \n463 insights. CoRR, abs/2305.11700, 2023.   \n464 [36] Youhua Li, Hanwen Du, Yongxin Ni, Pengpeng Zhao, Qi Guo, Fajie Yuan, and Xiaofang Zhou.   \n465 Multi-modality is all you need for transferable recommender systems. CoRR, abs/2312.09602,   \n466 2023.   \n467 [37] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen.   \n468 Towards universal sequence representation learning for recommender systems. In KDD, pages   \n469 585\u2013593. ACM, 2022.   \n470 [38] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. Zero-shot recommender   \n471 systems. CoRR, abs/2105.08318, 2021.   \n472 [39] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian J. McAuley.   \n473 Text is all you need: Learning language representations for sequential recommendation. In   \n474 KDD, 2023.   \n475 [40] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian J. McAuley. Bridging   \n476 language and items for retrieval and recommendation. CoRR, abs/2403.03952, 2024.   \n477 [41] Yupeng Hou, Zhankui He, Julian J. McAuley, and Wayne Xin Zhao. Learning vector-quantized   \n478 item representation for transferable sequential recommenders. In Ying Ding, Jie Tang, Juan F.   \n479 Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben, editors, WWW, 2023.   \n480 [42] Zhiming Mao, Huimin Wang, Yiming Du, and Kam-Fai Wong. Unitrec: A unified text-to-text   \n481 transformer and joint contrastive learning framework for text-based recommendation. In ACL,   \n482 2023.   \n483 [43] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. U-BERT: pre-training user representations   \n484 for improved recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence,   \n485 AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI   \n486 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,   \n487 Virtual Event, February 2-9, 2021. AAAI, 2021.   \n488 [44] Lingzi Zhang, Xin Zhou, Zhiwei Zeng, and Zhiqi Shen. Are ID embeddings necessary?   \n489 whitening pre-trained text embeddings for effective sequential recommendation. CoRR,   \n490 abs/2402.10602, 2024.   \n491 [45] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. Parameter-efficient   \n492 transfer from sequential behaviors for user modeling and recommendation. In SIGIR, 2020.   \n493 [46] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang,   \n494 Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for   \n495 recommendation. In Luz Angelica Caudillo-Mata, Silvio Lattanzi, Andr\u00e9s Mu\u00f1oz Medina,   \n496 Leman Akoglu, Aristides Gionis, and Sergei Vassilvitskii, editors, WSDM, 2024.   \n497 [47] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and   \n498 Chao Huang. Representation learning with large language models for recommendation. CoRR,   \n499 abs/2310.15950, 2024.   \n500 [48] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang,   \n501 Rui Zhang, and Yong Yu. Towards open-world recommendation with knowledge augmentation   \n502 from large language models. CoRR, abs/2306.10933, 2023.   \n503 [49] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou,   \n504 and Linjian Mo. Breaking the length barrier: Llm-enhanced CTR prediction in long textual   \n505 user behaviors. CoRR, abs/2403.19347, 2024.   \n506 [50] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua.   \n507 On generative agents in recommendation. CoRR, abs/2310.10108, 2023.   \n508 [51] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin Zhao,   \n509 Leyu Lin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language   \n510 agents for recommender systems. CoRR, abs/2310.09233, 2023.   \n511 [52] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph   \n512 collaborative filtering. In SIGIR, 2019.   \n513 [53] Yang Li, Tong Chen, Yadan Luo, Hongzhi Yin, and Zi Huang. Discovering collaborative   \n514 signals for next POI recommendation with iterative seq2graph augmentation. In IJCAI, 2021.   \n515 [54] Yehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for   \n516 recommender systems. Computer, 42(8):30\u201337, 2009.   \n517 [55] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. Lightgcn:   \n518 Simplifying and powering graph convolution network for recommendation. In SIGIR, 2021.   \n519 [56] A\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive   \n520 predictive coding. CoRR, abs/1807.03748, 2018.   \n521 [57] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu, and Xi  \n522 angnan He. On the effectiveness of sampled softmax loss for item recommendation. CoRR,   \n523 abs/2201.02327, 2022.   \n524 [58] Steffen Rendle. Item recommendation from implicit feedback. In Recommender Systems   \n525 Handbook. Springer US, 2022.   \n526 [59] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM   \n527 Trans. Interact. Intell. Syst., 5(4):19:1\u201319:19, 2016.   \n528 [60] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta  \n529 learned user preference estimator for cold-start recommendation. In KDD, 2019.   \n530 [61] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural   \n531 collaborative filtering. In WWW, 2017.   \n532 [62] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. Lightgcl: Simple yet effective graph   \n533 contrastive learning for recommendation. In ICLR, 2023.   \n534 [63] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. Variational   \n535 autoencoders for collaborative filtering. In WWW, 2018.   \n536 [64] Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, and Xiangnan He. Adap- $\\tau$ :   \n537 Adaptively modulating embedding magnitude for recommendation. In WWW, 2023.   \n538 [65] An Zhang, Leheng Sheng, Zhibo Cai, Xiang Wang, and Tat-Seng Chua. Empowering collabo  \n539 rative filtering with principled adversarial contrastive loss. In NeurIPS, 2023.   \n540 [66] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and Hongzhi Yin.   \n541 Xsimgcl: Towards extremely simple graph contrastive learning for recommendation. IEEE   \n542 Trans. Knowl. Data Eng., 36(2):913\u2013926, 2024.   \n543 [67] Abhilasha Ravichander, Yonatan Belinkov, and Eduard H. Hovy. Probing the probing paradigm:   \n544 Does probing accuracy entail task relevance? In EACL, 2021.   \n545 [68] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. BPR:   \n546 bayesian personalized ranking from implicit feedback. CoRR, abs/1205.2618, 2012.   \n547 [69] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh   \n548 Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu  \n549 cile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,   \n550 Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. CoRR,   \n551 abs/2310.06825, 2023.   \n552 [70] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,   \n553 Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav   \n554 Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr,   \n555 Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov,   \n556 Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive   \n557 pre-training. CoRR, abs/2201.10005, 2022.   \n558 [71] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr  \n559 embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog,   \n560 2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/.   \n561 [72] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in   \n562 recommender systems: A survey. ACM Comput. Surv., 55(5):97:1\u201397:37, 2023.   \n563 [73] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q.   \n564 Weinberger. Simplifying graph convolutional networks. In ICML, 2019.   \n565 [74] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. Revisiting graph based   \n566 collaborative flitering: A linear residual graph convolutional network approach. In AAAI, 2020.   \n567 [75] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen.   \n568 Are graph augmentations necessary?: Simple graph contrastive learning for recommendation.   \n569 In SIGIR, 2022.   \n570 [76] An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. Incorporating bias-aware margins   \n571 into contrastive loss for collaborative filtering. In NeurIPS, 2022.   \n572 [77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini   \n573 Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and   \n574 Ilya Sutskever. Learning transferable visual models from natural language supervision. In   \n575 ICML, 2021.   \n576 [78] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR,   \n577 2021.   \n578 [79] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian   \n579 Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng   \n580 Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie,   \n581 and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023.   \n582 [80] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie.   \n583 Self-supervised graph learning for recommendation. In SIGIR, 2021.   \n584 [81] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. Cross-domain   \n585 recommendation: Challenges, progress, and prospects. In IJCAI, 2021.   \n586 [82] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and Mehmet A. Orgun.   \n587 Sequential recommender systems: Challenges, progress and prospects. In IJCAI, 2019.   \n588 [83] Wang-Cheng Kang and Julian J. McAuley. Self-attentive sequential recommendation. In   \n589 ICDM, 2018.   \n590 [84] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W. Bruce Croft. Learning a hierarchi  \n591 cal embedding model for personalized product search. In SIGIR, 2017.   \n592 [85] OpenAI. GPT-4 technical report. CoRR, 2023.   \n593 [86] Keping Bi, Qingyao Ai, and W. Bruce Croft. A transformer-based embedding model for   \n594 personalized product search. In SIGIR, 2020.   \n595 [87] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier   \n596 probes. In ICLR (Workshop), 2017.   \n597 [88] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In   \n598 ICLR, 2022.   \n599 [89] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the   \n600 geometry of large language models. CoRR, abs/2311.03658, 2023.   \n601 [90] Xubin Ren, Wei Wei, Lianghao Xia, and Chao Huang. A comprehensive survey on self  \n602 supervised learning for recommendation. arXiv preprint arXiv:2404.03354, 2024.   \n603 [91] Zheng Chen. PALR: personalization aware llms for recommendation. CoRR, abs/2305.07622,   \n604 2023.   \n605 [92] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou,   \n606 Liang Pang, and Xiao Wang. Can small language models be good reasoners for sequential   \n607 recommendation? CoRR, abs/2403.04260, 2024.   \n608 [93] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing   \n609 Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n610 [94] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, and   \n611 Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems. In   \n612 ECIR, 2024.   \n613 [95] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt a good recommender?   \n614 A preliminary study. CoRR, abs/2304.10149, 2023.   \n615 [96] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun,   \n616 Xiao Zhang, and Jun Xu. Uncovering chatgpt\u2019s capabilities in recommender systems. In   \n617 RecSys, 2023.   \n618 [97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chat  \n619 rec: Towards interactive and explainable llms-augmented recommender system. CoRR,   \n620 abs/2303.14524, 2023.   \n621 [98] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. Ctrl: Connect tabular and language   \n622 model for ctr prediction. arXiv preprint arXiv:2306.02841, 2023.   \n623 [99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n624 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez,   \n625 Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation   \n626 language models. CoRR, abs/2302.13971, 2023.   \n627 [100] Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David   \n628 Bau. Function vectors in large language models. CoRR, abs/2310.15213, 2023.   \n629 [101] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,   \n630 Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language   \n631 models. In NeurIPS, 2022.   \n632 [102] Julian J. McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and   \n633 complementary products. In KDD, 2015.   \n634 [103] Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In KDD,   \n635 2020.   \n637 Representations in LMs. The impressive capabilities demonstrated by LMs across various tasks   \n638 raise a wide concern about what they have learned in the representation space. An important and   \n639 effective approach for interpreting and analyzing representations of LMs is linear probing [67, 87].   \n640 The main idea of linear probing is simple: training linear classifiers to predict some specific attributes   \n641 or concepts (e.g., lexical structure [9] ) from the representations in the hidden layers of LMs. A high   \n642 probing result (e.g., classification accuracy on the out-of-sample test set) tends to imply relevant   \n643 information has been implicitly encoded in the representation space of LMs, although this does   \n644 not imply LMs directly use these representations [67, 10]. Recent studies empirically demonstrate   \n645 that concepts such as color [88], game states [8]. and geographic position are encoded in LMs.   \n646 Furthermore, these concepts may even be linearly encoded in the representation space of LMs [8, 89].   \n647 Collaborative filtering. Collaborative filtering (CF) [90] is an advanced technique in modern   \n648 recommender systems. The prevailing CF methods tend to adopt an ID-based paradigm, where users   \n649 and items are typically represented as one-hot vectors, with an embedding table used for lookup [54].   \n650 Usually, these embedding parameters are learned by optimizing specific loss functions to reconstruct   \n651 the history interaction pattern [68]. Recent advances in CF mainly benefit from two aspects, graph   \n652 convolution [72] and contrastive learning [90]. These CF models exhibit superior recommendation   \n653 performance by conducting the embedding propagation [52, 55] and applying contrastive learning   \n654 objectives [80, 62, 66]. However, although effective, these methods are still limited, due to the   \n655 ID-based paradigm. Since one-hot vectors contain no feature information beyond being identifiers, it   \n656 is challenging to transfer pre-trained ID embeddings to other domains [37] or to leverage leading   \n657 techniques from computer vision (CV) and natural language processing (NLP) [34].   \n658 LMs for recommendation. The remarkable language understanding and reasoning ability shown by   \n659 LMs has attracted extensive attention in the field of recommendation. The application of LMs in rec  \n660 ommendation can be categorized into three main approaches: LM-enhanced recommendation, LM as   \n661 the modality encoder, and LLM-based recommendation. The first research direction, LLM-enhanced   \n662 recommendation, focuses on empowering traditional recommenders with the semantic representations   \n663 from LMs [48, 47, 46, 49, 91, 92]. Specifically, these methods introduce representations from LMs as   \n664 additional features for traditional ID-based recommenders, to capture complicated user preferences.   \n665 The second research line lies in adopting the LM as the text modality encoder, which is also known   \n666 as a kind of modality-based recommendation (MoRec) [34, 35]. These methods tend to train the   \n667 LM as the text modality encoder together with the traditional recommender. In previous studies,   \n668 BERT-style LMs are widely used as the text modality encoder. The third research line, LLM-based   \n669 recommendation, directly uses LLMs as the recommender and recommends items in a text generation   \n670 paradigm. Early attempts focus on adopting in-context learning (ICL) [93] and prompting pre-trained   \n671 LLMs [94\u201397]. However, such naive methods tend to yield poor performance compared to traditional   \n672 models. Therefore, recent studies concentrate on fine-tuning LLMs on recommendation-related cor  \n673 pus [16, 15, 26, 25, 29] and align the LLMs with the representations from traditional recommenders   \n674 as the additional modality [17, 20, 27, 98]. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "675 B Linear Mapping ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "676 B.1 Brief of Used LMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "677 We briefly introduce the LMs we use for linear mapping in Section 2. ", "page_idx": 15}, {"type": "text", "text": "678 \u2022 BERT [4] is an encoder-only language model based on the transformer architecture [3], pre-trained   \n679 on text corpus with unsupervised tasks. BERT adopts bidirectional self-attention heads to learn   \n680 bidirectional representations.   \n\u2022 RoBERTa [5] is an enhanced version of BERT. RoBERTa preserves the architecture of BERT but   \n682 improves it by training with more data and large batches, adopting dynamic masking, and removing   \n683 the next sentence prediction objective.   \n684 \u2022 Llama2-7B [6] is an open-source decoder-only LLM with 7 billion parameters. Llama2 adopts   \n685 grouped-query attention, with longer context length and larger size of the pre-training corpus   \n686 compared with Llama-7B [99].   \n687 \u2022 Mistral-7B [69] is an open-source pre-trained decoder-only LLM with 7 billion parameters. Mistral   \n688 7B leverages grouped-query attention, coupled with sliding window attention for faster and lower   \n689 cost inference.   \n690 \u2022 text-embedding-ada-v2 & text-embeddings-3-large [70] are leading text embedding models   \n691 released by OpenAI. These models are built upon decoder-only GPT models, pre-trained on   \n692 unsupervised data at scale with contrastive learning objectives.   \n693 \u2022 SFR-Embedding-Mistral [71] is a decoder-based text embedding model built upon the open  \nsource LLM Mixtral-7B [69]. SFR-Embedding-Mistral introduces task-homogeneous batching and   \n695 computes contrastive loss on \u201chard negatives\u201d, which brings a better performance than the vanilla   \n696 Mixtral-7B model. ", "page_idx": 15}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/43e83c0aea55651cee181524addd55ec0678233e68000c65d6a87687333b671c.jpg", "table_caption": ["Table 6: Linear mapping performance of randomly shuffled item representations "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/c3f5bc2b5a53801d60f0f87bd4daa3f6148e73e0b2cf1ba53936e127d48dc0f3.jpg", "table_caption": ["Table 7: Dataset statistics. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "697 B.2 Extracting Representations from LMs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "698 We present how to extract representations from LMs. For encoder-based LMs (e.g., BERT [4]   \n699 and RoBERTa [5]), we use the output representation corresponding to the [CLS] token [40]. For   \n700 decoder-based models (e.g., Llama-7B [6, 69], Mistral-7B, and SFR-Embedding-Mistral [71]),   \n701 we use the representation in the last transformer block [3], corresponding to the last input token   \n702 [10, 100, 70]. Especially, for the commercial closed-source model (e.g., text-embedding-ada-v2 and   \n703 text-embeddings-3-large 2 [70]), we directly call the API interface to obtain representations. ", "page_idx": 16}, {"type": "text", "text": "704 B.3 Empirical Findings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "705 We find more evidence about representations in leading LM encode collaborative signals beyond   \n706 better feature encoding ability. We randomly shuffle item representations and conduct the same linear   \n707 mapping experiment. As illustrated in Table 6, randomly shuffled representations, text-embeddings  \n708 3-large (Random), yield similar performance with BERT, lagging largely behind the vanilla linear   \n709 mapping method. These results indicate that BERT may only serve as a good feature encoder, while   \n710 the latest LM may further encode collaborative signals beyond naive feature encoding. ", "page_idx": 16}, {"type": "text", "text": "711 C Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "712 C.1 Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "713 We incorporate six datasets in this paper, including four datasets from the Amazon platform 3 [1]   \n714 (i.e., Books, Movies & TV, Video Games, and Industrial), and two datasets from other platforms (i.e.,   \n715 MovieLens-1M and Book Crossing). Table 7 reports the data statistics of each dataset.   \n716 We divide the history interaction of each user into training, validation, and testing sets with a ratio   \n717 of 4:3:3, and remove users with less than 20 interactions following previous studies [50]. We also   \n718 remove items from the testing and validation sets that do not appear in the training set, to address the   \n719 cold start problem. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Item Title Examples ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Books: Dismissed with Prejudice: A J.P. Beaumont Novel; Die for Love: A Jacqueline Kirby Novel of Suspense; The Cloud; Memories Before and After the Sound of Music: An Autobiography; Harry Potter and the Sorcerer\u2019s Stone; ", "page_idx": 17}, {"type": "text", "text": "Movies & TV: Batman Begins; Fantastic Four; Max Headroom: The Complete Series;   \nMadagascar; Land of the Dead; King Kong; ", "page_idx": 17}, {"type": "text", "text": "Video Games: Fighting Force; Tomb Raider II; Tomb Raider; WWF Warzone; Kartia: The Word of Fate; Snowboard Kids; Command & amp; Conquer: Tiberian Sun - PC; Final Fantasy VII; Grim Fandango - PC; Half-Life - PC; ", "page_idx": 17}, {"type": "text", "text": "MovieLens-1M: Basquiat (1996); Tin Cup (1996); Godfather, The (1972); Supercop (1992);   \nManny & Lo (1996); Bound (1996); Carpool (1996); ", "page_idx": 17}, {"type": "text", "text": "Book Crossing: Prague : A Novel; Chocolate Jesus; Wie Barney es sieht; To Kill a Mockingbird; Sturmzeit. Roman; A Soldier of the Great War; Pride and Prejudice (Dover Thrift Editions); ", "page_idx": 17}, {"type": "text", "text": "Industrial: Jurassic Perisphinctes Ammonites from France; FS9140: Spinosaurus - Dinosaur Tooth 20-30mm; FS9410: USA Eocene, Fossil Fish (Knightia alt), A-grade; Delta 50-857 Charcoal Filter for 50-868; Hitachi RP30SA 7-1/2 Gallon Stainless Steel Industrial Shop Vacuum (Discontinued by Manufacturer); Makita 632002-4 14-Inch Cut-Off Wheels (5-Pack) (Discontinued by Manufacturer); PORTER-CABLE 740001801 4 1/2-Inch by 10yd 180 Grit Adhesive-Backed Sanding Roll; ", "page_idx": 17}, {"type": "text", "text": "720 In this paper, we only use the item titles as the text description. Figure 4 gives some item title   \n721 examples from different datasets. ", "page_idx": 17}, {"type": "text", "text": "722 C.2 General Recommendation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "723 C.2.1 Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "724 We incorporate a series of CF models as our baselines for general recommendation. These models   \n725 are classified as classical CF methods (MF, MultVAE, and LightGCN), CL-based CF methods (SGL,   \n726 BC Loss, and XSimGCL), and LM-enhanced CF methods (KAR, RLMRec). For these LM-enhanced   \n727 CF methods, we adopt the leading CF method XSimGCL as the backbone.   \n728 \u2022 MF [54, 68] is the most basic CF model. It denotes users and items with ID-based embeddings and   \n729 conducts matrix factorization with Bayesian personalized ranking (BPR) loss.   \n730 \u2022 MultVAE [63] is a traditional CF model based on the variational autoencoder (VAE). It regards the   \n731 item recommendation as a generative process from a multinomial distribution and uses variational   \n732 inference to estimate parameters. We adopt the same model structure as suggested in the paper:   \n733 $600\\to200\\to600.$ .   \n734 \u2022 LightGCN [55] is a light graph convolution network tailored for the recommendation, which   \n735 deletes redundant feature transformation and activation function in NGCF [52].   \n736 \u2022 SGL [80] introduces graph contrastive learning into recommender models for the first time. By   \n737 employing node or edge dropout to generate augmented graph views and conduct contrastive   \n738 learning between two views, SGL achieves better performance than LightGCN.   \n739 \u2022 BC Loss [76] introduces a robust and model-agnostic contrastive loss, handling various data biases   \n740 in recommendation, especially for popularity bias.   \n741 \u2022 XSimGCL [66] directly generates augmented views by adding noise into the inner layer of   \n742 LightGCN without graph augmentation. The simplicity of XSimGCL leads to a faster convergence   \n743 speed and better performance.   \n744 \u2022 KAR [48] enhances recommender models by integrating knowledge from large language models   \n745 (LLMs). It generates textual descriptions of users and items and combine the LM representations   \n746 with traditional recommenders using a hybrid-expert adaptor.   \n747 \u2022 RLMRec [47] aligns semantic representations of users and items with the representations in CF   \n748 models through a contrastive loss, as an additional loss trained together with the CF model. The   \n749 fusion of semantic information and collaborative information brings performance improvement. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/180fca78cc61c49f5c74052421221b9c2cc57aade1cb351bd4e547bce26ec55d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "", "img_caption": ["Figure 6: The t-SNE visualization of representations on Movies & TV. (6a) The item representations in the LM space. (6b) The item representations obtained by replacing the MLP with a linear mapping matrix in AlphaRec. (6c) The item representations obtained from AlphaRec. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "750 C.2.2 Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "751 We conduct the same ablation study as introduced in Section 4.1 on Movies & TV and Video Games   \n752 datasets. As illustrated in Figure 5, each component in AlphaRec contributes positively, which is   \n753 consistent with our findings in Section 4.1. ", "page_idx": 18}, {"type": "text", "text": "754 C.2.3 The t-SNE Visualization Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "755 In this section, we aim to intuitively explore how the MLP in AlphaRec further helps in excavating   \n756 collaborative signals in language representations, compared to the linear mapping matrix. We   \n757 visualize the item representations from LMs, AlphaRec (w/o MLP), and AlphaRec in Figure 6, where   \n758 AlphaRec (w/o MLP) denotes replacing the MLP with a linear mapping matrix. We observed that   \n759 movies about superhero and monster cluster in all representation spaces, indicating both AlphaRec   \n760 (w/o MLP) and AlphaRec capture the preference similarities between these items and preserve   \n761 the clustering relationship. The difference between AlphaRec (w/o MLP) and AlphaRec may lie   \n762 in the ability to capture obscure preference similarities among items. As shown in Figure 6a,   \n763 homosexual movies are dispersed in the language space, indicating the possible semantic differences   \n764 between them. AlphaRec successfully captures the preference similarities and gathers these items   \n765 in the representation space, while AlphaRec (w/o MLP) remains some items dispersed. Moreover,   \n766 AlphaRec outperforms AlphaRec (w/o MLP) by a large margin, as indicated in Figure 5a. These   \n767 results indicate that AlphaRec exhibits a more fine-grained preference capture ability with the help of   \n768 nonlinear transformation. ", "page_idx": 18}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/5de1a21f0aaedb520abe0a916efab6a1b749211aea4b7b8fb9996a9df9b604d4.jpg", "table_caption": ["Table 8: The effect of the training dataset on zero-shot recommendation "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "769 C.3 Zero-shot Recommendation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "770 C.3.1 Co-training on Multiple Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "771 Co-training on multiple datasets is similar to training on one single dataset, where the only difference   \n772 lies in the negative sampling. When co-training on multiple datasets, the negative items are restricted   \n773 to the same dataset as the positive item rather than the full item pool. The other training procedures   \n774 remain the same with training on one single dataset. ", "page_idx": 19}, {"type": "text", "text": "775 C.3.2 Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "776 Since previous works about zero-shot recommendation mostly focus on sequential recommendation   \n777 [83, 82], we slightly modify two methods in sequential recommendation, ZESRec [38] and UniSRec   \n778 [37] as our baselines. Specifically, we maintain the model structure as provided in the paper, and   \n779 adopt the training paradigm of CF. ", "page_idx": 19}, {"type": "text", "text": "780 \u2022 Random denotes randomly recommending items from the entire item pool. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Pop denotes randomly recommending from the most popular items. Here popularity denotes the number of users that have interacted with the item. ", "page_idx": 19}, {"type": "text", "text": "\u2022 ZESRec [38] is the first work that defines the problem of zero-shot recommendation. To address this problem, this work introduces a hierarchical Bayesian model with representations from the pre-trained BERT. ", "page_idx": 19}, {"type": "text", "text": "786 \u2022 UniSRec [37] aims to learn universal item representations from BERT, with parametric whitening   \n787 and a MoE-enhanced adaptor. By pre-training on multiple source datasets, UniSRec can conduct   \n788 zero-shot recommendation on various datasets in a transductive or inductive paradigm. ", "page_idx": 19}, {"type": "text", "text": "789 C.3.3 The Effect of Training Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "790 The effect of the training dataset on zero-shot recommendation. We report the zero-shot   \n791 recommendation performance differences trained on different datasets in Table 8. Here AlphaRec   \n792 (trained on Books) denotes training on a single Books dataset, while AlphaRec (trained on mixed   \n793 dataset) denotes co-training on three Amazon datasets. Generally, training on more datasets lead to a   \n794 better zero-shot performance.   \n795 The performance comparison between training on the single dataset and the mixed dataset. In   \n796 Table 9, AlphaRec (trained on single dataset) denotes training and testing on the same single dataset,   \n797 while AlphaRec (trained on mixed dataset) denotes training on three Amazon datasets and testing   \n798 on one single dataset. Generally, co-training on three Amazon datasets yields similar performance   \n799 compared with training on one single dataset. The only exception lies in Video Games, which shows   \n800 some performance degradation. We attribute this to the difference between the selection of $\\tau$ . We use   \n801 $\\tau=0.15$ when trained on the mixed dataset, while the optimal $\\tau$ for Video Games lies around 0.2.   \n802 These results indicate that a single AlphaRec can capture user preferences among various datasets,   \n803 showcasing a general collaborative signal capture ability. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Intention Query Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Input   \nYou are an expert in generating queries for a target movie. Please help me generate the most suitable query for the target movie within one sentence, following the given example. Example:   \nTARGET: BUG-A-SALT 3.0 Black Fly Edition.   \nQUERY: I want a gun that I can use while gardening to get rid of stink bugs, ants, flies, and spiders in my house. It needs to be amazing and help me feel less scared.   \nTARGET: Toy Story (1995).   \nOutput   \nQUERY: I\u2019m looking for a heartwarming animated movie that follows the adventures of a group of toys who come to life when their owner is not around. ", "page_idx": 20}, {"type": "text", "text": "806 The user intention query is a natural language sentence implying the target item of interest. For   \n807 each item in the dataset, we generate a fixed user intention query. Following the previous work   \n808 [40], we generate user intention queries with the help of ChatGPT [85]. As shown in Figure 7, we   \n809 prompt ChatGPT in a Chain-of-Thought (CoT) [101] paradigm and adopt the output as the user   \n810 intention query. We adopt a rule-based strategy to ensure that the output query is in first person, and   \n811 regenerate the wrong query. Considering the huge amount of item title text, we use ChatGPT3.5 API   \n812 for generating all queries for the budget\u2019s sake. ", "page_idx": 20}, {"type": "text", "text": "813 C.4.2 Baseline ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "814 AlphaRec exhibits user intention capture abilities, although not specially designed for search tasks.   \n815 We compare AlphaRec with TEM [86] which falls in the field of personalized search [84, 102].   \n816 \u2022 TEM [86] uses a transformer to encode the intention query together with user history behaviors,   \n817 which enables it to achieve better search results by considering the user\u2019s historical interest. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "818 C.4.3 Case Study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "819 We conduct two more case studies to verify the user intention capture ability of AlphaRec. As   \n820 illustrated in Figure 8 and Figure 9, AlphaRec provides proper recommendation results, including the   \n821 target item for the user intention at the top. ", "page_idx": 20}, {"type": "text", "text": "822 C.4.4 Effect of the Intention Strength Alpha ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "823 The value of $\\alpha$ controls the balance between the user\u2019s historical interests and the user intention   \n824 query. A larger $\\alpha$ incorporates more about the user intention while considering less about the user\u2019s   \n825 historical interests. As shown in Figure 10, the effect of $\\alpha$ on Video Games shows a similar trend   \n826 with MovieLens-1M. ", "page_idx": 20}, {"type": "text", "text": "827 C.5 Trainig Cost ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "828 We report the training cost of AlphaRec in this section. Table 10 reports the seconds needed per   \n829 epoch and the total training cost until convergence. Here Amazon-Mix denotes the mixed dataset of   \n830 Books, Movies & TV, and Video Games. It\u2019s worth noting that AlphaRec converges quickly and only   \n831 requires a small amount of training time. ", "page_idx": 20}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/bcf4fc49eef13b14574e386c49003cc98cc43d6c577ff48c7dbf59c4375427f4.jpg", "img_caption": ["Figure 8: Case study of user intention capture on MovieLens-1M "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/9e9709f22260bb51f9912ea9e235cce302d02634a7e37b05d77f18ea1be0343f.jpg", "img_caption": ["Figure 9: Case study of user intention capture on Video Games "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "832 D Hyperparameter Settings and Implementation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "833 We conduct all the experiments in PyTorch with a single NVIDIA RTX A5000 (24G) GPU and a   \n834 64 AMD EPYC 7543 32-Core Processor CPU. We optimize all methods with the Adam optimizer.   \n835 For all ID-based CF methods, we set the layer numbers of graph propagation by default at 2, with   \n836 the embedding size as 64 and the size of sampled negative items $\\vert\\mathcal{S}_{u}\\vert$ as 256. We use the early stop   \n837 strategy to avoid overfitting. We stop the training process if the Recall $@20$ metric on the validation   \n838 set does not increase for 20 successive evaluations. In AlphaRec, the dimensions of the input and   \n839 output in the two-layer MLP are 3072 and 64 respectively, with the hidden layer dimension as 1536.   \n840 We apply the all-ranking strategy [103] for all experiments, which ranks all items except positive ones   \n841 in the training set for each user. We search hyperparameters for baselines according to the suggestion   \n842 in the literature. The hyperparameter search space is reported in Table 11. For these LM-enhanced   \n843 models, KAR and RLMRec, we also search the hyperparameter of their backbone XSimGCL.   \n844 For AlphaRec, the only hyperparameter is the temperature $\\tau$ and we search it in [0.05, 2]. We report   \n845 the temperature $\\tau$ we used for each dataset in Table 12. For the mixed dataset Amazon-Mix in   \n846 Section 4.2, we use a universal $\\tau=0.15$ . We adopt $\\tau=0.2$ for the MovieLens-1M dataset for the user   \n847 intention capture experiment in Section 4.3. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "848 E Broader Impact ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "849 The proposed AlphaRec can significantly improve the performance of zero-shot recommendation   \n850 and the capability of user intent capture, offering a good approach to crafting more personalized   \n851 recommendation results. One concern of AlphaRec is the potential for the representations generated   \n852 by language models can be maliciously attacked, which may result in erroneous or unexpected   \n853 recommendations. Therefore, we kindly advise researchers to cautiously check the quality of the   \n854 language representations before using AlphaRec.   \n856 The checklist is designed to encourage best practices for responsible machine learning research,   \n857 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n858 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n859 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n860 towards the page limit.   \n861 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n862 each question in the checklist:   \n863 \u2022 You should answer [Yes] , [No] , or [NA] .   \n864 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n865 relevant information is Not Available.   \n866 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n867 The checklist answers are an integral part of your paper submission. They are visible to the   \n868 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n869 (after eventual revisions) with the final version of your paper, and its final version will be published   \n870 with the paper.   \n871 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n872 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n873 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n874 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n875 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n876 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n877 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n878 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n879 please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "image", "img_path": "2Bef9YxSJc/tmp/6e3c07696f726c7fe3f2d6bb4368d1a9793c7502f5aa2b5a21ddf28b34f4d31e.jpg", "img_caption": ["Figure 10: Effect of $\\alpha$ on Video Games "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/c833c8d13aa65a23a4f0114bb16e3badc03cb539f7f38044267aaaa4a510f07a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/c39de4904c5a35659645060db4888c5e8962b4db3ad6c1da64ead18d169841ec.jpg", "table_caption": ["Table 11: Hyperparameters search spaces for baselines. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "2Bef9YxSJc/tmp/459f5a32c74fef4f47e05b446f81d143dc3752500dd3bfe453fdf377eba3cfcc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "880 IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "881 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n882 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n883 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n884 1. Claims   \n885 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n886 paper\u2019s contributions and scope?   \n887 Answer: [Yes]   \n888 Justification: We clearly state the claims made in the abstract and introduction.   \n889 Guidelines:   \n890 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n891 made in the paper.   \n892 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n893 contributions made in the paper and important assumptions and limitations. A No or   \n894 NA answer to this question will not be perceived well by the reviewers.   \n895 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n896 much the results can be expected to generalize to other settings.   \n897 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n898 are not attained by the paper.   \n899 2. Limitations   \n900 Question: Does the paper discuss the limitations of the work performed by the authors?   \n901 Answer: [Yes]   \n902 Justification: We discuss the limitations of this work in the Section 5.   \n903 Guidelines:   \n904 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n905 the paper has limitations, but those are not discussed in the paper.   \n906 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n907 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n908 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n909 model well-specification, asymptotic approximations only holding locally). The authors   \n910 should reflect on how these assumptions might be violated in practice and what the   \n911 implications would be.   \n912 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n913 only tested on a few datasets or with a few runs. In general, empirical results often   \n914 depend on implicit assumptions, which should be articulated.   \n915 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n916 For example, a facial recognition algorithm may perform poorly when image resolution   \n917 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n918 used reliably to provide closed captions for online lectures because it fails to handle   \n919 technical jargon.   \n920 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n921 and how they scale with dataset size.   \n922 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n923 address problems of privacy and fairness.   \n924 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n925 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n926 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n927 judgment and recognize that individual actions in favor of transparency play an impor  \n928 tant role in developing norms that preserve the integrity of the community. Reviewers   \n929 will be specifically instructed to not penalize honesty concerning limitations.   \n930 3. Theory Assumptions and Proofs   \n931 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n932 a complete (and correct) proof?   \n933 Answer: [NA]   \n934 Justification: This is an empirical article and contains no theoretical results.   \n935 Guidelines:   \n936 \u2022 The answer NA means that the paper does not include theoretical results.   \n937 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n938 referenced.   \n939 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n940 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n941 they appear in the supplemental material, the authors are encouraged to provide a short   \n942 proof sketch to provide intuition.   \n943 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n944 by formal proofs provided in appendix or supplemental material.   \n945 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n946 4. Experimental Result Reproducibility   \n947 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n948 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n949 of the paper (regardless of whether the code and data are provided or not)?   \n950 Answer: [Yes]   \n951 Justification: We present all the experiment details and datasets in Appendix C, and Hyper  \n952 parameters settings are reported in Appendix D. Moreover, we have uploaded the code and   \n953 data we used in the supplementary material.   \n954 Guidelines:   \n955 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide access to the data and code we used in the supplementary material. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 25}, {"type": "text", "text": "1010 \u2022 Providing as much information as possible in supplemental material (appended to the   \n1011 paper) is recommended, but including URLs to data and code is permitted.   \n1012 6. Experimental Setting/Details   \n1013 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n1014 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n1015 results?   \n1016 Answer: [Yes]   \n1017 Justification: Datasets and data split are presented in Appendix C.1, and hyperparameters   \n1018 are searched according to the suggestion in the literature. See more details in Appendix D.   \n1019 Guidelines:   \n1020 \u2022 The answer NA means that the paper does not include experiments.   \n1021 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n1022 that is necessary to appreciate the results and make sense of them.   \n1023 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n1024 material.   \n1025 7. Experiment Statistical Significance   \n1026 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n1027 information about the statistical significance of the experiments?   \n1028 Answer: [Yes]   \n1029 Justification: We validate the p-value to support the main claims of this paper.   \n1030 Guidelines:   \n1031 \u2022 The answer NA means that the paper does not include experiments.   \n1032 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n1033 dence intervals, or statistical significance tests, at least for the experiments that support   \n1034 the main claims of the paper.   \n1035 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1036 example, train/test split, initialization, random drawing of some parameter, or overall   \n1037 run with given experimental conditions).   \n1038 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1039 call to a library function, bootstrap, etc.)   \n1040 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1041 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1042 of the mean.   \n1043 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1044 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1045 of Normality of errors is not verified.   \n1046 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1047 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1048 error rates).   \n1049 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1050 they were calculated and reference the corresponding figures or tables in the text.   \n1051 8. Experiments Compute Resources   \n1052 Question: For each experiment, does the paper provide sufficient information on the com  \n1053 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1054 the experiments?   \n1055 Answer: [Yes]   \n1056 Justification: We conduct all the experiments in PyTorch with a single NVIDIA RTX A5000   \n1057 (24G) GPU and a 64 AMD EPYC 7543 32-Core Processor CPU. And Detailed time costs   \n1058 are shown in Appendix C.5.   \n1059 Guidelines:   \n1060 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Justification: The research adheres to all ethical guidelines outlined by NeurIPS. Specifically, we have ensured that our data collection methods are ethical, our experiments are conducted responsibly, and all potential biases are addressed. Additionally, we have considered the broader impacts of our work and have taken steps to mitigate any negative consequences. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Justification: We consider both the potential societal impacts and negative societal impacts, and also discuss possible mitigation strategies. Details are shown in Appendix E. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "12 Question: Does the paper describe safeguards that have been put in place for responsible 13 release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "1115 Answer: [NA]   \n1116 Justification: The paper poses no such risks.   \n1117 Guidelines:   \n1118 \u2022 The answer NA means that the paper poses no such risks.   \n1119 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1120 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1121 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1122 safety filters.   \n1123 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1124 should describe how they avoided releasing unsafe images.   \n1125 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1126 not require this, but we encourage authors to take this into account and make a best   \n1127 faith effort.   \n1128 12. Licenses for existing assets   \n1129 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1130 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1131 properly respected?   \n1132 Answer: [Yes]   \n1133 Justification: We incorporate six datasets, including four datasets from the Amazon   \n1134 platform[1](Books, Movies & TV, Video Games, and Industrial), Movielens-1M[59], and   \n1135 Book Crossing[60], all of which are open-source. The backend language models used in our   \n1136 research are BERT [4], RoBERTa [5], Llama2-7B [6], Mistral-7B [69], text-embedding-ada  \n1137 v2 & text-embeddings-3-large [70], and SFR-Embedding-Mistral [71].   \n1138 Guidelines:   \n1139 \u2022 The answer NA means that the paper does not use existing assets.   \n1140 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1141 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1142 URL.   \n1143 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1144 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1145 service of that source should be provided.   \n1146 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1147 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1148 has curated licenses for some datasets. Their licensing guide can help determine the   \n1149 license of a dataset.   \n1150 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1151 the derived asset (if it has changed) should be provided.   \n1152 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1153 the asset\u2019s creators.   \n1154 13. New Assets   \n1155 Question: Are new assets introduced in the paper well documented and is the documentation   \n1156 provided alongside the assets?   \n1157 Answer: [NA]   \n1158 Justification: This paper does not release new assets.   \n1159 Guidelines:   \n1160 \u2022 The answer NA means that the paper does not release new assets.   \n1161 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1162 submissions via structured templates. This includes details about training, license,   \n1163 limitations, etc.   \n1164 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1165 asset is used.   \n1166 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1167 create an anonymized URL or include an anonymized zip file.   \n168 14. Crowdsourcing and Research with Human Subjects   \n1169 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1170 include the full text of instructions given to participants and screenshots, if applicable, as   \n171 well as details about compensation (if any)?   \n1172 Answer: [NA]   \n1173 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1174 Guidelines:   \n1175 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1176 human subjects.   \n1177 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1178 tion of the paper involves human subjects, then as much detail as possible should be   \n1179 included in the main paper.   \n1180 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1181 or other labor should be paid at least the minimum wage in the country of the data   \n1182 collector.   \n183 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1184 Subjects   \n185 Question: Does the paper describe potential risks incurred by study participants, whether   \n1186 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1187 approvals (or an equivalent approval/review based on the requirements of your country or   \n1188 institution) were obtained?   \n189 Answer: [NA]   \n1190 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1191 Guidelines:   \n1192 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1193 human subjects.   \n1194 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1195 may be required for any human subjects research. If you obtained IRB approval, you   \n1196 should clearly state this in the paper.   \n1197 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1198 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1199 guidelines for their institution.   \n1200 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1201 applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}]