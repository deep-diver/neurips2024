[{"figure_path": "snxWD0Q4EI/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of k-IHT and Topk-WoodTaylor for sparse linear regression with standard Gaussian and MNIST priors.", "description": "This figure compares the performance of two algorithms, k-IHT and Topk-I-OBS, on a sparse linear regression task using two different prior distributions (standard Gaussian and MNIST).  The left panel shows the training loss curves for both algorithms, demonstrating the convergence rate of each. The right panel displays the L2 distance between the solution found by each algorithm and the true optimal solution.  The shaded regions represent the variability across multiple runs. The results suggest that Topk-I-OBS converges faster and achieves a solution closer to the optimum than k-IHT, especially when using the MNIST prior. ", "section": "4 Experiments"}, {"figure_path": "snxWD0Q4EI/figures/figures_7_2.jpg", "caption": "Figure 1: Comparison of k-IHT and Topk-WoodTaylor for sparse linear regression with standard Gaussian and MNIST priors.", "description": "This figure compares the performance of two algorithms, k-IHT and Topk-I-OBS, on a sparse linear regression task using two different priors: a standard Gaussian prior and an MNIST prior.  The left subplot shows the learning curves (loss vs. iteration count) for both algorithms with the standard Gaussian prior, demonstrating Topk-I-OBS's faster convergence rate. The right subplot visually presents the original MNIST digit images, images recovered using Topk-I-OBS, and images recovered using k-IHT for the MNIST prior; this illustrates the superior recovery quality of Topk-I-OBS.", "section": "4 Experiments"}, {"figure_path": "snxWD0Q4EI/figures/figures_13_1.jpg", "caption": "Figure 1: Comparison of k-IHT and Topk-WoodTaylor for sparse linear regression with standard Gaussian and MNIST priors.", "description": "This figure compares the performance of two algorithms, k-IHT and Topk-I-OBS, on a sparse linear regression task using two different priors (standard Gaussian and MNIST).  The left subplot (a) shows the learning curves for both algorithms using the standard Gaussian prior, demonstrating the faster convergence of Topk-I-OBS. The right subplot (b) shows the reconstruction quality of the MNIST prior by both methods, visually comparing the recovered image with the original.  The results highlight the superior convergence and reconstruction capabilities of Topk-I-OBS.", "section": "4 Experiments"}, {"figure_path": "snxWD0Q4EI/figures/figures_13_2.jpg", "caption": "Figure 2: I-OBS dynamics for Llama-2 7B (star corresponds to best validation score). (Left) Wikitext-2 Perplexity vs iteration. (Right) C4 Perplexity vs iteration.", "description": "This figure shows the perplexity (a measure of how well a language model predicts a sequence of words) for the Llama-2 7B language model on two different datasets, WikiText-2 and C4.  The x-axis represents the iteration number of the I-OBS (Iterative Optimal Brain Surgeon) algorithm, and the y-axis shows the perplexity. The left panel shows perplexity on the WikiText-2 dataset, while the right panel shows perplexity on the C4 dataset. The star indicates the iteration with the lowest perplexity score (best performance). The figure demonstrates how the perplexity changes over the iterations of the I-OBS algorithm. The initial decrease indicates improvement during pruning but is followed by an increase suggesting a possible overfitting to the training dataset in later iterations.", "section": "A.2 Extra experiments"}]