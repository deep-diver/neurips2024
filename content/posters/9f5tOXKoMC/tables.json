[{"figure_path": "9f5tOXKoMC/tables/tables_7_1.jpg", "caption": "Table 1: The average GPU memory and time usage over 100 steps. \"Base\" represent all non-DPSs.", "description": "This table presents a comparison of GPU memory and time usage across different methods for data point selection in the WebNLG task.  The \"Base\" row represents the memory and time usage for traditional methods (non-DPS), providing a baseline for comparison.  The table shows that BADS and CDS have similar memory and time usage to the baseline methods.  However, BLO and AskLLM-O require substantially more time, while ClassAct requires significantly more memory and time than other methods. Note that  CDS and AskLLM-O require additional time to calculate weights and perform LLM calls respectively. ", "section": "3.4.2 Latency"}, {"figure_path": "9f5tOXKoMC/tables/tables_8_1.jpg", "caption": "Table 2: Test accuracy of LLMs across four popular benchmarks in eval-harness [17]. Checkpoint selection is using next token prediction accuracy as the selection metric. Mixing represents standard IFT.", "description": "This table presents the test accuracy results for various Large Language Models (LLMs) evaluated on four popular benchmarks: MMLU, ARCC, ARCe, and HellaSwag.  The models were trained using different data selection methods, including a baseline ('Mixing') representing standard Instruction Fine-Tuning (IFT). The 'Checkpoint selection' method used next token prediction accuracy. The table compares the performance of different data selection approaches against the standard IFT approach, highlighting the effectiveness of different strategies in improving LLM performance.", "section": "4 Use Case: Large Language Model Instruction Fine-tuning"}, {"figure_path": "9f5tOXKoMC/tables/tables_16_1.jpg", "caption": "Table 3: Hyperparameters for all experiments.", "description": "This table lists the hyperparameter settings used in the experiments for different tasks and models.  It shows the values for various parameters used in the Bayesian Data Point Selection (BADS) and Contrastive Data Selection (CDS) methods, such as learning rates (\u03b7), impact constants (\u03c1\u03b8, \u03c1\u03c6, \u03c1w), noise standard deviation (\u03c3), sparsity level (\u03b2), running average step (Savg), floor ratio for CDS (rfloor), hidden layer size (H), and lr_halfv which likely refers to a learning rate adjustment. Each row represents a different experiment (MNIST, CIFAR, WebNLG, LLMs).", "section": "2.4 Implementation Details"}, {"figure_path": "9f5tOXKoMC/tables/tables_19_1.jpg", "caption": "Table 4: The average scores IFT examples get from BADS.", "description": "This table shows the average weights assigned by the BADS model to examples from different Instruction Fine-tuning (IFT) datasets.  It highlights the relative importance the model assigns to each dataset in the fine-tuning process, suggesting that some datasets contribute more significantly to the overall performance than others.  These weights inform the data selection strategy used by BADS to optimize the fine-tuning process.", "section": "4 Use Case: Large Language Model Instruction Fine-tuning"}]