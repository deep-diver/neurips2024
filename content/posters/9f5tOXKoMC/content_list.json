[{"type": "text", "text": "A Bayesian Approach to Data Point Selection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinnuo Xu\u2020\u2217 Microsoft Research Cambridge xinnuoxu@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Minyoung Kim\u2020 Samsung AI Center Cambridge, UK mikim21@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Royson Lee Samsung AI Center Cambridge, UK royson.lee@samsung.com ", "page_idx": 0}, {"type": "text", "text": "Brais Martinez Samsung AI Center Cambridge, UK brais.mart@samsung.com ", "page_idx": 0}, {"type": "text", "text": "Timothy Hospedales Samsung AI Center Cambridge, UK University of Edinburgh, UK t.hospedales@ed.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data point selection (DPS) is becoming a critical topic in deep learning due to the ease of acquiring uncurated training data compared to the difficulty of obtaining curated or processed data. Existing approaches to DPS are predominantly based on a bi-level optimisation (BLO) formulation, which is demanding in terms of memory and computation, and exhibits some theoretical defects regarding minibatches. Thus, we propose a novel Bayesian approach to DPS. We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model. We employ stochastic gradient Langevin MCMC sampling to learn the main network and instance-wise weights jointly, ensuring convergence even with minibatches. Our update equation is comparable to the widely used SGD and much more efficient than existing BLObased methods. Through controlled experiments in both the vision and language domains, we present the proof-of-concept. Additionally, we demonstrate that our method scales effectively to large language models and facilitates automated per-task optimization for instruction fine-tuning datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Practical machine learning efficacy is heavily dependent on the choice, quality and quantity of training data, especially so in the case of neural networks that can easily fit every detail of the training set. This leads to challenges from how to learn reliably with imbalanced data [22], noisy data, noisy labels [46], and so on. Similarly there is often a key subset of data, which is most informative for a given learning problem, but buried among a much larger set of less relevant data. If the most salient data could be efficiently identified, learning could potentially be accelerated [15]. All these challenges are only growing in the era of large scale training on web-scraped data, where curation and gold-standard quality control are not feasible. ", "page_idx": 0}, {"type": "text", "text": "Data Point Selection (DPS) algorithms aim to address these challenges by flitering or re-weighting the training data to reduce noise, imbalance, irrelevant background data and so on. The most established family of approaches [19, 15, 41, 45, 62] to DPS falls under the bi-level optimization or meta-learning umbrella, where one wraps the conventional learning problem with an outer loop that optimizes the dataset itself, so as to maximise performance on some validation set. These methods vary in the choice of their outer optimization variable (e.g., data point weights [41, 45, 19] vs mini-batch sampler [15]), the method of computing meta-gradients (e.g., reverse mode differentiation [45, 62] or reinforcement learning [15]), and the customization of their losses and other design parameters for the different scenarios (e.g., label-noise [45, 41], etc). However, all the BLO approaches are quite expensive in computation and/or memory, which limits their applicability to the most salient use case of large models trained on large web data. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper we revisit the DPS problem from the perspective of Bayesian learning. Rather than constructing expensive nested optimization problems whose convergence is hard to analyse, we treat it as a problem of inferring the joint posterior over the main neural network parameters and instance-wise weights induced by a second weight-estimation neural network. This framework has several advantages in terms of being more efficient and scalable than typical BLO competitors and having a clear convergence guarantee. It is also able to address a variety of DPS-related problems \u2013 from noise and imbalance to data curation \u2013 within a single framework. ", "page_idx": 1}, {"type": "text", "text": "Our empirical results present proof of concepts for all these capabilities on a variety of learning tasks in vision and language. We also show a use case of automating Large Language Models (LLMs) instruction fine-tuning (IFT) data curation for specific downstream tasks. The available IFT datasets are large, diverse, and of varying quality. This means that a key activity for natural language processing (NLP) researchers and developers is often finding the right composition of IFT data sources to optimize particular use cases. Our framework can automatically resample and curate the wide array of available auxiliary IFT data to optimize performance for each NLP task of interest. To our knowledge, no BLO alternative has been demonstrated on billion-parameter scale LLMs. We name our approach BADS (Bayesian Data Point Selection).3 ", "page_idx": 1}, {"type": "text", "text": "2 Our Approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For the DPS problem, we assume that we are given two datasets: the train set $\\mathcal{D}_{t}\\!=\\!\\{z_{i}^{t}\\}_{i=1}^{N_{t}}$ and the meta set $\\mathcal{D}_{m}\\!=\\!\\{z_{i}^{m}\\}_{i=1}^{N_{m}}$ . Each data point $z_{i}$ can be an input-target pair $z_{i}=(x_{i},y_{i})$ in the traditional (class-)labeled data scenarios. But in the autoregressive generative model scenarios (e.g., LLM), is simply a sequence of tokens in which the inputs and targets are rather implicitly defined (e.g., all the tokens up until current time as input and the next token as target). We will use the notation $l(z_{i};\\theta)$ for the loss of the model $\\theta$ on the data point $z_{i}$ , which must be well-defined in both scenarios. ", "page_idx": 1}, {"type": "text", "text": "The meta dataset $\\mathcal{D}_{m}$ is considered as in-domain, meaning that the distribution of $\\mathcal{D}_{m}$ matches that of the downstream test task of interest. The size of $\\mathcal{D}_{m}$ , denoted by $N_{m}$ , is typically small, due to the cost of curation/annotation processes in practice. The train dataset $\\mathcal{D}_{t}$ consists of out-of-domain samples, possibly noisy, imbalanced, and uncurated, but the size $N_{t}$ is usually large. The goal of DPS is to select a (soft weighted) subset of the train set $\\mathcal{D}_{t}$ with the guidance of the meta set $\\mathcal{D}_{m}$ , so that the model trained on the selected train and meta dataset points performs well. Typical baselines include training with the meta-set alone, or union of meta and train sets. ", "page_idx": 1}, {"type": "text", "text": "Perhaps one of the most widely adopted DPS techniques is the bi-level optimisation (BLO) formulation of the problem [41, 58]. Letting $w_{i}\\geq0)$ be the weight (or importance) variable associated with the training data point $z_{i}^{t}$ , the main intuition is to find the weights $w\\in\\mathbb{R}_{+}^{N_{t}}$ such that the model $\\theta$ trained with the weighted train data with weights $w$ yields the best performance on the meta set. More formally, this leads to the following BLO problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}_{+}^{N_{t}}}\\sum_{j=1}^{N_{m}}l(z_{j}^{m};\\theta^{*}(w))\\quad\\mathrm{s.t.}\\quad\\theta^{*}(w)=\\arg\\operatorname*{min}_{\\theta}\\sum_{i=1}^{N_{t}}w_{i}\\cdot l(z_{i}^{t};\\theta).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "However, a critical drawback is that solving this difficult problem is costly in computation and/or memory, and unrealible due the practical heuristics required. Typical BLO solutions to obtain the ", "page_idx": 1}, {"type": "text", "text": "hypergradient $d l/d\\omega$ rely on approximate Hessian estimation or reverse mode differentiation with few-step SGD approximation of the inner optimisation. Aside from cost, for practical neural network implementations computed over minibatches, there is no theoretical guarantee for convergence. ", "page_idx": 2}, {"type": "text", "text": "2.2 (Our Approach) Bayesian Data Point Selection (BADS) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We view the DPS problem from a completely different perspective, and tackle it via Bayesian learning. Our model\u2019s generative process, that is, the graphical model representation, is depicted in Fig. 1. The main neural network model parameters set $\\theta$ is a random variable, which can generate data points in the meta dataset $\\mathcal{D}_{m}$ (precisely speaking, the backbone $\\theta$ generates the target part of each data point). To make use of the train set $\\mathcal{D}_{t}$ in an appropriate way, we constrain $\\theta$ to follow a prior distribution governed by the weighted train data with weights $w\\in\\mathbb{R}_{+}^{N_{t}}$ which are also random variables. Before observing the meta set $\\mathcal{D}_{m}$ , the weight vector $w$ follows a ", "page_idx": 2}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/b64bfbd996777a955b10e82690f07241a841320ced502cd817d78a20f3ff83f6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Graphical model for $B A D S$ . Shaded nodes, representing curated $(D_{m})$ and uncurated $(D_{t})$ data, are evidence. Unshaded nodes, including model $\\theta$ and instance weights $w$ , are random variables. ", "page_idx": 2}, {"type": "text", "text": "prior distribution $p(w)-\\mathbf{a}$ specific distributional choice for $p(w)$ will be discussed later. Given $w$ and $\\mathcal{D}_{t}$ , our backbone $\\theta$ has to be compatible with the weight data $\\{(w_{i},z_{i}^{t})\\}_{i=1}^{N_{t}}$ . This can be interpreted as placing a weighted-data-driven prior on $\\theta$ , more specifically, ", "page_idx": 2}, {"type": "text", "text": "(Weighted-data-driven prior) $p(\\theta|w,\\mathcal{D}_{t})\\propto p(\\theta)\\cdot\\prod_{i=1}^{N_{t}}p(w_{i},z_{i}^{t}|\\theta)$ ", "page_idx": 2}, {"type": "text", "text": "where $p(\\theta)$ is a base prior (e.g., 0-centered Gaussian that amounts to weight decay regularisation), and $p(w_{i},z_{i}^{t}|\\theta)$ can be defined from the loss, e.g., $\\exp(-w_{i}\\cdot l(z_{i}^{t};\\theta))$ , following the conventional tricks [36, 25]. Then given $\\theta$ , the meta data are generated following the likelihood defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathcal{D}_{m}|\\theta)\\propto\\prod_{j=1}^{N_{m}}\\exp(-l(z_{j}^{m};\\theta))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The equations (2) and (3) fully constitute the prior and likelihood for our Bayesian model. Our ultimate goal is to describe the distributions of $\\theta$ and $w$ after observing all evidences $\\mathcal{D}_{t}$ and $\\mathcal{D}_{m}$ , which boils down to the posterior inference $p(\\theta,w|\\mathcal{D}_{t},\\mathcal{D}_{m})$ . Formally, we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\theta,w|\\mathcal{D}_{t},\\mathcal{D}_{m})\\propto p(w)\\cdot p(\\theta|w,\\mathcal{D}_{t})\\cdot p(\\mathcal{D}_{m}|\\theta)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The detailed derivations for Eq. (4) can be found in Appendix A. However, it is widely known that (4) does not admit any closed-form expressions. One main difficulty arises from the intractable normalizing constant in (4). ", "page_idx": 2}, {"type": "text", "text": "Stochastic Gradient Langevin Dynamic Sampling. For computationally efficient posterior inference, we adopt the stochastic-gradient MCMC technique, specifically the Stochastic Gradient Langevin Dynamic (SGLD) sampling [55]. Applied to our model, we can obtain samples from the posterior $p(\\theta,w|\\mathcal{D}_{t},\\mathcal{D}_{m})$ by running the Langevin dynamic system (until convergence, i.e., mixing): ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\theta,w]\\ \\gets\\ [\\theta,w]+\\frac{\\eta}{2}\\nabla_{\\theta,w}\\log p(\\theta,w|\\mathcal{D}_{t},\\mathcal{D}_{m})+\\epsilon\\sqrt{\\eta},\\quad\\epsilon\\sim\\mathcal{N}(0,I)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\eta$ is a small (constant) step size. There are two critical benefits: i) Since we differentiate the log-posterior, the difficult normalizing constant in (5) will disappear; ii) The update (5) is essentially gradient descent with additive Gaussian noise, leading to a computationally efficient update. ", "page_idx": 2}, {"type": "text", "text": "Going one step further, even though the log-posterior involves the entire train data (and entire meta data), it is shown in [55] that the stochastic-gradient version (SGLD) that replaces the whole batch likelihood with a minibatched one, theoretically guarantees that the SGLD update converges to the posterior samples. More specifically, the SGLD update equations (one for $\\theta$ and the other for $w$ ) can be written as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta\\ \\gets\\ \\theta+\\frac{\\eta}{2}\\nabla_{\\theta}\\Big(\\log p(\\theta)-N_{t}\\cdot\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\big[w_{i}\\cdot l(z_{i}^{t};\\theta)\\big]-N_{m}\\cdot\\mathbb{E}_{j\\sim\\mathcal{B}_{m}}\\big[l(z_{j}^{m};\\theta)\\big]\\Big)+\\epsilon_{\\theta}\\sqrt{\\eta}}\\\\ &{w\\ \\gets\\ w+\\frac{\\eta}{2}\\nabla_{w}\\Big(\\log p(w)-N_{t}\\cdot\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\big[w_{i}\\cdot l(z_{i}^{t};\\theta)\\big]\\Big)+\\epsilon_{w}\\sqrt{\\eta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $B_{t}$ and $B_{m}$ are minibatches from $\\mathcal{D}_{t}$ and $\\mathcal{D}_{m}$ , respectively, and $\\epsilon_{\\theta},\\epsilon_{w}\\sim\\mathcal{N}(0,I)$ are independent Gaussian samples. ", "page_idx": 3}, {"type": "text", "text": "Repeating (6) and (7) for a sufficient amount of iterations (until we reach good mixing) leads us to posterior samples $(\\theta,w)$ . There are several options to take these samples for a final model for test prediction. One option is to collect latest $M$ samples (either consecutive collection or thinning to take every $k$ th samples) from the iterations, and either take the average as posterior means or perform full Bayesian treatment with the collected samples. Alternatively, we can just take the last single iterate $(\\theta,w)$ as a point representative for the posterior distribution. For simplicity, we take the latter approach, which also works well empirically. ", "page_idx": 3}, {"type": "text", "text": "2.3 Interpretation and Benefits ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Interpretation We discuss several intuitions and implications of our proposed approach (Eq. 6-7). First, looking at the $\\theta$ update Eq. (6), our model essentially updates $\\theta$ in a way that it decreases the loss on the combined data of the whole meta data points and the weighted train data points with the current weights. This is a fairly intuitive strategy provided that the weights are properly determined. Then the next question is how the weights are determined. If we inspect the $w$ update (Eq. 7), and take the gradient of the expected loss term with respect to $w$ directly, we see that: i) those train data points $z_{i}^{t}\\mathbf{s}$ with smaller losses at current backbone $\\theta$ will get higher weights $w_{i}\\mathbf{s}$ ; ii) those train data points $z_{i}^{t}\\mathbf{s}$ with larger losses at current backbone $\\theta$ will get lower weights $w_{i}\\mathbf{s}$ . This essentially means that our model performs loss alignment for DPS \u2013 In the course of training/update, once the backbone $\\theta$ enters a good regime in the parameter space such that $\\theta$ can assign (valid) low loss values on the in-domain meta data points, then it starts putting high weights on those train data points that have low losses under the current backbone. In other words, the model will assign high weights to those train data points that are well-aligned with the meta data points in terms of loss. ", "page_idx": 3}, {"type": "text", "text": "Benefits over BLO Our Bayesian approach provides several benefits over BLO: (1) Efficiency. Our SGLD is efficient so does not require computationally demanding Hessian computations like implicit function theorem based methods (cf: [19]) or huge memory demand like reverse-mode differentiation methods [41, 19]. (2) Sparsity. Our method straightforwardly achieves sparsity on the $w$ weights allowing efficient sample selection unlike [19]. (3) Reliability. BLO-based methods rely on approximations (truncation, or Hessian approximations) for practical feasibility that make finding optimal solutions unreliable. Our straightforward Bayesian approach has reliable convergence properties thanks to being a standard application of SGLD. ", "page_idx": 3}, {"type": "text", "text": "Convergence of our SGLD algorithm In Appendix C we provide a theorem showing that our SGLD algorithm converges to the true posterior. Our analysis is based on [64] where we make some adjustments for our case. ", "page_idx": 3}, {"type": "text", "text": "2.4 Implementation Details ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Choice of Priors For the base prior $p(\\theta)$ , i.e., the prior before being driven by the weighted data, we adopt 0-mean Gaussian, which amounts to adding the weight decay regularisation for $\\theta$ . For the weight prior $p(w)$ , we have made a careful design effort to come up with a viable sparsity inducing prior. Although encouraging sparsity in learned weights is ideal to avoid overftiting, during our initial experiments we have found that most of the weights eventually tend to vanish to 0, which is not what we actually want. We need to be able to impose both sparsity and a certain level of non-zero weights. To this end, we first introduce a hyperparameter $\\beta$ (e.g., 0.01) for the target sparsity level that we want to attain. Roughly saying, among the $N_{t}$ training data points, we aim to select $\\lfloor N_{t}\\cdot\\beta\\rfloor$ . For the (soft) weights, we impose $\\textstyle\\sum_{i\\in{\\cal D}_{t}}w_{i}\\approx\\lfloor N_{t}\\cdot\\beta\\rfloor$ , which can be encoded in the prior form as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(w)\\propto e^{-\\big(\\sum_{i}w_{i}-\\lfloor N_{t}\\cdot\\beta\\rfloor\\big)^{2}/2\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma$ controls the strength of the regularisation. One technical difficulty in directly plugging (8) into the weight update (7) is that we have to load the whole $\\{w_{i}\\}_{i=1}^{N_{t}}$ in memory for backprop. To avoid this issue, we use the following fact: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{D}_{t}}w_{i}\\approx\\sum_{i\\in\\mathcal{B}_{t}}w_{i}+(N_{t}-|\\mathcal{B}_{t}|)\\cdot\\bar{w}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\bar{w}$ represent historic running average of the entire weights. We basically build a computation graph only for the first term of batch $B_{t}$ weight sum, and regard the (historic) running average of the entire weights as constant during backprop. After each SGLD iteration, we update the running weight average with the new updated weights on the recent batch. We use the simple averaging scheme for the running average. To approximate the average weight $\\bar{w}$ precisely, we only conduct the average over the most recent savg step. ", "page_idx": 4}, {"type": "text", "text": "Introducing impact constants In the SGLD principle, we have the log-likelihood terms that are proportional to the sizes of the datasets. In particular, we have $N_{t}$ and $N_{m}$ in (6). However, this scheme does not properly capture our preference to the in-domain meta data set in contrast to the noisy, out-of-domain train data set. To this end, we introduce the impact constants (hyperparameters) in the update equations where we downweigh or upweigh the loss terms of train and meta sets. ", "page_idx": 4}, {"type": "text", "text": "Weight Network Instead of directly optimising individual weights $w_{i}$ , we can consider a weight network, $w_{i}\\,=\\,w(z_{i}^{t};\\phi)$ , a neural network with parameters $\\phi$ that takes the train data point $z_{i}^{t}$ as input and returns its weight $w_{i}$ as output. We can then regard $\\phi$ as random variables and the $w$ update equation can be modified accordingly for $\\phi$ update straightforwardly. This weight network approach can be useful for smoothing/regularising the output weights thanks to the smooth functional property of neural networks. Furthermore, if one needs to supplement the train dataset with extra new train samples after the model training, the learned weight network can be used for assigning weights or selecting samples from the new set, without retraining the whole model from the scratch. The posterior distribution similar to Eq. (4) is derived in full detail in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments: Proof of Concept ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we assess the effectiveness of the proposed BADS method in three critical scenarios where DPS is essential: Data Balancing, Data Denoising, and Efficient Learning. We begin by introducing the baseline systems and then present the experimental results for each of these scenarios. ", "page_idx": 4}, {"type": "text", "text": "3.1 Baselines ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There are three types of DPS setups with different supervision signals: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Unsupervised DPS selects data without the guidance of a held-out meta set [47, 42]. Instead, it is guided by human-defined hypotheses, such as \u201cchallenging examples improve model performance\u201d. This approach aligns with curriculum learning. We include the online variant of AskLLM [42], i.e. AskLLM-O, in our baseline comparisons. It selects examples from training set by querying a pretrained OpenLLaMA 3B to obtain the sampling score for each training sample.4 ", "page_idx": 4}, {"type": "text", "text": "\u2022 Self-supervised DPS selects data with the guidance of a held-out meta set. However, the meta set does not share the same data distribution as the targeted test set [13, 4, 14, 49]. Typically, the examples in the meta set are selected from the training set based on specific hypotheses, such as \u201clearnable examples enhance model performance\u201d. We include two approaches in our baseline comparisons: Contrastive Data Selection (CDS) [49] is tailored for data denoising. The algorithm assigns weights to each data point in $\\mathcal{D}_{t}$ according to the difference between the denoised and the noisy log probability, predicted using a denoised and a noisy model trained on a clean dataset and an uncurated dataset, respectively. These weights are then used to sample data points from minibatches in the training of LM.5 Similar to CDS, ClassAct utilizes small proxy models trained on a limited portion of $\\mathcal{D}_{t}$ to calculate learnability scores for the remaining training data points.6 \u2022 Meta-set guided DPS selects data with the guidance of a small meta set that shares the same distribution as the test set, aiming to train a model that excels specifically on the target test set. The test set may encompass one or multiple downstream domains or tasks. This DPS is closely related to meta learning, domain adaptation, and transfer learning. Current methods primarily rely on Bilevel Optimization (BLO) for purposes such as data denoising [19, 37], data balancing [41], and efficient learning [28, 59, 29]. Considering both performance and code availability, we use the online $\\mathrm{\\mathbf{B}L O}^{7}$ [41, 19] as our baseline. Our approach, BADS, also falls under this category. ", "page_idx": 4}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/d7cf6449c1939e6396b2af4e5149eef700f645dd09143c6a15f0a0e9818c3220.jpg", "img_caption": ["Figure 2: Proof-of-Concept experiment results. The top row displays the overall test performance across the three scenarios throughout the training phase, with x and y axis denote the training steps and the evaluation metrics, respectively. The bottom row visualizes the model-predicted weights of data points in each mini-batches in the final 2000 steps in WebNLG training (scenario 3). x and y axis show the training steps and average weights, respectively. Data points in blue color are expected to get higher weights compared to their counterparts (in red color). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To ensure a fair comparison, we also introduce several baselines that train the backbone models using different combinations of the meta set and training set: Mixing trains the model using a combination of the train set $\\mathcal{D}_{t}$ and the meta set $\\mathcal{D}_{m}$ . Meta_Only trains the model exclusively on $\\mathcal{D}_{m}$ . Random_Select uses $\\mathcal{D}_{m}$ combined with a randomly selected subset from $\\mathcal{D}_{t}$ . Duplicate_Meta utilize $\\mathcal{D}_{t}$ along with multiple copies of the meta set, duplicating $\\mathcal{D}_{m}$ until it matches the size of $\\mathcal{D}_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "Note that, the selection ratio/sparsity level in AskLLM-O, ClassAct, Random_Select, $B L O$ , and CDS is the same as in BADS.8 ", "page_idx": 5}, {"type": "text", "text": "3.2 Scenario 1: Data Balancing (MNIST) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this scenario, we assess the model\u2019s capability to manage an imbalanced train set, $\\mathcal{D}_{t}$ . Despite being trained on this imbalanced dataset, the models are expected to perform effectively on a balanced test set. Following the setup in [41], we use the standard MNIST handwritten digit classification dataset [33] to create a class-imbalanced binary classification task. A total of 5,000 images from classes 4 and 9 were selected as the train set $\\mathcal{D}_{t}$ , with class 9 dominating the training data distribution (4,975 examples) and class 4 having only 25 examples. A balanced meta set $\\mathcal{D}_{m}$ is created by selecting another 25 examples from each of these two classes, ensuring no overlap between $\\mathcal{D}_{t}$ and $\\mathcal{D}_{m}$ . The models are tested on the original MNIST test set, including only classes 4 and 9. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/5c4a095d413f2ce3b14eb2d7e083c83bd22250079b0f38d89ba9a73e3114f024.jpg", "img_caption": ["Figure 3: The MNIST test accuracy when trained with meta sets in varying sizes (x-aixs). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In this scenario, the loss function is defined using the standard binary cross-entropy loss. Following [41], all classification models are LeNet5 [32]. The training is conducted on a single GPU, using SGD with a fixed learning rate of 1e-3 and a mini-batch size of 100, over a total of 15,000 steps. In $B A D S$ , the weight network is implemented as a single-layer Feedforward Neural Network (FNN) with a sigmoid activation function. It takes the top-layer image embeddings from LeNet5 as input and outputs a weight $w_{i}\\in[0,1]$ for each image. The learning rate for the weight network is 1e-3 and the target sparsity level $\\beta$ is 0.005. Other hyperparameters, including those in the baselines, are detailed in Table 3 (Appendix E). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.2.1 Experiment Results and Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The classification accuracy is presented in the top-left plot in Figure 2. All approaches, except for Mixing and ClassAct, achieve over $90\\%$ accuracy even when trained with a highly imbalanced train set. Meta_only demonstrates that training with a small amount of balanced data yields significantly better performance compared to training with a larger but imbalanced training set (Mixing). Both $B L O$ and $B A D S$ outperform non-DPS baselines in terms of both accuracy and convergence speed, with BADS further outperforming $B L O$ by a noticeable margin. $C D S$ underperforms all the non-DPS baselines, which we believe is due to the mini-batch-level discrete data selection. When dealing with an extremely imbalanced train set, it is possible that the minority class might not be present in some mini-batches. Under these circumstances, the model is compelled to learn from the top examples from the majority class in each mini-batch, potentially leading to biased training outcomes. The top row in Figure 13 and the left plot in Figure 14 (Appendix F) visualizes the weights assigned to the examples in each mini-batch by DPS approaches. All methods, except forClassAct, effectively assigns higher weights to the minority class than to the majority class, thereby directing the classifiers to focus more on the minority class in training. ", "page_idx": 6}, {"type": "text", "text": "We further evaluate the models\u2019 performance using meta sets $\\mathcal{D}_{m}$ of various sizes, with 5, 10, 25, and 50 examples per class included.9As illustrated in Figure 3, with a very limited number of meta examples (5 per class), only $B A D S$ and $B L O$ achieve over $90\\%$ accuracy on the balanced test set. As the number of available meta data increases, $B A D S$ consistently leads in performance. However, when sufficient meta data is provided, the gap between BADS and the other approaches narrows. ", "page_idx": 6}, {"type": "text", "text": "3.3 Scenario 2: Data Denoising (CIFAR) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this scenario, we evaluate the model\u2019s ability to manage a   \nnoisy train set $\\mathcal{D}_{t}$ . Although the models are trained using a   \ndataset with significant noise, they are anticipated to perform   \nwell on a clean test set. Our experiment utilizes the standard   \nCIFAR 10-class classification dataset [30]. Following the stan  \ndard CIFAR train/validation set split ratio, we first create a   \nclean and balanced meta set $\\mathcal{D}_{m}$ by randomly sampling 1000   \nexamples from each class in the training data. Then, we use the   \nremaining 40,000 examples to create a noisy train set $\\mathcal{D}_{t}$ by   \nintroducing noise based on the symmetric noise injection setup Figure 4: The CIFAR test accuracy described in [37, 8, 21]. We set the noise ratio to 0.5 and use when trained with $80\\%$ noisy data.   \nthe original CIFAR-10 test set in testing. ", "page_idx": 6}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/964d1c8aad555b483fe9030f91f896da716a331d7d54671072a1a471a4dbebdc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We use ResNet32 [23] as the backbone classification model. Training is performed on a single GPU using SGD with a fixed learning rate of 1e-1 and a mini-batch size of 120 over 20,000 steps. The loss function is the standard multi-class cross-entropy loss. For BADS, the weight network structure retains the same as in scenario 1. However, in this scenario, data point weighting takes into account both the image and its associated label. We represent each label using a one-hot embedding, which is then concatenated with the top-layer image embeddings from ResNet32 and fed into the weight network. The learning rate for the weight network is set to 1e-4. Note that in Eq 7, the gradient of $\\theta$ become large if $N_{t}$ is big. Therefore, we reduce the learning rate of the backbone classification model to 1e-4. The target sparsity level $\\beta$ is set to 0.8.10. ", "page_idx": 6}, {"type": "text", "text": "3.3.1 Experiment Results and Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The classification accuracy is shown in the top-middle plot in Figure 2. All methods, except for the BLO approach11, manage to achieve over $60\\%$ accuracy, even using a train set contaminated by $50\\%$ . ", "page_idx": 6}, {"type": "text", "text": "Notably, CDS, ClassAct and BADS deliver the highest three performances, with $B A D S$ surpasses all other methods by a noticeable margin. The bottom row of Figure 13 and the middle plot of Figure 14 (Appendix F) visualizes the weights allocated to the examples in each mini-batch by DPS approaches. ", "page_idx": 7}, {"type": "text", "text": "All methods, except for ClassAct, consistently gives lower weights to the noisy examples, guiding the classifiers to disregard the noisy examples during training. ClassAct assigned lower weights to the noisy examples during the initial stages of training. However, these weights exceeded those of the clean examples in the later phases of training. Interestingly, this behavior did not significantly impact the model\u2019s performance. ", "page_idx": 7}, {"type": "text", "text": "If we raise the noise label ratio in the train set to $80\\%$ (Figure 4), both $B L O$ and $B A D S$ still lead the performance, with $B A D S$ exceeds $B L O$ and non-DPS approaches by $15\\%$ and $20\\%$ in classification accuracy, respectively. $C D S$ does not exceed the performance of non-DPS approaches and starts to overfti on the noisy data after 15,000 training steps. When we lower the noise label ratio to $20\\%$ (Figure 5), all methods achieve a classification accuracy of around $80\\%$ . Although $B A D S$ continues to outperform both DPS and non-DPS approaches, the lead is narrower.12 ", "page_idx": 7}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/d86f1c302fa061c92f7f4d3faa9c1558b2032c56ea6ad3e9e5c33b4b176a9b8c.jpg", "img_caption": ["Figure 5: The CIFAR test accuracy when trained with $20\\%$ noisy data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.4 Scenario 3: Efficient Learning (WebNLG) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this scenario, we assess the model\u2019s ability to adapt to new domains with very few data points. To demonstrate the robustness of BADS across various research topics and backbone models, we focus on the Natural Language Generation (NLG) task in this section. NLG [18, 12] aims to accurately generate textual descriptions from input tuples. An example is shown in Figure 7 (Appendix D). The English benchmark introduced in WebNLG 2020 [6] includes a training set spanning 16 domains and a test set covering 3 different domains (for details, check Appendix D). We use the original training set (14,239 examples) as our train set $\\mathcal{D}_{t}$ , and create a single clean and balanced meta set $\\mathcal{D}_{m}$ by randomly sampling 30 examples from the WebNLG 2020 validation set in each test domain. ", "page_idx": 7}, {"type": "text", "text": "All backbone models are the encoder-decoder T5-small [26]. Training is on a single GPU using Adam with a fixed learning rate of 3e-5 and a mini-batch size of 20 over 40,000 steps. The loss function is the standard negative log likelihood loss. In BADS, the weight network structure remains the same as in scenario 1. Its input is the embedding of the input sequence, represented by the contextual embedding of the \u201c[EOS]\u201d token, and the learning rate is set to 1e-4. The sparsity level $\\beta$ is 0.05. ", "page_idx": 7}, {"type": "text", "text": "3.4.1 Experiment Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The BLEU scores are displayed in the top-right plot of Figure 2. BADS leads the performance, achieving a 2 BLEU score advantage over the second-best system, Duplicate_Meta, and surpassing the remaining systems by more than 5 BLEU scores. The other three DPS approaches do not distinguish themselves from the non-DPS methods. ", "page_idx": 7}, {"type": "text", "text": "In this scenario, we opt for a more controlled examination of data selection effectiveness due to the big amount of domains in the train set13. We select three occupationcentric domains\u2014Athlete, Politician, and Astronaut\u2014as our test domains. Additionally, we create a meta set $\\mathcal{D}_{m}$ by randomly selecting 50 examples in each of these domains from their WebNLG 2020 validation set. We then choose two distinct domains, Artist and City, for training. Artist, as another occupation-centric domain, shares a similar schema and vocabulary with the test domains, whereas City does not. Ideally, DPS should prioritize the Artist examples over those from City. The BLEU scores for text descriptions generated by the DPS methods are shown in Figure 12 (Appendix F). These results reinforce the findings from the original experimental setup, with BADS outperforming both $B L O$ and $C D S$ by over 10 BLEU scores. The weights given to the examples in each mini-batch are visualized in the bottom row of Figure 2. BADS effectively prioritizes the examples in the Artist domain. Instead, $B L O$ fails to differentiate between the two domains, and $C D S$ incorrectly weights the opposite domain higher. This illustrates that the effectiveness of DPS is linked to the overall performance of the models. ", "page_idx": 7}, {"type": "table", "img_path": "9f5tOXKoMC/tmp/15c83b8f6081742d40b59df2f587cecdacababfe6a91818a524535f4447c659f.jpg", "table_caption": [], "table_footnote": ["Table 1: The average GPU memory and time usage over 100 steps. \u201cBase\u201d represent all non-DPSs. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.4.2 Latency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the average GPU memory and time consumption using the WebNLG task. Table 1 shows that the training time for BADS and $C D S$ is similar to that of non-DPSs, while $B L O$ and AskLLM-O takes nearly twice as long, and ClassAct takes even longer. Note that CDS and AskLLM- $o$ requires additional time to weight examples in the train set. Given that the WebNLG 2020 train set consists of 35,426 examples, we execute the weighting with a batch size of 20, CDS takes a total of 700 seconds. For larger-scale tasks, such as foundation model fine-tuning (Section 4), the weighting process is estimated to consume approximately 22 hours per 1 million training examples. The offilne scoring in AskLLM- $o$ takes around four hours in our setup on a single NVIDIA A40 GPU. In terms of memory usage, CDS aligns with non-DPS approaches, while $B A D S$ and $B L O$ require approximately 1.5 and 2.5 times more GPU memory, respectively. ClassAct and AskLLM-O takes even more GPU memory. ", "page_idx": 8}, {"type": "text", "text": "4 Use Case: Large Language Model Instruction Fine-tuning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Instruction Fine-tuning (IFT) for LLMs is a practical application where all three mentioned scenarios are encountered simultaneously. IFT data can be acquired through prompting LLMs [52, 48, 56], gathering existing Natural Language Processing (NLP) benchmarks [35, 54, 50, 43, 53, 38, 56], or employing human annotation [52, 63, 48]. Noise is likely to accumulate during each of these data collection methods. Furthermore, since NLP benchmarks often vary greatly in size, the IFT data typically lacks ", "page_idx": 8}, {"type": "table", "img_path": "9f5tOXKoMC/tmp/95a10b70a01bf8fd82947e311329a8728227fa4e4e45ca95759a0b403d031ec0.jpg", "table_caption": [], "table_footnote": ["Table 2: Test accuracy of LLMs across four popular benchmarks in eval-harness [17]. Checkpoint selection is using next token prediction accuracy as the selection metric. Mixing represents standard IFT. "], "page_idx": 8}, {"type": "text", "text": "balance. Additionally, the IFT data does not include data points from the downstream tasks, leading to domain shift in testing. ", "page_idx": 8}, {"type": "text", "text": "We use the same IFT data as [57, 51] as our train set $\\mathcal{D}_{t}$ , which is a mix of FLAN V2 [35], COT [54], DOLLY [10], and OPEN ASSISTANT 1 [31]. Following [57, 5], we focus on four downstream tasks: MMLU [24], which consists of multiple-choice questions across 57 sub-tasks, ARC-challenge/-easy [9], and HellaSwag [61]. Following [57], 5 examples were selected from each sub-task to create the meta set $\\mathcal{D}_{m}$ for MMLU, totaling 285 examples. Additionally, following [5], for the other tasks, we randomly chose 25 examples from their validation set to create the respective meta sets. To facilitate checkpoint selection, we additionally create a validation set of equivalent size to the meta sets. ", "page_idx": 8}, {"type": "text", "text": "Due to limited computational resources, we use OpenLLaMA 3B 14 as the backbone model. Training uses one A40 GPU utilizing Adam with a fixed learning rate of 3e-5 and a mini-batch size of 3. In BADS, while the weight network remains the same as described in scenario 3, we modify the input to be the average contextual embedding of all tokens in the sequence. The sparsity level $\\beta$ is 0.05. Results are presented in Table 2. We excluded $B L O$ from this experiment due to their prohibitive GPU memory usage. Given that the training data for the Mixing baseline predominantly consists of IFT data points, it is considered as a standard IFT process. Table 2 indicates that while individual methods may excel in specific tasks, none of the non-DPS baselines nor the $C D S$ and ClassAct consistently surpass the others across all downstream tasks. However, BADS stands out by consistently outperforming all other baselines across every task, except for AskLLM-O, which, as indicated in Table 1, demands significantly more computational resources. According to Table 4 (Appendix F) BADS prefers the human-written-from-scratch, OPEN ASSISTANT 1 and DOLLY, over the rest two created using existing benchmarks. ", "page_idx": 8}, {"type": "text", "text": "To facilitate the use of our proposed datapoint selection method, we provide comprehensive guidelines for hyperparameter tuning in Appendix E. We also include an in-depth discussion and ablation study on the influence of hyperparameters on the method\u2019s effectiveness. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recent works on DPS broadly fall into four categories: i) Approaches based on meta learning (or BLO), ii) Gradient-based methods, iii) Methods based on domain adaptation and transfer learning methods, and iv) General sample reweighing strategies. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Meta learning (BLO) approaches. The DPS can be formulated as a meta learning BLO problem where the outer loss is defined with the training data selection variables, and the inner optimisation is to minimize the model\u2019s loss with the selected data [19, 15, 41, 45, 62]. These methods vary in the choice of their outer optimization variables: either directly using the data point weights [41, 45, 19] or minibatch samplers [15]. To solve the BLO, most approaches rely on computing meta-gradients via reverse mode differentiation [45, 62] while some works utilised reinforcement learning techniques [15]. All these BLO-based methods are computationally demanding with large memory footprint, hindering them from being applied to large-scale models/data. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Gradient-based methods. The key idea is to measure the importance of training data points based on the alignment scores between the loss gradients on training and meta data points. The rationale behind the gradient alignment can be theoretically underpinned by the BLO perspective. As shown in [41], the one-step inner loss update with zero initial weights in BLO reduces to the gradient alignment (cosine angle) between the train and meta data points [57]. However, the method in [57] requires evaluating and storing gradients of the entire training data points, thus computationally expensive. Furthermore, the final solution may not be optimal since the gradient computation is done with an initial network that is warm-up trained with a random subset of the training data. In [16] the weights are optimised by the expected gradient norms during the network training, which serves as a proxy for the importance of data points. In [28] they find the subsets that closely match the gradient of the training and meta sets using an orthogonal matching pursuit algorithm. Under online continual learning setup [2], they formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Domain adaptation and transfer learning methods. Selecting a subset of the train set that are best aligned with the in-domain meta set can be naturally seen as domain adaptation and transfer learning. In the NLP community, [1] finds that the large language models implicitly learn sentence representations that cluster by domains without supervision, while in [20], they show the effectiveness of domain-adaptive pre-training with data augmented using simple data selection strategies. In [39] they proposed domain adaptive transfer learning which computes the importance weights on the target dataset from the ideas in domain adaptation and estimation of the label distribution. In multi-task transfer learning community, some previous works identify the detrimental effects of the gradients from different tasks, and propose strategies to mitigate the conflicting gradients issue [60, 11, 34]. ", "page_idx": 9}, {"type": "text", "text": "\u2022 General sample reweighing strategies. Since DPS essentially involves finding the optimal reweighed distribution with the underlying domain itself unchanged, several approaches aim to tackle the problem via importance sampling techniques. In [27] they derive a tractable upper bound to the per-sample gradient norm, and derive an estimator of the variance reduction achieved with importance sampling. The curriculum learning [3] is also closely related as one can design an optimal schedule of the successive training distributions. For instance, in [44] they introduce the so-called data parameters, associating samples and classes in the train data with learnable parameters, which governs their importance in the learning process within the curriculum learning framework. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We revisited the DPS problem from the perspective of Bayesian learning. By treating the neural network of interest, as well as an auxiliary weighting neural network as random variables and inferring their joint posterior using SGLD, we achieve a simple and effective approach to data reweighting that is more reliable and scalable than BLO alternatives. Our framework is straightforwardly applicable to learning with data imbalance, label noise, and automating auxiliary data curation. We demonstrate our framework can apply to automating curation of the wide variety of auxiliary instruction fine-tuning data available for billion-scale language models. Overall this demonstrates a promising new kind of approach to the growing need for data optimization in neural network learning. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Our proposed BADS algorithm has the following limitations, which we plan to address as our future work. ", "page_idx": 10}, {"type": "text", "text": "1. There are several hyperparameters involved, which need to be carefully tuned for best performance. These include: the sparsity level parameter $\\beta$ , the relative impact constants $\\rho$ \u2019s of the objective terms in our SGLD update (detailed in App. E), and the standard hyperparameters (e.g., batch size, learning rates, weight decay). Some of these hyperparameters may be optimised via Bayesian model selection in a principled manner. For instance, the sparsity level $\\beta$ can be regarded as a latent random variable (a part of the model) with a proper prior distribution imposed on it, and we can do posterior inference of $\\beta$ (or marginalise it out) together with $w$ and $\\theta$ in our SGLD update equations. We will pursue this in our future study. 2. Although our method is computationally far more efficient than BLO and some other DPS approaches, its GPU memory footprint is demanding compared to non-DPS algorithms. This mainly originates from the entire weight variables or the whole weight network parameters loaded/maintained in the memory for frequent updates. One possible workaround is to load only the weight variables that are assoicated with the current minibatches. We will be investigating this code optimisation further in our ongoing future study. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Roee Aharoni and Yoav Goldberg. Unsupervised domain clusters in pretrained language models. In Annual Meeting of the Association for Computational Linguistics, 2020.   \n[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances on Neural Information Processing Systems, 2019.   \n[3] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In International Conference on Machine Learning, 2019.   \n[4] David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz, and Sham Kakade. Color-filter: Conditional loss reduction filtering for targeted language model pre-training. arXiv preprint arXiv:2406.10670, 2024.   \n[5] Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: Instruction data selection for tuning large language models, 2024. URL https://openreview.net/forum?id=kce6LTZ5vY.   \n[6] Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. The 2020 bilingual, bi-directional WebNLG $^+$ shared task: Overview and evaluation results $(\\mathrm{WebNLG}+2020$ ). In International Workshop on Natural Language Generation from the Semantic Web (WebNLG $^+$ ), 2020.   \n[7] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. Maybe only $0.5\\%$ data is needed: A preliminary exploration of low training data instruction tuning. arXiv preprint arXiv:2305.09246, 2023.   \n[8] Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deep neural networks trained with noisy labels. In International Conference on Machine Learning, 2019.   \n[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. CoRR, abs/1803.05457, 2018.   \n[10] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open instruction-tuned LLM, 2023. URL https://www.databricks.com/blog/2023/04/12/ dolly-first-open-commercially-viable-instruction-tuned-llm.   \n[11] Lucio M. Dery, Yann Dauphin, and David Grangier. Auxiliary task update decomposition: The good, the bad and the neutral. In International Conference on Learning Representations, 2021.   \n[12] Ond\u02c7rej Du\u0161ek, Jekaterina Novikova, and Verena Rieser. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge. Comput. Speech Lang., 59(C):123\u2013156, jan 2020.   \n[13] Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier J Henaff. Bad students make great teachers: Active learning accelerates large-scale visual understanding. arXiv preprint arXiv:2312.05328, 2023.   \n[14] Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, and Olivier J Henaff. Data curation via joint example selection further accelerates multimodal learning. arXiv preprint arXiv:2406.17711, 2024.   \n[15] Yang Fan, Fei Tian, Tao Qin, Jiang Bian, and Tie-Yan Liu. Learning what data to learn. In arXiv, 2017.   \n[16] Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Mohammad Taher Pilehvar, Yadollah Yaghoobzadeh, and Samira Ebrahimi Kahou. Bert on a data diet: Finding important examples by gradient-based pruning. ArXiv e-prints: arXiv:2211.05610, 2022.   \n[17] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.   \n[18] Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG challenge: Generating text from RDF data. In International Conference on Natural Language Generation, 2017.   \n[19] David Grangier, Pierre Ablin, and Awni Hannun. Adaptive training distributions with scalable online bilevel optimization. arXiv preprint arXiv:2311.11973, 2023.   \n[20] Suchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In Annual Meeting of the Association for Computational Linguistics, 2020.   \n[21] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances on Neural Information Processing Systems, 2018.   \n[22] Haibo He and E.A. Garcia. Learning from imbalanced data. 21(9):1263 \u20131284, 2009.   \n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. URL http://arxiv.org/abs/1512.03385.   \n[24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021.   \n[25] A. Immer, M. Bauer, V. Fortuin, G. Ratsch, and M. E. Khan. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, 2021.   \n[26] Mihir Kale and Abhinav Rastogi. Text-to-text pre-training for data-to-text tasks. In International Conference on Natural Language Generation, 2020.   \n[27] Angelos Katharopoulos and Fran\u00e7ois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In International Conference on Machine Learning, 2018.   \n[28] Krishnateja Killamsetty, Durga S, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, Proceedings of Machine Learning Research, 2021.   \n[29] KrishnaTeja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh K. Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In AAAI, 2021.   \n[30] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. URL https://www.cs.toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n[31] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyf,i Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant conversations - democratizing large language model alignment. CoRR, abs/2304.07327, 2023.   \n[32] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[33] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \n[34] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. In Advances in Neural Information Processing Systems, 2021.   \n[35] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, 2023.   \n[36] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, 2015.   \n[37] Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. Coresets for robust training of deep neural networks against noisy labels. In NeurIPS, 2020.   \n[38] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Annual Meeting of the Association for Computational Linguistics, 2022.   \n[39] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V Le, and Ruoming Pang. Domain adaptive transfer learning with specialist models. In arXiv preprint, 2018. URL https://arxiv.org/ abs/1811.07056.   \n[40] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. In Advances on Neural Information Processing Systems, 2021.   \n[41] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to Reweight Examples for Robust Deep Learning. International Conference on Machine Learning, 2018.   \n[42] Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H Chi, James Caverlee, Julian McAuley, and Derek Zhiyuan Cheng. How to train data-efficient llms. arXiv preprint arXiv:2402.09668, 2024.   \n[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.   \n[44] Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste. Data parameters: A new family of parameters for learning a differentiable curriculum. In NeurIPS, 2019.   \n[45] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. In NeurIPS, 2019.   \n[46] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. TNNLS, 2022.   \n[47] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=UmvSlP-PyV.   \n[48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023.   \n[49] Wei Wang, Taro Watanabe, Macduff Hughes, Tetsuji Nakagawa, and Ciprian Chelba. Denoising neural machine translation training with trusted data and online data selection. In Conference on Machine Translation: Research Papers, 2018.   \n[50] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. SuperNaturalInstructions: Generalization via declarative instructions on $1600+$ NLP tasks. In Conference on Empirical Methods in Natural Language Processing, 2022.   \n[51] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources. In Neural Information Processing Systems - Datasets and Benchmarks Track, 2023.   \n[52] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Annual Meeting of the Association for Computational Linguistics, 2023.   \n[53] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.   \n[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances on Neural Information Processing Systems, 2022.   \n[55] Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In International Conference on Machine Learning, 2011.   \n[56] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Aji. LaMini-LM: A diverse herd of distilled models from large-scale instructions. In Conference of the European Chapter of the Association for Computational Linguistics, 2024.   \n[57] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. CoRR, abs/2402.04333, 2024.   \n[58] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting Influential Data for Targeted Instruction Tuning. arXiv preprint arXiv:2402.04333, 2024.   \n[59] Hu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data. In IEEE International Conference on Computer Vision, 2023.   \n[60] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems, 2020.   \n[61] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In ACL (1), pages 4791\u20134800. Association for Computational Linguistics, 2019.   \n[62] Zizhao Zhang and Tomas Pfister. Learning fast sample re-weighting without reward data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 725\u2013734, 2021.   \n[63] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Advances on Neural Information Processing Systems, 2023.   \n[64] Difan Zou, Pan Xu, and Quanquan Gu. Faster convergence of stochastic gradient langevin dynamics for non-log-concave sampling. Uncertainty in Artificial Intelligence (UAI), 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Posterior Derivation for Eq. 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "From the graphical model in Fig. 1, we exploit the conditional independence $D_{m}\\perp\\left(w,D_{t}\\right)\\mid\\theta$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\theta,w|D_{t},D_{m})=\\frac{p(\\theta,w,D_{m}|D_{t})}{p(D_{m}|D_{t})}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{p(D_{m}|D_{t})}\\cdot p(w)\\cdot p(\\theta,D_{m}|w,D_{t})}\\\\ &{\\qquad\\qquad\\quad=\\frac{1}{p(D_{m}|D_{t})}\\cdot p(w)\\cdot p(D_{m}|\\theta,w,D_{t})\\cdot p(\\theta|w,D_{t})}\\\\ &{\\qquad\\qquad\\quad=\\frac{1}{p(D_{m}|D_{t})}\\cdot p(w)\\cdot p(D_{m}|\\theta)\\cdot p(\\theta|w,D_{t})}\\\\ &{\\qquad\\qquad\\quad\\simeq\\operatorname*{\\mathcalP}(w)\\cdot p(D_{m}|\\theta)\\cdot p(\\theta|w,D_{t})}\\\\ &{\\qquad\\qquad\\quad\\propto\\operatorname*{\\mathcalP}(w)\\cdot p(D_{m}|\\theta)\\cdot p(\\theta|w,D_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In (13) we use $D_{m}\\perp\\left(w,D_{t}\\right)\\mid\\theta$ , and in (14) $\\frac{1}{p(D_{m}|D_{t})}$ is regarded as constant for it has nothing to do with and $w$ . ", "page_idx": 14}, {"type": "text", "text": "B Posterior Derivation for Weight Network Cases ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we provide posterior derivation for the weight network case $w_{i}=w(z_{i}^{t};\\phi)$ . First, the weights become a deterministic function of $D_{t}$ and $\\phi$ (here $\\phi=$ weight network parameters) as shown in Fig. 6(a). But since $w$ is a deterministic function of $\\phi$ and $D_{t}$ , we can simplify it by having $w$ absorbed into $\\phi$ while introducing conditional dependence (an arrow) from $D_{t}$ to $\\phi$ , as depicted in Fig. 6(b). Note that $D_{t}$ is always given, we treat $\\phi$ as a random variable, and wherever $w_{i}$ appears, we replace it by $w(z_{i}^{t};\\phi)$ . More specifically, we make the following changes to the equations in the weight net scenario: ", "page_idx": 14}, {"type": "equation", "text": "$\\begin{array}{r}{p(\\theta|\\phi,D_{t})\\propto p(\\theta)\\cdot\\prod_{i=1}^{N_{t}}p(w(z_{i}^{t};\\phi),z_{t}^{i}|\\theta)}\\end{array}$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Eq. 4 (with detailed derivations): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p(\\theta,\\phi|D_{t},D_{m})=\\frac{p(\\theta,\\phi,D_{m}|D_{t})}{p(D_{m}|D_{t})}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}=\\frac{1}{p(D_{m}|D_{t})}\\cdot p(\\phi|D_{t})\\cdot p(\\theta,D_{m}|\\phi,D_{t})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{=\\frac{1}{p(D_{m}|D_{t})}\\cdot p(\\phi|D_{t})\\cdot p(D_{m}|\\theta,\\phi,D_{t})\\cdot p(\\theta|\\phi,D_{t})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{\\displaystyle\\propto\\cdot p(\\phi|D_{t})\\cdot p(D_{m}|\\theta)\\cdot p(D_{m}|\\theta)\\cdot p(\\theta|\\phi,D_{t})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\theta,\\phi]\\ \\leftarrow\\ |\\theta,\\phi]+\\frac{\\eta}{2}\\lor_{\\theta,\\phi}\\log p(\\theta,\\phi|{\\mathcal D}_{t},{\\mathcal D}_{m})+\\epsilon\\sqrt{\\eta},\\quad\\epsilon\\sim\\mathcal{N}\\left(0,I\\right)}\\\\ &{\\theta\\ \\leftarrow\\ \\theta+\\frac{\\eta}{2}\\nabla_{\\theta}\\Big(\\log p(\\theta)-N_{t}.\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\left[w(z_{i}^{t};\\phi)\\cdot l(z_{i}^{t};\\theta)\\right]-N_{m}.\\mathbb{E}_{j\\sim\\mathcal{B}_{m}}\\left[l(z_{j}^{m};\\theta)\\right]\\Big)+\\epsilon_{\\theta}\\sqrt{\\eta}}\\\\ &{\\phi\\ \\leftarrow\\ \\phi+\\frac{\\eta}{2}\\nabla_{\\phi}\\Big(\\log p(\\phi|{\\cal D}_{t})-N_{t}.\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\left[w(z_{i}^{t};\\phi)\\cdot l(z_{i}^{t};\\theta)\\right]\\Big)+\\epsilon_{\\phi}\\sqrt{\\eta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/7465eacf492788b759f7457807d509c4db60fd953d73bb82b7f6382f7fc7c331.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Graphical models when the weight network is adopted. (a) One possible representation. (b) More simplified representation by absorbing $w$ into $\\phi$ using deterministic $w=$ weightnet $(z_{i}^{t};\\phi)$ . See texts for details. ", "page_idx": 15}, {"type": "text", "text": "C Convergence Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Assumption C.1 (Adjusted from Assumption 4.3 in [64]). There exists $m>0$ and $b\\geq0$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{\\theta,w}\\log p(\\theta,w|D_{t},D_{m}),\\left[\\theta\\right]\\right\\rangle\\geq m\\Big\\|\\left[\\theta\\right]\\Big\\|_{2}^{2}-b\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for any $\\theta,w$ . ", "page_idx": 15}, {"type": "text", "text": "Assumption C.2 (Adjusted from Assumption 4.4 in [64]). Any minibatch gradient of the logposterior is Lipschitz continuous. That is, there exists a constant $L$ such that for any $z_{i}^{t}\\in D_{t}$ and $z_{j}^{m}\\in D_{m}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg\\|\\nabla_{\\theta,w}\\Big(\\log p(w)+\\log p(\\theta)-N_{t}w_{i}l(z_{i}^{t};\\theta)-N_{m}l(z_{i}^{m};\\theta)\\Big)-}\\\\ &{\\qquad\\nabla_{\\theta^{\\prime},w^{\\prime}}\\Big(\\log p(w^{\\prime})+\\log p(\\theta^{\\prime})-N_{t}w_{i}l(z_{i}^{t};\\theta^{\\prime})-N_{m}l(z_{i}^{m};\\theta^{\\prime})\\Big)\\bigg\\|_{2}\\le L\\bigg\\|\\left[\\theta\\right]-\\left[\\theta^{\\prime}\\right]\\bigg\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for any $\\theta,w,\\theta^{\\prime},w^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem C.3 (Adjusted from Theorem 4.5 in [64]). Let $d=\\dim(\\theta)+N_{t},$ , $B$ be the batch size, and $\\rho$ be the Cheeger constant. For any $\\epsilon\\in(0,1)$ , with the initial iterate satisfying $p{\\left(\\left\\|\\begin{array}{l}{\\left\\lceil\\theta^{i n i t}\\right\\rceil}\\\\ {w^{i n i t}}\\end{array}\\right\\|\\right)}\\leq$ $R/2\\bigg)\\leq\\epsilon/16$ for $R=\\overline{{R}}(\\epsilon K^{-1}/12),$ , and step size $\\eta=\\tilde{O}(\\operatorname*{min}\\{\\rho^{2}d^{-2},B^{2}\\rho^{2}d^{-4}\\})$ , the distribution $\\mu_{K}^{S G L D}$ of the $K$ -th iterate in our SGLD iterations Eq. (6\u20137) satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mu_{K}^{S G L D}-p(\\theta,w|D_{t},D_{m})\\right\\|_{T V}\\leq\\lambda(1-C_{0}\\eta)^{K}+B^{-1}C_{1}\\eta^{1/2}+C_{2}\\eta^{1/2}+\\epsilon/2}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some constant $\\lambda>0$ , $C_{0}=\\tilde{O}(\\rho^{2}),\\,C_{1}=\\tilde{O}(R d\\rho^{-1}),\\,C_{2}=\\tilde{O}(d\\rho^{-1})$ . Here $\\Vert\\cdot\\Vert_{T V}$ stands for the total variation distance, and $\\overline{{R}}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{R}}(z)=\\operatorname*{max}\\bigg\\{\\frac{625d\\mathrm{log}(4/z)}{m},\\frac{4d\\mathrm{log}(4L/m)+4b}{m},\\frac{4d+8\\sqrt{d\\mathrm{log}(1/z)}+8\\mathrm{log}(1/z)}{m}\\bigg\\}^{1/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Details of the Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "WebNLG 2020 We use the release_v3.0_en version of WebNLG benchmark. Domains in training set are: Building, Astronaut, City, University, MeanOfTransportation, SportsTeam, Food, Artist, Company, ComicsCharacter, Monument, Airport, Politician, Athlete, WrittenWork, CelestialBody. Domains in test set are MusicalWork, Scientist, Film. ", "page_idx": 15}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/82f76484332ed0c5e9b7a71fc346a5a4fb44b26bcba7f2bd89cb4b21f6638c44.jpg", "img_caption": ["Figure 7: An example of Natural Language Generation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Details of the Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Hyperparameters Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To enhance flexibility in managing the training process, we incorporate $\\rho_{\\theta}^{t},\\rho_{\\theta}^{m}$ , and $\\rho_{w}^{t}$ into $\\mathrm{Eq}\\,7$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\theta\\,\\,\\gets\\,\\,\\theta+\\frac{\\eta}{2}\\nabla_{\\theta}\\Big(\\log p(\\theta)-\\rho_{\\theta}^{t}N_{t}\\cdot\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\big[w_{i}\\cdot l(z_{i}^{t};\\theta)\\big]-\\rho_{\\theta}^{m}N_{m}\\cdot\\mathbb{E}_{j\\sim\\mathcal{B}_{m}}\\big[l(z_{j}^{m};\\theta)\\big]\\Big)+\\epsilon_{\\theta}\\sqrt{\\eta}}\\\\ {\\displaystyle w\\,\\,\\gets\\,\\,w+\\frac{\\eta}{2}\\nabla_{w}\\Big(\\log p(w)-\\rho_{w}^{t}N_{t}\\cdot\\mathbb{E}_{i\\sim\\mathcal{B}_{t}}\\big[w_{i}\\cdot l(z_{i}^{t};\\theta)\\big]\\Big)+\\epsilon_{w}\\sqrt{\\eta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The training hyperparameters for BADS and CDS are shown in the table below. Other hyperparameters have been shown in the main paper. ", "page_idx": 16}, {"type": "table", "img_path": "9f5tOXKoMC/tmp/6f28ebb5324e73940e6eb65c23b5abec5021f429b74d4994001f42fba2cce026.jpg", "table_caption": ["Table 3: Hyperparameters for all experiments. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.2 Guildline for Hyperparameters Tuning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "There are two primary sets of hyperparameters in BADS: ", "page_idx": 16}, {"type": "text", "text": "\u2022 hyperparameters for model\u2019s parameters update: $\\eta,\\epsilon_{\\theta},\\epsilon_{w},\\rho_{\\theta}^{t},$ , $\\rho_{\\theta}^{m}$ , and $\\rho_{w}^{t}$ (Eq. 24-25), \u2022 hyperparameters for the prior distribution of the example weights, which are \u03c3, \u03b2, $s_{a v g}$ (Eq. 8-9). ", "page_idx": 16}, {"type": "text", "text": "In general, ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 we set $\\eta$ to 1 and kept the Gaussian noise small with $\\epsilon_{\\theta}$ and $\\epsilon_{w}$ equal to 1e-5;   \n\u2022 $\\rho_{w}^{t}$ is set to 1;   \n\u2022 we set $\\rho_{\\theta}^{m}$ to 1 and primarily adjust $\\rho_{\\theta}^{t}$ ;   \n\u2022 in most cases, $\\rho_{\\theta}^{t}$ is simply set to 1. However, if the training set contains noise\u2014where the ground truth labels might be incorrect\u2014the loss from the training examples, particularly in the early stages of training, may be unreliable. In such cases, we decrease $\\rho_{\\theta}^{t}$ ;   \n\u2022 $s_{a v g}$ should not be too large, as it may incorporate outdated weights from earlier training steps. We set it to 10;   \n\u2022 we select $\\beta$ based on the proportion of training data we consider beneficial for the downstream tasks. For the LLMs Instruction Fine-tuning experiments, we adopt the same ratio used in the previous studies [52, 46].   \n\u2022 $\\sigma$ controls how tightly the weights should match $\\beta$ . We set $\\sigma$ based on our confidence in the selection ratio $\\beta$ : a smaller $\\sigma$ indicates greater confidence in $\\beta$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 data denoising scenario is a bit special, $\\operatorname{Eq}\\,7$ shows that high losses from the training batch push the example weights $w$ toward 0. With a high noise rate $50\\%$ and $80\\%$ ), the training batch losses remain high throughout the training process. To prevent the weights $w$ from collapsing to 0, we use a high selection ratio $\\beta$ and a low $\\sigma$ . ", "page_idx": 17}, {"type": "text", "text": "E.3 Ablation study: Influence of the impact constants $\\sigma$ and sparsity level $\\beta$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Impact constant $\\sigma$ Higher $\\sigma$ causes the example weights $w$ to drift away from $\\beta$ , occasionally collapsing to 0, and may result in incorrect example weights (see Figure 8). The right column in Figure 11 shows that in all three proof-of-concept scenarios, the models achieve similarly good performance when $\\sigma$ is reduced to around $10^{-5}$ . ", "page_idx": 17}, {"type": "text", "text": "Sparsity level $\\beta$ In both WebNLG and MNIST scenarios, high $\\beta$ leads to incorrect example weights (see Figure 9). Conversely, due to the reason we mentioned above, in data denoising (CIFAR) scenario, lower $\\beta$ leads to incorrect weights. The left column in Figure 11 shows that the backbone models\u2019 performance significantly declines in both the WebNLG and MNIST scenarios when the example weights are incorrect. In the CIFAR scenario, the impact is less pronounced. ", "page_idx": 17}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/76a3c42593285cb37e27e099025bad0d9cb4b8d75223cdd267a922d07416ef9d.jpg", "img_caption": ["Figure 8: Varying the impact constant $\\sigma$ . The data denoising scenario is conducted on CIFAR10 with $50\\%$ noisy data, while the sparsity level $\\beta$ is maintained at 0.8, consistent with the main experiments. From left to right, $\\sigma$ decreases from 0.5 to $5\\times10^{-5}$ . When $\\sigma=0.5$ , the weighting network assigns higher weights to the noisy examples. At $\\sigma=0.05$ , it assigns similar weights to both noisy and clean examples. Only when $\\sigma$ is sufficiently low $\\langle\\leq5\\times10^{-5}$ ) does the model assign distinctly higher weights to the clean examples. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/cc1442f4637e07718564d5a3700bee8ee904bace20df277459730ac61e2ab474.jpg", "img_caption": ["Figure 9: Varying the sparsity level $\\beta$ . This efficient learning scenario is conducted on WebNLG training with two domains: City and Artists. Since Artists has greater overlap in schema and vocabulary with the downstream domains, the model should assign higher weights to examples from this domain. The impact constant $\\sigma$ is maintained at $1\\times10^{-5}$ , consistent with the main experiments. As $\\beta$ decreases from 0.8 to 0.05, the weighting network behaves as follows: at $\\beta=0.8$ , it assigns higher weights to the City domain; at $\\beta=0.2$ , it assigns similar weights to both domains; and at $\\beta=0.05$ , it assigns significantly higher weights to the Artists domain. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "F Analysis of Main Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Learning Curves Study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The experimental results discussed in Section 3.3.1 demonstrate that $B L O$ converges more slowly compared to other approaches. In this section, we aim to examine the learning curves of DPS approaches, as illustrated in Figure 11. The trends for each approach appear consistent across both symmetric and asymmetric noise experiments. The convergence rate of $B A D S$ aligns with that of nonDPS approaches, whereas $C D S$ converges faster than $B A D S$ , and $B L O$ converges significantly slower. In the asymmetric noise experiments, overftiting is less pronounced compared to the symmetric noise experiments, where after 50,000 training steps, the test accuracy for all approaches decreases. $B L O$ and $B A D S$ exhibit a notably slower rate of overfitting compared to other methods. Conversely, $C D S$ overfits more quickly than non-DPS approaches, resulting in significantly lower test accuracy even compared to the non-DPS methods. ", "page_idx": 17}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/8d97131caf505b8f7077fc5791178653af90d9a6e6396e18e45f73b3ba7784c9.jpg", "img_caption": ["Figure 10: Model\u2019s performance in the three proof-of-concept scenarios with different $\\beta$ and $\\sigma$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F.2 Individual Scalar Weights vs. Weight Network ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In [41], each training example is associated with a learnable scalar weight. The scalability issues of this method compared to the weight network used in $B A D S$ are detailed in Section 2.4. In this section, we examine the CIFAR-10 denoising task to assess performance differences that result from replacing the BADS weight network with the individual weights strategy described in [41]. From the top row of Figure 11, it is evident that the performance of $B A D S$ with scalar weight (referred to as ", "page_idx": 18}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/6ca93ee11b757787468c6b354b773e4054b2edf4f6d811e386c3165987e02134.jpg", "img_caption": ["Figure 11: This graph shows the test/train accuracy over 200K training steps. The $\\mathbf{X}$ -axis denotes the training steps, and the y-axis indicates the accuracy levels. The top row displays the testing accuracy, while the bottom row shows the training accuracy. In the left column all models are trained using train set contaminated by $50\\%$ symmetric noise. While, in the right column, the train set contaminated by $40\\%$ asymmetric noise. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Bayesian_per_sample in the legend) slightly surpasses that of the original BADS (labeled as Bayesian in the legend). ", "page_idx": 19}, {"type": "table", "img_path": "9f5tOXKoMC/tmp/9f5f6f444c0921feb87615a03091eea0e9371092f9d3f2c18aac553b020e0708.jpg", "table_caption": ["Table 4: The average scores IFT examples get from BADS. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/685c57b5d2bcf1c25acce8a1e3009b0b97010f04d33e2fd701bcf013fb9fa59b.jpg", "img_caption": ["Figure 12: Scenario 3, domain adaptation using WebNLG benchmark. This plot shows the BLEU scores on WebNLG benchmark. All DPS models are trained on 2 domains, Artist and City, and tested on other 3 domains \u2013 Athlete, Politician, and Astronaut. The x-axis represents the training steps, while the y-axis shows the evaluation metric, BLEU. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/000655e56703bb0e0cc7d1c39c24f7d83bd90d82083ab4b7de237b7e079c05bc.jpg", "img_caption": ["Figure 13: Proof-of-Concept experiment supplementary results. All plots illustrate the average weights of data points within mini-batches during the last 2000 training steps, with the $\\mathbf{X}$ -axis representing the training steps and the y-axis showing the average weights. Classes depicted in blue are expected to receive higher weights compared to those in red. The top row displays the MNIST experiments from scenario 1, the middle row shows the CIFAR experiments from scenario 2, and the bottom row features the WebNLG experiments from scenario 3. The left, middle, and right columns correspond to BADS, BLO, and CDS, respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "9f5tOXKoMC/tmp/203728862f864a7f4d5c5017d3761bf65e8b9ed6d841da467c2c4cf2890ccf16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 14: Proof-of-Concept experiment supplementary results. All plots illustrate the average weights of data points within mini-batches during the training of ClassAct method, with the ${\\bf X}$ -axis representing the training steps and the y-axis showing the average weights. Classes depicted in blue are expected to receive higher weights compared to those in red. The left plot displays the MNIST experiments from scenario 1, the middle plot shows the CIFAR experiments from scenario 2, and the right plot features the WebNLG experiments from scenario 3. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have included our main claims in the abstract and introduction that accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed several limitations of the current work, together with workarounds and our plans to address them in our future work. Please see the Limitations section right before the References. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have described all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and conclusions of the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We plan to release code, with sufficient instructions to faithfully reproduce the main experimental results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper specifies all the training and test details necessary to understand the results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The results are accompanied by error bars wherever possible. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The information on the computer resources needed to reproduce the experiments is described in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper presents work whose goal is to advance the field of Machine Learning. At this point we do not see any negative impacts of the current work on society. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The authors cite the original papers that produced the code package or dataset under proper licenses. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]