[{"figure_path": "KHcB1drMRX/tables/tables_5_1.jpg", "caption": "Table 1: Multitask pretraining data for pre-training Chain-of-Sight. MeanL., 50%L., and 90%L. indicates the mean length, the 50 percentile, and the 90 percentile length of the input text tokens. We use the tokenizer from Vicuna [20], which is the same tokenizer we use for pre-training.", "description": "This table presents the dataset statistics used for pre-training the Chain-of-Sight model.  It shows the mean, 50th percentile, and 90th percentile lengths of the input text tokens for various tasks included in the multitask pre-training. The datasets used for each task (captioning, general visual question answering, knowledge-based question answering, text-based tasks, and referring expression comprehension) are listed. The tokenizer used is the same as in the Vicuna model.", "section": "3.1 Experimental setup"}, {"figure_path": "KHcB1drMRX/tables/tables_5_2.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table presents a comparison of the Chain-of-Sight model's performance against baseline models across various vision-language benchmarks.  The benchmarks include image captioning, visual question answering, text recognition, and several vision-language tasks.  The table shows performance metrics (e.g., accuracy scores) for different model configurations (with varying numbers of visual tokens during pre-training and fine-tuning) and compares them to existing methods.  The results highlight the model's performance and efficiency gains, even with significantly fewer visual tokens used during the pre-training stage.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_6_1.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table presents the results of image captioning, visual question answering, text recognition, and vision-language benchmark experiments.  The performance of the Chain-of-Sight model is compared against several baseline methods under different conditions (224x224 and 448x448 resolution fine-tuning, different numbers of tokens).  The use of * indicates that token extension was performed using existing strategies, and \u2020 indicates fine-tuning with a 224x224 resolution.  The best and second-best performing models are highlighted in bold and underlined, respectively.  The S-I column represents results from the image subset of the SEEDBench dataset.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_6_2.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table presents a comparison of the Chain-of-Sight model's performance against baseline models across various vision-language benchmarks.  The benchmarks cover image captioning, visual question answering, text recognition, and other vision-language tasks.  The results show performance with different numbers of visual tokens during fine-tuning, along with a comparison to the standard approach of using all visual tokens throughout the entire training process. The table also highlights the time savings achieved by Chain-of-Sight's pre-training acceleration.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_7_1.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table compares the performance of the Chain-of-Sight model against several baselines across a range of vision-language benchmarks, including image captioning, visual question answering, text recognition, and other vision-language tasks from the SEEDBench.  The results show the performance (measured using various metrics depending on the specific task) of each model under different configurations,  considering both the pre-training and fine-tuning phases. Noteworthy is that the table shows the impact of scaling up the token count during the fine-tuning phase. ", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_8_1.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table presents a comparison of the Chain-of-Sight model's performance against several baseline models across various vision-language benchmarks.  It shows the results for image captioning, visual question answering, and text recognition tasks. The table highlights the performance gains achieved by Chain-of-Sight, particularly when using a smaller number of visual tokens during pre-training but scaling them up during fine-tuning.  The impact of different resolutions (224x224 and 448x448) during fine-tuning is also shown. The use of existing token extension strategies is noted, and the best-performing models are clearly marked.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_15_1.jpg", "caption": "Table 1: Multitask pretraining data for pre-training Chain-of-Sight. MeanL., 50%L., and 90%L. indicates the mean length, the 50 percentile, and the 90 percentile length of the input text tokens. We use the tokenizer from Vicuna [20], which is the same tokenizer we use for pre-training.", "description": "This table presents the dataset statistics used for pre-training the Chain-of-Sight model. It lists the mean, 50th percentile, and 90th percentile lengths of the input text tokens for various tasks, including captioning, visual question answering, knowledge-based question answering, and referring expression comprehension. The datasets used for each task are also specified.  The tokenizer used is the same as the one used in the pre-training phase of the model.", "section": "3.1 Experimental setup"}, {"figure_path": "KHcB1drMRX/tables/tables_16_1.jpg", "caption": "Table 2: Image captioning, visual question answering, text recognition, and vision-language benchmarks, compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. S-I denotes the image subset of SEEDBench [43]. The best and second-best performances are marked bold and underlined.", "description": "This table presents a comparison of the Chain-of-Sight model's performance against baseline models across various vision-language benchmarks.  It shows results for image captioning, visual question answering, text recognition, and other vision-language tasks.  The table highlights the performance gains achieved by Chain-of-Sight, particularly when using different numbers of tokens and resolutions during fine-tuning.  The use of existing token-extension strategies is also noted.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_16_2.jpg", "caption": "Table 3: Ablations on referring expression comprehension compared with our baselines. \u2020 indicates fine-tuning with 224\u00d7224 resolution. * denotes token extended through existing strategies [57, 50, 55]. The best and second-best performances are marked bold and underlined.", "description": "This table presents the ablation study results on referring expression comprehension (REC) task. It compares the performance of Chain-of-Sight (CoS) with baseline methods under different settings of visual token numbers and resolutions. The results demonstrate the impact of different factors such as number of visual tokens, fine-tuning resolution, and usage of existing strategies on the final REC performance.", "section": "3.2 Ablations"}, {"figure_path": "KHcB1drMRX/tables/tables_17_1.jpg", "caption": "Table A4: Further empirical results. LR: Logic Reasoning, AR: Attribute Reasoning, RR: Relation Reasoning, FP-S: Fine-grained Perception (Single-instance), FP-C: Fine-grained Perception (Cross-instance), CP: Coarse Perception.", "description": "This table presents further empirical results obtained from the experiments conducted in the paper. It compares the performance of two models, CoS-7B and CoS-8B, across various vision-language benchmarks. The benchmarks are categorized into three groups: Regular, MMBench, and Other. Each group contains multiple tasks, with performance measured using different metrics depending on the task.  The abbreviations used in the column headers indicate the specific benchmarks or aspects being evaluated (e.g., OK for OK-VQA, COCO for COCO Captions). The table aims to provide a comprehensive evaluation of the models' performance across diverse and challenging tasks.", "section": "Further results"}]