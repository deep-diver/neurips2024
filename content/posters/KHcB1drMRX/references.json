{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that significantly influenced the development of multimodal LLMs and is directly relevant to the current work."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-01", "reason": "This paper introduces Qwen-VL, a strong competitor to existing multimodal LLMs, providing a comparative baseline for evaluating the proposed Chain-of-Sight method."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "BLIP-2 is a key prior work in multimodal LLMs that this paper builds upon, making it a crucial reference for understanding the context of the research."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This foundational paper on large language models (LLMs) is highly influential, demonstrating the capacity for few-shot learning that is also relevant in the multimodal context discussed."}, {"fullname_first_author": "Xi Chen", "paper_title": "Pali-x: On scaling up a multilingual vision and language model", "publication_date": "2023-05-01", "reason": "This paper focuses on scaling multilingual vision-language models, addressing a similar challenge of efficient training which is a central theme in the current research."}]}