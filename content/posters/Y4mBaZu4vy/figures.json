[{"figure_path": "Y4mBaZu4vy/figures/figures_2_1.jpg", "caption": "Figure 1: Efficiency, performance, and scaling comparisons between ESCAIP and baseline models on the Open Catalyst dataset (OC20). Force MAE Error (meV/\u00c5 \u2193) vs. Inference Speed (Sample/Sec \u2191) and Force MAE vs. Memory (GB/Sample \u2193) is reported. Results with Energy MAE can be found in the Appendix Fig. 7. ESCAIP achieves better performance with smaller time and memory cost.", "description": "This figure compares the performance of ESCAIP with other neural network interatomic potential (NNIP) models on the Open Catalyst 20 dataset.  Two plots are shown: one illustrating the relationship between inference speed and force mean absolute error (MAE), and another showing the relationship between memory usage and force MAE.  Both plots demonstrate that ESCAIP achieves state-of-the-art performance with significantly improved efficiency (faster speed and lower memory usage) compared to other models.", "section": "1 Introduction"}, {"figure_path": "Y4mBaZu4vy/figures/figures_3_1.jpg", "caption": "Figure 2: Results of ablation study of EquiformerV2 [Liao et al., 2024] on the OC20 2M dataset. Energy (eV) and force (eV/\u00c5) mean absolute error (MAE) are reported, along with the model's parameter counts. The leftmost column shows the original results from [Liao et al., 2024], where different L had a different number of trainable parameters. We look at scaling parameters through the attention mechanisms (AT) and spherical channels (SC) for the original L = 2 and L = 4 models, such that the number of parameters is approximately equal to the original L = 6 model. Scaling parameters in different ways affects the overall energy and forces error, and increasing attention parameters is particularly effective in improving model performance (More AT). We also modify the architecture to be invariant (L = 0), allowing us to examine the effects of excluding rotational equivariance while controlling for the number of parameters (BOO). After controlling for parameter counts, many of the models have comparable error to the original L = 6 model.", "description": "This figure presents the results of an ablation study conducted on the EquiformerV2 model to investigate how different strategies for scaling model parameters affect the model's performance in predicting energy and forces. The study systematically varies the number of parameters in different parts of the model while keeping the total number of parameters roughly constant across different configurations. The results demonstrate that increasing the attention mechanism parameters is particularly effective in improving performance, while other scaling strategies may not be as effective.", "section": "Investigation on How to Scale Neural Network Interatomic Potentials"}, {"figure_path": "Y4mBaZu4vy/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of the Efficiently Scaled Attention Interatomic Potential (ESCAIP) model architecture. The model consists of B graph attention blocks (dashed box), each of which contains a graph attention layer, a feed forward layer, and two readout layers for node and edge features. The concatenated readouts from each block are used to predict per-atom forces and system energy.", "description": "This figure illustrates the architecture of the Efficiently Scaled Attention Interatomic Potential (ESCAIP) model.  The model comprises multiple graph attention blocks, each block sequentially processing node and edge features.  Each block includes a graph attention layer, a feed-forward network, and two readout layers which aggregate information to predict both the per-atom forces and total system energy. The input features are atomic numbers, radial basis expansion of pairwise distances, and bond orientational order (BOO).", "section": "4 Efficiently Scaled Attention Interatomic Potential (ESCAIP)"}, {"figure_path": "Y4mBaZu4vy/figures/figures_6_1.jpg", "caption": "Figure 4: Detailed illustration of the graph attention block. The input attributes are projected and concatenated into a large message tensor. The tensor is fed into an optimized multi-head self-attention computation, where the max number of neighbors dimension is the sequence length dimension.", "description": "This figure shows the detailed steps of the graph attention block in the ESCAIP model. It begins by projecting and concatenating input attributes into a large message tensor.  This tensor is then processed by a multi-head self-attention mechanism, optimized for GPU acceleration using custom Triton kernels. The attention mechanism is parallelized over each neighborhood, with the maximum number of neighbors determining the sequence length. Finally, the resulting messages are aggregated back to the atom level.", "section": "4 Efficiently Scaled Attention Interatomic Potential (ESCAIP)"}, {"figure_path": "Y4mBaZu4vy/figures/figures_14_1.jpg", "caption": "Figure 6: Force MAE vs.Training Dataset Size for EquiformerV2 ablation study on the OC20 2M dataset. Slope is fitted by linear regression. We scale the parameters of the original L = 2 and L = 4 models from Liao et al. [2024] through the attention mechanisms and/or spherical channels, such that the number of parameters is approximately equal to the original L = 6 model. As the training dataset size increases, the scaled L = 2 and L = 4 models have a steeper slope, indicating faster performance improvement with increasing data.", "description": "This figure shows the results of an ablation study on the EquiformerV2 model, investigating the effect of scaling parameters through attention mechanisms and spherical channels on model performance with varying training dataset sizes.  It demonstrates that scaling parameters in attention mechanisms leads to faster performance improvements with larger datasets compared to scaling spherical channels or using the original model.", "section": "3.1 Optimal Components for Scaling Neural Network Interatomic Potentials"}, {"figure_path": "Y4mBaZu4vy/figures/figures_15_1.jpg", "caption": "Figure 1: Efficiency, performance, and scaling comparisons between ESCAIP and baseline models on the Open Catalyst dataset (OC20). Force MAE Error (meV/\u00c5 \u2193) vs. Inference Speed (Sample/Sec \u2191) and Force MAE vs. Memory (GB/Sample \u2193) is reported. Results with Energy MAE can be found in the Appendix Fig. 7. ESCAIP achieves better performance with smaller time and memory cost.", "description": "This figure compares the efficiency, performance, and scalability of the proposed Efficiently Scaled Attention Interatomic Potential (ESCAIP) model against three other baseline models (EquiformerV2, eSCN, and GemNet-OC) on the Open Catalyst 20 dataset.  The plots show the relationship between force mean absolute error (MAE), inference speed, and memory usage for each model.  Lower MAE values are better, indicating higher accuracy; higher inference speed is better, indicating faster computation; and lower memory usage is better, indicating less resource consumption.  ESCAIP consistently outperforms the baseline models across all three metrics.", "section": "1 Introduction"}, {"figure_path": "Y4mBaZu4vy/figures/figures_15_2.jpg", "caption": "Figure 8: Scaling experiment of ESCAIP on OC20. Forces MAE (meV/\u00c5) and Energy (meV) across 4 validation splits are reported. For 500k, 1M, and 2M split, the ESCAIP model is trained for 30 epochs; for All+MD, the ESCAIP model is trained for 8 epochs. Force and Energy MAE consistently decreases as model size and training data size increases.", "description": "This figure shows the results of scaling experiments performed on the Open Catalyst 20 dataset (OC20) using the Efficiently Scaled Attention Interatomic Potential (ESCAIP) model.  It demonstrates how both force MAE (Mean Absolute Error) and energy MAE decrease as the model size and the amount of training data increase.  The different data sizes used for training are indicated: 500k, 1M, 2M, and the full All+MD dataset.  The number of training epochs also varies depending on the dataset size, with fewer epochs used for the larger datasets.  The consistent downward trend in MAE signifies that ESCAIP scales well with increased data and model size.", "section": "Experiments"}]