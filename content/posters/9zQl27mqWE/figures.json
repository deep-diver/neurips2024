[{"figure_path": "9zQl27mqWE/figures/figures_2_1.jpg", "caption": "Figure 1: For both plots, we train either using gradient descent or the self-consistent dynamics from equation (1), with the scaling \u03b3\u03c32 = -1.85, \u03b3\u03c9 = 2.25 which lies in the active regime. (Left panel): We plot train and test error for both dynamics. We observe that the train/test error for gradient descent is very close to the train/test error for the self-consistent dynamics. (Right panel): We plot with a solid line the singular values of Ae(t) when running the self-consistent dynamics, and use a dashed line for the singular values from running gradient descent. In this experiment, RankA* = 5. We use different colors for the 5 largest singular values and the same color for the remaining singular values. We can see how the 5 largest singular values \u2018speed up\u2019 as they cross the \u03c3\u00b2w threshold, allowing them to converge earlier than the rest. The minimal test error is achieved in the short period where the large singular values have converged but not the rest.", "description": "This figure compares the training and testing error curves obtained using gradient descent and the self-consistent dynamics proposed in the paper, when the scaling parameters are set to \u03b3\u03c32 = -1.85 and \u03b3\u03c9 = 2.25, which corresponds to the active regime. The left panel shows that both methods yield similar training and testing error curves. The right panel shows the evolution of singular values for Ae(t), where the five largest singular values converge faster once they cross the \u03c3\u00b2w threshold.", "section": "1.2 Previous Works"}, {"figure_path": "9zQl27mqWE/figures/figures_7_1.jpg", "caption": "Figure 2: As a function of \u03b3\u03c3\u00b2, \u03b3\u03c9, we run GD and plot different quantities. Our theoretical results only apply to the top left region for w > 1 and below the red line, although these plots suggest that some results may extend to smaller ws. (Top left panel): We plot the smallest test error ||A\u03b8(t) \u2212 A\u2217|| in the whole run. The active region (below the black line) has a small error while the lazy region does not. (Top right panel): We plot the stable rank of A\u03b8(t) (defined as ||A\u03b8(t)||F/||A\u03b8(t)||2F) at the time of minimal test error. In this experiment, we took RankA\u2217 = 5. We see that the active region has approximately the correct rank while the lazy region overestimates it. (Bottom left panel): We plot the number of iterations until minimal test error, illustrating the trade-off between test error and training time. (Bottom right panel): We compute ln(||A\u03b8(t) \u2212 A\u03b8(t)||F) where A\u03b8(t) comes from GD and A\u03b8(t) from the self-consistent dynamics. We observe that this distance is not only small for the region where our theoretical results apply but also almost everywhere outside this region.", "description": "This figure shows a phase diagram illustrating the performance of gradient descent in training shallow linear networks for a matrix factorization task.  The diagram shows four different plots, each visualizing a different metric as a function of the scaling of width (\u03b3\u03c9) and initialization variance (\u03b3\u03c3\u00b2). The plots depict the test error, the stable rank of the learned matrix, the number of iterations until the minimal test error is achieved, and the log of the maximal distance between the gradient descent and self-consistent dynamics. The diagram highlights the transitions between lazy and active regimes, revealing the trade-offs between training time and generalization performance in the different regions.", "section": "2 Phase Diagram for MSE"}]