[{"figure_path": "F6L23TNlFW/figures/figures_1_1.jpg", "caption": "Figure 1: An annotation from JAFFE dataset [18].", "description": "This figure illustrates the challenges of binary annotation in multi-label learning.  A facial image is shown, and the task is to determine which emotions are present.  While some emotions (e.g., 'Sad', 'Happy') are clearly absent, others (e.g., 'Surprise', 'Fear') present more ambiguity.  The annotation scheme shows how using binary labels (+1 or -1) can be inaccurate in such cases, making a 'ternary label' approach with a third option (0 for 'uncertain') much more accurate. This ambiguity highlights the motivation for using ternary labels instead of binary labels.", "section": "1 Introduction"}, {"figure_path": "F6L23TNlFW/figures/figures_4_1.jpg", "caption": "Figure 2: Distributions of the values of PEs,s[\u03c8(Is,Is)]\u2265E\u00f4,s[\u03c8(I\u00f4,Is)], E\u00f4,s,\u03be,\u03c1,\u03c4,\u03ba[\u03c8(I\u00f4,Is)], and E\u015d,s,\u03be,\u03c1,\u03c4,\u03ba[\u03c8(I\u015d,Is)] over [\u03c4,\u03ba], where E\u00f4,s,\u03be,\u03c1,\u03c4,\u03ba[\u03c8(I\u00f4,Is)] and E\u015d,s,\u03be,\u03c1,\u03c4,\u03ba[\u03c8(I\u015d,Is)] (which are defined in Equation (7)) measures the average EAE of binary labels and ternary labels, respectively; PEs,s[\u03c8(Is,Is)]\u2265E\u00f4,s[\u03c8(I\u00f4,Is)] is defined in Equation (8), which measures the approximate proportion of cases where the ternary label is inferior to the binary label for different [\u03c4,\u03ba,\u03be,\u03c1].", "description": "This figure presents a quantitative comparison of the approximation error between ternary and binary labels.  It shows the distributions of the expected approximation error (EAE) for both ternary and binary labels across various parameter ranges (\u03c4, \u03ba, \u03be, \u03c1). The figure also illustrates the approximate proportion of cases where the ternary label's EAE is greater than that of the binary label. This visually demonstrates the superiority of ternary labels in approximating the ground truth label description degrees.", "section": "3 Quantitative analysis"}, {"figure_path": "F6L23TNlFW/figures/figures_4_2.jpg", "caption": "Figure 3: The visualization of \u2191 \u2264 k and ( \u2212 1)\u00b2 + ( \u2212 1)\u00b2 \u2264 4", "description": "This figure visualizes the conditions derived in Theorem 1, which ensures the superiority of ternary labels over binary labels in approximating ground-truth label description degrees. It shows the overlapping area of conditions where ternary labels outperform binary labels, highlighting the superiority of the ternary labeling approach for improved accuracy in label distribution prediction. The x-axis and y-axis represent the parameters \u2191 and \u03ba which defines the boundary of intervals for label description degree z.", "section": "3 Quantitative analysis"}, {"figure_path": "F6L23TNlFW/figures/figures_5_1.jpg", "caption": "Figure 4: Fundamental frameworks of existing LE methods. y can be either binary labels or multi-label rankings. The learning target of discriminative LE methods can be decomposed as the inconsistency between y and the label description degrees z, i.e., Dist(z, y), and the regularization term of z based on other data, i.e., \u03a9(z). The joint distribution of the generation process in generative LE methods can be decomposed as the conditional probability of y given z, i.e., p(y|z), and the generative distributions of other observed variables, i.e., p(\u00b7\u00b7\u00b7|z).", "description": "This figure illustrates the fundamental frameworks of existing Label Enhancement (LE) methods. It shows two main approaches: discriminative and generative. Discriminative LE focuses on minimizing the inconsistency between easily accessible labels (y) and label description degrees (z), while also considering regularization terms (\u03a9(z)). Generative LE models the generation process by describing the joint distribution of labels (y), z, and other variables.  The figure highlights how different LE methods handle the relationship between y and z.", "section": "4 CateMO: Categorical distribution with monotonicity and orderliness"}, {"figure_path": "F6L23TNlFW/figures/figures_5_2.jpg", "caption": "Figure 4: Fundamental frameworks of existing LE methods. y can be either binary labels or multi-label rankings. The learning target of discriminative LE methods can be decomposed as the inconsistency between y and the label description degrees z, i.e., Dist(z, y), and the regularization term of z based on other data, i.e., \u03a9(z). The joint distribution of the generation process in generative LE methods can be decomposed as the conditional probability of y given z, i.e., p(y|z), and the generative distributions of other observed variables, i.e., p(\u00b7\u00b7\u00b7|z).", "description": "This figure illustrates the fundamental frameworks of existing Label Enhancement (LE) methods, categorizing them into discriminative and generative approaches.  Discriminative LE directly models the relationship between accessible labels (y) and label description degrees (z) using a loss function (Dist(z,y)) and regularization (\u03a9(z)). Generative LE, in contrast, models the joint probability distribution of y, z, and other variables, decomposing it into p(y|z), p(\u00b7\u00b7\u00b7|z), and p(z).", "section": "4 CateMO: Categorical distribution with monotonicity and orderliness"}, {"figure_path": "F6L23TNlFW/figures/figures_6_1.jpg", "caption": "Figure 5: p(s|z) on different parameters. The horizontal and vertical axes denote the label description degree z and the probability p(s|z) defined by Equation (9) and Equation (10), respectively.", "description": "This figure visualizes the probability distributions p(s|z) of the CateMO model for different parameter settings.  The x-axis represents the label description degree (z), which is a continuous value between 0 and 1 representing how strongly a label describes an instance. The y-axis shows the probability p(s|z), representing the probability of assigning each of the three ternary labels (-1, 0, 1) given the label description degree z.  Each subplot shows the probability curves for the three ternary labels for a different set of parameters (\u03bb, \u03bb, \u03bb, x), illustrating how the shape of the distribution changes depending on parameter values. The parameter \u03bb represents the weight for the negative label, \u03bb for the positive label, and \u03bb for the uncertain label. x is a parameter that influences the transition point between negative and positive labels.", "section": "4 CateMO: Categorical distribution with monotonicity and orderliness"}, {"figure_path": "F6L23TNlFW/figures/figures_9_1.jpg", "caption": "Figure 6: Cost-benefit analysis of different forms of labels. The horizontal and vertical axes denote the average annotating time (in seconds) and performance, respectively.", "description": "This figure displays a cost-benefit analysis comparing the use of binary labels versus ternary labels in a label distribution learning task.  The x-axis represents the average time spent annotating each label, while the y-axis shows the performance achieved using different label types.  The graph demonstrates that using ternary labels (which allows for uncertain annotations) results in superior performance and requires less annotation time compared to binary labels.  This highlights the effectiveness of the proposed ternary annotation approach, balancing both efficiency and accuracy.", "section": "5.3 Results and discussions"}, {"figure_path": "F6L23TNlFW/figures/figures_9_2.jpg", "caption": "Figure 7: Recovery performance of GL-CateMO with varying precision parameters.", "description": "This figure shows how the precision parameters affect the recovery performance of GL-CateMO on JAFFE, Painting, and Music datasets.  The recovery performance is computed in two steps: first, running GL-CateMO to recover label distributions; second, calculating KL divergence between recovered and ground-truth distributions.  For a given precision parameter, the other two are set to values resulting in best recovery performance. The x-axis represents the values of precision parameters, and the y-axis represents the KL divergence (lower is better).  Three subplots display results for each dataset.", "section": "5.4 Effect of precision parameters"}]