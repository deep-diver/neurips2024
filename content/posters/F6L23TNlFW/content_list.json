[{"type": "text", "text": "Predicting Label Distribution from Ternary Labels ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunan Lu, Xiuyi Jia\u2217 ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science and Engineering Nanjing University of Science and Technology, Nanjing 210094, China {luyn, jiaxy}@njust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Label distribution learning is a powerful learning paradigm to deal with label polysemy and has been widely applied in many practical tasks. A significant obstacle to the effective utilization of label distribution is the substantial expenses of accurate quantifying the label distributions. To tackle this challenge, label enhancement methods automatically infer label distributions from more easily accessible multi-label data based on binary annotations. However, the binary annotation of multi-label data requires experts to accurately assess whether each label can describe the instance, which may diminish the annotating efficiency and heighten the risk of erroneous annotation since the relationship between the label and the instance is unclear in many practical scenarios. Therefore, we propose to predict label distribution from ternary labels, allowing experts to annotate labels in a three-way annotation scheme. They can annotate the label as \u201c0\u201d indicating \u201cuncertain relevant\u201d if it is difficult to definitively determine whether the label can describe the instance, in addition to the binary annotation of \u201c1\u201d indicating \u201cdefinitely relevant\u201d and \u201c\u22121\u201d indicating \u201cdefinitely irrelevant\u201d. Both the theoretical and methodological studies are conducted for the proposed learning paradigm. In the theoretical part, we conduct a quantitative comparison of approximation error between ternary and binary labels to elucidate the superiority of ternary labels over binary labels. In the methodological part, we propose a Categorical distribution with monotonicity and orderliness to model the mapping from label description degrees to ternary labels, which can serve as a loss function or as a probability distribution, allowing most existing label enhancement methods to be adapted to our task. Finally, we experimentally demonstrate the effectiveness of our proposal. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LDL (Label Distribution Learning) [2] is an effective learning paradigm for addressing label polysemy (i.e., the cases where an instance can be described by multiple labels). Distinct from multi-label learning [22], which assign a binary-valued vector to each instance, LDL assigns each instance a real-valued vector, akin to descrete probability distributions, to represent the description degree of each label to the instance. Label distributions provide fine-grained information about label polysemy, and thus have been applied to many practical tasks, such as sentiment analysis [8, 30, 35], facial age estimation [1, 4, 24] and movie rating prediction [3]. ", "page_idx": 0}, {"type": "text", "text": "A fundamental bottleneck hindering LDL is the difficulty in acquiring ground-truth label distributions, as the accurate quantification of these distributions can be prohibitively expensive. Therefore, LE (Label Enhancement) [29] is proposed to automatically infer label distributions from the more easily accessible multi-label data. Multi-label data is based on binary annotations (i.e., utilizing binary values $\\pm1$ to annotate each label) which demand that experts accurately identify whether each label can describe the instance. However, accurate identification is challenging in real-world tasks due to the prevalence of ambiguous instances and labels which provoke uncertain binary associations between the instances and the labels. For example, the facial image in Figure 1 can be definitely (negatively or positively) associated to \u201chappy\u201d, \u201csad\u201d, and \u201csurprise\u201d, whereas the association with \u201cfear\u201d is uncertain, since it is not clear whether the emotion comes from being stimulated by something frightening. If experts are coerced into providing a definitive annotation in uncertain cases, not only does it diminish the efficiency of annotating, but it also heightens the risk of erroneous annotation. ", "page_idx": 0}, {"type": "image", "img_path": "F6L23TNlFW/tmp/133c39669898178c15a3e13af05f9ef25f06169f1433831533a1cb576f1f1a82.jpg", "img_caption": ["Figure 1: An annotation from JAFFE dataset [18]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Therefore, we propose to predict label distribution from ternary labels. Inspired by the philosophies of three-way decision or three-world thinking [31], where decision-makers are provided with the flexibility to delay judgment when the available information is inadequate to support a determined decision, ternary labels take values from $\\{-1,0,1\\}$ , where $\"\\pm1\"$ indicates whether the label can describe the instance, and $\\bullet\\bullet\\bullet$ denotes that the relationship between the label and the instance is uncertain. For the proposed learning paradigm, we conduct theoretical and methodological studies. In the theoretical part, we first quantify the errors of approximating the ground-truth label description vector 2 from ternary labels and binary labels, respectively. Further, we conduct a quantitative comparison of approximation error between ternary and binary labels to elucidate the superiority of ternary labels. In the methodological part, we propose CateMO distribution (Categorical distribution with Monotonicity and Orderliness) to model the mapping from label description degrees to ternary labels, which can serve as the probability distribution for generating ternary labels or as the loss function for measuring the inconsistency between the ternary labels and label distributions, allowing most existing LE methods to be adapted to our task. Specifically, we first analyze the rules governing the generation of ternary labels from label description degrees, and formalize them as the assumptions about the probabilistic monotonicity and orderliness of ternary labels. Further, we derive the probability mass function of CateMO to ensure probabilistic monotonicity and orderliness. Finally, we create two comparison algorithms and evaluate the prediction performance on three real-world datasets. Experimental results unequivocally affirm the superiority of ternary labels. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose to predict label distribution from ternary labels, which not only enhances the annotation accuracy but also significantly reduces the annotating cost when contrasted with the traditional binary annotating methods.   \n\u2022 We rigorously analyze the error of approximating the ground-truth label description degrees by ternary and binary labels, respectively, which provides a quantitative elucidation of the superior performance of the ternary label.   \n\u2022 We propose the CateMO distribution specifically designed to capture the mapping from label description degrees to ternary labels, which is theoretically constructed to maintain the monotonicity and ordinality of the probabilities associated with ternary labels. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To address the challenge of obtaining accurate ground-truth label distributions, LE (Label Enhancement) [29] is proposed as a method to automatically infer label distributions from more accessible multi-label data, including binary labels [29] and multi-label rankings [13, 15]. Existing works on LE can be broadly categorized into discriminative LE methods and generative LE methods. Discriminative LE directly treats the label distribution as conditional probabilities of the labels given feature observations, and subsequently mine additional information to estimate the conditional probabilities. Generative LE, on the other hand, focus on describing the generation process of observations in a principled manner and uncovering the underlying patterns of the observed data. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Current discriminative LE methods generally strive to optimize both $\\mathrm{Dist}(z,y)$ and $\\Omega(z)$ , where $\\mathrm{Dist}(z,y)$ measures the inconsistency between the label description degrees $_{z}$ and the more accessible labels $\\textit{\\textbf{y}}$ (including binary labels and multi-label rankings), and $\\Omega(z)$ is the regularization terms based on various sources of information. Typically, $\\mathrm{Dist}(z,y)$ is modeled using MSE (Mean Squared Error), and the information for $\\Omega(z)$ includes instance relationships and label correlations. For instance, several algorithms [5, 10, 20, 21, 23, 33, 34] assume that the instance manifolds based on features are similar to the instance manifolds based on label distributions. Additionally, several algorithms [11, 26, 29, 32] operate under the assumption that instances with similar feature vectors exhibit similarity in their label distributions. To incorporate label correlations, some algorithms [17, 25] regularize the label distributions using the Graph Laplacian operators of label correlation graphs. LEPNLR [6], on the other hand, enhances the labels by preserving the ranking relation of labels. Existing generative LE methods generally decompose the joint distribution of complete data as $p(\\pmb{y}|\\pmb{z})$ , $p(\\cdot\\cdot\\cdot|z)$ , and $p(z)$ , where $p(\\pmb{y}|\\pmb{z})$ models the relationship between $\\textit{\\textbf{y}}$ and $_{z}$ , and $p(\\cdot\\cdot\\cdot|z)$ captures the relationships between $_{\\textit{z}}$ and other observed variables. Current works primarily focus on modeling $p(\\pmb{y}|\\pmb{z})$ and $p(\\cdot\\cdot\\cdot|z)$ . For instance, LEVI [27, 28] and GLEMR [12] model $p(\\pmb{y}|\\pmb{z})$ as a Gaussian distribution and a ranking-preserved distribution, respectively. GLERB [16] and LEIC [14] model $p(\\cdot\\cdot\\cdot|z)$ by incorporating instance relationships and label correlations into generation processes. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, similar to ternary labels, the binary labels with missing values [17] are formally represented by $\\{0,\\pm1\\}$ . However, it should be emphasised that they inherently differ in the following aspects. First, they differ in the origin of the labels with value of 0, i.e., the missing labels in the literature [17] and the uncertain label in ternary labels. The missing labels stem from the \u201cabsence\u201d, i.e., the association between the label and the corresponding instance is undocumented or unannotated rather than undeterminable. By contrast, the uncertain labels stem from the \u201cuncertainty\u201d, i.e., it is difficult for experts to definitively determine whether the label is relevant to the corresponding instance. Second, they differ in the range of the underlying label description degree. The description degree of the missing labels to the corresponding instance can take any value in the interval [0, 1], since the missing labels may be relevant labels, irrelevant labels, or uncertain labels. By contrast, the description degree of the uncertain labels to the corresponding instance take values in a small sub-interval of $[0,\\bar{1}]$ , which is much tigher than the range of the description degrees of missing labels. ", "page_idx": 2}, {"type": "text", "text": "3 Quantitative analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an instance with feature vector of $\\textbf{\\em x}$ , the label description vector of the instance is denoted by $_{z}$ whose $m$ -th element $z_{m}\\in[0,1]$ indicates the description degree of the $m$ -th label to the instance $\\textbf{\\em x}$ . If an instance is annotated with binary labels, the label description vector will be degenerately expressed as a vector of binary values $\\pmb{b}\\in\\{-1,1\\}^{M}$ whose $m$ -th element $b_{m}$ denotes whether the $m$ -th label can describe the instance or not. If an instance is annotated with ternary labels, the label description vector will be degenerately expressed as a vector of ternary values $\\pmb{\\mathscr{s}}\\in\\{-1,0,1\\}^{M}$ where $s_{m}=1$ and $s_{m}=-1$ denotes that the $m$ -th label can describe and cannot describe the instance, respectively, and $s_{m}=0$ denotes that the association between the $m$ -th label and the instance $\\textbf{\\em x}$ is uncertain. ", "page_idx": 2}, {"type": "text", "text": "3.2 Approximation error analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To quantify the advantages of the ternary label over the binary label, we leverage EAE (Expected Approximation Error) [13, 15] which measures the error of approximating the true label description degree by a more accessible label such as binary label, multi-label ranking, or ternary label. The definition of EAE is formalized as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 Given a label with the true label description degree $z\\in{\\mathcal{T}}$ , if the label is annotated by a reduced label with the approximate label description degree $\\hat{z}\\in\\hat{\\mathcal{Z}}$ , then the expected approximation error of the reduced label, i.e., $\\psi(\\hat{{\\mathbb Z}},{\\mathbb Z})$ , is quantified as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi(\\hat{\\mathcal{Z}},\\mathcal{Z})=\\int_{z\\in\\mathcal{Z}}\\int_{\\hat{z}\\in\\hat{\\mathcal{Z}}}\\frac{1}{V\\hat{V}}(z-\\hat{z})^{2}\\mathrm{d}\\hat{z}\\mathrm{d}z,\\quad V=\\int_{z\\in\\mathcal{Z}}\\mathrm{d}z,\\quad\\hat{V}=\\int_{\\hat{z}\\in\\hat{\\mathcal{Z}}}\\mathrm{d}\\hat{z}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "According to Definition 1, it can be seen that EAE depends on $\\mathcal{T}$ and $\\hat{\\mathcal{Z}}$ , so below we propose the following assumptions to determine $\\mathcal{T},\\hat{\\mathcal{Z}}$ and their relationships. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 As stated in the introduction, a label in practical cases can be positive (i.e., the label can describe the instance), negative (i.e., the label cannot describe the instance), and uncertain (i.e., the association between the label and the instance is uncertain). If a label is positive, then the true label description degree $z\\in(\\kappa,1]$ . If the label is negative, then the true label description degree $z\\in[0,\\tau)$ , where $\\tau\\in[0,\\kappa]$ . If the label is uncertain, then the true label description degree $z\\in[\\tau,\\kappa]$ . That is, $z\\in\\mathcal{Z}_{s}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{s}=\\big\\{z:\\big(s=1\\wedge z\\in(\\kappa,1]\\big)\\vee\\big(s=-1\\wedge z\\in[0,\\tau)\\big)\\vee\\big(s=0\\wedge z\\in[\\tau,\\kappa]\\big)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If a label is annotated by a ternary label $\\hat{s}$ , then we have the $\\hat{s}$ -based label description degree $\\hat{z}\\in\\mathcal{T}_{\\hat{s}}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\hat{s}}=\\big\\{\\hat{z}:\\big(\\hat{s}=1\\wedge\\hat{z}\\in(\\hat{\\kappa},1]\\big)\\vee\\big(\\hat{s}=-1\\wedge\\hat{z}\\in[0,\\hat{\\tau})\\big)\\vee\\big(\\hat{s}=0\\wedge\\hat{z}\\in[\\hat{\\tau},\\hat{\\kappa}]\\big)\\big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $0\\leq\\hat{\\tau}\\leq\\hat{\\kappa}\\leq1$ are the predefined parameters as approximations to $\\tau$ and $\\kappa$ , respectively, since $\\tau$ and $\\kappa$ are unavailable from the annotation results. ", "page_idx": 3}, {"type": "text", "text": "If a label is annotated by a binary label ${\\hat{b}},$ , then we have the $\\hat{b}$ -based label description degree $\\hat{z}\\in\\mathcal{T}_{\\hat{b}}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\hat{b}}=\\{\\hat{z}:(\\hat{b}=-1\\wedge\\hat{z}\\in[0,\\hat{\\xi}))\\vee(\\hat{b}=1\\wedge\\hat{z}\\in[\\hat{\\xi},1])\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\xi}\\in[\\hat{\\tau},\\hat{\\kappa}]$ must hold, otherwise there exists some label description degree corresponding to \u201c $\\hat{b}=1\\wedge\\hat{s}=-1$ \u201d or $\\begin{array}{r}{\\langle\\hat{b}=-1\\wedge\\hat{s}=1^{\\prime}}\\end{array}$ , which is semantically contradictory. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 Since a label can be positive, negative or uncertain in practical cases, we discuss the relationships between the true label and the annotation results in each of the three cases below. ", "page_idx": 3}, {"type": "text", "text": "If a positive or negative label (i.e., $s=\\pm1$ ) is annotated by a ternary value $\\hat{s}$ or a binary value $\\hat{b}$ , then we have $\\hat{s}=\\hat{b}=s$ . ", "page_idx": 3}, {"type": "text", "text": "If an uncertain label (i.e., $s=0$ ) is annotated by a ternary value $\\hat{s}$ or a binary value $\\hat{b},$ , then we have $\\hat{s}=s$ , $p(\\hat{b}=-1|s=0)=\\rho$ , and $p(\\hat{b}=1|s=\\overset{.}{0})=1-\\overset{.}{\\rho},$ , where $0\\leq\\rho\\leq1$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we give the analytical form of EAE of the binary label and the ternary label. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 Given a label equiprobably being positive, negative, and uncertain, if the label is annotated by a ternary value, then we have the expected approximation error: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]=\\frac{2}{9}(\\tau+\\kappa)^{2}+\\frac{2}{9}(\\hat{\\tau}+\\hat{\\kappa})^{2}-\\frac{1}{6}(\\hat{\\tau}\\kappa+\\hat{\\kappa}\\tau)-\\frac{1}{3}(\\hat{\\tau}+\\kappa)(\\hat{\\kappa}+\\tau)+\\frac{1}{18}(1-\\kappa-\\hat{\\kappa}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If the label is annotated by a binary value, then we have the expected approximation error: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]=\\rho\\left(\\frac{\\tau+\\kappa}{6}-\\frac{1+\\hat{\\xi}}{9}\\right)+\\frac{2(\\tau+\\kappa)^{2}}{9}+\\frac{\\hat{\\xi}^{2}-\\hat{\\xi}\\tau-\\hat{\\xi}\\kappa-\\tau\\kappa}{3}-\\frac{3\\tau+4\\kappa-\\hat{\\xi}-3}{18},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho=p(\\hat{b}=-1|s=0)$ . Suppose that $\\hat{\\xi}\\sim\\mathrm{Uni}(\\hat{\\xi}\\mid\\hat{\\tau}\\leq\\hat{\\xi}\\leq\\hat{\\kappa})$ , $\\rho\\sim\\mathrm{Uni}(\\rho\\mid0\\le\\rho\\le1)$ , $[\\tau,\\kappa]\\sim\\mathrm{Uni}([\\tau,\\kappa]\\mid0\\leq\\tau\\leq\\kappa\\leq1)$ ), we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\hat{s},s,\\tau,\\kappa}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]=36^{-1}(8\\hat{\\kappa}^{2}+4\\hat{\\tau}\\hat{\\kappa}-12\\hat{\\kappa}+8\\hat{\\tau}^{2}-8\\hat{\\tau}+7),}\\\\ &{\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]=16^{-1}(2\\hat{\\kappa}^{2}+2\\hat{\\tau}\\hat{\\kappa}-3\\hat{\\kappa}+2\\hat{\\tau}^{2}-3\\hat{\\tau}+3).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "F6L23TNlFW/tmp/9e35d3ea33e925cec5e83aac1dfce0279fa1b34f7731e2747df9330b231a013a.jpg", "img_caption": ["Figure 2: Distributions of the values of $\\tilde{\\mathbb{P}}_{\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathcal{T}_{\\hat{s}},\\mathcal{T}_{s})]\\geq\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{T}_{s})]}$ , $\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{T}_{s})]$ , and $\\mathbb{E}_{{\\hat{s}},s,{\\hat{\\xi}},\\rho,\\tau,\\kappa}[\\psi({\\mathcal{T}}_{\\hat{s}},{\\mathcal{T}}_{s})]$ over $[\\hat{\\tau},\\hat{\\kappa}]$ , where $\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{T}_{s})]$ and Es\u02c6,s,\u03be\u02c6,\u03c1,\u03c4,\u03ba[\u03c8(Is\u02c6, Is)] (which are defined in Equation (7)) measures the average EAE of binary labels and ternary labels, respectively; $\\tilde{\\mathbb{P}}_{\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathcal{T}_{\\hat{s}},\\mathcal{T}_{s})]\\geq\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{T}_{s})]}$ is defined in Equation (8), which measures the approximate proportion of cases where the ternary label is inferior to the binary label for different $[\\tau,\\kappa,\\hat{\\xi},\\rho]$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The detailed proof of Theorem 1 can be found in Appendix. Figure 2 visualizes the results of Theorem 1. Specifically, Figure 2(a) shows the relationship between the predefined parameters $[\\hat{\\tau},\\hat{\\kappa}]$ and $\\tilde{\\mathbb{P}}_{\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathcal{T}_{\\hat{s}},\\mathcal{T}_{s})]\\geq\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{T}_{s})]}$ which is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\mathbb{P}}_{\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]\\geq\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]}=\\frac{1}{|\\mathcal{G}|}{\\sum_{(\\tau,\\kappa,\\hat{\\xi},\\rho)\\in\\mathcal{G}}}\\mathbb{I}(\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]\\geq\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]),}\\\\ &{}&{\\mathcal{G}=\\{(\\tau,\\kappa,\\hat{\\xi},\\rho):\\kappa\\in r(1,10^{-3}),\\tau\\in r(\\kappa,10^{-3}),\\hat{\\xi}\\in r(1,10^{-3}),\\rho\\in r(1,10^{-3})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{I}(\\cdot)$ is an indicator function that outputs 1 if the internal condition is true, and 0 otherwise; $r(u,v)$ outputs an increasing sequence $[0,v,2v,\\ldots,u]$ . Equation (8) measures the approximate proportion of cases where the ternary label is inferior to the binary label for all possible $[\\tau,\\kappa,\\hat{\\xi},\\rho]$ in $\\mathcal{G}$ . ", "page_idx": 4}, {"type": "text", "text": "Figure 2(b) and Figure 2(c) show the distributions of the average EAE of the binary label and ternary label, respectively. Obviously, the ternary label outperforms the binary label in most cases. Specifically, the binary label shows superiority only in the extreme cases, i.e., $\\left(\\dot{\\hat{\\tau}},\\hat{\\kappa}\\right)\\;\\in$ $\\{(0,0),(0,0.1),(0.9,1),(1,1)\\}$ , but $\\hat{\\tau}$ and $\\hat{\\kappa}$ basically never take these values in practical applications since both the binary and ternary labels exhibit very high approximation error in these cases. In addition, Theorem 1 also derives that $\\mathbb{E}_{\\hat{s},s,\\tau,\\kappa}[\\psi(\\mathcal{T}_{\\hat{s}},\\mathcal{Z}_{s})]~\\le~\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathcal{T}_{\\hat{b}},\\mathcal{Z}_{s})]$ if and only if $\\left(\\hat{\\kappa}-\\frac{3}{4}\\right)^{2}+\\left(\\hat{\\tau}-\\frac{1}{4}\\right)^{2}\\,\\leq\\,\\frac{3}{8}$ and $\\hat{\\tau}\\ \\leq\\ \\hat{\\kappa}$ , which is visualized as the overlapping area of the red and blue regions in Figure 3. It can be seen that the overlapping area is essentially consistent to the blue area in Figure 2(a). Therefore, the ternary label is superior to the binary label w.r.t. approximating the ground-truth label description degrees. ", "page_idx": 4}, {"type": "image", "img_path": "F6L23TNlFW/tmp/9e219756aa0b999f4ee06af63e99d8eb160891fb053afcc9a00091fcd99ffa2b.jpg", "img_caption": ["Figure 3: The visualization of $\\hat{\\tau}\\leq$ $\\hat{\\kappa}$ and $\\begin{array}{r}{\\left(\\hat{\\kappa}-\\frac{3}{4}\\right)^{2}+\\left(\\hat{\\tau}-\\frac{1}{4}\\right)^{2}\\leq\\frac{3}{8}}\\end{array}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 CateMO: Categorical distribution with monotonicity and orderliness ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In order to learn label distributions from ternary labels, existing LE methods can be borrowed, the fundamental frameworks of which have been visualized in Figure 4 and illustrated in Section 2. It can be observed that both discriminative and generative LE methods necessitate modeling the relationship between the label description degrees $_{\\textit{z}}$ and the more accessible labels $\\textit{\\textbf{y}}$ . Therefore, ", "page_idx": 4}, {"type": "image", "img_path": "F6L23TNlFW/tmp/0943e36a510b827adf2830eff647e5e3bd44f6f072aa6620bda3149bf16f292e.jpg", "img_caption": ["(a) Discriminative label enhancement "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "F6L23TNlFW/tmp/e0e8a91c813e735c1746ae806a807c5c986a28fadc235563b64b2b37a24141fb.jpg", "img_caption": ["(b) Generative label enhancement "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Fundamental frameworks of existing LE methods. $\\textit{\\textbf{y}}$ can be either binary labels or multi-label rankings. The learning target of discriminative LE methods can be decomposed as the inconsistency between $\\textit{\\textbf{y}}$ and the label description degrees $_{\\textit{z}}$ , i.e., $\\mathrm{Dist}(z,y)$ , and the regularization term of $_{\\textit{z}}$ based on other data, i.e., $\\Omega(z)$ . The joint distribution of the generation process in generative LE methods can be decomposed as the conditional probability of $\\textit{\\textbf{y}}$ given $_{\\textit{z}}$ , i.e., $\\displaystyle p(\\pmb{y}|\\pmb{z})$ , and the generative distributions of other observed variables, i.e., $p(\\cdot\\cdot\\cdot|z)$ . ", "page_idx": 5}, {"type": "text", "text": "we propose CateMO (Categorical distribution with Monotonicity and Orderliness) to model the conditional probability of ternary label given the label description degree. CateMO can serve as $p(\\pmb{y}|\\pmb{z})$ in generative LE methods, and the negative likelihood function of CateMO can be used as $\\mathrm{Dist}(z,y)$ in discriminative LE methods, so that most existing LE methods can be employed to address our task by replacing $\\mathrm{Dist}(z,s)$ and $p(s|z)$ with CateMO. In the following subsections, we first provide an intuitive discussion of the rules governing the generation of ternary labels and then formalize these rules as the assumptions about the probability monotonicity and orderliness of ternary labels. Furthermore, we derive the parametric mass function for CateMO, which is theoretically guaranteed to maintain the probability monotonicity and orderliness of ternary labels. ", "page_idx": 5}, {"type": "text", "text": "4.1 Generation rules of ternary labels ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "On the one hand, we explore how the label description degree affects the probabilities of the label being positive, negative and uncertain. Obviously, a label is more likely to be positive if the description degree of the label is larger; a label is more likely to be negative if the description degree of the label is smaller. Besides, the uncertain label is used to encode situations where the expert is unsure whether the label can describe the instance, which arises from the fact that the probability of the label being positive is close to the probability of the label being negative. Therefore, we believe that a label is more likely to be uncertain if the probabilities of the label being positive and negative are closer. The above intuitions can be formalized as follows: ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Probability monotonicity of ternary labels) Given any two real values $v_{1}$ and $v_{2}$ between 0 and 1, if $v_{1}<v_{2}$ , then $p(s=1|z=v_{1})<p(s=1|z=v_{2})$ and $p(s=-1|z=v_{1})>$ $p(s=-1|z=v_{2})$ . If $\\begin{array}{r}{\\mathrm{~`0<}p(s=1|z=v_{1})-p(s=-1|z=v_{1})<}\\end{array}$ $p(s=-1|z=v_{1})<p(s=1|z=v_{2})-p(s=$ $-1|z=v_{2},$ ) or $0<p(s=-1|z=v_{1})-p(s=1|z=v_{1})<p(s=1|z=v_{1})-p(s=v_{1})>p(s=1|z=v_{1})$ = \u22121|z = v2) \u2212p(s = 1|z = v2), then $p(s=0|z=v_{1})>p(s=0|z=v_{2})$ . ", "page_idx": 5}, {"type": "text", "text": "On the other hand, we explore how the label description degree affects the orderliness among the probabilities of the label being positive, negative and uncertain. Obviously, a label is most likely to be negative and least likely to be positive if the label description degree is sufficiently small; a label is most likely to be positive and least likely to be negative if the label description degree is sufficiently large; a label is most likely to be uncertain if the label description degree is moderate. We formalize the intuition as follows: ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Probability orderliness of ternary labels) There exists two real values $0\\leq v_{1}<$ $v_{2}\\leq1$ , $p(s=1|z)<p(\\bar{s}=0|z)<p(s=-1|\\bar{z})$ holds for any $z<v_{1}$ , $p(s=-1|z)\\,<p(s=$ $0|z)<p(s=1|z)$ holds for any $z>v_{2}$ , $\\operatorname*{max}\\{p(s=1|z),p(s=-1|z)\\}<p(s=0|z)$ holds for any $v_{1}<z<v_{2}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Probability mass function of CateMO distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since a ternary label can take three possible values, i.e., $s\\in\\{0,\\pm1\\}$ , we model ternary label by a categorical distribution, which can be formalized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(s|z)={\\mathrm{Categorical}}(s\\mid[\\underline{{{\\varphi}}}(z),\\varphi(z),\\overline{{{\\varphi}}}(z)]),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "F6L23TNlFW/tmp/be476ddd44ff8849979038418d59e9e571cfac6704896477564556706d9e23b2.jpg", "img_caption": ["Figure 5: $p(s|z)$ on different parameters. The horizontal and vertical axes denote the label description degree $z$ and the probability $\\bar{p}(s|z)$ defined by Equation (9) and Equation (10), respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "where $\\underline{{\\varphi}}(z)=p(s=-1|z)$ , $\\varphi(z)=p(s=0|z)$ , and $\\overline{{\\varphi}}(z)=p(s=1|z)$ represent the generation principles from the label description degree to the negative, uncertain, and positive labels, respectively. We name $\\underline{{\\varphi}}(z),\\,\\varphi(z)$ , and $\\overline{{\\varphi}}(z)$ as ternary generation functions. According to Assumption 3, we preliminarily assume the parametric form of ternary generation functions as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}(z)=\\frac{1}{Z}e^{-\\underline{{\\lambda}}z^{2}},\\quad\\varphi(z)=\\frac{1}{Z}e^{-\\lambda(z-\\hat{z})^{2}},\\quad\\overline{{\\varphi}}(z)=\\frac{1}{Z}e^{-\\overline{{\\lambda}}(z-1)^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $Z=e^{-\\underline{{{\\lambda}}}z^{2}}+e^{-\\lambda(z-\\hat{z})^{2}}+e^{-\\overline{{{\\lambda}}}(z-1)^{2}},\\underline{{{\\lambda}}}>0,\\overline{{{\\lambda}}}>0,\\nonumber\\,$ $\\lambda>0$ and $0<\\hat{z}<1$ are parameters. Intuitively, the parameters $(\\underline{{\\lambda}},\\overline{{\\lambda}},\\lambda)$ largely governs the precision of CateMO distribution (despite the fact that they are not exactly equal), which is similar to the reciprocal of the temperature coefficient in the softmax layer of a deep neural network. Hence, we refer to the parameters $\\underline{{\\lambda}},\\,\\overline{{\\lambda}}$ , and $\\lambda$ as the precision of negative labels, positive labels, and uncertain labels, respectively. Figure 5 visualizes the shape of $\\bar{p(s|z)}$ on different parameters, which can be found that some parameter configurations violate the above three assumptions. For example, Figure 5(a) violates Assumption 3, Figure 5(c) violates Assumption 3 and Assumption 4 at the same time. Therefore, we propose Theorem 2 to ensure that the ternary generation functions defined by Equation (10) satisfy the proposed assumptions about probability monotonicity and orderliness of ternary labels. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 Given $\\underline{{\\lambda}}>0$ , $\\overline{{\\lambda}}>0,$ , and $\\lambda>0$ , the ternary genertaion functions satisfy Assumption 3 and Assumption 4 if the following conditions hold: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\neq-\\underline{{\\lambda}}\\overline{{\\lambda}}(\\hat{z}\\overline{{\\lambda}}-\\hat{z}\\underline{{\\lambda}}-\\overline{{\\lambda}})^{-1},\\hat{z}=(2\\lambda\\sqrt{\\overline{{\\lambda}}}+2\\lambda\\sqrt{\\underline{{\\lambda}}})^{-1}(2\\lambda\\sqrt{\\overline{{\\lambda}}}-\\underline{{\\lambda}}\\sqrt{\\overline{{\\lambda}}}+\\overline{{\\lambda}}\\sqrt{\\underline{{\\lambda}}}),}\\\\ &{\\operatorname*{max}\\{(\\hat{z}+\\hat{z}e^{\\overline{{\\lambda}}})^{-1}\\overline{{\\lambda}},((1+e^{\\underline{{\\lambda}}})(1-\\hat{z}))^{-1}\\underline{{\\lambda}}\\}<\\lambda<\\operatorname*{min}\\{\\underline{{\\lambda}}(1-\\hat{z})^{-1},\\overline{{\\lambda}}\\hat{z}^{-1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The details of the proof can be found in Appendix. Therefore, the probability mass function of CateMO distribution can be formalized as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{CateMO}(s=1\\mid z)=Z^{-1}e^{-\\overline{{\\lambda}}(z-1)^{2}},\\mathrm{CateMO}(s=0\\mid z)=Z^{-1}e^{-\\lambda(z-\\widehat{z})^{2}},}\\\\ &{\\mathrm{CateMO}(s=-1\\mid z)=Z^{-1}e^{-\\lambda z^{2}},Z=e^{-\\lambda z^{2}}+e^{-\\lambda(z-\\widehat{z})^{2}}+e^{-\\overline{{\\lambda}}(z-1)^{2}},}\\\\ &{\\mathrm{s.t.}\\ \\widehat{z}=(2\\lambda\\sqrt{\\widehat{\\lambda}}+2\\lambda\\sqrt{\\overline{{\\lambda}}})^{-1}(2\\lambda\\sqrt{\\overline{{\\lambda}}}-\\underline{{\\lambda}}\\sqrt{\\overline{{\\lambda}}}+\\overline{{\\lambda}}\\sqrt{\\overline{{\\lambda}}}),\\lambda\\neq-\\underline{{\\lambda}}\\overline{{\\lambda}}(\\widehat{z}\\overline{{\\lambda}}-\\widehat{z}\\underline{{\\lambda}}-\\overline{{\\lambda}})^{-1},}\\\\ &{\\lambda,\\lambda,\\overline{{\\lambda}}>0,\\operatorname*{max}\\{\\left(\\widehat{z}+\\widehat{z}e^{\\overline{{\\lambda}}}\\right)^{-1}\\overline{{\\lambda}},((1+e^{\\lambda})(1-\\widehat{z}))^{-1}\\}<\\lambda<\\operatorname*{min}\\{\\lambda(1-\\widehat{z})^{-1},\\overline{{\\lambda}}\\widehat{z}^{-1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, to apply CateMO, we simply replace $\\mathrm{Dist}(y,z)$ in Figure 4(a) with the negative log-likelihood of CateMO and replace $p(\\pmb{y}|\\pmb{z})$ in Figure $4({\\mathsf{b}})$ with CateMO, which can be formalized as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Dist}(s,z)=-\\sum_{m=1}^{M}\\log\\mathrm{CateMO}(s_{m}\\mid z_{m}),\\quad p(s|z)=\\prod_{m=1}^{M}\\mathrm{CateMO}(s_{m}\\mid z_{m}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Datasets and evaluation measure ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Although there are many LDL datasets, they all lack ground-truth ternary label data. Therefore, we select three real-world LDL datasets (i.e., JAFFE [18], Painting [19], and Music [9]), and manually re-annotate them with both binary labels and the ternary labels. The details of these datasets can be found in Appendix. We use five common LDL metrics to evaluate the algorithm performance, which are Cheb (Chebyshev distance), KL (Kullback-Leibler divergence), Cosine (cosine coefficient) [2], and Rho (Spearman\u2019s rho coefficient) [7]. The lower values of Cheb and KL indicate the better performance. The higher values of Cosine, Intersec, and Rho indicate the better performance. We use \u201c\u2191\u201d and \u201c\u2193\u201d to denote that the better performance is represented by the higher and lower values of a metric, respectively. ", "page_idx": 7}, {"type": "text", "text": "5.2 Comparison methods and experimental procedures ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison methods Since there is no LE method specifically for ternary labels, we design two approaches to construct the comparison algorithms. On the one hand, we design a data transformation method (which is abbreviated as DT method) to transform the dataset with ternary labels into an extended dataset with binary labels so that any existing LE algorithm can be applied to ternary labels. Specifically, for any instance with uncertain labels, we transform the instance into two instances which set all uncertain labels to positive and negative labels, respectively. For instance, the example $[{\\pmb x},[1,0,0]]$ will be transformed into $[\\pmb{x},[1,1,1]]$ and $[{\\pmb x},[1,-1,\\bar{-}1]]$ , respectively. On the other hand, we replace the loss term $\\mathrm{Dist}(z,b)$ with MSE $\\lVert(\\pmb{\\mathscr{s}}+1)/2-\\pmb{z}\\rVert_{2}^{2}$ , so that most existing LE methods can be used to enhance ternary labels. We abbreviate this method as MSE method. Besides, we select three recently proposed binary label enhancement algorithms: GL [29], LR [6], and MR [12]. The hyperparameter settings follow their respective literature. We combine GL, LR and MR algorithms with DT and MSE methods in pairs to construct six comparison algorithms: GL-DT, GL-MSE, LR-DT, LR-MSE, MR-DT, and MR-MSE. In terms of our proposal, we replace the conditional distribution $p(b|z)$ in MR with CateMO and replace $\\mathrm{Dist}(z,b)$ in GL and LR with the negative log-likelihood function of CateMO, which constructs three algorithms: GL-CateMO, LR-CateMO, and MR-CateMO. We set the parameters $[\\underline{{\\lambda}},\\lambda,\\overline{{\\lambda}}]$ in CateMO as [49, 48, 12]. ", "page_idx": 7}, {"type": "text", "text": "Experimental procedures We aim to test the performance of label distribution prediction based on different LE algorithms. Specifically, we use different LE methods to recover training label distributions and use these recovered label distributions to train an LDL model, whose performance on test instances will be reported. LDL-LRR [7] is used as the LDL model in this paper, whose hyperparameters $\\lambda$ and $\\beta$ are selected from $\\{10^{-6},10^{-5},\\dots,10^{-1}\\}$ and $\\{10^{-3},10^{-2^{\\circ}},\\dot{\\cdot}\\cdot\\cdot,10^{2}\\}$ as suggested [7]. We randomly partition the whole dataset ( $70\\%$ for training and $30\\%$ for testing), and repeat the above process ten times and report the average and standard deviation of the results. ", "page_idx": 7}, {"type": "text", "text": "5.3 Results and discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 shows the prediction performance of the comparison algorithms on three datasets. Each result is formatted as \u201cmean\u00b1std\u201d. In the first column of Table 1, \u201cMR\u201d, \u201cLR\u201d and \u201cGL\u201d denote the existing binary LE algorithms, the suffix \u201c-LL\u201d denotes that these algorithms run on binary labels directly, and the suffixes \u201c-DT\u201d, \u201c-MSE\u201d and \u201c-CateMO\u201d denote that these algorithms run on ternary labels by the DT method, MSE method and our proposed CateMO distribution, respectively. \u201cGround-Truth\u201d denotes that LDL-LRR is trained directly on the ground-truth label distributions. In each area separated by dashed lines, bold and italics denote the 1st and 2nd, respectively. The results of statistical significance test are shown in Appendix. It can be seen that the performance of our proposed CateMO is better than other three comparison algorithms in all cases, and is close to \u201cGround-Truth\u201d. To visualize the advantages of ternary labels over binary labels, we show the relationship between binary labels, ternary labels, and label distributions in terms of annotation time and prediction performance in Figure 6. The horizontal axis denotes the average time (in seconds) spent by an expert in annotating a label for an instance. The vertical axis denotes the average prediction performance calculated from Table 1. It can be seen that ternary labels is superior to binary labels in terms of both prediction performance and annotating cost. ", "page_idx": 7}, {"type": "table", "img_path": "F6L23TNlFW/tmp/be855ee82f318ee061bce30daf3332d6e752c5cf7d682c86e387192f7135856d.jpg", "table_caption": ["Table 1: Results shown as \u201cmean\u00b1std\u201d, where bold and italics denote the 1st and 2nd, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "F6L23TNlFW/tmp/14acfeffb45e2ce6faccf51fe7b2b650aeea02432288e51ea2666f4bb8e1a9dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: Cost-benefit analysis of different forms of labels. The horizontal and vertical axes denote the average annotating time (in seconds) and performance, respectively. ", "page_idx": 9}, {"type": "image", "img_path": "F6L23TNlFW/tmp/5972ddaa3a64a495f6e7d2bf69a8896474a00f8f3b5422a1ddd87d65046401fb.jpg", "img_caption": ["Figure 7: Recovery performance of GL-CateMO with varying precision parameters. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Effect of precision parameters ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Figure 7 shows how the precision parameters affect the recovery performance of GL-CateMO on JAFFE, Painting, and Music datasets. The recovery performance is computed by the following two steps. First, we run GL-CateMO algorithm on the dataset with ternary labels to recover the label distributions for instances. Second, we calculate the KL divergence between the recovered label distributions and the ground-truth by the LDL metrics. For a certain precision parameter, the other two precision parameters are set to the values that give rise to the best recovery performance. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations In this paper, the parameters (i.e., $\\underbar{\\lambda},\\lambda$ , and $\\overline{{\\lambda}}$ ) of CateMO are pre-fixed. In fact, a more adaptive approach is to collaboratively learn these parameters and other model parameters. However, since the parameters of CateMO satisfy the conditions shown in Theorem 2, in which the parameters are interdependent, it may lead to difficulties in directly exploiting gradient descent optimization methods. Therefore, in future works, we will explore how to appropriately reduce the parameter space of CateMO so that the parameters $\\underrightarrow{\\lambda},\\lambda$ , and \u03bb can be decoupled from each others. ", "page_idx": 9}, {"type": "text", "text": "Conclusion In this paper, we propose to predict label distribution from ternary labels, which reduces both the annotation inaccuracy and cost when contrasted with the traditional binary annotating methods. In the theoretical part, we analyze the approximation error for both ternary and binary labels, which provides a quantitative elucidation of the superior performance of the ternary label. In the methodological part, we propose the CateMO distribution to model the mapping from label description degrees to ternary labels, which is theoretically constructed to maintain the monotonicity and ordinality of the probabilities associated with ternary labels. In the experimental part, extensive experiments demonstrate the effectiveness of our proposal. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by the National Natural Science Foundation of China (62176123, 62476130), and the Natural Science Foundation of Jiangsu Province (BK20242045). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Binbin Gao, Hongyu Zhou, Jianxin Wu, and Xin Geng. Age estimation using expectation of label distribution learning. In International Joint Conference on Artificial Intelligence, pages 712\u2013718, 2018.   \n[2] Xin Geng. Label distribution learning. IEEE Transactions on Knowledge and Data Engineering, 28(7):1734\u20131748, 2016.   \n[3] Xin Geng and Peng Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In International Joint Conference on Artificial Intelligence, pages 3511\u20133517, 2015.   \n[4] Xin Geng, Kate Smith-Miles, and Zhi-Hua Zhou. Facial age estimation by learning from label distributions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:2401\u20132412, 2013.   \n[5] Peng Hou, Xin Geng, and Min-Ling Zhang. Multi-label manifold learning. In AAAI Conference on Artificial Intelligence, pages 1680\u20131686, 2016.   \n[6] Xiuyi Jia, Yunan Lu, and Fangwen Zhang. Label enhancement by maintaining positive and negative label relation. IEEE Transactions on Knowledge and Data Engineering, 35(2):1708\u2013 1720, 2023.   \n[7] Xiuyi Jia, Xiaoxia Shen, Weiwei Li, Yunan Lu, and Jihua Zhu. Label distribution learning by maintaining label ranking relation. IEEE Transactions on Knowledge and Data Engineering, 35(2):1695\u20131707, 2023.   \n[8] Xiuyi Jia, Xiang Zheng, Weiwei Li, Changqing Zhang, and Zechao Li. Facial emotion distribution learning by exploiting low-rank label correlations locally. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9841\u20139850, 2019.   \n[9] Harin Lee, Frank Hoeger, Marc Schoenwiesner, Minsu Park, and Nori Jacoby. Cross-cultural mood perception in pop songs and its alignment with mood detection algorithms. In International Society for Music Information Retrieval Conference, 2021.   \n[10] Xinyuan Liu, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Xiuyi Jia, and Lei Chen. Unified framework for learning with label distribution. Information Fusion, 75:116\u2013130, 2021.   \n[11] Xinyuan Liu, Jihua Zhu, Qinghai Zheng, Zhongyu Li, Ruixin Liu, and Jun Wang. Bidirectional loss function for label enhancement and distribution learning. Knowledge-Based System, 213:106690, 2021.   \n[12] Yunan Lu, Liang He, Fan Min, Weiwei Li, and Xiuyi Jia. Generative label enhancement with gaussian mixture and partial ranking. In AAAI Conference on Artificial Intelligence, pages 8975\u20138983, 2023.   \n[13] Yunan Lu and Xiuyi Jia. Predicting label distribution from multi-label ranking. In Advances in Neural Information Processing Systems, pages 36931\u201336943, 2022.   \n[14] Yunan Lu, Weiwei Li, and Xiuyi Jia. Label enhancement via joint implicit representation clustering. In International Joint Conference on Artificial Intelligence, pages 4019\u20134027, 2023.   \n[15] Yunan Lu, Weiwei Li, Huaxiong Li, and Xiuyi Jia. Predicting label distribution from tieallowed multi-label ranking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(12):15364\u201315379, 2023.   \n[16] Yunan Lu, Weiwei Li, Huaxiong Li, and Xiuyi Jia. Ranking-preserved generative label enhancement. Machine Learning, 112:4693\u20134721, 2023.   \n[17] Jiaqi Lv, Ning Xu, Renyi Zheng, and Xin Geng. Weakly supervised multi-label learning via label enhancement. In International Joint Conference on Artificial Intelligence, pages 3101\u20133107, 2019.   \n[18] Michael Lyons, Shigeru Akamatsu, Miyuki Kamachi, and Jiro Gyoba. Coding facial expressions with gabor wavelets. In IEEE International Conference on Automatic Face and Gesture Recognition, pages 200\u2013205, 1998.   \n[19] Jana Machajdik and Allan Hanbury. Affective image classification using features inspired by psychology and art theory. In ACM International Conference on Multimedia, page 83\u201392, 2010.   \n[20] Ruifeng Shao, Xin Geng, and Ning Xu. Multi-label learning with label enhancement. In IEEE International Conference on Data Mining, pages 437\u2013446, 2018.   \n[21] Haoyu Tang, Jihua Zhu, Qinghai Zheng, Jun Wang, Shanmin Pang, and Zhongyu Li. Label enhancement with sample correlations via low-rank representation. In AAAI Conference on Artificial Intelligence, pages 5932\u20135939, 2020.   \n[22] Grigorios Tsoumakas and Ioannis Katakis. Multi-label classification: An overview. International Journal of Data Warehousing and Mining, 3(3):1\u201313, 2006.   \n[23] Tao Wen, Weiwei Li, Lei Chen, and Xiuyi Jia. Semi-supervised label enhancement via structured semantic extraction. International Journal of Machine Learning and Cybernetics, 13:1131\u2013 1144, 2021.   \n[24] Xin Wen, Biying Li, Haiyun Guo, Zhiwei Liu, Guosheng Hu, Ming Tang, and Jinqiao Wang. Adaptive variance based label distribution learning for facial age estimation. In European Conference on Computer Vision, pages 379\u2013395, 2020.   \n[25] Ning Xu, Yun-Peng Liu, and Xin Geng. Label enhancement for label distribution learning. IEEE Transactions on Knowledge and Data Engineering, 33:1632\u20131643, 2021.   \n[26] Ning Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In AAAI Conference on Artificial Intelligence, pages 5557\u20135564, 2019.   \n[27] Ning Xu, Jun Shu, Yun-Peng Liu, and Xin Geng. Variational label enhancement. In International Conference on Machine Learning, volume 119, pages 10597\u201310606, 2020.   \n[28] Ning Xu, Jun Shu, Renyi Zheng, Xin Geng, Deyu Meng, and Min-Ling Zhang. Variational label enhancement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):6537\u2013 6551, 2023.   \n[29] Ning Xu, An Tao, and Xin Geng. Label enhancement for label distribution learning. In International Joint Conference on Artificial Intelligence, pages 1632\u20131643, 2018.   \n[30] Jufeng Yang, Dongyu She, and Ming Sun. Joint image emotion classification and distribution learning via deep convolutional neural network. In International Joint Conference on Artificial Intelligence, pages 3266\u20133272, 2017.   \n[31] Yiyu Yao. The dao of three-way decision and three-world thinking. International Journal of Approximate Reasoning, 162:109032, 2023.   \n[32] Min-Ling Zhang, Qian-Wen Zhang, Jun-Peng Fang, Yukun Li, and Xin Geng. Leveraging implicit relative labeling-importance information for effective multi-label learning. IEEE Transactions on Knowledge and Data Engineering, 33:2057\u20132070, 2021.   \n[33] Qian-Wen Zhang, Yun Zhong, and Min-Ling Zhang. Feature-induced labeling information enrichment for multi-label learning. In AAAI Conference on Artificial Intelligence, pages 4446\u20134453, 2018.   \n[34] Qinghai Zheng, Jihua Zhu, Haoyu Tang, Xinyuan Liu, Zhongyu Li, and Huimin Lu. Generalized label enhancement with sample correlations. IEEE Transactions on Knowledge and Data Engineering, 35(1):482\u2013495, 2023.   \n[35] Deyu Zhou, Xuan Zhang, Yin Zhou, Quan Zhao, and Xin Geng. Emotion distribution learning from texts. In Conference on Empirical Methods in Natural Language Processing, pages 638\u2013647, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Given a label equiprobably being positive, negative, and uncertain, if the label is annotated by a ternary value, then we have the expected approximation error: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{\\hat{s},s}[\\psi({Z}_{\\hat{s}},{Z}_{s})]=\\displaystyle\\sum_{u\\in\\{0,\\pm1\\}}p(\\hat{s}=u,s=u)\\psi({Z}_{\\hat{b}=u},{Z}_{s=u})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{3}\\frac{\\int_{0}^{\\hat{r}}\\int_{0}^{\\tau}(z-\\hat{z})^{2}\\mathrm{d}z\\mathrm{d}\\hat{z}}{\\hat{\\tau}\\tau}+\\frac{1}{3}\\frac{\\int_{\\hat{\\tau}}^{\\hat{k}}\\int_{\\tau}^{\\kappa}(z-\\hat{z})^{2}\\mathrm{d}z\\mathrm{d}\\hat{z}}{(\\hat{\\kappa}-\\hat{\\tau})(\\kappa-\\tau)}+\\frac{1}{3}\\frac{\\int_{\\hat{\\kappa}}^{1}\\int_{\\kappa}^{1}(z-\\hat{z})^{2}\\mathrm{d}z\\mathrm{d}\\hat{z}}{(1-\\hat{\\kappa})(1-\\kappa)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{2}{9}(\\tau+\\kappa)^{2}+\\frac{2}{9}(\\hat{\\tau}+\\hat{\\kappa})^{2}-\\frac{1}{6}(\\hat{\\tau}\\kappa+\\hat{\\kappa}\\tau)-\\frac{1}{3}(\\hat{\\tau}+\\kappa)(\\hat{\\kappa}+\\tau)+\\frac{1}{18}(1-\\kappa-\\hat{\\kappa}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "If the label is annotated by a binary value, then we have the expected approximation error: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]=\\sum_{u=\\pm1}p(\\hat{b}=u,s=u)\\psi(Z_{\\hat{b}=u},\\mathbb{Z}_{s=u})+p(\\hat{b}=u,s=0)\\psi(\\mathbb{Z}_{\\hat{b}=u},\\mathbb{Z}_{s=0})}\\\\ &{\\qquad\\qquad\\qquad=\\rho\\left(\\frac{\\tau+\\kappa}{6}-\\frac{1+\\hat{\\xi}}{9}\\right)+\\frac{2(\\tau+\\kappa)^{2}}{9}+\\frac{\\hat{\\xi}^{2}-\\hat{\\xi}\\tau-\\hat{\\xi}\\kappa-\\tau\\kappa}{3}-\\frac{3\\tau+4\\kappa-\\hat{\\xi}-3}{18},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\rho=p(\\hat{b}=-1|s=0)$ . Suppose that $\\hat{\\xi}\\sim\\mathrm{Uni}(\\hat{\\xi}\\mid\\hat{\\tau}\\leq\\hat{\\xi}\\leq\\hat{\\kappa})$ , $\\rho\\sim\\mathrm{Uni}(\\rho\\mid0\\le\\rho\\le1)$ , $[\\tau,\\kappa]\\sim\\mathrm{Uni}([\\tau,\\kappa]\\mid0\\leq\\tau\\leq\\kappa\\leq1)$ ), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\hat{s},s,\\tau,\\kappa}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]=\\bigg(\\displaystyle\\int_{0}^{1}\\int_{0}^{\\kappa}\\mathrm{d}\\tau\\mathrm{d}\\kappa\\bigg)^{-1}\\int_{0}^{1}\\int_{0}^{\\kappa}\\mathbb{E}_{\\hat{s},s}[\\psi(\\mathbb{Z}_{\\hat{s}},\\mathbb{Z}_{s})]\\mathrm{d}\\tau\\mathrm{d}\\kappa}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=36^{-1}(8\\hat{\\kappa}^{2}+4\\hat{\\tau}\\hat{\\kappa}-12\\hat{\\kappa}+8\\hat{\\tau}^{2}-8\\hat{\\tau}+7)}\\\\ &{\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]=\\frac{1}{\\displaystyle\\int_{0}^{1}\\int_{0}^{\\kappa}\\int_{0}^{1}\\int_{\\hat{\\tau}}^{\\hat{\\kappa}}\\mathrm{d}\\hat{\\xi}\\mathrm{d}\\rho\\mathrm{d}\\tau\\mathrm{d}\\kappa}\\int_{0}^{1}\\int_{0}^{\\kappa}\\int_{0}^{1}\\int_{\\hat{\\tau}}^{\\hat{\\kappa}}\\mathbb{E}_{\\hat{b},s}[\\psi(\\mathbb{Z}_{\\hat{b}},\\mathbb{Z}_{s})]\\mathrm{d}\\hat{\\xi}\\mathrm{d}\\rho\\mathrm{d}\\tau\\mathrm{d}\\kappa}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=16^{-1}(2\\hat{\\kappa}^{2}+2\\hat{\\tau}\\hat{\\kappa}-3\\hat{\\kappa}+2\\hat{\\tau}^{2}-3\\hat{\\tau}+3).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Furthermore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathbb{E}_{\\hat{s},s,\\tau,\\kappa}[\\psi(\\mathcal{L}_{\\hat{s}},\\mathcal{L}_{s})]\\leq\\mathbb{E}_{\\hat{b},s,\\hat{\\xi},\\rho,\\tau,\\kappa}[\\psi(\\mathcal{L}_{\\hat{b}},\\mathcal{L}_{s})]}}\\\\ {{\\Longleftrightarrow8\\hat{\\kappa}^{2}+4\\hat{\\tau}\\hat{\\kappa}-12\\hat{\\kappa}+8\\hat{\\tau}^{2}-8\\hat{\\tau}+7\\leq4\\hat{\\kappa}^{2}+4\\hat{\\tau}\\hat{\\kappa}-6\\hat{\\kappa}+4\\hat{\\tau}^{2}-6\\hat{\\tau}+6}}\\\\ {{\\Longleftrightarrow\\hat{\\kappa}^{2}-\\frac{3}{2}\\hat{\\kappa}+\\hat{\\tau}^{2}-\\frac{1}{2}\\hat{\\tau}+\\frac{1}{4}\\leq0}}\\\\ {{\\Longleftrightarrow\\left(\\hat{\\kappa}-\\frac{3}{4}\\right)^{2}+\\left(\\hat{\\tau}-\\frac{1}{4}\\right)^{2}\\leq\\frac{3}{8}}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To ensure that Equation (10) satisfies the probability monotonicity of negative and positive labels, we just need to ensure that $\\partial\\underline{{\\varphi}}(z)/\\partial z<0$ and $\\partial\\overline{{\\varphi}}(z)/\\partial z>0$ . The partial derivative of $\\underline{{\\varphi}}(z)$ w.r.t. $z$ can be formalized as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial\\varphi(z)}{\\partial z}=\\Big((\\lambda z-\\hat{z}\\lambda-\\underline{{{\\lambda}}}z)e^{v_{1}}+(\\overline{{{\\lambda}}}z-\\overline{{{\\lambda}}}-\\underline{{{\\lambda}}}z)e^{v_{2}}\\Big)\\cdot v_{3},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $v_{1},v_{2},v_{3}>0$ . Since $\\overline{{\\lambda}}z-\\overline{{\\lambda}}-\\underline{{\\lambda}}z<0$ , $(\\overline{{\\lambda}}z-\\overline{{\\lambda}}-\\underline{{\\lambda}}z)e^{v_{2}}<0$ . Therefore, $\\partial\\underline{{\\varphi}}(z)/\\partial z<0$ if $\\lambda z-\\hat{z}\\lambda-\\underline{{{\\lambda}}}z<0$ holds for any $z\\in(0,1)$ . This inequality can be equivalently transformed into: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\underline{{\\lambda}}-\\lambda}{\\hat{z}\\lambda}>-1>-\\frac{1}{z}\\Longleftrightarrow\\lambda<\\frac{\\lambda}{1-\\hat{z}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, the partial derivative of $\\overline{{\\varphi}}(z)$ w.r.t. $z$ can be formalized as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial\\overline{{\\varphi}}(z)}{\\partial z}=\\left((z\\lambda-\\hat{z}\\lambda-\\overline{{\\lambda}}z+\\overline{{\\lambda}})e^{v_{4}}+(\\overline{{\\lambda}}-\\overline{{\\lambda}}z+\\underline{{\\lambda}}z)e^{v_{5}}\\right)\\cdot v_{6},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $v_{4},v_{5},v_{6}>0$ . Since $\\overline{{\\lambda}}-\\overline{{\\lambda}}z+\\underline{{\\lambda}}z>0$ , $(\\overline{{\\lambda}}-\\overline{{\\lambda}}z+\\underline{{\\lambda}}z)e^{v_{5}}>0$ . Therefore, $\\partial\\overline{{\\varphi}}(z)/\\partial z>0$ if $z\\lambda-\\hat{z}\\lambda-\\overline{{\\lambda}}z+\\overline{{\\lambda}}>0$ , i.e., $\\lambda(z-\\hat{z})>\\overline{{\\lambda}}(z-1)$ holds for any $z\\in(0,1)$ . On the one hand, if $\\hat{z}<z<1$ , then $\\lambda>\\overline{{\\lambda}}(z-1)(z-\\hat{z})^{-1}$ . On the other hand, if $z<\\hat{z}$ , then $\\begin{array}{r}{\\lambda<\\frac{\\overline{{\\lambda}}}{\\widehat{z}}<\\frac{\\overline{{\\lambda}}(z-1)}{z-\\widehat{z}}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "To ensure that Equation (10) satisfies the probability orderliness, we just need to ensure that equations $\\overline{{\\varphi}}(z)=\\varphi(z)$ (i.e., $\\overline{{\\lambda}}(z-1)^{2}=\\lambda(z-\\hat{z})^{2})$ and $\\underline{{\\varphi}}(z)=\\varphi(z)$ (i.e., $\\underline{{\\lambda}}z^{2}=\\lambda(z-\\hat{z})^{2})$ have only one solution on $z\\in(0,1)$ . We denote $g_{1}(z)=\\overline{{\\lambda}}(z-1)^{2}-\\lambda(z-\\hat{z})^{2}$ . Since $g_{1}(1)=-\\lambda(\\hat{z}-1)^{2}<0$ and $g_{1}(0)\\,=\\,{\\overline{{\\lambda}}}-\\,{\\hat{z}}^{2}\\lambda\\,>\\,{\\overline{{\\lambda}}}-{\\hat{z}}^{2}{\\overline{{\\lambda}}}{\\hat{z}}^{-1}\\,=\\,{\\overline{{\\lambda}}}(1\\,-\\,{\\hat{z}})\\,>\\,0,\\,\\\"$ the quadratic equation $g_{1}(z)\\,=\\,0$ has only one solution in $z\\,\\in\\,(0,1)$ . Similarly, we denote $g_{2}(z)=\\bar{\\underline{{{\\}}}}z^{2}-\\lambda(\\bar{z}-\\hat{z})^{2}$ Since $g_{2}(1)\\,=$ $2\\hat{z}\\bar{\\lambda}-\\hat{z}^{2}\\lambda-\\lambda+\\underline{{{\\lambda}}}=\\underline{{{\\lambda}}}-(1-\\hat{z})^{2}\\lambda>\\underline{{{\\lambda}}}-(1-\\hat{z})^{2}\\underline{{{\\lambda}}}(1-\\hat{z})^{-1}=\\hat{z}\\underline{{{\\lambda}}}>0$ and $g_{2}(0)=-\\hat{z}^{2}\\lambda<0$ , the quadratic equation $g_{2}(z)=0$ has only one solution in $z\\in(0,1)$ . ", "page_idx": 13}, {"type": "text", "text": "To ensure that Equation (10) satisfies the probability monotonicity of uncertain label, we just need to ensure that $\\partial\\varphi(\\bar{z})/\\partial z>0$ holds f\u221aor an\u221ay $0<z<z_{0}$ and $\\partial\\varphi(z)\\dot{/}\\partial z<0$ holds for any $z_{0}<z<1$ , where $\\overline{{\\varphi}}(z_{0})=\\underline{{\\varphi}}(z_{0})$ , i.e., $z_{0}=\\sqrt{\\bar{\\lambda}}(\\sqrt{\\bar{\\lambda}}+\\sqrt{\\underline{{{\\lambda}}}})^{-1}$ That is, make sure that $z_{0}$ is the only point of maximum value of $\\varphi(z)$ . The partial derivative of $\\varphi(z)$ w.r.t. $z$ is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial\\varphi(z)}{\\partial z}=v_{1}\\cdot\\Big((\\hat{z}\\lambda-\\lambda z+\\underline{{{\\lambda z}}})e^{\\overline{{{\\lambda}}}\\big(z^{2}+1\\big)}+(\\hat{z}\\lambda-\\lambda z+\\overline{{{\\lambda}}}z-\\overline{{{\\lambda}}})e^{z(2\\overline{{{\\lambda}}}+\\underline{{{\\lambda z}}})}\\Big),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $v_{1}>0$ . We aim to make sure that $f(z)=\\partial\\varphi(z)/\\partial z=0$ has only one solution in $z\\in(0,1)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underbrace{(-\\hat{z}\\lambda+\\lambda z-\\underbrace{\\lambda z})(\\hat{z}\\lambda-\\lambda z+\\overline{{\\lambda}}z-\\overline{{\\lambda}})^{-1}}_{f(z)}=\\underbrace{\\exp(2\\overline{{\\lambda}}z+\\underbrace{\\lambda z^{2}-\\overline{{\\lambda}}z^{2}-\\overline{{\\lambda}}}_{g(z)})}_{g(z)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first need to examine whether the denominator of $f(z)$ is zero. Suppose that it is zero, we have $z\\,=\\,(\\overline{{{\\lambda}}}\\,-\\,\\hat{z}\\lambda)(\\overline{{{\\lambda}}}\\,-\\,\\lambda)^{-1}$ . Considering that $\\overline{{{\\lambda}}}-\\dot{z}\\lambda^{'}>\\,\\overline{{{\\lambda}}}-\\dot{z}\\overline{{{\\lambda}}}\\dot{z}^{-\\overline{{{1}}}}\\,=\\,0$ , if $\\overline{{\\lambda}}>\\lambda$ , then $z=(\\overline{{\\lambda}}\\!-\\!\\hat{z}\\lambda)(\\overline{{\\lambda}}\\!-\\!\\lambda)^{-1}>1$ , which contradicts $z\\in(0,1)$ . If ${\\overline{{\\lambda}}}<\\lambda$ , then $z=(\\overline{{\\lambda}}\\!-\\!\\hat{z}\\lambda)(\\overline{{\\lambda}}\\!-\\!\\lambda)^{-1}<0$ , which also contradicts $z\\in(0,1)$ . Besides, it is obvious that the denominator of $f(z)$ is non-zero when $\\lambda={\\overline{{\\lambda}}}$ . Therefore, the denominator of $f(z)$ is non-zero. Since whether the partial derivative of $f(z)$ w.r.t. $z$ (i.e., $\\partial f(z)/\\partial z=(\\cdot\\cdot\\cdot)^{-2}(\\hat{z}\\lambda\\overline{{\\lambda}}-\\hat{z}\\lambda\\underline{{\\lambda}}-\\lambda\\overline{{\\lambda}}+\\underline{{\\lambda}}\\overline{{\\lambda}}))$ is positive or negative has nothing to do with $z$ , we ensure the monotonicity of $f(z)$ by the following inequality: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda\\neq-\\underline{{{\\lambda}}}\\overline{{{\\lambda}}}(\\hat{z}\\overline{{{\\lambda}}}-\\hat{z}\\underline{{{\\lambda}}}-\\overline{{{\\lambda}}})^{-1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Besides, since $\\partial g(z)/\\partial z=(2\\bar{\\lambda}(1-z)+2\\underline{{{\\lambda}}}z)\\exp(\\cdot)>0,g(z$ is increasing. Therefore, to make sure that $\\varphi(z)$ has only one maximal point, we just need to ensure that $f(0)>\\bar{g}(0)$ and $f(1)<g(1)$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda>\\operatorname*{max}\\left\\{\\left(\\hat{z}+\\hat{z}\\exp(\\overline{{{\\lambda}}})\\right)^{-1}\\overline{{{\\lambda}}},((1+\\exp(\\underline{{{\\lambda}}}))(1-\\hat{z}))^{-1}\\underline{{{\\lambda}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, let the partial derivative of $\\varphi(z)$ at $z=z_{0}$ to be zero, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{z}=\\frac{(\\lambda z_{0}-\\underline{{\\lambda}}z_{0})\\exp(\\overline{{\\lambda}}\\left(z_{0}^{2}+1\\right))+(\\lambda z_{0}-\\overline{{\\lambda}}z_{0}+\\overline{{\\lambda}})\\exp\\left(z_{0}\\left(2\\overline{{\\lambda}}+\\underline{{\\lambda}}z_{0}\\right)\\right)}{\\lambda\\exp(\\overline{{\\lambda}}\\left(z_{0}^{2}+1\\right))+\\lambda\\exp\\left(z_{0}\\left(2\\overline{{\\lambda}}+\\underline{{\\lambda}}z_{0}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Considering z0 = $\\begin{array}{r}{z_{0}=\\frac{\\sqrt{\\overline{{\\lambda}}}}{\\sqrt{\\overline{{\\lambda}}}+\\sqrt{\\underline{{\\lambda}}}}}\\end{array}$ , we have $\\hat{z}=(2\\lambda\\sqrt{\\lambda}+2\\lambda\\sqrt{\\underline{{{\\lambda}}}})^{-1}(2\\lambda\\sqrt{\\lambda}-\\underline{{{\\lambda}}}\\sqrt{\\lambda}+\\overline{{{\\lambda}}}\\sqrt{\\underline{{{\\lambda}}}}).$ ", "page_idx": 13}, {"type": "text", "text": "A.3 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "JAFFE dataset The \u201cJAFFE\u201d dataset [18] comprises 213 facial emotion images posed by ten Japanese female models. Concerning the feature data, we employ the feature extraction method suggested in [2] to compress each image into a 243-dimensional feature vector. As for the label data, each image is annotated with scores from $\\{1,2,3,4,5\\}$ by 60 individuals, indicating the relevance of the corresponding emotion to the facial image. The average score for each emotion is utilized to represent the intensity of the emotion. Subsequently, we apply min-max normalization to scale the average scores to the interval $(0,1)$ and normalize the average scores of all emotions into the form of probability distributions to obtain the label distribution data. Besides, three experts annotate the instances in JAFFE with ternary labels and binary labels, and the final ternary labels and binary labels are determined by majority voting. ", "page_idx": 13}, {"type": "text", "text": "Painting dataset The \u201cPainting\u201d dataset [19] is created for exploring emotions within abstract artworks. Each image is represented by a 142-dimensional feature vector comprising three components: histogram features of the RGB attributes of the image, histogram features of the HSV attributes of the image, and GLCM (Grey-Level Co-occurrence Matrix) features of the image. Regarding the label data, approximately 230 individuals annotate emotional scores across eight categories: amusement, anger, awe, content, disgust, excitement, fear, sadness. Each image received about 14 annotations. Subsequently, we apply min-max normalization to scale the average emotional scores to the interval $(0,1)$ and normalize the average scores of all emotions into the form of probability distributions to obtain label distribution data. Besides, three experts annotate the instances in Painting with ternary and binary labels, and the final ternary and binary labels are determined by majority voting. ", "page_idx": 14}, {"type": "text", "text": "Music dataset The \u201cMusic\u201d dataset is an extension of a music dataset [9] and encompasses 360 popular songs from major music charts across different countries. We employ MFCC (Mel Frequency Cepstral Coefficients) to extracte a 5992-dimensional feature vector from each song and utilize PCA (Principal Component Analysis) to compress this vector into a 128-dimensional feature vector. Regarding the label data, participants from the UK, South Korea and Portugal rate their perceived moods (i.e., calm, tense, cheerful, sad, danceable, love, dreamy, electronic, and energy) for a given song on a 4-level scale. Subsequently, we apply min-max normalization to scale the scores to the interval $(0,1)$ , and normalize the average scores of all moods into the form of probability distributions to obtain label distribution data. Besides, three experts annotate the instances in Music with ternary and binary labels, and the final ternary and binary labels are determined by majority voting. ", "page_idx": 14}, {"type": "text", "text": "A.4 Results of statistical significance test ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 2: The counts of win/tie/loss after comparing CateMO with other comparison algorithms under a pairwise two-tailed $t$ -test with 0.05 significance level. Each entry is formatted as \u201cwin/tie/loss\u201d. ", "page_idx": 14}, {"type": "table", "img_path": "F6L23TNlFW/tmp/df71ac2c9b4ceea71b7dfff14f2b60c54ff77d08c6ee26ea381847c0fda20cce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We use pairwise two-tailed $t$ -test with 0.05 significance level to test whether CateMO is statistically superior or inferior to MSE, DT, LL, and ground-truth on three real-world datasets when combined with MR, LR, and GL. The counts of win/tie/loss after comparing CateMO with other comparison algorithms are shown in Table 2. ", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction clarifies both the theoretical and methodological contributions of the paper. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Section 6. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The assumptions of the theoretical result are formally described in Assumption 1, Assumption 2, Assumption 3, and Assumption 4 in Section 3.2 and Section 4.1. The proof of Theorem 1 is shown in Appendix A.1, and the proof of Theorem 2 is shown in Appendix A.2. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: In Section 5.2, we give the details of the comparison algorithms, including the hyperparameter configurations and the method of dataset partitioning. For our proposed CateMO, we also give the values of $\\underline{{\\lambda}},\\lambda,\\overline{{\\lambda}}$ . ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: In terms of the code, the implementation of our proposed CateMO is so simple that no additional code files are needed to describe it. In terms of the data, open access to the datasets involved in this paper requires a license from the corresponding creator, and this paper is not authorized to distribute them. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The training and test details are shown in Section 5.2. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The mean value and the standard derivation of the prediction performance is shown in Table 1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Computer resources have a negligible effect on both the experimental results and the main claims of this paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research conducted in this paper does not violate the NeurIPS Code of Ethics. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: There is no societal impact of the work performed. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have cited all original papers that produced the datasets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The expert\u2019s annotating task is as follows. An expert should annotate 1 if the label can describe the instance, $-1$ if the label cannot describe the instance, and 0 if it is uncertain whether the label can describe the instance. This process is described in Section 1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is no potential risk incurred by study participants. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]