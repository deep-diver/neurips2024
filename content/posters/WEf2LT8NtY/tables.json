[{"figure_path": "WEf2LT8NtY/tables/tables_7_1.jpg", "caption": "Table 1: Average returns of ARDT, vanilla DT and ESPER on Connect Four when trained on data mixed from both a near-random (30%, 50%) dataset and a near-optimal (90%, 90%) dataset. Evaluation is done against different optimality levels of adversary.", "description": "This table presents the average returns achieved by three different algorithms (ARDT, DT, and ESPER) when trained on datasets containing a mix of near-random and near-optimal trajectories. The performance is evaluated against adversaries with varying levels of optimality (30%, 50%, 70%, and 100%). The results show how the different algorithms perform under different adversary strengths and highlight the improved robustness of ARDT.", "section": "4.2 Discrete Game with Partial Data Coverage"}, {"figure_path": "WEf2LT8NtY/tables/tables_8_1.jpg", "caption": "Table 2: Results of DT, ESPER, and our algorithm ARDT with and without conditioned on past adversarial tokens: the Worst-case Return against 8 different online adversarial policies on MuJoCo Noisy Action Robust MDP tasks across 5 seeds. We tested with the same set of target returns and select the best target for each method. To expand the data coverage, we inject random trajectories collected by 0.1-greedy online policies into the pre-collected online datasets with high robust returns. We use low (suffix -lr), medium (suffix -mr) and high randomness (suffix -hr) to indicate the proportion of random trajectories.", "description": "This table presents the worst-case returns achieved by Decision Transformer (DT), Expected Return-Conditioned Decision Transformer (ESPER), and Adversarially Robust Decision Transformer (ARDT) across various continuous control tasks from the MuJoCo environment.  The results are obtained against eight different online adversarial policies and across five different random seeds. The data used in training included both pre-collected online datasets with high robust returns and additional random trajectories (with low, medium, and high randomness) to increase data coverage. The table shows that ARDT generally achieves higher worst-case returns compared to DT and ESPER.", "section": "4.3 Continuous Adversarial Environments"}, {"figure_path": "WEf2LT8NtY/tables/tables_16_1.jpg", "caption": "Table 3: Hyperparameters of ARDT.", "description": "This table lists the hyperparameters used in the Adversarially Robust Decision Transformer (ARDT) algorithm.  It is broken down into the hyperparameters for the transformer training phase and the minimax expectile regression phase. For each phase, it specifies the number of training steps, number of testing iterations, context length, learning rate, weight decay, warmup steps, dropout rate, batch size, optimizer, and other relevant parameters for the processes.", "section": "Implementation Details"}, {"figure_path": "WEf2LT8NtY/tables/tables_16_2.jpg", "caption": "Table 4: Data profile of MuJoCo NR-MDP.", "description": "This table presents the returns achieved by the behavior policy (protagonist and adversary) during data collection in the MuJoCo Noisy Action Robust MDP (NR-MDP) tasks.  Different levels of randomness are introduced in the data collection process, creating low (-lr), medium (-mr), and high (-hr) randomness datasets. The table shows that the behavior policy achieves varying returns depending on the task and the level of randomness.", "section": "4.3 Continuous Adversarial Environments"}, {"figure_path": "WEf2LT8NtY/tables/tables_16_3.jpg", "caption": "Table 4: Data profile of MuJoCo NR-MDP.", "description": "This table presents the return of behavior policy for different optimality levels of both the protagonist and adversary in three MuJoCo environments: Hopper, Walker2D, and Halfcheetah. The optimality is defined as (1 - \u03b5) * 100%, where \u03b5 is the exploration rate of the \u03b5-greedy policy used to collect the data. Each tuple represents the percentage of optimality for the protagonist and the adversary, respectively. The return is the average cumulative reward obtained from the behavior policy, along with its standard deviation in parentheses. The results are shown for low, medium, and high randomness settings, indicated by the suffix -lr, -mr, and -hr, respectively.", "section": "4.3 Continuous Adversarial Environments"}, {"figure_path": "WEf2LT8NtY/tables/tables_18_1.jpg", "caption": "Table 2: Results of DT, ESPER, and our algorithm ARDT with and without conditioned on past adversarial tokens: the Worst-case Return against 8 different online adversarial policies on MuJoCo Noisy Action Robust MDP tasks across 5 seeds. We tested with the same set of target returns and select the best target for each method. To expand the data coverage, we inject random trajectories collected by 0.1-greedy online policies into the pre-collected online datasets with high robust returns. We use low (suffix -lr), medium (suffix -mr) and high randomness (suffix -hr) to indicate the proportion of random trajectories.", "description": "This table presents the worst-case returns achieved by Decision Transformer (DT), Expected Return-Conditioned DT (ESPER), and Adversarially Robust Decision Transformer (ARDT) across various continuous control tasks from the MuJoCo Noisy Action Robust MDP benchmark.  The results are averaged across 5 random seeds and show the performance against 8 different online adversarial policies. The table further breaks down the results by including or excluding the past adversarial tokens in the model's input, and by varying the level of randomness in the training data.", "section": "4.3 Continuous Adversarial Environments"}, {"figure_path": "WEf2LT8NtY/tables/tables_18_2.jpg", "caption": "Table 1: Average returns of ARDT, vanilla DT and ESPER on Connect Four when trained on data mixed from both a near-random (30%, 50%) dataset and a near-optimal (90%, 90%) dataset. Evaluation is done against different optimality levels of adversary.", "description": "This table presents the average returns achieved by three different algorithms (ARDT, DT, and ESPER) when trained on a mixed dataset containing both near-random and near-optimal trajectories. The performance is evaluated against adversaries with varying optimality levels, showcasing the robustness of each algorithm in the face of different adversary strategies.  The results highlight how ARDT outperforms DT and ESPER, especially against stronger adversaries.", "section": "4.2 Discrete Game with Partial Data Coverage"}, {"figure_path": "WEf2LT8NtY/tables/tables_19_1.jpg", "caption": "Table 7: Methods tested in normal setting, i.e. without adversarial perturbation.", "description": "This table shows the performance comparison of three algorithms (DT, ESPER, and ARDT) in a normal setting without adversarial perturbations.  It presents the average normal return, the average worst-case return, and the difference (return drop) between the two for the Hopper-hr environment.  The results highlight that even in the absence of adversarial attacks, ARDT shows relatively high worst-case return compared to others. Note that the numbers in parentheses are standard deviations.", "section": "4.3 Continuous Adversarial Environments"}]