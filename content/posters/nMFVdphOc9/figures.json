[{"figure_path": "nMFVdphOc9/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDB-BINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative weights by blue arrows. The arrow thickness and color correspond to the absolute value of the weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the weights the 10 resp. 5 largest positive and negative weights.", "description": "This figure visualizes the learned parameters (weights and biases) of a RuleGNN model applied to two different graph datasets: DHFR and IMDB-BINARY.  Each subfigure shows the weights and biases for a single graph. Red arrows represent positive weights, blue arrows represent negative weights, and arrow thickness indicates the magnitude of the weight. Node size represents the bias.  The second column shows only the top 10 (DHFR) or 5 (IMDB-BINARY) positive and negative weights for better visualization.", "section": "Main Idea"}, {"figure_path": "nMFVdphOc9/figures/figures_11_1.jpg", "caption": "Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of ethylene (left) and cyclopropenylidene (right) and the rules RMol (5) and Rout (6). The input signal is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges of the same color denote shared weights in a layer. For more details see Appendix A.4.", "description": "This figure illustrates the information propagation within a two-layer Rule Graph Neural Network (RuleGNN) applied to molecule graphs of ethylene and cyclopropenylidene.  The left and right sides show the input and output layers respectively.  Each node represents a neuron, and edges connecting nodes represent weighted connections between neurons. The colors of the edges denote shared weights within each layer.  The rules RMol and Rout, defined in equations (5) and (6) of the paper, govern the arrangement of these connections and shared weights, reflecting the underlying molecule structure. The figure visually depicts how information flows from input to output through the network, demonstrating a key concept of how RuleGNNs dynamically incorporate structural information into the network architecture.", "section": "A.4 Example: RuleGNN for Molecule Graphs"}, {"figure_path": "nMFVdphOc9/figures/figures_11_2.jpg", "caption": "Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of ethylene (left) and cyclopropenylidene (right) and the rules RMol (5) and Rout (6). The input signal is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges of the same color denote shared weights in a layer. For more details see Appendix A.4.", "description": "This figure illustrates the information propagation through a two-layer RuleGNN applied to the molecule graphs of ethylene and cyclopropenylidene.  Each node in the graph represents a neuron, and edges connecting nodes represent the flow of information. The color-coding of the edges shows which connections share weights.  The process starts with an input signal at the left, which flows through the network according to the rules RMol and Rout, resulting in an output signal at the right. Appendix A.4 provides further detail.", "section": "A.4 Example: RuleGNN for Molecule Graphs"}, {"figure_path": "nMFVdphOc9/figures/figures_12_1.jpg", "caption": "Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of ethylene (left) and cyclopropenylidene (right) and the rules RMol (5) and Rout (6). The input signal is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges of the same color denote shared weights in a layer. For more details see Appendix A.4.", "description": "This figure illustrates information propagation in a RuleGNN (Rule-based Graph Neural Network) applied to ethylene and cyclopropenylidene molecules.  The molecules are represented as graphs where nodes are atoms and edges are bonds. The RuleGNN consists of two layers using rules RMol and Rout to dynamically determine weight matrices. The input signal propagates through the network. Nodes represent neurons, and edges of the same color indicate shared weights within a layer, illustrating the dynamic weight sharing based on rules.  Details are provided in Appendix A.4.", "section": "A.4 Example: RuleGNN for Molecule Graphs"}, {"figure_path": "nMFVdphOc9/figures/figures_12_2.jpg", "caption": "Figure 3: Molecule graphs of ethylene (left) and cyclopropenylidene (right). The indices denote the order of the nodes.", "description": "This figure shows the graph representations of two molecules, ethylene and cyclopropenylidene. Each node in the graph represents an atom (carbon or hydrogen), and each edge represents a bond between atoms.  The numbers on the atoms indicate their order which is arbitrary but consistent within each molecule.  These graphs serve as example inputs in the RuleGNN model described in the paper.", "section": "A.4 Example: RuleGNN for Molecule Graphs"}, {"figure_path": "nMFVdphOc9/figures/figures_14_1.jpg", "caption": "Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of ethylene (left) and cyclopropenylidene (right) and the rules RMol (5) and Rout (6). The input signal is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges of the same color denote shared weights in a layer. For more details see Appendix A.4.", "description": "This figure illustrates how information is propagated in a simple two-layer RuleGNN using molecule graphs of ethylene and cyclopropenylidene as examples.  The rules RMol and Rout guide the information flow. Each node in the graph represents a neuron, and edges of the same color represent shared weights within a layer. The direction of information flow is from left to right.", "section": "A.4 Example: RuleGNN for Molecule Graphs"}, {"figure_path": "nMFVdphOc9/figures/figures_14_2.jpg", "caption": "Figure 5: The graphs M0, M1, M2, M3 from [15] that are not distinguishable by the 1-WL test.", "description": "This figure shows four different graphs, labeled M0, M1, M2, and M3, that are used in the Snowflakes dataset.  These graphs are designed to be indistinguishable using the 1-Weisfeiler-Leman test, a common approach to graph isomorphism. This characteristic makes them challenging for graph neural networks that rely on this test for feature extraction.", "section": "4.2 Rule Graph Neural Networks (RuleGNNs)"}, {"figure_path": "nMFVdphOc9/figures/figures_16_1.jpg", "caption": "Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDB-BINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative weights by blue arrows. The arrow thickness and color corresponds to the absolute value of the weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the weights the 10 resp. 5 largest positive and negative weights.", "description": "This figure visualizes the learned parameters (weights and biases) of a RuleGNN model applied to two different graph datasets: DHFR and IMDB-BINARY.  Each subfigure shows the learned weights and biases for three example graphs from each dataset. Red arrows represent positive weights, and blue arrows represent negative weights. The thickness of the arrows and the color intensity corresponds to the magnitude of the weights. The node size represents the bias associated with each node. In the rightmost column, the visualization focuses on the top 10 (DHFR) and top 5 (IMDB-BINARY) weights (both positive and negative) to highlight the most influential connections in the model.", "section": "Main Idea"}]