[{"figure_path": "EbSSBvwUWw/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of existing works on the XOR (2-parity) problem. We mainly focus on the dependence on the input dimension d and test error e and treat other arguments as constant. Here WF denotes Wasserstein flow technique from the mean-field analysis, and GF denotes gradient flow. The sample requirement and convergence iteration in both Glasgow (2023) and our method do not explicitly depend on the test error \u20ac. Instead, the dependence on e is implicitly incorporated within the condition for d. Specifically, our approach requires that d > C log\u00b2(2m/e) while Glasgow (2023) requires d > exp((1/6)C) where C is a constant.", "description": "This table compares various existing works on solving the XOR (2-parity) problem using neural networks.  It contrasts different activation functions, loss functions, algorithms (e.g., gradient flow, SGD), width requirements of the neural network, sample complexity, and the number of iterations required to converge. The table highlights the dependence of these parameters on the input dimension (d) and the test error (e).  It specifically notes that the sample and iteration requirements for both Glasgow's (2023) method and the authors' method are implicitly dependent on the test error (e), and it details the specific conditions on the dimension (d) that must be met for each.", "section": "2 Related Work"}, {"figure_path": "EbSSBvwUWw/tables/tables_3_2.jpg", "caption": "Table 2: Comparison of existing works for the general k-parity problem, focusing primarily on the dimension d and error e, treating other parameters as constants. s in Edelman et al. (2024) is the sparsity of the initialization that satisfies s > k. The activation function by Suzuki et al. (2023) is defined as hw(x) = R[tanh(xw1 + W2) + 2tanh(w3)]/3, where w = (W1,W2, W3)\u315c \u2208 Rd+2 and R is a hyper-parameter determining the network's scale. For the sample requirement and convergence iteration, we focus on the dependency of d, e and omit another terms. Our method's sample requirement and convergence iteration are independent of the test error \u20ac, instead relying on a condition for d that implicitly includes \u20ac.", "description": "This table compares the existing methods for solving the k-sparse parity problem.  The table highlights key differences in activation function, loss function, algorithm used, network width requirement, sample requirement, and number of iterations to converge.  It emphasizes the dependence on input dimension (d) and error (\u20ac), showing how the proposed method achieves a sample complexity of \u00d5(dk-1), matching the established lower bound.", "section": "Related Work"}, {"figure_path": "EbSSBvwUWw/tables/tables_13_1.jpg", "caption": "Table 3: Test accuracy for solving k-sparse parity problem with k \u2208 {2,3,4}, averaged over 10 runs.", "description": "This table presents the test accuracy achieved by the proposed method for solving the k-sparse parity problem, for k values of 2, 3, and 4.  The accuracy is the average across 10 independent runs of the experiment, showing high accuracy for each value of k.", "section": "A Experiments"}]