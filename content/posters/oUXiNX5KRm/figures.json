[{"figure_path": "oUXiNX5KRm/figures/figures_1_1.jpg", "caption": "Figure 1: Schemantic sketch of the UPT learning paradigm. UPTs flexible encode different grids, and/or different number of particles into a unified latent space representation, and subsequently unroll dynamics in the latent space. The latent space is kept at a fixed size to ensure scalability to larger systems. UPTs decode the latent representation at any query point.", "description": "This figure illustrates the Universal Physics Transformer (UPT) learning process.  UPTs handle both grid-based and particle-based data by encoding them into a fixed-size latent space.  The dynamics are then efficiently propagated within this latent space before being decoded at any desired query point in space and time. This approach allows for scalability to larger and more complex simulations.", "section": "1 Introduction"}, {"figure_path": "oUXiNX5KRm/figures/figures_3_1.jpg", "caption": "Figure 2: Qualitative exploration of scaling limits. Starting from 32K input points (scale 1), we train a 68M parameter model for a few steps with batchsize 1 and measure the required GPU memory. Models without a compressed latent space (GNN, Transformer) quickly reach their limits while models with a compressed latent space (GINO, UPT) scale much better with the number of inputs. However, as GINO compresses the latent space onto a regular grid, the scaling benefits are largely voided on 3D problems. The efficient latent space compression of UPTs can fit up to 4.2M points (scale 128). We use a linear attention transformer [90] for this study. \u201cDisc. Conv.\u201d denotes \"Discretization Convergent\". Appendix D.7 outlines implementation details and complexities.", "description": "The figure qualitatively explores the scaling limits of different neural operator architectures when the number of input points increases.  Models with compressed latent spaces (like GINO and UPT) significantly outperform models without such compression (GNNs and Transformers) in terms of memory usage as the number of input points grows. However, GINO's reliance on regular grids limits its scalability in 3D. UPT offers the best scalability due to its efficient latent space compression.", "section": "3 Universal Physics Transformers"}, {"figure_path": "oUXiNX5KRm/figures/figures_3_2.jpg", "caption": "Figure 3: Left: UPT compresses information from various grids or differing particles with an encoder, propagates this information forward in time through the approximator and decodes at arbitrary query positions. Right: Training procedure to enable latent rollouts via inverse encoding/decoding losses.", "description": "The figure illustrates the Universal Physics Transformer (UPT) architecture and training process. The left panel shows the UPT's workflow: encoding data from various sources (grids or particles) into a fixed-size latent space, propagating the dynamics within the latent space using a transformer approximator, and decoding the results to obtain predictions at arbitrary query locations. The right panel details the training procedure, highlighting the use of inverse encoding and decoding losses to learn the encoding and decoding functions and allow for efficient latent space rollouts.  This setup is crucial for efficiently handling large-scale simulations because it avoids the computational cost of working directly with high-dimensional spatial data.", "section": "3 Universal Physics Transformers"}, {"figure_path": "oUXiNX5KRm/figures/figures_6_1.jpg", "caption": "Figure 4: Example rollout trajectories of the UPT-68M model, visually demonstrating the efficacy of UPT physics modeling. The UPT model is trained across different obstacles, different flow regimes, and different mesh discretizations. Interestingly, the absolute error might suggest that UPT trajectories diverge, although physics are still simulated faithfully. This stems from subtle shifts in predictions throughout the rollout duration, likely attributed to the point-wise decoding of the latent field.", "description": "This figure shows example rollout trajectories of the Universal Physics Transformer (UPT) model, highlighting its ability to simulate physical phenomena accurately across different simulation settings.  The UPT model was trained on datasets with varying obstacles, flow conditions, and mesh resolutions, demonstrating its robustness and generalization capabilities.  While the absolute error may appear to indicate divergence in some instances, closer examination reveals that the discrepancies originate from small, gradual shifts in predictions over time, possibly resulting from the model's point-wise decoding of the latent field representation.", "section": "4.2 Transient flows"}, {"figure_path": "oUXiNX5KRm/figures/figures_7_1.jpg", "caption": "Figure 5: Left and middle: MSE and correlation time on the testset. UPTs outperform compared methods on all model scales by a large margin. Right: We study discretization convergence by varying the number of input/output points or the number of gridpoints/supernodes of models that were trained on inputs between 8K and 24K points, 8K target points. UPT demonstrates a stable performance across different number of input/outputs even though it has never seen that number of input/output points during training. We study discretization convergence of supernodes in App. D.5.2, where UPT also shows a steady improvement when more supernodes are used during inference. Additionally, we study smaller UPT models and training on less data in Appendix D.5.4 and D.5.5.", "description": "This figure presents a comparison of UPTs against other methods across different model sizes and input/output data points. The left and center plots show that UPTs consistently outperform other methods in terms of Mean Squared Error (MSE) and correlation time. The right plot demonstrates the impact of varying the number of input/output points, showing the stable and robust performance of UPTs across different resolutions.", "section": "4.2 Transient flows"}, {"figure_path": "oUXiNX5KRm/figures/figures_8_1.jpg", "caption": "Figure 6: Conceptual difference between GNS/SEGNN on the left and UPT on the right side. GNS/SEGNN predicts the acceleration of a particle which is then integrated to calculate the next position. UPTs directly model the velocity field and allow for large timestep predictions.", "description": "This figure illustrates the core difference between Graph Neural Network-based simulators (GNS) and Steerable E(3) Equivariant Graph Neural Networks (SEGNN), and Universal Physics Transformers (UPTs).  GNS and SEGNN predict particle acceleration, which is then numerically integrated to find the next position.  This process requires small timesteps. In contrast, UPTs model the entire velocity field, making larger timesteps possible.  The figure depicts particle trajectories to show how the UPT approach handles the dynamics more directly.", "section": "4.3 Lagrangian fluid dynamics"}, {"figure_path": "oUXiNX5KRm/figures/figures_9_1.jpg", "caption": "Figure 7: Left: Velocity error over all particles for different timesteps. UPTs effectively learn the underlying field dynamics, resulting in lower error as the trajectory evolves in time. Right: Visualization of the velocity field modeled by UPT (white) vs the ground truth particle velocities.", "description": "This figure shows the comparison between the UPT model and other methods (GNS and SEGNN) for predicting particle velocities in a Lagrangian fluid dynamics simulation. The left panel displays a line graph showing the mean Euclidean norm of the velocity error over all particles across different timesteps. The right panel provides a visual comparison of the velocity field predicted by the UPT model and the ground truth particle velocities. The visualizations are given as quiver plots of the velocity vector fields.", "section": "4.3 Lagrangian fluid dynamics"}, {"figure_path": "oUXiNX5KRm/figures/figures_27_1.jpg", "caption": "Figure 2: Qualitative exploration of scaling limits. Starting from 32K input points (scale 1), we train a 68M parameter model for a few steps with batchsize 1 and measure the required GPU memory. Models without a compressed latent space (GNN, Transformer) quickly reach their limits while models with a compressed latent space (GINO, UPT) scale much better with the number of inputs. However, as GINO compresses the latent space onto a regular grid, the scaling benefits are largely voided on 3D problems. The efficient latent space compression of UPTs can fit up to 4.2M points (scale 128). We use a linear attention transformer [90] for this study. \u201cDisc. Conv.\u201d denotes \"Discretization Convergent\". Appendix D.7 outlines implementation details and complexities.", "description": "The figure qualitatively shows the scaling limits of various neural operators for increasing input sizes. Models with compressed latent space representations (GINO and UPT) scale much better than those without (GNN and Transformer).  GINO's scaling advantage is lost in 3D due to its reliance on regular grids. UPT exhibits the best scalability, handling up to 4.2 million points.", "section": "3 Universal Physics Transformers"}, {"figure_path": "oUXiNX5KRm/figures/figures_28_1.jpg", "caption": "Figure 9: Latent space scaling investigations of a 17M parameter UPT model for 10 epochs. Compound scaling scales the number of supernodes and latent tokens simultaneously where a compound scale of 1 uses nsupernodes=2048 and nlatent=512, i.e. compound scale 2 uses nsupernodes=4096 and nlatent=1024. Throughput is measured as number of samples processed per GPU-hour. Models are trained in a reduced setting with 10 epochs and 16K input points.", "description": "This figure shows the results of an experiment investigating the impact of scaling the latent space size on the performance of a 17M parameter UPT model.  Three different scaling experiments are performed: increasing the number of supernodes, increasing the number of latent tokens, and scaling both simultaneously (compound scaling).  The results are presented in terms of test MSE and throughput (samples processed per GPU hour). The experiment was conducted with a reduced training setting of 10 epochs and 16,000 input points.", "section": "D.5.3 Impact of a larger latent space"}, {"figure_path": "oUXiNX5KRm/figures/figures_28_2.jpg", "caption": "Figure 10: UPT is much more expressive than GINO and shows good scaling when increasing model size. UPT-1M achieves comparable performance to GINO-17M using 17x less parameters.", "description": "This figure compares the performance of Universal Physics Transformers (UPTs) and Geometry-Informed Neural Operators (GINOs) across different model sizes on a specific task.  It demonstrates that UPTs achieve better performance (lower test MSE) with significantly fewer parameters than GINOs.  This highlights the superior expressivity and efficiency of the UPT architecture.", "section": "D.5.4 Performance of smaller models"}, {"figure_path": "oUXiNX5KRm/figures/figures_29_1.jpg", "caption": "Figure 11: UPT scales well with more data and is data efficient, achieving comparable results to competitors with 4x less data.", "description": "This figure shows the scalability and data efficiency of UPTs. The model was trained on subsets of the data used for the transient flow experiments (2K and 4K out of the 8K training simulations).  The results demonstrate that UPTs achieve comparable performance to GINO-8M with only a quarter of the data, highlighting its data efficiency.", "section": "4 Experiments"}, {"figure_path": "oUXiNX5KRm/figures/figures_30_1.jpg", "caption": "Figure 12: OOD generalization study. Trained 68M parameter models are evaluated on OOD datasets with more objects, higher inflow velocities and different geometries. \"ID\" refers to an adaptive meshing algorithm with circles as obstacles. \"UMesh\" changes the meshing algorithm from adaptive to uniform. \"Triangle\" uses triangles instead of circles and \"Polygon\" uses polygon obstacles (with up to 9 edges) instead of circles. UPTs have similar OOD generalization capabilities as other models, outperforming them in all evaluations. Grey indicates OOD settings.", "description": "This figure shows the out-of-distribution generalization capabilities of the 68M parameter models trained on the transient flow dataset.  The left panel shows results when increasing the number of obstacles; the center panel increases the inflow velocity; and the right panel compares the results across different mesh geometries (uniform mesh, triangles, and polygons). UPTs show a strong performance across all OOD (out-of-distribution) scenarios.", "section": "4.2 Transient flows"}, {"figure_path": "oUXiNX5KRm/figures/figures_31_1.jpg", "caption": "Figure 13: Exemplary visualizations for transient flow simulation rollouts. Best viewed zoomed in.", "description": "This figure shows example rollout trajectories from the UPT-68M model.  Each row represents a different simulation, showcasing the model's ability to handle various obstacle configurations, flow regimes, and mesh discretizations. The leftmost column displays the ground truth, while subsequent columns show model predictions at different timesteps. The absolute error is also depicted, highlighting subtle prediction shifts that occur over time despite the model successfully simulating the overall physics. The figure emphasizes the UPTs flexibility and robustness in various situations.", "section": "4.2 Transient flows"}, {"figure_path": "oUXiNX5KRm/figures/figures_32_1.jpg", "caption": "Figure 14: Left: Mean Euclidean norm of the velocity error over all particles for different timesteps. UPTs effectively learn the underlying field dynamics, resulting in lower velocity error as the trajectory evolves in time. Right: Comparison of simulation/rollout runtimes for a TGV2D trajectory with 125 timesteps and 2500 particles across SPH simulation.", "description": "The left plot shows the velocity error of the UPT model over time, showcasing its ability to accurately learn and simulate the underlying field dynamics.  The right plot compares the runtime performance of UPT against SPH, SEGNN, and GNS for simulating a TGV2D trajectory. UPT demonstrates significantly faster simulation times.", "section": "4.3 Lagrangian fluid dynamics"}]