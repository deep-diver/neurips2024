[{"figure_path": "Swh8LxuycA/tables/tables_7_1.jpg", "caption": "Table 1: Majority vote versus Q-value filtering performance. Accuracy shows the final majority voting accuracy on the respective benchmarks. Prop. Correct is the proportion of the correct class in the final sample set considered. Avg. Filtered is the average percentage (out of 50 total generations) that were discarded across all problems in the benchmark. Avg. Tokens Saved is the average percentage of tokens that are saved with the assumption that the remaining tokens after the first negative Q-value is discarded across all the problems in the benchmark. Both the difference in accuracy and proportion of correct class are t-test significant.", "description": "This table compares the performance of majority voting with Q-value filtering for mathematical reasoning tasks.  It shows accuracy improvements after filtering and the percentage of tokens saved by filtering out low-scoring generations.", "section": "4.1.3 Directly Using the Learned Representations"}, {"figure_path": "Swh8LxuycA/tables/tables_8_1.jpg", "caption": "Table 2: Accuracy of the policy model trained via PPO using a preference ranking reward model vs our Q-Function reward model. We present average accuracy across 4 independent runs. The base model results are also shown as a reference point presented in [62] and DPO results are shown as a strong baseline. The full table with confidence intervals is in the appendix (Table 6).", "description": "This table compares the accuracy of policy models trained using Proximal Policy Optimization (PPO) with different reward models: a preference-ranking reward model and the proposed Q-Function reward model.  It shows the average accuracy across four independent runs for both in-distribution (ID) and out-of-distribution (OOD) datasets.  The results are compared to a baseline model and Direct Preference Optimization (DPO).  The full table with confidence intervals is available in Appendix Table 6.", "section": "4.1.4 Aligning Policy Models with RLHF"}, {"figure_path": "Swh8LxuycA/tables/tables_9_1.jpg", "caption": "Table 3: Comparison of model performances in different settings. (a) Reward model accuracy on the Helpful-Harmless test set. Our result highlights that the representations learned by the Q-function are helpful for discerning between desirable and undesirable sequences in the natural language setting. (b) Winrate (%) vs. SFT model. In the Q-Function prototype setting, we use the cosine similarity of the sequence and the prototype (made from 20 examples that score high on all 4 categories) to score each beam. In the SFT setting, the model is fine-tuned on the same examples as the prototype and guided decoding is run via model scores. Evaluation is done via GPT-4.", "description": "This table presents a comparison between different approaches for natural language alignment. Part (a) shows that the Q-function reward model outperforms the baseline Llama 8b reward model in terms of accuracy on the Helpful-Harmless dataset. Part (b) demonstrates the effectiveness of using the Q-function model representations for guided decoding, showing improvements in helpfulness, correctness, coherence, and complexity compared to supervised fine-tuning (SFT).", "section": "4.2 Aligning Policy Models with RLHF"}, {"figure_path": "Swh8LxuycA/tables/tables_17_1.jpg", "caption": "Table 4: Hyper-parameters for Reward Model training", "description": "This table shows the hyperparameters used for training the reward model.  The hyperparameters include batch size, learning rate, representation dimension, contrastive lambda, number of epochs, the preference ranking loss function, the optimizer used, and the random seed.  These settings were used to train both the baseline reward model and the Q-Function reward model, as well as any reward model ablations. ", "section": "A.1 Reward Model Training and Evaluation Details"}, {"figure_path": "Swh8LxuycA/tables/tables_18_1.jpg", "caption": "Table 5: Hyper-parameters for PPO training", "description": "This table lists the hyperparameters used for training the policy model using the Proximal Policy Optimization (PPO) algorithm.  The hyperparameters control various aspects of the training process, including batch sizes, learning rates, reward normalization, and the generation length of sequences.  The table provides values for each hyperparameter used in the experiments described in the paper. Note that the representation dimension refers to the dimensionality of the hidden state vectors used in the reward model.", "section": "4.1.4 Aligning Policy Models with RLHF"}, {"figure_path": "Swh8LxuycA/tables/tables_18_2.jpg", "caption": "Table 2: Accuracy of the policy model trained via PPO using a preference ranking reward model vs our Q-Function reward model. We present average accuracy across 4 independent runs. The base model results are also shown as a reference point presented in [62] and DPO results are shown as a strong baseline. The full table with confidence intervals is in the appendix (Table 6).", "description": "This table compares the performance of three different methods for training a policy language model using reinforcement learning with human feedback (RLHF).  The methods are: Proximal Policy Optimization (DPO), RLHF with a standard preference-ranking reward model (Codellama PPO), and RLHF with the proposed goal-conditioned Q-function reward model (Q-Function PPO). Performance is evaluated across several math benchmarks, distinguishing between in-distribution (ID) and out-of-distribution (OOD) datasets.  The table shows the average accuracy across four independent runs for each method and benchmark, along with confidence intervals (found in the appendix).", "section": "4.1.4 Aligning Policy Models with RLHF"}, {"figure_path": "Swh8LxuycA/tables/tables_19_1.jpg", "caption": "Table 7: Hyper-parameters for Natural Language Reward Model training", "description": "This table lists the hyperparameters used for training the natural language reward model.  It includes the batch size, learning rate, representation dimension, contrastive lambda, number of epochs, preference ranking loss function, optimizer used, and the random seed.", "section": "B.1 Training a Natural Language Reward Model"}, {"figure_path": "Swh8LxuycA/tables/tables_20_1.jpg", "caption": "Table 8: Majority vote accuracy and average proportion correct when filtering examples using last token and random token sampling for constructing the goal state.", "description": "This table presents the results of an ablation study comparing two methods for constructing the goal state in a Q-value filtering process.  The \"Last Token\" method uses the representation of the last token in the sequence, while the \"Random Token\" method randomly samples a token from the sequence.  The table shows that the \"Last Token\" method achieves slightly better accuracy and a higher proportion of correct sequences for both the GSM8k and MATH datasets. This suggests that the information contained in the last token of a sequence is more relevant for representing the overall goal than a randomly selected token.", "section": "4.1.3 Directly Using the Learned Representations"}, {"figure_path": "Swh8LxuycA/tables/tables_21_1.jpg", "caption": "Table 9: Comparison of AUROC scores for different training time contrastive goal states on several popular math benchmarks. Pref. rank is the reward model trained with the standard preference-ranking objective, Q-Function (SGS) is trained with the method described in Section 3.2, Q-Function (RS) trained with the randomly sampled goal-state and Q-Function (AVG) trained with the batch averaged goal state.", "description": "This table compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores achieved by different methods for training reward models on mathematical reasoning tasks.  It shows the performance across in-distribution (ID) and out-of-distribution (OOD) datasets using three different goal-state sampling techniques: Single Goal State (SGS), Random Sampling (RS), and Average Goal State (AVG). The results help analyze how the choice of goal state representation impacts the performance of the reward model.", "section": "D.1 Exploring Sources of Goal States"}, {"figure_path": "Swh8LxuycA/tables/tables_22_1.jpg", "caption": "Table 10: Comparison of AUROC scores for different source and goal state sampling method.", "description": "This table presents the Area Under the Receiver Operating Characteristic (AUROC) scores achieved by three different methods of sampling source and goal states during the training of a reward model.  The AUROC scores are given for both in-distribution (ID) and out-of-distribution (OOD) datasets across several math benchmarks.  The three methods are: \n1. Random Source and Goal State: Source and goal states are sampled randomly.\n2. Late Goal State: Goal states are sampled from later tokens in the sequence.\n3. Late Source and Goal State: Both source and goal states are sampled from later tokens in the sequence. The results indicate how the sampling strategy affects the model's performance in identifying correct and incorrect solutions.", "section": "D.2 Source State and Goal State Sampling Ablation"}, {"figure_path": "Swh8LxuycA/tables/tables_23_1.jpg", "caption": "Table 1: Majority vote versus Q-value filtering performance. Accuracy shows the final majority voting accuracy on the respective benchmarks. Prop. Correct is the proportion of the correct class in the final sample set considered. Avg. Filtered is the average percentage (out of 50 total generations) that were discarded across all problems in the benchmark. Avg. Tokens Saved is the average percentage of tokens that are saved with the assumption that the remaining tokens after the first negative Q-value is discarded across all the problems in the benchmark. Both the difference in accuracy and proportion of correct class are t-test significant.", "description": "This table compares the performance of majority voting with Q-value filtering for mathematical reasoning.  It shows accuracy, the proportion of correct classifications, the average percentage of generations filtered, and the average percentage of tokens saved by filtering for both GSM8k and MATH datasets.", "section": "4.1.3 Directly Using the Learned Representations"}, {"figure_path": "Swh8LxuycA/tables/tables_27_1.jpg", "caption": "Table 12: Comparison of AUROC scores for different reward projection strategy on several popular math benchmarks. Both Q-Function (Linear) and Q-Function (MLP) are trained with the method described in Section 3.2, the only difference being the reward head is a single linear layer versus a 3-layer MLP followed by a linear layer.", "description": "This table compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for different reward model projection strategies across several mathematical reasoning benchmarks. The AUROC score measures the ability of a reward model to correctly distinguish between correct and incorrect solutions.  Two methods are compared: one using a linear layer for the reward head, and the other using a multi-layer perceptron (MLP) with ReLU activation. The table shows the AUROC scores for both in-distribution (ID) and out-of-distribution (OOD) benchmarks.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/tables/tables_27_2.jpg", "caption": "Table 13: Correlation between Q-value and reward score at different percentile of model generations.", "description": "This table compares the correlation between Q-values and reward scores at different percentiles (0.2, 0.4, 0.6, 0.8) of model generations for two models: Q-Function (Linear) and Q-Function (MLP).  The Q-Function (Linear) model shows a much higher correlation than the Q-Function (MLP) model across all percentiles. This suggests that using a multi-layer perceptron (MLP) for the reward projection, as in Q-Function (MLP), helps to reduce the correlation between the Q-values and reward scores, potentially mitigating the risk of the model gaming the reward during training.", "section": "H Exploring Correlation Between Q-values and Reward Scores"}, {"figure_path": "Swh8LxuycA/tables/tables_28_1.jpg", "caption": "Table 6: Average accuracy and 95% confidence interval after the base policy model was trained via DPO, PPO using a preference ranking reward model, and PPO using our Q-Function reward model, across 4 different independent runs.", "description": "This table compares the performance of three different methods for training a policy model: Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO) with a standard preference ranking reward model, and PPO with a reward model trained using the proposed Q-Function method.  The accuracy and 95% confidence intervals are reported for both in-distribution (ID) and out-of-distribution (OOD) benchmark datasets. The results demonstrate the effectiveness of the proposed Q-Function method in improving the policy model's performance, especially for OOD datasets.", "section": "4.1.4 Aligning Policy Models with RLHF"}, {"figure_path": "Swh8LxuycA/tables/tables_31_1.jpg", "caption": "Table 1: Majority vote versus Q-value filtering performance. Accuracy shows the final majority voting accuracy on the respective benchmarks. Prop. Correct is the proportion of the correct class in the final sample set considered. Avg. Filtered is the average percentage (out of 50 total generations) that were discarded across all problems in the benchmark. Avg. Tokens Saved is the average percentage of tokens that are saved with the assumption that the remaining tokens after the first negative Q-value is discarded across all the problems in the benchmark. Both the difference in accuracy and proportion of correct class are t-test significant.", "description": "This table presents the results of comparing majority voting with a Q-value filtering method to prune incorrect solutions. It shows the accuracy, proportion of correct solutions, average percentage of filtered generations, and average percentage of tokens saved for both methods on the GSM8k and MATH datasets.  The statistical significance of the accuracy difference between the two approaches is noted.", "section": "4.1.3 Directly Using the Learned Representations"}]