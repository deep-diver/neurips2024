[{"figure_path": "APBq3KAmFa/figures/figures_2_1.jpg", "caption": "Figure 1: Two solutions to ReLU neural network interpolation (blue) of training data (red). The functions on the left and right both interpolate the data and both are global minimizers of (2) and (3), and minimize the second-order total variation of the interpolation function Parhi and Nowak (2021). In fact, all convex combinations of the two solutions above are also global solutions to both training problems.", "description": "The figure shows two different solutions obtained by training a ReLU neural network to interpolate the same dataset. Both solutions minimize the sum of squared weights and the second-order total variation of the interpolating function, highlighting the non-uniqueness of solutions in single-task neural network training.  The solutions differ significantly in their shape, demonstrating that even though they are both optimal solutions, one of them is visually much more appealing.", "section": "Introduction"}, {"figure_path": "APBq3KAmFa/figures/figures_4_1.jpg", "caption": "Figure 2: The connect-the-dots interpolant fD = (fD1, fD2, fD3) of three datasets D1, D2, D3.", "description": "This figure shows an example of the connect-the-dots interpolant for three different datasets. Each dataset has its own set of data points which are connected by a straight line. This creates a piecewise linear function for each dataset. The figure illustrates that the connect-the-dots interpolant is a simple, yet effective, way to approximate a function that fits the given data. This method is important in the context of the paper as it relates to finding unique solutions for multi-task neural network training problems.", "section": "3 Univariate Multi-Task Neural Network Solutions"}, {"figure_path": "APBq3KAmFa/figures/figures_5_1.jpg", "caption": "Figure 3: The function output ft around the knot at x, where \u03c4 = x\u2212x1/x2\u2212x1. Each line segment in the figure is labeled with its slope. For any particular output t, it may be the case that ft does not have a knot at x (in which case dt = 0); does not have a knot at x1 (in which case at = bt + \u03c4\u03b4t); and/or does not have a knot at x2 (in which case bt \u2212 (1\u2212\u03c4)\u03b4t = ct).", "description": "This figure illustrates the function output (ft) around a knot at point x.  It shows how the slopes (at, bt, ct, dt) change around the knot and how the removal of the knot affects the representation cost.  The figure clarifies that the absence of a knot at points x1 or x2 results in specific equalities among the slopes.", "section": "3.1 Connect-the-dots Interpolation is Almost Always the Unique Solution to (3)"}, {"figure_path": "APBq3KAmFa/figures/figures_9_1.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task ReLU neural network interpolation in 2D. Single-task solutions show multiple global minimizers, while multi-task solutions are nearly identical and close to the RKHS approximation.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_9_2.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares solutions obtained from single-task and multi-task training of ReLU neural networks. Single-task training yields multiple solutions, while multi-task training with many diverse tasks yields a unique solution that closely resembles the solution to a kernel method.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_9_3.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task ReLU neural network interpolation results on a simple 2D dataset.  Single-task solutions show non-uniqueness, while multi-task learning with many tasks produces a nearly unique, smooth solution similar to a kernel method.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_9_4.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving the multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task solutions for a 2D interpolation problem. Single-task solutions show non-uniqueness, while multi-task solutions are almost always unique and approximate the solution to a kernel method.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_9_5.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task ReLU neural network solutions for a simple 2D interpolation problem. Single-task solutions exhibit high variability, while multi-task solutions (with many diverse tasks) converge to a nearly identical, smooth solution resembling a kernel method.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_13_1.jpg", "caption": "Figure 3: The function output ft around the knot at x, where \u03c4 = (x2 \u2212 x)/(x2 \u2212 x1). Each line segment in the figure is labeled with its slope. For any particular output t, it may be the case that ft does not have a knot at x (in which case dt = 0); does not have a knot at x1 (in which case at = bt + \u03c4\u03b4t); and/or does not have a knot at x2 (in which case bt \u2212 (1 \u2212 \u03c4)\u03b4t = ct).", "description": "This figure shows a piecewise linear function with a knot at point x. It illustrates how the slopes of the function (at, bt, ct, dt) change around the knot, and how the removal of the knot affects the representational cost of the function.", "section": "3.1 Connect-the-dots Interpolation is Almost Always the Unique Solution to (3)"}, {"figure_path": "APBq3KAmFa/figures/figures_13_2.jpg", "caption": "Figure 5: Left: a function g which has a knot in one or more of its outputs at a point x \u2208 (x1, x2). Right: the connect-the-dots interpolant fD. The representational cost of g is strictly greater than that of fD.", "description": "This figure shows two plots that illustrate the concept behind Lemma 3.2. The left plot shows a function g with a knot at some point x between two data points x1 and x2. The right plot shows the connect-the-dots interpolant fD, which is a piecewise linear function connecting the data points without any extra knots. The caption highlights that the representational cost R(g) of the function with the knot is strictly greater than the representational cost R(fD) of the connect-the-dots interpolant.", "section": "3.1 Connect-the-dots Interpolation is Almost Always the Unique Solution to (3)"}, {"figure_path": "APBq3KAmFa/figures/figures_15_1.jpg", "caption": "Figure 6: Top Row: Three randomly initialized neural networks trained to interpolate the five red points with minimum path-norm. Bottom Row: Three randomly initialized two-output neural networks trained to interpolate a multi-task dataset with minimum path-norm. The labels for the first task are the five red points shown while the labels for the second were randomly randomly sampled from a standard Gaussian distribution.", "description": "This figure demonstrates the difference between solutions obtained from single-task vs. multi-task training. The top row shows three randomly initialized neural networks, each trained to interpolate five data points (red dots) with minimum representational cost, highlighting non-uniqueness in single-task solutions.  The bottom row presents the solution obtained for the first output of a multi-task network (with the same five red points as the first task, and a second task with labels randomly sampled from a standard Gaussian distribution). This illustrates the uniqueness and connect-the-dots nature of multi-task solutions.", "section": "Additional Experimental Results and Details"}, {"figure_path": "APBq3KAmFa/figures/figures_16_1.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving (23) are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task solutions to ReLU neural network interpolation problems in 2D. The single-task solutions show significant variability, while the multi-task solution is unique and similar to the solution of a kernel method.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_17_1.jpg", "caption": "Figure 8: Sparsity pattern for output weight matrix of the multi-task student network. The kth column in the matrix corresponds to the output weight of the kth neuron. We observe that each neuron either contributes to all the tasks or none.", "description": "This figure shows the sparsity pattern of the output weight matrix for a multi-task neural network. Each column represents a neuron, and each row represents a task. A black square indicates that the neuron contributes to that task, while a white square indicates that it does not. The pattern shows that most neurons either contribute to all tasks or to none, illustrating neuron sharing behavior in multi-task learning.", "section": "High Dimensional Multivariate Experiments"}, {"figure_path": "APBq3KAmFa/figures/figures_17_2.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task ReLU neural network interpolations in 2D.  Single-task solutions show non-uniqueness, while multi-task solutions (with many tasks) converge to a unique solution resembling a kernel method solution.", "section": "Multivariate Multi-Task Neural Network Training"}, {"figure_path": "APBq3KAmFa/figures/figures_18_1.jpg", "caption": "Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \u03bb \u2248 0. Top Row \u2013 Solutions to single-task training: Figures 4a, 4b and 4c show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of 0 at the vertices. The inner square has side-length one and values of 1 at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., T = 1). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). Bottom Row \u2013 Solutions to multi-task training: Figure 4d shows the solution to the first output of a multi-task neural network with T = 101 tasks. The first output is the original task depicted in the first row while the labels for other 100 tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix 7 showing that this phenomenon holds across many runs. Figure 4e shows the solution to fitting the training data by solving (23) over a fixed set of features learned by the multi-task neural network with T = 100 random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving the multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (23) is also similar to the solution of the full multi-task training problem with all T = 101 tasks.", "description": "This figure compares single-task and multi-task solutions for a 2D interpolation problem. Single-task solutions show non-uniqueness, while multi-task solutions are nearly identical and closely approximate the RKHS solution.", "section": "Multivariate Multi-Task Neural Network Training"}]