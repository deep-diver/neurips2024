[{"figure_path": "TzxSrNJE0T/tables/tables_6_1.jpg", "caption": "Table 1: Bilevel and Conditional Stochastic Optimization with our Biased Adaptive SA framework.", "description": "This table summarizes the applicability of the Biased Adaptive Stochastic Approximation framework to Stochastic Bilevel Optimization and Conditional Stochastic Optimization problems. It shows which assumptions (H2, H3, H5) are satisfied for each problem and the resulting convergence theorems used. It highlights the Lipchitz constant, bias control, gradient bound, and the convergence result for each optimization problem.", "section": "5 Applications and Experiments"}, {"figure_path": "TzxSrNJE0T/tables/tables_8_1.jpg", "caption": "Table 1: Bilevel and Conditional Stochastic Optimization with our Biased Adaptive SA framework.", "description": "This table summarizes the application of the Biased Adaptive Stochastic Approximation (BSA) framework to Bilevel and Conditional Stochastic Optimization problems.  It lists the key assumptions (Lipschitz Constant H2, Bias Control H3, Gradient Bound H5) required for the convergence analysis, and the corresponding lemmas and theorems that prove the convergence rate in each setting. The table allows readers to easily compare the conditions and results for the two different types of problems.", "section": "Applications and Experiments"}, {"figure_path": "TzxSrNJE0T/tables/tables_33_1.jpg", "caption": "Table 1: Bilevel and Conditional Stochastic Optimization with our Biased Adaptive SA framework.", "description": "This table summarizes the assumptions (H2, H3, H5) and convergence results for Stochastic Bilevel Optimization and Conditional Stochastic Optimization using the Biased Adaptive Stochastic Approximation framework proposed in the paper.  It shows how the theoretical results apply to these specific applications, indicating which assumptions are satisfied and the corresponding convergence rates (O(log n/\u221an + bn)).  The table highlights the similarities and differences in the application of the framework to these two distinct optimization problems.", "section": "Applications and Experiments"}, {"figure_path": "TzxSrNJE0T/tables/tables_37_1.jpg", "caption": "Table 3: Comparison of Negative Log-Likelihood on the FashionMNIST Test Set (Lower is Better).", "description": "This table presents the negative log-likelihood values achieved on the FashionMNIST test set using various optimization algorithms (SGD, SGD with momentum, Adagrad, RMSProp, and Adam) and different generative models (VAE, IWAE, and BR-IWAE). Lower values indicate better performance.  The results demonstrate the relative performance of these methods in this specific setting.", "section": "Experiments with IWAE and BR-IWAE"}]