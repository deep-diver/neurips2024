[{"figure_path": "w4AnTVxAO9/figures/figures_1_1.jpg", "caption": "Figure 1: Step skipping in equation simplification. We use the specified number of steps in the input as a stimulation to induce the model to perform skipping by using fewer steps.", "description": "This figure illustrates the concept of step skipping in solving an equation.  The left side shows a step-by-step solution with four steps.  The right side shows the same equation solved in two steps, skipping two intermediate steps. This highlights human expert behavior where practice enables more efficient problem-solving by omitting intermediate but obvious steps.", "section": "1 Introduction"}, {"figure_path": "w4AnTVxAO9/figures/figures_2_1.jpg", "caption": "Figure 2: Overall framework. The initialization phase aims to equip the model with the ability to reason according to a specified number of steps. During iterations, each cycle produces a mixed dataset Di, which is used to train a standard model to evaluate the model's step-skipping capabilities.", "description": "This figure illustrates the overall framework of the proposed method for inducing step-skipping behavior in language models.  It consists of two main phases: initialization and iteration.  \n\n**Initialization:** A standard model is initially trained on a dataset (D<sub>init</sub>) to solve problems following a specified number of steps. This establishes a baseline reasoning ability.\n\n**Iteration:** The model is iteratively refined. In each iteration, the model is prompted to solve problems using fewer steps than the initial specified number. Correct shorter solutions are added to the training dataset, creating a mixed dataset (D<sub>i</sub>) containing both full and shorter reasoning sequences. A new standard model (M<sub>i</sub>) is then trained on this updated dataset.  This process repeats until the model consistently generates efficient, step-skipping reasoning paths. The model's step-skipping ability is evaluated after each iteration using the standard fine-tuning and inference.", "section": "3 Method"}, {"figure_path": "w4AnTVxAO9/figures/figures_4_1.jpg", "caption": "Figure 3: Illustrations of three different tasks. Each question is accompanied by a comprehensive detailed step-by-step solution.", "description": "This figure provides example questions and detailed step-by-step solutions for three different reasoning tasks used in the paper's experiments: Analog of Algebra, Multi-digit Addition, and Directional Reasoning.  Each task is designed to have clearly defined intermediate steps, making it suitable for studying the model's ability to skip steps during reasoning.", "section": "4.1 Datasets"}, {"figure_path": "w4AnTVxAO9/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison of models across different phases relative to question length and complexity. Models achieve near perfect performance on in-domain data but diverge on lengthy OOD data.", "description": "This figure compares the performance of different models (Llama2 and phi-3-mini) across various iterations of training.  The models are evaluated on in-domain and out-of-distribution (OOD) datasets with varying lengths and complexities. The plots show that while the models achieve almost perfect accuracy on the in-domain datasets, they perform differently on OOD data; the accuracy decreases as the length and complexity increase.  The comparison highlights the impact of incorporating skipped reasoning steps on the models' ability to generalize to more complex scenarios.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/figures/figures_8_1.jpg", "caption": "Figure 3: Illustrations of three different tasks. Each question is accompanied by a comprehensive detailed step-by-step solution.", "description": "This figure shows three different reasoning tasks used in the paper's experiments: Analog of Algebra, Multi-digit Addition, and Directional Reasoning.  Each task is illustrated with an example question and its corresponding step-by-step solution.  The purpose of showing these examples is to highlight the types of problems used to evaluate the models' ability to skip steps in their reasoning process and to clearly define the intermediate steps required for solving each problem. The detailed, step-by-step solutions provide the baseline against which the model's ability to generate shorter, accurate solutions is measured.", "section": "4.1 Datasets"}, {"figure_path": "w4AnTVxAO9/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison of models across different phases relative to question length and complexity. Models achieve near perfect performance on in-domain data but diverge on lengthy OOD data.", "description": "This figure displays the performance comparison of models across different phases in relation to the question length and complexity.  The x-axis likely represents the length of questions (or number of steps), and the y-axis likely shows accuracy and/or the number of steps used in the model's reasoning process. The lines likely represent different training phases (e.g., initial training, iterations).  The figure shows that all models perform near perfectly on in-domain data (shorter questions), but their performance diverges significantly on out-of-domain (OOD) data (longer questions). This demonstrates the impact of the step-skipping training on the model's ability to generalize to more complex problems.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of models across different phases relative to question length and complexity. Models achieve near perfect performance on in-domain data but diverge on lengthy OOD data.", "description": "This figure presents a comparison of model performance across different phases of training, specifically focusing on how question length and complexity affect accuracy.  The in-domain data shows near-perfect accuracy for all models, regardless of the training phase. However, performance on out-of-distribution (OOD) data, particularly the lengthy OOD data, shows significant differences among various models and training phases. This illustrates how the models' ability to generalize to complex and less-structured problems varies based on the training method and iterative step-skipping process.", "section": "5 Results"}, {"figure_path": "w4AnTVxAO9/figures/figures_14_1.jpg", "caption": "Figure 8: Skipping data accuracy change during cold start in Analog of Algebra.", "description": "This figure shows how the accuracy of the model's ability to skip steps improves over the iterations during the training process with cold start.  The x-axis represents the iteration number, and the y-axis represents the percentage accuracy of correctly skipped steps. Two lines are shown, one for skipping one step (i=1) and one for skipping two steps (i=2).  The plot illustrates that the accuracy increases with each iteration, demonstrating the model's learning and improvement in its step-skipping capabilities.", "section": "B.2 Skipping data accuracy trend in cold start"}, {"figure_path": "w4AnTVxAO9/figures/figures_17_1.jpg", "caption": "Figure 3: Illustrations of three different tasks. Each question is accompanied by a comprehensive detailed step-by-step solution.", "description": "This figure shows three different reasoning tasks used in the paper's experiments: Analog of Algebra, Multi-digit Addition, and Directional Reasoning. Each task presents a problem with a detailed step-by-step solution to guide the model's reasoning process. The complexity of steps varies across tasks, making it suitable for evaluating models' ability to perform step-skipping behaviors during reasoning.", "section": "4.1 Datasets"}, {"figure_path": "w4AnTVxAO9/figures/figures_18_1.jpg", "caption": "Figure 3: Illustrations of three different tasks. Each question is accompanied by a comprehensive detailed step-by-step solution.", "description": "This figure shows three example tasks used in the paper's experiments to evaluate the model's step-skipping abilities.  Each task (Analog of Algebra, Multi-digit Addition, and Directional Reasoning) is illustrated with a sample question and its corresponding solution, which is broken down into a series of detailed steps. This helps demonstrate the type of reasoning problems the model is trained and tested on, highlighting the complexity that step-skipping can simplify.", "section": "4.1 Datasets"}, {"figure_path": "w4AnTVxAO9/figures/figures_19_1.jpg", "caption": "Figure 1: Step skipping in equation simplification. We use the specified number of steps in the input as a stimulation to induce the model to perform skipping by using fewer steps.", "description": "This figure illustrates the concept of step-skipping in equation solving.  The left side shows a problem solved step-by-step, following a complete and detailed solution process. The right side demonstrates the same problem solved by an expert, skipping some intermediate steps to reach the solution more efficiently.  This difference highlights the ability of human experts to streamline their reasoning by omitting redundant or obvious steps. The figure serves as a visual example used to guide language models to learn and adopt this step-skipping behavior.", "section": "1 Introduction"}]