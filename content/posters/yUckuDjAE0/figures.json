[{"figure_path": "yUckuDjAE0/figures/figures_3_1.jpg", "caption": "Figure 1: Learning a BD in two dimensions. (a) The original point is x = (0,0), the noisy perturbations are in blue, the corrupted points \u03c4(x) (in red) have angles between \u03c0 and \u03c0. (b) Heat map of the L2-distance to the origin, which is unable to distinguish corrupted from noisy points. (c) Heat map of our learned BD trained on the samples in (a), which considers corrupted points very close compared to noisy points.", "description": "This figure shows a comparison of L2 distance and learned Bregman divergence in a 2D space. (a) shows training samples with original point at (0,0), noisy points in blue, and corrupted points in red. (b) shows the heatmap of L2 distance, failing to distinguish between noisy and corrupted points. (c) shows the heatmap of learned Bregman divergence, successfully distinguishing between noisy and corrupted points.", "section": "3 Learning a BD"}, {"figure_path": "yUckuDjAE0/figures/figures_6_1.jpg", "caption": "Figure 2: (a) Noisy (blue) and contrast-corrupted (red) images chosen to have equal distribution in Euclidean distance to the clean image, and (b) the associated distributions of the learned BDs. Done over 10,000 CIFAR-10 test set images.", "description": "This figure compares the distributions of Euclidean distances and learned Bregman divergences between noisy images and contrast-corrupted images with respect to clean images.  The left panel (a) shows that both noisy and corrupted images have a similar distribution of Euclidean distances to the clean images.  The right panel (b) demonstrates that the learned Bregman divergence effectively distinguishes between noisy and corrupted images, showing that corrupted images are significantly closer to the clean image than noisy images are according to the learned Bregman divergence.", "section": "6.1 Learning the BD"}, {"figure_path": "yUckuDjAE0/figures/figures_6_2.jpg", "caption": "Figure 1: Learning a BD in two dimensions. (a) The original point is x = (0,0), the noisy perturbations are in blue, the corrupted points \u03c4(x) (in red) have angles between \u03c0 and \u03c0. (b) Heat map of the L2-distance to the origin, which is unable to distinguish corrupted from noisy points. (c) Heat map of our learned BD trained on the samples in (a), which considers corrupted points very close compared to noisy points.", "description": "This figure illustrates the concept of learning a Bregman Divergence (BD) in a simplified 2D space. It compares the L2 distance (Euclidean distance) and the learned BD in distinguishing between noisy and corrupted data points. The L2 distance fails to differentiate between noisy and corrupted points, while the learned BD successfully identifies corrupted points as closer to the original point than noisy points, showcasing its effectiveness in measuring semantic similarity.", "section": "3 Learning a BD"}, {"figure_path": "yUckuDjAE0/figures/figures_7_1.jpg", "caption": "Figure 1: Learning a BD in two dimensions. (a) The original point is x = (0,0), the noisy perturbations are in blue, the corrupted points \u03c4(x) (in red) have angles between \u03c0 and \u03c0. (b) Heat map of the L2-distance to the origin, which is unable to distinguish corrupted from noisy points. (c) Heat map of our learned BD trained on the samples in (a), which considers corrupted points very close compared to noisy points.", "description": "This figure illustrates the concept of learning a Bregman Divergence (BD) in a 2D space.  Panel (a) shows training samples: the origin (clean image), noisy points (blue), and corrupted points (red). Panel (b) displays the heatmap of the L2-distance from the origin, demonstrating its inability to differentiate between noisy and corrupted data points. Finally, panel (c) presents the heatmap of the learned BD, highlighting its superior performance in distinguishing corrupted from noisy samples by placing corrupted points closer to the origin than noisy points.", "section": "3 Learning a BD"}, {"figure_path": "yUckuDjAE0/figures/figures_17_1.jpg", "caption": "Figure 1: Learning a BD in two dimensions. (a) The original point is x = (0,0), the noisy perturbations are in blue, the corrupted points \u03c4(x) (in red) have angles between \u03c0 and \u03c0. (b) Heat map of the L2-distance to the origin, which is unable to distinguish corrupted from noisy points. (c) Heat map of our learned BD trained on the samples in (a), which considers corrupted points very close compared to noisy points.", "description": "This figure illustrates the difference between the learned Bregman Divergence and the standard L2 distance. In 2D space, the original point is (0,0), noisy points are shown in blue, and corrupted points are shown in red. The L2 distance heat map shows the inability to distinguish between noisy and corrupted points, whereas the learned BD heatmap correctly identifies corrupted points as much closer to the origin than noisy points.", "section": "3 Learning a BD"}, {"figure_path": "yUckuDjAE0/figures/figures_18_1.jpg", "caption": "Figure 6: Evaluation of the Bregman divergence of 256x256 images from ImageNet. The clean images are plotted in the first column, corrupted versions in the second column, and noisy versions (with different noise thresholds) thereafter.", "description": "This figure demonstrates the performance of the learned Bregman divergence on higher-dimensional images (256x256) from the ImageNet dataset.  It shows that even though the Bregman divergence was trained on lower-resolution images (32x32) from CIFAR-10, it is able to effectively distinguish corrupted images from noisy images, highlighting the generalizability of the learned divergence.", "section": "6 Corruption-specific Bregman divergences"}, {"figure_path": "yUckuDjAE0/figures/figures_19_1.jpg", "caption": "Figure 7: Symmetricity test of the trained BD over the 10,000 images of the test set.", "description": "This figure shows the distribution of the absolute differences between D(x' || x) and D(x || x') (blue) and D(\u03c4(x) || x) and D(x || \u03c4(x)) (red) for 10,000 images in the test set.  It demonstrates that the learned Bregman divergence is not perfectly symmetric, but the asymmetry is relatively small. This is important to note, as a perfectly symmetric Bregman divergence is simply a quadratic function, which limits its expressiveness.", "section": "Further results about the learned divergence"}, {"figure_path": "yUckuDjAE0/figures/figures_20_1.jpg", "caption": "Figure 6: Evaluation of the Bregman divergence of 256x256 images from ImageNet. The clean images are plotted in the first column, corrupted versions in the second column, and noisy versions (with different noise thresholds) thereafter.", "description": "This figure shows the performance of the learned Bregman divergence on higher-dimensional images (256x256) from the ImageNet dataset.  It demonstrates the ability of the Bregman divergence to distinguish between corrupted and noisy images even though the model was trained on lower-resolution (32x32) CIFAR-10 images.", "section": "6 Corruption-specific Bregman divergences"}, {"figure_path": "yUckuDjAE0/figures/figures_20_2.jpg", "caption": "Figure 6: Evaluation of the Bregman divergence of 256x256 images from ImageNet. The clean images are plotted in the first column, corrupted versions in the second column, and noisy versions (with different noise thresholds) thereafter.", "description": "This figure shows the performance of the learned Bregman divergence on higher-dimensional images (256x256) from the ImageNet dataset.  It demonstrates the ability of the Bregman divergence to distinguish between corrupted and noisy images even though the model was trained on lower-resolution (32x32) CIFAR-10 images. Each row represents a different image, showing the original, corrupted, and various noisy versions.", "section": "6 Corruption-specific Bregman divergences"}]