{"importance": "This paper is highly important for researchers working on large language models (LLMs) and data efficiency.  It introduces a novel, **scalable data selection method (S2L)** that significantly reduces the training data needed for fine-tuning, which is a crucial challenge in the field. The results demonstrate **substantial improvements in data efficiency across diverse domains**, opening up new avenues for research on efficient LLM training and specialized domain adaptation.", "summary": "SMALLTOLARGE (S2L) revolutionizes large language model (LLM) fine-tuning by using a small model to summarize training loss trajectories, enabling efficient data selection for larger models. ", "takeaways": ["S2L significantly improves data efficiency in LLM fine-tuning, reducing the data needed by up to 89%.", "S2L's effectiveness is validated across diverse tasks (mathematical problem-solving and clinical text summarization), outperforming current state-of-the-art methods.", "S2L is scalable, using a smaller reference model (100x smaller) to proportionally reduce computational cost."], "tldr": "Fine-tuning large language models (LLMs) for specialized tasks is computationally expensive and data-intensive. Existing data selection methods often struggle with data efficiency, especially for specialized domains, because they rely on large, computationally expensive models that generate representations for each training example. These methods are vulnerable to the quality of the training data and the computational cost of generating representations increases with model size. \nThe proposed method, SMALLTOLARGE (S2L), addresses these issues by training a small model to cluster training examples based on their loss trajectories.  S2L selects a subset of data from these clusters, ensuring that the selected data is representative of the entire dataset.  Experiments show that S2L significantly improves data efficiency in fine-tuning LLMs for mathematical problem-solving and clinical text summarization, achieving comparable or even better performance than training on the full dataset while using only a small fraction of the data.  S2L also achieves superior data efficiency compared to other state-of-the-art data selection methods, and it is scalable to large models, using a reference model that is 100x smaller than the target model.", "affiliation": "UC Los Angeles", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "K9IGlMQpif/podcast.wav"}