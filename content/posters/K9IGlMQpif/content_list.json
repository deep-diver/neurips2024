[{"type": "text", "text": "SMALLTOLARGE (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Loss Trajectories of Small Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yu Yang1 Siddhartha Mishra1 Jeffrey Chiang2 Baharan Mirzasoleiman1 1Department of Computer Science, 2Department of Computational Medicine University of California, Los Angeles (UCLA) ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the effectiveness of data selection for pretraining and instruction fine-tuning large language models (LLMs), improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SMALLTOLARGE (S2L), which trains a small model, clusters loss trajectories of the examples, and samples from these clusters to guide data selection for larger models. We prove that during fine-tuning, samples within the same loss trajectory cluster exhibit similar gradients. Then, we show that S2L subsets have a bounded gradient error w.r.t. the full data, hence guarantee convergence to the neighborhood of the optimal solution. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data requirement to just $11\\%$ of the original MathInstruct dataset [64] to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of $4.7\\%$ across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a $32.7\\%$ accuracy on the challenging MATH [19] benchmark, improving Phi-2 [28] by $16.6\\%$ . In clinical text summarization on the MIMIC-III dataset [21], S2L again outperforms training on the full dataset using only $50\\%$ of the data. Notably, S2L can perform scalable data selection using a reference model $100\\times$ smaller than the target model, proportionally reducing the computational cost. 1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, large language models (LLMs) have revolutionized artificial intelligence by demonstrating an unprecedented ability to understand and generate human language [7]. Among all the contributing factors, the quality and selection of data is becoming increasingly recognized for its importance in training LLMs effectively. Recent research indicates that LLMs benefit more from training for additional epochs on carefully curated data rather than on larger, uncurated ones during pretraining [48] and instruction fine-tuning [67], making data selection one of the most promising means to unlock the next level of LLMs\u2019 language capability. However, while generalist models obtained through pre-training or instruction fine-tuning excel in general language tasks, they may not deliver optimal outcomes in specialized domain, such as mathematics [3, 31, 63, 30, 64], code [42, 32], medicine [43, 44, 9], or finance [57, 9]. These domains are not only critical for real-world applications but also hold substantial economic and societal impacts. ", "page_idx": 0}, {"type": "image", "img_path": "K9IGlMQpif/tmp/7e03852a2356136c456f2d6a951989991fb3b105df53ffb1abaa5cd28ad18de0.jpg", "img_caption": ["(a) Hidden states of the Pile on pretrained Pythia-410M "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "K9IGlMQpif/tmp/de87bfa1eafaa2b45986e9c566883035577c4316e7f0aa378efe8b79d75c7ddd.jpg", "img_caption": ["(c) Increase in training time as the size of the model scales up "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "K9IGlMQpif/tmp/b94db3a56bed0cac2e2c8a1fc9b141ef16405cd08f01690a4a36d90ee5251143.jpg", "img_caption": ["Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain [34]. For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 1a), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 1b). Additionally, the cost of training a reference model escalates with model size (Figure 1c), making existing data selection methods for large models prohibitively expensive. ", "(b) Hidden states of MathInstruct on pretrained Pythia-410M "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To maximize performance in specialized domains, models fine-tuned on domain data offer superior capabilities over generalist models [20]. Yet, maximizing the data efficiency in supervised fine-tuning (SFT) for specialized domains remains a challenging and under-explored problem. Firstly, heuristic approaches that are effective in the instruction fine-tuning stage, like manual curation [67] or using advanced models such as GPT-4 for dataset evaluation [8], are less reliable due to the need for specialized knowledge and become costly with large volumes of uncurated fine-tuning data. Beyond these heuristic methods, other approaches rely on generating representations for each training example using a reference model, often utilizing metrics like perplexity [34], confidence [46, 53], or hidden states [1, 48, 61, 4] as features. However, these techniques also fall short in SFT for specialized domains for two reasons: (1) the significant shift between pretraining and SFT data can render these metrics less informative (Figure 1b), and (2) the computation and memory demands associated with generating representations for each training example become prohibitive, as these specialized domains often require larger models, some with up to 540 billion parameters [10, 43], leading to substantial scalability challenges (Figure 1c). ", "page_idx": 1}, {"type": "text", "text": "To tackle the challenges of data efficiency in SFT for specialized domains, we present SMALLTOLARGE (S2L), an effective and scalable data selection algorithm. S2L operates by first gathering training loss trajectories for each training example using a small model. These trajectories are then clustered, and similar number of examples are selected from these clusters uniformly at random. This process is grounded in our theoretical findings that examples within the same cluster exhibit similar gradients during training, thereby affecting the model similarly. Consequently, subsets sampled from these clusters have a bounded gradient error w.r.t. the full data, allowing for training a comparable model with only a subset of data. Furthermore, we provide a convergence rate analysis for training on these subsets, establishing a robust theoretical foundation for S2L\u2019s effectiveness and efficiency. ", "page_idx": 1}, {"type": "text", "text": "To validate S2L\u2019s effectiveness, we applied it to the challenging tasks of SFT for (1) mathematical problem-solving and (2) clinical text summarization. Our experiments on MathInstruct [64] shows that S2L can significantly reduce the required training data size to just $11\\%$ of the original dataset size while still matching the performance levels of the full dataset, outperforming current state-of-the-art one-shot and online data selection algorithms by an average of $4.7\\%$ across 6 in- and out-domain evaluation datasets. Remarkably, on the MATH benchmark [19], S2L attained a $32.7\\%$ accuracy with just 50K data points, improving the best open-sourced model under 3 billion parameters, Phi-2, by $16.6\\%$ . For clinical text summarization tasks on the MIMIC-III [21] dataset, S2L outperforms training on the full dataset, using only half of the data. Unlike existing methods that require training and getting features from large models, S2L achieves superior data efficiency using a model with as few as 70 million parameters, which is $100\\times$ smaller than the largest target model we train with 7 billion parameters. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Foundations of Data Selection. Data selection has been well studied for small models and classification tasks. There are one-shot algorithms that select data based on rankings of the proposed training statistics, for example, the L2-norms of error and gradient vectors (EL2N and GraNd) [39], confidence and its variability across epochs [46], and the number of times each example is learned but then forgot at the subsequent training step [49]. Besides these heuristic indicators, there are embedding-based pruning algorithms [45] and online selection algorithms with theoretical performance guarantees for efficiency [35, 23, 24, 40, 60] and robustness [59, 62, 16]. Coleman et al. proposed to use the intermediate feature representation of a small proxy model to select data for image classification. Most recently, data selection has shown great potential in more substantial training speedup when implemented on near-storage hardware [41], and data selection beyond supervised learning of image data, e.g., for self-supervised learning [22] and multimodal learning [1, 33], also emerged. ", "page_idx": 2}, {"type": "text", "text": "Data Efficient Training of Large Language Models. For the pre-training of LLMs, Marion et al. studied data quality indicators including Perplexity, Error L2-Norm (EL2N) [39], and memorization ranking [5], and found training on examples with middle Perplexity rankings outperforms training on examples selected based on the other two metrics, and sometimes even outperforms training on the entire dataset. Tirumala et al. uses pre-trained model embeddings to select data for LLM pre-training. The proposed algorithm, D4, first applies an embedding-based data de-duplication algorithm [1] and then discards data points that are the closest to the K-Means cluster centroids in the embedding space [45] to ensure diversity. On fine-tuning LLMs, existing work on data efficiency primarily focused on manually curating high-quality instructions [67], or using strong closed-source models (e.g., GPT-4 [2] or ChatGPT) to rate the quality of each training example [18, 27, 8]. Bhatt et al. implemented an experimental design framework to evaluate the existing data selection methods for instruction fine-tuning of LLMs and found selecting facility locations based on hidden representations (i.e., embeddings) is the most effective. As the only data selection algorithm for specialized domains, SCIP [61] focuses on pruning low-quality code data for training code LLMs. Since it relies on breaking the code syntax to understand the characteristics of low-quality code in the embedding (i.e, hidden states) space, adapting SCIP to domains other than Python code data is non-trivial. ", "page_idx": 2}, {"type": "text", "text": "Adapting Large Language Models for Specialized Domains. The rapid development of large language models (LLMs) gives rise to new state-of-the-art models in specialized domains. For mathematical reasoning, Galactica [47], MINERVA [26] and Llemma [3] continue to train an LLM on large-scale math-related web data to improve a model\u2019s general scientific reasoning; WizardMath [31] and TinyGSM [30] fine-tune LLMs using supervised data. Similarly for medical LLMs, Cheng et al. continued training pre-trained LLMs on medical text, and [43, 44] fine-tuned PaLM with instruction prompt tuning on medical domain data. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLM Fine-tuning Objective. Consider a transformer-based language model, parameterized by $\\pmb{\\theta}$ , and denoted as $p_{\\theta}$ . This model, when provided with a sequence of prompt tokens $\\mathbf{x}=(x_{1},\\hdots,x_{M})$ , generates a sequence of response tokens $\\mathbf{y}=(y_{1},\\dots,y_{L})$ . The conditional probability of generating $\\mathbf{y}$ given $\\mathbf{x}$ is then formulated as ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\pmb{\\theta}}(\\mathbf{y}|\\mathbf{x})=\\prod_{l=1}^{L}p_{\\pmb{\\theta}}(y_{l}|\\mathbf{y}_{1:l-1},\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that $\\mathbf{y}_{1:0}$ is an empty sequence. To adapt the pre-trained LLM for a specialized domain of distribution $\\mathcal{D}$ , supervised fine-tuning (SFT) is usually employed with a domain-specific training dataset $D_{\\mathrm{train}}\\,=\\,\\bar{\\{({\\bf x},{\\bf y})_{i}\\}}_{i=1}^{n}\\sim\\mathcal{D}$ containing pairs of prompt $\\mathbf{x}$ and annotated response y. The fine-tuning objective is thus to minimize the following negative log likelihood loss, expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta,D_{\\mathrm{train}})=-\\frac{1}{n}\\sum_{(\\mathbf{x},\\mathbf{y})_{i}\\in D_{\\mathrm{train}}}\\big[\\log p_{\\theta}(\\mathbf{y}_{i}|\\mathbf{x}_{i})\\big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Data Selection Objective. In a general setting for data selection, we consider a target language model $p_{\\theta}$ with parameters $\\pmb{\\theta}$ . Given a fixed data budget $B$ , which constrains the number of data points that can be used for training, our objective is to select a subset $S\\subseteq D_{\\mathrm{train}}$ to train the target model, such that it obtains a superior generalization performance. In practice, the subset $S$ is selected based on a reference model $r_{\\phi}$ parameterized by $\\phi$ , which generates representations, confidence scores, or other metrics for each data point $(\\mathbf{x},\\mathbf{y})_{i}\\in D_{\\operatorname{train}}$ , denoted by $r_{\\phi}((\\mathbf{x},\\mathbf{y})_{i})$ , which will be utilized by a data selection algorithm to produce $S$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In existing data selection algorithms, $\\phi$ is commonly either weights of the pre-trained target model or a target model that has been fully trained on the dataset $D_{\\mathrm{train}}$ . However, as evidenced by Figure 1, representations generated by the pretrained model may not always be good enough for data selection in specialized domains, and fine-tuning the target model significantly increases the computational cost of data selection. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Training a large target model to obtain feature representations for each example in $D_{\\mathrm{train}}$ can be computationally intensive. However, a recent finding demonstrates that the training dynamics of most examples are consistent across differently sized models of the same family, and this phenomena even generalizes across different model families [58]. Our proposed method, SMALLTOLARGE (S2L), leverages loss trajectories of training examples collected during fine-tuning a small reference model on the full or a subset of training data. ", "page_idx": 3}, {"type": "text", "text": "Loss Trajectory. Let $\\phi^{(t)}$ be the parameters of a small LM during training on $D_{\\mathrm{train}}$ at times $t_{q},q\\in\\{1,...,T\\}$ . S2L records the loss trajectory for each data point $i$ at times $t_{q}$ during training the reference model $[\\mathcal{L}_{i}^{\\mathrm{proxy}}(\\phi^{(t_{1})}),\\cdot\\cdot\\cdot,\\mathcal{L}_{i}^{\\mathrm{proxy}}(\\phi^{(t_{T})})]$ where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{i}^{\\mathrm{proxy}}(\\phi^{(t)})=\\mathcal{L}^{\\mathrm{proxy}}(\\phi^{(t)},(\\mathbf{x}_{i},\\mathbf{y}_{i}))=-\\log p_{\\phi^{(t)}}(\\mathbf{y}_{i}|\\mathbf{x}_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $T$ is the length of the loss trajectory. Note that $\\phi^{(t)}$ is trained for a fixed number of iterations from $\\phi^{(t-1)}$ . ", "page_idx": 3}, {"type": "text", "text": "Assume the parameter vector ${\\pmb\\theta}^{(t)}$ represents the parameters of the target model at the time $t$ . Define $\\mathbf{L}_{i}^{\\mathrm{proxy}}\\,=\\,[\\bar{\\mathcal{L}}_{i}^{\\mathrm{proxy}}(\\phi^{(t_{1})}),\\dots,\\mathcal{L}_{i}^{\\mathrm{proxy}}\\bar{(}\\phi^{(t_{T})})]$ and $\\mathbf{L}_{i}^{\\mathrm{target}}\\,=\\,[\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta}^{(\\Bar{t}_{1})}),\\dots,\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta}^{(t_{T})})]$ Ltiarget(\u03b8(tT ))] as the training loss trajectory of the example $i$ on the small proxy model and the large target model, respectively. Let $\\pmb{H}_{i}\\doteq\\mathbb{R}^{d\\times d}$ be the Hessian matrix for each example $i$ and assume that the loss function for each example during fine-tuning can be modeled by a second-order Taylor approximation with bounded curvature $\\displaystyle(c\\leq\\,\\|H_{i}\\|\\,\\leq\\,C)$ , a reasonable assumption in fine-tuning settings. The following lemma shows that examples with similar loss trajectories on the proxy model have similar gradients throughout the training of the target model. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. If examples i and $\\mathbf{L}_{j}^{p r o x y}\\parallel\\,\\leq\\,\\epsilon,$ and their loss trajectories on the proxy and target model is similar, i.e., $j$ have similar loss trajectories on the proxy model, i.e., $\\|{\\bf L}_{p}^{p r o x y}\\mathrm{~-~}$ $\\|{\\bf L}_{i}^{p r o x y}-$ $\\mathbf{L}_{p}^{\\bar{t}a r g e t}\\|\\leq\\delta$ for , then i and $j$ have similar gradients throughout training the target model: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}_{i}^{t a r g e t}(\\pmb{\\theta})-\\nabla\\mathcal{L}_{j}^{t a r g e t}(\\pmb{\\theta})\\|\\leq\\frac{2\\epsilon^{\\prime}+2C D^{2}}{d}=\\Delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon^{\\prime}=\\epsilon+2\\delta$ and $\\lVert\\pmb{\\theta}\\rVert\\leq D$ for all $t$ . ", "page_idx": 3}, {"type": "text", "text": "The proof of Theorem 4.1 can be found in Appendix A.1. Theorem 4.1 shows that examples with similar loss trajectories have similar gradients during the training, thereby influencing the model in a similar manner. ", "page_idx": 3}, {"type": "text", "text": "Data selection from Loss Trajectory Clusters. Once the loss trajectories are recorded on the proxy model, we apply a clustering algorithm to group examples based on the similarity of their loss trajectories. This results in a set of clusters $\\{C_{1},C_{2},\\ldots,C_{K}\\}$ , where each cluster $C_{i}$ contains examples with similar loss and gradient trajectory throughout the training: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{i}=\\{(\\mathbf{x},\\mathbf{y})_{j}\\in D_{\\operatorname{train}}|i=\\arg\\operatorname*{min}_{j\\in[K]}d(\\mathbf{L}_{j},\\mathbf{L}_{\\bar{C}_{j}})\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{L}_{\\bar{C_{i}}}$ is the centroid of the loss trajectories in cluster $C_{i}$ , and $d(\\cdot,\\cdot)$ is a distance metric, such as Euclidean distance, used for clustering. For datasets that contain different sources of data, we cluster each source separately. ", "page_idx": 3}, {"type": "image", "img_path": "K9IGlMQpif/tmp/40c0b1c03f3c0f6cf72e97a678881d46a628e1b37cff3d2d2ce9c31f33b6403f.jpg", "img_caption": ["(a) In the same cluster. (b) In different clusters. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Examples in the same clusters have very similar loss trajectories (Figure 2a) while the loss trajectories of examples in different clusters are very different (Figure 2b). ", "page_idx": 4}, {"type": "image", "img_path": "K9IGlMQpif/tmp/00d013c25aa19224bfd893b98b700b43855297068fcd755aa6fa21d483fc304f.jpg", "img_caption": ["Figure 3: Examples in the same clusters of training trajectories on a small model (Pythia-70M) also have similar training trajectories on a large model (Pythia-2.8B), even if the trends may not be the same on both models. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Data Selection Based on Training Trajectories (S2L) ", "page_idx": 4}, {"type": "text", "text": "Require: Training dataset $D_{\\mathrm{train}}$ with corresponding training trajectories, a fixed data budget $B$ , number of clusters $K$ .   \nEnsure: Subset $S\\subseteq D_{\\mathrm{train}},|S|\\leq B.$ .   \n1: Initialize $S$ as an empty set.   \n2: Train a small proxy model and cluster examples in (each data source of) $D_{\\mathrm{train}}$ based on their loss trajectories and sort them by size to get $\\mathcal{C}=\\{C_{1}^{\\cdot},C_{2},\\ldots,C_{K}\\}$ .   \n3: for each cluster $C_{k}$ in $\\mathcal{C}$ do   \n4: Calculate $R_{k}$ , the number of examples to randomly sample from $C_{k}$ , i.e., $\\dot{R}_{k}=(B-|S|)/(\\dot{K}-k\\Bar{+}1)$ .   \n5: if $|C_{k}|\\le R_{k}$ then   \n6: $S\\gets\\{S\\bigcup C_{k}\\}$ .   \n7: else   \n8: $S\\,\\leftarrow\\,\\{S\\bigcup S_{k}\\}$ , where $S_{k}\\,\\subset\\,C_{k}$ is selected uniformly at random from $C_{k}$ and $|S_{k}|=R_{k}$   \n9: end if   \n10: end for   \n11: Return $S$ ", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 2, clustering algorithms can effectively find groups of examples with similar training dynamics. In Figure 3, we empirically show that we can identify groups of examples with similar training dynamics on a larger model by clustering the training trajectories of $D_{\\mathrm{train}}$ on a smaller proxy model. With the clusters formed, the data selection strategy selects equal number of examples at random from all clusters, as detailed in Algorithm 1. In doing so, it effectively prioritizes selecting examples from smaller clusters. This is particularly important for datasets containing multiple imbalanced sources. In this setting, training and test distributions often differ, and balanced selection from clusters ensures superior test performance on all groups in the test data. ", "page_idx": 4}, {"type": "text", "text": "The following theorem shows that, under the assumptions of Theorem 4.1, training with Incremental Gradient (IG) methods on the subset selected by S2L converges to a close neighborhood of the optimal solution found by training the target model ", "page_idx": 4}, {"type": "text", "text": "on the full dataset. IG methods such as Stochastic Gradient Descent (SGD) update parameters iteratively based on the gradient of the loss of individual examples, multiplied by stepsize $\\alpha$ . Formally, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{t+1}=\\pmb{\\theta}^{t}-\\alpha\\nabla\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta}^{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Corollary 4.2. Under the assumptions of Theorem 4.1, applying $I G$ with stepsize $\\alpha$ to subsets found by $S2L,$ converges to the neighborhood of the optimal solution, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}^{t+1}-\\pmb{\\theta}^{*}\\|^{2}\\leq(1-\\alpha c)^{t+1}\\|\\pmb{\\theta}^{t}-\\pmb{\\theta}^{*}\\|^{2}+2\\xi R/c^{2}+\\alpha B^{2}(r_{\\mathrm{min}}/k)^{2}\\pmb{g}_{\\mathrm{max}}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c\\leq\\|H\\|$ , $B\\,=\\,k\\,\\cdot\\,K$ is the total size of the subset, $g_{\\mathrm{max}}$ is the largest gradient norm of individual examples during training, $\\begin{array}{r}{r_{\\operatorname*{min}}=\\operatorname*{min}_{j}|C_{j}|,r_{\\operatorname*{max}}=\\operatorname*{max}_{j}|C_{j}|,}\\end{array}$ , $R=\\operatorname*{min}\\{d_{0},B g_{\\operatorname*{max}}+$ $\\xi/c\\}$ and $d_{0}=\\lVert\\pmb{\\theta}^{0}-\\pmb{\\theta}^{*}\\rVert$ is the initial distance to the optimal solution $\\pmb{\\theta}^{*}$ , and $\\xi$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\xi=K[r_{\\operatorname*{min}}\\Delta+(r_{\\operatorname*{max}}-r_{\\operatorname*{min}})g_{\\operatorname*{max}}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof can be found in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the comprehensive experiments conducted to evaluate the efficacy of the proposed data selection method, SMALLTOLARGE (S2L), across two challenging domains (mathematical reasoning and clinical text summarization). ", "page_idx": 4}, {"type": "text", "text": "We systematically compare S2L against a comprehensive set of open-sourced data selection methods. These methods are categorized based on the type of representation they use and selected as the most representative or best-performing methods as identified in prior work. These include: (1) Random Sampling; selecting examples with the (2) Least Confidence [4] or (3)Middle Perplexity [34]; (4) High Learnability, determined by the loss decrease before and after full fine-tuning [68]; and (5) Facility Locations selection based on hidden states [4]. Additionally, we incorporate one online selection techniques: (6) Confidence Curriculum proposed by Varshney et al., which selects examples with decreasing confidence during the training. Given that the optimal reference model may vary for each one-shot selection method, we ensure a fair comparison by adopting the approach used in [34], which runs each method with both the fully fine-tuned target model and an early fine-tuning checkpoint as the reference model. We report the best results from these setups. ", "page_idx": 5}, {"type": "text", "text": "5.2 Specialized Domain 1: Mathematical Reasoning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training Settings. We focus on fine-tuning using the MathInstruct dataset [64] with 262,040 training examples for its comprehensive coverage of diverse mathematical fields and its capability in training models to achieve state-of-the-art performance on the standard evaluation benchmarks. We employ the open-source model suites Pythia [6], Phi-2 [28], Llama-2 [50] as our base models to validate our S2L algorithm and directly compare its performance against the state-of-the-art. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Datasets. We follow the framework established in [64] for a comprehensive assessment using several well-regarded datasets, including in-domain and out-of-domain datasets. For the in-domain datasets, we consider GSM8K [11], MATH [19], and NumGLUE [36]. For the outof-domain datasets, we consider SVAMP [38], Mathematics [13], SimulEq [25]. These datasets collectively span a diverse range of mathematical subjects, such as algebra, probability, number theory, calculus, and geometry. Additionally, some questions in these datasets require the application of commonsense, reading comprehension, and multi-step reasoning. All questions are open-formed. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metric. We use the standard evaluation metric for open-formed questions, exact match, which measures the model\u2019s accuracy by comparing its generated answers against the correct solutions. For an answer to be considered correct, it must match the reference solution precisely. ", "page_idx": 5}, {"type": "text", "text": "More details about the settings and baseline implementations can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "5.2.1 Setting 1: Less Data for Better Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the first setting, we standardize the number of training steps to correspond to 3 epochs on the full dataset, aligning with [64]. This allows us to maintain a consistent training schedule across different methods and data budgets, ensuring fair and accurate comparisons of the quality of data. ", "page_idx": 5}, {"type": "text", "text": "SCALING THE DATA: SOTA algorithms fail with small data budgets while S2L stands out across data scales. In Figure 4, we compare S2L against the baselines from Section 5.1 on Pythia410M across varying data sizes. The training trajectories used by S2L are based on Pythia-70M, a model approximately $6\\mathrm{x}$ smaller than Pythia-410M. With the same number of training steps as the full training, S2L exceeds the full dataset\u2019s performance using only 30K examples, only $11\\%$ of the full dataset. It leads the runner-up baselines by an average of $4.7\\%$ , $4.6\\%$ and $2.4\\%$ with data budget ", "page_idx": 5}, {"type": "image", "img_path": "K9IGlMQpif/tmp/96ec832dd46271957ad6acda89fb6428e283f4b6bacb1c38bc97974f3cedec19.jpg", "img_caption": ["Figure 5: Wall-clock time required to train the reference model and select 100K data from MathInstruct for training Pythia-410M. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "30K, 50K and 100K across all six evaluation datasets. While state-of-the-art data selection algorithms like Facility Locations [4] and High Learnability [68] have decent performance with a large enough data budget (i.e., 100K), all SOTA algorithms except S2L cannot even outperform the random sampling baseline when the allowed data size is small (i.e., 30K). Unlike the existing algorithms, S2L consistently outperforms all baselines and even full training across all data sizes. Note that compared to the runner-up algorithm in this setting, Facility Locations, the cost of S2L is much lower in both training the reference model and data selection stages (Figure 5), and therefore more scalable to both larger target models or larger data sizes. ", "page_idx": 5}, {"type": "image", "img_path": "K9IGlMQpif/tmp/36a54b2e5e9bf14f44fed48798110a15f930c8aa002788aa30677fa4677a6f54.jpg", "img_caption": ["Figure 4: Data Scaling: Accuracies (\u2191) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). See breakdowns in Figure 14. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "K9IGlMQpif/tmp/4c6cea62b0867738959e81a0df43c04613c86b1dd4ec44b944633b716e8a5de3.jpg", "table_caption": ["Table 1: Less Data, Same Compute: Zero-shot accuracies $(\\%,\\uparrow)$ when $\\boldsymbol{S2\\mathrm{L}}$ and the baselines select 50K data to train with the same number of iterations as the full-data training. Results surpassing full training are highlighted in bold. Figure 4 follows the same setting but uses the Pythia-410M model. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "SCALING THE MODEL: Data selected by S2L can transfer to larger models in different model suites. We also test whether this subset, chosen using Pythia-70M, can effectively train larger models beyond 410M and models outside the Pythia suite. As shown in Table 1, our experiments with Phi-2 reveal that fine-tuning on only 50K S2L-selected data again outperforms full dataset training on the most challenging MATH [19] benchmark improving the pretrained Phi-2 by $16.6\\%$ and is more data efficient than training on the full MathInstruct dataset to get the same performance. ", "page_idx": 6}, {"type": "text", "text": "5.2.2 Setting 2: Less Data for Faster Training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The second setting we consider is when fixing the number of times each example can be seen over the entire course of training, directly translating smaller datasets into reduced training and storage costs. This is particularly beneficial for large models that would require extensive training times without data selection. By experimenting with models of larger sizes than the previous setting, we observe in Table 2 that S2L can achieve comparable performance to full-data training when using only $50\\%$ data and thereby reducing both the data storage space and the training time by half. ", "page_idx": 6}, {"type": "text", "text": "5.2.3 Why is S2L So Effective? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Examples in Clusters Encode the Same Knowledge/Skill. In Appendix C, we compare actual training examples in MathInstruct that get clustered together due to their similar training trajectories on the small Pythia70M model. We observe that examples in the same cluster are of the same type and related to the same knowledge/skill, e.g., open-formed algebra questions (Figure 15), examples requiring extracting useful information from long text and writing programs (Figure 16), and multiple choice questions that require multi-step reasoning (Figure 17), etc. Therefore, by sampling from different clusters, we make sure the selected examples cover the knowledge required for all topics and skills required for all types of questions. ", "page_idx": 6}, {"type": "image", "img_path": "K9IGlMQpif/tmp/8c2ddb622e3abb559aca83ca2d00e0313c26be64afff61b2aff58e54761d40bd.jpg", "img_caption": ["Figure 6: Distribution of the coverage of top-1 topic in each cluster. Taller bars on the right end of the plot indicate clusters with a higher concentration of a single topic and therefore suggest a grouping of similar examples. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "K9IGlMQpif/tmp/203c72d92925aabb7eeb1e5a392db6bce1994cc66c0bbc435f7bd239f8d543c2.jpg", "table_caption": ["Table 2: Less Data, Same Epochs: Zero-shot accuracies $(\\%,\\uparrow)$ when $\\mathbf{S}2\\mathbf{L}$ trains $50\\%$ data for the same number of epochs as the full-data training. S2L can achieve comparable performance to full-data training while reducing both the data storage space and the training time by half. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Loss Trajectories can Capture the Similarity Between Data Points As Much As Embeddings of a Fully Fine-tuned Model. We conducted a quantitative analysis to assess how effectively S2L identifies similar examples using loss trajectories from a small model. Assuming math problems under the same topic require similar knowledge and share question formats, we used unknown topic labels during S2L\u2019s data selection to check if each cluster predominantly contains a single topic. By calculating the fraction of the most common topic in each cluster and plotting this in Figure 6 (with $K{=}100$ , excluding clusters of size one), we compared the loss trajectory clusters from S2L (in blue) against those from the embeddings of a fully fine-tuned Phi-2 model (in orange)\u2014considered the ground truth for similarity. Results show that most clusters formed by S2L using the Pythia-70M model are based on a single topic and capture topic similarities more effectively than those from the Phi-2 model\u2019s embeddings. This analysis not only confirms the homogeneity within S2L clusters but also highlights the computational efficiency of using loss trajectories on small models to identify representative examples. ", "page_idx": 7}, {"type": "text", "text": "5.3 Specialized Domain 2: Clinical Text Summarization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "S2L can improve data efficiency not only for fine-tuning data not only in mathematics but also in other specialized domains. This subsection explores its application to clinical text summarization within radiology reports. This task involves processing the detailed analysis and results listed in the findings section of a radiology report and distilling them into a concise impression section. Such summaries are crucial for providing clinicians with quick and actionable insights from radiological studies. ", "page_idx": 7}, {"type": "text", "text": "Dataset & Setup. We use the MIMIC-III dataset [21], a comprehensive collection of radiology reports and findings authored by attending physicians in routine clinical care. We use the same preprocessing procedures as [14, 15] to extract the findings and impression sections and remove invalid reports. Given that access to MIMIC-III requires specific credentials, we provide a synthetic example of a radiology report generated by GPT-4 [2] for illustrative purposes in Table 3. We employ the Pythia-1B model and keep the training setting consistent with the mathematical reasoning task. ", "page_idx": 7}, {"type": "text", "text": "Evaluation. Our evaluation of generated clinical summaries on the MIMIC-III dataset\u2019s test split employs three key metrics as recommended in [52, 51]: (1) BLEU [37], which measures word sequence overlap between the generated and reference texts; (2) ROUGE-L [29], assessing the longest common word sequence; and (3) BERTScore [65], evaluating semantic similarity using BERT\u2019s contextual embeddings. These metrics together offer a comprehensive evaluation, ensuring our summaries are not only precise in language but also meaningful and coherent in the context of clinical information. We compare S2L to random selection, a surprisingly strong baseline as evidenced in Section 5.2, to check the validity of the data selection problem on this dataset and then compare it to training on the full dataset to assess its effectiveness. ", "page_idx": 7}, {"type": "text", "text": "Results. We compare using 30K examples selected by random vs. selected through S2L. Even with only half of the data, the model trained with S2L selected data achieves similar BLEU and significantly higher ROUGE-L and BERTSCore compared to the model trained on the entire 61.5K data. Meanwhile, training on randomly selected 30K examples performs worse than training on the full dataset on all 3 metrics. These results together demonstrate S2L\u2019s effectiveness. ", "page_idx": 7}, {"type": "image", "img_path": "K9IGlMQpif/tmp/a0628fab4ae7b584d84d6e37be6c1d36447621816de73d627367a6e7e87df0e7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 7: Performance $(\\uparrow)$ of models trained on (1) randomly selected 30K examples, (2) S2L selected 30K examples, and (3) full 61K examples (none) evaluated with 3 different metrics. The minimum value on the y-axis is the performance of the model before fine-tuning. S2L improves the data efficiency for the clinical text summarization task by outperforming training on the full dataset with only less than half of the data. ", "page_idx": 8}, {"type": "text", "text": "recorded at any training stage (early, middle, or recorded sparsely throughout the training. ", "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies on MathInstruct and Pythia-410M to further understand the best practices for using S2L. ", "page_idx": 8}, {"type": "text", "text": "S2L is robust w.r.t. the length of the trajectories but can benefit more from longer trajectories. Figure 8 compares models trained with data selected by S2L based on training trajectories of different lengths. The shorter trajectories are derived from a uniform sample of the longer trajectories. From the small slopes of the lines, we can conclude that S2L is relatively robust to the length of the training trajectories. Nevertheless, we can also observe a slight increase in the performance on some of the datasets when longer trajectories are used, so having longer trajectories is still preferred. ", "page_idx": 8}, {"type": "text", "text": "S2L can utilize training trajectories collected at any stage of training but preferably denser ones. With the length of the trajectories fixed to 4, we can observe in Figure 9 that denser trajectories late) are more helpful for S2L than trajectories ", "page_idx": 8}, {"type": "text", "text": "S2L does not require the full training data to train the proxy and can scale efficiently to larger datasets. To further demonstrate the scalability of the proposed S2L method, we conducted experiments by training the proxy on a smaller sample of the data (100K examples) for the same number of epochs (3 epochs) and saving the loss for all examples. The results, shown in Figure 10, confirm that S2L remains effective when the proxy model is trained on a smaller subset of training data and therefore is scalable to larger datasets without a proportional increase in computational costs. ", "page_idx": 8}, {"type": "image", "img_path": "K9IGlMQpif/tmp/3b762a544cdcd5856532f0a4d527077a39564397e4cd992b1907bbdba5d0a2fc.jpg", "img_caption": ["Figure 8: S2L is robust Figure 9: S2L prefers to the length of training dense trajectories over trajectories. sparse ones. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "S2L is robust across different clustering parameter values for K. We conducted detailed ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "experiments varying the clustering parameter K, as shown in Figure 11. The results demonstrate that S2L maintains high performance across different values of K, highlighting the robustness of our method to different clustering parameter choices. We chose $K{=}100$ for our experiments as it provided the best average accuracy across the evaluation datasets for the math reasoning task. ", "page_idx": 8}, {"type": "text", "text": "S2L remains effective and efficient compared to using full data when trained for the same number of epochs. Figure 12 illustrates the relative accuracy to full data across different epochs, comparing S2L-selected data and full data with the same number of epochs. Both in-domain and overall average accuracy are shown. S2L demonstrates superior performance with fewer data and fewer training iterations. ", "page_idx": 8}, {"type": "text", "text": "S2L supports a range of small models as effective proxies. To understand whether different small models could serve as effective proxies, we used GPT-2 (124M) and Pythia-160M as proxy models for data selection to train Pythia-410M. The results, illustrated in Figure 13, show that both proxy models perform comparably in guiding the data selection, demonstrating the versatility and effectiveness of using different small models for S2L. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "K9IGlMQpif/tmp/dac548143b6eff262a5ddcb24b4a077fe9780b07046f594681dfc5275096f1cc.jpg", "img_caption": ["Figure 12: Relative accuracy to full data across different epochs, comparing S2L-selected data and full data. S2L achieves superior performance with fewer data and fewer training iterations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In this work, we introduced SMALLTOLARGE (S2L), a scalable data selection method to improve the data efficiency of supervised fine-tuning (SFT) for large language models (LLMs) in specialized domains. By clustering data points based on their training dynamics on smaller models and balanced sampling from all clusters, S2L significantly reduces the required training data size without compromising performance compared to using the entire training dataset. Our comprehensive experiments across the mathematical problem-solving and clinical text summarization domains demonstrate the effectiveness of S2L. ", "page_idx": 9}, {"type": "text", "text": "Our study does come with its limitations. S2L has been only tested within two domains, mathematics and medicine, and on models up to 7 billion parameters, constrained by our computational resources. Additionally, our experiments employed a fixed training schedule across all methods without further optimization or hyperparameter tuning for each method, including S2L. This unified approach, while it ensures a fair comparison, may not fully capture the potential performance improvement that could be achieved with more tailored training strategies. We encourage further research to extend the application of S2L across a broader spectrum of domains and investigate the impact of hyperparameter tuning on its effectiveness. ", "page_idx": 9}, {"type": "image", "img_path": "K9IGlMQpif/tmp/ae8e9708e20a4210b97da8727df3807ec78b6d6416ebdd86807e85d416ab4dbd.jpg", "img_caption": ["Figure 13: Per-dataset and average accuracy comparison between using different proxy models (Pythia-160M and GPT-2 (124M)) for data selection. Using both proxy models show comparable performance, demonstrating the effectiveness of different small models as reference models for S2L. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the National Science Foundation CAREER Award 2146492, National Science Foundation 2421782 and Simons Foundation, Cisco Systems, Optum AI, and a UCLA Hellman Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls, 2023. ", "page_idx": 10}, {"type": "text", "text": "2] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u2019on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Ryan Kiros, Matthew Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Adeola Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u2019ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen O\u2019Keefe, Jakub W. Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alexandre Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Pond\u00e9 de Oliveira Pinto, Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin D. Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas A. Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u2019on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. 2023. URL https://api.semanticscholar.org/CorpusID:257532815. ", "page_idx": 10}, {"type": "text", "text": "[3] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. ", "page_idx": 10}, {"type": "text", "text": "[4] Gantavya Bhatt, Yifang Chen, Arnav M Das, Jifan Zhang, Sang T Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon S Du, Kevin Jamieson, et al. An experimental design framework for label-efficient supervised finetuning of large language models. arXiv preprint arXiv:2401.06692, 2024.   \n[5] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158, 2023.   \n[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.   \n[8] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ FdVXgSJhvz.   \n[9] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehension. arXiv preprint arXiv:2309.09530, 2023.   \n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1\u2013113, 2023.   \n[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[12] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJg2b0VYDr.   \n[13] Alex Davies, Petar Velic\u02c7kovic\u00b4, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Toma\u0161ev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andr\u00e1s Juh\u00e1sz, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70\u201374, 2021.   \n[14] Jean-Benoit Delbrouck, Maya Varma, Pierre Chambon, and Curtis Langlotz. Overview of the RadSum23 shared task on multi-modal and multi-anatomical radiology report summarization. In Dina Demner-fushman, Sophia Ananiadou, and Kevin Cohen, editors, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 478\u2013482, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bionlp-1. 45. URL https://aclanthology.org/2023.bionlp-1.45.   \n[15] Dina Demner-fushman, Sophia Ananiadou, and Kevin Cohen, editors. The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023. bionlp-1.0.   \n[16] Yihe Deng, Yu Yang, Baharan Mirzasoleiman, and Quanquan Gu. Robust learning with progressive data expansion against spurious correlation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= 9QEVJ9qm46.   \n[17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazar\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou. The faiss library. 2024.   \n[18] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.   \n[19] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe.   \n[20] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Exploring the benefits of training expert language models over instruction tuning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 14702\u201314729. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/ v202/jang23a.html.   \n[21] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[22] Siddharth Joshi and Baharan Mirzasoleiman. Data-efficient contrastive self-supervised learning: Most beneficial examples for supervised learning contribute the least. In International conference on machine learning, pages 15356\u201315370. PMLR, 2023.   \n[23] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, pages 5464\u20135474. PMLR, 2021.   \n[24] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8110\u20138118, 2021.   \n[25] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.   \n[26] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022.   \n[27] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[28] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[29] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.   \n[30] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving> $80\\%$ on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023.   \n[31] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.   \n[32] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.   \n[33] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and Ari Morcos. Sieve: Multimodal dataset pruning using image-captioning models. In Conference on Computer Vision and Pattern Recognition, 2024. URL https://openreview. net/forum?id $\\cdot$ DBxBPGRWjw.   \n[34] Max Marion, Ahmet \u00dcst\u00fcn, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023.   \n[35] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6950\u20136960. PMLR, 13\u201318 Jul 2020. URL https://proceedings. mlr.press/v119/mirzasoleiman20a.html.   \n[36] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505\u20133523, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.246. URL https: //aclanthology.org/2022.acl-long.246.   \n[37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.   \n[38] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080\u2013 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.   \n[39] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:20596\u201320607, 2021.   \n[40] Omead Pooladzandi, David Davini, and Baharan Mirzasoleiman. Adaptive second order coresets for data-efficient machine learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 17848\u201317869. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/ v162/pooladzandi22a.html.   \n[41] Neha Prakriya, Yu Yang, Baharan Mirzasoleiman, Cho-Jui Hsieh, and Jason Cong. Nessa: Near-storage data selection for accelerated machine learning training. In Proceedings of the 15th ACM Workshop on Hot Topics in Storage and File Systems, HotStorage \u201923, page 8\u201315, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702242. doi: 10.1145/3599691.3603404. URL https://doi.org/10.1145/3599691.3603404.   \n[42] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \n[43] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172\u2013180, 2023.   \n[44] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023.   \n[45] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536, 2022.   \n[46] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9275\u20139293, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.746. URL https://aclanthology.org/2020. emnlp-main.746.   \n[47] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.   \n[48] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: Improving LLM pretraining via document de-duplication and diversification. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id $\\cdot$ CG0L2PFrb1.   \n[49] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id $\\cdot$ BJlxm30cKm.   \n[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[51] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334, 2023.   \n[52] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, William Collins, Neera Ahuja, et al. Clinical text summarization: adapting large language models can outperform human experts. arXiv preprint arXiv:2309.07430, 2023.   \n[53] Neeraj Varshney, Swaroop Mishra, and Chitta Baral. Let the model decide its curriculum for multitask learning. In Colin Cherry, Angela Fan, George Foster, Gholamreza (Reza) Haffari, Shahram Khadivi, Nanyun (Violet) Peng, Xiang Ren, Ehsan Shareghi, and Swabha Swayamdipta, editors, Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing, pages 117\u2013125, Hybrid, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deeplo-1.13. URL https://aclanthology.org/ 2022.deeplo-1.13.   \n[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.   \n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.   \n[56] Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and Chang Zhou. Self-evolved diverse data sampling for efficient instruction tuning. arXiv preprint arXiv:2311.08182, 2023.   \n[57] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.   \n[58] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. Training trajectories of language models across scales. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13711\u201313738, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.767. URL https://aclanthology.org/2023.acl-long. 767.   \n[59] Yu Yang, Tian Yu Liu, and Baharan Mirzasoleiman. Not all poisons are created equal: Robust training against data poisoning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25154\u201325165. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/ yang22j.html.   \n[60] Yu Yang, Hao Kang, and Baharan Mirzasoleiman. Towards sustainable learning: Coresets for data-efficient deep learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 39314\u201339330. PMLR, 23\u201329 Jul 2023.   \n[61] Yu Yang, Aaditya K Singh, Mostafa Elhoushi, Anas Mahmoud, Kushal Tirumala, Fabian Gloeckle, Baptiste Rozi\u00e8re, Carole-Jean Wu, Ari S Morcos, and Newsha Ardalani. Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data. arXiv preprint arXiv:2312.02418, 2023.   \n[62] Yu Yang, Eric Gan, Gintare Karolina Dziugaite, and Baharan Mirzasoleiman. Identifying spurious biases early in training through the lens of simplicity bias. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 2953\u20132961. PMLR, 02\u201304 May 2024. URL https://proceedings.mlr. press/v238/yang24c.html.   \n[63] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ N8N0hgNDRt.   \n[64] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id $\\equiv$ yLClGs770I.   \n[65] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.   \n[66] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023.   \n[67] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=KBMOKmX2he.   \n[68] Haotian Zhou, Tingkai Liu, Qianli Ma, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Lobass: Gauging learnability in supervised fine-tuning data. arXiv preprint arXiv:2310.13008, 2023. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. From the assumption that the loss trajectories of examples on the proxy and target models are close: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{L}_{i}^{\\mathrm{proxy}}-\\mathbf{L}_{i}^{\\mathrm{target}}\\|\\leq\\delta,\\quad\\forall i.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $i$ and $j$ are in the same cluster $C_{k}$ based on the proxy model, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{L}_{i}^{\\mathrm{proxy}}-\\mathbf{L}_{j}^{\\mathrm{proxy}}\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the triangle inequality: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{L}_{i}^{\\mathrm{taget}}-\\mathbf{L}_{j}^{\\mathrm{taget}}\\|\\leq\\|\\mathbf{L}_{i}^{\\mathrm{taget}}-\\mathbf{L}_{i}^{\\mathrm{proxy}}\\|+\\|\\mathbf{L}_{i}^{\\mathrm{proxy}}-\\mathbf{L}_{j}^{\\mathrm{proxy}}\\|+\\|\\mathbf{L}_{j}^{\\mathrm{proxy}}-\\mathbf{L}_{j}^{\\mathrm{taget}}\\|\\leq2\\delta+\\epsilon=\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, at any iteration $t$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta}^{(t)})-\\mathcal{L}_{j}^{\\mathrm{target}}(\\pmb{\\theta}^{(t)})|\\leq\\epsilon^{\\prime},\\quad\\forall t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Assuming that the loss functions can be approximated by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta})=\\frac{1}{2}d\\pmb{\\theta}^{\\top}H_{i}d\\pmb{\\theta}+\\pmb{g}_{i}^{\\top}d\\pmb{\\theta}+c_{i},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c_{i}$ is the loss of example $i$ at the beginning of fine-tuning, and $d\\pmb{\\theta}$ is the distance between the parameters of the pretrained model and those during fine-tuning. Similarly for $\\mathcal{L}_{j}^{\\mathrm{target}}(\\pmb{\\theta})$ . The loss difference between $i$ and $j$ is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}^{\\mathrm{target}}(\\theta)-\\mathcal{L}_{j}^{\\mathrm{target}}(d\\theta)=\\frac{1}{2}d\\theta^{\\top}(H_{i}-H_{j})d\\theta+(g_{i}-g_{j})^{\\top}d\\theta+(c_{i}-c_{j}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given that $|\\mathcal{L}_{i}^{\\mathrm{target}}(\\pmb{\\theta})-\\mathcal{L}_{j}^{\\mathrm{target}}(\\pmb{\\theta})|\\leq\\epsilon^{\\prime}$ , we can write: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{2}d\\pmb{\\theta}^{\\top}(\\pmb{H}_{i}-\\pmb{H}_{j})d\\pmb{\\theta}+(\\pmb{g}_{i}-\\pmb{g}_{j})^{\\top}d\\pmb{\\theta}+(c_{i}-c_{j})\\right|\\leq\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us choose two different values, $\\pmb{\\theta}^{(1)}$ and $\\pmb{\\theta}^{(2)}$ , to generate two inequalities. For $d\\pmb{\\theta}^{(1)}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{2}(d\\pmb{\\theta}^{(1)})^{\\top}(\\pmb{H}_{i}-\\pmb{H}_{j})d\\pmb{\\theta}^{(1)}+(\\pmb{g}_{i}-\\pmb{g}_{j})^{\\top}d\\pmb{\\theta}^{(1)}+(\\pmb{c}_{i}-\\pmb{c}_{j})\\right|\\leq\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for $d\\pmb{\\theta}^{(2)}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{2}(d\\pmb{\\theta}^{(2)})^{\\top}(\\pmb{H}_{i}-\\pmb{H}_{j})d\\pmb{\\theta}^{(2)}+(\\pmb{g}_{i}-\\pmb{g}_{j})^{\\top}d\\pmb{\\theta}^{(2)}+(\\pmb{c}_{i}-\\pmb{c}_{j})\\right|\\leq\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Subtracting these two inequalities, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left((d\\theta^{(1)})^{\\top}(H_{i}-H_{j})\\theta^{(1)}-(d\\theta^{(2)})^{\\top}(H_{i}-H_{j})d\\theta^{(2)}\\right)+(g_{i}-g_{j})^{\\top}(d\\theta^{(1)}-d\\theta^{(2)})\\Bigg|\\leq2\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(d\\theta^{(1)})^{\\top}(H_{i}-H_{j})d\\theta^{(1)}-(d\\theta^{(2)})^{\\top}(H_{i}-H_{j})d\\theta^{(2)}\\Big|\\leq\\|H_{i}-H_{j}\\|\\left(\\|d\\theta^{(1)}\\|^{2}+\\|d\\theta^{(2)}\\|^{2}\\right)}&{}\\\\ {\\leq(\\|H_{i}\\|+\\|H_{j}\\|)\\left(\\|d\\theta^{(1)}\\|^{2}+\\|d\\theta^{(2)}\\|^{2}\\right)}&{}\\\\ {\\leq4C D^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This gives us: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|(g_{i}-g_{j})^{\\top}(d\\pmb{\\theta}^{(1)}-d\\pmb{\\theta}^{(2)})\\right|\\leq2\\epsilon^{\\prime}+2C D^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Assuming $\\lVert d\\pmb{\\theta}^{(1)}-d\\pmb{\\theta}^{(2)}\\rVert\\geq d$ , we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\pmb{{g}}_{i}-\\pmb{{g}}_{j}\\|\\leq\\frac{2\\epsilon^{\\prime}+2C D^{2}}{d}=\\Delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Corollary 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Without loss of generality, assume we select $k$ example from each cluster and we have $k\\ \\leq$ $\\mathrm{min}_{j\\in[K]}\\left|C_{j}\\right|$ . Then the error of the subset in capturing the full gradient will be ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\xi\\leq\\sum_{j}(|C_{j}|-k)(\\bar{g}_{j}+\\Delta),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\bar{\\pmb{g}}_{j}$ is the norm of the average gradient of the selected examples from $C_{j}$ . In practice, we can weight elements of the subset by $r_{\\mathrm{min}}/k$ , which has a similar effect to scaling the step size when training on the subset. Let $g_{\\mathrm{max}}\\,=\\,\\mathrm{max}_{j}\\,\\|\\pmb{g}_{j}\\|$ be the maximum gradient norm during training, $r_{\\operatorname*{max}}=\\operatorname*{max}_{j}\\,|C_{j}|,r_{\\operatorname*{min}}=\\operatorname*{min}_{j}\\,|C_{j}|$ . Then, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\xi^{\\prime}\\leq\\sum_{j}(r_{\\operatorname*{min}}-k)\\Delta+(|C_{j}|-r_{\\operatorname*{min}})(\\bar{g_{j}}+\\Delta)}\\\\ {\\displaystyle\\leq K[r_{\\operatorname*{min}}\\Delta+(r_{\\operatorname*{max}}-r_{\\operatorname*{min}})g_{\\operatorname*{max}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term in RHS of Eq (23) is the error of the subset selected from $C_{j}$ to capture its full gradient and the second term is due to selecting the same number of examples, $k$ , from the larger clusters. ", "page_idx": 18}, {"type": "text", "text": "Using the above error and following the proof of Theorem 1 in [35], for a constant step size $\\alpha\\leq1/c$ we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}^{t+1}-\\pmb{\\theta}^{*}\\|^{2}\\leq(1-\\alpha c)^{t+1}\\|\\pmb{\\theta}^{t}-\\pmb{\\theta}^{*}\\|^{2}+2\\xi^{\\prime}R/c^{2}+\\alpha B^{2}(r_{\\mathrm{min}}/k)^{2}\\pmb{g}_{\\mathrm{max}}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $c\\leq\\|H\\|$ , and $B=k\\cdot K$ is the total size of the subset, $R=\\operatorname*{min}\\{d_{0},B g_{\\operatorname*{max}}+\\xi^{\\prime}/c\\}$ and $d_{0}=\\lVert\\pmb{\\theta}^{0}-\\pmb{\\theta}^{*}\\rVert$ is the initial distance to the optimal solution $\\theta^{*}$ . ", "page_idx": 18}, {"type": "text", "text": "If $k\\geq|C_{j}|$ for any cluster $C_{j}$ , one can simply add $\\left(r_{\\operatorname*{min}}/k-1\\right)\\cdot\\hat{g}_{j}$ to $\\xi^{\\prime}$ for the corresponding clusters, where $\\hat{\\pmb g_{\\j}}$ is the norm of the total gradient of cluster $C_{j}$ and we replace $r_{\\mathrm{min}}$ in Eq (23) with the size of smallest cluster that has larger than $k$ examples. ", "page_idx": 18}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Pythia. The Pythia models [6] are a suite of large language models (LLMs) developed by EleutherAI licensed under the Apache License 2.0. These models range in size from 70 million to 12 billion parameters and are designed to enable controlled scientific research on transparently trained LLMs across various scales. ", "page_idx": 18}, {"type": "text", "text": "Phi. The Phi models [28] developed by Microsoft are under the MIT License. Phi-1.5, a transformerbased model with 1.3 billion parameters, and its successor, Phi-2, with 2.7 billion parameters, have been trained on a diverse set of data sources, including synthetic texts and curated websites. The Phi models underscore the potential of small yet powerful language models in understanding and generating human language, empowering a range of NLP tasks. Phi-2, in particular, has raised the bar for reasoning and language understanding among foundation models, matching or even exceeding the performance of models 25 times its size on complex benchmarks. ", "page_idx": 18}, {"type": "text", "text": "LLaMA 2. The LLaMA 2 models [50], released by Meta AI and licensed under the LLaMA 2 Community License Agreement, are designed for improved natural language understanding and generation. LLaMA 2-7B, the smallest in this series with 7 billion parameters, has demonstrated competitive performance across various NLP benchmarks despite its moderate size. ", "page_idx": 18}, {"type": "text", "text": "B.2 Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "MathInstruct. The MathInstruct dataset [64] is compiled from 13 diverse math rationale datasets, using both chain-of-thought (CoT) and program-of-thought (PoT) rationales. It ensures comprehensive coverage across various mathematical fields in the 262K training examples, making it a popular resource for fine-tuning large language models (LLMs) for general math problem-solving. MathInstruct is licensed under the MIT license. ", "page_idx": 18}, {"type": "text", "text": "Table 3: A synthetic radiology report (MRI of the brain), generated by the GPT-4 model [2] to demonstrate the typical data format and content used in the clinical text summarization task. It is not suitable for clinical or diagnostic use. ", "page_idx": 19}, {"type": "table", "img_path": "K9IGlMQpif/tmp/5eb504aad45edd5fc8eb43e0aef64579aad21c91ad2a693f78e7f792f3d75a52.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "MIMIC-III. The MIMIC-III (Medical Information Mart for Intensive Care III) dataset [21] is a comprehensive collection of de-identified health data associated with over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center in Boston, Massachusetts. This large dataset includes information such as demographics, vital signs, laboratory tests, medications, and more, making it an invaluable resource for a wide range of research in healthcare, including clinical decision support systems, medical procedure efficacy studies, and patient care optimization strategies. ", "page_idx": 19}, {"type": "text", "text": "The MIMIC-III dataset is made freely available to the research community under the Health Insurance Portability and Accountability Act (HIPAA) compliance, ensuring patient confidentiality and data protection. Access to the dataset is granted under a data use agreement (DUA) to individuals affliiated with an institution that approves the use of the data for research purposes. Researchers seeking to utilize the MIMIC-III dataset must complete a required training course on human research protections, which ensures that all researchers are aware of the responsibilities involved in handling sensitive patient data. ", "page_idx": 19}, {"type": "text", "text": "B.3 Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "S2L The training trajectories for both MathInstruct and MIMIC-III are gathered from training a Pythia-70M model, the smallest model in the Pythia model suite, recorded every 500 training iterations. We utilize the Faiss library [17] to perform efficient K-means clustering of loss trajectories with Euclidean distance with $K=100$ and 20 iterations. The hyperparameter $K$ is tuned in the range of $\\{50,100,200\\}$ based on the average accuracy of the model trained on $30K$ selected data. We found $K=100$ worked the best for both datasets we studied in this paper. Ablations studies on the length and the best time in the training to record the trajectories can be found in Section 5.4. ", "page_idx": 19}, {"type": "text", "text": "Comparing Reference Models for the Baselines For one-shot selection methods (excluding S2L), we use representations from either step 1000 or the end of fine-tuning Pythia-410M on MathInstruct and reported the better result in Figure 4 and Table 1. In Table 4, we include the complete comparison between using early-fine-tuning vs. end-of-fine-tuning model checkpoints as the inference model. For Facility Locations, we further compared using the first hidden states as the feature representation as suggested in [4] to using the last hidden states [56] for the tasks we studied.The ranges for confidence, perplexity, and learnability are chosen according to the best-performing intervals reported in prior research (Section 5.1). ", "page_idx": 19}, {"type": "text", "text": "Due to memory and computational constraints, for Facility Locations, we calculate pairwise similarity and perform greedy selection on a per-data-source basis. We found this per-source selection approach also yields benefits for S2L as different data sources within MathInstruct exhibit distinct common patterns in their training trajectories. Therefore, we implement S2L also on a per-source basis for MathInstruct, and recommend applying S2L per source when dealing with datasets composed of multiple data sources. ", "page_idx": 19}, {"type": "text", "text": "Hyperparameters Following the setup used in [64], we adopt a training regimen with a learning rate of 2e-5, a batch size of 128, a maximum length of 512, and a cosine scheduler with a $3\\%$ warm-up period. ", "page_idx": 19}, {"type": "text", "text": "Experiments Compute Resources We fine-tune all the models with the Huggingface transformers library [55] with Fully Sharded Data Parallel (FSDP) [66] on 4 48G NVIDIA RTX A6000. ", "page_idx": 20}, {"type": "table", "img_path": "K9IGlMQpif/tmp/7aedc239e89380e80301ea5b1e215e85c59f978bbb32151099996f6baad0b8af.jpg", "table_caption": ["Table 4: Complete results used for selecting the best reference model for each one-shot data selection baseline. The choice of early-fine-tuning (step 1000) and end-of-fine-tuning checkpoint follows [34]. The best results selected for Figure 4 are highlighted in cyan. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.4 Evaluation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.4.1 MathInstruct ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Datasets. We utilize 6 diverse datasets with open-formed questions for evaluating the mathematical reasoning capabilities of models trained with both the full MathInstruct dataset and selected subsets. These datasets, detailed in Table 5, span a range of mathematical disciplines from early algebra to calculus and linear algebra, covering various types of questions such as multi-step reasoning, arithmetic word problems, and problems from mathematics competitions. This variety ensures a comprehensive assessment across both in-domain and out-domain tasks. ", "page_idx": 20}, {"type": "text", "text": "Pipeline. We utilize the pipeline provided by $[64]^{2}$ , designed to first determine whether the model can be prompted to generate a code snippet. This code snippet, if successfully generated, should be executable and produce the correct answer when run. This code-based evaluation is also used for Phi models [28]. In cases where the model does not directly produce a viable code solution, we employ a \u201cthink step-by-step\" prompting strategy [54]. This method prompts the model to break down its reasoning process, a technique that has been widely proven effective in fully exploiting the model\u2019s problem-solving capacity. ", "page_idx": 20}, {"type": "text", "text": "B.4.2 MIMIC-III ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Following [14, 15], we include the six most common modality/anatomy pairs: CT head, CT abdomen, CT chest, MRI head, CT spine, and CT neck, and five less common pairs in the text data: MRI spine, CT sinus, MRI abdomen, MRI pelvis, and MRI neck in the evaluation. There are in total 13.7K test examples after data preprocessing and train-test splitting. ", "page_idx": 20}, {"type": "table", "img_path": "K9IGlMQpif/tmp/2219aaabab23a806d05444e2292277c2f9505b7afa82ab5ac9f99512a146d0ae.jpg", "table_caption": ["Table 5: Types of questions in the evaluation datasets for the mathematical reasoning task. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C Examples in Different Clusters ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We compare data points in the same and different clusters based on training trajectories, in Figure 15, Figure 16 and Figure 17. We can observe that examples with similar training trajectories have the same question format. Therefore, balanced sampling from all clusters can ensure different types of examples can be covered in the selected subset of training data. ", "page_idx": 21}, {"type": "image", "img_path": "K9IGlMQpif/tmp/6e4f5096034f7899ed276a7285ccb035074b15b35284f201e3489b0cf7cf3f23.jpg", "img_caption": ["Figure 14: Break-down accuracies $(\\uparrow)$ on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Topic Distribution of Data Selected by S2L ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Beyond qualitative examples from different clusters, we study how S2L changes the data distribution to outperform using the full fine-tuning dataset as well as using random subsets of the same size that have the same distribution as the original dataset. In Figure 18, we can observe that S2L not only guarantees a thorough and balanced coverage across the spectrum of topics but also ensures sufficient representation of foundational topics, such as pre-algebra, which lays the groundwork for tackling more complex subjects. ", "page_idx": 22}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This paper introduces a data selection method for large language models (LLMs), aiming to enhance the data efficiency in the supervised fine-tuning (SFT) of these models. ", "page_idx": 22}, {"type": "text", "text": "Camden went swimming 16 times in March and Susannah went 24 times. If the number of times they went   \nthroughout the month was divided equally among 4 weeks, how many more times a week did Susannah swim   \nthan Camden?   \nCamden went swimming $16/4=4$ times a week   \nSusannah went swimming $24/4=6$ times a week   \nSusannah went $6-4=2$ more times a week than Camden   \nThe answer is 2 What is the perimeter of a Koch snowflake curve after five iterations, if the initial side length of the equilateral triangle used to construct the curve is $\\mathtt{8c m7}$   \nThe Koch snowflake is a fractal curve constructed by iteratively replacing each side of an equilateral triangle with a smaller equilateral triangle. After each iteration, the number of sides increases by a factor of $^{4,}$ and the length of each side is reduced by a factor of 3.   \nAfter five iterations, the number of sides in the Koch snowflake is given by:   \nnumber of sides $=3^{\\star}\\left(4^{\\wedge}5\\right)=3^{\\star}\\,1024=3072$   \nThe length of each side after five iterations is given by:   \nside length $\\equiv$ initial side length / $(3\\wedge5)=8\\wedge$ 243   \nThe perimeter of the Koch snowflake after five iterations is the product of the number of sides and the length of each side:   \nperimeter $\\equiv$ number of sides \\* side length $\\equiv$ 3072 \\* (8 / 243) \u2248 101.3333 cm Lilly and Fiona are cleaning a room. Between them, it takes 8 hours to clean the room. A quarter of the time spent cleaning was by Lilly and Fiona was responsible for the rest of the cleaning. How long, in minutes, was Fiona cleaning?   \n8 hours is $8^{\\ast}60=480$ minutes.   \nLilly cleaned for 480/4=120 minutes.   \nThis means that Fiona was cleaning for 480-120=360 minutes.   \nThe answer is 360 ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Figure 15: Examples in the cluster shown in Figure 7a: open-formed algebra. Questions are in black and answers are in cyan. ", "page_idx": 23}, {"type": "text", "text": "Passage:  The Raiders began their 2011 campaign at Sports Authority Field at Mile High, for a Week 1 AFC West duel with the Denver Broncos in the second game of Monday Night Football's doubleheader.  Oakland trailed early in the first quarter as Broncos kicker Matt Prater got a 28-yard field goal.  The Raiders answered in the second quarter as quarterback Jason Campbell found fullback Marcel Reece on a 3-yard touchdown pass, followed by a 37-yard, a 21-yard, and an NFL record tying 63-yard field goal from kicker Sebastian Janikowski. Janikowski's leg helped put the Raiders up 16-3 at halftime. Denver answered in the third quarter as wide receiver Eric Decker returned a punt 90 yards for a touchdown, followed by Prater getting a 30-yard field goal. Oakland struck back in the fourth quarter with Campbell's 1-yard touchdown.  The Broncos tried to rally with quarterback Kyle Orton completing a 9-yard touchdown pass to running back Lance Ball, yet the Raiders' offense was able to run out the clock. With the win, not only did Oakland begin their season at 1-0, but they also snapped their 8-straight opening day losing streak. Question: How many yards was the second longest field goal? ", "page_idx": 23}, {"type": "text", "text": "second $=37$ print(second) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Passage: The U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for iron in 2001. The current EAR for iron for women ages 14\u201318 is 7.9 mg/day, 8.1 for ages 19\u201350 and 5.0 thereafter (post menopause). For men the EAR is 6.0 mg/day for ages 19 and up. The RDA is 15.0 mg/day for women ages 15\u201318, 18.0 for 19\u201350 and 8.0 thereafter. For men, 8.0 mg/day for ages 19 and up. RDAs are higher than EARs so as to identify amounts that will cover people with higher than average requirements. RDA for pregnancy is 27 mg/day and, for lactation, $9\\,\\mathrm{mg}/\\mathrm{d}\\upalpha\\mathrm{y}$ . For children ages 1\u20133 years 7 mg/day, 10 for ages 4\u20138 and 8 for ages 9\u201313. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of iron the UL is set at 45 mg/day. Collectively the EARs, RDAs and ULs are referred to as Dietary Reference Intakes. Question: How many years does an RDA of 8 last for children? Let's write a Python program to solve it.   \nchild $=4$   \nprint(child) ", "page_idx": 23}, {"type": "text", "text": "Figure 16: Examples in the cluster shown in Figure 7b: reading comprehension $^+$ coding. Questions are in black and answers are in cyan; instructions are highlighted in orange. ", "page_idx": 23}, {"type": "text", "text": "Positive Impacts: Our method, by reducing the data requirements for training LLMs, can make fine-tuning LLMs more effective and accessible. This could lead to broader participation in AI research and application development across various fields, including healthcare and education. ", "page_idx": 23}, {"type": "text", "text": "Negative Impacts: Our method does not inherently involve or encourage applications with direct negative societal impacts. The focus is on a generic improvement in the field of machine learning, particularly in the training of LLMs. ", "page_idx": 23}, {"type": "image", "img_path": "K9IGlMQpif/tmp/0fbb30e5f197dcddd7c1f908cc2f4d223e5ddfe831ab549f2ce9c4f2d25decca.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 17: Examples in the cluster shown in Figure 7c: multiple-choice $^+$ multi-step reasoning.   \nQuestions are in black and answers are in cyan; instructions are highlighted in orange. ", "page_idx": 24}, {"type": "image", "img_path": "K9IGlMQpif/tmp/97236c567bab4c8be28c96681dff6163c5b08a542da4c8601f56d1bdbb2a7c7a.jpg", "img_caption": ["(a) Topic distribution of the full MathInstruct dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "K9IGlMQpif/tmp/f7a9f0dbd18e773d4f0d6d939715adaf5bf4205855707c450616c647dce598f7.jpg", "img_caption": ["(b) Topic distribution of 30K data selected by S2L. (c) Topic distribution of 50K data selected by S2L. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "K9IGlMQpif/tmp/bfd09cef25f737639d0a43c886e4594a7b42fbf5821cbe4e32fac41bd86f810e.jpg", "img_caption": ["(d) Topic distribution of 100K data selected by S2L. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 18: Compared to the original topic distribution, S2L prioritized easier topics (e.g., pre-algebra over intermediate algebra, algebra over other more advanced topics) while always ensuring complete and more balanced coverage of all topics. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The theoretical results provided in Section 4 (and Appendix A) and the experimental results provided in Section 5 support the main claims made in the abstract and introduction. We also conducted experiments with both different data scales and model scales (Section 5.2.1), in both the math and medical domains (Section 5.2 and Section 5.3), and ablation studies (Section 5.4), to reflect how much the results can be expected to generalize to other settings. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The complete proof for each theoretical result is provided in Appendix A. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper is provided in Section 5.2, Section 5.3 and Appendix B. Additionally, the anonymized code is provided with the instructions in the Supplementary Material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 26}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The anonymized code is provided with the instructions in the Supplementary Material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experimental settings for different tasks we considered are presented in the corresponding sections in the main paper (Section 5.2 and Section 5.3). More details are provided in ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. We ran every experiment independently from the data selection to the target model training without pre-defining random seeds to avoid over-fitting to one selected subset or cherrypicking the seeds. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Information about the GPU including the memory is provided in Appendix B.3. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and confirmed that the research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The broader impacts of the work are discussed in Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks because it does not release new data or models. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All assets used in this paper are properly cited. The license for each model and dataset is provided in Appendix B.1 and Appendix B.2 respectively. The code used for evaluation is introduced in Appendix B.4 with the URL provided in the footnote. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]