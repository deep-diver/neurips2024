[{"figure_path": "Wd1DFLUp1M/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between alignment strategies for LLMs and diffusion policies (ours).", "description": "This figure illustrates the similarities between the alignment strategies used for Large Language Models (LLMs) and diffusion policies in continuous control tasks.  The top half shows the LLM alignment process, where a pre-trained LLM is fine-tuned using human preferences to align its outputs with desired behavior. The bottom half shows the analogous process for diffusion policies, where a pre-trained diffusion model representing diverse behaviors is fine-tuned using Q-values to produce optimized policies for control tasks.  The figure highlights the parallel steps of pretraining, alignment with human feedback (preferences for LLMs, Q-functions for diffusion models), and the resulting enhancement of the model to reflect human intentions or optimal policy.", "section": "1 Introduction"}, {"figure_path": "Wd1DFLUp1M/figures/figures_3_1.jpg", "caption": "Figure 2: Algorithm overview. Left: In behavior pretraining, the diffusion behavior model is represented as the derivative of a scalar neural network with respect to action inputs. The scalar outputs of the network can later be utilized to estimate behavior density. Right: In policy fine-tuning, we predict the optimality of actions in a contrastive manner among K candidates. The prediction logit for each action is the density gap between the learned policy model and the frozen behavior model. We use cross-entropy loss to align prediction logits \u25b3fe := for - for with dataset Q-labels.", "description": "This figure illustrates the two-stage process of the Efficient Diffusion Alignment (EDA) algorithm. The left side shows the behavior pretraining stage, where a diffusion behavior model is represented as the derivative of a scalar neural network. This allows for direct density calculation, which is crucial for aligning diffusion behaviors with Q-functions. The right side depicts the policy fine-tuning stage, where the algorithm predicts the optimality of actions contrastively using a cross-entropy loss function.  The goal is to align the pretrained diffusion behavior with the provided Q-function values, improving the policy's performance.", "section": "3 Method"}, {"figure_path": "Wd1DFLUp1M/figures/figures_4_1.jpg", "caption": "Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column 1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the action's Q-value. Column 2 & 3: Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.", "description": "This figure shows the results of the Efficient Diffusion Alignment (EDA) algorithm on 2D bandit tasks. The left side demonstrates value-based optimization, while the right side shows preference-based optimization. Each column represents different aspects: the dataset, pretrained model, fine-tuned model, and predicted Q-values. The results illustrate EDA's ability to effectively decrease the density of low-Q-value actions after fine-tuning, aligning the predicted Q-values with the actual dataset values.", "section": "3.2 Policy Optimization by Aligning Diffusion Behaviors with Q-functions"}, {"figure_path": "Wd1DFLUp1M/figures/figures_7_1.jpg", "caption": "Figure 4: Average performance of EDA combined with different Q-learning methods in Locomotion tasks.", "description": "This figure compares the performance of EDA (Efficient Diffusion Alignment) combined with three different Q-learning methods (Online QL, Softmax QL, Implicit QL) against several baselines (Gaussian baseline, Diffusion baseline, Diffusion-QL, QGPO, IDQL, IQL, TD3+BC).  The results are presented in terms of average locomotion scores across multiple tasks. The goal is to demonstrate the effect of the Q-learning method on EDA's performance and to compare it against other approaches in continuous control tasks.", "section": "5.1 D4RL Evaluation"}, {"figure_path": "Wd1DFLUp1M/figures/figures_7_2.jpg", "caption": "Figure 5: Aligning pretrained diffusion behaviors with task Q-functions is fast and data-efficient.", "description": "This figure shows the results of experiments on the data and training efficiency of the proposed method, EDA, for aligning pretrained diffusion behaviors with task Q-functions.  Subfigure (a) demonstrates the sample efficiency by showing how EDA maintains high performance even with a small percentage of Q-labeled data during fine-tuning compared to other baselines. Subfigure (b) illustrates the training efficiency by plotting the normalized scores over gradient steps, highlighting that EDA converges quickly compared to other methods.", "section": "5.2 Fine-tuning Efficiency"}, {"figure_path": "Wd1DFLUp1M/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation of action numbers K and optimization methods.", "description": "This figure shows the ablation study on the number of contrastive actions (K) and the comparison between preference-based and value-based optimization methods. The results demonstrate that the value-based optimization method (EDA, ours) generally outperforms the preference-based optimization method (DPO).  The performance gap widens as K increases, suggesting that value-based optimization is more robust to the number of action samples considered.", "section": "5.3 Value Optimization v.s. Preference Optimization"}, {"figure_path": "Wd1DFLUp1M/figures/figures_12_1.jpg", "caption": "Figure 7: Comparison of various generative modeling methods in 2D modeling and sampling.", "description": "This figure compares the performance of various generative models (Ground truth, Gaussians, VAEs, EBMs with 100 and 1k steps, and the proposed Bottleneck Diffusion Models (BDMs) with 25 steps) in 2D modeling and sampling tasks. It visually demonstrates the efficiency and effectiveness of the proposed BDMs in generating high-quality samples that closely resemble the ground truth, especially when compared to other methods requiring substantially more steps for convergence.", "section": "A Comparing Bottleneck Diffusion Models with Energy-Based Models"}, {"figure_path": "Wd1DFLUp1M/figures/figures_13_1.jpg", "caption": "Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column 1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the action's Q-value. Column 2 & 3: Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.", "description": "This figure shows the results of the Efficient Diffusion Alignment (EDA) method applied to 2D bandit problems. It compares the performance of value-based and preference-based optimization strategies at different diffusion times. The visualization includes the behavior datasets, density maps estimated by BDM models (both pretrained and fine-tuned), and predicted action Q-values.  The results demonstrate that EDA effectively reduces the density of low-Q-value actions after fine-tuning.", "section": "3.2 Policy Optimization by Aligning Diffusion Behaviors with Q-functions"}, {"figure_path": "Wd1DFLUp1M/figures/figures_18_1.jpg", "caption": "Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column 1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the action's Q-value. Column 2 & 3: Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.", "description": "This figure shows the effectiveness of EDA in 2D bandit experiments at various diffusion times. It compares value-based and preference-based optimization methods, illustrating how the density of actions with low Q-values is reduced after fine-tuning the BDM model. The predicted Q-values from the model closely align with the actual Q-values in the dataset.", "section": "3.2 Policy Optimization by Aligning Diffusion Behaviors with Q-functions"}, {"figure_path": "Wd1DFLUp1M/figures/figures_18_2.jpg", "caption": "Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column 1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the action's Q-value. Column 2 & 3: Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.", "description": "This figure shows the results of the Efficient Diffusion Alignment (EDA) method on 2D bandit tasks. It compares the behavior datasets, density maps, and predicted Q-values before and after fine-tuning. The results demonstrate that EDA effectively reduces the density of actions with low Q-values, aligning the learned policy with the desired behavior.", "section": "3.2 Policy Optimization by Aligning Diffusion Behaviors with Q-functions"}, {"figure_path": "Wd1DFLUp1M/figures/figures_19_1.jpg", "caption": "Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. Column 1: Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the action's Q-value. Column 2 & 3: Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. Column 4: The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.", "description": "This figure shows the results of Efficient Diffusion Alignment (EDA) on 2D bandit tasks.  It compares the behavior dataset, the density maps of the pretrained and fine-tuned bottleneck diffusion models (BDMs), and the predicted Q-values before and after fine-tuning. The visualization demonstrates EDA's ability to effectively decrease the density of actions with low Q-values.", "section": "3.2 Policy Optimization by Aligning Diffusion Behaviors with Q-functions"}]