[{"figure_path": "EMkrwJY2de/tables/tables_6_1.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "This table presents the results of the Long Range Graph Benchmark (LRGB) experiments.  The LRGB is designed to specifically test graph neural networks (GNNs) for over-squashing. The table compares several methods including a baseline GCN, DRew (a graph rewiring method), FoSR (another graph rewiring method focused on spectral gap expansion), and the authors' proposed methods (ProxyAdd and ProxyDelete).  The results are shown for three datasets (PascalVOC-SP, Peptides-Func, and Peptides-Struct) and three metrics (Test F1, Test AP, and Test MAE).  Each metric measures a different aspect of GNN performance on the datasets.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_7_1.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "This table presents the results of the proposed spectral graph pruning methods (PROXYADD and PROXYDELETE), along with existing baseline methods (Baseline-GCN, DRew+GCN, and FoSR+GCN) on three datasets from the Long Range Graph Benchmark: PascalVOC-SP (node classification), Peptides-Func (graph classification), and Peptides-Struct (graph regression).  The metrics reported are Test F1 (for PascalVOC-SP), Test AP (for Peptides-Func), and Test MAE (for Peptides-Struct). The table shows the effectiveness of the proposed methods in improving performance compared to baselines, particularly in mitigating over-squashing and over-smoothing.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_7_2.jpg", "caption": "Table 2: Node classification on Roman-Empire dataset.", "description": "The table presents the results of node classification experiments on the Roman-Empire dataset using GCN and GAT models with different numbers of layers and edge modifications. For each model and layer configuration, the table shows the number of edges added or deleted using different methods (FoSR, Eldan, ProxyGap), the resulting accuracy, and the standard deviation of the accuracy across multiple runs. The results are compared with a baseline GCN/GAT model without any edge modifications. This table allows assessing the impact of the edge rewiring algorithms on the performance of the GCN and GAT models for node classification tasks on a specific dataset.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_8_1.jpg", "caption": "Table 5: Pruning for lottery tickets comparing UGS to our ELDANDELETE pruning and our PROXY-DELETE pruning. We report Graph Sparsity (GS), Weight Sparsity (WS), and Accuracy (Acc).", "description": "This table presents the results of experiments on graph lottery tickets.  It compares the performance of three different pruning methods: Unified Graph Sparsification (UGS), ELDANDELETE pruning, and PROXYDELETE pruning.  The table shows the graph sparsity (GS), weight sparsity (WS), and accuracy (Acc) achieved by each method on three different datasets: Cora, Citeseer, and Pubmed.", "section": "Pruning for graph lottery tickets"}, {"figure_path": "EMkrwJY2de/tables/tables_15_1.jpg", "caption": "Table 6: Computationally calculated criteria for the toy graph examples.", "description": "This table compares the results of Eldan's criterion and two proxy methods (APROXYDELETE and APROXYADD) for predicting the change in the spectral gap (\u0394\u03bb\u2081) after adding or deleting an edge in the toy graph examples shown in Figure 1.  Eldan's criterion provides a theoretical guarantee for the sign of the spectral gap change, while the proxy methods offer computationally efficient approximations. The table shows that the proxy methods accurately predict the sign of \u0394\u03bb\u2081 in most cases, demonstrating their effectiveness in approximating the spectral gap change.", "section": "3 Theoretical insights into spectral rewiring"}, {"figure_path": "EMkrwJY2de/tables/tables_17_1.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "This table presents the results of the Long Range Graph Benchmark (LRGB) experiments.  The LRGB is a benchmark specifically designed to assess the performance of Graph Neural Networks (GNNs) in addressing over-squashing. The table compares the performance of various methods (a baseline GCN and GCNs combined with different graph rewiring techniques: DRew, FoSR, ProxyAdd, and ProxyDelete) on three different datasets (PascalVOC-SP, Peptides-Func, and Peptides-Struct) and across three evaluation metrics (Test F1, Test AP, and Test MAE) relevant to the specific task of each dataset (node classification, link prediction, and graph classification).  The results highlight the effectiveness of the proposed spectral gap-based edge deletion and addition methods (ProxyDelete and ProxyAdd) in improving GNN performance on these challenging datasets.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_20_1.jpg", "caption": "Table 9: Cosine distance between nodes of different labels before and after deleting edges using PROXYDELETE.", "description": "This table presents the cosine distance between nodes with different labels for six datasets (Cornell, Wisconsin, Texas, Chameleon, Squirrel, and Actor) before training, after training on the original graph, and after training on a graph pruned using the PROXYDELETE method. The results show that the cosine distance increases after pruning, indicating that the method effectively prevents over-smoothing by preserving the distinguishability of nodes with different labels.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_21_1.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "The table presents the results of the proposed methods and baselines on three datasets from the Long Range Graph Benchmark (LRGB).  The datasets represent different graph structures and tasks (node classification and graph classification). The metrics used are F1 score, Average Precision (AP), and Mean Absolute Error (MAE), reflecting performance on different graph-level prediction tasks.  The baseline GCN model is compared against several variants incorporating graph rewiring techniques, demonstrating their impact on overall prediction accuracy. This shows the effectiveness of the proposed spectral graph pruning method compared to other existing state-of-the-art approaches on real-world data.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_21_2.jpg", "caption": "Table 11: Graph classification with GCN comparing FoSR, ELDANADD and PROXYADD.", "description": "This table compares the performance of Graph Convolutional Networks (GCNs) combined with three different graph rewiring methods on graph classification tasks. The methods compared are FoSR (First-order spectral rewiring), ELDANADD (Eldan's criterion-based edge addition), and PROXYADD (proxy spectral gap-based edge addition). The table shows the accuracy achieved by each method on six different datasets (ENZYMES, MUTAG, IMDB-BINARY, REDDIT-BINARY, COLLAB, PROTEINS).  The results demonstrate the effectiveness of the proposed ELDANADD and PROXYADD methods in improving the accuracy of GCNs on graph classification tasks, often outperforming FoSR.", "section": "Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_21_3.jpg", "caption": "Table 11: Graph classification with GCN comparing FoSR, ELDANADD and PROXYADD.", "description": "This table compares the performance of Graph Convolutional Networks (GCNs) combined with different graph rewiring methods on graph classification tasks. The methods compared are FoSR (First-order spectral rewiring), ELDANADD (Eldan's criterion-based edge addition), and PROXYADD (proxy spectral gap-based greedy graph addition).  The table shows the accuracy achieved by each method on several benchmark datasets (ENZYMES, MUTAG, IMDB-BINARY, REDDIT-BINARY, COLLAB, PROTEINS). The results highlight the relative performance of the different graph rewiring techniques in improving graph classification accuracy.", "section": "Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_22_1.jpg", "caption": "Table 13: Node classification using Relational-GCNs comparing FoSR, Eldan's criterion and PROXYADD.", "description": "This table presents the results of node classification experiments using Relational-GCNs on nine datasets.  Three different graph rewiring methods are compared: FoSR (First-order Spectral Rewiring), a method based on Eldan's criterion, and the authors' proposed PROXYADD method. The table shows the accuracy achieved by each method on each dataset, highlighting the performance differences between the approaches.", "section": "E.3 Node classification using Relational-GCN"}, {"figure_path": "EMkrwJY2de/tables/tables_22_2.jpg", "caption": "Table 14: Runtimes for 50 edge modifications in seconds.", "description": "This table presents the runtimes, in seconds, for modifying 50 edges in four different graph datasets using four different methods: FoSR, SDRF, PROXYADD, and PROXYDELETE.  The runtimes are broken down by dataset (Cora, Citeseer, Chameleon, and Squirrel) and method.  The table shows that PROXYDELETE is consistently the fastest method, with runtimes significantly lower than the other three methods, especially on larger datasets like Chameleon and Squirrel.  This highlights the computational efficiency of the PROXYDELETE method compared to alternative approaches for spectral graph rewiring.", "section": "4 Graph rewiring with Proxy spectral gap updates"}, {"figure_path": "EMkrwJY2de/tables/tables_23_1.jpg", "caption": "Table 15: Empirical runtime (RT) comparisons with different update periods for the criterion for 50 edges. We also report the spectral gap before (SG.B) and after rewiring (SG.A).", "description": "This table compares the runtimes (RT) of the PROXYADD and PROXYDELETE algorithms with different update periods (M) for modifying 50 edges in four different datasets (Cora, Citeseer, Chameleon, Squirrel). It also shows the spectral gap before (SG.B) and after (SG.A) the rewiring process.  The results demonstrate the trade-off between runtime and the spectral gap improvement achieved by varying the update frequency.", "section": "F Update period, empirical runtimes and spectral gap comparisons"}, {"figure_path": "EMkrwJY2de/tables/tables_23_2.jpg", "caption": "Table 16: We compare the spectral gap improvements of different rewiring methods for 50 edge modifications. From the table it is evident that our proposed PROXYADD and PROXYDELETE methods improve the spectral gap much better than FoSR.", "description": "This table compares the effectiveness of different graph rewiring methods in improving the spectral gap.  It shows the spectral gap before and after applying FoSR, PROXYADD, PROXYDELETE, ELDANADD, and ELDANDELETE methods. The results demonstrate that PROXYADD and PROXYDELETE significantly improve the spectral gap compared to FoSR.", "section": "Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_23_3.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "The table presents the results of experiments conducted on the Long Range Graph Benchmark datasets. Several GCN models are compared, including a baseline GCN and models augmented with different graph rewiring techniques: DRew, FoSR, ProxyAdd, and ProxyDelete.  The performance of each model is evaluated using three metrics: Test F1 (for PascalVOC-SP), Test AP (for Peptides-Func), and Test MAE (for Peptides-Struct).  The results show the effectiveness of the proposed spectral gap-based edge addition and deletion methods (ProxyAdd and ProxyDelete) in improving the performance of GCNs on these datasets.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_23_4.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "This table presents the results of the proposed spectral graph pruning methods (PROXYADD and PROXYDELETE) and compares them to several baselines on three datasets from the Long Range Graph Benchmark (LRGB).  The datasets represent different node classification and graph classification tasks. The metrics used for evaluation include F1-score (for classification tasks) and Average Precision (AP) and Mean Absolute Error (MAE) (for regression tasks). The results demonstrate the performance of the proposed methods compared to state-of-the-art baselines.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_24_1.jpg", "caption": "Table 19: Hyperparameters for GCN+our proposed rewiring algorithms.", "description": "This table lists the hyperparameters used for the Graph Convolutional Network (GCN) experiments with the proposed edge rewiring algorithms.  For each dataset (Cora, Citeseer, Pubmed, Cornell, Wisconsin, Texas, Actor, Chameleon, Squirrel), it specifies the learning rate (LR), hidden dimension size, dropout rate, and the number of edges added or deleted for the ELDANADD, ELDANDELETE, PROXYADD, and PROXYDELETE algorithms.", "section": "H Training details and hyperparameters"}, {"figure_path": "EMkrwJY2de/tables/tables_24_2.jpg", "caption": "Table 10: We compare the performance of GCN augmented with different graph rewiring methods on node classification.", "description": "This table presents the results of node classification experiments using Graph Convolutional Networks (GCNs) enhanced with various graph rewiring techniques.  The base GCN model is compared against versions incorporating DIGL, SDRF, FoSR, EldanDELETE, EldanADD, PROXYADD, PROXYDELETE, and RANDOMDELETE methods. The table shows the test accuracy achieved on several benchmark datasets (Cora, Citeseer, Pubmed, Cornell, Wisconsin, Texas, Actor, Chameleon, and Squirrel), highlighting the effectiveness of different rewiring strategies in improving node classification performance.  The results reveal how these methods impact accuracy, considering both adding and deleting edges to enhance the spectral gap and manage over-smoothing.", "section": "E.1 Node classification"}, {"figure_path": "EMkrwJY2de/tables/tables_24_3.jpg", "caption": "Table 11: Node classification using Relational-GCNs comparing FoSR, Eldan's criterion and PROXYADD.", "description": "This table compares the performance of Relational Graph Convolutional Networks (R-GCNs) augmented with three different graph rewiring methods on nine node classification datasets.  The methods compared are FoSR (First-order Spectral Rewiring), a method based on Eldan's criterion (a theoretical guarantee for spectral gap increase), and PROXYADD (a computationally efficient proxy method for spectral gap maximization). The table shows the test accuracy for each method on each dataset, highlighting the effectiveness of the proposed PROXYADD method in several cases.", "section": "E.3 Node classification using Relational-GCN"}, {"figure_path": "EMkrwJY2de/tables/tables_25_1.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "This table presents the results of the Long Range Graph Benchmark (LRGB) experiments.  The LRGB is designed to specifically test GNN performance in the presence of over-squashing. The table compares the performance of several methods (Baseline-GCN, DRew+GCN, FOSR+GCN, ProxyAdd+GCN, and ProxyDelete+GCN) across three different datasets: PascalVOC-SP, Peptides-Func, and Peptides-Struct, reporting their F1 score (for PascalVOC-SP), Average Precision (AP, for Peptides-Func), and Mean Absolute Error (MAE, for Peptides-Struct).  The baseline GCN is a standard Graph Convolutional Network, while the other methods incorporate various graph rewiring techniques to mitigate the effects of over-squashing.  The results illustrate the effectiveness of the proposed ProxyAdd and ProxyDelete methods relative to the baseline and other state-of-the-art rewiring methods.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_25_2.jpg", "caption": "Table 1: Results on Long Range Graph Benchmark datasets.", "description": "The table shows the results of different graph rewiring methods on three datasets from the Long Range Graph Benchmark.  The methods include a baseline GCN, DRew+GCN, FOSR+GCN, ProxyAdd+GCN, and ProxyDelete+GCN. The metrics reported are Test F1 for PascalVOC-SP, Test AP for Peptides-Func, and Test MAE for Peptides-Struct.  The Long Range Graph Benchmark is specifically designed to evaluate graph neural networks' performance on graphs with long-range dependencies.", "section": "5 Experiments"}, {"figure_path": "EMkrwJY2de/tables/tables_25_3.jpg", "caption": "Table 25: Hyperparameters for DIGL.", "description": "This table lists the hyperparameters used for the DIGL algorithm in the paper's experiments.  It shows the learning rate (LR), dropout rate, hidden dimension size, alpha (\u03b1), and kappa (\u03ba) values used for each dataset.  These settings were used to tune the DIGL model for node classification experiments.", "section": "E.1 Node classification"}, {"figure_path": "EMkrwJY2de/tables/tables_25_4.jpg", "caption": "Table 11: Graph classification with GCN comparing FoSR, ELDANADD and PROXYADD.", "description": "This table compares the performance of Graph Convolutional Networks (GCNs) combined with three different graph rewiring methods on graph classification tasks. The methods are: FoSR (First-order Spectral Rewiring), ELDANADD (greedy edge addition based on Eldan's criterion), and PROXYADD (greedy edge addition based on a proxy for spectral gap changes).  The table shows the accuracy achieved by each method on six different benchmark datasets: ENZYMES, MUTAG, IMDB-BINARY, REDDIT-BINARY, COLLAB, and PROTEINS. The results highlight the effectiveness of the proposed methods, especially PROXYADD, in improving graph classification accuracy compared to FoSR.", "section": "E.1 Node classification"}, {"figure_path": "EMkrwJY2de/tables/tables_25_5.jpg", "caption": "Table 11: Graph classification with GCN comparing FoSR, ELDANADD and PROXYADD.", "description": "This table compares the performance of Graph Convolutional Networks (GCNs) combined with different graph rewiring methods on graph classification tasks.  The methods compared are FoSR (First-Order Spectral Rewiring), ELDANADD (Greedy graph addition based on Eldan's criterion), and PROXYADD (Greedy graph addition using a proxy spectral gap update). The table shows the accuracy achieved by each method on several benchmark datasets including ENZYMES, MUTAG, IMDB-BINARY, REDDIT-BINARY, COLLAB, and PROTEINS.  The results illustrate the relative effectiveness of the different rewiring strategies for improving GCN performance on graph classification problems.", "section": "E.1 Node classification"}]