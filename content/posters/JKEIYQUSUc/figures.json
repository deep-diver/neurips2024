[{"figure_path": "JKEIYQUSUc/figures/figures_0_1.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the 3D scene graph construction pipeline used in the paper.  The pipeline starts with image collections, which are filtered to remove unsuitable images.  Open-vocabulary detection and segmentation then identify and extract object instances, which are further processed using metric depth estimation and camera calibration to create 3D point clouds.  These point clouds are then used to create the final 3D scene graph which includes region masks and spatial relation information.  Finally, the scene graph is used to generate region-aware spatial QAs using either template-based methods or an LLM-based approach for training.", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_3_1.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the automatic data curation pipeline used to construct 3D scene graphs from single 2D images. The pipeline consists of several stages: filtering to remove unsuitable images, open-vocabulary detection and segmentation to extract object instances, metric depth estimation to obtain depth information, camera calibration to project objects into 3D space, and finally point cloud processing to construct the 3D scene graph. Each stage is represented by a box in the figure, with arrows showing the flow of data between stages.  The figure also shows example images at various stages of the pipeline, illustrating the process of transforming a 2D image into a 3D scene graph.", "section": "3.1 3D Scene Graph from Single 2D Images"}, {"figure_path": "JKEIYQUSUc/figures/figures_4_1.jpg", "caption": "Figure 2: Example data entries from our Open Spatial Dataset. The first row contains template-based QAs, and the second row shows LLM-based entries.", "description": "This figure shows example question-answer pairs from the Open Spatial Dataset.  The first row demonstrates template-based questions and answers, which follow a predefined structure. These examples focus on fundamental spatial concepts like relative position and metric measurements.  The second row showcases examples generated using a Large Language Model (LLM).  These questions and answers are more complex and demonstrate the model's ability to reason about more nuanced spatial relationships than the simpler template-based examples. The figure illustrates the diversity of the data used to train the SpatialRGPT model and highlights its ability to generate both structured and free-form QA pairs.", "section": "3.2 Learning Spatial-aware VLMs from 3D Scene Graph"}, {"figure_path": "JKEIYQUSUc/figures/figures_5_1.jpg", "caption": "Figure 3: An architecture overview of SpatialRGPT. denotes freezed/trainable parameters.", "description": "The figure shows the architecture of SpatialRGPT, a multimodal language model that leverages both RGB and depth information for enhanced spatial reasoning.  It consists of a visual backbone (processing RGB and depth data), a region feature extractor (to process region masks/boxes), an RGB connector, a depth connector, and a large language model (LLM). The RGB and depth connectors project features into the LLM's embedding space, enabling the model to reason about spatial relationships within regions using multimodal inputs.", "section": "3.3 VLM Architecture"}, {"figure_path": "JKEIYQUSUc/figures/figures_7_1.jpg", "caption": "Figure 4: SpatialRGPT is capable of complex spatial reasoning, addressing gaps that current leading vision language models, such as GPT-4V, struggle with.", "description": "This figure showcases SpatialRGPT's ability to answer complex spatial reasoning questions.  It presents four examples, each with an image and a question-answer pair generated by SpatialRGPT and GPT-4V.  The examples demonstrate that SpatialRGPT can handle nuanced spatial relationships that GPT-4V struggles with, including relative positions, distances, object sizes, and even multi-hop reasoning.  This highlights SpatialRGPT's improved ability to interpret complex visual environments.", "section": "4 Experiments"}, {"figure_path": "JKEIYQUSUc/figures/figures_9_1.jpg", "caption": "Figure 5: Examples of SpatialRGPT performing multi-hop reasoning.", "description": "This figure showcases SpatialRGPT's ability to perform multi-hop reasoning by answering complex spatial questions that require multiple steps of inference. Each example shows a scene with identified regions and a question that demands understanding of multiple spatial relationships between objects within those regions.  The model's responses correctly identify objects, their locations, and distances, demonstrating its capacity for advanced spatial understanding.", "section": "4.3 Real-world Applications"}, {"figure_path": "JKEIYQUSUc/figures/figures_17_1.jpg", "caption": "Figure 7: SpatialRGPT-Bench statistics. Left: Category count and source count. Right: Object count.", "description": "This figure presents a statistical overview of the SpatialRGPT-Bench dataset. The left panel shows a donut chart illustrating the distribution of question categories (relative spatial relationships and metric measurements) and their sources (different datasets used). The right panel displays a bar chart showing the frequency distribution of objects present in the dataset.  This visualization helps in understanding the dataset's composition and balance across various aspects.", "section": "E Statistics and Samples of SpatialRGPT-Bench"}, {"figure_path": "JKEIYQUSUc/figures/figures_18_1.jpg", "caption": "Figure 8: Samples in SpatialRGPT-Bench.", "description": "This figure shows six example images from the SpatialRGPT-Bench dataset, each with a question and answer pair illustrating SpatialRGPT's ability to reason about spatial relationships.  The images cover diverse indoor and outdoor scenes with different objects, highlighting the variety and complexity of the benchmark.", "section": "E. Statistics and Samples of SpatialRGPT-Bench"}, {"figure_path": "JKEIYQUSUc/figures/figures_19_1.jpg", "caption": "Figure 8: Samples in SpatialRGPT-Bench.", "description": "This figure shows six example images from the SpatialRGPT-Bench dataset. Each image shows a scene with several objects, their segmentation masks, and examples of the kinds of questions and answers that can be generated from these scenes.  The top row features RGB images, while the bottom row shows the depth maps generated from those images.", "section": "E Statistics and Samples of SpatialRGPT-Bench"}, {"figure_path": "JKEIYQUSUc/figures/figures_19_2.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the automatic data curation pipeline used to construct 3D scene graphs from single 2D images. The pipeline consists of several stages: filtering unsuitable images, open-vocabulary detection and segmentation to extract object instances, metric depth estimation to obtain depth information, camera calibration to project objects into 3D space, and finally, constructing the 3D scene graph by connecting object nodes with edges representing spatial relationships.  Each stage is depicted with a visual representation of the process.", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_20_1.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the data curation pipeline used to automatically generate 3D region-aware annotations from 2D images at scale.  The pipeline consists of three main steps: (i) open-vocabulary detection and segmentation to extract object instances, (ii) metric depth estimation to obtain accurate depth information, and (iii) camera calibration to project objects into a 3D space. These scene graphs are subsequently transformed into region-aware spatial QA tasks to provide region-based VLMs with necessary spatial knowledge and enable complex spatial reasoning.", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_20_2.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the data curation pipeline used to generate 3D scene graphs from single 2D images.  It shows the steps involved, starting with image filtering to remove unsuitable images, followed by open-vocabulary detection and segmentation to identify object instances, metric depth estimation to project objects into 3D space, camera calibration to align them into a common coordinate system, and finally, the construction of the 3D scene graph itself, which represents the spatial relationships between objects. This pipeline enables the creation of training data for SpatialRGPT, a model designed for enhanced spatial reasoning in vision-language models.", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_22_1.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "The figure illustrates the data pipeline used to automatically generate 3D region-aware annotations from 2D images at scale.  It starts with image filtering, then uses open-vocabulary detection and segmentation to extract object instances and their masks. Metric depth estimation and camera calibration are used to project these objects into a 3D space. The resulting point cloud is processed to construct the 3D scene graph, where nodes represent 3D object instances and edges denote their spatial relationships.  Finally, these scene graphs are transformed into region-aware spatial QA tasks. ", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_23_1.jpg", "caption": "Figure 1: 3D scene graph construction via automatic data curation pipeline.", "description": "This figure illustrates the process of automatically generating 3D scene graphs from single 2D images.  It begins with filtering to remove unsuitable images, open-vocabulary object detection and segmentation to extract instances, metric depth estimation to determine distances, camera calibration to project into 3D, and finally, processing the point cloud to construct the scene graph.  Each step involves specific models and techniques to ensure accuracy and robustness in the 3D representation of the scene.", "section": "3 Method"}, {"figure_path": "JKEIYQUSUc/figures/figures_23_2.jpg", "caption": "Figure 11: Different types of bounding box.", "description": "This figure shows a comparison between axis-aligned bounding boxes (AABB) and oriented bounding boxes (OBB).  The AABB is a simple rectangular box aligned with the coordinate axes, while the OBB is a more complex box that rotates to better fit the object's shape and orientation. The figure illustrates how an OBB (purple) more accurately encloses an object (sofa) compared to an AABB (blue). The difference is particularly noticeable in cases where the object is not aligned with the coordinate axes, improving the accuracy of object detection and representation.", "section": "K More Discussion on Limitations"}]