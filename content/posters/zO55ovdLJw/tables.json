[{"figure_path": "zO55ovdLJw/tables/tables_7_1.jpg", "caption": "Table 1: Ablations for the correlated prompts.", "description": "This table presents ablation studies on the correlated prompts component of the proposed Deep Correlated Prompting (DCP) method.  It shows the impact of different configurations on the F1-Macro score, a performance metric. Specifically, it investigates the effects of using different prompt generation functions (No projection, Fc, MLP), varying prompt depths (Depth = 3, 6, 12), and considering only one modality versus incorporating information from both modalities (uni-modal, bi-modal) when generating the correlated prompts.", "section": "3.3 Deep Correlated Prompt Learning"}, {"figure_path": "zO55ovdLJw/tables/tables_9_1.jpg", "caption": "Table 4: Comparison with CoOp [44], MMP [19], MaPLe [16] and DePT [41] on the MM-IMDb [2], UPMC Food-101 [33], and Hateful Memes [17] datasets under various missing-modality cases with different missing rates. The bold number indicates the best performance.", "description": "This table presents a comparison of the proposed Deep Correlated Prompting (DCP) method with four other state-of-the-art methods (CoOp, MMP, MaPLe, and DePT) for handling missing modalities in multimodal learning.  The comparison is performed across three different datasets (MM-IMDb, UPMC Food-101, and Hateful Memes) and various missing modality scenarios (missing image, missing text, missing both) with different missing rates (50%, 70%, 90%). The results are evaluated using appropriate metrics for each dataset (F1-Macro for MM-IMDb, Accuracy for Food101, and AUROC for Hateful Memes). The bold numbers highlight the best performing method for each scenario.", "section": "4.2 Experimental Results"}, {"figure_path": "zO55ovdLJw/tables/tables_13_1.jpg", "caption": "Table 4: Comparison with CoOp [44], MMP [19], MaPLe [16] and DePT [41] on the MM-IMDb [2], UPMC Food-101 [33], and Hateful Memes [17] datasets under various missing-modality cases with different missing rates. The bold number indicates the best performance.", "description": "This table presents a comparison of the proposed Deep Correlated Prompting (DCP) method with four other methods (CoOp, MMP, MaPLe, and DePT) for handling missing modalities in multimodal learning.  The comparison is done across three different datasets (MM-IMDb, UPMC Food-101, and Hateful Memes) and various missing modality scenarios (image only missing, text only missing, both image and text missing).  The results are shown for different missing rates (50%, 70%, and 90%), demonstrating DCP's performance relative to existing approaches across different datasets and conditions.", "section": "4.2 Experimental Results"}, {"figure_path": "zO55ovdLJw/tables/tables_14_1.jpg", "caption": "Table 4: Comparison with CoOp [44], MMP [19], MaPLe [16] and DePT [41] on the MM-IMDb [2], UPMC Food-101 [33], and Hateful Memes [17] datasets under various missing-modality cases with different missing rates. The bold number indicates the best performance.", "description": "This table compares the performance of the proposed Deep Correlated Prompting (DCP) method against several other state-of-the-art methods for handling missing modalities in multimodal learning.  The comparison is done across three different datasets (MM-IMDb, UPMC Food-101, and Hateful Memes) and various missing modality scenarios (missing image, missing text, missing both). The results show the F1-Macro score for MM-IMDb, accuracy for Food101, and AUROC for Hateful Memes, demonstrating DCP's superior performance in most cases.", "section": "4.2 Experimental Results"}, {"figure_path": "zO55ovdLJw/tables/tables_14_2.jpg", "caption": "Table 8: Ablations for the prompt length on the MM-IMDb dataset with missing rate \u03b7 = 70%. The bold number indicates the best performance.", "description": "This table presents the results of ablation studies on the prompt length, focusing on its impact on the F1-Macro score using the MM-IMDb dataset.  The experiment was conducted with a missing rate (\u03b7) of 70%. The table shows that a prompt length of 36 achieves the best performance.", "section": "4.2 Experimental Results"}]