[{"figure_path": "HxGdbAmYYr/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our method. Adversarial perturbations, bounded by different budgets e, are incorporated into the clean query set. To construct the robust LoRAPool, the LoRA modules initialized with SVD results are meta-tuned on the adversarial examples, upon which adversarial perturbations are injected into singular values and vectors. The discriminative incremental updates of principal components are adaptively merged into the pre-trained weights for test-time task customization.", "description": "This figure illustrates the AMT method's architecture.  It starts with adversarial meta-tuning, where adversarial perturbations (with varying budgets 'e') are added to the query set to create an adversarial query set.  The model uses a robust LoRAPool (a set of low-rank adapters, LoRAs, meta-tuned with adversarial examples).  Each LoRA in the pool is initialized using Singular Value Decomposition (SVD) on the pre-trained weights and incorporates perturbations on singular values and vectors. A test-time merging mechanism dynamically combines the LoRAs based on their effectiveness for a given task, resulting in updated weights ('W'). The weights are then trimmed ('W') before being used for prediction.", "section": "4 Methods"}, {"figure_path": "HxGdbAmYYr/figures/figures_9_1.jpg", "caption": "Figure 2: Effectiveness of the adversarial perturbation on singular values and vectors. The accuracy on Meta-Dataset in the 5-way 1-shot is reported.", "description": "This figure shows the impact of applying adversarial perturbations to singular values and vectors on the model's accuracy. The left subplot shows the accuracy improvement achieved by including singular value and vector perturbations compared to not including them. The right subplot shows the changes in top singular values of FFN across layers. The results demonstrate that the adversarial perturbations enhance the model's robustness, particularly in the later layers of the network.", "section": "4.2 Adversarial Singular Value and Vector Perturbation"}, {"figure_path": "HxGdbAmYYr/figures/figures_19_1.jpg", "caption": "Figure 3: Changes in top singular values of MHA across layers", "description": "The figure shows the changes in top singular values of the projection weight matrix across multi-head self-attention layers.  It illustrates how the adversarial double-perturbation strategy enhances the model's principal components, improving its robustness against strong attacks during meta-tuning and leading to better generalization.", "section": "4.2 Adversarial Singular Value and Vector Perturbation"}, {"figure_path": "HxGdbAmYYr/figures/figures_24_1.jpg", "caption": "Figure 4: The robustness averaged over Meta-Dataset datasets of different methods in the 5-way 1-shot setting. Robustness is evaluated against 15 common distortions across four categories with varying severity levels.", "description": "This figure visualizes the robustness of different few-shot learning methods against various image corruptions.  The x-axis represents different corruption types (Gaussian Noise, Shot Noise, etc.) and the y-axis represents the accuracy.  Each bar represents a specific method (PM, StyleAdv, AMT, PMF, AMT-FT), showing their performance under each type of corruption.  The results indicate the relative robustness of each method in handling these common image distortions, highlighting the superior performance of AMT and AMT-FT in maintaining accuracy.", "section": "L Few-shot Robustness against Natural Corruptions Under Distribution Shifts"}]