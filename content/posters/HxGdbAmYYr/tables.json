[{"figure_path": "HxGdbAmYYr/tables/tables_7_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the clean accuracy results of different few-shot learning methods on the Meta-Dataset benchmark.  The results are broken down by the number of shots (1-shot and 5-shot) and the specific dataset within the benchmark.  It compares the performance of the proposed AMT method against several baselines, including PMF and ATTNSCALE, both with and without the AMT-FT enhancement. The table highlights the average accuracy across multiple domains for each method and setting.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_7_2.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the clean accuracy results of several few-shot learning methods on the Meta-Dataset benchmark.  The results are broken down by dataset (ImageNet, Omniglot, etc.), setting (1-shot or 5-shot), and whether test-time fine-tuning (TTF) was used.  The table compares the performance of several methods (PM [12], StyleAdv [86], AMT, and others) and highlights the best performance for each task configuration.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_8_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the clean accuracy results of various few-shot learning methods on the Meta-Dataset benchmark.  It shows the performance of different methods (including the proposed AMT) in both 1-shot and 5-shot scenarios across multiple datasets.  The results are broken down by dataset and whether test-time fine-tuning was used, providing a comprehensive comparison of the methods' generalization capabilities.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_8_2.jpg", "caption": "Table 4: Component ablation studies on Meta-Dataset in the 5-way 1-shot setting. APQ: adversarial perturbation on query set, APSV: adversarial perturbation on singular values and vectors, RLP: Robust LoRAPool, TTM: test-time merging, STr: singular value trimming.", "description": "This table presents an ablation study on the Meta-Dataset benchmark for a 5-way 1-shot setting. It evaluates the impact of different components of the proposed AMT method on the performance. The components include adversarial perturbation on the query set (APQ), adversarial perturbation on singular values and vectors (APSV), Robust LoRAPool (RLP), test-time merging (TTM), and singular value trimming (STr). The table shows the average accuracy across various tasks on ImageNet and OOD datasets for different combinations of these components.", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_18_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of a comparison of different methods for few-shot image classification on the Meta-Dataset benchmark.  It shows the average accuracy achieved by each method across various image datasets for both 1-shot and 5-shot scenarios.  The results are categorized by whether or not test-time fine-tuning was used.  The best performing method for each task and configuration is highlighted in bold.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_19_1.jpg", "caption": "Table 6: Comparison of AMT with the alternative merging strategies on Meta-Dataset in the 5-way 1-shot setting.", "description": "This table compares the performance of AMT with three alternative test-time merging strategies: Weight Average, Logit Average, and a Linear Classifier.  The comparison is made on the Meta-Dataset benchmark using a 5-way 1-shot setting.  The table shows the average accuracy across various datasets (INet, Omglot, Acraft, CUB, DTD, QDraw, Fungi, Flower, Sign, COCO) for both in-domain and out-of-domain performance. This allows for a direct comparison of how effectively each merging strategy integrates the robust LoRAPool into the pre-trained model for improved task adaptation and generalization.", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_20_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot image classification on the Meta-Dataset benchmark using various methods in both 1-shot and 5-shot settings.  The average accuracy across multiple domains (ImageNet, Omniglot, etc.) is shown for each method,  indicating their performance in out-of-distribution few-shot learning scenarios.  The table highlights the performance of the proposed AMT method compared to baselines and other state-of-the-art methods, emphasizing the improvements achieved by incorporating test-time fine-tuning.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_20_2.jpg", "caption": "Table 8: The influence of attack pool strategy on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset.", "description": "This table compares three different strategies for constructing the robust LoRAPool in the AMT model: uniform, random, and separate.  The uniform strategy uses the average attack strength across all training tasks. The random strategy randomly samples an attack budget for each training task. The separate strategy meta-tunes separate LoRA modules for each attack strength. The table presents the in-domain and out-of-domain average accuracy for each strategy across ten image datasets from Meta-Dataset. The results show that the proposed robust LoRAPool with perturbation-specific parameters effectively avoids interference between different attacks and significantly enhances out-of-domain generalization without compromising in-domain performance.", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_20_3.jpg", "caption": "Table 4: Component ablation studies on Meta-Dataset in the 5-way 1-shot setting. APQ: adversarial perturbation on query set, APSV: adversarial perturbation on singular values and vectors, RLP: Robust LoRAPool, TTM: test-time merging, STr: singular value trimming.", "description": "This table presents an ablation study to evaluate the impact of different components of the proposed AMT method on the Meta-Dataset benchmark using a 5-way 1-shot setting.  Each row represents a variation of the AMT method, removing one or more components.  The results show the effectiveness of each component in improving the overall performance.", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_21_1.jpg", "caption": "Table 10: The influence of adversarial perturbation space on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset.", "description": "This table shows the impact of using either weight space or spectral space perturbation in adversarial meta-tuning.  It compares the performance of the proposed AMT method against a baseline using standard LoRA initialization, both in terms of in-domain and out-of-domain accuracy. The results highlight the effectiveness of adversarial perturbations on singular values and vectors, as proposed in the AMT method. The bold entries indicate the best performance for each specific dataset within each domain (in-domain and out-of-domain).", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_21_2.jpg", "caption": "Table 11: The influence of loss trade-off coefficient \\lambda_{adv} on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. * denotes our choice.", "description": "This table shows the influence of the loss trade-off coefficient (\\lambda_{adv}) on the performance of the AMT model.  The experiment is conducted on the Meta-Dataset benchmark using a 5-way 1-shot setting.  The table displays the in-domain and out-of-domain accuracy for different values of \\lambda_{adv}, with the best results for each task bolded and the chosen \\lambda_{adv} marked with an asterisk.", "section": "5.2 Ablation Study"}, {"figure_path": "HxGdbAmYYr/tables/tables_21_3.jpg", "caption": "Table 3: Few-shot classification adversarial robust accuracy on Meta-Dataset in the 5-way 1-shot and 5-shot settings. Adv. TTF: adversarial test-time fine-tuning.", "description": "This table presents the results of adversarial robust accuracy on the Meta-Dataset benchmark for both 5-way 1-shot and 5-shot settings.  It compares different methods, including a baseline (PM [12]), StyleAdv [86], and the proposed AMT method, both with and without adversarial test-time fine-tuning (Adv. TTF). The table shows the accuracy for each of the ten domains in the Meta-Dataset, as well as the average accuracy across all domains.  It highlights the effectiveness of AMT in improving adversarial robustness in few-shot learning.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_21_4.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot image classification accuracy on the Meta-Dataset benchmark.  It compares the performance of different methods (PM[12], StyleAdv[86], AMT, PMF[12], PMF+AMT-FT, ATTNSCALE[59], ATTNSCALE+AMT-FT) in both 5-way 1-shot and 5-way 5-shot settings across various image domains (ImageNet, Omniglot, Acraft, CUB, DTD, QDraw, Fungi, Flower, Sign, COCO). The results are presented as average accuracy and include the effect of test-time fine-tuning (TTF).  Bold values highlight the best performing method for each task configuration.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_22_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot image classification accuracy on the Meta-Dataset benchmark.  It compares several methods in both 1-shot and 5-shot settings across multiple out-of-domain datasets. The table shows the average accuracy for each method on each dataset, highlighting the best-performing method for each task configuration.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_22_2.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot classification accuracy on the Meta-Dataset benchmark.  It compares several methods (including the proposed AMT and baselines) in both 1-shot and 5-shot scenarios across multiple domains.  The table shows average accuracy for each domain and overall average, highlighting the best-performing method for each task configuration.  The results demonstrate the effectiveness of the AMT method on a challenging few-shot learning benchmark.", "section": "5.1 Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_22_3.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot image classification accuracy on the Meta-Dataset benchmark.  It compares the performance of various methods (including the proposed AMT and baselines like PMF and ATTNSCALE) across different domains in both 1-shot and 5-shot settings. The table shows average accuracy and highlights the best-performing method for each task configuration.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_22_4.jpg", "caption": "Table 16: Comparison with other data augmentation methods on Meta-Dataset in the 5-way 1-shot setting. Single LoRA (P = 1) is used for all methods. Bold entries indicate the best for each task dataset.", "description": "This table compares the performance of AMT with other data augmentation methods (RandConv, ALT, and ALT with attack pool) on the Meta-Dataset benchmark for 5-way 1-shot image classification.  The table shows the in-domain and out-of-domain accuracy for each method. The goal is to assess the effectiveness of different data augmentation strategies in improving the model's robustness to distribution shifts and out-of-domain generalization.", "section": "5.3 More Analysis"}, {"figure_path": "HxGdbAmYYr/tables/tables_23_1.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of few-shot image classification experiments conducted on the Meta-Dataset benchmark.  It compares different methods' performance across various datasets in both 1-shot and 5-shot settings. The table shows the average accuracy achieved by each method in each dataset, along with whether test-time fine-tuning was used.  Bold entries indicate the best-performing method for each task configuration.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_23_2.jpg", "caption": "Table 1: Few-shot classification clean accuracy (%) on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration.", "description": "This table presents the results of clean few-shot image classification accuracy on the Meta-Dataset benchmark.  It compares several methods (PM[12], StyleAdv[86], AMT, PMF[12], PMF+AMT-FT, ATTNSCALE[59], ATTNSCALE+AMT-FT) in both 1-shot and 5-shot settings across various domains (ImageNet, Omniglot, Acraft, CUB, DTD, QDraw, Fungi, Flower, Sign, COCO).  The table shows average accuracy, highlighting the best performing methods for each task.", "section": "Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_24_1.jpg", "caption": "Table 3: Few-shot classification adversarial robust accuracy on Meta-Dataset in the 5-way 1-shot and 5-shot settings. Adv. TTF: adversarial test-time fine-tuning.", "description": "This table presents the adversarial robust accuracy results of the proposed AMT method and several baseline methods on the Meta-Dataset benchmark.  The results are shown for both 1-shot and 5-shot settings.  It shows the in-domain and out-of-domain performance under adversarial attacks, highlighting the effectiveness of AMT in improving adversarial robustness.", "section": "5.1 Comparison with State-Of-The-Art Methods"}, {"figure_path": "HxGdbAmYYr/tables/tables_25_1.jpg", "caption": "Table 3: Few-shot classification adversarial robust accuracy on Meta-Dataset in the 5-way 1-shot and 5-shot settings. Adv. TTF: adversarial test-time fine-tuning.", "description": "This table shows the results of adversarial robust accuracy on the Meta-Dataset benchmark for both 1-shot and 5-shot settings.  It compares the performance of the proposed AMT method against baseline methods (PM [12] and StyleAdv [86]) with and without adversarial test-time fine-tuning (Adv. TTF). The results are presented as the average accuracy across various domains (ImageNet, Omniglot, Acraft, CUB, DTD, QDraw, Fungi, Flower, Sign, COCO).  The table highlights the improvements in adversarial robustness achieved by AMT compared to the baseline methods, showing the impact of the adversarial meta-tuning approach.", "section": "5.1 Comparison with State-Of-The-Art Methods"}]