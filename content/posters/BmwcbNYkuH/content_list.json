[{"type": "text", "text": "Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dhananjay Tomar ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Binder ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Oslo dhananjt@ifi.uio.no ", "page_idx": 0}, {"type": "text", "text": "Otto-von-Guericke University Magdeburg, Singapore Institute of Technology alexander.binder@ovgu.de ", "page_idx": 0}, {"type": "text", "text": "Andreas Kleppe\u2217 Oslo University Hospital, University of Oslo, UiT The Arctic University of Norway andrekle@ifi.uio.no ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment. We hypothesise that focusing on nuclei can improve the out-of-domain (OOD) generalisation in cancer detection. We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection. Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement. Going beyond mere data augmentation, we introduce a regularisation technique that aligns the representations of masks and original images. We show, using multiple datasets, that our method improves OOD generalisation and also leads to increased robustness to image corruptions and adversarial attacks. The source code is available at https://github.com/ undercutspiky/SFL/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Domain generalisation in histopathology is a crucial challenge because domain shifts naturally occur among hospitals and even within a single hospital or laboratory, e.g., temporally or among human operators and observers such as pathologists. Non-biological factors that substantially alter the images include differences in scanners, staining protocols, fixation of tissue, and even minor aspects like the manufacturer and storage conditions of stains [1]. ", "page_idx": 0}, {"type": "text", "text": "Collecting data from numerous hospitals to address these domain shifts is often impractical and may not adequately reflect the full variability present in routine clinical practice, thus making it difficult to build computational histopathology models that generalise well. This leads us to focus on single-domain generalisation (S-DG) in this paper, specifically on how to train a model using data from only one hospital (considered a domain here) that generalises well to data from other hospitals. Popular S-GD methods in histopathology apply data augmentation and stain normalisation [2, 3]. The effectiveness of S-GD methods developed for natural images remains underexplored in histopathology [3]. Here, we compare these methods to a new, simple approach that we propose. ", "page_idx": 0}, {"type": "text", "text": "Research has shown that Convolutional Neural Networks (CNNs) tend to focus on texture over shape [4, 5]. However, in histopathology, the texture and colour of cell nuclei vary much more across domains than the shape and organisation of cell nuclei. As a result, focusing on shape features could improve a computational histopathology model\u2019s ability to generalise to unseen data because it may rely less on domain-specific features that vary across hospitals and more. ", "page_idx": 1}, {"type": "text", "text": "Nuclei in cancerous tissue exhibit distinct changes in shape, size, and overall organisation compared to nuclei in normal tissue [6\u20138]. Pathologists rely on these and other visual cues [9] for cancer diagnosis and grading, underscoring the biological importance and the consistency of nuclear morphology and organisation across domains. We hypothesise that focusing on nuclear morphology and organisation may be sufficient for cancer detection and that exploiting this during training could result in models with good generalisation. ", "page_idx": 1}, {"type": "text", "text": "We propose a method that encourages CNNs to focus more on nuclear morphology and organisation by using additional loss terms that prioritise shape-based features. Specifically, our method leverages nuclear segmentation masks during training to steer the learning towards nuclei. Through extensive experimentation, we demonstrate that this method improves performance on out-of-domain data without requiring nuclear segmentation masks at inference time, thus offering a promising and attractive solution for addressing domain generalisation in histopathology. Our contributions include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel training method that incentivises the model to focus on nuclei. \u2022 We evaluate our method on three datasets comprising hundreds of WSIs in total from various hospitals and organs. Our results show accuracy improvements over all other approaches. \u2022 We evaluate the sensitivity of our method to image corruptions and adversarial attacks. Our results show performance improvements over the baseline. \u2022 We conduct extensive ablation studies to show that models trained with our method focus on nuclei. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The prediction of various properties such as malignancy, grading, and HER2 expression using segmented nuclei has been a well-studied topic for many years [10\u201316]. Researchers have employed techniques such as watershed segmentation [17], thresholding, level sets [18], and snakes [19], often followed by extracting explicit morphometric features from the segmentations. For example, early work by Hasegawa et al. [20] focused on counting segmented regions, while Lee and Street [21] applied neural networks to the segmentation outputs. In contrast to these approaches, our method does not rely on segmentation during inference. Instead, we adjust the training process to encourage the extraction of nuclear features. ", "page_idx": 1}, {"type": "text", "text": "Stain normalisation methods convert the colours of a source image to match those of a target image. These methods were typically designed specifically for the most common type of histopathology images, which are images of tissue stained with haematoxylin and eosin (H&E). One of the earlier methods, Macenko normalisation [1], estimates stain vectors for source and target images and uses them to normalise the source image. Vahadane et al. [22] proposed a method that decouples stain \"density maps\" from \"colour appearances\", allowing the combination of the source image\u2019s density maps with the target image\u2019s colour appearances. Reinhard et al. [23] pioneered colour transfer by adjusting the global statistics of images in a different colour space, effectively transferring the colour characteristics of the target to the source image. Random Stain Normalization and Augmentation (RandStainNA) [24] combines stain normalisation and augmentation. Unlike traditional approaches that normalise using a fixed template, RandStainNA generates random virtual templates in the LAB [23] colour space and uses them to normalise the images during training. The templates are drawn from Gaussian distributions whose means and variances are derived from the training data. For a more comprehensive review, we refer the readers to [25]. In summary, the stain normalisation methods primarily focus on manipulating colour information to remove stain variability. On the other hand, our approach shifts the focus from colour manipulation to nuclear features. ", "page_idx": 1}, {"type": "text", "text": "Data augmentation is a common way to facilitate domain generalisation. Tellez et al. [2] evaluated several stain colour augmentation and stain normalisation methods and found that colour augmentation was crucial for good performance on external test sets in histopathology. Faryna et al. [26] extended RandAugment [27] by including certain histopathology-specific augmentations and excluding the ones that produce unrealistic-looking images. Tellez et al. [28] developed a data augmentation method specific to H&E-stained images and used it for domain generalisation in mitosis detection. Pohjonen et al. [29] developed StrongAugment, where varying numbers of transformations are applied to an image to improve domain generalisation. Marini et al. [30] proposed Data-driven colour augmentation (DDCA), which evaluates an augmented image as acceptable or not for training based on its distance from other images in a database. Faryna et al. [31] evaluated different data augmentation strategies in histopathology, including manually selected augmentations, and found them all to be competitive. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Single-domain generalisation (S-DG) methods do not require data from multiple domains during training. Representation Self-Challenging (RSC) [32] works by discarding the features with relatively high gradients, making the model predict with the remaining features during training. Adversarial Domain Augmentation (ADA) [33] generates adversarial examples iteratively to augment the source domain and creates an ensemble of models. Meta-Learning-based ADA (M-ADA) [34] uses Wasserstein Auto-Encoder [35] to generate new samples and uses adversarial training on top along with a meta-learning scheme. Progressive domain expansion network (PDEN) [36] uses multiple autoencoders to generate new samples to expand the training set. Learning to Diversify (L2D) [37] introduces a learnable style-complement module that generates augmented images. The style-complement module is trained to diversify the images as much as possible but still keep the semantic information intact. ", "page_idx": 2}, {"type": "text", "text": "Domain adaptation, unlike S-DG, requires having access to some samples from the target domain. In histopathology, domain adaptation methods commonly make use of GAN [38] and CycleGAN [39]. StainGAN [40] uses CycleGAN to make images in the source domain look like the target domain. Residual CycleGAN [41] modifies the CycleGAN objective to have the generator produce the residual between domains instead of recreating the input image. In [42], authors augment a generator in CycleGAN with a stain colour matrix as an auxiliary input to stabilise the training. NST_AD_HRNet [43] uses Neural Style Transfer [44, 45] and GAN to preserve the content of the source image while combining it with the style of the target image. In some earlier works [46, 47], the input image is converted to greyscale and then coloured using a generator network which is based on a target image. While domain adaptation is not S-DG and thus a bit tangential to the focus of this paper, it is worth noting that domain adaptation is impractical in many clinical settings and may result in worse generalisation than stain normalisation and colour augmentation [48]. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach aims to enhance S-DG by incentivising the model to focus on shaped-based features of nuclei in histopathological images and thereby reduce overftiting to irrelevant features that may carry higher label noise. ", "page_idx": 2}, {"type": "text", "text": "The first step involves generating segmentation masks that highlight specific areas of interest in the image. This step is applied only during training, while test-time evaluation relies solely on H&Estained images. As we hypothesised that nuclear morphology and organisation contain sufficient information for cancer detection, our segmentation masks are binary images with nuclear pixels as foreground and other pixels as background. ", "page_idx": 2}, {"type": "text", "text": "One possible approach to using the segmentation mask is to include it as a fourth channel in the input image. Alternatively, the mask can be used as the sole input to the model. However, both methods necessitate running the segmentation model during inference, which increases computational demands and slows down processing. ", "page_idx": 2}, {"type": "text", "text": "Our method circumvents the need for a nuclear segmentation network at inference time by incorporating additional loss terms during training. For a given input image $x$ and its corresponding segmentation mask $x^{\\prime}$ , our method involves the following steps: ", "page_idx": 2}, {"type": "text", "text": "1. Execute a forward propagation through the neural network model on both the H&E-stained image $x$ and its nuclear mask $x^{\\prime}$ , saving the embeddings generated by the network as $z$ and $z^{\\prime}$ for $x$ and $x^{\\prime}$ , respectively.   \n2. Compute the Binary Cross-Entropy (BCE) loss for both $x$ and $x^{\\prime}$ .   \n3. Compute the $\\ell_{2}$ -distance between the embeddings $z$ and $z^{\\prime}$ .   \n4. Minimise the sum of the two CE losses and the $\\ell_{2}$ -distance. ", "page_idx": 2}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/396ae59f7d9ed80c50048edb2de9be5f99281f8e20e2ecf248512cd06764b073.jpg", "img_caption": ["Figure 1: We pass the input image (or, with 0.5 probability, input image multiplied with its nuclear segmentation mask) and its nuclear segmentation mask through the network and minimise the Binary Cross-Entropy (BCE) loss for both the input image and its mask. Additionally, we minimise the $\\ell_{2}$ -distance between the input image\u2019s embedding vector and the mask\u2019s embedding vector just before the Global Average Pooling (GAP) layer. The embedding vector is ResNet-50\u2019s penultimate layer\u2019s feature map, i.e., stage 4\u2019s last feature map. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Our approach is illustrated in Figure 1. We employ a ResNet-50 [49] from Torchvision [50] as the base model. We next discuss some details of our approach. ", "page_idx": 3}, {"type": "text", "text": "$\\ell_{2}$ -regularisation: To encourage the network to focus on nuclei, we minimise the distance between the feature map of the original image and that of its nuclear segmentation mask. We use the flattened feature map from ResNet-50\u2019s penultimate layer, just before Global Average Pooling, to obtain the embeddings. The regularisation term consists of the $\\ell_{2}$ -distance between the embeddings of the original image $z$ and its mask $z^{\\prime}$ , which is added to the BCE losses for both the image and the mask. Let $\\hat{y}$ be the model\u2019s prediction for $x$ , $\\hat{y}^{\\prime}$ for $x^{\\prime}$ , and $y$ be the ground truth. Then, the total loss $L$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL=\\lambda\\|z-z^{\\prime}\\|_{2}^{2}+B C E(y,\\hat{y})+B C E(y,\\hat{y}^{\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B C E$ is the Binary Cross-Entropy loss function for labels in $\\{0,1\\}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nB C E(y,p)=-(y\\log(p)+(1-y)\\log(1-p))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Original image times mask: Since the embeddings of the original image and its binary segmentation mask may differ significantly, minimizing their $\\ell_{2}$ -distance can be challenging for the model. To address this, with a probability of 0.5, we multiply the original image by its segmentation mask, i.e., the network receives $x*x^{\\prime}$ as input half the time instead of $x$ . By multiplying with the segmentation mask, everything in the original image except the nuclei is set to 0. Figure 1 shows what the output looks like. By simplifying the task, the network can more easily reduce the distance between the embeddings of the nuclei-only image and the mask and gradually improve alignment between the embeddings of the original image and the mask. We found this augmentation to help stabilise training. ", "page_idx": 3}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "CAMELYON17 [51] dataset consists of 1000 H&E-stained Whole Slide Images (WSIs) of breast cancer metastases in lymph node sections from five medical centres in the Netherlands. It contains pixel-level annotations of tumours for 10 WSIs from each medical centre, giving us 50 WSIs to work with. WSIs from centres 0, 3 and 4 were scanned using the same scanner, while the other two centres used a different scanner each. All slides were scanned at $40\\times$ resolution. We treat each centre as a different domain. ", "page_idx": 3}, {"type": "text", "text": "BCSS [52] dataset consists of 151 H&E-stained WSIs of histologically-confirmed primary breast cancer cases from The Cancer Genome Atlas (TCGA) with triple-negative status determined from clinical data files. All WSIs have a resolution of $40\\times$ . The WSIs were annotated at the pixel level using crowdsourcing. Each pixel can have one of the many labels. We consider the label \"tumor\" to define pixels with a tumour and all other labels except \"outside_roi\", \"exclude\", or \"undetermined\" to define pixels without a tumour. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Ocelot [53] dataset consists of pixel-level annotations of tumour vs non-tumour pixels for 303 WSIs from TCGA. It consists of WSIs of primary tumour from six different organs: Bladder, Endometrium, Head-and-neck, Kidney, Prostate, and Stomach. The annotations in the dataset are at a low resolution, so we upscale the annotations to $40\\times$ . We exclude two WSIs that have only $20\\times$ resolution; all other WSIs have $40\\times$ resolution. ", "page_idx": 4}, {"type": "text", "text": "4.2 Dataset preparation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use code from WILDS [54, 55] to prepare the CAMELYON17 dataset with modifications. Tiles are sized $(270\\times270)$ at $40\\times$ resolution. For each domain (medical centre), data is split by patient, ensuring all tiles from a patient are in a single subset. Since the number of tiles varies drastically across patients, we shuffle patients so that the validation subset contains $20\\%{-}25\\%$ of all tiles per domain. Our processed version of CAMELYON17 is available at [56]. ", "page_idx": 4}, {"type": "text", "text": "4.3 Experiment setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We train models using the CAMELYON17 dataset, treating each medical centre as a distinct domain. We use the BCSS and Ocelot datasets as external test datasets. To avoid multiple comparisons and overly optimistic performance estimates, we use the external test datasets only once during the entire project, solely to evaluate the final models [57]. ", "page_idx": 4}, {"type": "text", "text": "For each combination of medical centre and method, we train ten models using the train subset of that centre. Thus, we train 50 models in total for each method. We use the loss on the validation dataset (of the training domain) to select the best model for each training. All models are trained for 50 epochs using the Adam optimiser with a learning rate $4\\mathrm{e}{-5}$ and a weight decay of $\\mathrm{1e-4}$ . We use exponential learning rate decay with a decay rate of 0.955. For our method, we set the parameter $\\lambda$ in equation (1) to $\\lambda=0$ for the first five epochs, effectively training without $\\ell_{2}$ -distance loss in these epochs, and then use $\\lambda=1$ for the rest of the training. We start saving models for selection of the one with lowest validation loss after ten epochs for our method to allow the network to stabilise while from the first epoch for other methods. For all experiments, unless stated otherwise, we use a ResNet-50 [49] model pre-trained on ImageNet [58]. We use HoVer-Net [59] trained on the CoNSeP [59] dataset to generate nuclear segmentation masks. ", "page_idx": 4}, {"type": "text", "text": "While domain generalisation encompasses a wide variety of methods, we have selected several exemplary baselines for comparison: Macenko normalisation [1], RSC [32], L2D [37], RandStainNA (RandSNA in result tables) [24], and DDCA [30]. We also include a baseline where we initialise ResNet-50 with pre-trained weights from HoVer-Net [59]. These methods represent different approaches, including stain normalisation (Macenko, RandStainNA) and generating augmented images (L2D). By selecting these diverse techniques, we ensure a comprehensive evaluation of our method\u2019s performance across various S-DG strategies. ", "page_idx": 4}, {"type": "text", "text": "It is important to note that our method can be integrated with many existing S-DG approaches, making it a flexible plug-in solution rather than a direct competitor. We evaluate most methods with and without the photometric augmentations selected for ERM. After testing various augmentation strategies available in Torchvision [50], we identified the most effective combination to be: ColorJitter(brightness $\\cdot=$ [0.5, 1.5], contrast=[0.5, 1.5], saturation=[0.5, 1.5], hue=[-0.3, 0.3]) and GaussianBlur(kernel_size $=\\!3$ ). Results using these augmentations are marked as \u2019-Aug\u2019 in the results tables. In all experiments, including those without photometric augmentations, we apply the basic geometric augmentations: random horizontal and vertical flips. For all ViT-Tiny [60] experiments, we also add affine augmentations: random rotation (up to $90^{\\circ}$ ) and translation (up to 45 pixels). ", "page_idx": 4}, {"type": "text", "text": "We ran the experiments on two clusters with GPUs with 64 GB (AMD MI250X) and 24 GB (Nvidia RTX 3090) GPU RAM each. Each job consumed about 21 to 31 GB of GPU RAM. The proposed method took 5 to 20 hours to train, depending on the train data size while ERM took 2.5 to 11 hours. ", "page_idx": 4}, {"type": "text", "text": "We report tile-level accuracy for tumour vs non-tumour tile classification for all datasets. Additionally, we measure robustness to image noise by measuring the accuracy drop on CAMELYON17 for image corruptions introduced in [61]. This includes Gaussian-, shot-, impulse- and snow-noise, and two blur types, elastic transform and JPEG compression. ", "page_idx": 4}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/50a5f3e6abf235b498f8ef588f0baf1408605f8c9d77c2bba5b07fe76d9361c3.jpg", "table_caption": ["Table 1: Out-of-domain accuracy on CAMELYON17. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. Method \"Ours-no- $\\ell_{2}$ -A\" is shorthand for \"Ours-no- $\\ell_{2}$ -Aug\" and refers to our approach without $\\ell_{2}$ - regularisation. Method \"Ours-MO-Aug\" refers to our approach with masks only, that is, neither using $\\ell_{2}$ -regularisation nor using mask-times-input augmentation of H&E images with $50\\%$ probability during training. A paired t-test for \"L2D-Aug\" versus \"Ours-Aug\" yields a p-value of $2\\cdot10^{-5}$ . "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Results on CAMELYON17 (lymph node sections) We test models on their respective out-ofdomain data. E.g., a model trained on Centre-3 is tested on all the data from Centre-0,1,2,4. Our method attains $10\\%$ higher accuracy than the next best method (L2D) when none used photometric augmentations and was also superior when photometric augmentations were used (Table 1). ", "page_idx": 5}, {"type": "text", "text": "Results on BCSS (primary breast cancer) The accuracy of the models trained on a centre in CAMELYON17 drops substantially $12\\%$ to $14\\%$ ) for all the methods when tested on BCSS (Table 2) compared to when tested on other centres in CAMELYON17 (Table 1). This could be due to a mismatch between pathologists\u2019 annotations on CAMELYON17 and BCSS but also due to the biological differences between these tissue types. In particular, epithelial cells in lymph nodes would almost certainly be tumour cells, while they could be benign cells in ordinary breast tissue. These results show that the relative performance on CAMELYON17 for different methods is indicative of relative performance on an external test set, as the performance drop is similar for all methods. ", "page_idx": 5}, {"type": "text", "text": "Results on Ocelot (primary non-breast cancer) We test our model on the Ocelot dataset to evaluate if our method helps to train models that generalise to other organs as well. Ocelot does not have any data from breast tissue nor does it include lymph node sections (which all the models have been trained on). We report the results of these experiments in Table 3. While our method achieves the highest accuracy also in this case, the difference between our method and L2D is not as big as it is for CAMELYON17 and BCSS. Taking a closer look into the performance for separate organs (Tables 4, 5, 6, 7, 8, and 9 in the Supplement), we can see that our method performs worse than L2D in Endometrium and Kidney, where accuracies are generally lower, and better in the four other organs. This indicates that models trained with our method generalise worse to organs where the transferability from breast tissue is generally low. This is at least the case for Kidney which has by far the lowest accuracies across all methods. Generalising to different cancer types is an emerging experimental topic; see, for example, [62]. ", "page_idx": 5}, {"type": "text", "text": "In summary, our method yields better accuracy than the baselines, including other S-DG approaches. ", "page_idx": 5}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/2b8bb99c76715ffff0250840009228f5e28718e06c0bf521fe121ff8ee5706f5.jpg", "table_caption": ["Table 2: Out-of-domain accuracy on BCSS. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. A paired t-test for \"L2D-Aug\" versus \"Ours-Aug\" yields a p-value of $4\\cdot10^{-5}$ . "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/d99e39689c48a5472b7a02f58f9a29a5f871db305cafa2f4cfe6dd4e9d262615.jpg", "table_caption": ["Table 3: Out-of-domain accuracy on Ocelot. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. A paired t-test for \"L2D-Aug\" versus \"Ours-Aug\" yields a p-value of 0.044. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Ablation Study and Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Impact of data augmentation Tables 1, 2, and 3 shows that data augmentation beneftis all methods substantially, which is consistent with well-established knowledge. ", "page_idx": 6}, {"type": "text", "text": "Impact of $\\ell_{2}$ -regularisation The result labelled Ours-no- $\\ell_{2}{\\it-}A$ in Table 1 shows that using data augmentation with nuclear masks alone is insufficient to achieve high accuracy. Without $\\ell_{2}$ -regularisation (i.e., setting $\\lambda=0$ in Equation (1)), our method only slightly outperforms most baselines that also uses data augmentation. The key factor for effective cross-domain generalisation is the ability to align the feature representation of input images with corresponding mask images, which lack colour and texture. Further evidence supporting this alignment effect is presented in the next paragraph. ", "page_idx": 6}, {"type": "text", "text": "Impact of mask-times-input-augmentation The result Ours-MO-Aug in Table 1 demonstrates an ablation with two changes: the absence of $\\ell_{2}$ -regulation and the removal of the $50\\%$ probability mask-times-input augmentation of H&E images during training (Figure 1), but still having two CE loss terms, one of them over nuclear masks. We see a further decline of accuracies below most baselines with photometric augmentations. ", "page_idx": 6}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/7df283ae5ea4df16d1b2fd20d69d1617fb94610288a6b4eea302eedfa7e136e1.jpg", "img_caption": ["Figure 2: Exemplary image ablations used in this study. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Impact of learned features without nuclear-mask-like features Here, we bring further evidence for the effect of $\\ell_{2}$ -regularisation about pulling the features towards the representation of nuclear masks. Note that nuclear masks are not used during inference in standard evaluations such as all those in Tables 1, 2, and 3. In Table 10 in the Supplement, we can see results for predictions in which the embeddings (features before the GAP layer) of the H&E images are modified by subtracting the embeddings of the corresponding nuclear masks. By comparing to Table 1, we see a drop in performance for all methods. However, the drop is largest for our method, with an accuracy below random guessing. This shows that the features computed from H&E-stained images at test time are indeed more similar to features from nuclear masks for our method than for other methods. ", "page_idx": 7}, {"type": "text", "text": "Impact of removal of intranuclear texture and colour In Tables 11 and 14 in the Supplement, we consider the performance on modified H&E images, in which intranuclear texture is removed by masking it out with a constant colour (see examples in Figure 2d and 2e). This is of interest due to the observation that intranuclear texture is often different in cancerous nuclei, which can be informative to humans. We can see that if it is replaced by a colour similar to the colour of nuclei, we for our method obtain a performance (Table 11) very similar to the performance with original H&E images (Table 1). On the other hand, changing the colour to white seems to reduce the performance notably (Table 14). This is possibly due to the creation of images with outlier statistics. A more likely explanation is that it is common for H&E stains to have small holes or gaps of white background colour in the stroma, which usually are not discriminative information but rather shear stress artefacts from the tissue cutting process. Therefore, masking nuclei with white masks may effectively remove discriminative information about nuclei. This domain-specific observation may explain the asymmetry in behaviour when masking nuclei with black versus white. ", "page_idx": 7}, {"type": "text", "text": "Impact of removal of extranuclear information Table 23 in the Supplement shows results on data where all the non-nuclear background is set to white (see example in Figure 2c). These images can be viewed as an inside-out inverted case of the images evaluated to give the results in Table 11. The common information in both sets of images is the morphology and organisation of nuclei. The performance for our proposed method remains high on these images (Table 23), being close to the best result on original H&E data (Table 1). The experiments in Tables 23 and 11 demonstrate the strong generalisability of focusing on nuclear morphology and organisation in out-of-domain settings. ", "page_idx": 7}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/db4cb3d7b538a6bde40a67e6462c8ccaf1c69f3d17057f9657cf9442da3cf13f.jpg", "img_caption": ["Figure 3: Robustness to added noise described in [61]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of dilution of nuclear shapes We expand nuclei masks by a classic morphological dilation and then blacken the dilated regions in the H&E images. The nuclear shape information in these images is thus progressively reduced compared to the images where only the nuclei are filled with black. Across all methods, we observe a drop in accuracy with an increase in dilation (Tables 11, 12, and 13 in the Supplement), highlighting the critical role of shape in this domain. The proposed method is more robust to moderate shape dilution with a mask size of 5 than the baseline methods. A similar but stronger trend appears in Tables 14, 15, and 16 in the Supplement for whitened nuclei. ", "page_idx": 8}, {"type": "text", "text": "Impact of removing nuclei We dilate the nuclear mask image with a kernel size of 5 to encapsulate remnants of the boundary of nuclei and then use the dilated mask to remove nuclei by inpainting [63]. The accuracy with the resulting images (see example in Figure 2g) drops to random guessing for our method (Tables 17 and 18 in the Supplement). Essentially no tiles are classified as tumour, giving a nearly zero recall and low precision (Tables 19, 20, 21, and 22 in the Supplement). Also, this supports that models trained using our method focus on nuclear morphology and organisation, and shows that the models reasonably associate the absence of nuclei with no tumour. ", "page_idx": 8}, {"type": "text", "text": "Saliency maps via Integrated Gradients To further demonstrate that our method steers models to focus on nuclei, we generate saliency (pixel attribution) maps using Integrated Gradients [64] and show some randomly selected examples in Figures 6,7 in the Supplement. The saliency maps also indicate that a model trained using our method focuses on nuclei. ", "page_idx": 8}, {"type": "text", "text": "Evaluation of L2D and RSC combined with the proposed method Tables 26, 27, and 28 in the Supplement show the results of combining L2D and RSC with the proposed method. Combining the proposed method, which regularises, with L2D, which diversifies, yields mixed results, likely due to the opposing effects of these two interventions. Combining it with RSC results in a small gain over using our method alone. Overall, this demonstrates the effectiveness of the method proposed. ", "page_idx": 8}, {"type": "text", "text": "Evaluation on segmentation mask data For the sake of completeness, we show in Tables 24 and 25 in the Supplement that our method also performs well when tested on nuclear masks (as exemplified in Figure 2b) and their inversions (see example in Figure 2f). ", "page_idx": 8}, {"type": "text", "text": "Evaluation of robustness to image corruptions Figure 3 shows that the proposed method has notably higher robustness to image corruptions for most experiments in eight types of corruptions described in [61]. Samples of corrupted images are shown in Figure 5 in the Supplement. ", "page_idx": 8}, {"type": "text", "text": "Evaluation of robustness against adversarial attacks We evaluated the robustness of models against adversarial attacks [65] using the Projected Gradient Descent (PGD) attack [66]. Figure 4a demonstrates that models trained using our method have significantly higher robustness than ERM and L2D, the latter being the second-best performing method in Tables 1, 2, and 3. Additionally, we conduct cross-model attacks by generating adversarial images using models trained with one method and evaluating them on models trained with other methods. The results indicate that our models exhibit minimal performance degradation when exposed to adversarial images generated by models from other methods (Figure 4b). In contrast, the accuracy of models trained with ERM and L2D drops substantially. This further demonstrates the superior robustness of models from our method. ", "page_idx": 8}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/946c8ba67f3ac28834b7c4452dced3ce4201574f3d0646a99e0d819edaa3d61e.jpg", "img_caption": ["Figure 4: (a) PGD attack on models. (b) Cross-model PGD attacks where adversarial images are generated using a model from a method but the accuracy for those images is tested on models from other methods. Results are for the validation subset of each centre in CAMELYON17. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "A preliminary evaluation on a transformer architecture We perform a comparison using a finetuned ViT-Tiny [60] model. The results are shown in Tables 29, 30 and 31 in the Supplement. These results show that our approach obtains superior out-of-domain performance for the CAMELYON17 dataset. The results are more mixed for the other datasets. In particular, it seems that models trained on one of the five centres (Center-4) in CAMELYON17 do not generalise well to other cancer types and are actually also performing sub-optimally in CAMELYON17. For models trained on each of the other four centres in CAMELYON17, the performance with our approach is, on average, better than with other approaches, but the performance increase is lower than for ResNet-50. However, in the same tissue type (CAMELYON17 data), the performance gain is similar for both ViT-Tiny and ResNet-50. Our interpretation of all these results is that our approach can improve out-of-domain performance also for ViT-Tiny, in particular across centres and scanners for the same tissue type, but that it might also fail for a minority of the training datasets. This experiment is preliminary because we took the same hyperparameters as used for ResNet-50, including the same learning rate and $\\lambda=1$ , both of which might not be optimal. Also, we note that ViT-Tiny has much fewer parameters than ResNet-50. Experiments with larger transformers might obtain bigger differences, as seen in [67]. ", "page_idx": 9}, {"type": "text", "text": "Limitations of this study As a limitation, we identify that we have performed these experiments for only one classification task. For medical practitioners, it would be of interest to measure the impact for other tasks, such as tumour grading and survival prediction, when evaluated in an out-of-domain generalisation setup. However, this would require access to multi-centre datasets with relevant labels available. Secondly, we ran the full set of experiments only on one base network, ResNet-50, because we preferred to run a larger set of ablation experiments to understand what actually has been learned when using our method. While we expect results to be qualitatively similar for other CNNs, transformer networks might have different learning dynamics, and results for those with a larger capacity than the ViT-Tiny are of interest in future work. Finally, an extension to other cancer types, such as prostate or colon cancer, would also be of interest. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown a simple method to enforce the learning of shape features at training time, which uses unmodified input images at inference time. It shows very good out-of-domain performance and can be combined as a plugin with other methods to enhance out-of-domain generalisation. Aside from out-of-domain accuracy, the proposed method gives improved robustness to image alterations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The computations were performed on resources provided by Sigma2\u2014the National Infrastructure for High-Performance Computing and Data Storage in Norway\u2014through Project NN8104K. Additionally, we acknowledge Sigma2 for access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through Sigma2, Norway, Project 465000262. This work was supported by the authors\u2019 institutions and a grant from the Research Council of Norway (project number 309439). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marc Macenko, Marc Niethammer, James S Marron, David Borland, John T Woosley, Xiaojun Guan, Charles Schmitt, and Nancy E Thomas. A method for normalizing histology slides for quantitative analysis. In 2009 IEEE international symposium on biomedical imaging: from nano to macro, pages 1107\u20131110. IEEE, 2009. [2] David Tellez, Geert Litjens, P\u00e9ter B\u00e1ndi, Wouter Bulten, John-Melle Bokhorst, Francesco Ciompi, and Jeroen Van Der Laak. Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. Medical image analysis, 58:101544, 2019. [3] Mostafa Jahanifar, Manahil Raza, Kesi Xu, Trinh Vuong, Rob Jewsbury, Adam Shephard, Neda Zamanitajeddin, Jin Tae Kwak, Shan E Ahmed Raza, Fayyaz Minhas, et al. Domain generalization in computational pathology: survey and guidelines. arXiv preprint arXiv:2310.19656, 2023. [4] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2018. [5] Nicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman. Deep convolutional networks do not classify based on global object shape. PLoS computational biology, 14(12): e1006613, 2018.   \n[6] Christopher W Elston and Ian O Ellis. Pathological prognostic factors in breast cancer. i. the value of histological grade in breast cancer: experience from a large study with long-term follow-up. Histopathology, 19(5):403\u2013410, 1991. [7] Daniele Zink, Andrew H Fischer, and Jeffrey A Nickerson. Nuclear structure in cancer cells. Nature reviews cancer, 4(9):677\u2013687, 2004.   \n[8] Edgar G Fischer. Nuclear morphology and the biology of cancer cells. Acta cytologica, 64(6): 511\u2013519, 2020. [9] William H Wolberg, W Nick Street, and Olvi L Mangasarian. Importance of nuclear morphology in breast cancer prognosis. Clinical Cancer Research, 5(11):3542\u20133548, 1999.   \n[10] Shivang Naik, Scott Doyle, Shannon Agner, Anant Madabhushi, Michael Feldman, and John Tomaszewski. Automated gland and nuclei segmentation for grading of prostate and breast cancer histopathology. In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, pages 284\u2013287, 2008. doi: 10.1109/ISBI.2008.4540988.   \n[11] Jean-Romain Dalle, Wee Kheng Leow, Daniel Racoceanu, Adina Eunice Tutac, and Thomas C Putti. Automatic breast cancer grading of histopathological images. In 2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pages 3052\u20133055. IEEE, 2008.   \n[12] Laura E. Boucheron, B. S. Manjunath, and Neal R. Harvey. Use of imperfectly segmented nuclei in the classification of histopathology images of breast cancer. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 666\u2013669, 2010. doi: 10.1109/ ICASSP.2010.5495124.   \n[13] Mitko Veta, Robert Kornegoor, Andr\u00e9 Huisman, Anoek H J Verschuur-Maes, Max A Viergever, Josien P W Pluim, and Paul J van Diest. Prognostic value of automatically extracted nuclear morphometric features in whole slide images of male breast cancer. Modern Pathology, 25(12): 1559\u20131565, 2012. ISSN 0893-3952. doi: https://doi.org/10.1038/modpathol.2012.126. URL https://www.sciencedirect.com/science/article/pii/S089339522202943X.   \n[14] Pawe\u0142 Filipczuk, Marek Kowal, and Andrzej Obuchowicz. Automatic breast cancer diagnosis based on k-means clustering and adaptive thresholding hybrid segmentation. In Image processing and communications challenges 3, pages 295\u2013302. Springer, 2011.   \n[15] Hela Masmoudi, Stephen M. Hewitt, Nicholas Petrick, Kyle J. Myers, and Marios A. Gavrielides. Automated quantitative assessment of her-2/neu immunohistochemical expression in breast cancer. IEEE Transactions on Medical Imaging, 28(6):916\u2013925, 2009. doi: 10.1109/TMI.2009. 2012901.   \n[16] L Sing Cheong, Angela Jean, Tsu Soo Tan, Waiming Kong, and Soo Yong Tan. Automated segmentation and measurement for cancer classification of her2/neu status in breast carcinomas. In Biotechno 2011: The Third International Conference on Bioinformatics. Citeseer, 2011.   \n[17] Serge Beucher and Christian Lantu\u00e9joul. Use of watersheds in contour detection. workshop published, September 1979. URL http://cmm.ensmp.fr/\\~beucher/publi/watershed. pdf.   \n[18] Stanley J. Osher and James A. Sethian. Fronts propagating with curvature-dependent speed: algorithms based on hamilton-jacobi formulations. Journal of Computational Physics, 79: 12\u201349, 1988. URL https://api.semanticscholar.org/CorpusID:205007680.   \n[19] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active contour models. International Journal of Computer Vision, 1(4):321\u2013331, Jan 1988. ISSN 1573-1405. doi: 10.1007/BF00133570. URL https://doi.org/10.1007/BF00133570.   \n[20] Akira Hasegawa, Kevin J. Cullen M.D., and Seong Ki Mun. Segmentation and analysis of breast cancer pathological images by an adaptive-sized hybrid neural network. In Murray H. Loew and Kenneth M. Hanson, editors, Medical Imaging 1996: Image Processing, volume 2710, pages 752 \u2013 762. International Society for Optics and Photonics, SPIE, 1996. doi: 10.1117/12.237980. URL https://doi.org/10.1117/12.237980.   \n[21] Kyoung-Mi Lee and W.N. Street. An adaptive resource-allocating network for automated detection, segmentation, and classification of breast cancer nuclei topic area: image processing and recognition. IEEE Transactions on Neural Networks, 14(3):680\u2013687, 2003. doi: 10.1109/ TNN.2003.810615.   \n[22] Abhishek Vahadane, Tingying Peng, Amit Sethi, Shadi Albarqouni, Lichao Wang, Maximilian Baust, Katja Steiger, Anna Melissa Schlitter, Irene Esposito, and Nassir Navab. Structurepreserving color normalization and sparse stain separation for histological images. IEEE transactions on medical imaging, 35(8):1962\u20131971, 2016.   \n[23] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer graphics and applications, 21(5):34\u201341, 2001.   \n[24] Yiqing Shen, Yulin Luo, Dinggang Shen, and Jing Ke. Randstainna: Learning stain-agnostic features from histology slides by bridging stain augmentation and normalization. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 212\u2013221. Springer, 2022.   \n[25] Md Ziaul Hoque, Anja Keskinarkaus, Pia Nyberg, and Tapio Sepp\u00e4nen. Stain normalization methods for histopathology image analysis: A comprehensive review and experimental comparison. Information Fusion, page 101997, 2023.   \n[26] Khrystyna Faryna, Jeroen Van der Laak, and Geert Litjens. Tailoring automated data augmentation to h&e-stained histopathology. In Medical imaging with deep learning, 2021.   \n[27] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702\u2013703, 2020.   \n[28] David Tellez, Maschenka Balkenhol, Nico Karssemeijer, Geert Litjens, Jeroen van der Laak, and Francesco Ciompi. H and e stain augmentation improves generalization of convolutional networks for histopathological mitosis detection. In Medical Imaging 2018: Digital Pathology, volume 10581, pages 264\u2013270. SPIE, 2018.   \n[29] Joona Pohjonen, Carolin St\u00fcrenberg, Atte F\u00f6hr, Reija Randen-Brady, Lassi Luomala, Jouni Lohi, Esa Pitk\u00e4nen, Antti Rannikko, and Tuomas Mirtti. Augment like there\u2019s no tomorrow: Consistently performing neural networks for medical imaging. arXiv preprint arXiv:2206.15274, 2022.   \n[30] Niccol\u00f2 Marini, Sebastian Otalora, Marek Wodzinski, Selene Tomassini, Aldo Franco Dragoni, Stephane Marchand-Maillet, Juan Pedro Dominguez Morales, Lourdes Duran-Lopez, Simona Vatrano, Henning M\u00fcller, et al. Data-driven color augmentation for h&e stained images in computational pathology. Journal of Pathology Informatics, 14:100183, 2023.   \n[31] Khrystyna Faryna, Jeroen van der Laak, and Geert Litjens. Automatic data augmentation to improve generalization of deep learning in h&e stained histopathology. Computers in Biology and Medicine, 170:108018, 2024.   \n[32] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves crossdomain generalization. In Computer vision\u2013ECCV 2020: 16th European conference, Glasgow, UK, August 23\u201328, 2020, proceedings, part II 16, pages 124\u2013140. Springer, 2020.   \n[33] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. Advances in neural information processing systems, 31, 2018.   \n[34] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12556\u201312565, 2020.   \n[35] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. In International Conference on Learning Representations, 2018.   \n[36] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Progressive domain expansion network for single domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 224\u2013233, 2021.   \n[37] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa Baktashmotlagh. Learning to diversify for single domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 834\u2013843, 2021.   \n[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[39] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017.   \n[40] M Tarek Shaban, Christoph Baur, Nassir Navab, and Shadi Albarqouni. Staingan: Stain style transfer for digital histological images. In 2019 Ieee 16th international symposium on biomedical imaging (Isbi 2019), pages 953\u2013956. IEEE, 2019.   \n[41] Thomas de Bel, John-Melle Bokhorst, Jeroen van der Laak, and Geert Litjens. Residual cyclegan for robust domain transformation of histopathological tissue slides. Medical Image Analysis, 70:102004, 2021.   \n[42] Niyun Zhou, De Cai, Xiao Han, and Jianhua Yao. Enhanced cycle-consistent generative adversarial network for color normalization of h&e stained images. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317, 2019, Proceedings, Part I 22, pages 694\u2013702. Springer, 2019.   \n[43] Harshal Nishar, Nikhil Chavanke, and Nitin Singhal. Histopathological stain transfer using style transfer network with adversarial loss. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings, Part V 23, pages 330\u2013340. Springer, 2020.   \n[44] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2414\u20132423, 2016.   \n[45] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694\u2013711. Springer, 2016.   \n[46] Edwin Yuan and Junkyo Suh. Neural stain normalization and unsupervised classification of cell nuclei in histopathological breast cancer images. arXiv preprint arXiv:1811.03815, 2018.   \n[47] Hyungjoo Cho, Sungbin Lim, Gunho Choi, and Hyunseok Min. Neural stain-style transfer learning using gan for histopathological images. arXiv preprint arXiv:1710.08543, 2017.   \n[48] Karin Stacke, Gabriel Eilertsen, Jonas Unger, and Claes Lundstr\u00f6m. Measuring domain shift for deep learning in histopathology. IEEE journal of biomedical and health informatics, 25(2): 325\u2013336, 2020.   \n[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.   \n[50] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https://github.com/pytorch/vision, 2016.   \n[51] Geert Litjens, Peter Bandi, Babak Ehteshami Bejnordi, Oscar Geessink, Maschenka Balkenhol, Peter Bult, Altuna Halilovic, Meyke Hermsen, Rob Van de Loo, Rob Vogels, et al. 1399 h&e-stained sentinel lymph node sections of breast cancer patients: the camelyon dataset. GigaScience, 7(6):giy065, 2018.   \n[52] Mohamed Amgad, Habiba Elfandy, Hagar Hussein, Lamees A Atteya, Mai AT Elsebaie, Lamia S Abo Elnasr, Rokia A Sakr, Hazem SE Salem, Ahmed F Ismail, Anas M Saad, et al. Structured crowdsourcing enables convolutional segmentation of histology images. Bioinformatics, 35(18): 3461\u20133467, 2019.   \n[53] Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Jinhee Lee, Wonkyung Jung, Soo Ick Cho, Kyunghyun Paeng, Chan-Young Ock, et al. Ocelot: Overlapped cell on tissue dataset for histopathology. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23902\u201323912, 2023.   \n[54] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.   \n[55] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised adaptation. In International Conference on Learning Representations (ICLR), 2022.   \n[56] Dhananjay Tomar. Replication Data for: Are nuclear masks all you need for improved out-ofdomain generalization? A closer look at cancer classification in histopathology, 2024. URL https://doi.org/10.18710/NXPLFL.   \n[57] Andreas Kleppe, Ole-Johan Skrede, Sepp De Raedt, Knut Liest\u00f8l, David J Kerr, and H\u00e5vard E Danielsen. Designing deep learning studies in cancer diagnostics. Nature Reviews Cancer, 21 (3):199\u2013211, 2021.   \n[58] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[59] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir Rajpoot. HoVer-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. Medical image analysis, 58:101563, 2019.   \n[60] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot$ YicbFdNTTy.   \n[61] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations (ICLR), 2019.   \n[62] Eugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Kristen Severson, Eric Zimmermann, James Hall, Neil Tenenholtz, Nicolo Fusi, Philippe Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, Eric Robert, Yi Kan Wang, Jeremy D. Kunz, Matthew C. H. Lee, Jan Bernhard, Ran A. Godrich, Gerard Oakley, Ewan Millar, Matthew Hanna, Juan Retamero, William A. Moye, Razik Yousf,i Christopher Kanan, David Klimstra, Brandon Rothrock, and Thomas J. Fuchs. Virchow: A million-slide digital pathology foundation model, 2024.   \n[63] Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics tools, 9(1):23\u201334, 2004.   \n[64] Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. In International conference on machine learning, pages 9269\u20139278. PMLR, 2020.   \n[65] C Szegedy. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.   \n[66] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[67] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1204\u20131213. IEEE, 2022. doi: 10.1109/CVPR52688. 2022.01179. URL https://doi.org/10.1109/CVPR52688.2022.01179. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Technical Appendix / Supplement ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/10ae0f4edb568d597bd5b46549ff215429a98cb7ba9f2dbdd960a9339120f293.jpg", "table_caption": ["Table 4: Out-of-domain accuracy on Ocelot evaluated only on the organ BLADDER. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/3e6dad9706e93ef5428ac677e0c766352b1185a74ef03535884fc264d92195c7.jpg", "table_caption": ["Table 5: Out-of-domain accuracy on Ocelot evaluated only on the organ ENDOMETRIUM. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/f0e97496248ad51b324dcd3aa639d3b977ca3e1b8dea21d88c7280307211da6f.jpg", "table_caption": ["Table 6: Out-of-domain accuracy on Ocelot evaluated only on the organ HEAD AND NECK. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/3654511edeb37fca706633a9c969155636cfe77ec9f86ce13a1a690bdab916d0.jpg", "table_caption": ["Table 7: Out-of-domain accuracy on Ocelot evaluated only on the organ KIDNEY. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/6b451cafd1e10dd88047010ddbb3d8b53163288b31027f77c0b5025b5ef44725.jpg", "table_caption": ["Table 8: Out-of-domain accuracy on Ocelot evaluated only on the organ PROSTATE. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/75ede23a7100b74295d68f07868bc171fc0ce54285f6ef3a17ba9a30ffe3c51a.jpg", "table_caption": ["Table 9: Out-of-domain accuracy on Ocelot evaluated only on the organ STOMACH. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/16680cafee3711dd8445439dcb1b66af46194489729ff4ce57ffa6bce3fbe87f.jpg", "table_caption": ["Table 10: Out-of-domain accuracy on CAMELYON17 where embeddings for segmentation masks were subtracted from the embeddings of the original image to see if the accuracy drops. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/87a68286b2abdf5395b805558ca659a361514173558eaefcf09b567df56ad015.jpg", "table_caption": ["Table 11: Out-of-domain accuracy on CAMELYON17 where nuclei are blackened out. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/a2a8752f21a2e39d6ad499cd34f580a909c8fcc7cee88da426bbabfdccab25a4.jpg", "table_caption": ["Table 12: Out-of-domain accuracy on CAMELYON17 where nuclei are blackened out after being expanded with fliter size 5, i.e., the blackened out part covers nuclei and some region around nuclei. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/6e2165dd19b6a5061525caeaca53af2d9369f706ef52dfa3a7b65f48150bb96c.jpg", "table_caption": ["Table 13: Out-of-domain accuracy on CAMELYON17 where nuclei are blackened out after being expanded with fliter size 9, i.e., the blackened out part covers nuclei and some region around nuclei. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/8b82be0b4a68714d385a4f63b3d8a242c1f8621369c4b317c8bc264aacfcb896.jpg", "table_caption": ["Table 14: Out-of-domain accuracy on CAMELYON17 where nuclei are whitened out. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/85245f195e3cf0bc068d3f54a633089df3bb153ae183da2c1d8c38169c7088a7.jpg", "table_caption": ["Table 15: Out-of-domain accuracy on CAMELYON17 where nuclei are whitened out after being expanded with filter size 5, i.e., the whitened out part covers nuclei and some region around nuclei. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/2ba10e3a8e6970b661bfe77dc642c8e08bdbbb9f8c6bc8333fe353cf7d9d0635.jpg", "table_caption": ["Table 16: Out-of-domain accuracy on CAMELYON17 where nuclei are whitened out after being expanded with filter size 9, i.e., the whitened out part covers nuclei and some region around nuclei. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/281b43374f9a81cbe306c620a2b7cf0f14b7c018575a04a3aab0d740604b0e6c.jpg", "table_caption": ["Table 17: Out-of-domain accuracy on where nuclei are inpainted after being expanded with fliter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/df648b28ac03a37abcb35532e95ff83fc2db61f14b0ee98cbc0f124b158a4db9.jpg", "table_caption": ["Table 18: In domain accuracy on CAMELYON17 where nuclei are inpainted after being expanded with filter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/7deb67eb43469ab5b05c5114e57d191d54711cc41511c8ca071535a8595d6f01.jpg", "table_caption": ["Table 19: OUT-OF-DOMAIN recall on CAMELYON17 where nuclei are inpainted after being expanded with filter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/6fe74268db2985b815a7c8de6962600ae7531f8d37278d54b3711f215f99ba1b.jpg", "table_caption": ["Table 20: IN DOMAIN recall on CAMELYON17 where nuclei are inpainted after being expanded with filter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/4071def9b43d93c12b568760b7832a902753c9e3d80fee3c432b824b19bb8828.jpg", "table_caption": ["Table 21: OUT-OF-DOMAIN precision on CAMELYON17 where nuclei are inpainted after being expanded with filter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/74e05f3992a9e94f7027f17a81196f02b4aa3787fa5ecb107607253dfb94f10f.jpg", "table_caption": ["Table 22: IN DOMAIN precision on CAMELYON17 where nuclei are inpainted after being expanded with filter size 5. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/e780cc259f316ff99d810d081fd7f07eb6433e33c28d212423de7eab7a27f44f.jpg", "table_caption": ["Table 23: Out-of-domain accuracy on CAMELYON17 with nuclei on white background. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/16161c6e56e420d3e0c3e24cde531f52f4e3e31a52b95385867bb24c8b8c409d.jpg", "table_caption": ["Table 24: Out-of-domain accuracy on CAMELYON17 evaluated on binary nuclei segmentation masks. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/92c2528f11c4080353145d840b5e4097c167882bfa642287de02f35ba8c98663.jpg", "table_caption": ["Table 25: Out-of-domain accuracy on CAMELYON17 evaluated on inverted nuclei segmentation masks. The column name indicates the centre used to train models. The best accuracy for each column is in bold face and the second best in italics. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 26: Accuracy when combining L2D or RSC with the proposed method on CAMELYON17. The best accuracy for each column is in bold face and the second best in italics. \"+Ours\" results are an average of five models for each combination of medical centre and method instead of an average of ten models. All results are using photometric augmentations described in 4.3 even though \"-Aug\" is omitted from the name in the table. ", "page_idx": 22}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/5581664723c8be795b2c4870882f896103773cdd9599b9481f9a44a325843f73.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/9e7f3685904a6ae3d82714d2f6298f145d313800be5b2cc6325e57dcbc188291.jpg", "table_caption": ["Table 27: Accuracy when combining L2D or RSC with the proposed method on BCSS. The best accuracy for each column is in bold face and the second best in italics. $\"+\\mathrm{Ours}\"$ results are an average of five models for each combination of medical centre and method instead of an average of ten models. All results are using photometric augmentations described in 4.3 even though \"-Aug\" is omitted from the name in the table. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 28: Accuracy when combining L2D or RSC with the proposed method on Ocelot. The best accuracy for each column is in bold face and the second best in italics. $\"+\\mathrm{Ours}\"$ results are an average of five models for each combination of medical centre and method instead of an average of ten models. All results are using photometric augmentations described in 4.3 even though \"-Aug\" is omitted from the name in the table. ", "page_idx": 22}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/cae9855dd99c8010680b23509199800d2256991895569e27fc5e3b45a5c6a2ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/8882e1b875f1c140a8e37235b2f2624c0bc99cf61c3faf3d7520e6a5b1e95421.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/7e2245fad54665c125bc2a4299ed6146e30b6127778987c7bbb455e36d815c99.jpg", "table_caption": ["Table 30: Accuracy when using a ViT-Tiny [60], comparing the baseline against the proposed method on BCSS. The best accuracy for each column is in bold face and the second best in italics. All results are using photometric augmentations described in 4.3 even though \"-Aug\" is omitted from the name in the table. \u2018AwoC4\u2019 is \u2018Average without Centre- $.4\\rangle$ . \u2018RSNA\u2019 refers to RandStainNA [24]. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "BmwcbNYkuH/tmp/ac22d801d09abd434c06d1bf32c69480bed00b8fb47ac7f446973c0841dcfa3a.jpg", "table_caption": ["Table 31: Accuracy when using a ViT-Tiny [60], comparing the baseline against the proposed method on Ocelot. The best accuracy for each column is in bold face and the second best in italics. All results are using photometric augmentations described in 4.3 even though \"-Aug\" is omitted from the name in the table. \u2018AwoC4\u2019 is \u2018Average without Centre- $.4\\rangle$ . \u2018RSNA\u2019 refers to RandStainNA [24]. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/d2530988ac57fc80449d83a2c541d7f3be56f875a0c10bc5bb45bc78b825959c.jpg", "img_caption": ["Figure 5: Exemplary image corruptions from [61] applied to an input image used in this study. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/d8da4cc446128705ee4203b7c74b453929a5e2a375f35fd987815760d5dbf148.jpg", "img_caption": ["Figure 6: Saliency maps for four randomly selected tiles using Integrated Gradients [64] for a model trained via ERM-Aug. The green-coloured map indicates the contribution towards the positive class (tumour), and the red one towards the negative class (non-tumour). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "BmwcbNYkuH/tmp/bb8473941ad8453f8eef1a2909a51aca0f2c2e33461bdaaa56b6d5d26159e067.jpg", "img_caption": ["Figure 7: Saliency maps for four randomly selected tiles using Integrated Gradients [64] for a model trained via Ours-Aug. The green-coloured map indicates the contribution towards the positive class (tumour), and the red one towards the negative class (non-tumour). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The claim made in the abstract and introduction are supported by the results in the main Tables 1, 2, and 3, and for robustness in Figures 3 and 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalise to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: see the end of the Section 5 Ablation Study and Discussion for limitations Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The method is described in Section Proposed method 3. The experiments are described in Section Experiments 4, and, there in particular in Subsection 4.3. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The datasets are created and available via their owners. Code is available, shareable and presentable to external parties. It is available at https://github.com/ undercutspiky/SFL/ ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The experiments are described in Section Experiments 4, and, in particular, in Subsection 4.3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We reported variances of the performance measures over the runs. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We disclosed the GPU types, the GPU RAM and training times, see Section 4.3. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have checked against the Neurips ethics code. No privacy, security, safety impact expected. \"Disclosure of essential elements for reproducibility\" has been done in the paper, and we consider it to be sufficient for reproduciblity ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No societal impact of the work expected ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No direct malicious use is expected from a method to improve out-of-domain generalisation. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The datasets, the model and the deep learning toolbox are all cited. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We use established datasets [51\u201353] containing patient data, which were collected by third parties mentioned in the cited publications and which are already published. Applied crowdsourcing is described in those publications. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We use established datasets [51\u201353] containing patient data, which were collected by third parties mentioned in the cited publications and which are already published. As a consequence, IRB approval was not required for our study. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]