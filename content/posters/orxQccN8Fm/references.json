{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for Reward Learning from Human Feedback (RLHF), a key method in aligning LLMs, and is directly referenced by the current paper as a state-of-the-art technique."}, {"fullname_first_author": "Zixiang Chen", "paper_title": "Self-play fine-tuning converts weak language models to strong language models", "publication_date": "2024-01-01", "reason": "This paper introduces Self-Play Fine-tune (SPIN), a method closely related to the proposed algorithm in the current paper, providing a comparative analysis."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-12-01", "reason": "This paper presents Direct Preference Optimization (DPO), another prominent LLM alignment method that is compared and contrasted with the current work's approach."}, {"fullname_first_author": "Brian D Ziebart", "paper_title": "Maximum entropy inverse reinforcement learning", "publication_date": "2008-01-01", "reason": "This paper introduces Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL), a core theoretical foundation underlying the reward learning approach used in the current paper."}, {"fullname_first_author": "St\u00e9phane Ross", "paper_title": "A reduction of imitation learning and structured prediction to no-regret online learning", "publication_date": "2011-01-01", "reason": "This paper provides theoretical grounding for imitation learning within the RL framework, which the current paper leverages and builds upon for LLM alignment."}]}