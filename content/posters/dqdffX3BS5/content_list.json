[{"type": "text", "text": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dong $\\mathbf{L}\\mathbf{i}^{2,3}$ , Aijia Zhang4, Junqi $\\mathbf{Gao^{4}}$ , Biqing $\\mathbf{Q}\\mathbf{i}^{1,2*}$ ", "page_idx": 0}, {"type": "text", "text": "1Department of Electronic Engineering, Tsinghua University, 2 Shanghai Artificial Intelligence Laboratory, 3 Institute for Advanced Study in Mathematics, Harbin Institute of Technology, 4 School of Mathematics, Harbin Institute of Technology {arvinlee826, zhangaijia065, gjunqi97, qibiqing7}@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Incremental graph learning has gained significant attention for its ability to address the catastrophic forgetting problem in graph representation learning. However, traditional methods often rely on a large number of labels for node classification, which is impractical in real-world applications. This makes few-shot incremental learning on graphs a pressing need. Current methods typically require extensive training samples from meta-learning to build memory and perform intensive finetuning of GNN parameters, leading to high memory consumption and potential loss of previously learned knowledge. To tackle these challenges, we introduce Mecoin, an efficient method for building and maintaining memory. Mecoin employs Structured Memory Units to cache prototypes of learned categories, as well as Memory Construction Modules to update these prototypes for new categories through interactions between the nodes and the cached prototypes. Additionally, we have designed a Memory Representation Adaptation Module to store probabilities associated with each class prototype, reducing the need for parameter fine-tuning and lowering the forgetting rate. When a sample matches its corresponding class prototype, the relevant probabilities are retrieved from the MRaM. Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model\u2019s memory. We analyze the effectiveness of Mecoin in terms of generalization error and explore the impact of different distillation strategies on model performance through experiments and VC-dimension analysis. Compared to other related works, Mecoin shows superior performance in accuracy and forgetting rate. Our code is publicly available on the MecoinGFSCIL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the field of graph learning, conventional methods often assume that graphs are static[1]. However, in the real world, graphs tend to grow over time, with new nodes and edges gradually emerging. For example, in citation networks, new papers are published and cited; in e-commerce, new products are introduced and updated; and in social networks, new social groups form as users join. In these dynamic contexts, simply updating graph representation learning methods with new data often leads to catastrophic forgetting of previously acquired knowledge. ", "page_idx": 0}, {"type": "text", "text": "Despite numerous methods proposed to mitigate the catastrophic forgetting problem in Graph Neural Networks(GNNs)[2, 3, 4], a critical and frequently neglected challenge is the scarcity of labels for newly introduced nodes. Most current graph incremental learning methods [5, 6] combat catastrophic forgetting by retaining a substantial number of nodes from previous graphs to preserve prior knowledge. However, these methods become impractical in graph few-shot class-incremental learning(GFSCIL) scenarios due to the limited labeled node samples. Some methods[7, 1] employ regularization to maintain the stability of parameters critical to the graph\u2019s topology. Yet, in GFSCIL, the label scarcity complicates accurate assessment of the relationship between parameter importance and the underlying graph structure, thus increasing the difficulty of designing effective regularization strategies. Consequently, the issue of label scarcity hampers the ability of existing graph continual learning methods to effectively generalize in graph few-shot learning scenarios. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "GFSCIL presents two critical challenges: 1)How can we empower models to learn effectively from limited samples? 2)How can we efficiently retain prior knowledge with imited samples? While the first challenge has been extensively explored in graph few-shot learning contexts[8, 9], this paper focuses on the second. Currently, discussions on the latter issue within GFSCIL are relatively scarce. Existing methods[10, 11] primarily focus on enhancing models\u2019 ability to learn from few-shot graphs and preserve prior knowledge by learning class prototypes through meta-learning and attention mechanisms. However, to strengthen inductive bias towards graph structures and learn more representative class prototypes, these approaches require caching numerous training samples from the meta-learning process for subsequent GFSCIL tasks. This caching not only consumes significant memory but also imposes substantial computational costs. Furthermore, these methods extensively adjust parameters during prototype updates, risking the loss of previously acquired knowledge[12, 13]. These challenges underscore the need for innovative strategies that maintain efficiency without compromising the retention of valuable historical data\u2014achieved through minimal memory footprint, high computational efficiency, and limited parameter adjustments. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned challenges, we introduce Mecoin, an efficient memory construction and interaction module. Mecoin consists of two core components: the Structured Memory Unit(SMU) for learning and storing class prototypes, and the Memory Representation Adaptive Module(MRaM) for dynamic memory interactions with the GNN. To effectively leverage graph structural information, we design the Memory Construction module(MeCs) within the SMU. MeCs facilitates interaction between features of the input node and prototype representations stored in the SMU through self-attention mechanisms, thereby updating sample representations. Then, it utilizes the local structural information of input nodes to extract local graph structural information and compute a local graph structure information matrix(GraphInfo). The updated sample representations and GraphInfo are used to calculate class prototypes for the input nodes. These newly obtained prototypes are compared with those stored in the SMU using Euclidean distance to determine whether the input samples belong to seen classes. If a sample belongs to a seen class, the corresponding prototype in the SMU remains unchanged. Conversely, if a sample belongs to an unseen class, its calculated prototype is added to the SMU. ", "page_idx": 1}, {"type": "text", "text": "To address catastrophic forgetting in class prototype learning caused by model parameter updates, we introduce the MRaM mechanism within Mecoin. MRaM stores probability distributions for each class prototype, allowing direct access via indexing once input nodes are processed through MeCs and corresponding class prototypes are retrieved from SMU. This mechanism separates class prototype learning from node probability distribution learning, effectively mitigating forgetting issues caused by parameter updates. Additionally, to enhance the maintenance and updating of knowledge base, we integrate a Graph Knowledge Interaction Module (GKIM) within MRaM. GKIM transfers information about identified classes from MRaM to GNN and extracts knowledge of new classes from GNN back to MRaM, facilitating continuous knowledge updating and maintenance. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The main contributions of this paper are as follows: i) We design Mecoin, a novel framework that effectively mitigates catastrophic forgetting in GFSCIL by integrating the SMU and MRaM; ii) We design the SMU, which efficiently learns class prototypes by facilitating interaction between node features and existing class prototypes, while extracting local graph structures of input nodes; iii) We propose the MRaM, which reduces the loss of prior knowledge during parameter fine-tuning by decoupling the learning of class prototypes from node probability distributions; iv) We analyze the benefits of separating class prototype learning from node probability distribution learning, considering generalization error bounds and VC dimensions. We also explore how different MRaM-GNN interaction patterns affect model performance; v) Through extensive empirical studies, we demonstrate Mecoin\u2019s significant advantages over current state-of-the-art methods. ", "page_idx": 1}, {"type": "image", "img_path": "dqdffX3BS5/tmp/2c62926ed558d75433986a33d19a9df6193984f7a580f2a82c0128c2def3f891.jpg", "img_caption": ["Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{G}_{0}\\,=\\,(\\mathcal{V}_{0},\\mathcal{E}_{0})$ be the base graph with node set ${\\dot{\\nu}}_{0}$ and edge set $\\mathcal{E}_{0}$ . We consider $T$ temporal snapshots of $\\mathcal{G}_{0}$ , each corresponding to a GFSCIL task or session. Denote $\\mathcal{S}=\\{S_{0},S_{1},\\ldots,\\bar{S_{T}}\\}$ as the set of sessions including the pre-training session $S_{0}$ , and $\\mathcal{C}=\\{C_{0},C_{1},\\hdots\\,\\overset{\\cdot}{,}C_{T}\\}$ as the family of class sets within each session. The graph under session $S_{i}(i\\,=\\,1,2,...,T)$ is denoted as $\\mathcal{G}_{i}\\,=$ $(\\mathcal{V}_{i},\\mathcal{E}_{i})$ , with node feature matrix and adjacency matrix represented by $\\mathbf{X}_{i}\\,=\\,(\\mathbf{x}_{i}^{1},...,\\mathbf{x}_{i}^{|\\mathcal{V}_{i}|})^{\\top}\\,\\in$ $\\mathbb{R}^{|\\gamma_{i}|\\times d}$ and $\\mathbf{A}_{i}\\in\\mathbb{R}^{d\\times d}$ respectively. For session $S_{i}$ , let $\\mathbf{X}_{i}^{t r}\\in\\mathbb{R}^{K|C_{i}|\\times d}$ and $\\mathbf{Y}_{i}^{t r}$ be the features and corresponding labels of the training nodes respectively, where $K$ is the sample size of each class in $C_{i}$ , thus defining a $C_{i}$ -way $K$ -shot GFSCIL task. Let $\\mathcal{y}_{i}$ be the label space of session $S_{i}$ , and we assume that the label spaces of different sessions are disjoint, i.e., $y_{i}\\cap y_{j}=\\emptyset$ if $i\\neq j$ . Our goal is to learn a model $f_{\\theta}$ across successive sessions that maintains strong performance in the current session and also retains memory of the past sessions. ", "page_idx": 2}, {"type": "text", "text": "3 Efficient Memory Construction and Interaction Module ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present a comprehensive overview of our proposed framework, Mecoin. Unlike previous methods that are hampered by inefficient memory construction, low computational efficiency, and extensive parameter tuning\u2014which often lead to the loss of prior knowledge\u2014Mecoin enhances the learning of representative class prototypes. It achieves this by facilitating interaction between input nodes and seen class prototypes stored in the SMU, while integrating local graph structure information of the input nodes. Moreover, Mecoin decouples the learning of class prototypes from their corresponding probability distributions, thereby mitigating the loss of prior knowledge during both the prototype learning and classification processes. Fig. 1 illustrates the architecture of Mecoin. ", "page_idx": 2}, {"type": "text", "text": "3.1 Structured Memory Unit ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To acquire and store representative class prototypes, we develop SMU within Mecoin. Let $\\mathcal{M}$ be the set of class prototypes $\\{\\mathbf{m}_{0},\\mathbf{m}_{1},\\dots,\\mathbf{m}_{n-1}\\}$ , where $n=|C_{T-1}|$ refers to the total number of classes learned from the past $T-1$ sessions, with each $\\mathbf{m}_{i}\\in\\mathbb{R}^{k}(\\forall i\\in[n])$ . For current session $S_{T}$ , ", "page_idx": 2}, {"type": "text", "text": "the training node features ${\\bf X}_{T}^{t r}$ are encoded through a pre-trained GNN model $g_{\\phi}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf Z}_{T}=g_{\\phi}({\\bf A}_{T},{\\bf X}_{T}^{t r}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Z}_{T}\\,\\,\\in\\,\\,\\mathbb{R}^{(|C_{T}|K)\\,\\times\\,h}$ . During the training process, the parameters of the pre-trained GNN, denoted as $\\phi$ , remain fixed, and the training set used for pre-training is excluded from the subsequent GFSCIL tasks. To mitigate the significant memory usage resulting from caching metalearning samples and incorporate as much graph structural information as possible, we design MeCs within the SMU. MeCs merges graph structure information from the past sessions with that of the current session by interacting the encoded features of node $\\mathbf{Z}_{T}$ with the class prototypes in $\\mathcal{M}$ . Specifically, MeCs firstly facilitates the interaction between $\\mathbf{Z}_{T}$ and the SMU-stored prototypes $\\mathbf{M}_{0:T-1}\\triangleq\\left(\\mathbf{m}_{0},\\mathbf{m}_{1},\\dots,\\mathbf{m}_{n-1}\\right)^{\\top}$ through a self-attention mechanism: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}_{T}:=\\operatorname{softmax}(\\frac{(\\mathbf{Z}_{T}\\mathbf{W}_{Q})(\\mathbf{M}_{0:T-1}\\mathbf{W}_{K})^{\\top}}{\\sqrt{m}})\\mathbf{M}_{0:T-1}\\mathbf{W}_{V}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}_{Q}\\ \\in\\ \\mathbb{R}^{h\\times m},\\mathbf{W}_{K}\\ \\in\\ \\mathbb{R}^{k\\times m},\\mathbf{W}_{V}\\ \\in\\ \\mathbb{R}^{k\\times h}$ are learnable weight matrices and ${\\bf H}_{T}\\,\\in\\,{\\bf\\Lambda}$ $\\mathbb{R}^{(|C_{T}|K)\\times\\dot{h}}$ . Subsequently, to reduce memory consumption, we perform dimensionality reduction on ${\\mathbf{H}}_{T}$ through Gaussian random projection, resulting in $\\tilde{\\mathbf{H}}_{T}\\in\\mathbb{R}^{\\bar{(|C_{T}|K)\\times r}}$ , where $0<r<k<h$ . MeCs then extracts local graph structure information of $\\mathcal{G}_{T}$ via the vanilla self-attention on $\\mathbf{X}_{T}^{t r}$ , and preserves this information in the GraphInfo ${\\bf G}_{T}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{G}_{T}:=\\mathrm{ATTENTION}(\\mathbf{X}_{T}^{t r}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{G}_{T}\\in\\mathbb{R}^{(|C_{T}|K)\\times(k-r)}$ and is integrated with $\\tilde{\\mathbf{H}}_{T}$ by concatenation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{U}_{T}=(\\mathbf{G}_{T},\\tilde{\\mathbf{H}}_{T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To determine the class prototypes under session $S_{T}$ , we perform $k$ -means clustering on $\\mathbf{U}_{T}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{M}_{T}:=\\mathop{\\mathrm{k}}\\!\\mathrm{-means}(\\mathbf{U}_{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{M}_{T}\\in\\mathbb{R}^{|C_{T}|\\times k}$ . In addition, to improve the utilization of graph structural information, we optimize ${\\bf U}_{T}$ by the edge loss $\\mathcal{L}_{e d g e}$ defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{e d g e}=\\|\\mathbf{X}_{T}^{t r}\\cdot(\\mathbf{X}_{T}^{t r})^{\\top}-\\mathbf{U}_{T}\\cdot\\left(\\mathbf{U}_{T}\\right)^{\\top}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For each node feature(i.e. each row) in $\\mathbf{X}_{i}$ , We identify the nearest class prototype by minimizing the Euclidean distance: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{m}_{j}^{*}=\\mathrm{argmin}_{m_{j}\\in\\mathcal{M}}\\:||\\mathbf{u}_{i}^{l}-\\mathbf{m}_{j}||_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{u}_{i}^{l}$ is the $l$ -th row in $\\mathbf{U}_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Memory Representation Adaptive Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the traditional GFSCIL paradigm, adapting to evolving graph structures requires continuous learning and updating of class prototypes based on the current task\u2019s graph structure, alongside classifier retraining. This process, which involves adjusting parameters during class prototype learning, can lead the classifier to forget information about past categories, exacerbating the model\u2019s catastrophic forgetting. To address this, we introduce the MRaM within Mecoin. MRaM tackles this challenge by decoupling class prototype learning from class representation learning, caching probability distributions of seen categories. This separation ensures that class prototype updates don\u2019t affect the model\u2019s memory of probability distributions for nodes in seen categories, thus enhancing the stability of prior knowledge retention [14, 15, 16]. ", "page_idx": 3}, {"type": "text", "text": "To maintain the model\u2019s memory of the prior knowledge, we introduce GKIM into MRaM for information exchange between Mecoin and the model. Specifically, let $\\mathcal{P}=\\left\\{\\mathbf{p}_{0},\\mathbf{p}_{1},\\ldots,\\mathbf{p}_{n-1}\\right\\}$ denote the class representations learned from the GNN and stored in MRaM, where each $\\mathbf{p}_{i}$ corresponds to its respective class prototype ${\\bf m}_{i}$ . For a node feature $\\mathbf{x}_{s}$ from the seen classes with its class representation $\\mathbf{p}_{\\mathbf{x}_{s}}$ , let $\\mathbf{p}_{\\mathbf{x}_{s}}^{\\mathbf{'}\\!M L P}$ be the category predicted probabilities learned from GNN and MLP, then GKIM transfers the prior knowledge stored in Mecoin to the model through distillation that based on the memory loss function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m e m o r y}=\\frac{1}{N_{s}}\\sum_{i=1}^{N_{s}}\\mathrm{KL}(\\mathbf{p}_{i}||\\mathbf{p}_{\\mathbf{x}_{s}}^{M L P})=\\frac{1}{N_{s}}\\sum_{i=1}^{N_{s}}\\mathbf{p}_{i}\\log\\frac{\\mathbf{p}_{i}}{\\mathbf{p}_{\\mathbf{x}_{s}}^{M L P}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N_{s}$ is the total number of samples from the seen classes. ", "page_idx": 4}, {"type": "text", "text": "Furthermore, to update the category representations of new classes in MRaM, GKIM updates the newly learned knowledge from the model to Mecoin via distillation with the update loss function in Eq.9. For any node feature $\\mathbf{x}_{u}$ from the unseen classes, its category representation is randomly initialized as $\\mathbf{p}_{\\mathbf{x}_{u}}^{0}$ , and its predicted probability vector is $\\mathbf{p}_{\\mathbf{x}_{u}}^{M L P}$ . Through distillation, $\\mathbf{p}_{\\mathbf{x}_{u}}^{0}$ is updated to incorporate the new classes\u2019 representations which are thus stored in Mecoin. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{u p d a t e}=\\frac{1}{N_{u}}\\sum_{i=1}^{N_{u}}\\mathrm{KL}(\\mathbf{p}_{\\mathbf{x}_{u}}^{M L P}||\\mathbf{p}_{i})=\\frac{1}{N_{u}}\\sum_{i=1}^{N_{u}}\\mathbf{p}_{\\mathbf{x}_{u}}^{M L P}\\log\\frac{\\mathbf{p}_{\\mathbf{x}_{u}}^{M L P}}{\\mathbf{p}_{i}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N_{u}$ is the total number of samples from the unseen classes. The overall loss of Mecoin consists of the MLP classification loss $\\mathcal{L}_{c l s},\\mathcal{L}_{e d g e},\\mathcal{L}_{m e m o r y}$ and $\\mathcal{L}_{u p d a t e}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M e c o i n}=\\mathcal{L}_{c l s}+\\mathcal{L}_{e d g e}+\\mathcal{L}_{m e m o r y}+\\mathcal{L}_{u p d a t e}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we analyze the advantages of Mecoin and how it improves models generalization from the perspective of generalization error. Besides, we also provide insights from the viewpoint of VC-dimension by comparing GKIM with non-parametric methods and MLPs in classifying category representations stored in MRaM, and distilling Mecoin as a teacher model with GNN model. ", "page_idx": 4}, {"type": "text", "text": "What are the advantages of Mecoin over other models? In few-shot learning, the limited training data and the overall samples in current session $S_{T}$ often have different distributions, which leads to overfitting. However, Mecoin can mitigate overfitting since it has a lower bound of generalization error than other corresponding models (Thm.1). Before introducing our theoretical result, we first supplement some notations. For the current session $S_{T}$ , let $\\scriptstyle{\\mathcal{X}}_{T}$ and $\\ y_{T}$ be the sample space and label space of $\\mathbf{X}_{T}^{t r}$ respectively, and $\\mathcal{T}_{T}=\\{(\\mathbf{x}_{T}^{i},\\mathbf{y}_{T}^{i})\\}_{i=1}^{N}$ be the training samples. Let $\\mathcal{F}$ be a hypothesis class, $f_{\\theta}^{M}$ and $\\hat{f}\\in\\mathcal{F}$ be Mecoin and other corresponding models trained on $\\tau_{T}$ , and assume that the inputs $\\mathbf{x}_{T}\\in\\mathcal{X}_{T}$ undergo distributional shifts through any function $g_{\\epsilon}$ . Then by comparing the generalization error bounds of $f_{\\theta}^{M}$ with $\\hat{f}$ , we demonstrate in Thm.1 (proof in Appendix B.1) that Mecoin excels in distributional shifts, indicating its stronger generalization capability. The result is given in Thm.1, which is derived from theorem 3.1 in [12]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1: For any model $f\\in\\{f_{\\theta}^{M},\\hat{f}\\}$ trained on $\\tau_{T}$ , denote $\\mathcal{R}$ as its generalization error, then there exists a constant c such that for any $\\delta>0$ , the following holds with probability at least $1-\\delta$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}\\leq\\mathcal{R}_{\\epsilon}+\\mathcal{B}_{\\hat{f}}\\mathbb{I}\\{f=\\hat{f}\\}+c\\sqrt{\\frac{2\\ln(e/\\delta)}{N}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathfrak{Z}_{\\epsilon}=\\frac{1}{N}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{\\mathbf{M}}^{\\mathbf{y}_{T}}}|\\mathbb{Z}_{m}^{\\mathbf{y}_{T}}|\\mathbb{E}_{\\mathbf{z}}\\big[\\mathbb{E}_{\\boldsymbol\\epsilon}\\big[\\ell\\big(f(g_{\\epsilon}(\\mathbf{x}_{T})),\\mathbf{y}_{T}\\big)\\big]-\\ell\\big(f(\\mathbf{x}_{T}),\\mathbf{y}_{T}\\big)|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}\\big],}\\\\ {\\mathrm{.},m\\,2\\mathbb{E}_{\\mathcal{T}_{T},\\xi}[\\operatorname*{sup}_{\\hat{f}\\in\\mathcal{F}}\\sum_{i=1}^{|\\mathcal{T}_{m}^{\\boldsymbol{\\gamma}}|}\\xi_{i}\\ell(\\hat{f}(\\mathbf{x}_{T}^{i})\\mathbf{y}_{T}^{i})|\\mathbf{x}_{T}^{i}\\in\\mathcal{C}_{m},\\mathbf{y}_{T}^{i}=\\mathbf{y}_{T}]+c\\sqrt{\\ln(2e/\\delta)/2N}}\\end{array}$ $B_{\\hat{f}}=$ where $\\{\\xi_{i}\\}_{i}$ are i.i.d. random variables uniformly taking values in $\\{-1,1\\}$ , and $\\ell$ is the loss function $\\mathcal{L}_{\\mathrm{Mecoin}}$ , $\\mathbf{z}=(\\mathbf{x}_{T},\\mathbf{y}_{T})$ , $\\mathcal{C}_{m}^{\\mathbf{y}_{T}}=\\{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{X}_{T}\\times\\mathcal{Y}_{T}\\ |\\ \\mathbf{y}=\\mathbf{y}_{T},m=\\mathrm{argmin}_{i\\in[N]}d(k_{\\alpha}(\\mathbf{x}),\\mathbf{m}_{i})\\}$ , $\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}\\,=\\,\\left\\{i\\,\\in\\,\\left[N\\right]\\,\\mid\\,\\mathbf{x}_{T}^{i}\\,\\in\\,\\mathcal{C}_{m},\\mathbf{y}_{T}^{i}\\,=\\,\\mathbf{y}_{T}\\right\\}$ $\\mathsf{I},\\,\\mathcal{C}_{m}\\,=\\,\\{\\mathbf{x}\\,\\in\\,\\mathcal{X}_{T}\\,\\mid\\,m\\,=\\,\\mathrm{argmin}_{i\\in[\\left|\\mathcal{M}\\right|]}d\\!\\left(k_{\\alpha}(\\mathbf{x},\\mathbf{m}_{i})\\right)$ , $I_{\\mathcal{M}}^{\\mathbf{y}_{T}}=m\\in\\left[|\\mathcal{M}|\\right]\\,|\\,|\\mathcal{T}_{m}^{\\mathbf{y}_{T}}|\\geq1\\right\\}$ and $k_{\\alpha}$ is the MeCs operation. ", "page_idx": 4}, {"type": "text", "text": "Why use GKIM to interact with GNN models? Unlike traditional knowledge distillation techniques that rely on high-capacity teacher models, GKIM uses probability distributions stored in MRaM to preserve node of seen class distributions. This prevents knowledge loss in GNN during teacher model training. For unseen classes, the classifier learns and stores their probability distributions in MRaM. Notably, updating unseen class distributions and extracting seen class distributions can also use non-parametric methods [12]. Thus, we must examine GKIM\u2019s advantages over conventional distillation and non-parametric methods. We analyze the VC dimension when category representations in MRaM are distilled into models, comparing scenarios where MRaM, non-parametric methods, and multi-layer perceptrons (MLPs) act as teacher models. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2: If $n$ class representations are selected from GKIM, i.e. $n$ is the input size of GNN, then the VC dimension of GKIM is: ", "page_idx": 5}, {"type": "equation", "text": "$$\nV C D=\\left\\{\\begin{array}{l l}{{\\mathcal{O}(n+1)}}&{{\\mathrm{voting~classifier}}}\\\\ {{\\mathcal{O}(p^{2}H^{2})}}&{{\\mathrm{MLP}}}\\\\ {{\\mathcal{O}(p^{2}n^{2}H^{2})}}&{{\\mathrm{GKIM}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H$ is the number of hidden neurons in MLP and $p$ denotes the number of parameters of GNN. ", "page_idx": 5}, {"type": "text", "text": "In the above theorem, we take the common voting classifier as an example for non-parametric methods. It averages the category representations stored in MRaM and updates them through backpropagation. While this method has lower complexity, it is highly sensitive to the initialization of category representations due to its reliance solely on the average while computing the prediction probabilities. When using MLP to update category representations, its parameters require fine-tuning to accommodate new categories that will lead to the forgetting of prior knowledge. In contrast, GKIM exhibits a higher VC-dimension compared to the other two methods, indicating superior expressive power. Additionally, GKIM selectively updates category representations in MRaM locally, preserving the model\u2019s memory of prior knowledge. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will evaluate Mecoin through experiments and address the following research questions: Q1). Does Mecoin have advantages in the scenarios of graph few-shot continual learning? Q2). How does MeCs improve the representativeness of class prototypes? Q3). What are the advantages of GKIM over other distillation methods? ", "page_idx": 5}, {"type": "text", "text": "4.1 Graph Few-Shot Continual Learning (Q1) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Experimental setting. We assess Mecoin\u2019s performance on three real-world graph datasets: CoraFull, CS, and Computers, comprising two citation networks and one product network. Datasets are split into a Base set for GNN pretraining and a Novel set for incremental learning. Tab.1 provides the statistics and partitions of the datasets. In our experiments, we freeze the pretrained GNN parameters and utilize them as encoders for subsequent few-shot class-incremental learning on graphs. For CoraFull, the Novel set is divided into 10 sessions, each with two classes, using a 2-way 5-shot GFSCIL setup. CS\u2019s Novel set is split into 10 sessions, each with one class, adopting a 1-way 5-shot GFSCIL configuration. Computers\u2019 Novel set is segmented into 5 sessions, each with one class, also using a 1-way 5-shot GFSCIL setup. During the training process, the training samples for session 0 are randomly selected from each class of the pre-trained testing set, with 5 samples chosen as the training set and the remaining testing samples used as the test set for training. It is worth noting that for each session, during the testing phase, we construct a new test set using all the testing samples of seen classes to evaluate the model\u2019s memory of all prior knowledge after training on the new graph data. Our GNN and GAT models feature two hidden layers. The GNN has a consistent dimension of 128, while GAT varies with 64 for CoraFull and 16 for CS and Computers. Training parameters are set at 2000 epochs and a learning rate of 0.005. ", "page_idx": 5}, {"type": "text", "text": "Baselines. 1) Elastic Weight Consolidation (EWC) [17]-imposes a quadratic penalty on weights to preserve performance on prior tasks. 2) Learning without Forgetting (LwF) [18]- retains previous knowledge by minimizing the discrepancy between old and new model outputs. 3) Topology-aware Weight Preserving (TWP) [7]-identifies and regularizes parameters critical for graph topology to maintain task performance. 4) Gradient Episodic Memory (GEM) [19]-employs an episodic memory to adjust gradients during learning, preventing loss increase from prior tasks. 5) Experience ", "page_idx": 5}, {"type": "table", "img_path": "dqdffX3BS5/tmp/e08e398044ac1fc82657742c9dcb704385b6f916e02c6c4a3709bfd2297a53d1.jpg", "table_caption": ["Table 1: Information of the expermental datasets. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Replay GNN (ER-GNN) [3]- incorporates memory replay into GNN by storing key nodes from prior tasks. 6) Memory Aware Synapses (MAS) [20] evaluates parameter importance through sensitivity to predictions, distinct from regularization-based EWC and TWP. 7) HAG-Meta [10],a GFSCIL approach, preserves model memory of prior knowledge via task-level attention and node class prototypes. 8) Geometer [11]- adjusts attention-based prototypes based on geometric criteria, addressing catastrophic forgetting and imbalanced labeling through knowledge distillation and biased sampling in GFSCIL. ", "page_idx": 5}, {"type": "table", "img_path": "dqdffX3BS5/tmp/2406fee56151cc118f698eb1c2f2ad6fcc49e7f22de48e646f6610c424e25da9.jpg", "table_caption": ["Table 2: Comparison with SOTA methods on CoraFull dataset for GFSCIL. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "dqdffX3BS5/tmp/45f3e2cd7cc3eb16553ff3d16125a6cfd4043b432d35eee0c71b263f0e2508e0.jpg", "img_caption": ["Figure 2: The comparative analysis of the mean performance, accuracy curves and memory utilization of HAG-Meta, Geometer and Mecoin across 10 sessions on CoraFull, conducted under the experimental conditions delineated in their respective publications. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Experimental results. We execute 10 tests for each method across three datasets, varying the random seed, and detailed the mean test accuracy in Tables 2, 3, and 4. Key findings are as follows:1) Mecoin surpasses other benchmarks on CoraFull, CS, and Computers datasets, exemplified by a $66.72\\%$ improvement over the best baseline on the Computers dataset across all sessions. 2) Mecoin maintains a performance edge and exhibits reduced forgetting rates, substantiating its efficacy in mitigating catastrophic forgetting. 3) Geometer and HAG-Meta perform poorly in our task setting, likely because, to ensure a fair comparison, the models do not use any training data from the metalearning or pre-training processes during training. The computation of class prototypes in these two methods heavily relies on the graph structure information provided by this data. Additionally, in Fig.2, we compare the results of Geometer and HAG-Meta tested under the experimental conditions given in their papers with the results of Mecoin on the CoraFull dataset. The experimental results indicate that our method still achieves better performance and forgetting rates than these two methods under low memory conditions, demonstrating the efficiency of our method in terms of memory. ", "page_idx": 6}, {"type": "text", "text": "4.2 MeCs for Memory (Q2) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct relevant ablation experiments on MeCs across the CoraFull, CS, and Computers datasets:1) Comparing the impact of MeCs usage on model performance and memory retention; 2) Analyzing the effect of feature of node interaction with class prototypes stored in SMU on model performance and memory retention;3)Assessing the impact of using graphInfo on model performance and memory retention. We conduct a randomized selection of ten classes from the test set, extracting 100 nodes per class to form clusters with their corresponding class prototypes within the SMU. Subsequently, we assess the efficacy of MeCs and its constituent elements on model performance and the rate of forgetting. In cases where the MeCs was not utilized, we employ the k-means method to compute class prototypes. In experiment, we use GCN as backbone. The experimental parameters and configurations adhered to the standards established in prior studies. ", "page_idx": 6}, {"type": "table", "img_path": "dqdffX3BS5/tmp/a8b17aaa41596ae10e6265863b20921c6d840b2b6bdd10d49737de0480e12381.jpg", "table_caption": ["Table 3: Comparison with SOTA methods on Computers dataset for GFSCIL. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "dqdffX3BS5/tmp/e8a631fae86f2bf9c5d9abc5204beaf1dff145bd3ea29907a2d4f105ad16e566.jpg", "table_caption": ["Table 4: Comparison with SOTA methods on CS dataset for GFSCIL. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Experimental Results. The results of the ablation experiments on CoraFull and CS are shown in Fig.3 and results on other datasets are shown in Appendix.A. We deduce several key findings from the figure:1) The class prototypes learned by MeCs assist the model in learning more representative prototype representations, demonstrating the effectiveness of the MeCs method. 2) The difference in accuracy between scenarios where graphInfo is not used and where there is no interaction with class prototypes is negligible. However, the scenario without graphInfo exhibits a higher forgetting rate, indicating that local graph structural information provides greater assistance to model memory. This may be because local structural information reduces the intra-class distance of nodes, thereby helping the model learn more discriminative prototype representations. ", "page_idx": 7}, {"type": "image", "img_path": "dqdffX3BS5/tmp/1aac543c9680294cc24966a08344442455653acaafceac2d2cf1392b4f03aa0f.jpg", "img_caption": ["Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs . The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 GKIM for Memory (Q3) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our experimental investigation across three distinct datasets, we scrutinized the influence of varying interaction modalities between MRaM and GNN models on model performance and the propensity for forgetting. We delineate three distinct scenarios for analysis: 1) Class representations stored in MRaM are classified using non-parametric methods and used as the teacher model to interact with the GNN. 2) Class representations stored in MRaM are classified using MLP and Mecoin is employed as the teacher model to transfer knowledge to the GNN model. 3) GNN models are deployed in isolation for classification tasks, without any interaction with Mecoin. In this experiment, we use GCN as backbone. Throughout the experimental process, model parameters were held constant, and the experimental configurations were aligned with those of preceding studies. Due to space constraints, we include the results on the Computers dataset in the Appendix.A. ", "page_idx": 8}, {"type": "text", "text": "Experimental Results. It is worth noting that due to the one-to-one matching between MRaM and SMU via non-parametric indexing, there is no gradient back-propagation between these two components. This implies that updates to MRaM during training do not affect the matching between node features and class prototypes. Our experimental findings are depicted in Fig.4, and they yield several key insights: 1) GKIM outperforms all other interaction methods, thereby substantiating its efficacy. 2) The second interaction mode exhibits superior performance compared to the first and third methods. This is attributed to the MLP\u2019s higher VC-dimension compared to the non-parametric method, which gives it greater expressive power to handle more complex sample representations. However, the use of MLP results in a higher forgetting rate compared to the non-parametric method. This is because when encountering samples from new classes, the parameters of MLP undergo changes, leading to the loss of prior knowledge. For the third method, extensive parameter finetuning leads to significant forgetting of prior knowledge. Method one performs less effectively than GKIM, primarily because GKIM, with its larger VC-dimension, ensures that the process of updating class representations does not affect representations of other classes stored in MRaM. ", "page_idx": 8}, {"type": "image", "img_path": "dqdffX3BS5/tmp/3b70528c38b4e41e0f3e7ef2faadabb88d401afc24aa90245970a8a2992edfe3.jpg", "img_caption": ["Figure 4: Left 2 columns: Line charts depict the performance of models across various sessions on the CoraFull and CS datasets when using different distillation methods; Right 2 columns: Histograms illustrate the forgetting rates of different distillation methods on these two datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Current GFSCIL methods typically require a large number of labeled samples and cache extensive past task data to maintain memory of prior knowledge. Alternatively, they may fine-tune model parameters at the expense of sacrificing the model\u2019s adaptability to current tasks. To address these challenges, we propose the Mecoin for building and interacting with memory. Mecoin is made up of two main parts: the Structured Memory Unit (SMU), which learns and keeps class prototypes, and the Memory Representation Adaptive Module (MRaM), which helps the GNN preserve prior knowledge. To leverage the graph structure information for learning representative class prototypes, SMU leverages MeCs to integrate past graph structural information with interactions between samples and the class prototypes stored in SMU. Additionally, Mecoin introduces the MRaM, which separates the learning of class prototypes and category representations to avoid excessive parameter fine-tuning during prototype updates, thereby preventing the loss of prior knowledge. Furthermore, MRaM injects knowledge stored in Mecoin into the GNN model through GKIM, preventing knowledge forgetting. We demonstrate our framework\u2019s superiority in graph few-shot continual learning with respect to both generalization error and VC dimension, and we empirically show its advantages in accuracy and forgetting rate compared to other graph continual learning methods. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Science and Technology Major Project (2023ZD0121403). We extend our gratitude to the anonymous reviewers for their insightful feedback, which has greatlycontributed to the improvement of this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Junshan Wang, Guojie Song, Yi Wu, and Liang Wang. Streaming graph neural networks via continual learning. In Proceedings of the 29th ACM international conference on information & knowledge management, pages 1515\u20131524, 2020.   \n[2] Hongbo Bo, Ryan McConville, Jun Hong, and Weiru Liu. Ego-graph replay based continual learning for misinformation engagement prediction. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 01\u201308. IEEE, 2022.   \n[3] Fan Zhou and Chengtai Cao. Overcoming catastrophic forgetting in graph neural networks with experience replay. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4714\u20134722, 2021.   \n[4] Yilun Liu, Ruihong Qiu, and Zi Huang. Cat: Balanced continual graph learning with graph condensation. In 2023 IEEE International Conference on Data Mining (ICDM), pages 1157\u2013 1162. IEEE, 2023.   \n[5] Xikun Zhang, Dongjin Song, and Dacheng Tao. Ricci curvature-based graph sparsification for continual graph representation learning. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[6] Xikun Zhang, Dongjin Song, and Dacheng Tao. Hierarchical prototype networks for continual graph representation learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4622\u20134636, 2022.   \n[7] Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming catastrophic forgetting in graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 8653\u20138661, 2021.   \n[8] Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv preprint arXiv:1711.04043, 2017.   \n[9] Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, and Huan Liu. Graph prototypical networks for few-shot learning on attributed networks. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 295\u2013304, 2020.   \n[10] Zhen Tan, Kaize Ding, Ruocheng Guo, and Huan Liu. Graph few-shot class-incremental learning. In Proceedings of the fifteenth ACM international conference on web search and data mining, pages 987\u2013996, 2022.   \n[11] Bin Lu, Xiaoying Gan, Lina Yang, Weinan Zhang, Luoyi Fu, and Xinbing Wang. Geometer: Graph few-shot class-incremental learning via prototype representation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 1152\u20131161, 2022.   \n[12] Frederik Tr\u00e4uble, Anirudh Goyal, Nasim Rahaman, Michael Curtis Mozer, Kenji Kawaguchi, Yoshua Bengio, and Bernhard Sch\u00f6lkopf. Discrete key-value bottleneck. In International Conference on Machine Learning, pages 34431\u201334455. PMLR, 2023.   \n[13] Biqing Qi, Xinquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou. Interactive continual learning: Fast and slow thinking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312892, 2024.   \n[14] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.   \n[15] Biqing Qi, Junqi Gao, Xingquan Chen, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou. Contrastive augmented graph2graph memory interaction for few shot continual learning. arXiv preprint arXiv:2403.04140, 2024.   \n[16] Biqing Qi, Junqi Gao, Xinquan Chen, Dong Li, Weinan Zhang, and Bowen Zhou. Sr-cis: Self-reflective incremental system with decoupled memory and reasoning. arXiv preprint arXiv:2408.01970, 2024.   \n[17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[18] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.   \n[19] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.   \n[20] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV), pages 139\u2013154, 2018.   \n[21] Marek Karpinski and Angus Macintyre. Polynomial bounds for vc dimension of sigmoidal and general pfaffian neural networks. Journal of Computer and System Sciences, 54(1):169\u2013176, 1997.   \n[22] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Ablation Experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Section 4.2, we presented the relevant ablation experiments on the CoraFull and CS dataset.   \nBelow are the experimental results of the model on the Computers datasets. ", "page_idx": 12}, {"type": "image", "img_path": "dqdffX3BS5/tmp/a881ab450be79f1c5cef8cc684302dfccf3388e0340a2518e282f127a4671fa1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 5: From left to right are the results of GKIM, without using GraphInfo, node features not interacting with class prototypes in SMU and without using MeCs , when performing the graph small-sample continuous learning task in the Computers dataset, four randomly selected categories from session1 and 400 randomly selected samples from the four categories are clustered at the class center of the class prototypes bit class centers obtained from the learning during the training process. ", "page_idx": 12}, {"type": "text", "text": "The experimental results in this section are similar to those on the CoraFull and CS dataset, and relevant experimental analyses can be referred to in the main text. Additionally, we designed related ablation experiments on the dimensions of GraphInfo and its integration position with node features. Detailed results are shown in tab.5, 6. From these results, we can draw the following conclusions:1)It is evident from the tables that concatenating GraphInfo to the left of node features with a dimension of 1 yields the best performance; 2)The model\u2019s performance decreases as the dimension of GraphInfo increases, while the forgetting rate generally exhibits a decreasing trend. This indicates that local graph structural information provides some assistance to the model\u2019s memory. When the dimension of class prototypes is fixed, an increase in the dimension of GraphInfo implies less graph structural information extracted from prototypes of seen classes in past sessions. Consequently, MeCs integrates less graph structural information, making it difficult for the model to learn representative class prototypes, thereby limiting the model\u2019s performance. ", "page_idx": 12}, {"type": "table", "img_path": "dqdffX3BS5/tmp/285e63c958d0e7784e9747282f3b9176da0d42f3b4901004005029a835206ca8.jpg", "table_caption": ["Table 5: The table counts the effect of GraphInfo dimension change on the model\u2019s performance on the CoraFull dataset as well as the forgetting rate when concatenating GraphInfo to the left-hand side of the node features. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Furthermore, for Section 4.3, the relevant ablation experiment results on the Computers dataset can be found in Figure 8. ", "page_idx": 12}, {"type": "text", "text": "Table 6: The table counts the effect of GraphInfo dimension change on the model\u2019s performance on the CoraFull dataset as well as the forgetting rate when concatenating GraphInfo to the right-hand side of the node features. ", "page_idx": 13}, {"type": "table", "img_path": "dqdffX3BS5/tmp/94152b1a60469daccf5fd4b572360d8f5f4ab8c6805eabdbcfc528ddd1969b62.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "dqdffX3BS5/tmp/2f73f78bbda242e19484a3349977a5b1907932f58d48ab243bbe9098c1ffeae1.jpg", "img_caption": ["Figure 6: HAG-Meta, Geometer, and Mecoin methods\u2019 average performance, performance curves, and memory consumption over 10 sessions on the Computers dataset under conditions set forth in their papers. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proof of Theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. By instituting $\\mathcal{C}_{m}^{{\\bf y}_{T}}$ with $\\mathcal{C}_{k}^{y}$ , ${\\mathcal{C}}_{m}$ with $\\mathcal{C}_{k}$ , $\\mathcal{T}_{m}^{\\mathbf{y}_{T}}$ with $\\mathcal{T}_{k}^{y}$ , $I_{\\mathcal{M}}^{\\mathbf{y}_{T}}$ with $I_{\\kappa}^{y}$ , ${\\mathbf{m}}_{i}$ with $\\kappa_{i}$ , and $k_{\\alpha}$ with $k_{\\beta}\\circ z_{\\alpha}$ , our proof is similar to the proof of theorem 3.1 in [12] (please refer to [12] for more details). For $f\\in\\{f_{\\theta}^{M},\\hat{f}\\}$ , through similar procedure in the proof of [12], we obtain the coarse upper bound ", "page_idx": 13}, {"type": "text", "text": "of generalization error $\\mathcal{R}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}=\\mathbb{E}_{\\mathbf{z},\\mathbf{c}}\\big[\\ell(f(g_{\\mathbf{c}}(\\mathbf{x}_{T})),\\mathbf{y}_{T})\\big]-\\frac{1}{N}\\sum_{i=1}^{N}\\ell(f(\\mathbf{x}_{T}^{i}),\\mathbf{y}_{T}^{i})}\\\\ &{\\quad\\leq\\frac{1}{N}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in\\mathbb{I}_{M}^{\\mathcal{Y}_{T}}}|T_{m}^{\\mathbf{y}_{T}}|\\mathbb{E}_{\\mathbf{z}}\\big[\\ell(f(g_{\\mathbf{c}}(\\mathbf{x}_{T})),\\mathbf{y}_{T})\\big]-\\ell(f(\\mathbf{x}_{T}),\\mathbf{y}_{T})|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}\\big]}\\\\ &{\\quad+\\displaystyle\\frac{1}{N}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in\\mathbb{I}_{M}^{\\mathcal{Y}_{T}}}|T_{m}^{\\mathbf{y}_{T}}|\\left(\\mathbb{E}_{\\mathbf{z}}\\big[\\ell(f(\\mathbf{x}_{T}),\\mathbf{y}_{T})\\big|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}\\big]-\\frac{1}{|\\mathcal{Z}_{m}^{\\mathcal{Y}_{T}}|}\\sum_{i\\in\\mathcal{X}_{m}^{\\mathcal{Y}_{T}}}\\ell\\big(f(\\mathbf{x}_{T}^{i}),\\mathbf{y}_{T}^{i}\\big)\\right)}\\\\ &{\\quad+\\displaystyle c\\sqrt{\\frac{2\\ln(e/\\delta)}{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then further conduct the tighter generalization bound for $\\hat{f}$ and $f_{\\theta}^{M}$ separately based on Eq.13. Consider the case of $f={\\hat{f}}$ , for the second term of right hand side of equation (13), according to Lemma 4 of (Pham et al., 2021) and the fact that $\\begin{array}{r}{\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{\\mathcal{M}}^{\\mathbf{y}_{T}}}|\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}\\bar{\\mathbf{\\Psi}}|=N}\\end{array}$ , we obtain that for any $\\delta>0$ , with probability at least $1-\\delta$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{1}{N}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{M}^{\\mathbb{V}_{T}}}|\\mathcal{Z}_{m}^{\\mathbb{V}_{T}}|\\left(\\mathbb{E}_{\\mathbf{z}}[\\ell(\\hat{f}(\\mathbf{x}_{T}),\\mathbf{y}_{T})|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}]-\\frac{1}{|\\mathcal{Z}_{m}^{\\mathbb{V}_{T}}|}\\sum_{i\\in\\mathcal{I}_{m}^{\\mathbb{V}_{T}}}\\ell(\\hat{f}(\\mathbf{x}_{T}^{i}),\\mathbf{y}_{T}^{i})\\right)}\\\\ &{}&{\\displaystyle\\leq2\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{M}^{\\mathbb{V}_{T}}}|\\mathcal{Z}_{m}^{\\mathbb{V}_{T}}|\\frac{\\mathcal{R}_{\\mathbf{y}_{T},m}(l\\circ\\hat{\\mathcal{F}})}{N}+M\\sqrt{\\frac{\\ln(|\\mathcal{Y}_{T}||\\mathcal{M}|/\\delta)}{2N}}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{M}^{\\mathbb{V}_{T}}}\\sqrt{\\frac{|\\mathcal{Z}_{m}^{\\mathbb{V}_{T}}|}{N}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{R}_{\\mathbf{y}_{T},m}(l\\circ\\hat{\\mathcal{F}})=\\mathbb{E}_{\\mathcal{T}_{T},\\xi}[\\operatorname*{sup}_{\\hat{f}\\in\\mathcal{F}}\\sum_{i=1}^{|\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}|}\\xi_{i}\\ell(\\hat{f}(\\mathbf{x}_{T}^{i})\\mathbf{y}_{T}^{i})|\\mathbf{x}_{T}^{i}\\in\\mathcal{C}_{m},\\mathbf{y}_{T}^{i}=\\mathbf{y}_{T}]}\\end{array}$ , and $\\{\\xi_{i}\\}_{i}$ are independently and identically distributed random variables that randomly taking values in $\\{-1,1\\}$ . Moreover, using Cauchy-Schwarz inequality, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{\\mathcal{M}}^{\\mathbf{y}_{T}}}\\frac{|\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}|}{N}\\leq\\sqrt{|\\mathcal{Y}_{T}||\\mathcal{M}|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then by $\\ln(|\\mathcal{V}_{T}||\\mathcal{M}|/\\delta)\\le\\operatorname*{max}(1,\\ln(|\\mathcal{V}_{T}||\\mathcal{M}|))\\ln(e/\\delta)$ , we obtain the final tighter generalization upper bound of $\\hat{f}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{z},\\epsilon}[\\ell(\\hat{f}(g_{\\epsilon}(\\mathbf{x}_{T})),\\mathbf{y}_{T})]-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\hat{f}(\\mathbf{x}_{T}^{i}),\\mathbf{y}_{T}^{i})}\\\\ &{\\ \\ \\leq\\displaystyle\\frac{1}{N}\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{N}^{\\mathbb{Y}_{T}}}|Z_{m}^{\\mathbf{y}_{T}}|\\mathbb{E}_{\\mathbf{z}}[\\ell(f(g_{\\epsilon}(\\mathbf{x}_{T})),\\mathbf{y}_{T})]-\\ell(f(\\mathbf{x}_{T}),\\mathbf{y}_{T})|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}]}\\\\ &{\\ \\ +\\displaystyle2\\sum_{\\mathbf{y}_{T}\\in\\mathcal{Y}_{T}}\\sum_{m\\in I_{N}^{\\mathbb{Y}_{T}}}|Z_{m}^{\\mathbf{y}_{T}}|\\frac{\\mathcal{R}_{\\mathbf{y}_{T},m}(l\\circ\\hat{\\mathcal{F}})}{N}+c(\\sqrt{\\frac{2\\ln(e/\\delta)}{N}}+\\sqrt{\\frac{\\ln(e/\\delta)}{2N}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the case of $f=f_{\\theta}^{M}$ , the second term yin the right hand side of equation (13) equals 0. In fact, based on the definition of $\\mathcal{C}_{m}^{{\\bf y}_{T}}\\mathcal{L}_{m}^{{\\bf y}_{T}}$ and $I_{\\mathcal{M}}^{\\mathbf{y}_{T}}$ , and the matching method (i.e. based on the smallest Euclidean distance) between the input node features $\\mathbf{x}_{T}$ and the class prototypes $\\mathbf{m}$ in Mecoin, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{z}}[\\ell(f_{\\theta}^{M}(\\mathbf{x}_{T}),\\mathbf{y}_{T})|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}]-\\frac{1}{|\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}|}\\displaystyle\\sum_{i\\in\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}}\\ell\\big(f_{\\theta}^{M}(\\mathbf{x}_{T}^{i}),\\mathbf{y}_{T}^{i}\\big)}\\\\ &{~~=\\mathbb{E}_{\\mathbf{z}}[\\ell(h_{\\theta}(\\mathbf{p}_{T}),\\mathbf{y}_{T})|\\mathbf{z}\\in\\mathcal{C}_{m}^{\\mathbf{y}_{T}}]-\\displaystyle\\frac{1}{|\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}|}\\displaystyle\\sum_{i\\in\\mathcal{Z}_{m}^{\\mathbf{y}_{T}}}\\ell\\big(h_{\\theta}(\\mathbf{p}_{T}),\\mathbf{y}_{T}\\big)}\\\\ &{~~=\\ell\\big(h_{\\theta}(\\mathbf{p}_{T}),\\mathbf{y}_{T}\\big)-\\ell\\big(h_{\\theta}(\\mathbf{p}_{T}),\\mathbf{y}_{T}\\big)=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{p}_{T}$ is the value that is uniquely corresponding to the class prototype $\\mathbf{m}=k_{\\alpha}(\\mathbf{x}_{T})$ , and $h_{\\theta}$ represents the knowledge interchange process in GKIM. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "proof: When we use a simple non-parametric decoder function which uses average pooling to calculate the element-wise average of all the fetched memory representation and then applies a softmax function to the output, the decoder is the same as a single hidden layer nets with fixed input weights. This network can be formulated as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(u)=c_{0}+\\sum_{i=1}^{n}c_{i}\\sigma(A_{i}u+b_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $A_{i}$ is the $i$ -th row of weight matrix, $b_{i}$ is the bias, $c_{i}$ is the output layer weights. For average pooling, $c_{i}$ and $b_{i}$ is 0, and $A_{i}$ is $\\frac{1}{n}$ . So is easy to prove that the VC-dimension is $n+1$ . Then, according to [21], the VC-dimension of MLP with one hidden layer is $n^{2}H^{2}$ . Then we consider the case when graph structure information is induced. When we use the distillation technique to inject graph structure information into GKIM via graph neural networks, the upper bound on the capacity of memory representation in GKIM is the expressive capacity of the graph neural networks. Thus, according to [22], we have that the VC-dimensinon of GKIM with graph structure information is $p^{2}n^{2}H^{2}$ . In summary, when graph structure information is introduced, the upper bound of the VC dimension of GKIM is increased and its expressive power is also enhanced. ", "page_idx": 15}, {"type": "text", "text": "C Parameters and Devices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The relevant experimental parameters in this paper were determined through grid search. The parameter settings for each dataset are shown in Table 7. ", "page_idx": 15}, {"type": "table", "img_path": "dqdffX3BS5/tmp/e3871d29c8add3b984883278899e95eb34bf6f471c44ef06d3ba2279deb35557.jpg", "table_caption": ["Table 7: Parameters used in each data set. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "The environment in which we run experiments is: ", "page_idx": 15}, {"type": "text", "text": "\u2022 CPU information:24 vCPU AMD EPYC 7642 48-Core Processor \u2022 GPU information:RTX A6000(48GB) ", "page_idx": 15}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "While our method has shown promising performance in graph few-shot incremental learning, there are still some issues that need to be addressed: 1) Our approach assumes that the graph structures across all sessions originate from the same graph. However, in real-world scenarios, data from different graphs may be encountered, and we have not thoroughly explored this issue; 2) Although our method currently maintains model performance with only a small number of samples, further validation is needed to assess whether it can still achieve excellent performance under low-resource conditions where graph structural information is scarce. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Contributions are detailed in the last paragraph of the Introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We give the limitations in Appendix D. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Theoretical results are detailed in Section3.3 and Appendix B. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We report all settings and details of reproducible experiments in Section 4 and Appendix C. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: We will make our code public after the paper is accepted. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We report all settings and details of experiments in Section 4 and Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Error bars are not reported because we repeated each experiment in the paper 10 times and reported the mean of the results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Appendix C of the paper reports the computer resources needed to reproduce the experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the thesis complied with the NeurIPS Code of Ethics in all respects. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our approach does not address social impacts and this question is not applicable to our approach. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use explicitly are mentioned and properly respected. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "dqdffX3BS5/tmp/f2f8605e35b06fdf1134747aa9beaace75e06d1538553fa75f6eb47abbaf3bca.jpg", "img_caption": ["Figure 7: Caption for Image 2 "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "dqdffX3BS5/tmp/743044fe3387db50c7803cf7d9f806557e2a17daa5636772de7121befc2687ee.jpg", "img_caption": ["Figure 8: Caption for Image 2 "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 9: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull and CS dataset. MeCs is the new name for MeCo(according to R3\u2019s suggestion). The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs . The experimental results for CoraFull are shown in the above figure, and the results for CS are in the figure below. ", "page_idx": 23}, {"type": "image", "img_path": "dqdffX3BS5/tmp/8d236bef44ae119912a6309e13a3d7a19d8603b4b2c0e6f7eae89e4feda39350.jpg", "img_caption": ["Figure 10: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model. "], "img_footnote": [], "page_idx": 24}]