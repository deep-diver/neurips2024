[{"figure_path": "hKcx2wa3P0/figures/figures_9_1.jpg", "caption": "Figure 1: Averaged log MSE and log empirical excess risk for KM and TKM versus \u03b1 for different \u03c4.", "description": "This figure displays the results of numerical experiments comparing the performance of the standard kernel method (KM) and the truncated kernel method (TKM) in kernel quantile regression.  The x-axis represents the model complexity parameter \u03b1, while the y-axis shows both the log mean squared error (MSE) and the log empirical excess risk.  Separate lines are plotted for each quantile level \u03c4 (0.3, 0.5, and 0.7).  The figure demonstrates the impact of model complexity on the performance of both methods and the quantile level.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_40_1.jpg", "caption": "Figure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying sample size n.", "description": "This figure shows the results of a numerical experiment comparing the performance of the standard kernel method (KM) and the truncated kernel method (TKM) for quantile regression.  The experiment varies the sample size (n) while holding other parameters constant.  The results are presented as plots of log MSE (mean squared error) and log empirical excess risk versus sample size. The plots show that as sample size increases, both KM and TKM improve in performance (error decreases), but TKM shows consistently better performance than KM across all sample sizes and quantile levels (\u03c4).", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_40_2.jpg", "caption": "Figure 3: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying truncation level rl = log(r/n).", "description": "The figure shows the averaged logarithmic mean squared error (MSE) and log empirical excess risk for both the standard kernel-based method (KM) and the truncated kernel-based method (TKM) under the check loss function.  The x-axis represents the logarithmic ratio of the truncation level (r) to the sample size (n), denoted as rl. Different lines represent results for different quantile levels (\u03c4 = 0.3, 0.5, 0.7).  The shaded area represents the standard deviation across multiple runs. The figure illustrates how the choice of truncation level r impacts the performance of both methods, particularly highlighting the benefits of TKM in achieving lower error for a carefully chosen r.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_41_1.jpg", "caption": "Figure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying sample size n.", "description": "This figure shows the performance of both the standard kernel method (KM) and the truncated kernel method (TKM) under different sample sizes (n).  The logarithmic mean squared error (MSE) and the logarithmic empirical excess risk are plotted for three different quantiles (\u03c4 = 0.3, 0.5, 0.7). The results demonstrate that TKM consistently outperforms KM across all quantiles and sample sizes, and the improvement is more pronounced at larger sample sizes. This finding supports the paper's theoretical claims regarding the superior performance of TKM, especially for larger sample sizes.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_41_2.jpg", "caption": "Figure 10: Kernel quantile regression; averaged log MSE and log empirical excess risk for KM and TKM versus log ratios (rl = log(r/n)) of the truncation level r to the sample size n across different quantile levels.", "description": "This figure presents the results of a numerical experiment comparing the performance of the standard kernel method (KM) and the truncated kernel method (TKM) in kernel quantile regression. The experiment is performed for three different quantile levels (\u03c4 = 0.3, 0.5, 0.7) and varies the logarithmic ratio of the truncation level r to the sample size n (rl = log(r/n)). The results are shown in terms of averaged log MSE and log empirical excess risk, which are both measures of the method's performance. The figure suggests that TKM's performance is comparable to KM's, except when the truncation level r is too small.", "section": "H.3 Exponential Decay Case"}, {"figure_path": "hKcx2wa3P0/figures/figures_42_1.jpg", "caption": "Figure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying sample size n.", "description": "This figure shows the results of a simulation comparing the performance of two kernel methods, KM and TKM, using the check loss function.  The x-axis represents the sample size (n), and the y-axis shows the log mean squared error (MSE) and log empirical excess risk.  The figure consists of four subplots, each corresponding to a different quantile level (\u03c4 = 0.3, 0.5, 0.7).  The results indicate that TKM outperforms KM across all quantile levels and sample sizes, with a greater performance improvement at larger sample sizes. This supports the paper's claim that TKM is more efficient than KM, especially when data is abundant.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_42_2.jpg", "caption": "Figure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying sample size n.", "description": "This figure shows the results of a simulation comparing the performance of two kernel methods, KM and TKM, for different sample sizes (n).  The results are averaged over multiple runs and presented for three different quantile levels (\u03c4). The plots show both the Mean Squared Error (MSE) and the empirical excess risk.  It illustrates the convergence of the methods and the impact of sample size on the accuracy. TKM consistently outperforms KM.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_43_1.jpg", "caption": "Figure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with varying sample size n.", "description": "The figure shows the results of a numerical experiment comparing the performance of two methods (KM and TKM) for kernel quantile regression. The experiment varied the sample size (n) and measured the mean squared error (MSE) and excess risk using a check loss function. The results show that TKM consistently outperforms KM across different sample sizes, suggesting its superiority in kernel quantile regression.", "section": "5 Numerical Verification"}, {"figure_path": "hKcx2wa3P0/figures/figures_43_2.jpg", "caption": "Figure 9: Kernel SVM; averaged classification error rate and log excess risk for KM and TKM versus \u03b1.", "description": "This figure shows the performance of Kernel Support Vector Machine (KSVM) using both standard kernel-based method (KM) and truncated kernel-based method (TKM) with varying model complexities (\u03b1). The plots illustrate the trade-off between model complexity and target-kernel alignment.  Specifically, it shows that at lower levels of \u03b1 (higher model complexity), both methods perform similarly. However, as \u03b1 increases (lower model complexity), TKM significantly outperforms KM in both classification error rate and excess risk, demonstrating its ability to handle strong target-kernel alignment.", "section": "H.2 SVM with varying Model Complexities"}, {"figure_path": "hKcx2wa3P0/figures/figures_44_1.jpg", "caption": "Figure 10: Kernel quantile regression; averaged log MSE and log empirical excess risk for KM and TKM versus log ratios (rl = log(r/n)) of the truncation level r to the sample size n across different quantile levels.", "description": "This figure shows the results of a kernel quantile regression experiment.  It compares the performance of the standard kernel method (KM) and the truncated kernel method (TKM) across various truncation levels (r) relative to the sample size (n). The results are presented for three different quantile levels (\u03c4 = 0.3, 0.5, 0.7) using two metrics: log Mean Squared Error (MSE) and log empirical excess risk.  The x-axis represents the log ratio of the truncation level to the sample size.  The plots illustrate how the choice of truncation level impacts the performance of both methods.", "section": "H.3 Exponential Decay Case"}]