[{"figure_path": "tPdJ2qHkOB/tables/tables_4_1.jpg", "caption": "Table 1: Comparative illustration of token-level, sentence-level, and option-level MCTS search nodes. y denotes a token sampled from the policy model. The arrow \u2192 represents the transition from one search node to the subsequent node within the search process.", "description": "This table compares three different levels of Monte Carlo Tree Search (MCTS): token-level, sentence-level, and option-level.  It shows how the search node is defined in each approach (a single token, a whole sentence, or a sequence of tokens), and what constitutes a termination condition for the search at that level.  The option-level is the most flexible, allowing for sequences of varying lengths and providing more control over the search process.", "section": "4.3 \u03b7MCTS"}, {"figure_path": "tPdJ2qHkOB/tables/tables_8_1.jpg", "caption": "Table 2: Comparison results of ALPHALLM on the GSM8K and MATH datasets. #Annotation indicates the quantity of labeled data employed for fine-tuning policy or training critic models. The annotation used for training are noted as RN for rationales and FA for final answers. SYN means models trained on synthetic prompts, where trajectories were generated using \u03b7MCTS.", "description": "This table compares the performance of ALPHALLM with various other large language models (LLMs) on two mathematical reasoning datasets: GSM8K and MATH.  It shows the accuracy of each model, indicating how well it solved the problems in each dataset.  The table also specifies whether the models were trained using rationales (RN), final answers (FA), or synthetic prompts (SYN), and how many annotations were used in training (indicated by #Annotation).  The key takeaway is that ALPHALLM, especially when using the MCTS search algorithm, achieves comparable performance to state-of-the-art models like GPT-4, demonstrating its effectiveness in improving LLMs through self-improvement.", "section": "5.2 Results"}, {"figure_path": "tPdJ2qHkOB/tables/tables_8_2.jpg", "caption": "Table 3: (a): Ablation studies on the GSM8K test set of various components of MCTS, including adaptive branching, PRM, fast-rollout with ORM, state merge, and large number of rollouts. (b): Ablation studies of the impacts of tool-augmented ORM and option-level formulation on MATH.", "description": "This table presents the ablation study results on the GSM8K and MATH datasets.  It shows how the performance of the ALPHALLM model changes when different components of the MCTS algorithm (adaptive branching, PRM, fast-rollout with ORM, state merge, large number of rollouts) and the option-level formulation are removed.  The results are presented in terms of accuracy and the number of rollouts required.", "section": "5.3 Ablation Study"}, {"figure_path": "tPdJ2qHkOB/tables/tables_9_1.jpg", "caption": "Table 4: (a): Ablation studies on the GSM8K test set of various components of MCTS, including adaptive branching, PRM, fast-rollout with ORM, state merge, and large number of rollouts. (b): Ablation studies of the impacts of tool-augmented ORM and option-level formulation on MATH.", "description": "This table presents ablation studies on the GSM8K and MATH datasets.  It shows the impact of different components and design choices of the proposed \u03b7MCTS algorithm on the accuracy.  (a) shows ablation results on GSM8K, evaluating the effects of using adaptive branching factors, the process reward model (PRM), the outcome reward model (ORM) with fast rollout, and state merge strategies. (b) focuses on the MATH dataset, examining the impact of tool-augmented ORM and option-level formulation.", "section": "5.3 Ablation Study"}, {"figure_path": "tPdJ2qHkOB/tables/tables_9_2.jpg", "caption": "Table 4: (a): Ablation studies on the GSM8K test set of various components of MCTS, including adaptive branching, PRM, fast-rollout with ORM, state merge, and large number of rollouts. (b): Ablation studies of the impacts of tool-augmented ORM and option-level formulation on MATH.", "description": "This table presents ablation studies performed on the GSM8K and MATH datasets to evaluate the impact of different components of the MCTS algorithm, including adaptive branching, PRM (process reward model), ORM (outcome reward model) with fast rollout, state merge, and varying numbers of rollouts.  The (a) part shows results on GSM8K, and (b) part focuses on MATH, examining the effects of tool-augmented ORM and the option-level formulation.", "section": "5.3 Ablation Study"}, {"figure_path": "tPdJ2qHkOB/tables/tables_17_1.jpg", "caption": "Table 5: Parameters for MCTS. The Small/Large means small #rollout and small #rollout", "description": "This table presents the hyperparameters used in the Monte Carlo Tree Search (MCTS) algorithm.  The hyperparameters are categorized into two groups: exploration vs. exploitation and adaptive branching.  Exploration/exploitation is controlled by the parameters 'c' and '\u03b1', with higher values generally leading to more exploration and lower values favoring exploitation. Adaptive branching controls the maximum and minimum number of child nodes that can be explored at each level of the search tree ('cmax' and 'cmin').  The values differ for the root node (t=0) versus other nodes (t>0).  The table also specifies parameter values used for experiments performed on two datasets, GSM8K and MATH, categorized further into 'small' and 'large' rollout experiments.  'Small' likely denotes experiments with a smaller number of simulations, whereas 'large' represents experiments with a more extensive number of simulations during the MCTS process.", "section": "4.3 \u03b7MCTS"}, {"figure_path": "tPdJ2qHkOB/tables/tables_18_1.jpg", "caption": "Table 6: Ablation study over different fast-rollout models on GSM8K.", "description": "This table presents the ablation study result of using different fast rollout models for the GSM8K dataset. It compares the performance (accuracy and speed) of using Abel-002-7B and Llama-2-70B as the fast rollout models in the Monte Carlo Tree Search (MCTS) algorithm.  The results show that while Llama-2-70B achieves slightly higher accuracy, Abel-002-7B is significantly faster.", "section": "5.3 Ablation Study"}, {"figure_path": "tPdJ2qHkOB/tables/tables_18_2.jpg", "caption": "Table 2: Comparison results of ALPHALLM on the GSM8K and MATH datasets. #Annotation indicates the quantity of labeled data employed for fine-tuning policy or training critic models. The annotation used for training are noted as RN for rationales and FA for final answers. SYN means models trained on synthetic prompts, where trajectories were generated using \u03b7MCTS.", "description": "This table compares the performance of different models on the GSM8K and MATH datasets.  It shows the accuracy achieved by various methods (including ALPHALLM with different configurations) and indicates the amount of labeled data used for training. The table highlights the impact of using MCTS and the self-improvement loop on model performance, particularly emphasizing the use of synthetic data generated by the model itself.", "section": "5.2 Results"}, {"figure_path": "tPdJ2qHkOB/tables/tables_18_3.jpg", "caption": "Table 2: Comparison results of ALPHALLM on the GSM8K and MATH datasets. #Annotation indicates the quantity of labeled data employed for fine-tuning policy or training critic models. The annotation used for training are noted as RN for rationales and FA for final answers. SYN means models trained on synthetic prompts, where trajectories were generated using \u03b7MCTS.", "description": "This table compares the performance of ALPHALLM against other LLMs on two mathematical reasoning datasets: GSM8K and MATH.  It shows the accuracy of various models, indicating the amount of labeled data used for training (annotations: rationales (RN), final answers (FA), or synthetic data (SYN) generated using Monte Carlo Tree Search (MCTS)).", "section": "5.2 Results"}, {"figure_path": "tPdJ2qHkOB/tables/tables_19_1.jpg", "caption": "Table 8: Performance comparison of the Value Function model and PRM on the GSM8K test set.", "description": "This table presents the performance comparison of the Value Function and PRM models on the GSM8K test set. The performance is evaluated using three metrics: precision, recall, and expected calibration error (ECE).  The Value Function model shows higher precision and better calibration, while the PRM model achieves higher recall. This highlights the strengths and weaknesses of each model, indicating that they might be complementary for use in a combined system.", "section": "A.10 Critic Performance"}]