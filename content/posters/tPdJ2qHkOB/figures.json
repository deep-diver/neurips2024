[{"figure_path": "tPdJ2qHkOB/figures/figures_1_1.jpg", "caption": "Figure 1: Imagination-Searching-Criticizing self-improvement loop: Imagination component synthesizes prompts as new learning examples, with MCTS searching better trajectories guided by signals from critics for policy improving.", "description": "This figure illustrates the ALPHALLM self-improvement loop, which consists of three main components: Imagination, Searching, and Criticizing.  The Imagination component generates new prompts by leveraging existing data and a language model (LLM). The Searching component utilizes Monte Carlo Tree Search (MCTS) to explore the vast response space, guided by the Criticizing component.  The Criticizing component consists of three critic models (Value Function, Step Reward, Outcome Reward) providing feedback to improve the policy. The improved policy is then used to refine the LLM, creating a continuous self-improvement loop.", "section": "4 ALPHALLM"}, {"figure_path": "tPdJ2qHkOB/figures/figures_9_1.jpg", "caption": "Figure 2: Empirical analysis on GSM8K of different self-improving data collection methods and number of iterations. Models are evaluated with greedy decoding, \u03b7MCTS with small #rollout and large #rollout.", "description": "This figure shows the empirical analysis on the GSM8K dataset for different self-improving data collection methods and various numbers of iterations. The models are evaluated using greedy decoding, \u03b7MCTS with small rollout numbers and \u03b7MCTS with large rollout numbers.  The results illustrate the performance improvements achieved through iterative self-improvement using different methods and highlight the effectiveness of \u03b7MCTS for improving the model's accuracy.", "section": "5.3 Ablation Study"}, {"figure_path": "tPdJ2qHkOB/figures/figures_14_1.jpg", "caption": "Figure 1: Imagination-Searching-Criticizing self-improvement loop: Imagination component synthesizes prompts as new learning examples, with MCTS searching better trajectories guided by signals from critics for policy improving.", "description": "The figure illustrates the ALPHALLM self-improvement loop.  It starts with an Imagination component that generates new prompts as training examples. These prompts are fed into an LLM, which then uses Monte Carlo Tree Search (MCTS) to explore better response trajectories.  The MCTS process is guided by three critic models (Value, PRM, ORM) that provide feedback on the quality of the generated responses. This feedback allows the LLM to refine its responses, improving its performance iteratively. The loop continues, with improved responses feeding back into the generation of new prompts and further refinements.", "section": "4 ALPHALLM"}]