[{"type": "text", "text": "Association Pattern-aware Fusion for Biological Entity Relationship Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lingxiang Jia1 Yuchen Ying1 Zunlei Feng1,2\u2217Zipeng Zhong1 Shaolun Yao1   \nJiacong $\\mathbf{H}\\mathbf{u}^{1}$ Mingjiang Duan1 Xingen Wang1,3 Jie Song1 Mingli Song1,2 ", "page_idx": 0}, {"type": "text", "text": "1State Key Laboratory of Blockchain and Data Security, Zhejiang University 2Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security 3Bangsheng Technology Co, Ltd. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health. However, existing researches only focus on entity-centric information mapping and aggregation, neglecting the crucial role of potential association patterns among different entities. To address the above limitation, we propose a novel association pattern-aware fusion method for biological entity relationship prediction, which effectively integrates the related association pattern information into entity representation learning. Additionally, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. Extensive experiments conducted on three biological datasets quantitatively demonstrate that the proposed method achieves about $4\\%{-}23\\%$ hit@1 improvements compared with state-of-the-art baselines. Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios. Our data and code are available at https://github.com/hry98kki/PatternBERP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Exploring potential associations among triple-wise biological entities (e.g., drug-target proteinadverse reaction) [1\u20136] holds significant implications for elucidating underlying biological mechanisms and advancing personalized therapies [7\u20139], thus promoting pharmaceutical innovation and ensuring human health. Recent deep learning-based methods have propelled auxiliary prediction tasks concerning biological entity relationship, with most focusing on binary associations (e.g., drug-target protein), while only a few methods offer insights for more complex triple-wise associations. Existing solutions for the association prediction task can be broadly categorized into three types: (1) non-graph methods [10\u201312]; (2) graph-based methods [13\u201316]; (3) hypergraph-based methods [17\u201322]. ", "page_idx": 0}, {"type": "text", "text": "As illustrated in Figure 1, non-graph methods typically concatenate the features of different entities, which are independently mapped by their respective entity encoders, to serve as representations. Graph-based methods adopt nodes and edges of the graph to represent entities and their relationships, and leverage the graph structure for feature propagation, thereby achieving effective representation learning of entity nodes. Similar yet distinct, hypergraph-based methods employ the hypergraph structure to obtain entity representations using the complex feature aggregation strategy. However, none of the aforementioned methods consider the significance of path patterns in the graph structure, which contain a vast amount of crucial information including hidden context and co-occurrence. ", "page_idx": 0}, {"type": "image", "img_path": "LI5KmimXbM/tmp/0411d11adf084a1488fefff08c7712af59a25314509f4eab0ec6b763f566e55d.jpg", "img_caption": ["(a) Non-graph Methods ", "(d) Association Pattern-aware Method "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparisons of feature update strategy among non-graph methods, graph-based methods, hypergraph-based methods, and the proposed association pattern-aware method. Unlike existing methods that map or aggregate node features, the proposed method mines and then fuses association patterns for each target entity node in the graph to enhance the model\u2019s representative ability. ", "page_idx": 1}, {"type": "text", "text": "To this end, we introduce a novel association pattern-aware message propagation strategy as shown in Figure 1(d). The module leverages the potential relationships, such as commonality and diversity, of association patterns as the rule for facilitating message passing among entity features, which can efficiently expand the potential for representing complex interactions within the perspective of both basic graph structure and intrinsic biological mechanisms. Specifically, the related patterns within the graph structure are sampled through the pre-defined distance relation for each entity node. The message passing is driven by the interaction of its assigned patterns, i.e., the feature of the entity node is subsequently updated through feature fusion using adaptive coefficients that capture pattern commonality, generated during the pattern interaction stage. This process is followed by the acquisition of potential common patterns with genuine biological significance for various entities. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel Association Pattern-aware Fusion method for Biological Entity Relationship Prediction, namely Pattern-BERP. First, we devise an association pattern-aware strategy to solve the limitation caused by entity-centric feature mapping and aggregation. The strategy utilizes the association patterns related to each entity node within the graph to extract the common feature based on the attention mechanism for these patterns, thus expanding the ability to represent hidden complex interactions. In addition, to preserve the information interaction of different entities, a hypergraph-based block is incorporated with the association pattern-aware fusion module, thereby enhancing the model ability to capture relationships among various types of entities. Furthermore, to explore low-order associations of biological bind relations, we introduce a bind-relation enhancement module which can reconstruct missing feature of bind-relation entities and thus generate harder negative sample than random selection. Experimental results conducted on different biological datasets show that the proposed method achieves superior performance compared to advanced baselines, demonstrating its effectiveness and robustness in handling various biological entity relationships. More importantly, the obtained association patterns for the relationship of drug, microbe, and disease are quantitatively visualized with the following biological verification in detail. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follow: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel association pattern-aware fusion method for biological entity relationship prediction. The introduce of association pattern-aware strategy can enhance the representation of complex interactions by aggregate features with potential association patterns. \u2022 A bind-relation enhancement module is devised to acquire low-order associations that reveal the biological bind relations, which is essential for reconstructing missing bind-relation entity features and generating challenging negative triplets to enhance the model training. \u2022 Extensive experiments are conducted to verify the superiority of Pattern-BERP, demonstrating its robustness for various biological scenarios. Uniquely compared to the other methods, the interpretability of association patterns is explained to reveal intrinsic biological mechanisms. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we elaborate on the related work from two distinct yet interconnected perspectives: Biological Entity Relationship Prediction and Network Search and Mining. Each perceptive represents a fundamental aspect of our research, addressing specific challenges and methodologies in applying machine learning techniques to the prediction task. ", "page_idx": 2}, {"type": "text", "text": "Biological Entity Relationship Prediction. The latest advancements in artificial intelligence have motivated researchers to employ deep learning methodologies for predicting triple-wise biological entity associations. Hypergraph neural network (HGNN)-based methods [17\u201322] have become the mainstream research direction in this field. Tu et al. [17] proposed a deep hyper-network embedding model to preserve both local and global proximities in the embedding space. Building upon this, Jiang et al. [18] incorporated a dynamic hypergraph construction strategy to capture the hidden and important relations in data structures. Zhang et al. [19] developed a self-attention-based graph neural network applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. Liu et al. [20] proposed a multi-way relation-enhanced hypergraph representation learning method to predict anti-cancer drug synergy. Liu et al. [21] proposed a multi-view contrastive learning-enhanced hypergraph model for drug-microbe-disease association prediction. In addition, Chen and Li [23] attempted to adopt the tensor decomposition strategy to predict which target a drug binds to when administered to a disease, and further proposed a neural tensor network model [24] that seamlessly combines tensor algebra and deep neural networks to effectively capture the complex nonlinear dependencies among drugs, targets, and diseases. ", "page_idx": 2}, {"type": "text", "text": "Network Search and Mining. Network search and mining techniques, particularly those utilizing path information including random walks [25\u201328] and meta-path [29\u201332], have been widely employed to extract local structural information from networks. These methods have found applications in areas such as content recommendation and community detection [33\u201337]. Brin and Page [25] introduced a classic ranking algorithm PageRank to determine the importance of web pages based on their link structure. Jeh and Widom [26] adopted a similarity measure based on pairwise random walk, which can capture the structural similarity between nodes, and further extended PageRank with a personalized version [27]. Perozzi et al. [28] proposed Deepwalk that leverages local random walk information to learn vertex latent representations based on deep learning techniques. These above methods are applicable to homogeneous networks and cannot fully utilize the rich semantic information in heterogeneous networks. To address the limitation, Sun et al. [29] introduced a meta path-based similarity framework for heterogeneous information networks, which can capture the subtle semantics of similarity among objects of the same type. Dong et al. [30] proposed a deep learning-based heterogeneous network representation learning method that automatically learns hidden meta-path semantics, generating general node embedding representations. Wang et al. [31] introduced a graph-based fraud detection method that addresses the issue of low homophily by integrating label information to generate distinguishable neighborhood information. Furthermore within the bioinformatics field, Chen et al. [32] proposed a computational algorithm that performs random walks on an integrated network to infer potential relations between proteins and ADRs. ", "page_idx": 2}, {"type": "text", "text": "In line with the above methods, Pattern-BERP utilizes path information for triple-wise heterogeneous biological network mining. By leveraging the fusion of association patterns, it facilitates message passing among various biological entities, which will be described comprehensively in Section 4.2. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given three distinct entity types in biological networks, termed as $\\mathcal{A}=\\{a_{1},a_{2},\\cdot\\cdot\\cdot,a_{i},\\cdot\\cdot\\cdot,a_{|\\mathcal{A}|}\\}$ , $\\mathcal{B}=\\left\\{b_{1},b_{2},\\cdot\\cdot\\cdot,b_{j},\\cdot\\cdot\\cdot,b_{|\\mathcal{B}|}\\right\\}$ , and $\\mathcal{C}=\\{c_{1},c_{2},\\cdot\\cdot\\cdot,c_{m},\\cdot\\cdot\\cdot,c_{|\\mathcal{C}|}\\}$ , their Cartesian product ${\\boldsymbol{S}}=$ $A\\times B\\times C$ is a set of all possible triple-wise biological entity associations. For simplicity, the relation $A-B-C$ is used to represent complex relational semantics for biological entities, such as \u201cdrug-microbe-disease\u201d, \u201csynergistic drug-drug-cell line\u201d or \u201cdrug-target protein-adverse reaction\u201d. ", "page_idx": 2}, {"type": "text", "text": "For each triplet $(a_{i},b_{j},c_{m})\\in S$ , we assign a label $p\\in\\{0,1\\}$ . A label of $p=1$ indicates that the existence of certain association has been confirmed, while $p=0$ represents an unknown association which denotes that the association is not yet known and could potentially exist. The objective is to develop a credible model that can predict these potential associations from unknown ones. ", "page_idx": 2}, {"type": "image", "img_path": "LI5KmimXbM/tmp/77459cc75aab55fcc04168e8bc259bd22a9a588234725e7589496f143028eb27.jpg", "img_caption": ["Figure 2: Overall framework of Pattern-BERP. First, entity attributes are initialized with different types of bio-encoders. Then, these existing associations are constructed into one hypergraph $\\mathcal{G}$ and two bipartite graphs $\\mathcal{G}_{A,B},\\mathcal{G}_{B,C}$ . After that, the hypergraph is encoded with Association Pattern-aware Fusion module based on the pattern commonality, thereby affecting target entity representation. In addition, the bipartite graphs are encoded to output the missing bind-relation feature and thus generate hard negative samples. Finally, the integrated entity feature are used for final association prediction. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Pattern-BERP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To enhance the ability to represent complex interactions, we propose the first association patternaware method, termed as Pattern-BERP to extract the rich semantic information embedded within the intricate structures of biological networks. As illustrated in Figure 2, the section is divided into four parts: First, these entity relationships are represented in hypergraph and bipartite graph structures for subsequent module input. Next, the association pattern-aware fusion module is proposed to update entity node feature through association pattern-aware interaction. In addition, the bind-relation enhancement module is introduced to reconstruct bind-relation feature and thus generate hard negative samples. Finally, the detailed summary of loss function and complexity analysis is provided. ", "page_idx": 3}, {"type": "text", "text": "4.1 Graph Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the adjacency relationships among entity nodes, the hypergraph and bipartite graph structures are constructed to facilitate the subsequent extraction of structural information and relational association patterns within the respective graphs, which serve for association pattern-aware fusion module and bind-relation enhancement module. ", "page_idx": 3}, {"type": "text", "text": "The attributes of each entity are initialized as node features on the graphs with the domain knowledge of bio-entities through its own type. Finally, the initialized node attributes of $\\mathbf{X}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ consist of features $\\mathbf{X}_{\\mathcal{A}}$ , $\\mathbf{X}_{B}$ and $\\mathbf{X}_{\\mathcal{C}}$ , where $d$ denotes the feature dimensional of initialized entity attributes. Details about entity attribute are provided at Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Hypergraph Construction. Triple-wise biological entity associations can be modeled as a hypergraph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , which includes a vertex set $\\mathcal{V}=\\mathcal{A}\\cup\\mathcal{B}\\cup\\mathcal{C}$ and a hyperedge set $\\mathcal{E}\\subset\\mathcal{S}$ that represents all known associations. Technically, $\\mathcal{G}$ is further formulated as an attributed hypergraph with node attributes $\\mathbf{X}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ and an incidence matrix $\\mathbf{H}\\in\\{0,1\\}^{|\\mathcal{V}|\\times|\\mathcal{E}|}$ , which is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(v,e)={\\binom{1,}{0,}}\\quad{\\mathrm{if~}}v\\in e\\qquad\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Bipartite Graph Construction. The premise of constructing the bind-relation module is to decompose the original triple-wise associations to separately obtain pairwise relations of different types of entities. Since the original triplet is in the form of $(a_{i},b_{j},c_{m})$ , and generally the interaction relationships between the three entities are hierarchical (e.g. drug $a_{i}$ acts on protein $b_{j}$ , and the activated $b_{j}$ then leads to certain adverse reaction $c_{m}$ ). Hence in this paper, we construct two bipartite graphs: $\\mathcal{G}_{A,B_{1}}$ for entity $A\\rightarrow$ entity ${{\\cal B}_{1}}^{2}$ , $\\mathcal{G}_{B_{2},\\mathcal{C}}$ for entity $\\beta_{2}\\rightarrow$ entity $\\mathcal{C}$ . Then the edge sets of the bipartite graphs can be formulated as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{A,B_{1}}=\\{\\left(a,b\\right)\\mid a\\in\\mathcal{A},b\\in\\mathcal{B}_{1},\\exists\\,e\\in\\mathbf{H}\\mathrm{~such~that~}a,b\\in e\\},}\\\\ &{\\mathcal{E}_{B_{2},\\mathcal{C}}=\\{\\left(b,c\\right)\\mid b\\in\\mathcal{B}_{2},c\\in\\mathcal{C},\\exists\\,e\\in\\mathbf{H}\\mathrm{~such~that~}b,c\\in e\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that, the relationship of $A\\to{\\mathcal{C}}$ is not under consideration due to there is no direct connection in this context. The constructed bipartite graphs $\\mathcal{G}_{A,B_{1}},\\mathcal{G}_{B_{2},C}$ serve as input for bind-relation module. ", "page_idx": 4}, {"type": "text", "text": "4.2 Association Pattern-aware Fusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To comprehensively account for the feature interactions within association patterns, the proposed Association Pattern-aware Fusion (APF) method comprises two fundamental components: Firstly, Association Pattern Sampling (APS) block is designed to sample association patterns by utilizing the distance tokens relevant to target nodes. Secondly, Association Pattern-aware Interaction (API) block is introduced to update node features by message interaction within the sampled association patterns and mine the intrinsic pattern commonality with biological support. ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Association Pattern Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 1. Given certain entity node, the distance token between the node and one hyperedge or association pattern is defined as $u$ -hop. Here, $^{\\,l}$ -hop patterns represent the hyperedges directly covering the node, 2-hop patterns represent the hyperedges directly covering all the $^{\\,l}$ -hop neighbor nodes of the node, and the relation continues for higher hop counts. If the node is unreachable when $u$ reaches the max step $U$ , we define these patterns as no-relation and set the distance to $-\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Based on the above definition3, we generate the distance tokens between each node and all hyperedges, ultimately obtaining a distance matrix $\\mathbf{D}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{E}|}$ , defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(v,e)={\\left\\{\\begin{array}{l l}{u,}&{{\\mathrm{if~node~}}v{\\mathrm{~is~}}u{\\mathrm{-}}h o p{\\mathrm{~away~from~pattern~}}e}\\\\ {-\\infty,}&{{\\mathrm{if~node~}}v{\\mathrm{~is~unreachable~from~pattern~}}e}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d(v,e)$ represents the defined distance from node $v$ to pattern $e;\\,u$ indicates the number of hops from node $v$ to pattern $e$ . ", "page_idx": 4}, {"type": "text", "text": "To represent an entity node with association patterns, the general principle is to prioritize and retain patterns that are closer for each node based on the distance tokens, with a total of $N$ patterns sampled. Formally, given an entity node $v$ , let $\\mathbf{D}_{v}\\,\\in\\,\\mathbb{R}^{N}$ and ${\\bf P}_{v}\\,\\in\\,\\mathbb{R}^{N\\times3d}$ represent the distance tokens and feature vectors of the selected sampled patterns, where each sampled pattern consists of three entity nodes and the $d$ -dimensional feature embeddings of each node are defined within the initial node embeddings $\\mathbf{X}$ . Then, the output pattern feature $z_{v}$ for node $v$ considers the relative position of these related patterns, which is thus formulated as $\\mathbf{z}_{v}=\\mathbf{P}_{v}+\\mathrm{POS}(\\mathbf{D}_{v})$ , where ${\\mathrm{POS}}(\\cdot)$ denotes the position encoding layer that maps from $\\mathbb{R}^{N}\\,\\rightarrow\\,\\mathbb{R}^{N\\times(3d)}$ . Finally, the integrated embedding $\\mathbf{z}_{v}\\in\\mathbb{R}^{N\\times3d}$ with distance information is produced for use in the subsequent API block. ", "page_idx": 4}, {"type": "text", "text": "4.2.2 Association Pattern-aware Interaction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before put into the API block, we adopt a hypergraph convolution layer implemented by [40] on the constructed hypergraph $\\mathcal{G}$ to facilitate neighbor-based information propagation among different entities. Then the updated node feature $\\mathbf{X}^{*}$ after hypergraph convolutions is used to construct the feature of patterns mentioned in the APS block, thereby yielding $\\mathbf{z}_{v}$ for the information interaction of these sampled patterns. Details of hypergraph convolution are provided at Appendix. ", "page_idx": 4}, {"type": "text", "text": "Pattern Interaction. The API block is designed to search for and extract commonalities among different sampled patterns related to a specific entity node, and thus consists of a composition of Transformer layers [38]. Each Transformer layer has two modules: a multi-head self-attention mechanism (MHA) and a position-wise feed-forward network (FFN). For simplicity, we consider the single-head setting, and the extension to multi-head attention is standard and straightforward. Specifically, let $\\mathbf{Z}^{(0)}=[\\mathbf{z}_{1}^{(0)},...\\,,\\mathbf{z}_{|\\mathcal{V}|}^{(0)}]\\in\\mathbb{R}^{|\\mathcal{V}|\\times N\\times(3d)}$ denotes the input of the self-attention module where $\\mathbf{z}_{v}\\in\\mathbb{R}^{N\\times(3d)}$ is the representation for node $v$ . The input $\\mathbf{z}_{v}$ is projected by three matrices $\\mathbf{W}_{Q}\\in\\mathbb{R}^{(3d)\\times d_{K}}$ , $\\mathbf{W}_{K}\\in\\mathbb{R}^{(3\\bar{d})\\times d_{K}}$ , and $\\mathbf{W}_{V}\\in\\mathbb{R}^{(3d)\\times d_{V}}$ to obtain the corresponding query, key, and value representations $\\mathbf{Q}_{v},\\mathbf{K}_{v}$ , and $\\mathbf{V}_{v}$ for node $v$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{v}=\\mathbf{z}_{v}\\mathbf{W}_{Q},\\mathbf{K}_{v}=\\mathbf{z}_{v}\\mathbf{W}_{K},\\mathbf{V}_{v}=\\mathbf{z}_{v}\\mathbf{W}_{V},\\mathbf{A}_{v}=\\frac{\\mathbf{Q}_{v}\\mathbf{K}_{v}^{\\top}}{\\sqrt{d_{K}}},\\mathrm{Attn}(\\mathbf{z}_{v})=\\mathrm{softmax}(\\mathbf{A}_{v})\\mathbf{V}_{v},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf A}_{v}\\in\\mathbb{R}^{N\\times N}$ is a matrix capturing the similarity between queries and keys; $d_{K}$ , $d_{V}$ denotes the feature dimensional of the key representations $\\mathbf{K}_{v}$ and the value representations ${\\mathbf{V}_{v}}^{4}$ . Then we will get the output of the self-attention module $\\mathbf{z}_{v}^{\\prime}\\,\\in\\,\\mathbb{R}^{N\\times(3d)}$ . To summarize the process of transformer layer, the output of association pattern-aware attention block is computed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}^{\\prime}(l)=\\mathrm{MHA}(\\mathrm{LN}(\\mathbf{Z}^{(l-1)}))+\\mathbf{Z}^{(l-1)},\\mathbf{Z}^{(l)}=\\mathrm{FFN}(\\mathrm{LN}(\\mathbf{Z}^{\\prime}(l)))+\\mathbf{Z}^{\\prime}(l),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where LN denotes layer normalization, and $\\mathbf{Z}^{(l)}$ is the output of the current transformer layer. After $L_{1}$ -layer transformers, we get the encoding output $\\mathbf{Z}^{(L_{1})}\\dot{=}[\\mathbf{z}_{1}^{(L_{1})},\\dots,\\mathbf{z}_{|\\mathcal{V}|}^{(L_{1})}]\\in\\mathbb{R}^{|\\mathcal{V}|\\times N\\dot{\\times}(3d)}$ , and then apply the mean function to similar entities in $N$ association patterns for each node $v$ to obtain the learned embedding $\\mathbf{z}_{v}\\in\\mathbb{R}^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "Pattern Commonality. To mine the commonality of $N$ sampled patterns, a score is defined to represent the quantitative relation of these association patterns, termed by Pattern Commonality Coefficient, based on the attention scores ${\\bf A}_{v}$ of trained API block, which is formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{A}}_{v}=\\operatorname{softmax}(\\mathbf{A}_{v}),\\mathbf{C}_{v}=\\frac{1}{N}\\sum_{n=1}^{N}\\tilde{\\mathbf{A}}_{v}[n,:],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf C}_{v}\\in\\mathbb{R}^{N}$ and ${\\bf C}_{v}[n]\\in(0,1)$ indicates the commonality coefficient of the $n$ -th pattern. Patterns with relatively high commonality coefficients tend to share the same or similar pathways, while showing significant differences in response compared to low commonality patterns. Corresponding biological validations are presented in the Section 5.3. ", "page_idx": 5}, {"type": "text", "text": "4.3 Bind-relation Enhancement ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the context of multi-entity relationships, there often exist strong pairwise bind relation between entities, such as drug Aspirin to treat common cold [41, 42]. To mitigate the weakening or overlooking of low-order bind relation, a Bind-relation Enhancement (BE) module is designed to effectively reconstruct the missing feature by capturing these important pairwise associations. Additionally, the module can generate confident and challenging negative samples to aid the training. ", "page_idx": 5}, {"type": "text", "text": "Bind-relation Feature Reconstruction. To efficiently learn entity representations in pairwise bind relations, we introduce an edge prediction classification task on the bipartite graphs $\\mathcal{E}_{A,B},\\mathcal{E}_{B,C}$ with the initial embeddings $\\mathbf{X}_{\\mathcal{A}}^{(0)}=\\mathbf{X}_{\\mathcal{A}},\\mathbf{X}_{\\mathcal{B}_{1}}^{(0)}=\\mathbf{X}_{\\mathcal{B}_{2}}^{(0)}=\\mathbf{X}_{\\mathcal{B}},\\mathbf{X}_{\\mathcal{C}}^{(0)}=\\mathbf{X}_{\\mathcal{C}}$ . Specifically, a $L_{2}$ -layer selfsupervised BGNN model [43] is employed to learn node features on the bipartite graphs, followed by the Multi-layer Perception (MLP) [10] layer to output association probabilities, which is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf X}_{\\boldsymbol{A}}^{(l+1)},{\\bf X}_{\\boldsymbol{B}_{1}}^{(l+1)}=\\mathrm{BGNN}({\\bf X}_{\\boldsymbol{A}}^{(l)},{\\bf X}_{\\boldsymbol{B}_{1}}^{(l)},\\mathcal{G}_{\\boldsymbol{A},\\boldsymbol{B}_{1}}),{\\bf X}_{\\boldsymbol{B}_{2}}^{(l+1)},{\\bf X}_{\\boldsymbol{C}}^{(l+1)}=\\mathrm{BGNN}({\\bf X}_{\\boldsymbol{B}_{2}}^{(l)},{\\bf X}_{\\boldsymbol{C}}^{(l)},\\mathcal{G}_{\\boldsymbol{B}_{2},\\boldsymbol{C}}),}\\\\ {\\hat{p}_{(a,b_{1})}=\\mathrm{MLP}({\\bf x}_{a}^{(L_{2})}\\parallel{\\bf x}_{b_{1}}^{(L_{2})}),\\hat{p}_{(b_{2},c)}=\\mathrm{MLP}({\\bf x}_{b_{2}}^{(L_{2})}\\parallel{\\bf x}_{c}^{(L_{2})}),~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{X}_{\\mathcal{A}}^{(l)},\\mathbf{X}_{\\mathcal{B}_{1}}^{(l)},\\mathbf{X}_{\\mathcal{B}_{2}}^{(l)},\\mathbf{X}_{\\mathcal{C}}^{(l)}$ represent the res at layer $l$ $;\\mathbf{x}_{a}^{(L_{2})},\\mathbf{x}_{b_{1}}^{(L_{2})},\\mathbf{x}_{b_{2}}^{(L_{2})},\\mathbf{x}_{c}^{(L_{2})}$ denote the final learned representations of entity $a,b_{1},b_{2},c$ respectively; $\\hat{p}_{(a,b_{1})},\\,\\hat{p}_{(b_{2},c)}$ represent the estimated probability of association between entities $a,b_{1}$ and entities $b_{2},c$ respectively. After that, the loss of the supervised prediction task can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A,B_{1}}/\\mathcal{L}_{B_{2},\\mathcal{C}}=-\\frac{1}{\\vert\\mathcal{E}_{A,B_{1}}\\vert/\\vert\\mathcal{E}_{B_{2},\\mathcal{C}}\\vert}\\sum_{e\\in\\mathcal{E}_{A,B_{1}}/\\mathcal{E}_{B_{2},\\mathcal{C}}}\\left(p_{e}\\log\\hat{p}_{e}+(1-p_{e})\\log\\left(1-\\hat{p}_{e}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The loss $\\mathcal{L}_{B E}$ of bind-relation task is defined as $\\mathcal{L}_{B E}=\\alpha\\mathcal{L}_{\\mathcal{A},\\mathcal{B}_{1}}+(1-\\alpha)\\mathcal{L}_{\\mathcal{B}_{2},\\mathcal{C}}$ , where $\\alpha$ is the balancing coefficient for the two losses. ", "page_idx": 5}, {"type": "text", "text": "Hard Negative Sampling. Based on the above bind-relation task, negative samples are adaptively generated for triple-wise associations, instead of randomly selecting from the vast sample space. Moreover, the generated negative samples are challenging, which contributes to efficient learning. As illustrated in Figure 2, three kinds of negative samples are considered as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\otimes}=\\{(a,b^{\\prime},c)\\mid(p_{(a,b^{\\prime})}<\\gamma)\\lor(p_{(b^{\\prime},c)}<\\gamma)\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $b^{\\prime}\\in\\mathfrak{B}$ represents another entity with random selection that differs from entity $b$ in the original triplet to form the negative sample $(\\bar{a},\\bar{b}^{\\prime},c)$ ; $\\mathcal{E}_{\\otimes}$ is the set of generated negative triplets; $\\gamma$ represents the threshold for the prediction probability to determine whether the association exists. ", "page_idx": 6}, {"type": "text", "text": "4.4 Total Loss ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Base on the above modules, $\\mathbf{z}_{v}$ of node $v$ learned by the APF module is updated with reconstructed features $\\mathbf{x}_{v}$ from the BE module according to the entity type, thereby generate the enhanced embedding of $v$ with $\\mathbf{z}_{v}^{\\ast}=\\mathbf{z}_{v}+\\mathbf{x}_{v}$ for the association predictor network. Hence, for the triple-wise association prediction, we utilize the learned embeddings $\\mathbf{z}_{a}^{\\ast}$ , $\\mathbf{z}_{b}^{\\ast}$ , and $\\mathbf{z}_{c}^{\\ast}$ of entity $a,b,c$ to output the probability of the association $\\hat{p}$ through a scoring function $\\bar{\\hat{p}}_{(a,b,c)}=\\mathrm{MLP}(\\mathbf{z}_{a}^{*}\\parallel\\mathbf{z}_{b}^{*}\\parallel\\mathbf{z}_{c}^{*})$ . The loss of the association prediction task for true sample set $\\mathcal{E}$ and negative sample set $\\mathcal{E}_{\\otimes}$ can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A P F}=-\\frac{1}{\\left|\\mathcal{E}\\cup\\mathcal{E}_{\\otimes}\\right|}\\sum_{e\\in(\\mathcal{E}\\cup\\mathcal{E}_{\\otimes})}\\left(p_{e}\\log\\hat{p}_{e}+(1-p_{e})\\log\\left(1-\\hat{p}_{e}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Hence, the total loss $\\mathcal{L}$ is computed through an alternating training strategy of the two modules, where the BE module is trained for the first 4 epochs of every 5-epoch cycle, followed by the APF module, which is trained for the final epoch of each cycle. During the training of each module, the parameters of the other one are frozen. The equation for $\\mathcal{L}$ is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(o)=\\mathcal{L}_{B E}(o)\\cdot(1-\\left\\lfloor\\frac{o\\mod5}{4}\\right\\rfloor)+\\mathcal{L}_{A P F}(o)\\cdot\\left\\lfloor\\frac{o\\mod5}{4}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $o$ represents the current epoch; $\\lfloor\\cdot\\rfloor$ denotes floor function; mod denotes modulo operation. ", "page_idx": 6}, {"type": "text", "text": "4.5 Complexity Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Considering the significantly higher complexity of the APF module in comparison to other network components, we only consider APF which aggregates multiple patterns across various entity nodes. Specifically, for one entity node with $N$ sampled patterns from the APS module, the input and hidden features in the MHA layers are of dimension $f_{M}$ , and hidden features in the FFN layers are $f_{F}$ . In APF, the query, key, and value matrices are derived from the same input sequence and share length $N$ . The primary operations for APF include scaled dot-product attention, multiplication of attention weights and values, MHA linear transformation, and FFN linear projection. The time complexity is expressed as $\\mathcal{O}(N^{2}\\cdot f_{M}+N\\cdot f_{M}^{2}+N\\cdot f_{M}\\cdot f_{F})$ . Hence, for the entire graph, the total complexity is $\\mathcal{O}\\left(|\\mathcal{V}|\\cdot\\left(N^{2}\\cdot f_{M}+N\\cdot f_{M}^{2}+N\\cdot f_{M}\\cdot f_{F}\\right)\\right)$ , where $\\vert\\nu\\vert$ is the number of entity nodes. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. In this paper, we adopt three biological entity association datasets with significant biological meaning, namely DMD (Drug-Microbe-Disease), DDC (synergistic Drug-Drug-Cell line) and DPA (Drug-target Protein-Adverse reaction), among which DPA dataset is directly constructed. In line with DMD and DDC, we utilize preprocessing tools provided by [21] to deal with the original data from ADReCS-Target [44], and collect a total of 1,079 triplets that are structured into the data schema <drug, protein, adr>. Appendix Table 3 presents the statistics and characteristics of these datasets. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To verify the effectiveness of Pattern-BERP, we compare it with three kinds of methods: (1) Non-graph methods. Following the work from [21], five non-graph methods are adopted including Random Forest (RF) [45], MLP [10], CP [11], Tucker [11], CoSTCo [12]; (2) Graph-based methods. We select four classical architectures of graph neural network, including GCN [13], GraphSAGE [14], GAT [15], GIN [16]; (3) Hypergraph-based methods. Recent hypergraph learning methods to address triple-wise biological entities associations or similar tasks are considered as the baselines, including DHNE [17], HyperSAGNN [19], HGSynergy [20], MCHNN [21]. ", "page_idx": 6}, {"type": "table", "img_path": "LI5KmimXbM/tmp/d718c0530e0d4df36bd31dcc16ed903ccc61ceaf30fd6562a6dd3bd1111ea181.jpg", "table_caption": ["Table 1: Performance comparison on three datasets of different biological entity associations. Each result of these methods is from the average of 5-fold cross-validation experiments with four scenarios. The best result for each dataset and metric is marked in bold. All the presented hits scores are in $\\%$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Implementation Details. To accurately evaluate model performance and prevent overftiting, 5-fold cross-validation is used to evaluate the performance. Specifically, we randomly split the dataset into a $90\\%$ cross-validation set and a $10\\%$ independent test set. On the cross-validation set, the 5-fold cross-validation is implemented. Moreover, the independent testing, in which the model is trained on the cross-validation set and tested on the independent test set, is conducted to obtain the prediction results. In the training stage, Binary Cross Entropy loss is adopted to measure model performance and Adam optimizer is adopted to optimize all of model parameters with a learning rate of $0.001^{5}$ . ", "page_idx": 7}, {"type": "text", "text": "Evaluations. To evaluate the prediction performance on the triplet associations, hit ratio $\\mathrm{(hit}@\\mathrm{n})$ and normalized discounted cumulative gain $({\\tt n d c g}@{\\tt n})$ , which are widely used in recommendation tasks [46, 47], are employed for model ability to provide a comprehensive assessment for the task. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance Comparison with Advanced Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 summarizes the prediction performances of Pattern-BERP in comparison with other baselines across the DMD, DDC and DPA datasets. It is evident that Pattern-BERP significantly surpasses the previous state-of-the-art baselines, including non-graph, graph-based, and hypergraph-based methods, across all three datasets by a large margin, with a particularly notable hit $@1$ improvement of approximately $23.6\\%$ on the DPA dataset (from 33.24 to 43.52). The results underscore the broad accuracy and applicability of the proposed method in various biological association scenarios. ", "page_idx": 7}, {"type": "text", "text": "A salient observation is that hypergraph-based methods achieve superior performance than those of the other two types. This phenomenon empirically demonstrates the advantages of utilizing high-order structure information over the other methods that we compare. Additionally, we observe that graphbased methods exceed non-graph methods for DMD and DDC datasets. The relatively large number of associations and proportion of DMD and DDC datasets, as shown in Appendix Table 3, indicate a higher density and stronger interconnectivity within the underlying entity relationships. Hence, graph-based methods acquire abundant information of entity interactions based on the graph structure. Furthermore, hypergraph-based methods acquire high-order structure information, ultimately leading to better performance. In contrast, for the DPA dataset, non-graph methods outperform graph-based methods, even demonstrating competitive performance compared to hypergraph-based methods. This can be attributed to the fact that the association proportion within the DPA dataset is conspicuously low (around $0.002\\%$ ), indicating a relatively sparse graph structure. Under such conditions, the non-graph methods are able to obtain more robust association information compared to graph-based models, which may struggle to capture meaningful patterns from the limited graph connectivity. ", "page_idx": 7}, {"type": "image", "img_path": "LI5KmimXbM/tmp/778ea3ffd226a79972204c39d813dea042ca577f8500a230c5c56c6dc5926dad.jpg", "img_caption": ["#207 S-(2-Thienyl)-L-cysteine #100 Diphenyl Disulfide  #209 Salicylic Acid #78 Chrysophanic Acid #160 Moxifloxacin "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: The interpretability cases of $N{=}100$ association patterns related to drug #53 and $\\#207$ in DMD dataset. The pattern commonality coefficients are represented in the form of a percentage to indicate the contribution for visualization, with each pattern typically assigned a default value of $1\\%$ . Larger pattern commonality coefficients indicate a more significant contribution to the target drugs, and these patterns frequently exhibit similar or even identical biological pathways. Conversely, smaller commonality coefficients suggest a lack of relevance to these drugs. ", "page_idx": 8}, {"type": "text", "text": "5.3 Association Pattern Interpretability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In order to investigate the potential relation among different association patterns, we visualize the pattern commonality coefficients of Cefadroxil (drug #53) and S-(2-Thienyl)-L-cysteine (drug #207) in DMD dataset, as shown in Figure 3. Larger pattern commonality coefficients contribute more to the original drug pathway, whereas smaller coefficients often relate to different biological mechanisms. It can be observed that: ", "page_idx": 8}, {"type": "text", "text": "\u2022 For Cefadroxil (drug #53), the common patterns consisted by Chlorhexidine acetate (drug #76) [48], Hetacillin (drug #132) [49], and Econazole (drug #102) [50, 51] are considered more similar as the mechanism [52] that actives on the cell wall and envelope leading the change of microbe physiological activities, thus treating the diseases. Instead, Tobramycin (drug #227) inhibits mRNA be translated into protein and thus promotes microbe cell death [53, 54]. \u2022 For S-(2-Thienyl)-L-cysteine (drug $\\#207\\$ ), the common patterns consisted by Diphenyl Disulfide (drug #100) [55, 56], Salicylic Acid (drug $\\#209$ ) [57], and Chrysophanic Acid (drug #78) [58, 59] are considered more similar as the mechanism that [60\u201362] inducts oxidative stress in bacteria and thus damage all components of the microbe cell. Instead, Moxifloxacin (drug #160) inhibits DNA gyrase and topoisomerase IV enzymes to separate bacterial DNA, thereby inhibiting cell replication [63, 64]. ", "page_idx": 8}, {"type": "text", "text": "In summary, the analyzed cases illustrate the high-commonality patterns derived from Pattern-BERP, demonstrating how these small molecule drugs influence microbial activities to treat diseases, generally following a consistent physiological route. These cases indicate that our method can discover potential common patterns through the API block, and obtain larger pattern coefficients through the attention mechanism, which drives the nodes to learn these important common interactions and acquire more expressive representations. Details about additional cases can be found at Appendix B.3.2. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Ablation study results on DDC dataset with different module designs. \u201cBFR\u201d denotes Bindrelation Feature Reconstruction; \u201cHNS\u201d denotes Hard Negative Sampling; \u201cHC\u201d denotes Hypergraph Convolution block; \u201cDE\u201d denotes Distance Embedding; \u201cAPI\u201d denotes Association Pattern-aware Interaction block. All the presented scores are in $\\%$ , and the best result is marked in bold. ", "page_idx": 9}, {"type": "table", "img_path": "LI5KmimXbM/tmp/09e6171ffe9d1d5a8ae865bd6078d5dd01c4e28dc91435442ad075fd34e0ed05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To investigate the necessity of each component in Pattern-BERP, we conduct several comparisons between Pattern-BERP and its variants on the test set. As illustrated in Table 2, when basic components of Pattern-BERP have been removed, the performances of corresponding variants on DDC dataset significantly decline, indicating that these components all contribute to the performance. Besides, we have other observations: (1) when only DE module is removed, the performance is inferior to the one removing the entire APA module containing DE, demonstrating that inaccurate entity distance information has a more detrimental impact on the prediction performance; (2) eliminating HNS module results in a significant drop in the performance, highlighting its crucial role in enhancing the model\u2019s robustness and discrimination capability and thus indicating that the proposed negative sampling strategy contributes to efficient learning; (3) when removing HC module leads to a slight performance degradation, the impact is relatively limited which suggests that the capability of HC module in representing complex associations is relatively modest for the highly dense-association DDC dataset, but it still provides some beneficial effects towards the final performance improvement. ", "page_idx": 9}, {"type": "text", "text": "To verify the affect of hyperparameter settings to model performance, we first conduct ablation experiments on the three main parameters of the APF module, namely number of attention heads, number of max pattern distance, and number of sampled association patterns. As illustrated in Appendix Figure 5, when the three hyperparameters are increased, the prediction performance on DDC dataset exhibits an overall upward trend, suggesting that increasing these hyperparameters helps the model better capture the complex association patterns, thereby improving the final performance. Additionally, we conduct experiments on the loss-balanced coefficient $\\alpha$ and the bind-relation prediction probability threshold $\\gamma$ , both adjusted from 0.1 to 0.9. Results in Appendix Figure 6 show that setting $\\alpha$ and $\\gamma$ to 0.5 yields the best performance. Specifically, for $\\alpha$ , since the final prediction task involves predicting the associations among entity $A,B$ , and $\\mathcal{C}$ , the two tasks of $A\\rightarrow B$ and ${\\mathcal{B}}\\to{\\mathcal{C}}$ are intuitively of equal importance, therefore the balanced coefficient $\\alpha$ set to 0.5; for $\\gamma$ , bind-relation prediction is fundamentally a binary classification task, thus the threshold $\\gamma$ is set to 0.5. Details about ablation experiments are presented at Appendix B.3.3. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a novel association pattern-aware fusion method Pattern-BERP for biological entity relationship prediction, which effectively combines the related association pattern information into entity representation learning. In addition, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. The evaluation on three biological datasets quantitatively demonstrate that the proposed method consistently achieve superior performance over the competing baselines. Moreover, the interpretability explanations of association patterns reveal the intrinsic biological mechanisms and thus promote the method to be deployed in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Due to the domain-specific task, Pattern-BERP focuses on the fixed-length association patterns. Extending the approach to capture variable-length pathways could further enhance the representational power. Additionally, exploring the applicability of the method in other domains beyond biology, such as general knowledge graph completion, would help evaluate its broader generalizability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Zhejiang Province High-Level Talents Special Support Program \u201cLeading Talent of Technological Innovation of Ten-Thousands Talents Program\u201d (No. 2022R52046), the Fundamental Research Funds for the Central Universities (226-2024-00145), and the Scientific Research Fund of Zhejiang University (No. XY2023020). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shengbo Wu, Shujuan Yang, Manman Wang, Nan Song, Jie Feng, Hao Wu, Aidong Yang, Chunjiang Liu, Yanni Li, Fei Guo, et al. Quorum sensing-based interactions among drugs, microbes, and diseases. Science China Life Sciences, 66(1):137\u2013151, 2023.   \n[2] Lei Wang, Yaqin Tan, Xiaoyu Yang, Linai Kuang, and Pengyao Ping. Review on predicting pairwise relationships between human microbes, drugs and diseases: from biological data to computational models. Briefings in Bioinformatics, 23(3):bbac080, 2022.   \n[3] Jia Jia, Feng Zhu, Xiaohua Ma, Zhiwei W Cao, Yixue X Li, and Yu Zong Chen. Mechanisms of drug combinations: interaction and network perspectives. Nature reviews Drug discovery, 8 (2):111\u2013128, 2009.   \n[4] Mathew J Garnett, Elena J Edelman, Sonja J Heidorn, Chris D Greenman, Anahita Dastur, King Wai Lau, Patricia Greninger, I Richard Thompson, Xi Luo, Jorge Soares, et al. Systematic identification of genomic markers of drug sensitivity in cancer cells. Nature, 483(7391):570\u2013575, 2012.   \n[5] Lun Yang, Jian Chen, and Lin He. Harvesting candidate genes responsible for serious adverse drug reactions from a chemical-protein interactome. PLoS computational biology, 5(7): e1000441, 2009.   \n[6] Lun Yang, Kejian Wang, Jian Chen, Anil G Jegga, Heng Luo, Leming Shi, Chunling Wan, Xizhi Guo, Shengying Qin, Guang He, et al. Exploring off-targets and off-systems for adverse drug reactions via chemical-protein interactome\u2014clozapine-induced agranulocytosis as a case study. PLoS computational biology, 7(3):e1002016, 2011.   \n[7] Anna Gogleva, Dimitris Polychronopoulos, Matthias Pfeifer, Vladimir Poroshin, Micha\u00ebl Ughetto, Matthew J Martin, Hannah Thorpe, Aurelie Bornot, Paul D Smith, Ben Sidders, et al. Knowledge graph-based recommendation framework identifies drivers of resistance in egfr mutant non-small cell lung cancer. Nature communications, 13(1):1667, 2022.   \n[8] Yiyuan Pu, Daniel Beck, and Karin Verspoor. Graph embedding-based link prediction for literature-based discovery in alzheimer\u2019s disease. Journal of Biomedical Informatics, 145: 104464, 2023.   \n[9] Ali Daowd, Samina Abidi, and Syed Sibte Raza Abidi. A knowledge graph completion method applied to literature-based discovery for predicting missing links targeting cancer drug repurposing. In International Conference on Artificial Intelligence in Medicine, pages 24\u201334. Springer, 2022.   \n[10] Fionn Murtagh. Multilayer perceptrons for classification and regression. Neurocomputing, 2(5): 183\u2013197, 1991. ISSN 0925-2312.   \n[11] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51 (3):455\u2013500, 2009.   \n[12] Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. Costco: A neural tensor completion model for sparse tensors. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining(KDD), pages 324\u2013334, 2019.   \n[13] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations(ICLR), 2016.   \n[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems(NeurIPS), 30, 2017.   \n[15] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations(ICLR), 2018.   \n[16] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations(ICLR), 2019.   \n[17] Ke Tu, Peng Cui, Xiao Wang, Fei Wang, and Wenwu Zhu. Structural deep embedding for hyper-networks. In Proceedings of the AAAI conference on artificial intelligence(AAAI), pages 426\u2013433, 2018.   \n[18] Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. Dynamic hypergraph neural networks. In Proceedings of International Joint Conferences on Artificial Intelligence(IJCAI), 2019.   \n[19] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for hypergraphs. In International Conference on Learning Representations (ICLR), 2020.   \n[20] Xuan Liu, Congzhi Song, Shichao Liu, Menglu Li, Xionghui Zhou, and Wen Zhang. Multi-way relation-enhanced hypergraph representation learning for anti-cancer drug synergy prediction. Bioinformatics, 38(20):4782\u20134789, 2022.   \n[21] Luotao Liu, Feng Huang, Xuan Liu, Zhankun Xiong, Menglu Li, Congzhi Song, and Wen Zhang. Multi-view contrastive learning hypergraph neural network for drug-microbe-disease association prediction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence(IJCAI), pages 4829\u20134837, 8 2023.   \n[22] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence(AAAI), pages 3558\u20133565, 2019.   \n[23] Huiyuan Chen and Jing Li. Modeling relational drug-target-disease interactions via tensor factorization with multiple web sources. In Proceedings of the 28th international conference on World Wide Web(WWW), pages 218\u2013227, 2019.   \n[24] Huiyuan Chen and Jing Li. Learning data-driven drug-target-disease interaction via neural tensor network. In International joint conference on artificial intelligence (IJCAI), 2020.   \n[25] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107\u2013117, 1998.   \n[26] Glen Jeh and Jennifer Widom. Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining(KDD), pages 538\u2013543, 2002.   \n[27] Glen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th international conference on World Wide Web(WWW), pages 271\u2013279, 2003.   \n[28] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining(KDD), pages 701\u2013710, 2014.   \n[29] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. Proceedings of the VLDB Endowment, 4(11):992\u20131003, 2011.   \n[30] Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining(KDD), pages 135\u2013144, 2017.   \n[31] Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng Ma, Yu Sun, Dianhai Yu, Fang Dong, Jiahui Jin, et al. Label information enhanced fraud detection against low homophily in graphs. In Proceedings of the ACM Web Conference 2023(WWW), pages 406\u2013416, 2023.   \n[32] Xiaowen Chen, Hongbo Shi, Feng Yang, Lei Yang, Yingli Lv, Shuyuan Wang, Enyu Dai, Dianjun Sun, and Wei Jiang. Large-scale identification of adverse drug reaction-related proteins through a random walk model. Scientific reports, 6(1):36325, 2016.   \n[33] Francois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on knowledge and data engineering(TKDE), 19(3):355\u2013369, 2007.   \n[34] Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science(FOCS), pages 475\u2013486. IEEE, 2006.   \n[35] Zihao Zhao, Jiawei Chen, Sheng Zhou, Xiangnan He, Xuezhi Cao, Fuzheng Zhang, and Wei Wu. Popularity bias is not always evil: Disentangling benign and harmful bias for recommendation. IEEE Trans actions on Knowledge and Data Engineering, 35(10):9920\u20139931, 2022.   \n[36] Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, and Xiangnan He. Adap- $\\tau$ : Adaptively modulating embedding magnitude for recommendation. In Proceedings of the ACM Web Conference 2023, pages 1085\u20131096, 2023.   \n[37] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems, 41(3):1\u201339, 2023.   \n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems(NeurIPS), 30, 2017.   \n[39] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[40] Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+: General hypergraph neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 45(3):3181\u20133199, 2022.   \n[41] Ronald Eccles. Efficacy and safety of over-the-counter analgesics in the treatment of common cold and flu. Journal of clinical pharmacy and therapeutics, 31(4):309\u2013319, 2006.   \n[42] Michael D Kogan, Gregory Pappas, M Yu Stella, and Milton Kotelchuck. Over-the-counter medication use among us preschool-age children. Jama, 272(13):1025\u20131030, 1994.   \n[43] Chaoyang He, Tian Xie, Yu Rong, Wenbing Huang, Junzhou Huang, Xiang Ren, and Cyrus Shahabi. Cascade-bgnn: Toward efficient self-supervised representation learning on large-scale bipartite graphs. arXiv preprint arXiv:1906.11994, 2019.   \n[44] Li-Hong Huang, Qiu-Shun He, Ke Liu, Jiao Cheng, Min-Dong Zhong, Lin-Shan Chen, LiXia Yao, and Zhi-Liang Ji. Adrecs-target: target profiles for aiding drug safety research and application. Nucleic acids research, 46(D1):D911\u2013D917, 2018.   \n[45] Leo Breiman. Random forests. Machine learning, 45:5\u201332, 2001.   \n[46] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. Improving sequential recommendation with knowledge-enhanced memory networks. In The 41st international ACM SIGIR conference on research & development in information retrieval(SIGIR), pages 505\u2013514, 2018.   \n[47] Jin Huang, Zhaochun Ren, Wayne Xin Zhao, Gaole He, Ji-Rong Wen, and Daxiang Dong. Taxonomy-aware multi-hop reasoning networks for sequential recommendation. In Proceedings of the twelfth ACM international conference on web search and data mining(WSDM), pages 573\u2013581, 2019.   \n[48] Jerrold B. Leikin and Frank P. Paloucek. Chlorhexidine gluconate. In Jerrold B. Leikin and Frank P. Paloucek, editors, Poisoning and Toxicology Handbook, pages 183\u2013184. Informa, 4th edition, 2008.   \n[49] Jed F Fisher, Samy O Meroueh, and Shahriar Mobashery. Bacterial resistance to $\\beta$ -lactam antibiotics: compelling opportunism, compelling opportunity. Chemical reviews, 105(2):395\u2013 424, 2005.   \n[50] D Thienpont, J Van Cutsem, JM Van Nueten, CJ Niemegeers, and R Marsboom. Bilogical and toxicological properties of econazole, a broad-spectrum antimycotic. Arzneimittel-forschung, 25(2):224\u2013230, 1975.   \n[51] Matthew C Fisher, Ana Alastruey-Izquierdo, Judith Berman, Tihana Bicanic, Elaine M Bignell, Paul Bowyer, Michael Bromley, Roger Br\u00fcggemann, Gary Garber, Oliver A Cornely, et al. Tackling the emerging threat of antifungal resistance to human health. Nature reviews microbiology, 20(9):557\u2013571, 2022.   \n[52] Donald J Tipper and Jack L Strominger. Mechanism of action of penicillins: a proposal based on their structural similarity to acyl-d-alanyl-d-alanine. Proceedings of the National Academy of Sciences, 54(4):1133\u20131141, 1965.   \n[53] Grace Yang, Joanna Trylska, Yitzhak Tor, and J Andrew McCammon. Binding of aminoglycosidic antibiotics to the oligonucleotide a-site model and 30s ribosomal subunit: Poissonboltzmann model, thermal denaturation, and fluorescence studies. Journal of medicinal chemistry, 49(18):5478\u20135490, 2006.   \n[54] Jalal Haddad, Lakshmi P Kotra, Beatriz Llano-Sotelo, Choonkeun Kim, Eduardo F Azucena, Meizheng Liu, Sergei B Vakulenko, Christine S Chow, and Shahriar Mobashery. Design of novel antibiotics that bind to the ribosomal acyltransfer site. Journal of the American Chemical Society, 124(13):3229\u20133237, 2002.   \n[55] R Munday. Toxicity of aromatic disulphides. ii. intra-erythrocytic hydrogen peroxide formation and oxidative damage by aromatic disulphides. Journal of applied toxicology, 5(6):409\u2013413, 1985.   \n[56] Victoria Osipova, Yulia Gracheva, Maria Polovinkina, Daria Burmistrova, and Nadezhda Berberova. Antioxidant activity and cytotoxicity of aromatic oligosulfides. Molecules, 27(12): 3961, 2022.   \n[57] Warren W Kaeding. Oxidation of aromatic acids. iv. decarboxylation of salicylic acids. The Journal of Organic Chemistry, 29(9):2556\u20132559, 1964.   \n[58] Chien-Hang Ni, Po-Yuan Chen, Hsu-Feng Lu, Jai-Sing Yang, Hui-Ying Huang, Shin-Hwar Wu, Siu-Wan Ip, Chin-Tung Wu, Su-Yin Chiang, Jaung-Geng Lin, et al. Chrysophanol-induced necrotic-like cell death through an impaired mitochondrial atp synthesis in hep3b human liver cancer cells. Archives of pharmacal research, 35:887\u2013895, 2012.   \n[59] Longfei Lin, Yuling Liu, Ming Zhong, Tanggui Xie, Jian Ni, Hui Li, et al. Hepatotoxicity and mechanism study of chrysophanol-8-o-glucoside in vitro. Biomedicine & Pharmacotherapy, 120:109531, 2019.   \n[60] Rajindar S Sohal. Role of oxidative stress and protein oxidation in the aging process. Free Radical Biology and Medicine, 33(1):37\u201344, 2002.   \n[61] Bernd Moosmann and Christian Behl. Mitochondrially encoded cysteine predicts animal lifespan. Aging cell, 7(1):32\u201346, 2008.   \n[62] Candice E Paulsen and Kate S Carroll. Cysteine-mediated redox signaling: chemistry, biology, and tools for discovery. Chemical reviews, 113(7):4633\u20134679, 2013.   \n[63] Edwin M Ory and Ellard M Yow. The use and abuse of the broad spectrum antibiotics. Jama, 185(4):273\u2013279, 1963.   \n[64] Karl Drlica and Xilin Zhao. Dna gyrase, topoisomerase iv, and the 4-quinolones. Microbiology and molecular biology reviews, 61(3):377\u2013392, 1997.   \n[65] Serkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince, Moncef Gabbouj, and Daniel J Inman. 1d convolutional neural networks and applications: A survey. Mechanical systems and signal processing, 151:107398, 2021.   \n[66] James Z Wang, Zhidian Du, Rapeeporn Payattakool, Philip S Yu, and Chin-Fu Chen. A new method to measure the semantic similarity of go terms. Bioinformatics, 23(10):1274\u20131281, 2007.   \n[67] Susan L Holbeck, Richard Camalier, James A Crowell, Jeevan Prasaad Govindharajulu, Melinda Hollingshead, Lawrence W Anderson, Eric Polley, Larry Rubinstein, Apurva Srivastava, Deborah Wilsker, et al. The national cancer institute almanac: a comprehensive screening resource for the detection of anticancer drug pairs with enhanced therapeutic activity. Cancer research, 77(13):3564\u20133576, 2017.   \n[68] Yuanjing Ma and Hongmei Jiang. Ninimhmda: neural integration of neighborhood information on a multiplex heterogeneous network for multiple types of human microbe\u2013disease association. Bioinformatics, 36(24):5665\u20135671, 2020.   \n[69] Yanli Wang, Jewen Xiao, Tugba O Suzek, Jian Zhang, Jiyao Wang, and Stephen H Bryant. Pubchem: a public information system for analyzing bioactivities of small molecules. Nucleic acids research, 37(suppl_2):W623\u2013W633, 2009.   \n[70] Emmanuel Boutet, Damien Lieberherr, Michael Tognolli, Michel Schneider, and Amos Bairoch. Uniprotkb/swiss-prot: the manually annotated section of the uniprot knowledgebase. In Plant bioinformatics: methods and protocols, pages 89\u2013112. Springer, 2007.   \n[71] Mei-Chun Cai, Quan Xu, Yan-Jing Pan, Wen Pan, Nan Ji, Yin-Bo Li, Hai-Jing Jin, Ke Liu, and Zhi-Liang Ji. Adrecs: an ontology database for aiding standardization and hierarchical classification of adverse drug reaction terms. Nucleic acids research, 43(D1):D907\u2013D913, 2015.   \n[72] Amit S Kalgutkar. Designing around structural alerts in drug discovery. Journal of Medicinal Chemistry, 63(12):6276\u20136302, 2019.   \n[73] Sidney D Nelson. Metabolic activation and drug toxicity. Journal of medicinal chemistry, 25 (7):753\u2013765, 1982.   \n[74] Allan B Bailey, Ronald Chanderbhan, Nancy Collazo-Braier, Mitchell A Cheeseman, and Michelle L Twaroski. The use of structure\u2013activity relationship analysis in the food contact notification program. Regulatory Toxicology and Pharmacology, 42(2):225\u2013235, 2005.   \n[75] Lingxiang Jia, Zunlei Feng, Haotian Zhang, Jie Song, Zipeng Zhong, Shaolun Yao, and Mingli Song. Explainable fragment-based molecular property attribution. Advanced Intelligent Systems, 4(10):2200104, 2022.   \n[76] RP Elander. Industrial production of $\\beta$ -lactam antibiotics. Applied microbiology and biotechnology, 61(5):385\u2013392, 2003.   \n[77] Md Badrul Islam, Md Inshaful Islam, Nikhil Nath, Talha Bin Emran, Md Rezaur Rahman, Rohit Sharma, Mohammed Mahbubul Matin, et al. Recent advances in pyridine scaffold: Focus on chemistry, synthesis, and antibacterial activities. BioMed research international, 2023, 2023.   \n[78] Rajib K Singh, Arvind Kumar, and Arun K Mishra. Chemistry and pharmacology of acetanilide derivatives: a mini review. Letters in Organic Chemistry, 16(1):6\u201315, 2019.   \n[79] George G Zhanel, James A Karlowsky, Ethan Rubinstein, and Daryl J Hoban. Tigecycline: a novel glycylcycline antibiotic. Expert review of anti-infective therapy, 4(1):9\u201325, 2006.   \n[80] Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this appendix, we provide a comprehensive elaboration of the methodologies, experimental details, and additional insights that support the findings presented in the main manuscript. The appendix is structured into details of the proposed method, details of the experiments, and other discussion contents including limitation and extension. Furthermore, our data, code and all raw experimental results are provided in the Github repository https://github.com/hry98kki/PatternBERP. ", "page_idx": 15}, {"type": "text", "text": "A Details of Pattern-BERP Method ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Entity Attribute ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before constructing the graphs, the attributes of each entity are initialized as node features on the graphs with the domain knowledge of bio-entities. Due to the varying types of entities being studied, multiple encoders are employed to align the attribute embeddings for different types of entities. Hence, three specific types of encoders are considered as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 For drug entities $\\mathcal{D}$ , their SMILES strings can be converted into molecular graphs $\\mathbf{G}_{\\mathcal{D}}\\,=$ $(\\mathbf{S}_{\\mathcal{D}},\\mathbf{A}_{\\mathcal{D}})$ , where $\\mathbf{S}_{\\mathcal{D}}$ is the attribute matrix of all nodes representing the atoms and $\\mathbf{A}_{\\mathcal{D}}$ is the adjacency matrix of these nodes. Vallina GIN [16] is adopted to learn atom representations and then these atom representations are summarized into a drug-level feature vector through a global max pooling (GMP), which is formulated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{X}_{\\mathcal{D}}^{(l+1)}=\\mathrm{MLP}^{(l+1)}((\\mathbf{A}_{\\mathcal{D}}+(1+\\epsilon)\\mathbf{I})\\mathbf{X}_{\\mathcal{D}}^{(l)}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $l$ is the current epoch of GIN, $\\mathbf{I}$ is the identity matrix, $\\epsilon$ is a fixed scalar, and $\\mathbf{X}_{\\mathcal{D}}^{(0)}=\\mathbf{S}_{\\mathcal{D}}$ After applying the GMP over all molecular graphs, the features of all drug entities can be compiled into $\\mathbf{X}_{\\mathcal{D}}\\in\\mathbb{R}^{|\\mathcal{D}|\\times d}$ , where $d$ represents the dimension of entity attribute vector. ", "page_idx": 15}, {"type": "text", "text": "\u2022 For protein entities $\\mathcal{P}$ , taken the one-dimensional protein sequence $\\mathbf{T}$ as the input, we first convert the sequence string to an integer vector as the initialized embedding $\\mathbf{S}_{\\mathcal{P}}\\in\\mathbb{R}^{\\mathcal{T}}$ . Then, considering X(P0) $\\bar{\\mathbf{X}_{\\mathcal{P}}^{\\left(0\\right)}}=\\mathbf{S}_{\\mathcal{P}}$ , the 1D CNN [65] model is used to extract the protein representation. The propagation mechanism of each CNN layer works as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf X}_{\\mathcal{P}}^{(l+1)}=\\sigma\\big(\\mathrm{CNN}({\\bf X}_{\\mathcal{P}}^{(l)},d_{i n}^{(l)},d_{o u t}^{(l)},k s^{(l)})\\big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "twivheelrye; $\\mathbf{X}_{\\mathcal{P}}^{\\left(l\\right)}$ $\\mathbf{X}_{\\mathcal{P}}^{(l+1)}$ ea rteh et hhei dnduemnb feer aotfu rceh avnencteolsr si no ft hteh ie $l^{t h}$ t,a nndu $(l+1)^{t h}$ haCnNneNl sl apyreord, urceesdp ebcy$d_{i n}^{(l)}$ $\\mathbf{\\Phi},d_{o u t}^{(l)},k s^{(l)}$   \nthe convolution and the convolving kernel size of the $l^{t h}$ CNN layer; $\\sigma(\\cdot)$ represents nonlinear   \nactivation function, specially ReLU. After CNN layers, the $d_{\\cdot}$ -dimensional feature vectors of all   \ntarget proteins are denoted as $\\mathbf{X}_{\\mathcal{P}}\\in\\mathbb{R}^{|\\mathcal{P}|\\times d}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 For other entities $\\scriptscriptstyle\\mathcal{O}$ (such as diseases), we compile similarity matrices to acquire the initial embedding $\\mathbf{S}_{\\mathcal{O}}\\in\\mathbb{R}^{|\\mathcal{O}|\\times|\\mathcal{O}|}$ based on the methods provided by [32, 66\u201368] and followed by fully-connected networks to obtain the entity representation, denoted as $\\mathbf{X}_{\\mathcal{O}}\\in\\mathbb{R}^{|\\mathcal{O}|\\times d}$ . More specifically, for microbe nodes, cell lines and disease nodes, we compile similarity matrices $\\mathbf{S}_{\\mathcal{M}}\\in\\{1,0\\}^{|\\mathcal{M}|\\times|\\mathcal{M}|}$ , $\\mathbf{S}_{\\mathcal{L}}\\in\\mathbb{R}^{|\\mathcal{L}|\\times|\\mathcal{L}|}$ and $\\mathbf{S}_{\\mathcal{N}}\\in\\mathbb{R}^{|\\mathcal{N}|\\times|\\mathcal{N}|}$ based on the methods provided by [66\u201368], which then are transformed into $\\mathbf{X}_{\\mathcal{M}}\\in\\mathbb{R}^{|\\mathcal{M}|\\times d}$ , $\\mathbf{X}_{\\mathcal{L}}\\in\\mathbb{R}^{|\\mathcal{L}|\\times d}$ and $\\bar{\\mathbf{X}_{\\mathcal{N}}}\\in\\mathbb{R}^{|\\mathcal{N}|\\times\\dot{d}}$ by fully-connected networks. For ADRs nodes, we first use the co-occurrence of drugs to evaluate ADRs similarity. For two ADRs $i$ and $j$ , the Jaccard score is calculated as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Jaccard\\_score}=\\frac{|D_{i}\\cap D_{j}|}{|D_{i}\\cup D_{j}|},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D_{\\mathrm{i}}$ and $D_{\\mathrm{j}}$ denote drug sets that cause ADR $i$ and ADR $j$ , respectively. The ADR similarity matrix $\\mathbf{S}_{\\mathcal{R}}\\in\\mathbb{R}^{|\\mathcal{R}|\\times|\\mathcal{R}|}$ is constructed according to the Jaccard score, finally denoted as XR \u2208R|R|\u00d7d. ", "page_idx": 15}, {"type": "text", "text": "Based on the above construction rules, we generate the corresponding initial attributes for entity datasets $A,B$ , and $\\mathcal{C}$ through the entity types. Finally, the entity attributes $\\mathbf{X}$ consists of features $\\mathbf{X}_{\\mathcal{A}}$ , $\\mathbf{X}_{B}$ and $\\mathbf{X}_{\\mathcal{C}}$ . When encountering specific datasets, the three entity types can be substituted into $\\mathcal{A}$ , $B,{\\mathcal{C}})$ respectively. For example, $\\mathbf{X}$ consists of $\\mathbf{X}_{\\mathcal{D}}$ , $\\mathbf{X}_{\\mathcal{P}}$ and $\\mathbf{X}_{\\mathcal{R}}$ in DPA dataset. ", "page_idx": 15}, {"type": "table", "img_path": "LI5KmimXbM/tmp/b94c045829ef8a8aa97b384817a9005314c8214ed8e825ad12ac3f6fb5e9dcff.jpg", "table_caption": ["Table 3: Detailed information of three datasets of different biological entity association. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.2 Association Pattern Distance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given the maximum number of hops $U$ (i.e., the max positional distance for certain entity node), the distance matrix $\\mathbf{D}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{E}|}$ is constructed based on Definition 1 as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 1-hop distance: For each node $v$ and hyperedge $e$ , if hyperedge $j$ directly contains node $i$ , then $\\mathbf \u1e0a \\mathbf \u1e0a \\delta \u1e0c \u1e0c _{v,e}=1$ represents the association of hyperedge $j$ is the 1-hop pattern. \u2022 2-hop distance: If hyperedge $e$ does not directly contain node $v$ but shares a node with a hyperedge that does, then $\\mathbf \u1e0a \\mathbf \u1e0a \\boldsymbol \u1e0a v \u1e0c \u1e0c _{v,e}=2$ represents the association of hyperedge $e$ is the 2-hop pattern. \u2022 u-hop distance: Recursively calculate further distances up to $u$ -hop, then $\\mathbf \u1e0a \\mathbf \u1e0a \\boldsymbol \u1e0a v \u1e0c \u1e0c _{v,e}=u$ . \u2022 Unreachable: If node $v$ cannot reach hyperedge $e$ within $U$ -hop, then $\\mathbf{D}_{v,e}=-\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "Hence, the distance tokens $\\mathbf{D}_{v}$ for node $v$ are transformed by one position encoding layer, thereby yielding the positional embeddings for the self-attention mechanism of the subsequent API block. ", "page_idx": 16}, {"type": "text", "text": "A.3 Theory Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As an empirical method, the Pattern-BERP method extracts common patterns or rules from large bio-associated networks, similar to the $\\boldsymbol{\\mathrm{k}}$ -means clustering. Specifically, given a triplet-wise dataset ${\\cal S}\\,=\\,\\{{\\bf s}_{1},{\\bf s}_{2},\\ldots,{\\bf s}_{t},\\ldots,{\\bf s}_{T}\\}$ and $K$ common patterns, the optimization goal is to minimize the total distance of sampled patterns to their respective pattern centers. First, initialize the pattern centers $\\mathcal{M}=\\left\\{\\mathbf{m}_{1},\\mathbf{m}_{2},...,\\mathbf{m}_{k},...,\\mathbf{m}_{K}\\right\\}$ . Next, assign each sample to the nearest pattern center by computing the center index with arg mink $\\|\\mathbf{s}_{i}-\\mathbf{m}_{k}\\|$ for all the centers in $\\mathcal{M}$ . Then, update the position of these pattern centers using $\\begin{array}{r}{\\mathbf{m}_{k}\\,=\\,\\frac{1}{|C_{k}|}\\sum_{\\mathbf s_{t}\\in C_{k}}\\mathbf s_{t}}\\end{array}$ , where $C_{k}$ is the set of samples assigned to pattern $k$ calculated as above. Iterate these assignment and update steps until the pattern centers converge. The objective function to minimize is $\\begin{array}{r}{\\bar{J}=\\sum_{k=1}^{K}\\sum_{\\mathbf s_{t}\\in C_{k}}^{}\\|\\bar{\\mathbf s}_{t}-\\mathbf m_{k}\\|^{2}}\\end{array}$ st\u2208Ck \u2225st \u2212mk\u22252, and by minimizing $J$ , Pattern-BERP extracts representative biological  association patterns effectively. ", "page_idx": 16}, {"type": "text", "text": "B Details of the Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this paper, three biological datasets are used to evaluate the efficacy of the proposed method. Each dataset encompasses three different entities and their associations. Appendix Table 3 provides a detailed presentation of the specific entities within each dataset, including the count of nodes per entity, the number of associations among these nodes, and the corresponding association ratio. All entity associations are structured into triplet scheme, such as <drug, protein, adr> for DPA dataset. ", "page_idx": 16}, {"type": "text", "text": "DPA dataset. DPA dataset is first constructed and preprocessed with the origin data from [44]. Specifically, only <drug, target, adr> triplets that have complete field information are retained, that is, drug Pubchem CID, target protein identifier UniProt ID, ADR term name. Then, the detail information is obtained through these unique identifiers, respectively. For drugs, the PubChem database [69] is queried with the PubChem CID of each drug, and their Canonical SMILES are recorded. For proteins, the UniProtKB database [70] is queried with the UniProtKB ID of each protein, and their protein sequences are recorded. For ADRs, the incidence matrix about the reaction of specific ADRs to ${\\sim}15\\small{,}000$ drugs is obtained from publicly available datasets [71]. In this matrix, a value of 1 indicates that a particular drug causes a specific ADR, while a value of 0 denotes no relation between them. Furthermore, following the method described in Appendix A.1, the ADR similarity matrix can be constructed for subsequent feature generation fo ADR entities. ", "page_idx": 16}, {"type": "text", "text": "Table 4: Detailed information of memory usage (GB) with varying the number of sampled patterns $N$ across three datasets on a single NVIDIA A6000 GPU with Intel(R) Xeon CPU (24 cores). ", "page_idx": 17}, {"type": "table", "img_path": "LI5KmimXbM/tmp/1f1b7f1744aa0d8c422c884234fe8d98ce6d7c2dc16cc7c4a770a77baec988aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Details of Implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.2.1 Implementation of Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The implementation of all baseline methods is conducted using their respective publicly accessible source codes. Optimal or default configurations for each method are employed to ensure robustness. Specifically, for methods such as RF, MLP [10], CP and Tucker [11], meticulous parameter tuning is engaged in to elicit their peak performance levels. For GCN [13], GAT [15], GraphSAGE [14] and GIN [16], the original triple-wise associations are initially decomposed into two pair-wise associations. Subsequently, each pair-wise association is independently modeled using the corresponding graph neural network model, and the prediction probabilities of the two pair-wise associations are multiplied to obtain the final prediction value for the triple-wise association. Additionally, for DHNE [17] and HyperSAGNN [19], biological embeddings are integrated with their original structural embeddings to ensure fairness. For CoSTCo [12], HGSynergy [20] and MCHNN [21], the parameter settings outlined in the original publications are followed. It is important to note that for the sake of equitable comparison, all of these methods utilize negative sampling setting from MCHNN that is consistent across implementations. ", "page_idx": 17}, {"type": "text", "text": "B.2.2 Implementation Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The initialized entity embedding size $d$ is fixed to 128. The number of BGNN, APF layers are all fixed to 2. The training epoch is setting to 1,000 for DMD, DPA datasets and 2,000 for DDC dataset. The number of max hop in pattern sampling $U$ is setting to 3. The number of sampled patterns $N$ is setting to 100. The number of attention heads is 32 for DMD dataset, 16 for DDC dataset, 4 for DPA dataset. The loss-balanced coefficient for bind-relation task $\\alpha$ is fixed to 0.5. The threshold for bind-relation prediction probability $\\gamma$ is fixed to 0.5. In addition, in line with the evaluation strategy of [21], 29 negative samples are randomly generate for each test triplet in four scenarios for the evaluation of all methods, and thus display the average metrics over all test triplets. These scenario settings can comprehensively evaluate the model ability to identify positive and negative samples under these stringent conditions. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, all experiments are conducted on a single NVIDIA A6000 Tensor Core GPU (48GB) and Intel(R) Xeon CPU with 24 cores and 500G memory. The whole training time for DMD, DDC, DPA datasets is about 8, 8, 4 hours, respectively. In addition, Appendix Table 4 exhibits the GPU memory usage with varying the number of sampled patterns $N$ across three datasets, and Appendix Figure 7 presents the inference time of Pattern-BERP in comparison with these advanced baselines for each 100 samples with milliseconds. ", "page_idx": 17}, {"type": "text", "text": "B.3 Details of Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.3.1 Additional Performance Comparison on the ndcg ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As a supplement to Table 1, Appendix Table 5 incorporates additional metrics $\\mathtt{n d c g}@1$ , ndcg $@3$ and ndcg $@5$ . Considering all these metrics, Pattern-BERP significantly outperforms previous SOTA baselines on all three datasets, exhibiting the remarkable advantages of association pattern mining. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, each result of these methods is from the average of 5-fold cross-validation experiments with four scenarios. However, significant variation in prediction difficulty across different scenarios makes it relatively unreasonable to provide an error bar for all 20 results. Instead, to demonstrate the statistical validity of Pattern-BERP, we have provided the raw experimental results. ", "page_idx": 17}, {"type": "table", "img_path": "", "table_caption": ["Table 5: Performance comparison on three datasets of different biological entity associations. Each result of these methods is from the average of 5-fold cross-validation experiments with four scenarios. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "LI5KmimXbM/tmp/f417c7d566d15c8dd0dd6de0f9d9945b3692c6a33a471a1f5c0d9cb11ad374ff.jpg", "img_caption": ["Figure 4: The interpretability case of $N{=}100$ association patterns related to drug $\\#52$ in DMD dataset. The pattern commonality coefficients are represented in the form of a percentage to indicate the contribution for visualization. Larger pattern commonality coefficients indicate a more significant contribution to the target drug $\\#52$ , and these patterns frequently exhibit similar or even identical biological pathways. Conversely, smaller coefficients suggest a lack of relevance to drug #52. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3.2 Additional Interpretability Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to investigate the potential relation among different association patterns, we visualize the pattern coefficients of drug #52 in DMD dataset, as shown in Appendix Figure 4. It can be observed that the patterns that make important contributions are not necessarily 1-hop patterns, with pattern coefficients of $0.41\\%$ and $0.65\\%$ . In contrast, the 2-hop patterns can exhibit considerable relevance, due to the similar mechanisms of toxicity against microbes exhibited by the corresponding drugs (#53, #162, #207) compared to the target drug $\\#52$ under investigation. Additionally, the weakening of drug #52 itself is because the original drug $\\#52$ is a complex peptide structure with repetitive and redundant information, thus acquiring simpler and more straightforward representations through the aforementioned information interaction. ", "page_idx": 18}, {"type": "text", "text": "Furthermore, from the view of structural and functional groups mentioned by [72\u201375], we have several salient observations as follows: (1) As the drug in the most influenced pattern, drug #53 ", "page_idx": 18}, {"type": "table", "img_path": "LI5KmimXbM/tmp/1bffbaa611403b7a2b69f772b16bb075f5d0691953a4f0be3faddbaface00e5a.jpg", "table_caption": ["Table 6: Ablation study results on DMD dataset with different module designs. \u201cBFR\u201d denotes Bindrelation Feature Reconstruction; \u201cHNS\u201d denotes Hard Negative Sampling; \u201cHC\u201d denotes Hypergraph Convolution block; \u201cDE\u201d denotes Distance Embedding; \u201cAPI\u201d denotes Association Pattern-aware Interaction block. All the presented scores are in $\\%$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 7: Ablation study results on DPA dataset with different module designs. \u201cBFR\u201d denotes Bindrelation Feature Reconstruction; \u201cHNS\u201d denotes Hard Negative Sampling; \u201cHC\u201d denotes Hypergraph Convolution block; \u201cDE\u201d denotes Distance Embedding; \u201cAPI\u201d denotes Association Pattern-aware Interaction block. All the presented scores are in $\\%$ . ", "page_idx": 19}, {"type": "table", "img_path": "LI5KmimXbM/tmp/b31e700401029a68258c9b2279b34ba9c651b59342f487deff824b0535f5afc5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "contains $\\beta$ -lactam as shown in the red rectangle of Figure 3. $\\beta$ -lactam is the crucial component of $\\beta$ -lactam antibiotics [76], which is one of the most widely used classes of antibiotics available. In addition, Islam et al. [77] also prove that pyridine scaffold (aromatic ring with nitrogen) bearing poor basicity generally improves water solubility in pharmaceutically potential molecules and has led to the discovery of numerous broad-spectrum therapeutic agents. (2) Similar with drug #53, drug $\\#162$ has acetanilide structure, which serves as the basis for antimicrobial activity and disease treatment [78]. (3) Drug $\\#207$ has the dimethylglycine structure, which is crucial to Tigecycline [79]. Tigecycline binds to the bacterial ribosome, blocking the binding of amino-acyl-tRNA to the acceptor site on the mRNA-ribosome complex, thereby inhibiting protein synthesis. (4) Unlike the previous three, drug $\\#266$ may interact with membrane lipids to alter membrane fluidity and permeability, thereby exerting its effect on microorganisms. However, this mechanism of action is distinct from the specific functional group interactions of the previous three drugs. ", "page_idx": 19}, {"type": "text", "text": "As a compound with a complex structure containing multiple functional groups, drug #52 is similar to the previous three drugs due to its complex structure and diverse functional groups. These characteristics enable it to exert toxicity through specific interactions with the critical pocket of microbes. Compared to drug $\\#266$ , these complex drugs have more diverse and specific toxicity mechanisms, which is why they have received less attention. ", "page_idx": 19}, {"type": "text", "text": "B.3.3 Ablation Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Network Module Designs. To investigate the necessity of each component in Pattern-BERP, we conduct several comparisons between Pattern-BERP and its variants on the independent test set. Table 2, 6, 7 denotes the results of DDC, DMD, DPA dataset, respectively. ", "page_idx": 19}, {"type": "text", "text": "When basic components of Pattern-BERP have been removed, the performances of corresponding variants on all datasets exhibit significant declines, though the degree of performance degradation varies. This indicates that these components all contribute to the performance. Besides, we have other observations: (1) for the relatively sparse datasets, DMD and DPA, the performance decline after removing BFR is more significant compared to the DDC dataset. (2) for all datasets, eliminating HNS module results in a drop in the performance, highlighting its effect in enhancing the model\u2019s robustness and discrimination capability and thus indicating that the proposed sampling strategy contributes to efficient learning; (3) for all datasets, when only DE module is removed, the performance is inferior to the one removing the entire API module containing DE, demonstrating that inaccurate entity distance information has a more detrimental impact on the prediction performance; (4) when removing HC module leads to a slight performance degradation, the impact is relatively limited which suggests that the capability of HC module in representing complex associations is relatively modest for the relatively dense-association DDC and DMD dataset, but it still provides some beneficial effects towards the final performance improvement. But for highly sparse DPA datasets, removing HC module leads to performance improvement, demonstrating that the combined performance of HC and APA is instead constrained by the confusion introduced by the two aggregation modes on the extremely sparse structures. ", "page_idx": 19}, {"type": "image", "img_path": "LI5KmimXbM/tmp/da4b2a1dca7eab7bc0531667c0e45eae3c3d5e9032f5524be913634a93d4879d.jpg", "img_caption": ["Figure 5: Ablation study results with different hyperparameter settings. The three rows are for DMD, DDC, DPA dataset, respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Hyperparamter Settings. To verify the ablation of hyperparameter settings, we conduct ablation experiments on the three main parameters, namely number of attention heads, number of max pattern distance, and number of sampled association patterns. As illustrated in Appendix Figure 5, when the number of attention head are increased, the prediction performance on all datasets exhibits an overall upward trend, suggesting that increasing the attention heads helps the model better capture the complex association patterns, thereby improving the final performance. In addition, given the significant disparity in the association ratios across the three datasets, the performance of DDC dataset improves as the max hop hyperparameter is increased. In contrast, for the relatively sparse DMD dataset, it is advisable to restrict the max hop to the range from 1 to 3, as the performance drops sharply when the parameter is set to 4. Moreover, for the sparsest DPA dataset, selecting only 1-hop patterns appears to be the optimal choice. Moreover, regarding the number of sampled patterns, a similar trend is observed across the datasets. For DMD and DPA datasets, which are relatively sparse, increasing the number of sampled patterns is not necessarily beneficial for performance. In contrast, on DDC dataset, augmenting the number of sampled patterns continues to yield performance. ", "page_idx": 20}, {"type": "image", "img_path": "LI5KmimXbM/tmp/48dd59f1a7c52c759cce655d67c2b84c79cda26ad8a5c2fcdca20af47d4108bf.jpg", "img_caption": ["Figure 6: Ablation study results on the loss-balanced coefficient $\\alpha$ and the probability threshold of bind-relation task $\\gamma$ , both varying from 0.1 to 0.9. Note that the red cross in (b) indicates that valid negative samples cannot be generated when $\\gamma=0.1$ , hence the predictions cannot be made. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "LI5KmimXbM/tmp/42680cc78dda3bcb6b9012b7fe2f186c210f5b81ffc4b2276951e166f5fbbc57.jpg", "img_caption": ["Figure 7: Inference time comparison on three datasets of different biological entity associations. Each result of these methods is for 100 triplet samples. All the presented scores are in milliseconds (ms). Note that the original implementation of CP, Tucker and MCHNN is CPU-only based. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Furthermore, we also conduct ablation experiments on another two parameters, namely loss-balanced coefficient $\\alpha$ and the bind-relation prediction probability threshold $\\gamma$ . Results in Appendix Figure 6 show that when $\\alpha$ and $\\gamma$ are both set to 0.5 by our default, the performance surpasses those of other settings. Specifically, for $\\alpha$ , since the final prediction task involves predicting the associations among entity $A,B$ , and $\\mathcal{C}$ , the two tasks of $A\\rightarrow B$ and ${\\boldsymbol{B}}\\to{\\mathcal{C}}$ should be intuitively considered to be of equal importance, therefore the balanced coefficient $\\alpha$ set to 0.5; for $\\gamma$ , bind-relation prediction is fundamentally a binary classification task, thus the threshold $\\gamma$ is set to 0.5. ", "page_idx": 21}, {"type": "text", "text": "C Discussions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Computation Complexity Discussion. To facilitate applying across a wider range of dataset types and particularly large datasets, it is imperative to rigorously address the issue of complexity explosion. Regarding computation complexity, the discussion in Section 4.5 indicates that the decisive factors are the number of entity nodes $\\vert\\nu\\vert$ and the number of sampled patterns $N$ . Hence, when encountering larger datasets or more complex biological networks, we can employ the following two strategies to avoid a computation complexity explosion: (1) Graph Sampling. By sampling smaller subgraphs [80], computational resource consumption reduces significantly, boosting algorithm speed and efficiency; (2) High-confidence Pattern Selection. As shown in Appendix Figure 5 (g)-(i), reducing $N$ from 100 to 5 across three datasets slightly decreases performance but still surpasses these baselines. Thus, adjusting the sampling quantity is acceptable to effectively reduce time complexity. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Moreover, Appendix Table 4 shows the GPU usage varying from the number of sampled pattern across three datasets with significant differences in the number of entities and associations. The inference time of each 100 samples with milliseconds compared to baselines are provided in Appendix Figure 7. It is evident that the current computational resources are sufficient to handle existing datasets, and the difference in inference time for our method is constant and even faster than several baselines. ", "page_idx": 22}, {"type": "text", "text": "Potential Application in Real-world Scenarios. The preliminary evaluation results on three biological datasets quantitatively demonstrate that the proposed method consistently achieves superior performance over the competing baselines. Additionally, the learned association patterns show potential in interpreting biological mechanisms. This finding provides hope for the future practical application of our approach, particularly in addressing the \u201cblack box\u201d issue in the field of bioinformatics. As a computer-aided tool, our method holds the potential to exhibit a broad array of applications in real-world scenarios, contingent upon sufficient clinical tests or validations. Consequently, it may significantly contribute to human health and well-being. ", "page_idx": 22}, {"type": "text", "text": "Limitations and Future Work. While the proposed Pattern-BERP has demonstrated strong performance on biological entity relationship prediction, there are several limitations and potential directions for future research. One limitation is that the current model only considers fixed-length association patterns. In real-world biological systems, relevant relationships may be expressed through variable-length pathways involving multiple intermediate entities and relations. Extending the model to capture and leverage such variable-length association patterns could further enhance its representational power and predictive accuracy. Additionally, the applicability of Pattern-BERP method has so far been explored only in the biological domain. Investigating the effectiveness of this approach in other domains, such as knowledge graph completion for general entities or relation extraction from text, would help evaluate its broader generalizability and potential to benefit other fields that rely on structured knowledge representations. Adapting the model architecture and training strategies to accommodate the unique characteristics of different domains could lead to fruitful avenues for future research. ", "page_idx": 22}, {"type": "text", "text": "By addressing these limitations and exploring these future research directions, we believe PatternBERP can be further improved and extended to have an even greater impact on advancing our understanding of complex biological systems and knowledge representation in general. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See \u201cLimitations and Future Work\u201d part in the Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Section 4.2 - \u201cAssociation Pattern-aware Fusion\u201d & Section 5.2 - \u201cPerformance Comparison with Advanced Baselines\u201d. Furthermore, the data and source code are also provided in the Github repository. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The data and source code are provided at https://github.com/hry98kki/ PatternBERP. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See Section 5.1 - \u201cImplementation Details\u201d & Appendix B.2.2 - \u201cImplementation Settings\u201d. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the raw experimental results of all 5-fold cross-validation results across the four scenarios, resulting in a total of 20 individual results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix B.2.2-\u201cImplementation Settings\u201d. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper adheres to the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix C-\u201cApplication in Real-world Scenarios\u201d. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The source code & citations in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The source code provided in our own repository. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]