[{"figure_path": "6AepMNrz7a/tables/tables_3_1.jpg", "caption": "Table 2: Comparison of our methods and other SOTA methods on the neuromorphic datasets. Size refers to the input resolution of SNNs.", "description": "This table compares the performance of the proposed methods (IMP and IMP+TET-S) with other state-of-the-art (SOTA) methods on two neuromorphic datasets: CIFAR10-DVS and N-Caltech101.  The comparison includes the SNN architecture used, input size, number of time steps, and the achieved accuracy.  It highlights the superior performance of the proposed methods, especially when using the improved TET loss (TET-S).", "section": "5.3 Performances on the Neuromorphic Data Classification"}, {"figure_path": "6AepMNrz7a/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracy of TET and SDT on the static and neuromorphic datasets.", "description": "This table presents the test accuracy achieved by using two different loss functions, SDT (Standard Direct Training) and TET (Temporal Efficient Training), on several datasets.  The datasets are categorized into static datasets (ImageNet1k, ImageNet100, CIFAR10/100) and neuromorphic datasets (CIFAR10DVS, DVSG128, N-Caltech101). The table shows the performance of each loss function on each dataset, allowing for a comparison of their effectiveness across different types of datasets.", "section": "5 Experiments"}, {"figure_path": "6AepMNrz7a/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of our methods and other SOTA methods on the neuromorphic datasets. Size refers to the input resolution of SNNs.", "description": "This table compares the performance of the proposed methods (IMP, IMP+TET-S) against other state-of-the-art (SOTA) methods on two neuromorphic datasets: CIFAR10-DVS and N-Caltech101.  For each method, the table shows the SNN architecture used, the input size, the number of time steps, and the achieved accuracy.  The results demonstrate the improved accuracy of the proposed methods, particularly IMP+TET-S, which achieves state-of-the-art performance on both datasets.", "section": "5.3 Performances on the Neuromorphic Data Classification"}, {"figure_path": "6AepMNrz7a/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of our methods and other methods on the ImageNet1k dataset.", "description": "This table compares the performance of the proposed IMP+LTS method with several other state-of-the-art methods on the ImageNet1k dataset.  It shows the accuracy achieved by each method using different network architectures (SEW ResNet-18, SEW ResNet-34, SEW ResNet-50, etc.), with and without the proposed improvements (IMP and LTS). The table highlights the improvement in accuracy achieved by incorporating the IMP and LTS methods.", "section": "5.4 Performances on the Static Data Classification"}, {"figure_path": "6AepMNrz7a/tables/tables_9_1.jpg", "caption": "Table 4: Ablation Study on CIFAR10DVS and Imagenet100.", "description": "This table presents the results of ablation studies conducted on the CIFAR10-DVS and ImageNet100 datasets.  The goal was to analyze the impact of various factors (different loss functions, smoothing factors, and the learnable IMP) on model performance to understand their roles and optimize model design.  The results highlight the best-performing configurations and parameter choices for each dataset.", "section": "5.5 Further Ablation Studies"}, {"figure_path": "6AepMNrz7a/tables/tables_14_1.jpg", "caption": "Table 5: Accuracy and theoretical energy consumption compared with Transformer-based SNNs.", "description": "This table compares the performance of the proposed method (SEW-R50-LTS and SEW-R50-LTS+IMP) with other state-of-the-art Transformer-based spiking neural networks (SNNs).  The metrics compared include the number of parameters (Param),  the number of synaptic operations (SOPs), power consumption (Power), and accuracy on the ImageNet1k dataset. The results demonstrate that the proposed methods achieve competitive accuracy while maintaining relatively low computational costs and power consumption.", "section": "5.4 Performances on the Static Data Classification"}, {"figure_path": "6AepMNrz7a/tables/tables_15_1.jpg", "caption": "Table 6: Accuracy and theoretical energy consumption on ImageNet1k dataset.", "description": "This table shows the accuracy, number of multiply-accumulate operations (SOPs), and power consumption (in millijoules) for different models (SEW-ResNet18, SEW-ResNet34, SEW-ResNet50) trained using various methods (TET, SDT, LTS, LTS+IMP).  It demonstrates the impact of each training method on both accuracy and energy efficiency for different network sizes.", "section": "A.5 Energy Consumption"}, {"figure_path": "6AepMNrz7a/tables/tables_15_2.jpg", "caption": "Table 7: Accuracy on CIFAR10-DVS dataset with different time-steps.", "description": "This table presents the accuracy results of three different training methods (TET, SDT, and LTS) on the CIFAR10-DVS dataset using VGG architecture with varied time steps (T=4, T=8, T=10, and T=16).  It showcases how the accuracy changes with different training methods and time steps, allowing comparison of their effectiveness across different temporal resolutions. ", "section": "5.3 Performances on the Neuromorphic Data Classification"}, {"figure_path": "6AepMNrz7a/tables/tables_16_1.jpg", "caption": "Table 8: Experimental configurations on static task.", "description": "This table lists the key hyperparameters used for training on the static datasets ImageNet1k, ImageNet100, CIFAR10, and CIFAR100.  It shows the architecture, number of time steps, whether temporal effective batch normalization (TEBN) was used, if the reset of the membrane potential was detached, the type of spiking neuron, the surrogate gradient function, the membrane decay, the optimizer, learning rate, weight decay, momentum, number of epochs, warm-up period, learning rate schedule, loss function, label smoothing, data augmentation methods used, and whether cutmix and mixup were enabled.  The number of GPUs used for training is also indicated.", "section": "A.8 Experimental Configurations and Hyperparameter Settings"}, {"figure_path": "6AepMNrz7a/tables/tables_16_2.jpg", "caption": "Table 2: Comparison of our methods and other SOTA methods on the neuromorphic datasets. Size refers to the input resolution of SNNs.", "description": "This table compares the performance of the proposed methods (IMP and IMP+TET-S) with other state-of-the-art (SOTA) methods on two neuromorphic datasets: CIFAR10-DVS and N-Caltech101.  For each method, it lists the SNN architecture used, the input size (resolution), the number of time steps, and the achieved accuracy. The table highlights the superior performance of the proposed methods, particularly IMP+TET-S, which achieves state-of-the-art accuracy on both datasets.", "section": "5.3 Performances on the Neuromorphic Data Classification"}]