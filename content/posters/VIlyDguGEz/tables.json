[{"figure_path": "VIlyDguGEz/tables/tables_14_1.jpg", "caption": "Table A: Statistics of the natural image subset. The first column lists identifiers for each object category in ImageNet-1k. The \u201cClass Name\u201d in the second column is in the format as \u201cprediction by the stronger model\u201d-\u201cprediction by the base model.\u201d", "description": "This table presents statistics for a subset of natural images used in the editing benchmark.  Each row represents a group of images categorized based on predictions from two different ViT models (a stronger and a base model). The \"Group Identifier\" column uses ImageNet-1k category indices. The \"Class Name\" column shows the prediction of the stronger model first, then the prediction of the base model, separated by a hyphen. The \"Sample Number\" column indicates the quantity of images within that group.", "section": "4.1 Natural Image Subset"}, {"figure_path": "VIlyDguGEz/tables/tables_15_1.jpg", "caption": "Table B: Statistics of the AI-generated image subset.", "description": "The table shows the number of classes and the number of samples for each of the two AI-generated image subsets: oil painting and stage light.  These subsets were created to introduce subpopulation shifts in the editing benchmark, exposing the weaknesses of pre-trained ViTs when generalizing to AI-generated images with different artistic styles or lighting conditions.", "section": "4 Editing Benchmark with Subpopulation Shifts"}]