[{"figure_path": "M0ncNVuGYN/figures/figures_1_1.jpg", "caption": "Figure 1: Common symmetric graphs. Equivariant GNNs on symmetric graphs will degenerate to a zero function if the degree of their representations is fixed as 1.", "description": "This figure displays various symmetric graph structures, including odd and even k-fold rotations, tetrahedron, cube (hexahedron), octahedron, dodecahedron, and icosahedron.  The caption highlights a key finding from the paper: equivariant graph neural networks (GNNs) operating on these symmetric graphs will produce a zero function output if the degree of their representation is fixed at 1.  This illustrates a limitation of certain GNN models and motivates the research presented in the paper.", "section": "3.2 Symmetric Graph"}, {"figure_path": "M0ncNVuGYN/figures/figures_5_1.jpg", "caption": "Figure 2: The different architectures of our HEGNN, EGNN [1] and TFN [19]. HEGNN exploits the scalarization trick inspired by EGNN to enable steerable features to interact between different degrees, avoiding the high computational cost of using CG tensor products in TFN.", "description": "The figure compares the architectures of three different equivariant graph neural network models: EGNN, TFN, and the proposed HEGNN.  EGNN uses a simple scalarization technique for efficient equivariant message passing. TFN leverages higher-degree steerable representations and Clebsch-Gordan tensor products, resulting in higher computational costs. HEGNN combines the efficiency of EGNN's scalarization with the expressivity of high-degree representations by incorporating high-degree steerable vectors, but using inner products for message passing rather than computationally expensive Clebsch-Gordan tensor products.", "section": "4 The Proposed HEGNN"}]