[{"type": "text", "text": "Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiacheng $\\mathbf{Cen^{1\\,2}}$ , Anyi $\\mathbf{Li}^{1\\,2}$ , Ning Lin1 2, Yuxiang $\\mathbf{Ren}^{3}$ , Zihe Wang1 2, Wenbing Huang1 2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Gaoling School of Artificial Intelligence, Renmin University of China 2 Beijing Key Laboratory of Big Data Management and Analysis Methods 3 2012 Laboratories, Huawei Technologies, Shanghai {jiacc.cn, li_anyi, ninglin00}@outlook.com; renyuxiang1@huawei.com; wang.zihe@ruc.edu.cn; hwenbing@126.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Equivariant Graph Neural Networks (GNNs) that incorporate the E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN [1] leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1stdegree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$ -fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while still maintaining EGNN\u2019s advantage through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on a toy dataset consisting of symmetric structures, but also shows substantial improvements on other complicated datasets without obvious symmetry, including $N$ -body and MD17. Our study potentially showcase an effective way of modeling high-degree representations in equivariant GNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecules, proteins, crystals, and many other scientific data can be effectively modeled and represented through geometric graphs [2\u20138]. This type of data structure encapsulates not only node characteristics and edge information but also a 3D vector (such as position, velocity, etc.) for each node. To process geometric graphs, equivariant Graph Neural Networks (GNNs) have been developed, which undergo equivariant message passing over nodes, conforming to the E(3) or SE(3) symmetry of physical laws. These models have achieved remarkable successes in a lot of scientific tasks, such as physical dynamics simulation [9\u201311], molecular generation [12\u201315] and protein design [16\u201318]. ", "page_idx": 0}, {"type": "text", "text": "Pioneer equivariant GNNs [19\u201322] derive high-degree steerable representations beyond scalars and 3D coordinates with the help of spherical harmonics and conduct equivariant message passing between representations of different degrees through the Clebsch-Gordan (CG) tensor product. While these high-degree models are able to approximate any function of fully connected geometric graphs in theory [23], they usually suffer from expensive computational costs in practice. In contrast, ", "page_idx": 0}, {"type": "image", "img_path": "M0ncNVuGYN/tmp/a5c43bfa5e55ef43a741b9edd8bdf24ad21e446b24c194bd1d5eecaabf84463a.jpg", "img_caption": ["Figure 1: Common symmetric graphs. Equivariant GNNs on symmetric graphs will degenerate to a zero function if the degree of their representations is fixed as 1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "EGNN [1] leverages a simple scalarization technique to allow equivariant message passing over only 3D vectors (i.e. the 1st-degree steerable features). Specifically, the scalarization technique first encodes 3D vectors into scalars as invariant messages, which are passed as geometric messages after the multiplication with the 3D vectors to recover the orientation information. Despite its simplicity, EGNN achieves remarkably better efficacy and efficiency against conventional high-degree models for a broad range of applications [24; 25]. Such successes suggest that higher-degree representations might be unnecessary. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we challenge and disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric graphs. Fig. 1 illustrates the examples of $k$ -fold rotations and regular polyhedra, which are invariant to rotations up to certain rotating angles. Taking the cube for example, conducting $90^{\\circ}$ rotation around the axes crossing the center of the two opposite faces keeps its shape and orientation unchanged. Interestingly, by making use of group theory, we theoretically prove that any equivariant GNN (after translating the coordinate center to the origin and conducting graphlevel readout) on these symmetric graphs will degenerate to a zero function if the degree of their representations is fixed to be 1. The direct deduction of this theorem is that EGNN can only output a zero 3D vector no matter how we rotate the input graph, indicating that EGNN totally loses the recognition ability of orientation. Additionally, this statement points out the limitation of the methods that rely on constructing global features for symmetric graphs 2 (e.g. frames in frame averaging [26; 27], virtual nodes in FastEGNN [28]), equivariant pooling in EGHN [29], and meshes in Neural $\\mathrm{P^{3}M}$ [30]), since it is impossible to output another non-collinear 3D vector except the center coordinate. ", "page_idx": 1}, {"type": "text", "text": "Based on the above theoretical insights, we propose a novel equivariant GNN model termed HEGNN3, which enhances EGNN by incorporating high-degree steerable vectors while inheriting the desired benefit from EGNN through the scalarization trick. In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We theoretically investigate the expressivity reduction issue of equivariant GNNs on symmetric graphs.   \n\u2022 We propose HEGNN, to further incorporate high-degree steerable representations into EGNN. Moreover, since the equivariant message passing process between different-degree representations is conducted via inner products, it shares the same benefit as EGNN, compared to traditional high-degree models.   \n\u2022 Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric graphs, but also shows substantial improvements on more complicated datasets without explicit symmetry, such as $N$ -body and MD17. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Equivariant GNNs. Equivariant GNNs can be divided into two classes: scalarization-based models and high-degree steerable models [31]. Scalarization-based models adopt norms or inner products to convert equivariant 3D vectors into invariant scalars, which are considered as coefficients to linearly combine 3D vectors for node update. EGNN [1] is the first work falling into this category. Concurrently, PAINN [32] further enhances the expressive ability of the model by introducing multi-channel equivariant features. On the contrary, high-degree steerable models (e.g. TFN [19], ", "page_idx": 1}, {"type": "text", "text": "SEGNN [33] and SE(3)-Transformer) use spherical harmonics to ensure the equivariance of message passing, and realize interaction between steerable features of different degrees through CG tensor products. Our HEGNN also uses high-degree steerable features, but it leverages the scalarization trick for the interaction between steerable features of different degrees, thus leading to more expressivity than EGNN and less computational cost than other high-degree models. ", "page_idx": 2}, {"type": "text", "text": "Expressivity of Equivariant GNNs. The theoretical expressivity of equivariant GNNs is initially explored by [23], which proves the university of the high-degree steerable model, i.e., TFN [19], over fully-connected geometric graphs. GemNet [34] further demonstrates that the universality holds with just spherical representations other than the full SO(3) representations that are required in the proof of [23]. More recently, the GWL framework [35] extends the Weisfeiler-Lehman (WL) test into a geometric version [36] to study the expressive power of geometric GNNs operating on sparse graphs from the perspective of discriminating geometric graphs. Different from all the above works, our paper investigates the expressivity of equivariant GNNs on symmetric graphs, and demonstrates the necessity of involving high-degree representations. Although the GWL test paper [35] has experimentally compared different models on $k$ -fold structures that are allowed to rotate only in the 2D space, the conclusions of this paper are proved both theoretically and experimentally. Moreover, our discussions cover a full range of examples including $k$ -folds (rotation in 3D space) and regular polyhedra. ", "page_idx": 2}, {"type": "text", "text": "3 Theoretical Analyses ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present the necessary preliminaries related to geometric graphs and group representation. Then, we define and illustrate typical examples of symmetric graphs. Finally, we will discuss when equivariant GNNs will degenerate to a zero function on symmetric graphs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Geometric graph. A geometric graph of $N$ nodes is defined as $\\mathcal{G}:=\\left(H,\\vec{X};A\\right)$ , where $H:=$ $\\{h_{i}\\,\\in\\,\\mathbb{R}^{C_{H}}\\}_{i=1}^{N}$ and $\\vec{X}\\,:=\\,\\{\\vec{\\pmb{x}}_{i}\\,\\in\\,\\mathbb{R}^{3}\\}_{i=1}^{N}$ are node features and 3D coordinates, respectively; $A\\in\\mathbb{R}^{N\\times N}$ represents the adjacency matrix and can be assigned with edge features $e_{i j}$ if necessary. Throughout our theoretical analyses in this section, we assume the node features and edge features to be identical for all. ", "page_idx": 2}, {"type": "text", "text": "Transformation of geometric graph. We are interested in the transformations of a geometric graph $\\mathcal{G}$ with respect to a group $\\mathfrak{G}$ , which is defined as ${\\mathfrak{g}}\\cdot{\\mathcal{G}}$ , for ${\\mathfrak{g}}\\in{\\mathfrak{G}}$ and $\\cdot$ denoting the group action. For instance, ${\\mathfrak{g}}\\cdot{\\mathcal{G}}$ can be explained as translation, rotation, or reflection of the coordinates $\\vec{X}$ . These transformations form a 3D Euclidean group denoted as $\\mathrm{E}(3)$ , and its subgroup without translation is called the orthogonality group O(3). With the aid of group representation $\\rho({\\mathfrak{g}})$ , the transformation of a coordinate $\\vec{\\pmb{x}}$ is represented as $\\rho({\\mathfrak{g}}){\\vec{x}}$ . For example, orthogonal matrices are the trivial representations of O(3), that is, the orthogonal transformation of a vector $\\vec{\\pmb{x}}$ is represented by ${\\cal O}\\vec{\\bf x}$ with $\\boldsymbol{O}\\in\\mathbb{R}^{3\\times3}$ being an orthogonal matrix. Besides, there are other representations of O(3), such as the irreducible representations which will be detailed below. ", "page_idx": 2}, {"type": "text", "text": "Equivariance. Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be the input and output vector spaces, respectively. A function $f:\\mathcal X\\to\\mathcal Y$ is called equivariant with respect to group $\\mathfrak{G}$ if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall\\mathfrak{g}\\in\\mathfrak{G},f(\\rho_{\\mathcal{X}}(\\mathfrak{g})\\vec{x})=\\rho_{\\mathcal{Y}}(\\mathfrak{g})f(\\vec{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\rho_{\\mathcal{X}}$ and $\\rho_{\\mathcal{Y}}$ are the group representations in the input and output spaces, respectively. Since we can eliminate the translation effect by simply translating the center of all coordinates to the origin, we only discuss equivariance with respect to O(3) in this section. In other words, we default that the center of $\\vec{X}$ is at the origin. ", "page_idx": 2}, {"type": "text", "text": "Irreducible representations and steerable features. O(3) consists of rotation and inversion, implying $\\mathrm{O(3)}^{\\displaystyle^{-}=\\,\\mathrm{SO(3)}\\times C_{i}}$ , where $\\mathrm{SO}(3)$ is the rotation group and $C_{i}\\;=\\;\\{\\mathfrak{e},\\mathfrak{i}\\}$ denotes the inverse group. We first discuss the irreducible representations of SO(3). For each rotation $\\mathfrak{r}\\in\\mathrm{SO}(3)$ , its irreducible representations are Wigner-D matrices $\\mathbf{\\deltaD}^{(l)}(\\mathfrak{r})\\in\\mathbb{R}^{(2l+1)\\,\\times\\,(2l+1)}$ of different degree $l~\\in~\\mathbb N$ [21; 37]. When $l\\:=\\:1$ , it becomes the common rotation matrix $R_{\\mathrm{t}}$ acting on the 3D coordinate space. Under the irreducible representations, the equivariant constraint in Eq. (1) turns into $f^{(l)}(R_{\\mathfrak{t}}\\vec{x})=D^{(l)}({\\mathfrak{r}})f^{(l)}(\\vec{x})$ , if the output degree is $l$ . According to [38], spherical harmonics $Y^{(l)}=[Y_{m}^{(l)}(\\vec{\\pmb{x}})]_{m=-l}^{l}$ offer a unique and complete set of function bases satisfying the equivariant constraint. We further define a modulated spherical harmonics as $f^{(l)}({\\vec{\\pmb{x}}})=\\varphi(\\|{\\vec{\\pmb{x}}}\\|)\\cdot Y^{(l)}({\\vec{\\pmb{x}}}/\\|{\\vec{\\pmb{x}}}\\|)$ by adding a continuous radial function $\\varphi:\\mathbb{R}^{+}\\rightarrow\\mathbb{R}$ of vector norm $\\|\\cdot\\|$ for re-scaling. Such a function $f^{l}$ and its output $f^{l}(\\vec{\\pmb{x}})$ are called type- $l$ steerable function and steerable feature, respectively. We now deduce the irreducible representations from SO(3) to O(3). Note that spherical harmonics satisfy $Y^{(l)}(-\\vec{\\pmb{x}})=(-1)^{l}Y^{(l)}(\\vec{\\pmb{x}})$ ; in other words, they are inverse-equivariant when $l$ is odd, but inverse-invariant when $l$ is even. We thus specify the group representation of O(3) as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho^{(l)}(\\mathfrak{r m}):=\\sigma^{(l)}(\\mathfrak{m})D^{(l)}(\\mathfrak{r}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma^{(l)}({\\mathfrak{m}})=1$ for $\\mathfrak{m}=\\mathfrak{e}$ (the identity) and $\\sigma^{(l)}(\\mathfrak{m})=(-1)^{l}$ if $\\mathfrak{m}=\\mathfrak{i}$ (the inverse). Readers can refer to the discussion in $\\mathrm{e}3\\mathrm{nn}$ [39] with another representation method by using the concept of parity and construct this through methods such as Clebsch-Gordan (CG) tensor product [40]. For concision, the type- $l$ steerable feature is denoted as $\\tilde{\\pmb{v}}^{(l)}$ with a tilde notation. ", "page_idx": 3}, {"type": "text", "text": "3.2 Symmetric Graph ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In $\\S\\ 1$ , we present that $k$ -fold rotations and regular polyhedra exhibit certain symmetries. In this subsection, we formally describe them via the notion of the symmetric graph. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Symmetric Graph). A geometric graph $\\mathcal{G}$ is called a symmetric graph, if there exists a finite and nontrivial subgroup ${\\bar{\\mathfrak{H}}}\\leq0({\\bar{3}}),{\\mathfrak{H}}\\neq\\{{\\bar{\\mathfrak{e}}}\\}$ , satisfying that $\\forall\\mathfrak{h}\\in\\mathfrak{H},\\mathfrak{h}\\cdot\\mathcal{G}=\\mathcal{G}$ . All subgroups making $\\mathcal{G}$ symmetric yields a set $\\mathbb{H}(\\mathcal{G})$ , and all geometric graphs that are symmetric w.r.t. ${\\mathfrak H}$ constitute a set denoted as $\\mathbb{G}({\\mathfrak{H}})$ . ", "page_idx": 3}, {"type": "text", "text": "Here ${\\mathfrak{h}}\\cdot{\\mathcal{G}}={\\mathcal{G}}$ is defined in the graph level. Particularly for the coordinates $\\vec{X}\\in\\mathbb R^{3\\times N}$ , it implies that $\\forall O\\in{\\mathfrak{H}}$ , $\\exists P\\in S_{N}$ , $O\\vec{X}=\\vec{X}P$ and $P A=A P$ , where $S_{N}$ is the permutation group of order $N$ . Essentially, rotating the coordinates of a symmetric graph leads to a copy of this graph up to a different permutation of the nodes. ", "page_idx": 3}, {"type": "text", "text": "Without considering inversion, the finite subgroups of $\\mathrm{SO}(3)$ are only cyclic group $C_{n}$ , dihedral group $D_{n}$ , tetrahedral group $T$ , octahedral group $O$ , and Icosahedral group $I$ [41]. We provide several examples of symmetric graphs as follows. ", "page_idx": 3}, {"type": "text", "text": "Example 3.2 $k$ -folds). On the one hand, for a geometric graph $\\mathcal{G}$ corresponding to a $2k$ -fold with nodes $\\{(\\cos(i\\cdot\\pi/k),\\sin(i\\cdot\\pi/k),0)\\}_{i=0}^{2k-1}$ , the inverse group $C_{i}$ and the dihedral group $D_{2k}$ (rotation around $z$ -axis with angle $\\pi/k$ , and reflection around the axis connecting the midpoints of opposite sides or the axis connecting opposite vertices), are symmetric groups on $\\mathcal{G}$ , namely, $C_{i},D_{2k}\\in\\mathbb{H}(\\mathcal{G})$ . On the other hand, for a geometric graph $\\mathcal{G}$ corresponding to a $(2k+1)$ -fold with nodes $\\{(\\cos(i\\cdot2\\pi/(2k+1),\\sin(i\\cdot2\\pi/(2k+1)),0)\\}_{i=0}^{2k},\\nonumber\\,\\}$ $\\mathbb{H}(\\mathcal{G})$ includes the dihedral group $D_{2k+1}$ but without the inverse group $C_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "Example 3.3 (Regular Polygons). The symmetric groups of regular polygons in the plane and regular prisms in space include the dihedral group $D_{n}$ . Regular tetrahedra are symmetric with respect to three rotation axes of the second order and four axes of the third order, corresponding to 12 group elements. Regular hexahedra (cubes) and the regular octahedra (which are dual to each other and share the same symmetric groups) are symmetric about six axes of the second order, four axes of the third order, and three axes of the fourth order, corresponding to 24 group elements. Regular dodecahedra and regular icosahedra (which are also dual to each other) are symmetric about six axes of the fifth order, ten axes of the third order, and fifteen axes of the second order, corresponding to 60 group elements. Additionally, except tetrahedra, all other four regular polygons are central symmetric, indicating that $C_{i}$ is their common symmetric group. ", "page_idx": 3}, {"type": "text", "text": "3.3 Equivariant GNNs on symmetric graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now demonstrate that equivariant GNNs on symmetric graphs will encounter the issue of expressivity degeneration. Here, we assume that the graph functions we explore are invariant to the permutation of the nodes. This fits the case when we add a readout layer to all nodes globally or just focus on the message passing process for each node individually. ", "page_idx": 3}, {"type": "text", "text": "We first derive a crucial theorem that greatly facilitates our analyses. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4. Suppose that $f^{(l)}$ is an O(3)-equivariant function on geometric graphs, regarding the group representation $\\rho^{(l)}$ defined in Eq. (2). Then, for any symmetric graph $\\mathcal{G}$ induced by the group ${\\mathfrak{H}}\\leq{\\mathrm{O}}(3)$ , namely, $\\forall\\mathcal{G}\\in\\mathbb{G}(\\mathfrak{H})$ , we always have ", "page_idx": 4}, {"type": "equation", "text": "$$\nf^{(l)}({\\mathcal{G}})=\\rho^{(l)}({\\mathfrak{H}})f^{(l)}({\\mathcal{G}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here we have defined group average as $\\begin{array}{r}{\\rho^{(l)}(\\mathfrak{H}):=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h}\\in\\mathfrak{H}}\\rho^{(l)}(\\mathfrak{h}).}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Eq. (3) is interesting and it shows that the function $f^{(l)}$ is symmetric with respect to the group average $\\rho^{(\\bar{l})}(\\mathfrak{H})$ . More importantly, it indicates an linear equation $\\left(I_{2l+1}-\\rho^{(l)}(\\mathbf{\\bar{\\mathfrak{H}}})\\right)f^{(l)}(\\bar{\\mathcal{G}})=\\stackrel{\\cdot}{0}$ , where $I_{2l+1}\\in\\mathbb{R}^{(2l+1)\\times(2l+1)}$ is the identity matrix. We can immediately attain the following conclusion. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5. If and only if the matrix $I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})$ is non-singular, the $\\mathrm{O}(3)$ -equivariant function $f^{(l)}$ is always a zero function on $\\mathcal{G}$ , namely, ", "page_idx": 4}, {"type": "equation", "text": "$$\nf^{(l)}(\\mathcal{G})\\equiv\\mathbf{0},\\quad\\forall\\mathcal{G}\\in\\mathbb{G}(\\mathfrak{H}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A more general version of Theorem 3.5 is that the output space of $f^{(l)}$ corresponds to the null space of the matrix $I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})$ , indicating that $\\mathrm{dim}(f^{(l)})=(2l+1)-\\mathrm{rank}\\left(I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})\\right)$ . Therefore, even the function $f^{(l)}$ ) will not exactly reduce to a zero function when $I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})$ is singular, its output space is still limited to a subspace and suffers from diminished expressivity owing to the symmetry of the input geometric graph. ", "page_idx": 4}, {"type": "text", "text": "In practice, it is difficult to determine if the matrix $I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})$ is singular. This determination becomes easier if we can show that the group average $\\rho^{(l)}(\\mathfrak{H})$ is equal to the zero matrix. Fortunately, we have the following property. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.6. For a finite group ${\\mathfrak H}$ with its representation $\\rho^{(l)}$ , $\\rho^{(l)}(\\mathfrak{H})$ is a zero matrix (i.e., $\\boldsymbol{\\rho}^{(l)}(\\mathfrak{H})=$ 0) if and only $i f\\mathrm{tr}(\\rho^{(l)}(\\mathfrak{H}))=0$ . In this case, $f^{(l)}(\\mathcal{G})\\equiv\\mathbf{0},\\forall\\mathcal{G}\\in\\mathbb{G}(\\mathfrak{H})$ . ", "page_idx": 4}, {"type": "text", "text": "According to Theorem 3.6, we calculate the trace of the group average for each symmetric graph of interest and check if the trace is equal to zero. We summarize the conclusions for $k$ -fold structures and regular polyhedra in Table 1. We find that when $l\\,=\\,1$ , $f^{(1)}\\equiv\\mathbf{0}$ for all cases. In addition, the function degenerates when $l$ is odd, if the symmetric graph is induced by the inverse group $C_{i}$ . We defer more details of the calculations in the Appendix. Compared to the conclusions drawn by the GWL paper [35] which only experimentally discusses the $k$ -fold structures under 2D rotations, here we apply rigorous theoretical derivations to analyze more cases besides $k$ -folds, regarding more symmetric subgroups of O(3). ", "page_idx": 4}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/200a0d9b623df7f1ae93afa304aaa1f4efbf880bee55b10014c444347481582a.jpg", "table_caption": ["Table 1: Expressivity degeneration of equivariant GNNs on symmetric graphs. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 The Proposed HEGNN ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The analyses in the last section imply the necessity of involving the representations with more and higher degrees in equivariant GNNs. Therefore, we propose HEGNN by further conducting the update of high-degree steerable features upon EGNN [1]. As illustrated in Fig. 2, HEGNN is composed of the three key components: initialization of high-degree steerable features, calculation of cross-degree invariant messages, and aggregation of neighbor messages, the latter two of which are conducted over multiple layers. We depict each component separately in what follows. ", "page_idx": 4}, {"type": "text", "text": "Initialization of high-degree steerable features. Given a geometric graph $\\mathscr{G}\\left(H,\\vec{X};A\\right)$ where each node contains only type-0 feature $h_{i}$ and type-1 feature $\\vec{\\pmb{x}}_{i}$ , we first obtain the initialization ", "page_idx": 4}, {"type": "image", "img_path": "M0ncNVuGYN/tmp/a6bd958073ea79c1292cadf06544d56bd3ff08ca52f7a4c871e85e908f4da133.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: The different architectures of our HEGNN, EGNN [1] and TFN [19]. HEGNN exploits the scalarization trick inspired by EGNN to enable steerable features to interact between different degrees, avoiding the high computational cost of using CG tensor products in TFN. ", "page_idx": 5}, {"type": "text", "text": "of high-degree steerable features $\\{\\tilde{v}_{i}^{(l)}\\}_{l=0}^{L}$ by using spherical harmonics on normalized relative coordinates. In detail, we aggregate spherical harmonics from all neighbors as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{v}_{i,\\mathrm{init}}^{(l)}=\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}\\varphi_{\\tilde{v},\\mathrm{init}}^{(l)}(m_{i j,\\mathrm{init}})\\cdot Y^{(l)}\\left(\\frac{\\vec{x}_{i}-\\vec{x}_{j}}{\\|\\vec{x}_{i}-\\vec{x}_{j}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $m_{i j,\\mathrm{init}}=\\varphi_{m,\\mathrm{init}}\\left(h_{i},h_{j},e_{i j},d_{i j}^{2}\\right)$ is an invariant scalar with $\\varphi_{m,\\mathrm{init}}$ being an arbitrary MultiLayer Perceptron (MLP), and $\\mathcal{N}(i)$ denotes the neighbors of $i^{4}$ . ", "page_idx": 5}, {"type": "text", "text": "Calculation of cross-degree invariant messages. EGNN [1] employs a scalarization trick by transforming the relative coordinate $\\Vec{\\mathbf{x}}_{i}-\\Vec{\\mathbf{x}}_{j}$ (the usage of relative coordinates is for translation invariance) into an invariant scalar via the vector norm, which will be used to compute invariant message for both node features and coordinates. We generalize this scalarization trick to the case of high-degree steerable features. To be specific, we carry out the inner product between $\\tilde{v}_{i}^{(l)}$ and $\\tilde{v}_{j}^{(l)}$ for each degree $l$ individually, resulting in an invariant scalar zi(jl) . Then, we get the invariant message between node $i$ and $j$ , namely $\\pmb{m}_{i j}$ after undergoing an MLP of all invariant quantities. The above processes are summarized as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{i j}=\\|\\vec{x}_{i}-\\vec{x}_{j}\\|,\\quad z_{i j}^{(l)}=\\left\\langle\\tilde{v}_{i}^{(l)},\\tilde{v}_{j}^{(l)}\\right\\rangle,\\quad m_{i j}=\\varphi_{m}\\left(h_{i},h_{j},e_{i j},d_{i j}^{2},\\bigoplus_{l=0}^{L}z_{i j}^{(l)}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\oplus$ refers to concatenation. It should be noted that the form of SO3KRATES introduced in the concurrent work [42] is equivalent to the expression for $z_{i j}^{(l)}$ in Eq. (6). Furthermore, our scalarization trick simplifies the formulation by bypassing the Clebsch-Gordan coefficients, making it more straightforward and easier to understand. ", "page_idx": 5}, {"type": "text", "text": "Aggregation of neighbor messages. With the invariant message $\\pmb{m}_{i j}$ at hand, we then update $h_{i},\\vec{\\pmb{x}}_{i},\\tilde{\\pmb{v}}_{i}^{(l)}$ via message aggregation over all neighbors. We define $\\Delta h_{i},\\bar{\\Delta\\pmb{x}_{i}},\\Delta\\tilde{\\pmb{v}_{i}^{(l)}}$ as the residues, which are calculated by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta h_{i}=\\varphi_{h}\\left(h_{i},\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}m_{i j}\\right),\\,\\Delta\\vec{x}_{i}=\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}\\varphi_{\\vec{\\pmb{i}}}(m_{i j})\\cdot\\left(\\vec{x}_{i}-\\vec{x}_{j}\\right),}\\\\ {\\displaystyle\\Delta\\tilde{v}_{i}^{(l)}=\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}\\varphi_{\\vec{v}}^{(l)}(m_{i j})\\cdot\\left(\\tilde{v}_{i}^{(l)}-\\tilde{v}_{j}^{(l)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where \u03c6h, \u03c6\u20d7x, \u03c6v\u02dc are different MLPs, and \u03c6\u20d7x, \u03c6(v\u02dcl) both output a 1D scalar. Note that the application of Eq. (8) for all degrees can be compactly rewritten as $\\begin{array}{r l}{\\bigoplus_{l=0}^{L}\\Delta\\tilde{\\mathbf{v}}_{i}^{(l)}}&{=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{|\\mathcal{N}(i)|}\\sum_{j\\in\\mathcal{N}(i)}1\\otimes_{\\mathtt{c g}}^{\\varphi_{\\tilde{v}}(m_{i j})}\\left(\\bigoplus_{l=0}^{L}\\left(\\tilde{v}_{i}^{(l)}-\\tilde{v}_{j}^{(l)}\\right)\\right)}\\end{array}$ in the form of CG tensor product with the weights $\\varphi_{\\tilde{v}}(\\pmb{m}_{i j})\\,:=\\,\\oplus_{l=0}^{L}\\,\\varphi_{\\tilde{v}}^{(l)}$ . This form can be easily implemented using existing libraries such as e3nn.o3.FullyConnectedTensorProduct [39]. The resulting residues are used for the update: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{i}=\\pmb{h}_{i}+\\Delta\\pmb{h}_{i},\\quad\\vec{\\pmb{x}}_{i}=\\vec{\\pmb{x}}_{i}+\\Delta\\vec{\\pmb{x}}_{i},\\quad\\tilde{\\pmb{v}}_{i}^{(l)}=\\tilde{\\pmb{v}}_{i}^{(l)}+\\Delta\\tilde{\\pmb{v}}_{i}^{(l)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In addition, we can augment the update of $\\vec{\\pmb{x}}_{i}$ with 1st-degree feature $\\tilde{\\pmb{v}}_{i}^{(1)}$ , leading to $\\pmb{\\vec{x}}_{i}=\\pmb{\\vec{x}}_{i}+$ $\\Delta\\vec{\\pmb{x}}_{i}+\\phi_{\\tilde{\\pmb{v}}}^{(1)}(h_{i})\\tilde{\\pmb{v}}_{i}^{(1)}$ , which yet is not explored in our experiments for the sake of simplicity. The final output of $h_{i}$ and $\\vec{\\pmb{x}}_{i}$ can be used for the node-level invariant prediction and equivariant prediction, respectively. We can also obtain a graph-level prediction by further adding a readout layer of all nodes. ", "page_idx": 6}, {"type": "text", "text": "We now analyze the expressivity of HEGNN. Apparently, by including high-degree features, HEGNN is able to avoid the loss of expressive ability even on symmetric graphs. Moreover, when tackling general geometric graphs, HEGNN is capable of characterizing the complete angle information of the input graph, if its maximal degree $L$ is sufficiently large. For concision and without losing the generality, we assume the steerable features $\\tilde{\\pmb{v}}_{i}^{(1)}$ are initialized with only spherical harmonics without the weights \u03c6(v\u02dcl,)init in Eq. (5). Let $\\vec{\\pmb{x}}_{i s}=(\\vec{\\pmb{x}}_{i}-\\vec{\\pmb{x}}_{s})/\\|\\vec{\\pmb{x}}_{i}-\\vec{\\pmb{x}}_{s}\\|$ , the inner product $z_{i j}^{(l)}$ can be expanded as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\langle\\sum_{s\\in\\mathcal{N}(i)}Y^{(l)}\\left(\\vec{\\pmb{x}}_{i s}\\right),\\sum_{t\\in\\mathcal{N}(j)}Y^{(l)}\\left(\\vec{\\pmb{x}}_{j t}\\right)\\right\\rangle=\\frac{4\\pi}{2l+1}\\sum_{s\\in\\mathcal{N}(i)}\\sum_{t\\in\\mathcal{N}(j)}P^{(l)}\\left(\\left\\langle\\vec{\\pmb{x}}_{i s},\\vec{\\pmb{x}}_{j t}\\right\\rangle\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $P^{(l)}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is Legendre polynomial of degree $l$ , and Eq. (10) is based on the properties of spherical harmonics that $\\langle Y^{(l)}(\\bar{\\pmb x}),\\bar{Y}^{(l)}(\\vec{\\pmb y})\\rangle=4\\bar{\\pi}/(2l+1)\\cdot\\bar{P^{(l)}}(\\langle\\vec{\\pmb x},\\vec{\\pmb y}\\rangle),\\|\\vec{\\pmb x}\\|=\\|\\vec{\\pmb y}\\|=$ . We have the following result. ", "page_idx": 6}, {"type": "text", "text": "nTehre oprreomdu c4t.s1 . $\\{z_{i j}^{(l)}\\}_{l=1}^{|\\mathbb{A}_{i j}|}$ ggiveeonm ebtryi cE qg.r a(p1h0,)  tahnedre t heex isstest  aof  beijdegceti oann glbeest $\\begin{array}{r c l}{\\mathbb{A}_{i j}}&{=}&{\\{\\theta_{i s,j t}~:=}\\end{array}$ arccos\u27e8\u20d7xis, \u20d7xjt\u27e9}s\u2208N(i),t\u2208N(j). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 states that the inner products of full degrees can recover the information of all angles between each pair of edges, affirming the enhanced expressivity of our HEGNN. The proof is derived mainly based on the fact that Legendre polynomials are orthogonal polynomial bases which can injectivly represent the set $\\mathbb{A}_{i j}$ thanks to Newton\u2019s identities. The details are deferred to the appendix. Although the upper-bound of the degree in Theorem 4.1 grows rapidly with the graph size, it will be shown in our experiments that HEGNN with only $L\\leq6$ is sufficient to outperform traditional models like EGNN [1] and TFN [19] in practice. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Expressivity on Symmetric Graphs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Design of experiments: To experimentally verify the conclusion we proved above, we design a more comprehensive experiment based on code5 in [35]. This experiment uses four $k$ -fold structures $(k\\in\\{2,3,5,10\\})$ and five convex regular polyhedra shown in Fig. 1 as test objects, and the center of each is at the origin. In detail, an arbitrary rotation in $3D$ is acted on such symmetric structures called $\\mathcal{G}_{\\mathrm{0}}$ which ensures the geometric graph after rotation called $\\mathcal{G}_{1}$ does not coincide with the original one. The goal of our experiments is to check whether different equivariant neural networks can distinguish $\\mathcal{G}_{\\mathrm{0}}$ and $\\mathcal{G}_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "The models we select include two models that only use Cartesian coordinates: EGNN and GVPGNN; and two models that use high-degree steerable features: TFN and MACE. However, TFN and MACE (denoted as $\\mathrm{TFN/MACE}_{l\\le L})$ ) always use all degrees $l\\in\\{0,\\ldots,L\\}$ , so it is not clear which degree(s) of steerable features distinguish the two geometric graphs. In our HEGNN, all steerable features corresponding to unwanted degrees could be masked during initialization in Eq. (5), and we let $\\mathrm{HEGNN}_{l=L}$ be a HEGNN with only lth-degree steerable features. Additionally, to align with TFN/MACE, we also test the performance of HEGNN with all $l\\in\\{0,1,\\ldots,L\\}$ donated as $\\mathrm{HEGNN}_{l\\leq L}$ . Following the settings6 in [35], the output of each graph is the concatenation of invariant scalars, coordinates, and high-degree steerable features pooling among all nodes. We then map this spliced vector to a two-dimensional vector and input it into a simple classifier to determine whether the equivariant graph neural network can distinguish $\\mathcal{G}_{0}$ from $\\mathcal{G}_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results: The results on $k$ -fold are deferred to Appendix for saving space, and the results on regular polyhedra are shown in Table 2. From Table 1, we can know steerable features in which degree could not distinguish specific symmetry structure, and both results on $k$ -fold and regular polyhedra are also in perfect agreement with our conclusions. Models (EGNN and GVP-GNN) only with Cartesian vectors cannot distinguish any symmetric graph at all. Taking $\\mathrm{HEGNN}_{l=5}$ as an example, since $D^{(5)}(\\mathfrak{H})=0,\\forall\\mathfrak{H}\\in\\bar{\\{T}}\\!,O,I\\}$ , no matter which kind of regular polyhedron, $f^{(5)}$ could only output 0 thus failing to distinguish the structures. ", "page_idx": 7}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/595804e705a85a608dde5f406a5e2050e81e60208a810bef2beaf844b8b2962a.jpg", "table_caption": ["Table 2: Regular polyhedra. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Physical Dynamics Simulation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets: We benchmark our HEGNN in two scenarios, including: $N$ -body system [43] is a dataset generated from simulations. In our simulations, each system contains $N$ charged particles with random charge $c_{i}\\in\\{0,1\\}$ , whose movements are driven by Coulomb forces. To verify the efficiency and effectiveness of our HEGNN on datasets of different sizes, we select $N$ from $\\{\\bar{5},20,50,100\\}$ . We use 5000 samples for training, 2000 for validation, and 2000 for testing. The task is to estimate the positions of the $N$ particles after 1,000 timesteps. MD17 [44] dataset contains trajectory data for eight molecules generated through molecular dynamics simulations. The goal of this experiment is to predict the future positions of the atoms based on their current state. We follow the dataset partitioning scheme from [45], splitting the dataset into 500/2000/2000 frame pairs for training, validation and testing, respectively. All experiments are run on a single NVIDIA A100-80G GPU. ", "page_idx": 7}, {"type": "text", "text": "Baselines: To demonstrate the advantages of our HEGNN over both models with scalarization techniques and models with high-degree steerable vectors at the same time, our baseline needs to consider the selection issues of both models simultaneously. Therefore, we select some representative models as baselines, including the invariant RF [46], the equivariant EGNN [1], TFN [19] and SE(3)- Tr. [20]. In addition, we select classical models such as Linear dynamics [1], the non-equivariant ", "page_idx": 7}, {"type": "text", "text": "Message Passing Neural Network (MPNN) [47], the invariant SchNet [48], and the equivariant GVP-GNN [49] for the $N$ -body experiments. For MD17 experiments, we also select GMN [45]. ", "page_idx": 8}, {"type": "text", "text": "Metrics: 1. Loss function: We use Mean Squared Error (MSE) to measure the accuracy of the prediction results in both experiments. 2. Inference time: Given that the $N$ -body system we use contains data of varying sizes, we test the inference time of each model on this dataset. The inference time for each model is calculated relative to the benchmark, which is the inference time of EGNN at the corresponding scale. ", "page_idx": 8}, {"type": "text", "text": "Results on $N$ -Body systems: The main results of $N$ -body system simulation are presented in Table 3. From these results, we observe the following: 1. Overall performance: Our HEGNN significantly outperforms other models across datasets of all sizes. Although EGNN [1] performs better than high-degree steerable models like TFN or SE(3)-Transformer in this task, our HEGNN is still better than EGNN, which show that the method of HEGNN introducing high-degree steerable features is more effective. 2. Stability: Although the performance of the model $(\\mathrm{HEGNN}_{l\\le6})$ ) using high-degree steerable features declines slightly when the geometric graph is small, overall, HEGNN performs better than other models. 3. Inference time: Our model\u2019s inference time is significantly faster than that of high-degree steerable models like TFN, reflecting the simplicity and efficiency of our HEGNN. ", "page_idx": 8}, {"type": "text", "text": "Results on MD17: The main results of MD17 experiment are shown in Table 4, with some data sourced from [45]. From these results, we draw the following insights: 1. Overall performance: Our HEGNN outperforms other models on six out of eight molecules. The effect on the remaining two molecules is only not as good as GMN [45] and this is because GMN introduces additional knowledge such as chemical bonds. 2. Advantage of high-degree vectors: Most of the best results are obtained on $\\mathrm{HEGNN}_{l\\le6}$ , indicating that the use of high-degree steerable features can enhance model expression capabilities. ", "page_idx": 8}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/26fa75892750c0219319c6b072717339048b8bdfd92b280894e7b458c739a445.jpg", "table_caption": ["Table 3: MSE and time-consuming ratio with EGNN [1] on $N$ -body system. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/9aada5fa3a3bf9f62c76727173dc6afae1165f8cb896995ae2ec5182ac7186d2.jpg", "table_caption": ["Table 4: Prediction error $(\\times10^{-2})$ ) on MD17 dataset. Results averaged across 3 runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Perturbation Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In practical scenarios, slight perturbations (such as molecular vibrations) can disrupt strict symmetry, potentially mitigating the conclusions outlined in Theorems 3.5 and 3.6. We therefore designed this perturbation experiment for a simple study and were surprised to find that HEGNN can still bring better robustness through the introduction of high-degree steerable features. ", "page_idx": 9}, {"type": "text", "text": "Design of experiments: We take the tetrahedron as an example and compare the cases of EGNN, $\\mathrm{HEGNN}_{l=3}$ , and $\\mathrm{HEGNN}_{l\\le3}$ when adding noise perturbations with results in Table 5. Here, $\\varepsilon$ represents the ratio of noise, and the modulus of the noise obeys $\\mathcal{N}(0,\\varepsilon\\cdot\\mathbb{E}[\\|\\vec{x}-\\vec{x_{c}}\\|]\\cdot I)$ . ", "page_idx": 9}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/11dcb4912a0b303357d303bb0e80ecdb8b95d0a2774743d3d293c3b59269befc.jpg", "table_caption": ["Table 5: Results for perturbation experiment. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Results: It can be observed that the performance of EGNN is slightly improved in the presence of noise (from $50\\%$ when $\\varepsilon\\,=\\,0.01$ to $60\\%$ when $\\varepsilon\\,=\\,0.5)$ ), while HEGNN demonstrates better robustness. Even though symmetry-breaking factors will make the geometric graph deviate from the symmetric state, the deviated graph is still roughly symmetric. In other words, the outputs of equivariant GNNs on the derivated graphs keep close to zero if the degree value is chosen to be those in Table 1, which will still lead to defective performance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we challenged the prevailing notion that higher-degree steerable vectors are unnecessary for achieving expressivity in equivariant Graph Neural Networks (GNNs). Through rigorous theoretical analysis, we demonstrated that equivariant GNNs constrained to 1st-degree representations inevitably degenerate to zero functions when applied to symmetric structures, such as $k$ -fold rotations and regular polyhedra. To address this limitation, we introduced HEGNN, a high-degree extension of the EGNN model. HEGNN enhances expressivity by integrating higher-degree steerable vectors while retaining the efficiency of the original model through a scalarization technique. Our extensive empirical evaluations on various datasets, including the symmetric toy dataset, $N$ -body, and MD17, validate our theoretical predictions. HEGNN not only adheres to our theoretical insights but also exhibits significant performance improvements over existing models. These findings underscore the critical role of higher-degree representations in fully leveraging the potential of equivariant GNNs. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was jointly supported by the following projects: the National Science and Technology Major Project under Grant 2020AAA0107300, the National Natural Science Foundation of China (No. 62376276, No. 62172422); Beijing Nova Program (No. 20230484278); Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098), the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (23XNKJ19); Public Computing Cloud, Renmin University of China. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In International Conference on Machine Learning, pages 9323\u20139332. PMLR, 2021.   \n[2] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, 2021.   \n[3] Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang, and Yang Liu. Equivariant pretrained transformer for unified geometric learning on multi-domain 3d molecules. In ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2024.   \n[4] Jun Wu, Xiangzhe Kong, Ningguan Sun, Jing Wei, Sisi Shan, Fuli Feng, Feng Wu, Jian Peng, Linqi Zhang, Yang Liu, et al. Better prior distribution for antibody design. Available at SSRN 4909414, 2024.   \n[5] Thorben Frank, Oliver Unke, and Klaus-Robert M\u00fcller. So3krates: Equivariant attention for interactions on arbitrary length-scales in molecular systems. Advances in Neural Information Processing Systems, 35:29400\u201329413, 2022.   \n[6] Yusong Wang, Tong Wang, Shaoning Li, Xinheng He, Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing. Nature Communications, 15(1):313, 2024.   \n[7] Runfa Chen, Jiaqi Han, Fuchun Sun, and Wenbing Huang. Subequivariant graph reinforcement learning in 3d environments. In International Conference on Machine Learning, pages 4545\u2013 4565. PMLR, 2023.   \n[8] Runfa Chen, Ling Wang, Yu Du, Tianrui Xue, Fuchun Sun, Jianwei Zhang, and Wenbing Huang. Subequivariant reinforcement learning in 3d multi-entity physical environments. In Forty-first International Conference on Machine Learning, 2024.   \n[9] Jiaqi Han, Wenbing Huang, Hengbo Ma, Jiachen Li, Josh Tenenbaum, and Chuang Gan. Learning physical dynamics with subequivariant graph neural networks. Advances in Neural Information Processing Systems, 35:26256\u201326268, 2022.   \n[10] Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, and Wenbing Huang. Equivariant spatiotemporal attentive graph networks to simulate physical dynamics. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, and Stefano Ermon. Geometric trajectory diffusion models. arXiv preprint arXiv:2410.13027, 2024.   \n[12] Yuxuan Song, Jingjing Gong, Hao Zhou, Mingyue Zheng, Jingjing Liu, and Wei-Ying Ma. Unified generative modeling of 3d molecules with bayesian flow networks. In The Twelfth International Conference on Learning Representations, 2023.   \n[13] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592\u201338610. PMLR, 2023.   \n[14] Yanru Qu, Keyue Qiu, Yuxuan Song, Jingjing Gong, Jiawei Han, Mingyue Zheng, Hao Zhou, and Wei-Ying Ma. Molcraft: Structure-based drug design in continuous parameter space. In Forty-first International Conference on Machine Learning, 2024.   \n[15] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022.   \n[16] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[17] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher $\\mathrm{Ng}$ -Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, pages 1\u20139, 2023.   \n[18] Ziyang Yu, Wenbing Huang, and Yang Liu. Rigid protein-protein docking via equivariant elliptic-paraboloid interface prediction. In The Twelfth International Conference on Learning Representations, 2024.   \n[19] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \n[20] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970\u20131981, 2020.   \n[21] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022.   \n[22] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14(1):579, 2023.   \n[23] Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. arXiv preprint arXiv:2010.02449, 2020.   \n[24] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International Conference on Machine Learning, pages 8867\u20138887. PMLR, 2022.   \n[25] Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, and Anima Anandkumar. Equivariant graph neural operator for modeling 3d dynamics. arXiv preprint arXiv:2401.11037, 2024.   \n[26] Omri Puny, Matan Atzmon, Edward J Smith, Ishan Misra, Aditya Grover, Heli Ben-Hamu, and Yaron Lipman. Frame averaging for invariant and equivariant network design. In International Conference on Learning Representations, 2021.   \n[27] Alexandre Agm Duval, Victor Schmidt, Alex Hernandez-Garcia, Santiago Miret, Fragkiskos D Malliaros, Yoshua Bengio, and David Rolnick. Faenet: Frame averaging equivariant gnn for materials modeling. In International Conference on Machine Learning, pages 9013\u20139033. PMLR, 2023.   \n[28] Yuelin Zhang, Jiacheng Cen, Jiaqi Han, Zhiqiang Zhang, Jun Zhou, and Wenbing Huang. Improving equivariant graph neural networks on large geometric graphs via virtual nodes learning. In Forty-first International Conference on Machine Learning, 2024.   \n[29] Jiaqi Han, Wenbing Huang, Tingyang Xu, and Yu Rong. Equivariant graph hierarchy-based neural networks. Advances in Neural Information Processing Systems, 35:9176\u20139187, 2022.   \n[30] Yusong Wang, Chaoran Cheng, Shaoning Li, Yuxuan Ren, Bin Shao, Ge Liu, Pheng-Ann Heng, and Nanning Zheng. Neural $\\bar{\\mathsf{p}}^{3}\\;\\mathsf{m}$ : A long-range interaction modeling enhancer for geometric gnns. Advances in Neural Information Processing Systems, 2024.   \n[31] Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, et al. A survey of geometric graph neural networks: Data structures, models and applications. arXiv preprint arXiv:2403.00485, 2024.   \n[32] Kristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388. PMLR, 2021.   \n[33] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e (3) equivariant message passing. In International Conference on Learning Representations, 2021.   \n[34] Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. Advances in Neural Information Processing Systems, 34:6790\u20136802, 2021.   \n[35] Chaitanya K Joshi, Cristian Bodnar, Simon V Mathis, Taco Cohen, and Pietro Lio. On the expressive power of geometric graph neural networks. In International Conference on Machine Learning, pages 15330\u201315355. PMLR, 2023.   \n[36] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. nti, Series, 2(9):12\u201316, 1968.   \n[37] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural Information Processing Systems, 35:11423\u201311436, 2022.   \n[38] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. Advances in Neural Information Processing Systems, 31, 2018.   \n[39] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. arXiv preprint arXiv:2207.09453, 2022.   \n[40] David J Griffiths and Darrell F Schroeter. Introduction to quantum mechanics. Cambridge university press, 2018.   \n[41] Lev Davidovich Landau and Evgenii Mikhailovich Lifshitz. Quantum mechanics: nonrelativistic theory, volume 3. Elsevier, 2013.   \n[42] J Thorben Frank, Oliver T Unke, Klaus-Robert M\u00fcller, and Stefan Chmiela. A euclidean transformer for fast and stable machine learned force fields. Nature Communications, 15(1):6539, 2024.   \n[43] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pages 2688\u20132697. PMLR, 2018.   \n[44] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017.   \n[45] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant graph mechanics networks with constraints. In International Conference on Learning Representations, 2022.   \n[46] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: sampling configurations for multi-body systems with symmetric energies. arXiv preprint arXiv:1910.00753, 2019.   \n[47] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263\u20131272. PMLR, 2017.   \n[48] Kristof T Sch\u00fctt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M\u00fcller. Schnet\u2013a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24), 2018.   \n[49] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2020.   \n[50] Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3) equivariant graph neural networks with complete local frames. In International Conference on Machine Learning, pages 5583\u20135608. PMLR, 2022.   \n[51] James F Epperson. An introduction to numerical methods and analysis. John Wiley & Sons, 2013.   \n[52] Michael Engel. Point group analysis in particle simulation data. arXiv preprint arXiv:2106.14846, 2021.   \n[53] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in Neural Information Processing Systems, 30, 2017.   \n[54] Hannah Lawrence, Vasco Portilheiro, Yan Zhang, and S\u00e9kou-Oumar Kaba. Improving equivariant networks with probabilistic symmetry breaking. In ICML 2024 Workshop on Geometrygrounded Representation Learning and Generative Modeling, 2024.   \n[55] Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. Advances in Neural Information Processing Systems, 35:650\u2013664, 2022.   \n[56] Kelin Xia and Guo-Wei Wei. Persistent homology analysis of protein structure, flexibility, and folding. International journal for numerical methods in biomedical engineering, 30(8):814\u2013844, 2014.   \n[57] Floor Eijkelboom, Rob Hesselink, and Erik J Bekkers. E(n) equivariant message passing simplicial networks. In International Conference on Machine Learning, pages 9071\u20139081. PMLR, 2023.   \n[58] Claudio Battiloro, Ege Karaismailo\u02d8glu, Mauricio Tec, George Dasoulas, Michelle Audirac, and Francesca Dominici. E (n) equivariant topological neural networks. arXiv preprint arXiv:2405.15429, 2024.   \n[59] Ralf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. Physical Review B, 99(1):014104, 2019.   \n[60] Genevieve Dusson, Markus Bachmayr, G\u00e1bor Cs\u00e1nyi, Ralf Drautz, Simon Etter, Cas van Der Oord, and Christoph Ortner. Atomic cluster expansion: Completeness, efficiency and stability. Journal of Computational Physics, 454:110946, 2022.   \n[61] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling. Advances in Neural Information Processing Systems, 35:2550\u2013 2563, 2022.   \n[62] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Generalist equivariant transformer towards 3d molecular interaction learning. In Forty-first International Conference on Machine Learning, 2023.   \n[63] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Full-atom peptide design with geometric latent diffusion. Advances in Neural Information Processing Systems, 2024.   \n[64] Miltiadis Kofinas, Naveen Nagaraja, and Efstratios Gavves. Roto-translated local coordinate frames for interacting dynamical systems. Advances in Neural Information Processing Systems, 34:6417\u20136429, 2021.   \n[65] Yi-Lun Liao, Brandon M Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. In The Twelfth International Conference on Learning Representations, 2024.   \n[66] Ziqiao Meng, Liang Zeng, Zixing Song, Tingyang Xu, Peilin Zhao, and Irwin King. Towards geometric normalization techniques in se(3) equivariant graph neural networks for physical dynamics simulations. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), pages 5981\u20135989, 2024.   \n[67] Zian Li, Xiyuan Wang, Yinan Huang, and Muhan Zhang. Is distance matrix enough for geometric deep learning? Advances in Neural Information Processing Systems, 36, 2024.   \n[68] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023.   \n[69] Weiliang Luo, Gengmo Zhou, Zhengdan Zhu, Yannan Yuan, Guolin Ke, Zhewei Wei, Zhifeng Gao, and Hang Zheng. Bridging machine learning and thermodynamics for accurate p k a prediction. JACS Au, 2024.   \n[70] Gengmo Zhou, Zhen Wang, Feng Yu, Guolin Ke, Zhewei Wei, and Zhifeng Gao. S-molsearch: 3d semi-supervised contrastive learning for bioactive molecule search. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.   \n[71] Fanmeng Wang, Hongteng Xu, Xi Chen, Shuqi Lu, Yuqing Deng, and Wenbing Huang. Mperformer: An se (3) transformer-based molecular perceptron. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 2512\u20132522, 2023.   \n[72] Fanmeng Wang, Wentao Guo, Minjie Cheng, Shen Yuan, Hongteng Xu, and Zhifeng Gao. Mmpolymer: A multimodal multitask pretraining framework for polymer property prediction. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM \u201924, 2024.   \n[73] Johannes Klicpera, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2020.   \n[74] Angxiao Yue, Dixin Luo, and Hongteng Xu. A plug-and-play quaternion message-passing module for molecular conformation representation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 16633\u201316641, 2024.   \n[75] Yang Zhang, Wenbing Huang, Zhewei Wei, Ye Yuan, and Zhaohan Ding. Equipocket: an e (3)-equivariant geometric graph neural network for ligand binding site prediction. In Forty-first International Conference on Machine Learning, 2024.   \n[76] Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for 3d molecular graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8096\u20138104, 2023.   \n[77] Ziyang Yu, Wenbing Huang, and Yang Liu. Force-guided bridge matching for full-atom time-coarsened dynamics of peptides. arXiv preprint arXiv:2408.15126, 2024.   \n[78] Qi Li, Rui Jiao, Liming Wu, Tiannian Zhu, Wenbing Huang, Shifeng Jin, Yang Liu, Hongming Weng, and Xiaolong Chen. Powder diffraction crystal structure determination using generative models. arXiv preprint arXiv:2409.04727, 2024.   \n[79] Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure prediction by joint equivariant diffusion. Advances in Neural Information Processing Systems, 36, 2024.   \n[80] Rui jiao, Xiangzhe Kong, Wenbing Huang, and Yang Liu. 3d structure prediction of atomic systems with flow-based direct preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.   \n[81] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. In The Eleventh International Conference on Learning Representations, 2022.   \n[82] Xiangzhe Kong, Wenbing Huang, and Yang Liu. End-to-end full-atom antibody design. In Proceedings of the 40th International Conference on Machine Learning, pages 17409\u201317429, 2023.   \n[83] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Theoretical Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Equivariance/Invariance of HEGNN ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we demonstrate the equivariance of our HEGNN. In order to further illustrate the connection between our HEGNN, EGNN, and TFN, a general proof is given here. ", "page_idx": 15}, {"type": "text", "text": "Table 6: Comparison between our HEGNN and the scalarization-based model representing EGNN [1], and the high-degree steerable model representing TFN [19]. HEGNN combines the scalarization trick of EGNN that only uses invariant scalars (0th degree steerable features) to interact between steerable features corresponding to different degrees, avoiding the high computational cost of using CG tensor products in TFN. ", "page_idx": 15}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/c4bfefc28b599501c71c0385d24d7865484aa2beef1751169079d9c6393e5c71.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Theorem A.1 (Equivariance/Invariance of HEGNN). $h_{i},\\tilde{v}_{i}^{(0)}$ in HEGNN is E(3) invariant, $\\vec{\\pmb{x}}_{i}$ is E(3) equivariant. In addition, all $\\tilde{v}_{i}^{(l)}$ are $O(3)$ equivariant and translation invariant when $l$ is odd; all $\\tilde{v}_{i}^{(l)}$ is $S O(3)$ equivariant and inversion/translation invariant to when $l$ even. ", "page_idx": 15}, {"type": "text", "text": "Proof. Consider a sequence composed of functions $\\{\\varphi_{i}:\\mathcal{X}^{(i-1)}\\to\\mathcal{X}^{(i)}\\}_{i=1}^{N}$ equivariant to a same group ${\\mathfrak H}$ , the equivariance lead to an interesting property that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varphi_{N}\\circ\\cdots\\circ\\varphi_{i+1}\\circ\\rho_{X^{(i)}}(\\mathfrak{h})\\varphi_{i}\\circ\\cdots\\circ\\varphi_{1}=\\varphi_{N}\\circ\\cdots\\circ\\varphi_{j+1}\\circ\\rho_{X^{(j)}}(\\mathfrak{h})\\varphi_{j}\\circ\\cdots\\circ\\varphi_{1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for all $i,j\\in\\{1,2,\\dots,N\\}$ and ${\\mathfrak{h}}\\in{\\mathfrak{H}}$ , which means that the group elements $\\mathfrak{h}$ can be freely exchanged in the composite sequence of equivariant functions. In particular, if one of the equivariant functions $(e.g.\\ \\varphi_{k})$ is replaced by an invariant function, the group element $\\mathfrak{h}$ will be absorbed, which means ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varphi_{N}\\circ\\cdots\\circ\\varphi_{k}\\circ\\cdots\\circ\\varphi_{i+1}\\circ\\rho_{X^{(i)}}(\\mathfrak{h})\\varphi_{i}\\circ\\cdots\\circ\\varphi_{1}=\\varphi_{N}\\circ\\cdots\\circ\\varphi_{1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds for all ${\\mathfrak{h}}\\in{\\mathfrak{H}}$ but only $i\\in\\{1,2,\\ldots,k\\}$ . Although $\\varphi_{N}\\circ\\cdots\\circ\\varphi_{k}$ is still equivariant, because the group elements must be input starting from $\\varphi_{1}$ , the overall $\\varphi_{N}\\circ\\cdots\\circ\\varphi_{1}$ is still an invariant function. That is to say, to conclude that the entire HEGNN is equivariant, we only need to prove that HEGNN is equivariant in initialization and each layer. ", "page_idx": 15}, {"type": "text", "text": "The initialization of HEGNN is based on spherical harmonics, which is similar to TFN. Spherical harmonics are inherently equivariant, that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\nY^{(l)}(R_{\\mathfrak{r}}\\vec{x})=D^{(l)}({\\mathfrak{r}})Y^{(l)}(\\vec{\\pmb{x}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that variables participating in the coefficient in Eq. (5) are all invariant scalars, so the initialization of HEGNN is consistent with the spherical harmonic function. Note that for Cartesian vectors, they can be aligned by arranging the spherical harmonics of 1st degree [19], that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\vec{x}}\\propto Y^{(1)}(\\pmb{\\vec{x}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From this perspective, EGNN can also be considered to be initialized using spherical harmonics, but this step is omitted because the value is proportional to the input Cartesian vector. ", "page_idx": 15}, {"type": "text", "text": "It is worth explaining that spherical harmonics are inversion invariant when $l$ is even, that is, ", "page_idx": 15}, {"type": "equation", "text": "$$\nY^{(l)}({\\mathfrak{m}}{\\vec{x}})=Y^{(l)}({\\vec{\\pmb{x}}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equivariant ones are not necessarily better than invariant ones. When we need to predict pseudovectors (such as moments), we need to inversion invariant 1st-degree steerable features, because pseudovectors are inversion invariant. This is why introducing the cross product $\\vec{x}\\times\\vec{y}$ (the result is inversion invariant) into a linear combination can only build a SE(3) equivariant network, but not a $\\mathrm{E}(3)$ network [50]. ", "page_idx": 16}, {"type": "text", "text": "In fact, inversion equivariant/invariant high degree steerable features can be obtained by calculating the CG tensor product of spherical harmonics and inversion equivariant Cartesian vectors like $\\vec{\\pmb{x}}_{i}-\\vec{\\pmb{x}}_{c}$ [39]. Moreover, the equivariance at each layer is also easy to prove and the internal Wigner-D matrix can be extracted through the CG tensor product [31]. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{D}^{(l)}(\\mathfrak{h})\\tilde{\\pmb{v}}^{(l)}=\\left[\\left(\\pmb{D}^{(l_{1})}(\\mathfrak{h})\\tilde{\\pmb{v}}^{(l_{1})}\\right)\\otimes_{\\mathrm{cg}}^{W}\\left(\\pmb{D}^{(l_{2})}(\\mathfrak{h})\\tilde{\\pmb{v}}^{(l_{2})}\\right)\\right]^{(l)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When the output result is an invariant scalar, the Wigner-D matrix degenerates into a trivial representation 1. Norm and inner product are all special cases of this type, so equivariance is established. From this perspective, EGNN and HEGNN are equivalent to using only the weight coefficients of $(l,\\,l)\\rightarrow0$ and $\\bar{(0,\\,l)}\\rightarrow l$ . Similar ideas include the steerable MLPs in [33] . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Other Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem A.2 (Theorem 3.4). Suppose that $f^{(l)}$ is an O(3)-equivariant function on geometric graphs, regarding the group representation $\\rho^{(l)}$ defined in $E q$ . (2). Then, for any symmetric graph $\\mathcal{G}$ induced by the group ${\\mathfrak{H}}\\leq{\\mathrm{O}}(3)$ , namely, $\\forall\\mathcal{G}\\in\\mathbb{G}(\\mathfrak{H})$ , we always have ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{(l)}({\\mathcal{G}})=\\rho^{(l)}({\\mathfrak{H}})f^{(l)}({\\mathcal{G}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here we have defined group average as $\\begin{array}{r}{\\rho^{(l)}(\\mathfrak{H}):=\\frac{1}{|\\mathfrak{H}|}\\sum_{\\mathfrak{h}\\in\\mathfrak{H}}\\rho^{(l)}(\\mathfrak{h}).}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. If $f^{(l)}$ is $\\mathrm{O}(3)$ -equivariant, then it is also a ${\\mathfrak H}$ -equivariant, thus ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{(l)}(\\mathcal{G})=\\frac{1}{\\left|\\mathfrak{H}\\right|}\\sum_{\\mathfrak{h}\\in\\mathfrak{H}}f^{(l)}(\\mathfrak{h}\\cdot\\mathcal{G})=\\frac{1}{\\left|\\mathfrak{H}\\right|}\\sum_{\\mathfrak{h}\\in\\mathfrak{H}}\\rho(\\mathfrak{h})f^{(l)}(\\mathcal{G})=\\left(\\frac{1}{\\left|\\mathfrak{H}\\right|}\\sum_{\\mathfrak{h}\\in\\mathfrak{H}}\\rho(\\mathfrak{h})\\right)f^{(l)}(\\mathcal{G})=\\rho(\\mathfrak{H})f^{(l)}(\\mathcal{G}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem A.3 (Theorem 3.5). If and only if the matrix $I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})$ is non-singular, the O(3)- equivariant function $f^{(l)}$ is always a zero function on $\\mathcal{G}$ , namely, ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{(l)}({\\mathcal{G}})\\equiv\\mathbf{0},\\quad\\forall{\\mathcal{G}}\\in\\mathbb{G}({\\mathfrak{H}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. From Theorem 3.4, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\nf^{(l)}(\\mathcal{G})=\\rho(\\mathfrak{H})f^{(l)}(\\mathcal{G})\\iff\\left(I_{2l+1}-\\rho^{(l)}(\\mathfrak{H})\\right)f^{(l)}(\\mathcal{G})=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the theorem holds for basic knowledge of linear algebra. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.4 (Theorem 3.6). For a finite group ${\\mathfrak H}$ with its representation $\\rho^{(l)},\\;\\rho^{(l)}(\\mathfrak{H})$ is a zero matrix (i.e., $\\boldsymbol{\\rho}^{(l)}(\\mathfrak{H})=\\mathbf{0},$ ) $i f$ and only $i f\\mathrm{tr}(\\rho^{(l)}(\\mathfrak{H}))=0$ . In this case, ${f^{(l)}}(\\mathcal{G})\\equiv\\mathbf{0},\\forall\\mathcal{G}\\in\\mathbb{G}(\\mathfrak{H})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. It is obvious that $\\rho^{(l)}(\\mathfrak{H})\\,=\\,\\mathbf{0}\\;\\implies\\;\\mathrm{tr}(\\rho^{(l)}(\\mathfrak{H}))\\,=\\,0$ since all elements are zero not to mention the main diagonal, and we only to prove $\\mathrm{tr}(\\rho^{(l)}(\\mathfrak{H}))=0\\implies\\rho^{(l)}(\\mathfrak{H})=\\mathbf{0}$ . ", "page_idx": 16}, {"type": "text", "text": "A basic fact is ${\\mathfrak{h}}\\cdot{\\mathfrak{H}}={\\mathfrak{H}}$ , thereby ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho^{(l)}({\\mathfrak{h}})\\rho^{(l)}({\\mathfrak{H}})=\\rho^{(l)}({\\mathfrak{H}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we can use group average and get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\rho^{(l)}({\\mathfrak{H}})\\right)^{2}=\\rho^{(l)}({\\mathfrak{H}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Such operation can be repeated many times, so that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\rho^{(l)}({\\mathfrak{H}})\\right)^{k}=\\rho^{(l)}({\\mathfrak{H}}),\\qquad\\forall k\\in\\mathbb{N}_{+}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we calculate the trace for each matrix and find ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\left(\\rho^{(l)}(\\mathfrak{H})\\right)^{k}\\right)=\\mathrm{tr}\\left(\\rho^{(l)}(\\mathfrak{H})\\right)=0,\\qquad\\forall k\\in\\mathbb{N}_{+}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Newton\u2019s identity [51], all eigenvalues of the matrix $\\rho({\\mathfrak{H}})$ are 0, that is, the matrix is a zero matrix. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Table 7: The traces of symmetric groups based on [52]. Trace for polyhedral groups can be calculated by $D^{(l)}(H)=\\lfloor l/r\\rfloor\\stackrel{}{+}b[l\\,\\mathrm{mod}\\,\\bar{r}]$ with repeat length $r$ , where $b$ is a string only with 0 or 1. For exmaple, for Tetrahedral group $T$ , $D^{(5)}(T)=\\lfloor5/6\\rfloor+b[5\\,\\mathrm{mod}\\,6]=0+0=0.$ . ", "page_idx": 17}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/97c1898551bb0e5390743e600e0277fd37c95f975cad98624a631603284c3842.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Theorem A.5 (Theorem 4.1). For any geometric graph, there exists a bijection between the set of inner products $\\{z_{i j}^{(l)}\\}_{l=1}^{|\\mathbb{A}_{i j}|}$ given by Eq. (10) and the set of edge angles $\\mathbb{A}_{i j}\\;=\\;\\{\\theta_{i s,j t}\\::=$ \u27e8\u20d7xis, \u20d7xjt\u27e9}s\u2208N(i),t\u2208N(j). ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that the Legendre polynomial is a set of orthogonal polynomial bases, and there is a bijection to the power function polynomial space, that is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{span}\\left\\{\\sum_{n=1}^{|\\mathbb{A}_{i j}|}P^{(l)}\\left(\\cos\\theta_{n}\\right)\\right\\}_{l=0}^{M}=\\operatorname{span}\\left\\{\\sum_{n=1}^{|\\mathbb{A}_{i j}|}\\cos^{\\alpha}\\theta_{n}\\right\\}_{\\alpha=0}^{M},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M$ is any non-negative integer represents the degree of the polynomial space. Moreover, from the knowledge of Newton\u2019s identities, the space of power sums can be converted to space of elementary symmetric polynomials as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{span}\\left\\{\\sum_{n=1}^{|\\mathbb{A}_{i j}|}\\cos^{\\alpha}\\theta_{n}\\right\\}_{\\alpha=0}^{M}=\\mathrm{span}\\left\\{\\sum_{1\\leq n_{1}<n_{2}<...n_{m}\\leq|A_{i j}|}\\left(\\prod_{\\nu=1}^{k}\\cos\\theta_{n_{k}}\\right)\\right\\}_{\\alpha=0}^{M}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Vieta\u2019s formulas, when $M=\\left|\\mathbb{A}_{i j}\\right|$ , with the $M+1$ polynomial in the space of elementary symmetric polynomials being coefficients, we can build a $|\\mathbb{A}_{i j}|$ -degree polynomial with $\\{\\cos\\theta\\mid\\theta\\in$ $\\mathbb{A}_{i j}\\}$ as its all roots7. Since all angles are in $[0,\\pi)$ , the cosine uniquely determines the angle value, and the proposition is established. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.3 Further Discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our theory in fact shows that the degeneration of global features of a certain degree (in Table 1) are inevitable on symmetric geometric graphs. This raises two points worth discussing: ", "page_idx": 17}, {"type": "text", "text": "1. The degree not indicated to degenerate not necessarily produce a non-zero representation, which may still be affected by the model form and the edge situation.   \n2. There are some tricks to get around this degeneration: for example, making the output a set or relaxing equivariance constraints (e.g. probabilistic symmetry breaking [54]). ", "page_idx": 17}, {"type": "text", "text": "It is worth mentioning that outputting a set can solve most problems, although such operators may be quite intractable to implement in computer systems. The failure cases of frame averaging [26; 27] and Neural $\\mathrm{P^{3}M}$ [30] which depend on singular value decomposition or eigenvalue decomposition, is caused by the non-unique matrix decomposition. Some other examples include ComENet [55], which uses the scatter_min() operator in PyTorch to extract the nearest neighbors, making it intractable to handle the situation where multiple neighbors are simultaneously closest, which is quite common in chemical molecules (e.g. - $\\mathrm{{\\cdotCH_{3}}}$ , $\\mathrm{-NH_{2}}$ ). ", "page_idx": 18}, {"type": "text", "text": "Moreover, from Theorem 4.1, we show the expressivity of our HEGNN, which is able to recover the information of all angles between each pair of edge. However, it should be noted that $\\lvert\\mathbb{A}_{i j}\\rvert$ may be an extremely large number, which is unacceptable in practical applications to achieve completeness. The same problem also arises in discussions based on CG tensor product models (e.g. TFN [19]), such as discussions based on $D$ -spanning [23], because a sufficiently high-degree $D$ is unacceptable. However, in terms of actual results, the performance of both our HEGNN and TFN is remarkable. From this perspective, how to bridge the gap between completeness and actual performance with features of limited channels and degrees is a question worth considering. ", "page_idx": 18}, {"type": "text", "text": "To get the ball rolling, we raise an interesting question here. Is there a performance gap between this type of purely mathematical representation and other features based on physical and biochemical prior knowledge? ", "page_idx": 18}, {"type": "text", "text": "\u2022 Purely mathematical representation: topological characteristics [56\u201358], cluster expansion basis [59; 60], subgraph blocks [61\u201363], frames [26; 27; 64], normalization operators [65; 66];   \n\u2022 Features based on physical and biochemical prior knowledge: distance matrix [67\u201372], chemical bond length, angle and dihedral angle [55; 73\u201375], force [76; 77], fractional coordinates [78\u201380], canonical ordering [81; 82]. ", "page_idx": 18}, {"type": "text", "text": "In fact, some of these features are directly related (e.g. frames and fractional coordinates). How to construct effective and interpretable pure mathematical features based on those with prior knowledge will become a key point in network design, and we will consider further exploration in future work. ", "page_idx": 18}, {"type": "text", "text": "B More Experimental Details and Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Comparison of parameters between models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Like EGNN [1], different features use different numbers of channels, so the inference time does not obviously reflect the time complexity of $\\mathcal{O}(L^{2})$ . We list the details of our HEGNN of different degree in Table 8. Intuitively, we add one of each steerable feature on the basis of EGNN (using 64 invariant scalars and 2 Cartesian vectors, i.e. coordinate and velocity). ", "page_idx": 18}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/cd696c220f56912733399a113e4ef16838c239714e3ae791423a4f6e15435c77.jpg", "table_caption": ["Table 8: Channels for steerable features of different degrees and total dimensions of HEGNN of different degrees. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/fad8e83075eab07937f666df01f4aaeb2c951670d54c768be2fd3c58bc77cdc5.jpg", "table_caption": ["Table 9: Parameters and inference time (on 100-body dataset) of EGNN and other high-degree models. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.2 Experiment on $k$ -fold Structure. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The results on the $k$ -fold structure are completely consistent with the conclusion of Table 1. The few results that cannot reach $100\\%$ are due to the small number of training epochs, so the classifier fails to perform perfect classification. In fact, the accuracy of models can achieve $100.00{\\scriptstyle\\pm\\,0.0}$ after increasing the number of training rounds of the model like 500 epochs. Since from 2nd-degree steerable features can distinguish $\\mathcal{G}_{\\mathrm{0}}$ and $\\mathcal{G}_{1}$ , and HEGNN/TFN $^{\\mathrm{MACE}_{l\\leq L}}$ contain 2nd-degree steerable features when $l\\geq3$ , the extra results are hidden but all their values are $100.00{\\scriptstyle\\pm\\,0.0}$ . ", "page_idx": 19}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/fba0cf558ae821c2b09dfcece1ff40bcd8d9804b06690213106311e5eafd86ad.jpg", "table_caption": ["Table 10: $k$ -fold symmetric structures. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.3 Further Verification on Regular Polyhedra ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Implementation details: Since the $\\tt e3n n$ [39] library only implements spherical harmonics up to the 11th-degree, we verified the expressivity of our HEGNN in disguise. In this experiment, we measure the expressivity of our HEGNN by calculating whether the high-degree feature $\\tilde{v}_{i}^{(l)}$ updates on regular polyhedra. Namely, if $\\Delta\\tilde{{v}}_{i}^{(L)}=0$ , then $\\mathrm{HEGNN}_{l=L}$ loses the expressivity. This judgment is necessary and sufficient. We calculated the sum of all degree vectors from $L=1$ to 30, which covers the maximum repeat length $r$ in Table 7. We implemented the calculation of the high-degree features based on scipy [83] library. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Results: Our calculation results are shown in Table 11 which is completely consistent with the theoretical results in Table 7. The simple experiment shows that the sum of spherical harmonics $\\begin{array}{r}{\\sum_{i}Y^{(l)}(\\Vec{\\mathbf{x}}_{i}-\\Vec{\\mathbf{x}}_{c})}\\end{array}$ , as a function on graph, will actually vanish on regular polyhedra for some integer degree $l$ . ", "page_idx": 20}, {"type": "text", "text": "Table 11: Expressivity analysis of HEGNN using sums of spherical harmonics. Here, \"True\" indicates that our HEGNN can distinguish the orientations, meaning the norm of the sum of spherical harmonics is greater than 1. \"False\" in the table means that no distinction can be made, with the norm of the corresponding spherical harmonic being less than $10^{-3}$ . ", "page_idx": 20}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/66fc6f934119e9adca8df15badb548ce9b5b62cb8b35428bdc050a2d8c05ca83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.4 Results on other dataset settings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The $N$ -body dataset setting of this paper refers to FastEGNN $[28]^{8}$ , that is, 5,000 samples are used as the training set (instead of 3,000). We tested the situation of using different data segmentation in Table 12. We also add ClofNet [50], a local frame based scalarization method and SEGNN [33] (select $l=1$ according to the optimal situation in the paper) and MACE [37] $(l=2)$ ), two classic high-degree steerable models for comparison. ", "page_idx": 20}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/e270f7b5be768b3f9cab597e816b294e91694a79aaf52d434a920000e0164cda.jpg", "table_caption": ["Table 12: Results of $N$ -body dataset under two partitions. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/5fd75fb665acd08781f5b224688216614c97a41df7f6ee3c4ba3a34ee59d8aab.jpg", "table_caption": ["Table 13: Comparison between EGNN, SEGNN and HEGNN on $N$ -body from [33] "], "table_footnote": ["Note that SEGNN in Table 12 does not show the effect in the original paper. We also tried to reproduce the dataset in the original paper [33], and the effect of SEGNN on 5-body dataset can be reproduced. However, see Table 13, it is also shown that SEGNN performs poorly on larger datasets. "], "page_idx": 21}, {"type": "text", "text": "We found that the GMN-L method proposed in [29] generally performs the best on MD17 dataset, but our HEGNN-6 also achieves comparable performance to GMN-L in most cases. Given that GMN-L requires careful handcrafting of constraints for chemical bonds into the model design, our model\u2019s ability to derive promising results without such enhancements supports its competitive performance. ", "page_idx": 21}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/f0b69441246090dac030372f239926ecdbce4db33aa5e0ec4731103e2c7a7a97.jpg", "table_caption": ["Table 14: Prediction error $\\left(\\times10^{-2}\\right)$ ) on MD17 dataset. Results averaged across 3 runs. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.5 Expressiveness of initialization layer ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Note that both EGNN [1] and $\\mathrm{HEGNN}_{l\\le1}$ only use Cartesian vectors. However, in Table 3, the effect of the latter is greatly improved. We speculate that there are two possible factors for this improvement: 1) the extra layer of message passing brought in by Eq. (5); 2) the multi-channel of Cartesian vectors (see Table 8). Therefore, we tested the effect of $\\mathrm{HEGNN}_{l\\le1}$ -3layers, and the results are shown in Table 15. From the improvement, the first factor contributes more. ", "page_idx": 22}, {"type": "table", "img_path": "M0ncNVuGYN/tmp/d74d9fe3c957879baf1e336ae8ce75a6f3603b1b37679274173fc65ba36de92c.jpg", "table_caption": ["Table 15: Comparison between EGNN and HEGNN on $N$ -body. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C Limitation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our current experiments are mainly limited to testing on small molecules and have not been verified on large-scale molecules or large-scale physical systems. Whether our HEGNN is effective on large-scale geometric graph data sets remains to be verified. ", "page_idx": 22}, {"type": "text", "text": "D Broader Impact ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our research belongs to the field of AI for Science. The HEGNN proposed in this article is expected to better model scientific problems, thereby promoting the development of higher-precision and efficient AI scientific models. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our abstract and introduction nicely summarize the theoretical contributions mentioned in $\\S\\,3$ , the model design mentioned in $\\S\\,4$ , and the experimental results mentioned in $\\S\\ S$ . ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our current experiments are mainly limited to testing on small molecules and have not been verified on large-scale molecules or large-scale physical systems. Whether our HEGNN is effective on large-scale geometric graph data sets remains to be verified. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our main theoretical results are given in $\\S\\ 3$ and the proofs are given in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The details of our experimental results are in \"Design of experiments\" in $\\S\\ 5.1$ and \"Datasets\" in $\\S\\ 5.2$ . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: https://github.com/GLAD-RUC/HEGNN. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the training and test details have listed in \"Design of experiments\" in $\\S\\ 5.1$ and \"Datasets\" in $\\S\\ 5.2$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report the variance of the model runs in Table 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We use NVIDIA A100-80G, which has written in $\\S\\ S.2$ . ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research belongs to the field of AI for Science. The HEGNN proposed in this article is expected to better model scientific problems, thereby promoting the development of higher-precision and efficient AI scientific models. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper poses no such risk. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All existing assets used in our paper have used with citation. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: https://github.com/GLAD-RUC/HEGNN. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]