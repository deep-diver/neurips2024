[{"figure_path": "qp5VbGTaM0/figures/figures_2_1.jpg", "caption": "Figure 1: Framework of S-DPO. Different from existing methods which fine-tune LMs with a language modeling loss without tailoring for recommendations, S-DPO proposes to explicitly instill ranking information into LMs. To take one step further, S-DPO incorporates multiple negatives in user preference data and generalizes pairwise DPO loss to softmax ranking loss.", "description": "This figure illustrates the framework of the proposed Softmax-DPO (S-DPO) method and contrasts it with existing LM-based recommendation approaches.  It shows three stages: Supervised Fine-Tuning, Direct Preference Optimization, and Softmax-DPO.  The first stage uses autoregressive loss to fine-tune the language model (LM). The second stage utilizes pairwise ranking loss with pairwise preference data. The final stage, S-DPO, incorporates multiple negatives into the preference data and leverages softmax ranking loss for enhanced performance in distinguishing preferred items from negative ones.  It highlights the evolution of the method from language modeling to a direct preference optimization approach. ", "section": "2 Preliminary"}, {"figure_path": "qp5VbGTaM0/figures/figures_7_1.jpg", "caption": "Figure 2: Study on S-DPO. (2a) Ablation study of S-DPO compared with SFT and DPO on three datasets. (2b) Comparison of the trend of validation loss between DPO and S-DPO on LastFM. (2c) Comparison of the reward of preferred items between DPO and S-DPO on LastFM.", "description": "This figure presents a comprehensive analysis of the proposed S-DPO method. Subfigure (a) shows an ablation study comparing S-DPO's performance against the standard supervised fine-tuning (SFT) and the original DPO method across three datasets (LastFM, Goodreads, and MovieLens).  Subfigure (b) illustrates the validation loss curves for both DPO and S-DPO on the LastFM dataset, highlighting the faster convergence of S-DPO.  Finally, subfigure (c) displays the reward (preference score) for the preferred item over the training steps for both methods on the LastFM dataset, demonstrating that S-DPO consistently assigns a higher reward to the preferred item.", "section": "4.2 Study on S-DPO"}, {"figure_path": "qp5VbGTaM0/figures/figures_8_1.jpg", "caption": "Figure 2: Study on S-DPO. (2a) Ablation study of S-DPO compared with SFT and DPO on three datasets. (2b) Comparison of the trend of validation loss between DPO and S-DPO on LastFM. (2c) Comparison of the reward of preferred items between DPO and S-DPO on LastFM.", "description": "This figure presents the results of an ablation study comparing S-DPO with supervised fine-tuning (SFT) and DPO on three different datasets, demonstrating the superior performance of S-DPO.  Additionally, it shows the validation loss and reward of preferred items curves over training steps for both DPO and S-DPO on the LastFM dataset, highlighting S-DPO's faster convergence and better rewards for preferred items.", "section": "4.2 Study on S-DPO"}]