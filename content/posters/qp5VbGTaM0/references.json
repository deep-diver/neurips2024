{"references": [{"fullname_first_author": "Steffen Rendle", "paper_title": "Item recommendation from implicit feedback", "publication_date": "2022-01-01", "reason": "This paper provides a foundational understanding of implicit feedback in recommendation systems, which is directly relevant to the core methodology of the current paper."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a crucial method that forms the basis of the proposed S-DPO in this work."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "As one of the base large language models used in the experiments, Llama 2 is foundational to the empirical results presented."}, {"fullname_first_author": "Keqin Bao", "paper_title": "TALLRec: An effective and efficient tuning framework to align large language model with recommendation", "publication_date": "2023-01-01", "reason": "TALLRec is a strong baseline for LM-based recommendation methods, providing context for comparison and highlighting the advancement made by the current work."}, {"fullname_first_author": "Jiayi Liao", "paper_title": "LLaRA: Aligning large language models with sequential recommenders", "publication_date": "2023-12-02", "reason": "LLaRA, another strong baseline, directly addresses the problem of aligning LMs with recommendation tasks, thus offering valuable context for the contributions of the current work."}]}