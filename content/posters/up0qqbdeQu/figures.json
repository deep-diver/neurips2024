[{"figure_path": "up0qqbdeQu/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our methods applied to zero-shot CLIP (ZSCLIP) [32]. (a) Class concept is formed from the text descriptions that contain rich contextual information with relevant class names and other related words, yielding substantially improved performance without aligning with visual features yet. (b\u2192c) Context-guided visual feature is transformed from visual feature so that it is in the same linear space as class concept representation, yielding significantly improved performance.", "description": "This figure illustrates the three steps of the proposed method applied to the zero-shot CLIP model.  The first step (a) shows the baseline using only hand-crafted class name prompts. Step (b) improves the model by introducing a class concept representation formed from rich contextual descriptions, achieving a mean average precision (mAP) improvement. Finally, step (c) further enhances the model by aligning the visual features with the class concept representation through a context-guided visual feature, resulting in the highest mAP score.  Each step progressively improves the model's ability to exploit rich contextual information for multi-label image recognition.", "section": "3 Method"}, {"figure_path": "up0qqbdeQu/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM's text features from the rich contextual descriptions associated with each class name are used to construct the class concept. 2) Context-guided visual features: VLM's visual features are sequentially transformed onto the class concept representation space using (b) sequential attention mechanism.", "description": "This figure illustrates the overall pipeline of the proposed method for multi-label image recognition. It shows how the class concept representation is constructed from rich contextual text descriptions and then aligned with the visual features using sequential attention. The left panel (a) provides an overview of the process, highlighting the class concept representation and context-guided visual features. The right panel (b) shows the details of the sequential attention mechanism, which enhances the alignment between visual features and the class concepts in the same linear space.", "section": "3 Method"}, {"figure_path": "up0qqbdeQu/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM's text features from the rich contextual descriptions associated with each class name are used to construct the class concept. 2) Context-guided visual features: VLM's visual features are sequentially transformed onto the class concept representation space using (b) sequential attention mechanism.", "description": "This figure illustrates the overall pipeline of the proposed method for multi-label image recognition.  It shows two main stages: (a) Class concept representation, where the model uses contextual text descriptions to create a vector representation of each class concept; and (b) Context-guided visual representation, where the model uses sequential attention to transform the visual features from the vision language model to align them with the class concept representations. The sequential attention mechanism is detailed in a separate sub-figure, showing the process of transforming visual features to match the class concept vector space.", "section": "3 Method"}, {"figure_path": "up0qqbdeQu/figures/figures_13_1.jpg", "caption": "Figure 4: Results of hyperparameter searching of CALIP [19] on MS-COCO [26] on \u03b22 and \u03b23. Applying the parametric-free attention module of CALIP consistently decreases performance as compared to the zero-shot CLIP (ZSCLIP) [32].", "description": "This figure shows the results of a hyperparameter search performed using the CALIP method on the MS-COCO dataset.  The search focused on two parameters (\u03b22 and \u03b23) related to the attention module within the CALIP model. The 3D bar chart shows that the performance (measured by mean average precision or mAP score) of CALIP consistently decreases with the attention module compared to a zero-shot CLIP model. This suggests that the attention module in CALIP may not be beneficial in this context and that a simpler zero-shot approach could perform better.", "section": "4.3.3 Analysis of Inference Time"}, {"figure_path": "up0qqbdeQu/figures/figures_14_1.jpg", "caption": "Figure 5: Additional examples of local alignment enhancement via our method. We visualized the test image in the left column and its corresponding spatial similarity map of each class name in the right column. The yellow and red boxes refer to the bounding boxes for different labels in a multi-label setting. By applying our method, the local alignment is enhanced across multiple objects in a test image, thereby suppressing false-positive predictions.", "description": "This figure shows examples where the proposed method improves local alignment between visual and textual features, resulting in better object recognition and fewer false positives in multi-label image classification.  The left column displays the input images with bounding boxes highlighting different objects (labels). The right column shows the spatial similarity maps, with the left side representing results without the proposed method and the right side with the method applied. The improved local alignment is evident in the right-hand maps by a clearer focus on the relevant areas corresponding to each label.", "section": "3.2 Context-Guided Visual Feature"}, {"figure_path": "up0qqbdeQu/figures/figures_15_1.jpg", "caption": "Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM's text features from the rich contextual descriptions associated with each class name are used to construct the class concept. 2) Context-guided visual features: VLM's visual features are sequentially transformed onto the class concept representation space using (b) sequential attention mechanism.", "description": "This figure illustrates the overall pipeline of the proposed method for multi-label image recognition. It consists of two main stages: 1) Class concept representation: This stage uses a Vision-Language Model (VLM) to extract text features from rich contextual descriptions associated with each class name. These features are then used to construct a class concept representation. 2) Context-guided visual representation: This stage takes the VLM's visual features as input and sequentially transforms them into the same linear space as the class concept representation using a sequential attention mechanism.  The sequential attention mechanism is further detailed in sub-figure (b), showing how softmax is applied to weight the relevance of the various text features.", "section": "3 Method"}]