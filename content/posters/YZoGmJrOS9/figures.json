[{"figure_path": "YZoGmJrOS9/figures/figures_3_1.jpg", "caption": "Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.", "description": "This figure provides a table and a diagram summarizing the 12 hybrid models explored in the paper.  The table details the variations in three key architectural sub-blocks: positional embeddings, feed-forward networks, and normalization methods.  Each variation is assigned a label for easy reference in the text. The diagram visually represents how the different variations affect the overall model architecture, showing the various combinations of these sub-blocks.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_5_1.jpg", "caption": "Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are 99% confidence intervals.", "description": "This figure showcases squared error profiles for specific model-task pairs that demonstrate suboptimal performance.  It highlights the performance of different model architectures (GPT-2, GPT-2 with RMS norm, Mamba) on two tasks: Sparse Linear and Decision Tree Regression.  The shaded regions represent the 99% confidence intervals for each model's performance, indicating the uncertainty in the measurements.  The plots illustrate how certain architectural modifications lead to models that fail to achieve near-optimal regression performance compared to baseline models (such as Lasso for Sparse Linear Regression).  This visualization helps in understanding the impact of architectural choices on ICL ability.", "section": "4 Results"}, {"figure_path": "YZoGmJrOS9/figures/figures_5_2.jpg", "caption": "Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are 99% confidence intervals.", "description": "This figure displays the squared error for two specific tasks: Sparse Linear and Decision Tree.  It highlights instances where model performance deviates significantly from optimal or near-optimal behavior. The graphs show squared error as a function of context length. Shaded regions represent 99% confidence intervals, indicating the uncertainty around the error measurements.  These examples were selected because they showcase interesting, non-optimal behaviors observed in the experiments, helping to illustrate challenges and variations in model performance across different tasks and architectures.", "section": "Results"}, {"figure_path": "YZoGmJrOS9/figures/figures_6_1.jpg", "caption": "Figure 4: Detailing plots to showcase GPT-2 RMS SwiGLU (model 1.4) learning a more general but sub-optimal regression scheme when trained on Sparse Linear. Shaded regions are 99% confidence intervals.", "description": "This figure shows the squared error for the GPT-2 RMS SwiGLU model (model 1.4) trained on the Sparse Linear task, evaluated on both Sparse Linear and Linear tasks. The left subplot (a) displays the squared error for different checkpoints (100k, 200k, 300k, 400k, and 500k steps) of the model during training on Sparse Linear. It shows that the model converges to a suboptimal solution, similar to the Least Squares solution. The right subplot (b) shows the squared error when this model is evaluated on the Linear regression task, instead of the Sparse Linear task that it was trained on. This further suggests that the model converges to a suboptimal least squares regression scheme rather than the optimal lasso regression scheme.", "section": "3.3 Evaluation"}, {"figure_path": "YZoGmJrOS9/figures/figures_7_1.jpg", "caption": "Figure 4: Detailing plots to showcase GPT-2 RMS SwiGLU (model 1.4) learning a more general but sub-optimal regression scheme when trained on Sparse Linear. Shaded regions are 99% confidence intervals.", "description": "The figure shows two plots.  The left plot (a) displays the squared error of GPT-2 RMS SwiGLU model checkpoints at various training stages (50k, 100k, 200k, 300k, 500k steps) when trained on the Sparse Linear task.  It compares these results to the baseline performances of GPT-2, Least Squares, and Lasso. It highlights how, despite initially performing similarly to Lasso, the model converges to the Least Squares solution. The right plot (b) shows the same model evaluated on the Linear Regression task.  Surprisingly,  it outperforms other models. This demonstrates that while the model failed to optimize for the Sparse Linear task, it learned a generalizable regression scheme.", "section": "Results"}, {"figure_path": "YZoGmJrOS9/figures/figures_7_2.jpg", "caption": "Figure 4: Detailing plots to showcase GPT-2 RMS SwiGLU (model 1.4) learning a more general but sub-optimal regression scheme when trained on Sparse Linear. Shaded regions are 99% confidence intervals.", "description": "This figure shows two plots. The left plot displays the squared error for model 1.4 (GPT-2 RMS SwiGLU) trained on Sparse Linear at different checkpoints (100k, 200k, 300k, 400k, and 500k steps) against the context length.  It demonstrates that the model converges to the least-squares solution, not the optimal Lasso solution. The right plot shows the squared error when the same model is evaluated on a Linear Regression task.  The performance is better than other models which suggests the model learned a different regression scheme compared to the Linear Regression task.", "section": "3.3 Evaluation"}, {"figure_path": "YZoGmJrOS9/figures/figures_7_3.jpg", "caption": "Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are 99% confidence intervals.", "description": "This figure displays the squared error profiles for two specific tasks (Sparse Linear and Decision Tree) across various context lengths. It highlights the performance differences between different model architectures (GPT-2, GPT-2 with variations, and Mamba).  The shaded regions represent 99% confidence intervals, indicating the uncertainty in the error measurements.  The plots reveal that certain model architectures either fail to converge to the optimal solutions or converge to suboptimal ones, demonstrating how architectural choices impact ICL performance.", "section": "4 Results"}, {"figure_path": "YZoGmJrOS9/figures/figures_7_4.jpg", "caption": "Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are 99% confidence intervals.", "description": "This figure shows the squared error profiles for two specific tasks: Sparse Linear and Decision Tree.  It highlights instances where model performance deviates significantly from the expected optimal behavior.  The plots illustrate how certain model architectures struggle to learn these tasks effectively, indicating suboptimal convergence to regression schemes. Shaded areas represent 99% confidence intervals.", "section": "4 Results"}, {"figure_path": "YZoGmJrOS9/figures/figures_12_1.jpg", "caption": "Figure 7: The GPT-2, Llama, and Mamba architectures used in our regression tasks", "description": "This figure presents the base architectures used in the paper's experiments.  It shows three different model architectures: GPT-2, Llama, and Mamba. Each architecture is depicted as a block diagram, illustrating the flow of information through the different layers (linear, layer normalization, multi-head attention, feed-forward networks) and components (absolute positional embedding, rotary positional embedding). The figure highlights the key architectural differences between the three models, such as the type of feed-forward network (GELU MLP vs. SwiGLU vs. Mamba Mixer), normalization method (Layer Norm vs. RMS Norm), and positional embedding technique (Absolute vs. ROPE vs. None).  These architectural variations are the basis for the hybrid models explored later in the paper, illustrating the focus on understanding the impact of architectural choices on in-context learning performance.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_12_2.jpg", "caption": "Figure 8: The hybrid architectures as modifications to Llama", "description": "This figure shows three variations of Llama architecture. The first one replaces the feed-forward block with a Mamba Mixer block. The second one removes ROPE embeddings and prepends a Mamba Mixer to serve as a positional embedder. The third one combines the changes from the previous two variations.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_13_1.jpg", "caption": "Figure 9: The hybrid architectures as modifications to GPT-2", "description": "This figure shows variations of GPT-2 architecture where different components are replaced or modified. The modifications include:\n(a) Replacing GELU MLP with SwiGLU\n(b) Removing absolute positional encodings and including rotary position embeddings in attention\n(c) Replacing Layer Norm with RMS Norm\n(d) Combining SwiGLU, removing absolute positional encodings, including rotary position embeddings in attention, and replacing Layer Norm with RMS Norm\n(e) Removing absolute positional encodings and including rotary position embeddings in attention and replacing Layer Norm with RMS Norm\n(f) Replacing GELU MLP with SwiGLU, removing absolute positional encodings, including rotary position embeddings in attention, and replacing Layer Norm with RMS Norm. Each variation is illustrated with a diagram showing its structure.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_13_2.jpg", "caption": "Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.", "description": "This figure presents a table and a diagram to illustrate the 12 different hybrid transformer models explored in the paper. The models are created by combining components from three different base architectures: GPT-2, Llama, and Mamba.  The table details the specific components (positional embeddings, feed-forward network, and normalization) used in each hybrid model. The diagram visually shows how these components are arranged in each model's architecture, highlighting the variations and modifications made to the base architectures. This visual representation helps to understand the design space explored for studying in-context learning (ICL).", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_14_1.jpg", "caption": "Figure 10: Linear Regression Runs with Residual Plots", "description": "This figure shows the results of linear regression experiments with various GPT-2/Llama and Llama/Mamba hybrid architectures. The top row displays the squared error for each architecture across different context lengths. The bottom row presents the residual squared error, calculated as the difference between the model's predictions and the least squares solution.  The plots provide a visual comparison of the performance of different architectures on this task.", "section": "B.1 Linear Regression"}, {"figure_path": "YZoGmJrOS9/figures/figures_15_1.jpg", "caption": "Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.", "description": "This figure provides a table and a diagram to illustrate the 12 hybrid models explored in the paper.  The table lists the different variations of architectural sub-blocks (positional embeddings, feed-forward network, and normalizations) used in each architecture. The diagram shows how these variations affect the overall architecture of the models, including the use of absolute positional embeddings, ROPE, GELU MLP, SwiGLU, Layer Norm, and RMS Norm.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_15_2.jpg", "caption": "Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.", "description": "This figure presents a table and a diagram summarizing the 12 different hybrid transformer models explored in the paper.  The table lists the variations made to three key architectural sub-blocks: positional embeddings, feed-forward networks, and normalizations. The diagram visually shows how these variations combine to form the overall architecture of each model.  The models are categorized into three main types (GPT-2, Llama, and Mamba), with hybrid models incorporating components from different types. The figure helps to understand the different architectural variations investigated in the study and how they relate to each other.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/figures/figures_16_1.jpg", "caption": "Figure 10: Linear Regression Runs with Residual Plots", "description": "This figure presents the results of linear regression experiments.  It includes four subplots: two showing the squared error for GPT-2 Llama hybrid runs and Llama Mamba hybrid runs, and two displaying residuals for those same sets of runs, calculated against least squares.  The x-axis represents context length, and the y-axis represents the squared error and residual squared error, respectively. The plots visually demonstrate the performance of different hybrid model architectures on linear regression tasks, allowing for a comparison of their accuracy and how well they approximate a least-squares model.", "section": "B.1 Linear Regression"}, {"figure_path": "YZoGmJrOS9/figures/figures_16_2.jpg", "caption": "Figure 14: Sparse Parity Runs with Residual Plots", "description": "This figure visualizes the performance of different model architectures on the Sparse Parity task.  Panel (a) shows the squared error for each model architecture across varying context lengths.  Panel (b) displays the residual squared error, calculated by subtracting the baseline model's squared error from each model's error.  This provides a clearer view of the relative performance of each model compared to a baseline. The models evaluated include GPT-2, Llama, Mamba, and various hybrid models combining components from these base architectures.", "section": "B.5 Sparse Parity"}, {"figure_path": "YZoGmJrOS9/figures/figures_16_3.jpg", "caption": "Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.", "description": "This figure provides a visual representation of the hybrid models explored in the paper. It includes a table summarizing the modifications made to the GPT-2, Llama, and Mamba architectures (positional embeddings, feed-forward network, and normalization), as well as a block diagram illustrating how these variations affect the overall architecture. The table lists the three base models (GPT-2, Llama, and Mamba) and 9 hybrid models, each formed by combining different components from the base models. The diagram shows the flow of information through the models, highlighting which components are present or absent in each model. This visualization helps to clarify the relationships between different models and the specific modifications used to create the hybrid models. The figure helps the reader understand the architecture space studied in the paper. ", "section": "3.2 Architectures"}]