{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper introduced GPT-2, a foundational model for the study of In-Context Learning (ICL), which is the central focus of the current paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper established the ability of large language models to perform few-shot learning, a key concept related to ICL, which is central to the current paper's investigation."}, {"fullname_first_author": "Shivam Garg", "paper_title": "What can transformers learn in-context? A case study of simple function classes", "publication_date": "2022-00-00", "reason": "This paper is a seminal work directly investigating the in-context learning capabilities of transformer models, providing a foundation and benchmark for the research done in the current paper."}, {"fullname_first_author": "Ivan Lee", "paper_title": "Is attention required for ICL? Exploring the relationship between model architecture and in-context learning ability", "publication_date": "2023-00-00", "reason": "This paper expands on prior work by investigating the impact of model architecture on ICL performance, making it highly relevant to the current study, which analyzes the same aspects."}, {"fullname_first_author": "Jongho Park", "paper_title": "Can Mamba learn how to learn? A comparative study on in-context learning tasks", "publication_date": "2024-00-00", "reason": "This paper investigates ICL performance across various models, including the Mamba model, which is a key component of the hybrid models examined in the current research."}]}