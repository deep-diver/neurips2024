[{"type": "text", "text": "Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In-Context Learning (ICL) is a phenomenon where task learning occurs through   \n2 a prompt sequence without the necessity of parameter updates. ICL in Multi  \n3 Headed Attention (MHA) with absolute positional embedding has been the focus   \n4 of more study than other sequence model varieties. We examine implications   \n5 of architectural differences between GPT-2 and LLaMa as well as Llama and   \n6 Mamba. We extend work done by Garg et al. (2022) and Park et al. (2024)   \n7 to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models \u2013 examining the   \n8 interplay between sequence transformation blocks and regressive performance   \n9 in-context. We note that certain architectural changes cause degraded training   \n10 efficiency/ICL accuracy by converging to suboptimal predictors or converging   \n11 slower. We also find certain hybrids showing optimistic performance improve  \n12 ments, informing potential future ICL-focused architecture modifications. Ad  \n13 ditionally, we propose the \"ICL regression score\", a scalar metric describing a   \n14 model\u2019s whole performance on a specific task. Compute limitations impose re  \n15 strictions on our architecture-space, training duration, number of training runs,   \n16 function class complexity, and benchmark complexity. To foster reproducible and   \n17 extensible research, we provide a typed, modular, and extensible Python package   \n18 on which we run all experiments. This code is available at https://github.   \n19 com/anonymousforneurips64/neurips2024-submission21757. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Popularized by Large Language Models such as GPT-2 [1] and GPT-3 [2], In-Context Learning (ICL)   \n22 is the ability for highly expressive generative sequence models to predict phenomena by processing   \n23 demonstrations without performing traditional gradient steps. Such phenomena vary from effective   \n24 control systems [3] to answering questions in natural language [4, 5]. A large body of recent work   \n25 has studied this phenomenon in transformer models [6, 7, 2, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,   \n26 19, 20, 21, 22, 23, 24, 25] , which derive in structure from Vaswani et al. [26].   \n27 Some recent examples of this research on ICL include Garg et al [6], which studies ICL by providing   \n28 a variety of function classes for models to learn, additionally benchmarking robustness by testing per  \n29 formance on out-of-distribution data. Guo et al[11] shows the validity of composing simple function   \n30 classes to produce complex ones, while Liu et al [20] produced a metric for model information recall.   \n31 These works give us a set of metrics with which we can use to compare model performance on ICL.   \n32 ICL was initially primarily studied in attention-based models but has recently been explored in   \n33 other sequence models, creating discussion on its differences across those models and why these   \n34 occur architecturally. In our paper, we study this by substituting key modern transformer (Llama)   \n35 components with Mamba blocks and GPT-2 components and richly benchmarking.   \n36 Since ICL for complete natural language understanding often requires training models with over a   \n37 billion parameters, the effects of architectural changes on fine-grained ICL abilities are often left   \n38 unexplored. As a consequence, although language models have progressed quickly and entertained   \n39 radically new architectures, there is limited extensible research that explores the effects of fine-grained   \n40 architecture choices on ICL ability [8, 14]. Garg et al. established using simple function classes to   \n41 evaluate ICL ability and examined solely GPT-2 as a sequence model. Lee et al. [8] expanded this   \n42 analysis on a slightly different set of function classes for a variety of base models. Park et al. [14]   \nevaluated ICL performance of 2 hybrid architectures between Mamba and GPT-2. Using unmodified   \n44 Llama/Mamba/GPT-2 as a control, we analyze GPT2-Llama and Llama-Mamba hybrid architectures   \n45 derived from replacing portions of GPT2 components with analogous Llama sections and LLama   \n46 with Mamba blocks, respectively, in 12 total architectures (3 unmodified $+\\,9$ hybrid).   \n47 We observe that the code written to analyze ICL with simple function classes \u2013 although almost   \n48 unanimously extensions of Garg et al.\u2019s \u2013 often requires substantial, structural changes to the parent   \n49 codebase1, greatly heightening the barrier to extending each project in turn. Inspired by Donoho\u2019s   \n50 ideal of Frictionless Reproducibility [27], we provide a set of simple abstractions and interfaces to   \n51 facilitate extensions and modifications to our code while promoting interoperability between forks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "table", "img_path": "YZoGmJrOS9/tmp/0e30c78a42765ce9a8a5ffe8b66ff0b16b72c287151ea30d77eefa43d0543570.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of tasks. Each regression target $f_{\\theta}(x_{i})$ is either parametrized by a randomly sampled $\\theta$ or directly computed/sampled as detailed above. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "52 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "53 There are many ways to capture qualitative aspects of ICL with quantitative measures. Weber et al.   \n54 [17] compare the agreement between generations of a language model under varying prompts of   \n55 equal meaning to test robustness to variations. Olsson et al. [22] compute a heuristic \"ICL score\" to   \n56 measure an accuracy increase in predictions of a model given more context. We adapt this metric to   \n57 fti our experimental setup more aptly, regularizing along both the number of in-context examples and   \n58 against a baseline predictor.   \n59 In general, evaluating ICL ability has been approached from two primary avenues: both when the   \n60 only solution at train time is to meta-learn an algorithm [6, 8, 28, 11, 19] and when optimal loss   \n61 at train time can also be satisfied by memorization or otherwise leveraging previously trained-on   \n62 data [10, 23]. In this work, we take the former approach through learning a regression algorithm to   \n63 randomized simple function classes [6, 11, 15].   \n64 Further still, non-transformer architectures are capable of ICL [8]. Lee et al. [8] observed ICL   \n65 in numerous sequence model architectures (e.g. RNNs, Mamba, S4, CNNs, GPT-2, and Llama)   \n66 and found qualitative differences in each architecture\u2019s performance. Chan et al. [25] found that   \n67 Transformers depend on \"burstiness\" and long-tail distributions of natural data to outperform RNNs   \n68 and LSTMs in ICL tasks. Park et al. [14] uses simple function classes similar to Garg et al. [6]   \n69 in evaluating the ICL ability of Mamba, S4, S4-Mamba, and GPT-2. They find an overlapping but   \n70 inequivalent set of function classes for which each model succeeds and construct a hybrid architecture   \n71 to achieve the union of these abilities. We further this work by closely examining the contributions of   \n72 individual architectural changes for GPT-2 and Llama-style transformers towards ICL ability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "73 3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "74 As established by Garg et al. and extended by recent work, our ICL tasks take the following form   \n75 [6, 8, 14]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{x_{0},f_{\\theta}(x_{0}),x_{1},f_{\\theta}(x_{1}),...,\\widehat{x_{N}}}_{\\mathrm{prompt}\\,P},\\underbrace{f_{\\theta}(x_{N})}_{\\mathrm{completion}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "76 where $P$ is a series of input-output pairs followed by a lone query. The model predicts a completion   \n77 based on the prompt it received. The function parameters $\\theta$ and the inputs $x_{i}$ are randomly sampled   \n78 from a function class domain and an input domain, respectively. The tasks we regress to are   \n79 summarized in Table 1 and detailed in Section 3.1   \n80 We train models for ICL by minimizing the expected loss over a distribution of prompts and cor  \n81 responding function outputs. This approach allows us to observe qualitative differences in model   \n82 architectures by their ability to behave similarly to optimal or baseline estimators. To further simplify   \n83 ICL aptitude evaluation, we introduce a proxy value summarizing a given model\u2019s ICL ability for   \n84 a specific task. This metric averages the error of a model normalized by the baseline error at each   \n85 context length. We detail this further in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "86 3.1 Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "87 To determine task-specific ICL ability, our sequence models regress onto the functions shown   \n88 above [14]. We replicate the function classes Linear Regression, Sparse Linear Regression,   \n89 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they   \n90 present a wide range of \"difficulty\" for sequence models. In addition, to capture the existence   \n91 of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity   \n92 function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR).   \n93 Unless otherwise specified, we train all models with 12 layers, 8 attention heads, an expansion factor   \n94 of 4 (in the case of models with Mamba Mixer layers), and linear layers to transform the input   \n95 sequences into and from the embedding dimension of 256. We use the ADAM optimizer with a   \n96 learning rate of 0.0001 for $500\\mathrm{k}$ steps. Our expansion factor was selected to ensure similar parameter   \n97 counts across baselines and all other hyperparameters were chosen for consistency with Garg et al.   \n98 [6]. Note for the four function classes from Garg et al., the same curriculum was used during training.   \n99 No curriculum is used for the two new function classes from Park et al. [14]. For our compute2, we   \n100 utilized 898.90 hours on an A10, 55.74 hours on an RTX 3090, 151.90 hours on an RTX 4090, 75.48   \n101 hours on an RTX 4070 Ti, and 9.83 hours on an RTX 6000.   \n102 Linear Regression and Sparse Linear Regression Each function in these tasks is parametrized as a   \n103 single weight vector $(w)$ of dimension equal to that of the $x$ -values (i.e. 20) so that $y=w^{T}x$ . We   \n104 sample the coordinate values from a normal distribution and (in the Sparse Linear case) zero out all   \n105 values except a uniformly at random selected $k$ coordinates. In essence, one can consider Linear   \n106 Regression to be the degenerate case where the $k=20$ . We preserve these tasks from Garg et al. [6]   \n107 to verify that none of our hybrid modifications lose the near-optimal performance that was already   \n108 found with GPT-2.   \n109 2-Layer MLP Regression We fill two weight matrices $W^{(1)}\\in\\mathrm{R}^{100\\times20}$ and $W^{(2)}\\in\\mathrm{R}^{1\\times100}$ with   \n110 scalar samples from a normal distribution. $y$ values are computed as the result of a forward pass   \n111 through a 2-layer multi layer perceptron with a ReLU activation. That is: $\\boldsymbol{y}=\\boldsymbol{W}^{(2)}\\mathrm{ReLU}(\\boldsymbol{W}^{(\\bar{1})}\\boldsymbol{x})$ .   \n112 This is a more complex function class that Garg et al. [6] found that GPT-2 can perform very well at,   \n113 suggesting that this task can capture some ICL ability of an architecture.   \n114 Decision Tree Regression We construct full decision trees of depth 4 with leaf values sampled from a   \n115 normal distribution and branching conditions to be selected uniformly at random over the coordinates   \n116 of the input dimension. The left branch is taken if the selected input coordinate is less than 0 and the   \n117 right branch is taken otherwise. Garg et al. [6] found that GPT-2 was able to achieve much lower   \n118 error for lower context lengths than XGBoost or Greedy Tree Learning, suggesting that this task can   \n119 capture some ICL ability of an architecture.   \n120 Sparse Parity We select $k=2$ values to consider and compute their parity, expressed as either $-1$ or   \n121 1. That is, we uniformly sample without replacement $\\theta\\sim\\{1,...,10\\}^{k}$ and compute $\\textstyle y=\\prod_{i\\in\\theta}x[i]$ .   \n122 Along with a higher learning rate of 0.0004, this is identical to the scheme implemented in  Park et al.   \n123 [14]. They [14] found that GPT-2 style transformers do not perform well on this task, suggesting that   \n124 this is a discerning proxy for measuring ICL ability. Finally, as convergence was quick for this task,   \n125 we only trained models up to $200\\mathbf{k}$ steps.   \n126 Vector MQAR We sample $2N$ points from the $d_{\\cdot}$ -sphere of radius $\\sqrt{d}$ and group them randomly into   \n127 pairs to forming $N$ key-value pairs. For consistency with the experiments of Park et al. [14] and to   \n128 reliably allow for the formation of transformer circuits highly relevant to this task [22, 14], we reduce   \n129 model complexity by using an embedding dimension of 128, 2 layers, and a higher learning rate of   \n130 0.0002. Park et al. [14] found that Mamba, our representative of SSM-type models, performed poorly,   \n131 suggesting that this task can serve to ensure we don\u2019t lose capabilities provided by transformers. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/5dc86d5150c6681a11e2c1b142d9f1e77290fb19b543e6f483453912e1013bda.jpg", "img_caption": ["Figure 1: Visual aid for our explored hybrid models in tabular and graphical format. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "132 3.2 Architectures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 As detailed by Radford et al. [1], GPT-2 is almost identical to the original decoder-only transformer,   \n134 with absolute positional embedding, pre-norm layer normalization, and a GELU activation function   \n135 in the feed-forward network (FFN) (which is otherwise a multi-layer perceptron). In contrast, Llama   \n136 [29, 30] combines a number of modern transformer modifications, including swapping layer norm   \n137 with RMS norm [31], changing the architecture and activation function of the FFN, and using rotary   \n138 positional embeddings instead of absolute positional embeddings [32]. We acknowledge that the   \n139 larger variations of Llama2 [30] and both variations of Llama3 [33] used Grouped-Query Attention   \n140 (GQA), however we surmise that at our model scales of ${\\sim}10$ million parameters, GQA will not   \n141 significantly affect the performance of our models. From an entirely different method of sequence   \n142 modeling, Mamba forgoes positional embedding entirely, combining features of the Gated Linear   \n143 Unit and state space expansion to remove the need for distinct attention and feed-forward blocks.   \n144 We summarize these architectural differences in Table 2. We examine all combinations of these   \n145 different components, training 12 total architectures (listed in Figure 1a) on our 6 tasks for a total of   \n146 72 model-task pairs. Figure 1b illustrates how each of these variations compose into a model. We   \n147 provide individual diagrams of each architecture in Appendix A. ", "page_idx": 3}, {"type": "table", "img_path": "YZoGmJrOS9/tmp/aa75ba34f5ad945542321bc4c3cf7e4264da8db842409bf043340e8d49e0c787.jpg", "table_caption": ["Table 2: A summary of the primary architectural differences between GPT-2, Llama, and Mamba. We examine all variations between GPT-2 and Llama and all variations between Llama and Mamba. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "148 3.3 Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "149 In addition to the baseline metric (squared error as a function of context length) from Garg et. al.   \n150 [6], we\u2019ve established another metric: ICL regression score. This is a scalar expressing overall   \n151 performance of a model on a task. Abstractly, the metric aims to capture the proportion of the baseline   \n152 error saved by a model. The regression score is calculated by (1) computing the difference in error   \n153 achieved by the model and the zero estimator at each context length, (2) computing the average of   \n154 this value over the length of the sequence, (3) computing the same value for the baseline estimator,   \n155 and (4) taking the ratio of these. ", "page_idx": 4}, {"type": "text", "text": "156 In summary, ICL regression score can be calculated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{\\mathrm{model}}=\\frac{\\sum_{i}\\left(\\xi_{\\mathrm{model}}^{(i)}-\\xi_{0}^{(i)}\\right)}{\\sum_{i}\\left(\\xi_{\\mathrm{base}}^{(i)}-\\xi_{0}^{(i)}\\right)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "115578 awnhde re \u03be(mio)d ihse  tzheer os qeustairemda teorrror of the model of interest at context length $i$ . Sim. $\\xi_{\\mathrm{base}}^{(i)}$ s)e for baseline $\\xi_{0}^{(i)}$ ", "page_idx": 4}, {"type": "text", "text": "159 Summation over context length allows our ICL regression score to be used for the comparison of   \n160 tasks with significantly differing context lengths. An interpretation for each of different possible   \n161 values of our ICL regression score is given in 2a. This approach builds off of Olsson et al.\u2019s \"ICL   \n162 Score\" [22] by generalizing their selection of 500 and 50 in-context examples and reducing along the   \n163 context length, allowing for tasks with widely different context lengths to be directly compared. We   \n164 list our baselines in Table 2b.   \n165 We replicate the baseline predictors for linear regression, sparse linear regression, and MLP regression   \n166 from Garg et al. [6] due to the lack of a higher-performing baseline. However, we opted to use   \n167 a pretrained GPT-2 model with identical structure to that used in Garg et al. to serve as a more   \n168 calibrated baseline than Greedy Tree Learning or XGBoost. They showed superior decision tree ICL   \n169 performance for a trained GPT-2 transformer compared to Greedy Tree Learning or XGBoost. For   \n170 consistency with Park et al. [14] and due to the algorithmic hardness of Sparse Parity, we used   \n171 our Mamba model trained on this task. Park et al. showed that Mamba can effectively learn this task,   \n172 so we repeat our strategy as in Decision Tree Regression with our Mamba model (instead of   \n173 GPT-2) as a baseline. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "174 3.4 Reproducibility Statement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "175 For ease of experimentation and reproducibility, we have built a typed, extensible, and modular   \n176 Python codebase. We achieved this by identifying isolated processes in the training regime and ", "page_idx": 4}, {"type": "table", "img_path": "YZoGmJrOS9/tmp/7dc03844bfb599295728fb5f98a013b3a6653ee1d33f7445273348d5583d31ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "(a) Interpretation of possible $S_{\\mathrm{model}}$ values computed over context length. ", "page_idx": 5}, {"type": "table", "img_path": "YZoGmJrOS9/tmp/6400135a5c5f00a48eddf008e315aa2baa65e30e7ae4685ca911baa028ce5e6b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "(b) The baselines for each task. The 2-layer NN is trained for 1000 gradient steps, with a batch consisting of a randomly selected point in the context. GPT-2 and Mamba are trained for 500k steps on the specified task in the same format as all other models. ", "page_idx": 5}, {"type": "text", "text": "Figure 2: Predictors and conditions for computation and interpretation of ICL regression score. ", "page_idx": 5}, {"type": "text", "text": "177 structuring our code to reflect them. In particular, the specification of (1) a function class, (2) a   \n178 model type, (3) an evaluation scheme, and (4) a stage of training under a curriculum are all inherent   \n179 to the experiment archetype as proposed by Garg et al. [6] and repeated by others [8, 15, 14]. We   \n180 integrate standard reporting software Weights and Biases [34] and leverage fast implementations   \n181 of attention [35] and 1-D convolutions [36]. We also implement a configuration-based system for   \n182 training, loading, and evaluating models to facilitate frictionless repeatability of all experiments. ", "page_idx": 5}, {"type": "text", "text": "183 4 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "184 We confirm the results from Garg et al. [6] and Park et al. [14] that GPT-2 and Mamba can   \n185 learn our first four regression tasks in context. Park et al. [14] that Mamba struggles to perform   \n186 Vector MQAR while transformers and hybrid architectures excel. We note that Llama and GPT-2   \n187 have very comparable performance in Sparse Parity and Vector MQAR. We plot all qualitatively   \n188 non-optimal squared error profiles in Figure 3 and all squared error profiles in Appendix B. ", "page_idx": 5}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/3375bb970861fc0efd0d330950ac48e9b2f0af510c106aff2b6e5994c8862879.jpg", "img_caption": ["(a) Notable phenomena for Sparse Linear. We observe that while GPT-2 (orange) performs very similarly to our baseline, adding RMS norm without RoPE (red and green) leads to models performing notably worse than optimal. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/ca8b74ded1da3edc24b07c78193e7f92e201ee3fce016e1db4d92f4775e22fec.jpg", "img_caption": ["(b) Notable phenomena for Decision Tree. We note that Mamba (green) performs somewhat suboptimally while GPT-2 RMS (orange) fails to learn the task entirely. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are $99\\%$ confidence intervals. ", "page_idx": 5}, {"type": "text", "text": "189 Models can converge to suboptimal regression schemes. We find that some model-task pairs   \n190 produce suboptimal predictions, not as a result of insufficient training. A clear example is GPT-2   \n191 RMS SwiGLU (model 1.4) on Sparse Linear. This model appears to not achieve optimal error   \n192 \u2013 achieving an ICL Regression Score of only 0.754, opposed to $\\sim0.93$ by other models \u2013 and yet   \n193 its performance does not significantly improve with more gradient steps. We plot the squared error   \n194 achieved by various checkpoints for model 1.4 in Figure 4a. We observe that this error proflie appears   \n195 similar to that of models trained on the Linear task and so also examine the prediction quality of the ", "page_idx": 5}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/267d6eabfdf86530f03755c39e5c66a86061514f74476185e7f3646794835e4b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["(a) GPT-2 RMS SwiGLU Checkpoints on Sparse Linear. We see that GPT-2 RMS SwiGLU converges to the least squares solution, despite Lasso being the optimal solution. This suggests that GPT-2 RMS SwiGLU fails to learn to utilize its context to its fullest extent. ", "Sparse Linear and evaluated on Linear. When evaluated on a similar task to which it was trained on, GPT-2 RMS SwiGLU appears to perform better than its siblings, despite the fact that it performed worse than its siblings on its original task! This suggests that it learned a different regression scheme than GPT-2 on the same training data. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Detailing plots to showcase GPT-2 RMS SwiGLU (model 1.4) learning a more general but sub-optimal regression scheme when trained on Sparse Linear. Shaded regions are $99\\%$ confidence intervals. ", "page_idx": 6}, {"type": "text", "text": "196 same model (GPT-2 RMS SwiGLU trained on Sparse Linear) on Linear in Figure 4b. We find   \n197 that it indeed mimics the error proflie of least squares. This result builds on Aky\u00fcrek et al.\u2019s findings   \n198 [19] in what functions transformer models develop representations of. Aky\u00fcrek et al. analyzed   \n199 algorithms representable by GPT-2 like architectures. We note that they did not examine other layer   \n200 types such as Mamba Mixer or SwiGLU.   \n201 Models can escape suboptimal regression schemes. We see that GPT-2 SwiGLU (model 1.3)   \n202 Sparse Linear on adopts a suboptimal regression scheme (least squares) partway in training,   \n203 eventually unlearning its scheme in favor of the optimal regression scheme (lasso). We plot the   \n204 squared error on Sparse Linear achieved by various checkpoints for Model 1.3 in Figure 5a, noting   \n205 that the error of the checkpoint at $100\\mathrm{k}$ steps closely matches the error of least squares. Further, we   \n206 examine the squared errors on Linear Regression for the various checkpoints for Model 1.3 in 5b and   \n207 see that the checkpoint at 100k most closely matches least squares. This suggests that model 1.3   \n208 learned the linear regression scheme in the beginning of training, but was eventually able to learn to   \n209 utilize the sparse nature of its training data.   \n210 Models can fail to converge within our training horizon. We find that a number of models   \n211 performed strikingly poorly in their trained task. In particular, GPT-2 with Layer norm replaced by   \n212 RMS norm (model 1.1) performed very poorly on Sparse Linear Regression and Decision   \n213 Tree, as indicated by the lowest ICL Regression Score achieved in those tasks (0.535 and 0.114,   \n214 respectively) and in Figures 3a and 3b. We also observe that GPT-2 with RMS and SwiGLU (model   \n215 1.4) also did not converge to a regression scheme, despite apparently modelling a different regression   \n216 scheme entirely. Similarly, Mamba (model 3) did not converge to a training scheme on Decision   \n217 Tree as illustrated in Figure 6a. We believe this suggests a lower training efficiency for certain   \n218 architectures on these tasks.   \n219 Models can fail to learn the task entirely. In the case of Decision Tree, GPT-2 with RMS (model   \n220 1.1) failed to learn the task entirely as not only indicated by its final ICL Regression Score but also   \n221 its consistency in achieving very high error throughout training. We plot squared error for various   \n222 checkpoints in Figure 6b.   \n223 ICL Regression Scores reflect qualitative information contained in squared-error plots. Com  \n224 puted ICL Regression Scores are summarized in Table 3. Overall, most models are able to perform   \n225 comparably to our baseline estimators, with nearly all examined models achieving a regression score   \n226 of approximately 1 on all four function classes from Garg et al. (Linear Regression, Sparse   \n227 Linear Regression, 2-Layer MLP, Decision Tree). The ICL Regression Scores for Linear ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/d0bbe1813cb68d72232cdf36dc874c9e8d92ce7bc9c42de105d0d69abfd64143.jpg", "img_caption": ["(a) GPT-2 SwiGLU Checkpoints on Sparse Linear. In the beginning of training, GPT-2 SwiGLU quickly converges to least squares, but it is able to escape this regression scheme and eventually has its error profile approach that of Lasso. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/55cd690fa41b1ac7d294e5c43b98dc2e069be302b434b75e9ca99de0c6c2f9cf.jpg", "img_caption": ["(b) GPT-2 SwiGLU Checkpoints trained on Sparse Parity and evaluated on Linear Regression. We see that an earlier checkpoint (100k) of GPT-2 SwiGLU outperforms later checkpoints on a similar task different from the task it was trained on. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Detailing plots to showcase GPT-2 SwiGLU (model 1.3) starting by learning a more general but sub-optimal regression scheme but eventually converging to the optimal regression scheme when trained on Sparse Linear. Shaded regions are $99\\%$ confidence intervals. ", "page_idx": 7}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/4666e4afdbd9c18bb85bbfbbc8c9ffc2d6730c9b4bf915e52b6abe852dac5244.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/736b5966a3b82f5e40a2a092d2d93b517c18b766662ee6d726dde9d8d8b808a8.jpg", "img_caption": ["(a) Mamba Checkpoints on Decision Tree. (b) GPT-2 RMS Checkpoints on Decision Tree. We see that all checkpoints of GPT-2 perror profile throughout training. This suggests that form very similarly, with little to no change in error Mamba did not reach convergence, and thus has profile throughout training. lower training efficiency on this task. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: Squared error as a function of context length computed for various checkpoints for both Mamba (model 3) and GPT-2 RMS (model 1.1) on Decision Tree. Shaded regions are $99\\%$ confidence intervals. ", "page_idx": 7}, {"type": "text", "text": "228 Regression and 2-Layer MLP, along with their corresponding graphs of squared error as a function   \n229 of context length, corroborate the claims from Garg et al. [6] that transformers can \"learn\" these tasks.   \n230 Further, the ICL Regression Scores for Sparse Parity are consistent with Park et al. [14], with all   \n231 hybrids between GPT-2, and Llama failing to \"learn\" the task and all hybrids between Llama and   \n232 Mamba succeeding in \"learning\" the task. Indeed, the ICL Regression Score achieved by Mamba   \n233 captures the qualitatively sub-optimal performance detailed above on Decision Tree. ", "page_idx": 7}, {"type": "text", "text": "234 5 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "235 Even simple function classes leave room for local minima. We find that despite distilling down the   \n236 phenomenon of In Context Learning to regression against simple function classes, there still exists   \n237 room for models to adopt various regression schemes. This is supported by the apparent convergence   \n238 of the error profiles of GPT-2 RMS (model 1.1) and GPT-2 RMS SwiGLU (model 1.4) to least   \n239 squares regression for shorter context lengths.   \n240 Hybrid architectures and function classes have varying levels of compatibility. Specific hybrid   \n241 architectures can hesitate to learn/converge for certain function classes. This behavior is especially   \n242 apparent in GPT-2 RMS\u2019s (model 1.1) Decision Tree error graph and GPT-2 RMS SwiGLU\u2019s (model   \n243 1.4) Sparse Linear performance. It seems that GPT-2 RMS SwiGLU shows greater affinity towards   \n244 learning least squares instead of LASSO. Certain hybrid architecture variations may place inductive   \n245 biases on certain solution forms, resulting in extreme convergence times when these solution forms   \n246 greatly vary from the optimal predictor\u2019s form.   \n247 Extensible Research as Reproducible Research. In the development of this work, continuously   \n248 iterating to minimize the friction of reproduction has enabled rapid extension of our Python artifacts   \n249 to support even abstractly defined hybrid architectures, which are often considered inextricable from   \n250 highly bespoke code or dedicated packages such as xFormers [37]. We implore the reader to seriously   \n251 consider the value of making their research extensible with a minimum of friction. We hope that our   \n252 attempts to maximize extensibility and reproducibility contribute to the longevity of this work as a   \n253 reliable, tested, and simple framework to use for studying simple function classes in context. ", "page_idx": 7}, {"type": "table", "img_path": "YZoGmJrOS9/tmp/c73c8782ec0bd7b38d4a4433d4d326b4899e2b132e2a4090af1cc221eb3bceb8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "254 5.1 Limitations and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "255 We have only one training run performed on each model-task pair. As a result, we have no   \n256 estimation for how consistently observed phenomena appear with the given architectures. We only   \n257 train each model for a maximum of 500K steps. Thus, when a model fails to converge within this   \n258 window, we lose information on insightful trends that could possibly occur with further training.   \n259 We do not empirically evaluate the effectiveness of ICL Regression Score or the usability of our   \n260 provided code platform. We compute no verifying metrics to establish how well ICL Regression   \n261 Score generalizes or is robust to qualitatively distinct ICL regression tasks. Similarly, we perform no   \n262 user study on the effectiveness of our code platform, presenting only our own experience.   \n263 Future Work In this paper we analyze ICL performance for GPT-2-Llama and Llama-Mamba   \n264 hybrid architectures (9 total) on 6 tasks. Future relevant research could entail 1) expanding our   \n265 architecture-space and streamlining our training-to-evaluation pipeline by creating an architecture   \n266 search mechanism, 2) assessing our models on other sets of tasks, such as ones relating to lan  \n267 guage modeling or image classification, 3) verifying our results with additional training runs, 4)   \n268 benchmarking model performance along hardware-related metrics. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "269 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "270 [1] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language   \n271 models are unsupervised multitask learners. 2019.   \n272 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n273 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n274 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n275 [3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter   \n276 Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning   \n277 via sequence modeling. CoRR, abs/2106.01345, 2021.   \n278 [4] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan   \n279 Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.   \n280 [5] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,   \n281 Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,   \n282 Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,   \n283 Jan Leike, and Ryan Lowe. Training language models to follow instructions with human   \n284 feedback, 2022.   \n285 [6] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers   \n286 learn in-context? a case study of simple function classes. Advances in Neural Information   \n287 Processing Systems, 35:30583\u201330598, 2022.   \n288 [7] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and   \n289 Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning   \n290 work? arXiv preprint arXiv:2202.12837, 2022.   \n291 [8] Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Is attention required for icl? exploring the   \n292 relationship between model architecture and in-context learning ability, 2023.   \n293 [9] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina   \n294 Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking.   \n295 [10] Aaditya Singh, Stephanie Chan, Ted Moskovitz, Erin Grant, Andrew Saxe, and Felix Hill.   \n296 The transient nature of emergent in-context learning in transformers. Advances in Neural   \n297 Information Processing Systems, 36, 2024.   \n298 [11] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.   \n299 How do transformers learn in-context beyond simple functions? a case study on learning with   \n300 representations. arXiv preprint arXiv:2310.10616, 2023.   \n301 [12] Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau.   \n302 Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.   \n303 [13] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:   \n304 Provable in-context learning with in-context algorithm selection. Advances in neural information   \n305 processing systems, 36, 2024.   \n306 [14] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,   \n307 Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative   \n308 study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.   \n309 [15] Kartik Ahuja and David Lopez-Paz. A closer look at in-context learning under distribution   \n310 shifts. arXiv preprint arXiv:2305.16704, 2023.   \n311 [16] Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning:   \n312 Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.   \n313 [17] Lucas Weber, Elia Bruni, and Dieuwke Hupkes. The icl consistency test. arXiv preprint   \n314 arXiv:2312.04945, 2023.   \n315 [18] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning.   \n316 Advances in Neural Information Processing Systems, 36, 2024.   \n317 [19] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning   \n318 algorithm is in-context learning? investigations with linear models, 2023.   \n319 [20] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.   \n320 What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.   \n321 [21] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao   \n322 Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently.   \n323 arXiv preprint arXiv:2303.03846, 2023.   \n324 [22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom   \n325 Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning   \n326 and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n327 [23] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of   \n328 in-context learning as implicit bayesian inference, 2022.   \n329 [24] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mord  \n330 vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient   \n331 descent, 2023.   \n332 [25] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,   \n333 Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive   \n334 emergent in-context learning in transformers, 2022.   \n335 [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n336 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n337 processing systems, 30, 2017.   \n338 [27] David Donoho. Data science at the singularity. arXiv preprint arXiv:2310.00865, 2023.   \n339 [28] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing   \n340 attention glitches with flip-flop language modeling, 2023.   \n341 [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n342 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,   \n343 Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation   \n344 language models, 2023.   \n345 [30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n346 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas   \n347 Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,   \n348 Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony   \n349 Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian   \n350 Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut   \n351 Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,   \n352 Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,   \n353 Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao  \n354 qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng   \n355 Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien   \n356 Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation   \n357 and fine-tuned chat models, 2023.   \n358 [31] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019.   \n359 [32] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer:   \n360 Enhanced transformer with rotary position embedding, 2021.   \nd. 2024   \n362 [34] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from   \n363 wandb.com.   \n364 [35] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.   \n365 [36] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces,   \n366 2023.   \n367 [37] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,   \n368 Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza,   \n369 Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack  \n370 able transformer modelling library. https://github.com/facebookresearch/xformers,   \n371 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/719cace9868c4478a0b73666ff8bbdbf3881934eb819a50d07c50f886715b411.jpg", "img_caption": ["Figure 7: The GPT-2, Llama, and Mamba architectures used in our regression tasks "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/4ab51a427fe03920c8ae20916ff846ecabe4c529f1c3c076367bf680da749888.jpg", "img_caption": ["(a) Llama with the feed-forward (b) Llama with rope embeddings (c) Llama with the feed-forward block replaced by a Mamba Mixer removed and a Mamba Mixer block replaced by a Mamba Mixer block prepended to serve as a \"posi- block, rope embeddings removed, tional embedder\" and a Mamba Mixer prepended to serve as a \"positional embedder\" ", "Figure 8: The hybrid architectures as modifications to Llama "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/8c515596ab3506248b081e7576799aa929de62e5efb9c071b7e0d5bcd7d9c1c5.jpg", "img_caption": ["(a) GPT-2 with the GELU MLProtary position embeddings in-(c) GPT-2 with the Layer Norm replaced by a SwiGLU cluded in attention replaced by an RMS Norm "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/c04b906a4b082e23ad6066b8a72a7c7507b76b3d51eff4219b7eff2e5603d1b6.jpg", "img_caption": ["(e) GPT-2 with absolute posi-(f) GPT-2 with the GELU MLP re(d) GPT-2 with the GELU MLPtional encodings removed, rotaryplaced by a SwiGLU, absolute poreplaced by a SwiGLU and theposition embeddings included insitional encodings removed, and Layer Norm replaced by an RMSattention, and the Layer Norm re-rotary position embeddings inNorm placed by an RMS Norm cluded in attention ", "Figure 9: The hybrid architectures as modifications to GPT-2 "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "373 B Complete Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "374 B.1 Linear Regression ", "page_idx": 14}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/e5895c450543f821708dbc6e11b1e539a931fb5c1410d5e514239c1f80bffca3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "(c) Residuals for GPT-2-Llama Hybrid Runs, taken(d) Residuals for Llama-Mamba Hybrid Runs, against Least Squares taken against Least Squares ", "page_idx": 14}, {"type": "text", "text": "Figure 10: Linear Regression Runs with Residual Plots ", "page_idx": 14}, {"type": "text", "text": "375 B.2 Sparse Linear Regression ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/0e7101d4e8b0736b0bdbcb25469c0b2494bf86aa2264ad9e06c46b6b4ba1295d.jpg", "img_caption": ["Figure 11: Sparse Linear Regression Runs "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "against Lasso with with ", "page_idx": 15}, {"type": "text", "text": "376 B.3 Decision Trees ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/df733f2268ee90ca55aca7d870472bdbf82180f42389794e74b6300824599af9.jpg", "img_caption": ["Figure 12: Decision Tree Runs "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/1dd1d101763de0272737c18eff28fba059c8dd22b5e4f4473a4dfe96d27a8f4d.jpg", "img_caption": ["(a) GPT-2-Llama Hybrid Runs ", "Figure 13: 2-Layer NN Regression Runs ", "(b) Llama-Mamba Hybrid Runs "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "378 B.5 Sparse Parity ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/1510ebd4e3336fcfbcc516b4eab931720360bc0745f89074f399b1f7c38ac2a3.jpg", "img_caption": ["Figure 14: Sparse Parity Runs with Residual Plots "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "379 B.6 Vector MQAR ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "YZoGmJrOS9/tmp/104d655614c300faaf14069b4da966fad20abaf1b2cad5714ca9b46bc594107b.jpg", "img_caption": ["Figure 15: Vector MQAR Training Runs "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "380 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract/introduction briefly covers the limitations of the paper while introducing the main claims/findings/contributions. We reference relevant work we are building off of. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We briefly mention some limitations in our analysis and experiments in Section 5.1. We acknowledge our limited training runs, inexhaustive training horizon, incomplete evaluation of ICL Regression Score, and no metrics on the usability of our codebase. Similarly here we acknowledge that this list of limitations is by no means exhaustive. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "432 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "35 Answer: [NA]   \n36 Justification: There are no theoretical results or claims in this paper, and thus no assumptions   \n37 and proofs are required.   \n38 Guidelines:   \n39 \u2022 The answer NA means that the paper does not include theoretical results.   \n40 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n41 referenced.   \n42 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n43 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n44 they appear in the supplemental material, the authors are encouraged to provide a short   \n45 proof sketch to provide intuition.   \n46 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n47 by formal proofs provided in appendix or supplemental material.   \n48 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "449 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss training and evaluation in our paper while making our codebase accessible. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "8 5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "89 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n90 tions to faithfully reproduce the main experimental results, as described in supplemental   \n91 material?   \n92 Answer: [Yes]   \n93 Justification: This paper contributes its codebase, which contains sufficient information in   \n94 the ReadME for reproducibility. This paper\u2019s \"Methods\" section discusses data generation   \n95 for the simple function classes.   \n96 Guidelines:   \n97 \u2022 The answer NA means that paper does not include experiments requiring code.   \n98 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n99 public/guides/CodeSubmissionPolicy) for more details.   \n00 \u2022 While we encourage the release of code and data, we understand that this might not be   \n01 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n02 including code, unless this is central to the contribution (e.g., for a new open-source   \n03 benchmark).   \n04 \u2022 The instructions should contain the exact command and environment needed to run to   \n05 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n06 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n07 \u2022 The authors should provide instructions on data access and preparation, including how   \n08 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n09 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n10 proposed method and baselines. If only a subset of experiments are reproducible, they   \n11 should state which ones are omitted from the script and why.   \n12 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n13 versions (if applicable).   \n14 \u2022 Providing as much information as possible in supplemental material (appended to the   \n15 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: These details were explained under the \"Methods\" section are sufficient to understand our results. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "529 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Justification: Every figure and number presented in this paper has confidence of intervals of ", "page_idx": 19}, {"type": "text", "text": "$95\\%$ or $99\\%$ (specified in each case).   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "556 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "57 Question: For each experiment, does the paper provide sufficient information on the com  \n58 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n59 the experiments? ", "page_idx": 20}, {"type": "text", "text": "Justification: The list of all GPU types utilized for the experiments were included, along with the time spent for compute on each of them. Furthermore, we provide a breakdown of the time spent for each experiment type on the GPU that we utilized the most (an A10). ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "573 9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "74 Question: Does the research conducted in the paper conform, in every respect, with the   \n75 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "576 Answer: [Yes]   \n577 Justification: We study ICL using simple function classes and do not use real world data.   \n78 No human subjects are involved and there are no direct paths for negative societal impact.   \n79 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "585 10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "591 Guidelines:   \n592 \u2022 The answer NA means that there is no societal impact of the work performed.   \n593 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n594 impact or why the paper does not address societal impact.   \n595 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n596 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n597 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n598 groups), privacy considerations, and security considerations.   \n599 \u2022 The conference expects that many papers will be foundational research and not tied   \n600 to particular applications, let alone deployments. However, if there is a direct path to   \n601 any negative applications, the authors should point it out. For example, it is legitimate   \n602 to point out that an improvement in the quality of generative models could be used to   \n603 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n604 that a generic algorithm for optimizing neural networks could enable people to train   \n605 models that generate Deepfakes faster.   \n606 \u2022 The authors should consider possible harms that could arise when the technology is   \n607 being used as intended and functioning correctly, harms that could arise when the   \n608 technology is being used as intended but gives incorrect results, and harms following   \n609 from (intentional or unintentional) misuse of the technology.   \n610 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n611 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n612 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n613 feedback over time, improving the efficiency and accessibility of ML).   \n614 11. Safeguards   \n615 Question: Does the paper describe safeguards that have been put in place for responsible   \n616 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n617 image generators, or scraped datasets)?   \n618 Answer: [NA]   \n619 Justification: As this paper discusses ICL on simple function classes, it does not utilize   \n620 real-world data or models real-world capabilities. Thus, there is no risk for misuse.   \n621 Guidelines:   \n622 \u2022 The answer NA means that the paper poses no such risks.   \n623 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n624 necessary safeguards to allow for controlled use of the model, for example by requiring   \n625 that users adhere to usage guidelines or restrictions to access the model or implementing   \n626 safety filters.   \n627 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n628 should describe how they avoided releasing unsafe images.   \n629 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n630 not require this, but we encourage authors to take this into account and make a best   \n631 faith effort.   \n632 12. Licenses for existing assets   \n633 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n634 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n635 properly respected?   \n636 Answer: [Yes]   \n637 Justification: We provide credit to the authors of the three codebases that inspired some of   \n638 the features in our own and cite that their codebases fall under the MIT License for the first   \n639 two, and could not be found for the third. URLs are provided for each codebase as well.   \n640 Guidelines:   \n641 \u2022 The answer NA means that the paper does not use existing assets.   \n642 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n43 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n44 URL.   \n645 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n646 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n647 service of that source should be provided.   \n648 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n649 package should be provided. For popular datasets, paperswithcode.com/datasets   \n650 has curated licenses for some datasets. Their licensing guide can help determine the   \n651 license of a dataset.   \n652 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n653 the derived asset (if it has changed) should be provided.   \n654 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n655 the asset\u2019s creators.   \n656 13. New Assets   \n657 Question: Are new assets introduced in the paper well documented and is the documentation   \n658 provided alongside the assets?   \n659 Answer: [Yes]   \n660 Justification: Our codebase contains a ReadME providing thorough documentation. Our   \n661 paper also explains high-level functionality of our codebase.   \n662 Guidelines:   \n663 \u2022 The answer NA means that the paper does not release new assets.   \n664 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n665 submissions via structured templates. This includes details about training, license,   \n666 limitations, etc.   \n667 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n668 asset is used.   \n669 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n670 create an anonymized URL or include an anonymized zip file.   \n671 14. Crowdsourcing and Research with Human Subjects   \n672 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n673 include the full text of instructions given to participants and screenshots, if applicable, as   \n674 well as details about compensation (if any)?   \n675 Answer: [NA]   \n676 Justification: There are no crowdsourcing experiments or research with human subjects, and   \n677 thus no text of instructions or compensation information is included.   \n678 Guidelines:   \n679 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n680 human subjects.   \n681 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n682 tion of the paper involves human subjects, then as much detail as possible should be   \n683 included in the main paper.   \n684 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n685 or other labor should be paid at least the minimum wage in the country of the data   \n686 collector.   \n687 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Subjects ", "page_idx": 22}, {"type": "text", "text": "89 Question: Does the paper describe potential risks incurred by study participants, whether 90 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or 92 institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: As there were no study participants in this study, this information was not ", "page_idx": 23}, {"type": "text", "text": "695 included in this paper.   \n696 Guidelines:   \n697 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n698 human subjects.   \n699 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n700 may be required for any human subjects research. If you obtained IRB approval, you   \n701 should clearly state this in the paper.   \n702 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n703 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n704 guidelines for their institution.   \n705 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n706 applicable), such as the institution conducting the review. ", "page_idx": 23}]