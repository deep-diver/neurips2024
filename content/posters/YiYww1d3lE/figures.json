[{"figure_path": "YiYww1d3lE/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Standard protocol directly calculates distances between raw images and class names in the joint V-L space. (b) Prompt-based methods enhance inputs with post-trained visual or textual prompts to provide the task-specific context. (c) Augment-based method enriches raw inputs with image transformations and class descriptions, requiring no additional training. Upon this, we propose AWT, which considers both intra-modal importance variations and cross-modal semantic correlations. (d) AWT is evaluated against SOTA methods across four tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition.", "description": "This figure illustrates three different approaches for adapting Vision-Language Models (VLMs) to new tasks and compares their performance. (a) shows the standard approach that calculates distances between raw image and text embeddings. (b) depicts a prompt-based method enriching inputs with additional task-specific information in the form of visual or textual prompts. (c) illustrates the augment-based AWT method enriching raw inputs with image transformations and more detailed class descriptions from language models.  Finally, (d) shows a performance comparison of the AWT method with state-of-the-art (SOTA) methods across multiple tasks (zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition).", "section": "1 Introduction"}, {"figure_path": "YiYww1d3lE/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline of AWT: Augment, Weight, then Transport. Given an image and candidate class names, we first augment each input into diverse views. These views are then fed into the CLIP model to obtain coarse predictions. To assess the importance of each view, we use prediction confidence as a proxy and introduce an entropy-based weighting mechanism. Next, we measure the distance between image-text view sets by solving an optimal transport (OT) problem. Finally, the resulting OT distance is used to represent the distance between the input image and each class name.", "description": "This figure illustrates the AWT framework's pipeline. It starts with an input image and candidate class names, which are augmented into multiple diverse views using image transformations and LLM-generated descriptions.  These augmented views are fed into CLIP to generate initial predictions, and their importance is scored based on prediction confidence. Optimal Transport then calculates the distance between image and text view sets, ultimately determining the distance between the input image and each class name.", "section": "3.2 AWT: Augment, Weight, then Transport"}, {"figure_path": "YiYww1d3lE/figures/figures_8_1.jpg", "caption": "Figure 4: Versatility analysis of AWT. Average top-1 accuracy (%) on 18 image datasets is reported.", "description": "This figure demonstrates the versatility and generalizability of the AWT framework across different vision-language model (VLM) backbones and architectures. Subfigure (a) shows AWT's performance using different CLIP backbones, from lightweight models (RN50, RN101) to more powerful transformer-based models (ViT-B/32, ViT-B/16, ViT-L/14, L/14@336). Subfigures (b) and (c) show AWT's adaptability to various VLMs (ALIGN, SigLIP, and EVA-CLIP), highlighting its consistent improvement over baseline performance.", "section": "4.5 Versatility Study"}, {"figure_path": "YiYww1d3lE/figures/figures_9_1.jpg", "caption": "Figure 5: Comparison of image augmentation techniques on low-resolution images. We present images from the CIFAR-10/100 datasets, where each image is 32 \u00d7 32 pixels. The comparison includes images generated by traditional image transformations and DALL\u00b7E 2.", "description": "This figure compares three different image augmentation techniques on low-resolution images from the CIFAR-10 and CIFAR-100 datasets.  The original images are shown alongside versions augmented using random resized cropping and flipping, and versions generated using the DALL-E 2 diffusion model. The purpose is to illustrate the impact of different augmentation strategies on low-resolution images and how these different augmentation methods deal with the low resolution issue.", "section": "4.6 Failure Case Analysis"}, {"figure_path": "YiYww1d3lE/figures/figures_16_1.jpg", "caption": "Figure 6: Visualization of weighting image views. We show the weights assigned to the same image view set under varying candidate class names. Our dynamic weighting strategy effectively allocates importance to contextually relevant image views.", "description": "This figure visualizes the results of the entropy-based weighting strategy used in the AWT framework. It shows how the importance weights assigned to different image augmentations of the same image vary depending on the candidate class names.  The weights are dynamically assigned, prioritizing views that are contextually relevant to the predicted class.", "section": "A Visualization"}, {"figure_path": "YiYww1d3lE/figures/figures_17_1.jpg", "caption": "Figure 6: Visualization of weighting image views. We show the weights assigned to the same image view set under varying candidate class names. Our dynamic weighting strategy effectively allocates importance to contextually relevant image views.", "description": "This figure visualizes the weights assigned to image views using the entropy-based weighting mechanism.  It demonstrates how the weights change depending on the candidate class names. The results highlight how the model prioritizes image views that are relevant to the class being considered, effectively allocating importance to contextually significant features.", "section": "A Visualization"}, {"figure_path": "YiYww1d3lE/figures/figures_17_2.jpg", "caption": "Figure 8: Visualization of cross-modal correlations captured by optimal transport.", "description": "This figure visualizes the cross-modal correlations discovered by the optimal transport method used in AWT.  After augmenting the input image and class names to generate diverse views, the optimal transport method identifies and quantifies the correlations between these views. The heatmap shows the strength of correlation between different image views (columns) and textual descriptions (rows). High values (darker blue) indicate stronger semantic relationships between the image and text modalities, demonstrating how AWT effectively captures cross-modal interactions.", "section": "A Visualization"}, {"figure_path": "YiYww1d3lE/figures/figures_18_1.jpg", "caption": "Figure 1: (a) Standard protocol directly calculates distances between raw images and class names in the joint V-L space. (b) Prompt-based methods enhance inputs with post-trained visual or textual prompts to provide the task-specific context. (c) Augment-based method enriches raw inputs with image transformations and class descriptions, requiring no additional training. Upon this, we propose AWT, which considers both intra-modal importance variations and cross-modal semantic correlations. (d) AWT is evaluated against SOTA methods across four tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition.", "description": "This figure illustrates the three main approaches for adapting vision-language models (VLMs) to new tasks.  (a) shows the standard approach where distances between image and class embeddings are directly calculated.  (b) demonstrates the prompt-based approach, which uses post-trained visual or textual information to enrich the input. (c) presents the augmentation-based method, similar to the proposed AWT method.  (c) depicts the AWT approach where inputs are augmented by transformations and class descriptions, importance variations are considered, and semantic correlations are mined. (d) shows the performance of AWT compared to other state-of-the-art methods across various tasks.", "section": "1 Introduction"}, {"figure_path": "YiYww1d3lE/figures/figures_21_1.jpg", "caption": "Figure 10: Architecture of the adapter module and its integration with the Transformer.", "description": "This figure shows the architecture of a multi-modal adapter module used in few-shot transfer learning. The adapter module is inserted after the multi-head self-attention and MLP module within each transformer layer.  It consists of a down-projection layer, a GeLU activation layer, an up-projection layer, and a trainable scale parameter. The module is applied to both the image and text embeddings to reduce the number of parameters while maintaining performance.  The figure highlights which components are trainable and which are frozen.", "section": "3.2 AWT: Augment, Weight, then Transport"}]