[{"figure_path": "y929esCZNJ/tables/tables_6_1.jpg", "caption": "Table 1: Perplexity (PPL) of momentum-based SMoE vs. SMoE baseline on clean/attacked WikiText-103.", "description": "This table presents the perplexity (PPL) scores, a measure of language model performance, for different models on the WikiText-103 dataset.  It compares the performance of standard Sparse Mixture of Experts (SMoE) models with three variations incorporating momentum: MomentumSMoE, AdamSMoE, and Robust MomentumSMoE.  Results are shown for both clean and attacked (word-swap) versions of the WikiText-103 dataset, indicating model robustness.  The table includes two sizes of SMoE models and also compares with a GLaM model to demonstrate the applicability of the proposed methods across different model architectures.", "section": "5.1 WikiText-103 Language Modeling"}, {"figure_path": "y929esCZNJ/tables/tables_7_1.jpg", "caption": "Table 2: Top-1 accuracy (%) and mean corruption error (mCE) of MomentumV-MoE and Robust MomentumV-MoE vs. the V-MoE baseline on ImageNet-1K and popular robustness benchmarks for image classification.", "description": "This table presents a comparison of the top-1 accuracy and mean corruption error (mCE) achieved by three different vision models on the ImageNet-1K dataset and several robustness benchmark datasets.  The models compared are the baseline V-MoE, MomentumV-MoE (incorporating heavy-ball momentum), and Robust MomentumV-MoE (incorporating robust momentum). The results show the performance of each model on clean ImageNet-1K data, as well as its robustness to various corruptions and adversarial attacks (ImageNet-C, ImageNet-R, and ImageNet-A).", "section": "5.2 ImageNet-1K Object Recognition Task"}, {"figure_path": "y929esCZNJ/tables/tables_8_1.jpg", "caption": "Table 3: Top-1/top-5 accuracy (%) of Momentum-Soft MoE vs. Soft MoE baseline on ImageNet-1K (IN-1K).", "description": "This table presents a comparison of the top-1 and top-5 accuracy results for two models on the ImageNet-1K dataset: a baseline Soft MoE model and a Momentum-Soft MoE model (which incorporates momentum into the Soft MoE architecture).  The table shows that the Momentum-Soft MoE model achieves a notable improvement in both top-1 and top-5 accuracy compared to the baseline Soft MoE model, highlighting the benefit of incorporating momentum.", "section": "5.2 ImageNet-1K Object Recognition Task"}, {"figure_path": "y929esCZNJ/tables/tables_20_1.jpg", "caption": "Table 1: Perplexity (PPL) of momentum-based SMoE vs. SMoE baseline on clean/attacked WikiText-103.", "description": "This table presents the results of perplexity (PPL) on both clean and attacked versions of the WikiText-103 dataset. It compares the performance of different momentum-based Sparse Mixture of Experts (SMoE) models against a standard SMoE baseline.  The models are categorized by size (medium and large) and type (MomentumSMoE, AdamSMoE). The table shows that the momentum-based models generally achieve lower perplexity scores (indicating better performance) than the baseline, particularly on the attacked dataset.  This demonstrates the effectiveness of incorporating momentum in enhancing the stability and robustness of SMoE, especially in dealing with noisy or corrupted data.", "section": "5.1 WikiText-103 Language Modeling"}, {"figure_path": "y929esCZNJ/tables/tables_23_1.jpg", "caption": "Table 5: Perplexity (PPL) results on clean and attacked WikiText-103 validation and test data for standard MomentumSMoE with tuned hyperparameters \u03bc and \u03b3 and MomentumSMoE trained with different learning settings for \u03bc and \u03b3 that do not require tuning: i) Both \u03bc and \u03b3 are scalar learnable parameters in Pytorch. ii) Only \u03b3 is learned with fixed \u03bc = 0.7.", "description": "This table presents the perplexity (PPL) results of different MomentumSMoE models on the WikiText-103 dataset.  It compares the performance of the standard MomentumSMoE model with tuned hyperparameters (\u03bc and \u03b3) to two variations where these hyperparameters are learned during training: one where both are learned, and another where only \u03b3 is learned while \u03bc is fixed. The results are shown for both clean and attacked versions of the dataset, allowing for evaluation of model robustness.", "section": "5.1 WikiText-103 Language Modeling"}, {"figure_path": "y929esCZNJ/tables/tables_24_1.jpg", "caption": "Table 5: Perplexity (PPL) results on clean and attacked WikiText-103 validation and test data for standard MomentumSMoE with tuned hyperparameters \u03bc and \u03b3 and MomentumSMoE trained with different learning settings for \u03bc and \u03b3 that do not require tuning: i) Both \u03bc and \u03b3 are scalar learnable parameters in Pytorch. ii) Only \u03b3 is learned with fixed \u03bc = 0.7. ", "description": "This table shows the perplexity (PPL) results on clean and attacked WikiText-103 validation and test datasets for different versions of the MomentumSMoE model.  It compares the performance of the standard MomentumSMoE model with tuned hyperparameters (\u03bc and \u03b3) against MomentumSMoE models where either both or just \u03b3 are learned during training, eliminating the need for manual hyperparameter tuning. The results for both clean and attacked datasets are presented to evaluate model robustness.", "section": "Additional Experimental Results"}, {"figure_path": "y929esCZNJ/tables/tables_25_1.jpg", "caption": "Table 1: Perplexity (PPL) of momentum-based SMoE vs. SMoE baseline on clean/attacked WikiText-103.", "description": "This table presents the perplexity (PPL) scores, a measure of language model performance, for various models on the WikiText-103 dataset.  It compares the performance of standard Sparse Mixture of Experts (SMoE) models against versions incorporating momentum (MomentumSMoE), Adam (AdamSMoE), and a medium-sized Generalist Language Model (GLaM) with and without momentum. The results are shown for both clean and \"attacked\" (word-swapped) versions of the WikiText-103 dataset, providing insight into the models' robustness to data corruption.  Lower PPL values indicate better performance.", "section": "5.1 WikiText-103 Language Modeling"}, {"figure_path": "y929esCZNJ/tables/tables_25_2.jpg", "caption": "Table 2: Top-1 accuracy (%) and mean corruption error (mCE) of MomentumV-MoE and Robust MomentumV-MoE vs. the V-MoE baseline on ImageNet-1K and popular robustness benchmarks for image classification.", "description": "This table presents a comparison of the Top-1 accuracy and mean corruption error (mCE) achieved by three different vision models (V-MoE, MomentumV-MoE, and Robust MomentumV-MoE) on the ImageNet-1K dataset and several robustness benchmarks.  The benchmarks assess the models' performance under various image corruptions and perturbations. The table shows that MomentumV-MoE and Robust MomentumV-MoE provide improved accuracy and robustness compared to the baseline V-MoE model.", "section": "5.2 ImageNet-1K Object Recognition Task"}, {"figure_path": "y929esCZNJ/tables/tables_26_1.jpg", "caption": "Table 9: Perplexity (PPL) results on clean and word swap attacked WikiText-103 validation and test data for baseline SMOE, NAG-SMOE, SR-SMOE, rms-SMOE, SAM-SMOE, Robust MomentumSMOE, Negative-MomentumSMOE, and Complex-MomentumSMOE.", "description": "This table presents the perplexity (PPL) results on the WikiText-103 dataset for several variations of the SMoE model, including the baseline model and those incorporating different momentum-based optimization techniques.  The results are shown for both clean and attacked (word swap) versions of the dataset, allowing for a comparison of model performance under different conditions. Lower PPL values indicate better performance.  The table highlights the impact of various momentum strategies on language modeling performance and robustness against data corruption.", "section": "Additional Experimental Results"}, {"figure_path": "y929esCZNJ/tables/tables_27_1.jpg", "caption": "Table 2: Top-1 accuracy (%) and mean corruption error (mCE) of MomentumV-MoE and Robust MomentumV-MoE vs. the V-MoE baseline on ImageNet-1K and popular robustness benchmarks for image classification.", "description": "This table presents a comparison of the performance of three different models (V-MoE baseline, MomentumV-MoE, and Robust MomentumV-MoE) on the ImageNet-1K dataset and three robustness benchmarks (ImageNet-C, ImageNet-R, and ImageNet-A).  The metrics used are Top-1 accuracy and mean corruption error (mCE).  The results demonstrate the improved robustness of the Momentum-enhanced models, particularly the Robust MomentumV-MoE, against various image corruptions and distortions.", "section": "5.2 ImageNet-1K Object Recognition Task"}, {"figure_path": "y929esCZNJ/tables/tables_28_1.jpg", "caption": "Table 1: Perplexity (PPL) of momentum-based SMoE vs. SMoE baseline on clean/attacked WikiText-103.", "description": "This table presents the perplexity (PPL) scores, a metric for evaluating language model performance,  for different model variations on the WikiText-103 dataset.  It compares the performance of standard Sparse Mixture of Experts (SMoE) models with three momentum-enhanced versions: MomentumSMoE, AdamSMoE, and Robust MomentumSMoE.  The results are shown for both clean and \"attacked\" (adversarially perturbed) versions of the WikiText-103 dataset, providing insights into the models' robustness to data contamination.  Different model sizes (medium and large) are included to analyze the effect of model scale on performance and robustness. ", "section": "5.1 WikiText-103 Language Modeling"}, {"figure_path": "y929esCZNJ/tables/tables_28_2.jpg", "caption": "Table 12: Total computation time for SMOE, MomentumSMoE and AdamSMoE to reach 38 PPL on WikiText-103 validation data.", "description": "This table presents the total training time in minutes for three different models (SMOE, MomentumSMoE, and AdamSMoE) to achieve a perplexity (PPL) score of 38 on the WikiText-103 validation dataset.  It highlights the computational efficiency of the proposed MomentumSMoE model relative to the baseline SMOE and AdamSMoE.", "section": "5.1 WikiText-103 Language Modeling"}]