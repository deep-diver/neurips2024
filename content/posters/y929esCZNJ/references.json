{"references": [{"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This paper introduces the Sparse Mixture of Experts (SMoE) architecture, which is the foundation of the proposed MomentumSMoE method."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper presents the Switch Transformer architecture, a type of SMoE model used in the experiments, thus highlighting its importance for evaluation."}, {"fullname_first_author": "Nan Du", "paper_title": "GLaM: Efficient scaling of language models with mixture-of-experts", "publication_date": "2022-07-17", "reason": "This paper introduces the GLaM model, another type of SMoE model used in the experiments."}, {"fullname_first_author": "Carlos Riquelme", "paper_title": "Scaling vision with sparse mixture of experts", "publication_date": "2021-00-00", "reason": "This paper introduces the V-MoE model, a vision-specific SMoE model used in the experiments."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper is a seminal work in large language models, providing foundational context for the scalability challenges that SMoE aims to address."}]}