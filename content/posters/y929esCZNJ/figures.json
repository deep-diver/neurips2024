[{"figure_path": "y929esCZNJ/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of SMoE (Left) and MomentumSMoE layer (Right). We establish a connection between Multiple-Gradient Descent and SMoE to introduce momentum into the model, leading to better accuracy, enhanced robustness, and faster convergence.", "description": "This figure illustrates the architecture of both the standard Sparse Mixture of Experts (SMoE) layer and the proposed MomentumSMoE layer.  The SMoE layer shows a simple residual connection where the output is the input plus the output of the expert network. The MomentumSMoE layer extends this by adding a momentum term to stabilize and accelerate convergence.  The momentum term is calculated from the negative gradient and the previous momentum, and is then added to the input to generate the next layer's input.  This illustrates how momentum is integrated into the SMoE framework.", "section": "2 Momentum Sparse Mixture of Experts"}, {"figure_path": "y929esCZNJ/figures/figures_3_1.jpg", "caption": "Figure 2: Average output norms at layers 1 and 6 of the MoE/SMoE during 80 training epochs on WikiText-103.", "description": "This figure displays the average output norms at layers 1 and 6 of both the MoE and SMoE models during the training process on the WikiText-103 dataset. The x-axis represents the training epochs (iterations), and the y-axis represents the average output norm.  The plot shows the trend of the norms for both models across the training epochs for both the training and validation datasets.  It visually demonstrates differences between MoE and SMoE in terms of their output norm behavior throughout training, which may offer insights into their convergence characteristics.", "section": "2 Momentum Sparse Mixture of Experts"}, {"figure_path": "y929esCZNJ/figures/figures_3_2.jpg", "caption": "Figure 3: Average output norm at each layer across 1K train/validation samples of the (S)MoE trained on WikiText-103.", "description": "This figure shows the average output norm at each layer of both MoE and SMoE models trained on the WikiText-103 dataset.  The x-axis represents the layer number (1 through 5), and the y-axis represents the average output norm. Separate lines show the training and validation data for each model.  The figure is used to empirically demonstrate the connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem.", "section": "2 Momentum Sparse Mixture of Experts"}, {"figure_path": "y929esCZNJ/figures/figures_7_1.jpg", "caption": "Figure 4: Left: WikiText-103 train/validation perplexity (PPL) curves during the first 5 training epochs for MomentumSMoE, AdamSMoE, and SMoE. AdamSMoE has significantly faster convergence compared to SMoE. Right: Training loss/top-1 accuracy (%) of Momentum-Soft MoE vs. Soft MoE baseline on ImageNet-1K across 120 epochs of training. Momentum-Soft MoE has faster convergence and improved accuracy.", "description": "This figure compares the performance of MomentumSMoE and AdamSMoE with the baseline SMoE model on two different tasks: WikiText-103 language modeling and ImageNet-1K image classification. The left panel shows the training and validation perplexity (PPL) curves for the three models during the first five epochs on the WikiText-103 dataset, highlighting the faster convergence of AdamSMoE. The right panel displays the training loss and top-1 validation accuracy for Momentum-Soft MoE and the baseline Soft MoE model over 120 epochs on the ImageNet-1K dataset, demonstrating the superior performance and faster convergence of Momentum-Soft MoE.", "section": "Experimental Results"}, {"figure_path": "y929esCZNJ/figures/figures_8_1.jpg", "caption": "Figure 5: Left: Proportion of each expert chosen, ordered from the largest norm of each expert output to the smallest norm, in layers 3 and 5 of SMoE, MomentumSMoE, and AdamSMoE, averaged over the WikiText-103 validation set. Right: Log validation perplexity (PPL) during the finetuning of hyperparameters, \u03bc and \u03b3, for 40 training epochs in MomentumSMoE. When tuning \u03b3, we keep \u03bc = 0.7 and vice versa with \u03b3 = 1.0.", "description": "The figure demonstrates the expert selection in SMoE, MomentumSMoE, and AdamSMoE models trained on WikiText-103.  The left panel shows the proportion of times each expert is chosen, ordered by the magnitude of its output norm.  This illustrates the impact of momentum on load balancing across experts. The right panel shows how the validation perplexity changes during hyperparameter tuning (momentum coefficient \u03bc and step size \u03b3) for the MomentumSMoE model, highlighting the model's sensitivity to these parameters.", "section": "6 Empirical Analysis"}, {"figure_path": "y929esCZNJ/figures/figures_19_1.jpg", "caption": "Figure 2: Average output norms at layers 1 and 6 of the MoE/SMoE during 80 training epochs on WikiText-103.", "description": "This figure shows the average output norms at layer 1 and layer 6 of the MoE and SMoE models during training on the WikiText-103 dataset.  The x-axis represents the training epoch, and the y-axis represents the average output norm.  Separate lines are shown for training and validation data for both the MoE and SMoE models.  The figure illustrates the trends in output norms over the course of training, potentially highlighting differences in the training dynamics between MoE and SMoE.", "section": "2 Momentum Sparse Mixture of Experts"}, {"figure_path": "y929esCZNJ/figures/figures_19_2.jpg", "caption": "Figure 2: Average output norms at layers 1 and 6 of the MoE/SMoE during 80 training epochs on WikiText-103.", "description": "This figure displays the average output norms at layers 1 and 6 of the MoE (Mixture of Experts) and SMoE (Sparse Mixture of Experts) models during 80 training epochs on the WikiText-103 dataset.  The plots show the norms for both the training and validation sets, providing a visual representation of how the model's output changes over time and across different stages of training. This helps to understand the stability and convergence behavior of the two models, where lower norms generally indicate better stability.", "section": "2 Momentum Sparse Mixture of Experts"}, {"figure_path": "y929esCZNJ/figures/figures_20_1.jpg", "caption": "Figure 8: Proportion of each expert chosen, ordered from the largest norm of each expert output to the smallest norm, in all layers of baseline SMoE.", "description": "This figure shows the proportion of times each expert in a Sparse Mixture of Experts (SMoE) model is chosen during inference, ordered by the magnitude of the norm of its output.  The x-axis represents the experts, ordered from the one with the largest norm to the one with the smallest norm. The y-axis represents the proportion of times each expert was selected across all layers (1-6) of the SMoE model.  The figure demonstrates a significant load imbalance, with a small number of experts being selected much more frequently than others.", "section": "Empirical Evidence"}, {"figure_path": "y929esCZNJ/figures/figures_20_2.jpg", "caption": "Figure 8: Proportion of each expert chosen, ordered from the largest norm of each expert output to the smallest norm, in all layers of baseline SMoE.", "description": "This figure shows the proportion of times each expert is selected in each layer of the baseline SMoE model.  The experts are ordered from the largest to the smallest norm of their output.  The visualization helps to understand the load imbalance problem in SMoE, where some experts are chosen much more frequently than others.  The uneven distribution is a key characteristic of standard SMoE training.", "section": "Empirical Evidence"}, {"figure_path": "y929esCZNJ/figures/figures_21_1.jpg", "caption": "Figure 8: Proportion of each expert chosen, ordered from the largest norm of each expert output to the smallest norm, in all layers of baseline SMoE.", "description": "This figure shows the proportion of times each expert is selected during inference for each layer of the baseline SMoE model.  Experts are ordered on the x-axis from the largest to smallest norm of their outputs. The y-axis shows the proportion of times each expert was selected.  The figure visually represents the load imbalance among experts in the baseline SMoE model, highlighting those experts that are consistently chosen over others during inference.", "section": "Empirical Evidence"}, {"figure_path": "y929esCZNJ/figures/figures_26_1.jpg", "caption": "Figure 14: mCE (lower is better) of baseline V-MoE, MomentumV-MoE, Robust MomentumV-MoE and SAM-V-MoE on increasing severities of impulse noise and gaussian noise corruption. As the severity increases, the effect of momentum, robust momentum and SAM becomes increasingly apparent.", "description": "The figure shows the mean corruption error (mCE) for four different vision models (V-MoE, MomentumV-MoE, Robust MomentumV-MoE, and SAM-V-MoE) on ImageNet-C dataset under impulse and Gaussian noise with increasing severity levels.  It demonstrates that the incorporation of momentum and robust momentum techniques improves the robustness of the models against these corruptions, especially at higher severity levels.", "section": "5.2 ImageNet-1K Object Recognition Task"}]