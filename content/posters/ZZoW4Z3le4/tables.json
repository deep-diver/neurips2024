[{"figure_path": "ZZoW4Z3le4/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five different datasets using various activation functions.  The goal is to compare the performance of DIGRAF, a novel graph-adaptive activation function, against several baseline activation functions, including standard activation functions (ReLU, Tanh, Sigmoid, etc.), learnable activation functions (PReLU, Maxout, Swish), and existing graph-specific activation functions (Max, Median, GReLU). The table shows the accuracy achieved by each method on each dataset.  The top three performing methods for each dataset are highlighted.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_7_2.jpg", "caption": "Table 2: Comparison on ZINC-12K under the 500K parameter budget. The top three methods are First, Second, Third.", "description": "This table presents the results of a regression task on the ZINC-12K dataset, focusing on predicting the constrained solubility of molecules.  It compares various activation functions within a Graph Neural Network (GNN) architecture, specifically using a GIN backbone. The table highlights the Mean Absolute Error (MAE) achieved by each activation function, indicating their performance in predicting molecular solubility. The best three performing methods are highlighted.", "section": "5.2 Graph Classification and Regression"}, {"figure_path": "ZZoW4Z3le4/tables/tables_8_1.jpg", "caption": "Table 3: A comparison of DIGRAF to natural baselines, standard, and graph activation layers on OGB datasets, demonstrating the advantage of our approach. The top three methods are marked by First, Second, Third.", "description": "This table compares the performance of DIGRAF against various baseline activation functions on four datasets from the Open Graph Benchmark (OGB).  The baselines include standard activation functions (Identity, Sigmoid, ReLU, LeakyReLU, Tanh, GeLU, ELU), learnable activation functions (PReLU, Maxout, Swish), and graph-specific activation functions (Max, Median, GReLU).  The table shows that DIGRAF consistently outperforms the baselines across different evaluation metrics (RMSE and ROC-AUC) for various tasks, highlighting its effectiveness as a graph-adaptive activation function.", "section": "5 Experiments"}, {"figure_path": "ZZoW4Z3le4/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five different datasets (BLOGCATALOG, FLICKR, CITESEER, CORA, and PUBMED).  It compares the performance of DIGRAF against various baselines, categorized as Standard Activations, Learnable Activations, and Graph Activations.  The top three performing methods for each dataset are highlighted. The accuracy is measured as a percentage and higher values indicate better performance.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_21_1.jpg", "caption": "Table 5: Graph classification accuracy (%) \u2191 on TUDatasets. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of graph classification experiments on five datasets from the TUDataset collection.  The table compares the performance of DIGRAF against various baseline activation functions, including standard activation functions (ReLU, Tanh, Sigmoid, etc.), learnable activation functions (PReLU, Maxout, Swish), and other graph-adaptive activation functions (Max, Median, GReLU). The accuracy is reported as a percentage, and the top three performing methods for each dataset are highlighted.  The table shows that DIGRAF consistently achieves high accuracy across the different datasets and often outperforms the other methods.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_21_2.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five different datasets (BLOGCATALOG, FLICKR, CITESEER, CORA, and PUBMED).  The experiments compare the performance of DIGRAF against various baseline activation functions, categorized as standard activation functions (e.g., ReLU, Tanh, Sigmoid), learnable activation functions (e.g., PReLU, Maxout, Swish), and graph-specific activation functions (e.g., Max, Median, GReLU).  The table shows the accuracy achieved by each activation function on each dataset.  The top three performing methods for each dataset are highlighted.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_22_1.jpg", "caption": "Table 7: Performance Comparison of DIGRAF with ReLU variants of increased parameter budget. The number of parameters is reported within the parenthesis adjacent to the metric. We use GINE [37] as a backbone. Increasing the parameter count with ReLU does not yield significant improvements, and DIGRAF outperforms all variants, even those with a higher number of parameters. Note that, DIGRAF (W/O ADAP.) has only Np \u2013 1 additional parameters, where Np is the tessellation size.", "description": "This table compares the performance of DIGRAF against different ReLU variants with varying parameter budgets. It shows that increasing the number of parameters in ReLU does not significantly improve performance. DIGRAF consistently outperforms all ReLU variants, highlighting its efficiency and effectiveness even with a relatively smaller number of parameters.", "section": "E.5 Parameter Count Comparison"}, {"figure_path": "ZZoW4Z3le4/tables/tables_23_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments conducted on five different datasets using various activation functions. The results show the accuracy of each activation function on the datasets, allowing for a comparison of performance between traditional activation functions, learnable activation functions, graph-specific activation functions, and DIGRAF (both with and without adaptivity). The top three methods for each dataset are highlighted.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_24_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents a comparison of node classification accuracy achieved using different activation functions on various benchmark datasets.  The activation functions are categorized into standard, learnable, and graph-specific activations, with DIGRAF being the proposed method.  The table shows that DIGRAF consistently achieves state-of-the-art performance across all the datasets, highlighting its effectiveness. The top three performing methods are clearly marked for each dataset.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_24_2.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five different datasets using various activation functions.  The performance of DIGRAF is compared against baseline activation functions categorized as standard, learnable, and graph-specific.  The table shows the accuracy achieved by each activation function on each dataset, highlighting DIGRAF's superior performance.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_25_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on several datasets using different activation functions.  The performance of DIGRAF is compared against various baselines including standard activation functions (ReLU, Tanh, Sigmoid, etc.), learnable activation functions (PReLU, Maxout, Swish), and other graph-specific activation functions (Max, Median, GReLU). The table shows the accuracy achieved by each method on each dataset and highlights the top three performing methods.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_25_2.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table compares the performance of DIGRAF against various baselines on five node classification datasets.  The baselines include standard activation functions (Identity, Sigmoid, ReLU, LeakyReLU, Tanh, GeLU, ELU), learnable activation functions (PReLU, Maxout, Swish), and graph-specific activation functions (Max, Median, GReLU).  The table shows the accuracy achieved by each method on each dataset, highlighting DIGRAF's superior performance.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_26_1.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five different datasets using various activation functions, including DIGRAF and several baselines.  The accuracy is measured and the top three performing methods for each dataset are highlighted.  The baselines encompass standard activation functions, learnable activation functions, and graph-specific activation functions.  This table demonstrates the superior performance of DIGRAF compared to the baselines across multiple datasets.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_26_2.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table presents the results of node classification experiments on five datasets using different activation functions.  The table compares the performance of DIGRAF against several baseline methods, including standard activation functions (Identity, Sigmoid, ReLU, LeakyReLU, Tanh, GeLU, ELU), learnable activation functions (PReLU, Maxout, Swish), and graph-specific activation functions (Max, Median, GReLU). The top three performing methods for each dataset are highlighted.  It demonstrates DIGRAF's superior performance and its consistent improvement across various graph datasets and tasks.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_26_3.jpg", "caption": "Table 1: Comparison of node classification accuracy (%) \u2191 on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third.", "description": "This table compares the performance of DIGRAF against various baseline activation functions (standard, learnable, graph-specific) on five different node classification datasets (BLOGCATALOG, FLICKR, CITESEER, CORA, PUBMED).  The accuracy is reported as a percentage, with the top three methods for each dataset highlighted. This demonstrates DIGRAF's performance advantage.", "section": "5.1 Node Classification"}, {"figure_path": "ZZoW4Z3le4/tables/tables_26_4.jpg", "caption": "Table 2: Comparison on ZINC-12K under the 500K parameter budget. The top three methods are First, Second, Third.", "description": "This table presents the results of a regression task on the ZINC-12K dataset, focusing on predicting the constrained solubility of molecules.  It compares various activation functions within a Graph Isomorphism Network (GIN) architecture, all under a 500K parameter budget.  The table highlights the mean absolute error (MAE) achieved by each activation function, indicating the accuracy of the solubility prediction.  The top three performing activation functions are identified as \"First,\" \"Second,\" and \"Third.\"", "section": "5.2 Graph Classification and Regression"}]