[{"figure_path": "ZZoW4Z3le4/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of DIGRAF. Node features H<sup>(l-1)</sup> and adjacency matrix A are fed to a GNN<sub>LAYER</sub> to obtain updated intermediate node features \u0124<sup>(l)</sup>, which are passed to our activation function layer, DIGRAF. First, an additional GNN network GNN<sub>ACT</sub> takes \u0124<sup>(l)</sup> and A as input to determine the activation function parameters \u03b8<sup>(l)</sup>. These are used to parameterize the transformation T<sup>(l)</sup>, which operates on \u0124<sup>(l)</sup> to produce the activated node features H<sup>(l)</sup>.", "description": "The figure illustrates the architecture of DIGRAF, a novel activation function for graph neural networks.  It shows how node features and the adjacency matrix are processed through a GNN layer, then fed into the DIGRAF activation function. DIGRAF uniquely uses a second GNN (GNN<sub>ACT</sub>) to learn graph-adaptive parameters (\u03b8<sup>(l)</sup>) for the activation function, allowing it to dynamically adapt to different graph structures. These parameters are used in a transformation (T<sup>(l)</sup>) applied to the intermediate node features to produce the final activated node features.", "section": "4 DIGRAF"}, {"figure_path": "ZZoW4Z3le4/figures/figures_2_1.jpg", "caption": "Figure 2: Approximation of traditional activation functions using CPAB and Piecewise ReLU with varying segment counts K \u2208 {1, 2, 3} on a closed interval \u03a9 = [-5, 5], demonstrating the advantage of utilizing CPAB and its flexibility to model various activation functions.", "description": "This figure demonstrates the ability of Continuous Piecewise-Affine Based (CPAB) transformations and Piecewise ReLU to approximate traditional activation functions like ELU and Tanh. It showcases the flexibility of CPAB, which can model various activation functions by adjusting the number of segments (K).  The plots show how well CPAB and Piecewise ReLU, with different numbers of segments, approximate the shapes of the ELU and Tanh activation functions.  It highlights the benefit of using CPAB to model activation functions.", "section": "Related Work"}, {"figure_path": "ZZoW4Z3le4/figures/figures_3_1.jpg", "caption": "Figure 3: An example of CPA velocity fields v\u03b8 defined on the interval \u03a9 = [\u22125, 5] with a tessellation P consisting of five subintervals. The three different parameters, \u03b81, \u03b82, and \u03b83 define three distinct CPA velocity fields (Figure 3a) resulting in separate CPAB diffeomorphisms f\u03b8(x) (Figure 3b).", "description": "This figure shows examples of how different CPA velocity fields are generated using different parameters (\u03b81, \u03b82, \u03b83) and how these fields lead to different CPAB diffeomorphisms.  The left panel (a) displays the velocity fields, demonstrating the piecewise-affine nature, and showing how different parameterizations yield different velocity profiles. The right panel (b) shows the resulting diffeomorphisms, illustrating the effect of the velocity fields on the transformation of the input space. Each colored curve in (b) corresponds to the velocity field with the same color in (a), highlighting the relationship between the velocity field and the resulting diffeomorphic transformation.", "section": "3 Mathematical Background and Notations"}, {"figure_path": "ZZoW4Z3le4/figures/figures_4_1.jpg", "caption": "Figure 4: Different transformation strategies. The input function (red), CPAB transformation (blue), and DIGRAF transformation (green), within \u03a9 = [-5, 5] using the same \u03b8. While CPAB stretches the input, DIGRAF stretches the output, showcasing the distinctive impact of each approach.", "description": "This figure illustrates the conceptual difference between CPAB and DIGRAF transformations.  CPAB transforms the input function horizontally, while DIGRAF transforms it vertically.  This highlights DIGRAF's unique application of CPAB, adapting it to function as a graph activation function by modifying the output rather than the input.  Both methods use CPAB, but their applications and the resulting effects differ.", "section": "4.1 A CPAB Blueprint for Graph Activation Functions"}, {"figure_path": "ZZoW4Z3le4/figures/figures_8_1.jpg", "caption": "Figure 5: Convergence analysis of DIGRAF compared to baseline activation functions. The plot illustrates the training loss over epochs, showcasing the overall faster convergence of DIGRAF.", "description": "This figure shows the training loss curves for various activation functions, including DIGRAF, across three different datasets: CORA, FLICKR, and ZINC-12K.  The plots demonstrate that DIGRAF generally converges faster than other activation functions, indicating improved training stability and efficiency.", "section": "5 Experiments"}, {"figure_path": "ZZoW4Z3le4/figures/figures_20_1.jpg", "caption": "Figure 6: The approximation error of the Peaks function (Equation (27)) with ReLU, Tanh, and DIGRAF.", "description": "The figure shows the approximation error of the Peaks function using three different activation functions: ReLU, Tanh, and DIGRAF.  The x-axis represents the number of iterations during the training process of a Multilayer Perceptron (MLP) designed to approximate the Peaks function. The y-axis (on a logarithmic scale) shows the Mean Squared Error (MSE) between the approximated function and the actual Peaks function.  The plot visually demonstrates the superior approximation power of DIGRAF compared to both ReLU and Tanh, achieving a significantly lower MSE after the same number of training iterations.", "section": "E.1 Function Approximation with CPAB"}, {"figure_path": "ZZoW4Z3le4/figures/figures_22_1.jpg", "caption": "Figure 7: Activation function learned by DIGRAF after the last GNN layer on two randomly selected graphs from ZINC. Different node colors indicate different node features. DIGRAF yields different activations for different graphs.", "description": "This figure shows the activation function learned by DIGRAF for two different graphs from the ZINC dataset after the last GNN layer. Each node in the graphs is represented by a different color, indicating its feature. The plot shows that DIGRAF produces different activation functions for different graphs, highlighting its adaptivity to different graph structures.", "section": "E.4 Visualization of DIGRAF"}, {"figure_path": "ZZoW4Z3le4/figures/figures_23_1.jpg", "caption": "Figure 5: Convergence analysis of DIGRAF compared to baseline activation functions. The plot illustrates the training loss over epochs, showcasing the overall faster convergence of DIGRAF.", "description": "The figure shows the training loss curves for various activation functions (DIGRAF and baselines) across three different datasets (CORA, Flickr, and ZINC-12k).  The plots demonstrate that DIGRAF converges faster than most baseline activation functions on all three datasets, indicating improved training efficiency.", "section": "5 Experiments"}]