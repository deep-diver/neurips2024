{"references": [{"fullname_first_author": "Thomas N. Kipf", "paper_title": "Semi-Supervised Classification with Graph Convolutional Networks", "publication_date": "2017-00-00", "reason": "This paper introduced the Graph Convolutional Network (GCN), a foundational model for many subsequent GNN architectures, making it highly influential in the field."}, {"fullname_first_author": "Oren Freifeld", "paper_title": "Highly-expressive spaces of well-behaved transformations: Keeping it simple", "publication_date": "2015-00-00", "reason": "This paper introduced the CPAB framework, which DIGRAF leverages, providing the foundation for DIGRAF's flexible and efficient diffeomorphic transformations."}, {"fullname_first_author": "Petar Veli\u010dkovi\u0107", "paper_title": "Graph Attention Networks", "publication_date": "2017-00-00", "reason": "This paper introduced the Graph Attention Network (GAT), another highly influential GNN architecture that uses attention mechanisms, which have become central to many GNN advancements."}, {"fullname_first_author": "Justin Gilmer", "paper_title": "Neural Message Passing for Quantum Chemistry", "publication_date": "2017-00-00", "reason": "This paper introduced the Neural Message Passing (NMP) framework, which is a fundamental concept for many GNN architectures, making it significant to GNN development."}, {"fullname_first_author": "Keyulu Xu", "paper_title": "How Powerful are Graph Neural Networks?", "publication_date": "2019-00-00", "reason": "This paper provides a theoretical analysis of the expressive power of GNNs, offering important insights into their capabilities and limitations, informing the development of more expressive models."}]}