[{"type": "text", "text": "DIGRAF: Diffeomorphic Graph-Adaptive Activation Function ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Krishna Sri Ipsit Mantri\u2217 Purdue University mantrik@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Xinzhi (Aurora) Wang\u2217 Purdue University wang6171@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Carola-Bibiane Sch\u00f6nlieb University of Cambridge cbs31@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Bruno Ribeiro Purdue University ribeirob@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Beatrice Bevilacqua\u2020 Purdue University bbevilac@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Moshe Eliasof\u2020 University of Cambridge me532@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we propose a novel activation function tailored specifically for graph data in Graph Neural Networks (GNNs). Motivated by the need for graph-adaptive and flexible activation functions, we introduce DIGRAF, leveraging Continuous Piecewise-Affine Based (CPAB) transformations, which we augment with an additional GNN to learn a graph-adaptive diffeomorphic activation function in an end-to-end manner. In addition to its graph-adaptivity and flexibility, DIGRAF also possesses properties that are widely recognized as desirable for activation functions, such as differentiability, boundness within the domain, and computational efficiency. We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DIGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs. Our code is available at https://github.com/ ipsitmantri/DiGRAF. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have found application across diverse domains, including social networks, recommendation systems, bioinformatics, and chemical analysis [82, 91, 68]. Recent advancements in GNN research have predominantly focused on exploring the design space of key architectural elements, ranging from expressive GNN layers [57, 23, 88, 89, 65], to pooling layers [87, 46, 5, 80], and positional and structural encodings [18, 67, 20]. Despite the exploration of these architectural choices, a common trend persists where most GNNs default to employing standard activation functions, such as ReLU [26], among a few others. ", "page_idx": 0}, {"type": "text", "text": "Activation functions play a crucial role in neural networks, as they are necessary for modeling non-linear input-output mappings. Importantly, different activation functions exhibit distinct behaviors, and the choice of the activation function can significantly influence the performance of the neural network [60]. It is wellknown [61, 72] that, from a theoretical point of view, non-convex and highly oscillatory activation functions offer better approximation power. However, due to their strong non-convexity, they amplify optimization challenges [39]. ", "page_idx": 0}, {"type": "text", "text": "Therefore, as a middle-ground between practice and theory, it has been suggested that a successful activation function should possess the following properties: (1) be differentiable everywhere [16, 54], (2) have non-vanishing gradients [16]; (3) be bounded to improve the training stability [48, 16]; (4) be zero-centered to accelerate convergence [16]; and (5) be efficient and not increase the complexity of the neural network [45]. In the context of graph data, the activation function should arguably also be what we define as graph-adaptive, that is, tailored to the input graph and capable of capturing the unique properties of graph-structured data, such as degree differences or size changes. This adaptivity ensures that the activation function can effectively leverage the structural information present in the graph data, potentially leading to improved performance in graph tasks. ", "page_idx": 0}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/22168a0308dd8069554165bcc6a41747aa65c0d5e372767c310d0374ee47fa4f.jpg", "img_caption": ["Figure 1: Illustration of DIGRAF. Node features $\\mathbf{H}^{(l-1)}$ and adjacency matrix A are fed to a $\\bar{\\mathbf{G}}\\bar{\\mathbf{N}}\\bar{\\mathbf{N}}_{\\mathrm{LAYER}}^{(l)}$ to obtain updated intermediate node features $\\bar{\\mathbf{H}}^{(l)}$ , which are passed to our activation function layer, DIGRAF. First, an additional GNN network $\\mathrm{GNN_{ACT}}$ takes $\\bar{\\mathbf{H}}^{(l)}$ and A as input to determine the activation function parameters ${\\pmb\\theta}^{(l)}$ . These are used to parameterize the transformation $T^{(l)}$ , which operates on $\\bar{\\mathbf{H}}^{(l)}$ to produce the activated node features $\\mathbf{H}^{(l)}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent work in graph learning has investigated the impact of activation functions specifically designed for graphs, such as Iancu et al. [38] that proposes graph-adaptive max and median activation filters, and Zhang et al. [92] that introduces GReLU, which learns piecewise linear activation functions with a graph-adaptive mechanism. Despite the potential demonstrated by these approaches, the proposed activation functions still have predefined fixed structures (max and median functions in Iancu et al. [38] and piecewise linear in Zhang et al. [92]), restricting the flexibility of the activation functions that can be learned. Additionally, in the case of GReLU, the learned activation functions inherit the drawback of points of non-differentiability, which are undesirable according to the properties mentioned above. As a consequence, to the best of our knowledge, none of the existing activation functions prove to be consistently beneficial across different graph datasets and tasks. Therefore, our objective is to design a flexible activation function tailored for graph data, offering consistent performance gains. This activation function should possess many, if not all, of the properties recognized as beneficial for activation functions, with an emphasis on blueprint flexibility, as well as task and input adaptivity. ", "page_idx": 1}, {"type": "text", "text": "Our Approach: DIGRAF. In this paper, we leverage the success of learning diffeomorphisms, particularly through Continuous Piecewise-Affine Based transformations (CPAB) [24, 25], to devise an activation function tailored for graph-structured data. Diffeomorphisms, characterized as bijective, differentiable, and invertible mappings with a differentiable inverse, inherently possess many desirable properties of activation functions, like differentiability, boundedness within the input-output domain, and stability to input perturbations. To augment our activation function with graph-adaptivity, we employ an additional GNN to derive the parameters of the learned diffeomorphism. This integration yields our node permutation equivariant activation function, dubbed DIGRAF \u2013 DIffeomorphism-based GRaph Activation Function, illustrated in Figure 1, that dynamically adapts to different graphs, providing a flexible framework capable of learning activation functions for specific tasks and datasets in an end-to-end manner. This comprehensive set of characteristics positions DIGRAF as a promising approach for designing activation functions for GNNs. ", "page_idx": 1}, {"type": "text", "text": "To evaluate the efficacy of DIGRAF, we conduct an extensive set of experiments on a diverse set of datasets across various tasks, including node classification, graph classification, and regression. Our evaluation compares the performance of DIGRAF with three types of baselines: traditional activation functions, activation functions with trainable parameters, and graph activation functions. Our experimental results demonstrate that DIGRAF repeatedly exhibits better downstream performance than other approaches, reflecting the theoretical understanding and rationale underlying its design and the properties it possesses. Importantly, while existing activation functions offer different behavior in different datasets, DIGRAF maintains consistent performance across diverse experimental evaluations, further highlighting its effectiveness. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. The contributions of this work are summarized as follows: (1) We introduce a learnable graph-adaptive activation function based on flexible and efficient diffeomorphisms \u2013 DIGRAF, which we show to have properties advocated in literature; (2) an analysis of such properties, reasoning about the design choices of our method; and, (3) a comprehensive experimental evaluation of DIGRAF and other activation functions. ", "page_idx": 1}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/98a30de156f434de069ae1688bd973e710e29824109a185bae1219bd615d38b5.jpg", "img_caption": ["Figure 2: Approximation of traditional activation functions using CPAB and Piecewise ReLU with varying segment counts $K\\in\\{1,2,3\\}$ on a closed interval $\\Omega=[-5,5]$ , demonstrating the advantage of utilizing CPAB and its flexibility to model various activation functions. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffeomorphisms in Neural Networks. A bijection mapping function $f:\\mathcal{M}\\to\\mathcal{N}$ , given two differentiable manifolds $\\mathcal{M}$ and $\\mathcal{N}$ , is termed a diffeomorphism if its inverse $f^{-1}:\\bar{\\mathcal{N}}\\to\\mathcal{M}$ is also differentiable. The challenge in learning diffeomorphisms arises from their computational complexity: early research is often based on complicated infinite dimensional spaces [76], and later advancements have turned to Markov Chain Monte Carlo methods, which still suffer from large computational complexity [1, 2, 90]. To address these drawbacks, Freifeld et al. [24, 25] introduced the Continuous Piecewise-Affine Based transformation (CPAB) approach, offering a more pragmatic solution to learning diffeomorphisms by starting from a finite-dimensional space, and allowing for exact diffeomorphism computations in the case of 1D diffeomorphisms \u2013 an essential trait in our case, given that activation functions are 1D functions. CPAB has linear complexity and is parallelizable, which can lead to sub-linear complexity in practice [25]. Originally designed for alignment and regression tasks by learning diffeomorphisms, in recent years, CPAB was found to be effective in addressing numerous applications using neural networks, posing it as a suitable framework for learning transformation. For instance, Detlefsen et al. [15] learns CPAB transformations to improve the flexibility of spatial transformer layers, Martinez et al. [52] combines CPAB with neural networks for temporal alignment, Weber and Freifeld [81] introduces a novel loss function that eliminates the need for CPAB deformation regularization in time-series analysis, and Wang et al. [79] utilizes CPAB to model complex spatial transformation for image animation and motion modeling. ", "page_idx": 2}, {"type": "text", "text": "General-Purpose Activation Functions. In the last decades, the design of activation functions has seen extensive exploration, resulting in the introduction of numerous high-performing approaches, as summarized in Dubey et al. [16], Kunc and Kl\u00e9ma [45]. The focus has gradually shifted from traditional, static activation functions such as ReLU [26], Sigmoid [45], Tanh [35], and ELU [11], to learnable functions. In the landscape of learnable activation functions, the Maxout [29] unit selects the maximum output from learnable linear functions, and PReLU [33] extends ReLU by learning a negative slope. Additionally, the Swish function [66] augments the SiLU function [19], a Sigmoid-weighted linear unit, with a learnable parameter controlling the amount of non-linearity. The recently proposed AdAct [51] learns a weighted combination of several activation functions, and DiTAC [9] learns a diffeomorphic activation function for CNNs. However, these activation functions are not input-adaptive, a desirable property in GNNs. ", "page_idx": 2}, {"type": "text", "text": "Graph Activation Functions. Typically, GNNs are coupled with conventional activation functions [43, 78, 84], which were not originally tailored for graph data, graph tasks, or GNN models. This implies that these activation functions do not inherently adapt to the structure of the input graph, which was found to be an important property in other GNN components, such as graph normalization [21]. Recent works have suggested various approaches to bridge this gap. Early works such as Scardapane et al. [70] propose learning activation functions based on graph kernels, and Iancu et al. [38] introduces Max and Median filters, which operate on local neighborhoods in the graph, thereby offering adaptivity to the input graphs. A notable advancement in graph-adaptive activation functions is GReLU [92], a parametric piecewise affine activation function achieving graph adaptivity by learning parameters through a hyperfunction that takes into account the node features and the connectivity of the graph. While these approaches demonstrate the potential to enhance GNN performance compared to standard activation functions, they are constrained by their blueprint, often relying on piecewise ReLU composition, which can be performance-limiting [41]. Moreover, a fixed blueprint limits flexibility, i.e., the ability to express a variety of functions. As we show in Figure 2, attempts to approximate traditional activation functions such as ELU and Tanh using piecewise ReLU composition with different segment counts $[K=1$ , 2, and 3), reveal limited approximation power. On the contrary, our DIGRAF, which leverages CPAB, yields significantly better approximations. Furthermore, we demonstrate the approximation power of activations learned with the CPAB framework in our DIGRAF in Appendix E.1. ", "page_idx": 2}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/a0cb176d131c1f1a7b2f6159bcd7f5f7f243178dadae3253792448091709ad32.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: An example of CPA velocity fields $v^{\\theta}$ defined on the interval $\\Omega=[-5,5]$ with a tessellation $\\mathcal{P}$ consisting of five subintervals. The three different parameters, $\\theta_{1}$ , $\\theta_{2}$ , and $\\theta_{3}$ define three distinct CPA velocity fields (Figure 3a) resulting in separate CPAB diffeomorphisms $f^{\\theta}(x)$ (Figure 3b). ", "page_idx": 3}, {"type": "text", "text": "3 Mathematical Background and Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we utilize the definitions from CPAB \u2014 a framework for efficiently learning flexible diffeomorphisms [24, 25], alongside basic graph learning notations, to develop activation functions for GNNs. Consequently, this section outlines the essential details needed to understand the foundations of our DIGRAF. ", "page_idx": 3}, {"type": "text", "text": "3.1 CPAB Diffeomorphisms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\Omega=[a,b]\\subset\\mathbb{R}$ be a closed interval, where $a<b$ . We discretize $\\Omega$ using a tessellation $\\mathcal{P}$ with $\\mathcal{N}_{\\mathcal{P}}$ intervals, which, in practice, is oftentimes an equispaced 1D meshgrid with $\\mathcal{N}_{\\mathcal{P}}$ segments [25] (see Appendix $\\mathrm{^C}$ for a formal definition of tessellation). Our goal in this paper is to learn a diffeomorphism $f:\\Omega\\to\\Omega$ that we will use as an activation function. Formally, a diffeomorphism is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Diffeomorphism on a closed interval $\\Omega$ ). A diffeomorphism on a closed interval $\\Omega\\subset\\mathbb{R}$ is any function $f:\\Omega\\to\\Omega$ that is (1) bijective, (2) differentiable, and (3) has a differentiable inverse $f^{-1}$ . ", "page_idx": 3}, {"type": "text", "text": "To instantiate a CPAB diffeomorphism $f$ , we define a continuous piecewise-affine (CPA) velocity field $v^{\\theta}$ parameterized by $\\pmb{\\theta}\\in\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ . We display examples of velocity fields $v^{\\theta}$ for various instances of $\\pmb{\\theta}$ in Figure 3a to demonstrate the distinct influence of $\\pmb{\\theta}$ on $v^{\\theta}$ . Formally, a velocity field $v^{\\theta}$ is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (CPA velocity field $v^{\\theta}$ on $\\Omega$ ). Given a tessellation $\\mathcal{P}$ with $\\mathcal{N}_{\\mathcal{P}}$ intervals on a closed domain $\\Omega$ , any velocity field $v^{\\theta}:\\Omega\\to\\mathbf{\\dot{R}}$ is termed continuous and piecewise-affine if (1) $v^{\\theta}$ is continuous, and (2) $v^{\\theta}$ is an affine transformation on each interval of $\\mathcal{P}$ . ", "page_idx": 3}, {"type": "text", "text": "The CPA velocity field $v^{\\theta}$ defines a differentiable trajectory $\\phi^{\\pmb\\theta}(x,t):\\Omega\\times\\mathbb R\\rightarrow\\Omega$ for each $x\\in\\Omega$ . The trajectories are computed by integrating the velocity field $v^{\\theta}$ to time $t$ , and are used to construct the CPAB diffeomorphism. We visualize the resulting diffeomorphism in Figure 3b with matching colors denoting corresponding pairs of $v^{\\theta}$ and $f^{\\theta}(x)$ . Mathematically, ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3 (CPAB Diffeomorphism). Given a CPA velocity field $v^{\\theta}$ , the CPAB diffeomorphism $f$ at point $x$ , is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf^{\\theta}(x)\\triangleq\\phi^{\\theta}(x,t=1)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "such that $\\phi^{\\theta}(x,t=1)$ solves the integral equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi^{\\pmb\\theta}(x,t)=x+\\intop_{0}^{t}v^{\\pmb\\theta}\\bigl(\\phi^{\\pmb\\theta}(x,\\tau)\\bigr)\\,d\\tau.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In arbitrary dimensions, computing Definition 3.3 required using an ordinary differential equation solver and can be expensive. However, for 1D diffeomorphisms, as in our DIGRAF, there are closed-form solutions to the CPAB diffeomorphism and its gradients [25], offering an efficient framework for learning activation functions. ", "page_idx": 3}, {"type": "text", "text": "3.2 Graph Learning Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a graph $G=(V,E)$ with $N\\in\\mathbb{N}$ nodes, where $V=\\{1,\\ldots,N\\}$ is the set of nodes and $E\\subseteq V\\times V$ is the set of edges. Let $\\mathbf{A}\\in\\{0,1\\}^{N\\times N}$ be the adjacency matrix of $G$ , and $\\mathbf{X}\\in\\mathbb{R}^{N\\times F}$ the node feature matrix, ", "page_idx": 3}, {"type": "text", "text": "where $F$ is the number of input features. We denote the feature vector of node $v\\in V$ as $\\mathbf{x}_{v}\\,\\in\\,\\mathbb{R}^{F}$ , which corresponds to the $v$ -th row of $\\mathbf{X}$ . The input node features $\\mathbf{X}$ are transformed into the initial node representations $\\mathbf{H}^{(0)}\\in\\mathbb{R}^{N\\times C}$ , using an embedding function emb : $\\mathbb{R}^{F}\\rightarrow\\mathbb{R}^{C}$ to $\\mathbf{X}$ , where $C$ is the hidden dimension, that is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}^{(0)}=\\operatorname{emb}(\\mathbf{X}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The initial features $\\mathbf{H}^{(0)}$ are fed to a GNN comprised of $L\\in\\mathbb N$ layers, where each layer $l\\in\\{1,\\ldots,L\\}$ is followed by an activation function $\\sigma^{(l)}\\big(\\cdot;\\pmb{\\theta}^{(l)}\\big):\\bar{\\mathbb{R}}\\rightarrow\\mathbb{R}$ , and $\\pmb{\\theta}^{(l)}$ is a set of possibly learnable parameters of $\\sigma^{(l)}$ . Specifically, the intermediate output of the $l$ -th GNN layer is denoted as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{H}}^{(l)}=\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\mathrm{LAYER}}^{(l)}(\\mathbf{H}^{(l-1)},\\mathbf{A})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{\\mathbf{H}}^{(l)}\\in\\mathbb{R}^{N\\times C}$ . The activation function $\\sigma^{(l)}$ is then applied element-wise to $\\bar{\\mathbf{H}}^{(l)}$ , yielding node features $h_{u,c}^{(l)}=\\sigma^{(l)}(\\bar{h}_{u,c}^{(l)};\\pmb{\\theta}^{(l)})\\,\\forall u\\in V,\\forall c\\in[\\$ . Therefore, the application of $\\sigma^{(l)}$ can be equivalently written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}^{(l)}=\\sigma^{(l)}(\\bar{\\mathbf{H}}^{(l)};\\pmb{\\theta}^{(l)}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the following section, we will show how this abstraction is translated to our DIGRAF. ", "page_idx": 4}, {"type": "text", "text": "4 DIGRAF ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formalize our approach, DIGRAF, illustrated in Figure 1, which leverages diffeomorphisms to learn adaptive and flexible graph activation functions. ", "page_idx": 4}, {"type": "text", "text": "4.1 A CPAB Blueprint for Graph Activation Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our approach builds on the highly flexible CPAB framework [24, 25] and extends it by incorporating Graph Neural Networks (GNNs) to enable the learning of adaptive graph activation functions. While the original CPAB framework was designed for grid deformation and alignment tasks, typically in 1D, 2D, or 3D spaces, we propose a novel application of CPAB in the context of learning activation functions, as described below. ", "page_idx": 4}, {"type": "text", "text": "In DIGRAF, we treat a node feature (single channel) as a one-dimensional (1D) point. Given the node features matrix $\\bar{\\mathbf{H}}\\in\\mathbb{R}^{N\\times C}$ , we apply DIGRAF per entry in $\\bar{\\bf H}$ , in accordance with the typical element-wise computation of activation functions. We mention that, while CPAB was originally designed to learn grid deformations, it can be utilized as an activation function blueprint by considering a conceptual shift that we demonstrate in Figure 4. Given an input function (shown in red in the figure), CPAB deforms grid coordinates, i.e., it transforms it along the horizontal axis, as shown in the blue curve. In contrast, DIGRAF transforms the original data points along the vertical axis, resulting in the green curve. This conceptual shift can be seen visually from the arrows showing the different dimensions of transformations. We therefore refer to the vertical transformation of the data as their activations. Formally, we define the transformation function $T^{(l)}$ as the element-wise application of the diffeomorphism $f^{\\theta}$ from Equation (1): ", "page_idx": 4}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/e2e732aae2762088adf4ead8e710754bd8bec98fca51fb78770fdf833c06edad.jpg", "img_caption": ["Figure 4: Different transformation strategies. The input function (red), CPAB transformation (blue), and DIGRAF transformation (green), within $\\Omega=$ $[-5,5]$ using the same $\\pmb{\\theta}$ . While CPAB stretches the input, DIGRAF stretches the output, showcasing the distinctive impact of each approach. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\nT^{(l)}(\\bar{h}_{u,c}^{(l)};\\pmb{\\theta}^{(l)})\\triangleq f^{\\pmb{\\theta}^{(l)}}(\\bar{h}_{u,c}^{(l)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\pmb\\theta}^{(l)}$ denotes learnable parameters of the transformation function $T^{(l)}$ , that parameterize the underlying CPA velocity field as discussed in Section 3. In Section 4.2 , we discuss the learning of ${\\pmb\\theta}^{(l)}$ in DIGRAF. ", "page_idx": 4}, {"type": "text", "text": "The transformation $T^{(l)}:\\Omega\\to\\Omega$ described in Equation (6) is based on CPAB and therefore takes as input values within a domain $\\Omega=[a,b]$ , and outputs a value within that domain, where $a<b$ are hyperparameters. In practice, we take $a=-b$ , such that the activation function can be symmetric and centered around 0, a property known to be desirable for activation functions [16]. For any entry in the intermediate node features $\\bar{\\mathbf{H}}^{(\\bar{l})}$ (Equation (4)) that is outside the domain $\\Omega$ , we use the identity function. Therefore, a DIGRAF activation function reads: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{DIGRAF}(\\bar{h}_{u,c}^{(l)},\\pmb{\\theta}^{(l)})=\\left\\{\\frac{T^{(l)}(\\bar{h}_{u,c}^{(l)};\\pmb{\\theta}^{(l)}),}{\\bar{h}_{u,c}^{(l)},},\\right.\\mathrm{~If~}\\bar{h}_{u,c}^{(l)}\\in\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, DIGRAF is applied element-wise in parallel over all entries, and we use the following notation, which yields the output features post the activation of the $l$ -th GNN layer: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}^{(l)}=\\mathsf{D I G R A F}(\\bar{\\mathbf{H}}^{(l)},\\pmb{\\theta}^{(l)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Learning Diffeomorphic Velocity Fields ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "DIGRAF, defined in Equation (7), introduces graph-adaptivity into the transformation function $T^{(l)}$ by employing an additional GNN, denoted as $\\mathrm{GNN_{ACT}}$ , that returns the diffeomorphism parameters ${\\pmb\\theta}^{(l)}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{(l)}(\\bar{\\mathbf{H}}^{(l)},\\mathbf{A})=\\mathrm{PooL}\\left(\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\mathrm{acr}}(\\bar{\\mathbf{H}}^{(l)},\\mathbf{A})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where POOL is a graph-wise pooling operation, such as max or mean pooling. The resulting vector $\\pmb{\\theta}^{(l)}\\in$ $\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ , which is dependent on the tessellation size $\\mathcal{N P}$ , is then used to compute the output of the $l$ -th layer, $\\mathbf{H}^{(l)}$ , as described in Equation (8). We note that Equation (9) yields a different ${\\pmb\\theta}^{(l)}$ for every input graph and features pair $(\\bar{\\mathbf{H}}^{(l)},\\mathbf{A})$ , which implies the graph-adaptivity of DIGRAF. Furthermore, since $\\mathrm{GNN_{ACT}}$ is trained with the other network parameters in an end-to-end fashion, DIGRAF is also adaptive to the task of interest. In Appendix B, we provide and discuss the implementation details of $\\mathrm{GNN_{ACT}}$ and POOL. ", "page_idx": 5}, {"type": "text", "text": "Variants of DIGRAF. Equation (9) describes an approach to introduce graph-adaptivity to ${\\pmb\\theta}^{(l)}$ using $\\mathrm{GNN_{ACT}}$ . An alternative approach is to directly optimize the parameters $\\pmb{\\theta}^{(l)}\\bar{\\in}\\,\\mathbf{\\dot{R}}^{\\mathcal{N}_{\\mathcal{P}}-\\mathbf{\\hat{1}}}$ , without using an additional GNN. Note that in this case, input and graph-adaptivity are sacrificed in favor of a computationally lighter solution. We denote this variant of our method by DIGRAF (W/O ADAP.). Considering this variant is important because it allows us to: (i) offer a middle-ground solution in terms of computational effort, and (ii) it allows us to directly quantify the contribution of graph-adaptivity in DIGRAF. In Section 5, we compare the performance of the methods. ", "page_idx": 5}, {"type": "text", "text": "Velocity Field Regularization. To ensure the smoothness of the velocity field, which will encourage training stability [81], we incorporate a regularization term in the learning procedure of ${\\pmb\\theta}^{(l)}$ . Namely, we follow the Gaussian smoothness prior on the CPA velocity field from Freifeld et al. [24], which was shown to be effective in maintaining smooth transformations. The regularization term is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\{\\pmb{\\theta}^{(l)}\\}_{l=1}^{L})=\\sum_{l=1}^{L}\\pmb{\\theta}^{(l)^{\\top}}\\Sigma_{\\mathrm{CPA}}^{-1}\\pmb{\\theta}^{(l)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Sigma_{\\mathrm{CPA}}$ represents the covariance of a zero-mean Gaussian smoothness prior defined as in Freifeld et al. [24]. We further maintain the boundedness of ${\\pmb\\theta}^{(l)}$ by employing a hyperbolic tangent function (Tanh). In this way, ${\\pmb\\theta}^{(l)}$ remains in $[-1,1]$ when applied in $T^{(l)}$ in Equation (7), ensuring that the velocity field parameters remain bounded, encouraging the overall training stability of the model. ", "page_idx": 5}, {"type": "text", "text": "4.3 Properties of DIGRAF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we focus on understanding the theoretical properties of DIGRAF, highlighting the compelling attributes that establish it as a performant activation function for GNNs. ", "page_idx": 5}, {"type": "text", "text": "DIGRAF yields differentiable activations. By construction, DIGRAF learns a diffeomorphism, which is differentiable by definition. Being differentiable everywhere is considered beneficial as it allows for smooth weight updates during backpropagation, preventing the zigzagging effect in the optimization process [77]. ", "page_idx": 5}, {"type": "text", "text": "DIGRAF is bounded within the input-output domain $\\Omega$ . We point out in Remark D.3 that the diffeomorphism $T^{(l)}(\\cdot;\\pmb\\theta^{(l)})$ is a $\\Omega\\rightarrow\\Omega$ transformation. Any diffeomorphism is continuous, and by the extreme value theorem, $T^{(l)}(\\cdot;\\pmb\\theta^{(l)})$ is bounded in $\\Omega$ . This prevents the activation values from becoming excessively large, a property linked to faster convergence [16]. ", "page_idx": 5}, {"type": "text", "text": "DIGRAF can learn to be zero-centered. Benefiting from its flexibility, DIGRAF has the capacity to learn activation functions that are inherently zero-centered. As an input-adaptive activation function governed by a parameters vector ${\\pmb\\theta}^{(l)}$ , DIGRAF can be adjusted through ${\\pmb\\theta}^{(l)}$ to maintain a zero-centered nature. This property is associated with accelerated convergence in neural network training [16]. ", "page_idx": 5}, {"type": "text", "text": "DIGRAF is efficient. DIGRAF exhibits linear computational complexity, and can further achieve sub-linear running times via parallelization in practice [25]. Moreover, with the existence of a closed-form solution for $\\boldsymbol{f}^{\\theta^{(l)}}$ and its gradient in the 1D case [24], the computations of CPAB can be done efficiently. Additionally, the measured runtimes, detailed in Appendix $_\\mathrm{H}$ , underscore the complexity comparability of DIGRAF with other graph activation functions. ", "page_idx": 5}, {"type": "text", "text": "In addition to the above properties, which follow from our design choice of learning diffeomorphisms through the CPAB framework, we briefly present the following properties, which are formalized and proven in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "DIGRAF is permutation equivariant. We demonstrate in Proposition D.4 that DIGRAF exhibits permutation equivariance to node numbering, ensuring that its behavior remains consistent regardless of the ordering of the graph nodes, which is a key desired property in designing GNN components [8]. ", "page_idx": 6}, {"type": "text", "text": "DIGRAF is Lipschitz continuous. We show in Proposition D.2 that DIGRAF is Lipschitz continuous and derive its Lipschitz constant. Since it is also bounded, we can combine the two results, which leads us to the following proposition: ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1 (The boundedness of $T(\\cdot;\\pmb\\theta^{(l)})$ in DIGRAF). Given a bounded domain $\\Omega=[a,b]\\subset\\mathbb{R}$ where $a<b$ , and any two arbitrary points $x,y\\,\\in\\,\\Omega,$ , the maximal difference of a diffeomorphism $T(\\cdot;\\pmb\\theta^{(l)})$ with parameter ${\\pmb\\theta}^{(l)}$ in DIGRAF is bounded as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n|T(x;\\pmb{\\theta}^{(l)})-T(y;\\pmb{\\theta}^{(l)})|\\leq\\operatorname*{min}(|b-a|,|x-y|\\exp(C_{v^{\\theta}}(l)\\,))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where Cv\u03b8(l) is the Lipschitz constant of the CPA velocity field v\u03b8(l). ", "page_idx": 6}, {"type": "text", "text": "DIGRAF extends commonly used activation functions. CPAB [24, 25], which is used as a framework to learn the diffeomorphism in DIGRAF, is capable of learning and representing a wide range of diffeomorphic functions. When used as an activation function, the transformation $T^{(l)}(\\cdot;\\pmb\\theta^{(l)})$ in DIGRAF adapts to the specific graph and task by learning different ${\\pmb\\theta}^{(l)}$ parameters, rather than having a fixed diffeomorphism. Examples of popular and commonly used diffeomorphisms utilized as activations include Sigmoid, Tanh, Softplus, and ELU, as we show in Appendix D. Extending this approach is our DIGRAF that learns the diffeomorphism during training rather than selecting a pre-defined function. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct an extensive set of experiments to demonstrate the effectiveness of DIGRAF as a graph activation function. Our experiments seek to address the following questions: ", "page_idx": 6}, {"type": "text", "text": "(Q1) Does DIGRAF consistently improve the performance of GNNs compared to existing activation functions on a broad set of downstream tasks?   \n(Q2) To what extent is graph-adaptivity in DIGRAF beneficial when compared to our baseline of DIGRAF (W/O ADAP.) and existing activation functions that lack adaptivity?   \n(Q3) Compared with other graph-adaptive activation functions, how does the added flexibility offered by DIGRAF impact downstream performance?   \n(Q4) How do the considered activation functions compare in terms of training convergence? ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare DIGRAF with three categories of relevant and competitive baselines: (1) Standard Activation Functions, namely Identity, Sigmoid [69], ReLU [26], LeakyReLU [50], Tanh [35], GeLU [34], and ELU [12] to estimate the benefit of learning activation functions parameters; (2) Learnable Activation Functions, specifically PReLU [33], Maxout [29] and Swish [66], to assess the value of graph-adaptivity; and (3) Graph Activation Functions, such as Max [38], Median [38] and GReLU [92], to evaluate the effectiveness of DIGRAF\u2019s design in capturing graph structure and the blueprint flexibility of DIGRAF as discussed in Section 4. ", "page_idx": 6}, {"type": "text", "text": "All baselines are integrated into GCN [43] for node tasks and GIN [84] (GINE [37] where edge features are available) for graph tasks, to ensure fair and meaningful comparisons, isolating the impact of other design choices. We provide additional details on the experimental settings and datasets in Appendix G, as well as additional experiments, including ablation studies, in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "5.1 Node Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our results are summarized in Table 1, where we consider the BLOGCATALOG [86], FLICKR [86], CITESEER [73], CORA [53], and PUBMED [58] datasets. As can be seen from the Table, DIGRAF consistently outperforms all standard activation functions, as well as all the learnable activation functions. Additionally, DIGRAF outperforms other graph-adaptive activation functions. We attribute this positive performance gap to the ability of DIGRAF to learn complex non-linearities due to its diffeomorphism-based blueprint, compared to piecewise linear or pre-defined functions as in other methods. Finally, we compare the performance of DIGRAF and DIGRAF (W/O ADAP.). We remark that in this experiment, we are operating in a transductive setting, as the data consists of a single graph, implying that both DIGRAF and DIGRAF (W/O ADAP.) are adaptive in this case. Still, we see that DIGRAF slightly outperforms the DIGRAF (W/O ADAP.) and we attribute this performance gain to the GNN layers within DIGRAF that are (i) explicitly graph-aware, and (ii) can facilitate the learning of better diffeomorphism parameters ${\\pmb\\theta}^{(l)}$ (Equation (9)) due to the added complexity. ", "page_idx": 6}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/4a939d96a32c58c434107edf46a05dcc527ab84b7aa1465aeddd1518077554e4.jpg", "table_caption": ["Table 1: Comparison of node classification accuracy $(\\%)\\uparrow$ on different datasets using various baselines with DIGRAF. The top three methods are marked by First, Second, Third. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Graph Classification and Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "ZINC-12K. In Table 2 we present results on the ZINC-12K [75, 31, 18] dataset for the regression of constrained solubility of molecules. We note that DIGRAF achieves an MAE of 0.1302, surpassing the best-performing activation on this dataset, Maxout, by 0.0285, which translates to a relative improvement of $\\sim18\\%$ . ", "page_idx": 7}, {"type": "text", "text": "OGB. We evaluate DIGRAF on 4 datasets from the OGB benchmark [36], namely, MOLESOL, MOLTOX21, MOLBACE, and MOLHIV. The results are reported in Table 3, where it is noted that DIGRAF achieves significant improvements compared to standard, learnable, and graph-adaptive activation functions. For instance, DIGRAF obtains a ROC-AUC score of $80.28\\%$ on MOLHIV, an absolute improvement of $4.7\\%$ over the best performing activation function (ReLU). ", "page_idx": 7}, {"type": "text", "text": "TUDatasets.. In addition to the aforementioned datasets, we evaluate DIGRAF on the popular TUDatasets [56]. We present results on MUTAG, PTC, PROTEINS, NCI1 and NCI109 in Table 5 in Appendix E. The results show that DIGRAF is always within the top-three performing activations across all datasets. As an example, on PROTEINS dataset, we see an absolute improvement of $1.1\\%$ over the best-performing activation functions (Maxout and GReLU). ", "page_idx": 7}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/dae76294a484b75dd296153e3b0254cba6e876cf181d8b5aeef7e44c455e352c.jpg", "table_caption": ["Table 2: Comparison on ZINC-12K under the $500\\mathrm{K}$ parameter budget. The top three methods are First, Second, Third. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Convergence Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Besides improved downstream performance, another important aspect of activation functions is their contribution to training convergence [16]. We therefore present the training curves of DIGRAF as well as the rest of the considered baselines to gain insights into their training convergence. Results for representative datasets are presented in Figure 5, where DIGRAF achieves similar or better training convergence than other methods, while also demonstrating better generalization abilities due to its better performance. ", "page_idx": 7}, {"type": "text", "text": "5.4 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our extensive experiments span across 15 different datasets and benchmarks, consisting of both node- and graph-level tasks. Our key takeaways are as follows: ", "page_idx": 7}, {"type": "text", "text": "Table 3: A comparison of DIGRAF to natural baselines, standard, and graph activation layers on OGB datasets, demonstrating the advantage of our approach. The top three methods are marked by First, Second, Third. ", "page_idx": 8}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/df8f69b0f22980f8186bbc299e8dc75406590bc1b920d6ad42375ee377898f41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/1a10ab9a5b917d346f911f4beeb640a3c1f421e58e1d84591aa22613ca189c48.jpg", "img_caption": ["Figure 5: Convergence analysis of DIGRAF compared to baseline activation functions. The plot illustrates the training loss over epochs, showcasing the overall faster convergence of DIGRAF. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "(A1) Overall Performance of DIGRAF: The performance offered by DIGRAF is consistent and on par with or better than other activation functions, across all datasets. These results establish DIGRAF as a highly effective approach for learning graph activation functions.   \n(A2) Benefit of Graph-Adaptivity: DIGRAF outperforms the learnable (although not graph-adaptive) activation functions such as PReLU, Maxout, and Swish, as well as our non-graph adaptive baseline DIGRAF (W/O ADAP.), on all considered datasets. This observation highlights the crucial role of graph-adaptivity in activation functions for GNNs.   \n(A3) The Benefit of Blueprint Flexibility: DIGRAF consistently outperforms other graph-adaptive activation functions like Max, Median, and GReLU. We tie this positive performance gap to the ability of DIGRAF to model complex non-linearities due to its diffeomorphism-based blueprint, compared to piecewise linear or pre-defined functions as in other methods.   \n(A4) Convergence of DIGRAF: As shown in Section 5.3, in addition to overall better downstream performance, DIGRAF allows to achieve better training convergence. ", "page_idx": 8}, {"type": "text", "text": "In summary, compared with 12 well-known activation functions used in GNNs, and across multiple datasets and benchmarks, DIGRAF demonstrates a superior, learnable, flexible, and versatile graph-adaptive activation function, highlighting it as a strong approach for designing and learning graph activation functions. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we introduced DIGRAF, a novel activation function designed for graph-structured data. Our approach leverages Continuous Piecewise-Affine Based (CPAB) transformations to integrate a graph-adaptive mechanism, allowing DIGRAF to adapt to the unique structural features of input graphs. We show that DIGRAF exhibits several desirable properties for an activation function, including differentiability, boundedness within a defined interval, and computational efficiency. Furthermore, we demonstrated that DIGRAF maintains stability under input perturbations and is permutation equivariant, therefore suitable for graph-based applications. Our extensive experiments on diverse datasets and tasks demonstrate that DIGRAF consistently outperforms traditional, learnable, and existing graph-specific activation functions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations and Broader Impact. While DIGRAF demonstrates consistent superior performance compared to existing activation functions, there remain areas for potential improvement. For instance, the current formulation is limited to learning activation functions that belong to the class of diffeomorphisms, which, despite encompassing a wide range of functions, might not be optimal. By improving the performance on real-world tasks like molecule property prediction, and offering faster training convergence, we envision a positive societal impact by DIGRAF in drug discovery and in achieving a lower carbon footprint. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "BR acknowledges support from the National Science Foundation (NSF) awards, CCF-1918483, CAREER IIS-1943364 and CNS-2212160, Amazon Research Award, AnalytiXIN, and the Wabash Heartland Innovation Network (WHIN), Ford, NVidia, CISCO, and Amazon. Computing infrastructure was supported in part by CNS-1925001 (CloudBank). This work was supported in part by AMD under the AMD HPC Fund program. ME is funded by the Blavatnik-Cambridge fellowship, the Cambridge Accelerate Programme for Scientific Discovery, and the Maths4DL EPSRC Programme. The authors thank Shahaf Finder, Ron Shapira-Weber, and Oren Freifeld for the discussions on CPAB. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] St\u00e9phanie Allassonni\u00e8re, Estelle Kuhn, and Alain Trouv\u00e9. Construction of bayesian deformable models via a stochastic approximation algorithm: a convergence study. Bernoulli, 2010.   \n[2] St\u00e9phanie Allassonni\u00e8re, Stanley Durrleman, and Estelle Kuhn. Bayesian mixed effect atlas estimation with a diffeomorphic deformation model. SIAM Journal on Imaging Sciences, 8(3):1367\u20131395, 2015.   \n[3] Andrea Apicella, Francesco Donnarumma, Francesco Isgr\u00f2, and Roberto Prevete. A survey on modern trainable activation functions. Neural Networks, 138:14\u201332, 2021.   \n[4] Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, and Haggai Maron. Efficient subgraph gnns by learning effective selection policies. arXiv preprint arXiv:2310.20082, 2023.   \n[5] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural networks for graph pooling. In International conference on machine learning, pages 874\u2013883. PMLR, 2020.   \n[6] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com.   \n[7] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022.   \n[8] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u02c7ckovi\u00b4c. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.   \n[9] Irit Chelly, Shahaf E Finder, Shira Ifergane, and Oren Freifeld. Trainable highly-expressive activation functions. In European Conference on Computer Vision, 2024.   \n[10] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International conference on machine learning, pages 1725\u20131735. PMLR, 2020.   \n[11] Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.   \n[12] Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus), 2016.   \n[13] Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova. Nonlinear approximation and (deep) relu networks. Constructive Approximation, 55(1):127\u2013172, 2022.   \n[14] Tim De Ryck, Samuel Lanthaler, and Siddhartha Mishra. On the approximation of functions by tanh neural networks. Neural Networks, 143:732\u2013750, 2021.   \n[15] Nicki Skafte Detlefsen, Oren Freifeld, and S\u00f8ren Hauberg. Deep diffeomorphic transformer networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4403\u20134412, 2018.   \n[16] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing, 503:92\u2013108, 2022.   \n[17] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In International Conference on Learning Representations, 2022.   \n[18] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1\u201348, 2023.   \n[19] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3\u201311, 2018.   \n[20] Moshe Eliasof, Fabrizio Frasca, Beatrice Bevilacqua, Eran Treister, Gal Chechik, and Haggai Maron. Graph positional encoding via random feature propagation. In International Conference on Machine Learning, pages 9202\u20139223. PMLR, 2023.   \n[21] Moshe Eliasof, Beatrice Bevilacqua, Carola-Bibiane Sch\u00f6nlieb, and Haggai Maron. Granola: Adaptive normalization for graph neural networks. arXiv preprint arXiv:2404.13344, 2024.   \n[22] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019.   \n[23] Fabrizio Frasca, Beatrice Bevilacqua, Michael M Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. In Advances in Neural Information Processing Systems, 2022.   \n[24] Oren Freifeld, Soren Hauberg, Kayhan Batmanghelich, and John W Fisher. Highly-expressive spaces of well-behaved transformations: Keeping it simple. In Proceedings of the IEEE International Conference on Computer Vision, pages 2911\u20132919, 2015.   \n[25] Oren Freifeld, Soren Hauberg, Kayhan Batmanghelich, and Jonn W Fisher. Transformations based on continuous piecewise-affine velocity fields. IEEE transactions on pattern analysis and machine intelligence, 39(12):2496\u20132509, 2017.   \n[26] Kunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold elements. IEEE Transactions on Systems Science and Cybernetics, 5(4):322\u2013333, 1969.   \n[27] Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning, pages 2083\u20132092. PMLR, 2019.   \n[28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \n[29] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1319\u20131327, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.   \n[30] Thomas Hakon Gronwall. Note on the derivatives with respect to a parameter of the solutions of a system of differential equations. Annals of Mathematics, pages 292\u2013296, 1919.   \n[31] Rafael G\u00f3mez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268\u2013276, January 2018. ISSN 2374-7951. doi: 10.1021/acscentsci.7b00572.   \n[32] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse problems, 34(1): 014004, 2017.   \n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.   \n[34] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.   \n[35] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.   \n[36] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.   \n[37] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, 2020.   \n[38] Bianca Iancu, Luana Ruiz, Alejandro Ribeiro, and Elvin Isuf.i Graph-adaptive activation functions for graph neural networks. In 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP), pages 1\u20136. IEEE, 2020.   \n[39] Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning. Foundations and Trends\u00ae in Machine Learning, 10(3-4):142\u2013363, 2017.   \n[40] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[41] Sammy Khalife and Amitabh Basu. On the power of graph neural networks and the role of the activation function, 2023.   \n[42] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021.   \n[43] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.   \n[44] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34: 21618\u201321629, 2021.   \n[45] Vladim\u00edr Kunc and Ji\u02c7r\u00ed Kl\u00e9ma. Three decades of activations: A comprehensive survey of 400 activation functions for neural networks. arXiv preprint arXiv:2402.09092, 2024.   \n[46] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International conference on machine learning, pages 3734\u20133743. PMLR, 2019.   \n[47] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[48] Shan Sung Liew, Mohamed Khalil-Hani, and Rabia Bakhteri. Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems. Neurocomputing, 216: 718\u2013734, 2016. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2016.08.037.   \n[49] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Solja\u02c7ci\u00b4c, Thomas Y. Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks, 2024.   \n[50] Andrew L. Maas. Rectifier nonlinearities improve neural network acoustic models. In International conference on machine learning, 2013.   \n[51] Ritabrata Maiti. Adact: Learning to optimize activation function choice through adaptive activation modules. In The Second Tiny Papers Track at ICLR 2024, 2024.   \n[52] I nigo Martinez, Elisabeth Viles, and Igor G Olaizola. Closed-form diffeomorphic transformations for time series alignment. In International Conference on Machine Learning, pages 15122\u201315158. PMLR, 2022.   \n[53] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127\u2013163, 2000.   \n[54] Akash Mishra, Pravin Chandra, and Udayan Ghose. A non-monotonic activation function for neural networks validated on benchmark tasks. In Modern Approaches in Machine Learning and Cognitive Science: A Walkthrough: Latest Trends in AI, Volume 2, pages 319\u2013327. Springer, 2021.   \n[55] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 4602\u20134609, 2019.   \n[56] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs, 2020.   \n[57] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M. Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. Journal of Machine Learning Research, 24(333):1\u201359, 2023.   \n[58] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In 10th international workshop on mining and learning with graphs, volume 8, page 1, 2012.   \n[59] Emmanuel Noutahi, Dominique Beaini, Julien Horwood, S\u00e9bastien Gigu\u00e8re, and Prudencio Tossou. Towards interpretable sparse graph representation learning with laplacian pooling. arXiv preprint arXiv:1905.11577, 2019.   \n[60] Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, and Stephen Marshall. Activation functions: Comparison of trends in practice and research for deep learning. arXiv preprint arXiv:1811.03378, 2018.   \n[61] Joost AA Opschoor, Philipp C Petersen, and Christoph Schwab. Deep relu networks and high-order finite element methods. Analysis and Applications, 18(05):715\u2013770, 2020.   \n[62] Abhishek Panigrahi, Abhishek Shetty, and Navin Goyal. Effect of activation functions on the training of overparametrized neural nets. arXiv preprint arXiv:1908.05660, 2019.   \n[63] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \n[64] Ilan Price, Nicholas Daultry Ball, Samuel CH Lam, Adam C Jones, and Jared Tanner. Deep neural network initialization with sparsity inducing activations. arXiv e-prints, pages arXiv\u20132402, 2024.   \n[65] Omri Puny, Derek Lim, Bobak Kiani, Haggai Maron, and Yaron Lipman. Equivariant polynomials for graph neural networks. In International Conference on Machine Learning, pages 28191\u201328222. PMLR, 2023.   \n[66] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.   \n[67] Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501\u201314515, 2022.   \n[68] Patrick Reiser, Marlen Neubert, Andr\u00e9 Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, et al. Graph neural networks for materials science and chemistry. Communications Materials, 3(1):93, 2022.   \n[69] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533\u2013536, 1986.   \n[70] Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, and Aurelio Uncini. Improving graph convolutional networks with non-parametric activation functions. In 2018 26th European Signal Processing Conference (EUSIPCO), pages 872\u2013876. IEEE, 2018.   \n[71] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.   \n[72] Christoph Schwab, Andreas Stein, and Jakob Zech. Deep operator network approximation rates for lipschitz operators. arXiv preprint arXiv:2307.09835, 2023.   \n[73] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[74] Ron A Shapira Weber, Matan Eyal, Nicki Skafte, Oren Shriki, and Oren Freifeld. Diffeomorphic temporal alignment nets. Advances in Neural Information Processing Systems, 32, 2019.   \n[75] Teague Sterling and John J. Irwin. ZINC 15 \u2013 ligand discovery for everyone. Journal of Chemical Information and Modeling, 55(11):2324\u20132337, 11 2015. doi: 10.1021/acs.jcim.5b00559.   \n[76] Ganesh Sundaramoorthi and Anthony Yezzi. Variational pdes for acceleration on manifolds and application to diffeomorphisms. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[77] Tomasz Szanda\u0142a. Review and comparison of commonly used activation functions for deep neural networks. Bio-inspired neurocomputing, pages 203\u2013224, 2021.   \n[78] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[79] Hexiang Wang, Fengqi Liu, Qianyu Zhou, Ran Yi, Xin Tan, and Lizhuang Ma. Continuous piecewise-affine based motion model for image animation. arXiv preprint arXiv:2401.09146, 2024.   \n[80] Yu Guang Wang, Ming Li, Zheng Ma, Guido Montufar, Xiaosheng Zhuang, and Yanan Fan. Haar graph pooling. In International conference on machine learning, pages 9952\u20139962. PMLR, 2020.   \n[81] Ron Shapira Weber and Oren Freifeld. Regularization-free diffeomorphic temporal alignment nets. In International Conference on Machine Learning, pages 30794\u201330826. PMLR, 2023.   \n[82] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems: a survey. ACM Computing Surveys, 55(5):1\u201337, 2022.   \n[83] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network, 2015.   \n[84] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.   \n[85] Zhen Xu, Xiaojin Zhang, and Qiang Yang. Tafs: Task-aware activation function search for graph neural networks. 2023.   \n[86] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Sourav S. Bhowmick, and Juncheng Liu. Pane: scalable and effective attributed network embedding. The VLDB Journal, 32(6):1237\u20131262, March 2023. ISSN 0949-877X. doi: 10.1007/s00778-023-00790-4.   \n[87] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018.   \n[88] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. In International Conference on Machine Learning, 2023.   \n[89] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. arXiv preprint arXiv:2301.09505, 2023.   \n[90] Miaomiao Zhang and P Thomas Fletcher. Bayesian statistical shape analysis on the manifold of diffeomorphisms. Algorithmic Advances in Riemannian Geometry and Applications: For Machine Learning, Computer Vision, Statistics, and Optimization, pages 1\u201323, 2016.   \n[91] Xiao-Meng Zhang, Li Liang, Lin Liu, and Ming-Jing Tang. Graph neural networks and their current applications in bioinformatics. Frontiers in genetics, 12:690049, 2021.   \n[92] Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 1331\u20131339, 2022.   \n[93] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Graph Neural Networks. Graph Neural Networks [71] (GNNs) have emerged as a transformative approach in machine learning, notably following the popularity of the message-passing scheme [28]. GNNs enable effective learning from graph-structured data, and can be applied to different tasks, ranging from social network analysis [43] to bioinformatics [40]. In recent years, various GNN architectures were proposed, aiming to address various aspects, from alleviating oversmoothing [10], concerning attention mechanisms in the message passing scheme [78, 7, 42], or focusing on the expressive power of the architectures [57, 23, 88, 89, 65, 4], given that message-passing based architectures are known to be bounded by the WL graph isomorphism test [84, 55]. ", "page_idx": 14}, {"type": "text", "text": "Despite advancements, the poor performance of deep GNNs has led to a preference for shallow architectures GCNs [47]. To enhance performance, techniques such as pooling functions have been proposed, introducing generalization by reducing feature map sizes [93]. Methods such as HGP-SL [93], GraphUNet [27], and LaPool [59] introduce pooling layers specifically designed for GNNs. Beyond node feature, the importance of graph structure and positional features is increasingly recognized, with advancements such as GraphGPS [67] and SAN [44] integrating positional and structural encodings through attention-based mechanisms. ", "page_idx": 14}, {"type": "text", "text": "Evaluation of Rectified Activation Functions. Rectified activation functions, represented by the Rectified Linear Unit (ReLU), have been widely applied and studied in various neural network architectures due to their simplicity and effectiveness [60, 3, 45]. The prevalent assumption that ReLU\u2019s performance is predominantly due to its sparsity is critically examined by $\\mathrm{Xu}$ et al. [83], suggesting introducing a non-zero slope in the negative part can significantly enhance network performance. Extending this, Price et al. [64] investigates sparsity-inducing activation functions, such as the shifted ReLU, in network initialization and early stages of training. These functions can mitigate overftiting and boost model generalization capabilities. Conversely, it was shown that in overparameterized networks, smoother activation functions, like Tanh and Swish, can enhance the convergence rate, in contrast to the non-smooth characteristics of ReLU [62]. However, the fixed nature of ReLU and many of its variants restricts their ability to adapt the input, resulting in limited power to capture dynamics in learning. ", "page_idx": 14}, {"type": "text", "text": "Advancements in Learnable Activation Functions. Recent research has increasingly focused on adaptive and learnable activation functions, which are optimized alongside the learning process of the network. The AdAct framework [51] introduces learnability by combining multiple activation functions into a single module with learnable weighting coefficients. However, these coefficients are fixed after training, limiting the framework\u2019s adaptability to varying inputs. A concurrent work by Liu et al. [49] introduces KolmogorovArnold Networks (KAN), a novel architecture that diverges from traditional Multi-Layer Perceptron (MLP) configurations, which applies activation functions to network edges instead of nodes. Unlike our current work, which focuses only on the design of activation functions for GNNs, their research extends beyond this scope and considers a fundamental architecture design. Finally, the recently proposed TAFS [85] learns a task-adaptive (but not graph-adaptive) activation function for GNNs through a bi-level optimization. ", "page_idx": 14}, {"type": "text", "text": "B Implementation Details of DIGRAF ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Multiple Graphs in one Batch. Consider a set of graphs $S=\\{G_{1},G_{2},\\cdot\\cdot\\cdot,G_{B}\\}$ with a batch size of $B$ . Let $N_{S}=N_{1}+N_{2}+\\cdot\\cdot+N_{B}$ represent the cumulative number of nodes across the graph dataset. The term $N_{\\operatorname*{max}}\\triangleq\\operatorname*{max}(N_{1},N_{2},\\cdot\\cdot\\cdot,N_{B})$ denotes the largest node count present in any single graph within $S$ . ", "page_idx": 14}, {"type": "text", "text": "To create a unified feature matrix for $S$ that encompasses all graphs in the batch, we standardize the dimension by padding each feature matrix $\\mathbf{X}_{i}\\in\\mathbb{R}^{N_{i}\\times C}$ , $i\\in[B]$ for graph $G_{i}\\,\\in\\,S$ from $N_{i}$ to $N_{\\mathrm{max}}$ with zeros. The combined feature matrix $\\mathbf{X}_{S}$ is constructed by concatenating the transposed feature matrices $\\mathbf{X}_{i}^{\\top}\\;\\forall i\\in[B]$ , resulting in a matrix that lies in the domain $\\mathbb{R}^{(B\\cdot C)\\times N_{\\operatorname*{max}}}$ . This matrix is permutation invariant; while relabeling nodes changes the row indices, it does not affect the overall transformation process. Therefore, DIGRAF can handle multiple graphs in a batch. In practice, to avoid the overhead of padding, we use the batching support from Pytorch-Geometric [22]. ", "page_idx": 14}, {"type": "text", "text": "Implementation Details of $\\mathbf{GNN_{ACT}}$ . In Section 4.2, we examined two distinct approaches to learn the diffeomorphism parameters ${\\pmb\\theta}^{(l)}$ , either directly or through $\\mathrm{GNN_{ACT}}$ . As shown in Appendix C, ${\\pmb\\theta}^{(l)}$ determines the velocity field $\\boldsymbol{v}^{\\theta^{(l)}}$ . Predicting a graph-dependent ${\\pmb\\theta}^{(l)}$ adds graph-adaptivity to the activation function $T^{(l)}$ . In DIGRAF we achieve this by employing another GNN $\\mathrm{GNN_{ACT}}$ , described below. ", "page_idx": 14}, {"type": "text", "text": "The backbone of $\\mathrm{GNN_{ACT}}$ utilizes the same structure as the primary network layers $\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\mathrm{LAYER}}^{(l)}$ , that is, GCN [43] or GIN [84]. It is important to note, that while $\\mathrm{GNN_{ACT}}$ has a similar structure to the primary network GNN with ReLU activation function, it has its own set of learnable weights, and it is shared among the layers, unlike the primary GNN layers $\\mathrm{GNN}_{\\mathrm{LAYER}}^{(l)}$ . The hidden dimensions and the number of layers of $\\mathrm{GNN_{ACT}}$ are hyperparameters. The weight parameters of $\\mathrm{GNN_{ACT}}$ are trained concurrently with the main network weights. As described in Equation (9), after the computation of $\\mathrm{GNN_{ACT}}$ , a pooling layer denoted by POOL is placed to aggregate node features. This aggregation squashes the node dimension such that the output is not dependent on the specific order of nodes, and it yields the vector of parameters ${\\pmb\\theta}^{(l)}$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Rescaling $\\bar{\\mathbf{H}}^{(l)}$ . Following the implementation of Freifeld et al. [24], the default 1D domain for CPAB is set as [0, 1]. To enhance the flexibility of $T^{(l)}$ and ensure its adaptability across various input datasets, DIGRAF extends the domain to $\\Omega=[a,b]\\subset\\mathbb{R}$ with $a<b$ as shown in Section 3.1. To match the two domains, we rescale the intermediate feature matrix $\\bar{\\mathbf{H}}^{(l)}$ from $\\Omega$ to the unit interval [0, 1] before passing it to $T^{(l)}$ . Let $\\begin{array}{r}{r=\\frac{b-a}{2}}\\end{array}$ , then rescaling is performed using the function $f(x)=(x+r)/(2r)$ . Data points outside this range will retain their original value, effectively acting as an identity function outside the domain $\\Omega$ . ", "page_idx": 15}, {"type": "text", "text": "Training Loss Function. As described in Equation (10), we employ a regularization term for the velocity field to maintain the smoothness of the activation function. To control the strength of regularization, we introduce a hyperparameter $\\lambda$ . We denote $\\mathcal{L}_{\\mathrm{TASK}}$ as the loss function of the downstream task (i.e. cross-entropy loss in case of classification and mean absolute error in case of regression tasks), and the overall training loss of DIGRAF, denoted as $\\mathcal{L}_{\\mathrm{TOTAL}}$ is given as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ToTaL}}=\\mathcal{L}_{\\mathrm{TasK}}+\\lambda\\,\\mathcal{R}(\\{\\pmb{\\theta}^{(l)}\\}_{l=1}^{L}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Overview of CPA Velocity Fields and CPAB Transformations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this Section, we drop the layer notations $l$ for simplicity. In Section 3.1, we introduce the concept of a diffeomorphism on a closed interval in Definition 3.1, which can be learned through the integration of a Continuous Piecewise Affine (CPA) velocity field. As detailed in Definition 3.2, the velocity field $v^{\\theta}$ is governed by the parameter $\\pmb{\\theta}$ and the tessellation $\\mathcal{P}$ . We now discuss how the velocity fields are computed following the methodologies presented by Freifeld et al. [24, 25] and highlight the relations between $v^{\\theta}$ , $\\pmb{\\theta}$ and $\\mathcal{P}$ . We start by formally defining the tessellation on $\\Omega$ : ", "page_idx": 15}, {"type": "text", "text": "Definition C.1 (Tessellation of a closed interval [24]). A tessellation $\\mathcal{P}$ of size $\\mathcal{N}_{\\mathcal{P}}$ subintervals of a closed interval $\\Omega=[a,b]$ in $\\mathbb{R}$ is a partitioning $\\{[x_{i},x_{i+1}]\\}_{i=0}^{\\mathcal{N}_{\\mathcal{P}}-1}$ that satisfies the following properties: ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{0}=a\\;\\mathrm{and}\\;x_{\\mathcal{N}_{\\mathscr{P}}}=b\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(2) Each point $x\\in\\Omega$ lies in at least one subinterval $[x_{i},x_{i+1}]$ ", "page_idx": 15}, {"type": "text", "text": "(3) The intersection of any two subintervals $[x_{i},x_{i+1}]$ and $[x_{i+1},x_{i+2}]$ is exactly $\\{x_{i+1}\\}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bigcup_{i=0}^{\\mathcal{N}_{\\mathcal{P}}-1}[x_{i},x_{i+1}]=\\Omega\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The vector of parameters $\\pmb{\\theta}$ is linked to the subintervals in $\\mathcal{P}$ , whose dimension is determined by the number of intervals $\\mathcal{N P}$ . Similar to Freifeld et al. [24], we impose boundary constraints that mandate the velocity at the boundary of the tessellation to be zero, i.e., $\\ddot{v}^{\\theta}(0)\\stackrel{\\cdot}{=}v^{\\theta}(1)=0$ . This boundary condition allows us to compose the diffeomorphism in the domain $\\Omega$ with an identity function for any values outside the domain. Under this constraint, the degrees of freedom (number of parameters) for $\\theta$ is $\\mathcal{N}_{\\mathcal{P}}-1$ . ", "page_idx": 15}, {"type": "text", "text": "The velocity field is then defined as follows: ", "page_idx": 15}, {"type": "text", "text": "Definition C.2 (Relation between $\\pmb{\\theta}$ and $v^{\\theta}$ , taken from Freifeld et al. [25]). Given a tessellation $\\mathcal{P}$ with $\\mathcal{N}_{\\mathcal{P}}$ intervals on a closed domain $\\Omega=[a,b]$ , as defined in Definition C.1. Given a parameter $\\pmb{\\theta}\\in\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ and an arbitrary point $_x$ within the domain, a continuous piecewise-affine velocity field $v^{\\theta}$ can present as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nv^{\\theta}(x)=\\sum_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}\\theta_{j}\\mathbf{b}_{j}\\tilde{x},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\{\\mathbf{b}_{j}\\}_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}$ is an orthonormal basis of the space of velocity fields $\\nu$ , such that $v^{\\theta}\\in\\mathcal{V}$ , and $\\tilde{\\pmb{x}}=\\left[\\!\\!\\begin{array}{l}{x}\\\\ {1}\\end{array}\\!\\!\\right]$ oTfh .r thNoontoer tmhaalt iiss $\\{\\mathbf{b}_{j}\\}_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}$ ofonrs ttrhaei nvienlgo ctithye  fcieoledf cfiacine bnet so obtfa ienaecdh t hcroonutignhu Soiunsg upliaerc eVwaliusee -aDfefcinoem vpeolsoitciiotny $\\mathbf{L}$ $\\mathbf{L}$ function by ensuring that the velocity value at the shared endpoints is the same [52]. Let $v e c(\\mathbf{A})$ be a column vector containing the coefficients for each interval, for instance, $[a_{0},b_{0},a_{1},b_{1}]^{T}$ for consecutive intervals interval 0 and interval 1. The shared endpoint is $x_{1}$ . To achieve the constrain, we have the equation $a_{0}*x_{1}+b_{0}+a_{1}*(-x_{1})+b_{1}*(-1)=0$ . In this example, the constrain matrix $\\mathbf{L}$ is $\\mathbf{L}=[x_{1},1,-\\Bar{x_{1}},-1]$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/47d367aca8acdaf670c19d1bb385b07e669a7daa78614197e0dbfcb2946b3788.jpg", "table_caption": ["Table 4: A summary of the properties of activation functions. \u2013 means not studied in the corresponding paper. \u2217Median-of-medians algorithm can achieve linear time complexity on average. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "By generalizing the previous example, the constraint can be expressed as $\\mathbf{L}*v e c(\\mathbf{A})=\\overrightarrow{0}$ . As the endpoints are decided by the tessellation setup, we can build the constrain matrix $\\mathbf{L}$ without knowing $v e c(\\mathbf{A})$ . And thus, orthonormal basis {bj}jN=P0 \u2212 can be computed by giving tessellation setup. ", "page_idx": 16}, {"type": "text", "text": "Proposition C.3 (DIGRAF has a closed form solution). Equation 2 can be expressed as an equivalent $O D E$ . By allowing $_x$ to vary and fixing $t$ , the solution to this ODE can be written as a composition of a finite number of solutions $\\psi$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{\\phi}^{\\theta}(\\boldsymbol{x},t)=(\\psi_{\\theta,c_{m}}^{t_{m}}\\circ\\psi_{\\theta,c_{m-1}}^{t_{m-1}}\\circ\\cdots\\circ\\psi_{\\theta,c_{2}}^{t_{2}}\\circ\\psi_{\\theta,c_{1}}^{t_{1}})(\\boldsymbol{x})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here m represents the number of cells visited. Given $x,\\theta$ , time $t$ , and the smallest cell index containing $_x$ , c, we can compute each $\\psi_{\\theta,c_{i}}^{t_{i}}(x),\\dot{i}\\in\\{1,\\dots,m\\}$ from $\\psi_{\\theta,c_{1}}^{t_{1}}(x)$ to $\\psi_{\\boldsymbol{\\theta},c_{m}}^{t_{m}}(\\boldsymbol{x})$ . In other words, DIGRAF has a closed-form solution. ", "page_idx": 16}, {"type": "text", "text": "Proof. The proof follows the steps in Martinez et al. [52]. Equation (2) can be expressed as the equivalent ODE: d\u03d5\u03b8d(tx,t) = v\u03b8(\u03d5\u03b8(x, t)). By allowing x to vary and fixing t, the solution to this ODE can be written as a composition of a finite number of solutions $\\psi$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{\\phi}^{\\theta}(\\boldsymbol{x},t)=(\\psi_{\\theta,c_{m}}^{t_{m}}\\circ\\psi_{\\theta,c_{m-1}}^{t_{m-1}}\\circ\\cdots\\circ\\psi_{\\theta,c_{2}}^{t_{2}}\\circ\\psi_{\\theta,c_{1}}^{t_{1}})(\\boldsymbol{x})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given $x,\\theta$ , time $t$ , and a function $\\gamma$ that returns the smallest cell index containing $x$ , namely $c=\\gamma(x)$ , then we can compute each $\\psi_{\\theta,c_{i}}^{t_{i}}(x),i\\in\\{1,\\ldots,m\\}$ and use them in the closed form solution for the ODE. The cell boundary $x_{c}$ is determined based on the velocity value $v(x)$ at point $_x$ . If $v(x)\\geq0$ , $x_{c}$ is the largest point in the interval; otherwise, it is the smallest point. In this setup, a cell is a 1D interval with two endpoints. At the hitting time $t_{h i t}$ , $\\psi_{c}^{\\theta}(x,t_{h i t})$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi_{c}^{\\theta}(x,t_{h i t})=x_{c},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where t\u03b8hit =a1\u03b8 log aac\u03b8xxc++bb\u03b8c . The CPAB velocity field is continuous piecewise-affine, and for each interval with index $c$ , it has coefficients $a_{c}^{\\theta}$ (slope) and $b_{c}^{\\theta}$ (bias). These can be computed given $\\theta$ . If $t_{h i t}^{\\theta}>t$ , then $\\phi^{\\theta}(x,t)=\\psi_{c}(x,t)$ . Otherwise, we repeat the process with updated values $t=\\dot{t}-t_{h i t}^{\\delta}$ , $x=x_{c}$ , and $c$ adjusted based on the sign of $v(x)$ . ", "page_idx": 16}, {"type": "text", "text": "This iterative process continues until convergence, with an upper bound for $m$ being $m a x(c_{1},N_{P}-c_{1}+1)$ , where $c_{1}$ refers to the first visited cell index, and $N_{P}$ is the number of closed intervals in the space $\\Omega$ . With the above steps, we can precisely compute each $\\psi_{\\theta,c_{i}}^{t_{i}}(x),i\\in\\{1,\\ldots,m\\}$ from $\\psi_{\\theta,c_{1}}^{t_{1}}(x)$ to $\\psi_{\\boldsymbol{\\theta},c_{m}}^{t_{m}}(\\boldsymbol{x})$ following the equation $\\phi^{\\theta}(x,t)=(\\psi_{\\theta,c_{m}}^{t_{m}}\\circ\\cdots\\circ\\psi_{\\theta,c_{1}}^{t_{1}})(x)$ . This allows us to determine the exact solution for $\\phi^{\\theta}(x,t)$ . ", "page_idx": 16}, {"type": "text", "text": "D Properties and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present a summary of the properties offered by our DIGRAF that are absent in general-purpose activation functions or existing graph activations in Table 4. ", "page_idx": 16}, {"type": "text", "text": "Similar to Appendix C, for simplicity, in this Section, we drop the layer notations $l$ . ", "page_idx": 16}, {"type": "text", "text": "In this section, we present the propositions and proofs for the properties outlined in Section 4.3. We begin by remarking that as shown in Section 4.3, DIGRAF is bounded within the domain $\\Omega=[a,b]$ , where $a<b$ by construction. We then present Proposition D.1 that outlines the Lipschitz constant of the velocity field $v^{\\theta}$ , followed by Proposition D.2, showing that DIGRAF is also Lipschitz continuous, and provide an upper bound on its Lipschitz constant. ", "page_idx": 16}, {"type": "text", "text": "Proposition D.1 (The Lipschitz Constant of $v^{\\theta}$ ). Given two arbitrary points $x,y\\,\\in\\,\\mathbb{R},$ , and velocity field parameters $\\pmb{\\theta}\\in\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ that define the continuous piecewise-affine velocity field $v^{\\theta}$ , there exists a Lipschitz constant Cv\u03b8 = jN=P0 \u22122|\u03b8j| such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Big|v^{\\theta}(x)-v^{\\theta}(y)\\Big|\\leq C_{v^{\\theta}}\\|(\\tilde{\\pmb{x}}-\\tilde{\\pmb{y}})\\|_{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\ |\\cdot|\\ a n d\\ ||\\cdot||_{2}$ denote the absolute value of a scalar and the $\\ell_{2}$ norm of a vector, respectively. ", "page_idx": 17}, {"type": "text", "text": "Proof. First, we note that it was shown in Freifeld et al. [24, 25] that $v^{\\theta}$ is Lipschitz continuous, and now we provide a derivation of that Lipschitz constant. Following Definition C.2, the velocity field $v^{\\theta}$ is defined as $\\begin{array}{r}{\\tilde{v}^{\\theta}(x)=\\sum_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}\\pmb{\\theta}_{j}\\mathbf{b}_{j}\\tilde x}\\end{array}$ , where $\\{\\mathbf{b}_{j}\\}_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}$ is an orthonormal basis of the velocity space. By the definition of $v^{\\theta}(x)$ and $v^{\\theta}(y)$ , we have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|v^{\\theta}(x)-v^{\\theta}(y)\\Big|=\\left|\\frac{N v-2}{\\sum_{\\sigma}\\theta_{1}\\theta_{2}\\tilde{u}_{1}\\tilde{u}-\\sum_{\\tau=0}^{N-2}\\theta_{3}\\tilde{u}_{1}\\tilde{y}}\\right|}&{}\\\\ {=\\left|\\frac{N v-(x)-(y)}{\\sum_{\\sigma}\\theta_{3}\\tilde{b}_{2}(\\tilde{u}-\\tilde{y})}\\right|}\\\\ &{\\leq\\displaystyle\\sum_{j=0}^{N-p-2}|\\theta_{j}||\\mathbb{B}_{j}||_{2}|\\!\\!\\right|(\\tilde{x}-\\tilde{y})||_{2}}\\\\ &{=\\displaystyle\\sum_{j=0}^{N-2}|\\theta_{j}||(\\tilde{x}-\\tilde{y})||_{2}}\\\\ &{=\\|(\\tilde{x}-\\tilde{y})\\|_{2}\\sum_{j=0}^{N-2}|\\theta_{j}|}\\\\ &{=C_{\\epsilon}\\|(\\tilde{x}-\\tilde{y})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the transition between Equation (16) and Equation (17) follows from the triangle inequality, and the transition between Equation (17) and Equation (18) follows from ${\\bf b}_{j}$ being an orthonormal vector. ", "page_idx": 17}, {"type": "text", "text": "From the derivation above, and the fact that we know from Freifeld et al. [24, 25] that the velocity field is Lipschitz continuous, we conclude that the Lipschitz constant $C_{v}\\theta$ of $v^{\\theta}$ reads $\\begin{array}{r}{C_{v}\\pmb{\\omega}=\\sum_{j=0}^{\\mathcal{N}_{\\mathcal{P}}-2}|\\pmb{\\theta}_{j}|}\\end{array}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Given the Lipschitz constant $C_{v}\\theta$ for $v^{\\theta}$ , we proceed to demonstrate that the transformation $T(\\cdot;\\theta)$ in DIGRAF is Lipschitz continuous, as well as bounding its Lipschitz constant. ", "page_idx": 17}, {"type": "text", "text": "Proposition D.2 (The Lipschitz Constant of DIGRAF). The diffeomorphic function $T(\\cdot;\\theta)$ in DIGRAF is defined in Equation (6) for a given set of weights $\\pmb{\\theta}$ , which in turn define the velocity field $v^{\\theta}$ . Let $x,y\\in\\mathbb{R}\\,b e$ two arbitrary points, then the following inequality is satisfied: ", "page_idx": 17}, {"type": "equation", "text": "$$\n|T(x;\\pmb\\theta)-T(y;\\pmb\\theta)|\\leq|x-y|\\exp(C_{v}\\pmb\\theta)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{v}\\theta$ is the Lipschitz constant of $v^{\\theta}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We begin by substituting $T(\\cdot;\\theta)$ with Equation (1) and Equation (6). Utilizing Proposition D.1, we then establish an upper bound for $|{\\bar{T}}(x;\\mathbf{\\bar{\\theta}})-T(y;\\mathbf{\\bar{\\theta}})|$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert T(x;\\theta)-T(y;\\theta)\\right\\rvert=\\displaystyle\\left\\lvert x+\\int_{0}^{1}v^{\\theta}\\big(\\phi^{\\theta}(x,\\tau)\\big)\\,d\\tau-y-\\int_{0}^{1}v^{\\theta}\\big(\\phi^{\\theta}(y,\\tau)\\big)\\,d\\tau\\right\\rvert}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\left\\lvert x-y\\right\\rvert+C_{v}\\theta\\displaystyle\\int_{0}^{1}\\Big\\lvert\\big(\\phi^{\\theta}(x,\\tau)-\\phi^{\\theta}(y,\\tau))\\Big\\rvert}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\left\\lvert x-y\\right\\rvert\\exp(C_{v}\\theta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{v}\\theta$ is the Lipschitz constant of $v^{\\theta}$ (Proposition D.1) and the last transition follows from Gr\u00f6nwall\u2019s inequality [30]. Consequently, the Lipschitz constant of DIGRAF is bounded from above by $\\exp(C_{v}\\theta)$ .\u53e3 ", "page_idx": 17}, {"type": "text", "text": "Now that we established that $T(\\cdot;\\theta)$ is Lipschitz continuous and presented an upper bound, we investigate what is the maximal difference in the output of $T(\\cdot;\\theta)$ with respect to two arbitrary inputs $x,y\\in\\Omega$ , and whether it can be bounded. To address this, we present the following remark: ", "page_idx": 17}, {"type": "text", "text": "Remark D.3. Given a bounded domain $\\Omega=[a,b],\\;a<b$ , by construction, the diffeomorphism $T(\\cdot;\\theta)$ with parameter $\\pmb{\\theta}$ in DIGRAF, as in Equation (7), is a $\\Omega\\to\\Omega$ transformation [24, 25]. Therefore, by the max value theorem, the maximal output discrepancy for arbitrary $x,y\\in\\Omega$ is $|b-a|$ , i.e., $|T(x;\\pmb\\theta)-T(y;\\pmb\\theta)|\\leq|b-a|$ . ", "page_idx": 18}, {"type": "text", "text": "Combining the Proposition D.1, Proposition D.2 and Remark D.3, we formalize and prove the following proposition: ", "page_idx": 18}, {"type": "text", "text": "Proposition 4.1 (The boundedness of $T(\\cdot;\\pmb\\theta^{(l)})$ in DIGRAF). Given a bounded domain $\\Omega=[a,b]\\subset\\mathbb{R}$ where $a<b$ , and any two arbitrary points $x,y\\,\\in\\,\\Omega,$ , the maximal difference of a diffeomorphism $T(\\cdot;\\pmb\\theta^{(l)})$ with parameter ${\\pmb\\theta}^{(l)}$ in DIGRAF is bounded as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n|T(x;\\pmb{\\theta}^{(l)})-T(y;\\pmb{\\theta}^{(l)})|\\leq\\operatorname*{min}(|b-a|,|x-y|\\exp(C_{v^{\\theta(l)}}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where Cv\u03b8(l) is the Lipschitz constant of the CPA velocity field v\u03b8(l). ", "page_idx": 18}, {"type": "text", "text": "Proof. In Proposition D.2 we presented an upper bound on the Lipschitz constant of $T(\\cdot;\\theta)$ , and in D.3 we also presented an upper bound on the maximal difference between the application of $T(\\cdot;\\theta)$ on two inputs $x,y$ . Combining the two bounds, we get the following inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n|T(x;\\pmb\\theta)-T(y;\\pmb\\theta)|\\leq\\operatorname*{min}(|b-a|,|x-y|\\exp(C_{v}\\pmb\\theta)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The result in Proposition 4.1 gives us a tighter upper bound on the boundedness of the transformation $T(\\cdot;\\theta)$ in our DIGRAF that is related both to the hyperparameters $a,b$ , as well as the learned velocity field parameters $\\pmb{\\theta}$ . ", "page_idx": 18}, {"type": "text", "text": "Next, we discuss another property outlined in Section 4.3, demonstrating that DIGRAF is permutation equivariant \u2013 a desirable property when designing a GNN component [8]. ", "page_idx": 18}, {"type": "text", "text": "Proposition D.4 ( DIGRAF is permutation equivariant.). Consider a graph encoded by the adjacency matrix ${\\bf A}\\in\\mathbb{R}^{N\\times N}$ , where $N$ is the number of nodes. Let $\\bar{\\mathbf{H}}^{(l)}\\in\\mathbb{R}^{N\\times C}$ be the intermediate node features at layer $l$ before the element-wise application of our DIGRAF. Let $\\mathbf{P}$ be an $N\\times N$ permutation matrix. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{DIGRAF}(\\mathbf{P}\\bar{\\mathbf{H}}^{(l)},\\pmb{\\theta}_{P}^{(l)})=\\mathbf{P}\\,\\mathrm{DIGRAF}(\\bar{\\mathbf{H}}^{(l)},\\pmb{\\theta}^{(l)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{\\theta}_{P}^{(l)}$ and $\\pmb{\\theta}^{(l)}$ are obtained by feeding $\\mathbf{P}\\bar{\\mathbf{H}}^{(l)}$ and $\\bar{\\mathbf{H}}^{(l)}$ , respectively, to Equation (9). ", "page_idx": 18}, {"type": "text", "text": "Proof. We break down the proof into two parts. First, we show that $\\mathrm{GNN_{ACT}}$ outputs the same $\\pmb{\\theta}$ under permutations, that is we show ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{P}^{(l)}=\\pmb{\\theta}^{(l)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Second, we prove that the activation function $T^{(l)}$ is permutation equivariant, ensuring the overall method maintains this property. ", "page_idx": 18}, {"type": "text", "text": "To begin with, recall that Equation (9) is composed by $\\mathrm{GNN_{ACT}}$ , which is permutation equivariant, and by a pooling layer, which is permutation invariant. Therefore their composition is permutation invariant, that is $\\bar{\\pmb\\theta}_{P}^{(l)}=\\pmb\\theta^{(l)}$ . ", "page_idx": 18}, {"type": "text", "text": "Prior to the activation function layer $T^{(l)}$ , $\\bar{\\mathbf{H}}^{(l)}$ undergoes rescaling as described in Appendix B, which is permutation equivariant as it operates element-wise. Finally, since activation function $T^{(l)}$ acts element-wise, and given that $\\pmb{\\theta}$ remains unchanged, the related CPA velocity fields are identical, resulting in the same transformed output for each entry, despite the entries being permuted in $\\mathbf{P}\\bar{\\mathbf{H}}^{(l)}$ . Therefore, DIGRAF is permutation equivariant. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D.1 Diffeomorphic Activation Functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide several examples of popular and well-known diffeomorphic functions, contributing to our motivation to utilize diffeomorphisms as a blueprint for learning graph activation functions. We remark that, differently from standard activation functions, our DIGRAF does not need to follow a predefined, fixed template, but can instead learn a diffeomorphism best suited for the task and input, as $T^{(l)}$ within CPAB can represent a wide range of diffeomorphisms [24, 25]. ", "page_idx": 18}, {"type": "text", "text": "We recall that, as outlined in Section 3.1, a function is classified as a diffeomorphism if it is (1) bijective, (2) differentiable, and (3) has a differentiable inverse. ", "page_idx": 18}, {"type": "text", "text": "Sigmoid. We denote the Sigmoid activation function as $\\sigma:\\mathbb{R}\\rightarrow(0,1)$ , defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma(x)=\\frac{1}{1+e^{-x}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove that $\\sigma$ is a diffeomorphism, we first establish its bijectivity. Injectivity follows from observing that for any distinct points $x_{1}$ and $x_{2}$ in $\\mathbb{R}$ , $\\begin{array}{r}{\\sigma(x_{1})=\\frac{1}{1+e^{-x_{1}}}}\\end{array}$ can only equal $\\begin{array}{r}{\\bar{\\sigma}(x_{2})\\overset{\\cdot}{=}\\frac{1}{1+e^{-x_{2}}}}\\end{array}$ if and only if $x_{1}=x_{2}$ . For surjectivity, we represent $_x$ as a function of $y$ , such that $\\begin{array}{r}{y=\\frac{1}{1+e^{-x}}\\implies x=-\\ln\\left(\\frac{1-y}{y}\\right)}\\end{array}$ , ensuring that for every $y\\in(0,1)$ there is an element $x\\in\\mathbb R$ such that $\\sigma(x)=y$ . ", "page_idx": 19}, {"type": "text", "text": "To demonstrate differentiability, we examine the derivative of $\\sigma$ . The derivative ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d x}\\sigma(x)=\\sigma(x)(1-\\sigma(x)),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is continuous. Additionally, the inverse function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma^{-1}(y)=-\\ln\\left(\\frac{1-y}{y}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is also bijective and differentiable. Thus, with all these requirements satisfied, $\\sigma$ is indeed a diffeomorphism. ", "page_idx": 19}, {"type": "text", "text": "Tanh. The hyperbolic tangent function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{tanh}(x)={\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is a diffeomorphism from $\\mathbb{R}$ to $(-1,1)$ . To establish this, we demonstrate that tanh is bijective and differentiable, with a differentiable inverse function. ", "page_idx": 19}, {"type": "text", "text": "Firstly, tanh is injective because if $\\operatorname{tanh}(x_{1})=\\operatorname{tanh}(x_{2})$ , then $x_{1}=x_{2}$ . It is also surjective because for any $y\\in(-1,1)$ , there exists $\\begin{array}{r}{x=\\frac{1}{2}\\ln\\left(\\frac{1+y}{1-y}\\right)}\\end{array}$ such that $\\operatorname{tanh}(x)=y$ . ", "page_idx": 19}, {"type": "text", "text": "The derivative ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d x}\\operatorname{tanh}(x)=1-\\operatorname{tanh}^{2}(x)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is continuous and positive. Additionally, the inverse function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{tanh}^{-1}(y)=\\frac{1}{2}\\ln\\left(\\frac{1+y}{1-y}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is continuously differentiable. Therefore, tanh qualifies as a diffeomorphism. ", "page_idx": 19}, {"type": "text", "text": "Softplus. To establish the Softplus function ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathrm{softplus}}(x)=\\ln(1+e^{x})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as a diffeomorphism from $\\mathbb{R}$ to $(0,\\infty)$ , we first demonstrate its injectivity and surjectivity. ", "page_idx": 19}, {"type": "text", "text": "Assuming softplus $(x_{1})=\\{{\\mathrm{softplus}}(x_{2})$ , we obtain $e^{x_{1}}=e^{x_{2}}$ , implying $x_{1}=x_{2}$ , hence establishing injectivity.   \nFor any $y\\in(0,\\infty)$ , we find an $x\\in\\mathbb R$ such that $y=\\ln(1+e^{x})$ , ensuring surjectivity. ", "page_idx": 19}, {"type": "text", "text": "The derivative of the Softplus function, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{d}{d x}}\\mathrm{softplus}(x)={\\frac{e^{x}}{1+e^{x}}}=\\sigma(x),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\sigma(x)$ is the Sigmoid function, known to be continuous and differentiable. Therefore, softplus $(x)$ is continuously differentiable. ", "page_idx": 19}, {"type": "text", "text": "Considering the inverse of the Softplus function, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathrm{softplus}}^{-1}(y)=\\ln(e^{y}-1),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "its derivative is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d y}\\mathrm{softplus}^{-1}(y)=\\frac{e^{y}}{e^{y}-1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is continuous for all $y>0$ , indicating that softplus $^{-1}(y)$ is continuously differentiable for all $y>0$ .   \nTherefore, we conclude that the softplus function qualifies as a diffeomorphism. ", "page_idx": 19}, {"type": "text", "text": "ELU. The ELU activation function [11] is defined as below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{ELU}(x)={\\left\\{\\!\\!\\begin{array}{l l}{x}&{{\\mathrm{if~}}x>0}\\\\ {\\alpha(e^{x}-1)}&{{\\mathrm{if~}}x\\leq0}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\alpha\\in\\mathbb{R}$ is a constant that scales the negative part of the function. To demonstrate that ELU is bijective, we analyze its injectivity and surjectivity. For $x>0$ , ELU acts as the identity function, which is inherently injective. For $x\\leq0$ , $\\bar{\\alpha}(e^{x_{1}}-1)=\\alpha(e^{x_{2}}-1)$ , implies $x_{1}=x_{2}$ . The inverse function for ELU is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{ELU}^{-1}(y)={\\left\\{\\begin{array}{l l}{y}&{{\\mathrm{if~}}y>0}\\\\ {\\ln({\\frac{y}{\\alpha}}+1)}&{{\\mathrm{if~}}y\\leq0}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This inverse maps every value in the codomain back to a unique value in the domain, proving that ELU is surjective. ", "page_idx": 20}, {"type": "text", "text": "Next, we examine the continuity of ELU. At $x\\,=\\,0$ , $\\mathrm{ELU}(x=0)=\\alpha(e^{0}-1)=0$ . Next, we check the limits for both sides of 0. For $x\\,>\\,0$ , $\\begin{array}{r}{\\operatorname*{lim}_{x\\to0^{+}}\\mathrm{ELU}(x)\\,=\\,\\operatorname*{lim}_{x\\to0^{+}}{x}\\,=\\,0}\\end{array}$ , while for $x\\,\\leq\\,0$ , we have $\\begin{array}{r}{\\operatorname*{lim}_{x\\to0^{-}}\\operatorname{ELU}(x)=\\operatorname*{lim}_{x\\to0^{-}}\\alpha(e^{x}-1)=0}\\end{array}$ . Since both limits are equal, the ELU function is continuous at $x=0$ . For the derivative of ELU, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{d}{d x}}\\mathrm{ELU}(x)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}x>0}\\\\ {\\alpha e^{x}}&{{\\mathrm{if~}}x\\leq0}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "at $x=0$ , we have $\\begin{array}{r}{\\frac{d}{d x}\\mathrm{ELU}(x)=\\alpha e^{0}=\\alpha}\\end{array}$ . By setting $\\alpha=1$ , the derivative at $x=0$ matches the derivative for $x>0$ , making the derivative continuous. ", "page_idx": 20}, {"type": "text", "text": "The derivative for the inverse function is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d}{d y}\\mathrm{ELU}^{-1}(y)=\\left\\{1\\atop\\frac{1}{y+\\alpha}\\right.\\qquad\\mathrm{if~}y\\geq0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is also continuously differentiable. Hence, ELU is a diffeomorphism. ", "page_idx": 20}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Function Approximation with CPAB ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/14b3ad44a70d14ea21bb82bc4e46803c27760515044ae4aeb24fc2ef765f6cb8.jpg", "img_caption": ["Figure 6: The approximation error of the Peaks function (Equation (27)) with ReLU, Tanh, and DIGRAF. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The combination of learned linear layers together with non-linear functions such as ReLU and Tanh are wellknown to yield good function approximations [13, 14]. Therefore, when designing an activation function blueprint, i.e., the template by which the activation function is learned, it is important to consider its approximation power. In Section 1 and in particular in Figure 2, we demonstrate the ability of the CPAB framework to approximate known activation functions. We now show additional evidence for the flexibility and power of CPAB as a framework for learning activation functions, leading to our DIGRAF. To this end, we consider the ability of a multilayer perceptron (MLP) with various activation functions (ReLU, Tanh, and DIGRAF) to approximate the well-known \u2018peaks\u2019 function that mathematically reads: ", "page_idx": 20}, {"type": "equation", "text": "$$\ng(x,y)=3(1-x)^{2}\\exp(-(x^{2})-(y+1)^{2})-10(\\frac{x}{5}-x^{3}-y^{5})\\exp(-x^{2}-y^{2})-\\frac{1}{3}\\exp(-(x+1)^{2}-y^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The peaks function in Equation (27) is often times used to measure the ability of methods to approximate functions [32], where the input is point pairs $(x,y)\\in\\mathbb{R}^{2}$ , and the goal is to minimize the mean-squared-error between the predicted function value $g$ and the actual function value $x$ . Formally, we consider the following MLP: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{g}(x,y)=(\\sigma(\\sigma(\\left[x,y\\right]W_{1})W_{2})W_{3}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sigma$ is the activation of choice (ReLU, Tanh, or DIGRAF), and $W_{1}\\in\\mathbb{R}^{2\\times64}$ , $W_{2}\\in\\mathbb{R}^{64\\times64}$ , $W_{3}\\in\\mathbb{R}^{64\\times1}$ are the trainable parameter matrices of the linear layers in the MLP. The goal, as discussed above, is to minimize the loss $\\|\\hat{g}(x,y)-g(x,y)\\|_{2}$ , for data triplets $(x_{i},y_{i},g(x_{i},y_{i}))$ sampled from the peaks function. In our experiment, we sample 50,000 points, and report the obtained approximation error in terms of MSE in Figure 6. As can be seen, our DIGRAF, based on the CPAB framework, allows to obtain a significantly lower approximation error, up to 10 times lower (better) than ReLU, and 3 times better than Tanh. This example further motivates us to harness CPAB as the blueprint of DIGRAF. ", "page_idx": 20}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/4dc7e1aab2bdf517e456684027d7eedcbb3bdc23c392254a9b443c8e368390ab.jpg", "table_caption": ["Table 5: Graph classification accuracy $(\\%)\\uparrow$ on TUDatasets. The top three methods are marked by First, Second, Third. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/a31622fa51d9f9feb5fcb8df2e38fed19c4f87502270fe840fdcebd221367810.jpg", "table_caption": ["Table 6: Different GNN architectures (GCN, GAT, GIN, SAGE) coupled with ReLU, DIGRAF (W/O ADAP.) and DIGRAF activation functions. The top performing model is marked with the corresponding color for each architecture. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2 Results on TUDatasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our results are summarized in Table 5, where we consider MUTAG, PTC, PROTEINS, NCI1 and NCI109 datasets from the TU repository [56]. As can be seen from the table, DIGRAF is consistently among the top-3 best-performing activation functions, and it consistently outperforms other graph-adaptive activation functions. These results support our design choices for DIGRAF and the flexibility offered by CPAB diffeomorphisms. ", "page_idx": 21}, {"type": "text", "text": "E.3 Comparison with Different GNN Architectures ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We now provide a comparison between DIGRAF and the ReLU activation function coupled with GCN, GAT, GIN, and SAGE backbones in Table 6. Notably, DIGRAF consistently outperforms ReLU regardless of the backbone architecture. ", "page_idx": 21}, {"type": "text", "text": "E.4 Visualization of DIGRAF ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To gain a qualitative understanding of the behavior of DIGRAF, we now illustrate the activation function learned by DIGRAF after the last GNN layer on different graphs. To this end, we randomly selected two graphs from the ZINC dataset, as shown in Figure 7. The original graphs are presented in the lower right, with each color representing a feature. Nodes with the same color share the same feature. The comparison of the figures demonstrates that for different graphs, with different features and structures, DIGRAF learns distinct activation functions, showing its adaptivity to the input graph. ", "page_idx": 21}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/0a43fb83857aa516011e60cab64475c287e76fe78e73400bf8561dbc98ed0ee7.jpg", "img_caption": ["Figure 7: Activation function learned by DIGRAF after the last GNN layer on two randomly selected graphs from ZINC. Different node colors indicate different node features. DIGRAF yields different activations for different graphs. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 7: Performance Comparison of DIGRAF with ReLU variants of increased parameter budget. The number of parameters is reported within the parenthesis adjacent to the metric. We use GINE [37] as a backbone. Increasing the parameter count with ReLU does not yield significant improvements, and DIGRAF outperforms all variants, even those with a higher number of parameters. Note that, DIGRAF (W/O ADAP.) has only $\\mathcal{N}_{\\mathcal{P}}-1$ additional parameters, where $\\mathcal{N}_{\\mathcal{P}}$ is the tessellation size. ", "page_idx": 22}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/d3ff86f1df6027de91de8d94fe17dde4a34cca00b11c2881d5550c1c09481810.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.5 Parameter Count Comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "$\\mathrm{GNN_{ACT}}$ is a core component of DIGRAF, which ensures graph-adaptivity by generating the parameters $\\pmb{\\theta}^{(l)}$ of the activation function conditioned on the input graph. While the beneftis of graph-adaptive activation functions are evident from our experiments in Section 5, as DIGRAF consistently outperforms DIGRAF (W/O ADAP.), the variant of our method that is not graph adaptive, it comes at the cost of additional parameters to learn $\\mathrm{GNN_{ACT}}$ (Equation (9)). Specifically, because in all our experiments $\\mathrm{GNN_{ACT}}$ is composed of 2 layers and a hidden dimension of 64, DIGRAF adds at most approximately 20K additional parameters. The number of added parameters in DIGRAF (W/O ADAP.) is significantly lower, counting at $\\mathcal{N}_{\\mathcal{P}}\\stackrel{^{-}}{-1}$ , where $\\mathcal{N}_{\\mathcal{P}}$ is the tessellation size. Note in our experiments, the tessellation size does not exceed 16. To further understand whether the improved performance of DIGRAF is due to the increased number of parameters, we conduct an additional experiment using the ReLU activation function where we increase the number of parameters of the model and compare the performances. In particular, we consider following settings: (1) The standard variant $(\\mathrm{GIN}+\\mathrm{ReLU})$ , (2) The variant obtained by doubling the number of layers, and (3) The variant is obtained by doubling the number of hidden channels. ", "page_idx": 22}, {"type": "text", "text": "We present the results of the experiment described above on the ZINC-12K and MOLHIV datasets in Table 7. We observed that adding more parameters to the ReLU baseline does not produce significant performance improvements, even in cases where the baselines have ${\\sim}4$ times more parameters than DIGRAFand its baseline. On the contrary, with DIGRAF significantly improved performance is obtained compared to the baselines. ", "page_idx": 22}, {"type": "text", "text": "F Ablation Studies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present the impact of several key components of DIGRAF, namely the tessellation size $\\mathcal{N}_{\\mathcal{P}}$ , the depth of $\\mathrm{GNN_{ACT}}$ (Equation (9)) and the regularization coefficient $\\lambda$ of $\\pmb{\\theta}^{(l)}$ (Equation (12)). We choose a few ", "page_idx": 22}, {"type": "image", "img_path": "ZZoW4Z3le4/tmp/ce2d511a32229e31029005966a063775c5959bbd384ebe3db634680efb0b9edf.jpg", "img_caption": ["Figure 8: Impact of tessellation size $\\mathcal{N}_{\\mathcal{P}}$ on the performance of DIGRAF on CORA, CITESEER, FLICKR, BLOGCATALOG, and MOLHIV datasets. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/5dac4799ddb769b0b05621824d8581ccd339d1ae24966cecb541f9c1ed9190bc.jpg", "table_caption": ["Table 8: Effect of depth of $\\mathrm{GNN_{ACT}}$ on DIGRAF. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "representative datasets, i.e., CORA, CITESEER, FLICKR and BLOGCATALOG for which we use GCN [43]; and ZINC- $12\\mathtt{K}$ and MOLHIV for which we use GINE [37] as GNN respectively. ", "page_idx": 23}, {"type": "text", "text": "F.1 Tessellation Size ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall that the tessellation size $\\mathcal{N}_{\\mathcal{P}}$ determines the dimension of $\\pmb{\\theta}^{(l)}\\in\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ that parameterizes the velocity fields within DIGRAF. We study the effect of the tessellation size on the performance of DIGRAF in Figure 8. We can see that a small tessellation size is sufficient for good performance, and increasing its size results in marginal changes. This observation suggests that CPAB is highly flexible, and aligns with the conclusions in previous studies on different applications of CPAB [52], which have shown that small sizes are sufficient in most cases. ", "page_idx": 23}, {"type": "text", "text": "F.2 Depth of $\\mathbf{GNN_{ACT}}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "DIGRAF exhibits graph adaptivity by predicting $\\pmb{\\theta}^{(l)}\\,\\in\\,\\mathbb{R}^{\\mathcal{N}_{\\mathcal{P}}-1}$ conditioned on the input graph through $\\mathrm{GNN_{ACT}}$ . Table 8 shows the impact of the number of layers $L_{\\mathrm{ACT}}$ of $\\mathrm{GNN_{ACT}}$ on the performance of DIGRAF. In particular, we maintain a fixed architecture for DIGRAF and vary only $L_{\\mathrm{ACT}}$ . The results show that increasing the depth of $\\mathrm{GNN_{ACT}}$ improves only marginally the performance of DIGRAF, demonstrating that the increased number of parameters is not the main factor of the better performance of DIGRAF. On the contrary, the flexibility and adaptivity offered by DIGRAF are the main factors of the improvements, as demonstrated by DIGRAF consistently outperforming DIGRAF (W/O ADAP.) and other activation functions (Section 5). ", "page_idx": 23}, {"type": "text", "text": "F.3 Regularization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As discussed in Section 4.2, the regularization enforces the smoothness of the velocity field. We investigate the impact of the value of the regularization coefficient $\\lambda$ on DIGRAF (Equation (12)) in Table 9. The results reveal that the optimal value of $\\lambda$ depends on the dataset of interest, with small positive values yielding generally good results across all datasets. ", "page_idx": 23}, {"type": "text", "text": "F.4 Comparison of DIGRAF and DIGRAF (W/O ADAP.) with Equal Parameter Budget ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To demonstrate the efficacy of graph adaptivity provided by $\\mathrm{GNN_{ACT}}$ , we conduct an experiment where we increase the number of layers and channels of $\\mathrm{GNN_{LAYER}}$ in DIGRAF (W/O ADAP.) to match the total number of parameters in DIGRAF. As shown in Table 10, the increase in the number of parameters does not translate to better performance. Rather, the effective usage of the extra parameters as done by $\\mathrm{GNN_{ACT}}$ is the reason behind the performance boost offered by DIGRAF. ", "page_idx": 23}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/0a923d53141ad8ee00aab7efed5213517ecf3f5e2f8b9c115dc7aaf65ed16818.jpg", "table_caption": ["Table 9: Effect of velocity field regularization coefficient $\\lambda$ on DIGRAF. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/e66ba7aa1ba59aed0154c56fe5dc8f5b4833dfda236c216edf232ad2c4734f33.jpg", "table_caption": ["Table 10: Results on ZINC and MOLHIV datasets along with number of parameters in parenthesis. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "G Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We implemented DIGRAF using PyTorch [63] (offered under BSD-3 Clause license) and the PyTorch Geometric library [22] (offered under MIT license). All experiments were conducted on NVIDIA RTX A5000, NVIDIA GeForce RTX 4090, NVIDIA GeForce RTX 4070 Ti Super, NVIDIA GeForce GTX 1080 Ti, NVIDIA TITAN RTX and NVIDIA TITAN V GPUs. For hyperparameter tuning and model selection, we utilized the Weights and Biases (wandb) library [6]. We used the difw package [52, 25, 24, 74] (offered under MIT license) for the diffeomorphic transformations based on the closed-form integration of CPA velocity functions. In the following subsections, we present the experimental procedure, dataset details, and hyperparameter configurations for each task. ", "page_idx": 24}, {"type": "text", "text": "Hyperparameters. The hyperparameters include the number of layers $L$ and embedding dimension $C$ of GNN(LlA)YER , learning rates and weight decay factors for both $\\mathrm{GNN}_{\\mathrm{LAYER}}^{(l)}$ and $\\mathrm{GNN_{ACT}}$ , dropout rate $p$ , tessellation size $\\mathcal{N P}$ , and regularization coefficient $\\lambda$ . We additionally include the number of layers $L_{\\mathrm{ACT}}$ and embedding dimension $C_{\\mathrm{ACT}}$ of $\\mathrm{GNN_{ACT}}$ . We employed a combination of grid search and Bayesian optimization. All hyperparameters were chosen according to the best validation metric. For the baselines, we include only the applicable hyperparameters in our search space. ", "page_idx": 24}, {"type": "text", "text": "Node Classification. For each dataset, we train a 2-layer GCN [43] as the backbone architecture, and integrate each of the activation functions into this model. Following Zhang et al. [92], we randomly choose 20 nodes from each class for training and select 1000 nodes for testing. For each activation function, we run the experiment 10 times with random partitions. We report the mean and standard deviation of node classification accuracy on the test set. Table 11 summarizes the statistics of the node classification datasets used in our experiments. All models were trained for 1000 epochs with a fixed batch size of 32 using the Adam optimizer. Tables 12 and 13 lists the hyperparameters and their search ranges or values. ", "page_idx": 24}, {"type": "text", "text": "Graph Classification. The statistics of various datasets can be found in Table 14. We consider the following setup: ", "page_idx": 24}, {"type": "text", "text": "\u2022 ZINC-12K: We consider the splits provided in Dwivedi et al. [17]. We use the mean absolute error (MAE) both as the loss and evaluation metric and report the mean and standard deviation over the test set calculated using five different seeds. We use the Adam optimizer and decay the learning rate by 0.5 every 300 epochs, with a maximum of 1000 epochs. In all our experiments, we adhere to the $500\\mathrm{K}$ parameter budget [17]. We use GINE [37] layers both for $\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\mathrm{LAYER}}^{(\\bar{l})}$ and within $\\mathrm{GNN_{ACT}}$ and we fix $C_{\\mathrm{ACT}}\\,=\\,64\\$ and $L_{\\mathrm{ACT}}\\,=\\,2$ . We report the hyperparameter search space for all the other hyperparameters in Table 15. ", "page_idx": 24}, {"type": "text", "text": "\u2022 TUDatasets: We follow the standard procedure prescribed in $\\mathrm{Xu}$ et al. [84] for evaluation. That is, we use a 10 fold cross-validation and report the mean and standard deviation of the accuracy at the epoch .  oWf e3 5p0r eespeonct htsh.e  Whye puesrep aGrIaNm [et8e4r]  sleaayrecrhs  sbpotahc ef ofro $\\mathrm{GNN}_{\\mathrm{LAYER}}^{(l)}$ p aanrad mweittehrisn $\\mathrm{GNN_{ACT}}$ 1, 5a.nd we fix $L_{\\mathrm{ACT}}=2$ ", "page_idx": 24}, {"type": "text", "text": "\u2022 OGB: We consider 4 datasets from the OGB repository, with one, namely MOLESOL, being a regression problem, while the others are classification tasks. We run each experiment using five different seeds and report the mean and standard deviation of RMSE/ROC-AUC. We use the Adam 5o0pt0i empizoecrh, sd. ecWaey iunseg  tthhee  lGeIaNrnEi nmg ordateel  bwyit ha  tfhaec teonr coofd 0e.r5s  eprveesrcyr i1b0e0d  einp oHcuh se, t aanl.d  [t3ra6i] nb footrh  af omr $\\mathrm{GNN}_{\\mathrm{LAY}}^{(l)}$ EoRf and within $\\mathrm{GNN_{ACT}}$ , and we set $C_{\\mathrm{ACT}}=64$ and $L_{\\mathrm{ACT}}=2$ . We present the hyperparameter search space for all other parameters in Table 15 ", "page_idx": 24}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/23e7c3a53e39f3406f936fc9d269ced5b969b2e4a7a4cf3311559d8dcc8ac8ca.jpg", "table_caption": ["Table 11: Statistics of the node classification datasets [53, 73, 58, 86]. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/925140f2f1f64b1809d53f39f17a08a814030f40b816c3a067eab37bcc06031a.jpg", "table_caption": ["Table 12: Hyperparameter configurations for the Planetoid datasets [53, 73, 58]. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "H Complexity and Runtimes ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Time Complexity. We now provide an analysis of the time complexity of DIGRAF. Let us recall the following details: (i) As described in Equation (8), DIGRAF is applied element-wise in parallel for each dimension of the output of $\\mathbf{G}\\mathbf{N}\\mathbf{N}_{\\mathrm{LAYER}}^{(l)}$ . (ii) As described in Equation (9), we employ an additional GNN denoted by $\\mathrm{GNN_{ACT}}$ to compute ${\\pmb\\theta}^{(l)}$ . In all our experiments, both the backbone GNN and $\\mathrm{GNN_{ACT}}$ are message-passing neural networks (MPNNs) [28]. (iii) As described in Theorem 2 of Freifeld et al. [25], for 1-dimensional domain, there exists a closed form for $\\dot{T}^{(l)}(\\cdot;\\pmb\\theta^{(l)})$ , and the complexity for the CPAB computations are linear with respect to the tesselation size, which is a constant of up to 16 in our experiments. Therefore, using DIGRAF with any linear complexity (with respect to the number of nodes and edges) MPNN-based backbone maintains the linear complexity of the backbone MPNN. Put precisely, each MPNN layer has linear complexity in the number of nodes $|V|$ and $|E|$ . We use $L_{\\mathrm{ACT}}$ layers in $\\mathrm{GNN_{ACT}}$ , the computational complexity of a DIGRAF layer is $\\mathcal{O}(L_{\\mathrm{{ACT}}}\\cdot(|V|+|E|))$ . Since we have $L$ layers in overall GNN, the computational complexity of an MPNN-based GNN coupled with DIGRAF is $\\mathcal{O}(L\\cdot L_{\\mathrm{ACT}}\\cdot(|V|+|E|))$ . In our experiments, we fix the hyperparameter $L_{\\mathrm{ACT}}=2$ , resulting in $\\mathcal{O}(L\\cdot(|V|+|E|))$ computational complexity in practice. ", "page_idx": 25}, {"type": "text", "text": "Memory Complexity. DIGRAF uses $\\mathrm{GNN_{ACT}}$ which is an MPNN and hence has linear space complexity (with respect to the number of nodes and edges). CPAB computations require constant memory with respect to the graph size for a 1-dimensional domain due to the analytical implementation. We use $L$ layers in overall GNN and $L_{\\mathrm{ACT}}$ layers in $\\mathrm{GNN_{ACT}}$ resulting in a memory complexity of $\\tilde{\\mathcal{O}}(L\\cdot L_{\\mathrm{ACT}}\\cdot(|V|+|E|))$ . In our experiments, we fix the hyperparameter $L_{\\mathrm{{ACT}}}=2$ , resulting in $\\mathcal{O}(L\\cdot(|V|+|E|))$ memory complexity in practice. ", "page_idx": 25}, {"type": "text", "text": "Runtimes. Despite having linear computational complexity in the size of the graph, DIGRAF performs additional computations to obtain ${\\pmb\\theta}^{(l)}$ using $\\mathrm{GNN_{ACT}}$ . To understand the impact of these computations, we measured the training and inference times of DIGRAF and present it in Table 16. Specifically, we report the average time per batch and standard deviation of the same measured on an NVIDIA A5000 GPU, using a batch size of 128. For a fair comparison, we use the same number of layers, batch size, and channels in all methods. Additionally, for our DIGRAF, we set the number of layers within $\\mathrm{GNN_{ACT}}$ to $L_{\\mathrm{ACT}}=2$ , and the embedding dimension to $C_{\\mathrm{ACT}}=64$ . Our analysis indicates that while DIGRAF requires additional computational time, it yields significantly better performance. For example, compared to the best activation function on the dataset, namely Maxout, DIGRAF requires an additional $\\sim6.21\\mathrm{{ms}}$ at inference, but results in a relative improvement in the performance of $\\sim17.95\\%$ . ", "page_idx": 25}, {"type": "text", "text": "On the ZINC dataset, using GIN as the primary model, DIGRAF exhibits approximately 4.5 times slower training times and 3.5 times slower inference times compared to ReLU. DIGRAF demonstrates an inference time that is approximately 1.35 times faster than GReLU, while also achieving superior performance. ", "page_idx": 25}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/06f513939519741e53397887a55f3c3725707f1500731ce14296d338135ba4cc.jpg", "table_caption": ["Table 13: Hyperparameter configurations for the social network datasets [86]. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/dd42550fdb8c36639659d0bcf6cfbf194affd3fa3bba343dd59b63a0f0466bae.jpg", "table_caption": ["Table 14: Statistics of the graph classification datasets [55, 36, 17]. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/837ea3b23fabfc3527fd236768269d30479ee34f355e8de150156d8cace66fa9.jpg", "table_caption": ["Table 15: Hyperparameters and search ranges/values for TUDatasets [55], OGB [36], and ZINC12K [17] datasets. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 16: Batch runtimes on an NVIDIA RTX A5000 GPU of DIGRAF and other activation functions, with 4 GNN layers, batch size 128, 64 embedding dimensions, and $\\mathrm{GNN_{ACT}}$ with $L_{\\mathrm{{ACT}}}=2$ layers and $C_{\\mathrm{ACT}}=64$ embedding dimension, on ZINC-12K dataset. ", "page_idx": 26}, {"type": "table", "img_path": "ZZoW4Z3le4/tmp/7e4f6abc53d20dfe72fc0a59eb7ccdb624e2bc928f5f6f68d6ef88284158c0ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See Section 1 and Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See Section 6 and Appendix H. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We introduce the idea in Section 4.3, with additional propositions and detailed proofs show in Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: See Section 5, Appendix E, Appendix B and Appendix G. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide extensive details on the implementation and evaluation of our method, and we released our code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix G and Section 5. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The standard deviations for all the metrics have been presented. See Table 1, Table 2, Table 3, Table 5, Table 7, Table 8, Table 9, and Table 16. Also see Section 5, Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the computational resources in Appendix G and Appendix H. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: See Appendix G. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discuss the societal impact in the conclusion, see Section 6. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We don\u2019t scrape any datasets not we release any models of high risk for misuse. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 30}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cited each datasets we used in Section 5 and Appendix G. The license of the packages we used is in Appendix G. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We don\u2019t release any new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We don\u2019t involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]