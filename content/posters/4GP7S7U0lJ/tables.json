[{"figure_path": "4GP7S7U0lJ/tables/tables_7_1.jpg", "caption": "Table 1: Starting from fully-annotated MSVD training samples, exploiting data from MSR-VTT with our AL algorithm can further boost the performances on the MSVD test set.", "description": "This table presents the results of an experiment where the model was initially trained on the fully annotated MSVD dataset.  Then, the active learning algorithm was used to select additional data from the large, unannotated MSR-VTT dataset. The table shows the performance (using BLEU_4, METEOR, ROUGE_L, CIDEr, and SPICE metrics) on the MSVD test set after adding different percentages of data from MSR-VTT.  The \"Starting Point\" row shows the initial performance before any data from MSR-VTT was added.  The \"Random\" row shows the performance when data from MSR-VTT was randomly selected.  The rows labeled \"Ours\" show the performance after using the proposed active learning algorithm to select data from MSR-VTT, with varying amounts added at each step.", "section": "4.2 Main Active Learning Performances"}, {"figure_path": "4GP7S7U0lJ/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study on MSVD.", "description": "This table presents the results of an ablation study conducted on the MSVD dataset to evaluate the effectiveness of individual components of the proposed active learning scheme.  The study incrementally integrates various components, starting with a baseline of random sampling, and then adding uncertainty (L<sup>4</sup><sub>n</sub>), diversity (L<sup>3</sup><sub>n</sub>), learnability (L<sup>2</sup><sub>n</sub>), and finally the caption-wise protocol (CP). The Area Under the Curve (AUC) scores for CIDEr and SPICE are reported for each step, demonstrating the cumulative improvement in performance as components are added. The final row shows the results for the complete proposed method (+CP (Ours)).", "section": "4.3 Ablation Study on Learnability, Uncertainty, and Diversity"}, {"figure_path": "4GP7S7U0lJ/tables/tables_18_1.jpg", "caption": "Table 3: Results of SwinBERT on MSR-VTT test set. Specifically, SwinBert is trained with 25% of training data in MSR-VTT. Among them, Ours and Random are active learning methods where data is chosen based on various acquisition functions. The remaining four are decided by Dataset Maps.", "description": "This table presents the results of using SwinBERT on the MSR-VTT test set.  The model was trained with only 25% of the training data.  The results compare the performance of the proposed active learning method ('Ours') against a random sampling baseline and four other methods based on different data selection strategies derived from Dataset Maps (Easy, Moderate, Hard, Collective Outliers). The metrics used to evaluate performance are BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE.", "section": "4.2 Main Active Learning Performances"}, {"figure_path": "4GP7S7U0lJ/tables/tables_18_2.jpg", "caption": "Table 4: Results of SwinBERT trained with different mixed MVR-VTT datasets combined with 5% ground truths (i.e.initial seed set) and BLIP2 captions filtered by threshold th.", "description": "This table shows the results of using SwinBERT, a video captioning model, trained on different combinations of the MSR-VTT dataset and BLIP2 captions.  The MSR-VTT dataset is a large video dataset used for training video captioning models, while BLIP2 captions are captions generated by a large language model. The experiment started with 5% of the original MSR-VTT dataset (ground truth) and then added varying numbers of BLIP2 captions based on different threshold values (th).  The table reports the CIDEr and SPICE scores for each experimental setup.  These metrics assess the quality of the generated captions compared to human-written captions. The results show how the performance of SwinBERT improves as more BLIP2 captions are added, up to a certain point, after which performance starts to decrease.", "section": "4.2 Main Active Learning Performances"}]