[{"type": "text", "text": "A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Saeed Masoudian \\* Churney ApS, Denmark saeed@churney.io ", "page_idx": 0}, {"type": "text", "text": "Julian Zimmert Google Research zimmert@google.com ", "page_idx": 0}, {"type": "text", "text": "Yevgeny Seldin University of Copenhagen seldin@di.ku.dk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. In contrast to prior work, which required prior knowledge of the maximal delay $d_{\\mathrm{max}}$ and had a linear dependence of the regret on it, our algorithm can tolerate arbitrary excessive delays up to order $T$ (where $T$ is the time horizon). The algorithm is based on three technical innovations, which may all be of independent interest: (1) We introduce the first implicit exploration scheme that works in bestof-both-worlds setting. (2) We introduce the first control of distribution drift that does not rely on boundedness of delays. The control is based on the implicit exploration scheme and adaptive skipping of observations with excessive delays. (3) We introduce a procedure relating standard regret with drifted regret that does not rely on boundedness of delays. At the conceptual level, we demonstrate that complexity of best-of-both-worlds bandits with delayed feedback is characterized by the amount of information missing at the time of decision making (measured by the number of outstanding observations) rather than the time that the information is missing (measured by the delays). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Delayed feedback is an ubiquitous challenge in real-world applications. Study of multi-armed bandits with delayed feedback has started at least four decades ago in the context of adaptive clinical trials (Simon, 1977, Eick, 1988), the same problem that has earlier motivated introduction of the bandit model itself (Thompson, 1933). We focus on robustness to delay outliers and to the loss generation mechanism. In practice occasional delay outliers are common (e.g., observations that never arrive). Robustness to the loss generation mechanism implies that the algorithm does not need to know whether the losses are stochastic or adversarial, but still provides regret bounds that match the optimal stochastic rates if the losses happen to be stochastic, while guaranteeing the adversarial rates if they are not (so-called best-of-both-worlds regret bounds). Such algorithms are important from a practical viewpoint, because the loss generation mechanism can rarely assumed to be stochastic, but it is still desirable to have tighter regret bounds if it happens to be. From the theoretical perspective both forms of robustness are interesting and challenging, requiring novel analysis tools and yielding better understanding of the problems. ", "page_idx": 0}, {"type": "text", "text": "Joulani et al. (2013) have studied multi-armed bandits with delayed feedback under the assumption that the rewards are stochastic and the delays are sampled from a fixed distribution. ", "page_idx": 0}, {"type": "text", "text": "Table 1: Comparison to state-of-the-art. The following notation is used: $T$ is the time horizon, $K$ is the number of arms, $i$ indexes the arms, $\\Delta_{i}$ is the suboptimality gap or arm $i$ $\\sigma_{\\mathrm{max}}$ is the maximal number f outstanding observations, $\\textstyle D=\\sum_{t=1}^{T}d_{t}$ is th total delay, $S\\subseteq[T]$ is a set of skipped rounds, $\\bar{S}=[T]\\setminus S$ is the set of non-skipped rounds, $\\textstyle D_{\\bar{S}}=\\sum_{t\\in\\bar{S}}d_{t}$ is the total delay in the non-skipped rounds, and $d_{\\mathrm{max}}$ is the maximal delay. We have $\\mathrm{min}_{\\mathcal{S}}$ $(|S|+\\sqrt{D_{\\bar{S}}})\\leq\\sqrt{D}$ and $\\sigma_{\\mathrm{max}}\\le d_{\\mathrm{max}}$ , and in some cases $\\mathrm{min}_{\\mathcal{S}}$ $\\left(\\left\\vert S\\right\\vert+\\sqrt{D_{\\bar{S}}}\\right)\\ll\\sqrt{D}$ and $\\sigma_{\\mathrm{max}}\\ll d_{\\mathrm{max}}$ ", "page_idx": 1}, {"type": "table", "img_path": "LDzrQB4X5w/tmp/a30fc6846c3d6b0d2572f58aa3f93011dda86a40a2324027196ff07aec95f9f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "They provided a modification of the UCB1 algorithm for stochastic bandits with non-delayed fedback (Auer et al., 2002). They have shown that the regret of the modified algorithm is $\\begin{array}{r}{O\\left(\\sum_{i:\\Delta_{i}>0}\\left(\\frac{\\log T}{\\Delta_{i}}+\\sigma_{\\operatorname*{max}}\\Delta_{i}\\right)\\right)}\\end{array}$ (+ maxa), where indexes the ams, \u25b3 is the suboptimalitygap of am , $T$ is the time horizon (unknown to the algorithm), and $\\sigma_{\\mathrm{max}}$ is the maximal number of outstanding observations. (An observation is counted as outstanding at round $t$ if it originates from round $t$ or earlier, but due to delay it was not revealed to the algorithm by the end of round $t$ . The number of outstanding observations $\\sigma_{t}$ at round $t$ is the number of actions that have already been played, but their outcome was not observed yet. We also call $\\sigma_{t}$ the [running] count of outstanding observations. The maximal number of outstanding observations $\\sigma_{\\mathrm{max}}$ is the maximal value that $\\sigma_{t}$ takes and is unknown to the algorithm.) The result implies that in the stochastic setting the delays introduce an additive term in the regret bound, proportional to the maximal number of outstanding observation. ", "page_idx": 1}, {"type": "text", "text": "In the adversarial setting, multi-armed bandits with delayed feedback were first analyzed under the assumption of uniform delays (Neu et al., 2010, 2014). For this setting Cesa-Bianchi et al. (2019) have shown an $\\Omega(\\sqrt{K T}+\\sqrt{d T\\log K})$ lower bound and an almost matching upper bound, where $K$ is the number of arms and $d$ is a fixed delay. The algorithm of Cesa-Bianchi et al. is a modification of the EXP3 algorithm of Auer et al. (2002b). Cesa-Bianchi et al. used a fixed learning rate that is tuned based on the knowledge of $d$ . The analysis is based on control of the drift of the distribution over arms played by the algorithm from round $t$ to round $t+d$ . Thune et al. (2019) and Bistritz et al. (2019) provided algorithms for variable adversarial delays, but under the assumption that the delays are known \u201cat action time\", meaning that the delay $d_{t}$ is known at time $t$ , when the action is taken, rather that at time $t+d_{t}$ , when the observation arrives. The advanced knowledge of delays was used to tune the learning rate and control the drift of played distribution from round $t$ , when an action is played, to round $t+d_{t}$ , when the observation arrives. Alternatively, an advance knowledge of the cumulative delay up to the end of the game could be used for the same purpose. Finally, Zimmert and Seldin (2020) derived an algorithm for the adversarial setting that required no advance knowledge of delays and matched the lower bound of Cesa-Bianchi et al. (2019) within constants. The algorithm and analysis of Zimmert and Seldin avoid explicit control of the distribution drift and are parameterized by running counts of the number of outstanding observations $\\sigma_{t}$ , which is an empirical quantity that is observed at time $t$ (\"at the time of action\"). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Masoudian et al. (2022) attempted to extend the algorithm of Zimmert and Seldin (2020) to the best-of-both-worlds setting. The stochastic part of the analysis of Masoudian et al. is based on a direct control of the distribution drift. The control is achieved by damping the learning rate to make sure that the played distribution on arms is not changing too much from round $t$ , when an action is played, to round $t+d_{t}$ , when the loss is observed. Highly varying delays cannot be treated with this approach, because fast learning rates limit the range $d_{t}$ for which the drift is under control, while slow learning rates prevent learning. Therefore, Masoudian et al. had to reintroduce the assumption that that the maximal delay $d_{\\mathrm{max}}$ is known, and used it to tune the learning rate. Unfortunately, damping of the learning rate to control the drift over $d_{\\mathrm{max}}$ rounds made $d_{\\mathrm{max}}$ show up additively in the bound, meaning that potential presence of even a single delay of order $T$ made both the stochastic and the adversarial bounds linear in the time horizon. We emphasise that the linear dependence of the regret on $d_{\\mathrm{max}}$ is real and not an artefact of the analysis, because it comes from damped learning rate. ", "page_idx": 2}, {"type": "text", "text": "We introduce a different best-of-both-worlds modification of the algorithm of Zimmert and Seldin (2020) that is fully parameterized by the running count of outstanding observations and requires no advance knowledge of delays or the maximal delay $d_{\\mathrm{max}}$ . Our algorithm is based on a careful augmentation of the algorithm of Zimmert and Seldin with implicit exploration (described below), followed by application of a skipping technique (also described below) as a tool to limit the time span over which we need to control the distribution shift. ", "page_idx": 2}, {"type": "text", "text": "Implicit exploration was introduced by Neu (2015) to control the variance of importance-weighted loss estimates in adversarial bandits. But the exploration parameters add up linearly to the regret bound, making it highly challenging to design a scheme for best-of-both-worlds setting. The implicit exploration schedule of Neu leads to $\\Omega({\\sqrt{T}})$ regret bound and, therefore, unsuitable for that. Jin et al. (2022) introduced a different schedule for adversarial Markov decision processes with delayed feedback. However, it is unknown whether their schedule can work in a stochastic analysis. We introduce a novel schedule and show that it works in best-of-both-worlds setting. ", "page_idx": 2}, {"type": "text", "text": "Skipping was introduced by Thune et al. (2019) as a way to limit the dependence of an algorithm on a small number of excessively large delays. The idea is that it is \u201ccheaper\u2019 to skip a round with an excessively large delay and bound the regret in the corresponding round by 1, than to include it in the core analysis. Thune et al. have assumed prior knowledge of delays, but Zimmert and Seldin (2020) have perfected the technique by basing it on a running count of outstanding observations. In both works skipping was an optional add-on aimed to improve regret bounds in case of highly unbalanced delays. In our work skipping becomes an indispensable part of the algorithm, because, apart from making the algorithm robust to a few excessively large delays, it also limits the time span over which the control of distribution drift is needed. ", "page_idx": 2}, {"type": "text", "text": "In Table 1 we compare our results to state of the art. In a nutshell, we replace terms dependent on $d_{\\mathrm{max}}$ by terms dependent on $\\sigma_{\\mathrm{max}}$ , and terms dependent on the square root of the total cumulative delay $\\begin{array}{r}{\\dot{D}=\\sum_{t=1}^{T}\\bar{d}_{t}}\\end{array}$ by terms dependent on the umber of skipped rounds $|{\\cal S}|$ and a square root of the cumulative delay $\\textstyle D_{\\bar{S}}=\\sum_{t\\in\\bar{S}}d_{t}$ in the non-skipped rounds $\\bar{S}$ (those with the smaller delay). This yields robustness to excessive delays, because neither $\\sigma_{\\mathrm{max}}$ nor $\\mathrm{\\min}_{\\mathcal{S}}$ $\\left(\\left\\vert S\\right\\vert+\\sqrt{D_{\\bar{S}}}\\right)$ depend on the magnitude of delay outliers. By contrast, both the stochastic and the adversarial regret bounds of Masoudian et al. (2022) become linear in $T$ in presence of a single delay of order $T$ ", "page_idx": 2}, {"type": "text", "text": "There are also additional benefits. It has been shown that $\\sigma_{\\mathrm{max}}\\le d_{\\mathrm{max}}$ , and in some cases $\\sigma_{\\mathrm{max}}\\ll$ $d_{\\mathrm{max}}$ (Joulani et al., 2013, Masoudian et al., 2022). For example, if the first observation has delay $T$ , and the remaining observations have zero delay, then $d_{\\operatorname*{max}}=T$ , but $\\sigma_{\\mathrm{max}}=1$ . We also have that $\\mathrm{min}_{\\mathcal{S}}\\left(|S|+\\sqrt{D_{\\bar{\\mathcal{S}}}}\\right)\\leq\\sqrt{D}$ ,because $s=\\emptyset$ is part of the minimization on the left, and in some cases $\\mathrm{min}_{\\mathcal{S}}$ $\\left(\\left\\vert S\\right\\vert+\\sqrt{D_{\\bar{S}}}\\right)\\ll\\sqrt{D}$ . For example, if the delays in the first $\\sqrt{T}$ rounds are of order $T$ , and the delays in the remaining rounds are zero, then mins $\\left(\\left|S\\right|+\\sqrt{D_{\\bar{S}}}\\right)=\\mathcal{O}\\left(\\sqrt{T}\\right)$ , but ${\\sqrt{D}}=\\Omega\\left(T^{3/4}\\right)$ (Thune et al., 2019). Therefore, bounds that exploit skipping are preferable over bounds that do not, and for some problem instances the improvement is significant. In Appendix $\\boldsymbol{\\mathrm F}$ we show that bounds with an additive term $d_{\\mathrm{max}}$ , including the results of Masoudian et al. (2022), cannot benefit from skipping, in contrast to ours. ", "page_idx": 2}, {"type": "text", "text": "1. We provide the first best-of-both-worlds algorithm for bandits with delayed feedback that is robust to delay outliers. It improves both the stochastic and the adversarial regret bounds relative to the work of Masoudian et al. (2022), which lacks such robustness. For some problem instances the improvement is dramatic, e.g., in presence of a single delay of order $T$ both the stochastic and the adversarial regret bounds of Masoudian et al. are of order $T$ whereas our bounds are unaffected.   \n2. We provide an effcient technique to control the distribution drift under highly varying delays.   \n3. We provide the first implicit exploration scheme that works in best-of-both-worlds setting.   \n4. We provide a procedure relating drifted regret to normal regret in presence of delay outliers.   \n5. At the conceptual level, we show that best-of-both-worlds regret depends on the amount of information missing at the time of decision making (the number of outstanding observations) rather than the time that the information is missing (the delays). It was shown to be the case for the stochastic and adversarial regimes in isolation (Joulani et al., 2013, Zimmert and Seldin, 2020), but we are the first to show that it is also the case for best-of-both-worlds. ", "page_idx": 3}, {"type": "text", "text": "2 Problem setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study the problem of multi-armed bandit with variable delays. In each round $t=1,2,\\ldots$ the learner picks an action $I_{t}$ from a set of $K$ arms and immediately incurs a loss $\\ell_{t,I_{t}}$ from a loss vector $\\ell_{t}\\in[0,1]^{K}$ . However, the incurred loss is observed by the learner only after a delay of $d_{t}$ at the end of round $t+d_{t}$ . The delays are arbitrary and chosen by the environment. We use $\\sigma_{t}$ to denote the number of outstanding observations at time $t$ defined as $\\begin{array}{r}{\\sigma_{t}=\\sum_{s\\leq t}\\mathbb{1}(s+d_{s}>t)}\\end{array}$ and $\\sigma_{\\operatorname*{max}}=\\operatorname*{max}_{t\\in[T]}\\sigma_{t}$ to be the maximal number of outstanding observations. We consider two regimes for generation of losses by the environment: oblivious adversarial and stochastic. ", "page_idx": 3}, {"type": "text", "text": "We use pseudo-regret to compare the expected total loss of the learner's strategy to that of the best fixed action in hindsight. Specifically, the pseudo-regret is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{t,I_{t}}\\right]-\\operatorname*{min}_{i\\in[K]}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{t,i}\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\ell_{t,I_{t}}-\\ell_{t,i_{T}^{*}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\begin{array}{r}{i_{T}^{*}\\,=\\,\\operatorname*{min}_{i\\in[K]}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell_{t,i}\\right]}\\end{array}$ is the best action in hindsight. In the oblivious adversarial setting, the losses are assumed to be deterministic and independent of the actions taken by the algorithm. As a result, the expectation in the definition of $i_{T}^{*}$ can be omitted and the pseudo-regret definition coincides with the expected regret. Throughout the paper we assume that $i_{T}^{*}$ is unique. This is a common simplifying assumption in best-of-both-worlds analysis (Zimmert and Seldin, 2021). Tools for elimination of this assumption can be found in Ito (2021). ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The algorithm is a best-of-both-worlds modification of the adversarial FTRL algorithm with hybrid regularizer by Zimmert and Seldin (2020). It is provided in Algorithm 1 display. The modification includes biased loss estimators (implicit exploration) and adjusted skipping threshold. The algorithm maintains a set of skipped rounds $\\mathcal{S}_{t}$ (initially empty), a cumulative count of \u201c\"active\u201d outstanding observations (those that have not been skipped yet), and a vector of cumulative observed loss estimates $\\widehat{L}_{t}^{o b s}$ from non-skiped rounds At round $t$ the algorithmconstructs anFTRLdistributon $x_{t}$ over arms using regularizer $F_{t}$ defined in equation (2) below, and samples an arm according to $x_{t}$ . Then it receives the observations that arrive at round $t$ , except those that come from the skipped rounds, and updates the vector $\\widehat{L}_{t}^{o b s}$ of cumulative loss estimates. The los estimates $\\widehat{\\ell}_{t}$ are defined below in equation (1). Then it counts the number of \u201cactive\u201d outstanding observations ${\\widehat{\\sigma}}_{t}$ (those that belong to non-skipped rounds), updates the cumulative count of outstanding observations $\\mathcal{D}_{t}$ , and computes th skipng threshold $\\begin{array}{r}{d_{\\mathrm{max}}^{t}=\\sqrt{\\frac{\\mathcal{D}_{t}}{49K^{2/3}\\log K}}}\\end{array}$ $s$ for which he observation has not arrived yet and the waiting time $(t-s)$ exceeds the skipping threshold $d_{\\mathrm{max}}^{t}$ to the set of skipped rounds $\\boldsymbol{S}_{t}$ . Lemma 20, which is an adaptation of Zimmert and Seldin (2020, Lemma 5) to our skipping rule, shows that at most one round $s$ is skipped at a time (at most one index $s$ satisfies the if-condition for skipping in Line 15 of the algorithm for a given $t$ ", "page_idx": 3}, {"type": "image", "img_path": "LDzrQB4X5w/tmp/ef81208e4f404c034eb2a06507491958672fa6b4ce6443349f21b499d512ac63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We use implicit exploration to control importance-weighted loss estimates. The idea of using implicit exploration is inspired by the works of Neu (2015) and Jin et al. (2022), but its parametrization and purpose are different from prior work. To the best of our knowledge, it is the first time implicit exploration is used for best-of-both-worlds bounds. For any $s,t\\in[T]$ Wwith $s\\leq t$ we define implicit explorationterms $\\lambda_{s,t}=e^{-\\frac{\\mathcal{D}_{t}}{\\mathcal{D}_{t}-\\mathcal{D}_{s}}}$ . Our biased importance-weighted loss estimators are defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t,i}=\\frac{\\ell_{t,i}\\mathbb{1}(I_{t}=i)}{\\operatorname*{max}\\left\\{x_{t,i},\\lambda_{t,t+\\widehat{d}_{t}}\\right\\}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{d}_{s}=\\operatorname*{min}\\left(d_{s},\\operatorname*{min}\\left\\{(t-s):t-s\\ge d_{\\operatorname*{max}}^{t}\\right\\}$ ) denotes the time that the algorithm waits for the observation from round $s$ . It is the minimum of the delay $d_{s}$ , and the time $(t-s)$ to the first round whnthwaiing mexceds theskipping trldx ", "page_idx": 4}, {"type": "text", "text": "We use a hybrid regularizer based on a combination of the negative Tsallis entropy and the negative entropy, with separate learning rates, ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{t}(x)=-2\\eta_{t}^{-1}\\left(\\sum_{i=1}^{K}\\sqrt{x_{i}}\\right)+\\gamma_{t}^{-1}\\left(\\sum_{i=1}^{K}x_{i}\\log x_{i}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\eta_{t}^{-1}\\,=\\,\\sqrt{t}$ $\\begin{array}{r}{\\gamma_{t}^{-1}\\,=\\,\\sqrt{\\frac{49\\mathcal{D}_{t}}{\\log K}}}\\end{array}$ one used by Zimmert and Seldin (2020). By inheriting their regularizer we inherit their adversarial regret bound, which is minimax optimal, with just a minor adjustment due to introduction of implicit exploration and a slight change in the learning rates and skipping threshold. The main contribution of our work is carrying out the stochastic analysis while staying within the algorithmic framework of Zimmert and Seldin and keeping the adversarial regret bound almost unscathed. ", "page_idx": 4}, {"type": "text", "text": "The update rule for $x_{t}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t}=\\nabla\\bar{F}_{t}^{*}(-\\widehat{L}_{t}^{o b s})=\\arg\\operatorname*{min}_{x\\in\\Delta^{K-1}}\\langle\\widehat{L}_{t}^{o b s},x\\rangle+F_{t}(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{L}_{t}^{o b s}\\,=\\,\\sum_{s=1}^{t-1}\\widehat{\\ell}_{s}\\mathbb{1}(s+d_{s}\\,<\\,t)\\mathbb{1}(s\\,\\notin\\,S_{t-1})}\\end{array}$ isthe cumulative importane-wighted los estimate of observations that have arrived by time $t$ and have not been skipped. We use $S^{*}=S_{T}$ to denote the final set of skipped rounds at time $T$ ", "page_idx": 5}, {"type": "text", "text": "4 Regret Bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The following theorem provides best-of-both-worlds regret bounds for Algorithm 1. A proof is provided in Section 5 and a bound on $S^{*}$ can be found in Appendix $_\\mathrm{H}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. The pseudo-regret of Algorithm $^{\\,I}$ for any sequence of delays and losses satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}=\\mathcal{O}\\bigg(\\sqrt{K T}+\\operatorname*{min}_{\\mathcal{S}\\subseteq[T]}\\Big\\{|{\\cal S}|+\\sqrt{{\\mathscr{D}}_{\\bar{S}}\\log K}\\Big\\}+{\\cal S}^{*}+K\\widehat{\\sigma}_{\\operatorname*{max}}\\bigg),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widehat{\\sigma}_{\\operatorname*{max}}=\\operatorname*{max}_{t\\in[T]}\\left\\{\\widehat{\\sigma}_{t}\\right\\}$ is the maximal number of outstanding oservations after skipping and ", "page_idx": 5}, {"type": "equation", "text": "$$\nS^{*}=\\mathcal{O}\\left(\\operatorname*{min}\\left(d_{\\operatorname*{max}}K^{1/3}\\log K\\,,\\operatorname*{min}_{S\\subseteq[T]}\\left\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}K^{\\frac23}\\log K}\\right\\}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore,if the losses are stochastic,the pseudo-regret also satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}=\\mathcal{O}\\left(\\sum_{i\\neq i^{*}}\\left(\\frac{\\log T}{\\Delta_{i}}+\\frac{\\widehat{\\sigma}_{\\operatorname*{max}}}{\\Delta_{i}\\log K}\\right)+K\\widehat{\\sigma}_{\\operatorname*{max}}+S^{*}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Masoudian et al. (2022) provide an $\\Omega\\left(\\sqrt{K T}+\\operatorname*{min}_{S\\subset[T]}\\left\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}\\log K}\\right\\}\\right)$ regret lowerbound for adversarial environments with variable delays, which is matched within constants by the algorithm of (Zimmert and Seldin, 2020) for adversarial environments. Our algorithm matches the lower bound within a multiplicative factor of $K^{\\frac{1}{3}}$ on the delay-dependent term, which is the price we pay for obtaining a best-of-both-worlds guarantee. The price comes from a reduction of the skipping threshold of Zimmert and Seldin (2020) that we had to make to control the distribution drift that is due to the loss shift (see Appendix B.2). It is an open question whether this factor can be reduced. ", "page_idx": 5}, {"type": "text", "text": "In the stochastic regime, assuming that the delays in the first $\\sigma_{\\mathrm{max}}$ rounds are of order $T$ , and that the losses come from Bernoulli distributions with bias close to $\\frac{1}{2}$ , a trivial regret lower bound is $\\begin{array}{r}{\\Omega\\left(\\sigma_{\\operatorname*{max}}\\frac{\\sum_{i\\neq i*}\\Delta_{i}}{K}+\\sum_{i\\neq i*}\\frac{\\log T}{\\Delta_{i}}\\right)}\\end{array}$ . This bound is almost matched by the algorithm of Joulani et al. (2013) for th stochastic regime only Our bound has some extra terms, most notably ii\\*:ng K and $S^{*}$ . It is an open question whether these terms are inevitable or can be reduced. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 provides three major improvements relative to the results of Masoudian et al. (2022): (1) it requires no advance knowledge of $d_{\\mathrm{max}}$ ; (2) it replaces terms dependent on $d_{\\mathrm{max}}$ by terms dependent on $\\widehat{\\sigma}_{\\mathrm{max}}$ , which never exceeds $d_{\\mathrm{max}}$ , and in some cases may be significantly smaller; and (3) it makes skipping possible and beneficial, making the algorithm robust to a small number of excessively large delays and replacing $\\sqrt{D\\log K}$ term with $\\left.\\operatorname*{min}_{S\\subseteq[T]}\\left\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}K^{\\frac23}\\log K}\\right\\}$ , which is never much larger, but in some cases significantly smaller. ", "page_idx": 5}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present a proof of Theorem 1. We begin with the stochastic part of the bound in Section 5.1, followed by the adversarial part in Section 5.2. ", "page_idx": 5}, {"type": "text", "text": "5.1  Stochastic Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start by defining the drifted regret $\\begin{array}{r}{\\overline{{R e g}}_{T}^{d r i f t}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\langle x_{t},\\widehat{\\ell}_{t}^{o b s}\\rangle-\\widehat{\\ell}_{t,i_{T}^{*}}^{o b s}\\right)\\right]}\\end{array}$ where $\\widehat{\\ell}_{t}^{o b s}=$ $\\begin{array}{r}{\\sum_{s=1}^{t}\\widehat{\\ell}_{s}\\mathbb{1}(s+\\widehat{d}_{s}=t)\\mathbb{1}(s\\notin\\mathcal{S}_{t})}\\end{array}$ is the cumulative vector of losses received at time $t$ . Lemma 2 is thefrst majorcontribtionestablishing arelationshipbetwee $\\overline{{R e g}}_{T}^{d r i f t}$ and the actual regret $\\overline{{R e g}}_{T}$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 2 (Drift of the Drifted Regret). Let $\\sigma_{\\mathrm{max}}^{t}=\\operatorname*{max}_{s\\in[t]}\\left\\{\\widehat{\\sigma}_{s}\\right\\}$ .Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}^{d r i f t}\\geq\\frac{1}{4}\\overline{{R e g}}_{T}-2K\\sum_{t=1}^{T}\\left(\\lambda_{t,t+\\widehat{d}_{t}}+\\lambda_{t,t+\\widehat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}}\\right)-\\frac{\\sigma_{\\operatorname*{max}}}{4}-S^{*},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $S^{*}$ is the total number of rounds skipped by the algorithm. ", "page_idx": 6}, {"type": "text", "text": "The core of Lemma 2 is based on controlling the distribution drift using implicit exploration and skipping. In prior work on bounded delays the relation between $\\overline{{R e g}}_{T}^{d r i f t}$ and $\\overline{{R e g}}_{T}$ was achieved by shifting all the arrivals by $d_{\\mathrm{max}}$ , leading to an additive term of order $d_{\\mathrm{max}}$ . This approach fails for unbounded delays, because a single delay of order $T$ prevents shifting and leads to linear regret. We address the challenge by introducing a procedure to rearrange the arrivals (Algorithm 2 below) and advanced control of the drift (Lemma 3 below). A proof of Lemma 2 is provided at the end of the section. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2: Greedy Rearrangement ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1 Initialize $\\boldsymbol{v}_{t}^{n e w}=0$ for all $t=1,\\dots,T+d_{\\operatorname*{max}}^{T}$   \n2 for $t=1,\\dots,T\\,.$ do   \n3 for $s=1,\\ldots,t:s+\\widehat{d}_{s}=t\\,\\mathrm{c}$ do   \n45 $\\pi(s)\\in[t,t+d_{\\operatorname*{max}}^{t}]$ $s$ $\\pi(s)$ $\\ v_{\\pi(s)}^{n e w}=0$ $\\ v_{\\pi(s)}^{n e w}=1$ ", "page_idx": 6}, {"type": "text", "text": "The drift control lemma (Lemma 3) is the second major contribution of the paper. Prior work on bounded delays controlled the drift by slowing the learning rate in accordance with. $d_{\\mathrm{max}}$ This does not work for highly varying delays, because slow learning rates prevent learning, whereas fast learning rates fail to control the drift. Lemma 3 relies on implicit exploration terms in the loss estimators in equation (1) and on skipping of excessive delays, leaving the learning rates intact. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Drift Control Lemma). Let $d_{\\mathrm{max}}^{t}$ be the skipping threshold at time $t$ Then, for any $i\\in[K]$ and $s,t\\in[T]$ where $s\\leq t$ and $t-s\\leq d_{\\operatorname*{max}}^{t}$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{t,i}\\leq4\\operatorname*{max}(x_{s,i},\\lambda_{s,t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof is based on introduction of an intermediate variable $\\widetilde{x}_{s}=\\nabla\\bar{F}_{s}^{*}(-\\widehat{L}_{t-1}^{o b s})$ , which is based on the regularizer from round $s$ and the loss estimate from round $t$ . It exploits the implicit exploration term $\\lambda_{s,t}$ to show that at max(t,.\u2264 2 and skipping to show that \u2264 2. The later implies that mbnatwiththeferest fdsf tht steps are provided in Appendix $\\mathbf{B}$ ", "page_idx": 6}, {"type": "text", "text": "Given Lemmas 2 and Lemma 3, we apply standard FTRL analysis, similar to Masoudian et al. (2022), to obtain an upper bound for RegT $\\overline{{R e g}}_{T}^{d r i f t}$ . Specifically, in Appendix A we show that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\overline{{\\mathrm{t}e}}g_{T}^{d r i f t}\\leq\\mathbb{E}\\Bigg[a\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\neq i^{*}}\\eta_{t}x_{t,i}^{1/2}+b\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\gamma_{t+\\hat{d}_{t}}(v_{t+\\hat{d}_{t}}-1)x_{t,i}\\Delta_{i}+c\\displaystyle\\sum_{t=2}^{T}\\sum_{i=1}^{K}\\displaystyle\\frac{\\hat{\\sigma}_{t}\\gamma_{t}x_{t,i}\\log(1/x_{t,i})}{\\log K}}\\\\ &{\\quad}&{+\\displaystyle\\mathcal{O}\\left(K\\displaystyle\\sum_{t=1}^{T}\\lambda_{t,t+\\hat{d}_{t}}\\right),\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $a,b,c\\geq0$ are constants and $\\begin{array}{r}{v_{t}=\\sum_{s=1}^{t}\\mathbb{1}\\left(s+\\widehat{d}_{s}=t\\right)}\\end{array}$ is the number of arrivals at time $t$ (if a round $s$ is skipped at time $t$ it counts as an \u201cempty\u201d\u2019 arrival with loss estimate set to zero). By combining (4) with Lemma 2, we obtain ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathrm{3}e g}}_{T}\\leq\\mathbb{E}\\Bigg[2a\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\neq i^{*}}\\eta_{t}x_{t,i}^{1/2}+2b\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\gamma_{t+\\hat{d}_{t}}(v_{t+\\hat{d}_{t}}-1)x_{t,i}\\Delta_{i}+2c\\displaystyle\\sum_{t=2}^{T}\\sum_{i=2}^{K}\\displaystyle\\sum_{i=1}^{\\hat{\\sigma}_{t}\\gamma_{t}x_{t,i}\\log(1/x_{t,i})}}\\\\ &{\\qquad\\quad+\\mathcal{O}\\left(K\\displaystyle\\sum_{t=1}^{T}\\left(\\lambda_{t,t+\\hat{d}_{t}}+\\lambda_{t,t+\\hat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}}\\right)+\\sigma_{\\operatorname*{max}}+S^{*}\\right).\\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then we apply a self-bounding analysis, similar to Masoudian et al. (2022), and get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}=\\mathcal{O}\\Bigg(\\sum_{i\\neq i^{*}}\\bigg(\\frac{1}{\\Delta_{i}}\\log(T)+\\frac{\\sigma_{\\operatorname*{max}}}{\\Delta_{i}\\log K}\\bigg)+\\sigma_{\\operatorname*{max}}+K\\sum_{t=1}^{T}\\left(\\lambda_{t,t+\\hat{d}_{t}}+\\lambda_{t,t+\\hat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}}\\right)+S^{*}\\Bigg).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The details of the self-bounding analysis are provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "The stochastic analysis is completed by the following lemma, which bounds the sum of implicit exploration terms above. It constitutes the third key result of the paper and shows that the bias from implicit exploration does not deteriorate neither the stochastic nor the adversarial bound. The proof is based on a careful study of the evolution of $\\mathcal{D}_{t}$ throughout the game, and is deferred to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4 (Summation Bound). For all $s\\in[T]$ let $\\begin{array}{r}{\\mathcal{D}_{s}=\\sum_{r=1}^{s}\\widehat{\\sigma}_{r}}\\end{array}$ and $\\lambda_{s,t}=e^{-\\frac{\\mathcal{D}_{t}}{\\mathcal{D}_{t}-\\mathcal{D}_{s}}}$ ,then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\Big(\\lambda_{t,t+\\widehat{d}_{t}}+\\lambda_{t,t+\\widehat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}}\\Big)=\\mathcal{O}(\\widehat{\\sigma}_{\\operatorname*{max}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof of Lemma 2 (Mrift of the Drifted Regret) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We start with the definition of the drifted regret. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\overline{{\\widehat{\\tau}_{e g_{T}}}}^{d r i f t}=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\langle x_{t},\\widehat{\\ell}_{t}^{b s}\\rangle-\\widehat{\\ell}_{t,i_{T}^{b}}^{b s}\\right)\\right]=\\sum_{t=1}^{T}\\sum_{s=\\widehat{d}_{s}=t}^{T}\\sum_{i=1}^{K}{\\mathbb{E}\\left[\\frac{\\ell_{s,i}x_{s,i}x_{t,i}}{\\operatorname*{max}\\left\\{x_{s,i},\\lambda_{s,t}\\right\\}}-\\frac{\\ell_{s,i_{T}^{b}}x_{s,i_{T}^{b}}x_{t,i}}{\\operatorname*{max}\\left\\{x_{s,i_{T}^{b}},\\lambda_{s,t}\\right\\}}\\right.}}\\\\ {\\displaystyle\\geq\\sum_{t=1}^{T}\\sum_{s=\\widehat{d}_{s+\\widehat{d}_{s+\\ell}}}\\sum_{i=1}^{K}{\\mathbb{E}\\left[\\frac{\\ell_{s,i}x_{s,i}x_{t,i}}{\\operatorname*{max}\\left\\{x_{s,i},\\lambda_{s,t}\\right\\}}-\\ell_{s,i_{T}^{b}}x_{t,i}\\right]}}\\\\ {\\displaystyle}&{\\displaystyle\\geq\\sum_{t=1}^{T}\\sum_{s+\\widehat{d}_{s+\\ell}}\\sum_{i=1}^{K}{\\mathbb{E}\\left[\\underbrace{\\frac{\\ell_{s,i}x_{s,i}x_{t,i}}{\\operatorname*{max}\\left\\{x_{s,i},\\lambda_{s,t}\\right\\}}}_{\\displaystyle\\sum_{k},\\qquad k,i\\right\\}-\\ell_{s,i_{T}^{b}}x_{t,i}\\right]}-\\mathcal{S}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that when taking the expectation, we rely on the fact that $\\widehat{\\ell}_{s}$ with $s+\\widehat{d_{s}}=t$ does not affect $x_{t}$ If max $\\{x_{s,i},\\lambda_{s,t}\\}=\\bar{x_{s,i}}$ , then $\\star=\\ell_{s,i}x_{t,i}$ , otherwise ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\star=\\ell_{s,i}x_{t,i}-\\frac{\\ell_{s,i}x_{t,i}\\left(\\lambda_{s,t}-x_{s,i}\\right)}{\\lambda_{s,t}}\\geq\\ell_{s,i}x_{t,i}-\\frac{4\\lambda_{s,t}(\\lambda_{s,t}-x_{s,i})}{\\lambda_{s,t}}\\geq\\ell_{s,i}x_{t,i}-4\\lambda_{s,t},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the first inequality uses $x_{t,i}\\leq4\\operatorname*{max}(x_{s,i},\\lambda_{s,t})=4\\lambda_{s,t}$ by Lemma 3, and $\\ell_{s,i}\\geq1$ , and the second inequality follows by $x_{s,i}\\geq0$ . Plugging (7) into (6) gives ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{R e g}}_{T}^{d r i f t}\\geq\\displaystyle\\sum_{t=1}^{T}\\sum_{s+\\widehat{d}_{s}=t}\\sum_{i=1}^{K}\\mathbb{E}\\left[\\left(\\ell_{s,i}x_{t,i}-4\\lambda_{s,t}-\\ell_{s,i_{T}^{*}}x_{t,i}\\right)\\right]-S^{*}}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{s+\\widehat{d}_{s}=t}\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}\\right]-4K\\displaystyle\\sum_{t=1}^{T}\\sum_{s+\\widehat{d}_{s}=t}\\mathbb{E}\\left[\\lambda_{s,t}\\right]-S^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It suffices to give a lower bound for $R_{T}$ in terms of the actual regret $\\overline{{R e g}}_{T}$ . The difference between $R_{T}$ and $\\overline{{R e g}}_{T}$ is that $\\begin{array}{r}{\\overline{{R e g}}_{T}\\,=\\,\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}\\right]}\\end{array}$ whereas in $R_{T}$ the sum $\\textstyle\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}$ .s multiplied by the number of arrivals $\\begin{array}{r}{v_{t}=\\sum_{s=1}^{t}\\mathbb{1}\\left(s+\\widehat{d}_{s}=t\\right)}\\end{array}$ at time $t$ , and $\\upsilon_{t}$ might be larger than one or zero due to delays. ", "page_idx": 7}, {"type": "text", "text": "Our main idea here is to leverage the drift control lemma to provide a lower bound for $R_{T}$ in terms of $\\overline{{R e g}}_{T}$ . Specifically, by Lemma 3 for all $r\\in[0,d_{\\mathrm{max}}^{t}]$ , we have $\\begin{array}{r}{\\operatorname*{max}(x_{t,i},\\lambda_{t,t+r})\\geq\\frac{1}{4}x_{t+r,i}.}\\end{array}$ which implies $\\begin{array}{r}{x_{t,i}\\geq\\frac{1}{4}x_{t+r,i}-\\lambda_{t,t+r}}\\end{array}$ Thus, we btain the following bound for any $r\\in[0,d_{\\mathrm{max}}^{t}]$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}\\geq\\frac14\\sum_{i=1}^{K}\\Delta_{i}x_{t+r,i}-K\\lambda_{t,t+r}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In Algorithm 2 we provide a greedy procedure to rearrange the arrivals by postponing some arrivals to future rounds to create a hypothetical rearranged sequence with at most one arrival at each round. Colliding arrivals are postponed to the first available (unoccupied) slot in the future. In Lemma 5 belowwe show that arival originallyreceived a time $t$ stays in the $[t,t+\\sigma_{\\mathrm{max}}^{t}]$ interval (note that $\\sigma_{\\mathrm{max}}^{t}\\,\\leq\\,d_{\\mathrm{max}}^{t})$ When an observation from round $s$ is postponed from arriving at round $t$ to arriving at round $t+r$ for $r\\,\\in\\,[0,d_{\\mathrm{max}}^{t}]$ by (9) it is equivalent to replacing $\\textstyle\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}$ by $\\begin{array}{r}{\\frac{1}{4}\\sum_{i=1}^{K}\\Delta_{i}x_{t+r,i}-K\\lambda_{t,t+r}}\\end{array}$ $R_{T}$ Note that Algorithm 2 may pushan arival toaround larger than $T$ which is equivalent to replacing $\\textstyle\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}$ by zero. ", "page_idx": 8}, {"type": "text", "text": "Let $\\upsilon_{t}^{n e w}$ for all $t\\in[T+d_{\\operatorname*{max}}^{T}]$ be the total arivalsa ime $t$ after the rearangemnt, a t $\\pi(t)$ be the round to which we have mapped round $t$ for all $t\\in[T]$ . Then for any rearrangement ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}=\\mathbb{E}\\left[\\sum_{t=1}^{T}v_{t}\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}\\right]\\geq\\mathbb{E}\\left[\\sum_{t=1}^{T}{\\frac{1}{4}}v_{t}^{n e w}\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}-K\\sum_{t=1}^{T}\\lambda_{t,\\pi(t)}\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The following lemma provides properties of the rearrangement procedure. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5. Let $\\sigma_{\\mathrm{max}}^{t}\\,=\\,\\mathrm{max}_{s\\in[t]}\\left\\{\\widehat{\\sigma}_{s}\\right\\}$ ThenAloriensref $t\\,\\in\\,[T+d_{\\operatorname*{max}}^{T}]$ that $\\boldsymbol{v}_{t}^{n e w}\\in\\{0,1\\}$ Furthermore, for any round $t\\in[T]$ it keeps all the arrivals at time $t$ in the interval $[t,t+\\sigma_{\\mathrm{max}}^{t}]$ suchthat $\\forall s\\leq t:s+\\widehat{d}_{s}=t\\Rightarrow\\bar{\\pi(s)}-t\\leq\\sigma_{\\operatorname*{max}}^{t}.$ ", "page_idx": 8}, {"type": "text", "text": "$\\sigma_{\\mathrm{max}}^{T}$ be no arivals ater T + max andT+ $\\begin{array}{r}{\\sum_{t=1}^{T+\\sigma_{\\operatorname*{max}}^{T}}v_{t}^{n e w}=\\sum_{t=1}^{T}v_{t}=T}\\end{array}$ which implies there are at most $\\sigma_{\\mathrm{max}}^{T}$ zero arialashrveatstil ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{g}\\left[\\sum_{t=1}^{T}\\boldsymbol{v}_{t}^{n e w}\\sum_{i=1}^{K}\\Delta_{i}\\boldsymbol{x}_{t,i}\\right]=\\overline{{R e g}}_{T}-\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}(\\boldsymbol{v}_{t}^{n e w}=0)\\sum_{i=1}^{K}\\Delta_{i}\\boldsymbol{x}_{t,i}\\right]}}\\\\ &{}&{\\leq\\overline{{R e g}}_{T}-\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}(\\boldsymbol{v}_{t}^{n e w}=0)\\right]\\leq\\overline{{R e g}}_{T}-\\mathbb{E}\\left[\\sigma_{\\operatorname*{max}}^{T}\\right]\\leq\\overline{{R e g}}_{T}-\\sigma_{\\operatorname*{max}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the frst equality uses the definition of $\\begin{array}{r}{\\overline{{R e g}}_{T}=\\mathbb{E}[\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\Delta_{i}x_{t,i}]}\\end{array}$ and that $\\forall t\\in[T]:$ $\\boldsymbol{v}_{t}^{n e w}\\in\\{0,1\\}$ ", "page_idx": 8}, {"type": "text", "text": "Sine $\\forall t\\in[T]:\\pi(t)\\leq t+\\widehat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}$ wehave $\\lambda_{t,\\pi(t)}\\le\\lambda_{t,t+\\widehat{d}_{t}+\\sigma_{\\operatorname*{max}}^{t}}$ Together with 1) 10) and (8) it completes the proof. ", "page_idx": 8}, {"type": "text", "text": "5.2 Adversarial Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The adversarial analysis is similar to the analysis of Zimmert and Seldin (2020, Theorem 2). In Appendix $\\mathrm{G}$ weshowthat ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}=\\mathcal{O}\\left(\\sqrt{K T}+\\operatorname*{min}_{S\\subseteq[T]}\\Big\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}\\log K}\\Big\\}+S^{*}+K\\sum_{t=1}^{T}\\lambda_{t,t+\\hat{d}_{t}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the first two terms originate from the analysis of Zimmert and Seldin due to structural similarity of the algorithm, $S^{*}$ isdue toadjustd skiping threold and $K\\textstyle\\sum_{t=1}^{T}\\lambda_{t,t+\\widehat{d}_{t}}.$ is duet impit exploration bias and is bounded by Lemma 4. The proof is completed by the following bound on $S^{*}$ \uff0c which is shown in Appendix $\\mathrm{H}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S^{*}=\\mathcal{O}\\left(\\operatorname*{min}\\left(d_{\\operatorname*{max}}K^{\\frac{2}{3}}\\log K\\;,\\operatorname*{min}_{S\\subseteq[T]}\\left\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}K^{\\frac{2}{3}}\\log K}\\right\\}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have successfully addressed the challenge of handling varying and potentially unbounded delays in best-of-both-worlds setting. The success was based on three technical innovations, which may be interesting in their own right: (1) A relation between the drifted and the standard regret under unbounded delays (given by Lemma 2, Algorithm 2, and Lemma 5); (2) A novel control of distribution drift based on implicit exploration and skipping that does not alter the learning rates and exhibits efficiency under highly varying delays (Lemma 3); and (3) An implicit exploration scheme applicable in best-of-both-worlds setting (Lemma 4). ", "page_idx": 9}, {"type": "text", "text": "The work leads to several directions for future research. One question is whether the best-of-bothworlds bounds could be improved further. In particular, whether the $K^{\\frac{1}{3}}$ term in the adversarial regret bound could be reduced or eliminated. The term arose due to the need to decrease the skipping threshold of Zimmert and Seldin (2020) to control the distribution drift. It would also be yaluable to explore whetherit is possibeto reduce the S\\* term andreduce oreliminate theig K moog term in the stochastic bound, or to derive lower bounds showing that these terms are unavoidable. Another interesting direction is to find more applications for implicit exploration and skipping in the context of best-of-both-worlds bounds. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "SM and YS acknowledge partial support by the Independent Research Fund Denmark, grant number 9040-00361B. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In Advances in Neural Information Processing Systems (NeurIPS), 2015.   \nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 2002.   \nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32, 2002b.   \nIlai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, and Jose Blanchet. Online exp3 learning in adversarial bandits with delayed feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nNicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Delay and cooperation in nonstochastic bandits. Journal of Machine Learning Research, 20:1-38, 2019.   \nStephen G. Eick. The two-armed bandit with delayed responses. The Annals of Statistics, 1988.   \nShinji Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Proceedings of the Conference on Learning Theory (COLT), 2021.   \nTiancheng Jin, Tal Lancewicki, Haipeng Luo, Yishay Mansour, and Aviv Rosenberg. Near-optimal regret for adversarial MDP with delayed bandit feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nPooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In Proceedings of the International Conference on Machine Learning (ICML), 2013.   \nSaeed Masoudian, Julian Zimmert, and Yevgeny Seldin. A best-of-both-worlds algorithm for bandits with delayed feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nGergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In Advances in Neural Information Processing Systems, 2015.   \nGergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos._ Online markov decision processes under bandit feedback. In Advances in Neural Information Processing Systems, 2010.   \nGergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos. Online markov decision processes under bandit feedback. IEEE Transactions on Automatic Control, 59:676-691, 2014.   \nFrancesco Orabona. A modern introduction to online learning, 2022. https : //arxiv . org/abs/ 1912.13213.   \nRichard Simon. Adaptive treatment assignment methods and clinical trials. Biometrics, 33, 1977.   \nWilliam R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25, 1933.   \nTobias Sommer Thune, Nicolo Cesa-Bianchi, and Yevgeny Seldin. Nonstochastic multiarmed bandits with unrestricted delays. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nJulian Zimmert and Yevgeny Seldin. An optimal algorithm for adversarial bandits with arbitrary delays. In Proceedings on the International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \nJulian Zimmert and Yevgeny Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Details of the Drifted Regret Analysis ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section we prove the bound on drifted regret in equation (4). The derivation is same as the one by Masoudian et al. (2022), however, for the sake of completeness we reproduce it here. The analysis follows the standard FTRL approach, decomposing the drifted pseudo-regret into penalty and stability terms as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}^{d r i f t}=\\mathbb{E}\\left[\\underbrace{\\sum_{t=1}^{T}\\langle x_{t},\\widehat{\\ell}_{t}^{o b s}\\rangle+\\bar{F}_{t}^{*}(-\\widehat{L}_{t+1}^{o b s})-\\bar{F}_{t}^{*}(-\\widehat{L}_{t}^{o b s})}_{s t a b i l i t y}\\right]+\\mathbb{E}\\left[\\underbrace{\\sum_{t=1}^{T}\\bar{F}_{t}^{*}(-\\widehat{L}_{t}^{o b s})-\\bar{F}_{t}^{*}(-\\widehat{L}_{t+1}^{o b s})}_{p e n a l t y}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The penalty term is bounded by the following inequality, derived by Abernethy et al. (2015) ", "page_idx": 11}, {"type": "equation", "text": "$$\np e n a l t y\\leq\\sum_{t=2}^{T}\\left(F_{t-1}(x_{t})-F_{t}(x_{t})\\right)+F_{T}(\\mathrm{e}_{i\\frac{\\ast}{T}})-F_{1}(x_{1}),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $\\mathrm{e}_{i_{T}^{*}}$ represents the unit vector in $\\mathbb{R}^{K}$ with the $i_{T}^{*}$ -th element being one and zero elsewhere. This leads to the following bound for penalty term ", "page_idx": 11}, {"type": "equation", "text": "$$\np e n a l t y\\leq\\mathcal{O}\\left(\\sum_{t=2}^{T}\\sum_{i\\neq i^{*}}\\eta_{t}x_{t,i}^{\\frac{1}{2}}+\\sum_{t=2}^{T}\\sum_{i=1}^{K}\\frac{\\sigma_{t}\\gamma_{t}x_{t,i}\\log\\left(1/x_{t,i}\\right)}{\\log K}\\right),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where we substitute the explicit form of the regularizer into (12) and exploit the properties $\\eta_{t}^{-1}-$ ", "page_idx": 11}, {"type": "text", "text": "For the stability term, following a similar analysis as presented by Masoudian et al. (2022, Lemma 5), but incorporating implicit exploration terms, for any $\\overline{{\\alpha_{t}}}\\leq\\gamma_{t}^{-1}$ weobtain ", "page_idx": 11}, {"type": "equation", "text": "$$\ns t a b i l i t y\\le\\sum_{t=1}^{T}\\sum_{i=1}^{K}2f_{t}^{\\prime\\prime}(x_{t,i})^{-1}(\\widehat{\\ell}_{t,i}^{o b s}-\\alpha_{t})^{2}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Let $A_{t}\\,=\\,\\left\\{s\\leq t:s+\\widehat{d}_{s}=t\\right\\}$ then due to the chic of skipping threhld, $\\begin{array}{r}{\\alpha_{t}\\,=\\,\\sum_{s\\in A_{t}}\\bar{\\ell}_{s,t}}\\end{array}$ satisfies the condition $\\alpha_{t}\\leq\\gamma_{t}^{-1}$ ,where ) ) Thus wehave ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle s t a b i l i t y\\leq\\sum_{t=1}^{T}\\sum_{i=1}^{K}2f_{t}^{\\prime\\prime}(x_{t,i})^{-1}\\left(\\sum_{s\\in A_{t}}\\widehat{\\ell}_{s,i}-\\bar{\\ell}_{s,t}\\right)^{2}}}\\\\ {{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\sum_{s\\in A_{t}}2f_{t}^{\\prime\\prime}(x_{t,i})^{-1}\\left(\\widehat{\\ell}_{s,i}-\\bar{\\ell}_{s,t}\\right)^{2}}}\\\\ {{\\displaystyle~+\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\sum_{r,s\\in A_{t},r\\neq s}2f_{t}^{\\prime\\prime}(x_{t,i})^{-1}\\left(\\widehat{\\ell}_{s,i}-\\bar{\\ell}_{s,t}\\right)\\left(\\widehat{\\ell}_{r,i}-\\bar{\\ell}_{r}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "For brevity we define $z_{t,i}=f_{t}^{'\\prime}(x_{t,i})^{-1}$ and $m_{s,i}^{t}=\\operatorname*{max}\\left\\{x_{s,i},\\lambda_{s,t}\\right\\}$ for any $s\\leq t$ and $i\\in[K]$ We begin bounding $S_{1}$ by replacing definition of loss estimators from (i) and get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[S_{1}]=\\sum_{t=1}^{T}\\sum_{s=1}^{K}\\sum_{\\ell=1}^{2\\ensuremath{\\mathbb{E}}}\\left[\\varepsilon_{t,i}\\left(\\frac{\\ell_{s,l}}{m_{s,i}^{\\prime}},\\frac{\\mathbf{I}(I_{s}=i)}{m_{s,i}^{\\prime}}-\\frac{\\varepsilon_{t,i}\\ell_{s,l}}{m_{s,L}^{\\prime}\\sum_{j=1}^{K}z_{\\ell,j}}\\right)^{2}\\right]}}\\\\ &{\\leq\\sum_{t=1}^{T}\\sum_{s=1}^{K}\\sum_{s=1}^{2\\ensuremath{\\mathbb{E}}}\\left[z_{i,t}\\left(\\frac{\\mathbf{I}(I_{s}=i)}{m_{s,i}^{\\prime}}-\\frac{z_{i,t}}{m_{s,i}^{\\prime}\\sum_{j=1}^{K}z_{\\ell,j}}\\right)^{2}\\right]}\\\\ &{=\\sum_{t=1}^{T}\\sum_{s=1}^{T}\\frac{\\ensuremath{\\mathbb{E}}}{\\underset{s=1}{\\underbrace{\\mathrm{i}}}}\\left[z_{i,t}\\left(\\frac{\\mathbf{I}(I_{s}=i)}{m_{s,i}^{\\prime}}-\\frac{z_{i,t}\\mathbf{I}(I_{s}=i)}{m_{s,i}^{\\prime}m_{s,L}^{\\prime}\\sum_{j=1}^{K}z_{\\ell,j}}\\right)\\right]}\\\\ &{+\\sum_{t=1}^{T}\\sum_{s\\in A_{1}}^{2\\ensuremath{\\mathbb{E}}}\\left[\\left(\\frac{z_{i,t}^{2}}{m_{s,i}^{\\prime}\\sum_{j=1}^{K}z_{\\ell,j}}-\\sum_{i=1}^{K}\\frac{z_{i,t}\\mathbf{I}_{s}=i\\mathbf{I}(I_{s}=i)}{m_{s,i}^{\\prime}m_{s,i}^{\\prime}\\sum_{j=1}^{K}z_{\\ell,j}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Where the first inequality uses $\\ell_{s,I_{s}}\\leq1$ We show that $S_{1}^{2}$ has negative contribution to $S_{1}$ by taking expectation w.r.t. $I_{s}$ as the following ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{1}^{2}=\\sum_{t=1}^{T}\\sum_{s\\in A_{t}}\\mathbb{E}\\left[\\sum_{i=1}^{K}\\frac{z_{t,i}^{2}x_{s,i}}{{m_{s,i}^{t}}^{2}(\\sum_{j=1}^{K}z_{t,j})}-\\sum_{i=1}^{K}\\frac{z_{t,i}^{2}x_{s,i}}{{m_{s,i}^{t}}^{2}\\sum_{j=1}^{K}z_{t,j}}\\right]=0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus we only need to bound $S_{1}^{1}$ , for which we take expectation w.r.t. $I_{s}$ and separate $i^{*}$ from the other arms to get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\theta}^{\\theta}\\lefteqn{\\frac{\\sum_{i=1}^{K}\\theta\\left[\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}^{2}}-\\frac{\\alpha_{i}}{\\alpha_{i}\\alpha_{i}\\alpha_{j}}\\right)\\right]}{\\theta^{2}}}\\quad}\\\\ &{\\leq\\sum_{i=1}^{K}\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}^{2}}\\right)}{\\alpha_{i}\\alpha_{j}^{2}}+\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}-\\frac{\\alpha_{i}}{\\alpha_{i}\\alpha_{j}}\\right)}{\\alpha_{i}\\alpha_{j}^{2}}\\right]\\right.}\\\\ &{\\quad\\times\\left.\\sum_{i=K}\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)}{\\alpha_{i}\\alpha_{j}}+\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1-\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)}{\\alpha_{i}\\alpha_{j}}\\right]\\right]}\\\\ &{\\leq\\sum_{i=K}^{K}\\mathbb{E}\\left[\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)+\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1-\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)}{\\alpha_{i}\\alpha_{j}}\\left(1-\\frac{\\alpha_{i}}{\\alpha_{i}\\alpha_{j}}\\right)\\right]\\right]}\\\\ &{\\leq\\sum_{i=K}^{K}\\mathbb{E}\\left[\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)+\\mathbb{E}\\left[\\frac{\\sin_{\\theta}\\left(1-\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\alpha_{j}}\\right)}{\\alpha_{i}\\alpha_{j}}\\right]\\right]}\\\\ &{\\leq\\sum_{i=K}^{K}\\mathbb{E}\\left[\\sin_{\\theta}\\left(1\\frac{I_{i}(L_{1},\\alpha)}{\\alpha_{i}\\\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the second inequlity uses $z_{t,i}\\,=\\,f_{t}^{\\prime\\prime}(x_{t,i})^{-1}\\,\\le\\,\\eta_{t}x_{t,i}^{3/2}$ along $x_{t,i}\\,\\leq\\,m_{s,i}^{t}$ from Lemma 3, the third inequality is due the fact that $\\begin{array}{r}{z_{t,i^{*}}\\left(1-\\frac{z_{t,i^{*}}}{\\sum_{j=1}^{K}z_{t,j}}\\right)}\\end{array}$ is an increasing function in terms of both $z_{t,i^{*}}$ and $\\sum_{i\\neq i^{*}}z_{t,i}$ and we substitute $z_{t,i^{*}}\\leq\\eta_{t}x_{t,i^{*}}^{3/2}$ and $\\begin{array}{r}{\\sum_{j\\neq i^{*}}z_{t,j}\\,\\le\\,\\sum_{j\\neq i^{*}}\\eta_{t}x_{t,j}^{3/2}\\le}\\end{array}$ $\\eta_{t}(1-x_{t,i^{*}})^{3/2}$ , the fourth inequality is due to $(1-a)^{3/2}+a^{3/2}\\leq2^{-1/2}$ , the fth and the sixth inequalitiesrly on Lemma 3 and fiallytelast inequalityis fllowed by $\\forall i:x_{s,i}\\leq x_{s,i}^{1/2}$ and tht $\\eta_{t}\\leq\\eta_{s}$ . Combining bounds for $S_{1}^{1}$ and $S_{1}^{2}$ gives the following bound for $S_{1}$ ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[S_{1}]\\le\\mathcal{O}\\left(\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\eta_{t}\\mathbb{E}[x_{t,i}^{1/2}]+\\sum_{t=1}^{T}K\\lambda_{t,t+\\widehat{d}_{t}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For $S_{2}$ , we take expectation with respect to $I_{s},I_{r}$ , and randomness of losses, all separately to get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}[S_{2}]=\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{K}\\sum_{r,s\\in A_{t},r\\neq s}2\\mathbb{E}\\left[z_{t,i}\\left(\\widehat{\\ell}_{s,i}-\\bar{\\ell}_{s}\\right)\\left(\\widehat{\\ell}_{r,i}-\\bar{\\ell}_{s}\\right)\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{K}\\sum_{r,s\\in A_{t},r\\neq s}2\\mathbb{E}\\left[z_{t,i}\\left(\\frac{\\mu_{i}x_{s,i}}{m_{s,i}^{t}}-\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}x_{s,j}/m_{s,j}^{t}}{\\sum_{j=1}^{K}z_{t,j}}\\right)\\left(\\frac{\\mu_{i}x_{r,i}}{m_{r,i}^{t}}-\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}x_{r,j}}{\\sum_{j=1}^{K}z_{t,j}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For simplicity we define es, = \u03bc $\\begin{array}{r}{\\epsilon_{s,i}^{t}=\\mu_{i}-\\frac{\\mu_{i}x_{s,i}}{m_{s,i}^{t}}}\\end{array}$ for any $s\\leq t$ and any $i\\in[K]$ for which we have the following bounds ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\epsilon_{s,i}^{t}\\leq\\frac{\\lambda_{s,t}}{m_{s,i}^{t}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We then continue from 15 and bound it as the following ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{=\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{r,\\neq s}\\displaystyle\\sum_{i=1}^{K}2\\mathbb{E}\\left[z_{t,i}\\left(\\mu_{i}-\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}}{\\sum_{j=1}^{K}z_{t,j}}-\\epsilon_{s,i}^{t}+\\frac{\\sum_{j=1}^{K}z_{t,j}\\epsilon_{s,j}^{t}}{\\sum_{j=1}^{K}z_{t,j}}\\right)\\Bigg(\\mu_{i}-\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}}{\\sum_{j=1}^{K}z_{t,j}}-\\epsilon_{r,i}^{t}-\\sum_{j=1}^{K}z_{t,j}\\Bigg)\\right]}}\\\\ {{\\le\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{r,\\neq s}\\ _{i}^{2\\mathbb{E}}\\left[\\underbrace{\\sum_{i=1}^{K}z_{t,i}\\left(\\mu_{i}-\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}}{\\sum_{j=1}^{K}z_{t,j}}\\right)^{2}}_{S_{2}^{1}}+\\underbrace{\\sum_{j=1}^{K}z_{t,i}\\epsilon_{s,i}^{t}\\epsilon_{r,i}^{t}}_{S_{2}^{2}}+2z_{t,i}(\\epsilon_{s,i}^{t}+\\epsilon_{r,i}^{t})}+\\underbrace{\\sum_{i=1}^{K}z_{t,i}\\epsilon_{s,i}^{t}\\epsilon_{s,i}^{t}}_{\\sum_{j=1}^{K}z_{t,j}}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality holds because we ignore the negative terms after multiplication and that $|(\\mu_{i}-$ $\\frac{\\sum_{j=1}^{K}z_{t,j}\\mu_{j}}{\\sum_{j=1}^{K}z_{t,j}}\\big)|\\leq1$ $S_{2}^{1}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}^{1}=\\displaystyle\\sum_{i=1}^{K}z_{i,i}\\left(\\mu_{i}-\\frac{\\sum_{j=1}^{K}z_{i,j}\\mu_{j}}{\\sum_{j=1}^{K}z_{i,j}}\\right)^{2}}\\\\ &{=\\displaystyle\\sum_{i=1}^{K}z_{i,i}\\mu_{i}^{2}-\\frac{\\left(\\sum_{j=1}^{K}z_{i,j}\\mu_{j}\\right)^{2}}{\\sum_{i=1}^{K}z_{i,i}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{K}z_{i,i}\\mu_{i}^{2}-\\frac{\\left(\\sum_{j=1}^{K}z_{i,i}\\mu_{i}\\right)^{2}}{\\sum_{i=1}^{K}z_{i,i}}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{K}z_{i,i}(\\mu_{i}^{2}-\\mu_{i}^{2})}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{K}2\\gamma_{i}\\pi_{i}\\Delta_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We bound $S_{2}^{2}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{S_{2}^{2}=\\displaystyle\\sum_{i=1}^{K}z_{t,i}\\epsilon_{s,i}^{t}\\epsilon_{r,i}^{t}+2z_{t,i}(\\epsilon_{s,i}^{t}+\\epsilon_{r,i}^{t})}\\\\ {\\displaystyle}&{\\le\\displaystyle\\sum_{i=1}^{K}z_{t,i}\\frac{\\epsilon_{s,i}^{t}+\\epsilon_{r,i}^{t}}{2}+2z_{t,i}(\\epsilon_{s,i}^{t}+\\epsilon_{r,i}^{t})}\\\\ {\\displaystyle}&{\\le\\displaystyle\\frac{5}{2}\\displaystyle\\sum_{i=1}^{K}\\frac{z_{t,i}\\lambda_{s,t}}{m_{s,i}^{t}}+\\frac{z_{t,i}\\lambda_{r,i}}{m_{r,i}^{t}}}\\\\ {\\displaystyle}&{\\le\\displaystyle\\frac{5}{2}K\\gamma_{t}(\\lambda_{s,t}+\\lambda_{r,i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds because $z_{t,i}\\leq\\gamma_{t}x_{t,i}$ and that $x_{t,i}\\leq4m_{s,i}^{t},4m_{r,i}^{t}$ from Lemma 3. It remains to give upper bound for $S_{2}^{3}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}^{3}=\\frac{\\left(\\sum_{i=1}^{K}z_{t,i}\\epsilon_{s,i}^{t}\\right)\\left(\\sum_{i=1}^{K}z_{t,i}\\epsilon_{r,i}^{t}\\right)}{\\sum_{i=1}^{K}z_{t,i}}}\\\\ &{\\quad\\leq\\frac{\\left(\\sum_{i=1}^{K}z_{t,i}\\lambda_{s,t}/m_{s,i}^{t}\\right)\\left(\\sum_{i=1}^{K}z_{t,i}\\lambda_{r,t}/m_{r,i}^{t}\\right)}{\\sum_{i=1}^{K}z_{t,i}}}\\\\ &{\\quad\\leq\\frac{1}{2}K\\gamma_{t}(\\lambda_{s,t}+\\lambda_{r,t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "wherethe second inequalityrelyon $z_{t,i}\\leq\\gamma_{t}x_{t,i},\\lambda_{s,t}\\leq m_{s,i}^{t},\\lambda_{r,t}\\leq m_{r,i}^{t};$ and $x_{t,i}\\leq4m_{s,i}^{t},x_{t,i}\\leq$ $4m_{r,i}^{t}$ from Lemma 3. It is suffices to plug bounds in (17), (18), and (19) to obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}[S_{2}]\\leq\\sum_{t=1}^{T}\\sum_{i\\neq t^{*}}4\\Delta_{i}\\gamma_{t}\\mathbb{E}[x_{t,i}]v_{t}(v_{t}-1)+6\\displaystyle\\sum_{t=1}^{T}K\\gamma_{t_{t}+\\hat{d}_{t}}(v_{t+\\hat{d}_{t}}-1)\\lambda_{t,t+\\hat{d}_{t}}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\neq t^{*}}\\sum_{s\\in A_{t}}4\\Delta_{i}\\gamma_{t}\\mathbb{E}[x_{s,i}+\\lambda_{s,t}](v_{t}-1)+6\\displaystyle\\sum_{t=1}^{T}K\\gamma_{t_{t}+\\hat{d}_{t}}(v_{t+\\hat{d}_{t}}-1)\\lambda_{t,t+\\hat{d}_{t}}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\neq t^{*}}\\sum_{s\\in A_{t}}4\\Delta_{i}\\gamma_{t}\\mathbb{E}[x_{s,i}](v_{t}-1)+10\\displaystyle\\sum_{t=1}^{T}K\\gamma_{t_{t}+\\hat{d}_{t}}(v_{t+\\hat{d}_{t}}-1)\\lambda_{t,t+\\hat{d}_{t}}}\\\\ &{\\leq\\displaystyle\\mathcal{O}\\left(\\sum_{t=1}^{T}\\sum_{i\\neq t^{*}}\\gamma_{t_{t}+\\hat{d}_{t}}\\Delta_{i}\\mathbb{E}[x_{t,i}](v_{t+\\hat{d}_{t}}-1)+K\\displaystyle\\sum_{t=1}^{T}\\lambda_{t,t+\\hat{d}_{t}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the third inequality uses Lemma 3 and the last inequality holds because of the skipping that ensures $\\gamma_{t+\\widehat{d_{t}}}(v_{t+\\widehat{d_{t}}}-1)\\leq1.$ Now, it is suficient to combine the bounds for $S_{1}$ and $S_{2}$ in (14) and (20) and get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[s t a b i l i t y]\\le\\mathcal{O}\\left(\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\eta_{t}\\mathbb{E}[x_{t,i}^{1/2}]+\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\gamma_{t+\\hat{d}_{t}}\\mathbb{E}[x_{t,i}](v_{t+\\hat{d}_{t}}-1)+K\\sum_{t=1}^{T}\\lambda_{t,t+\\hat{d}_{t}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the stability bound from (21) and the penalty bound from (13) concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "BProof of the Drift Control Lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we provide a proof of Lemma 3. We start with a few auxiliary results, and then prove the lemma. ", "page_idx": 14}, {"type": "text", "text": "B.1  Auxiliary results for the proof of the key lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the proof we use two facts and a lemma from Masoudian et al. (2022), and a new lemma. Recall that $f_{t}(\\bar{x})=-2\\eta_{t}^{-1}\\sqrt{x}+\\gamma_{t}^{-1}x\\log x$ ", "page_idx": 14}, {"type": "text", "text": "Fact 7. (Masoudian et al., 2022, Fact 15) $f_{t}^{'}(x)$ is a concave function. ", "page_idx": 15}, {"type": "text", "text": "Fact 8. (Masoudian et al., 2022, Fact 16) $f_{t}^{'\\prime}(x)^{-1}$ is a convex function. ", "page_idx": 15}, {"type": "text", "text": "Lemma 9.(Masoudian et al.,2022,Lemma $I7_{.}$ )Fix t and $s$ with $t\\geq s$ andassumethatthereexists $\\alpha$ .such that $x_{t,i}\\,\\leq\\,\\alpha\\operatorname*{max}(x_{s,i},\\lambda_{s,t})$ for all $i\\in[K]$ ,and let $f(x)\\,=\\,\\bigl(-2\\eta_{t}^{-1}\\sqrt{x}+\\gamma_{t}^{-1}x\\log x\\bigr)$ \uff0c thenwehavethefollowinginequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{t,j})^{-1}\\widehat{\\ell}_{s,j}}{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{t,j})^{-1}}\\leq2\\alpha(K-1)^{\\frac{1}{3}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 10. If $t>s$ and $(t-s)\\leq d_{\\operatorname*{max}}^{t}$ then ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{\\operatorname*{max}}^{t}\\leq\\sqrt{2}d_{\\operatorname*{max}}^{s},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to $\\mathcal{D}_{t}\\leq2\\mathcal{D}_{s}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. It suffices to prove that $\\mathcal{D}_{t}\\leq2\\mathcal{D}_{s}$ , which is equivalent to proving that $\\begin{array}{r}{(\\mathcal{D}_{t}-\\mathcal{D}_{s})\\le\\frac{1}{2}\\mathcal{D}_{t}}\\end{array}$ We have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{D}_{t}-\\mathcal{D}_{s}=\\sum_{r=s+1}^{t}\\widehat{\\sigma}_{r}\\leq(t-s)d_{\\operatorname*{max}}^{t}\\leq\\big(d_{\\operatorname*{max}}^{t}\\big)^{2}=\\frac{\\mathcal{D}_{t}}{49K^{\\frac{2}{3}}\\log K}\\leq\\frac{\\mathcal{D}_{t}}{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality holds because due to skipping, for all $r\\leq t$ wehave $\\widehat{\\sigma}_{r}\\,\\leq\\,d_{\\operatorname*{max}}^{t}$ ,and $(t-s)\\leq d_{\\operatorname*{max}}^{t}$ ", "page_idx": 15}, {"type": "text", "text": "B.2Proof of the Drift Control Lemma ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Now we are ready to provide a proof of Lemma 3. Similar to the analysis of Masoudian et al. (2022), the proof relies on induction on valid pairs $(\\boldsymbol{t},\\boldsymbol{s})$ , where a pair $(\\boldsymbol{t},\\boldsymbol{s})$ is considered valid if $s\\leq t$ and $(t-s)\\leq d_{\\operatorname*{max}}^{t}$ . The induction step for pair $(\\boldsymbol{t},\\boldsymbol{s})$ involves proving that $x_{t,i}\\leq4\\operatorname*{max}(x_{s,i},\\lambda_{s,t})$ for all $i\\in[K]$ . To establish this, we use the induction assumption for all valid pairs $(t^{\\prime},s^{\\prime})$ such that $s^{\\prime},t^{\\prime}<t$ , as well as all valid pairs $(t^{\\prime},s^{\\prime})$ , such that $t^{\\prime}=t$ and $s<s^{\\prime}\\leq t$ . The induction base encompasses all pairs $(t^{\\prime},t^{\\prime})$ for all $t^{\\prime}\\in[T]$ , where the statement $x_{t^{\\prime},i}\\leq4x_{t^{\\prime},i}$ holds trivially. ", "page_idx": 15}, {"type": "text", "text": "Tocontrol max(we frst inroduce an axiliary variable = F\\* (La). We the adres the problem of drift control by breaking it down into two sub-problems: ", "page_idx": 15}, {"type": "text", "text": "1. max(t;s,t)\u2264 2: the drift due to change of regularizer,   \n2. $\\begin{array}{r}{\\frac{\\widetilde{x}_{i}}{x_{s,i}}\\le2}\\end{array}$ : the dift due to loss shift. ", "page_idx": 15}, {"type": "text", "text": "Deviation induced by the change of regularizer ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The regularizer at round $r$ is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{r}(x)=\\sum_{i=1}^{K}f_{r}(x_{i})=\\sum_{i=1}^{K}\\left(-2\\eta_{r}^{-1}\\sqrt{x_{i}}+\\gamma_{r}^{-1}x_{i}\\log x_{i}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have $x_{t}=\\nabla\\bar{F}_{t}^{*}(-\\widehat{L}_{t-1}^{o b s})$ and $\\widetilde x=\\nabla\\bar{F}_{s}^{*}(-\\widehat{L}_{t-1}^{o b s})$ AccoringKdis s Lagrange multipliers $\\mu$ and $\\widetilde{\\mu}$ , such that for all $i$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{s}^{'}(\\widetilde{x}_{i})=-\\widehat{L}_{t-1,i}^{o b s}+\\widetilde{\\mu},}\\\\ {f_{t}^{'}(x_{t,i})=-\\widehat{L}_{t-1,i}^{o b s}+\\mu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also know that there exists an index $j$ , such that $\\widetilde{x}_{j}\\ge x_{t,j}$ . This leads to the following inequality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\widehat{L}_{t-1,j}^{o b s}+\\mu=f_{t}^{'}(x_{t,j})\\leq f_{s}^{'}(x_{t,j})\\leq f_{s}^{'}(\\widetilde{x}_{j})=-\\widehat{L}_{t-1,j}^{o b s}+\\widetilde{\\mu},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality holds because the learning rates are decreasing, and the second inequality is due to the fact that $\\bar{f_{s}^{'}}(x)$ is increasing. This implies that $\\mu\\leq\\widetilde{\\mu}$ , which gives us the following inequality for all $i$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{t}^{'}(x_{t,i})=-\\frac{1}{\\eta_{t}\\sqrt{x_{t,i}}}+\\frac{\\log(x_{t,i})}{\\gamma_{t}}\\leq-\\frac{1}{\\eta_{s}\\sqrt{\\widetilde{x_{i}}}}+\\frac{\\log(\\widetilde{x}_{i})}{\\gamma_{s}}=f_{s}^{'}(\\widetilde{x}_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we have two cases, either $\\begin{array}{r}{-\\frac{1}{\\eta_{t}\\sqrt{x_{t,i}}}\\leq-\\frac{1}{\\eta_{s}\\sqrt{\\widetilde{x}_{i}}}}\\end{array}$ or $\\begin{array}{r}{\\frac{\\log(x_{t,i})}{\\gamma_{t}}\\leq\\frac{\\log(\\widetilde{x}_{i})}{\\gamma_{s}}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Case i: If $\\begin{array}{r}{-\\frac{1}{\\eta_{t}\\sqrt{x_{t,i}}}\\leq-\\frac{1}{\\eta_{s}\\sqrt{\\widetilde{x}_{i}}}}\\end{array}$ holds, then we have $\\begin{array}{r}{\\frac{x_{t,i}}{\\widetilde{x}_{i}}\\le\\frac{\\eta_{s}^{2}}{\\eta_{t}^{2}}=\\frac{t}{s}}\\end{array}$ . On the other hand, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nt-s\\le d_{\\operatorname*{max}}^{t}=\\sqrt{\\frac{\\sum_{r=1}^{t}\\widehat{\\sigma}_{r}}{K^{3/2}\\log K}}\\le\\sqrt{\\frac{t^{2}/2}{K^{3/2}\\log K}}\\le\\frac{t}{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "whhesqalaly $\\widehat{\\sigma}_{r}\\le r$ This imples that $\\frac{x_{t,i}}{\\widetilde{x}_{i}}\\,\\leq\\,2$ ", "page_idx": 16}, {"type": "text", "text": "Case i If $\\begin{array}{r}{\\frac{\\log(x_{t,i})}{\\gamma_{t}}\\leq\\frac{\\log(\\widetilde{x}_{i})}{\\gamma_{s}}}\\end{array}$ t imples that $x_{t,i}\\leq\\widetilde{x}_{i}^{\\frac{\\gamma_{t}}{\\gamma_{s}}}$ Using $\\widetilde{x}_{i}\\le\\operatorname*{max}(\\widetilde{x}_{i},\\lambda_{s,t})$ we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t,i}\\leq\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})^{\\frac{\\gamma_{t}}{\\gamma_{s}}}}\\\\ &{\\qquad=\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})^{\\frac{\\gamma_{t}}{\\gamma_{s}}-1}}\\\\ &{\\qquad\\leq\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times\\lambda_{s,t}^{\\frac{\\gamma_{t}}{\\gamma_{s}}-1}}\\\\ &{\\qquad=\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times\\lambda_{s,t}^{-\\frac{\\sqrt{\\gamma_{t}}-\\sqrt{\\gamma_{s}}}{\\sqrt{\\gamma_{t}}}}}\\\\ &{\\qquad=\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times\\lambda_{s,t}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad=\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times e^{\\frac{\\gamma_{t}}{\\gamma_{t}-\\gamma_{s}}\\times\\frac{\\sqrt{\\gamma_{t}}-\\sqrt{\\gamma_{s}}}{\\sqrt{\\gamma_{t}}}}}\\\\ &{\\qquad=\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times e^{\\frac{\\sqrt{\\gamma_{t}}}{(\\sqrt{\\gamma_{t}}+\\sqrt{\\gamma_{s}})}}\\leq\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times e^{\\frac{1}{1+\\sqrt{\\sqrt{\\frac{\\gamma}{2}}}}}\\leq\\operatorname*{max}(\\widetilde x_{i},\\lambda_{s,t})\\times2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, in both cases we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{t,i}\\leq2\\operatorname*{max}(\\tilde{x}_{i},\\lambda_{s,t}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Deviation Induced by the Loss Shift ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The initial steps of the proof of this part are the same as in Masoudian et al. (2022). However, for the sake of completeness, we restate them here. ", "page_idx": 16}, {"type": "text", "text": "Since we have $x_{s}=\\nabla\\bar{F}_{s}^{*}(-\\widehat{L}_{s-1}^{o b s})$ and $\\widetilde x=\\nabla\\bar{F}_{s}^{*}(-\\widehat{L}_{t-1}^{o b s})$ , they both share the same regularizer $\\begin{array}{r}{F_{s}(x)=\\sum_{i=1}^{K}f_{s}(x_{i})}\\end{array}$ For brevity, we drop $s$ from $f_{s}(x)$ . By the KKT conditions $\\exists\\mu,\\widetilde{\\mu}$ s.t. $\\forall i$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f^{'}(x_{s,i})=-\\widehat{L}_{s-1,i}^{o b s}+\\mu,}\\\\ {f^{'}(\\widehat{x}_{i})=-\\widehat{L}_{t-1,i}^{o b s}+\\widetilde{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\widetilde{\\ell}=\\widehat{L}_{t-1}^{o b s}-\\widehat{L}_{s-1}^{o b s}$ , then by the concavity of $f^{'}(x)$ from Fact 7, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n(x_{s,i}-\\widetilde{x}_{i})f^{\\prime\\prime}(x_{s,i})\\le\\underbrace{f^{\\prime}(x_{s,i})-f^{\\prime}(\\widetilde{x}_{i})}_{\\mu-\\widetilde{\\mu}+\\widetilde{\\ell}_{i}}\\le(x_{s,i}-\\widetilde{x}_{i})f^{\\prime\\prime}(\\widetilde{x}_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $f^{\\prime\\prime}(x_{s,i})\\geq0$ from the left side of (23) we get $x_{s,i}-\\widetilde{x}_{i}\\leq f^{\\prime\\prime}(x_{s,i})^{-1}\\left(\\mu-\\widetilde{\\mu}+\\widetilde{\\ell}_{i}\\right)$ Taking summation over all $i$ and using the fact that both vectors $x_{s}$ and $\\widetilde{x}$ are probability vectors, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\displaystyle\\sum_{i=1}^{K}\\left(x_{s,i}-\\widetilde{x}_{i}\\right)\\leq\\displaystyle\\sum_{i=1}^{K}f^{\\prime\\prime}(x_{s,i})^{-1}\\left(\\mu-\\widetilde{\\mu}+\\widetilde{\\ell}_{i}\\right),}\\\\ &{\\qquad\\qquad\\Rightarrow\\widetilde{\\mu}-\\mu\\leq\\displaystyle\\frac{\\sum_{i=1}^{K}f^{\\prime\\prime}(x_{s,i})^{-1}\\widetilde{\\ell}_{i}}{\\sum_{i=1}^{K}f^{\\prime\\prime}(x_{s,i})^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the right hand sides of (23) and (24) gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\widetilde{x}_{i}-x_{s,i})f^{\\prime\\prime}(\\widetilde{x}_{i})\\le\\widetilde{\\mu}-\\mu-\\widetilde{\\ell}_{i}\\le\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}\\widetilde{\\ell}_{j}}{\\sum_{j=1}^{K},f^{\\prime\\prime}(x_{s,j})^{-1}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and by rearrangement we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widetilde{x}_{i}\\le x_{s,i}+f^{\\prime\\prime}(\\widetilde{x}_{i})^{-1}\\times\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}\\widetilde{\\ell}_{j}}{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}}}}\\\\ &{}&{\\le x_{s,i}+\\gamma_{s}\\widetilde{x}_{i}\\times\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}\\widetilde{\\ell}_{j}}{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequlity holds because $f^{\\prime\\prime}(\\widetilde{x}_{i})^{-1}\\ =\\ \\left(\\eta_{s}^{-1}\\frac{1}{2}\\widetilde{x}_{i}^{-3/2}+\\gamma_{s}^{-1}\\widetilde{x}_{i}^{-1}\\right)^{-1}$ . The next step for bounding $\\widetilde{x}_{i}$ is to bound E\"(m) in (25), where ls = Zrer,s and $A=\\Big\\{r:s\\leq r+\\widehat{d}_{r}<t\\Big\\}.$ ", "page_idx": 17}, {"type": "text", "text": "If there exists $r\\,\\in\\,{\\cal A}$ , such that $r\\ >\\ s$ and $4\\operatorname*{max}(x_{r,i},\\lambda_{r,r+\\hat{d}_{r}})\\,\\leq\\,x_{s,i}$ , then combining it with the induction assumption for $(r+\\widehat{d}_{r},r)$ , where we have $x_{r+\\hat{d}_{r},i}\\leq4\\operatorname*{max}(x_{r,i},\\lambda_{r,r+\\hat{d}_{r}})$ eads to $x_{r+\\widehat{d}_{r},i}\\leq x_{s,i}$ . On the other hand, by the induction assumption for pair $(r+\\widehat{d}_{r},t)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t,i}\\leq4\\operatorname*{max}(x_{r+\\hat{d}_{r},i},\\lambda_{r+\\hat{d}_{r},t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So using $x_{r+\\widehat{d}_{r},i}\\leq x_{s,i}$ and $\\lambda_{r+\\widehat{d}_{r},t}\\leq\\lambda_{s,t}$ we can derive $x_{t,i}\\leq4\\operatorname*{max}(x_{s,i},\\lambda_{s,t})$ This inequality satisfies the condition we wanted to prove in the drift lemma. Therefore, we assume that for all $r\\in A$ we have either $r\\leq s$ 0 $x_{s,i}\\leq4\\operatorname*{max}(x_{r,i},\\lambda_{r,r+\\widehat{d}_{r}})$ If $r\\leq s$ , using the the induction assumption for $(s,r)$ together with the fact that $\\lambda_{r,s}\\leq\\lambda_{r,r+\\widehat{d}_{r}}$ ,results in $x_{s,i}\\leq4\\operatorname*{max}(x_{r,i},\\lambda_{r,s})$ . Consequently, in either case, the following inequality holds for all $r\\in A$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{s,i}\\leq4\\operatorname*{max}(x_{r,i},\\lambda_{r,r+\\widehat{d}_{r}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, inequality in (26) satisfies the condition of Lemma 9, and for all $r\\in A$ weget: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}\\widehat{\\ell}_{r,j}}{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}}\\leq8(K-1)^{\\frac{1}{3}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We proceed by summing both sides of the inequality (27) over all $r\\_{\\mathrm{~\\scriptsize~\\in~}\\textit{A}}$ andobtain $\\begin{array}{r}{\\frac{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}\\widetilde{\\ell}_{j}}{\\sum_{j=1}^{K}f^{\\prime\\prime}(x_{s,j})^{-1}}\\le4|A|(K-1)^{\\frac{1}{3}}}\\end{array}$ Now it sufice to plug thisrsutinto (25): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{x}_{i}\\le x_{s,i}+8|A|\\gamma_{s}\\widetilde{x}_{i}(K-1)^{\\frac{1}{3}}\\Rightarrow}\\\\ &{\\widetilde{x}_{i}\\le x_{s,i}\\times\\left(\\cfrac{1}{1-8|A|\\gamma_{s}(K-1)^{1/3}}\\right)}\\\\ &{\\quad\\le x_{s,i}\\times\\left(\\cfrac{1}{1-24\\gamma_{s}d_{\\operatorname*{max}}^{s}(K-1)^{1/3}}\\right)}\\\\ &{\\quad\\le x_{s,i}\\times\\left(\\cfrac{1}{1-1/2}\\right)=2x_{s,i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality uses $|A|\\,\\le\\,d_{\\operatorname*{max}}^{s}+t-s\\,\\le\\,d_{\\operatorname*{max}}^{t}+d_{\\operatorname*{max}}^{s}$ and that $d_{\\mathrm{max}}^{t}\\leq2d_{\\mathrm{max}}^{s}$ by Lemma,adflastiqualtywtdeionsf andx Combining (29) and (22) completes the induction step. ", "page_idx": 17}, {"type": "text", "text": "C  Self-Bounding Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we show the details of how to apply self-bounding analysis to bound the right hand side of (5). ", "page_idx": 18}, {"type": "text", "text": "We start from (5) and decompose it as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathrm{3}e g}}_{T}\\leq\\mathbb{E}\\left[a\\underbrace{\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\neq i^{*}}\\eta_{t}x_{t,i}^{1/2}}_{A}+\\underbrace{b\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\neq i^{*}}\\gamma_{t+d_{t}}(v_{t+d_{t}}-1)x_{t,i}\\Delta_{i}}_{B}+c\\underbrace{\\sum_{t=2}^{T}\\displaystyle\\sum_{i=1}^{K}\\frac{\\widehat{\\sigma}_{t}\\gamma_{t}x_{t,i}\\log(1/x_{t,i})}{\\log K}}_{c}\\right]}\\\\ &{\\qquad+\\underbrace{\\mathcal{O}\\left(K\\displaystyle\\sum_{t=1}^{T}\\left(\\lambda_{t,t+\\hat{d}_{t}}+\\lambda_{t,t+\\hat{d}_{t+\\sigma_{\\operatorname*{max}}^{t}}}\\right)+\\sigma_{\\operatorname*{max}}+S^{*}\\right)}_{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We rewrite the pseudo-regret as $\\overline{{R e g}}_{T}=4\\overline{{R e g}}_{T}-3\\overline{{R e g}}_{T}$ , and then based on the decomposition above we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{R e g}}_{T}\\leq\\mathbb{E}\\left[4a A-\\overline{{R e g}}_{T}\\right]+\\mathbb{E}\\left[4b B-\\overline{{R e g}}_{T}\\right]+\\mathbb{E}\\left[4c C-\\overline{{R e g}}_{T}\\right]+4D.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Masoudian et al. (2022) provide the following three lemmas that give the bounds for the first three terms in (30). Although the algorithm of Masoudian et al. differs from ours, their bounds remain applicable, because they are based on the worst-case choice of $x_{t,i}$ , which is algorithm-independent. Lemma 11. (Masoudian et al., 2022, Lemma $6$ )For any $a\\geq0$ wehave: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n4a A-\\overline{{R e g}}_{T}\\leq\\sum_{i\\neq i^{*}}\\frac{4a^{2}}{\\Delta_{i}}\\log(T+1)+1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 12. (Masoudian et al.,2022, Lemma 7) Let $v_{m a x}=\\operatorname*{max}_{t\\in[T]}\\upsilon_{t},$ then for any $b\\geq0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n4b B-\\overline{{R e g}}_{T}\\leq64b^{2}v_{m a x}\\log K.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is evident that $v_{m a x}\\leq\\sigma_{\\mathrm{max}}$ , so the bound in Lemma 12 is dominated by ${\\mathcal{O}}(K\\sigma_{\\operatorname*{max}})$ term in the regret bound. ", "page_idx": 18}, {"type": "text", "text": "Lemma 13. (Masoudian et al., 2022, Lemma 8) For any $c\\geq0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n4c C-\\overline{{R e g}}_{T}\\le\\sum_{i\\neq i^{*}}\\frac{128c^{2}\\sigma_{\\operatorname*{max}}}{\\Delta_{i}\\log K}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By plugging (31),(32),(33) into (30) we get the desired bound. ", "page_idx": 18}, {"type": "text", "text": "DA Proof of Lemma 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First we provide two facts and two auxiliary lemmas. ", "page_idx": 18}, {"type": "text", "text": "Lemma 14. For any $t$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\mathcal{D}_{t}\\geq\\sum_{s=1}^{t}\\widehat{d}_{s}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We show that for any $t\\in[T]$ we have $\\begin{array}{r}{\\sum_{s=1}^{t}\\widehat{d}_{s}-{\\mathscr{D}_{t}}\\leq{\\mathscr{D}_{t}}}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\widehat{d}_{s}-\\mathcal{D}_{t}=\\sum_{\\scriptstyle(s\\leq t){\\scriptstyle\\wedge(s+\\widehat{d}_{s}>t)}}(\\widehat{d}_{s}-\\widehat{\\sigma}_{s})}}\\\\ &{}&{\\leq\\sum_{\\scriptstyle(s\\leq t){\\scriptstyle\\wedge(s+\\widehat{d}_{s}>t)}\\atop{\\scriptstyle(s\\leq t){\\scriptstyle\\wedge(s+\\widehat{d}_{s}\\geq t)}}}\\widehat{d}_{s}}\\\\ &{}&{\\leq\\left(d_{\\operatorname*{max}}^{t}\\right)^{2}=\\frac{\\mathcal{D}_{t}}{49K^{\\frac{2}{3}}\\log K}\\leq\\mathcal{D}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequalityholds because $\\widehat{d}_{s}\\leq d_{\\operatorname*{max}}^{t}$ and the total number of steps that satisfy $(s\\leq t)\\wedge(s+\\widehat{d}_{s}>t)$ is less than the skipping threshold at time $t$ which is again $d_{\\mathrm{max}}^{t}$ . Rearranging the inequality completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma 15 (Orabona, 2022, Lemma 4.13). Let $a_{0}~\\geq~0$ and $f~:~[0;+\\infty)~\\rightarrow~[0;+\\infty)$ bea nonincreasingfunction.Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}a_{t}f\\left(a_{0}+\\sum_{i=1}^{t}a_{i}\\right)\\leq\\int_{a_{0}}^{\\sum_{t=0}^{T}a_{t}}f(x)d x.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Fact 16. For any $x\\geq0$ we have $\\begin{array}{r}{e^{-x}\\leq\\frac{1}{x}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Fact 17. For any $x\\geq1$ we have $\\begin{array}{r}{e^{-x}\\leq\\frac{1}{x\\log^{2}(x)}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 4. We have two summations as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}e^{-\\frac{\\mathcal{D}_{t+\\hat{d}_{t}}}{\\mathcal{D}_{t+\\hat{d}_{t}}-\\mathcal{D}_{t}}}+\\sum_{t=1}^{T}e^{-\\frac{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}}{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}-\\mathcal{D}_{t}}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we show an upper bound of $\\mathcal{O}(\\widehat{\\sigma}_{\\mathrm{max}})$ for each of them. ", "page_idx": 19}, {"type": "text", "text": "Bounding the First Summation: Let $T_{0}$ be the time satisfying $\\begin{array}{r}{\\sqrt{D_{T_{0}}}=\\frac{\\widehat{\\sigma}_{\\mathrm{max}}}{K^{1/3}\\log(K)}}\\end{array}$ K1/3log(K), then using Facts 16 and 17 we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}e^{-\\frac{\\mathcal{D}_{t+\\hat{d}_{t}}}{\\mathcal{D}_{t+\\hat{d}_{t}}-\\mathcal{D}_{t}}}\\leq\\underbrace{\\sum_{t=1}^{T_{0}}\\frac{\\mathcal{D}_{t+\\hat{d}_{t}}-\\mathcal{D}_{t}}{\\mathcal{D}_{t+\\hat{d}_{t}}}}_{A}+\\underbrace{\\sum_{t=T_{0}+1}^{T}\\frac{\\mathcal{D}_{t+\\hat{d}_{t}}-\\mathcal{D}_{t}}{\\mathcal{D}_{t+\\hat{d}_{t}}\\log^{2}\\left(\\frac{\\mathcal{D}_{t+\\hat{d}_{t}}}{\\mathcal{D}_{t+\\hat{d}_{t}}-\\mathcal{D}_{t}}\\right)}}_{B}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $A$ we give the following bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{A=\\sum_{i=1}^{T_{0}}\\sum_{n=t+1}^{t+\\hat{d}_{i}}\\frac{\\hat{\\sigma}_{s}}{D_{t+\\hat{d}_{i}}}=\\sum_{s=1}^{T_{0}}\\sum_{\\ell=0}^{s-1}\\hat{\\sigma}_{s}\\mathbf{s}^{1}(t+\\hat{d}_{i}\\geq s)}}\\\\ &{\\leq\\sum_{s=1}^{T_{0}}\\frac{\\hat{\\sigma}_{s}^{2}}{D_{s}}}\\\\ &{\\leq\\sum_{s=1}^{T_{0}}\\frac{\\hat{\\sigma}_{s}\\sqrt{D_{s}}}{K^{1/3}\\log(K))^{D_{s}}}}\\\\ &{=\\sum_{s=1}^{T_{0}}\\frac{\\hat{\\sigma}_{s}}{K^{1/3}\\log(K)\\sqrt{D_{s}}}}\\\\ &{\\leq\\mathcal{O}\\left(\\frac{\\sqrt{D_{T_{0}}}}{K^{1/3}\\log(K)}\\right)=\\mathcal{O}\\left(\\frac{\\hat{\\sigma}_{\\operatorname*{max}}}{K^{2/3}\\log^{2}(K)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where thesedequaltysba atst qltl $\\mathcal{D}_{t+\\widehat{d}_{t}}\\geq$ $\\mathcal{D}_{s}$ th thid inequalityuses $\\begin{array}{r}{\\widehat{\\sigma}_{s}\\leq d_{\\operatorname*{max}}^{s}\\leq\\frac{\\sqrt{\\mathcal{D}_{s}}}{K^{1/3}\\log K}}\\end{array}$ an the lat inequality uses Lema 15s. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon=}&{\\sum_{i=1}^{\\frac{N}{2}}\\frac{\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\partial}{\\partial\\tau_{i}+\\varepsilon_{i}}\\frac{\\varepsilon_{i}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\leq\\leq\\frac{\\sum_{\\substack{\\tau}}^{\\frac{N}{2}}\\sum_{\\substack{\\tau}=1}^{\\infty}\\lambda_{\\tau}+\\frac{\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\leq\\frac{\\beta_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}}\\\\ &{=\\sum_{\\tau}\\frac{\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\leq\\frac{\\beta_{i}\\ln\\tau_{i}+\\varepsilon_{i}\\geq0}{\\varepsilon_{i}+\\varepsilon_{i}}}\\\\ &{=\\sum_{\\tau}\\frac{\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\leq\\frac{\\beta_{i}\\ln\\tau_{i}+\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}}\\\\ &{\\leq\\frac{\\sum_{\\substack{\\tau}}^{\\frac{N}{2}}\\sum_{\\substack{\\tau}=1}^{\\infty}\\frac{\\beta_{i}\\ln\\tau_{i}+\\varepsilon_{i}\\geq0}{4\\tau_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}-\\varepsilon_{i}+\\varepsilon_{i}}}\\\\ &{\\leq\\frac{\\sum_{\\tau}}{\\varepsilon_{i}+\\varepsilon_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\frac{\\partial^{2}}{\\partial\\tau_{i}+\\varepsilon_{i}}\\leq\\frac{\\beta_{i}\\ln\\tau_{i}+\\varepsilon_{i}}{\\varepsilon_{i}+\\varepsilon_{i}}}\\\\ &{\\leq\\delta_{\\mathrm{shas}}\\leq\\frac{\\sum_{\\substack{\\tau}=1}^{\\infty}\\frac{\\beta_{i}\\ln\\tau_{i}}{4\\tau_{i}+\\varepsilon_{i}}\\leq\\frac{\\beta_{i}\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the frst inequality follows by $\\widehat{\\sigma}_{s}\\leq\\widehat{\\sigma}_{\\operatorname*{max}}$ and our skipping procedure that ensures $\\widehat{d}_{t}\\leq d_{\\operatorname*{max}}^{t}\\leq$ $\\frac{\\sqrt{D_{t+\\hat{d}_{t}}}}{K^{1/3}\\log K}$ t edqualtsbwanst dqlt $\\mathcal{D}_{t+\\widehat{d}_{t}}\\geq\\mathcal{D}_{s}$ and $\\begin{array}{r}{\\sum_{t=1}^{s-1}\\mathbb{1}(t+\\widehat{d}_{t}\\geq s)=\\widehat{\\sigma}_{s}}\\end{array}$ , the last inequality folows by Lemma 15 , and the last equalityuses $\\begin{array}{r}{\\int\\frac{1}{x\\log^{2}(x/\\widehat{\\sigma}_{\\operatorname*{max}}^{2})}d x=\\frac{-1}{\\log(x/\\widehat{\\sigma}_{\\operatorname*{max}}^{2})}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Bound the Second Summation: The bound for the second summation follows the same approach, but it requires additional care due to existence of $\\sigma_{\\mathrm{max}}^{t}$ in it. Let $T_{0}$ to be the time satisfying $\\begin{array}{r}{\\sqrt{D_{T_{0}}}=\\frac{\\widehat{\\sigma}_{\\mathrm{max}}}{K^{1/3}\\log(K)}}\\end{array}$ thenusin Fats1 and 1 we ave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}e^{-\\frac{\\frac{D_{t+\\sigma_{\\operatorname*{max}}}^{2}+\\hat{d}_{t}}{D_{t+\\sigma_{\\operatorname*{max}}^{2}+\\hat{d}_{t}}-\\frac{D_{t}}{D_{t}}}}{\\sigma_{t+\\sigma_{\\operatorname*{max}}^{2}+\\hat{d}_{t}}-\\frac{D}{D_{t}}}}\\leq\\underset{A}{\\underbrace{\\sum_{t=1}^{T}\\frac{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}-\\mathcal{D}_{t}}{D_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}}}}+\\underset{\\underbrace{t=T_{0}+1}}{\\underbrace{\\sum_{t=1}^{T}\\frac{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}-\\mathcal{D}_{t}}{D_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}\\log^{2}\\left(\\frac{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}}{\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\hat{d}_{t}}-\\mathcal{D}_{t}}\\right)}}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\tau\\rightarrow\\tau}^{\\frac{1}{n}}\\!\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}-\\!\\sum_{s=1}^{n}\\!\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}-\\!D_{\\tau}}\\\\ {\\leq}&{\\displaystyle\\sum_{s=1}^{n}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}+\\frac{\\gamma}{n}+\\frac{\\gamma}{n}+\\frac{\\gamma}{n}}\\frac{\\hat{\\tau}_{s,s}}{T_{1}\\!+\\!\\tau_{s,s}\\!+\\!\\tau_{s}^{\\frac{n}{n}}}}\\\\ &{-\\frac{\\gamma}{n}\\sum_{s=1}^{n}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!}\\\\ {\\leq}&{\\displaystyle\\sum_{s=1}^{n}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}}\\\\ &{\\leq\\displaystyle\\sum_{s=1}^{n}\\!(\\frac{2n\\gamma_{s}}{n}\\!+\\!\\frac{\\gamma}{n}\\!+\\!\\frac{\\gamma}{n}\\!)\\!+\\!\\frac{\\gamma}{n}\\!}\\\\ {\\leq}&{\\displaystyle\\sum_{s=1}^{n}\\!\\frac{3}{n}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}}\\\\ &{=\\displaystyle\\sum_{s=1}^{n}\\!\\frac{3}{n}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}}\\\\ &{=\\frac{\\gamma}{n}\\frac{3}{n}\\frac{\\hat{\\tau}_{s,s}^{\\frac{n}{n}+1}\\!+\\!\\log(\\lambda_{s})\\!}{T_{1}\\!+\\!\\lambda_{s}\\!\\exp(\\!-\\!\\lambda_{s}\\!\\tau_{s}\\!)}}\\\\ &{\\leq\\sigma\\left(\\frac{\\gamma}{n+1}\\!\\exp_{s\\rightarrow\\tau}^{\\frac{n}{n}}\\right)-\\sigma\\!(\\frac{\\lambda_{s}}{K^{2}\\!+\\!\\lambda_{s}\\!})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is by Fact 16, the second inequality holds by swapping the summations and that $\\mathcal{D}_{t+\\sigma_{\\operatorname*{max}}^{t}+\\widehat{d}_{t}}\\geq\\mathcal{D}_{s}$ , third inequality usethe fllowing derivation ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lfloor(t+\\sigma_{\\operatorname*{max}}^{t}+\\widehat{d}_{t}\\geq s)\\leq1(t+\\widehat{d}_{t}\\geq s)+1(s>t+\\widehat{d}_{t}\\geq s-\\sigma_{\\operatorname*{max}}^{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1(t+\\widehat{d}_{t}\\geq s)+1(t\\in[s-\\sigma_{\\operatorname*{max}}^{t},s-1])+1(t<s-\\sigma_{\\operatorname*{max}}^{t}\\wedge t+\\widehat{d}_{t}\\geq s-\\sigma_{\\operatorname*{max}}^{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "thethideqltsyai ha th qayes $\\begin{array}{r}{\\widehat{\\sigma}_{s}\\leq d_{\\operatorname*{max}}^{s}\\leq\\frac{\\sqrt{\\mathcal{D}_{s}}}{K^{1/3}\\log K}}\\end{array}$ \uff0c and finally the last inequality uses Lemma 15. ", "page_idx": 21}, {"type": "text", "text": "The bound for $B$ is as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{1}=}&{\\frac{\\displaystyle\\sum_{s=1}^{n-1}\\frac{\\sum_{i=1}^{i+1}s_{i-1}^{i+1}-\\sum_{s=1}^{s}\\mu_{i,s}}}{\\displaystyle\\sum_{s=1}^{n}\\sum_{i=1}^{i+1}s_{i-1,i}^{i+1}\\mu_{i+1,s}^{i}}\\left(\\frac{\\sum_{s=1}^{i}s_{i-1,i+1}}{\\displaystyle\\sum_{s=1}^{i+1}s_{i-1,i+1}^{i+1}}\\right)}\\\\ {*}&{\\frac{\\displaystyle\\sum_{s=1}^{s}\\sum_{i=1}^{i+1}s_{i-1,i+1}^{i+1}}{\\displaystyle\\sum_{s=1}^{i}s_{i-1,i+1}^{i}}\\frac{\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}\\frac{\\mu_{i+1,s}^{i}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}\\frac{\\mu_{i+1,s}^{i+1}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}}\\\\ {*}&{\\frac{\\displaystyle\\sum_{s=1}^{s}\\sum_{i=1}^{i}\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}+\\frac{2\\lambda_{1}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}\\frac{\\mu_{i+1,s}^{i+1}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}\\frac{\\mu_{i+1,s}^{i}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}}\\\\ &{-\\displaystyle\\sum_{s=1}^{i}\\sum_{i=1}^{i}\\frac{\\sum_{s=1}^{i}s_{i-1,i+1}^{i}}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}\\frac{\\mu_{i+1,s}^{i}(s_{i-1,i+1}^{i}+s_{i,i-1}^{i})}{\\displaystyle\\sum_{s=1}^{s}\\mu_{i+1,s}^{i}}}\\\\ &{\\leq\\displaystyle\\sum_{s= \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the frst inequality is due to our skippin procedure that ensures max $\\left\\{\\sigma_{\\operatorname*{max}}^{t},\\widehat{d}_{t}\\right\\}\\leq d_{\\operatorname*{max}}^{t}\\leq$ $\\sqrt{D_{t+\\sigma_{\\operatorname*{max}}^{t}+\\widehat{d}_{t}}}$ the second equalty isbyswaping t sumatos, the seond iquality fows by $\\mathcal{D}_{t+\\widehat{d}_{t}}\\,\\geq\\,\\mathcal{D}_{s}$ and (34), the last inequality follows by Lmma 15, and the last equality uses $\\begin{array}{r}{\\int\\frac{\\dot{1}}{x\\log^{2}(x/\\widehat{\\sigma}_{\\operatorname*{max}}^{2})}d x=\\frac{-1}{\\log(x/\\widehat{\\sigma}_{\\operatorname*{max}}^{2})}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "E A proof of Lemma 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We use the term free round to refer to a round $r$ such that $\\upsilon_{r}^{n e w}$ is zero. By applying induction on the time step $t$ , we show that if the algorithm is currently at time $t$ and intends to rearrange the $\\upsilon_{t}$ arrivals, there exist $\\upsilon_{t}$ free rounds in the interval $[t,t+\\sigma_{\\operatorname*{max}}^{t}-\\widehat{\\sigma}_{t}+v_{t}]$ to which the algorithm can push the arrivals. This ensures that the arrival from round $s$ , will be rearranged to round $\\pi(s)\\geq s\\!+\\!\\widehat{d}_{s}$ such that $\\pi(s)-(s+\\widehat{d}_{s})\\leq\\sigma_{\\operatorname*{max}}^{t}$ To thisnd we assume thnutin autions f all $r<t$ , and then proceed with induction step for $t$ ", "page_idx": 22}, {"type": "text", "text": "Induction Base: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The induction base corresponds to the first arrival time, denoted as $t_{0}$ . At this time step, all $\\upsilon_{t_{0}}$ arrivals can be rearranged to the free rounds in the interval $[t_{0},t_{0}+v_{t_{0}}-1]$ , which is a subset of $[t_{0},t_{0}+\\sigma_{\\operatorname*{max}}^{t_{0}}-\\widehat{\\sigma}_{t_{0}}+\\widehat{v}_{t_{0}}-1]$ Therefoe the inductionbae holds. ", "page_idx": 22}, {"type": "text", "text": "Induction step: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Assume that we are at round $t$ , and our aim is to rearrange the arrivals of round $t$ .We define $t_{1}$ as thelastccuiedrude $t_{1}\\geq t$ So it sufies to prove $t_{1}-t\\leq\\sigma_{\\operatorname*{max}}^{t}-\\widehat{\\sigma}_{t}$ We first ote that since the algorithm is greedy, all rounds $t,t+1,\\ldots,t_{1}-1$ must also be occupied by some arrivals from the past. ", "page_idx": 22}, {"type": "text", "text": "Let $t_{0}~<~t$ be the first round where one of its arrivals has been rearranged to $t$ , and let $\\boldsymbol{v}_{t_{0}}^{'}$ be the number of arrivals at time $t_{0}$ that are rearranged to some rounds before $t$ . Then by induction assumption we know ", "page_idx": 23}, {"type": "equation", "text": "$$\nt-t_{0}\\leq\\sigma_{\\operatorname*{max}}^{t_{0}}-\\widehat{\\sigma}_{t_{0}}+v_{t_{0}}^{'}+1=\\sigma_{\\operatorname*{max}}^{t_{0}}-\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t_{0})+v_{t_{0}}^{'}+1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, by the choice of $t_{0}$ , each occupied round $t,t+1,\\ldots,t_{1}$ must be occupied by exactly one arrival among the arrivals of rounds $t_{0},\\ldots,t-1$ , except for the $\\boldsymbol{v}_{t}^{'}$ arrivals of $t_{0}$ that are rearranged to some rounds before $t$ . So we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{1}-t+1\\leq\\displaystyle\\sum_{r=1}^{t-1}\\mathbb{1}(t_{0}\\leq r+\\widehat{d}_{r}\\leq t-1)-v_{t_{0}}^{'}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(t_{0}\\leq r+\\widehat{d}_{r}\\leq t-1)+\\displaystyle\\sum_{r=t_{0}}^{t-1}\\mathbb{1}(t_{0}\\leq r+\\widehat{d}_{r}\\leq t-1)-v_{t_{0}}^{'}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(t_{0}\\leq r+\\widehat{d}_{r}\\leq t-1)+t-t_{0}-\\displaystyle\\sum_{r=t_{0}}^{t-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t)-v_{t_{0}}^{'},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second equaityholds ecause $\\begin{array}{r}{\\sum_{r=t_{0}}^{t-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t_{0})=t-t_{0}}\\end{array}$ We use(35 to bound in the above inequality and get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{t_{1}-t\\leq\\sigma_{\\operatorname*{max}}^{t_{0}}+\\displaystyle\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(t_{0}\\leq r+\\widehat{d}_{r}\\leq t-1)-\\displaystyle\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t_{0})-\\displaystyle\\sum_{r=t_{0}}^{t-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t)}}\\\\ {{\\qquad=\\sigma_{\\operatorname*{max}}^{t_{0}}-\\displaystyle\\sum_{r=1}^{t_{0}-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t)-\\displaystyle\\sum_{r=t_{0}}^{t-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t)}}\\\\ {{\\qquad=\\sigma_{\\operatorname*{max}}^{t_{0}}-\\displaystyle\\sum_{r=1}^{t-1}\\mathbb{1}(r+\\widehat{d}_{r}\\geq t)\\leq\\sigma_{\\operatorname*{max}}^{t}-\\widehat{\\sigma}_{t},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequalityfollows by the fact that $\\{\\sigma_{\\operatorname*{max}}^{r}\\}_{r\\in[T]}$ is anon-reaing sequen if th algorithm rearranges the $\\upsilon_{t}$ arrivals at round $t$ to rounds $t_{1}+{\\dot{1}},\\dots,t_{1}+v_{t}$ , then, using the inequality (36),we canconclude that theerounds fall within theinteral $[t,t+\\sigma_{\\operatorname*{max}}^{t}-\\widehat{\\sigma}_{t}+v_{t}]$ ", "page_idx": 23}, {"type": "text", "text": "F  Adversarial bounds with $d_{\\mathrm{max}}$ cannot benefit from skipping ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we show that adversarial regret bounds that involve terms that are linear in $d_{\\mathrm{max}}$ such as the bounds of Masoudian et al. (2022), cannot benefit from skipping. We prove the following lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma 18. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{D}\\leq\\operatorname*{min}_{S}\\left(|S|+\\sqrt{D_{\\bar{S}}}\\right)+d_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For any split of the rounds $[T]$ into $\\boldsymbol{S}$ and $\\bar{S}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\nD=D_{\\bar{S}}+D_{S}\\leq D_{\\bar{S}}+|S|d_{\\operatorname*{max}}\\leq D_{\\bar{S}}+|S|^{2}+d_{\\operatorname*{max}}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{D}\\le\\sqrt{D_{\\bar{S}}+|S|^{2}+d_{\\operatorname*{max}}^{2}}\\le|S|+\\sqrt{D_{\\bar{S}}}+d_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and since the above holds for any $\\boldsymbol{S}$ , we obtain the statement of the lemma. ", "page_idx": 23}, {"type": "text", "text": "We remind that skipping allows to replace a term of order $\\sqrt{D}$ by a term of order $\\operatorname*{min}_{\\mathcal{S}}\\left(|\\mathcal{S}|+\\sqrt{D_{\\bar{\\mathcal{S}}}}\\right)$ (for simplicity we ignore factors dependent on $K_{.}$ 0. Thus, it may potentially replace a bound of order $\\sqrt{D}+d_{\\mathrm{max}}^{\\phantom{\\dagger}}$ by a bound of order : $\\mathrm{min}_{S}\\left(\\left|S\\right|+\\sqrt{D_{\\bar{S}}}\\right)+d_{\\mathrm{max}}$ , but since by the lemma $\\operatorname*{min}_{S}\\left(|S|+\\sqrt{D_{\\bar{S}}}\\right)+d_{\\operatorname*{max}}=\\Omega(\\sqrt{D})$ , this would not improve the order of the bound. ", "page_idx": 23}, {"type": "text", "text": "G  Details of the Adversarial Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The only difference between our algorithm and the algorithm of Zimmert and Seldin (2020) is the implicit exploration and the slightly modified skipping rule. Let $\\ell_{t}$ be the original loss sequence, then the adversary can create an adaptive sequence $\\widetilde{\\ell}_{t}$ that forces the player to play according to the implicit exploration rule by simply down-scaling all the losses by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{t i}=\\frac{x_{t i}\\ell_{t i}}{\\operatorname*{max}\\left\\{x_{t,i},\\lambda_{t,t+\\widehat{d}_{t}}\\right\\}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Our regret bound decomposes now into ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{R e g}}_{T}=\\underset{i_{T}^{*}}{\\operatorname*{max}}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left<x_{t},\\ell_{t}\\right>-\\ell_{t,i_{T}^{*}}\\right]}\\\\ &{\\qquad\\le\\underset{i_{T}^{*}}{\\operatorname*{max}}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left<x_{t},\\widetilde{\\ell}_{t}\\right>-\\widetilde{\\ell}_{t,i_{T}^{*}}\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left<x_{t},\\ell_{t}-\\widetilde{\\ell}_{t}\\right>\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the second term we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left<x_{t},\\ell_{t}-\\widetilde{\\ell}_{t}\\right>\\leq\\sum_{i=1}^{K}\\sum_{t=1}^{T}(1-\\frac{x_{t i}}{x_{t i}+\\lambda_{t,t+\\widehat{d}_{t}}})x_{t i}\\leq K\\sum_{t=1}^{T}\\lambda_{t,t+\\widehat{d}_{t}}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which can be controlled via Lemma 4. ", "page_idx": 24}, {"type": "text", "text": "The first term is bounded by Zimmert and Seldin (2020, Theorem 3) (since the player plays their algorithm on the modified loss sequence) by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t=1}{\\operatorname*{max}}\\ E\\left[\\displaystyle\\sum_{t=1}^{T}(x_{t},\\ell_{t})-\\ell_{t},\\iota_{t}\\right]\\leq4\\sqrt{K T}+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\hat{\\sigma}_{t}+\\gamma_{T}^{-1}\\log K+S^{*}}\\\\ &{\\leq4\\sqrt{K T}+\\displaystyle\\sum_{t=1}^{T}\\frac{\\hat{\\sigma}_{t}\\sqrt{\\log K}}{\\sqrt{\\mathcal{P}_{t}}}+\\tau\\sqrt{\\mathcal{P}_{T}\\log K}+S^{*}}\\\\ &{=4\\sqrt{K T}+\\sqrt{\\log K}\\displaystyle\\sum_{t=1}^{T}\\frac{\\bar{D}_{t}-\\mathcal{D}_{t-1}}{\\mathcal{T}\\sqrt{\\mathcal{P}_{t}}}+7\\sqrt{\\mathcal{D}_{T}\\log K}+S^{*}}\\\\ &{\\leq4\\sqrt{K T}+\\displaystyle\\frac{2\\sqrt{\\log K}}{\\mathcal{T}}\\displaystyle\\sum_{t=1}^{T}\\sqrt{\\mathcal{D}_{t}}-\\sqrt{D_{t-1}}+7\\sqrt{\\mathcal{D}_{T}\\log K}+S^{*}}\\\\ &{=4\\sqrt{K T}+\\displaystyle\\frac{5}{\\mathcal{T}}\\displaystyle\\sqrt{\\mathcal{D}_{T}\\log K}+S^{*}}\\\\ &{\\leq4\\sqrt{K T}+\\displaystyle\\frac{1}{\\mathcal{T}}\\displaystyle\\sum_{t=1}^{T}\\frac{\\sin\\theta}{\\mathcal{T}\\log K}+S^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the fis equality uses the defnition of $\\gamma_{t}$ the third inequlity folows by $\\begin{array}{r}{\\forall a,b>0:\\frac{a-b}{\\sqrt{a}}\\leq}\\end{array}$ $2({\\sqrt{a}}-{\\sqrt{b}})$ , and the last inequality uses the following lemma ", "page_idx": 24}, {"type": "text", "text": "Lemma 19. The skipping technique guarantees the following bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{\\mathcal{D}_{T}K^{\\frac{2}{3}}\\log K}\\leq\\operatorname*{min}_{\\mathcal{S}\\subseteq[T]}\\left\\{|\\cal{S}|+\\sqrt{\\mathcal{D}_{\\bar{\\cal{S}}}K^{\\frac{2}{3}}\\log K}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining the bounds on the first and the second terms provides the regret bound in Section 5.2. It only remains to provide a proof for Lemma 19. ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 19. For any $t~\\in~[T]$ wehave $\\widehat{d}_{t}\\ \\leq\\ \\sqrt{D_{T}/(49K^{\\frac23}\\log(K))}$ ,therefore for any $R\\subset[T]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]\\backslash R}d_{t}\\geq\\sum_{t\\in[T]\\backslash R}\\widehat{d}_{t}\\geq\\mathcal{D}_{T}-|R|\\sqrt{\\mathcal{D}_{T}/(49K^{\\frac{2}{3}}\\log(K))}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence we can dereive the following lower bound, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\ell\\subseteq[T]}|R|+\\sqrt{\\sum_{s\\in[T]\\backslash R}d_{s}K_{3}^{\\frac{2}{3}}\\log(K)}\\ge\\operatorname*{min}_{r\\in\\left[0,\\sqrt{49D_{T}K^{\\frac{2}{3}}\\log(K)}\\right]}r+\\sqrt{\\mathcal{D}_{T}K_{3}^{\\frac{2}{3}}\\log(K)-\\frac{1}{7}r\\sqrt{\\mathcal{D}_{T}K_{3}^{\\frac{2}{3}}\\log(K)}}}\\\\ &{}&{\\ge\\sqrt{\\mathcal{D}_{T}K^{\\frac{2}{3}}\\log(K)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second inequality uses the concavity in $r$ ", "page_idx": 25}, {"type": "text", "text": "H A Bound on $S^{*}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Next, we reason about the nature of skips. The following lemma is an adaptation of Zimmert and Seldin (2020, Lemma 5) to our skipping threshold. To this end we provide two lemmas and then conclude then proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma 20. Algorithm 1 will not skip more than 1 point at a time. ", "page_idx": 25}, {"type": "text", "text": "Proof. We prove the lemma by contradiction. Assume that $s_{1},s_{2}$ are both deactivated at time $t$ W.1.o.g. let $s_{2}\\,\\leq\\,s_{1}\\,-\\,1$ . Skipping of $s_{1}$ at time $t$ means $t-s_{1}\\,\\geq\\,\\sqrt{\\mathscr{D}_{t}/(K^{\\frac{2}{3}}\\log(K))}\\,\\geq$ $\\sqrt{{\\mathscr{D}_{t-1}}/(K^{\\frac{2}{3}}\\log(K))}$ . At the same time we assumed $t-1-s_{2}\\geq t-s_{1}$ , which means that $s_{2}$ would have been deactivated at round $t-1$ or earlier. ", "page_idx": 25}, {"type": "text", "text": "Recall that $\\widehat{d}_{t}$ is the contribution of a timestep $t$ to the sum $\\mathcal{D}_{T}$ . Let $(t_{1},\\dots,t_{S^{*}})$ be an indexing of $\\boldsymbol{S}$ and $c=49K^{\\frac{2}{3}}\\log(K)$ Webound thenumber ofskipsby ", "page_idx": 25}, {"type": "equation", "text": "$$\nS^{*}\\leq2c\\widehat{d}_{t_{S}^{*}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above bound together with the fact that incurred delay $\\widehat{d}_{t_{S}^{*}}$ must be less than the the skipping threshold and the maximal delay $d_{\\mathrm{max}}$ give us ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S^{*}\\leq\\mathcal{O}\\left(K^{\\frac{2}{3}}\\log K\\widehat{d}_{t_{S}^{*}}\\right)}\\\\ &{\\quad\\leq\\mathcal{O}\\left(\\operatorname*{min}\\left\\{d_{\\operatorname*{max}}K^{\\frac{2}{3}}\\log K,\\sqrt{\\mathcal{D}_{T}K^{\\frac{2}{3}}\\log K}\\right\\}\\right)}\\\\ &{\\quad\\leq\\mathcal{O}\\left(\\operatorname*{min}\\left\\{d_{\\operatorname*{max}}K^{\\frac{2}{3}}\\log K,\\displaystyle\\operatorname*{min}_{S\\subseteq[T]}\\left\\{|S|+\\sqrt{\\mathcal{D}_{\\bar{S}}K^{\\frac{2}{3}}\\log K}\\right\\}\\right\\}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows by Lemma 19. ", "page_idx": 25}, {"type": "text", "text": "Proof of bound (37). By Lemma 20 we skip at most one outstanding observation per round. Thus, wehavethat ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{d}_{t_{m}}\\geq\\sqrt{\\mathcal{D}_{t_{m}+\\widehat{d}_{t_{m}}}/c}\\geq\\sqrt{\\sum_{i=1}^{m}\\widehat{d}_{t_{i}}/c}=\\frac{\\sqrt{\\widehat{d}_{t_{m}}+\\sum_{i=1}^{m-1}\\widehat{d}_{t_{i}}}}{\\sqrt{c}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By solving the quadratic inequality in $\\widehat{d}_{t_{m}}$ we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{d}_{t_{m}}\\geq\\frac{1+\\sqrt{1+4c\\sum_{i=1}^{m-1}\\widehat{d}_{t_{i}}}}{2c}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we prove by induction that $\\begin{array}{r}{\\widehat{d}_{t_{m}}\\geq\\frac{m}{2c}}\\end{array}$ . The induction base holds since $\\widehat{d}_{t_{1}}=1$ . For the inductive stepwe have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{d}_{t_{m}}\\geq\\frac{1+\\sqrt{1+4c\\sum_{i=1}^{m-1}\\widehat{d}_{t_{i}}}}{2c}\\geq\\frac{1+\\sqrt{1+m(m-1)}}{2c}\\geq\\frac{m}{2c}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the induction step is satisfied. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We list all our contributions in the abstract and introduction, and included a 548 literature review in the introduction to reflect the scope ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We present all our bounds in Table 1 and discuss about the gap between our results and existing lower and upper bounds in the introduction section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Theorems state their assumptions explicitly. Proofs are provided concisely within the main body, with full details in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work is based on theoretical analysis. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work does not include experiments. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We confirm that we adhere to ethical standards where applicable, and we note that no experiments were conducted as part of this research. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper focuses on theoretical foundations, and discussing potential societal impacts, positive or negative, is beyond its scope. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]