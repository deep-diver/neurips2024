[{"figure_path": "vU1SiBb57j/tables/tables_17_1.jpg", "caption": "Table 1: Hyperparameter setup for all tasks. For RPG, we use the default hyperparameters for sparse reward.", "description": "This table lists the hyperparameters used for all baselines and tasks in the experiments.  It shows values for parameters such as the number of environments, learning rates (critic, actor, and action), alpha learning rate (for SAC), V_min and V_max for distributional RL, the number of atoms, the optimizer used, target update rate, batch size, UTD ratio, discount factor, gradient clipping, replay buffer size, reclustering frequency, and mode embedding dimension. Note that separate hyperparameters are given for DDiffPG/DIPO/Diffusion-QL/Consistency-AC/TD3/SAC and for RPG.", "section": "5.1 Setup"}, {"figure_path": "vU1SiBb57j/tables/tables_18_1.jpg", "caption": "Table 3: Number of modes discovered, success rate (S.R.), and episode length (E.L.) for AntMazes and the maze with randomly initialized obstacles, averaged over 20 random seeds per case.", "description": "This table presents a quantitative comparison of the performance of DDiffPG against several baselines across four AntMaze environments and a randomized AntMaze environment with obstacles.  The metrics include the number of distinct behavioral modes learned, the success rate (percentage of episodes successfully completed), and the average episode length.  This data demonstrates the multimodal capabilities of DDiffPG and how it compares to single-mode approaches in terms of exploration, solution diversity, and efficiency.", "section": "Additional experimental results"}, {"figure_path": "vU1SiBb57j/tables/tables_18_2.jpg", "caption": "Table 4: Number of modes, success rate (S.R.), and episode length (E.L.) for robotic tasks, averaged over 20 random seeds.", "description": "This table presents the results of experiments conducted on four robotic manipulation tasks: Reach, Peg-in-hole, Drawer-close, and Cabinet-open.  For each task, it shows the number of distinct behavioral modes learned by DDiffPG and several baseline algorithms (TD3, SAC, DIPO, Diffusion-QL, Con-AC).  The success rate (S.R.) indicates the percentage of successful task completions, and the episode length (E.L.) represents the average number of steps taken to complete the task.  The data is averaged over 20 trials with different random seeds to provide a statistical measure of the algorithm's performance.", "section": "5.2 DDiffPG Masters Multimodal Behaviors"}]