[{"figure_path": "ZupoMzMNrO/figures/figures_0_1.jpg", "caption": "Figure 1: (a) Generate 512\u00d7512 images using DiT-XL/2, sampled by DDIM with 50 NFEs. (b) Generate 256x256 images using U-ViT-H/2, sampled by DPM-Solver-2 with 50 NFEs.", "description": "This figure shows image generation results from two different diffusion transformer models: DiT-XL/2 and U-ViT-H/2.  Subfigure (a) displays 512x512 images generated using DiT-XL/2 and the DDIM sampler, while subfigure (b) shows 256x256 images generated using U-ViT-H/2 and the DPM-Solver-2 sampler.  Both used 50 noise-removing steps (NFEs).  The figure highlights the visual quality achieved by each model and serves as an initial comparison of the two models.", "section": "Abstract"}, {"figure_path": "ZupoMzMNrO/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of Learning-to-Cache. When a layer is activated, the calculation proceeds as usual. In contrast, when a layer is disabled, the computation of the non-residual path is bypassed, and the results from the previous step are utilized instead. The router \u03b2 smoothly controls the transition between two endpoints e\u0189(xs, s) and e0(xm, m).", "description": "This figure illustrates the Learning-to-Cache method. It shows how the model dynamically chooses to either perform a full calculation for a layer or reuse the results from a previous step. This choice is controlled by a 'router' (\u03b2) that smoothly interpolates between performing all computations and using only cached values. The figure highlights that the method operates layer-wise, showing the computation flow through multiple transformer layers (MHSA and Feedforward) and demonstrating the activation/disabling of computations for each layer based on the router's decision.", "section": "3 Method"}, {"figure_path": "ZupoMzMNrO/figures/figures_4_1.jpg", "caption": "Figure 3: Approximation Error for DiT and U-ViT in different timesteps and different layers", "description": "This figure visualizes the approximation error for both DiT and U-ViT models across various timesteps and layers.  It illustrates the error in approximating the output of a layer using a cached value from a previous timestep. The plots show the error for both Multi-head Self-Attention (MHSA) and Feedforward layers separately for each model.  The x-axis represents the layer index, and the y-axis represents the approximation error.  Different colored lines represent different timesteps within the denoising process.  The figure is crucial to show the different redundancy patterns in DiT and U-ViT models which motivates the design of the proposed Learning-to-Cache method, as it highlights the varying degrees of error that would result from using cached values.", "section": "3.3 Caching the Layer: A Feasible Choice for the Interpolation I"}, {"figure_path": "ZupoMzMNrO/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of Learning-to-Cache. When a layer is activated, the calculation proceeds as usual. In contrast, when a layer is disabled, the computation of the non-residual path is bypassed, and the results from the previous step are utilized instead. The router \u03b2 smoothly controls the transition between two endpoints e\u03b8(xs, s) and e\u03b8(xm, m).", "description": "This figure illustrates the Learning-to-Cache method.  It shows how a router (\u03b2) dynamically controls whether a layer's computation is performed (activated) or skipped (disabled) by reusing results from a previous step. This is done to balance speed and accuracy. When a layer is disabled, the non-residual path is bypassed, saving computation time.", "section": "3 Method"}, {"figure_path": "ZupoMzMNrO/figures/figures_7_1.jpg", "caption": "Figure 4: Speed-Quality Tradeoff for DiT-XL/2 and U-ViT-H/2 with 20 denosing steps as the basis. The dashed line indicates the performance without applying inference acceleration.", "description": "This figure shows the trade-off between FID (Frechet Inception Distance, a metric for image quality) and inference latency for different methods on DiT-XL/2 and U-ViT-H/2 models.  The x-axis represents latency (in seconds), and the y-axis represents FID. Lower FID values indicate better image quality, and lower latency values indicate faster inference.  Several heuristic methods for selecting layers to cache are compared against the proposed \"Learning-to-Cache\" method. The dashed line shows the baseline performance without any acceleration techniques. The plot demonstrates that the proposed method achieves a better balance between image quality and inference speed compared to other methods.", "section": "4 Experiments"}, {"figure_path": "ZupoMzMNrO/figures/figures_8_1.jpg", "caption": "Figure 5: Learned Router \u03b2 for DiT-XL/2 (Top) and U-ViT-H/2 (Bottom). Different caching patterns are observed in different types of diffusion transformers.", "description": "This figure visualizes the learned router beta (\u03b2) for two different transformer architectures, DiT-XL/2 and U-ViT-H/2. The heatmaps show the caching patterns learned by the model for each layer across different denoising steps.  The lighter colors indicate that the corresponding layer is more likely to be cached (computation skipped), while darker colors mean the layer is less likely to be cached.  The figure highlights that different transformer models exhibit distinct caching behaviors, implying that an adaptive caching strategy is necessary for optimal performance across different architectures. The top row shows the learned router for DiT-XL/2, and the bottom row shows the learned router for U-ViT-H/2.  The visualization is separated by self-attention (a) and MLP (b) layers.", "section": "4.3 Analysis"}, {"figure_path": "ZupoMzMNrO/figures/figures_8_2.jpg", "caption": "Figure 6: Effect of threshold \u03b8.", "description": "This figure shows the impact of different threshold values (\u03b8) on the trade-off between image quality (measured by FID) and inference speed (measured by latency).  Various threshold values are tested, showing a relationship between the threshold and the speed-quality balance.  A higher threshold leads to faster inference but potentially compromises image quality, while a lower threshold gives better quality but slower speed. The optimal choice of threshold depends on the desired balance between speed and quality.", "section": "4.3 Analysis"}, {"figure_path": "ZupoMzMNrO/figures/figures_15_1.jpg", "caption": "Figure 7: The quantitative results for layer dropping and layer caching in Section 4.3. (a) DDIM Pipeline with 20 NFE. (2) Our method L2C with 20 NFE (3) Learn to drop the layers by our algorithm. (4) Randomly drop layers. The results here, except the first line as the baseline, all speed up the inference by around 1.18x-1.19x.", "description": "This figure compares the image generation results of four different methods: DDIM, L2C (the proposed method), a method that learns to drop layers, and a method that randomly drops layers.  Each method is evaluated using 20 noise-removing steps (NFEs), a measure of computational cost. The top row shows the original images from the ImageNet dataset. The subsequent rows show the generated images from each method, demonstrating the effects of layer caching and dropping techniques on image quality. The caption indicates that the proposed L2C method and the layer-dropping method achieve faster inference than DDIM, but only the proposed L2C method maintain the quality.", "section": "B.2 Layer Dropout v.s. Layer Cache"}]