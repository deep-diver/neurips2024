[{"figure_path": "qKfiWNHp6k/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of open-world zero-shot object recognition performance using ground-truth (GT) boxes, SAM proposals generate by automatic mask generator, and GLIP boxes on the LVIS dataset. * indicate finetune the CLIP with Adapter. The training time test on one V100 GPU", "description": "This table compares the performance of open-world zero-shot object recognition using three different methods: ground-truth boxes, SAM proposals (generated automatically), and GLIP boxes.  It shows the Average Precision (AP) for different metrics (APr, APf, APall) and training time on a single V100 GPU. The table also highlights the impact of finetuning CLIP with an adapter and the use of different proposal generation methods on performance.", "section": "4 Experiments"}, {"figure_path": "qKfiWNHp6k/tables/tables_5_2.jpg", "caption": "Table 2: Evaluation of zero-shot object detection on the LVIS minival dataset.", "description": "This table presents the performance comparison of zero-shot object detection methods on the LVIS minival dataset.  It shows the Average Precision (AP) and Average Precision for rare categories (APr) for three different models: GLIP-L, GroundingDINO-L, and RegionSpot-Pro1336.  The table highlights RegionSpot-Pro1336's superior performance, exceeding both GLIP-L and GroundingDINO-L in both AP and APr, despite using less training data.", "section": "4.1 Zero-shot Inference for Region Recognition"}, {"figure_path": "qKfiWNHp6k/tables/tables_6_1.jpg", "caption": "Table 3: Comparison under the ViLD protocol [7]. All methods use the ResNet50 backbone. * indicate pre-training with CC-3M", "description": "This table compares the performance of various methods under the ViLD protocol [7] for zero-shot object detection using the ResNet50 backbone. The table shows the Average Precision (AP) scores for rare and all categories. The * indicates that the RegionCLIP model was pre-trained with CC-3M. The table highlights the performance of the proposed RegionSpot model, demonstrating its superiority over the other methods, especially in terms of AP for rare categories.", "section": "4.1 Zero-shot Inference for Region Recognition"}, {"figure_path": "qKfiWNHp6k/tables/tables_6_2.jpg", "caption": "Table 4: Evaluation of zero-shot instance segmentation on the LVIS minival dataset.", "description": "This table presents the performance comparison of zero-shot instance segmentation on the LVIS minival dataset among three different methods: X-Decoder, OpenSeed, and the proposed RegionSpot-Pro1336.  The metrics used for comparison are Average Precision (AP) for different aspects: AP<sub>r</sub> (Average Precision for region), AP<sub>c</sub> (Average Precision for category), AP<sub>f</sub> (Average Precision for both region and category), and overall AP. The results show that RegionSpot-Pro1336 significantly outperforms the other two methods in all metrics.", "section": "4.2 Ablation Study"}, {"figure_path": "qKfiWNHp6k/tables/tables_7_1.jpg", "caption": "Table 5: Evaluation of zero-shot object detection on the ODinW dataset.", "description": "This table presents the performance comparison of several zero-shot object detection methods on the ODinW dataset.  The methods are evaluated across six categories (Aerial, Drone, Aquarium, PascalVOC, shellfish, vehicles), and an average performance is also reported.  The results highlight the improvements achieved by RegionSpot-Pro1336, which significantly outperforms other methods like GLIP-T, GLIP-L, and GroundingDINO-T.", "section": "4.2 Ablation Study"}, {"figure_path": "qKfiWNHp6k/tables/tables_7_2.jpg", "caption": "Table 6: Ablation experiments on LVIS. (a) The effective of CLIP vision encoder; (b) Position-aware tokens selection; (c) Depth of RegionSpot.", "description": "This ablation study analyzes the impact of different components on the RegionSpot model's performance on the LVIS dataset. It investigates three aspects: (a) the effect of integrating CLIP vision embeddings, (b) the selection of position-aware tokens generated at different stages in the SAM model, and (c) the effect of varying the depth of the RegionSpot model architecture. The results quantify how each component contributes to the overall performance improvement.", "section": "4.2 Ablation Study"}, {"figure_path": "qKfiWNHp6k/tables/tables_7_3.jpg", "caption": "Table 7: Ablation experiments on LVIS. (a) The effective of propmpt engineering; (b) The effective of SAM", "description": "This table presents the ablation study results on the LVIS dataset, focusing on two aspects: prompt engineering and the impact of different SAM (Segment Anything Model) sizes.  The first part shows how using multiple boxes as prompts and incorporating text prompts affects the performance, measured by Average Precision (AP) across different metrics. The second part demonstrates how different SAM backbone sizes (ViT-B and ViT-L) influence the box and mask Average Precision.", "section": "4.3 Visualization"}, {"figure_path": "qKfiWNHp6k/tables/tables_13_1.jpg", "caption": "Table 8: Comparisons with the training efficiency.", "description": "This table compares the training efficiency (in GPU hours) and the number of learnable parameters (in millions) of different methods for open-world object detection.  The methods compared are RegionCLIP, GLIP-T, GLIP-L, GDINO-L, and RegionSpot-Pro.  The training datasets used for each method are also listed.  RegionSpot-Pro demonstrates significantly greater training efficiency than other methods.", "section": "4 Experiments"}, {"figure_path": "qKfiWNHp6k/tables/tables_13_2.jpg", "caption": "Table 9: Effect of increasing the detection training data.", "description": "This table shows the performance improvements observed when augmenting the training data size.  By integrating additional detection data from diverse sources, the model's performance consistently improves. Compared to training with only Objects365, adding OpenImages improves AP from 15.9 to 20.1. Further including V3DET results in an overall AP of 23.7. This improvement is particularly significant for rare categories (an increase of +8.5 AP to 24.9), highlighting the benefit of a larger vocabulary in the training data.", "section": "4 Experiments"}]