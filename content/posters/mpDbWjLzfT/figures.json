[{"figure_path": "mpDbWjLzfT/figures/figures_1_1.jpg", "caption": "Figure 1: Problem setup. Consider several source models trained using data from different weather conditions. During the deployment of these models, they may encounter varying weather conditions that could be a combination of multiple conditions in varying proportions (represented by the pie charts on top). Our goal is to infer on the test data using the ensemble of models by automatically figuring out proper combination weights and adapting the appropriate models on the fly.", "description": "The figure illustrates a scenario where multiple source models, each trained on a specific weather condition (sunny, rainy, snowy), are used to make predictions on a test data set that has a dynamic mix of weather conditions. The pie charts at the top represent the proportions of different weather conditions in each test set. The goal is to automatically learn optimal combination weights for the source models and adapt only the models relevant to the current test data distribution.", "section": "1 Introduction"}, {"figure_path": "mpDbWjLzfT/figures/figures_4_1.jpg", "caption": "Figure 2: Overall Framework. During test time, we aim to adapt multiple source models in a manner such that it optimally blends the sources with suitable weights based on the current test distribution. Additionally, we update the parameters of only one model that exhibits the strongest correlation with the test distribution.", "description": "This figure illustrates the CONTRAST framework's operation during the test phase.  It involves two steps: first, calculating optimal combination weights for multiple source models based on the current test batch's distribution; second, updating the parameters of the model most strongly correlated with the test distribution using existing test-time adaptation methods. The figure visually depicts these steps, highlighting the backpropagation of combination weights and model parameter updates. The use of a weighted pseudo-label aids in combining source model outputs.", "section": "3 CONTRAST Framework"}, {"figure_path": "mpDbWjLzfT/figures/figures_6_1.jpg", "caption": "Figure 2: Overall Framework. During test time, we aim to adapt multiple source models in a manner such that it optimally blends the sources with suitable weights based on the current test distribution. Additionally, we update the parameters of only one model that exhibits the strongest correlation with the test distribution.", "description": "This figure illustrates the CONTRAST framework's two main steps for each test batch: 1) learning optimal combination weights for source models using weighted pseudo-labels and Shannon entropy minimization, and 2) updating parameters of the source model with the highest weight using a state-of-the-art TTA method.  The figure visually depicts the flow of data, weight calculation, model updates and information between time steps, emphasizing the continuous adaptation process.", "section": "3 CONTRAST Framework"}, {"figure_path": "mpDbWjLzfT/figures/figures_9_1.jpg", "caption": "Figure 3: Comparison with baselines in terms of source knowledge forgetting. Maintaining the same setting as in Table 2, we demonstrate that by integrating single-source methods with CONTRAST, the source knowledge is better preserved during dynamic adaptation. Unlike all these single-source methods, our algorithm demonstrates virtually no forgetting throughout the entire adaptation process.", "description": "This figure compares the performance of CONTRAST against three baseline single-source test time adaptation methods (Tent, CoTTA, and EaTA) in terms of maintaining source knowledge during dynamic adaptation.  The top panel shows the classification accuracy of each method across 15 noisy domains of CIFAR-100C. The bottom panel shows the loss of classification accuracy (forgetting) on the source test sets.  The results demonstrate that CONTRAST effectively mitigates catastrophic forgetting, maintaining consistently high accuracy on source domains even after adapting to multiple target domains.", "section": "Analysis of Forgetting"}, {"figure_path": "mpDbWjLzfT/figures/figures_22_1.jpg", "caption": "Figure 4: Visual Comparison of CONTRAST with Baselines for Semantic Segmentation Task. Each row in the figure corresponds to a different weather condition (rain, snow, fog, and night from top to bottom). It is evident that CONTRAST outperforms the baselines in terms of segmentation results.", "description": "This figure compares the semantic segmentation results of CONTRAST with the baseline Tent method on the ACDC dataset. Four rows represent different weather conditions (rain, snow, fog, night) with the input image, ground truth mask, Tent-best results, and CONTRAST results.  It visually demonstrates that CONTRAST provides better segmentation results than the baseline method, especially in challenging weather conditions.", "section": "F Semantic Segmentation"}]