[{"type": "text", "text": "Parametric model reduction of mean-field and stochastic systems via higher-order action matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jules Berman\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tobias Blickhan\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Courant Institute of Mathematical Sciences New York University New York, NY 10012 jmb1174@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Courant Institute of Mathematical Sciences New York University New York, NY 10012 obias.blickhan@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Benjamin Peherstorfer Courant Institute of Mathematical Sciences New York University New York, NY 10012 pehersto@cims.nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters. The learned models can act as surrogates of classical numerical models to efficiently predict the system behavior over the physics parameters. Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameterand time-dependent gradient fields that represent approximations of the population dynamics. The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters. We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process. We demonstrate on Vlasov-Poisson instabilities as well as on high-dimensional particle and chaotic systems that our approach accurately predicts population dynamics over a wide range of parameters and outperforms state-of-the-art diffusion-based and flow-based modeling that simply condition on time and physics parameters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting the behavior of time-dependent processes $X_{t,\\mu}$ over time $t$ and across varying physics parameters $\\mu$ is a key challenge in computational science and engineering [46, 65]. The dynamics of $X_{t,\\mu}$ typically are described by systems of (stochastic) differential equations, which are derived from physics models and can be computationally expensive to simulate [40, 32]. Thus, it is desirable to learn reduced or surrogate models that can be rapidly evaluated to predict the system behavior across varying physics parameters [72, 10, 11, 45]. ", "page_idx": 0}, {"type": "text", "text": "Reduced modeling via learning population dynamics Given a data set of samples, i.e., realizations of the random variable $X_{t,\\mu}$ on a suitable domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\{X_{t_{j},\\mu_{k}}^{i}\\,|\\,i=1,\\dots,N_{x},\\quad j=1,\\dots,N_{t},\\quad k=1,\\dots,N_{\\mu}\\}\\subset\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "we aim to learn a dynamical-system reduced model to rapidly predict samples that approximately follow the same law $\\rho_{t,\\mu}$ as $X_{t,\\mu}$ over time $t$ and varying physics parameter $\\mu$ . We refer to the evolution of $\\rho_{t,\\mu}$ in time as population dynamics. Learning the population dynamics instead of learning the dynamics of the individual trajectories $t\\mapsto X_{t,\\mu}^{i}$ for all $i=1,\\ldots,N_{x}$ and $\\mu$ can be beneficial: There are cases where $\\rho_{t,\\mu}$ does not change in time, yet every sample trajectory $t\\mapsto X_{t,\\mu}^{i}$ follows complicated dynamics. For example, consider incompressible fluid dynamics with constant density. Samples corresponding to particles that comprise the fluid can have complicated trajectories, whereas on a distribution level, the density of the fluid is constant and so are the population dynamics. Furthermore, learning population dynamics seamlessly treats deterministic and stochastic systems because the stochastic models that we consider can be expressed as deterministic Fokker-Planck equations on the population level. ", "page_idx": 1}, {"type": "text", "text": "Our approach: Learning parametric minimal energy vector fields that represent population dynamics Building on standard literature on optimal transport theory [8] as well as the so-called action-matching loss introduced in [61], we pose a variational problem to learn gradient fields $\\nabla s_{t,\\mu}$ so that the continuity equation corresponding to the vector field given by $\\nabla s_{t,\\mu}$ approximates the population dynamics $\\rho_{t,\\mu}$ of the samples (1). In the spirit of reduced modeling [72, 10, 11, 45], we seek a vector field $s_{t,\\mu}$ that generalizes to different values of the physics parameters $\\mu$ . We therefore optimize for $s_{t,\\mu}$ that minimizes the average objective of a variational problem over all parameters $\\mu\\sim\\nu$ , where $\\nu$ describes the distribution of parameters on the domain $\\mathcal{D}\\subset\\mathbb{R}^{p}$ . We parametrize $s_{t,\\mu}$ with a neural network with weight modulation [39, 13] so that it can be evaluated quickly over $t$ and $\\mu$ . ", "page_idx": 1}, {"type": "text", "text": "Rapid sample generation in inference phase Predictions at inference time at new physics parameters $\\mu$ are made by sampling based on the vector field $\\nabla s_{t,\\mu}$ , which means that our approach represents $\\rho_{t,\\mu}$ through the application of $\\nabla s_{t,\\mu}$ on an initial condition. Importantly, time $t$ in the inference step corresponds to the time of the physics problem so that in one inference step a whole sample trajectory is obtained, rather than a sample at one specific time point as in regular conditioning-based methods (see literature review). Thus, we can rapidly generate samples that follow the law $\\rho_{t,\\mu}$ in the inference phase. ", "page_idx": 1}, {"type": "text", "text": "Stabilizing training with higher-order quadrature An important part of our contribution is stabilizing the training procedure by accurately estimating the objective of the variational problems from few data samples. In particular, instead of uniformly sampling over the data (1), we introduce an empirical loss (8) that utilizes higher-order quadrature [27] in the time direction so that the learned $\\nabla s_{t,\\mu}$ accurately captures the dynamics over time $t$ . Consequently, we refer to our approach as higher-order action matching (HOAM). Our numerical experiments show that the higher-order quadrature in the empirical loss is key for learning gradient fields $\\nabla s_{t,\\mu}$ that accurately capture the evolution in time $t$ and that generalize across physics parameters $\\mu$ . ", "page_idx": 1}, {"type": "text", "text": "Literature review We review relevant literature; see Figure 1 for an overview. ", "page_idx": 1}, {"type": "text", "text": "Non-intrusive and data-driven surrogate modeling There is a range of surrogate and latent modeling methods that aim to learn or reduce the sample dynamics of the realizations rather than the population dynamics, such as dynamic mode decomposition, Koopman-based methods, and others [71, 76, 86, 12, 46, 58, 92] as well as neural network-based methods such as neural ordinary differential equations [19, 28, 48]. There also are methods for stochastic systems [51, 42, 88, 19, 28, 73, 21]. However, all of these methods ignore physics parameter dependencies and/or aim to learn the sample dynamics, whereas we focus on parametric population dynamics. ", "page_idx": 1}, {"type": "text", "text": "Population dynamics and trajectory inference Learning population dynamics has been considered extensively in computational biology in the context of gene expression, where the focus is on learning from independent samples at selected time points rather than from sample trajectories [34, 30, 93, 75, 85, 47]; however, many of these approaches [17, 84] are simulation-based and thus require integrating dynamics during the training or parameterizing the density additionally to the vector field. These works also are not concerned with generalizing over a range of physics parameters in many cases. ", "page_idx": 1}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/d620aa5f446ee1f7e4fe197a02c52a05f0f4c142f055524f6806942b2a639fb6.jpg", "img_caption": ["Figure 1: Parametric model reduction with our HOAM seeks to learn vector fields that represent population dynamics $\\rho_{t}$ over time $t$ . In contrast, parametric model reduction with score-based diffusion denoising and flow-based modeling requires conditioning on time $t$ which leads to separate, costly inference steps for each time step of a sample trajectory. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Diffusion- and flow-based modeling There is a large body of work on diffusion-based [91, 79, 36, 41, 81, 82] and flow-based modeling [2, 54]; see [1] for a detailed review. These approaches are not taking into account time $t$ because they learn paths between a reference and a target distribution only. There are works that condition on time $t$ and a parameter $\\mu$ such as [68, 14, 26, 37, 33, 38, 52], but this requires then generating a path for each time step at inference time, which is computationally expensive. Furthermore, the conditioning on time $t$ means that the target distribution $\\rho_{t,\\mu}$ at each time $t$ and $\\mu$ is different, and thus a separate hyper-parameter tuning can be required, which is impractical over many time steps and physics parameters as in our physics problems; see our numerical experiments. The works [15, 78, 50] compute transport-based solutions but parametrize different quantities than our approach, require actively sampling data, and ignore physics parameters $\\mu$ . We note that there also is work on forecasting with diffusion- and flow-based modeling [68, 62, 18, 20], which is a different task than our task of predicting across varying physics parameters. ", "page_idx": 2}, {"type": "text", "text": "Optimal transport Besides the machine learning literature, variational approaches for inferring vector fields are extensively used in optimal transport theory [5, 4]. Of particular importance to us is the formulation by Benamou and Brenier [8]. The Bennamou-Brenier formula describes a joint optimization problem over vector fields and paths in probability space and the action matching loss [61] is the restriction of this optimization problem to the case of a fixed path and the vector field parametrized by a neural network, which are core building blocks for us that we show can be used together with a parameter dependency. ", "page_idx": 2}, {"type": "text", "text": "Contributions We summarize our contributions: ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "(a) Developing a loss to learn population dynamics that remain valid across varying physics parameters by building on optimal transport literature [8] and action matching [61]. (b) Introducing higher-order quadrature schemes for the loss to efficiently couple the gradient fields over time. This leads to lower variance estimators of the loss that critically stabilize training. (c) Demonstrating on a range of physics problems from Vlasov-Poisson instabilities to high-dimensional chaotic systems that our approach leads to (i) accurate predictions of population dynamics and (ii) orders of magnitude speedups in inference/prediction over classical methods that numerically solve the underlying partial differential equations as well as standard diffusion- and flow-based models that condition on physical time. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We provide an implementation of our method at https://github.com/julesberman/HOAM. ", "page_idx": 3}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Parameter-dependent population dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continuity equation Let us consider data (1) corresponding to the probability measure $\\rho_{t,\\mu}$ , which is absolutely continuous for $t\\in[0,1]$ and $\\mu\\in\\mathcal D$ . We use the same notation for the measure and its density. The density $\\nu$ of $\\mu$ is also assumed to be absolutely continuous on $\\mathcal{D}$ . We consider population dynamics of $X_{t,\\mu}\\sim\\rho_{t,\\mu}$ that can be described by the continuity equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\rho_{t,\\mu}=-\\nabla\\cdot\\left(\\rho_{t,\\mu}v_{t,\\mu}\\right),\\qquad\\mathrm{for~all~}t\\in\\left[0,1\\right],\\mu\\in\\mathcal{D}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the initial condition $\\rho_{t=0,\\mu}=:\\rho_{0,\\mu}$ and vector field $v_{t,\\mu}$ . Notice that in our case the continuity equation (2) depends on the physics parameter $\\mu\\sim\\nu$ . There can be many vector fields $v_{t,\\mu}$ that lead to the same population dynamics (2). For example, if $v_{t,\\mu}$ is a vector field that describes the dynamics of $\\rho_{t,\\mu}$ via (2), then another vector field is given by $v_{t,\\mu}^{\\prime}=v_{t,\\mu}+w/\\rho_{t,\\mu}$ with any other $w$ that satisfies $\\nabla\\cdot w=0$ as long as $\\rho_{t,\\mu}$ is positive. ", "page_idx": 3}, {"type": "text", "text": "Uniqueness via gradient fields and the corresponding elliptic problems Because we aim to learn a vector field from sample data (1) that describes the population dynamics (2) of the corresponding law $\\rho_{t,\\mu}$ , it is helpful to remove this non-uniqueness. One way to do so is to restrict the vector field to $v_{t,\\mu}=\\nabla s_{t,\\mu}$ so that it is a gradient field [4, p. 45]. Plugging $v_{t,\\mu}=\\nabla s_{t,\\mu}$ into (2), together with the assumptions $\\rho_{t,\\mu}>0$ and $\\begin{array}{r}{\\int_{\\mathcal X}\\partial_{t}\\rho_{t,\\mu}\\mathrm{d}x=0}\\end{array}$ , leads to parametric elliptic problems in $s_{t,\\mu}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\nabla\\cdot(\\rho_{t,\\mu}\\nabla s_{t,\\mu})=\\partial_{t}\\rho_{t,\\mu}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with coefficient function $\\rho_{t,\\mu}$ , right-hand side (source term) $\\partial_{t}\\rho_{t,\\mu}$ , and homogeneous Neumann boundary conditions $\\rho_{t,\\mu}\\nabla s_{t,\\mu}\\cdot\\hat{\\boldsymbol{n}}=0$ on $\\partial\\mathcal{X}$ with normal vector $\\hat{\\boldsymbol{n}}$ for all $t\\in[0,1]$ and $\\mu\\in\\mathcal D$ . The weak forms of the elliptic problems (3) lead to energy minimization problems that can be used to learn the gradient field $s_{t,\\mu}$ via optimization: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{s\\in H^{1}(\\rho_{t,\\mu},x)}E_{t,\\mu}(s):=\\operatorname*{min}_{s\\in H^{1}(\\rho_{t,\\mu},x)}\\frac{1}{2}\\int_{\\mathcal{X}}|\\nabla s|^{2}\\rho_{t,\\mu}\\mathrm{d}x-\\int_{\\mathcal{X}}\\partial_{t}\\rho_{t,\\mu}s\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for each $t~\\in~[0,1]$ and $\\mu\\ \\in\\ {\\mathcal{D}}$ . The space $H^{1}(\\rho_{t,\\mu},\\mathcal{X})$ contains functions $s$ with $\\int_{\\mathcal{X}}|\\nabla s|^{2}\\rho_{t,\\mu}\\mathrm{d}x<\\infty$ , which is the energy (semi-)norm corresponding to the $\\rho_{t,\\mu}$ -weighted inner product [29, Sec. 2.3.2]. ", "page_idx": 3}, {"type": "text", "text": "Optimal transport Standard elliptic theory guarantees unique solutions up to constants of (4) in the Sobolev space $H^{1}(\\mathcal{X})$ under strong assumptions on $\\rho_{t,\\mu}$ such as uniform boundedness by a positive constant for all $t$ and $\\mu$ ; see [29, Proposition 2.2] and [11, Section 3.2]. The theory of optimal transport allows treating the much more general case when $\\rho_{t,\\mu}$ is not uniformly bounded away from zero and possibly atomic; we refer to [8] and [74, Section 5.3.1] for details. Among all vector fields $v_{t,\\mu}$ that are compatible to $\\rho_{t,\\mu}$ in the sense of (2), gradient fields $\\nabla s_{t,\\mu}$ have the smallest associated kinetic energy $\\textstyle{\\frac{1}{2}}\\int_{\\mathcal{X}}|v|^{2}\\rho_{t,\\mu}\\mathrm{d}x$ , which is the objective considered in [8]. In the language of optimal transport and in particular the formalism of [63], vector fields with minimal kinetic energy describe tangent vectors to the curve $t\\,\\mapsto\\,\\rho_{t,\\mu}$ . The metric is the inner product of $L^{2}(\\rho_{t,\\mu},\\mathcal{X},\\mathbb{R}^{d})$ . This is the weak Riemannian structure of $\\mathcal P(\\mathcal X)$ equipped with the Kantorovich-Rubinstein metric and described in detail in [5, Chapter 8]. We give a short description in Appendix $\\mathrm{E}$ . ", "page_idx": 3}, {"type": "text", "text": "Energy functional with entropy term Instead of the energy (4), we can also use other choices of the energy to select gradient fields, as long as energy functions are convex to maintain uniqueness. We consider an energy that is based on a different notion of discrepancy on $\\mathcal{P}(\\mathcal{X})$ , the entropic optimal transport or Schr\u00f6dinger bridge problem [77, 56], ", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{t,\\mu}^{\\epsilon}(s)=\\frac12\\int_{\\mathcal{X}}|\\nabla(s-\\frac{\\epsilon^{2}}{2}\\log\\rho_{t,\\mu})|^{2}\\rho_{t,\\mu}\\mathrm{d}x-\\int_{\\mathcal{X}}\\partial_{t}\\rho_{t,\\mu}s\\mathrm{d}x\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which depends on $\\epsilon\\geq0$ . The energy $E_{t,\\mu}^{\\epsilon}$ is of particular interest for two reasons: One, the Euler-Lagrange equation of (5) in strong form is the Fokker-Planck equation for $s_{t,\\mu}^{\\mathrm{\\epsilon}}$ : $\\begin{array}{r}{\\partial_{t}\\rho_{t,\\mu}=-\\nabla\\cdot(\\rho_{t,\\mu}\\nabla s_{t,\\mu}^{\\epsilon})+\\frac{\\epsilon^{2}}{2}\\Delta\\rho_{t,\\mu}}\\end{array}$ , again with homogeneous Neumann boundary conditions for all $t\\in[0,1]$ and $\\mu\\in\\mathcal D$ ; see Appendix C. This means we can efficiently generate samples after learning $s_{t,\\mu}^{\\epsilon}$ via corresponding stochastic differential equations (SDEs). Two, it can be interpreted as regularizing the field $s_{t,\\mu}$ , which we discuss in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "2.2 Loss for learning vector fields over time $t$ and physics parameter $\\mu$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Variational formulation over $t$ and $\\mu$ So far we just carried along time $t$ and physics parameter $\\mu$ but did not address them in the variational problems, i.e., we had separate variational problems (4) for all $t\\in[0,1]$ and $\\mu\\sim\\nu$ . We now propose to consider the average energy over $t$ and $\\mu$ to infer a map $s:[0,1]\\times\\mathcal{D}\\to H^{1}(\\rho_{t,\\mu},\\mathcal{X}),(t,\\mu)\\mapsto s_{t,\\mu}$ , which is called a solution map in reduced modeling [72, 10, 11, 45], ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{s:[0,1]\\times\\mathscr{D}\\to H^{1}(\\rho_{t,\\mu},x)}E^{\\epsilon}(s):=\\operatorname*{min}_{s}\\int_{\\mathcal{D}}\\int_{0}^{1}E_{t,\\mu}^{\\epsilon}(s_{t,\\mu})\\,\\mathrm{d}t\\,\\mathrm{d}\\nu(\\mu).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that time $t$ and physics parameter $\\mu$ have two different effects on the gradient field $\\nabla s_{t,\\mu}$ : Time $t$ couples the elliptic problems (i.e., (3) for $\\epsilon\\,=\\,0$ ) via the time derivative $\\partial_{t}\\rho_{t,\\mu}$ ; see Appendix D. In contrast, the elliptic problems are uncoupled over $\\mu$ and can be considered separately. This means that to compute the solution to an elliptic problem for one value of $\\mu\\in\\mathcal D$ , one does not need to consider any other $\\mu^{\\prime}\\in\\mathcal{D}$ . This will allow us to sample the physics parameters over $\\mathcal{D}$ independently from each other when estimating the corresponding loss, whereas we will use higher-order quadrature to obtain an accurate approximation of the time integral to ensure the coupling between the time points is reflected in $s_{t,\\mu}$ ; see Section 2.3. ", "page_idx": 4}, {"type": "text", "text": "Loss for learning gradient fields from samples over $t$ and $\\mu$ The energy $E_{t,\\mu}$ defined in (4) as well as the energy $E_{t,\\mu}^{\\epsilon}$ defined in (5) leads to a loss that can be estimated from samples (1). The quantity $\\partial_{t}\\rho_{t,\\mu}$ appears in (4) and (5), which is typically unavailable when we have access to data samples (1) only. Integration by parts of the term involving $\\partial_{t}\\rho_{t,\\mu}$ eliminates it, see also Appendix D. We arrive at ", "page_idx": 4}, {"type": "equation", "text": "$$\nE^{\\epsilon}(s)\\!=\\!\\int_{\\mathcal{D}}\\left[\\int_{0}^{1}\\int_{\\mathcal{X}}\\left(\\frac{1}{2}|\\nabla s_{t,\\mu}|^{2}\\!+\\!\\partial_{t}s_{t,\\mu}+\\frac{\\epsilon^{2}}{2}\\Delta s_{t,\\mu}\\right)\\rho_{t,\\mu}\\mathrm{d}x\\mathrm{d}t-\\int_{\\mathcal{X}}s_{t,\\mu}\\rho_{t,\\mu}\\mathrm{d}x\\right|_{t=0}^{t=1}\\!\\right]\\mathrm{d}\\nu(\\mu)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that this loss is comprised only of expectation values with respect to $\\rho_{t,\\mu}$ and is therefore well-defined also for empirical distributions. The choice $\\varepsilon>0$ assumes that the Fisher information of $\\rho_{t,\\mu}$ is finite. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Loss functions of the form as (7) but without the parameter dependence have been used in [61] and [47, Theorem 2.1]. In fact, the case with $\\epsilon=0$ appears already in $[8$ , Equation 35] and [64, Section 3]. We build on these results but work with population dynamics that depend on physics parameters, which leads to the loss shown in (7). ", "page_idx": 4}, {"type": "text", "text": "2.3 Parameterizing the vector field, estimating the loss from data, sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Parametrizing $s_{t,\\mu}$ with weight modulations We parametrize the vector field $s_{t,\\mu}$ via a neural network with continuous versions of low-rank adaptation (CoLoRA) layers, which have been successfully used for parametric model reduction of deterministic time-dependent dynamical systems [13]; see also [39]. The layers have the form $\\mathcal{C}(x)=W x+\\phi(t,\\mu)A B x+b$ , where $W$ is a weight matrix, $A,B$ are low-rank matrices, $b$ is a bias vector, and $\\phi(t,\\mu)\\in\\mathbb{R}$ is a scalar weight modulation; see Appendix B. Only the weight modulations $\\phi(t,\\mu)$ depend on time $t$ and physics parameter $\\mu$ . We use a hyper-network $h:[0,1]\\times\\mathcal{D}\\times\\Psi\\to\\mathbb{R}$ that depends on the weight vector $\\psi\\,\\in\\,\\Psi\\,\\subseteq\\,\\mathbb{R}^{q}$ to map $t$ and $\\mu$ to the modulation weights $\\phi(t,\\mu)=h(t,\\mu;\\psi)$ . The weights $W,A,B,b$ , which are independent of $t$ and $\\mu$ , over all layers are collected into the weight vector $\\theta\\in\\Theta\\subseteq\\mathbb{R}^{q^{\\prime}}$ . Typically $q\\ll q^{\\prime}$ . Using the hyper-network encourages continuity of $s_{t,\\mu}$ in time $t$ , which is key for many physics problems [13]. ", "page_idx": 4}, {"type": "text", "text": "Combining higher-order quadrature and Monte Carlo sampling for estimating the loss from sample data Estimating the loss (7) from data can be challenging because the three nested integrals (expectations) over the samples $X_{t,\\mu}^{i}$ , time $t$ , and physics parameter $\\mu$ can have different properties and correspondingly need different numerical treatment. Our numerical results show that it is critical to accurately estimate the loss to avoid instabilities in the training; see Section 3 and Figure 2. ", "page_idx": 5}, {"type": "text", "text": "We propose a combination of higher-order numerical quadrature and Monte Carlo sampling to estimate the loss (7). In particular, we propose to use a higher-order quadrature rule for the time $t$ integral. Because it is a one-dimensional integral, standard higher-order quadrature rules from numerical analysis are applicable [27]. The time integral needs to be estimated with particular high accuracy to ensure the coupling between the time points as well as the coupling to the boundary terms to match the path from $\\rho_{0,\\mu}$ at time $t=0$ to $\\rho_{1,\\mu}$ at time $t=1$ . Our numerical results will show that estimating the time integral to high accuracy is essential for stabilizing the training. In contrast to the one-dimensional integral over time, the integrals over $\\mathcal{X}$ and the parameter domain $\\mathcal{D}$ can be high dimensional and thus we estimate them via Monte Carlo estimation. ", "page_idx": 5}, {"type": "text", "text": "We consider two high-order quadrature rules, composite Simpson\u2019s quadrature and GaussLegendre quadrature [27]; see Appendix A. We refer to our method as HOAM-S and HOAM-G when using either quadrature, respectively. Importantly, these quadrature rules require samples on specifically spaced time points, equidistant in the case of Simpson\u2019s and at the Gauss-Legendre nodes in the case of Gauss quadrature. If the data set (1) does not contain samples at these time points then we interpolate the data to the appropriate times. We note that for Simpson\u2019s quadrature, interpolation is typically unnecessary as data simulated with numerical methods often come at equispaced points in time. ", "page_idx": 5}, {"type": "text", "text": "We denote a Monte Carlo estimate of an expectation value obtained from a mini-batch as $\\begin{array}{r}{\\underline{{\\hat{\\mathbb{E}}}}_{x\\sim\\rho}^{n}[f]\\;:=\\;\\sum_{i=1}^{n}f(X^{i})}\\end{array}$ where $X^{1},X^{2},\\ldots,X^{n}\\;\\sim\\;\\rho$ . Then, the empirical loss with mini-batching o f sizes $n_{x},n_{\\mu}$ and $n_{t}$ quadrature points in time is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\!\\!\\hat{E}^{\\epsilon}(s)\\!=\\!\\hat{\\mathbb{E}}_{\\mu\\sim\\nu}^{n_{\\mu}}\\!\\left[\\sum_{n=1}^{n_{t}}w_{n}\\,\\hat{\\mathbb{E}}_{x\\sim\\rho_{t_{n},\\mu}}^{n_{x}}\\left[\\frac{1}{2}|\\nabla s_{t_{n},\\mu}|^{2}\\!+\\!\\partial_{t}s_{t_{n},\\mu}\\!+\\!\\frac{\\epsilon^{2}}{2}\\Delta s_{t_{n},\\mu}\\right]-\\hat{\\mathbb{E}}_{x\\sim\\rho_{t,\\mu}}^{n_{x}}\\left[s_{t,\\mu}\\right]\\right|_{t=0}^{t=1}\\!\\!\\!\\right]\\!\\!\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w_{n}$ are numerical quadrature weights and $t_{n}$ are the corresponding nodes; see Appendix A for the Simpson\u2019s quadrature and Gauss-Legendre weights and nodes. ", "page_idx": 5}, {"type": "text", "text": "Rapid predictions (inference) with learned reduced models Making predictions in the inference step means drawing samples that follow the law represented by the learned gradient field $\\nabla s_{t,\\mu}$ , which approximates the law $\\rho_{t,\\mu}$ of $X_{t,\\mu}$ . Because we train with the loss (7), we integrate the SDE $\\mathrm{d}\\hat{X}_{t,\\mu}=\\nabla s_{t,\\mu}(\\hat{X}_{t,\\mu})\\mathrm{d}t+\\epsilon\\mathrm{d}W_{t}$ , where $W_{t}$ are Wiener processes and $\\epsilon$ is the same $\\epsilon$ that is used in the training loss (7); see Appendix C. As initial condition, we use samples from $\\rho_{0,\\mu}$ at time $t=0$ . Of course other sampling schemes can be used [70]. ", "page_idx": 5}, {"type": "text", "text": "Notice that the time $t$ in the SDE used for generating samples is the same time as of the physics problem and thus of the sample trajectory. This means that the costs of the inference step of our HOAM for generating a trajectory of length $K$ scales as $\\mathcal{O}(K)$ . In contrast, introducing a conditioning on time and physics parameter in, e.g., noise-conditioned score matching (NCSM) [80] and conditional flow matching (CFM) or stochastic interpolants [2, 54] requires inferring a separate sampling path for each $t$ and $\\mu$ pair of interest. In particular, the inference costs of CFM scale as $O(K\\tau)$ , where $\\tau$ is the number of steps taken in the differential equation for generating one sample at one time point. For NCSM with annealed Langevin sampling, the inference costs scale as ${\\cal O}(K\\tau\\sigma)$ , where $\\sigma$ is the number of annealing steps. Contrasting this to the scaling of $\\mathcal{O}(K)$ of our HOAM approach shows that HOAM is well suited for fast predictions over $t$ and $\\mu$ as required in parametric model reduction. ", "page_idx": 5}, {"type": "text", "text": "3 Numerical experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Examples We consider the following parametric dynamical systems; details in Appendix B. 1. Harmonic oscillator: A collection of particles evolves in four-dimensional phase-space subject to a quadratic potential $V(x)=-{\\textstyle\\frac{1}{2}}\\omega^{2}|x|^{2}$ . In the experiments shown $\\omega=8$ . The particles are initially at rest and follow a normal Gaussian distribution in space with mean $m_{0}=[1,1]$ . To avoid the formation of a singularity at $\\begin{array}{r}{\\omega t=\\,\\frac{1}{2}\\pi}\\end{array}$ , we add white noise of strength $\\eta=5\\times10^{-2}$ to the momentum equation. For the case $\\eta=0$ , we have analytical expressions for $\\rho$ and $\\boldsymbol{s}$ , see Appendix B.1. ", "page_idx": 5}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/85e36525a6758f38c69851f1ea80a02ce0985039581932315e174a4e1b58a5e3.jpg", "img_caption": ["Figure 2: Left: During training the high-variance function $q(s)(t)$ needs to be numerically integrated for estimating the loss. Center left: The high variance leads to inaccurate estimates of the time integral by Monte Carlo, whereas higher-order numerical quadrature produces accurate estimates. Center right: Numerical quadrature in HOAM leads to stable estimates of the loss whereas Monte Carlo integration in AM leads to unstable behavior. Right: HOAM based on higher-order quadrature is stable and more accurate than AM. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "$\\scriptstyle{\\frac{\\upsilon}{\\mathcal{Q}}}$ . Two-stream instability We numerically solve the Vlasov-Poisson partial differential equations using a particle-in-cell method to generate samples (1). We consider the two-stream instability [22, 43] in a 1D1V configuration with collisions [87, Sec 2(b)(i)], with $\\beta=10^{-3}$ and $v_{0}=1$ as in [49]. These collisions lead to stochastic sample trajectories. The parameter $\\mu\\in[1.2,1.9]$ is a normalization constant related to the Debye length [83]. It controls the ratio between electric and inertial effects in the simulation. ", "page_idx": 6}, {"type": "text", "text": "3. Bump-on-tail instability Using the same numerical setup of the Vlasov-Poisson equation as for the two-stream instability, we also consider the the bump-on-tail instability [7, 35, 43]. The parameter varies as $\\mu\\in[1.3,2.0]$ . ", "page_idx": 6}, {"type": "text", "text": "4. Strong Landau damping We consider the strong Landau damping phenomenon that is governed by Vlasov-Poisson partial differential equations again but now in a 3D3V (sixdimensional) setup. A perturbation in the $x_{1}$ -direction leads to the formation of phase-space structures [59]. The parameter $\\mu\\in[0.5,1.5]$ is the mass of the charged particles. ", "page_idx": 6}, {"type": "text", "text": "5. High-dimensional chaos A Rayleigh\u2013B\u00e9nard convection leads to a density gradient that sets a fluid in motion. We consider a nine-dimensional dynamical system that is derived from such a flow, which exhibits cascades that lead to chaos [69]. The parameter $\\mu\\in[13.7,14.4]$ is the reduced Rayleigh number. ", "page_idx": 6}, {"type": "text", "text": "6. Particles in aharmonic trap We consider 50 particles in an aharmonic trap [16], which lead to 100-dimensional samples $X_{t,\\mu}^{i}$ that encode the positions of the particles. The particle positions are governed by a stochastic differential equation. The parameter $\\mu\\in[0.3,0.9]$ controls the velocity of the trap. ", "page_idx": 6}, {"type": "text", "text": "Setup We compare our higher-order action matching (HOAM) to the original version of action matching (AM) [61], where we handle the parameter dependence on $\\mu$ in the same way as in our approach. Additionally, we compare to noise-conditioned score matching (NCSM) where samples are generated via annealed Langevin dynamics [80] and conditional flow matching (CFM) [2, 54], for which we condition on time $t$ and $\\mu$ ; see Appendix B. ", "page_idx": 6}, {"type": "text", "text": "hHarOmAonMic  sotsacbilillaitzoers  etxraaminpilen.g  Wwei tleha rhni gthhee rf-ieolrd $s_{t}$ ra qndu apldorta $q(s)(t)=\\mathbb{\\hat{E}}_{\\mu\\sim\\nu}^{n_{\\mu}}\\mathbb{\\hat{E}}_{x\\sim\\rho_{t,\\mu}}^{n_{x}}[\\frac{1}{2}|\\nabla s_{t,\\mu}|^{2}+$ $\\begin{array}{r}{\\partial_{t}s_{t,\\mu}+\\frac{\\epsilon^{2}}{2}\\Delta s_{t_{n},\\mu}\\big]}\\end{array}$ over time $t$ , which is the function that needs to be integrated in time to estimate the loss (7). As Figure 2 (left) shows, this function is far from smooth and exhibits several sharp peaks, which make estimating the loss challenging. In AM [61], the time integral is estimated by averaging samples uniformly taken in time, which is equivalent to Monte Carlo estimation. Figure 2 (center left) shows the relative error in estimating the time integral using Monte Carlo integration as done by AM versus numerical quadrature as in our HOAM. The trapezoidal rule, the composite Simpson\u2019s rule, and Gauss-Legendre quadrature all produce highly accurate estimates, whereas Monte Carlo integration yields inaccurate estimates of the integral with high variance. ", "page_idx": 6}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/85f6971f8c477866a11bb4bb7729e9edb486c88d164030ebc4df2f09a9f1222c.jpg", "img_caption": ["Figure 3: Histograms of solution fields. Top: Bump-on-tail $t=20$ ) instability. Middle top: two-stream ( $t=20$ ) instability. Middle bottom: Strong Landau damping ( $t=4$ ) instability. HOAM with Simpson\u2019s and Gauss quadrature accurately predicts the fine scale features and multi-modality of the population density in the Vlasov problems. AM does not converge on the 6 dimensional problem. Bottom: High-dimensional chaos [69] ( $t=3.7$ , dim 3 vs dim 9). HOAM accurately predicts the low probability region that connects the two high probability regions while AM does not converge. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Poor numerical estimates by Monte Carlo lead to unstable and inaccurate estimates of the loss function in AM, which eventually causes the optimization to diverge as shown in Figure 2 (center right). In contrast, our HOAM, where training is done with higher-order quadrature (in this case Simpsons\u2019 rule), the loss curve is stable and of low variance. In Figure 2 (right) we plot the mean Wasserstein distance over time of solutions generated via a gradient field $\\boldsymbol{s}$ trained with with Simpson\u2019s (HOAM-S), Gauss quadrature (HOAM-G), and with Monte Carlo (AM) for five seeds, which determine the random initialization of the neural network. For some seeds, AM yields reasonable solutions while for others numerically instabilities lead AM to fail. In contrast, our quadrature-based HOAM is consistently stable and provides orders of magnitude more accurate results. ", "page_idx": 7}, {"type": "text", "text": "Accurate predictions with speedups for Vlasov-Poisson equations Our VlasovPoisson problems describe the interaction of charged particles with dynamics that depend on all other particles, which leads to mean-field dynamics for large numbers of particles $N_{x}$ . Thus, reduced modeling with HOAM is well suited for this problem because the natural dynamics to learn from such a system are the population dynamics $\\rho_{t,\\mu}$ rather than the sample dynamics; see Appendix B.2. We observe the particles computed with a particle-incell method and learn the gradient field $\\nabla s_{t,\\mu}$ with the proposed HOAM approach. For a test physics parameter $\\mu$ that controls the wave number, we then generate samples with $\\nabla s_{t,\\mu}$ and plot a histogram in Figure 3 for the bump-on-tail (top) and two-stream (middle top) instability. Our approach approximates well the histogram obtained with the classical particle-in-cell method. Figure 5 (right) shows that HOAM is the only method which provides speedup over the classical particle-in-cell (full) model, as NCSM and CFM lead to 1\u20132 orders of magnitude longer inference times than HOAM and the full models. ", "page_idx": 7}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/906de8a4b04ad087f4763f1bffe409a0ddb5b797a2baacad2f0a613902e105d6.jpg", "img_caption": ["Figure 4: Electric energy of bump-on-tail (top) and two-stream (bottom) instability. HOAM with Simpson\u2019s and Gauss quadrature accurately predicts the energy growth in the transient regime and oscillations at later times. The ground truth is displayed in blue. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "For the strong Landau damping problem in six dimensions (three spatial and three velocity), our HOAM approach achieves about 2 orders of magnitude speedup. This is because the runtime of the full model based on the traditional particle-in-cell method to compute the mean-field dynamics scales poorly with the dimension. In this example, the runtime of the full model increases by almost two orders of magnitude. In contrast, the runtime of our HOAM reduced model increases only from 6 to 8 seconds. This importantly shows that the computational costs of the inference step of reduced models built with HOAM avoid exponential scaling with the dimension in this example. ", "page_idx": 8}, {"type": "text", "text": "We now compute the electric energy as a quantity of interest from the generated samples over time $t$ for the test physics parameters, which we plot in Figure 4 and its relative error averaged over time (e.e.) in Table 1 (see (25)). Our HOAM approximates the electric energy well at later times, whereas NCSM and CFM lead to poorer approximations at later times $t$ . This is relevant because this non-linear regime is where numerical solvers become important; the initial (linear) growth regime can be approximated well by analytical perturbation theory. Also for the six-dimensional strong Landau damping problem, our HOAM approach provides accurate predictions of the electric energy with orders of magnitude speedups; see Table 1 and Figure 3 as well as Figure 7 in the appendix. ", "page_idx": 8}, {"type": "text", "text": "Speedups in inference step (predictions) Recall two limitations of introducing a time and physics parameter dependence in NCSM/CFM via conditioning (see page 3 and Section 2.3): (i) For each $t$ and $\\mu$ , a separate sampling path has to be computed, which leads to orders of magnitude higher inference runtimes than in HOAM; see Table 1, Section 2.3. (ii) For each $t$ and $\\mu$ pair, the target distribution $\\rho_{t,\\mu}$ is different, which can require $t$ - and $\\mu$ -specific tuning of hyper-parameters of the inference step, which is impractical and thus can lead to a deterioration of accuracy compared to our HOAM approach; see Figure 3\u20134. ", "page_idx": 8}, {"type": "text", "text": "Predicting statistics of chaotic and particle dynamics in high dimensions We now consider the nine-dimensional dynamical system introduced in [69], which leads to chaotic behavior. We show in Figure 3 (bottom) the sample histogram corresponding to a test physics parameter that represents the Rayleigh number. At time $t=3.7$ and projecting onto dimension three and nine, the histograms show that the proposed HOAM accurately matches the low probability region that connects the two high probability regions, whereas AM fails to converge. Consider now the example of the particles in an aharmonic trap, which leads to 100-dimensional samples $X_{t,\\mu}^{i}$ . For a test physics parameter, Figure 5 shows that HOAM accurately predicts the mean particle positions even for this high dimensional system. ", "page_idx": 8}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/fd12a38e467d97c3097de74daac33e9a2c657baa858aa937d4387bb843676fa2.jpg", "img_caption": ["Figure 5: Left: HOAM accurately predicts the time evolution of the mean position of a 100- dimensional particle system in an aharmonic moving trap (dim 1 vs $\\dim100$ ). Right: HOAM reduced models provide about 2 orders of magnitude speedup over traditional numerical (full) models for the 6 dimensional strong Landau problem. HOAM is also 1\u20132 orders of magnitude faster than CFM and NCSM, which provide no speedup over the full models in our problems. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "qyaz3XP0FN/tmp/d652f944d05a24be37b183aaf44ba49e72874d2baf4a5199a0e9ca09ee162c90.jpg", "table_caption": [], "table_footnote": ["Table 1: HOAM with Simpson\u2019s and Gauss quadrature outperforms state-of-the-art methods w.r.t. inference runtime (r.t.) with comparable errors when applied to various physics problems for parametric model reduction. Metrics: e.e. is the relative error in electric energy, see (25); for the Sinkhorn divergence, see Appendix B.5. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4 Conclusions, limitations, and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For parametric model reduction, learning population dynamics via minimal-energy vector fields over time $t$ and physics parameter $\\mu$ with our variational approach helps reduce inference runtime compared to standard diffusion- and flow-based modeling that condition on $t$ and $\\mu$ and therefore have to solve a separate inference problem for each time step and physics parameter at test time. Because we learn the dynamics over time $t$ , it is critical to accurately capture the coupling over the time steps, for which we propose to use higher-order quadrature schemes when estimating time integrals in the training loss. The higher-order quadrature of the time integrals considerably improves training stability. Our approach achieves comparable errors as state-of-the-art methods while at the same time reducing inference runtime by 1\u20132 orders of magnitude. Additionally, HOAM provides speedups of up to 2 orders of magnitude to classical numerical full models. ", "page_idx": 9}, {"type": "text", "text": "Limitations: First, if there are only very few samples in time, even numerical quadrature cannot provide an accurate enough estimation of the loss, which could be a limitation in computational biology [23, 9]. Second, we currently seek a vector field that minimizes the kinetic energy or a variant thereof. Investigating other notions of energy that might lead to vector fields with other desired properties in certain problems remains a challenge. ", "page_idx": 9}, {"type": "text", "text": "We do not expect that this work has negative societal impacts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Berman and Peherstorfer were partially supported by the Air Force Office of Scientific Research under award FA9550-21-1-0222. Peherstorfer and Blickhan were partially supported by the Office of Naval Research, United States under award N00014-22-1-2728. We thank Stefan Possanner and Dominik Bell (Max Planck Institute for Plasma Physics) for their support with using the high-fidelity code used in the six-dimensional Vlasov experiments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv, 2303.08797, 2023.   \n[2] M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023.   \n[3] L. Ambrosio and W. Gangbo. Hamiltonian ODEs in the Wasserstein space of probability measures. Communications on Pure and Applied Mathematics, 61(1):18\u201353, Jan. 2008.   \n[4] L. Ambrosio and N. Gigli. A User\u2019s Guide to Optimal Transport. In Modelling and Optimisation of Flows on Networks, volume 2062, pages 1\u2013155. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. Series Title: Lecture Notes in Mathematics.   \n[5] L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient Flows. Lectures in Mathematics ETH Z\u00fcrich. Birkh\u00e4user-Verlag, Basel, 2005.   \n[6] V. Arnold. Sur la g\u00e9om\u00e9trie diff\u00e9rentielle des groupes de Lie de dimension infinie et ses applications \u00e0 l\u2019hydrodynamique des fluides parfaits. Annales de l\u2019institut Fourier, 16(1):319\u2013361, 1966.   \n[7] J. W. Banks and J. A. F. Hittinger. A new class of nonlinear finite-volume methods for Vlasov simulation. IEEE Transactions on Plasma Science, 38(9 PART 1):2198 \u2013 2207, 2010.   \n[8] J.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, Jan. 2000. [9] J.-D. Benamou, T. O. Gallou\u00ebt, and F.-X. Vialard. Second-Order Models for Optimal Transport and Cubic Splines on the Wasserstein Space. Foundations of Computational Mathematics, 19(5):1113\u20131143, Oct. 2019.   \n[10] P. Benner, S. Gugercin, and K. Willcox. A survey of projection-based model reduction methods for parametric dynamical systems. SIAM Review, 57(4):483\u2013531, 2015.   \n[11] P. Benner, M. Ohlberger, A. Cohen, and K. Willcox, editors. Model Reduction and Approximation: Theory and Algorithms. Society for Industrial and Applied Mathematics, Philadelphia, PA, July 2017.   \n[12] P. Benner and M. Redmann. Model reduction for stochastic systems. Stochastic Partial Differential Equations: Analysis and Computations, 3(3):291\u2013338, Sep 2015.   \n[13] J. Berman and B. Peherstorfer. CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3565\u20133583. PMLR, 21\u201327 Jul 2024.   \n[14] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023.   \n[15] N. M. Boffi and E. Vanden-Eijnden. Probability flow solution of the fokker\u2013planck equation. Machine Learning: Science and Technology, 4(3):035012, jul 2023.   \n[16] J. Bruna, B. Peherstorfer, and E. Vanden-Eijnden. Neural Galerkin schemes with active learning for high-dimensional evolution equations. Journal of Computational Physics, 496:112588, 2024.   \n[17] C. Bunne, L. Papaxanthos, A. Krause, and M. Cuturi. Proximal optimal transport modeling of population dynamics. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 6511\u20136528. PMLR, 28\u201330 Mar 2022.   \n[18] S. R. Cachay, B. Zhao, H. James, and R. Yu. DYffusion: A dynamics-informed diffusion model for spatiotemporal forecasting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[19] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[20] Y. Chen, M. Goldstein, M. Hua, M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Probabilistic forecasting with stochastic interpolants and F\u00f6llmer processes. arXiv, 2403.13724, 2024.   \n[21] Y. Chen and D. Xiu. Learning stochastic dynamical system via flow map operator. Journal of Computational Physics, 508:112984, 2024.   \n[22] Y. Cheng, A. J. Christlieb, and X. Zhong. Energy-conserving discontinuous Galerkin methods for the Vlasov\u2013Amp\u00e8re system. Journal of Computational Physics, 256:630\u2013655, 2014.   \n[23] S. Chewi, J. Clancy, T. Le Gouic, P. Rigollet, G. Stepaniants, and A. Stromme. Fast and Smooth Interpolation on Wasserstein Space. In A. Banerjee and K. Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3061\u20133069. PMLR, Apr. 2021.   \n[24] S.-N. Chow, W. Li, and H. Zhou. Wasserstein Hamiltonian flows. Journal of Differential Equations, 268(3):1205\u20131219, Jan. 2020.   \n[25] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[26] A. Davtyan, S. Sameni, and P. Favaro. Efficient video prediction via sparsely conditioned flow matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 23263\u201323274, October 2023.   \n[27] P. Deuflhard and A. Hohmann. Numerical Analysis in Modern Scientific Computing. Springer, 2003.   \n[28] E. Dupont, A. Doucet, and Y. W. Teh. Augmented neural ODEs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[29] A. Ern and J.-L. Guermond. Theory and Practice of Finite Elements, volume 159 of Applied Mathematical Sciences. Springer New York, New York, NY, 2004.   \n[30] J. A. Farrell, Y. Wang, S. J. Riesenfeld, K. Shekhar, A. Regev, and A. F. Schier. Single-cell reconstruction of developmental trajectories during zebrafish embryogenesis. Science, 360(6392):eaar3131, June 2018.   \n[31] I. Gentil, C. L\u00e9onard, and L. Ripani. Dynamical aspects of the generalized Schr\u00f6dinger problem via Otto calculus \u2013 A heuristic point of view. Revista Matem\u00e1tica Iberoamericana, 36(4):1071\u20131112, Jan. 2020.   \n[32] R. G. Ghanem and P. D. Spanos. Stochastic Finite Elements: A Spectral Approach. Springer, 1991.   \n[33] W. Harvey, S. Naderiparizi, V. Masrani, C. D. Weilbach, and F. Wood. Flexible diffusion modeling of long videos. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[34] T. Hashimoto, D. Gifford, and T. Jaakkola. Learning population-level diffusions with generative RNNs. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2417\u20132426, New York, New York, USA, 20\u201322 Jun 2016. PMLR.   \n[35] J. Hittinger and J. Banks. Block-structured adaptive mesh refinement algorithms for Vlasov simulation. Journal of Computational Physics, 241:118\u2013140, 2013.   \n[36] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.   \n[37] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 8633\u20138646. Curran Associates, Inc., 2022.   \n[38] B. Holzschuh, S. Vegetti, and N. Thuerey. Solving inverse physics problems with score matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[39] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[40] T. J. R. Hughes. The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Dover Publications, 2012.   \n[41] A. Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695\u2013709, 2005.   \n[42] P. Kidger, J. Foster, X. Li, and T. J. Lyons. Neural SDEs as infinite-dimensional GANs. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5453\u20135463. PMLR, 18\u201324 Jul 2021.   \n[43] K. Kormann and A. Yurova. A generalized Fourier\u2013Hermite method for the Vlasov\u2013Poisson system. BIT Numerical Mathematics, 61(3):881\u2013909, Sept. 2021.   \n[44] T. Koshizuka and I. Sato. Neural Lagrangian Schr\u00f6dinger Bridge: Diffusion Modeling for Population Dynamics. In The Eleventh International Conference on Learning Representations, 2023.   \n[45] B. Kramer, B. Peherstorfer, and K. E. Willcox. Learning nonlinear reduced models from data with operator inference. Annual Review of Fluid Mechanics, 56(Volume 56, 2024):521\u2013548, 2024.   \n[46] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. Dynamic mode decomposition: data-driven modeling of complex systems. SIAM, 2016.   \n[47] H. Lavenant, S. Zhang, Y.-H. Kim, and G. Schiebinger. Toward a mathematical theory of trajectory inference. The Annals of Applied Probability, 34(1A):428 \u2013 500, 2024.   \n[48] K. Lee and E. J. Parish. Parameterized neural ordinary differential equations: applications to computational physics problems. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 477(2253):20210162, 2021.   \n[49] A. Lenard and I. B. Bernstein. Plasma Oscillations with Diffusion in Velocity Space. Physical Review, 112(5):1456\u20131459, Dec. 1958.   \n[50] L. Li, S. Hurault, and J. Solomon. Self-consistent velocity matching of probability flows. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[51] X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In C. Zhang, F. Ruiz, T. Bui, A. B. Dieng, and D. Liang, editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference, volume 118 of Proceedings of Machine Learning Research, pages 1\u201328. PMLR, 08 Dec 2020.   \n[52] M. Lienen, D. L\u00fcdke, J. Hansen-Palmus, and S. G\u00fcnnemann. From zero to turbulence: Generative modeling for 3d flow simulation. In The Twelfth International Conference on Learning Representations, 2024.   \n[53] E. M. Lifshitz and L. P. Pitaevski. Chapter III - Collisionless Plasmas. In E. M. Lifshitz and L. P. Pitaevski, editors, Physical Kinetics, volume 10 of Course of Theoretical Physics, pages 115\u2013167. Pergamon, Amsterdam, Jan. 1981.   \n[54] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[55] J. Lott. Some Geometric Calculations on Wasserstein Space. Communications in Mathematical Physics, 277(2):423\u2013437, Nov. 2007.   \n[56] C. L\u00e9onard. A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems - A, 34(4):1533\u20131574, 2014.   \n[57] D. B. Melrose. Instabilities in Space and Laboratory Plasmas. Cambridge University Press, Aug. 1986. Publication Title: Instabilities in Space and Laboratory Plasmas ADS Bibcode: 1986islp.book.....M.   \n[58] I. Mezi\u0107. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dynamics, 41(1-3):309\u2013325, 2005.   \n[59] C. Mouhot and C. Villani. On Landau damping. Acta Mathematica, 207(1):29\u2013201, 2011.   \n[60] A. Muntean, J. Rademacher, and A. Zagaris, editors. Macroscopic and Large Scale Phenomena: Coarse Graining, Mean Field Limits and Ergodicity, volume 3 of Lecture Notes in Applied Mathematics and Mechanics. Springer International Publishing, Cham, 2016.   \n[61] K. Neklyudov, R. Brekelmans, D. Severo, and A. Makhzani. Action Matching: Learning Stochastic Dynamics from Samples. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 25858\u201325889. PMLR, July 2023.   \n[62] F. No\u00e9, A. Tkatchenko, K.-R. M\u00fcller, and C. Clementi. Machine learning for molecular simulation. Annual Review of Physical Chemistry, 71(Volume 71, 2020):361\u2013390, 2020.   \n[63] F. Otto. The geometry of dissipative evolution equations: the porous medium equation. Communications in Partial Differential Equations, 26(1-2):101\u2013174, Jan. 2001.   \n[64] F. Otto and C. Villani. Generalization of an Inequality by Talagrand and Links with the Logarithmic Sobolev Inequality. Journal of Functional Analysis, 173(2):361\u2013400, June 2000.   \n[65] B. Peherstorfer, K. Willcox, and M. Gunzburger. Survey of multifidelity methods in uncertainty propagation, inference, and optimization. SIAM Review, 60(3):550\u2013591, 2018.   \n[66] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville. Film: visual reasoning with a general conditioning layer. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI\u201918/IAAI\u201918/EAAI\u201918. AAAI Press, 2018.   \n[67] S. Possanner, F. Holderied, Y. Li, B. K. Na, D. Bell, S. Hadjout, and Y. G\u00fc\u00e7l\u00fc. HighOrder Structure-Preserving Algorithms for Plasma Hybrid Models. In F. Nielsen and F. Barbaresco, editors, Geometric Science of Information, volume 14072, pages 263\u2013271. Springer Nature Switzerland, Cham, 2023. Series Title: Lecture Notes in Computer Science.   \n[68] K. Rasul, C. Seward, I. Schuster, and R. Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8857\u20138868. PMLR, 18\u201324 Jul 2021.   \n[69] P. Reiterer, C. Lainscsek, F. Sch\u00fcrrer, C. Letellier, and J. Maquet. A nine-dimensional lorenz system to study high-dimensional chaos. Journal of Physics A: Mathematical and General, 31(34):7121, aug 1998.   \n[70] C. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2004.   \n[71] C. W. Rowley, I. Mezi\u0107, S. Bagheri, P. Schlatter, and D. S. Henningson. Spectral analysis of nonlinear flows. Journal of Fluid Mechanics, 641:115\u2013127, 2009.   \n[72] G. Rozza, D. Huynh, and A. Patera. Reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations. Archives of Computational Methods in Engineering, 15(3):1\u201347, 2007.   \n[73] C. Salvi, M. Lemercier, and A. Gerasimovics. Neural stochastic pdes: Resolutioninvariant learning of continuous spatiotemporal dynamics. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 1333\u20131344. Curran Associates, Inc., 2022.   \n[74] F. Santambrogio. Optimal Transport for Applied Mathematicians, volume 87 of Progress in Nonlinear Differential Equations and Their Applications. Springer International Publishing, Cham, 2015.   \n[75] G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube, L. Lee, J. Chen, J. Brumbaugh, P. Rigollet, K. Hochedlinger, R. Jaenisch, A. Regev, and E. S. Lander. Optimal-Transport Analysis of SingleCell Gene Expression Identifies Developmental Trajectories in Reprogramming. Cell, 176(4):928\u2013943.e22, Feb. 2019.   \n[76] P. J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics, 656:5\u201328, 2010.   \n[77] E. Schr\u00f6dinger. \u00dcber die Umkehrung der Naturgesetze. Technical Report 1931 IX, Akademie der Wissenschaften, Berlin, 1931.   \n[78] Z. Shen, Z. Wang, S. Kale, A. Ribeiro, A. Karbasi, and H. Hassani. Self-consistency of the fokker planck equation. In P.-L. Loh and M. Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 817\u2013841. PMLR, 02\u201305 Jul 2022.   \n[79] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[80] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[81] Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, page 204, 2019.   \n[82] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[83] E. Sonnendr\u00fccker, A. Wacher, R. Hatzky, and R. Kleiber. A split control variate scheme for PIC simulations with collisions. Journal of Computational Physics, 295:402\u2013419, Aug. 2015.   \n[84] A. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. TrajectoryNet: A dynamic optimal transport network for modeling cellular dynamics. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9526\u20139536. PMLR, 13\u201318 Jul 2020.   \n[85] C. Trapnell, D. Cacchiarelli, J. Grimsby, P. Pokharel, S. Li, M. Morse, N. J. Lennon, K. J. Livak, T. S. Mikkelsen, and J. L. Rinn. The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells. Nature Biotechnology, 32(4):381\u2013386, Apr. 2014.   \n[86] J. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz. On dynamic mode decomposition: Theory and applications. Journal of Computational Dynamics, 1(2):391\u2013421, 2014.   \n[87] T. M. Tyranowski. Stochastic variational principles for the collisional Vlasov\u2013Maxwell and Vlasov\u2013Poisson equations. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 477(2252):20210167, Aug. 2021.   \n[88] B. Tzen and M. Raginsky. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. arXiv, 1905.09883, 2019.   \n[89] C. Villani. Optimal transport: old and new. Number 338 in Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009.   \n[90] C. Villani. Topics in optimal transportation. Number 58 in Graduate studies in mathematics. American Mathematical Society, Providence, Rhode Island, reprinted with corrections edition, 2016.   \n[91] P. Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661\u20131674, 2011.   \n[92] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley. A data\u2013driven approximation of the Koopman operator: Extending dynamic mode decomposition. Journal of Nonlinear Science, 25(6):1307\u20131346, 2015.   \n[93] F. A. Wolf, F. K. Hamey, M. Plass, J. Solana, J. S. Dahlin, B. G\u00f6ttgens, N. Rajewsky, L. Simon, and F. J. Theis. PAGA: graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells. Genome Biology, 20(1):59, Dec. 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Quadrature rules ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Monte Carlo estimation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Monte Carlo estimation approximates an integral by evaluating the integrand at randomly sampled nodes within an interval $[a,b]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{a}^{b}f(t)\\,d t\\approx{\\frac{b-a}{N}}\\sum_{i=1}^{N}f(t_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We consider only the case where the nodes $t_{i}$ are uniformly distributed random variables in $[a,b]$ . The weights are: ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{i}=\\frac{b-a}{N},\\quad i=1,2,\\dots,N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The root mean-squared integration error of Monte Carlo estimation decays as $\\mathcal{O}(N^{-1/2})$ , for integrands with bounded variance. ", "page_idx": 16}, {"type": "text", "text": "A.2 Trapezoidal Rule ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The trapezoidal rule approximates the integral of a function $f$ over an interval $[a,b]$ by dividing it into $N$ subintervals of equal width $h={\\frac{b-a}{N}}$ . The approximation is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{a}^{b}f(t)\\,d t\\approx h\\left[{\\frac{1}{2}}f(t_{0})+\\sum_{i=1}^{N-1}f(t_{i})+{\\frac{1}{2}}f(t_{N})\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the nodes $t_{i}$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\nt_{i}=a+i h,\\quad i=0,1,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The trapezoidal rule is a second-order rule, which means that the integration error decays as $\\mathcal{O}(h^{2})$ for sufficiently smooth functions. ", "page_idx": 16}, {"type": "text", "text": "A.3 Composite Simpson\u2019s Rule ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Composite Simpson\u2019s rule approximates the integral by fitting parabolas through intervals. It divides $[a,b]$ into an even number $N$ of subintervals of width $h={\\frac{b-a}{N}}$ . The approximation is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{a}^{b}f(t)\\,d t\\approx\\frac{h}{3}\\left[f(t_{0})+2\\sum_{i=1}^{N/2-1}f(t_{2i})+4\\sum_{i=1}^{N/2}f(t_{2i-1})+f(t_{N})\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with nodes: ", "page_idx": 16}, {"type": "equation", "text": "$$\nt_{i}=a+i h,\\quad i=0,1,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Composite Simpson\u2019s rule is a fourth-order rule, which means that the integration error decays as ${\\mathcal O}(h^{4})$ for sufficiently smooth functions. ", "page_idx": 16}, {"type": "text", "text": "A.4 Gauss-Legendre Quadrature ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Gauss-Legendre quadrature approximates the integral over $[-1,1]$ by choosing nodes $t_{i}$ and weights $w_{i}$ so that polynomials of the highest possible degree are integrated exactly. A Gauss-Legendre quadrature has the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{-1}^{1}f(t)\\,d t\\approx\\sum_{i=1}^{n}w_{i}f(t_{i}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t_{i}$ are the roots of the Legendre polynomial $P_{n}(t)$ , and the weights are: ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{i}=\\frac{2}{(1-t_{i}^{2})[P_{n}^{\\prime}(t_{i})]^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For integration over $[a,b]$ , a linear transformation maps $[-1,1]$ to $[a,b]$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{t}_{i}=\\frac{b-a}{2}t_{i}+\\frac{a+b}{2},\\quad\\tilde{w}_{i}=\\frac{b-a}{2}w_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The approximation becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{a}^{b}f(t)\\,d t\\approx\\sum_{i=1}^{n}{\\tilde{w}}_{i}f({\\tilde{t}}_{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Gauss-Legendre quadrature exactly integrates polynomials of degree $2n-1$ , where $n$ is the number of nodes. ", "page_idx": 17}, {"type": "text", "text": "B Details about numerical examples ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Harmonic oscillator with background collisions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The equation of motion in four-dimensional phase-space for $X=[X_{1},X_{2},V_{1},V_{2}]$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\begin{array}{l}{X_{1}}\\\\ {X_{2}}\\\\ {V_{1}}\\\\ {V_{2}}\\end{array}\\right](t)=\\left[\\begin{array}{c}{V_{1}}\\\\ {V_{2}}\\\\ {-\\omega^{2}X_{1}}\\\\ {-\\omega^{2}X_{2}}\\end{array}\\right](t)+\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\eta}\\\\ {\\eta}\\end{array}\\right]\\xi(t)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\eta\\,=\\,5\\,\\times\\,10^{-2}$ and $\\xi$ denotes white noise. The initial configuration is a Gaussian centered at $m_{0}=\\lfloor1,1\\rfloor$ with covariance equal to $\\Sigma_{0}=10^{-2}\\times\\mathrm{Id}$ in the spatial coordinates $X_{1},X_{2}$ and Gaussian in the velocity coordinates $V_{1},V_{2}$ centered at zero and with covariance $\\Sigma_{0}=10^{-2}\\times\\mathrm{Id}$ . ", "page_idx": 17}, {"type": "text", "text": "B.2 Vlasov-Poisson problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Mean field approximations The Vlasov-Poisson system describes the interaction of charged particles. Due to the presence of the Coulomb force, the dynamics of a single particle . Given the fact th $N$ is in practice extremely large, it $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}X_{t,\\mu}^{i}=_{\\cdot}v(t,X_{t,\\mu}^{i};\\mu,X_{t,\\mu}^{1},\\cdot\\cdot\\cdot,X_{t,\\mu}^{N})}\\end{array}$ $N$   \nis natural to pass to the mean-field limit. Assuming the particles are indistinguishable, the result is a PDE of the form $\\begin{array}{r}{\\partial_{t}\\rho_{t,\\mu}+\\nabla\\cdot(\\rho_{t,\\mu}v_{\\mathrm{mf}}(t,\\cdot;\\mu,\\rho_{t,\\mu}))=0}\\end{array}$ that describes the evolution of the collection (or population, ensemble) of particles denoted by $\\rho_{t,\\mu}$ . In the specific case of the Vlasov-Poisson problem, Coulomb interactions in the mean-field limit give rise to a Poisson equation determining an electric field that is generated by the collection of particles and influences its dynamics. For completeness sake, we mention that the singularity of the Coulomb interaction poses a considerable technical challenge when passing to this limit. We refer to [53, 59] for the derivation of the Vlasov-Poisson equation and [60] for more examples of mean-field systems. The theory behind the test-cases we run in this work can be found in [57], Chapter 3. ", "page_idx": 17}, {"type": "text", "text": "Governing equation We slightly change the notation here to be consistent with the references. $f:\\mathcal{X}_{x}\\times\\mathbb{R}^{d}\\times\\mathbb{R}\\times\\mathcal{D}\\to\\mathbb{R}$ , $d\\in\\{1,2,3\\}$ , denotes the distribution function governed by the Vlasov-Poisson system ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\partial_{t}f(x,v,t;\\mu)=-v\\cdot\\nabla_{x}f(x,v,t;\\mu)-\\nabla\\phi(x,t)\\cdot\\nabla_{v}f(x,v,t;\\mu)=0\\,,}\\\\ &{\\ \\ -\\mu^{2}\\Delta\\phi(x,t;\\mu)=1-\\displaystyle\\int_{\\mathbb R^{d}}f(x,v,t;\\mu)\\mathrm{d}v\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the notation of the rest of this work, $f(\\cdot,\\cdot,t;\\mu)=\\rho_{t,\\mu}$ , $\\mathcal{X}_{x}\\times\\mathbb{R}^{d}=\\mathcal{X}$ . The spatial domain $\\chi_{x}$ is a subset of $\\mathbb{R}^{d}$ , in all our examples it is of the form $[0,l_{1}]\\times[0,l_{2}]\\times[0,l_{3}]$ with periodic boundary conditions. ", "page_idx": 17}, {"type": "text", "text": "Two-stream instability In this case, $d=2$ , so the particle positions vary in $\\mathcal{X}_{x}=[0,l_{1}]$ with periodic boundary conditions and their velocity evolves in $\\mathbb{R}$ . For the two-stream instability, we set the initial distribution to ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{0}(x,v):=\\frac{1}{2\\sqrt{2\\pi}}\\left(1+\\alpha\\cos\\left(2\\pi\\frac{x}{l_{1}}\\right)\\right)\\left(\\exp\\left(-\\frac{(v-v_{0})^{2}}{2}\\right)+\\exp\\left(-\\frac{(v+v_{0})^{2}}{2}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\alpha=0.05,l_{1}=50,v_{0}=3$ . The parameter $\\mu$ varies as $\\mu_{\\mathrm{train}}\\in\\{1.2,1.3,\\ldots,1.9\\}$ and $\\mu_{\\mathrm{test}}\\in\\{1.25,1.85\\}$ . We use a particle-in-cell method for generating the data based on the repository https://github.com/pmocz/pic-python. The number of marker particles is $N=25000$ and for the sake of computing the electric field, a uniform grid of $N/8$ cells is used. Integration in time is done via a St\u00f6rmer-Verlet splitting over $t\\in[0,40]$ with time-step size $10^{-2}$ . ", "page_idx": 18}, {"type": "text", "text": "Bump-on tail We consider the initial distribution ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{0}(x,v)=\\frac{1}{\\sqrt{2\\pi}}\\left(1+\\alpha\\cos\\left(2\\pi\\frac{x}{l_{1}}\\right)\\right)\\left(\\frac{\\delta}{\\sigma_{1}}\\exp\\left(-\\frac{v^{2}}{2\\sigma_{1}^{2}}\\right)+\\frac{1-\\delta}{\\sigma_{2}}\\exp\\left(-\\frac{(v-v_{b})^{2}}{2\\sigma_{2}^{2}}\\right)\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\alpha=0.05,l_{1}=50,v_{b}=4,\\delta=9/10,\\sigma_{1}=1,\\sigma_{2}=1/\\sqrt{2}$ . The parameter $\\mu$ varies as $\\mu_{\\mathrm{train}}\\in\\{1.3,1.4,\\dots,2.0\\}$ and $\\mu_{\\mathrm{test}}\\in\\{1.35,1.95\\}$ . The other parameters are the same as in the two-stream case. ", "page_idx": 18}, {"type": "text", "text": "Strong Landau damping In this case, $d=6$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{0}(x,v)=\\frac{1}{\\sqrt{2\\pi}^{3}}\\left(1+\\alpha\\cos\\left(2\\pi\\frac{x_{1}}{l_{1}}\\right)\\right)\\exp\\left(-\\frac{|v|^{2}}{2}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $l_{1}=4\\pi$ and $l_{2}=l_{3}=1$ . The data is generated using the Struphy package [67], the exact specifications of the simulation are available at https://gitlab.mpcdf.mpg.de/struphy as an example problem. The physics parameter we vary is the mass of the charged particles, which has the effect of changing the strength of the inertial term accelerating the particles relative to the advection term $v\\cdot\\nabla_{x}f$ . This implies $\\mu\\in\\{0.5,0.6,\\ldots,1.5\\}$ , where $\\mu=1.0$ corresponds to the default settings. This $\\mu=1.0$ is also the test parameter and is excluded from the training set. The timing for the full order method has been obtained on a computing cluster with AMD EPYC Genoa 9554 CPUs using 8 MPI processes, which is a default option of the used code. For a single MPI process, it extends to 27 minutes while for 16, it can be reduced to 4 minutes. ", "page_idx": 18}, {"type": "text", "text": "The high-fidelity data we generate is using a control variate approach in order to reduce numerical noise introduced by the finite number of marker particles. Since we require the particles to be identical for our method, we assume they are all weighted equally when re-constructing the electric potential. This biases our reconstructed potential in comparison to the physical one, but we observe in practice that this is only by a multiplicative constant. We save $10^{5}$ marker particles from the high-order simulations and use $N=25000$ of them as input data for our method. We integrate in time over $t\\in[0,8.75]$ ", "page_idx": 18}, {"type": "text", "text": "B.3 High-dimensional chaos ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider the dynamical system introduced in [69]. We generate samples by initializing a 9 dimensional Gaussian centered at the origin with width equal to $2\\times10^{-2}$ . We then integrate these samples forward as an SDE whose drift is given by the 9-dimensional system of ODEs described in [69] and whose diffusion term is given as diagonal noise equal to $5\\times10^{-2}$ . We integrate 25000 particles of the system up to $T=20$ using the Euler-Maruyama scheme with time step size equal to $10^{-2}$ . The parameter $\\mu$ varies as $\\mu_{\\mathrm{train}}\\in\\{13.5,13.6,\\dots,14.2\\}$ and $\\mu_{\\mathrm{test}}\\in\\{13.65,14.05\\}$ . ", "page_idx": 18}, {"type": "text", "text": "B.4 Particles in aharmonic trap ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We consider the evolution of interacting particles in an aharmonic trap [16]. The twodimensional particle positions $Z_{1}(t,\\mu),\\ldots,Z_{M}(t,\\mu)$ are governed by an SDE ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm d Z_{i}=g(t,Z_{i})\\mathrm d t+\\sum_{j=1}^{M}K\\bigl(Z_{i},Z_{j}\\bigr)\\mathrm d t+\\sqrt{2\\gamma}\\mathrm d W_{i}\\,,\\qquad i=1,\\dots,M\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\gamma\\,>\\,0$ is the diffusion coefficient and $W_{i}$ are independent Wiener processes. The function $g(t,Z)=(a(t)-Z)^{3}$ describes a time-dependent one-body force, where $a(t)\\,=$ $5/4(\\sin(\\pi t)\\!+\\!3/2))\\!+\\!\\mu\\cos(2\\pi t)$ is the position of the trap. The function $\\begin{array}{r}{K(Z,Z^{\\prime})=\\frac{\\alpha}{M}(Z^{\\prime}\\!-\\!Z)}\\end{array}$ describes a pairwise interaction term. We set $\\alpha=-1/4$ and $\\gamma=10^{-2}$ . The parameter is in the range $\\mathcal{D}=[0.3,0.9]$ and modifies the position of the trap. A sample $X_{t,\\mu}^{i}$ corresponds to a vector $[Z_{1}(t,\\mu),\\ldots,Z_{M}(t,\\mu)]^{\\scriptscriptstyle T}$ of dimension 100, because we have $M=50$ particles and each position $Z_{j}(t,\\mu)$ as two dimensions. We generate samples via Monte Carlo by using the Euler-Maruyama scheme. The time step size is $\\delta t=10^{-3}$ and we integrate up to final time 2. ", "page_idx": 19}, {"type": "text", "text": "B.5 Other details about numerical experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In terms of network architecture, we follow [13] closely because we use their network architecture. We use MLPs to parameterize both the main network and the hyper-network with swish activation functions. The main network is depth 7 and width 64 linear layers while the hyper-network is depth 3 with width 15 linear layers. The rank of the CoLoRA modulations is set to 3. Identical CoLoRA architectures are used for all HOAM experiments as well as the comparisons with AM, NCSM, and CFM. The only difference is the size of the output layer for NCSM and CFM whose outputs must be the same dimensionality as their inputs. ", "page_idx": 19}, {"type": "text", "text": "For all experiments we use an Adam optimizer at a $2\\times10^{-3}$ learning rate with a cosine learning rate scheduler. For all experiments unless otherwise noted, we take a batch size of 256 particles over 256 time points. We optimize for 50, 000 Adam iterations for Vlasov examples and for 25, 000 Adam iterations for all other examples. ", "page_idx": 19}, {"type": "text", "text": "The results were computed on NVIDIA Quadro RTX 8000 GPUs. All code was implemented in Python using the JAX library with JIT complication where possible. ", "page_idx": 19}, {"type": "text", "text": "Hyper-parameter $\\epsilon$ in the loss (7) searched over $\\{0,1,2,5,7\\}\\times10^{-2}$ for both HOAM and AM. ", "page_idx": 19}, {"type": "text", "text": "The relative error in the electric energy is computed as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\frac{|e_{\\mathrm{true}}(t)-e_{\\mathrm{predict}}(t)|}{|e_{\\mathrm{true}}(t)|},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $e_{\\mathrm{true}}(t)$ is the electric energy predicted by the high-fidelity numerical simulations at time $t$ and $e_{\\mathrm{predict}}(t)$ is the electric energy computed from samples of either HOAM (ours), AM, NCSM, or CFM. The relative error in the mean is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\frac{|\\mathbb{E}[\\rho_{\\mathrm{true}}(t)]-\\mathbb{E}[\\rho_{\\mathrm{predict}}(t)]|}{|\\mathbb{E}[\\rho_{\\mathrm{true}}(t)]|}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the expected values are estimated via Monte Carlo from the generated samples. ", "page_idx": 19}, {"type": "text", "text": "The Sinkhorn distance is computed with https://ott-jax.readthedocs.io/en/latest/ with threshold $10^{-3}$ ; see also [25]. ", "page_idx": 19}, {"type": "text", "text": "B.6 Additional numerical results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 6 we show the various projections at time $t\\,=\\,3.7$ of the sample distribution corresponding to the nine-dimensional chaotic system [69]. ", "page_idx": 19}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/c28a674f4f8969eaf22b211660cbcfd26f9810d78ce532165debf14c2daf22c1.jpg", "img_caption": ["Figure 6: Shows the projections of other dimensions of the nine-dimensional chaotic system [69]; see also Figure 3. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/2477165061924580136e1a3d2eab73f413cc8df0a3e628f9729ab378060503bd.jpg", "img_caption": ["Figure 7: Electric energy and solution field at time $t=4$ for the 6 dimensional strong Landau example. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In Figure 7 we show the particle histograms and the electric energy curves for the sixdimensional Vlasov-Poisson problem corresponding to strong Landau damping. ", "page_idx": 20}, {"type": "text", "text": "In Figure 8, for the linear oscillator example, we compare CoLoRA to two other modulation schemes: FiLM [66] and MLP. For the MLP the inputs $x,t,\\mu$ are concatenated together and input directly to the model. There is no hyper-network or modulation scheme. For FiLM, we closely follow the original paper. The main network takes $x$ as input and the hyper-network $t,\\mu$ as input. The hyper-network and main network have the same parameter counts as in the CoLoRA experiments. The output of the hyper-network then directly modulates the activation of each layer of the main network as detailed in the original FiLM paper [66]. Figure 8 shows that parameterizing the vector field $s_{t,\\mu}$ with CoLoRA layers achieves the lowest mean Wasserstein distance, which motivates the use of the CoLoRA modulation scheme [13]. ", "page_idx": 20}, {"type": "text", "text": "C Calculations regarding the entropic loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the following, assume that $\\rho\\,\\in\\,\\mathcal P(\\mathcal X)$ is a smooth density bounded away from zero. We begin by showing some calculation rules of the operator $-\\Delta_{\\rho}:s\\mapsto-\\nabla\\cdot(\\rho\\nabla s)$ with ", "page_idx": 20}, {"type": "image", "img_path": "qyaz3XP0FN/tmp/3bcc94f8028b580924a4fc88a6488537f5b21ba7d24a5e21c34907b3635ec9cd.jpg", "img_caption": ["Figure 8: Comparison of CoLoRA modulation scheme [13] versus FiLM [66] and MLP. CoLoRA layers achieve the lowest mean Wasserstein distance compared to FiLM and MLP. In particular, CoLoRA avoids outliers with larger errors. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "homgeneous Neumann boundary conditions. In its weak form, it reads ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\int_{X}f\\Delta_{\\rho}s\\mathrm{d}x=\\int_{X}\\nabla f\\cdot\\nabla s\\,\\rho\\mathrm{d}x\\quad\\forall f\\in{\\mathcal{C}}^{\\infty}({\\mathcal{X}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With the choice $f\\,=\\,\\log\\rho$ , we find the useful identity $\\Delta_{\\rho}\\log\\rho\\,=\\,\\Delta\\rho$ . Next, recall the objective $E^{\\epsilon}$ from (5): ", "page_idx": 21}, {"type": "equation", "text": "$$\nE^{\\varepsilon}(s)=\\frac12\\int_{\\mathcal{X}}\\left|\\nabla\\left(s-\\frac{\\varepsilon^{2}}{2}\\log\\rho\\right)\\right|^{2}\\rho\\mathrm{d}x-\\int_{\\mathcal{X}}\\partial_{t}\\rho s\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now denote by $\\delta s$ an arbitrary element of $\\mathcal{C}^{\\infty}(\\mathcal{X})$ . Then, if $s^{\\epsilon}$ is a minimizer of the (strictly convex) objective, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\stackrel{!}{=}\\frac{\\mathrm{d}}{\\mathrm{d}\\tau}E^{\\varepsilon}(s^{\\varepsilon}+\\tau\\delta s)\\bigg|_{\\tau=0}=-\\int_{\\mathcal{X}}\\delta s\\Delta_{\\rho}\\left(s^{\\varepsilon}-\\frac{\\varepsilon^{2}}{2}\\log\\rho\\right)\\mathrm{d}x-\\int_{\\mathcal{X}}\\partial_{t}\\rho\\delta s\\mathrm{d}x\\quad\\forall\\delta s.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n0=\\Delta_{\\rho}\\left(s^{\\epsilon}-\\frac{\\varepsilon^{2}}{2}\\log\\rho\\right)+\\partial_{t}\\rho=\\nabla\\cdot(\\rho\\nabla s^{\\epsilon})-\\frac{\\varepsilon^{2}}{2}\\Delta\\rho+\\partial_{t}\\rho.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, note that (5) is identical to ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{t,\\mu}^{\\epsilon}(s)=\\int_{\\mathcal{X}}\\left(\\frac{1}{2}|\\nabla s|^{2}+\\frac{\\epsilon^{2}}{2}\\Delta s\\right)\\rho_{t,\\mu}\\mathrm{d}x-\\int_{\\mathcal{X}}\\partial_{t}\\rho_{t,\\mu}s\\mathrm{d}x+\\frac{\\epsilon^{2}}{8}\\int_{\\mathcal{X}}|\\nabla\\log\\rho_{t,\\mu}|^{2}\\rho_{t,\\mu}\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "after integration by parts. The last term is the Fisher information of the data at $t,\\mu$ and plays no role in the optimization. ", "page_idx": 21}, {"type": "text", "text": "D Motivating the partial integration in time in the loss ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Note that the problems from Equation (3) corresponding to different values of $t$ are coupled through the term $\\partial_{t}\\rho_{t,\\mu}$ . This is most apparent when one discretizes the equation in time. Denote by $\\{t_{i}\\}_{i=0}^{n_{t}}$ a strictly increasing sequence with $t_{0}=0,t_{n_{t}}=1$ , and $t_{i+1}-t_{i}=\\delta t_{i}$ Then, for fixed but arbitrary $\\mu$ , we obtain $n_{t}$ coupled problems of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{s_{t_{i}}\\in H^{1}(\\rho_{t_{i},\\mu},\\chi)}}\\frac{1}{2}\\int_{\\mathcal{X}}|\\nabla s_{t_{i},\\mu}|^{2}\\rho_{t_{i},\\mu}\\mathrm{d}x-\\frac{1}{\\delta t}\\int_{\\mathcal{X}}\\bigl(\\rho_{t_{i+1},\\mu}-\\rho_{t_{i},\\mu}\\bigr)s_{t_{i},\\mu}\\mathrm{d}x\\quad\\forall i,\\mu.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding these problems and shifting the indices, one can eliminate $\\rho_{t_{i+1,\\mu}}$ , explicitly coupling $s_{t_{i},\\mu}$ and $s_{t_{i+1},\\mu}$ . The continuous equivalent of this of course is an integration over $t$ , followed by an integration by parts. ", "page_idx": 21}, {"type": "text", "text": "E Geometric picture of the optimization problem ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We omit the dependence on the parameter $\\mu$ here for the sake of simpler notation and write $\\mathrm{d}\\rho$ for $\\rho\\,\\mathrm{d}x$ for brevity. Note that the following considerations are purely formal. They are meant to illustrate a geometric picture of the optimization problems we consider. We claim no originality of these ideas; the exposition is based on Chapter 7 of [89] as well as [24, 55]. ", "page_idx": 22}, {"type": "text", "text": "Otto calculus Based on the identification of the tangent space of $P(\\mathcal X)$ with the space of gradients (more rigorously, at point $\\rho_{t}\\,\\in\\,P(\\mathcal{X})$ , the closure of $\\{\\nabla f\\,:\\,f\\,\\in\\,\\mathcal{C}^{\\infty}(\\mathcal{X})\\}$ in $L^{2}(\\mathcal{X},\\rho_{t},\\mathbb{R}^{d})$ , see Definition 8.4.1 in [5]), one can view $\\mathcal P(\\mathcal X)$ formally as a Riemannian manifold: ", "page_idx": 22}, {"type": "text", "text": "Definition 1 ([63]). Let $\\tau\\mapsto\\rho_{\\tau}^{1}$ and $\\tau\\mapsto\\rho_{\\tau}^{2}$ be two curves valued in $\\mathcal{P}(\\mathcal{X})$ for $\\tau\\in(t\\!-\\!\\epsilon,t\\!+\\!\\epsilon)$ such that $\\left.\\rho_{\\tau}^{1}\\right|_{\\tau=t}=\\rho_{\\tau}^{2}\\right|_{\\tau=t}=\\rho_{t}$ . The optimal transport metric on $T\\mathcal P(\\mathcal X)$ at $\\rho_{t}\\in P(\\mathcal{X})$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(\\rho_{t})(\\partial_{\\tau}\\rho_{\\tau}^{1}\\big|_{\\tau=t},\\partial_{\\tau}\\rho_{\\tau}^{2}\\big|_{\\tau=t})=\\displaystyle\\int_{\\mathcal{X}}(\\nabla s_{t}^{1}\\cdot\\nabla s_{t}^{2})d\\rho_{t}:}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\partial_{\\tau}\\rho_{\\tau}^{1}+\\nabla\\cdot(\\rho_{t}\\nabla s_{t}^{1})=0,\\partial_{\\tau}\\rho_{\\tau}^{2}+\\nabla\\cdot(\\rho_{t}\\nabla s_{t}^{2})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This formalism is commonly named after the author of [63] and is closely linked to Arnold\u2019s considerations on geometric hydrodynamics [6] $^2$ As both the identification of $s_{t}$ from $\\partial_{t}\\rho_{t}$ and the metric depend on $\\rho_{t}$ , the geometry defined on $\\mathcal{P}(\\mathcal{X})$ in this way is non-trivial. ", "page_idx": 22}, {"type": "text", "text": "Action of a curve The optimization Equation (3) has an appealing physical interpretation: The vector field we define as tangent to the curve is, among all compatible ones, the one with the smallest integrated kinetic energy. In analogy with the physical literature, we call $\\begin{array}{r}{\\frac{1}{2}\\int_{0}^{1}\\int_{\\mathcal{X}_{\\bullet}}|\\nabla s_{t}|^{2}\\mathrm{d}\\rho_{t}}\\end{array}$ the action of the curve $t\\mapsto\\rho_{t}$ with tangent velocity $\\nabla s_{t}$ . We want to stress that while this procedure is reminiscent of physical action principles, in the latter a solution corresponds to a stationary point given boundary conditions at the beginning and end of the curve. The problem we consider in Equation (6) is more narrow and concerned with finding $\\nabla s_{t}$ that matches a given curve $t\\mapsto\\rho_{t}$ . Determining curves of minimal action in $\\mathcal{P}(\\mathcal{X})$ , leads to the Benamou-Brenier formula ([4], Proposition 2.30)): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{2}W_{2}^{2}(\\rho_{0},\\rho_{1})=\\operatorname*{inf}_{\\rho,s}\\left(\\frac{1}{2}\\int_{0}^{1}\\int_{X}|\\nabla s_{t}|^{2}d\\rho_{t}\\,d t:\\partial_{t}\\rho_{t}+\\nabla\\cdot(\\rho_{t}\\nabla s_{t})=0,\\rho_{t=0}=\\rho_{0},\\rho_{t=1}=\\rho_{1}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with $W_{2}$ the Wasserstein (or Kantorochiv-Rubinstein) distance. ", "page_idx": 22}, {"type": "text", "text": "Lagrangian functions The selection criterion based on kinetic energy alone is not without alternatives. In [24], the relation $\\partial_{t}\\rho=-\\Delta_{\\rho}s$ is interpreted as a form of Legendre transform, hence $\\boldsymbol{s}$ plays the role of a momentum and $\\begin{array}{r}{L(\\rho_{t},\\partial_{t}\\rho_{t},t)=\\int_{\\mathcal{X}}|\\nabla\\Delta_{\\rho}^{\\dagger}\\partial_{t}\\rho|^{2}\\mathrm{d}\\rho}\\end{array}$ that of a Lagrangian. Here, we introduced the notation $\\Delta_{\\rho}^{\\dagger}$ to denote the pseudo inverse operator. Note that, formally, it is sensible to consider $\\partial_{t}\\rho$ as an element of the tangent space of $\\mathcal{P}(\\mathcal{X})$ . After all, $\\rho+\\tau\\partial_{t}\\rho\\in\\mathcal{P}(\\mathcal{X})$ for $\\rho$ strictly positive and $\\tau$ small enough. In this picture, $\\boldsymbol{s}$ is an element of the cotangent space. The introduction of [63] addresses the two concepts and how they relate. ", "page_idx": 22}, {"type": "text", "text": "Any function $L\\,:\\,(\\rho,\\partial_{t}\\rho,t)\\,\\mapsto\\,L(\\rho,\\partial_{t}\\rho,t)$ , strictly convex and superlinear in its second argument, can be chosen to define the minimization objective.3 Details can be found in Chapter 7 of [89], which also features a comprehensive discussion of the history and applications of this problem. In recent years, this formulation has been applied for modeling purposes, e.g. in [44]. To give an example, the choice $\\begin{array}{r}{L(\\rho,\\partial_{t}\\rho,t)=\\frac{1}{2}\\int_{\\mathcal{X}}|\\nabla\\Delta_{\\rho}^{\\dagger}\\partial_{t}\\rho|^{2}\\mathrm{d}\\rho-}\\end{array}$ $\\int_{\\mathcal X}V d\\rho$ for a potential $V:\\mathcal{X}\\rightarrow\\mathbb{R}$ can be used to model obstacles in the path of the samples. ", "page_idx": 22}, {"type": "text", "text": "There exist a number of partial differential equations whose solutions $\\rho_{t}$ can be described as curves of stationary action with respect to such Lagrangians, described in [3, 24], as well as [89], Chapter 23, and [90], Chapter 8. ", "page_idx": 23}, {"type": "text", "text": "Schr\u00f6dinger Bridge The objective defined in Equation (5) corresponds to the choice ", "page_idx": 23}, {"type": "equation", "text": "$$\nL^{\\epsilon}(\\rho,\\partial_{t}\\rho,t):=\\frac{1}{2}\\int_{\\mathcal{X}}\\left|\\nabla\\left(-\\Delta_{\\rho}^{\\dagger}\\partial_{t}\\rho+\\frac{\\epsilon^{2}}{2}\\log\\rho\\right)\\right|^{2}\\mathrm{d}\\rho.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The associated momentum $s^{\\epsilon}$ therefore satisfies $\\begin{array}{r}{s^{\\epsilon}=\\frac{\\delta L^{\\epsilon}}{\\delta(\\partial_{t}\\rho)}}\\end{array}$ , hence $\\begin{array}{r}{-\\Delta_{\\rho}s^{\\epsilon}+\\frac{\\epsilon^{2}}{2}\\Delta\\rho=\\partial_{t}\\rho}\\end{array}$ , a Fokker-Planck equation. Furthermore, the action of the curve $t\\mapsto\\rho_{t}$ is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{1}L^{\\epsilon}(\\rho_{t},\\partial_{t}\\rho,t)\\mathrm{d}t=\\int_{0}^{1}\\left(\\frac{1}{2}\\int_{\\mathcal{X}}|\\nabla\\Delta_{\\rho}^{\\dagger}\\partial_{t}\\rho|^{2}\\mathrm{d}\\rho+\\frac{\\epsilon^{4}}{8}\\int_{\\mathcal{X}}|\\nabla\\log\\rho_{t}|^{2}\\,d\\rho_{t}\\right)\\,d t}}\\\\ &{}&{+\\left.\\frac{\\epsilon^{2}}{2}\\left(\\int_{\\mathcal{X}}\\log\\rho_{t}\\mathrm{d}\\rho_{t}\\right|_{t=1}-\\int_{\\mathcal{X}}\\log\\rho_{t}\\mathrm{d}\\rho_{t}\\right|_{t=0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This expression is known as the dual formulation of the Kantorovich-Schr\u00f6dinger problem ([31], Theorem 36, except for the fact that the $\\epsilon$ therein corresponds to $\\epsilon^{2}/2$ here). While the classical optimal transport problem is concerned with the path connecting $\\rho_{0}$ and $\\rho_{1}$ minimizing the time integral of the kinetic energy (which coincides with the transport cost), the Schr\u00f6dinger-Bridge problem is concerned with finding the most likely configuration at intermediate times, subject to the information that the configuration is given at times $\\boldsymbol{0}$ and 1 and assuming that the particles $X_{t}$ undergo Brownian motion with diffusivity $\\varepsilon^{2}/2$ . Unless $\\rho_{1}$ is the result of a convolution of $\\rho_{0}$ with a Gaussian kernel of width $\\varepsilon$ , the evolution of the system towards $\\rho_{1}$ is a rare event and the most likely solution is to be understood conditional on the observation of this event. ", "page_idx": 23}, {"type": "text", "text": "Rigorous results can be found in Section 5 of [31]. Another derivation of the loss function from Equation (5), starting from the static formulation and linking to the dynamical picture presented here, can also be found in [47], Theorem 2.1. In their notation, $\\Psi=-s$ . ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Section 3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 2.1, Appendix C\u2013E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 3, Appendix A\u2013B, code implementation link in Section 1 (retracted for review). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be ", "page_idx": 25}, {"type": "text", "text": "possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 1 provides link to code. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 3, Appendix A\u2013B, code publication discussed in Section 1. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Figure 2 shows replicates, results reported in Figure 3\u20136, Table 1 are based on thousands of samples. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Table 1, Appendix B. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Only data from numerical simulations are used. We do not expect that this work has negative societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Section 4. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Appendix B. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Section 1 and Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]