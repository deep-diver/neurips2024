[{"type": "text", "text": "Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuehui $\\mathbf{Y}\\mathbf{u}^{1,2}$ Mhairi Dunion2 Xin Li1 Stefano V. Albrecht2 1Harbin Insitute of Technology 2University of Edinburgh {yuxuehui,22s103169}@stu.hit.edu.cn {mhairi.dunion,s.albrecht}@ed.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviour). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the log- $\\mathcal{K}$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$ -sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the log- $K$ curse. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) agents often learn policies that do not generalise across tasks in which the environmental features and optimal skills are different [des Combes et al., 2018, Garcin et al., 2024]. Consider a set of cube-moving tasks where an agent is required to move a cube to a goal position on a table (Figure 1). These tasks become challenging if environmental features, such as table friction, vary between tasks. When facing an unknown environment, the agent ", "page_idx": 0}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/ec54e0a8c3f4e98d66d32da1dffa9eac89a7b97f0f76cba62287e7a239aa0ef5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: (a) In a cube-moving environment, tasks are defined according to different environmental features. (b) Different tasks have different transition dynamics caused by underlying environmental features, hence optimal skills are different across tasks. ", "page_idx": 0}, {"type": "text", "text": "needs to explore effectively, understand the environment, and adjust its behaviour accordingly within an episode. For instance, if the agent tries to push a cube across a table covered by a tablecloth and finds it \u201cunpushable,\u201d it should infer that the table friction is relatively high and adapt by lifting the cube to avoid friction, rather than continuing to push. Recent advances in Meta-Reinforcement Learning (Meta-RL) [Lee et al., 2020, Agarwal et al., 2021, Mu et al., 2022, Dunion et al., 2023b,a, McInroe et al., 2024] enable agents to understand environmental features by inferring context embeddings from a small amount of exploration, and to train a policy conditioned on the context embedding to generalise to novel tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent years, unsupervised contrastive learning algorithms have been shown to learn context embeddings that perform remarkably well on generalisation tasks [Clavera et al., 2019a, Lee et al., 2020]. In particular, some Meta-RL algorithms [Fu et al., 2021, Wang et al., 2021, Li et al., 2021, Sang et al., 2022] train context encoders by maximising InfoNCE [Oord et al., 2019], which has yielded vital insights into contrastive learning through the lens of mutual information (MI) analysis. The InfoNCE objective can be interpreted as a $K$ -sample lower bound on the MI between trajectories and context embeddings. Despite significant advances, integrating contrastive learning with Meta-RL poses several unresolved challenges, of which two are particularly relevant to this research: (i) Existing context encoders based on contrastive learning do not distinguish tasks that require different skills. Many prior algorithms only pull embeddings of the same tasks together and push those of different tasks apart. However, for example, a series of cube-moving tasks with high friction may only require a Pick&Place skill (picking the cube off the table and placing it at the goal position), making further differentiation unnecessary. (ii) Existing $\\pmb{K}$ -sample MI estimators are sensitive to the sample size $\\pmb{K}$ (i.e., the log- $\\mathbfcal{K}$ curse) [Poole et al., 2019]. The low sample efficiency of RL [Franke et al., 2021] and the sample limitations in zero-shot generalisation make collecting a substantial quantity of samples often impractical [Arora et al., 2019, Nozawa and Sato, 2021]. The effectiveness of $K$ -sample MI estimators breaks down with a finite sample size and leads to a significant performance drop in downstream RL tasks [Mnih and Teh, 2012, Guo et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "To enhance RL generalisation across different tasks, we propose that the context embeddings should optimise downstream tasks and indicate whether the current skill remains optimal or requires further exploration. This also reduces the necessary sample size by focusing solely on skill-related information. In this work, (1) we introduce Skill-aware Mutual Information (SaMI), a generalised form of MI objective between context embeddings, skills, and trajectories, designed to address issue (i). We provide a theoretical proof showing that by introducing skills as a third variable into the MI of context embeddings and trajectories, the resulting SaMI is smaller and easier to optimise. Furthermore, (2) we propose a data-efficient $K$ -sample estimator, Skill-aware Noise Contrastive Estimation (SaNCE) to optimise SaMI, effectively addressing issue (ii). Additionally, (3) we propose a practical skill-aware trajectory sampling strategy that shows how to sample positive and negative examples without relying on any prior skill distribution. In that way, Meta-RL agents autonomously acquire a set of skills applicable to many tasks, with these skills emerging solely from the SaMI learning objective and data. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate empirically in MuJoCo [Todorov et al., 2012] and Panda-gym [Gallou\u00e9dec et al., 2021] that SaMI enhances the zero-shot generalisation capabilities of two Meta-RL algorithms [Yu et al., 2020, Fu et al., 2021] by achieving higher returns and success rates on previously unseen tasks, ranging from moderate to extreme difficulty. Visualisation of the learned context embeddings reveals distinct clusters corresponding to different skills, suggesting that the SaMI learning objective enables the context encoder to capture skill-related information from trajectories and incentivise Meta-RL agents to acquire a diverse set of skills. Moreover, SaNCE enables Meta-RL algorithms to use smaller sample spaces while achieving improved downstream control performance, indicating their potential to overcome the log- $K$ curse. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Meta-RL. By conditioning on an effective context embedding, Meta-RL policies can zero-shot generalise to new tasks with a small amount of exploration Kirk et al. [2023]. Existing algorithms can be categorised into three types based on different context embeddings. In the first category, the context embedding is learned by minimising the downstream RL loss [Rakelly et al., 2019, Yu et al., 2020]. PEARL [Rakelly et al., 2019] learns probabilistic context embeddings by recovering the value function. Multi-task SAC with task embeddings as inputs to policies (TESAC) [Hausman et al., 2018, Yu et al., 2020] parameterises the learned policies through a shared embedding space, aiming to maximise the average returns across tasks. However, the update signals from the RL loss are stochastic and weak, and may not capture the similarity relations among tasks [Fu et al., ", "page_idx": 1}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/52bcbb0a20c9bbacc46d60e73452b20c414e4f9a9de0b27cc560067d9bf371de.jpg", "img_caption": ["Figure 2: A policy $\\pi$ conditioned on a fixed context embedding $c$ is defined as a skill $\\pi(\\cdot|c)$ (shortened as $\\pi_{c}$ ). The policy $\\pi$ conditioned on a fixed $c$ alters the state of the environment in a consistent way, thereby exhibiting a mode of skill. The skill $\\pi(\\cdot|c_{1})$ moves the cube on the table in trajectory $\\tau_{c_{1}}^{+}$ and is referred to as the Push skill; correspondingly, the Pick&Place skill $\\pi(\\cdot|c_{2})$ takes the cube off the table and places it in the goal position in the trajectory \u03c4 c+2. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2021]. The second category involves learning context embeddings through dynamics prediction [Lee et al., 2020, Zhou et al., 2019], which can make the context embeddings noisy, as they may model irrelevant dependencies and overlook task-specific information [Fu et al., 2021]. The third category employs contrastive learning [Fu et al., 2021, Wang et al., 2021, Li et al., 2021, Sang et al., 2022], achieving significant improvements in context learning. However, these methods overlook the similarity of skills between different tasks, thus failing to achieve effective zero-shot generalisation by executing different skills. Our improvements build upon this third category by distinguishing context embeddings according to different optimal skills. ", "page_idx": 2}, {"type": "text", "text": "Contrastive learning. Contrastive learning has been applied to RL due to its significant momentum in representation learning in recent years, attributed to its superior effectiveness [Tishby and Zaslavsky, 2015, Hjelm et al., 2019, Dunion and Albrecht, 2024], ease of implementation [Oord et al., 2019], and strong theoretical connection to MI estimation [Poole et al., 2019]. MI is often estimated using InfoNCE [Oord et al., 2019] that has gained recent attention due to its lower variance [Song and Ermon, 2020] and superior performance in downstream tasks. However, InfoNCE may underestimate the true MI when the sample size $K$ is finite. To address this limitation, CCM [Fu et al., 2021] uses a large number of samples for maximising InfoNCE. DOMINO [Mu et al., 2022] reduces the true MI by introducing an independence assumption; however, this results in biased estimates. We focus on proposing an unbiased alternative MI objective and a more data-efficient $K$ -sample estimator tailored for downstream RL tasks, which, to our knowledge, have not been addressed in previous research. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reinforcement learning. In Meta-RL, we assume an environment is a distribution $\\xi(e)$ of tasks $e$ (e.g. uniform in our experiments). Each task $e\\sim\\xi(e)$ has a similar structure that corresponds to a Markov Decision Process (MDP) [Puterman, 2014], defined by $\\mathcal{M}_{e}=(S,\\mathcal{A},R,P_{e},\\gamma)$ , with a state space $\\boldsymbol{S}$ , an action space $\\boldsymbol{\\mathcal{A}}$ , a reward function $R(s_{t},a_{t})$ where $s_{t}\\in\\mathcal S$ and $a_{t}\\in\\mathcal A$ , state transition dynamics $P_{e}(s_{t+1}|s_{t},a_{t})$ , and a discount factor $\\gamma\\in[0,1)$ . In order to address the problem of zeroshot generalisation, we consider the transition dynamics $P_{e}(s_{t+1}|s_{t},a_{t})$ vary across tasks $e\\sim\\xi(e)$ according to multiple environmental features $e=\\mathbf{\\Psi}^{\\prime}\\{e^{0},e^{1},...,e^{N}\\}$ that are not included in states $s$ and can be continuous random variables, such as mass and friction, or discrete random variables, such as the cube\u2019s material. For instance, in a cube-moving environment (Figure 1), an agent has different tasks that are defined by different environmental features (e.g., mass and friction). The Meta-RL agent\u2019s goal is to learn a generalisable policy $\\pi$ that is robust to such dynamic changes. Specifically, given a set of training tasks $e$ sampled from $\\xi_{\\mathrm{train}}(e)$ , we aim to learn a policy that can maximise the discounted returns, $\\begin{array}{r}{\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{e\\sim\\xi_{\\operatorname*{min}}(e)}[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|a_{t}\\sim\\pi(a_{t}|s_{t}),s_{t+1}\\sim P_{e}(s_{t+1}|s_{t},a_{t})]}\\end{array}$ , and can produce accurate control for unseen test tasks sampled from $\\xi_{\\mathrm{test}}(e)$ . ", "page_idx": 2}, {"type": "text", "text": "Contrastive learning. In Meta-RL, the context encoder $\\psi(c|\\tau_{c,0:t})$ first takes the trajectory $\\tau_{c,0:t}=$ $\\{s_{0},a_{0},r_{0},...,s_{t}\\}$ from the current episode as input and compresses it into a context embedding $c$ [Fu et al., 2021]. Then, the policy $\\pi$ , conditioned on context embedding $c$ , consumes the current state $s_{t}$ and outputs the action $a_{t}$ . As a key component, the quality of context embedding $c$ can affect algorithms\u2019 performance significantly. MI is an effective measure of embedding quality [Goldfeld et al., 2019], hence we focus on a context encoder that optimises the InfoNCE objective $I_{\\mathrm{InfoNCE}}(x;y)$ , which is a $K$ -sample estimator and lower bound of the MI $I(x;y)$ [Oord et al., 2019]. Given a query $x$ and a set $Y={\\bar{\\{}y_{1},...,y_{K}\\}}$ of $K$ random samples containing one positive sample $y_{1}$ and $K-1$ negative samples from the distribution $p(y)$ , $I_{\\mathrm{InfoNCE}}(x;y)$ is obtained by comparing pairs sampled from the joint distribution $x,y_{1}\\sim p(x,y)$ to pairs $x,y_{k}$ built using a set of negative examples $y_{2:K}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{\\mathrm{InfoNCE}}(x;y|\\psi,K)=\\mathbb{E}\\left[\\log\\frac{f_{\\psi}(x,y_{1})}{\\frac{1}{K}\\sum_{k=1}^{K}f_{\\psi}(x,y_{k})}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "InfoNCE constructs a formal lower bound on the MI, i.e., $I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)\\le I(x;y)$ [Guo et al., 2022, Chen et al., 2021]. Given two inputs $x$ and $y$ , their embedding similarity is $f_{\\psi}(x,y)\\;=\\;$ $e^{\\psi(x)^{\\top}\\cdot\\psi(y)/\\beta}$ , where $\\psi$ is the context encoder that projects $x$ and $y$ into the context embedding space, the dot product is used to calculate the similarity score between $\\psi({x}),\\psi({y})$ pairs $\\mathrm{[Wu}$ et al., 2018, He et al., 2020], and $\\beta$ is a temperature hyperparameter that controls the sensitivity of the product. Some previous Meta-RL methods [Lee et al., 2020, Mu et al., 2022] learn a context embedding $c$ by maximising $I_{\\mathrm{InfoNCE}}(c;\\tau_{c}|\\psi,K)$ between the context $c$ embedded from a trajectory in the current task, and the historical trajectories $\\tau_{c}$ under the same environmental features setting. ", "page_idx": 3}, {"type": "text", "text": "4 Skill-aware mutual information optimisation for Meta-RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 The log- $\\kappa$ curse of $\\kappa$ -sample MI estimators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a theoretical analysis of the challenge inherent in learning a $K$ -sample estimator for MI, commonly referred to as the log- $\\mathcal{K}$ curse. Based on this theoretical analysis, we give insights to overcome this challenge. Given that we focus on the generalisation of RL, we only consider cases with a finite sample size of $K$ . If a context encoder $\\psi$ in Equation (1) has sufficient training epochs, then $I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)\\approx\\log K$ [Mnih and Teh, 2012, Guo et al., 2022]. Hence, the MI we can optimise is bottlenecked by the number of available samples, formally expressed as: ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 Learning a context encoder $\\psi$ with a $K$ - sample estimator and finite sample size $K$ , we have $I_{I n f o N C E}(x;y|\\psi,K)\\le\\log K\\!\\le\\!I(x;y),$ , when $x$ \u0338\u22a5\u22a5 $y$ (see proof in Appendix $A$ ). ", "page_idx": 3}, {"type": "text", "text": "We do not consider the case when $x~\\perp\\!\\!\\!\\perp~y$ , i.e., $\\log K\\,\\geq\\,I(x;y)\\,=\\,0$ $\\forall K\\geq1)$ , because a MetaRL agent learns a context encoder by maximising MI between trajectories $\\tau_{c}$ and context embeddings $c$ , which are not independent (as shown in Figure 2). While an unbounded sample size $K$ for learning effective context embeddings is theoretically feasible and assumed in many studies Mu et al. [2022], Lee et al. [2020], it is often impractical in practice. Therefore, building on Lemma 1, we derive two key insights with limited samples: (1) generalising the current MI objective to be smaller than $I(x;y)$ (see Section 4.2); (2) developing a $K$ -sample estimator tighter than $I_{\\mathrm{InfoNCE}}$ (see Section 4.3). ", "page_idx": 3}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/23b7cc51c7cf36725bf46770b2d00abefd54f26989285c98d8a777ea75d2adf3.jpg", "img_caption": ["Figure 3: $I_{\\mathrm{InfoNCE}(c;\\pi_{c};\\tau_{c})}$ , with a finite sample size of $K$ , is a loose lower bound of $I(c;\\tau_{c})$ and leads to lower performance embeddings. $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ is a lower ground-truth MI, and $I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c})$ is a tighter lower bound. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4.2 Skill-aware mutual information: a smaller ground-truth MI ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim for our MI learning objective to incentivise agents to acquire a diverse set of skills, enabling them to generalise effectively across tasks. To start with, we define skills [Eysenbach et al., 2018]: ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Skills) $A$ policy $\\pi$ conditioned on a fixed context embedding c is defined as a skill $\\pi(\\cdot|c)$ , abbreviated as $\\pi_{c}$ . If a skill $\\pi_{c}$ is conditioned on a state $s_{t}$ , we can sample actions $a_{t}~\\sim$ $\\pi(\\cdot|c,s_{t})$ . After sampling actions from $\\pi_{c}$ at consecutive timesteps, we obtain a trajectory $\\tau_{c,t:t_{T}}=$ $\\left\\{s_{t},a_{t},r_{t},s_{t+1},\\ldots,s_{t+T},a_{t+T},r_{t+T}\\right\\}$ which demonstrates a consistent mode of behaviour. ", "page_idx": 3}, {"type": "text", "text": "After a limited amount of exploration, an agent should be able to infer the task (i.e., environmental features $e\\,=\\,e^{0},e^{1},\\cdot\\cdot\\cdot,e^{N}\\bar{)}$ and adapt accordingly within the current episode. The context embedding should encompass skill-related information, guiding the policy on when to explore new skills or switch between existing ones. We propose that the context encoder $\\psi$ should be trained by maximising the MI between the context embedding $c$ , skills $\\pi_{c}$ , and trajectories $\\tau_{c}$ . To this end, we propose a novel MI optimisation objective, Skill-aware Mutual Information (SaMI), defined as: ", "page_idx": 3}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/1ba7321412d07d7b586acdb4367de78fdc58132e44379619f86f537d32919093.jpg", "img_caption": ["Figure 4: A comparison of sample spaces for task $e_{1}$ . Positive samples $\\tau_{c_{1}}$ or $\\tau_{c_{1}}^{+}$ are always from current task $e_{1}$ . For SaNCE, in a task $e_{k}$ with embedding $c_{k}$ , the positive skill $\\pi_{c_{k}}^{+}$ conditions on $c_{k}$ and generates positive trajectories $\\tau_{c_{k}}^{+}$ , and the negative skill $\\pi_{c_{k}}^{-}$ generates negative trajectories $\\tau_{c_{k}}^{-}$ . The top graphs show the relationship between $c$ , $\\pi_{c}$ and $\\tau_{c}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})=I(c;\\tau_{c})-I(c;\\tau_{c}|\\pi_{c}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "SaMI is defined according to interaction information [McGill, 1954], serving as a generalisation of MI for three variables $\\{c,\\pi_{c},\\tau_{c}\\}$ . Although we cannot evaluate $p(c,\\pi_{c},\\tau_{c})$ directly, we approximate it by Monte-Carlo sampling, using $K$ samples from $p(c,\\pi_{c},\\tau_{c})$ . As illustrated in Figure 3, a context encoder $\\psi$ trained with the objective of maximising $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ converges more quickly, as $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})\\leq I(c;\\tau_{c})$ (see proof in Appendix B). By focusing more on skill-related information, $I_{S a M I}$ enables agents to autonomously discover a diverse range of skills for handling multiple tasks. ", "page_idx": 4}, {"type": "text", "text": "4.3 Skill-aware noise contrastive estimation: a tighter $\\kappa$ -sample estimator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite InfoNCE\u2019s success as a $K$ -sample estimator for approximating MI [Laskin et al., 2020, Eysenbach et al., 2022], its learning efficiency plunges due to limited numerical precision, which is called the log- $K$ curse, i.e., $I_{\\mathrm{InfoNCE}}\\le\\log K\\le I_{\\mathrm{SaMI}}$ [Chen et al., 2021] (see proof in Appendix B). When $K\\rightarrow+\\infty$ , we can expect $I_{\\mathrm{InfoNCE}}\\approx\\log K\\approx I_{\\mathrm{SaMI}}$ [Guo et al., 2022]. However, increasing $K$ is too expensive, especially in complex environments with enormous negative sample space. To address this, we propose a novel $K$ -sample estimator that requires a significantly smaller sample size $K\\ll+\\infty$ . First, we define $K^{*}$ : ", "page_idx": 4}, {"type": "text", "text": "Definition 2 $(K^{*})$ $K^{*}=|\\boldsymbol{c}|\\cdot\\left|\\pi_{c}\\right|\\cdot M$ is defined as the number of trajectories in the replay buffer (i.e., the sample space), in which $|c|$ represents the number of different context embeddings c, $|\\pi_{c}|$ represents the number of different skills $\\pi_{c}$ , and $M$ is a natural number. ", "page_idx": 4}, {"type": "text", "text": "To ensure that $I_{\\mathrm{InfoNCE}}$ is a tight bound of $I_{\\mathrm{SaMI}}$ , we require that $I_{\\mathrm{InfoNCE}}\\approx\\log K\\approx I_{\\mathrm{SaMI}}$ when $K\\rightarrow K^{*}$ . Under the definition of $K^{*}$ , the replay buffer can be divided according to the different context embeddings $c$ and skills $\\pi_{c}$ (i.e., observing context embeddings $c$ and skills $\\pi_{c}$ ). In real-world robotic control tasks, the sample space size significantly increases due to multiple environmental features $e=\\{e^{0},e^{1},...,e^{N}\\}$ . Taking the sample space of InfoNCE as an example (Figure 4(a)), in the current task $e_{1}$ with context embedding $c_{1}$ , positive samples are trajectories $\\tau_{c_{1}}$ generated after executing the skill $\\pi_{c_{1}}$ in task $e_{1}$ , and negative samples are trajectories $\\{\\tau_{c_{2},\\,\\cdots}\\}$ from other tasks $\\{e_{2},...\\}$ . The permutations and combinations of $N$ environmental features lead to an exponential growth in task number $|c|$ , which in turn results in an increase of sample space $K_{\\mathrm{InfoNCE}}^{*}=|c|\\cdot|\\pi|\\cdot M$ ", "page_idx": 4}, {"type": "text", "text": "We introduce a tight $K$ -sample estimator, Skill-aware Noise Contrastive Estimation (SaNCE), which is used to approximate $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ with $K_{\\mathrm{SaNCE}}^{*}<K_{\\mathrm{InfoNCE}}^{*}$ . For SaNCE, both positive samples $\\tau_{c_{1}}^{+}$ and negative samples $\\tau_{c_{1}}^{-}$ are sampled from the current tasks $e_{1}$ , but are generated by executing positive skills $\\pi_{c_{1}}^{+}$ and negative skills $\\pi_{c_{1}}^{-}$ , respectively. Here, a positive skill is intuitively defined by whether it is optimal for the current task $e$ , with a more formal definition provided in Section 4.4. For instance, in a cube-moving task under a large friction setting, the agent executes a skill $\\pi_{c}^{+}$ after several iterations of learning, and obtains corresponding trajectories $\\tau_{c}^{+}$ where the cubes leave the table surface. This indicates that the skill $\\pi_{c}^{+}$ is Pick&Place and other skills $\\pi_{c}^{-}$ may include Push or Flip (filpping the cube to the goal position), with corresponding trajectories $\\tau_{c}^{-}$ where the cube remains stationary or rolls on the table. Formally, we can optimise the $K$ -sample lower bound $I_{\\mathrm{SaNCE}}$ to approximate $I_{\\mathrm{SaMI}}$ : ", "page_idx": 4}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/96c32d132c5b7552c58ba842382b0fdc1c1bf6208a715baa21e7c03f892bc28e.jpg", "img_caption": ["Figure 5: A practical framework for using SaNCE in the meta-training phase. During meta-training, we sample trajectories from the replay buffer for off-policy training. Queries are generated by a context encoder $\\psi$ , which is updated with gradients from both the SaNCE loss $\\mathcal{L}_{\\mathrm{SaNCE}}$ and the RL loss $\\mathcal{L}_{R L}$ . negative/positive embeddings are encoded by a momentum context encoder $\\psi^{*}$ , which is driven by a momentum update with the encoder $\\psi$ . During meta-testing, the meta-trained context encoder $\\psi$ embeds the current trajectory, and the RL policy takes the embedding as input together with the state for adaptation within an episode. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)}\\\\ &{=\\mathbb{E}_{p(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})p(\\tau_{c_{1},2:K}^{-})}\\left[\\log\\left(\\frac{K\\cdot f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1},k}^{-})}\\right)\\right]}\\\\ &{\\le I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}})\\,=\\,e^{\\psi(\\tau_{c_{1}})^{\\top}\\cdot\\psi^{*}(\\tau_{c_{1}})/\\beta}$ . The query $c_{1}\\,=\\,\\psi(\\tau_{c_{1}})$ is generated by the context ennecgoatdievre $\\psi$ .m bFeodr dtirnagins.in gS astNaCbiEli tsyi, gnwief icuasnet lay  mreodmuceenst utmhe  ernecqoudierre $\\psi^{*}$ atmo ppler osdpuaccee  tshiez ep a nbdy $K_{\\mathrm{SaNCE}}^{*}$ sampling trajectories $\\tau_{c}$ based on different skills $\\pi_{c}$ (Figure $4({\\mathfrak{b}}))$ in task $e_{1}$ , so that $K_{\\mathrm{SaNCE}}^{*}=$ $|c|\\cdot|\\pi_{c}|\\cdot M=|\\pi_{c_{1}}|\\cdot M\\leq K_{\\mathrm{InfoNCE}}^{*}\\left(|c|=|c_{1}|=1\\right)$ . Therefore, $I_{\\mathrm{SaNCE}}$ satisfies Lemma 2: ", "page_idx": 5}, {"type": "text", "text": "Lemma 2 With a context encoder $\\psi$ and finite sample size $K$ , we have $I_{I n f o N C E}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le$ $I_{S a N C E}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le\\log K\\le I_{S a M I}(c;\\pi_{c};\\tau_{c})\\le I(c;\\tau_{c}).$ . (see proof in Appendix $B$ ) ", "page_idx": 5}, {"type": "text", "text": "SaNCE can be used alone or combined with other optimisation objectives to train context encoders in Meta-RL algorithms. For instance, integrating SaNCE with InfoNCE diversifies the negative sample space, with KS\u2217a+InfoNCE = $\\begin{array}{r}{K_{\\mathrm{Sa+InfoNCE}}^{*}=\\left(\\sum_{i=1}^{|c|}|\\pi_{c_{i}}^{-}|+|\\pi_{c_{1}}^{+}|\\right)\\cdot M}\\end{array}$ . The sample space for $I_{\\mathrm{Sa+InfoNCE}}$ is depicted in Figure 4(c) and further analysed in detail in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.4 Skill-aware trajectory sampling strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we propose a practical trajectory sampling method. Methods focusing on skill diversity often rely heavily on accurately defining and identifying individual skills [Eysenbach et al., 2018]. Some of these methods require a prior skill distribution, which is often inaccessible [Shi et al., 2022], and it is impractical to enumerate all possible skills that we hope the model to learn. Besides, we believe that distinctiveness of skills is inherently difficult to achieve \u2014 a slight difference in states can make two skills distinguishable, and not necessarily in a semantically meaningful way. Consequently, we do not directly teach any of these skills or assume any prior skill distribution. Diverse skills naturally emerge from the incentives of the SaMI learning objective in a multi-task setting, driven by the inherent need to develop generalisable skills. For example, in high-friction tasks, the agent must acquire the Pick&Place skill to avoid large frictional forces, whereas in high-mass tasks, the agent must learn the Push skill since it cannot lift the cube. In each task, we only identify whether the skills are optimal; in this way, under a multi-task setting, the agent will acquire a set of general skills that are applicable to many tasks. ", "page_idx": 5}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/df22d4b3aabab89ba3fafcbaf58b819b051d92da58af24e72f581670921a5da8.jpg", "img_caption": ["Figure 6: (a) UMAP visualisation of context embeddings for the SaCCM in the Panda-gym environment, with points in the yellow box representing the Push skill in high-mass tasks. Heatmap of (b) success rate, (c) Push skill probability, and (d) Pick&Place skill probability for SaCCM. In large-mass scenarios, the Push skill is more likely to be executed than Pick&Place. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In a given task $e$ , positive skills $\\pi_{c}^{+}$ are defined as optimal skills achieving highest return $\\textstyle\\sum_{i=t}^{t+T}R(s_{i},a_{i})$ , whereas negative skills $\\pi_{c}^{-}$ are those that result in lower returns. As a result, the positive sample $\\tau_{c}^{+}$ consists of trajectories generated by the skills with the highest ranked returns, while the negative samples correspond to those with the lowest returns. This straightforward approach of selecting positive samples based on the ranked highest return effectively aligns with positive skills and mitigates the challenge of hard negative examples [Robinson et al., 2021]. The SaNCE loss is then minimised to bring the context embeddings of the highest return trajectories closer while distancing those of negative trajectories. By the end of training, the top-ranked trajectories in the ranked replay buffer correspond to positive samples $\\tau_{c}^{+}$ with high returns, while the lower-ranked trajectories represent negative samples $\\tau_{c}^{-}$ with low returns. However, at the beginning of training, it is likely that all trajectories have low returns. Therefore, our SaNCE loss is a soft variant of the $K$ -sample SaNCE: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SaNCE}}=-\\operatorname*{max}\\left(||\\psi(\\tau_{c}^{+}),\\psi(\\tau_{c}^{-})||_{L^{2}},1\\right)\\cdot I_{\\mathrm{SaNCE}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $||\\cdot||_{L2}$ represents the Euclidean distance [Tabak, 2014]. Figure 5 provides a practical framework of SaNCE, with a cube-moving example task $e_{1}$ under high friction. In task $e_{1}$ , the positive skill $\\pi_{c_{1}}^{+}$ is the Pick&Place skill, which is used to generate queries $\\psi(\\tau_{c_{1}}^{+})$ and positive embeddings ${\\psi}^{*}(\\tau_{c_{1}}^{+})$ ; after executing Push skill we get negative samples $\\tau_{c_{1}}^{-}$ and negative embeddings $\\psi^{*}(\\tau_{c_{1}}^{-})$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our experiments aim to answer the following questions: (1) Does optimising SaMI lead to increased returns during training and zero-shot generalisation (see Table 1 and 2)?; (2) Does SaMI help the RL agents to be versatile and embody multiple skills (see Figure 6)?; (3) Can SaNCE overcome the log- $\\mathcal{K}$ curse in sample-limited scenarios (see Table 1 and 2, and Section 5.4)? ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Modified benchmarks with multiple environmental features.1We evaluate our method on two benchmarks, Panda-gym [Gallou\u00e9dec et al., 2021] and MuJoCo [Todorov et al., 2012] (details in Sections 5.2 and 5.3). The benchmarks are modified to be influenced by multiple environmental features, which are sampled at the start of each episode during meta-training and meta-testing. During meta-training, we uniform-randomly select a combination of environmental features from a training task set. At test time, we evaluate each algorithm in unseen tasks with environmental features outside the training range. Generalisation performance is measured in two different regimes: moderate and extreme. The moderate regime draws environmental features from a closer range to the training range compared to the extreme. Our results report the mean and standard deviation of models trained over five seeds in both training and test tasks. Further details are available in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Baselines. The loss function of the context encoder in all algorithms consists of two key components: the RL loss and the contrastive loss. The RL loss $\\mathcal{L}_{R L}$ , which is the same across all methods, corresponds to the RL value function loss. Our primary comparison focuses on the contrastive loss, taking the form of either SaNCE, InfoNCE, or no contrastive loss. Accordingly, we select baselines based on contrastive loss: CCM [Fu et al., 2021], which utilises InfoNCE, and TESAC [Yu et al., 2020], which relies solely on the RL loss, allowing assessment of the context encoder without contrastive loss. Since CCM and TESAC use an RNN encoder, we also include PEARL [Rakelly et al., 2019], which utilises an MLP context encoder and follows the similar RL loss. Additionally, Appendix G includes comparisons with DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020], using the same environmental setup in the MuJoCo benchmark. ", "page_idx": 7}, {"type": "text", "text": "Our method.2 We use Soft Actor-Critic (SAC) [Haarnoja et al., 2018] as the base RL algorithm, training agents for 1.6 million timesteps in each environment (details in Appendix D.3). SaNCE is a simple objective based on MI that can be used to train any context encoder. We integrate SaNCE into two Meta-RL algorithms: (1) SaTESAC is TESAC with SaNCE, which uses SaNCE for contrastive learning, using a $|c|$ times smaller sample space (Figure 4(b)); (2) SaCCM is CCM with SaNCE, where the contrastive learning combines InfoNCE and SaNCE, as shown in Figure 4(c). ", "page_idx": 7}, {"type": "text", "text": "5.2 Panda-gym ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task description. Our modified Pandagym benchmark involves a robot arm control task using the Franka Emika Panda [Gallou\u00e9dec et al., 2021], where the robot moves a cube to a target position. Unlike previous works, we simultaneously modify multiple environmental features (cube mass and table friction) that characterise the transition dynamics, and the robot can flexibly execute different skills (Push and Pick&Place) for different tasks. This environment requires high skill diversity; for instance, the agent must use Pick&Place in high-friction tasks and Push in high-mass tasks. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Comparison of success rate $\\pm$ standard deviation with baselines in Panda-gym (over 5 seeds). Bold text signifies the highest average return. $^*$ next to the number means that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI. All significance claims based on paried t-tests with significance threshold of $p<0.05$ . ", "page_idx": 7}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/daf87a600e47d8d4303faa175997925ab1c88c18749acebd130c329a44052816.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results and skill analysis. As shown in Table 1, SaTESAC and SaCCM achieve superior generalisation performance compared to PEARL, TESAC, and CCM, with a smaller sample space. The t-test results in Table 1 show that SaMI significantly improves success rates across training, moderate, and extreme test sets at a 0.05 significance level. Video demos2 show that agents equipped with SaMI acquired multiple skills (Push, Pick&Place) to handle various tasks. When faced with an unknown task, the agents explore by attempting to lift the cube, infer the context, and adjust their skills accordingly within the episode. We visualised the context embeddings using UMAP [McInnes et al., 2020] (Figure 6(a) and Appendix F) and t-SNE [Van der Maaten and Hinton, 2008] (Appendix F), plotting the final step from 100 tests per task. Skills were identified through contact points between the end effector, cube, and table (see Appendix D for more details), and heatmaps [Waskom, 2021] were used to visualise executed skills. Figure 6 shows that SaCCM agents learned the Push skill for large cube masses $(30\\,\\mathrm{Kg},10\\,\\mathrm{Kg})$ and Pick&Place for smaller masses, while CCM showed no clear skill grouping (Figure 16 in Appendix F.1). Overall, SaMI incentivises agents to autonomously learn diverse skills, enhancing generalisation across a wider range of tasks. Specifically, through the cycle of effective exploration, context inference, and adaptation, diverse skills emerge solely from the data. Further visualisation results are in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "5.3 MuJoCo ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task description. We extended the modified MuJoCo benchmark introduced in DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020]. It contains ten typical robotic control environments based on the MuJoCo physics engine [Todorov et al., 2012]. Hopper, Walker, Half-cheetah, Ant, HumanoidStandup, and SlimHumanoid are influenced by continuous environmental features (i.e., mass, damping) that affect transition dynamics. Crippled Ant, Crippled Hopper, Crippled Walker, and ", "page_idx": 7}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/74ddd95081fb74c5615aa50ff982bd36c111653d68eab2cd45e9bcfe7e92a722.jpg", "table_caption": ["Table 2: Comparison of average return $\\pm$ standard deviation with baselines in modified MuJoCo benchmark (over 5 seeds). Bold number signifies the highest return. $^*$ next to the number means that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI. All significance claims based on t-tests with significance threshold of $p<0.05$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Crippled Half-cheetah are more challenging due to the addition of discrete environmental features (i.e., randomly crippled leg joints), requiring agents to master different skills (e.g., switching from running to crawling after a leg is crippled). ", "page_idx": 8}, {"type": "text", "text": "Results and skill analysis. Table 2 shows the average return of our method and baselines on training and test tasks. SaTESAC and SaCCM achieved higher returns in most tasks, except for Ant, Half-Cheetah, and Hopper, where only a single skill was needed. For instance, the Hopper robot learned to hop forward, adapting to different mass values. When environments become complex and require diverse skills for different tasks (Crippled Ant, Crippled Hopper, Crippled Half-Cheetah, SlimHumanoid, HumanoidStandup, and Crippled Walker), SaNCE brings significant improvements. For example, when the Ant robot has 3 or 4 legs available, it learns to roll to generalise across varying mass and damping. In more challenging zero-shot settings, when only 2 legs are available, the ant robot can no longer roll and it adapts by walking using its 2 healthy legs. This aligns with the results in Table 2, where SaMI significantly improved performance in extreme test sets. In summary, i) SaMI helps the RL agents to be versatile and embody multiple skills; ii) SaMI leads to increased returns ", "page_idx": 8}, {"type": "text", "text": "during training and zero-shot generalisation, especially in environments that require different skills.   \nOur video demos2 and visualisation results in Appendix F.2 show different skills in all environments. ", "page_idx": 9}, {"type": "text", "text": "5.4 Analysis of the log- $\\kappa$ curse in sample-limited scenarios ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section analyses whether SaNCE can overcome the log- $K$ curse. During training, environmental features are sampled at the start of each episode, requiring the context encoder to learn the context embedding distribution across multiple tasks. Since InfoNCE samples negative examples from all tasks and SaNCE samples from the current task, SaNCE\u2019s negative sample space is $|c|$ times smaller than InfoNCE\u2019s. For instance, in the SlimHumanoid environment, where both mass and damping have five values, InfoNCE\u2019s sampling space can be 25 times larger than SaNCE\u2019s. As shown in Tables 1 and 2, RL algorithms using SaNCE achieve better or comparable performance with significantly fewer negative samples $(K)$ than InfoNCE. This suggests SaNCE effectively addresses the log- $\\mathcal{K}$ curse, and the SaMI objective helps the contrastive context encoder extract critical information for downstream RL tasks. ", "page_idx": 9}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/9946dae2d370d1400e5a7af43ff5bfd6667eb7231238178a155744779ebb90d0.jpg", "img_caption": ["Figure 7: Effect of (a) buffer size (TESAC, CCM, SaTESAC, SaCCM) and (b) contrastive batch size (CCM, SaTESAC, SaCCM) in the SlimHumanoid environment. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The number of negative samples $K$ is influenced by two hyperparameters: buffer size, which determines the negative sample space, and contrastive batch size, which controls the number of samples used to train the contrastive context encoder per update. We analysed these hyperparameters further, and as shown in Figure 7, reductions in buffer and contrastive batch size do not significantly impact the average return for SaCCM and SaTESAC, which maintain state-of-the-art performance with small buffers and batch sizes. The results in Table 2 correspond to a buffer size of 100,000 and a batch size of 12. Results across all environments (refer to Appendix E.2) show that SaNCE exhibits low sensitivity to $K$ , highlighting its potential to overcome the log- $\\mathcal{K}$ curse. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zero-shot generalisation has been a longstanding challenge concerning Meta-RL agents, with skill diversity and sample efficiency being key to generalising to previously unseen environments. In this paper, we proposed Skill-aware Mutual Information (SaMI) to learn context embeddings for zero-shot generalisation in downstream RL tasks, and Skill-aware Noise Contrastive Estimation (SaNCE) to optimise SaMI and overcome the log- $\\mathcal{K}$ curse, along with a practical skill-aware trajectory sampling strategy. Experimental results showed that RL algorithms equipped with SaMI achieved state-ofthe-art performance in MuJoCo and Panda-gym benchmarks, particularly in zero-shot generalisation within more complex environments. During the zero-shot generalisation, when faced with an unseen task, SaMI assists agents in exploring effectively, inferring context, and rapidly adapting their skills within the current episode. SaNCE\u2019s optimisation uses a significantly smaller negative sample space than baselines, and our analysis on buffer and contrastive batch sizes demonstrated its effectiveness in addressing the log- $K$ curse. ", "page_idx": 9}, {"type": "text", "text": "Given that environmental features are often interdependent, such as a cube\u2019s material correlating with friction and mass, SaMI does not introduce independence assumptions like DOMINO [Mu et al., 2022]. Therefore, future work will focus on verifying and enhancing SaMI\u2019s potential in more complex tasks where environmental features are correlated [Dunion et al., 2023a]. This will contribute to our ultimate goal: developing a generalist and versatile agent capable of working across multiple tasks and even real-world tasks in the near future. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. arXiv preprint arXiv:2101.05265, 2021.   \nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229, 2019.   \nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019.   \nJunya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung Chung, Yi Xu, Belinda Zeng, Wenlian Lu, et al. Simpler, faster, stronger: Breaking the log-k curse on contrastive learners with flatnce. arXiv preprint arXiv:2107.01152, 2021.   \nIgnasi Clavera, Anusha Nagabandi, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. In International Conference on Learning Representations, 2019a.   \nIgnasi Clavera, Anusha Nagabandi, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. In International Conference on Learning Representations, 2019b.   \nRemi Tachet des Combes, Philip Bachman, and Harm van Seijen. Learning invariances for policy generalization, 2018. URL https://openreview.net/forum?id $\\equiv$ BJHRaK1PG.   \nMhairi Dunion and Stefano V Albrecht. Multi-view disentanglement for reinforcement learning with multiple cameras. In Reinforcement Learning Conference, 2024.   \nMhairi Dunion, Trevor McInroe, Kevin Sebastian Luck, Josiah P. Hanna, and Stefano V Albrecht. Conditional mutual information for disentangled representations in reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.   \nMhairi Dunion, Trevor McInroe, Kevin Sebastian Luck, Josiah P. Hanna, and Stefano V Albrecht. Temporal disentanglement of representations for improved generalisation in reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023b.   \nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function, 2018. URL https://arxiv.org/abs/1802.06070.   \nBenjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35:35603\u201335620, 2022.   \nJ\u00f6rg K.H. Franke, Gregor Koehler, Andr\u00e9 Biedenkapp, and Frank Hutter. Sample-efficient automated deep reinforcement learning. In International Conference on Learning Representations, 2021.   \nHaotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu. Towards effective context for meta-reinforcement learning: an approach based on contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7457\u20137465, 2021.   \nQuentin Gallou\u00e9dec, Nicolas Cazin, Emmanuel Dellandr\u00e9a, and Liming Chen. panda-gym: OpenSource Goal-Conditioned Environments for Robotic Learning. 4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS, 2021.   \nSamuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas, and Stefano V. Albrecht. DRED: Zero-shot transfer in reinforcement learning via data-regularised environment design. In International Conference on Machine Learning (ICML), 2024.   \nZiv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and Yury Polyanskiy. Estimating information flow in deep neural networks, 2019. URL https: //arxiv.org/abs/1810.05728.   \nQing Guo, Junya Chen, Dong Wang, Yuewei Yang, Xinwei Deng, Jing Huang, Larry Carin, Fan Li, and Chenyang Tao. Tight mutual information estimation with contrastive fenchel-legendre optimization. Advances in Neural Information Processing Systems, 35:28319\u201328334, 2022.   \nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \nKarol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.   \nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.   \nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks, 2016.   \nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76: 201\u2013264, 2023.   \nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International conference on machine learning, pages 5639\u20135650. PMLR, 2020.   \nKimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. Context-aware dynamics model for generalization in model-based reinforcement learning. In International Conference on Machine Learning, pages 5757\u20135766. PMLR, 2020.   \nLanqing Li, Yuanhao Huang, Mingzhe Chen, Siteng Luo, Dijun Luo, and Junzhou Huang. Provably improved context-based offline meta-rl with attention and contrastive learning. arXiv preprint arXiv:2102.10774, 2021.   \nQiang Li. Functional connectivity inference from fmri data using multivariate information measures. Neural Networks, 146:85\u201397, 2022.   \nWilliam McGill. Multivariate information transmission. Transactions of the IRE Professional Group on Information Theory, 4(4):93\u2013111, 1954.   \nLeland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction, 2020. URL https://arxiv.org/abs/1802.03426.   \nTrevor McInroe, Lukas Sch\u00e4fer, and Stefano V. Albrecht. Multi-horizon representations with hierarchical forward models for reinforcement learning. Transactions on Machine Learning Research (TMLR), 2024.   \nAndriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.   \nYao Mu, Yuzheng Zhuang, Fei Ni, Bin Wang, Jianyu Chen, Jianye HAO, and Ping Luo. DOMINO: Decomposed mutual information optimization for generalized context in meta-reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \nLeland Gerson Neuberg. Causality: models, reasoning, and inference, by judea pearl, cambridge university press, 2000. Econometric Theory, 19(4):675\u2013685, 2003.   \nKento Nozawa and Issei Sato. Understanding negative samples in instance discriminative selfsupervised representation learning. Advances in Neural Information Processing Systems, 34: 5784\u20135797, 2021.   \nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2019.   \nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171\u2013 5180. PMLR, 2019.   \nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.   \nKate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pages 5331\u20135340. PMLR, 2019.   \nJohn A Rice and John A Rice. Mathematical statistics and data analysis, volume 371. Thomson/Brooks/Cole Belmont, CA, 2007.   \nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples, 2021. URL https://arxiv.org/abs/2010.04592.   \nTong Sang, Hongyao Tang, Yi Ma, Jianye Hao, Yan Zheng, Zhaopeng Meng, Boyan Li, and Zhen Wang. Pandr: Fast adaptation to new environments from offline experiences via decoupling policy and environment representations, 2022.   \nYounggyo Seo, Kimin Lee, Ignasi Clavera Gilaberte, Thanard Kurutach, Jinwoo Shin, and Pieter Abbeel. Trajectory-wise multiple choice learning for dynamics generalization in reinforcement learning. Advances in Neural Information Processing Systems, 33:12968\u201312979, 2020.   \nLucy Xiaoyang Shi, Joseph J. Lim, and Youngwoon Lee. Skill-based model-based reinforcement learning. In Conference on Robot Learning, 2022.   \nJiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. In International Conference on Learning Representations, 2020.   \nJohn Tabak. Geometry: the language of space and form. Infobase Publishing, 2014.   \nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pages 1\u20135. IEEE, 2015.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \nBernie Wang, Simon Xu, Kurt Keutzer, Yang Gao, and Bichen Wu. Improving context-based meta-reinforcement learning with self-supervised trajectory contrastive learning, 2021.   \nMichael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.   \nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020. ", "page_idx": 13}, {"type": "text", "text": "Wenxuan Zhou, Lerrel Pinto, and Abhinav Gupta. Environment probing interaction policies. In International Conference on Learning Representations, 2019. ", "page_idx": 13}, {"type": "text", "text": "A Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given a query $x$ and a set $Y=\\{y_{1},\\ldots,y_{K}\\}$ of $K$ random samples, containing one positive sample $y_{1}$ and $K-1$ negative samples drawn from the distribution $p(y)$ , a $K$ -sample InfoNCE estimator is obtained by comparing pairs sampled from the joint distribution $(x,y_{1})\\sim p(x,y)$ with pairs $(x,y_{k})$ , constructed using the set of negative examples $y_{2:K}$ . InfoNCE compares the positive pairs $(x,y_{1})$ with the negative pairs $(x,y_{k})$ , where $y_{k}\\sim y_{2:K}$ , as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nI_{\\mathrm{InfoNCE}}(x;y|\\psi,K)=\\mathbb{E}_{p(x,y_{1})p(y_{2:K})}\\left[\\log\\left(\\frac{f_{\\psi}(x,y_{1})}{\\frac{1}{K}\\sum_{k=1}^{K}f_{\\psi}(x,y_{k})}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Step 1. Let us prove that the $K$ -sample InfoNCE estimator is upper-bounded by $\\log K$ . According to Mu et al. [2022], kK=1 f\u03c8(x,yk) =f\u03c8(x,y1)+ kK=2 f\u03c8(x,yk) \u22641. So we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)=\\mathbb{E}_{p(x,y_{1})p(y_{2:K})}\\left[\\log\\left(\\frac{f_{\\psi}(x,y_{1})}{\\frac{1}{K}\\sum_{k=1}^{K}f_{\\psi}(x,y_{k})}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{p(x,y)}\\left[\\mathbb{E}_{p(y_{2:K})}\\log\\left(\\frac{K\\cdot f_{\\psi}(x,y_{1})}{\\sum_{k=1}^{K}f_{\\psi}(x,y_{k})}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\log K}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, we have $I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)\\le\\log K$ . ", "page_idx": 14}, {"type": "text", "text": "Step 2. We have the $I(x;y)\\ge I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)$ according to: ", "page_idx": 14}, {"type": "text", "text": "Proposition 1 [Poole et al., 2019] A $K$ -sample estimator is an asymptotically tight lower bound on the MI, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\nI(x;y)\\ge I_{I n f o N C E}(x;y|\\psi,K),\\operatorname*{lim}_{x\\to+\\infty}I_{I n f o N C E}(x;y|\\psi,K)\\to I(x;y)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See Poole et al. [2019] for a neat proof of how the multi-sample estimator (e.g., InfoNCE) lower bounds MI. ", "page_idx": 14}, {"type": "text", "text": "Step 3. In this research, the context encoder $\\psi$ in $f_{\\psi}(x,y)$ is implemented using an RNN to approximate pp((yy|x)) [Oord et al., 2019]. With a sufficiently powerful deep learning model for $\\psi$ and a finite sample size $K$ , such that $I(x;y)\\geq\\log K$ , we can reasonably expect that $I_{\\mathrm{InfoNCE}}\\approx\\log K$ after a few training epochs. Therefore, during training, when $K\\ll+\\infty$ , we always have $I(x;y)\\geq\\log K$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. See Chen et al. [2021] for more detailed proof. ", "page_idx": 14}, {"type": "text", "text": "Step 4. Let us prove that the $K$ -sample InfoNCE bound is asymptotically tight. The specific choice of the context encoder $\\psi$ influences the tightness of the $K$ -sample NCE bound. InfoNCE [Oord et al., 2019] sets f\u03c8(x, y) \u221d pp((yy|x)) to model a density ratio that preserves the MI between $x$ and $y$ , where ${\\displaystyle\\propto}$ stands for \u2019proportional to\u2019 (i.e., up to a multiplicative constant). Substituting $\\begin{array}{r}{f_{\\psi}(x,y)=f_{\\psi}^{*}(x,y)=\\frac{p(y|x)}{p(y)}}\\end{array}$ into InfoNCE, we obtain: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{\\mathrm{Hobxc}}(x;y|\\psi,K)=\\mathbb{E}\\left[\\log\\left(\\frac{f_{y}^{\\ast}(x,y_{1})}{\\sum_{k=1}^{K}f_{y}^{\\ast}(x,y_{k})}\\right)\\right]+\\log K}&{}\\\\ {=-\\mathbb{E}\\left[\\log\\left(1+\\frac{p(y)}{p(y)}\\sum_{k=2}^{K}\\frac{p(y_{k}|x)}{p(y_{k})}\\right)\\right]+\\log K}&{}\\\\ {\\approx-\\mathbb{E}\\left[\\log\\left(1+\\frac{p(y)}{p(y)}(K-1)\\mathbb{E}_{y\\sim\\mathcal{r}(y)}\\frac{p(y_{k}|x)}{p(y_{k})}\\right)\\right]+\\log K}&{}\\\\ {=-\\mathbb{E}\\left[\\log\\left(1+\\frac{p(y_{1})}{p(y_{1})(x)}(K-1)\\right)\\right]+\\log K}&{}\\\\ {\\approx-\\mathbb{E}\\left[\\log\\frac{p(y)}{p(y|x)}\\right]-\\log(K-1)+\\log K}&{}\\\\ {=I(x;y)-\\log(K-1)+\\log K}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now taking $K\\rightarrow+\\infty$ , the last two terms cancel out. ", "page_idx": 15}, {"type": "text", "text": "Putting it together. Combining $I(x;y)\\geq\\log K$ with Proposition 1 and Equation (6), we have Lemma 1: ", "page_idx": 15}, {"type": "equation", "text": "$$\nI_{\\mathrm{InfoNCE}}(x;y|\\psi,K)\\le\\log K\\le I(x;y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, according to Equation (7), as the sample size $K\\rightarrow+\\infty$ , the $K$ -sample InfoNCE bound becomes sharp and approaches the true MI $I(x;y)$ , i.e., $I_{\\mathrm{InfoNCE}}(x;y|\\psi,K)\\approx\\log K\\approx I(x;y)$ . ", "page_idx": 15}, {"type": "text", "text": "B Proof for Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Step 1. According to Lemma 1, we have $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le\\log K\\le I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ (shown in Figure 3). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{p(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})p(\\tau_{c_{1},2:K}^{-})}\\left[\\log\\left(\\frac{K\\cdot f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1},k}^{-})}\\right)\\right]}\\\\ &{=\\mathbb{E}_{p(c_{1},\\pi_{c_{1}})}\\left[\\mathbb{E}_{p(\\tau_{c_{1},2:K}^{-})}\\log\\left(\\frac{K\\cdot f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1},k}^{-})}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we obtain $I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le\\log K$ , similar to Equation (6). ", "page_idx": 15}, {"type": "text", "text": "Step 3. With the definition of $K^{*}$ , we can prove that $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)$ with the same sample size $K$ . In task $e_{1}$ , SaNCE obtains positive and negative samples from the current task $e_{1}$ . Since the variable $c=c_{1}$ is constant, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{SaNCE}}(c;\\tau_{c},\\tau_{c}|\\psi,K)}\\\\ &{=\\mathbb{E}_{p(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})p(\\tau_{c_{1},2,K}^{-})}\\left[\\log\\left(\\frac{K\\cdot f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1},k}^{-})}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{p(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})p(\\tau_{c_{1},2,K}^{-})}\\left[\\log\\left(\\frac{K_{\\mathrm{SaNCE}}^{3}\\cdot f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(c_{1},\\pi_{c_{1}},\\tau_{c_{1}}^{-})}\\right)\\right]}\\\\ &{=\\mathbb{E}_{p(\\pi_{c_{1}},\\tau_{c_{1}}^{+})p(\\tau_{c_{1},2,K}^{-}\\kappa_{\\mathrm{saNC}}^{})}\\left[\\log\\left(\\frac{K_{\\mathrm{SaNCE}}^{5}\\cdot f_{\\psi}(\\pi_{c_{1}},\\tau_{c_{1}}^{+})}{f_{\\psi}(\\pi_{c_{1}},\\tau_{c_{1}}^{+})+\\sum_{k=2}^{K}f_{\\psi}(\\pi_{c_{1}},\\tau_{c_{1},k}^{-})}\\right)\\right](c_{1}\\mathrm{\\,is\\,constant})}\\\\ &{\\approx\\log K_{\\mathrm{SaNCE}}^{5}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The required sample size is $K_{\\mathrm{SaNCE}}^{*}\\;=\\;|c_{1}|\\cdot|\\pi|\\,\\cdot\\,M\\;=\\;|\\pi|\\,\\cdot\\,M$ . As $K\\;\\rightarrow\\;K_{\\mathrm{SaNCE}}^{*}$ , we have $I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\approx I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ . Correspondingly, for InfoNCE, in the current task $e_{1}$ with context embedding $c_{1}$ , positive samples are trajectories $\\tau_{1}$ generated after executing the skill $\\pi_{1}$ in task $e_{1}$ , while negative samples are trajectories $\\{\\tau_{c_{2}}^{-},...\\}$ from other tasks $\\{e_{2},...\\}$ . Under the definition of $K^{*}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)}\\\\ &{=\\mathbb{E}_{p(c,\\pi_{c},\\tau_{c_{1}})p(\\tau_{c_{2};\\lfloor c\\rfloor},2;K)}\\left[\\log\\left(\\frac{K\\cdot f_{\\psi}(c,\\pi_{c},\\tau_{c_{1}})}{f_{\\psi}(c,\\pi_{c},\\tau_{c_{1}})+\\sum_{k=2}^{K}f_{\\psi}(c,\\pi_{c},\\tau_{c_{2};\\lfloor c\\rfloor},k)}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{p(c,\\pi_{c},\\tau_{c_{1}})p(\\tau_{c_{2};\\lfloor c\\rfloor},2;K_{\\mathrm{infoNCE}}^{*})}\\left[\\log\\left(\\frac{K_{\\mathrm{infoNCE}}^{*}\\cdot f_{\\psi}(c,\\pi_{c},\\tau_{c_{1}})}{f_{\\psi}(c,\\pi_{c},\\tau_{c_{1}})+\\sum_{k=2}^{K_{\\mathrm{infoNCE}}^{*}}f_{\\psi}(c,\\pi_{c},\\tau_{c_{2:\\lfloor c\\rfloor},k})}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $K_{\\mathrm{InfoNCE}}^{*}\\,=\\,|c|\\cdot|\\pi|\\cdot M\\,\\approx\\,|c|\\cdot K_{\\mathrm{SaNCE}}^{*}$ . In real-world robotic control tasks, the sample space size increases significantly due to multiple environmental features $e=\\{e^{0},e^{1},\\cdot\\cdot\\cdot,e^{N}\\}$ . The number of different tasks $|c|$ grows exponentially due to the permutations and combinations of the $N$ environmental features. When the current task $e_{1}$ has context embedding $c_{1}$ , the $c_{2:|c|}$ refer to the context embeddings for the other tasks. As $K\\rightarrow K_{\\mathrm{InfoNCE}}^{*}$ , we have $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\approx$ $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ . Thus, during the training process, $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)$ with the same sample size $K$ . ", "page_idx": 16}, {"type": "text", "text": "Step 4. According to the definition of SaMI in Equation (2), we have $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})\\leq I(c;\\tau_{c})$ , as illustrated using Venn diagrams in Figure 8 (a) and (b). SaMI is formulated based on interaction information [McGill, 1954], which aims to capture the relationships among multivariate variables by quantifying the amount of information (redundancy or synergy) shared among three variables. Interaction information has been less extensively studied, partly due to its challenging interpretation from both information theory and neuroscience perspectives, as it can be either positive or negative [Li, 2022]. ", "page_idx": 16}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/d77805fd49b6b2ead18711aabc051938c74a1372d2d6578bd9ca0e6e2a69cdd5.jpg", "img_caption": ["Figure 8: Venn diagrams illustrating (a) mutual information $I(c;\\tau_{c})$ , (b) interaction information $\\bar{I_{\\mathrm{SaMI}}}(c;\\pi_{c};\\tau_{c})$ , and (c) the MDP graph of the context embedding $c$ , skill $\\pi_{c}$ , and trajectory $\\tau_{c}$ , which represents a common-cause structure [Neuberg, 2003]. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In the Meta-RL setting, the MDP (causal) graph structure, shown in Figure 8 (c), illustrates that the context embedding $c$ , skill $\\pi_{c}$ , and trajectory $\\tau_{c}$ form a common-cause structure, where $c$ acts as a shared cause influencing both the skill $\\pi_{c}$ and the trajectory $\\tau_{c}$ . Consequently, $I(\\pi_{c},\\tau_{c}\\mid c)<$ $I(\\pi_{c},\\tau_{c})$ . Therefore, $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})>0$ is guaranteed, making it interpretable in real-world robotic control tasks. ", "page_idx": 16}, {"type": "text", "text": "Putting it together. Thus, we establish Lemma 2: we always have $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\le$ $I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)\\,\\le\\,\\log K\\,\\le\\,I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})\\,\\le\\,I(c;\\tau_{c})$ , as shown in Figure 3, while learning a skill-aware context encoder $\\psi$ with the SaNCE estimator. Since $K_{\\mathrm{SaNCE}}^{*}~\\ll~K_{\\mathrm{InfoNCE}}^{*}$ , $I_{\\mathrm{SaNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)$ serves as a much tighter lower bound for the true $I_{\\mathrm{SaMI}}(c;\\pi_{c};\\tau_{c})$ than $I_{\\mathrm{InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)$ . ", "page_idx": 16}, {"type": "text", "text": "C Sample size of $I_{\\mathbf{Sa+InfoNCE}}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we illustrate the sample size of $I_{\\mathrm{Sa+InfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)$ . Sa+InfoNCE incorporates SaNCE into InfoNCE, using positive samples $\\tau_{c_{1}}^{+}$ from task $e_{1}$ after executing skill $\\pi_{c_{1}}^{+}$ , and negative ", "page_idx": 16}, {"type": "text", "text": "samples are trajectories $\\tau_{c_{1:K}}^{-}$ from executing skills $\\pi_{c_{1:K}}^{-}$ in tasks $e_{1:K}$ , respectively. Therefore, this approach is equivalent to first observing the variable $c$ and then observing the variable $\\pi_{c}$ , i.e., sampling from the distribution $p(\\pi_{c},\\tau_{c}|c)p(c)$ . We have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{Sa+hfoNCE}}(c;\\pi_{c};\\tau_{c}|\\psi,K)}\\\\ &{=\\mathbb{E}_{p(c)p(\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}|c)p\\left(\\Big(\\pi_{2:|c|}^{-},\\tau_{2:|c|}^{-}\\Big)_{2:\\kappa}\\right)}\\left[\\log\\left(\\frac{K\\cdot f_{\\psi}\\left(c,\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}\\right)}{f_{\\psi}\\left(c,\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}\\right)+\\sum_{k=2}^{K}f_{\\psi}\\left(c,\\pi_{2:|c|,k}^{-},\\tau_{2:|c|,k}^{-}\\right)}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{p(c)p(\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}|c)p\\left(\\Big(\\pi_{2:|c|}^{-},\\tau_{2:|c|}^{-}\\Big)_{2:K_{\\mathsf{s a i n c e}}^{+}}\\right)}\\left[\\log\\left(\\frac{K_{\\mathsf{S a i n f o N C E}}^{*}\\cdot f_{\\psi}\\left(c,\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}\\right)}{f_{\\psi}\\left(c,\\pi_{c_{1}}^{+},\\tau_{c_{1}}^{+}\\right)+\\sum_{k=2}^{K_{\\mathsf{s a i n c e}}^{-}}f_{\\psi}\\left(c,\\pi_{2:|c|,k}^{-},\\tau_{2:|c|,k}^{-}\\right)}\\right)\\right]}\\\\ &{\\approx\\log K_{\\mathsf{S a i n f o N C E}}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It should be noted that such a combination increases the size of the negative sample space, i.e., $\\begin{array}{r}{K_{\\mathrm{Sa+InfoNCE}}^{*}=\\left(\\sum_{i=1}^{|c|}|\\pi_{c_{i}}^{-}|+|\\pi_{c_{1}}^{+}|\\right)\\cdot M\\geq K_{\\mathrm{SaNCE}}^{*}}\\end{array}$ . The misaligned bars in Figure 4 (c) illustrate that negative sample spaces may vary across tasks. This variation arises because we define negative samples as trajectories with low returns, making the size of the negative sample space influenced by sampling randomness. With the same number $K$ of samples, $I_{\\mathrm{Sa+InfoNCE}}$ is less precise and looser than $I_{\\mathrm{SaNCE}}$ . Therefore, a trade-off between sample diversity and the precision of the $K$ -sample estimator is required. ", "page_idx": 17}, {"type": "text", "text": "D Environmental setup ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/f2a28e3a419ce38fa407b28658acd56d4bde1246d841ea60fec135d7b702700a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 9: (a) Modified Panda-gym benchmarks, (b) the training tasks, moderate test tasks, and extreme test tasks. The moderate test task setting involves combinatorial interpolation, while the extreme test task setting includes unseen ranges of environmental features and represents an extrapolation. ", "page_idx": 17}, {"type": "text", "text": "D.1 Modified Panda-gym ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We modified the original Pick&Place task in Panda-gym [Gallou\u00e9dec et al., 2021] by setting the $z$ dimension (i.e., the desired height) of the cube\u2019s goal position to $0\\,^{3}$ and maintaining the freedom of the grippers 4, allowing the agent to explore whether it should push or grasp the cube. Skills in this benchmark are defined as: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Pick&Place skill: This skill specifically refers to the agent using the gripper to grasp the cube, lift it off the table, and place it in the goal position. We determine the Pick&Place skill by detecting no contact points between the table and the cube, two contact points between the robot\u2019s end effector and the cube, and the cube\u2019s height being greater than half its width. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Push skill: This skill involves the agent moving the cube on the table to the goal position, either by dragging or sliding it. We confirm the Push skill by detecting that the cube\u2019s height equals half its width.   \n\u2022 Other skills: Any behaviour modes other than Pick&Place and Push are classified as other skills. ", "page_idx": 18}, {"type": "text", "text": "Some elements in the RL framework are defined as follows: ", "page_idx": 18}, {"type": "text", "text": "State space: We use feature vectors that contain the cube\u2019s position (3 dimensions), cube rotation (3 dimensions), cube velocity (3 dimensions), cube angular velocity (3 dimensions), end-effector position (3 dimensions), end-effector velocity (3 dimensions), gripper width (1 dimension), desired goal (3 dimensions), and achieved goal (3 dimensions). Environmental features are not included in states. ", "page_idx": 18}, {"type": "text", "text": "Action space: The action space has 4 dimensions; the first three dimensions represent changes in the end-effector\u2019s position, and the last dimension represents the change in the gripper\u2019s width. ", "page_idx": 18}, {"type": "text", "text": "During training, we randomly select a combination of environmental features from a training set by sampling combinations from the following sets: mass $=1.0$ and friction $\\in\\{0.1,1.0,5.0,10.0\\}$ ; mass $\\bar{\\in}~\\{\\bar{1}.0,5.0,10.0\\}$ and friction $=~0.1$ . At test time, we evaluate each algorithm on all tasks from the moderate test setting, where mass $\\in\\{5.0,10.0\\}$ and friction $\\in\\{1.0,5.0,10.0\\}$ (shown in Figure 9(b)), and on all tasks from the extreme test setting: mass $=~30.0$ and friction $\\in\\ \\{0.1,1.0,5.0,10.0,30.0\\}$ ; mass $\\in\\{1.0,5.0,10.0,30.0\\}$ and friction $=~30.0$ (shown in Figure 9(b)). ", "page_idx": 18}, {"type": "text", "text": "D.2 Modified MuJoCo ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We extended the modified MuJoCo benchmark introduced in DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020]. In our extension, four new environments were added (Walker, Crippled Hopper, Crippled Walker, HumanoidStandup), compared to the original benchmark. Additionally, in our experiments, we used a different task set design (Table 3) than those used in the DOMINO and CaDM papers. For Hopper, Walker, Half-Cheetah, Ant, HumanoidStandup, and SlimHumanoid, we used the MuJoCo physics engine environments and implemented settings from Clavera et al. [2019b] and Seo et al. [2020], scaling the mass of every rigid link by a scale factor $m$ and the damping of every joint by a scale factor $d$ . For Crippled Ant, Crippled Hopper, Crippled Walker, and Crippled Half-Cheetah, we used the implementation available from Seo et al. [2020], scaled the mass of each rigid link by a factor of $m$ , scaled the damping of each joint by a factor of $d$ , and randomly selected joints to be uncontrollable (i.e., masking the corresponding actions with 0). Generalisation performance is measured in two different regimes: moderate and extreme, where the moderate regime draws environmental features from a range closer to the training range compared to the extreme regime. Our settings for training, extreme, and moderate test tasks are provided in Table 3. ", "page_idx": 18}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/68265a496bf5c0e07d4b32729ef63177be65e38c5af1eb41450933be7b18c6c0.jpg", "img_caption": ["Figure 10: Ten environments in modified MuJoCo benchmark. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/602525533d5f94ba97f1ee2b3cafc36c60e696d7b6e2a2ea88a7fa05fc625c08.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/6d761eaecf2f0d8bab90c0c6f71f4b3b32bfd31c572e1272e32b2d2668eae72f.jpg", "table_caption": ["Table 4: Hyperparameters used in the Panda-gym and MuJoCo benchmarks. Most hyperparameter values remain unchanged across tasks, except for the contrastive batch size and the SaNCE loss coefficient. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.3 Implementation details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide the implementation details for SaMI. We present the pseudo-code for using SaNCE during meta-training and meta-testing in Algorithms 1 and 2. Our codebase is built on top of the publicly released implementation of Stable Baselines3 by Raffin et al. [2021] and the implementation of InfoNCE by Oord et al. [2019]. A public, open-source implementation of SaMI is available at https://github.com/uoe-agents/SaMI. ", "page_idx": 20}, {"type": "text", "text": "Base algorithm. We use SAC [Haarnoja et al., 2018] for the downstream evaluation of the learned context embedding. SAC is an off-policy actor-critic method that leverages the maximum entropy framework for soft policy iteration. At each iteration, SAC performs soft policy evaluation and improvement steps. We use the same SAC implementation across all baselines and other methods. During the meta-training phase, we trained agents for 1.6 million timesteps in each environment on the Panda-gym and MuJoCo benchmarks. For meta-testing, we evaluated 100 episodes in each environment, with tasks randomly sampled from the moderate and extreme task sets. ", "page_idx": 20}, {"type": "text", "text": "Encoder architecture. In our method, the context encoder $\\psi$ is modelled as a Long Short-Term Memory (LSTM) network that produces a 128-dimensional hidden state vector, which is subsequently processed through a single-layer feed-forward network to generate a 6-dimensional context embedding. We aim for the agent to complete the three steps of \"explore effectively, infer, adapt\" within an episode. Therefore, we initialise the hidden state and cell state of the LSTM to zero at the start of each episode. Both the actor and critic use the same context encoder to embed trajectories. For contrastive learning, SaNCE utilises a momentum encoder $\\psi^{*}$ to generate positive and negative context embeddings [Laskin et al., 2020, He et al., 2020]. Formally, denoting the parameters of $\\psi$ as $\\theta_{\\psi}$ and those of $\\psi^{*}$ as $\\theta_{\\psi^{*}}$ , we update $\\theta_{\\psi^{*}}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{\\psi^{*}}\\leftarrow m\\cdot\\theta_{\\psi}+(1-m)\\cdot\\theta_{\\psi^{*}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here $m\\in[0,1)$ is a soft-update rate. Only the parameters $\\theta_{\\psi}$ are updated by back-propagation. The momentum update in Equation (13) makes $\\theta_{\\psi^{*}}$ evolve more smoothly by having them slowly track the $\\theta_{\\psi}$ with $m\\ll1$ (e.g., $m=0.05$ in this research). This means that the target values are constrained to change slowly, greatly improving the stability of learning. ", "page_idx": 20}, {"type": "text", "text": "Hyperparameters. A full list of hyperparameters is displayed in Table 4. ", "page_idx": 20}, {"type": "text", "text": "Hardware. For each experiment run we use a single NVIDIA Volta V100 GPU with 32GB memory and a single CPU. ", "page_idx": 20}, {"type": "text", "text": "Algorithm 1 SaNCE Meta-training ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Require: Batch of training tasks $\\{e_{n}\\}_{n=1,...,N}$ from $\\xi_{t r a i n}(e)$ , soft-update rate $m$ ;   \n1: Initialize RL replay buffer $B_{R L}$ , encoder replay buffer $\\beta_{e n c}$ ;   \n2: Initialize parameters $\\psi$ for context encoder, $\\psi^{*}$ for momentum context encoder and $\\phi$ for the   \noff-policy SAC;   \n3: while not done do   \n4: for each task $e_{n}$ do   \n5: for Roll-out time steps do   \n6: for time step $t\\!<$ maximum episode length $T$ do   \n7: Update context embedding $c_{n}\\sim\\psi(c_{n}|\\tau_{c_{n},0:t})$   \n8: Roll-out policy $\\pi_{c_{n}}(a_{t}|s_{t},c_{n})$ and accumulate transition $(s_{t},a_{t},r_{t},s_{t+1})$ ;   \n9: end for   \n10: Add trajectory $\\tau_{c_{n}}=\\{s_{0},a_{0},r_{0},s_{1},r_{1},...,s_{T},a_{T},r_{T}\\}$ to replay buffer $B_{R L}^{n}$ and $B_{e n c}^{n}$ ;   \n11: end for   \n12: end for   \n13: for each training step do   \n14: for each task $e_{n}$ do   \n15: Sample RL batch $\\{\\tau_{c_{n}}\\}\\sim{\\cal B}_{R L}^{n}$ ;   \n16: Sample a positive sample $\\tau_{c_{n}}^{+}$ for generating query with highest return, positive samples   \n$\\{\\tau_{c_{n}}^{-}\\}$ and negative samples $\\bigl\\{\\tau_{c_{n}}^{-}\\bigr\\}$ for encoding positive and negative embeddings;   \n17: Update $\\phi$ with RL loss $\\mathcal{L}_{\\mathrm{RL}}$ ;   \n18: Update $\\psi$ with SaNCE loss $\\mathcal{L}_{\\mathrm{SaNCE}}$ and RL loss $\\mathcal{L}_{\\mathrm{RL}}$ ;   \n19: $\\theta_{\\psi^{*}}\\leftarrow m\\cdot\\theta_{\\psi}+(1-m)\\cdot\\theta_{\\psi^{*}}$ ;   \n20: end for   \n21: end for   \n22: end while ", "page_idx": 21}, {"type": "text", "text": "Algorithm 2 SaNCE Meta-testing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Require: Batch of training tasks $\\{e_{n}\\}_{n=1,...,N}$ from $\\xi_{t e s t}(e)$ ;   \n1: while not done do   \n2: for each task $e_{n}$ do   \n3: for each episode do   \n4: for time step $t\\!<$ maximum episode length $T$ do   \n5: Update context embedding $c_{n}\\sim\\psi(c_{n}|\\tau_{c_{n},0:t})$   \n6: Roll-out policy $\\pi_{c_{n}}(a_{t}|s_{t},c_{n})$ and accumulate transition $(s_{t},a_{t},r_{t},s_{t+1})$ ;   \n7: end for   \n8: end for   \n9: end for   \n10: end while ", "page_idx": 21}, {"type": "text", "text": "E Additional results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Balance contrastive and RL updates: loss coefficient $\\alpha$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "While past work has optimised hyperparameters to balance the contrastive loss coefficient $\\alpha$ relative to the RL objective [Jaderberg et al., 2016, Bachman et al., 2019], we use both the contrastive and RL objectives with equal weight, setting $\\alpha=1.0$ for the MuJoCo benchmark and $\\alpha=0.01$ for the Panda-gym benchmark. Additionally, we analyse the effect of the loss coefficient $\\alpha$ for CCM, SaTESAC, and SaCCM in the MuJoCo (Figure 12) and Panda-gym (Figure 11) benchmarks. ", "page_idx": 22}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/eda2717796ea3185fe17e9ac5a4ad37747394a694bd55db32ae7360affa00374.jpg", "img_caption": ["Figure 11: Loss coefficient $\\alpha$ analysis of Panda-gym benchmark in training and test (moderate and extreme) tasks. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/6b823890fc62cf07e402d6707eb0f7d996bc262690f424e7bad5c04bf0b8177b.jpg", "img_caption": ["Figure 12: Loss coefficient $\\alpha$ analysis of MuJoCo benchmark in training tasks. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2 Result of log- $K$ curse analysis ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/1c9dc0f50b8a1796bcbec529a9a5baaa80fb95c81554dcc6cd905e929025e054.jpg", "img_caption": ["E.2.1 Buffer size "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 13: Comparison of different buffer sizes in the MuJoCo benchmark on training tasks (averaged over 5 seeds). Buffer sizes are 400,000, 100,000, 10,000, and 1,000. ", "page_idx": 23}, {"type": "text", "text": "E.2.2 contrastive batch size ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/998e7be281a9cc1ab901a71886842b7d943e9caba840212202a5f0c880f9c336.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 14: Comparison of different contrastive batch sizes in the MuJoCo benchmark on training tasks (averaged over 5 seeds). Contrastive batch sizes are 512, 128, 16, and 8. ", "page_idx": 23}, {"type": "text", "text": "F Further skill analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1 Panda-gym ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1.1 Visualisation of context embedding ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We visualise the context embedding using UMAP [McInnes et al., 2020] (Figure ??) and t-SNE [Van der Maaten and Hinton, 2008] (Figure 16). When the mass of the cube is high ( $30\\,\\mathrm{kg}$ and 10 $\\mathrm{kg})$ , the agent learned the Push skill (indicated by the yellow bounding box in Figure 1(a)), whereas with lower masses, the agent learned the Pick&Place skill. However, as shown in Figure 16(b), CCM did not display clear skill grouping. This indicates that SaMI extracts high-quality skill-related information from the trajectories and enables agents to autonomously discover a diverse range of skills for handling multiple tasks. ", "page_idx": 24}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/1e3de183b19d122e376c2f6023ae3199f1f0564b9a6544d64d899cec66c50e3f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 15: UMAP visualisation of context embeddings extracted from trajectories collected in the Panda-gym environments. ", "page_idx": 24}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/2b432250da564561e91e65cd0e2944b50c0892fbb28c02bfa25fcd51837602c9.jpg", "img_caption": ["Figure 16: t-SNE visualisation of context embeddings extracted from trajectories collected in the Panda-gym environments. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/0ed2cd09c1e86be6ec3bda20f56064a4d0cd7da58a0b531287ad2ea28cc5fd7e.jpg", "img_caption": ["F.1.2 Heatmap of Panda-gym benchmark ", "Figure 17: Heatmap of success rates and learned skills of SaCCM. For each grid, the $(i,j)^{\\mathrm{th}}$ location shows the probability of the skills executed over 100 evaluations with (mass $=i$ , friction $=j$ ). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "This section presents the heatmap results and further analysis of TESAC, CCM, and SaTESAC on the Panda-gym benchmark. From the heatmap results of SaTESAC and SaCCM (Figure 6), we observe that, with higher cube masses (30 and $10~\\mathrm{kg})$ ), the agent executed the Push skill (indicated by the clustered points within the yellow bounding box in Figure 1(a)). At lower masses, the agent executed the Pick&Place skill. ", "page_idx": 25}, {"type": "text", "text": "In contrast, as shown in Figures 17(e-f), CCM predominantly learned the Pick&Place skill, resulting in a drop in success rates for tasks with $m a s s=30$ , as the agent could not lift the cube off the table using the Pick&Place skill, as depicted in Figure 17(d). The visualisation of the context embedding (Figure 16) did not reveal clear skill grouping across different tasks. ", "page_idx": 25}, {"type": "text", "text": "Finally, TESAC primarily mastered the Push skill. The Push skill is relatively simpler to learn than the Pick&Place skill, as it does not require the agent to manipulate its fingers to pick up cubes. Consequently, TESAC\u2019s success rate notably decreased in environments with higher friction. ", "page_idx": 25}, {"type": "text", "text": "F.2 MuJoCo ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "SaMI enables RL agents to be versatile and embody multiple skills. Additionally, video demos (available at https://github.com/uoe-agents/SaMI) provide a better demonstration of the different skills. ", "page_idx": 26}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/e81e5c3b1bb0cfbfedd18361c65b32879269217f143e90ab53f2e102c87808c4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 18: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in the Crippled Half-Cheetah environment. \"cripple_set\" refers to the index of the crippled joint. The figure shows the context embeddings of tasks in three moderate test settings. Combined with the video demos2for skill analysis, the Crippled Half-Cheetah robot executed three distinct forward running skills. ", "page_idx": 26}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/89ccd0640ac8a579177f4d4fbf0c9f2a2f0866a86ea5ce9074dcc7a90186a909.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 19: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in the Crippled Ant environment. \"cripple_set\" refers to the index of the crippled joint. When 3 or 4 legs are available, the Ant robot (trained with SaCCM) rolls to adapt to varying mass and damping. However, with only 2 adjacent legs during zero-shot generalisation, it switches to walking. If 2 opposite legs are available, the Ant can still roll but eventually tips over. Please refer to the video demos2. ", "page_idx": 26}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/2e69d27b820676cb9a109bb6d6cb5e0f9f4ed4d99a013d2c5a38281469e5f62b.jpg", "img_caption": ["Figure 20: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in the Half-Cheetah environment. During zero-shot generalisation, SaCCM demonstrates different skills for running forwards at various speeds, as well as skills for doing filps and faceplanting. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/6a01b72e92fd729a400c34c730488bed9f5eba3a5b370f30a82154955996df86.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 21: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in Ant environment. The Ant robot learned a single skill, rolling, to adapt to different mass and damping values. ", "page_idx": 27}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/bb96abcb3876d7b86550ef164322fd1dd9e2ed98f7478a7f5d2846db130a3ae9.jpg", "img_caption": ["Figure 22: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in SlimHumanoid environment. The Humanoid Robot crawls on the ground using one elbow. When the damping is relatively high (damping $_{\\leftmoon1.8}$ ), the Humanoid Robot can crawl forward stably, but when the damping is low (damping $=\\!0.4\\!\\!$ ), it tends to roll. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/49383fbc5f3cabe1bdc160fd3428a742b0f0a1c7597abe9ac62fc75d67e2acef.jpg", "img_caption": ["Figure 23: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in HumanoidStandup environment. SaCCM and SaTESAC learned a sitting posture that makes it easier to stand up, allowing it to generalise well when mass and damping change. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/d165d245815314ecd7cd52bf47ed05d24fb7d0248de91923742aa5b7c7629dce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 24: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in Hopper environment. Combined with the video demos 2for skill analysis, the plots for SaCCM and SaTESAC show two skills: 1) when the mass is low, the Hopper hops in an upright posture; 2) when the mass is higher, the Hopper hops forward on the floor. ", "page_idx": 28}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/838b5d67f88c154db8ce09baf5dd00c7266bf6881582f45d1e42d7f2283a2837.jpg", "img_caption": ["Figure 25: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in Crippled Hopper environment. \"cripple_set\" refers to the index of the crippled joint. SaCCM and SaTESAC learned to take a big hop forward at the beginning (i.e., effective exploration) and then switch to different skills based on environmental feedback. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/4cedb5fabcd43c020d128a4f80a3e8681b6f09dd255011981a352ebc496ea1c5.jpg", "img_caption": ["Figure 26: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in Walker environment. The Walker learned a single skill, hopping forward on the floor through both right and left legs. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "GtbwJ6mruI/tmp/e25b3a6d63eeb5b24f6c724a7324df62fc71a60ec92c456baa7b7b022ae50c6e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 27: t-SNE and UMAP visualisations of context embeddings extracted from trajectories collected in Crippled Walker environment. \"cripple_set\" refers to the index of the crippled joint. The Crippled Walker (trained with SaTESAC and SaCCM) learned to hop forward using the right leg (cripple_set:0 and cripple_set:2) in the training and moderate test tasks and switched to hopping forward using the left leg (cripple_set:4) in the extreme test tasks. ", "page_idx": 29}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/31e98e9481f2eaa0d4da997b894a137f34d0a5abf0be8d31476cc06fdebbefbe.jpg", "table_caption": ["Table 5: Environmental features used for MuJoCo benchmark from DOMINO and CaDM. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/dd9407b57e20bdd3c10e36b8624a8987fdc51a57b3eaf1788a36159418bfb0f2.jpg", "table_caption": ["Table 6: Comparison of average return $\\pm$ standard deviation with baselines in MuJoCo benchmark (over 5 seeds). The bold text signifies the highest average return. The numerical results for $\\scriptstyle\\mathrm{PPO}+\\scriptstyle\\mathrm{DOMINO}$ are copied from Mu et al. [2022]; the numerical results for $\\mathrm{PPO+CaDM}$ , Vanilla $+\\mathrm{CaDM}$ , and PE $-\\mathrm{TS}+\\mathrm{CaDM}$ are copied from Lee et al. [2020]. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "G A comparison with DOMINO and CaDM ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we give a brief comparison between our methods and methods from DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020] in the MuJoCo benchmark because we are using the exact same environmental setting (shown in Table 5). ", "page_idx": 31}, {"type": "text", "text": "DOMINO [Mu et al., 2022] is based on the InfoNCE $K$ -sample estimator. Their implementation, PPO+DOMINO, is a model-free RL algorithm with a pre-trained context encoder. This encoder reduces the demand for large contrastive batch sizes during training by decoupling representation learning for each modality, simplifying tasks while leveraging shared information. However, a pre-trained encoder necessitates a large sample volume, with DOMINO training PPO agents for 5 million timesteps on the MuJoCo benchmark. In contrast, SaTESAC and SaCCM, trained for 1.6 million timesteps without pre-trained encoders, achieve considerably higher average returns across four environments (Table 6). Therefore, it is crucial to focus on extracting MI in contrastive learning that directly optimises downstream tasks, integrating rather than segregating representation learning from task performance. ", "page_idx": 31}, {"type": "text", "text": "CaDM [Lee et al., 2020] proposes a context-aware dynamics model adaptable to changes in dynamics. Specifically, they utilise contrastive learning to learn context embeddings, and then predict the next state conditioned on them. We copy the numerical results of $\\mathrm{PPO+CaDM}$ , Vanilla $^{+}$ CaDM, and PE-TS+CaDM from CaDM [Lee et al., 2020] as their environmental setting is identical to ours, where $\\mathrm{PPO+CaDM}$ is a model-free RL algorithm, while Vanilla $^{+}$ CaDM and PE- $\\mathrm{TS+CaDM}$ are modelbased. The model-free RL approach, $\\mathrm{PPO+CaDM}$ , is trained for 5 million timesteps on the MuJoCo benchmark. As shown in Table 6, SaTESAC and SaCCM significantly outperform $\\mathrm{PPO+CaDM}$ . The model-based RL algorithms, Vanilla $^{+}$ CaDM and PE-TS+CaDM, require 2 million timesteps for learning in model-based setups, compared to our fewer samples (i.e., million timesteps). In the Ant environment, Vanilla $^{+}$ CaDM and P $\\Xi_{\\mathrm{-TS+CaDM}}$ achieve higher returns than SaTESAC and SaCCM; similarly, in the Half-cheetah environment, PE-TS $+\\cdot$ CaDM outperforms them. Results in the SlimHumanoid and Crippled Half-cheetah environments show that skill-aware context embeddings are notably effective. An insight here is that our method outperforms the model-free CaCM approach, but not the model-based one. This is consistent with what is empirically found in CaDM [Lee et al., 2020]: prediction models are more effective when the transition function changes across tasks. Therefore, we consider that a model-based approach to SaMI could be an interesting extension for future work. ", "page_idx": 31}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/d12aafbeffade483191c0132ac52b86ee639a9bfdace346cc0095d6e58677078.jpg", "table_caption": ["Table 7: The p-value of the statistical hypothesis tests (paried t-tests) for comparing the effectiveness of SaMI in MuJoCo benchmark (over 5 seeds). $^*$ next to the number means that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI at a significance level of 0.05. The \u201cSaTESAC-TESAC\u201d row indicates the p-value for the return improvement brought by SaMI to TESAC; the \u201cSaCCM-CCM\u201d row indicates the $\\mathbf{p}$ -value for the return improvement brought by SaMI to CCM. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 8: The p-value of the statistical hypothesis tests (paried t-tests) for comparing the effectiveness of SaMI in Panda-gym benchmark (over 5 seeds). $^*$ next to the number means that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI at a significance level of 0.05. The \u201cSaTESAC-TESAC\u201d row indicates the p-value for the return improvement brought by SaMI to TESAC; the \u201cSaCCM-CCM\u201d row indicates the p-value for the return improvement brought by SaMI to CCM. ", "page_idx": 32}, {"type": "table", "img_path": "GtbwJ6mruI/tmp/e7b207969868cb69a57a16f7e5fb9f1d5b8883215cdf4d13a46ee36dd4ae6efa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "H Statistical hypothesis tests (paried t-tests) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We used a t-test [Rice and Rice, 2007] to conduct a statistical hypothesis test to determine whether SaMI brought a statistically significant improvement. we reported the p-value of the t-test in MuJoCo (Table 7) and Panda-gym (Table 8) benchmarks. $^*$ next to the number is used to indicate that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI at a significance level of 0.05. From Table 7 and 8, SaMI brings significant improvement on the extreme test set in which the RL agent needs to execute diverse skills. The statistically significant test aligns with our results in the skill analysis (i.e., video demos, t-SNE and UMAP visualisation). In complex environments that require high skill diversity from the RL agent, the statistically significant improvement and higher returns/success rates brought by SaMI are evident. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our abstract and introduction clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalise to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed the limitations and future work of this work in Section 6. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Proofs are provided in Appendix A and B. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The code will be open-sourced and all the implementation details for both the environment setup and algorithms are in the Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We will open-source our modified benchmarks (i.e., data) and code after the double-blind review phase ends. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided experimental details in Appendix D including hyperparameters, how they were chosen, type of optimizer, etc. ", "page_idx": 35}, {"type": "text", "text": "Guidelines:but ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We report the mean and standard deviation of our environmental results over five seeds. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have provided sufficient information on the computer resources (type of compute workers, memory, time of execution) in Appendix D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have fully considered the potential societal impacts of our research. Our work is conducted in simulated environments, and there is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All assets used in our study are open-source, and we have acknowledged the authors\u2019 copyrights in the References. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide detailed descriptions of the assets we will release in Appendix D and on our open-source homepage. During the double-blind review phase, we have anonymised the URLs. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]