{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper introduces the foundational concept of in-context learning, a paradigm heavily leveraged and analyzed within the UniBias paper."}, {"fullname_first_author": "Nelson Elhage", "paper_title": "A mathematical framework for transformer circuits", "publication_date": "2021-00-00", "reason": "This paper provides a theoretical foundation for interpreting the internal mechanisms of transformer models which is crucial for UniBias's analysis of attention and FFN contributions."}, {"fullname_first_author": "Zihao Zhao", "paper_title": "Calibrate before use: Improving few-shot performance of language models", "publication_date": "2021-07-18", "reason": "This paper directly addresses LLM bias and proposes calibration methods, forming a key comparative baseline for UniBias's novel internal bias mitigation approach."}, {"fullname_first_author": "Yu Fei", "paper_title": "Mitigating label biases for in-context learning", "publication_date": "2023-07-00", "reason": "This paper investigates vanilla label bias in LLMs, a specific bias type also analyzed and addressed by the UniBias approach, making it a relevant comparative work."}, {"fullname_first_author": "Mor Geva", "paper_title": "Transformer feed-forward layers are key-value memories", "publication_date": "2021-11-00", "reason": "This paper offers insights into the functionality of FFN layers within transformers, which is directly relevant to UniBias's focus on FFN vector manipulation for bias reduction."}]}