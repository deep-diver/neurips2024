[{"figure_path": "bE7GWLQzkM/figures/figures_2_1.jpg", "caption": "Figure 1: (A) Schematic representation of the continual linear regression model with low-dimensional latent variables. (B-D) Examples of continual learning with two tasks in the MNIST setting. The tasks have low feature similarity in panel C (input pixels are partially permuted) and low readout similarity in panel D (output labels are partially permuted). Panels C and D correspond to the green and orange points in panel B, respectively.", "description": "Figure 1(A) shows a schematic of a continual linear regression model.  The model uses a low-dimensional latent variable (s) to generate both inputs (x) and target outputs (y*). The student model learns a linear mapping (W) from the input to the output. Figure 1(B) is a phase diagram showing how knowledge transfer and retention depend on the feature (Pa) and readout (Pb) similarity between two tasks.  Panels C and D illustrate continual learning scenarios with low feature similarity (permuted input pixels) and low readout similarity (permuted output labels) in the MNIST dataset, respectively. These examples correspond to the green and orange points on the phase diagram in panel B.", "section": "3 Teacher-student model with low-dimensional latent variables"}, {"figure_path": "bE7GWLQzkM/figures/figures_3_1.jpg", "caption": "Figure 2: Transfer and retention performance of the vanilla model. (A) Illustration of \u0394\u03b5TF and \u0394\u03b5RT. Red and blue lines represent the error on task 1 and task 2, respectively. Here, the model was trained on task 1 for 100 iterations and then trained on task 2 for another 100 iterations. (B, C) Transfer performance under various task similarity. Points in panel B are numerical results (the means and the standard deviations over ten random seeds), while solid lines are analytical results (Eq. 4). (D-G) Retention performance under various task similarity. Panel G magnifies the 0.9 < \u03c1b \u2264 1.0 region of panel E, and the white dashed line in panel G represents local minima/maxima.", "description": "This figure displays results from a vanilla model (no gating or regularization) of continual learning performance.  Panel A shows the calculation of transfer and retention error, while panels B and C show how transfer performance varies with feature similarity (\u03c1a) and readout similarity (\u03c1b).  Panels D-G show how retention performance varies with \u03c1a and \u03c1b, with panel G magnifying a specific region of panel E to highlight local minima/maxima.", "section": "4 Impact of task similarity on knowledge transfer and retention"}, {"figure_path": "bE7GWLQzkM/figures/figures_5_1.jpg", "caption": "Figure 3: Random task-dependent activity gating model. (A) Knowledge transfer performance under Pa = 1.0. The gating level a is defined as the fraction of active input neurons (i.e., a = Pr[gi = 1]). (B) The transfer performance under the optimal gating level a* = min{0,1}. (C) Retention performance under Pa = 1.0. (D) Average transfer and retention performance over uniform prior on 0 \u2264 Pa, Pb \u2264 1. Horizontal dashed lines are the performance of the vanilla model, while solid lines are the performance of the random gating model. Points are numerical estimations.", "description": "This figure shows the results of a random task-dependent activity gating model. It shows how the transfer and retention performance change depending on different parameters such as feature similarity, readout similarity, and gating level. The results are shown in four different subplots to illustrate how each parameter affects the performance. The results are obtained through numerical estimations, and horizontal dashed lines are added to compare the results against the baseline performance.", "section": "5 Task-dependent gating"}, {"figure_path": "bE7GWLQzkM/figures/figures_5_2.jpg", "caption": "Figure 3: Random task-dependent activity gating model. (A) Knowledge transfer performance under Pa = 1.0. The gating level a is defined as the fraction of active input neurons (i.e., a = Pr[gi = 1]). (B) The transfer performance under the optimal gating level a* = min{0,1}. (C) Retention performance under Pa = 1.0. (D) Average transfer and retention performance over uniform prior on 0 \u2264 Pa, Pb \u2264 1. Horizontal dashed lines are the performance of the vanilla model, while solid lines are the performance of the random gating model. Points are numerical estimations.", "description": "Figure 3 presents the results of simulations using a random task-dependent activity gating model. The results are shown as a function of the gating level (\u03b1), which is defined as the proportion of active input neurons. Panel (A) shows the transfer performance when input feature similarity (\u03c1a) is 1.0. Panel (B) shows the transfer performance under the optimal gating level, with the optimal level determined by maximizing transfer performance. Panel (C) shows the retention performance when \u03c1a = 1.0, and Panel (D) displays the average transfer and retention performances across a uniform distribution of task similarities (0 \u2264 \u03c1a, \u03c1b \u2264 1). The performance of the vanilla model without gating is shown as a horizontal dashed line in each panel for comparison.", "section": "5 Task-dependent gating"}, {"figure_path": "bE7GWLQzkM/figures/figures_6_1.jpg", "caption": "Figure 5: Performance of weight regularization in Euclidean metric. (A,B) Transfer (A) and retention (B) performance. The amplitude of the weight regularization scales with \u03b3. (C) Regularizer coefficient \u03b3 that optimizes the retention performance. (D) Average performance over uniform task similarity distribution in 0 \u2264 \u03c1a, \u03c1b \u2264 1. Horizontal dashed lines are the performance of the vanilla model.", "description": "This figure shows the results of using weight regularization in a Euclidean metric for continual learning.  Panels A and B illustrate the transfer and retention performance, respectively, as a function of the regularizer coefficient (\u03b3) and task similarity (\u03c1a and \u03c1b). Panel C displays the optimal regularizer coefficient (\u03b3) that maximizes retention performance for various levels of task similarity. Finally, Panel D presents the average performance (both transfer and retention) across a range of task similarities.", "section": "6 Weight regularization"}, {"figure_path": "bE7GWLQzkM/figures/figures_7_1.jpg", "caption": "Figure 6: Weight regularization in the Fisher information metric. (A,B) The retention performance under various task similarities and regularizer coefficients. (C,D) Average transfer and retention performance under the regularization with the exact Fisher information metric (C) and its diagonal approximation (D).", "description": "This figure shows the results of applying weight regularization using the Fisher information metric and its diagonal approximation.  Panels A and B illustrate how retention performance varies with the regularizer coefficient (gamma) and different levels of task similarity (feature and readout). Panels C and D present the average transfer and retention performance across various task similarity conditions for both the exact Fisher information metric and its diagonal approximation. The diagonal approximation is shown to be less robust against task similarity.", "section": "6 Weight regularization"}, {"figure_path": "bE7GWLQzkM/figures/figures_8_1.jpg", "caption": "Figure 7: Permuted MNIST with latent variables. (A,B) Transfer and retention performance of the vanilla model. (C,D) Performance of random (dashed lines) and adaptive (solid lines) activity gating models. (E-H) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics (red: layer-wise approximation; orange: synapse-wise/diagonal approximation). Here, lines are linear interpolations and error bars are standard errors over random seeds, not standard deviations.", "description": "This figure shows the results of numerical experiments using a permuted MNIST dataset with a latent structure.  Panels A and B display the transfer and retention performance of a vanilla model, demonstrating the asymmetric and non-monotonic relationship between task similarity and performance. Panels C and D illustrate the impact of random and adaptive activity gating on transfer and retention, highlighting how adaptive gating can mitigate the tradeoff between these two objectives. Finally, panels E through H compare the performance of weight regularization using different metrics (Euclidean, Fisher information, diagonal approximation of Fisher information, layer-wise approximation of Fisher information), showcasing that the layer-wise approximation of the Fisher information metric achieves the best retention performance.", "section": "Numerical experiments"}, {"figure_path": "bE7GWLQzkM/figures/figures_17_1.jpg", "caption": "Figure 8: The gating level dependence of the transfer and retention performance (A) Phase diagram of the gating level dependence. (B-D) Transfer and retention performance as a function of the gating level at a representative point of each phase.", "description": "This figure examines how the gating level affects the trade-off between transfer and retention performance in continual learning.  Panel A shows a phase diagram illustrating the different regions of gating level behavior as a function of feature and readout similarity. Panels B-D then delve deeper into specific regions of this phase diagram, showing the transfer and retention performance curves for different gating levels within those regions. The results demonstrate the complex interplay between gating, feature similarity, and readout similarity in determining continual learning success.", "section": "5 Task-dependent gating"}, {"figure_path": "bE7GWLQzkM/figures/figures_21_1.jpg", "caption": "Figure 9: Weight regularization in Euclidean metric. (A,B) Optimal regularizer coefficient \u03b3 that maximizes the transfer performance (A), and the maximum performance at the optimal \u03b3 (B) under various (\u03c1\u03b1, \u03c1b) pairs. (C,D) Optimal regularizer coefficient for retention (C), and the resultant performance (D). Panel C is the same with Fig. 5C (replicated for completeness).", "description": "Figure 9 shows the results of weight regularization in Euclidean metric.  Panels A and B illustrate how the optimal regularizer coefficient \u03b3 that maximizes transfer performance and the resulting performance vary depending on feature similarity (\u03c1a) and readout similarity (\u03c1b). Panels C and D show the optimal regularizer coefficient \u03b3 for retention performance and the resulting performance under different combinations of feature and readout similarity.  Note that panel C is the same as Figure 5C.", "section": "6 Weight regularization"}, {"figure_path": "bE7GWLQzkM/figures/figures_26_1.jpg", "caption": "Figure 7: Permuted MNIST with latent variables. (A,B) Transfer and retention performance of the vanilla model. (C,D) Performance of random (dashed lines) and adaptive (solid lines) activity gating models. (E-H) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics (red: layer-wise approximation; orange: synapse-wise/diagonal approximation). Here, lines are linear interpolations and error bars are standard errors over random seeds, not standard deviations.", "description": "This figure shows the results of experiments on a permuted MNIST dataset with latent variables.  It demonstrates the transfer and retention performance for several continual learning algorithms, including a vanilla model, random and adaptive activity gating, and weight regularization using different metrics (Euclidean, Fisher information matrix, layer-wise and diagonal approximations).  The plots illustrate how performance varies based on task similarity (feature and readout), highlighting the effects of different continual learning strategies. Error bars represent standard error.", "section": "7 Numerical experiments"}, {"figure_path": "bE7GWLQzkM/figures/figures_27_1.jpg", "caption": "Figure 7: Permuted MNIST with latent variables. (A,B) Transfer and retention performance of the vanilla model. (C,D) Performance of random (dashed lines) and adaptive (solid lines) activity gating models. (E-H) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics (red: layer-wise approximation; orange: synapse-wise/diagonal approximation). Here, lines are linear interpolations and error bars are standard errors over random seeds, not standard deviations.", "description": "This figure shows the results of experiments conducted on a permuted MNIST dataset with latent variables.  The experiments test the impact of task similarity and different continual learning algorithms on transfer and retention performance.  Panel A and B illustrate the baseline transfer and retention for a vanilla model. Panels C and D show results using random and adaptive activity gating. Panels E-H present results for weight regularization using three different methods: Euclidean metric, Fisher information metric (layer-wise approximation), and Fisher information metric (diagonal approximation). The graphs display transfer and retention performance as a function of task similarity (feature and readout similarity) and regularization strength.", "section": "Numerical experiments"}, {"figure_path": "bE7GWLQzkM/figures/figures_29_1.jpg", "caption": "Figure 12: Permuted MNIST with pixel and label permutations. (A,B) Transfer and classification performance measured by the classification accuracy (A) and the cross-entropy loss (B). (C) Transfer performance of random (dashed lines) and adaptive (solid lines) activity gating models. (D-F) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics.", "description": "This figure shows the results of experiments on the permuted MNIST dataset where both input pixels and output labels were permuted to control feature and readout similarity.  Panels A and B show the transfer and classification performance using classification accuracy and cross-entropy loss, respectively, as performance metrics.  Panel C compares transfer performance using random versus adaptive activity gating. Panels D-F illustrate the effect of weight regularization using different metrics (Euclidean, Fisher information matrix (FIM) with diagonal and layer-wise approximations) on transfer and retention performance under varying levels of feature and readout similarity.", "section": "7 Numerical experiments"}]