[{"figure_path": "wT6GHk5ShC/tables/tables_19_1.jpg", "caption": "Table 2: The results of Algorithm 1 on SST-2", "description": "This table presents the results of applying Algorithm 1, a method for enhancing ICL inference through SVD-based weight pruning, to the SST-2 dataset.  It shows the optimal clipping rate (\u03be*) determined for different modules (MLP and ATTN) of two different LLMs (GPT-J-6B and LLAMA2-7B) and the corresponding test accuracy/F1 scores along with the improvement achieved compared to the baseline ICL performance. The optimal clipping rate signifies the degree of weight pruning that yielded the best performance on the validation set before testing on the held-out test set.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_21_1.jpg", "caption": "Table 1: The prompts of the datasets used in our experiments. Here regard the types of tasks: classification, multiple-choice as (cls.), (mch.). <> represents input from the dataset. For (mch.) tasks, we put in different candidates in the prompt and calculate the average log-likelihood of each candidate, and select the candidate with the highest score.", "description": "This table shows the prompts used for each dataset in the experiments.  It specifies the type of task (classification or multiple-choice) and provides a template showing how the input data from each dataset was formatted into a prompt for the model. The table helps understand how the input data was prepared for the in-context learning experiments.", "section": "C.1 Prompts"}, {"figure_path": "wT6GHk5ShC/tables/tables_21_2.jpg", "caption": "Table 2: The results of Algorithm 1 on SST-2", "description": "This table presents the results of applying Algorithm 1, a novel method for enhancing in-context learning (ICL) performance via SVD-based weight pruning, to the SST-2 dataset.  It shows the model name (GPT-J-6B and LLAMA2-7B), the number of layers considered, the module type (MLP or ATTN), the optimal clipping rate (\u03be*) found by the algorithm, and the resulting test accuracy and improvement compared to the baseline ICL performance.  The results demonstrate the effectiveness of Algorithm 1 in improving ICL performance on this specific dataset.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_23_1.jpg", "caption": "Table 2: The results of Algorithm 1 on SST-2", "description": "This table presents the results of applying Algorithm 1, a novel method for enhancing ICL inference, to the SST-2 dataset.  It shows the model name, layer number, module type (MLP or ATTN), the optimal clipping rate (\u03be*) found by Algorithm 1, and the improvement in test accuracy achieved compared to the baseline ICL performance.  The upward or downward arrow indicates whether the accuracy increased or decreased after applying Algorithm 1.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_1.jpg", "caption": "Table 3: The results of Algorithm 1 on AGNEWS", "description": "This table presents the results of applying Algorithm 1, a novel method for enhancing In-context Learning (ICL) performance through SVD-based weight pruning, to the AGNEWS dataset.  It shows the model name (GPT-J-6B and LLAMA2-7B), layer number, module type (MLP or ATTN), the optimal clipping rate (\u03be*) determined by Algorithm 1, and the resulting test accuracy improvement compared to the baseline ICL performance.  Positive values indicate improvement, while negative values indicate a decrease in performance.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_2.jpg", "caption": "Table 4: The results of Algorithm 1 on EmoC", "description": "This table presents the results of applying Algorithm 1, which is a method for enhancing ICL inference by using SVD-based weight pruning, on the EmoC dataset.  The table shows the model name, layer number, module type (MLP or ATTN), optimal clipping rate (\u03be*), and the improvement in test accuracy. The results indicate the effectiveness of Algorithm 1 for enhancing performance on this dataset, although some layers and modules show no improvement or even a slight decrease.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_3.jpg", "caption": "Table 5: The results of Algorithm 1 on MRPC", "description": "This table presents the results of applying Algorithm 1, a method for enhancing in-context learning (ICL) performance using SVD-based weight pruning, to the MRPC dataset.  The table shows the model name (GPT-J-6B and LLAMA2-7B), the layer number (26, 27, and 30), the module name (MLP and ATTN), the optimal clipping rate (\u03be*) determined by the algorithm, and the resulting change in test accuracy.  Positive values in the 'Test Acc Improve' column indicate an improvement in accuracy after applying the algorithm, while negative values show a decrease.", "section": "C.5 The Detailed Results of Algorithm 1"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_4.jpg", "caption": "Table 7: The results of Algorithm 1 on CB", "description": "This table presents the results of applying Algorithm 1, a method for enhancing ICL inference using SVD-based weight pruning, to the CommitmentBank (CB) dataset.  It shows the test accuracy improvements achieved for different models (GPT-J-6B and LLAMA2-7B) and module types (MLP and ATTN) at various layers.  The \"Optimal \u03be*\" column indicates the clipping rate that yielded the best performance for each configuration.  Positive values in the \"Test Acc Improve\" column denote performance improvement compared to the baseline ICL model. ", "section": "3.4 Applications of Our Theoretical Understanding"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_5.jpg", "caption": "Table 8: The results of Algorithm 1 on COPA", "description": "This table presents the results of applying Algorithm 1, a method for enhancing in-context learning (ICL) performance via SVD-based weight pruning, to the COPA dataset.  The table shows the model name (GPT-J-6B and LLAMA2-7B), the layer number (26, 27, 30), the module name (MLP and ATTN), the optimal clipping rate (\u03be*) determined by Algorithm 1, and the resulting improvement in test accuracy/F1 score.  The improvement is shown as the difference between the original performance and the performance after applying the algorithm.  A positive value indicates an improvement, while a negative value indicates a decrease in performance.", "section": "C Extension to Experiments"}, {"figure_path": "wT6GHk5ShC/tables/tables_24_6.jpg", "caption": "Table 8: The results of Algorithm 1 on COPA", "description": "This table presents the results of applying Algorithm 1, a method for enhancing in-context learning (ICL) performance via SVD-based weight pruning, to the COPA dataset.  For both GPT-J-6B and LLAMA2-7B models, the algorithm was applied to both MLP and ATTN modules at specific layers. The table shows the optimal clipping rate (\u03be*) found by the algorithm for each model and module, and the resulting test F1 score improvement (or reduction) compared to the baseline ICL performance.", "section": "C.5 The Detailed Results of Algorithm 1"}]