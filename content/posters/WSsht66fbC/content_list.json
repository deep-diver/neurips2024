[{"type": "text", "text": "Safety through feedback in Constrained RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shashank Reddy $\\mathbf{Chirra^{*1}}$ , Pradeep Varakantham1, Praveen Paruchuri2 1Singapore Management University, 2IIIT Hyderabad {shashankc,pradeepv}@smu.edu.sg, praveen.p@iiit.ac.in ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In safety-critical RL settings, the inclusion of an additional cost function is often favoured over the arduous task of modifying the reward function to ensure the agent\u2019s safe behaviour. However, designing or evaluating such a cost function can be prohibitively expensive. For instance, in the domain of self-driving, designing a cost function that encompasses all unsafe behaviours (e.g., aggressive lane changes, risky overtakes) is inherently complex, it must also consider all the actors present in the scene making it expensive to evaluate. In such scenarios, the cost function can be learned from feedback collected offilne in between training rounds. This feedback can be system generated or elicited from a human observing the training process. Previous approaches have not been able to scale to complex environments and are constrained to receiving feedback at the state level which can be expensive to collect. To this end, we introduce an approach that scales to more complex domains and extends beyond state-level feedback, thus, reducing the burden on the evaluator. Inferring the cost function in such settings poses challenges, particularly in assigning credit to individual states based on trajectorylevel feedback. To address this, we propose a surrogate objective that transforms the problem into a state-level supervised classification task with noisy labels, which can be solved efficiently. Additionally, it is often infeasible to collect feedback for every trajectory generated by the agent, hence, two fundamental questions arise: (1) Which trajectories should be presented to the human? and (2) How many trajectories are necessary for effective learning? To address these questions, we introduce a novelty-based sampling mechanism that selectively involves the evaluator only when the the agent encounters a novel trajectory, and discontinues querying once the trajectories are no longer novel. We showcase the efficiency of our method through experimentation on several benchmark Safety Gymnasium environments and realistic self-driving scenarios. Our method demonstrates nearoptimal performance, comparable to when the cost function is known, by relying solely on trajectory-level feedback across multiple domains. This highlights both the effectiveness and scalability of our approach. The code to replicate these results can be found at https://github.com/shshnkreddy/RLSF ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) is known to suffer from the problem of reward design, especially in safety-related settings [4]. In such a case, constrained RL settings have emerged as a promising alternative to generate safe policies[3, 16, 34]. This framework introduces an additional cost function to split the task related information (rewards) from the safety related information (costs). In this paper, we address a scenario where the reward function is well-defined, while the cost function remains unknown a priori and requires inference from feedback. This arises in cases where the cost function is expensive to design or evaluate. For instance, consider the development of an autonomous driving system, where the reward function may be defined as the time taken to reach a destination which is easy to define. However, formulating the cost function presents significant challenges. Designing a comprehensive cost function that effectively penalizes all potential unsafe behaviors is non-trivial, and ensuring safety often involves subjective judgments, which can vary based on individual preferences. Even if one succeeds in devising such a function, it must account for all environmental factors, such as neighboring vehicles and pedestrians. The evaluation of such a cost function in high-fidelity simulators can be prohibitively expensive. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Feedback can be collected from either a human observer, who monitors the agent\u2019s training process and periodically provides feedback on presented trajectories, or from a system that computes the cost incurred on selected trajectories. Throughout this paper, we use the term evaluator to refer to the entity providing feedback, whether human or system-generated. ", "page_idx": 1}, {"type": "text", "text": "Cost inference in Constrained RL settings has gained recent attention. One key thread of research has focused on learning cost from from constraint abiding expert demonstrations [28, 32]. However, these expert demonstrations are not easily available in all settings. For example, consider robotic manipulation tasks where the human and the robot have different morphologies. This paper focuses on the second line of research in this area, where we generate trajectories and gather feedback from an evaluator to infer the underlying cost function. Prior works in this thread make limiting assumptions on the nature of the cost function such as smoothness or linearity [28, 32, 11, 5] and are limited to obtaining feedback at the state level [8] which is expensive to collect from human evaluators. We do not make such assumptions and can take feedback provided at over longer horizons, in some cases the entire trajectory. ", "page_idx": 1}, {"type": "text", "text": "Frameworks for learning from feedback must exhibit the following properties as emphasized in [8, 18, 12]: 1) Feedback must be collected offline in between rounds, since the agent may need to act in real time. 2) The amount of feedback collected must be minimized. 3) Binary feedback is more suitable as compared to numeric feedback as it is more intuitive to provide for humans. It has also been shown that humans provide less consistent feedback if it is numeric [12]. 4) Each state is assigned a binary cost value, indicating that it is inherently safe or unsafe, which is more intuitive for humans when assessing the safety of policies [31] 1. ", "page_idx": 1}, {"type": "text", "text": "To this end we propose the Reinforcement Learning from Safety Feedback (RLSF) algorithm, which embodies all the aforementioned properties. The key contributions of this algorithm include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extends prior work to collect feedback over longer horizons. This is done by presenting the evaluator with the entire trajectory, breaking the trajectory into segments and eliciting feedback at the segment level. Inferring the costs directly by maximizing likelihood presents new challenges due to the problem of credit assignment over longer horizons. To tackle this, we present a surrogate loss that converts the problem from trajectory level cost inference to a supervised binary classification problem with noisy labels.   \n\u2022 Introduces a novelty based sampling mechanism that reduces the number of queries by sampling novel trajectories for feedback.   \n\u2022 Learns safe policies across diverse benchmark safety environments. We also show that the learnt cost function can be transferred to train an agent with different dynamics/morphology from scratch without collecting additional feedback. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Markov Decision Process A Markov Decision Process (MDP) $\\mathcal{M}$ is defined by the tuple $(S,{\\mathcal{A}},{\\mathcal{P}},r,\\gamma,\\mu)$ , where $\\boldsymbol{S}$ denotes the set of states, $\\boldsymbol{\\mathcal{A}}$ is the set of actions, $\\mathcal{P}(s^{\\prime}|s,\\stackrel{\\cdot}{a})\\;\\in\\;[0,1]$ is the transition probability, $r(s,a)\\in\\mathbb{R}$ is the reward function, $\\gamma\\in[0,1]$ is the discount factor and $\\mu(s)\\in\\Delta(S)$ is the initial state distribution. A policy $\\pi(.|s)\\in\\Delta(A)$ is a distribution over the set of valid actions for state $s$ . We denote the set of all stationary policies as $\\Pi$ . A trajectory $\\tau=\\{(s_{t},a_{t})\\}$ denotes the state-action pairs encountered by executing $\\pi$ in $\\mathcal{M}$ . We use the short hand $\\tau_{i:j}$ to denote a trajectory segment, i.e, the subsequence of $(s_{t},a_{t})$ pairs encountered from timestep $i$ to $j$ . The expected value of a function $f$ under $\\pi$ as $\\begin{array}{r}{\\mathcal{I}^{f}(\\pi)\\triangleq E_{\\tau\\sim\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}f(s_{t},a_{t})]}\\end{array}$ . We also employ the shorthand $f(\\tau)$ to represent the discounted sum of $f$ along the trajectory $\\tau$ . The occupancy measure of a policy is define as $\\begin{array}{r}{\\rho(s,a)=E_{\\tau\\sim\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{I}[(s_{t},a_{t})=(s,a)]]}\\end{array}$ , where $\\mathbb{I}[.]$ denotes the indicator function. $\\rho$ describes the frequency with which a state-action pair is visited by $\\pi$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Constrained Markov Decision Process A Constrained MDP [4] introduces a function $c(s,a)\\in\\mathbb{R}$ and a cost threshold $c_{m a x}\\in\\mathbb{R}$ that defines the maximum cost that can be accrued by a policy. The set of feasible policies is defined as $\\Pi_{c}=\\{\\pi\\in\\Pi:\\mathcal{J}^{c}(\\pi)\\leq c_{m a x}\\}$ . A policy is considered to be safe w.r.t $c$ if it belongs to $\\Pi_{c}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider the constrained RL problem defined as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{*}=\\underset{\\pi\\in\\Pi_{c}}{\\mathrm{argmin}}\\ \\mathcal{J}^{r}(\\pi)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We assume the threshold $c_{m a x}$ is known, but the cost function is not known and must be inferred from feedback collected from an external evaluator. In many scenarios $c_{m a x}$ is typically known, representing a predefined limit on acceptable costs or risks in the environment. However, crafting the cost function $c(s,a)$ such that it penalizes all unsafe behaviour can be infeasible. ", "page_idx": 2}, {"type": "text", "text": "We incorporate an additional constraint enforcing the cost function to be binary, i.e, $c(s,a)\\in\\{0,1\\}$ . This ensures that each state-action pair is inherently categorized as either safe or unsafe. We opt for this approach because it is simpler for human evaluators to assign a binary safety value to state-actions when assessing policy safety, as emphasized in [31]. ", "page_idx": 2}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce Reinforcement Learning from Safety Feedback (RLSF), an on-policy algorithm that consists of two alternating stages: 1) Data/Feedback collection and 2) Constraint inference/Policy improvement. In the first stage, data is collected via rollouts of the current policy for a fixed number of trajectories. Next, a subset of these trajectories is presented for feedback from evaluator, which are then stored in a separate buffer. The second stage consists of two parts: i) Estimation of the cost function from the feedback data and ii) Improvement of the policy using the collected trajectories and their inferred costs. We repeat stages (1) and (2) until convergence. ", "page_idx": 2}, {"type": "text", "text": "First, we highlight how the feedback is collected and propose a method to infer the constraint function using this data. Next, we recognize the practical limitations of acquiring feedback for every trajectory during training and detail our approach to sampling a subset of trajectories for efficient cost learning of the cost function. Finally, we detail how the inferred cost function is used to improve the policy. ", "page_idx": 2}, {"type": "text", "text": "4.1 Nature of the Feedback ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the feedback process, the evaluator is first presented with the entire trajectory $\\tau_{0:T}$ . Afterward, the trajectory is divided into contiguous segments $\\tau_{i:j}$ of length $k$ , and feedback is collected for each segment. The segment length can be adjusted based on the complexity of the environment: in simpler environments, feedback can be gathered for the entire trajectory, whereas for environments with long horizons and sparse cost violations, shorter segments may be used. This approach simplifies the challenge of assigning credit to individual states. Similar methods have been adopted in other works that rely on human feedback [12, 18]. However, reducing segment length comes at an increased cost of obtaining feedback from the evaluator. ", "page_idx": 2}, {"type": "text", "text": "The evaluator is tasked with classifying a segment as unsafe if the agent encounters an unsafe state at any point within the segment. This decision was made to ensure consistent feedback from the evaluator. Alternative approaches\u2014such as marking a segment unsafe based on the number of unsafe states visited or leaving the classification to the evaluator\u2019s subjective judgment are more prone to generating inconsistent feedback in the case of human evaluators [12]. ", "page_idx": 2}, {"type": "text", "text": "4.2 Inferring the Cost Function ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $P=\\{\\tau_{i:j},y^{s a f e}\\}$ represent the feedback collected from the evaluator, where $y^{s a f e}=1$ if the segment was labelled safe and $\\boldsymbol{y}^{s a f e}=0$ otherwise. We assume there exists an underlying ground ", "page_idx": 2}, {"type": "text", "text": "truth cost function $c_{g t}(s,a)\\,\\in\\,[0,1]$ based on which the evaluator provides feedback. Then, the probability that a state is safe is defined as $p_{g t}^{s a f e}(s,a)=\\mathbb{I}[c_{g t}(s,a)=0]$ . ", "page_idx": 3}, {"type": "text", "text": "Now, let $p^{s a f e}(s,a)$ represent our estimate of $p_{g t}^{s a f e}(s,a)$ that we intend to estimate from the collected feedback. Then, by definition of how the feedback is collected, the probability that a trajectory segment $\\tau_{i:j}$ is labelled as safe is given by, ", "page_idx": 3}, {"type": "equation", "text": "$$\np^{s a f e}(\\tau_{i:j})=\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can infer $p^{s a f e}(s,a)$ by minimizing the likelihood loss, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{{\\cal L}^{m l e}=-E_{(\\tau_{i:j},y^{s a f e})\\sim P}\\left[y^{s a f e}\\log p^{s a f e}(\\tau_{i:j})+(1-y^{s a f e})\\log1-p^{s a f e}(\\tau_{i:j})\\right]}}\\\\ {{\\quad\\quad=-E_{(\\tau_{i:j},y^{s a f e})\\sim P}\\left[y^{s a f e}\\sum_{t=i}^{j}\\log p^{s a f e}(s_{t},a_{t})+(1-y^{s a f e})\\log\\big(1-\\displaystyle\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})\\big)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Directly minimizing Eq 3 is challenging as the term $\\textstyle\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})$ would collapse to 0 when the segment length is long, causing unstable gradien ts. To address this issue, we propose using a surrogate loss function where we replace $\\begin{array}{r}{1-\\prod_{t=i}^{j}p^{s a f e}\\big(s_{t},a_{t}\\big)}\\end{array}$ by $\\begin{array}{r}{\\prod_{t=i}^{j}(1-p^{s a f e}(s_{t},a_{t}))}\\end{array}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L^{s u r}=-E_{(\\tau_{i};\\,y^{s a f e})\\sim P r}\\left[y^{s a f e}\\displaystyle\\sum_{t=i}^{j}\\log p^{s a f e}(s_{t},a_{t})+(1-y^{s a f e})\\displaystyle\\sum_{t=i}^{j}\\log\\left(1-p^{s a f e}(s_{t},a_{t})\\right)\\right]}\\\\ &{\\qquad=-E_{(\\tau_{i};\\,y^{s a f e})\\sim P r}\\displaystyle\\sum_{t=i}^{j}\\sum_{s,a}\\mathbb{I}[(s_{t},a_{t})=(s,a)]\\Bigg[\\mathbb{I}[y^{s a f e}=1]\\log p^{s a f e}(s,a)}\\\\ &{\\qquad\\qquad+\\,\\mathbb{I}[y^{s a f e}=0]\\log\\left(1-p^{s a f e}(s,a)\\right)\\Bigg]}\\\\ &{\\qquad=-\\left[E_{(s,a)\\sim d_{g}}\\log p^{s a f e}(s,a)+E_{(s,a)\\sim d_{b}}\\log\\left(1-p^{s a f e}(s,a)\\right)\\right]}\\end{array}\\quad\\mathrm{(4)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r l r}{d_{g}(s,a)}&{{}=}&{E_{(\\tau_{i:j},y_{s a f e})\\sim P}\\left[\\sum_{t=i}^{j}\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap y^{s a f e}=1]\\right]}\\end{array}$ and $\\begin{array}{r l}{d_{b}(s,a)}&{{}=}\\end{array}$ $\\begin{array}{r}{E_{(\\tau_{i:j},y_{s a f e})\\sim P}\\left[\\sum_{t=i}^{j}\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap y^{s a f e}=0]\\right]}\\end{array}$ represent the densities with which states occur in safe and unsafe segments respectively. ", "page_idx": 3}, {"type": "text", "text": "The surrogate loss reformulates the objective from the segment level\u2014where collapsing probabilities over long segments can be problematic\u2014to the state level, where this issue does not occur. Optimizing Eq 4 involves breaking down the segments into individual states and assigning each state the label of the segment. Subsequently, states are uniformly sampled at random, and the binary cross-entropy loss is minimized. Consequently, $L^{s u r}$ represents a binary classification problem in which one class contains noisy labels. While unsafe states are always accurately labeled\u2014since their presence necessitates an unsafe classification for the segment\u2014a safe state may receive confilcting labels based on the status of the segment it belongs to. Thus, with a sufficient number of samples, we believe it becomes feasible to reliably differentiate between safe and unsafe states. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. The surrogate loss $L^{s u r}$ is an upper bound on the likelihood loss $L^{m l e}$ . ", "page_idx": 3}, {"type": "text", "text": "Thus, minimizing $L^{s u r}$ guarantees an upper bound on the likelihood loss of the estimated cost function. ", "page_idx": 3}, {"type": "text", "text": "Having discussed the surrogate loss, we now examine the characteristics of its optimal solution. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. The optimal solution to Eq $^{4}$ yields the estimate, ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{*}^{s a f e}(s,a)=\\frac{d_{g}(s,a)}{d_{g}(s,a)+d_{b}(s,a)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Subsequently, we define the inferred cost function as $c_{*}(s,a)\\triangleq\\mathbb{I}[p_{*}^{s a f e}(s,a)\\,<\\,\\frac{1}{2}]$ . Employing $c_{*}(s,a)$ in policy updates instead of $c_{g t}(s,a)$ introduces a bias in estimating the cost accrued by the ", "page_idx": 3}, {"type": "text", "text": "policy, that we analyse below. For this analysis, we assume that the feedback is sufficient, i.e, the density $d(s,a)$ is greater than zero for every state, otherwise $p^{*}(s,a)$ is not defined. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. For a fixed policy $\\pi$ , the bias in the estimation of the incurred costs is given by, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{\\pi}[\\gamma^{t}c_{*}(s,a)]-E_{\\pi}[\\gamma^{t}c_{g t}(s,a)]=E_{(s,a)\\sim\\rho_{g}^{\\pi}}[\\mathbb{I}[d_{b}(s,a)>d_{g}(s,a)]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u03c1 $\\begin{array}{r}{\\mathfrak{z}_{g}^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{T}\\gamma^{t}[\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap c_{g t}(s,a)=0]]\\;i s\\;t h e\\;a}\\end{array}$ ccupancy measure of safe states visited by $\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3 illustrates that $c^{*}(s,a)$ misclassifies certain safe states as unsafe when their frequency in segments labeled unsafe exceeds that in segments labeled safe by the evaluator. We contend that this misclassification is likely to diminish with increased data collection or shorter segment lengths. However, it is important to note that this misclassification is guaranteed to be zero only when the segment length is reduced to one, meaning feedback is provided at the state level. ", "page_idx": 4}, {"type": "text", "text": "Additionally, note that the bias is non-negative, meaning that the expected cost $\\textstyle E_{\\pi}[c_{*}(s,a)]$ acts as an upper bound on the true cost incurred by $\\pi$ . Therefore, ensuring that the policy does not exceed the threshold $c_{m a x}$ on $c_{*}$ guarantees that it adheres to the threshold on $c_{g t}$ . ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Any policy $\\pi$ that is safe w.r.t $c_{*}$ is guaranteed to be safe $w_{\\cdot}r t\\ c_{g t}$ ", "page_idx": 4}, {"type": "text", "text": "In practice, we represent $p_{\\theta}^{s a f e}(s,a)$ using a neural network with parameters $\\theta$ . The resulting cost function is defined as $c_{\\theta}(s,a)\\triangleq\\mathbb{I}[p^{s a f e}(s,a)]<\\textstyle{\\frac{1}{2}}$ . ", "page_idx": 4}, {"type": "text", "text": "4.3 Efficient Subsampling of Trajectories ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To reduce the burden on the evaluator and minimize the cost of feedback, we present a subset of the trajectories collected by the policy for feedback. The common approach is to break the problem into two parts: (1) define a schedule $N_{q u e r i e s}(i)$ that determines the number of trajectories to be shown to the user at the end of each data collection round $i$ . Subsequently, $N_{q u e r i e s}(i)$ trajectories are sampled from those collected by the policy at data collection round $i$ . While the ideal goal is to sample a subset of trajectories that maximizes the expected value of information [19], achieving this is computationally intractable [2]. To address this challenge, various sampling methods have been employed, seeking to maximize a surrogate measure of this value. Among these, uncertainty sampling stands out as the most prominent approach, wherein trajectories are sampled based on the estimator\u2019s uncertainty about their predictions [12, 18, 20]. However, quantifying the uncertainty is challenging given the lack of calibration in neural network predictions. To address this challenge, ensemble methods are frequently employed where the disagreement among the models is used as an uncertainty measure. However, the training of $n$ distinct neural networks can exact substantial resource costs, prompting consideration for alternative approaches. ", "page_idx": 4}, {"type": "text", "text": "In light of this, we introduce a new form of uncertainty sampling called novelty sampling. With novelty sampling, we gather all the novel trajectories after each round and present them to the evaluator for feedback. Formally, we define a state as novel if its density in the feedback data collected so far $\\begin{array}{r}{d(s)=\\sum_{a}d(s,a)}\\end{array}$ is 0. A trajectory is deemed novel if it comprises of at least $e$ novel states. This can be interpreted as ensuring that the edit distance\u2014a well-known measure of trajectory distance [27]\u2014between the current trajectory and previously seen trajectories exceeds a threshold $e$ . We do not consider novelty for state-action pairs as we found that extending to this case adversely impacted the performance. ", "page_idx": 4}, {"type": "text", "text": "The central notion is that the model is prone to errors on novel states- those it has not encountered during training. This arises because the policy evolves over time, venturing into states that were not previously encountered during data collection rounds. Therefore, this sampling strategy effectively reduces the epistemic uncertainty of the model\u2014error arising from insufficient training data\u2014thereby making it a form of uncertainty sampling. Furthermore, this sampling method offers the advantage of implicitly establishing a decreasing querying schedule as novelty of trajectories reduces over time as shown in Figure 6 in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "We compute the density $d(s)$ through a count-based method, utilizing a hashmap to track the frequency of state occurrences in trajectories presented to the evaluator. Employing SimHash [10], we discretize the state space using a hash function $\\phi:S\\rightarrow\\{-1,1\\}^{n}$ , which maps locally similar states (measured by angular distance) to a binary code as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi(s)=s g n(A g(s))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g:S\\to\\mathbb{R}^{d}$ is an optional prepossessing function and $A$ is an $n\\times d$ matrix with i.i.d entries drawn from a standard normal distribution. Also, note that $n$ controls the granularity of the hash function, i.e, the number of states mapped to the same value. In our setting, $\\bar{g(s)}=s$ as we observed no improvement when employing functions like autoencoders for feature extraction. ", "page_idx": 5}, {"type": "text", "text": "4.4 Policy Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After the data collection round, where we sample trajectories $\\{\\tau\\}$ and their corresponding rewards $\\{r(\\tau)\\}$ using $\\pi$ , we estimate their costs $c(\\tau)$ using the inferred cost function. Our proposed method allows for the policy to be updated utilizing any on-policy constrained RL algorithm. In this study, we employ the PPO-Lagrangian algorithm[3] that combines the PPO algorithm [26] with a lagrangian multiplier to ensure safety. ", "page_idx": 5}, {"type": "text", "text": "A detailed description of the proposed method can be found in Algorithm 1. Lines [6-17] describe the data and feedback collection stage, and Lines [18-23] describe the cost inference and policy improvement stage. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Reinforcement Learning from Safety Feedback (RLSF) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: cost threshold $c_{m a x}$ , segment length $k$ , novelty criterion $e$   \n2: Initialize: policy $\\pi_{0}$   \n3: Initialize: classifier $c_{\\theta}$ , learning rate $l r_{\\theta}$ and feedback buffer $D$   \n4: Initialize: $A\\in\\mathbb{R}^{n\\times d}$ with entries sampled i.i.d from $\\mathcal{N}(0,1)$ , $\\phi(.)=s g n(A^{T}(.))$ , density map   \n$d(.)\\equiv0$ .   \n5: while not converged do   \n6: Collect trajectories $\\{\\tau\\},\\{r(\\tau)\\}\\sim\\pi$ \u25b7Data Collection   \n7: for each trajectory $\\tau^{i}\\in\\{\\tau\\}$ do \u25b7Feedback Collection   \n8: novel $\\leftarrow$ True if $\\exists$ e states $\\{s_{e}\\}\\in\\tau^{i}$ such that $d(\\phi(s_{e}))>0$   \n9: if novel then   \n10: Show $\\tau^{i}$ to the evaluator   \n11: for each segment $\\tau_{j:j+k-1}\\in\\tau^{i}$ do   \n12: Obtain feedback $y^{s a f e}$ for the segment $\\tau_{j:j+k-1}^{i}$ .   \n13: $\\begin{array}{l}{D\\leftarrow D\\cup\\{((s,a),y^{s a f e})\\ \\ \\forall(s,a)\\in\\tau_{j:j+k-1}^{i^{'}\\setminus}\\}}\\\\ {d(\\phi(s))\\leftarrow d(\\phi(s))+1\\ \\forall s\\in\\tau_{j:j+k-1}^{i}}\\end{array}$   \n14: \u25b7Update the densities   \n15: end for   \n16: end if   \n17: end for   \n18: for each gradient step do \u25b7Update cost estimates   \n19: Sample random minibatch $b\\gets\\{(s,a),y_{s a f e}\\}\\sim D$   \n20: $\\theta\\xleftarrow{}\\theta-l r_{\\theta}\\nabla L^{s u r}(b)$   \n21: end for   \n22: Infer costs $\\{c_{\\theta}(\\tau_{i})\\}$ for all $\\tau^{i}\\sim\\{\\tau\\}$ .   \n23: Update $\\pi$ using $\\{r(\\tau_{i})\\}$ and $\\{c_{\\theta}(\\tau_{i})\\}$ . \u25b7Policy Improvement   \n24: end while ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We investigate the following questions in our experiments: ", "page_idx": 5}, {"type": "text", "text": "1. Does RLSF succeed in effectively learning safe behaviours?   \n2. Can the inferred cost function be transferred across agents in the same task?   \n3. How does the proposed novelty based sampling scheme compare with other methods used in the literature?   \n4. How accurate is inferred cost function compared to the true cost function?   \n5. How can we address the overestimation bias of the inferred cost function as described in Section 4.2? ", "page_idx": 5}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: Performance of different algorithms on the Safety Benchmarks. The first 7 environments represent the hard constraint case. The remaining environments illustrate the soft constraint case, with values in brackets indicating the cost threshold. Each algorithm is run for 6 independent seeds. (orange) and (blue) indicate the best performance in the known costs and inferred costs settings, respectively. Algorithms with a cost violation (C.V) rate below $1\\%$ are deemed to have equal performance in terms of safety. ", "page_idx": 6}, {"type": "table", "img_path": "WSsht66fbC/tmp/041c07dba2a0fd3b0fc1dd0d5c427fe7d3154bbe3b623fe5d5ab381d0edf62b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We evaluate RLSF on multiple continuous control benchmarks in the Safety Gymnasium environment [17] and Mujoco [30] based environments introduced in [22]. The Circle, Blocked Swimmer and Biased Pendulum environments constrain the position of the agent whereas the Half Cheetah, Hopper and Walker- $_{2d}$ environments constrain the velocity of the agent. The Goal and Push tasks are the most challenging as they contain static and dynamic obstacles that the agent must avoid while completing the task.All of the above environments reflect safety challenges that an agent could potentially face in real-world scenarios. Additionally, we conduct experiments using the Driver simulator introduced in [21], which presents two scenarios that an autonomous driving agent is likely to encounter on the highway: lane changes and blocked paths. In this setup, the cost function is based on multiple variables, including speed, position, and distance to other vehicles. Additionally, we introduce a third scenario: overtaking on a two-lane highway. A detailed description of the tasks can be found in the Appendix B. ", "page_idx": 6}, {"type": "text", "text": "We split the environments into two settings: (1) hard constraint setting $\\left(c_{m a x}=0\\right)$ ) where the safety of the policy is measured in terms of the cost violation (CV) rate defined as the number of cost violations divided by the length of the episode and (2) soft constraint setting where $c_{m a x}>0$ . We utilize an automated script that leverages the underlying cost function to simulate the feedback provided by the evaluator. ", "page_idx": 6}, {"type": "text", "text": "We compare the performance of our algorithm against the following baselines: Self Imitation Safe Reinforcement Learning (SIM) [16]: SIM is a state-of-the-art method in constrained RL that also supports the case where feedback is elicited from an external evaluator. Similar to RLSF, the method consists of two stages, a data collection/feedback stage and a policy optimization stage. In the first stage, a trajectory $\\tau$ is labelled as good if $[r(\\tau)\\geq r_{g o o d}\\;\\;\\cap\\;\\;\\mathbb{I}[c(\\tau)\\leq c_{m a x}]$ , and labelled as bad if $[r(\\tau)\\geq r_{b a d}\\;\\;\\cup\\;\\;\\mathbb{I}[c(\\tau)\\geq c_{m a x}]$ , where $r_{g o o d}$ and $r_{b a d}$ are predefined thresholds on the reward. The information $\\mathbb{I}[c(\\tau)\\,\\leq\\,c_{m a x}]$ is received from feedback. The idea is then to imitate the good trajectories and stay away from the bad trajectories. If $\\rho^{\\pi}$ is the occupancy measure of the current policy $\\pi$ and $\\rho^{G},\\rho^{\\check{B}}$ are the occupancy measures of the good and bad trajectories respectively, then, $\\pi$ is optimized as, ", "page_idx": 6}, {"type": "image", "img_path": "WSsht66fbC/tmp/356a368ec5d650f50059fd1f6a17325949a69f3cf5dd8d4914fad3624789c186.jpg", "img_caption": ["Figure 1: Cost Violation rate of different algorithms in the Driver environment. Each algorithm is run for 6 independent seeds, with the curves representing the mean and the shaded regions indicating the standard error. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi^{*}=\\operatorname*{argmax}_{\\pi}\\ \\mathrm{KL}(\\rho^{\\pi,G}||\\rho^{B})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\rho^{\\pi,G}=(\\rho^{\\pi}+\\rho^{G})/2$ and KL denotes the Kullback\u2013Leibler divergence. ", "page_idx": 7}, {"type": "text", "text": "Safe Distribution Matching (SDM): SIM combines rewards with safety feedback into a joint notion of good and bad which may not be desirable when the cost function is unknown. Thus, we introduce an additional baseline that keeps these two signals separate by labelling the trajectory as good if $c(\\tau)\\leq c_{m a x}$ and bad otherwise. The policy is then updated as, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi^{*}=\\underset{\\pi}{\\operatorname{argmax}}\\:\\:r+\\lambda\\mathrm{KL}(\\rho^{\\pi,G}||\\rho^{B})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda\\in[0,1]$ controls the tradeoff between the two objectives. ", "page_idx": 7}, {"type": "text", "text": "As an upper bound, we compare the performance of our algorithm to scenarios where the cost function is known: PPO-Lagrangian (PPOLag) [3] and SIM with known costs (SIMKC) [16]. The objective is not to surpass their performance, but to match it. Therefore, when reporting the results of these algorithms, we present the best-performing seed across multiple runs. ", "page_idx": 7}, {"type": "text", "text": "All results presented use novelty-based sampling unless stated otherwise. We use a segment length of 1 in the Driver, Goal and Push environments. This is because the Safety Goal and Push environments contain small obstacles that the agent interacts with for very brief periods of time, hence requiring more fine-grained feedback. An example of this is present in Figure 11 in the Appendix. In the Driver environments, a randomly initialized policy was highly unsafe. Thus a long segment length would force the evaluator to label every segment unsafe, making cost inference infeasible. In the remaining environments, the segment length corresponds to the length of the episode. Details on the number of queries generated for feedback can be found in Table 4 in the Appendix. We grant the two baseline methods (with unknown costs) an advantage by providing feedback for every trajectory generated. ", "page_idx": 7}, {"type": "text", "text": "5.2 Cost Inference across various tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Benchmark Environments Table 1 presents the performance of the various algorithms on the benchmark environments. RLSF significantly outperforms the two baselines in terms of reward and safety in all of the environments 2. RLSF comes to within $\\approx80\\%$ of the performance of the best run of PPOLag in $7/11$ environments thereby underscoring its effectiveness in learning safe policies. ", "page_idx": 7}, {"type": "text", "text": "Driving Scenarios In the driving environments, we evaluate the performance of our algorithm against two baseline methods: a naive PPO policy that solely maximizes reward and the PPO-Lag algorithm. Figure 1 presents these results. Notably, our method demonstrates comparable safety performance to PPO-Lag while significantly outperforming PPO in terms of cost violations. This highlights the effectiveness of our approach in learning safe autonomous driving policies. Video clips showcasing the learned policies in the 3 scenarios are included in the supplementary material. ", "page_idx": 7}, {"type": "image", "img_path": "WSsht66fbC/tmp/9e77f3466d5d48356ad9e9a6a8eefe7e17aeab56f1f7bb0fe9806c96fa98179f.jpg", "img_caption": ["Figure 2: Comparison of different sampling and scheduling schemes. Results are averaged over 3 independent seeds. The proposed sampling method generates on average $\\approx1950$ queries, hence for fair comparison the other methods were given a budget of 2000 queries. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Cost Transfer ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also showcase the potential of using the inferred cost function to train an agent with a different embodiment or morphology to solve the same task from scratch, without requiring any additional feedback. We utilize the inferred cost function obtained during the training of a Point agent to train the Doggo agent for the Circle and Goal tasks. The observation space in the environments consists of agent-related telemetry (acceleration, velocity) and task-related information (lidar sensors for goal/boundary/obstacle detection). In these experiments, we do not incorporate agent-related information when learning the cost function, ensuring that it can be transferred to a new agent. Next, the inferred cost function remains fixed during the training of the second agent, and the task-related features are utilized for cost inference. We compare the performance of this approach with that of the PPO-Lagrangian algorithm that utilizes the underlying cost function of the environment. From Table 2 we can infer that agents trained using transferred cost function are comparable in performance to agents trained using the true underlying cost function. ", "page_idx": 8}, {"type": "table", "img_path": "WSsht66fbC/tmp/8799e134cb38ed954b26ebecfacb550c8a3b7f7976ac48c134497c066058f5e1.jpg", "table_caption": ["Table 2: Comparison of PPOLag performance when trained with the underlying task cost function versus the transferred cost function. Results are averaged over three independent seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Effect of Novelty Sampling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we demonstrate the effectiveness of the proposed novelty-based sampling mechanism by comparing it with other popular querying methods from the literature [18, 20]. These methods require a predefined budget for the number of trajectories presented to the evaluator. We evaluate two schedules: (1) Uniform schedule, where a fixed number of trajectories is shown for feedback after each round, and (2) Decreasing schedule, where the number of trajectories decreases in proportion to $\\frac{1}{t}$ each round. For each schedule, we test the following strategies for sampling the subset of trajectories to be presented to the evaluator: (a) Random sampling, which selects a random subset of trajectories, and (b) Entropy sampling, where trajectories are sampled in descending order of the average entropy $\\mathcal{H}(p_{\\theta}^{s a f e}(s_{t},a_{t}))$ of the estimator. As shown in Figure 2, entropy based sampling outperforms random sampling, and a decreasing schedule outperforms a uniform schedule. However, all the methods fall short compared the proposed novelty based sampling mechanism. ", "page_idx": 8}, {"type": "text", "text": "5.5 Analyzing the Inferred Cost function ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To further evaluate the quality of the inferred reward function, we plot it alongside the true underlying cost function of the environment as presented in Figure 3. In the Point Goal environment, where feedback is collected for each state, the inferred cost function closely aligns with the true cost given sufficient data. However, in the Point Circle environment, where feedback is collected at the trajectory level, the inferred cost function exhibits a slight overestimation bias, as discussed in Section 4.2. Nevertheless, from Figure 3, we can conclude that our method learns cost functions that strongly correlate with the true costs. ", "page_idx": 9}, {"type": "text", "text": "5.6 Bias Correction ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Section 4.2, we established that when feedback is collected over segments, the inferred cost function tends to exhibit an overestimation bias. As a result, optimizing for a policy that is deemed safe with respect to $c_{m a x}$ using the inferred cost function can lead to overly conservative policies. If the bias $b$ , could be calculated apriori, then adjusting the safety threshold to $c_{m a x}+b$ would still satisfy the safety guarantees outlined in Corollary 1. However, since estimating $b$ in advance is infeasible, we propose a heuristic approach where a constant $\\delta\\in\\mathbb{R}^{+}$ is added to $c_{m a x}$ to account for the bias. It\u2019s important to note that Corollary 1 only holds if $\\delta\\leq b$ , meaning this heuristic doesn\u2019t guarantee that the resulting policy will always be safe. Nevertheless, we tested this approach with different $\\delta$ values in the $C a r$ Circle environment, as this setting exhibited a higher overestimation bias (as reflected in Table 1). The results, shown in Figure 4, demonstrate that adding this bonus does improve performance (with higher $\\delta$ values yielding better results), though it introduces the additional challenge of tuning $\\delta$ . ", "page_idx": 9}, {"type": "image", "img_path": "WSsht66fbC/tmp/3bb4052a471e3913048d64818fe520896de041814b793ebd9b5baee37be412d4.jpg", "img_caption": ["Figure 3: Comparing the inferred cost to the true cost. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Limitations, Future Work and Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we consider the problem of learning cost functions from feedback in scenarios where evaluating or defining such functions is either costly or infeasible. We present a novel framework which significantly reduces the burden on the evaluator by eliciting feedback over extended horizons, a challenge that has not been addressed in previous research. To further reduce their load, we propose a novelty-based sampling method that only presents previously unseen trajectories to the evaluator. Through experiments on multiple benchmark environments, we demonstrate the effectiveness of our framework and the proposed sampling algorithm in learning safe policies. However, there are a few limitations we would like to acknowledge. First, our method relies on state-level feedback in some environments, which can be expensive to obtain. Second, while we simulate feedback using the true cost function, real-world feedback is often noisy, especially when provided by human evaluators. It would be valuable to conduct experiments involving real human subjects to validate the approach. ", "page_idx": 9}, {"type": "text", "text": "By enabling safer autonomous systems, our approach has the potential to enhance safety across a range of applications, including autonomous vehicles, drones used for delivery, and industrial robots operating in environments like warehouses or manufacturing plants. These improvements could significantly reduce accidents, protect human operators, and increase the overall reliability of automated systems. ", "page_idx": 9}, {"type": "image", "img_path": "WSsht66fbC/tmp/778ae5ed82419ec231c2240f5f571b6344665be9debd01c48de2bde9f6f82351.jpg", "img_caption": ["Figure 4: Increasing $c_{m a x}$ by $\\delta$ to correct for the overestimation bias. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-016) and Lee Kuan Yew Fellowship awarded to Pradeep Varakantham. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Achiam, D. Held, A. Tamar, and P. Abbeel. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning, 2017. [2] N. Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. Journal of Machine Learning Research, 2010. [3] D. A. Alex Ray, Joshua Achiam. Benchmarking safe exploration in deep reinforcement learning. arXiv:1910.01708, 2019. [4] E. Altman. Constrained Markov Decision Processes. CRC Press, 1999. [5] S. Amani, C. Thrampoulidis, and L. F. Yang. Safe reinforcement learning with linear function approximation. In Proceedings of the 38th International Conference on Machine Learning, 2021. [6] U. Anwar, S. Malik, A. Aghasi, and A. Ahmed. Inverse constrained reinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, 2021.   \n[7] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In 30th Conference on Neural Information Processing Systems, 2016. [8] A. Bennett, D. K. Misra, and N. Kallus. Provable safe reinforcement learning with binary feedback. In 26th International Conference on Artificial Intelligence and Statistics, 2022. [9] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. In 7th International Conference on Learning Representations, 2019.   \n[10] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the 34th Annual ACM Symposium on Theory of Computing, 2002.   \n[11] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In 33rd AAAI Conference on Artificial Intelligence, 2019.   \n[12] P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In 31st Conference on Neural Information Processing Systems, 2017.   \n[13] J. Fu, J. D. Co-Reyes, and S. Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In 31st Conference on Neural Information Processing Systems, 2017.   \n[14] D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. Iq-learn: Inverse soft-q learning for imitation. In 35th Conference on Neural Information Processing Systems, 2021.   \n[15] J. Ho and S. Ermon. Generative adversarial imitation learning. In 30th Conference on Neural Information Processing Systems, 2016.   \n[16] H. Hoang, T. Mai, and P. Varakantham. Imitate the good and avoid the bad: An incremental approach to safe reinforcement learning. In 39th Annual AAAI Conference on Artificial Intelligence, 2024.   \n[17] J. Ji, B. Zhang, J. Zhou, X. Pan, W. Huang, R. Sun, Y. Geng, Y. Zhong, J. Dai, and Y. Yang. Safety gymnasium: A unified safe reinforcement learning benchmark. In 37th Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[18] K. Lee, L. M. Smith, and P. Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In Proceedings of the 38th International Conference on Machine Learning, 2021.   \n[19] J. S. Leonard. The Foundations of Statistics. John Wiley & Sons, 1978.   \n[20] X. Liang, K. Shu, K. Lee, and P. Abbeel. Reward uncertainty for exploration in preference-based reinforcement learning. In 10th International Conference on Learning Representations, 2022.   \n[21] D. Lindner, S. Tschiatschek, K. Hofmann, and A. Krause. Interactively learning preference constraints in linear bandits. In Proceedings of the 39th International Conference on Machine Learning, 2022.   \n[22] G. Liu, Y. Luo, A. Gaurav, K. Rezaee, and P. Poupart. Benchmarking constraint inference in inverse reinforcement learning. In 11th International Conference on Learning Representations, 2023.   \n[23] S. Poletti, A. Testolin, and S. Tschiatschek. Learning constraints from human stop-feedback in reinforcement learning. In 22nd International Conference on Autonomous Agents and Multiagent Systems, 2023.   \n[24] D. Sadigh, A. D. Dragan, S. S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions. In Robotics: Science and Systems, 2017.   \n[25] W. Saunders, G. Sastry, A. Stuhlm\u00fcller, and O. Evans. Trial without error: Towards safe reinforcement learning via human intervention. In 16th International Conference on Autonomous Agents and Multiagent Systems, 2017.   \n[26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.   \n[27] H. Su, S. Liu, B. Zheng, X. Zhou, and K. Zheng. A survey of trajectory distance measures and performance evaluation. The VLDB Journal, 2020.   \n[28] Y. Sui, A. Gotovos, J. Burdick, and A. Krause. Safe exploration for optimization with gaussian processes. In Proceedings of the 32nd International Conference on Machine Learning, 2015.   \n[29] H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. D. Turck, and P. Abbeel. #exploration: A study of count-based exploration for deep reinforcement learning. In 31st Conference on Neural Information Processing Systems, 2017.   \n[30] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.   \n[31] A. Wachi, W. Hashimoto, and K. Hashimoto. Long-term safe reinforcement learning with binary feedback. In 39th Annual AAAI Conference on Artificial Intelligence, 2024.   \n[32] A. Wachi and Y. Sui. Safe reinforcement learning in constrained markov decision processes. In International Conference on Machine Learning, 2020.   \n[33] S. Xu and G. Liu. Uncertainty-aware constraint inference in inverse constrained reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[34] L. Yang, J. Ji, J. Dai, L. Zhang, B. Zhou, P. Li, Y. Yang, and G. Pan. Constrained update projection approach to safe policy optimization. In 36th Conference on Neural Information Processing Systems, 2022.   \n[35] Y. Zhang, Q. H. Vuong, and K. W. Ross. First order constrained optimization in policy space. In 34th Conference on Neural Information Processing Systems, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proposition 1. The surrogate loss $L^{s u r}$ is an upper bound on the likelihood loss $L^{m l e}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We have, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L^{s u r}-L^{m l e}=-E_{(\\tau_{i:j},y^{s a f e})\\sim P}(1-y^{s a f e})\\Bigg[\\underset{t=i}{\\overset{j}{\\sum}}\\log\\left(1-p^{s a f e}(s_{t},a_{t})\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\ensuremath{\\log\\left(1-\\displaystyle\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})\\right)}\\Bigg]}\\\\ &{\\qquad\\qquad=E_{(\\tau_{i:j},y^{s a f e})\\sim P}(1-y^{s a f e})\\Bigg[\\log\\left(\\frac{1-\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})}{\\prod_{t=i}^{j}1-p^{s a f e}(s_{t},a_{t})}\\right)\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Notice that the term, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\log\\left(\\frac{1-\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})}{\\prod_{t=i}^{j}1-p^{s a f e}(s_{t},a_{t})}\\right)\\geq0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This is because ", "page_idx": 12}, {"type": "equation", "text": "$$\n1-\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})\\geq\\prod_{t=i}^{j}1-p^{s a f e}(s_{t},a_{t})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which we prove subsequently. ", "page_idx": 12}, {"type": "text", "text": "Lemma 1. For any sequence of numbers $\\{x_{1},\\ldots,x_{n}\\}$ , where $x_{i}\\in[0,1]$ for all $i\\in{1,\\dots,n}$ we have $\\begin{array}{r}{1-\\prod_{i=1}^{n}x_{i}\\stackrel{.}{\\geq}\\prod_{i=1}^{n}(1-{\\bar{x}}_{i})}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Using the AM-GM inequality we have, ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{~~{\\sqrt{\\displaystyle\\prod_{i=1}^{n}x_{i}}}\\leq{\\frac{\\sum_{i=1}^{n}x_{i}}{n}}}\\\\ &{1-\\displaystyle\\prod_{i=1}^{n}x_{i}\\geq1-\\left({\\frac{\\sum_{i=1}^{n}x_{i}}{n}}\\right)^{n}\\geq1-\\left({\\frac{\\sum_{i=1}^{n}x_{i}}{n}}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, we have, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sqrt[n]{\\prod_{i=1}^{n}(1-x_{i})}\\leq\\frac{\\sum_{i=1}^{n}(1-x_{i})}{n}}\\\\ {\\displaystyle-\\prod_{i=1}^{n}(1-x_{i})\\geq-\\left(\\frac{\\sum_{i=1}^{n}(1-x_{i})}{n}\\right)^{n}\\geq-\\left(\\frac{\\sum_{i=1}^{n}(1-x_{i})}{n}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Adding Eq 10 and $\\mathrm{Eq}\\ 11$ , we get, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle1-\\prod_{i=1}^{n}x_{i}-\\prod_{i=1}^{n}(1-x_{i})\\geq1-\\left(\\frac{\\sum_{i=1}^{n}x_{i}}{n}\\right)-\\left(\\frac{\\sum_{i=1}^{n}(1-x_{i})}{n}\\right)}\\\\ {\\displaystyle1-\\prod_{i=1}^{n}x_{i}-\\prod_{i=1}^{n}(1-x_{i})\\geq0}\\\\ {\\displaystyle1-\\prod_{i=1}^{n}x_{i}\\geq\\prod_{i=1}^{n}(1-x_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proposition 2. The optimal solution to Eq $^{4}$ yields the estimate, ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{*}^{s a f e}(s,a)=\\frac{d_{g}(s,a)}{d_{g}(s,a)+d_{b}(s,a)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The proof is obtained by differentiating Eq 4 w.r.t $p^{s a f e}(s,a)$ and setting it to 0. ", "page_idx": 13}, {"type": "text", "text": "Proposition 3. For a fixed policy $\\pi$ , the bias in the estimation of the incurred costs is given by, ", "page_idx": 13}, {"type": "equation", "text": "$$\nE_{\\pi}[c_{*}(s,a)]-E_{\\pi}[c_{g t}(s,a)]=E_{(s,a)\\sim d_{g}^{\\pi}}[\\mathbb{I}[d_{b}>d_{g}]]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho_{g}^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{T}[\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap c_{g t}(s,a)=0]]}\\end{array}$ is the occupancy measure of true safe states visited by $\\pi$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The estimated cost function $c_{*}(s,a)=\\mathbb{I}[p_{*}^{s a f e}(s,a)<\\frac{1}{2}]$ , from Eq 5 we get $p_{*}^{s a f e}(s,a)<\\frac{1}{2}$ when $d_{b}(s,a)>d_{g}(s,a)$ . Thus, $c_{*}(s,a)=\\mathbb{I}[d_{b}(s,a)>d_{g}(s,\\tilde{a})]$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\pi}[c_{*}(s,a)-c_{g t}(s,a)]=E_{\\tau\\sim\\pi}\\displaystyle\\sum_{t=0}^{T}\\left[c_{*}(s_{t},a_{t})-c_{g t}(s_{t},a_{t})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}\\\\ &{=E_{\\tau\\sim\\pi}\\displaystyle\\sum_{s,a}^{T}\\prod[\\{(s_{t},a_{t})=(s,a)}][c_{*}(s_{t},a_{t})-c_{g t}(s_{t},a_{t})]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{=E_{\\tau\\sim\\pi}\\displaystyle\\sum_{t=0}^{T}\\displaystyle\\sum_{s,a}^{\\tau}\\left[\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap c_{g t}(s,a)=0]\\left[c_{*}(s_{t},a_{t})-c_{g t}(s_{t},a_{t})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathbb{I}[(s_{t},a_{t})=(s,a)\\cap c_{g t}(s,a)=1]\\left[c_{*}(s_{t},a_{t})-c_{g t}(s_{t},a_{t})\\right]\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =E_{\\tau\\sim\\pi}E_{(s,a)\\sim\\rho^{\\tau}}[c_{*}(s,a)-0]+E_{(s,a)\\sim\\rho^{\\tau}}[c_{*}(s,a,)-1]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{where}~~\\rho_{g}^{\\pi}(s,a)}&{=}&{E_{\\tau\\sim\\pi}[\\sum_{t=0}^{T}[\\mathbb{I}[(s_{t},a_{t})}&{=}&{(s,a)\\,\\cap\\,\\,c_{g t}(s,a)}&{=}&{0]]\\mathrm{~\\and~}~\\rho_{b}^{\\pi}(s,a)}\\\\ {E_{\\tau\\sim\\pi}[\\sum_{t=0}^{T}[\\mathbb{I}[(s_{t},a_{t})=(s,a)\\,\\cap\\,c_{g t}(s,a)=1]].}\\end{array}\n$$$\\begin{array}{r l}{\\rho_{b}^{\\pi}(s,a)}&{{}=}\\end{array}$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that when a state is unsafe, i.e, $c_{g t}(s,a)=1$ , then, $p_{*}^{s a f e}(s,a)=0$ as $n_{g}(s,a)=0$ . Thus for all of these states, $c_{*}(s,a)=1$ . Hence, we are left with, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\pi}[c_{*}(s,a)-c_{g t}(s,a)]=E_{(s,a)\\sim\\rho_{g}^{\\pi}}\\big[c_{*}(s,a)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=E_{(s,a)\\sim\\rho_{g}^{\\pi}}[\\mathbb{I}[d_{b}>d_{g}]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Corollary 2. Any policy $\\pi$ that is safe w.r.t $c_{*}$ is guaranteed to be safe $w_{\\cdot}r t\\ c_{g t}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. From Proposition 3, we know that ${\\cal E}_{\\pi}[c_{*}(s,a)]\\,-\\,{\\cal E}_{\\pi}[c_{g t}(s,a)]\\,=\\,{\\cal E}_{(s,a)\\sim\\rho_{g}^{\\pi}}[\\mathbb{I}[d_{b}\\,>\\,d_{g}]].$ Since $E_{(s,a)\\sim\\rho_{g}^{\\pi}}[\\mathbb{I}[d_{b}>d_{g}]]\\ge0$ , we have $E_{\\pi}[c_{*}(s,a)]\\geq E_{\\pi}[c_{g t}(s,a)]$ . ", "page_idx": 13}, {"type": "text", "text": "B Environments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Benchmark environments: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Mujoco [30]-based environments extend traditional locomotion tasks by incorporating safety constraints. These environments are categorized into two types: position-based and velocity-based constraints. A detailed description of each is provided below: ", "page_idx": 13}, {"type": "text", "text": ". Position Based: These environments, recently introduced as benchmarks in [22], are explained in more detail below. ", "page_idx": 13}, {"type": "text", "text": "(a) Blocked Swimmer: The agent controls a robot composed of three segments joined at two points. The objective is to move the robot to the right as quickly as possible by applying torque through rotors at the joints. The agent receives a reward at each time step, proportional to its displacement in the $X$ -direction. The episode concludes after 1000 time steps. To increase the challenge, a cost is imposed when the agent\u2019s $X$ -coordinate exceeds 0.5, constraining it to the region $-\\infty\\leq X<0.5$ . ", "page_idx": 14}, {"type": "text", "text": "(b) Biased Pendulum: The agent\u2019s goal is to balance a pole on a cart. Each episode ends if the pole falls or after a maximum of 1000 time steps. The agent receives a reward of 1 when its $X$ -coordinate is within the range $(-\\infty,-0.01]$ , which monotonically decreases to 0.1 as the $X$ -coordinate approaches 0. Beyond this, the agent receives a constant reward of 0.1, encouraging movement in the leftward direction. To prevent excessive leftward movement, a cost of 1 is incurred when the $X$ -coordinate is in the region $(-\\infty,-0.015]$ , constraining the agent from moving too far left. ", "page_idx": 14}, {"type": "text", "text": "2. Velocity Based: These environments are part of the Safety Gymnasium benchmarks, introducing a velocity constraint for the Half-Cheetah, Hopper, and Walker- $^{2d}$ agents. The agent incurs a cost of 1 whenever it exceeds a velocity threshold of $v_{m a x}/2$ , where $v_{m a x}$ is the maximum velocity achieved by the agent when trained with PPO for $\\mathrm{\\Deltal\\times10^{7}}$ steps. ", "page_idx": 14}, {"type": "text", "text": "Safety Gymnasium [17] has emerged as a prominent benchmark for evaluating constrained reinforcement learning algorithms, providing a challenging framework for cost inference. The environments feature multiple agents, including the Point, Car, and Doggo, listed in increasing order of difficulty in controlling the agent. The objective is to complete specific tasks using one of these agents. ", "page_idx": 14}, {"type": "text", "text": "In the Circle task, the agent must navigate around the boundary of a circle centered at the origin while remaining within a safe region defined by two vertical boundaries at $x=\\pm0.7$ . A cost of 1 is incurred each time the agent exceeds this boundary $(|x|\\geq0.7)$ . ", "page_idx": 14}, {"type": "text", "text": "In the Goal task, the agent must reach a designated goal area as quickly as possible while avoiding both static and dynamic obstacles, incurring a cost of 1 for each collision. ", "page_idx": 14}, {"type": "text", "text": "The Push task builds upon the Goal task by adding the challenge of pushing a block to the goal area while still avoiding collisions with obstacles in the environment. ", "page_idx": 14}, {"type": "text", "text": "B.2 Safe Driver ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Driving-based environments are emerging as key benchmarks for addressing the problem of constraint inference [22], particularly due to the complex safety considerations involved in driving. We evaluate our algorithm using the Driver simulator introduced in [21, 24] to learn safe behaviors from feedback. This environment encompasses two scenarios that a self-driving agent is likely to encounter on the highway: a blocked road and a lane change. Additionally, we introduce a third scenario that involves overtaking on a two-lane highway, where the agent must safely pass a slower car in its lane by utilizing the second lane. This maneuver poses risks, as traffic in this lane is moving in the opposite direction, necessitating careful execution. Below, we provide a brief description of the environment. ", "page_idx": 14}, {"type": "text", "text": "The Driver environment utilizes point-mass dynamics and features continuous state and action spaces. The state of a vehicle in the simulation is represented by the tuple $(x,y,\\phi,v)$ , which includes the agent\u2019s position $(x,y)$ , heading $\\phi$ and velocity $v$ . The observation of the ego vehicle is formed by concatenating the states of all vehicles in the scene. The environment allows for two actions $(a_{1},a_{2})$ where $a_{1}$ represents the steering input $a_{2}$ denotes the applied acceleration. ", "page_idx": 14}, {"type": "text", "text": "The state of the agents evolves as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{s_{t+1}=(x+\\delta x,y+\\delta y,\\phi+\\delta\\phi,v+\\delta v)}}\\\\ {{\\left(\\delta x,\\delta y,\\delta\\phi,\\delta v\\right)=\\left(v\\cos\\phi,v\\sin\\phi,a_{2}-\\alpha v\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha$ is used to control the friction. Additionally, the velocity is clipped to the range $[-1,1]$ at each timestep. ", "page_idx": 14}, {"type": "text", "text": "The task is to control the ego vehicle (white in Figure 5) to reach the top of the road as soon as possible while adhering to costs on the velocity and proximity to other vehicles. The trajectories of the other vehicles are fixed, with some random noise applied to their steering and acceleration inputs. ", "page_idx": 14}, {"type": "image", "img_path": "WSsht66fbC/tmp/4f222b006c3c8bb51b2e48649ed33d47197b7aaae91797c79c6d2bebec6e4f0c.jpg", "img_caption": ["Figure 5: Driver Environments "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The agent receives a reward $r_{t}=10(y_{t}-y_{t-1})$ at each time step, incentivizing it to navigate through traffic rapidly. Additionally, it incurs a penalty of $-1$ for going off the road, driving backward, or failing to stay in the center of the lane. A collision results in a penalty of $-100$ . The episode terminates when the agent reaches the top of the road or collides with another vehicle. Although these constraints can be integrated into the reward function, they can still lead to unsafe behaviors, highlighting the necessity for an additional cost function. The PPO agent is trained using this reward function. ", "page_idx": 15}, {"type": "text", "text": "In the CMDP settings, the cost function incorporates some of the constraints from the reward function. For instance, the agent incurs a cost of 1 when it goes off the road or drives backward. Additional costs are incurred when the agent crosses the speed limit $v_{m a x}$ or gets too close to any nearby vehicles. The ego car is deemed to close to a neighbouring vehicle if $\\exp{-b(c_{1}d_{x}^{2}+c_{2}d_{y}^{2})+b a}\\geq\\bar{0}.4$ , where $a=0.01$ , $b=30$ , $c_{1}=10$ , $c_{2}=2$ and $d_{x}$ and $d_{y}$ represent the distance of the nearby vehicle from the ego vehicle. ", "page_idx": 15}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Constrained RL The Constrained Markov Decision Process (CMDP) framework [4] has emerged as a valuable tool in safety-critical settings, offering a clear separation between task-related information (rewards) and safety-related information (costs). This separation simplifies environment specification and allows for easier transfer of cost functions across similar environments. While previous research has extensively addressed this problem [1, 35, 34, 16], these studies typically assume that the cost function is known. However, this assumption can be limiting when designing the cost function is complex or costly to evaluate. Consequently, there is a need to infer the cost function from external data in such cases. ", "page_idx": 15}, {"type": "text", "text": "Cost Inference Historically, cost inference has primarily depended on two types of data: (1) evaluator feedback and (2) constraint abiding demonstrations obtained from an expert. ", "page_idx": 16}, {"type": "text", "text": "Previous work on learning from feedback often relies on limiting assumptions such as deterministic transitions and smoothness [28, 32, 11] or linearity assumptions [5] on the cost function. Moreover, none of these approaches address the problem of actively querying humans to minimize the required feedback. Although this has been theoretically explored in [8], the proposed algorithm has not yet been adapted for complex environments. Additionally, these methods are typically limited to feedback on individual state-action pairs, which can be expensive when relying on human evaluators. Our RLSF approach addresses these limitations by reducing the amount of feedback required and avoiding assumptions about the cost function. ", "page_idx": 16}, {"type": "text", "text": "Inferring cost functions from constraint-satisfying expert demonstrations has gained significant attention recently [6, 33, 22]. However, acquiring such expert demonstrations can be challenging in many scenarios. An alternative form of feedback is human interventions. For instance, [23] rely on the human evaluator to terminate the episode when the agent is about to engage in unsafe behavior, while [25] depend on the human to take control of the agent in such situations and guide it to a safe state. ", "page_idx": 16}, {"type": "text", "text": "Novelty-Based Sampling Novelty estimation has been widely explored in the context of active exploration in reinforcement learning. The connection between prediction accuracy and state novelty has been extensively studied, as highlighted by [9]. Additionally, state density estimation has been investigated in works such as [29, 7, 13], where infrequently visited states are assigned an exploration bonus. Notably, [29] also uses Simhash [10] to maintain state density estimates. ", "page_idx": 16}, {"type": "text", "text": "Reward Inference This work also connects to the analogous problem of reward inference in standard MDPs, which similarly rely on human feedback [12, 18, 20] and expert demonstrations [15, 14]. ", "page_idx": 16}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conducted the experiments on a cluster quipped with 4 NVIDIA RTX A5000 GPUs and 96 core CPUs. The experiments took an approximate of two weeks to run. The detailed hyperparameters utilized in the experiments are presented in Table 3. ", "page_idx": 16}, {"type": "table", "img_path": "WSsht66fbC/tmp/0aaa507aaf68820d7efc52388f000139179ae8a434b80f4e323a6034cc2b7126.jpg", "table_caption": ["Table 3: Hyper Parameters "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Main Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The training curves for the benchmark experiments and the Driver environment are presented in Figure 7 and Figure 8. Additionally information on the number of queries presented for feedback is presented in Table 4. ", "page_idx": 16}, {"type": "text", "text": "D.3 Novelty based Sampling ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 6 shows the implicit decreasing schedule encountered when using novelty based sampling in various environments. Figure 9 shows additional comparison with other querying mechanisms in the Biased Pendulum environment. To further assess the effectiveness of novelty based sampling, we categorized the trajectories collected during a data collection round based on novelty. We then evaluated the model\u2019s accuracy on the pseudo-prediction task for these two trajectory types, as ", "page_idx": 16}, {"type": "image", "img_path": "WSsht66fbC/tmp/6907f145a59c83630e07b06fab7699c496c33ff98259f608b443a18befb72e75.jpg", "img_caption": ["Figure 7: Training curves depicting the performance of different algorithms. The curves portray the mean returns, with shaded regions indicating the standard error. Parallel lines represents the best-performing run. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "WSsht66fbC/tmp/8b7e81be9606e8538f5c25bc5bfb1defd00c0471c271e91b04d98ba31f171ccf.jpg", "table_caption": ["Table 4: Number of trajectories presented to the evaluator for feedback "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "WSsht66fbC/tmp/fc798283467ab0702338782107f97896f2b85b3fa9364196e6f93f0378a2012b.jpg", "img_caption": ["Figure 8: Performance of algorithms in different driving scenarios. Each algorithm is run for 6 independent seeds, curves represent the mean and shaded regions represent the standard error. Normalized return is given by the formula $\\begin{array}{r}{r=\\frac{r-r_{\\mathrm{PPO}}}{r-r_{\\mathrm{random}}}}\\end{array}$ where $r_{\\mathrm{PPO}}$ is the return of PPO and rrandom is the return of a random policy. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "WSsht66fbC/tmp/875bfe550bd0410c3b1e2f30da4edaa03f023d9801d65830d962acfe852c9bc0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 9: Comparison of different sampling and scheduling schemes. The results are averaged over 3 independent seeds. The proposed sampling method generates on average 3600 queries, hence for fair comparison the other methods were given a budget of 4000 queries. ", "page_idx": 19}, {"type": "image", "img_path": "WSsht66fbC/tmp/296101cc9b23527b927741a635f99df3143d7bf8c7bf68f78e9fa9a926e8e026.jpg", "img_caption": ["Figure 10: Model prediction accuracy (mean $\\pm$ standard error) in the surrogate task (averaged over 3 seeds) with trajectory level feedback. Predictions are made on the next 50k steps following $250\\mathbf{k}$ training steps. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "shown in Figure 10. This analysis indicates that the model\u2019s accuracy is lower for novel trajectories, corroborating the claims made in Section 4.3. ", "page_idx": 19}, {"type": "text", "text": "D.4 Number of Queries generated ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The number of trajectories queried for feedback is of the order of $\\mathcal{O}(1e^{3})$ , the exact number differs across environments and individual runs due to novelty based sampling. Detailed information on the number of queries can be found in Table 4. We provide the two baseline methods with an advantage, whereby feedback is obtained for every trajectory generated by the policy, which is of the order $\\bar{\\mathcal{O}(1e^{4})}$ . ", "page_idx": 19}, {"type": "text", "text": "D.5 Complexity of Constraint Inference ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 11 highlights the difficulty of inferring constraints when cost violations are sparse. The brief interaction periods with obstacles in the environment make it challenging to assign credit to unsafe states when feedback is collected over long horizons. Consequently, feedback was collected at the state level in such settings. This is illustrated in Figure 12, where the overestimation bias is considerably ", "page_idx": 19}, {"type": "image", "img_path": "WSsht66fbC/tmp/bf98632924e40a65949cc35eccdbd54df330d07596b13aed8122069434f7e28b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Implicit decreasing schedule observed when following novelty based sampling across different environments. The results are averaged over 6 independent seeds. ", "page_idx": 19}, {"type": "text", "text": "higher in the Point Goal environment compared to Point Circle, resulting in a significant drop in performance. ", "page_idx": 19}, {"type": "image", "img_path": "WSsht66fbC/tmp/4e946e437bc0234e297b048c70588d9f2d4e147ba59d1f22f9bdd505f0579bc9.jpg", "img_caption": ["Figure 11: Costs incurred by a policy after 500K and 20M training interactions over a randomly sampled trajectory in the Point Goal environment. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.6 Ablation on the size of the feedback buffer ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conducted an ablation study on the size of the feedback buffer, the results of which are presented in Figure 13. We can observe that the policy becomes overly conservative when the size of the feedback buffer is small (100k, which corresponds to approximately two data collection rounds). We hypothesize that this is because of two interacting factors: (a) discarding past feedback can result in forgetting, which in turn leads to inaccuracies in estimating unsafe regions, and (b) there exists a feedback loop in which the inferred cost function influences on-policy data collection. If the inferred cost function overestimates costs in certain regions, the policy will avoid them and fail to gather the necessary feedback to correct this error. Conversely, if the costs are underestimated, the policy will visit those states and receive feedback, allowing for corrections. Therefore, smaller feedback buffer sizes can result in the learning of overly conservative policies. ", "page_idx": 20}, {"type": "text", "text": "D.7 Need for the surrogate loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 4.2, we highlighted the challenges associated with minimizing Eq 3 particularly due to the term $\\begin{array}{r}{\\prod_{t=i}^{j}p^{s a f e}(s_{t},a_{t})}\\end{array}$ which tends to collapse to 0 with longer segment lengths, resulting in unstable gradients. To investigate this further, we conducted a simple experiment in which we performed a few gradient steps to optimize $\\operatorname{Eq}3$ and plotted the gradient norm. The results, shown in Figure 14 reveal that (a) directly optimizing Eq 3 produces gradients with a large norm that either quickly vanish or explode, rendering the learning process infeasible. In contrast, the surrogate objective in Eq 4 yields stable gradients allowing for convergence. ", "page_idx": 20}, {"type": "image", "img_path": "WSsht66fbC/tmp/df28393fbb706f3c05a717c65230ad170d41ea8e7957a3ca8fa48087f75f8072.jpg", "img_caption": ["Figure 12: Comparison of state level and trajectory level feedback elicitation. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "WSsht66fbC/tmp/e23baa82b61b6f77c50b58686960d497e7f5de5ddfc543ce42436e28853a3615.jpg", "img_caption": ["Figure 13: Impact of the size of the feedback buffer on performance. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "WSsht66fbC/tmp/bd5c99641afb4d486fce8239c507167fb57db05a7441c8c7abdb1e2849dc1ce0.jpg", "img_caption": ["Figure 14: Norm of the gradient when optimising the MLE loss v/s the proposed surrogate loss with trajectory level feedback. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction are backed by theoretical and empirical results shown in Section 4.2 and 5 respectively. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The limitations of the proposed approach are discussed in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the assumptions are discussed in Sections 3 and 4 and the complete proof can be found in the Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Apart from the Safety Gymansium environments that are publicly available, all other environments are described in detail so that they can be reproduced from scratch. The hyper parameters used to train the proposed approach are given in Appendix D.1 to ensure reproducibility. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the environments used are described in Section B. The code (with instructions on how to run) is publicly available: https://github.com/shshnkreddy/RLSF ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The hyperparameters used to run our propsed approach is present in Section D.1 in the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All results presented in the paper are averaged over multiple seeds and are presented with the standard error. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Details on the compute and time taken to produce the results is available in Section D.1 in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The presented research is compliant with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The social impacts of our work is discussed in Section 6. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The work presented in this paper has no risk of misuse. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The README flie present in the code credits the authors, and the license and terms of use of has been properly respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The Safety Driver environments are described in Section B in the Appendix and instructions to run them are present in the README file included with the code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not conduct research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not conduct research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}]