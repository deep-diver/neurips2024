[{"type": "text", "text": "Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Khai Nguyen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nhat Ho ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Sciences The University of Texas at Austin Austin, TX 78712 khainb@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Sciences The University of Texas at Austin Austin, TX 78712 minhnhat@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator, i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators, i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport [55, 47] is a powerful mathematical tool for machine learning, statistics, and data sciences. As an example, Wasserstein distance [47], defined as the optimal transportation cost between two distributions, has been used successfully in many areas of machine learning and statistics, such as generative modeling on images [2, 53], representation learning [35], vocabulary learning [57], and so on. Despite being accepted as an effective distance, Wasserstein distance has been widely known as a computationally expensive distance. In particular, when comparing two distributions that have at most $n$ supports, the time complexity and the memory complexity of the Wasserstein distance scale with the order of $O(n^{3}\\log{n})$ [45] and ${\\mathcal{O}}(n^{2})$ respectively. In addition, the Wasserstein distance requires more samples to approximate a continuous distribution with its empirical distribution in high dimension since its sample complexity is of the order of $O(n^{-1/d})$ [20], where $n$ is the sample size and $d$ is the number of dimensions. Therefore, Wasserstein distance is not statistically and computationally scalable, especially in high dimensions. ", "page_idx": 0}, {"type": "text", "text": "Along with entropic regularization [17] which can reduce the time complexity and memory complexity of computing optimal transport to $\\bar{\\mathcal{O}}(n^{2})$ and $O(n^{2})$ in turn, sliced Wasserstein (SW) distance [11] is one alternative approach for the original Wasserstein distance. The key benefti of the SW distance is that it scales the order $\\mathcal{O}(n\\log n)$ and $O(n)$ in terms of time and memory respectively. The reason behind that fast computation is the closed-form solution of optimal transport in one dimension. To leverage that closed-form, sliced Wasserstein utilizes Radon Transform [23] (RT) to transform a high-dimensional distribution to its one-dimensional projected distributions, then the final distance is calculated as the average of all one-dimensional Wasserstein distance. By doing that, the SW distance has a very fast sample complexity i.e., $O(n^{-1/2})$ , which makes it computationally and statistically scalable in any dimension. Therefore, the SW distance has been applied successfully in various domains of applications including generative models [19], domain adaptation [32], clustering [27], 3D shapes [30, 29], gradient flows [34, 8], Bayesian inference computation [37, 58], texture synthesis [22], and many other tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite being useful, the SW distance is not as flexible as the Wasserstein distance in terms of choosing the ground metric. In greater detail, the number of ground metrics in one dimension is limited, especially ground metrics that lead to the closed-form solution. As a result, the role of capturing the structure of distributions belongs to the slicing/projecting operators. To generalize RT to non-linear projection, generalized Radon Transform (GRT) is introduced in [3] with circular projection [28], polynomial projection [51], and so on. With GRT, Generalized Sliced Wasserstein (GSW) distance is proposed in [26]. In addition, there is a line of works on developing sliced Wasserstein variants on different manifolds such as hyper-sphere [6, 54, 49, 50], hyperbolic manifolds [7], the manifold of symmetric positive definite matrices [10], general manifolds and graphs [52]. In those works, special variants of GRT are proposed. ", "page_idx": 1}, {"type": "text", "text": "Although the SW has become more effective on multiple domains, no SW variant is designed specifically for heterogeneous joint distributions i.e., joint distributions that have marginals supported on different domains, except for the product of Hadamard manifolds [9]. It is worth noting that marginal domains of heterogeneous joint distributions can be any metric space and are not necessary manifolds. Heterogeneous joint distributions appear in many applications, e.g., domain adaptation domains [15, 4], comparing datasets with labels [1], 3D shape deformation [29], and so on. In this case, Wasserstein distance can be adapted by using a mixed ground metric, i.e., a weighted sum of metrics on domains [15, 1]. In contrast to the Wasserstein distance, the adaptation of SW has not been well-investigated. Using GSW directly with one type defining function for all marginals cannot separate the information within and among groups of arguments. ", "page_idx": 1}, {"type": "text", "text": "Contribution: In this work, we tackle the challenge of designing a sliced Wasserstein variant for heterogeneous joint distributions. In summary, our main contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "1. We first extend the partial Radon Transform to the partial generalized Radon Transform (PGRT) to inject non-linearity into local transformation. We discuss the injectivity of PGRT for some choices of defining functions. We then propose a novel slicing operator for heterogeneous joint distributions, named Hierarchical Hybrid Radon Transform (HHRT). In particular, HHRT is a hierarchical transformation that first applies partial generalized Radon Transform with different defining functions on arguments of each marginal to gather marginal information, then applies partial Radon Transform on the joint transformed arguments to gather information among marginals. We show that HHRT is injective as long as the partial generalized Radon Transform is injective. ", "page_idx": 1}, {"type": "text", "text": "2. We propose Hierarchical Hybrid Sliced Wasserstein (H2SW) which is a novel metric for comparing heterogeneous joint distributions by utilizing the HHRT. Moreover, we investigate the topological properties, statistical properties, and computational properties of H2SW. In particular, we show that H2SW is a valid metric on the space of distribution over the joint space, H2SW does not suffer from the curse of dimensionality and enjoys the same computational scalability as SW distance. ", "page_idx": 1}, {"type": "text", "text": "3. A 3D mesh can be effectively represented by a point-cloud and corresponding surface normal vectors. Therefore, it can be seen as an empirical heterogeneous joint distribution. We conduct experiments on optimization-based 3D mesh deformation and deep 3D mesh autoencoder to show the favorable performance of H2SW compared to SW and GSW. Moreover, we also illustrate that H2SW can also provide a meaningful comparison for probability distributions on the product of Hadamard manifolds by conducting experiments on dataset comparison. ", "page_idx": 1}, {"type": "text", "text": "Organization. We first provide some preliminaries on SW distance, GSW distance, and joint Wasserstein distance in Section 2. We then define the hierarchical hybrid Radon transform and hierarchical hybrid sliced Wasserstein distance s in Section 3. Section 4 contains experiments on 3D mesh deformation, deep 3D mesh autoencoder, and datasets comparison. We conclude the paper in Section 5. Finally, we defer the proofs of key results, and additional materials in the Appendices. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Wasserstein distance. For $p\\geq1$ , the Wasserstein- $\\boldsymbol{p}$ distance [55, 47] between two distributions $\\mu\\,\\in\\,{\\mathcal{P}}({\\mathcal{X}})$ and $\\nu\\,\\in\\,\\mathcal{P}(\\mathcal{Y})$ , where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are subsets of $\\mathbb{R}^{d}$ and they share a ground metric $c:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{+}$ , is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{W}_{p}^{p}(\\mu,\\nu;c):=\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int_{\\chi\\times y}c(x,y)^{p}d\\pi(x,y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi(\\mu,\\nu)\\,:=\\,\\Bigl\\{\\pi\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\}|\\int_{\\mathcal{Y}}d\\pi(x,y)=\\mu(x),\\int_{\\mathcal{X}}d\\pi(x,y)=\\nu(y)\\Bigr\\}$ . When $\\mu$ and $\\nu$ are discrete with at most $n$ supports, the time complexity and the space complexity of the Wasserstein distance is $O(n^{3}\\log{n})$ and $\\bar{O(n^{2})}$ in turn which are very expensive. Therefore, sliced Wasserstein is proposed as an alternative solution. We first review the ", "page_idx": 2}, {"type": "text", "text": "Radon Transform [23] The Radon Transform $\\mathcal{R}:\\mathbb{L}_{1}(\\mathbb{R}^{d})\\to\\mathbb{L}_{1}\\left(\\mathbb{R}\\times\\mathbb{S}^{d-1}\\right)$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathcal{R}f)(t,\\theta)=\\int_{\\mathbb{R}^{d}}f(x)\\delta(t-\\langle x,\\theta\\rangle)d x.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Radon Transform defines a linear bijection [23]. Given a projecting direction $\\theta$ , $(\\mathcal{R}f)(\\cdot,\\theta)$ is an one-dimensional function. With Radon Transform, we can now define the sliced Wasserstein distance. ", "page_idx": 2}, {"type": "text", "text": "Sliced Wasserstein distance. For $p~\\geq~1$ , the Sliced Wasserstein (SW) distance [11] of $p$ -th order between two distributions $\\mu\\in{\\mathcal{P}}({\\mathcal{X}})$ and $\\nu\\in\\mathcal{P}(\\mathcal{y})$ with an one-dimensional ground metric $c:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}^{+}$ is defined as follow: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{SW}_{p}^{p}(\\mu,\\nu;c)=\\mathbb{E}_{\\theta\\sim\\mathcal{U}(\\mathbb{S}^{d-1})}[\\mathbf{W}_{p}^{p}(\\mathcal{R}_{\\theta}\\sharp\\mu,\\mathcal{R}_{\\theta}\\sharp\\nu;c)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{R}_{\\theta}\\sharp\\mu$ and $\\mathcal{R}_{\\theta}\\sharp\\nu$ are the one-dimensional push-forward distributions created by applying Radon Transform (RT) [23] on the pdf of $\\mu$ and $\\nu$ with the projecting direction $\\theta$ . The computational benefti of SW distance comes from the closed-form solution when the one-dimensional ground metric $c(x,y)=h(x-y)$ for $h$ is a strictly convex function: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{W}_{p}^{p}(\\mathcal{R}_{\\theta}\\sharp\\mu,\\mathcal{R}_{\\theta}\\sharp\\nu;c)=\\int_{0}^{1}c\\left(F_{\\mathcal{R}_{\\theta}\\sharp\\mu}^{-1}(z),F_{\\mathcal{R}_{\\theta}\\sharp\\nu}^{-1}(z)\\right)^{p}d z,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F_{\\mathcal{R}_{\\theta}\\sharp\\mu}^{-1}$ and $F_{\\mathcal{R}_{\\theta}\\sharp\\nu}^{-1}$ are inverse CDF of $\\mathcal{R}_{\\theta}\\sharp\\mu$ and $\\mathcal{R}_{\\theta}\\sharp\\nu$ respectively. When $\\mu$ and $\\nu$ are discrete with at most $n$ supports, the time complexity and the space complexity of the closed-form is $\\mathcal{O}(n\\log n)$ and ${\\mathcal{O}}(n)$ respectively. ", "page_idx": 2}, {"type": "text", "text": "Generalized Radon Transform and Generalized Sliced Wasserstein distance. To generalize RT to non-linear operator, the Generalized Radon Transform $(G R T)$ was proposed [3]. Given a defining function [26] $\\dot{g}:\\mathbb R^{d}\\times\\Omega\\rightarrow\\mathbb R$ , the Generalized Radon Transform [3] $\\bar{\\mathcal{G}}\\bar{\\mathcal{R}}:\\mathbb{L}_{1}(\\bar{\\mathbb{R}}^{d})\\rightarrow\\mathbb{L}_{1}\\left(\\mathbb{R}\\times\\bar{\\Omega}\\right)$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathcal{G R f})(t,\\theta)=\\int_{\\mathbb{R}^{d}}f(x)\\delta(t-g(x,\\theta))d x.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For example, we can have the circular function [28], i.e., $g(x,\\theta)=\\|x-r\\theta\\|_{2}$ for $r\\in\\mathbb{R}^{+}$ and $\\theta\\in$ $\\Omega:=\\mathbb{S}^{d-1}$ , homogeneous polynomials with an odd degree [51] $(m)$ , i.e., $\\begin{array}{r}{g(x,\\theta)=\\sum_{|\\alpha|=m}\\theta_{\\alpha}x^{\\alpha}}\\end{array}$ with $\\begin{array}{r}{\\alpha=\\left(\\alpha_{1},\\ldots,\\alpha_{d_{\\alpha}}\\right)\\in\\mathbb{N}^{d_{\\alpha}},\\,|\\alpha|=\\sum_{i=1}^{d_{\\alpha}}\\alpha_{i},\\,x^{\\alpha}=\\prod_{i=1}^{d_{\\alpha}}x_{i}^{\\alpha_{i}},\\,\\Omega=\\mathbb{S}^{d_{\\alpha}},\\,\\mathrm{and}}\\end{array}$ d so on. Using GRT, the Generalized Sliced Wasserstein (G SW) distance is i ntroduced in [26], which is formally defined as follow : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{GSW}_{p}^{p}(\\mu,\\nu;c,g)=\\mathbb{E}_{\\theta\\sim\\mathcal{U}(\\mathbb{S}^{d-1})}[\\mathbf{W}_{p}^{p}(\\mathcal{G}\\mathcal{R}_{\\theta}^{g}\\sharp\\mu,\\mathcal{G}\\mathcal{R}_{\\theta}^{g}\\sharp\\nu;c)].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is worth noting that the injectivity of GRT is required to have the identity of indiscernible GSW. ", "page_idx": 2}, {"type": "text", "text": "Heterogeneous joint distributions comparison. We are given two joint distributions $\\mu(x_{1},x_{2})\\in$ $\\mathcal{P}(\\mathcal{X}_{1}\\times\\mathcal{X}_{2})$ and $\\bar{\\nu}(y_{1},y_{2})\\in\\mathcal{P}(\\mathcal{V}_{1}\\times\\mathcal{V}_{2})$ where $X_{1}$ are $Y_{1}$ share a ground metric $c_{1}:\\mathcal{X}_{1}\\times\\mathcal{Y}_{1}\\to\\mathbb{R}^{+}$ and $X_{2}$ are $Y_{2}$ share a ground metric $c_{2}:\\mathcal{X}_{2}\\times\\mathcal{Y}_{2}\\to\\mathbb{R}^{+}$ with $(c_{1}\\neq c_{2})$ ). In this case, previous works utilize the joint distribution Wasserstein distance [15, 1] to compare $\\mu$ and $\\nu$ : ", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{W}_{p}^{p}(\\mu,\\nu;c_{1},c_{2}):=\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int_{\\mathcal{X}_{1}\\times\\mathcal{X}_{2}\\times\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2}}(c_{1}(x_{1},y_{1})^{p}+c_{2}(x_{2},y_{2})^{p})d\\pi(x_{1},x_{2},y_{1},y_{2}),$ (5) where $\\begin{array}{r}{\\Pi(\\mu,\\nu):=\\left\\{\\pi\\in\\mathcal{P}(\\mathcal{X}_{1}\\times\\mathcal{X}_{2}\\times\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2})\\right\\}|\\int_{\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2}}d\\pi(x_{1},x_{2},y_{1},y_{2})=\\mu(x_{1},x_{2}),}\\end{array}$ $\\begin{array}{r}{\\int_{\\mathcal{X}_{1}\\times\\mathcal{X}_{2}}d\\pi(x_{1},x_{2},y_{1},y_{2})=\\nu(y_{1},y_{2})\\Big\\}}\\end{array}$ . We can easily extend the definition to joint distributions with more than two marginals (see Appendix B). In contrast to the Wasserstein distance, there is no variant of SW that is designed specifically for this case. SW variants can still be used by treating $\\mathcal{X}_{1}\\times\\mathcal{X}_{2}$ and $y_{1}\\!\\times\\!\\ y_{2}$ as homogeneous spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ which share the same Radon Transform variant and one-dimensional ground metric $c$ . However, that approach cannot differentiate the difference between $\\chi_{1}$ and $\\scriptstyle{\\mathcal{X}}_{2}$ , and leverage the hierarchical structure, i.e., inside and among marginals. ", "page_idx": 3}, {"type": "text", "text": "3 Hierarchical Hybrid Sliced Wasserstein Distance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose the Hierarchical Hybrid Radon Transform (HHRT) which first applies P(G)RT on each marginal argument to gather each marginal information, then applies PRT on the joint transformed arguments from all marginals to gather information among marginals. After that, we introduce Hierarchical Hybrid Sliced Wasserstein distance by using HHRT as the slicing operator. ", "page_idx": 3}, {"type": "text", "text": "3.1 Hierarchical Hybrid Radon Transform ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first introduce the first building block in HHRT, i.e., Partial Generalized Radon Transform (PGRT). ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Partial Generalized Radon Transform). Given a defining function $g:\\mathbb{R}^{d_{1}}\\times\\Omega\\rightarrow\\mathbb{R},$ , Partial Generalized Radon Transform $\\mathcal{P G R}:\\mathbb{L}_{1}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})\\rightarrow\\mathbb{\\tilde{L}}_{1}\\left(\\mathbb{\\tilde{R}}^{\\ast}\\times\\Omega\\times\\mathbb{R}^{d_{2}}\\right)$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathcal{P G R f})(t,\\theta,y)=\\int_{\\mathbb{R}^{d_{1}}}f(x,y)\\delta(t-g(x,\\theta))d x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $g(x,\\theta)=\\langle x,\\theta\\rangle$ , PGRT reverts into Partial Radon Transform (PRT) [33]. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. For some defining function $g$ such as linear, circular, and homogeneous polynomials with an odd degree; the Partial Generalized Radon Transform is injective, i.e., for any functions $f_{1},f_{2}\\in\\mathbb{L}^{1}(\\mathbb{R}^{d})$ , $(\\mathcal{P G R f}_{1})(t,\\theta,y)=(\\mathcal{P G R f}_{2})(t,\\theta,y)\\,\\forall t,\\theta,y$ implies $f_{1}=f_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof of Proposition 1 is given in Appendix A.1. The main idea to prove the injectivity of PGRT is to show that given a fixed $y$ , the PGRT is the GRT of $f(\\cdot,y)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Hierarchical Hybrid Radon Transform). Given defining functions $g_{1}:\\mathbb{R}^{d_{1}}\\times\\Omega_{1}\\rightarrow\\mathbb{R}$ and $g_{2}\\;:\\;\\mathbb{R}^{d_{2}}\\,\\times\\,\\Omega_{2}\\,\\rightarrow\\,\\mathbb{R},$ , Hierarchical Hybrid Radon Transform $\\mathcal{H H R}\\,:\\,\\mathbb{L}_{1}(\\mathbb{R}^{d_{1}}\\,\\times\\,\\mathbb{R}^{d_{2}})\\,\\rightarrow$ $\\mathbb{L}_{1}\\left(\\mathbb{R}\\times\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S}\\right)$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathcal{H H K f})(t,\\theta_{1},\\theta_{2},\\psi)=\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}f(x_{1},x_{2})\\delta\\left(t-\\psi_{1}g_{1}(x_{1},\\theta_{1})-\\psi_{2}g_{2}(x_{2},\\theta_{2})\\right)d x_{1}d x_{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\psi=(\\psi_{1},\\psi_{2})\\in\\mathbb{S}$ . ", "page_idx": 3}, {"type": "text", "text": "The reason for using PRT for the final transform is that the previous PGRTs are assumed to be able to transform the non-linear structure to a linear line. However, PGRT can still be used as a replacement for PRT. Definition 2 can be extended to more than two marginals (see Appendix B). ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. For some defining functions $g_{1},g_{2}$ such as linear, circular, and homogeneous polynomials with an odd degree; Hierarchical Hybrid Radon Transform is injective, i.e., for any functions $f_{1},f_{2}\\in\\mathbb{L}_{1}(\\mathbb{R}^{d}),$ , $(\\mathcal{H H R}f_{1})(t,\\theta_{1},\\theta_{2},\\psi)=(\\mathcal{H H R}f_{2})(t,\\theta_{1},\\theta_{2},\\psi)\\;\\forall t,\\theta_{1},\\theta_{2},\\psi$ implies $f_{1}=f_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof of Proposition 2 is given in Appendix A.2. The main idea to prove the injectivity of HHRT is to show that HHRT is the composition of PRT and multiple PGRTs. ", "page_idx": 3}, {"type": "image", "img_path": "XwrMd1njqq/tmp/c0ea4799882ca4a2826c8b4d78aa1b4176279cef5a4a21edca9aa38b3392e787.jpg", "img_caption": ["Figure 1: Generalized Radon Transform and Hierarchical Hybrid Radon Transform on a discrete distribution. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "HHRT of discrete distributions. We are given $\\begin{array}{r l r}{f(x)}&{{}=}&{\\sum_{i=1}^{n}\\alpha_{i}\\delta((x_{1},x_{2})\\;\\;-\\;}\\end{array}$ $f(x)$ is $\\begin{array}{r l}{(\\mathcal{H H R f})(t,\\theta_{1},\\theta_{2},\\psi)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{i=1}^{n}\\alpha_{i}\\dot{\\delta}\\left(t-\\psi_{1}g_{1}(x_{i1},\\theta_{1})-\\psi_{2}g_{2}(x_{i2},\\theta_{2})\\right)}\\end{array}$ . For $g_{1}$ and $g_{2}$ that are the linear function and (or) the circular function, the time complexity of the transform is $\\mathcal{O}(d_{1}+d_{2})$ which is the same as the complexity of using RT and GRT directly. However, HHRT has an additional constant complexity scaling linearly with the number of marginals, i.e., two marginals in Definition 2. ", "page_idx": 4}, {"type": "text", "text": "Example 1. In this paper, we focus on $3D$ shape data (mesh) with points and normals representation, i.e., shapes as points representation $[46]$ . In particular, we can transform a $3D$ shape into a set of points and normals by sampling from the surface of the mesh. In addition, we can convert back to the $3D$ shape from points and normals with Poisson surface reconstruction $I25J$ algorithm. In this setup, $a$ shape is represented by a $6$ -dimensional vector $\\boldsymbol{x}=\\left(\\boldsymbol{x}_{1},\\boldsymbol{x}_{2}\\right)$ where $x_{1}\\in\\mathcal{X}_{1}\\in\\mathbb{R}^{3}$ and $x_{2}\\in\\mathcal{X}_{2}\\in\\mathbb{S}^{2}$ . For the set $\\mathcal{X}_{1}\\,\\in\\,\\mathbb{R}^{3}$ , we can use directly the linear defining function $g_{1}(x_{1},\\theta_{1})\\,=\\,\\langle x_{1},\\theta_{1}\\rangle$ with $\\theta_{1}\\in\\mathbb{S}^{2}$ . For the set $\\ensuremath{\\mathcal{X}}_{2}\\in\\mathbb{S}^{2}$ , we can utilize the circular defining function $g_{2}(x_{2},\\theta_{2})=\\|x_{2}-r\\theta_{2}\\|_{2}$ with $r\\,\\in\\,\\mathbb{R}^{+}$ and $\\theta_{2}\\,\\in\\,\\mathbb{S}^{2}$ . As alternative options for $\\scriptstyle{\\mathcal{X}}_{2}$ , we can use other defining functions from special cases of GRT including Vertical Slice Transform $I^{49}J$ , Parallel Slice Transform $[5O J_{:}$ , Spherical Radon Transform $I6J,$ and Stereographic Spherical Radon Transform $I54J$ . ", "page_idx": 4}, {"type": "text", "text": "Inversion. In Proposition 2, we show that HHRT is the composition of PRT and multiple PGRTs. Therefore, the inversion of HHRT is the composition of the inversion of multiple PGRT (invertibility of PGRT depends on the choice of defining functions [3, 28]) and the inversion of PRT [23]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Hierarchical Hybrid Sliced Wasserstein Distance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By using HHRT, we obtain a novel variant of SW which is specifically designed for comparing heterogeneous joint distributions. ", "page_idx": 4}, {"type": "text", "text": "Definitions. We now define the Hierarchical Hybrid Sliced Wasserstein (H2SW) distance. ", "page_idx": 4}, {"type": "text", "text": "Definition 3. For $p\\geq1$ , defining functions $g_{1},g_{2}$ , the hierarchical hybrid sliced Wasserstein- $p$ (H2SW) distance between two distributions $\\mu\\in\\mathcal{P}(\\mathcal{X}_{1}\\times\\mathcal{X}_{2})$ and $\\nu\\in\\mathcal{P}(\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2})$ with an onedimensional ground metric $c:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}^{+}$ is defined as: ", "page_idx": 4}, {"type": "text", "text": "$\\begin{array}{r}{H2S W_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2})=\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})}[W_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)],}\\end{array}$ (8) where $\\mathcal{H H R}_{\\theta_{1},\\theta_{2},\\psi}\\sharp\\mu$ and $\\mathcal{H H R}_{\\theta_{1},\\theta_{2},\\psi}\\sharp\\nu$ are the one-dimensional push-forward distributions created by applying HHRT. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 can be easily extended to more than two marginals (see Appendix B) ", "page_idx": 4}, {"type": "text", "text": "Topological Properties. We first show that H2SW is a valid metric on the space of distributions on any sets $\\mathcal{X}\\times\\mathcal{Y}\\in\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}$ $(d_{1},d_{2}\\geq1)$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. For any $p\\geq1$ , ground metric c, and defining functions $g_{1},g_{2}$ which lead to the injectivity of GRT, the hierarchical hybrid sliced Wasserstein $H2S W_{p}(\\cdot,\\cdot;c,g_{1},g_{2})$ is a metric on $\\mathcal{P}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ i.e., it satisfies the symmetry, non-negativity, triangle inequality, and identity of indiscernible. ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 is given in Appendix A.3. It is worth noting that the identity of indiscernible property is proved by the injectivity of HHRT (Proposition 2). We now discuss the connection of H2SW to GSW and Wasserstein distance in some specific cases. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. For any $p\\geq1$ , $c(x,y)=|x-y|$ , and $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ , we have: $(i)\\,H2S W_{p}(\\mu,\\nu;c,g_{1},g_{2})\\leq G S W_{p}(\\mu_{1},\\nu_{1};c,g_{1})+G S W_{p}(\\mu_{2},\\nu_{2};c,g_{2})$ , where $\\mu_{1}(X)=\\mu(X\\!\\times\\!\\mathbb{R}^{d_{2}})$ and $\\mu_{2}(Y)=\\mu(\\mathbb{R}^{d_{1}}\\times Y)$ (similar with $\\nu_{1}$ and $\\nu_{2.}$ ). (ii) If $g_{1},\\,g_{2}$ are linear defining functions, $H2S W_{p}(\\mu,\\nu;c,g_{1},g_{2})\\leq W_{p}(\\mu_{1},\\nu_{1};c)+W_{p}(\\mu_{2},\\nu_{2};c).$ (iii) $I f p=1$ , $g_{1},\\,g_{2}$ are linear defining functions, $H2S W_{1}(\\mu,\\nu;c,g_{1},g_{2})\\leq W_{1}(\\mu,\\nu;c)$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 3 is given in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Sample Complexity. We now discuss the sample complexity of H2SW. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. For any $p\\geq1$ , dimension $d_{1},d_{2}\\geq1$ , $q>p$ , $c(x,y)=|x-y|$ , $g_{1},g_{2}$ are linear defining functions or circular defining functions, and $\\mu,\\nu\\in\\mathcal{P}_{q}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ with the corresponding empirical distributions $\\mu_{n}$ and $\\nu_{n}$ $n\\geq1,$ ), there exists a constant $C_{p,q}$ depending on $p,q$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left\\vert H2S W_{p}(\\mu_{n},\\nu_{n};c,g_{1},g_{2})-H2S W_{p}(\\mu,\\nu;c,g_{1},g_{2})\\right\\vert}}\\\\ &{}&{\\leq C_{p,q}^{\\frac{1}{p}}\\left(\\sum_{i=0}^{q}q^{i}C_{g_{1},g_{2}}^{q-i}(M_{i}(\\mu)+M_{i}(\\nu))\\right)^{\\frac{1}{p}}\\left\\{\\begin{array}{l}{n^{-1/2p}\\;i f\\,q>2p,}\\\\ {n^{-1/2p}\\log(n)^{\\frac{1}{p}}\\;i f q=2p,}\\\\ {n^{-(q-p)/p q}\\;i f\\,q\\in(p,2p),}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M_{q}(\\mu)$ and $M_{q}(\\nu)$ are the $q$ -th moments of $\\mu$ and $\\nu$ , $C_{g_{1},g_{2}}$ is a constant depends on $g_{1},\\,g_{2}$ ", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 4 is given in Appendix A.5. The rate in Proposition 4 is as good as the rate of SW in [38], however, it is slightly worse than the rate of SW in [44, 36, 5] due to the usage of the circular defining functions and simpler assumptions. To the best of our knowledge, the sample complexity of GSW has not been investigated. ", "page_idx": 5}, {"type": "text", "text": "Monte Carlo Estimation. Since the expectation in H2SW (Equation 8) is intractable, Monte Carlo estimation and Quasi-Monte Carlo approximation [39] can be used to form a practical evaluation of H2SW. Here, we utilize Monte Carlo estimation for simplicity. In particular, we sample $\\theta_{11},\\ldots,\\theta_{1L}\\stackrel{i.i.d}{\\sim}\\mathcal{U}(\\Omega_{1}),\\,\\theta_{21},\\ldots,\\theta_{2L}\\stackrel{i.i.d}{\\sim}\\mathcal{U}(\\Omega_{2})$ , and $\\psi_{1},\\ldots,\\psi_{L}\\stackrel{i.i.d}{\\sim}\\mathcal{U}(\\mathbb{S})$ . After that, we form the following estimation of H2SW: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{H2SW}}_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2},L)=\\frac{1}{L}\\sum_{l=1}^{L}\\mathrm{W}_{p}^{p}(\\mathcal{H H R}_{\\theta_{1l},\\theta_{2l},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H R}_{\\theta_{1l},\\theta_{2l},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\nu;c).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 5. For any $p\\geq1$ , dimension $d_{1},d_{2}\\geq1$ , and $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}_{1}^{d}\\times\\mathbb{R}^{d_{2}})$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|\\widehat{H2S W}_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2},L)-H2S W_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2})|}\\\\ &{\\qquad\\le\\cfrac{1}{\\sqrt{L}}V a r\\left[W_{p}^{p}(\\mathcal{H H R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)\\right]^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the variance is with respect to $\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of Proposition 5 is given in Appendix A.6. From the proposition, we see that the estimation error of H2SW is the same as SW which is $\\mathcal{O}(L^{-1/2})$ . ", "page_idx": 5}, {"type": "text", "text": "Computational Complexities. The time complexity and memory complexity of H2SW with linear and circular defining functions are $\\mathcal{O}(L n\\log n+L(d_{1}+d_{2}+k)n)$ and $\\mathcal{O}(L n+(d_{1}+d_{2}+k)n)$ with $k$ is the number of marginals i.e., 2. We can see that the complexities of H2SW are the same as those of SW in terms of the number of supports $n$ and the number of dimensions $d$ . We demonstrate the process of HHRT compared to GRT on a discrete distribution with $L$ realization of $\\theta_{1},\\theta_{2},\\psi$ in Figure 1. Overall, the complexities of defining functions are often different in the number of dimensions, hence, H2SW is always scaled the same as SW in the number of supports i.e., $\\mathcal{O}(n\\log n)$ . ", "page_idx": 5}, {"type": "text", "text": "Gradient Estimation. In applications, it is desirable to estimate the gradient $\\nabla_{\\phi}\\mathrm{H}2\\mathrm{SW}_{p}^{p}(\\mu_{\\phi},\\nu;c,g_{1},g_{2})$ . We can move the gradient operator to inside the expectation and then apply Monte Carlo estimation. The gradient $\\nabla_{\\phi}\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu_{\\phi},\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)]$ can be computed easily since the functions $g_{1},g_{2}$ are usually differentiable. ", "page_idx": 5}, {"type": "table", "img_path": "XwrMd1njqq/tmp/3f03683f36a3981360c23d79c69b49f736647e7e23df854b8e4aac98d2e5258f.jpg", "table_caption": ["Table 1: Summary of joint Wasserstein distances across time steps from deformation from the sphere mesh to the Armadillo mesh. "], "table_footnote": ["Table 2: Summary of joint Wasserstein distances (multiplied by 100) across time steps from deformation from the sphere mesh to the Stanford Bunny mesh. "], "page_idx": 6}, {"type": "table", "img_path": "XwrMd1njqq/tmp/0aecf248bdb4f1a07eb0f1a5172b062ff99700b7f12a63ac6f7886fb9722f9c6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Beyond uniform slicing distribution. H2SW is defined with the uniform slicing distribution in Definition 3, however, it is possible to extend it to other slicing distributions such as the maximal projecting direction [18], distributional slicing distribution [42], and energy-based slicing distribution [41]. Since the choice of slicing distribution is independent of the main contribution i.e., the slicing operator, we leave this investigation to future work. ", "page_idx": 6}, {"type": "text", "text": "H2SW for distributions on the product of Hadamard manifolds. A recent work [9] extends sliced Wasserstein on hyperbolic manifolds [7] and on the manifold of symmetric positive definite matrices [10] to Hadamard manifolds i.e., manifold non-positive curvature. The work discusses the extension of SW to the product of Hadamard manifolds. For the geodesic projection, the closed-form for the projection is intractable. For the Busemann projection, the Busemann projection on the product manifolds is the weighted sum of the Busemann projection with the weights belonging to the unit-sphere. In the work, the weights are a fixed hyperparameter i.e., Cartan-Hadamard Sliced-Wasserstein (CHSW) utilizes only one Busemann function to project the joint distribution. In contrast, H2SW utilizes the Radon Transform on the joint spaces of projections i.e., considering all distributed weighted combinations which is equivalent to considering all Busemann functions under a probability law. As a result, the H2SW is a valid metric as long as the Busemann projections can be proven to be injective (the injectivity of the Busemann projection has not been known at the moment) while Cartan-Hadamard Sliced-Wasserstein is only pseudo metric since the injectivity of a fixed weighted combination is not trivial to show. Moreover, H2SW does not only focus on the product of Hadamard manifolds i.e., H2SW is a generic distance for heterogeneous joint distributions in which marginal domains are not necessary manifolds e.g., images [40], functions [21], and so on. In the later experiments, we conduct experiments on comparing 3D shapes which are represented by a distribution on the product of the Euclidean space and the 2D sphere (not a Hadamard manifold). ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first compare the performance of the proposed H2SW with SW and GSW in the 3D mesh deformation application. After that, we further evaluate the performance of H2SW in training a deep 3D mesh autoencoder compared to SW and GSW. Finally, we compare H2SW with SW and Cartan-Hadamard Sliced-Wasserstein (CHSW) in datasets comparison on the product of Hadamard manifolds. In the experiments, we use $c(x,y)=|x-y|$ and $p=2$ for all SW variants. ", "page_idx": 6}, {"type": "text", "text": "4.1 3D Mesh Deformation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this task, we would like to move from a source mesh to a target mesh. To represent those meshes, we sample 10000 points by Poisson disk sampling and their corresponding normal vectors of the mesh surface at those points. Let the source mesh be denoted as $X(0)=\\{x_{1}(0),\\ldots,x_{n}(0)\\}$ and the target mesh be denoted as $Y=\\{y_{1},\\ldots,y_{n}\\}$ . We deform $X(0)$ to $Y$ by integrating the ordinary differential equation ${\\dot{X}}(t)=-n\\nabla_{X(t)}$ $\\begin{array}{r}{\\left[S\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\delta(x-x_{i}(t)),\\frac{1}{n}\\sum_{i=1}^{n}\\delta(y-y_{i})\\right)\\right]}\\end{array}$ , where $\\boldsymbol{S}$ denotes a SW variant. We utilize the Euler discretization scheme with step size 0.01 and 5000 steps. The normal vectors are projected back to the sphere after taking an Euler step. For evaluation, we use the joint Wasserstein distance in Equation 5 with the mixed distance from the Euclidean distance and the great circle distance. We use the circular defining function for GSW, and use the linear defining function and the circular defining function for H2SW. We vary the number of projections $L\\in\\{10,100\\}$ for all variants. For H2SW and GSW, we select the best hyperparameter of the circular defining function $r\\in\\{0.5,0.7,0.8,0.9,1,5,10,50,100\\}$ . ", "page_idx": 6}, {"type": "image", "img_path": "XwrMd1njqq/tmp/2f045bbf0fdded80ef22dedc130cb1f83173e3fa1fa1fe62305b76f42c9c2c3c.jpg", "img_caption": ["Figure 2: Visualization of deformation from the sphere mesh to the Armadillo mesh with $L=10$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XwrMd1njqq/tmp/3624350b76e6c9e93e01903b30c98e9900f4aa159d0fa2a47b5fddd976e06208.jpg", "img_caption": ["Figure 3: Visualization of deformation from the sphere mesh to the Stanford Bunny mesh with $L=10$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results. We compare H2SW with GSW and SW by deforming the sphere mesh to the Armadillo mesh [59]. We report the quantitative results in Table 1 after 3 independent runs and the qualitative result for $L=10$ in Figure 2 and $L=100$ in Figure 6 in Appendix D. From Table 1, we observe that H2SW helps the deformation convergence faster at the beginning and better at the end in terms of the joint Wasserstein distance, especially for a small value of the number of projections i.e., $L=10$ . The result for $L=100$ is better than $L=10$ which is consistent with Proposition 5. The qualitative results in Figure 2 and Figure 6 also reinforce the favorable performance of H2SW since they are visually consistent with quantitative scores. We also conduct deformation to the Stanford Bunny mesh [16, 59] in Table 2, Figure 3, and Figure 7 in Appendix D and we observe the same phenomenon that H2SW is the best variant for 3D meshes. From those experiments, H2SW has shown the benefti of the HHRT in transforming a joint distribution over the product of the Euclidean space and the 2D sphere compared to the conventional RT of SW and the conventional GRT of GSW. ", "page_idx": 7}, {"type": "text", "text": "4.2 Training deep 3D mesh autoencoder ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We utilize the processed ShapeNet dataset [12] from [46], then sample 2048 points and the corresponding normal vectors from each shape in the dataset. Formally, we would like to train an autoencoder that contains an encoder $f_{\\phi}$ that maps a mesh $X\\in\\mathbb{R}^{2048\\times6}$ to a latent code $z\\in\\mathbb{R}^{1024}$ , and a decoder $g_{\\psi}$ that maps the latent code $z$ back to the reconstructed mesh $\\tilde{X}\\in\\mathbb{R}^{2048\\times6}$ . We adopt ", "page_idx": 7}, {"type": "table", "img_path": "XwrMd1njqq/tmp/30793e74749fbcc25ee5ff548d2f50bee8369e38e14137290d52649f2cf78c7e.jpg", "table_caption": ["Table 3: Joint Wasserstein distance reconstruction errors (multiplied by 100) from three different runs of autoencoders trained by SW, GSW, and H2SW with the number of projections $L=100$ and $L=1000$ . "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "XwrMd1njqq/tmp/bfc6f2235b25fdc51ba68f3f6818720491a0ad3c6bb80ceba66dc2dbcc4dda98.jpg", "img_caption": ["Figure 4: Visualization of some randomly selected reconstruction meshes from autoencoders trained by SW, GSW, and H2SW in turn with the number of projections $L=100$ at epoch 2000. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Point-Net [48] architecture to construct the autoencoder. We want to train the encoder $f_{\\phi}$ and the decoder $g_{\\psi}$ such that $\\tilde{X}=g_{\\psi}(f_{\\phi}(X))\\approx X$ for all shapes $X$ in the dataset. To do that, we solve the following optimization problem: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi,\\gamma}\\mathbb{E}_{X\\sim\\mu(X)}[S(P_{X},P_{g_{\\gamma}(f_{\\phi}(X)))}],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\boldsymbol{S}$ is a sliced Wasserstein variant, and $\\begin{array}{r}{P_{X}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta(x-x_{i})}\\end{array}$ denotes the empirical distribution over the point cloud $X=(x_{1},\\ldots,x_{n})\\,$ . We train the autoencoder for 2000 epochs on the training set of the ShapeNet dataset using an SGD optimizer with a learning rate of $1e-3$ , and a batch size of 128. For evaluation, we also use the joint Wasserstein distance in Equation 5 with the mixed distance from the Euclidean distance and the great circle distance to measure the average reconstruction loss on the testing set of the ShapeNet dataset. We use the circular defining function for GSW, and use the linear defining function and the circular defining function for H2SW. For H2SW and GSW, we select the best hyperparameter of the circular defining function $r\\in\\{0.5,0.7,0.8,0.9,1,5,10\\}$ . For more details such as the neural network architectures, we refer the reader to Appendix B. ", "page_idx": 8}, {"type": "text", "text": "Results. We report the joint Wasserstein reconstruction errors (measured in three independent times) on the testing set in Table 3 with trained autoencoder at epoch 500, 1000, and 2000 from SW, GSW, and H2SW with the number of projections $L=100$ and $L=1000$ . In addition, we show some randomly reconstructed meshes for epoch 2000in Figure 4 and for epoch 500 in Figure 8 in Appendix D. From Table 3, we observe that H2SW yields the lowest reconstruction errors for both $\\bar{L}\\,=\\,100$ and $L\\,=\\,1000$ . Moreover, we see that the reconstruction errors are lower with $L=1000$ than ones with $L=100$ for all SW variants. The qualitative reconstructed meshes in Figure 4 and Figure 8reflect the same relative comparison. It is worth noting that both the qualitative and the qualitative performance of autoencoders can be improved by using more powerful neural networks. Since we focus on comparing SW, GSW, and H2SW, we only use a light neural network i.e., Point-Net [48] architecture. The trained autoencoders can be further used to reduce the size of 3D meshes for data compression and for dimension reduction, however, such downstream applications are not our focus in the current investigation of the paper. ", "page_idx": 8}, {"type": "table", "img_path": "XwrMd1njqq/tmp/4b6e1064eed13ec7d2e57874177a45a7bfd8c76e6ca74bc9b2c2bc0cede45fd4.jpg", "table_caption": ["Table 4: Relative error to the joint Wasserstein distance of SW, CHSW, and H2SW. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "XwrMd1njqq/tmp/ca1f26da04f88a7581f06894bd8739d16b0450f05e812d6fc62c106a7e4c25bc.jpg", "img_caption": ["Figure 5: Cost matrices between datasets from SW, CHSW, and H2SW with $L=2000$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Comparing Datasets on The Product of Hadamard Manifolds ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We follow the same experimental setting from [9]. Here, we have datasets as sets of feature-label pairs which are embedded in the space of $\\mathbb{R}^{d_{1}}\\stackrel{\\cdot\\ }{\\times}\\mathbb{L}^{d_{2}}$ where $\\mathbb{L}^{d_{2}}$ denotes a Lorentz model of $d_{2}$ dimension (a hyperbolic space). We uses MNIST [31] dataset, EMNIST dataset [14], Fashion MNIST dataset [56], KMNIST dataset [13], and USPS dataset [24]. For CHSW, we use Busemann projection on the product space of Euclidean and the Lorentz model. For H2SW, we use the linear defining function and the Busemann function on the Lorentz model. We refer the reader to Appendix B for greater detail on Busemann functions and experimental setups. We compare SW, CHSW, and H2SW by varying $L\\in\\{100,500,1000,2000\\}$ . For evaluation, we use the joint Wasserstein distance in [1] as the ground truth. In particular, let $C_{W}$ be the cost matrix from the joint Wasserstein distance and $C$ be a given cost matrix, we use $|C/\\operatorname*{max}(C)-C_{W}/\\operatorname*{max}(C_{W})|$ as the relative error. ", "page_idx": 9}, {"type": "text", "text": "Results. We report the relative errors from SW, CHSW, and H2SW in Table 4 after 100 independent runs. In addition, we show the cost matrices from SW, CHSW, H2SW. and joint Wasserstein distance with $L\\,=\\,2000$ in Figure 5. Cost matrices for $L\\,=\\,100$ , $L\\,=\\,500$ , and $L\\,=\\,1000$ are given in Figure 9- 11 in Appendix D. From Table 4, we see that H2SW gives a lower relative error than CHSW and SW. Therefore, using H2SW for comparing datasets is the most equivalent to the joint Wasserstein distance in terms of the relative error. We also observe that increasing the value of the number of projections also reduces the relative errors for all SW variants. Again, we would like to recall that H2SW can be used for heterogeneous joint distributions beyond the product of Hadamard manifolds as shown in previous experiments. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented Hierarchical Hybrid Sliced Wasserstein (H2SW) distance, a novel sliced probability metric for heterogeneous joint distributions i.e., joint distributions have marginals on different domains. The key component of H2SW is the proposed hierarchical hybrid Radon Transform (HHRT) which is the composition of partial Radon Transform and multiples proposed partial generalized Radon Transform. We then discuss the injectivity of the proposed transforms and theoretical properties of H2SW including topological properties, statistical properties, and computational properties. On the experimental side, we show that H2SW has favorable performance in applications of 3D mesh deformation, training deep 3D mesh autoencoder, and datasets comparison. In those applications, heterogeneous joint distributions appear in the form of joint distributions on the product of Euclidean space and 2D sphere, and the product of Hadamard manifolds. In the future, we will extend the application of H2SW to more complicated heterogeneous joint distributions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] D. Alvarez-Melis and N. Fusi. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33:21428\u201321439, 2020.   \n[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pages 214\u2013223, 2017.   \n[3] G. Beylkin. The inversion problem and applications of the generalized Radon transform. Communications on pure and applied mathematics, 37(5):579\u2013599, 1984. [4] B. Bhushan Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and N. Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 447\u2013463, 2018.   \n[5] M. T. Boedihardjo. Sharp bounds for the max-sliced Wasserstein distance. arXiv preprint arXiv:2403.00666, 2024.   \n[6] C. Bonet, P. Berg, N. Courty, F. Septier, L. Drumetz, and M.-T. Pham. Spherical slicedWasserstein. International Conference on Learning Representations, 2023.   \n[7] C. Bonet, L. Chapel, L. Drumetz, and N. Courty. Hyperbolic sliced-Wasserstein via geodesic and horospherical projections. In Topological, Algebraic and Geometric Learning Workshops 2023, pages 334\u2013370. PMLR, 2023. [8] C. Bonet, N. Courty, F. Septier, and L. Drumetz. Efficient gradient flows in sliced-Wasserstein space. Transactions on Machine Learning Research, 2022.   \n[9] C. Bonet, L. Drumetz, and N. Courty. Sliced-Wasserstein distances and flows on CartanHadamard manifolds. arXiv preprint arXiv:2403.06560, 2024.   \n[10] C. Bonet, B. Mal\u00e9zieux, A. Rakotomamonjy, L. Drumetz, T. Moreau, M. Kowalski, and N. Courty. Sliced-Wasserstein on symmetric positive definite matrices for m/eeg signals. In International Conference on Machine Learning, pages 2777\u20132805. PMLR, 2023.   \n[11] N. Bonneel, J. Rabin, G. Peyr\u00e9, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 1(51):22\u201345, 2015.   \n[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.   \n[13] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.   \n[14] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pages 2921\u20132926. IEEE, 2017.   \n[15] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In Advances in Neural Information Processing Systems, pages 3730\u20133739, 2017.   \n[16] B. Curless and M. Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303\u2013312, 1996.   \n[17] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pages 2292\u20132300, 2013.   \n[18] I. Deshpande, Y.-T. Hu, R. Sun, A. Pyrros, N. Siddiqui, S. Koyejo, Z. Zhao, D. Forsyth, and A. G. Schwing. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10648\u201310656, 2019.   \n[19] I. Deshpande, Z. Zhang, and A. G. Schwing. Generative modeling using the sliced Wasserstein distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3483\u20133491, 2018.   \n[20] N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162:707\u2013738, 2015.   \n[21] R. C. Garrett, T. Harris, B. Li, and Z. Wang. Validating climate models with spherical convolutional Wasserstein distance. arXiv preprint arXiv:2401.14657, 2024.   \n[22] E. Heitz, K. Vanhoey, T. Chambon, and L. Belcour. A sliced Wasserstein loss for neural texture synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9412\u20139420, 2021.   \n[23] S. Helgason. The Radon transform on r n. In Integral Geometry and Radon Transforms, pages 1\u201362. Springer, 2011.   \n[24] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5):550\u2013554, 1994.   \n[25] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7, page 0, 2006.   \n[26] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced Wasserstein distances. In Advances in Neural Information Processing Systems, pages 261\u2013272, 2019.   \n[27] S. Kolouri, G. K. Rohde, and H. Hoffmann. Sliced Wasserstein distance for learning Gaussian mixture models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3427\u20133436, 2018.   \n[28] P. Kuchment. Generalized transforms of Radon type and their applications. In Proceedings of Symposia in Applied Mathematics, volume 63, page 67, 2006.   \n[29] T. Le, K. Nguyen, S. Sun, K. Han, N. Ho, and X. Xie. Diffeomorphic mesh deformation via efficient optimal transport for cortical surface reconstruction. International Conference on Learning Representations, 2024.   \n[30] T. Le, K. Nguyen, S. Sun, N. Ho, and X. Xie. Integrating efficient optimal transport and functional maps for unsupervised shape correspondence learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024.   \n[31] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[32] C.-Y. Lee, T. Batra, M. H. Baig, and D. Ulbricht. Sliced Wasserstein discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10285\u201310295, 2019.   \n[33] Z.-P. Liang and D. C. Munson. Partial Radon transforms. IEEE transactions on image processing, 6(10):1467\u20131469, 1997.   \n[34] A. Liutkus, U. Simsekli, S. Majewski, A. Durmus, and F.-R. St\u00f6ter. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pages 4104\u20134113. PMLR, 2019.   \n[35] M. Luong, K. Nguyen, N. Ho, R. Haf, D. Phung, and L. Qu. Revisiting deep audio-text retrieval through the lens of transportation. In The Twelfth International Conference on Learning Representations, 2024.   \n[36] T. Manole, S. Balakrishnan, and L. Wasserman. Minimax confidence intervals for the sliced Wasserstein distance. Electronic Journal of Statistics, 16(1):2252\u20132345, 2022.   \n[37] K. Nadjahi, V. De Bortoli, A. Durmus, R. Badeau, and U. \u00b8Sim\u00b8sekli. Approximate Bayesian computation with the sliced-Wasserstein distance. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5470\u20135474. IEEE, 2020.   \n[38] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli. Statistical and topological properties of sliced probability divergences. Advances in Neural Information Processing Systems, 33:20802\u201320812, 2020.   \n[39] K. Nguyen, N. Bariletto, and N. Ho. Quasi-monte carlo for 3d sliced Wasserstein. In The Twelfth International Conference on Learning Representations, 2024.   \n[40] K. Nguyen and N. Ho. Revisiting sliced Wasserstein on images: From vectorization to convolution. Advances in Neural Information Processing Systems, 2022.   \n[41] K. Nguyen and N. Ho. Energy-based sliced Wasserstein distance. Advances in Neural Information Processing Systems, 2023.   \n[42] K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021.   \n[43] K. Nguyen, T. Ren, H. Nguyen, L. Rout, T. Nguyen, and N. Ho. Hierarchical sliced Wasserstein distance. International Conference on Learning Representations, 2023.   \n[44] S. Nietert, R. Sadhu, Z. Goldfeld, and K. Kato. Statistical, robustness, and computational guarantees for sliced Wasserstein distances. Advances in Neural Information Processing Systems, 2022.   \n[45] O. Pele and M. Werman. Fast and robust earth mover\u2019s distances. In 2009 IEEE 12th International Conference on Computer Vision, pages 460\u2013467. IEEE, September 2009.   \n[46] S. Peng, C. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, and A. Geiger. Shape as points: A differentiable poisson solver. Advances in Neural Information Processing Systems, 34:13032\u2013 13044, 2021.   \n[47] G. Peyr\u00e9 and M. Cuturi. Computational optimal transport, 2020.   \n[48] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.   \n[49] M. Quellmalz, R. Beinert, and G. Steidl. Sliced optimal transport on the sphere. Inverse Problems, 39(10):105005, 2023.   \n[50] M. Quellmalz, L. Buecher, and G. Steidl. Parallelly sliced optimal transport on spheres and on the rotation group. arXiv preprint arXiv:2401.16896, 2024.   \n[51] F. Rouviere. Nonlinear Radon and Fourier transforms, 2015.   \n[52] R. M. Rustamov and S. Majumdar. Intrinsic sliced Wasserstein distances for comparing collections of probability distributions on manifolds and graphs. In International Conference on Machine Learning, pages 29388\u201329415. PMLR, 2023.   \n[53] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In International Conference on Learning Representations, 2018.   \n[54] H. Tran, Y. Bai, A. Kothapalli, A. Shahbazi, X. Liu, R. D. Martin, and S. Kolouri. Stereographic spherical sliced Wasserstein distances. International Conference on Machine Learning, 2024.   \n[55] C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.   \n[56] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[57] J. Xu, H. Zhou, C. Gan, Z. Zheng, and L. Li. Vocabulary learning via optimal transport for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7361\u20137373, 2021.   \n[58] M. Yi and S. Liu. Sliced Wasserstein variational inference. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2021.   \n[59] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplement to \u201cHierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions\" ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first provide skipped proofs in the main paper in Appendix A. We then provide some additional materials including additional background and extended definitions in Appendix B. After that, we discuss some related works in Appendix C. We report additional experimental results in Appendix D. Finally, we report computational infrastructure in Appendix E. ", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For any $t,\\theta,y$ , we are given $(\\mathcal{P G R f}_{1})(t,\\theta,y)=(\\mathcal{P G R f}_{2})(t,\\theta,y)$ . By Definition 1, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d_{1}}}f_{1}(x,y)\\delta(t-g(x,\\theta))d x=\\int_{\\mathbb{R}^{d_{1}}}f_{2}(x,y)\\delta(t-g(x,\\theta))d x.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For any $\\varepsilon\\in\\mathbb{R}^{d_{2}}$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d_{2}}}\\int_{\\mathbb{R}^{d_{1}}}f_{1}(x,y)\\delta(t-g(x,\\theta))e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d x d y=\\int_{\\mathbb{R}^{d_{2}}}\\int_{\\mathbb{R}^{d_{1}}}f_{2}(x,y)\\delta(t-g(x,\\theta))e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d x d y.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Applying the Fubini\u2019s theorem, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d_{1}}}f_{1}(x,y)\\int_{\\mathbb{R}^{d_{2}}}e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y\\delta(t-g(x,\\theta))d x=\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f_{2}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y\\delta(t-g(x,\\theta))d x,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\mathcal{G R}\\int_{\\mathbb{R}^{d_{2}}}f_{1}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y\\right)=\\left(\\mathcal{G R}\\int_{\\mathbb{R}^{d_{2}}}f_{2}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the injectivity of GRT, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d_{2}}}f_{1}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y=\\int_{\\mathbb{R}^{d_{2}}}f_{2}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}d y.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, for any $\\epsilon\\in\\mathbb{R}^{d_{1}}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f_{1}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}e^{-i2\\pi\\langle\\varepsilon,x\\rangle}d y d x=\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f_{2}(x,y)e^{-i2\\pi\\langle\\varepsilon,y\\rangle}e^{-i2\\pi\\langle\\varepsilon,x\\rangle}d y d x.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is $(\\mathcal{F}f_{1}(x,y))=(\\mathcal{F}f_{2}(x,y)))$ with $\\mathcal{F}$ denotes the Fourier transform. By the injectivity of the Fourier Transform, we have $f_{1}(x,y)=f_{2}(x,y)$ for any $x,y$ , which concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first show that HHRT is the composition of PGRT and PRT. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathcal{P R}(\\mathcal{P G R}(\\mathcal{P G}\\mathcal{R}f)))(t,\\theta_{1},\\theta_{2},\\psi)}\\\\ &{=\\displaystyle\\int_{\\mathbb{R}^{2}}\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f(x,y)\\delta(t_{1}-g_{1}(x,\\theta_{1}))\\delta(t_{2}-g_{2}(y,\\theta_{2}))\\delta(t-\\psi_{1}t_{1}-\\psi_{2}t_{2})d x d y d t_{1}d t_{2}}\\\\ &{=\\displaystyle\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f(x,y)\\int_{\\mathbb{R}^{2}}\\delta(t_{1}-g_{1}(x,\\theta_{1}))\\delta(t_{2}-g_{2}(y,\\theta_{2}))\\delta(t-\\psi_{1}t_{1}-\\psi_{2}t_{2})d t_{1}d t_{2}d x d y}\\\\ &{=\\displaystyle\\int_{\\mathbb{R}^{d_{1}}}\\int_{\\mathbb{R}^{d_{2}}}f(x,y)\\delta\\left(t-\\psi_{1}g_{1}(x,\\theta_{1})-\\psi_{2}g_{2}(y,\\theta_{2})\\right)d x d y}\\\\ &{=(\\mathcal{H}\\mathcal{H}\\mathcal{R}f)(t,\\theta_{1},\\theta_{2},\\psi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For any $t,\\theta_{1},\\theta_{2},\\psi$ , we are given $(\\mathcal{H H R f}_{1})(t,\\theta_{1},\\theta_{2},\\psi)=(\\mathcal{H H R f}_{2})(t,\\theta_{1},\\theta_{2},\\psi)$ , which is equivalent to: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\mathcal{P R}(\\mathcal{P G R}(\\mathcal{P G R}f_{1})))(t,\\theta_{1},\\theta_{2},\\psi)=(\\mathcal{P R}(\\mathcal{P G R}(\\mathcal{P G R}f_{2})))(t,\\theta_{1},\\theta_{2},\\psi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the injectivity of the PRT and the PGRT, we obtain $f_{1}(x,y)\\,=\\,f_{2}(x,y)$ for any $x,y$ which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To prove that the hierarchical hybrid sliced Wasserstein $H2S W_{p}(\\cdot,\\cdot;c,g_{1},g_{2})$ is a metric on the space of distributions on $\\mathcal{P}(\\mathbb{R}^{d_{1}}\\,\\times\\,\\mathbb{R}^{d_{2}})$ for any $p\\geq1$ , ground metric $c$ , and defining functions $g_{1},g_{2}$ , we need to show that it satisfies non-negativity, symmetry, triangle inequality, and identity of indiscernible. ", "page_idx": 14}, {"type": "text", "text": "Non-Negativity. Since $\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)\\geq0$ [47] for any $\\theta_{1},\\theta_{2},\\psi$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})}\\big[\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)\\big]\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means that $H2S W_{p}(\\mu,\\nu;c,g_{1},g_{2})\\geq0$ for any $\\mu$ and $\\nu$ . ", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r l r}{\\dot{\\mathrm{W}}_{p}^{p}(\\mathcal{H}\\mathcal{H}\\dot{\\mathcal{R}}_{\\theta_{1},\\theta_{2},\\psi}^{\\bar{g}_{1},g_{2}}\\sharp\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)}&{{}=}&{\\mathrm{W}_{p}^{\\dot{p}}(\\mathcal{H}\\mathcal{H}\\dot{\\mathcal{R}}_{\\theta_{1},\\theta_{2},\\psi}^{\\bar{g}_{1},g_{2}}\\sharp\\nu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu;c)}\\end{array}$ [4di7s]tanfcoer any $\\theta_{1},\\theta_{2},\\psi$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})}[\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu;c)]}\\\\ &{\\ =\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})}[\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu,\\mathcal{H H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu;c)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means that $H2S W_{p}(\\mu,\\nu;c,g_{1},g_{2})=H2S W_{p}(\\nu,\\mu;c,g_{1},g_{2})$ any $\\mu$ and $\\nu$ . ", "page_idx": 14}, {"type": "text", "text": "Triangle Inequality. Given $c$ to be a valid metric on $\\mathbb{R}$ , we can use the triangle inequality of the Wasserstein distance. For any distributions $\\mu_{1},\\mu_{2},\\mu_{3}\\in\\mathcal{P}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{12\\mathrm{SW}_{p}(\\mu_{1},\\mu_{2};c,g_{1},g_{2})=\\bigg(\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times{\\mathbb{S}})}\\big|\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{1},\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{2};c)\\big)\\bigg|^{\\frac{1}{p}}}&{}\\\\ {\\leq\\bigg(\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times{\\mathbb{S}})}\\big[(\\mathbf{W}_{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{1},\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{\\mathcal{B}};c)}\\\\ &{\\qquad+\\mathbf{W}_{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{1},\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{2};c))^{p}\\big]^{\\frac{1}{p}}}&{}\\\\ {\\leq\\bigg(\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times{\\mathbb{S}})}\\big|\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{1},\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}};\\mu_{\\mathcal{B}};c)\\big|\\bigg)^{\\frac{1}{p}}}&{}\\\\ {\\quad+\\bigg(\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the final inequality is due to Minkowski\u2019s inequality. Therefore, we complete the proof for the triangle inequality of the hierarchical hybrid sliced Wasserstein. ", "page_idx": 14}, {"type": "text", "text": "Identity of indiscernible. For any $\\begin{array}{r l r l}{p}&{{}\\geq}&{1}\\end{array}$ , ground metric $c$ , and $g_{1},g_{2}$ , when   \n$\\ensuremath{\\mu}\\mathrm{~\\ensuremath~{~\\mu~}~}=\\ensuremath{\\phantom{\\rho}}\\nu$ $\\begin{array}{r l r}{\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu}&{{}=}&{(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu}\\end{array}$   \n$\\begin{array}{r l r}{\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu_{1},\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu_{2};c)}&{=}&{0}\\end{array}$ $\\mathrm{H}2{\\bf S W}_{p}(\\mu,\\nu;c,g_{1},g_{2})\\;\\;=\\;\\;0.$   \ntfNhoore  wa,Wl aamssosssuetrm seteev tiehnra ytd $\\mathrm{H}2\\mathrm{SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})=0$ $\\mathbf{W}_{p}^{p}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu_{1},\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu_{2};c)=0$ $\\theta_{1}\\;\\;\\in\\;\\;\\Omega_{1},\\theta_{2}\\;\\;\\in\\;\\;\\Omega_{2},\\psi\\;\\;\\in\\;\\;\\mathrm{\\bf~\\bar{S}}$ $\\begin{array}{r}{\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu\\ =\\ (\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\nu}\\end{array}$   \n$\\theta_{1}\\in\\Omega_{1},\\theta_{2}\\in\\Omega_{2},\\psi\\in\\mathbb{S}$ . Since the HHRT is injective (proved in Proposition 2), we obtain $\\mu=\\nu$ . ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Proposition 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "(i) For any $p\\geq1$ , $c(x,y)=|x-y|$ , and $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ , we have: ", "page_idx": 14}, {"type": "text", "text": "H2SWp(\u00b5, \u03bd; c, g1, g2) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\times\\left(\\mathbb{E}_{(\\theta_{1},\\theta_{2},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})}[\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\vec{\\mu}/\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\vec{\\nu}\\nu;c)]\\right)^{\\frac{1}{p}}}\\\\ &{=\\left(\\mathbb{E}\\left[\\underset{\\pi\\in\\Pi(\\mu,\\nu)}{\\operatorname*{inf}}\\int|\\psi_{1}(g_{1}(\\theta_{1},x_{1})-g_{1}(\\theta_{1},y_{1}))+\\psi_{2}(g_{2}(\\theta_{2},x_{2})-g_{1}(\\theta_{2},y_{2}))|^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By applying the Cauchy-Schwartz inequality, we have: $\\mathrm{H}2\\mathrm{SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int(\\sqrt{v_{1}^{2}+v_{2}^{2}})^{p}(\\gamma(\\mathcal{Q}_{1}(\\theta_{1},x_{1})-g_{1}(\\theta_{1},y_{1}))^{2}+(\\mathcal{Q}_{2}(\\theta_{2},x_{2})-g_{2}(\\theta_{2},y_{2}))^{2})^{p}d x\\right.}\\\\ &{\\leq\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int(\\lvert g_{1}(\\theta_{1},x_{1})-g_{1}(\\theta_{1},y_{1})\\rvert+\\lvert g_{2}(\\theta_{2},x_{2})-g_{2}(\\theta_{2},y_{2})\\rvert)^{p}d x(x_{1},x_{2},y_{1},y_{2})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\leq\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int\\lvert g_{1}(\\theta_{1},x_{1})-g_{1}(\\theta_{1},y_{1})\\rvert\\,r\\,d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\quad+\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int\\lvert g_{2}(\\theta_{2},x_{2})-g_{2}(\\theta_{2},y_{2})\\rvert^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{=\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int\\lvert g_{2}(\\theta_{1},x_{1})-g_{1}(\\theta_{1},y_{1})\\rvert\\,r\\,d\\pi(x_{1},y_{1})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\quad+\\Bigg(\\mathbb{E}\\left[\\underset{r\\in\\mathrm{H}\\{r_{0},r\\}}{\\operatorname*{inf}}\\int\\lvert g_{2}(\\theta_{2},x_{2})\\rvert\\,r\\,d\\theta \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is due to the Minkowski\u2019s inequality. ", "page_idx": 15}, {"type": "text", "text": "(ii) From (i), we have $\\mathrm{H}2\\mathrm{SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})\\leq\\mathrm{GSW}_{p}(\\mu_{1},\\nu_{1};g_{1},c)+\\mathrm{GSW}_{p}(\\mu_{2},\\nu_{2};g_{2},c)$ . When, $g_{1},g_{2}$ , and $c(x,y)=|x-y|$ are linear defining functions, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathrm{GSW}_{p}}(\\mu_{1},\\nu_{1};g_{1},c)=\\Bigg(\\mathbb{E}\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu_{1},\\nu_{1})}\\int(|\\theta^{\\top}x_{1}-\\theta^{\\top}y_{1}|^{p}d\\pi(x_{1},y_{1})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Bigg(\\mathbb{E}\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu_{1},\\nu_{1})}\\int(|\\theta||_{2}||x_{1}-y_{1}||_{2}^{p}d\\pi(x_{1},y_{1})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\leq\\Bigg(\\mathbb{E}\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu_{1},\\nu_{1})}\\int(||x_{1}-y_{1}||^{p}d\\pi(x_{1},y_{1})\\right]\\Bigg)^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad=\\Bigg(\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu_{1},\\nu_{1})}\\int(||x_{1}-y_{1}||^{p}d\\pi(x_{1},y_{1})\\Bigg)^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we have $\\mathrm{GSW}_{p}(\\mu_{2},\\nu_{2};g_{1},c)\\ \\leq\\ W_{p}(\\mu_{2},\\nu_{2};c)$ . Therefore, we obtain the proof of $\\mathrm{H}2\\mathrm{SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})\\leq W_{p}(\\mu_{1},\\nu_{1};c)+W_{p}(\\mu_{1},\\nu_{1};c).$ ", "page_idx": 15}, {"type": "text", "text": "(iii) When $g_{1},\\,g_{2}$ are linear defining functions, we have: $\\mathrm{H}2\\mathrm{SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\bigg(\\mathbb E\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int(|\\theta_{1}^{\\top}x_{1}-\\theta_{1}^{\\top}y_{1})|+|\\theta_{2}^{\\top}x_{2}-\\theta_{2}^{\\top}y_{2}|)^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\bigg)^{\\frac1p}}\\\\ &{\\leq\\bigg(\\mathbb E\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int(|\\theta_{1}^{\\top}x_{1}-\\theta_{1}^{\\top}y_{1})|+|\\theta_{2}^{\\top}x_{2}-\\theta_{2}^{\\top}y_{2}|)^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\bigg)^{\\frac1p}}\\\\ &{\\leq\\bigg(\\mathbb E\\left[\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int(|x_{1}-y_{1})|+|x_{2}-y_{2}|)^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\right]\\bigg)^{\\frac1p}}\\\\ &{=\\bigg(\\pi\\in\\Pi(\\mu,\\nu)\\bigg)\\Big(|x_{1}-y_{1}|)+|x_{2}-y_{2}|\\Big)^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\bigg)^{\\frac1p}}\\\\ &{=\\bigg(\\pi\\in\\Pi(\\mu,\\nu)\\bigg)\\Big(||x_{1}-y_{1}|\\Big)+|x_{2}-y_{2}|\\Big)^{p}d\\pi(x_{1},x_{2},y_{1},y_{2})\\bigg)^{\\frac1p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $p=1$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H}2\\mathrm{SW}_{1}(\\mu,\\nu;c,g_{1},g_{2})\\leq\\bigg(\\underset{\\pi\\in\\Pi(\\mu,\\nu)}{\\operatorname*{inf}}\\int(|x_{1}-y_{1})|+|x_{2}-y_{2}|)d\\pi(x_{1},x_{2},y_{1},y_{2})\\bigg)^{\\frac{1}{p}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=W_{1}(\\mu,\\nu;c,c),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "A.5 Proof of Proposition 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let $p\\geq1$ , $c(x,y)=\\vert x-y\\vert$ , $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ with the corresponding empirical distribution $\\mu_{n}$ , we assume that there exists $q>p$ such that the $q-$ th order moment of $\\mu$ i.e, $\\begin{array}{r}{\\bar{M_{q}}(\\mu)=\\int_{\\mathbb{R}}|x|^{q}d\\mu(x)}\\end{array}$ , is bounded by $B<\\infty$ . From Theorem 1 in [20], there exists a constant $C_{p,q}$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[W_{p}^{p}(\\mu_{n},\\mu;c)\\right]\\leq C_{p,q}B\\left\\{\\underset{n^{-(q-p)/q}\\;\\mathrm{if}\\;q\\in\\;(p,\\,2p).}{n^{-1/2}\\operatorname{if}q>2p,}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We show that $\\mathcal{H H R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\sharp\\mu$ has finite bounded moments. In particular, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{k}(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\d\\|\\mu)=\\int_{\\mathbb{R}}|t|^{k}d(\\mathcal{H H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{g_{1},g_{2}}\\d\\|\\mu)(t)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}|\\psi_{1}g_{1}(\\theta_{1},x_{1})+\\psi_{2}g_{2}(\\theta_{2},x_{2})|^{k}d\\mu(x_{1},x_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}(\\psi_{1}^{2}+\\psi_{2}^{2})^{k/2}(g_{1}(\\theta_{1},x_{1})^{2}+g_{2}(\\theta_{2},x_{2})^{2})^{k/2}d\\mu(x_{1},x_{2})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}(|g_{1}(\\theta_{1},x_{1})|+|g_{2}(\\theta_{2},x_{2})|)^{k}d\\mu(x_{1},x_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is due to the Cauchy-Schwarz inequality and the second inequality is due to the fact that $\\|x\\|_{2}\\leq|x|$ . For the linear defining functions $g({\\dot{\\theta}},x)=\\theta^{\\top}x$ , we have $|g(\\theta,x)|=$ $|\\theta^{\\top}x|\\leq\\|x\\|_{1}$ . For the circular defining functions $g(\\theta,x)\\,=\\,\\|x-r\\theta\\|_{2}\\,\\leq\\,\\|x-r\\theta\\|_{1}\\,\\leq\\,\\|x\\|_{1}\\,+$ $\\|r\\theta\\|_{1}\\leq\\|x\\|_{1}+r$ . Therefore, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle M_{k}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi}^{\\eta,\\eta_{2}}\\{\\mu\\}\\le\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}(|x_{1}|+|x_{2}|+C_{g_{1},g_{2}})^{k}d\\mu(x_{1},x_{2})}\\\\ &{\\displaystyle=\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}\\sum_{i=0}^{k}k^{i}(|x_{1}|+|x_{2}|)^{i}C_{g_{1},g_{2}}^{k-i}d\\mu(x_{1},x_{2})}\\\\ &{\\displaystyle=\\sum_{i=0}^{k}k^{i}C_{g_{1},g_{2}}^{k-i}\\int_{\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}}(|x_{1}|+|x_{2}|)^{i}d\\mu(x_{1},x_{2})}\\\\ &{\\displaystyle\\le\\sum_{i=0}^{k}k^{i}C_{g_{1},g_{2}}^{k-i}M_{i}(\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C_{g_{1},g_{2}}=0$ if $g_{1},g_{2}$ are linear, $C_{g_{1},g_{2}}=r$ if $g_{1}$ and $g_{2}$ are linear and circular respectively (exchangeable), and $C_{g_{1},g_{2}}=2r$ if both $g_{1}$ and $g_{2}$ are circular. ", "page_idx": 16}, {"type": "text", "text": "Now, using the triangle inequality of H2SW (Theorem 1), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left|\\mathrm{H2SW}_{p}(\\mu_{n},\\nu_{n};c,g_{1},g_{2})-\\mathrm{H2SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})\\right|}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left|\\mathrm{H2SW}_{p}(\\mu,\\mu_{n};c,g_{1},g_{2})+\\mathrm{H2SW}_{p}(\\nu,\\nu_{n};c,g_{1},g_{2})\\right|}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left|\\mathrm{H2SW}_{p}(\\mu,\\mu_{n};c,g_{1},g_{2})\\right|+\\mathbb{E}\\left|\\mathrm{H2SW}_{p}(\\nu,\\nu_{n};c,g_{1},g_{2})\\right|}\\\\ &{\\qquad\\qquad\\leq\\left(\\mathbb{E}\\left|\\mathrm{H2SW}_{p}^{p}(\\mu,\\mu_{n};c,g_{1},g_{2})\\right|\\right)^{\\frac{1}{p}}+\\left(\\mathbb{E}\\left|\\mathrm{H2SW}_{p}^{p}(\\nu,\\nu_{n};c,g_{1},g_{2})\\right|\\right)^{\\frac{1}{p}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is due to Holder\u2019s inequality. Combining with previous results, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left|\\mathrm{H2SW}_{p}(\\mu_{n},\\nu_{n};c,g_{1},g_{2})-\\mathrm{H2SW}_{p}(\\mu,\\nu;c,g_{1},g_{2})\\right|}\\\\ &{\\qquad\\qquad\\leq C_{p,q}^{\\frac{1}{p}}\\left(\\displaystyle\\sum_{i=0}^{q}q^{i}C_{g_{1},g_{2}}^{q-i}(M_{i}(\\mu)+M_{i}(\\nu))\\right)^{\\frac{1}{p}}\\left\\{\\!\\!\\!\\begin{array}{l}{n^{-1/2p}\\;\\mathrm{if}\\;q>2p,}\\\\ {n^{-1/2p}\\log(n)^{\\frac{1}{p}}\\;\\mathrm{if}\\;q=2p,}\\\\ {n^{-(q-p)/p q}\\;\\mathrm{if}\\;q\\in(p,2p),}\\end{array}\\ \\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "A.6 Proof of Proposition 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For any $p\\geq1$ , and $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}})$ , using the Holder\u2019s inequality, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\widehat{\\Pi}2\\widehat{\\cal S}\\overline{{\\mathrm{W}}}_{p}^{\\nu}(\\mu,\\nu;c,g_{1},g_{2},L)-\\mathbb{H}2{\\cal S}\\mathbf{W}_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2})\\|}\\\\ &{\\leq\\Big(\\mathbb{E}\\|\\widehat{\\Pi}2\\widehat{\\cal S}\\mathbf{W}_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2},L)-\\mathbb{H}2{\\cal S}\\mathbf{W}_{p}^{p}(\\mu,\\nu;c,g_{1},g_{2})|^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{=\\Bigg(\\mathbb{E}\\left|\\frac{1}{L}\\sum_{l=1}^{L}\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{H}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{\\theta_{1},g_{2}}\\sharp\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\nu;c)-\\mathbb{E}\\left[\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}})\\right]\\Bigg|}\\\\ &{=\\Bigg(V a r\\left[\\frac{1}{L}\\sum_{l=1}^{L}\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\nu;c)\\right]\\Bigg)^{\\frac{1}{2}}}\\\\ &{=\\frac{1}{\\sqrt{L}}V a r\\left[\\mathbf{W}_{p}^{p}(\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\mu,\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\theta_{2},\\psi_{l}}^{g_{1},g_{2}}\\sharp\\nu;c)\\right]^{\\frac{1}{2}},} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "B Additional Materials ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "HHRT with more than two marginals. We now extend the definition of HHRT to $K>2$ mariginals. Definition 4 (Hierarchical Hybrid Radon Transform). Given $K\\ \\geq\\ 2,$ , given defining functions $\\{g_{k}:\\mathbb{R}^{d_{k}}\\times\\mathring{\\Omega}_{i}\\rightarrow\\mathbb{R}\\}_{i=k}^{K}$ , the Hierarchical Hybrid Radon Transform $\\mathcal{H H R}:\\mathbb{L}_{1}(\\mathbb{R}^{d_{1}}\\times\\ldots\\times$ $\\mathbb{R}^{d_{K}})\\rightarrow\\mathbb{L}_{1}\\left(\\mathbb{R}\\times\\Omega_{1}\\ldots\\times\\Omega_{K}\\times\\mathbb{S}^{K-1}\\right)$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{(\\mathcal{H}\\mathcal{H}\\mathcal{R}f)(t,\\theta_{1},\\ldots,\\theta_{K},\\psi)}}\\\\ &{}&{=\\displaystyle\\int_{\\mathbb{R}^{d_{1}}\\times\\dots\\times\\mathbb{R}^{d_{K}}}f(x_{1},\\dots,x_{K})\\delta\\left(t-\\sum_{k=1}^{K}\\psi_{k}g_{k}(x_{k},\\theta_{k})\\right)d x_{1}\\dots d x_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "H2SW with more than two marginals. From the new definition of HRRT on $K>2$ mariginals, we now can define H2SW between joint distributions with $K$ mariginals. ", "page_idx": 17}, {"type": "text", "text": "Definition 5. For $p\\,\\geq\\,1,K\\,\\geq\\,2$ , defining functions $g_{1},\\ldots,g_{K}$ , the hierarchical hybrid sliced Wasserstein- $\\boldsymbol{p}$ (H2SW) distance between two distributions $\\mu\\in\\mathcal{P}(\\mathcal{X}_{1}\\times...\\times\\mathcal{X}_{K})$ and $\\nu\\in\\mathcal{P}(\\mathcal{Y}_{1}\\times$ $\\cdots\\times{\\mathcal{Y}}_{K})$ ) with an one-dimensional ground metric $c:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}^{+}$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H2S W_{p}^{p}(\\mu,\\nu;c,g_{1},\\ldots,g_{K})}\\\\ &{\\quad=\\mathbb{E}_{(\\theta_{1},\\ldots,\\theta_{K},\\psi)\\sim\\mathcal{U}(\\Omega_{1}\\times\\ldots\\times\\Omega_{K}\\times\\mathbb{S}^{K-1})}\\big[W_{p}^{p}(\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\ldots,\\theta_{K},\\psi}^{g_{1},\\ldots,g_{K}}\\sharp\\mu,\\mathcal{H}\\mathcal{H}\\mathcal{R}_{\\theta_{1},\\ldots,\\theta_{K},\\psi}^{g_{1},\\ldots,g_{K}}\\sharp\\nu;c)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "cwrheeartee $\\mathcal{H H R}_{\\theta_{1},\\dots,\\theta_{K},\\psi}^{g_{1},\\dots,g_{K}}\\sharp\\mu$ aT.nd $\\mathcal{H H R}_{\\theta_{1},\\dots,\\theta_{K},\\psi}^{g_{1},\\dots,g_{K}}\\sharp\\nu$ are the one-dimensional push-forward distributions ", "page_idx": 17}, {"type": "text", "text": "Lorentz Model and Busemann function. The Lorentz model $\\mathbb{L}^{d}\\,\\in\\,\\mathbb{R}^{d+1}$ of a d-dimensional hyperbolic space is [7]: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{L}^{d}=\\left\\{(x_{1},\\ldots,x_{d})\\in\\mathbb{R}^{d+1},-x_{0}y_{0}+\\sum_{i=1}^{d}x_{i}y_{i}=-1,x_{0}>0\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given a direction $\\theta\\in T_{x_{0}}\\mathbb{L}^{d}\\cap\\mathbb{S}^{d}$ , $x\\in\\mathbb{L}^{d}$ , the Busemann function is: ", "page_idx": 17}, {"type": "equation", "text": "$$\nB(x,\\theta)=\\log(-\\langle x,x_{0}+\\theta\\rangle).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Busemann function on product Hadamard manifolds. For distributions supports on the product of $K\\ge2$ Hadamard manifolds with the corresponding Busemann functions $B_{1},\\ldots,B_{K}$ , we have a Busemann function of the product manifolds is: ", "page_idx": 17}, {"type": "equation", "text": "$$\nB(x_{1},\\ldots,x_{K},\\theta_{1},\\ldots,\\theta_{K})=\\sum_{k=1}^{K}\\lambda_{k}B_{k}(x_{k},\\theta_{k}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "XwrMd1njqq/tmp/668303a80084bb911dea9feb6a531b2efd618b6332ca2643412bc297f68a36d6.jpg", "img_caption": ["Figure 6: Visualization of deformation from the sphere mesh to the Armadillo mesh with $L=100$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "for $(\\lambda_{1},\\ldots,\\lambda_{K})\\in\\mathbb{S}^{K-1}$ . The Cartan-H\u221ayperbolic S\u221aliced-Wasserstein distance use a fixed value of $\\left(\\lambda_{1},\\ldots,\\lambda_{K}\\right)$ e.g., $\\big(\\lambda_{1},.~.~,\\lambda_{K}\\big)=(1/\\sqrt{K},.~.~.,1/\\sqrt{K})$ (see 2). In our proposed H2SW, we treat $\\left(\\lambda_{1},\\ldots,\\lambda_{K}\\right)$ as a random variable follows $\\mathcal{U}(\\mathbb{S}^{K-1})$ and the value of H2SW is defined as the mean of such random variable. ", "page_idx": 18}, {"type": "text", "text": "C Related Works ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "HHRT and Generalized Radon Transform. HHRT can be also seen as a special case of GRT [3] with the defining function $g(x,\\theta)\\;=\\;\\psi_{1}g_{1}(x_{1},\\theta_{1})\\,+\\,\\psi_{2}g_{2}(y,\\theta_{2})$ with $\\bar{x_{\\mathrm{~}}}=\\mathrm{{~}}(x_{1},x_{2})$ and $\\theta\\,=$ $(\\theta_{1},\\theta_{2},\\psi)$ $(\\Omega=\\Omega_{1}\\times\\Omega_{2}\\times\\mathbb{S})$ . However, without approaching via the hierarchical construction, the injectivity of the transform might be a challenge to obtain. ", "page_idx": 18}, {"type": "text", "text": "HHRT and Hierarchical Radon Transform. Hierarchical Radon Transform (HRT) [43] is the composition of Partial Radon Transform and Overparameterized Radon Transform, which is designed specifically for reducing projection complexity when using Monte Carlo estimation. Moreover, HRT is introduced with linear projection and does not focus on the problem of comparing heterogeneous joint distributions. In contrast to HRT, the proposed HHRT is the composition of multiple partial Generalized Radon Transform and Partial Random Transform, which is suitable for comparing heterogeneous joint distributions. ", "page_idx": 18}, {"type": "text", "text": "HHRT and convolution slicers. Convolution slicers [40] are introduced to project an image into a scalar. It can be viewed as a Hierarchical Partial Radon Transform i.e., small parts of the image are transformed first, then be aggregated later. Although convolution slicers can separate global and local information as HHRT, they focus on the domain of images only and have not been proven to be injective. Again, HHRT is designed to compare heterogeneous joint distributions and is proven to be injective in Proposition 2. As a result, H2SW is a valid metric while convolution sliced Wasserstein [40] is only a pseudo metric. Moreover, H2SW can also use convolution slicers when having marginal domains as images. ", "page_idx": 18}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "3D Mesh Deformation. As mentioned in the main text, we present the deformation visualization to the Armadillo mesh with $L=100$ in Figure 6, and the deformation visualization to the Stanford Bunny o mesh with $L=10$ and $L\\,=\\,100$ in Figure 3- 7 in turn. The quantitative result for the Armadillo mesh is given in Table 2. Here, we set the step size to 0.1. From these results, we see that the proposed H2SW gives the best flow deformation flow in general. The performance gap is especially larger when $L=10$ i.e., having a small number of projections. ", "page_idx": 18}, {"type": "image", "img_path": "XwrMd1njqq/tmp/20bfcaac8bd4ef8929fa97473f13bada2a420e582f72de728503c13722b8baff.jpg", "img_caption": ["Figure 7: Visualization of deformation from the sphere mesh to the Stanford Bunny mesh with $L=100$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XwrMd1njqq/tmp/6b67c083a5f4c989731e2d7579a176a4d092595a77d4402ff7be2a719c064135.jpg", "img_caption": ["Figure 8: Visualization of some randomly selected reconstruction meshes from autoencoders trained by SW, GSW, and H2SW in turn with the number of projections $L=100$ at epoch 500. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XwrMd1njqq/tmp/b93bb65c85863f42bd8f26bee59a3c6d27e13290d445847b459ff7cafa4fc237.jpg", "img_caption": ["Figure 9: Cost matrices between datasets from SW, CHSW, and H2SW with $L=100$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XwrMd1njqq/tmp/1109ee6905596cecc22b003aae53939e60228cefbf2083fb9c4d8c1fbd985504.jpg", "img_caption": ["Figure 10: Cost matrices between datasets from SW, CHSW, and H2SW with $L=500$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XwrMd1njqq/tmp/87a26036a757cc7c51f07434ce8bfa571152a2730fc9fd75668a4c98751d8a17.jpg", "img_caption": ["Figure 11: Cost matrices between datasets from SW, CHSW, and H2SW with $L=1000$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Deep 3D mesh autoencoder. We first report the neural network architectures that we use in the experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The encoder: $\\mathrm{Conv1d(6,64,1)}\\rightarrow\\mathrm{BatchNorm1d}\\rightarrow\\mathrm{LeakyReL}$ U(0.2) $\\rightarrow$ Conv1d(64, 128, 1) $\\rightarrow$ BatchNorm1 $\\mathrm{{d}\\ \\rightarrow\\ L e a k y R e L U(0.2)\\ \\rightarrow\\ C o n v l d(128,\\ 256,\\ 1}$ ) $\\rightarrow$ BatchNorm1d $\\rightarrow\\mathrm{LeakyReLU(0.2)}\\rightarrow\\mathrm{Conv1d(256,\\,512,\\,1)}\\rightarrow\\mathrm{BatchNorm1d}\\rightarrow\\mathrm{LeakyReLU(0.2)}\\rightarrow\\mathrm{Conv1d(256,\\,512,\\,1)}\\rightarrow\\mathrm{Conv1d}.\\rightarrow\\mathrm{Conv1d(5.156,\\,512)}.$ $\\rightarrow$ Conv1d(512, 1024, 1) $\\rightarrow$ BatchNorm1d $\\rightarrow$ LeakyReLU(0.2) $\\rightarrow$ Max-Pooling $\\rightarrow$ Linear(1024, 1024). \u2022 The decoder: Linear(1024, 1024) $\\rightarrow$ BatchNorm1d $\\rightarrow$ LeakyReLU(0.2) $\\rightarrow$ Linear(1024, $2048)\\rightarrow$ BatchNorm $\\mathrm{d}\\rightarrow\\mathrm{LeakyReLU}(0.2)\\rightarrow\\mathrm{Linear}(2048,4096)\\rightarrow\\mathrm{BatchNorm1d}\\rightarrow$ LeakyReLU $(0.2)\\rightarrow$ $2)\\rightarrow\\operatorname{Linear}(2048,2048$ $2048^{*}6)$ ). The output of the decoder is the concatenation of the location and normal vector. We normalize the normal vector to the unit-sphere. ", "page_idx": 20}, {"type": "text", "text": "As mentioned in the main text, we report the reconstruction of randomly selected meshes for $L=100$ at epoch 500 in Figure 8. We see that the reconstructed meshes at epoch 500 are visually worse than the reconstructed meshes at epoch 2000. Therefore, the joint Wasserstein distances in Table 3 are consistent with the qualitative results. ", "page_idx": 20}, {"type": "text", "text": "Dataset Comparison. We follow the same procedure in Section 6.2 in [9]. We refer the reader to the reference for a detailed description. Here, we show the cross-dataset cost matrices with the number of projections $L=100$ in Figure 9, $L=500$ in Figure 10, and $L=1000$ Figure 11. ", "page_idx": 20}, {"type": "text", "text": "E Computational Infrastructure ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For the non-deep-learning experiments, we use a HP Omen 25L desktop for conducting experiments.   \nFor 3D mesh autoencoder experiments, we use a single NVIDIA A100 GPU. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: As in the abstract and introduction, we focus on designing a sliced Wasserstein variant for heterogeneous joint distributions. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The proposed hierarchical hybrid Radon transform costs slightly more computation as discussed in Section 3.1. Also, the injectivity of the hierarchical hybrid Radon transform depends on the injectivity of its partial generalized Radon Transform component as discussed in Section 3.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We state all assumptions for our theoretical results in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We report all experimental settings for our experiments in Section 4 and Appendices. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We submitted the anonymized code for experiments in the paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report the training and test details in the experimental parts of the paper in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We run our experiments at least three independent times and report the error bars. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report the computational devices that we use in Appendix E Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics when conducting the research. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We propose a new metric for comparing heterogeneous joint distributions. As shown in the paper, the proposed metric can improve applications of 3D mesh and datasets comparison. We believe that there are no direct negative societal impacts of the work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not collect any data in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We cite and credit all used assets in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide anonymized code for the paper with instructions for running the code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not use crowdsourcing experiments and research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]