[{"figure_path": "aBMESB1Ajx/figures/figures_7_1.jpg", "caption": "Figure 1: A qualitative plot showing the relationship between the density \u03b3 of a winning ticket and the overparameterization required by Theorem 3 for a target network with mt parameters. Earlier results from Pensia et al. [24] and Malach et al. [20] are shown for comparison.", "description": "This figure shows the relationship between the density (\u03b3) of a winning ticket and the overparameterization required to achieve a certain level of approximation error (\u03b5).  The x-axis represents the density of the winning ticket (ranging from very sparse to dense), and the y-axis represents the overparameterization (m/mt, where m is the number of parameters in the random network and mt is the number of parameters in the target network).  The blue curve represents the overparameterization needed according to Theorem 3 of the paper, showing a decrease in overparameterization as density increases.  Points for previous results by Malach et al. and Pensia et al. are also plotted for comparison.", "section": "4 Sparse Strong Lottery Ticket Hypothesis (SSLTH)"}, {"figure_path": "aBMESB1Ajx/figures/figures_12_1.jpg", "caption": "Figure 2: Simplified representation of the procedure for finding Lottery Tickets (LTH). A large random neural network (step 1) is trained by iterative pruning with rewind: when the loss reaches a local minimum (step 2), some weights with smallest absolute value are pruned (step 3) and the value of the remaining edges is then reset to that of the initialization (step 4); finally, training is resumed and the final network is obtained (step 5). Remarkably, the sparser subnetwork is consistently able to reach a loss not larger than that right after pruning.", "description": "This figure illustrates the Lottery Ticket Hypothesis (LTH).  It shows a large, fully-connected neural network. The network is iteratively pruned, where the weights with the smallest absolute values are removed and the remaining weights are reset to their initial values. This process is repeated until a sparse subnetwork is obtained. The key finding is that this sparse network achieves comparable or better performance than the original network.", "section": "2 Related Work"}, {"figure_path": "aBMESB1Ajx/figures/figures_12_2.jpg", "caption": "Figure 2: Simplified representation of the procedure for finding Lottery Tickets (LTH). A large random neural network (step 1) is trained by iterative pruning with rewind: when the loss reaches a local minimum (step 2), some weights with smallest absolute value are pruned (step 3) and the value of the remaining edges is then reset to that of the initialization (step 4); finally, training is resumed and the final network is obtained (step 5). Remarkably, the sparser subnetwork is consistently able to reach a loss not larger than that right after pruning.", "description": "This figure illustrates the Lottery Ticket Hypothesis (LTH).  It shows a process where a large, randomly initialized neural network is iteratively pruned.  The pruning involves removing weights with small magnitudes and resetting the remaining weights to their initial values.  The key observation is that even after significant pruning, the resulting smaller network can still achieve comparable performance to the original larger network, demonstrating the existence of 'winning tickets' within larger networks.", "section": "Visualizations"}]