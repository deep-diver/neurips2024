[{"figure_path": "bnZZedw9CM/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons of gradient backpropagation between KL, DKL, and IKL losses. DKL loss is equivalent to KL loss regarding backward optimization. M and N can be the same one (like in adversarial training) or two separate (like in knowledge distillation) models determined by application scenarios. Similarly, Xm, Xn \u2208 X can also be the same one (like in knowledge distillation) or two different (like in adversarial training) images. om, on are logits output with which the probability vectors are obtained when applying the softmax activation. Black arrows represent the forward process while colored arrows indicate the backward process driven by the corresponding loss functions in the same color. \u201cwMSE\u201d is a weighted Mean Square Error (MSE) loss. \u201cWMSE", "description": "This figure compares the gradient backpropagation of three different loss functions: KL loss, DKL loss, and IKL loss.  It illustrates how the KL loss is mathematically equivalent to the DKL loss, which is further improved upon by the IKL loss. The figure highlights the asymmetric optimization property of the KL and DKL losses, and how the IKL loss addresses this limitation by enabling gradient flow for all parameters. It also shows how class-wise global information is incorporated into the IKL loss to mitigate biases from individual samples.  The symbols M and N represent models (which may or may not be the same model depending on the application), Xm and Xn are inputs (which may or may not be the same input depending on the application), om and on are the logits, and the arrows illustrate the forward and backward passes of gradients during training.", "section": "3 Method"}, {"figure_path": "bnZZedw9CM/figures/figures_1_2.jpg", "caption": "Figure 2: We achieve SOTA robustness on CIFAR-100. \u201cstar\u201d represents our method while \u201ccircle\u201d denotes previous methods. \u201cBlack\u201d means adversarial training with image preprocessing only including random crop and flip, \u201cBlue\u201d is for methods with AutoAug or CutMix, and \u201cred\u201d represents methods using synthesized data. AA is short for Auto-Attack [10].", "description": "This figure shows the state-of-the-art (SOTA) results achieved by the proposed method (IKL-AT) on the CIFAR-100 dataset in terms of adversarial robustness.  The y-axis represents the clean accuracy and the x-axis represents the adversarial accuracy under Auto-Attack.  Different colored markers represent different categories of methods: black for basic methods, blue for methods using AutoAug or CutMix, and red for those using synthetic data. The star marker specifically indicates the IKL-AT method's performance, showcasing its superior robustness compared to other state-of-the-art methods.", "section": "1 Introduction"}, {"figure_path": "bnZZedw9CM/figures/figures_5_1.jpg", "caption": "Figure 3: Visualization comparisons. (a) t-SNE visualization of the model trained by IKL-AT on CIFAR-100; (b) t-SNE visualization of the model trained by TRADES on CIFAR-100. (c) Class margin differences between models trained by IKL-AT and TRADES.", "description": "This figure presents a comparison of the models trained using IKL-AT and TRADES methods on the CIFAR-100 dataset.  The t-SNE visualizations (a) and (b) show the distribution of features learned by each model, illustrating how IKL-AT produces more compact and separable feature clusters.  (c) provides a histogram comparing the class margin differences between the two models, further demonstrating that IKL-AT leads to larger decision boundaries and stronger robustness.", "section": "3.4 A Case Study and Analysis"}]