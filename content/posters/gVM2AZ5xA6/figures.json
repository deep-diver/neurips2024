[{"figure_path": "gVM2AZ5xA6/figures/figures_0_1.jpg", "caption": "Figure 2: Our method consists of two branches: a reconstruction branch (Sec. 3.1) and an expression branch (Sec. 3.2). We render dual-lifting and expressed Gaussians to get coarse results, and then use a neural renderer to get fine results. Only a small driving part needs to be run repeatedly to drive the expression, while the rest is executed only once.", "description": "This figure illustrates the architecture of the proposed GAGAvatar model. It consists of two main branches: a reconstruction branch that reconstructs a 3D Gaussian-based head avatar from a single source image, and an expression branch that controls the facial expressions of the avatar using a driving image.  The reconstruction branch employs a dual-lifting method to predict the parameters of 3D Gaussians from the source image. These Gaussians, along with additional expression Gaussians generated by the expression branch, are then rendered using splatting, followed by a neural renderer to refine the results and produce a high-fidelity reenacted image. The figure highlights that only a small portion of the model (the driving part) needs to be executed repeatedly for expression control, enhancing efficiency.", "section": "3 Method"}, {"figure_path": "gVM2AZ5xA6/figures/figures_3_1.jpg", "caption": "Figure 2: Our method consists of two branches: a reconstruction branch (Sec. 3.1) and an expression branch (Sec. 3.2). We render dual-lifting and expressed Gaussians to get coarse results, and then use a neural renderer to get fine results. Only a small driving part needs to be run repeatedly to drive the expression, while the rest is executed only once.", "description": "This figure shows a detailed overview of the proposed method's architecture. It consists of two main branches: a reconstruction branch and an expression branch. The reconstruction branch takes a source image as input and reconstructs the 3D structure of the head using a novel dual-lifting method. The expression branch leverages a 3D Morphable Model (3DMM) to control facial expressions by predicting expression Gaussians from both global and local features extracted from the source and driving images. Both branches' outputs are combined and rendered using a neural renderer to generate the final, high-fidelity reenacted image. The diagram highlights that only a small portion of the process (the driving part) is executed repeatedly during reenactment.", "section": "3 Method"}, {"figure_path": "gVM2AZ5xA6/figures/figures_5_1.jpg", "caption": "Figure 3: Cross-identity qualitative results on the VFHQ [Xie et al., 2022] dataset. Compared with baseline methods, our method has accurate expressions and rich details.", "description": "This figure presents a qualitative comparison of cross-identity head avatar reconstruction results between the proposed GAGAvatar method and several state-of-the-art baselines on the VFHQ dataset.  Each row shows results for a different subject, where the first image is the source image, followed by driving images showing expression changes, and the remaining images show the reconstruction results from different methods.  The results highlight GAGAvatar's superior performance in terms of capturing fine facial details and accurate expressions compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "gVM2AZ5xA6/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation results on VFHQ [Xie et al., 2022] datasets. We can see that our full method performs best, especially on facial edges such as glasses in large view angles.", "description": "This figure shows the ablation study results of the proposed method. By comparing different model variations, it demonstrates the impact of each component (dual-lifting, global feature, neural renderer, and lifting loss) on the overall performance of head avatar reconstruction.  The results highlight that the full model with all components achieves superior performance, especially in preserving fine details like glasses, particularly at large view angles.", "section": "3.1 Dual-lifting and Reconstruction Branch"}, {"figure_path": "gVM2AZ5xA6/figures/figures_8_1.jpg", "caption": "Figure 5: Lifting results of an in-the-wild image, include the front view and the top view. Points are filtered by Gaussian opacity. We color two parts of the dual-lifting separately, and the black points are the image plane. It can be seen that the lifted 3D structure is relatively flat without Llifting.", "description": "This figure shows the results of the dual-lifting method on a real-world image, comparing the full dual-lifting approach to a simplified single-plane lifting and to a version without the lifting distance loss.  The visualizations highlight the 3D structure of the reconstructed Gaussian points, revealing how the dual-lifting method produces a more complete and accurate 3D representation compared to the alternatives, particularly when considering the effects of including the lifting distance loss term.", "section": "3.4 Training Strategy and Loss Functions"}, {"figure_path": "gVM2AZ5xA6/figures/figures_9_1.jpg", "caption": "Figure 6: The robustness of our model. Our method can produce reasonable results for low-quality images, challenging lighting conditions, significant occlusions, and extreme expressions.", "description": "This figure demonstrates the robustness of the proposed method across various challenging conditions.  It showcases reenactment results for images with low quality, challenging lighting, significant occlusions (e.g., sunglasses), and extreme expressions.  The results highlight that even under these difficult scenarios, the model is able to generate plausible and realistic results, showcasing its robustness and generalizability.", "section": "4.2 Main Results"}, {"figure_path": "gVM2AZ5xA6/figures/figures_9_2.jpg", "caption": "Figure 2: Our method consists of two branches: a reconstruction branch (Sec. 3.1) and an expression branch (Sec. 3.2). We render dual-lifting and expressed Gaussians to get coarse results, and then use a neural renderer to get fine results. Only a small driving part needs to be run repeatedly to drive the expression, while the rest is executed only once.", "description": "This figure shows the overall architecture of the proposed method, GAGAvatar. It consists of two main branches: a reconstruction branch and an expression branch. The reconstruction branch takes a source image as input and reconstructs the 3D Gaussian representation of the head using the proposed dual-lifting method.  The expression branch leverages a 3D Morphable Model (3DMM) and combines it with global and local features extracted from the source and driving images to control the expression of the generated avatar. Both branches' outputs are combined and rendered using a neural renderer to produce the final, high-fidelity reenacted image. The diagram highlights the efficient design, where only a small part of the network needs to be processed repeatedly for each frame of the driving video to produce real-time results.", "section": "3 Method"}, {"figure_path": "gVM2AZ5xA6/figures/figures_16_1.jpg", "caption": "Figure 8: Reenactment and multi-view results of our method on in-the-wild images. From left to right: input image, driving image, driving and novel view results.", "description": "This figure shows several examples of the proposed method's ability to reenact and generate novel views of faces from in-the-wild images.  For each example, three images are shown. The first image is the input, which serves as the source for the reenactment. The second is the driving image, depicting the target expression and pose. Finally, the third section shows the reenacted image and multiple views of the head. The results demonstrate the method's capability in producing realistic and consistent outputs across different viewpoints.", "section": "3 Method"}, {"figure_path": "gVM2AZ5xA6/figures/figures_17_1.jpg", "caption": "Figure 9: Per-part rendering of the dual-lifting and expression Gaussians. We can see that the dual-lifting Gaussians reconstruct the head's base structure and facial details respectively. It is worth noting that our Gaussians are not purely RGB Gaussians. Instead, our Gaussians include 32-D features (as described in Sec. 3.3). We visualize the first 3 dimensions of these features (i.e., the RGB values of the Gaussians) here without the neural rendering module. So this visualization is intended to intuitively display the functionality of each part and the importance of each branch should not be judged based on RGB values alone.", "description": "This figure shows the results of rendering the dual-lifting Gaussians from the reconstruction branch and the Gaussians from the expression branch separately. The dual-lifting Gaussians reconstruct the head's base structure and facial details, while the expression Gaussians control facial expressions. The visualization uses the first three dimensions (RGB) of the 32-dimensional Gaussian features, without the neural rendering module, for better understanding of each part's role.", "section": "3. Method"}, {"figure_path": "gVM2AZ5xA6/figures/figures_17_2.jpg", "caption": "Figure 5: Lifting results of an in-the-wild image, include the front view and the top view. Points are filtered by Gaussian opacity. We color two parts of the dual-lifting separately, and the black points are the image plane. It can be seen that the lifted 3D structure is relatively flat without Llifting.", "description": "This figure shows the results of the dual-lifting method applied to an in-the-wild image. The left column shows the source images. The middle column shows the results of the dual-lifting method applied. The right column shows the results of dual-lifting method without the lifting distance loss. The results clearly show that the dual-lifting method with the lifting distance loss produces a 3D structure that is more consistent with the source image, while the dual-lifting method without the lifting distance loss produces a relatively flat 3D structure.", "section": "3.4 Training Strategy and Loss Functions"}, {"figure_path": "gVM2AZ5xA6/figures/figures_18_1.jpg", "caption": "Figure 11: Self-identity reenactment results on VFHQ [Xie et al., 2022] and HDTF [Zhang et al., 2021] datasets. The top six rows are from VFHQ and the bottom three rows are from HDTF.", "description": "This figure shows a comparison of self-identity reenactment results between the proposed method and several baseline methods on two datasets, VFHQ and HDTF.  The top six rows display results from VFHQ, while the bottom three rows show results from HDTF. Each row represents a different video sequence, with the source image followed by the results generated by each method. This allows for a visual comparison of the quality and accuracy of different approaches in recreating head avatar expressions.", "section": "4 Experiments"}, {"figure_path": "gVM2AZ5xA6/figures/figures_18_2.jpg", "caption": "Figure 1: Our method can reconstruct animatable avatars from a single image, offering strong generalization and controllability with real-time reenactment speeds.", "description": "This figure demonstrates the results of the proposed Generalizable and Animatable Gaussian head Avatar (GAGAvatar) method. It shows that the model can successfully reconstruct animatable avatars from a single input image.  The reconstruction maintains high fidelity to the source image, showing strong generalization capabilities.  Furthermore, the avatars can be animated with real-time reenactment speeds, and they have strong controllability in terms of expressions. This showcases the core capabilities of the GAGAvatar method: one-shot reconstruction, strong generalization, high fidelity, and real-time reenactment.", "section": "Abstract"}, {"figure_path": "gVM2AZ5xA6/figures/figures_19_1.jpg", "caption": "Figure 3: Cross-identity qualitative results on the VFHQ [Xie et al., 2022] dataset. Compared with baseline methods, our method has accurate expressions and rich details.", "description": "This figure displays a comparison of cross-identity qualitative results between the proposed method and several baseline methods using the VFHQ dataset.  Each column represents a different method, with the first column being the source image and the second column being the driving image. The subsequent columns showcase the reenactment results of each method.  The caption highlights that the proposed method outperforms the baselines in terms of expression accuracy and detail.", "section": "4 Experiments"}, {"figure_path": "gVM2AZ5xA6/figures/figures_20_1.jpg", "caption": "Figure 14: Reenactment and multi-view results of our method on in-the-wild images. From left to right: input image, driving image, driving and novel view results.", "description": "This figure showcases the performance of the proposed method on real-world images. For each row, the leftmost image is the input source image, the second image is the driving image used to animate the avatar, and the remaining images present the reenactment results from various viewpoints.  The results demonstrate the method's ability to generate realistic and consistent avatars with accurate expression transfer, even under significant viewpoint changes and in complex scenarios.", "section": "More Qualitative Results"}, {"figure_path": "gVM2AZ5xA6/figures/figures_21_1.jpg", "caption": "Figure 1: Our method can reconstruct animatable avatars from a single image, offering strong generalization and controllability with real-time reenactment speeds.", "description": "This figure shows the pipeline of the proposed method. Given a single source image, the model reconstructs an animatable 3D Gaussian-based head avatar. The figure demonstrates the input source images, the driving images used for animation, and the resulting reconstructed images, highlighting the method's ability to generate realistic and animatable avatars from limited input.", "section": "Abstract"}, {"figure_path": "gVM2AZ5xA6/figures/figures_22_1.jpg", "caption": "Figure 16: Qualitative results and video continuous frame results with highlighted attention regions. We selected competitive methods to show continuous frames. Better to view it zoomed in.", "description": "This figure shows a comparison of qualitative results between the proposed method and other state-of-the-art methods for self-reenactment and cross-reenactment tasks. The highlighted regions in the images draw attention to the details reconstructed by each method. The figure demonstrates that the proposed method achieves superior performance compared to existing methods by producing high-quality reconstruction, detailed expressions, and better handling of pose variations, especially in challenging views.", "section": "4.2 Main Results"}, {"figure_path": "gVM2AZ5xA6/figures/figures_22_2.jpg", "caption": "Figure 6: The robustness of our model. Our method can produce reasonable results for low-quality images, challenging lighting conditions, significant occlusions, and extreme expressions.", "description": "This figure demonstrates the robustness of the proposed method across various challenging conditions. It showcases successful avatar reconstruction even with low-quality input images, images with challenging lighting, images with significant occlusions, and images featuring extreme facial expressions.  This highlights the model's ability to generalize well and produce high-quality results despite these difficulties.", "section": "4 Experiments"}]