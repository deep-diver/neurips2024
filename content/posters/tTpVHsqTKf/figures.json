[{"figure_path": "tTpVHsqTKf/figures/figures_1_1.jpg", "caption": "Fig. 1. Comparison of video instance segmentation paradigms. Previous methods (left part) like VITA [15] adopt asynchronous query-sensitive structures to model instance appearances and trajectories. Our model (right part) employs frame and video embeddings in a query-robust synchronous manner, and they synchronize with each other through the transformer decoder to generate the refined video-level query embeddings for the prediction. Also, we employ a synchronized embedding optimization strategy 'Sync. Optim.' instead of the classic optimization approach.", "description": "This figure compares the asynchronous and synchronous approaches of video instance segmentation.  The left side shows the traditional approach, where frame-level queries are processed independently before being integrated with video-level queries which may result in a loss of information and increased complexity. The right side illustrates the SyncVIS approach where frame and video-level queries are processed synchronously. The synchronized processing allows for mutual learning and refinement, resulting in a more robust and effective model.", "section": "1 Introduction"}, {"figure_path": "tTpVHsqTKf/figures/figures_3_1.jpg", "caption": "Fig. 2. Overview of the proposed synchronous video-frame modeling framework SyncVIS. The developed synchronized video-frame modeling paradigm enables video-level embeddings and frame-level ones to synchronize with each other in each stage of the decoder. SyncVIS also suggests a new synchronized embedding optimization strategy. As shown in the right part, SyncVIS decouples the input video frames into several sub-clips and feeds each sub-clip into the mask and classification head. By applying these modules, SyncVIS can incorporate both semantics and movement of instances in each frame in a synchronous manner for superior characterizing ability.", "description": "The figure illustrates the SyncVIS framework, emphasizing its two key components: synchronized video-frame modeling and synchronized embedding optimization.  The synchronized video-frame modeling uses a transformer decoder to integrate video-level and frame-level embeddings, enhancing interaction and mutual learning. The synchronized embedding optimization divides the video into smaller clips, simplifying the optimization process and improving performance.  The overall effect is a more robust and effective approach to video instance segmentation.", "section": "3.2 Overall Architecture"}, {"figure_path": "tTpVHsqTKf/figures/figures_7_1.jpg", "caption": "Fig. 3. Ablation study on the complexity of video scenarios regarding the number of input frames T.", "description": "This figure shows the performance of Mask2Former-VIS and SyncVIS models on YouTube-VIS 2019 validation set with varying numbers of input frames.  The x-axis represents the number of input frames (T), while the y-axis represents the average precision (AP) in percentage.  The plot demonstrates that Mask2Former-VIS performance degrades as the number of input frames increases, while SyncVIS shows improved and more stable performance, indicating its better ability to handle the complexity of long-range video sequences.", "section": "4.2 Ablation Studies"}, {"figure_path": "tTpVHsqTKf/figures/figures_14_1.jpg", "caption": "Fig. 4. Visual comparison of our SyncVIS with Mask2Former-VIS ('M2F') [6] and VITA [15]. SyncVIS shows impressive accuracy in long, complex scenarios where objects share similar appearances and have heavy occlusions.", "description": "This figure compares the performance of SyncVIS against Mask2Former-VIS and VITA on challenging video instance segmentation tasks.  It showcases SyncVIS's ability to accurately segment objects even in videos with long sequences, similar-looking objects, and significant occlusions, highlighting its superior performance compared to existing methods.", "section": "Visualization"}, {"figure_path": "tTpVHsqTKf/figures/figures_14_2.jpg", "caption": "Fig. 4. Visual comparison of our SyncVIS with Mask2Former-VIS ('M2F') [6] and VITA [15]. SyncVIS shows impressive accuracy in long, complex scenarios where objects share similar appearances and have heavy occlusions.", "description": "This figure compares the performance of SyncVIS against two state-of-the-art video instance segmentation methods: Mask2Former-VIS and VITA.  The figure highlights SyncVIS's superior accuracy in challenging video sequences characterized by long durations, objects with similar appearances, and significant occlusions.  The visual examples demonstrate that SyncVIS produces more precise and complete segmentations compared to the other methods in these difficult scenarios.", "section": "C Visualization"}, {"figure_path": "tTpVHsqTKf/figures/figures_15_1.jpg", "caption": "Fig. 4. Visual comparisons of our SyncVIS with Mask2Former-VIS ('M2F') [6] and VITA [15]. SyncVIS shows impressive accuracy in long, complex scenarios where objects share similar appearances and have heavy occlusions.", "description": "This figure compares the performance of SyncVIS against two state-of-the-art video instance segmentation methods, Mask2Former-VIS and VITA, on challenging video sequences.  The examples demonstrate SyncVIS's superior ability to accurately segment and track objects even when they have similar appearances or are heavily occluded.  The improved performance highlights the effectiveness of SyncVIS in handling complex real-world scenarios.", "section": "C Visualization"}, {"figure_path": "tTpVHsqTKf/figures/figures_15_2.jpg", "caption": "Fig. 7. Visual comparison of our SyncVIS with Mask2Former-VIS (abbreviated as 'M2F') [6] and VITA [15]. SyncVIS shows impressive accuracy in long videos, while the previous methods have either low confidence (the confidence of the car in blue masks in the first row is 77% while in the third row is 98%) or incomplete masks (the first frame in the second row).", "description": "This figure compares the performance of SyncVIS against Mask2Former-VIS and VITA on long videos.  SyncVIS demonstrates superior accuracy in maintaining instance segmentation consistency across longer video sequences where the previous methods struggle. The example shows that Mask2Former-VIS and VITA have either low confidence scores or produce incomplete masks for some objects in the video.", "section": "4. Visualization"}, {"figure_path": "tTpVHsqTKf/figures/figures_16_1.jpg", "caption": "Fig. 8. Qualitative comparisons with different designs of embeddings. Video-level embeddings are from a set of shared instance queries for all sampled frames. In the first row, video-level embeddings successfully capture most of the instances, but fail to mask the fish in the middle of the image. Frame-level embeddings are assigned to each sampled frame. In the second row, frame-level embeddings segment instances better than the first row, but fail to maintain the trajectories of fish in the bottom right. When synchronizing these two sets of embeddings, our model achieves better segmentation results even under such a complex scenario.", "description": "This figure compares the video instance segmentation results using different embedding strategies: only video-level embeddings, only frame-level embeddings, and the combined synchronized video-frame embeddings.  It highlights the advantages of using a synchronized approach, showing better segmentation accuracy and tracking of objects, particularly in complex scenarios with occlusion and motion.", "section": "C Visualization"}]