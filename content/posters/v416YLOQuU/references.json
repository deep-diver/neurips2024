{"references": [{"fullname_first_author": "Boris T Polyak", "paper_title": "Acceleration of stochastic approximation by averaging", "publication_date": "1992-01-01", "reason": "This paper introduces the model exponential moving average (EMA), a key technique analyzed in the current paper."}, {"fullname_first_author": "Diederik Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-01-01", "reason": "This paper introduces the Adam optimizer, which is the main focus of the theoretical analysis in the current paper."}, {"fullname_first_author": "Ashok Cutkosky", "paper_title": "Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion", "publication_date": "2023-01-01", "reason": "This paper provides the online-to-nonconvex conversion framework, the foundation of the current paper's analysis."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Understanding Adam optimizer via online learning of updates: Adam is FTRL in disguise", "publication_date": "2024-01-01", "reason": "This paper offers crucial insights into the inner workings of Adam, which are leveraged in the current paper's analysis."}, {"fullname_first_author": "Qinzi Zhang", "paper_title": "Random scaling and momentum for non-smooth non-convex optimization", "publication_date": "2024-01-01", "reason": "This paper combines insights from discounted online learning and nonconvex optimization, providing a critical theoretical building block for the current work."}]}