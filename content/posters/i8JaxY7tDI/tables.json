[{"figure_path": "i8JaxY7tDI/tables/tables_5_1.jpg", "caption": "Table 1: Details of router design. Following the standard Transformer architecture [31], the inserted router adds only 18 million additional parameters.", "description": "This table provides details about the design of the router used in the Read-ME model.  It shows the number of layers, heads, vocabulary size, embedding and feature dimensions, MLP intermediate dimension, activation function, positional embedding method, normalization technique, and the total number of parameters in the router.  The router is a relatively small component, adding only 18 million parameters to the overall model architecture.", "section": "5.1 Experimental Details"}, {"figure_path": "i8JaxY7tDI/tables/tables_6_1.jpg", "caption": "Table 3: Downstream task evaluation of our proposed method (Read-ME) compared to open-source models, including dense models Pythia and Open-Llama-v2, the MoE model OpenMoE, and the compression method Sheared-Llama. The \"#Param\" column presents in the form of (# Activated-Parameters - # Total-Parameters). Training cost is measured by the number of tokens used. For compression methods like ours and Sheared-Llama, only tokens used for conversion are counted, excluding Llama-2 pre-training costs.", "description": "This table compares the performance of the proposed Read-ME model against several other open-source large language models (LLMs) on various downstream tasks.  It shows metrics such as accuracy on different benchmarks (MMLU, WinoGrande, ARC-Easy, ARC-Challenge, LogiQA, CoQA) and the number of parameters used. Importantly, it highlights the training cost (number of tokens) which emphasizes the efficiency of the Read-ME approach.", "section": "5.2 Downstream Task Evaluations"}, {"figure_path": "i8JaxY7tDI/tables/tables_7_1.jpg", "caption": "Table 3: Downstream task evaluation of our proposed method (Read-ME) compared to open-source models, including dense models Pythia and Open-Llama-v2, the MoE model OpenMoE, and the compression method Sheared-Llama. The \"#Param\" column presents in the form of (# Activated-Parameters - # Total-Parameters). Training cost is measured by the number of tokens used. For compression methods like ours and Sheared-Llama, only tokens used for conversion are counted, excluding Llama-2 pre-training costs.", "description": "This table compares the performance of the proposed Read-ME model against several other open-source LLMs on various downstream tasks.  It shows the number of parameters, training cost (in tokens), and performance metrics (accuracy) across different benchmarks including MMLU, HellaSwag, Winogrande, ARC-Easy, ARC-Challenge, LogiQA, and CoQA.  The table highlights Read-ME's superior performance despite a significantly lower training cost.", "section": "5.2 Downstream Task Evaluations"}, {"figure_path": "i8JaxY7tDI/tables/tables_9_1.jpg", "caption": "Table 4: Cache hit ratio measured in batched inference setup.", "description": "This table presents the cache hit ratios achieved by three different cache replacement policies (Random, LRU, and Belady) under varying cache capacities (2, 3, 4, and 5).  The Belady policy consistently demonstrates the highest hit ratio, highlighting its superiority in the context of batched inference in the Read-ME system. The results showcase the effectiveness of the Belady-inspired caching strategy employed in Read-ME, which optimizes memory usage and latency during inference by pre-computing expert selections.", "section": "4.2 Expert-aware Batching"}, {"figure_path": "i8JaxY7tDI/tables/tables_14_1.jpg", "caption": "Table 5: We compare the Read-ME performance with dense model, and report the MMLU performance and perplexity on 7 data domains. By adopting an MoE as the target structure instead of dense model, our model achieve significantly better overall performance.", "description": "This table compares the performance of the Read-ME model with a dense model.  The comparison is done using the MMLU benchmark and perplexity scores across seven different datasets (Arxiv, Books, C4, Common Crawl, Github, StackExchange, Wikipedia). Read-ME shows significantly better overall performance by using a Mixture-of-Experts (MoE) architecture instead of a dense model.", "section": "A.2 MoE Achieves Better Efficiency-Accuracy Trade-off than Dense Models."}, {"figure_path": "i8JaxY7tDI/tables/tables_14_2.jpg", "caption": "Table 3: Downstream task evaluation of our proposed method (Read-ME) compared to open-source models, including dense models Pythia and Open-Llama-v2, the MoE model OpenMoE, and the compression method Sheared-Llama. The \"#Param\" column presents in the form of (# Activated-Parameters - # Total-Parameters). Training cost is measured by the number of tokens used. For compression methods like ours and Sheared-Llama, only tokens used for conversion are counted, excluding Llama-2 pre-training costs.", "description": "This table compares the performance of the Read-ME model against several other open-source models on various downstream tasks.  It shows the number of parameters, training cost (in tokens), and performance metrics (accuracy) for each model on tasks like MMLU, HellaSwag, Winogrande, ARC, LogiQA, and CoQA.  The table highlights the efficiency of Read-ME in achieving comparable or better performance with significantly reduced training cost and parameters.", "section": "5.2 Downstream Task Evaluations"}, {"figure_path": "i8JaxY7tDI/tables/tables_15_1.jpg", "caption": "Table 3: Downstream task evaluation of our proposed method (Read-ME) compared to open-source models, including dense models Pythia and Open-Llama-v2, the MoE model OpenMoE, and the compression method Sheared-Llama. The \"#Param\" column presents in the form of (# Activated-Parameters - # Total-Parameters). Training cost is measured by the number of tokens used. For compression methods like ours and Sheared-Llama, only tokens used for conversion are counted, excluding Llama-2 pre-training costs.", "description": "This table compares the performance of the proposed Read-ME model against several other open-source large language models (LLMs) on various downstream tasks.  It shows the number of parameters, training cost (in tokens), and performance metrics (accuracy) across different benchmarks, such as MMLU, Hellaswag, and ARC.  It highlights that Read-ME achieves competitive performance while using significantly fewer training tokens than other methods, demonstrating its efficiency in leveraging pre-trained models.", "section": "5.2 Downstream Task Evaluations"}, {"figure_path": "i8JaxY7tDI/tables/tables_15_2.jpg", "caption": "Table 8: Latency [ms] comparison between Traditional router and Auto-regressive router", "description": "This table compares the latency in milliseconds (ms) between the traditional router and the auto-regressive router for different batch sizes (bsz). It breaks down the latency into three components: Router, Attention, and Expert/MLP, and provides the total sum for each configuration.  The auto-regressive router consistently shows lower latency across all batch sizes.", "section": "5.3 Pre-gating and Expert-aware Batching"}, {"figure_path": "i8JaxY7tDI/tables/tables_15_3.jpg", "caption": "Table 9: Latency breakdown comparison between Traditional router and Auto-regressive router", "description": "This table presents a breakdown of latency across different components (Router, Attention, Expert/MLP) for both traditional and auto-regressive routers under varying batch sizes (bsz). It demonstrates how the latency is distributed across these components and how this distribution changes with batch size for the two router types.", "section": "5.3 Pre-gating and Expert-aware Batching"}]