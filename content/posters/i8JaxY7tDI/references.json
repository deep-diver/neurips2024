{"references": [{"fullname_first_author": "Carlos Riquelme", "paper_title": "Scaling vision with sparse mixture of experts", "publication_date": "2021-00-00", "reason": "This paper is foundational to the concept of sparse Mixture-of-Experts (MoE) models, which are central to the current work's methodology."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper introduces Switch Transformers, a significant advancement in scaling large language models efficiently, directly relevant to the current paper's focus on efficient LLM inference."}, {"fullname_first_author": "Albert Q Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-01-00", "reason": "The Mixtral model, detailed in this paper, serves as a primary example of a state-of-the-art MoE model, providing a benchmark for comparison and analysis in the current research."}, {"fullname_first_author": "Aran Komatsuzaki", "paper_title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints", "publication_date": "2022-12-00", "reason": "This paper presents the technique of 'upcycling' pre-trained dense LLMs into MoE models, which is contrasted and improved upon by the novel framework in the current paper."}, {"fullname_first_author": "Gyeong-In Yu", "paper_title": "Orca: A distributed serving system for {Transformer-Based} generative models", "publication_date": "2022-00-00", "reason": "This paper addresses distributed serving of transformer-based models, a relevant topic to the current work's considerations of efficient LLM inference in resource-constrained settings."}]}