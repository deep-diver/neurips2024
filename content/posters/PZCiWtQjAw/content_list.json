[{"type": "text", "text": "Continual Audio-Visual Sound Separation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weiguo Pian1 Yiyang Nan2 Shijian Deng1 Shentong $\\mathbf{M0}^{3}$ Yunhui Guo1 Yapeng Tian1 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The University of Texas at Dallas 2 Brown University 3 Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans can effortlessly separate and identify individual sound sources in daily experience [25, 7, 64, 33]. This skill plays a crucial role in our ability to understand and interact with the complex auditory environments that surround us [34]. However, replicating this capability in machines remains a significant challenge due to the inherent complexity of real-world auditory scenes [7, 77]. Inspired by the multisensory perception of humans [62, 60], audio-visual sound separation tackles this challenge by utilizing visual information to guide the separation of individual sound sources in an audio mixture. ", "page_idx": 0}, {"type": "text", "text": "Recent advances in deep learning have led to significant progress in audio-visual sound separation [84, 23, 21, 67, 14, 65, 81, 63, 11, 71]. Beneftiing from more advanced architectures (e.g., U-Net [84, 23], Transformer [14], and diffusion models [27]) and discriminative visual cues (e.g., grounded visual objects [67], motion [83], and dynamic gestures [21]), audio-visual separation models are able to separate sounds ranging from domain-specific speech, musical instrument sounds to open-domain general sounds within training sound categories. However, a limitation of these studies is their focus on scenarios where all sound source classes are presently known, overlooking the potential inclusion of unknown sound source classes during inference in real-world applications. This oversight leads to the catastrophic forgetting issue [32, 3], where the fine-tuning of models on new classes detrimentally impacts their performance on previously learned classes. Despite Chen et al. [14] demonstrating that their iQuery model can generalize to new classes well through simple fine-tuning, it still suffers from the catastrophic forgetting problem on old classes. This prevents the trained models from continuously updating in real-world scenarios, impeding their adaptability to dynamic environments. The question how to effectively leverage visual guidance to continuously separate sounds from new categories while preserving separation ability for old sound categories remains open. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To bridge this gap, we introduce a novel continual audio-visual sound separation task by integrating audiovisual sound separation with continual learning principles. The goal of this task is to develop an audio-visual model that can continuously separate sound sources in new classes while maintaining performance on previously learned classes. The key challenge we need to address is catastrophic forgetting during continual audio-visual learning, which occurs when the model is updated solely with data from new classes or tasks, resulting in a significant performance drop on old ones. We illustrate our new task and the catastrophic forgetting issue in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "Unlike typical continual learning problems such as task-, domain-, or classincremental classification in visual domains [2, 57, 38, 53, 85], which result in progressively increasing logits (or probability distribution) across all ob", "page_idx": 1}, {"type": "image", "img_path": "PZCiWtQjAw/tmp/04e707a5a583c4d3e9a3929f6e839c25d640a43ad323d9b42f036a537f1b8884.jpg", "img_caption": ["Figure 1: Top: Illustration of the continual audio-visual sound separation task, where the model (separator) learns from sequential audio-visual sound separation tasks. Bottom: Illustration of the catastrophic forgetting problem in continual audio-visual sound separation and its mitigation by our proposed method. Fine-tuning: Directly fine-tune the separation model on new sound source classes; Upper bound: Train the model using all training data from seen sound source classes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "served classes at each incremental step, our task uniquely produces fixed-size separation masks throughout all incremental steps. In this context, each entry in the mask does not directly correspond to any specific classes. Additionally, the new task involves both audio and visual modalities. Therefore, simply applying existing visual-only methods cannot fully exploit and preserve the inherent cross-modal semantic correlations. Very recently, Pian et al. [53] and Mo et al. [44] extended continual learning to the audio-visual domain, but both focused on classification tasks. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, in this paper, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). Upon the framework, we introduce a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to not only maintain the cross-modal semantic similarity through incremental tasks but also preserve previously learned knowledge of semantic similarity in old models to counter catastrophic forgetting. The CrossSDC is a generic constraint that can be seamlessly integrated into the training process of different audio-visual sound separators. To evaluate the effectiveness of our proposed ContAV-Sep, we conducted experiments on the MUSIC-21 dataset within the framework of continual learning, using the state-of-the-art audio-visual sound separation model iQuery [14] and a representative audio-visual sound separation model Co-Separation [23], as our separation base models. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance than other continual learning baselines. In summary, this paper contributes follows: ", "page_idx": 1}, {"type": "text", "text": "(i) To explore more practical audio-visual sound separation, in which the separation model should be generalized to new sound source classes continually, we pose a Continual Audio-Visual Sound Separation task that trains the separation model under the setting of continual learning. To the best of our knowledge, this is the first work on continual learning for audio-visual sound separation. ", "page_idx": 1}, {"type": "text", "text": "(ii) We propose ContAV-Sep for the new task. It uses a novel cross-modal similarity distillation constraint to preserve cross-modal semantic similarity knowledge from previously learned models. ", "page_idx": 1}, {"type": "text", "text": "(iii) Experiments on the MUSIC-21 dataset can validate the effectiveness of our ContAV-Sep, demonstrating promising performance gain over baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Audio-Visual Sound Separation. Audio-visual sound separation aims to separate individual sound sources from an audio mixture guided by visual cues. A line of research emerges under various scenarios, such as separating musical instruments [23, 84, 79, 21, 83, 67], human speech [20, 1, 18, 49, 15], or sound sources in in-the-wild videos [22, 70]. Many frameworks and methods have been proposed to address challenges within specific problem settings. For instance, the extraction of face embeddings proves beneficial for speech audio separation [18]. Moreover, incorporating object detection can provide an additional advantage [23, 22]. The utilization of trajectory optical flows to leverage temporal motion information in videos, as demonstrated by [83], also yields improvements. In this work, not competing on designing stronger separators, we would advance the exploration of the audio-visual sound separation within the paradigm of continual learning. We investigate how a model can learn to consistently separate sound sources from sequential separation tasks without forgetting previously acquired knowledge. ", "page_idx": 2}, {"type": "text", "text": "Continual Learning. The field of continual learning has drawn significant attention, especially in visual domains, with various approaches addressing this challenge. Notable among these are regularization-based methods, exemplified in works such as [32, 3, 31, 39]. These approaches involve applying regularization to crucial parameters associated with old tasks to maintain the model\u2019s capabilities and during incremental steps, less important parameters are given higher priority for updates compared to important ones. Conversely, several works [57, 9, 5, 26, 12, 54, 8, 10, 40] applied rehearsal-based pipelines to enable the model review previously learned knowledge. For instance, Rebuff iet al. [57] proposed one of the most representative exemplars selection strategy Nearest-Meanof-Exemplars (NME), selects the most representative exemplars in each class based on the distance to the feature center of the class. Meanwhile, pseudo-rehearsal [47, 48, 66] employs generative models to create pseudo-exemplars based on the estimated distribution of data from previous classes. Moreover, architecture-based/dynamic architecture methods[52, 4, 46, 24, 28, 37, 40] proposed to modify the model architecture itself to enable the model to acquire new knowledge while mitigating the forgetting of old knowledge. Specifically, Pham et al. [52] proposed a dual network architecture, in which one is to learn new tasks while the other one is for retaining knowledge learned from old tasks. Wang et al. [72] combined the dynamic architecture and distillation constraint to mitigate the issue of continual-increasing overhead problem in dynamic architecture-based continual learning method. However, above studies mainly concentrate on the area of continual image classification. Recently, researchers also explored other continual learning scenarios beyond image classification. For instance, Park et al. [50] extend the knowledge distillation-based [2, 17] continual image classification method to the domain of video by proposing the time-channel distillation constraint. Douillard et al. [16] proposed to tackle the continual semantic segmentation task with multi-view feature distillation and pseudo-labeling. Xiao et al. [78] further addressed the continual semantic segmentation problem through weights fusion strategy between old and current models. Wang et al. [75] addressed the continual sound classification task through generative replay. Furthermore, continual learning has also been explored in the domain of language/vision-language learning tasks [30, 43, 59, 61, 19, 86], self-supervised representation learning [19, 42, 55, 80, 35, 74], audio classification [6, 73] and fake audio detection [41, 82], etc. Despite the success of existing continual learning methods in various scenarios, their applicability in the domain of continual audio-visual sound separation is still unexplored. Although Pian et al. [53] and Mo et al. [44] proposed to tackle the catastrophic problem in audio-visual learning, their studies mainly concentrated in the area of audio-visual video classification. In contrast to existing works in continual learning, in this paper, we delves into the continual audio-visual sound separation, aiming to tackle the challenge of catastrophic forgetting specifically in the context of separation mask prediction for complicated mixed audio signals within joint audio-visual modeling. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Audio-Visual Sound Separation. Audio-visual sound separation aims to separate distinctive sound signals according to the given associated visual guidance. Following previous works [14, 23, 67, 21, 79], we adopt the common \u201cmix-and-separation\u201d training strategy to train the model. Given two videos $\\bar{V}_{1}(s_{1},\\bar{v}_{1})$ and $V_{2}(s_{2},v_{2})$ , we can obtain the input mixed sound signal $\\boldsymbol{S}$ by mixing two video sound signals $s_{1}$ and $s_{2}$ , and then we can have the ratio masks $M^{1}=s_{1}/S$ and $M^{2}=s_{2}/S^{1}$ . The goal of the task is to utilize the corresponding visual guidance $\\pmb{v}_{1}$ and $v_{2}$ to predict the ratio masks for reconstructing the two individual audio signals. This process can be formulated as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol{M}}^{1}=\\mathcal{F}_{\\Theta}(\\boldsymbol{S},\\boldsymbol{v}_{1}),}\\\\ {\\hat{\\boldsymbol{M}}^{2}=\\mathcal{F}_{\\Theta}(\\boldsymbol{S},\\boldsymbol{v}_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\mathcal{F}}_{\\Theta}$ is the separation model with trainable parameters $\\Theta$ . And then, the original sound signals $s_{1}$ and $s_{2}$ are used to calculate the loss function for optimizing the model: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\operatorname*{argmin}_{\\Theta}\\mathbb{E}_{(V_{1},V_{2})\\sim\\mathcal{D}}\\Big[\\mathcal{L}(\\hat{M}^{1},M^{1})+\\mathcal{L}(\\hat{M}^{2},M^{2})\\Big],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{D}$ denotes the training set, and $\\mathcal{L}$ is the loss function between the prediction and ground-truth. ", "page_idx": 3}, {"type": "text", "text": "Continual Audio-Visual Sound Separation. Our proposed continual audio-visual sound separation task aims to train a model ${\\mathcal{F}}_{\\Theta}$ continually on a sequence of $T$ separation tasks $\\{\\tau_{1},\\tau_{2},...,\\tau_{T}\\}$ . For the $t$ -th task $\\mathcal{T}_{t}$ (incremental step $t$ ), we have a training set $\\mathcal{D}_{t}=\\{V^{i}(\\pmb{s}^{i},\\pmb{v}^{i}),y_{t}^{i}\\}_{i=1}^{n_{t}}$ , where $i$ and $n_{t}$ denote the $i$ -th video sample and the total number of samples in $\\mathcal{D}_{t}$ respectively, and $y_{t}^{i}\\in\\mathcal{C}_{t}$ is the corresponding sound source class of video $V^{i}$ , where $\\ensuremath{\\mathcal{C}}_{t}$ is the training sound class label space of task $\\mathcal{T}_{t}$ . For any two tasks $\\mathcal{T}_{t_{1}}$ and ${\\mathcal{T}}_{t_{2}}$ and their corresponding training sound class label space $\\mathcal{C}_{t_{1}}$ and $\\mathcal{C}_{t_{2}}$ , we have $\\mathcal{C}_{t_{1}}\\cap\\mathcal{C}_{t_{2}}=\\emptyset$ . Following previous works in continual learning [57, 2, 53, 44, 29, 76], for a task $\\mathcal{T}_{t}$ , where $t>1$ , holding a small size of memory/exemplar set $\\mathcal{M}_{t}$ to store some data from old tasks is permitted in our setting. Therefore, with the memory/exemplar set $\\mathcal{M}_{t}$ , all available data that can be used for training in task $\\mathcal{T}_{t}$ $t>1)$ ) can be denoted as $\\mathcal{D}_{t}^{\\prime}=\\mathcal{D}_{t}\\cup\\mathcal{M}_{t}$ . Finally, the training process of Eq. 2 in our continual audio-visual sound separation setting can be denoted as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\begin{array}{c}{{\\Theta_{t}=\\arg\\!\\operatorname*{min}\\mathbb{E}_{(V_{1},V_{2})\\sim\\mathcal{D}_{t}^{\\prime}}\\Big[\\mathcal{L}(\\hat{M}^{1},M^{1})+\\mathcal{L}(\\hat{M}^{2},M^{2})\\Big],}}\\\\ {{s.t.~~\\hat{M}^{1}=\\mathcal{F}_{\\Theta_{t-1}}(S,v_{1}),~\\hat{M}^{2}=\\mathcal{F}_{\\Theta_{t-1}}(S,v_{2}),}}\\end{array}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which means that the new model $\\mathbf{{\\Theta}}_{t}$ is obtained by updating the old model $\\Theta_{t-1}$ which was trained on the previous task, using the current task\u2019s available data $\\mathcal{D}_{t}^{\\prime}$ . After the training process for task $\\mathcal{T}_{t}$ with $\\mathcal{D}_{t}^{\\prime}$ , the updated model will be evaluated on a testing set which includes video samples from all seen sound source classes up to continual step $t$ (task ${\\mathcal{T}}_{t}$ ). And the evaluation also follows the common \u201cmix-and-separation\u201d strategy. During this continual learning process, the model\u2019s separation performance on the previously learned tasks drops significantly after training on new tasks. This learning issue is referred to as the catastrophic forgetting [32, 38, 3] problem, which poses a considerable challenge in continual audio-visual sound separation. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the challenge of catastrophic forgetting in continual audio-visual sound separation, we introduce ContAV-Sep. This new framework, illustrated in Fig. 2, consists of three key components: a separation base model, an output mask distillation module, and our proposed Cross-modal Similarity Distillation Constraint (CrossSDC). We use a recent state-of-the-art audio-visual separator: iQuery [14] as the base model of our approach, which contains a video encoder to extract the global motion feature, an object detector and image encoder to obtain the object feature, a U-Net [58] for mixture sound encoding and separated sound decoding, and an audio-visual Transformer to get the separated sound feature through multi-modal cross-attention mechanism and class-aware audio queries. For the object detector, we follow iQuery [14] and use the pre-trained Detic [87], a universal object detector, to detect the sound source objects in each frame. For the video encoder and the image encoder, inspired by the excellent generalization ability of recent self-supervised pre-trained models, which has been proven to be effective and appropriate in continual learning [53], we apply two self-supervised pre-trained models VideoMAE [69] and CLIP [56] as the video encoder and the image encoder, respectively. Note that, during the training process, the object detector, video encoder, and image encoder are frozen. ", "page_idx": 3}, {"type": "image", "img_path": "PZCiWtQjAw/tmp/a52df1477bc6ebed6b07e711cc21d035e8637b360e5dbd35ff3946dc4277be96.jpg", "img_caption": ["Figure 2: Overview of our proposed ContAV-Sep, which consists of an audio-visual sound separation base model architecture, an Output Mask Distillation, and our proposed Cross-modal Similarity Distillation Constraint. The fire icon denotes the module is trainable, while the snowflake icon denotes that the module is frozen. The (i)STFT stands for (inverse) Short-Time Fourier Transform. Please note that, the old model $\\mathcal{F}_{\\Theta_{t-1}}$ is frozen during training. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Given a pair of videos $V_{1}(s_{1},v_{1})$ and $V_{2}(s_{2},v_{2})$ , at incremental step $t$ (task $\\mathcal{T}_{t}$ ), the U-Net audio encoder $\\hat{\\mathcal{F}}_{t}^{A E}$ takes the mixed audio signal $\\boldsymbol{S}$ obtained by mixing $s_{1}$ and $s_{2}$ as input, and generates the latent mixed audio feature. This process can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{t}^{l a t.}=\\mathcal{F}_{t}^{A E}(S),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the audio-visual Transformer $\\mathcal{F}_{t}^{T r a n s}$ .is employed to generate the separated sound feature by taking the latent mixed audio feature and visual features as inputs: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{f}_{t}^{a,1}=\\mathcal{F}_{t}^{T r a n s.}(\\pmb{f}_{t}^{l a t.},\\pmb{f}_{t}^{o,1},\\pmb{f}_{t}^{m,1}),}\\\\ {s.t.\\quad\\pmb{f}_{t}^{o,1}=\\pmb{U}_{t}^{o}(O b j.^{1}),\\ \\pmb{f}_{t}^{m,1}=\\pmb{U}_{t}^{m}(M o.^{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{f}_{t}^{a,1}$ denotes the separated sound feature of video $V_{1};O b j$ .1 and $M o$ .1 denote the object and motion features extracted by the frozen pre-trained image and video encoders respectively from the visual signal $\\pmb{v}_{1}$ of video $\\bar{V_{1}},U_{t}^{o}(\\cdot)$ and $\\mathbf{\\bar{\\boldsymbol{U}}}_{t}^{m}(\\cdot)$ are learnable projection layers to map the object and motion features into the same dimension. Similarly, we can also obtain the separated sound feature of $V_{2}$ guided by the associated visual features. ", "page_idx": 4}, {"type": "text", "text": "The extracted separated sound feature and the latent mixed audio feature are combined to generate a mask. This mask is subsequently applied to the mixed audio, leading to the reconstruction of the separated sound spectrogram. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{M}_{t}^{1}=\\mathcal{F}_{t}^{A D}(\\pmb{f}_{t}^{l a t.})\\odot M L P_{t}(\\pmb{f}_{t}^{a,1}),}\\\\ {\\hat{M}_{t}^{2}=\\mathcal{F}_{t}^{A D}(\\pmb{f}_{t}^{l a t.})\\odot M L P_{t}(\\pmb{f}_{t}^{a,2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\boldsymbol M}_{t}^{1}$ and $\\hat{\\boldsymbol M}_{t}^{2}$ denote the predicted masks for audio signals of video $V_{1}$ and $V_{2}$ , respectively; $\\mathcal{F}_{t}^{A D}$ is the U-Net decoder at incremental step $t$ ; $M L P_{t}(\\cdot)$ denotes a MLP module; and $\\odot$ denotes channel-wise multiplication. The sound $s_{1}$ at this incremental step can be reconstructed by applying $\\boldsymbol{S}\\odot\\hat{\\boldsymbol{M}}_{t}^{1}$ and then performing an inverse STFT to obtain the audio waveform. ", "page_idx": 4}, {"type": "text", "text": "3.3 Cross-modal Similarity Distillation Constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recent studies [53, 45] have highlighted the importance of cross-modal semantic correlation in audio-visual modeling. However, this correlation tends to diminish during subsequent incremental phases, which leads to catastrophic forgetting in our continual audio-visual sound separation task. To address this challenge, we propose a novel Cross-modal Similarity Distillation Constraint (CrossSDC) ", "page_idx": 4}, {"type": "text", "text": "that serves two crucial purposes (1) maintaining cross-modal semantic similarity through incremental tasks, and (2) preserving previous learned semantic similarity knowledge from old tasks. ", "page_idx": 5}, {"type": "text", "text": "CrossSDC preserves cross-modal semantic similarity from two perspectives: instance-aware semantic similarity and class-aware semantic similarity. Both similarities are enforced by integrating contrastive loss and knowledge distillation. Instead of exclusively focusing on the similarities within current and memory data generated by the current training model, CrossSDC incorporates the cross-modal similarity knowledge acquired from previous tasks into the contrastive loss. This integration not only facilitates the learning of cross-modal semantic similarities in new tasks but also ensures the preservation of previously acquired knowledge. In the incremental step $t$ $\\mathit{\\Omega}\\left(t>1\\right)$ ), the instance-aware part of our CrossSDC can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i n s t.}=-\\mathbb{E}_{V^{i}\\sim\\mathcal{D}_{t}^{\\prime}}\\left[\\frac{1}{\\sum_{j}\\mathbb{1}[i=j]}\\sum_{j}\\mathbb{1}[i=j]\\log\\frac{\\exp(\\sin(f_{\\tau_{1},i}^{m o d_{1}},f_{\\tau_{2},j}^{m o d_{2}}))}{\\sum_{k}\\exp(\\sin(f_{\\tau_{1},i}^{m o d_{1}},f_{\\tau_{2},k}^{m o d_{2}}))}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{I}[i=j]$ is an indicator that equals 1 when $i=j$ , denoting that video samples $V^{i}$ and $V^{j}$ are the same video; The sim function represents the cosine similarity function with temperature scaling; The modalities $m o d_{1}$ and $m o d_{2}$ , where $(m o d_{1},m o d_{2})\\in\\{(a,\\bar{o}),(a,m),(m,o)\\}$ , denote different pairs of features to be compared: separated sound and object features, sound and motion features, and motion and object features. Here, $\\tau$ denotes the incremental step, for which we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{1},\\tau_{2}\\in\\mathbf{T},\\,\\,w h e r e\\,\\mathbf{T}=\\left\\{\\begin{array}{l l}{\\{t,t-1\\},}&{\\mathrm{if~}V\\in\\mathcal{M}_{t},}\\\\ {\\{t\\},}&{\\mathrm{if~}V\\in\\mathcal{D}_{t},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which means that, for current task\u2019s data $\\mathcal{D}_{t}$ , we calculate the contrastive loss using features from the current model $(\\tau_{1}=\\tau_{2}=t)$ , while for memory set data $\\mathcal{M}_{t}$ , we use features from both the old and current models (e.g., $\\tau_{1}=t$ and $\\tau_{2}=t-1)$ ). In this way, knowledge distillation would be integrated into the cross-modal semantic similarity constraint for the current task, which ensures better preservation of learned cross-modal semantic similarity from previous tasks. ", "page_idx": 5}, {"type": "text", "text": "While the instance-aware similarity provides valuable semantic correlation modeling, it does not account for the class-level semantic correlations, which is also crucial for audio-visual similarity modeling. To capture and preserve the semantic similarity within each class across incremental tasks, we also incorporate a class-aware component specifically designed for inter-class cross-modal semantic similarity, which can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c l s.}=-\\mathbb{E}_{(V^{i},y^{i})\\sim\\mathcal{D}_{t}^{\\prime}}\\left[\\frac{1}{\\sum_{j}\\mathbb{1}[y^{i}=y^{j}]}\\sum_{j}\\mathbb{1}[y^{i}=y^{j}]\\log\\frac{\\exp(\\sin(f_{\\tau_{1},i}^{m o d_{1}},f_{\\tau_{2},j}^{m o d_{2}}))}{\\sum_{k}\\exp(\\sin(f_{\\tau_{1},i}^{m o d_{1}},f_{\\tau_{2},k}^{m o d_{2}}))}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this context, visual and audio features from two videos are encouraged to be close when they belong to the same class. The overall formulation of our CrossSDC is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C r o s s S D C}=\\lambda_{i n s}\\mathcal{L}_{i n s}+\\lambda_{c l s}\\mathcal{L}_{c l s},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{i n s}$ and $\\lambda_{c l s}$ are two scalars that balance the two loss terms. In this way, the model captures and preserves semantic correlations not just between instances but also within the same classes. ", "page_idx": 5}, {"type": "text", "text": "3.4 Overall Loss Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous subsection, we introduced our proposed CrossSDC constraint. To effectively combine CrossSDC with the overall objective, we incorporate it alongside output distillation and the main separation loss function. ", "page_idx": 5}, {"type": "text", "text": "Output distillation is a widely used technique in continual learning [38, 2, 53] to preserve the knowledge gained from previous tasks while learning new ones. In our approach, we utilize the output of the old model as the distillation target to preserve this knowledge. Note that we only distill knowledge for data from the memory set, as represented by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d i s t.}=\\mathbb{E}_{(V_{1}^{i},V_{2}^{i})\\sim\\mathcal{M}_{t}}\\Big[||\\hat{M}_{t}^{1}-\\hat{M}_{t-1}^{1}||_{1}+||\\hat{M}_{t}^{2}-\\hat{M}_{t-1}^{2}||_{1}\\Big],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{M}_{t-1}^{1}$ and $\\hat{M}_{t-1}^{2}$ are predicted masks generated by the old model that is trained at incremental step $t-1$ . For the loss function here, we follow [84, 14] and adopt the per-pixel $L_{1}$ loss [84]. For ", "page_idx": 5}, {"type": "text", "text": "the main separation loss function, we also apply the per-pixel $L_{1}$ loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m a i n}=\\mathbb{E}_{(V_{1}^{i},V_{2}^{i})\\sim\\mathcal{M}_{t}}\\Big[||\\hat{M}_{t}^{1}-M^{1}||_{1}+||\\hat{M}_{t}^{2}-M^{2}||_{1}\\Big],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, our overall loss function is denoted as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C o n t A V-S e p}=\\mathcal{L}_{m a i n}+\\lambda_{d i s t.}\\mathcal{L}_{d i s t.}+\\mathcal{L}_{C r o s s S D C}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.5 Management of Memory Set ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In alignment with the work of [76], our framework maintains a compact memory set throughout incremental updates. Each old class is limited to a maximum number of exemplars. After completing training for each task, we adopt the exemplar selection strategies in [2, 53] by randomly selecting exemplars for each current class and combining these new exemplars with the existing memory set. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first introduce the setup of our experiments, i.e., dataset, baselines, evaluation metrics, and the implementation details. After that, we present the experimental results of our ContAV-Sep compared to the baselines, as well as ablation studies. We also conduct experiments on the AVE [68] and the VGGSound [13] datasets, which contain sound categories beyond the music domain. We put the experimental results on the AVE and the VGGSound datasets, the comparison to the uni-modal semantic similarity preservation method, the performance evaluation on old classes in incremental tasks, and the visualization of separating results in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. Following common practice [83, 88, 14], we conducted experiments on MUSIC-21 [83], which contains solo videos of 21 instruments categories: accordion, acoustic guitar, cello, clarinet, erhu, flute, saxophone, trumpet, tuba, violin, xylophone, bagpipe, banjo, bassoon, congas, drum, electric, bass, guzheng, piano, pipa, and ukulele. In our experiments, we randomly selected 20 of them to construct the continual learning setting. Specifically, we split the selected 20 classes into 4 incremental tasks, each of which involves 5 classes. The total number of available videos is 1040, and we randomly split them into training, validation, and testing sets with 840, 100, and 100 videos, respectively. To further validate the efficacy of our method across a broader sound domain, we conduct experiments using the AVE [68] and the VGGSound [13] datasets in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare our proposed approach with vanilla Fine-tuning strategy, and continual learning methods EWC [32] and LwF [38]. As we mentioned before, typical continual learning methods, e.g., class-incremental learning methods, which yield progressively increasing logits (or probability distribution) across all observed classes at each incremental step and design specific technique in the classifier, we consider that these methods are not an optimal choice for our proposed continual audio-visual sound separation problem. Thus, considering that continual semantic segmentation problem has a more similar form compared to conventional class-incremental learning, we also select two state-of-the-art continual semantic segmentation methods PLOP [16] and EWF [78] as our baselines. Moreover, we compare our method to the recently proposed audio-visual continual learning method AV-CIL [53], in which we adapt the original class-incremental version to the form of continual audio-visual sound separation by replacing their task-wise logits distillation with the output mask distillation. Further, we also present the experimental results of the Oracle/Upper Bound, which means that using the training data from all seen classes to train the model. For fair comparison, all compared continual learning methods and our ContAV-Sep use the same state-of-the-art separator, i.e. iQuery [14], as the base separation model. Further, we also incorporate our proposed and baseline methods into another representative audio-visual sound separation model Co-Separation [23]. Notably, the Co-Separation model does not utilize the motion modality. Therefore, when CrossSDC is applied to Co-Separation, the $(m o d_{1},m o d_{2})$ in Eq. 7 and 9 is constrained to $(m o d_{1},m o d_{2})=(a,o)$ . For baselines that involve memory sets, we ensure that each of them is allocated the same number of memory as our proposed method for fair comparison. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Following [14], we use a 7-layers U-Net [58] as the audio net, and subsample the audio at $11\\mathrm{kHz}$ , each of which is approximately 6 seconds. We apply the STFT with the Hann window size of 1022 and the hop length of 256, to obtain the $512\\times256$ Time-Frequency representation of each audio signal, followed by a re-sampling on the log-frequency scale to generate the magnitude spectrogram with $T,F=256$ . We set the video frame rate (FPS) to 1, and detect the object using the pre-trained universal detector Detic [87] to detect the sound source object on each frame, and then, each detected object is resized and randomly cropped to the size of $224\\times224$ . For the image encoder and the video encoder, we apply the self-supervised pre-trained CLIP [56] and VideoMAE [69] to yield the object feature and motion feature, respectively. For the audio-visual Transformer module, we follow the design in [14]. For all the baseline methods, we apply the same model architecture and modules with ours for them, including the mentioned Detic, CLIP, VideoMAE, audio-visual Transformer, etc. Please note that, during our training process, the pre-trained Detic, CLIP, and VideoMAE are frozen. In our proposed Cross-modal Similarity Distillation Constraint (CrossSDC), the balance weights $\\lambda_{i n s}$ and $\\lambda_{c l s}$ are set to 0.1 and 0.3, respectively. And the balance weight $\\lambda_{d i s t}$ . for the output distillation loss is set to 0.3 in our experiments. For the memory set, we set the number of samples in each old class to 1, so as other baselines that involve the memory set. All the experiments in this paper are implemented by Pytorch [51]. We train our proposed method and all baselines on a NVIDIA RTX A5000 GPU. We follow previous works [67, 14] in sound separation, and evaluate the performance of all the methods using three common metrics in sound separation tasks: Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), and Signal to Artifact Ratio (SAR). The SDR measures the interference and artifacts, while SIR and SAR measure the interference and artifacts, respectively. In our experiments, we report the SDR, SIR, and SAR of all the methods after training at last incremental steps, i.e., testing results on all classes. For all these three metrics, higher values denote better results. ", "page_idx": 6}, {"type": "table", "img_path": "PZCiWtQjAw/tmp/8e9b46e4e83903571c95814504dc2cc06539ec6fa46b2121af7b969527a2b6e9.jpg", "table_caption": ["Table 1: Main results of different methods on MUSIC-21 dataset under the setting of Continual Audio-Visual Sound Separation with base separation models of iQuery [14] and Co-Separation [23], respectively. The bold part denotes the best results. Our proposed ContAV-Sep achieves the best performance among all baselines. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The main experimental comparisons are shown in Tab. 1. Our proposed method, ContAV-Sep, outperforms the state-of-the-art baselines by a substantial margin. Notably, compared to baselines using state-of-the-art audio-visual sound separator iQuery [14] as the separation base model, ContAVSep achieves a 0.3 improvement in SDR over the compared best-performing method. Additionally, our method surpasses the top baseline by 0.25 in SIR and 0.41 in SAR. Furthermore, compared to continual learning baselines with Co-Separation [23], our ContAV-Sep still outperforms other approaches. This consistent superior performance across different model architectures highlights not only the effectiveness but also the broad applicability and generalizability of our proposed CrossSDC. ", "page_idx": 7}, {"type": "text", "text": "Our observations further demonstrate that retaining a small memory set significantly enhances the performance of each baseline method. For instance, for the iQuery-based continual learning methods, equipping LwF [38] with a small memory set results in improvements of 3.31, 3.99, and 1.94 on SDR, SIR, and SAR, respectively. Similarly, the addition of a small memory set to EWC [32] leads to enhancements of 2.98, 3.43, and 1.43 in the respective metrics. The memory-augmented version of PLOP [16] exhibits superior performance with margins of 3.21, 3.24, and 1.68 for SDR, SIR, and SAR, respectively. Finally, incorporating memory into EWF [78] results in improvements of 1.37, ", "page_idx": 7}, {"type": "image", "img_path": "PZCiWtQjAw/tmp/4b5e606f0c2df457af723f28c9c7c6a2581cbd0f1ef699933b907fa56e7c0f3a.jpg", "img_caption": ["Figure 3: Testing results of different continual learning methods with iQuery [14] on the metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "1.67, and 0.29 for the three metrics. This phenomenon can be attributed to the inherent nature of the sound separation training process. In training, the audio signal from each sample mixes with others, giving a composite audio signal. This mixed audio signal, coupled with the corresponding visual data pair for each separated audio, constitutes the actual training sample for the separation task. As a result, even a single memory sample can be associated with multiple samples from the current training set, generating a diverse array of effective training pairs. ", "page_idx": 8}, {"type": "text", "text": "We also present the testing results of SDR, SIR, and SAR at each incremental step in Figures 3a, 3b, and 3c, respectively. Our method is consistently observed to outperform others in terms of SDR at all incremental steps. While our approach may not always produce the best SIR and SAR results at the intermediate steps (specifically, steps 2 and 3 for SIR, and step 3 for SAR), it ultimately achieves the highest performance at the final step. This demonstrates the robustness of our method, indicating minimal forgetting throughout the incremental learning process. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study on CrossSDC and Memory Size ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we conduct an ablation study to investigate the effectiveness of our proposed CrossSDC. By removing single or multiple components of the CrossSDC, we evaluate the impact of each on the final results. The results of the ablation study are presented in Tab. 2. From the table, we can see that our full model achieves the best performance compared to the variants, which further demonstrates the effectiveness of our proposed CrossSDC. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we also discuss the effect of memory size on our proposed ContAV-Sep. In our main experiments, the default setting of the memory size is 1 sample per old class. In this subsection, we conduct experiments by increasing the memory size from 1 sample per old class to 30 samples per old class. The experimental results are shown in Tab. 3 and Figure 4. Observations from the table indicate a positive correlation between the size of the memory and the overall performance metrics. As the memory size increases, there is a discernible trend of improvement in the results. ", "page_idx": 8}, {"type": "table", "img_path": "PZCiWtQjAw/tmp/cef39901ab4e6c4443beb3a77fddffa259a6cd1ca110d827912739b9f14d6503.jpg", "table_caption": ["Table 2: Ablation study on our proposed ContAV-Sep. Our full approach achieves best results compared to the variants. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Limitation and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our experimental findings reveal that the utilization of a small memory set, even a single sample per old class, markedly improves the performance of each baseline method. This improvement is attributed to the ability of a single memory sample to pair with diverse samples from the current training set, thereby generating numerous effective training pairs. Consequently, this process enables the model to acquire new knowledge for old classes in subsequent tasks, as the memory data can be ", "page_idx": 8}, {"type": "text", "text": "Table 3: Experimental results of our proposed ContAV-Sep with different memory size from 1 to 30 samples per memory class. ", "page_idx": 9}, {"type": "table", "img_path": "PZCiWtQjAw/tmp/95481f92505f420b4f93dd6c4e1cfa0fdab5035c641cd7d146719ae4894079f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "PZCiWtQjAw/tmp/337b0768e8f4c9d5edf4c5b2e3d471d2adb94a5bcd3052765d02dc90ea183fa1.jpg", "img_caption": ["Figure 4: Testing results with different memory size (number of samples per class in the memory) on the metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "paired with data from previously unseen new classes \u2014 this is different from conventional continual learning tasks, where old classes do not acquire new knowledge in new tasks. This could be a potential reason why the baseline continual learning methods do not perform well in our continual audio-visual sound separation problem. In this work, our method also mainly focuses on preserving old knowledge of old tasks, which may prevent the model from acquiring new knowledge of old classes when training in new tasks. Recognizing this, we identify the exploration of this problem as a key avenue for future research in this field. ", "page_idx": 9}, {"type": "text", "text": "Additionally, the base model architectures used in our approach and baselines require object detectors to identify sounding objects. Although iQuery [14] can supplement object features with global video representations, it may still suffer from undetected objects. It is a fundamental limitation of the object-based audio-visual sound separators [23, 14]. While our work, unlike previous efforts, does not compete on designing a stronger audio-visual separation base model, enhancing the robustness of sounding object detection presents a promising direction for future research. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore training audio-visual sound separation models under a more practical continual learning scenario, and introduce the task of continual audio-visual sound separation. To address this novel problem, we propose ContAV-Sep, which incorporates a Cross-modal Similarity Distillation Constraint to maintain cross-modal semantic similarity across incremental tasks while preserving previously learned semantic similarity knowledge. Experiments on the MUSIC-21 dataset demonstrate the effectiveness of our method in this new continual separation task. This paper opens a new direction for real-world audio-visual sound separation research. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. Our proposed continual audio-visual sound separation allows the model to adapt to new environments and sounds without full retraining, which could enhance efficiency and privacy by reducing the need to transmit and store sensitive audio data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We thank the anonymous reviewers and area chair for their valuable suggestions and comments. This work was supported in part by a Cisco Faculty Research Award, an Amazon Research Award, and a research gift from Adobe. The article solely reflects the opinions and conclusions of its authors but not the funding agents. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement. arXiv preprint arXiv:1804.04121, 2018. [2] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. SSIL: separated softmax for incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 824\u2013833, 2021. [3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV), pages 139\u2013154, 2018.   \n[4] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. arXiv preprint arXiv:2201.12604, 2022.   \n[5] Eden Belouadah and Adrian Popescu. Il2m: Class incremental learning with dual memory. In Proceedings of the IEEE/CVF international conference on computer vision, pages 583\u2013592, 2019.   \n[6] Ruchi Bhatt, Pratibha Kumari, Dwarikanath Mahapatra, Abdulmotaleb El Saddik, and Mukesh Saini. Characterizing continual learning scenarios and strategies for audio analysis. arXiv preprint arXiv:2407.00465, 2024.   \n[7] Albert S Bregnian. Auditory scene analysis: Hearing in complex environments. Thinking in Sounds, pages 10\u201336, 1993.   \n[8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.   \n[9] Francisco M Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 233\u2013248, 2018.   \n[10] Sungmin Cha, Sungjun Cho, Dasol Hwang, Sunwon Hong, Moontae Lee, and Taesup Moon. Rebalancing batch normalization for exemplar-based class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20127\u201320136, 2023.   \n[11] Moitreya Chatterjee, Narendra Ahuja, and Anoop Cherian. Learning audio-visual dynamics using scene graphs for audio source separation. Advances in Neural Information Processing Systems, 35:16975\u201316988, 2022.   \n[12] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.   \n[13] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721\u2013725. IEEE, 2020.   \n[14] Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, Ziyao Zeng, and Jianbo Shi. iquery: Instruments as queries for audio-visual sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14675\u201314686, 2023.   \n[15] Soo-Whan Chung, Soyeon Choe, Joon Son Chung, and Hong-Goo Kang. Facefilter: Audiovisual speech separation using still images. arXiv preprint arXiv:2005.07074, 2020.   \n[16] Arthur Douillard, Yifu Chen, Arnaud Dapogny, and Matthieu Cord. Plop: Learning without forgetting for continual semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4040\u20134050, 2021.   \n[17] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16, pages 86\u2013102. Springer, 2020.   \n[18] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speakerindependent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018.   \n[19] Enrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9621\u20139630, 2022.   \n[20] Aviv Gabbay, Asaph Shamir, and Shmuel Peleg. Visual speech enhancement. arXiv preprint arXiv:1711.08789, 2017.   \n[21] Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenenbaum, and Antonio Torralba. Music gesture for visual sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10478\u201310487, 2020.   \n[22] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning to separate object sounds by watching unlabeled video. In Proceedings of the European Conference on Computer Vision (ECCV), pages 35\u201353, 2018.   \n[23] Ruohan Gao and Kristen Grauman. Co-separating sounds of visual objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3879\u20133888, 2019.   \n[24] Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476, 2019.   \n[25] Simon Haykin and Zhe Chen. The cocktail party problem. Neural computation, 17(9):1875\u2013 1902, 2005.   \n[26] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 831\u2013839, 2019.   \n[27] Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Davis: High-quality audio-visual separation with generative diffusion models. arXiv preprint arXiv:2308.00122, 2023.   \n[28] Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and ChuSong Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16050\u201316059, 2022.   \n[30] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual learning of language models. In International Conference on Learning Representations (ICLR), 2023.   \n[31] Sanghwan Kim, Lorenzo Noci, Antonio Orvieto, and Thomas Hofmann. Achieving a better stability-plasticity trade-off via auxiliary networks in continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11930\u201311939, 2023.   \n[32] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.   \n[33] Saksham Singh Kushwaha. Analyzing the effect of equal-angle spatial discretization on sound event localization and detection. In Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022), 2022.   \n[34] Saksham Singh Kushwaha and Magdalena Fuentes. A multimodal prototypical approach for unsupervised sound classification. arXiv preprint arXiv:2306.12300, 2023.   \n[35] Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, and Sung Ju Hwang. Lifelong audio-video masked autoencoder with forget-robust localized alignments. arXiv preprint arXiv:2310.08204, 2023.   \n[36] Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 312\u2013321, 2019.   \n[37] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In International Conference on Machine Learning, pages 3925\u20133934. PMLR, 2019.   \n[38] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.   \n[39] Yan-Shuo Liang and Wu-Jun Li. Adaptive plasticity improvement for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7816\u20137825, 2023.   \n[40] Zilin Luo, Yaoyao Liu, Bernt Schiele, and Qianru Sun. Class-incremental exemplar compression for class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11371\u201311380, 2023.   \n[41] Haoxin Ma, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, and Chenglong Wang. Continual learning for fake audio detection. arXiv preprint arXiv:2104.07286, 2021.   \n[42] Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representational continuity for unsupervised continual learning. In International Conference on Learning Representations (ICLR), 2022.   \n[43] Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, and Boi Faltings. Continual learning for natural language generation in task-oriented dialog systems. In Findings of the Association for Computational Linguistics: EMNLP 2020, volume EMNLP 2020, pages 3461\u20133474, 2020.   \n[44] Shentong Mo, Weiguo Pian, and Yapeng Tian. Class-incremental grouping network for continual audio-visual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7788\u20137798, 2023.   \n[45] Shentong Mo and Yapeng Tian. Audio-visual grouping network for sound localization from mixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10565\u201310574, 2023.   \n[46] Xing Nie, Shixiong Xu, Xiyan Liu, Gaofeng Meng, Chunlei Huo, and Shiming Xiang. Bilateral memory consolidation for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16026\u201316035, 2023.   \n[47] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In International conference on machine learning, pages 2642\u20132651. PMLR, 2017.   \n[48] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11321\u201311329, 2019.   \n[49] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European conference on computer vision (ECCV), pages 631\u2013648, 2018.   \n[50] Jaeyoo Park, Minsoo Kang, and Bohyung Han. Class-incremental learning for action recognition in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13698\u201313707, 2021.   \n[51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[52] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34:16131\u201316144, 2021.   \n[53] Weiguo Pian, Shentong Mo, Yunhui Guo, and Yapeng Tian. Audio-visual class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7799\u20137811, 2023.   \n[54] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 524\u2013540. Springer, 2020.   \n[55] Senthil Purushwalkam, Pedro Morgado, and Abhinav Gupta. The challenges of continuous self-supervised learning. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVI, pages 702\u2013721. Springer, 2022.   \n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[57] Sylvestre-Alvise Rebuff,i Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2001\u20132010, 2017.   \n[58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.   \n[59] Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6107\u20136122, 2022.   \n[60] Charles Spence. Audiovisual multisensory integration. Acoustical science and technology, 28(2):61\u201370, 2007.   \n[61] Tejas Srinivasan, Ting-Yun Chang, Leticia Leonor Pinto Alva, Georgios Chochlakis, Mohammad Rostami, and Jesse Thomason. CLiMB: A continual learning benchmark for vision-andlanguage tasks. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[62] Barry E Stein and Terrence R Stanford. Multisensory integration: current issues from the perspective of the single neuron. Nature reviews neuroscience, 9(4):255\u2013266, 2008.   \n[63] Yiyang Su, Ali Vosoughi, Shijian Deng, Yapeng Tian, and Chenliang Xu. Separating invisible sounds toward universal audiovisual scene-aware sound separation. arXiv preprint arXiv:2310.11713, 2023.   \n[64] Elyse S Sussman. Integration and segregation in auditory scene analysis. The Journal of the Acoustical Society of America, 117(3):1285\u20131298, 2005.   \n[65] Reuben Tan, Arijit Ray, Andrea Burns, Bryan A Plummer, Justin Salamon, Oriol Nieto, Bryan Russell, and Kate Saenko. Language-guided audio-visual source separation via trimodal consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10575\u201310584, 2023.   \n[66] Yu-Ming Tang, Yi-Xing Peng, and Wei-Shi Zheng. Learning to imagine: Diversify memory for incremental learning using unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9549\u20139558, 2022.   \n[67] Yapeng Tian, Di Hu, and Chenliang Xu. Cyclic co-learning of sounding object visual grounding and sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2745\u20132754, 2021.   \n[68] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247\u2013263, 2018.   \n[69] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:10078\u201310093, 2022.   \n[70] Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel PW Ellis, and John R Hershey. Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds. arXiv preprint arXiv:2011.01143, 2020.   \n[71] Efthymios Tzinis, Scott Wisdom, Tal Remez, and John R Hershey. Audioscopev2: Audio-visual attention architectures for calibrated open-domain on-screen sound separation. In European Conference on Computer Vision, pages 368\u2013385. Springer, 2022.   \n[72] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. FOSTER: feature boosting and compression for class-incremental learning. In In Proceedings of the European Conference on Computer Vision (ECCV), pages 398\u2013414, 2022.   \n[73] Yu Wang, Nicholas J Bryan, Mark Cartwright, Juan Pablo Bello, and Justin Salamon. Fewshot continual learning for audio classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 321\u2013325. IEEE, 2021.   \n[74] Zhepei Wang, Cem Subakan, Xilin Jiang, Junkai Wu, Efthymios Tzinis, Mirco Ravanelli, and Paris Smaragdis. Learning representations for new sound classes with continual self-supervised learning. IEEE Signal Processing Letters, 29:2607\u20132611, 2022.   \n[75] Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis, and Laurent Charlin. Continual learning of new sound classes using generative replay. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 308\u2013312. IEEE, 2019.   \n[76] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149, 2022.   \n[77] Adam Weisser. Complex acoustic environments: Concepts, methods, and auditory perception. PhD thesis, PhD thesis). Macquarie University, Sydney, Australia. doi: 1959.14/1266534, 2018.   \n[78] Jia-Wen Xiao, Chang-Bin Zhang, Jiekang Feng, Xialei Liu, Joost van de Weijer, and Ming-Ming Cheng. Endpoints weight fusion for class incremental semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7204\u20137213, 2023.   \n[79] Xudong Xu, Bo Dai, and Dahua Lin. Recursive visual sound separation using minus-plus net. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 882\u2013891, 2019.   \n[80] Shipeng Yan, Lanqing Hong, Hang Xu, Jianhua Han, Tinne Tuytelaars, Zhenguo Li, and Xuming He. Generative negative text replay for continual vision-language pretraining. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVI, pages 22\u201338. Springer, 2022.   \n[81] Yuxin Ye, Wenming Yang, and Yapeng Tian. Lavss: Location-guided audio-visual spatial audio separation. arXiv preprint arXiv:2310.20446, 2023.   \n[82] Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, and Chu Yuan Zhang. Do you remember? overcoming catastrophic forgetting for fake audio detection. In International Conference on Machine Learning, pages 41819\u201341831. PMLR, 2023.   \n[83] Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proceedings of the IEEE International Conference on Computer Vision, pages 1735\u20131744, 2019.   \n[84] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In The European Conference on Computer Vision (ECCV), September 2018.   \n[85] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Deep class-incremental learning: A survey. arXiv preprint arXiv:2302.03648, 2023.   \n[86] Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Learning without forgetting for vision-language models. arXiv preprint arXiv:2305.19270, 2023.   \n[87] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision, pages 350\u2013368. Springer, 2022.   \n[88] Lingyu Zhu and Esa Rahtu. Visually guided sound source separation and localization using self-supervised motion representations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1289\u20131299, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The abstract and introduction clearly reflect the paper\u2019s contributions and scope. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Sec. 4.4 ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper does not contain theoretical results or proof. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We include an implementation paragraph section in Sec. 4.1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our source code has been released. Datasets used in our experiments are public dataset. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We include an implementation paragraph section in Sec. 4.1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Because of the limitation of computation resource, we didn\u2019t report the error bars. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper provides sufficient information of the computer resources needed to reproduce the experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper does not violate any respect of the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The paper discusses the potential societal impacts. We include it in Sec. 5. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited and are the license and terms of use explicitly mentioned and properly respected. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No new assets are introduced in the paper ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]