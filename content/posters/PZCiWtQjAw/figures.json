[{"figure_path": "PZCiWtQjAw/figures/figures_1_1.jpg", "caption": "Figure 1: Top: Illustration of the continual audio-visual sound separation task, where the model (separator) learns from sequential audio-visual sound separation tasks. Bottom: Illustration of the catastrophic forgetting problem in continual audio-visual sound separation and its mitigation by our proposed method. Fine-tuning: Directly fine-tune the separation model on new sound source classes; Upper bound: Train the model using all training data from seen sound source classes.", "description": "This figure illustrates the continual audio-visual sound separation task. The top panel shows how the model learns from sequential tasks, each with a different set of sound sources, while the bottom panel compares different continual learning approaches to show that the proposed method effectively mitigates catastrophic forgetting in this task.", "section": "1 Introduction"}, {"figure_path": "PZCiWtQjAw/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of our proposed ContAV-Sep, which consists of an audio-visual sound separation base model architecture, an Output Mask Distillation, and our proposed Cross-modal Similarity Distillation Constraint. The fire icon denotes the module is trainable, while the snowflake icon denotes that the module is frozen. The (i)STFT stands for (inverse) Short-Time Fourier Transform.", "description": "This figure illustrates the architecture of the ContAV-Sep model.  It shows three main components: the base audio-visual sound separation model (using iQuery [14] as an example), an output mask distillation module, and the Cross-modal Similarity Distillation Constraint (CrossSDC). The base model takes mixed audio and visual input (video and object features) to generate separated audio. The Output Mask Distillation uses the output masks from the previous task to guide training on new tasks.  CrossSDC aims to maintain cross-modal semantic similarity across tasks using contrastive loss, ensuring that information from both audio and visual modalities is effectively retained during continual learning.", "section": "3.2 Overview"}, {"figure_path": "PZCiWtQjAw/figures/figures_8_1.jpg", "caption": "Figure 3: Testing results of different continual learning methods with iQuery [14] on the metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step.", "description": "This figure shows the performance comparison of different continual learning methods (Fine-tuning, LwF, EWC, PLOP, EWF, and ContAV-Sep) using iQuery [14] as the base model for audio-visual sound separation.  The results are presented for each incremental step across 20 classes, showing SDR (Signal-to-Distortion Ratio), SIR (Signal-to-Interference Ratio), and SAR (Signal-to-Artifacts Ratio). It demonstrates ContAV-Sep's consistent superior performance across all incremental steps compared to other methods, highlighting its ability to effectively mitigate catastrophic forgetting.", "section": "4.2 Experimental Comparison"}, {"figure_path": "PZCiWtQjAw/figures/figures_9_1.jpg", "caption": "Figure 3: Testing results of different continual learning methods with iQuery [14] on the metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step.", "description": "This figure displays the performance of various continual learning methods, including the proposed ContAV-Sep, on the tasks of separating sound sources.  The results are shown across different incremental steps and are evaluated based on three metrics: Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), and Signal to Artifact Ratio (SAR). Each subplot represents one of these metrics.  The figure illustrates how the performance of different models changes as the number of tasks increases (on the x-axis), showing the impact of continual learning on maintaining performance on previous tasks while learning new ones. The goal is to see which approach mitigates catastrophic forgetting most effectively.", "section": "4.2 Experimental Comparison"}]