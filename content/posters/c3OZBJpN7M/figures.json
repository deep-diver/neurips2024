[{"figure_path": "c3OZBJpN7M/figures/figures_13_1.jpg", "caption": "Figure 1: Flow diagram demonstrating the computation of Cluster Knowledge Fusion (CKF) in Federated Learning. The diagram highlights the steps involved in extracting features, generating soft predictions, and performing GMM clustering to compute prototype features and predictions, followed by the aggregation of CKF at the server.", "description": "This figure illustrates the process of computing Cluster Knowledge Fusion (CKF) in a federated learning setting.  It shows how individual clients process their data, extract features, generate soft predictions, and perform Gaussian Mixture Model (GMM) clustering to create prototype features and soft predictions.  These are then aggregated at the server using the Discrepancy-Aware Aggregation Technique (DAT).  The diagram highlights the multi-step process from individual client data processing to the final aggregation of CKF at the server.", "section": "3.2 Utilizing Cluster Knowledge Fusion"}, {"figure_path": "c3OZBJpN7M/figures/figures_14_1.jpg", "caption": "Figure 2: Flow diagram demonstrating the computation of Discrepancy-Aware Aggregation Technique (DAT) in Federated Learning. The diagram details the steps involved in computing initial weights, aggregating soft predictions, calculating discrepancies, and performing the final aggregation of CKF at the server.", "description": "This figure illustrates the process of Discrepancy-Aware Aggregation Technique (DAT) in the FedGMKD framework.  It starts by calculating initial weights for each client's contribution based on the proportion of samples for each class. Then, soft predictions are aggregated using these initial weights. Next, discrepancies (using KL-divergence) between local and global data distributions are calculated for each class. Finally, aggregation weights are adjusted based on both the initial weights and the calculated discrepancies to produce the final aggregated CKF.", "section": "3.3 Discrepancy-Aware Aggregation Technique"}, {"figure_path": "c3OZBJpN7M/figures/figures_15_1.jpg", "caption": "Figure 3: Visualization of the FedGMKD framework. Each client trains a local model and extracts CKF using its local data. The server aggregates the CKF and model updates using Discrepancy-Aware Aggregation Technique (DAT) to improve the global CKF and model. This process iterates over multiple global rounds.", "description": "This figure illustrates the iterative process of the FedGMKD algorithm.  It shows how each client trains a local model, extracts its Cluster Knowledge Fusion (CKF), and sends both the CKF and model updates to a central server.  The server then uses the Discrepancy-Aware Aggregation Technique (DAT) to aggregate these updates, improving both the global CKF and the global model. This process repeats over multiple rounds.", "section": "3.5 Overall Framework of FedGMKD"}, {"figure_path": "c3OZBJpN7M/figures/figures_20_1.jpg", "caption": "Figure 4: Qualitative comparison of t-SNE visualization among FedAvg, FedProto, FPL and FedGMKD. Compared with other methods, the feature distribution of the FedGMKD is more compact within each category, and more discriminative across classes.", "description": "This figure shows a comparison of t-SNE visualizations for four different federated learning methods: FedAvg, FedProto, FPL, and FedGMKD.  The t-SNE plots illustrate the distribution of feature representations in a 2D space. FedAvg shows features widely dispersed with significant overlap between classes. FedProto shows slightly more distinct clusters than FedAvg but still with overlap. FPL shows very similar results to FedAvg.  In contrast, FedGMKD demonstrates the clearest separation between classes, with compact and well-defined clusters. This visual representation highlights FedGMKD's superior ability to learn discriminative features for class separation, benefiting both local and global model performance.", "section": "A.5 Feature Representations"}]