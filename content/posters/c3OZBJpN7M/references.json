{"references": [{"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduces the FedAvg algorithm, a foundational method in federated learning that is extensively used and compared against in the current paper."}, {"fullname_first_author": "Peter Kairouz", "paper_title": "Advances and open problems in federated learning", "publication_date": "2021-00-00", "reason": "This review paper provides a comprehensive overview of federated learning, highlighting key challenges and opportunities, which are directly relevant to the focus and context of the current research."}, {"fullname_first_author": "Tian Li", "paper_title": "FedProx: Federated optimization in heterogeneous networks", "publication_date": "2020-00-00", "reason": "This paper addresses the issue of data heterogeneity in federated learning, which is a central challenge tackled by the current paper's proposed FedGMKD framework."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "This paper introduces the concept of knowledge distillation, a core technique that is extended and applied within the FedGMKD framework for improved efficiency and performance."}, {"fullname_first_author": "Huancheng Chen", "paper_title": "The best of both worlds: Accurate global and personalized models through federated learning with data-free hyper-knowledge distillation", "publication_date": "2023-00-00", "reason": "This work is highly relevant as it explores a similar concept of data-free knowledge distillation in federated learning, providing a foundation and context for the current research's novel approach."}]}