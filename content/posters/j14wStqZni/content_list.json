[{"type": "text", "text": "Public-data Assisted Private Stochastic Optimization: Power and Limitations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enayat Ullah ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Michael Menart ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meta \u2217 enayat@meta.com ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science & Engineering The Ohio State University \u2020 Department of Computer Science, University of Toronto Vector Institute menart.2@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Raef Bassily ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Crist\u00f3bal Guzm\u00e1n ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science & Engineering Translational Data Analytics Institute (TDAI) The Ohio State University bassily.1@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Inst. for Mathematical and Comput. Eng. Fac. de Matem\u00e1ticas and Esc. de Ingenier\u00eda Pontificia Universidad Cat\u00f3lica de Chile crguzmanp@uc.cl ", "page_idx": 0}, {"type": "text", "text": "Raman Arora Department of Computer Science Johns Hopkins University arora@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$ -PA-DP has excess risk $\\begin{array}{r}{\\tilde{\\Omega}\\big(\\operatorname*{min}\\big\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{1}{\\sqrt{n}}\\,+\\,\\frac{\\sqrt{d}}{n\\epsilon}\\big\\}\\big)}\\end{array}$ , where $d$ is the dimension, $n_{\\mathrm{pub}}$ is the number of public samples, $n_{\\mathrm{priv}}$ is the number of private samples, and $n\\,=\\,n_{\\mathrm{pub}}\\,+\\,n_{\\mathrm{priv}}$ These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with unlabeled public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}(n_{\\mathrm{priv}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{1}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}\\big)}\\end{array}$ . We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The framework of differential privacy has become the primary standard for protecting individual privacy in data analysis and machine learning. Unfortunately, this rigorous framework has also been shown to lead to worse performance on such tasks both empirically and in theory [BST14, $\\mathrm{PVX}^{+}23]$ . However, it is often the case that, in addition to a collection of privacy-sensitive data points, analysts have access to a pool of public data, for which guaranteeing privacy protections is not required. This can happen, for example, when consumers deem their own data non-sensitive and opt-in to sell this data to a company. This has motivated a long line of work analyzing how public data can be leveraged in tandem with private data to provide better utility [BNS13, ABM19, $\\bar{\\mathbf{B}}{\\mathbf{C}}{\\mathbf{M}}^{+}20$ , ZWB21, BKS22, $\\mathrm{AGM}^{+}22$ , $\\mathrm{NM}\\bar{\\Gamma}^{+}23^{\\scriptscriptstyle*}$ . In machine learning. for example, two commonly proposed strategies are public pretraining and using public data to identify gradient subspaces [ZWB21, KDRT21]. Public pretraining, in particular, has proven effective in practice $[\\mathrm{YNB}^{+}\\bar{22}\\mathrm{a}$ , BWZK22], and prior work has even identified a specific problem instance where public and private data used in tandem leads to better rates than is possible using only the public or private datasets in isolation $[\\mathrm{GHN}^{+}23]$ . Despite this surge of work, theory has struggled to show that public data leads to fundamental rate improvements more generally. Recent work has even shown that, for the problem of pure PA-DP stochastic convex optimization, a small amount of public data, $n_{\\mathrm{pub}}\\,\\leq\\,n\\epsilon/d$ , leads to no rate improvement, where $n=n_{\\mathrm{pub}}+n_{\\mathrm{priv}}$ and $n_{\\mathrm{priv}}$ is the number of private samples [LLHR23]. ", "page_idx": 1}, {"type": "text", "text": "One particularly important version of this problem is in supervised learning when the public data is unlabeled. This setting has found importance in medical domains and deep learning more generally [LW19, ${\\mathrm{SCZ}}^{+}20$ , $\\mathrm{PAE^{+}}17]$ . Notably, unlabeled data is much less time intensive to collect than labeled data. Due to this fact, and the fact that the unlabeled public data does not contain the same kind of information contained in the private data, the regime $\\bar{n}_{\\mathrm{pub}}=\\Omega(n_{\\mathrm{priv}})$ is meaningful both in theory and in practice. We also note this setting is a stronger (in terms of privacy) version of the label-private setting, where only the labels of the dataset are considered private [CH11, BNS13]. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the importance of these settings and the lack of existing theory for them in stochastic optimization, we study fundamental limitations and applications of public data in $(\\epsilon,\\delta)$ -PA-DP stochastic optimization. In the case where the public data is complete/labeled, we show that the application of public data is fundamentally limited. We then contrast this result with new results in the unlabeled public data setting. In this setting, we provide new results for GLMs, and extend these results to more general hypothesis classes, with finite fat-shattering dimension, and non-Euclidean geometries. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We outline our primary contributions in the following. ", "page_idx": 1}, {"type": "text", "text": "Limits of Private Stochastic Convex Optimization with Public Data. First, we show a tight lower bound for the problem of differentially-private stochastic convex optimization (DP-SCO) assisted with complete public data, that is, the public data and private data have the same number of features (and labels when applicable). Specifically, we show a lower bound of $\\begin{array}{r}{\\Omega\\Bigl(\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\right\\}\\Bigr)}\\end{array}$ on the excess population risk for this problem. Wh\u221aen $d\\ge n\\epsilon$ and $n_{\\mathrm{pub}}\\ \\leq\\ \\frac{n}{\\log(1/\\delta)}$ , we further improve this lower bound to \u2126 min \u221an1pub ,\u221a1n + d long\u03f5(1/\u03b4)  . This lower bound is matched by the simple upper bound strategy which either discards the private data entirely and outputs the public mean or simply treats all data-points as private. Barring constant factors, this shows more sophisticated attempts at leveraging public data will yield no benefti. These results also hold even for generalized linear models. Our results are based on new results we establish for DP mean estimation with public data, and a reduction of mean estimation to SCO. We note that previous work [LLHR23], on this problem either focused on the pure PA-DP case when $n_{\\mathrm{pub}}\\leq n\\epsilon/d$ , or, in the approximate PA-DP case, did not obtain the dimension dependence. Our mean estimation lower bound uses a novel analysis of fingerprinting codes [BUV14], and our SCO reduction further builds on ideas from [BST14, CWZ21]. We also show that, when $d\\geq n\\epsilon$ , our lower bounds for approximate PA-DP SCO directly imply a tight lower bound for pure PA-DP. ", "page_idx": 1}, {"type": "text", "text": "Private Supervised Learning with Unlabeled Public Data. While the previously discussed results show there is no hope for leveraging public data in \u201cinteresting\u201d ways, even for GLMs, they do not preclude settings where the public data is less informative. In particular, in the setting where the public data is unlabeled, it makes sense to even consider $n_{\\mathrm{pub}}\\geq n_{\\mathrm{priv}}$ . In this setting, we provide the following results. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 For (Euclidean) GLMs we develop an efficient algorithm which, given $\\tilde{O}(n_{\\mathrm{priv}}\\epsilon)$ unlabeled public data points, achieves the dimension independent rate $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{1}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}\\big)}\\end{array}$ . We obtain this result via a dimensionality reduction procedure of the private feature vectors using the public data, and then running an efficient private algorithm in the lower dimensional space. The key idea is that public data can be used to identify a low dimensional subspace, which under the appropriate metric acts as a cover for the higher dimensional space. We elucidate the tightness of our upper bound by proving two new lower bounds which show that access to a greater number of unlabeled public samples cannot improve this rate, and that any fewer public samples lead to a worse rate. While dimension independent rates for the GLMs have previously been developed in the unconstrained setting [SSTT21, $\\mathrm{ABG}^{+}22]$ , in the constrained setting which we study, dependence on dimension is known to be unavoidable even for GLMs if no public data is available [BST14]. Our result thus allows us to bypass these limitations. ", "page_idx": 2}, {"type": "text", "text": "\u2022 By observing that the key requirement in our GLM result is the construction of an appropriate cover, we extend this result to general hypothesis classes with bounded fat-shattering dimension. In the non-private setting, it is known that finiteness of fat-shattering dimension characterizes learnability of real-valued predictors with scale-sensitive losses [BLW94, ABDCBH97]. In the private setting, such a result is not known, and is in fact impossible in the proper learning setting. This follows from the fact that norm bounded linear predictors, regardless of their (ambient) dimension $d$ , have the same fat-shattering dimension [SST10]. However, it is known that they are not learnable privately in high dimensions $d\\geq(n\\epsilon)^{2}$ [BST14]. In contrast, in the PA-DP setting, we show that it is possible to properly learn such classes with a rate of roughly $\\begin{array}{r}{O\\left(\\Re_{n_{\\mathrm{priv}}}(\\mathcal{H})+\\operatorname*{inf}_{\\alpha>0}\\left(\\frac{\\mathrm{fat}_{\\alpha}(\\mathcal{H})}{n_{\\mathrm{priv}}\\epsilon}+\\alpha\\right)\\right)}\\end{array}$ , where $\\Re_{n_{\\mathrm{priv}}}({\\mathcal{H}})$ denotes the Rademacher complexity of $\\mathcal{H}$ and $\\mathrm{fat}_{\\alpha}(\\mathcal{H})$ denotes its fat-shattering dimension at scale $\\alpha$ (see Section 2 for preliminaries). ", "page_idx": 2}, {"type": "text", "text": "\u2022 As applications of our result for hypothesis classes with bounded fat-shattering dimension, we obtain guarantees for learning feed-forward neural networks and non-Euclidean GLMs. In particular, for depth $M$ feed-forward neural networks with weights bounded as $\\|W_{j}\\|_{F}\\,\\le\\,R_{j}$ and 1-Lipschitz positive homogeneous activation, we achieve an excess risk bound of essentially $\\begin{array}{r}{\\tilde{O}\\left(\\frac{\\sqrt{M}\\prod_{j=1}^{M}R_{j}}{\\sqrt{n_{\\mathrm{priv}}}}+\\left(\\frac{M\\left(\\prod_{j=1}^{M}R_{j}\\right)^{2}}{n_{\\mathrm{priv}}\\epsilon}\\right)^{1/3}\\right)}\\end{array}$ . For non-Euclidean GLMs, our guarantees are dimension-independent which is not known to be achievable, as of yet, even in the unconstrained setting with no public data (unlike Euclidean GLMs). ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With regards to labeled public data, the most directly related work to ours is the recent work of [LLHR23]. This work proves a lower bound of $\\begin{array}{r}{\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{1}{n\\epsilon}\\right\\}\\right)}\\end{array}$ for approximate PA-DP mean-estimation/SCO. We note that our results for approximate PA-DP crucially obtain a dependence on $d$ that is the key \u201cprice\u201d paid for privacy in this setting. [LLHR23] also show a lower bound of $\\begin{array}{r}{\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{\\stackrel{.}{1}}{\\sqrt{n}}+\\frac{d}{n\\epsilon}\\right\\}\\right)}\\end{array}$ on a pure PA-DP mean estimation/SCO, but this result only holds when $\\begin{array}{r}{d\\leq\\frac{n\\epsilon}{n_{\\mathrm{pub}}}}\\end{array}$ . As such, their result is orthogonal to our result in the pure PA-DP setting, which operates in the regime $d\\geq n\\epsilon$ . In both cases, our proof technique is fundamentally different than theirs.3 Tangentially, [BKS22] showed a small amount of public data is useful in pure-DP mean estimation when the range parameters on the data are unknown. ", "page_idx": 2}, {"type": "text", "text": "An important setting where public data is shown to be useful is PAC learning. Non-privately, it is known that the finiteness of $V C$ dimension characterizes learnability [VC71, BEHW89]. However, under DP, it is impossible to PAC learn even the class of thresholds, which has VC dimension of one [BNS13]. The works of [BNS13, BTGT18, ABM19] showed that given access to a small unlabelled public data, it is possible to go beyond this limitation and privately learn VC classes, essentially by reducing a hypothesis class with finite VC dimension to a finite hypothesis class ", "page_idx": 2}, {"type": "text", "text": "A number of works have studied the impact of public data in applied settings as well. A common technique is to use public data to reduce the problem dimension in some way [ZWB21, YZCL21, PHYS24]. The work of $[\\mathrm{GHN}^{+}23]$ identified a specific problem instance which supports the method of public pretraining commonly used in practice. ", "page_idx": 3}, {"type": "text", "text": "With regards to unlabelled public data, there are several existing works. Transfer learning is a common approach in this setting. Besides the benefits in PAC learning, this setting also has applications in deep learning, where (empirically) unlabeled public data has been used to obtain performance improvements $[\\mathrm{PAE^{+}}17\\$ , $\\bar{\\mathrm{PSM}}^{+}1\\bar{8}\\mathrm{J}$ . Unlabeled public data has also yielded impressive results used for pre-training large language models [LTLH22, $\\Upsilon\\mathrm{NB}^{+}22\\mathrm{b}]$ ]. We also remark that, in practice, it is reasonable to expect the private and public datasets to come from slightly different distributions. Accounting for this distribution shift has also been the study of several recent works [BKS22, $\\mathbf{BDBC}^{+}23\\mathbf{\\overline{{\\Gamma}}}$ ]. However, in this work we focus on first characterizing the more fundamental problem where the public and private datasets are drawn i.i.d. from the same distribution. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we describe the concepts and assumptions used in the rest of this paper. In this work, $\\Vert\\cdot\\Vert$ always denotes the $\\ell_{2}$ norm unless stated otherwise. ", "page_idx": 3}, {"type": "text", "text": "Public-Data Assisted Differential Privacy. We first present the traditional notion of differential privacy (DP). Let $n,d\\in\\mathbb{N}$ and $\\mathcal{X}$ be some data domain. When no public data is present, we say that an algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies $(\\epsilon,\\delta)$ -differential privacy (DP) if for all datasets $S$ and $S^{\\prime}$ differing in one data point and all events $\\mathcal{E}$ in the range of $\\begin{array}{r}{\\bar{\\mathcal{A}},\\mathbb{P}\\left[\\bar{\\mathcal{A}}(S)\\in\\mathcal{E}\\right]\\leq e^{\\epsilon}\\mathbb{P}\\left[\\mathcal{A}(S^{\\prime})\\in\\mathcal{E}\\right]+\\delta}\\end{array}$ [DMNS06]. In our work, we denote the number of public samples in the dataset, $S=(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\in\\chi^{n}$ , as $n_{\\mathrm{pub}}$ and the number of private samples as $n_{\\mathrm{priv}}$ , such that $n=n_{\\mathrm{pub}}+n_{\\mathrm{priv}}$ . In keeping with previous work [BNS13, $\\mathbf{BCM}^{+}20]$ , we define public data assisted differentially private algorithms in the following way 4. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (PA-DP). An algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\epsilon,\\delta)$ public-data assisted differentially private (PA$D P_{\\star}$ ) algorithm with public sample size $n_{p u b}$ and private sample size $n_{p r i\\nu}$ if for any public dataset $S_{p u b}\\in\\mathcal{X}^{n_{p u b}}$ , and any pair of private datasets $\\mathrm{S}_{p r i\\nu},S_{p r i\\nu}^{\\prime}\\in\\mathcal{X}^{n_{p r i\\nu}}$ differing in at most one entry, it holds for any event $\\mathcal{E}$ that $\\begin{array}{r}{\\mathbb{P}\\left[A(S_{p u b},S_{p r i\\nu})\\in\\mathcal{E}\\right]\\leq e^{\\dot{\\epsilon}}\\mathbb{P}[A(S_{p u b},S_{p r i\\nu}^{\\prime})\\in\\mathcal{E}]+\\delta}\\end{array}$ . When $\\delta=0$ , we refer to this notion as pure PA-DP, denoted as $\\epsilon$ -PA-DP. ", "page_idx": 3}, {"type": "text", "text": "Stochastic Convex Optimization Let $\\mathcal{D}$ be a distribution supported on $\\mathcal{X}$ . Given some constraint set $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ of diameter at most $D$ , and a $G$ Lipschitz convex loss $\\ell:\\mathcal{W}\\times\\mathcal{X}\\,\\to\\,\\mathbb{R}$ , we are interested in minimizing the population loss, $L(\\bar{w};\\mathcal{D})=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\ell(w;x)\\right]$ . Denote the minimizer as $\\begin{array}{r}{w^{*}=\\operatorname*{min}_{w\\in\\mathcal{W}}\\left\\{L(w;\\mathcal{D})\\right\\}}\\end{array}$ . We evaluate the quality of the approximate solution, $w$ , via the excess risk, ${\\cal L}(w;D)-{\\cal L}(w^{*};D)$ . Specifically, we are interested in PA-DP algorithms which minimizes this quantity when given $S_{\\mathrm{pub}},S_{\\mathrm{priv}}\\overset{i.i.d.}{\\sim}\\mathcal{D}$ . For a datset $S$ we also define the empirical loss $\\begin{array}{r}{\\widehat{L}(w;S)=\\frac1{|S|}\\sum_{x\\in S}\\ell(w;x)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Supervised Learning and Generalized Linear Models (GLMs) In the supervised learning setting, in addition to the feature space $\\mathcal{X}$ , we define the label space $\\boldsymbol{\\wp}$ . We here let $\\mathcal{D}$ be a joint probability distribution over $\\mathcal X\\times\\mathcal X$ and $\\mathcal{D}_{\\mathcal{X}}$ and $\\mathcal{D}_{\\mathcal{Y}}$ denote the respective marginal distributions. Let $\\mathcal{H}\\subseteq\\mathbb{R}^{\\check{\\mathcal{X}}}$ be a hypothesis class of real-valued predictors, and let fat $_{\\cdot\\alpha}(\\mathcal{H})$ denote its fat shattering dimension at scale $\\alpha$ . Consider the loss function $\\ell:\\mathcal{H}\\times\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ , such that $\\ell(h;x,y)=\\phi_{y}(\\bar{h}(x))$ for some function $\\phi_{y}$ . We assume that the map $\\phi_{y}:\\mathbb{R}\\to\\mathbb{R}$ is $G$ -Lipschitz for all $y\\in\\mathcal{V}$ and is $B$ -bounded. Further, we assume that $\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|h(x)\\right|\\leq R$ and define $\\operatorname*{sup}_{x\\in\\mathcal{X}}\\|x\\|=\\|\\mathcal{X}\\|$ . ", "page_idx": 3}, {"type": "text", "text": "GLMs are a special case of supervised learning setting where the hypothesis class is that of linear predictors, $\\mathcal{H}=\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ , over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , and $h(x)\\stackrel{\\bullet}{=}w^{\\top}x$ . We refer to the public dataset of unlabeled feature vectors as $X_{\\mathrm{pub}}$ . ", "page_idx": 3}, {"type": "text", "text": "Covering numbers, fat-shattering and Rademacher Complexity Given $X=\\left(x_{1},x_{2},\\cdot\\cdot\\cdot,x_{m}\\right)$ the $\\ell_{p}$ distance between two hypothesis $h_{1},h_{2}\\in\\mathcal{H}$ with respect to the empirical measure over $X$ , is defined as, $\\begin{array}{r}{\\|h_{1}-h_{2}\\|_{p,X}=\\left(\\frac{1}{m}\\sum_{x\\in X}|h_{1}(x)-h_{2}(x)|^{p}\\right)^{1/p}}\\end{array}$ . Similarly, the distance with respect to the population, is given by $\\left\\|h_{1}-h_{2}\\right\\|_{p,\\mathcal{D}_{\\mathcal{X}}}~=~\\left(\\mathbb{E}_{x\\sim\\mathcal{D}_{\\mathcal{X}}}\\left|h_{1}(x)-h_{2}(x)\\right|^{p}\\right)^{1/p}$ . The covering number of $\\mathcal{H}$ at scale $\\alpha>0$ and given dataset $X$ , denoted as $\\mathcal{N}_{p}(\\mathcal{H},\\alpha,X)$ is the size of the minimal set of hypothesis, $\\tilde{\\mathcal{H}}$ , such that for any $h\\in\\mathcal H$ there exists $\\tilde{h}$ with $\\|h-\\tilde{h}\\|_{p,X}\\,\\leq\\,\\alpha$ . We define $\\begin{array}{r}{\\mathcal{N}_{p}(\\mathcal{H},\\alpha,m)=\\operatorname*{sup}_{X:|X|=m}\\mathcal{N}_{p}(\\mathcal{H},\\alpha,X)}\\end{array}$ , the covering number with respect to all datasets of size $m$ . We define fat-shattering dimension below. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 2. [BLW94] Let $\\mathcal{H}\\subseteq\\mathbb{R}^{\\mathcal{X}}$ and $\\alpha>0$ . We say that $\\mathcal{H}$ $\\alpha$ -shatters $X=\\{x_{1},x_{2},\\ldots,x_{m}\\}$ $\\begin{array}{r}{i f\\operatorname*{sup}_{r\\in\\mathbb{R}^{m}}\\operatorname*{min}_{y\\in\\{-1,1\\}^{m}}\\operatorname*{sup}_{h\\in\\mathcal{H}}\\operatorname*{min}_{i\\in[m]}y_{i}(h(x_{i})\\,-\\,r_{i})\\;\\ge\\;\\alpha}\\end{array}$ . The fat-shattering dimension, $f\\!a t_{\\alpha}(\\mathcal{H})$ , is the size of the largest $\\alpha$ -shattered set. ", "page_idx": 4}, {"type": "text", "text": "We define $\\Re_{m}(\\mathcal{H})$ , the worst-case Rademacher complexity of $\\mathcal{H}$ with respect to $m$ data points, as $\\begin{array}{r}{\\mathfrak{R}_{m}(\\mathcal{H})\\;{=}\\;\\operatorname*{sup}_{X:\\vert X\\vert=m}\\mathbb{E}_{\\sigma_{i}}\\operatorname*{sup}_{h\\in\\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}h(\\dot{x}_{i}).}\\end{array}$ An important example is that of normbounded linear predictors ${\\mathcal{H}}=\\{w:x\\mapsto\\langle w,x\\rangle:\\|w\\|\\leq D\\}$ over $\\mathcal{X}=\\{x:\\|x\\|\\leq\\|\\mathcal{X}\\|\\}$ . Herein, $\\begin{array}{r}{\\mathrm{fat}_{\\alpha}(\\mathcal{H})=\\Theta\\left(\\frac{D^{2}\\|\\mathcal{X}\\|^{2}}{\\alpha^{2}}\\right)}\\end{array}$ and $\\begin{array}{r}{\\Re_{m}(\\mathcal{H})=\\Theta\\left(\\frac{D\\|\\mathcal{X}\\|}{\\sqrt{m}}\\right)}\\end{array}$ [KST08, SST10]. ", "page_idx": 4}, {"type": "text", "text": "3 Private Stochastic Convex Optimization with Labeled Public Data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our lower bounds for private stochastic convex optimization with public data. When interpreting the following results, it is helpful to note that in the nontrivial regime, $n_{\\mathrm{pub}}=\\Theta(n)$ and $n_{\\mathrm{pub}}=o(n)$ , although our results\u221a hold regardless. Further, recall that an upper bound for this problem of O R min \u221an1pub ,\u221a1n $\\begin{array}{r}{O\\Big(R\\operatorname*{min}\\big\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}\\big\\}\\Big)}\\end{array}$ can be obtained by simply either applying an optimal SCO algorithm to only the public data (and discarding the private data) or applying an optimal DP-SCO algorithm and treating the entire dataset as private [BFTGT19]. As we will see, this strategy is essentially optimal. ", "page_idx": 4}, {"type": "text", "text": "3.1 Lower Bound for Stochastic Convex Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by stating our lower bound for public-data assisted differentially private SCO. ", "page_idx": 4}, {"type": "text", "text": "aTnhye -1P.A -LDetP $\\delta\\ \\leq\\ \\frac{1}{16n d}$ ,t $\\epsilon\\ \\leq\\ 1$ i,s tsa nad $d$ stbreib luatirogner ,h aann ds oam -uLniipvsecrhsiatlz  cloosnsst asuntc.h tFhoart $(\\epsilon,\\delta)$ $\\mathcal{D}$ $G$   \n$\\mathbb{E}\\big[L\\big(A(S_{p u b},S_{p r i v});\\mathcal{D}\\big)-\\operatorname*{min}_{w:\\|w\\|\\leq D}\\left\\{L\\big(w;\\mathcal{D}\\big)\\right\\}\\big]=\\Omega\\left(G D\\cdot\\Psi\\big(n_{p u b},n,d,\\epsilon,\\delta\\big)\\right)$ , where for some uni  \nversal constant $c$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Psi(n_{p u b},n,d,\\epsilon,\\delta)=\\left\\{\\begin{array}{l l}{\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{p u b}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}\\right\\},}&{d\\geq c n\\epsilon,\\,n_{p u b}\\leq\\frac{n\\epsilon}{c\\log\\left(1/[\\sqrt{n d}\\delta]\\right)}}\\\\ {\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{p u b}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\right\\},}&{e l s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The function $\\Psi$ is defined to avoid repetitive notation in the rest of this section. Barring the mild restriction on $n_{\\mathrm{pub}}$ , even though the $\\sqrt{\\log\\left(1/\\delta\\right)}$ term is only obtained when $d\\geq n\\epsilon$ , the \u201caggregate\u201d lower bound is tight for all $\\begin{array}{r}{d\\not\\in\\lbrack\\frac{n\\epsilon^{2}}{\\log(1/\\delta)},n\\epsilon]}\\end{array}$ since when $\\begin{array}{r}{d\\leq\\frac{n\\epsilon^{2}}{\\log\\left(1/\\delta\\right)}}\\end{array}$ the non-private \u221a1 lower bound dominates. It is also pertinent to our results in Section 4 that the problem construction used to achieve this lower bound is a convex GLM, and as a result this lower bound holds even for GLMs. ", "page_idx": 4}, {"type": "text", "text": "Finally, similar statements can be made about strongly convex optimization. We again provide just one such statement here. ", "page_idx": 4}, {"type": "text", "text": "-hsetroornegmly  2c.o nLveet $\\begin{array}{r}{\\delta\\leq\\frac{1}{16n d}}\\end{array}$ ,p $\\epsilon\\leq1$ .l oFsso rs uacnhy $(\\epsilon,\\delta)$ -PA-DP algorithm there exists a distribution $\\mathcal{D}$ $\\lambda$ $G$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[L\\big(\\mathcal{A}(S_{p u b},S_{p r i v});\\mathcal{D}\\big)-\\underset{w:\\,|w|\\,|\\leq D}{\\operatorname*{min}}\\,\\big\\{L\\big(w;\\mathcal{D}\\big)\\big\\}\\,\\big]=\\Omega\\,\\Big(\\frac{G^{2}}{\\lambda}\\Psi^{2}\\big(n_{p u b},n,\\epsilon,\\delta\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The crux of the proofs for both the above results lies in establishing new mean estimation lower bounds for PA-DP mean estimation, which we give in Appendix B.1. These mean estimation lower bound use a novel application of a construction known as fingerprinting codes. In particular, the introduction of public data introduces significant challenges in the traditional analysis of fingerprinting codes. As these challenges are more technical in nature, we defer their discussion to Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "After establishing the mean estimation lower bounds, we can adapt the reductions first used in [BST14] that show mean estimation lower bounds can be used to provide lower bounds for risk minimization without public data. Full proofs for the above claims, and in particular details for the above reductions, are found in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "Lower Bound for Pure DP Case. While not the primary focus of this work, the previous lower bound directly leads to a lower bound for pure PA-DP SCO. Since any $\\epsilon$ -DP algorithm is $(\\epsilon,\\delta)$ -DP for any $\\delta>0$ , we can use the above theorem to obtain a non-trivial lower bound for the pure DP case by setting \u03b4 small. Specifically, by setting \u03b4 such that log (1/\u03b4) = 120n2\u03f5npub , one immediately obtains a lower bound of \u2126 min{\u221an1pub ,\u221anp\u221aubd\u00b7n\u03f5} for d large enough. Simplifying this expression yields the following. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Let $d\\geq$ cn\u03f5 for a constant $c,$ , and $\\boldsymbol{\\mathcal{A}}$ be an $\\epsilon$ -PA-DP algorithm. There exist a distribution $\\mathcal{D}$ and a $G$ -Lipschitz loss such that $\\begin{array}{r}{\\mathbb{E}\\big[L(A(S_{p u b},S_{p r i v});\\mathcal{D})-\\underset{w:\\,|\\,w|\\,|\\leq D}{\\operatorname*{min}}\\left\\{L(w;\\mathcal{D})\\right\\}\\big]=\\Omega\\left(\\frac{G D}{\\sqrt{n_{p u b}}}\\right)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The known $\\begin{array}{r}{O\\left(G D\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{d}{n\\epsilon}\\right\\}\\right)}\\end{array}$ upper bound for this problem shows that this bound is tight (in the regime in which it holds). Essentially, this bound states that when , the public dataset is not useful (at least asymptotically). Previously [LLHR23, Theorem 31] established that when $d\\leq n\\epsilon/n_{\\mathrm{pub}}$ , a tight lower bound of $\\begin{array}{r}{\\Omega\\left(G D(\\frac{d}{n\\epsilon}+\\frac{1}{\\sqrt{n}})\\right)}\\end{array}$ holds, effectively showing that in this regime the public dataset is not useful5. We leave the remaining regime where $\\begin{array}{r}{d\\in(\\frac{n\\epsilon}{n_{\\mathrm{pub}}},n\\epsilon)}\\end{array}$ as an interesting open problem for future work. Finally, we note that similar statements can be made about strongly convex losses using Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "4 Private Supervised Learning with Unlabeled Public Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we consider supervised learning with real-valued predictors given labeled private data and unlabeled public data. Our results show that, in this setting, it is possible to go beyond the limitations established in the prior section. ", "page_idx": 5}, {"type": "text", "text": "4.1 Efficient PA-DP learning of Convex Generalized Linear Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start with learning linear predictors with convex loses a.k.a. convex generalized learning models. We propose Algorithm 1, which uses the public unlabeled data to perform dimensionality reduction of the private labeled feature vectors. In the following, we use span to denote the span of a set of vectors and dim to denote the dimension of a subspace. The dataset of public unlabeled feature vectors is denote as $X_{\\mathrm{pub}}$ . Our algorithm projects the private feature vectors onto the subspace spanning $\\mathcal{W}\\cap\\operatorname{span}(X_{\\mathrm{pub}}^{\\cdot})$ to get $\\dim(\\operatorname{span}(X_{\\mathrm{pub}})\\cap\\mathcal{W})$ -dimensional representation of the private feature vectors. It then reparametrizes the loss function so that its domain is $\\dim(\\operatorname{span}(S_{\\mathrm{pub}})\\cap\\mathcal{W})\\cdot$ - dimensional and applies a private subroutine in the lower dimensional space. The output of the subroutine is then embedded back in $\\mathbb{R}^{d}$ . Algorithms similar to Algorithm 1 have appeared in the literature (e.g. [PHYS24]). We emphasize that our key contribution is the formal analysis of this technique and the fact that we provide tight upper and lower bounds while simultaneously avoiding many of the strong assumptions seen in previous work, such as large margin assumptions. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Efficient PA-DP learning of GLMs with unlabeled public data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Private labeled dataset $S_{\\mathrm{priv}}$ , public unlabeled dataset $X_{\\mathrm{pub}}$ , privacy parameters $\\epsilon,\\delta>0$ .   \n1: Let $U\\in\\mathbb{R}^{d\\times\\dim(\\mathcal{W}\\cap\\operatorname{span}(X_{\\mathrm{pub}}\\hat{)})}$ denote the orthogonal projection onto span $(X_{\\mathrm{pub}})\\cap\\mathcal{W}$ .   \n2: Define $\\tilde{S}_{\\mathrm{priv}}=\\left\\{(U^{\\top}x_{i},y_{i})\\right\\}_{i=1}^{n_{\\mathrm{priv}}}$ and let $\\tilde{\\mathcal{W}}=\\left\\{U^{\\top}w:w\\in\\mathcal{W}\\right\\}$ .   \n3: Apply $(\\epsilon,\\delta)$ -DP subroutine, $\\tilde{\\mathcal A}$ , on loss function $w\\mapsto\\phi_{y}(\\langle w,x\\rangle)$ with dataset $\\tilde{S}_{\\mathrm{priv}}$ over the constraint set W\u02dc, to get w\u02dc \u2208Rdim(W\u2229span(Spub)). ", "page_idx": 5}, {"type": "text", "text": "Output: $\\widehat{w}=U\\widetilde{w}$ . ", "page_idx": 5}, {"type": "text", "text": "Our main result for convex Lipschitz losses is the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let $\\epsilon\\;>\\;0,\\delta\\;>\\;0$ and $\\epsilon\\,\\le\\,\\log\\left(1/\\delta\\right)$ . For a $G$ -Lipschitz, $B$ -bounded convex loss function, Algorithm $^{\\,l}$ satisfies $(\\epsilon,\\delta)$ -PA-DP. If the private subroutine $\\tilde{\\mathcal A}$ guarantees the following, with probability at least $1-\\beta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{L}(\\tilde{\\mathcal{A}}(\\tilde{S}_{p r i v});\\tilde{S}_{p r i v})-\\operatorname*{min}_{w\\in\\tilde{\\mathcal{W}}}\\widehat{L}(w;\\tilde{S}_{p r i v})=O\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{\\sqrt{n_{p u b}\\log{(1/\\delta)}}+\\sqrt{\\log{(1/\\beta)}}}{n_{p r i v}\\epsilon}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then with (log(2/\u03b2)n+plriovg\u03f5(1/\u03b4))1/2 , with probability at least 1 \u2212\u03b2, L( w ; D) \u2212L(w\u2217; D) is ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\cal O}\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{p r i v}}}+\\frac{\\left(\\log\\left(2/\\beta\\right)+\\log\\left(1/\\delta\\right)\\right)^{1/4}}{\\sqrt{n_{p r i v}}\\epsilon}\\right)+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{p r i v}}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We note that DP algorithms such as projected noisy SGD [BST14] and the regularized exponential mechanism [GLL22], both of which can be implemented efficiently, are can be used to achieve (1), since the projected problem is at most $n_{\\mathrm{pub}}$ dimensional. ", "page_idx": 6}, {"type": "text", "text": "The above result shows that in the usual regime of $\\epsilon=\\Theta(1)$ , there is no price of privacy, thereby obtaining the non-private rate of $\\begin{array}{r}{O\\left(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}\\right)}\\end{array}$ . We contrast this with the rate of $\\begin{array}{r}{O\\left(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{\\sqrt{d}}{n_{\\mathrm{priv}}\\epsilon}\\right)}\\end{array}$ achievable without public data. Our result is better when $d\\geq n_{\\mathrm{priv}}\\epsilon$ , which is the interesting regime since herein the private error dominates the non-private error. Further, our lower bound (Theorem 4 below) shows that this is the non-trivial regime (for any $\\epsilon\\,=\\,O(1)$ ), since otherwise, even with unlimited public data, the optimal rate is achieved without using any of it. We also note that the above rate is achievable without public data, but in the unconstrained setting where the output $\\widehat{w}$ can have very large norm and so may lie outside $\\mathcal{W}$ $\\mathcal{V}\\left[\\mathrm{ABG}^{+}22\\right]$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of the result primarily follows from the more general result with fat-shattering hypothesis classes (Theorem 7). We provide the key ideas as well as some details pertaining to linear predictors in Section 4.2 after Theorem 7. The full proof of this result is deferred to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Lower Bounds. The above rate as well as the number of public samples used are nearly-optimal. The first claim is due to the following result, which gives a lower bound on excess risk of DP algorithms under full knowledge of the marginal distribution, for Lipschitz GLMs. As unlabeled public data can only reveal information about the marginal distribution, this shows that further unlabeled public samples cannot hope to improve the rate we give in Theorem 3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let $\\epsilon\\;\\leq\\;1,\\delta\\;\\leq\\;\\epsilon$ and $\\boldsymbol{\\mathcal{A}}$ be an $(\\epsilon,\\delta)$ -DP algorithm. There exists a $G$ -Lipschitz convex GLM loss function, and joint distribution $\\mathcal{D}$ such that given a dataset $S$ comprising $n$ i.i.d. samples from $\\mathcal{D}$ and full knowledge of the marginal distribution $D_{\\mathcal{X}}$ , we have the following: $\\begin{array}{r}{\\mathbb{E}_{A,S}\\left[L(A(S);\\mathcal{D})-\\operatorname*{min}_{w:\\|w\\|\\leq D}L(w;\\mathcal{D})\\right]=\\Omega\\left(G D\\left\\|\\mathcal{X}\\right\\|\\left(\\frac{1}{\\sqrt{n}}+\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n\\epsilon}},\\frac{\\sqrt{d}}{n\\epsilon}\\right\\}\\right)\\right).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We note that the bound with $\\frac{\\sqrt{d}}{n_{\\mathrm{priv}}\\epsilon}$ can be achieved without using any public data via standard results [BFTGT19, $\\mathrm{ABG}^{+}22]$ . This result is largely a corollary of $[\\mathrm{ABG^{+}}22$ , Theorem 6]. We provide full details in Appendix C.3.1. ", "page_idx": 6}, {"type": "text", "text": "To establish optimality of public sample complexity, we give the following lower bound which shows that $\\tilde{\\Omega}(n_{\\mathrm{priv}}\\epsilon)$ samples are necessary to achieve the above rate. See Appendix C.3.2 for proof. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Let $\\begin{array}{r}{n_{p r i\\nu},n_{p u b},d\\mathrm{~\\boldmath~\\in~}\\dot{\\mathbb{N}},\\epsilon\\mathrm{~\\boldmath~\\leq~}\\mathrm{~1,~}\\delta\\mathrm{~\\boldmath~<~}\\mathrm{~\\frac{1}{16d n}~}}\\end{array}$ and $d\\ =\\ \\omega(n_{p r i\\nu}\\epsilon)$ . If there exists an $(\\epsilon,\\delta)$ -PA-DP algorithm $\\boldsymbol{\\mathcal{A}}$ , which, for any $G$ -Lispschitz convex GLM, achie\u221aves excess risk $\\begin{array}{r l r}{\\mathbb{E}\\left[L(A(X_{p u b},S_{p r i v});\\mathcal{D})-\\operatorname*{min}_{w:\\|w\\|\\leq D}L(w;\\mathcal{D})\\right]}&{=}&{O\\big(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{1}{\\sqrt{n_{p r i v}}}\\;+\\;\\frac{\\sqrt{\\log(1/\\delta)}}{\\sqrt{n_{p r i v}}}\\right)\\big)}\\end{array}$ )  = O GD \u2225X\u2225 \u221an1priv + l\u221aongp(ri1v/\u03f5\u03b4) , for $S_{p r i\\nu}\\sim{\\cal D}^{n_{p r i\\nu}}$ and $X_{p u b}\\sim{\\mathcal{D}}_{\\mathcal{X}}^{n_{p u b}}$ , then $\\begin{array}{r}{n_{p u b}=\\Omega(\\frac{n_{p r i v}\\epsilon}{\\log(1/\\delta)})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Optimistic rates. We now consider additional assumptions that the loss function is non-negative and $H$ -smooth, such as in the case of linear regression where $\\phi_{y}(a)=(a-y)^{2}$ . This is a well-studied setting [SST10] especially since it allows for obtaining optimistic rates: those that interpolate between a slow worst-case rate and a faster rate under (near) realizability or interpolation conditions. The main result is the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. Let $\\epsilon>0,\\delta>0$ and $\\epsilon\\,\\le\\log\\left(1/\\delta\\right)$ . For a $G$ -Lipschitz, $B$ -bounded non-negative $H$ - smooth loss function, Algorithm $^{\\,l}$ satisfies $(\\epsilon,\\delta)$ -PA-DP. If the private subroutine $\\tilde{\\mathcal A}$ guara\u221antees Equation (1) with probability at least 1\u2212\u03b2, then with npub = O\u02dc (HGD2\u2225/X3(\u2225l)o2g/(31(/n\u03b4p)r)iv1\u03f5/)32/3 $\\begin{array}{r}{n_{p u b}=\\tilde{O}\\bigg(\\frac{(H D\\|\\chi\\|)^{2/3}(n_{p r i v}\\epsilon)^{2/3}}{G^{2/3}(\\log(1/\\delta))^{1/3}}+\\frac{\\sqrt{H}n_{p r i v}\\epsilon\\sqrt{\\widehat{L}(\\widehat{w}^{*};S_{p r i v})}}{G\\sqrt{\\log(1/\\delta)})}\\bigg)}\\end{array}$ with probability at least $1-\\beta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\cdot}{\\underset{\\cdot}{\\sim}}(\\widehat{w};\\mathcal{D})-\\widehat{L}(\\widehat{w}^{*};S_{p r i v})=\\tilde{O}\\left(\\left(\\frac{\\sqrt{H}D\\|\\mathcal{X}\\|}{\\sqrt{n_{p r i v}}\\epsilon}+\\sqrt{\\frac{B}{n_{p r i v}}}\\right)\\sqrt{\\widehat{L}(\\widehat{w}^{*};S_{p r i v})}+\\frac{H^{1/4}D\\|\\mathcal{X}\\|\\|\\sqrt{G}\\widehat{L}(\\widehat{w}^{*};S_{p r i v})^{1/4}}{\\sqrt{n_{p r i v}}\\epsilon}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\ \\tilde{O}\\left(\\frac{G D\\|\\mathcal{X}\\|}{n_{p r i v}\\epsilon}+\\left(\\frac{\\sqrt{H}D^{2}\\|\\mathcal{X}\\|^{2}\\,G}{n_{p r i v}\\epsilon}\\right)^{2/3}+\\frac{H\\|\\mathcal{X}\\|^{2}\\,D^{2}}{n_{p r i v}\\epsilon}+\\frac{B}{n_{p r i v}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\widehat{w}^{*}$ is the minimizer of $\\widehat{L}\\,w.r{t\\,S_{p r i v}}$ and $\\tilde{O}$ hides poly $\\left(\\log\\left(1/\\delta\\right),\\log\\left(1/\\beta\\right)\\right)$ terms. ", "page_idx": 7}, {"type": "text", "text": "A similar result as above can be obtained with $\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})$ replaced by $L(w^{*};\\mathcal{D})$ above \u2013 see Theorem 14 for the full theorem statement. Thi s r a te, in the worst-case, is essentially the same as that of Theorem 3, which is $\\begin{array}{r}{\\tilde{O}\\left(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{1}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}\\right)}\\end{array}$ . However, optimistically, when $L(w^{*};\\mathcal{D})$ or $\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})$ is small, we get a faster rate of $\\begin{array}{r}{\\tilde{O}\\Big(\\frac{1}{n_{\\mathrm{priv}}}+\\frac{1}{(n_{\\mathrm{priv}}\\epsilon)^{2/3}}\\Big)}\\end{array}$ . We note that this is seemingly weaker than what is known in the unconstrained setting, where $[\\mathbf{ABG}^{+}22]$ obtained a worst-case rate of $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{1}{(n_{\\mathrm{priv}}\\epsilon)^{2/3}}\\big)}\\end{array}$ (npriv1\u03f5)2/3 . We show that we can recover this faster rate under an extra assumption that the global minimizer of the risk, lies in the constraint set $\\mathcal{W}$ \u2013 note that this is trivially true in the unconstrained setup; see Theorem 15 for the statement. ", "page_idx": 7}, {"type": "text", "text": "We note that projected noisy SGD [BST14] and the regularized exponential mechanism [GLL22], both of which can be implemented efficiently, are possible choices for the private sub-routine $\\tilde{\\mathcal A}$ that realize the above theorem statements. ", "page_idx": 7}, {"type": "text", "text": "4.2 PA-DP Supervised learning of Fat-Shattering Classes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we consider a general supervised learning setting with fat-shattering hypothesis classes and potentially non-convex losses, with unlabeled public data. Our proposed algorithm is similar to that of [ABM19], which uses the pubic unlabeled data to construct a small finite, yet representative, subset of the hypothesis class. Our construction uses a cover of the hypothesis class with respect to the $\\ell_{2}$ distance of predictions on the public data points. We then use the exponential mechanism to privately select a hypothesis using the empirical loss on private data as the score function. ", "page_idx": 7}, {"type": "text", "text": "We note that we operate under the pure DP setting (as opposed to approximate DP). Our techniques are based on selection which do not exhibit improved guarantees under approximate DP. Further, we note that, without public data, with non-convex losses, there is no separation of optimal rates between pure and approximate DP [GTU23]. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Supervised private learning with public unlabeled data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Datasets $X_{\\mathrm{pub}}$ and $S_{\\mathrm{priv}}$ , privacy parameter $\\epsilon>0$ , scale of cover $\\alpha>0,\\gamma>0$ . 1: Construct $\\tilde{\\mathcal{H}}$ , a minimal $\\alpha$ -cover of $\\mathcal{H}$ , with respect to the following metric ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|h_{1}-h_{2}\\right\\|_{2,X_{\\mathrm{pub}}}=\\sqrt{\\frac{1}{n_{\\mathrm{pub}}}\\sum_{x\\in X_{\\mathrm{pub}}}\\left(h_{1}(x)-h_{2}(x)\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2: Return $\\widehat{h}$ sampled with probability $p(h)\\propto\\mathrm{\\dot{exp}}\\left(-\\gamma\\widehat{L}(h;S_{\\mathrm{priv}})\\right)$ over h \u2208H\u02dc ", "page_idx": 7}, {"type": "text", "text": "Our main result for the Lispchitz setting is the following. ", "page_idx": 7}, {"type": "text", "text": "Theorem 7. Algorithm 2 with $\\begin{array}{r}{\\gamma\\,=\\,\\frac{2\\operatorname*{min}(B,G R)}{n_{p r i\\nu}\\epsilon}}\\end{array}$ satisfies $\\epsilon$ -PA-DP. For any $\\alpha\\,>\\,0$ and $n_{p u b}\\,=$ $\\begin{array}{r}{O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:l o g^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)<\\infty,}\\end{array}$ , with probability at least $1-\\beta$ , we have $L(\\widehat{h};{\\mathcal{D}})-\\operatorname*{min}_{h\\in{\\mathcal{H}}}L(h;{\\mathcal{D}})$ is at most ", "page_idx": 7}, {"type": "equation", "text": "$$\n2G\\mathfrak{R}_{n_{p r i v}}(\\mathcal{H})+O\\left(\\frac{B\\sqrt{\\log{(4/\\beta)}}}{\\sqrt{n_{p r i v}}}\\right)+\\tilde{O}\\left(\\frac{\\operatorname*{min}{(B,G R)}(f a t_{c\\alpha}(\\mathcal{H})+\\log{(4/\\beta)})}{n_{p r i v}\\epsilon}\\right)+2G\\alpha,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $c$ is an absolute constant. ", "page_idx": 8}, {"type": "text", "text": "Our result shows that the model of PA-DP with unlabeled public data allows for obtaining nontrivial rates for supervised learning with any fat-shattering class, as is the case in the non-private setting. Further, in many standard settings, such as that of (Euclidean) GLMs, the Rademacher complexity is $\\begin{array}{r}{\\Re_{m}(\\mathcal{H})=\\dot{O}\\left(\\frac{1}{\\sqrt{m}}\\right)}\\end{array}$ which implies that $\\begin{array}{r}{\\mathrm{fat}_{\\alpha}(\\mathcal{H})=O\\left(\\frac{1}{\\alpha^{2}}\\right)}\\end{array}$ (see Theorem 9). In those cases, our guarantee simplifies to essentially yield a rate of $\\begin{array}{r}{O\\left(\\mathfrak{R}_{n_{\\mathrm{priv}}}(\\mathcal{H})+\\frac{1}{(n_{\\mathrm{priv}}\\epsilon)^{1/3}}+\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}\\right)-\\mathrm{see}}\\end{array}$ Corollary 4 for the exact statement for GLMs. ", "page_idx": 8}, {"type": "text", "text": "Proof Idea. We briefly discuss some main ideas in the proof. The key is to show that if $\\tilde{\\mathcal{H}}$ is a cover of $\\mathcal{H}$ with respect to the empirical distance on public feature vectors, $\\Vert\\cdot\\Vert_{2,X_{\\mathrm{pub}}}$ , then with enough public feature vectors, it is also a cover with respect to the population distance $\\|\\cdot\\|_{2,\\mathcal{D}_{\\mathcal{X}}}$ . This is captured in the following result. ", "page_idx": 8}, {"type": "text", "text": "Lemma 1. Let H\u02dc be a \u03c4-cover of H with respect to \u2225\u00b7\u22252,Xpub. For $\\begin{array}{r l r l}{n_{p u b}}&{{}}&{=}\\end{array}$ R2 lo\u03b1g2(1/\u03b2), min m : log3(m)R2m(H) \u2264\u03b12   < \u221e, for every h \u2208H, with probability at least $1-\\beta$ , there exists $\\tilde{h}\\in\\tilde{\\mathcal{H}}$ such that $\\|h-\\tilde{h}\\|_{2,\\mathcal{D}_{\\mathcal{X}}}\\le\\alpha+\\tau$ . ", "page_idx": 8}, {"type": "text", "text": "This result allows us to appropriately approximate a hypothesis class with enough public unlabeled points. This approximation roughly translates to the same additive error in the final bound while concurrently allowing for the use of the smaller finite hypothesis class $\\tilde{\\mathcal{H}}$ of size $|\\tilde{\\mathcal{H}}|=\\tilde{O}(\\mathrm{fat}_{\\tau}(\\mathcal{H}))$ . ", "page_idx": 8}, {"type": "text", "text": "For linear predictors with convex losses, as in Theorem 3, we show that the $\\operatorname{span}(X_{\\mathrm{pub}})\\cap\\mathcal{W}$ is a valid 0-cover w.r.t. $\\Vert\\cdot\\Vert_{2,X_{\\mathrm{pub}}}$ . However, the cover being continuous and convex allows application of convex optimization techniques (as opposed to selection, as above), thereby obtaining stronger results with efficient procedures. The above procedure yields optimistic rates for non-negative and smooth losses; see Theorem 17 for details. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Application: Neural Networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we instantiate our general result to give a guarantee for learning feed-forward neural networks in the PA-DP setting. We use the result of [GRS18] but note that other results which give bounds on the Rademacher complexity of neural networks, such as [BFT17, Sel23] can also be used. ", "page_idx": 8}, {"type": "text", "text": "We consider a depth $M$ feed-forward neural network which implements the function $x\\mapsto$ $W_{M}(\\sigma(W_{M-1}\\ldots\\sigma(W_{1}x))\\ldots)$ . Here, $W_{1},W_{2},\\dots,W_{M}$ are the weight matrices and $\\sigma$ is a (nonlinear) activation function. We consider 1-Lipschtiz positive-homogeneous activation such as the ReLU function, $\\sigma(z)=\\operatorname*{max}(0,z)$ , applied coordinate-wise. Our main result is the following. ", "page_idx": 8}, {"type": "text", "text": "Corollary 2. Let $\\left(R_{j}\\right)_{j=1}^{M}$ be a sequence of scalars and $M\\in\\mathbb{N}$ . In the setting of Theorem 7 with $\\mathcal{X}\\,=\\,\\bigl\\{\\boldsymbol{x}\\in\\mathbb{R}^{d}:\\|\\boldsymbol{x}\\|\\overset{\\cdot}{\\leq}\\|\\boldsymbol{\\mathcal{X}}\\|\\bigr\\}$ and $\\mathcal{H}$ being the class of depth $M$ feed-forward neural networks, with 1-Lipschitz positive-homogenous activation, and weight matrices, bounded as $\\|W_{j}\\|_{F}\\leq R_{j}$ , with $n_{p u b}=\\tilde{O}\\left((\\|\\mathcal{X}\\|\\,(\\prod_{j=1}^{M}R_{j}))^{2/3}(n_{p r i v}\\epsilon)^{2/3}M^{1/3}\\log{(2/\\beta)}\\right)$ , with probability at least $1-\\beta$ , $\\begin{array}{r}{L(\\widehat{h};\\mathcal{D})-\\operatorname*{min}_{h\\in\\mathcal{H}}L(h;\\mathcal{D})}\\end{array}$ is at most $>\\left(\\frac{G\\left\\|X\\right\\|\\sqrt{M}\\prod_{j=1}^{M}R_{j}}{\\sqrt{n_{p r i v}}}+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{p r i v}}}+\\frac{B\\log\\left(4/\\beta\\right)}{n_{p r i v}\\epsilon}\\right)+\\tilde{O}\\left(\\left(\\frac{B G^{2}M\\left\\|X\\right\\|^{2}\\left(\\prod_{j=1}^{M}R_{j}\\right)^{2}}{n_{p r i v}\\epsilon}\\right)^{1/3}\\right).$ ", "page_idx": 8}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We note that the above result has a polynomial dependence on the depth $M$ , which is a consequence of the (non-private) Rademacher complexity of [GRS18]. It is also possible to get fully size-independent bounds by utilizing such existing results, however they require more stringent norm bounds on the weight matrices [Sel23]. Further, a similar result follows for non-negative smooth losses from [SST10], but we omit this extension for brevity. ", "page_idx": 8}, {"type": "text", "text": "4.2.2 Application: Non-Euclidean GLMs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In the non-Euclidean GLM setting, we consider $(\\mathbb{X},\\|\\cdot\\|)$ as a $d$ dimensional (where $d\\in\\mathbb{N}\\cup\\{\\infty\\})$ Banach space, and $\\left(\\mathbb{W},\\lVert\\cdot\\rVert_{*}\\right)$ is its dual space. The feature vectors $x$ are bounded as $\\chi\\,\\,=$ $\\{x\\in\\mathbb{X}:\\|x\\|\\leq\\|\\mathcal{X}\\|\\}$ and $\\mathcal{W}\\subseteq\\{w\\in\\mathbb{W}:\\overline{{\\Delta}}(w)\\leq D^{r}\\}$ where $\\Delta$ is a $r$ -uniformly convex function6 with respect to $\\left\\|\\cdot\\right\\|_{*}$ . A canonical example is the $(\\ell_{p},\\ell_{q})$ -setup [KST08, FGV17], wherein the functions $\\begin{array}{r}{\\Delta(w)=\\frac{\\log(d)}{2}\\left\\|w\\right\\|_{1+(1/\\log(d))}^{2}}\\end{array}$ , $\\begin{array}{r}{\\Delta(w)\\,=\\,\\frac{1}{2(p-1)}\\,\\left\\Vert w\\right\\Vert_{p}^{2}}\\end{array}$ and $\\begin{array}{r}{\\Delta(w)=\\frac{2^{p-2}}{p}\\left\\lVert w\\right\\rVert_{p}^{p}}\\end{array}$ are 2, 2 and -uniformly convex with respect to $\\left\\|\\cdot\\right\\|_{p}$ for $p=1,1<p\\leq2$ and $p\\geq2$ respectively. ", "page_idx": 9}, {"type": "text", "text": "The GLM loss function $\\ell(w;x,y)=\\phi_{y}(\\langle w,x\\rangle)$ where $\\langle\\cdot,\\cdot\\rangle:\\mathcal{X}\\times\\mathcal{W}\\to\\mathbb{R}$ is a duality pairing. In this case, the Rademacher complexity of linear functions, is bounded as $O\\left({\\frac{D\\|{\\boldsymbol{\\mathcal{X}}}\\|}{m^{1/r}}}\\right)$ , where $s$ is the conjugate of $r$ i.e. $\\textstyle{\\frac{1}{r}}+{\\frac{1}{s}}=1$ (see, e.g. [FGV17]). We obtain the following result by instantiating Theorem 7 with the Rademacher complexity and fat-shattering dimension of non-Euclidean GLMs. ", "page_idx": 9}, {"type": "text", "text": "Corollary 3. In the setting of Theorem 7, together with $\\mathcal{X}=\\left\\{x\\in\\mathbb{R}^{d}:\\lVert x\\rVert\\leq\\lVert\\mathcal{X}\\rVert\\right\\}$ and $\\mathcal{H}=$ $\\{x\\mapsto\\langle w,x\\rangle\\,,x\\in\\mathcal{X},\\Delta(w)\\leq D^{r}\\}$ . Given $n_{p u b}=\\tilde{O}\\left((n_{p r i\\nu}\\epsilon)^{r/(r+1)}\\log\\left(2/\\beta\\right)\\right)$ , with probability at least $1-\\beta$ , $\\begin{array}{r}{L(\\widehat{\\boldsymbol{w}};\\mathcal{D})-\\operatorname*{min}_{\\boldsymbol{w}\\in\\mathcal{W}}L(\\boldsymbol{w};\\mathcal{D})}\\end{array}$ is at most ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\tilde{\\cal O}\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{1}{n_{p r i v}1/r}+\\frac{\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{p r i v}}}+\\frac{\\log\\left(2/\\beta\\right)}{\\left(n_{p r i v}\\epsilon\\right)^{\\frac{1}{r+1}}}+\\frac{\\log\\left(4/\\beta\\right)}{n_{p r i v}\\epsilon}\\right)+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{p r i v}}}\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The above yields guarantees for the special case of $(\\ell_{p},\\ell_{q})$ -setup with $r=\\operatorname*{max}{\\{2,p\\}}$ . We remind that in the (constrained) convex Euclidean GLM setting, our dimension-independent rate in Theorem 3, with public unlabeled data recover the rates which were known to be achievable in the unconstrained setting. Further the above rate for $p=1$ case can be used to obtain guarantees for the polyhedral setting with $\\|w\\|_{1}\\leq D$ constraint, resulting in a $\\begin{array}{r}{O\\left(\\sqrt{\\frac{\\log(d)}{n_{\\mathrm{priv}}}}+\\left(\\frac{\\log(d)}{n_{\\mathrm{priv}}\\epsilon}\\right)^{1/3}\\right)}\\end{array}$ rate. We note that [BGM21] showed a rate of $\\tilde{O}\\left(\\sqrt{\\frac{\\log(d)}{n}}+\\frac{\\sqrt{\\log(d)}}{\\sqrt{n\\epsilon}}\\right)$ for this setting, with convex losses without public data. Importantly, for the other cases, i.e. $p>1,p\\neq2$ , there are no such (nearly) dimensionindependent analogs of our result without public data, as of yet. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "R. Arora\u2019s and E. Ullah\u2019s research was supported, in part, by NSF BIGDATA award IIS-1838139 and NSF CAREER award IIS-1943251. R. Bassily\u2019s and M. Menart\u2019s research was supported by NSF CAREER Award 2144532 and, in part, by NSF Award 2112471. C. Guzm\u00e1n\u2019s research was partially supported by INRIA Associate Teams project, ANID FONDECYT 1210362 grant, ANID Anillo ACT210005 grant, and National Center for Artificial Intelligence CENIA FB210017, Basal ANID. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ABDCBH97] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scalesensitive dimensions, uniform convergence, and learnability. Journal of the ACM (JACM), 44(4):615\u2013631, 1997. ", "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{ABG}^{+}22]$ Raman Arora, Raef Bassily, Crist\u00f3bal Guzm\u00e1n, Michael Menart, and Enayat Ullah. Differentially private generalized linear models revisited. Advances in Neural Information Processing Systems, 35:22505\u201322517, 2022. ", "page_idx": 9}, {"type": "text", "text": "[ABM19] Noga Alon, Raef Bassily, and Shay Moran. Limits of private learning with access to public data. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. ", "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{AGM}^{+}22]$ Ehsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas Steinke, Thomas Steinke, Vinith M Suriyakumar, Om Thakkar, and Abhradeep Thakurta. Public data-assisted mirror descent for private model training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 517\u2013535. PMLR, 17\u201323 Jul 2022.   \n$[\\mathbf{BCM}^{+}20]$ Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and Steven Wu. Private query release assisted by public data. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 695\u2013703. PMLR, 13\u201318 Jul 2020.   \n[BDBC $^{+}23$ ] Shai Ben-David, Alex Bie, Clement Louis Canonne, Gautam Kamath, and Vikrant Singhal. Private distribution learning with public data: The view from sample compression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM), 36(4):929\u2013965, 1989. [BFT17] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in neural information processing systems, 30, 2017.   \n[BFTGT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems, 32, 2019.   \n[BGM21] Raef Bassily, Crist\u00f3bal A Guzm\u00e1n, and Michael Menart. Differentially private stochastic optimization: New results in convex and non-convex settings. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [BKS22] Alex Bie, Gautam Kamath, and Vikrant Singhal. Private estimation with public data. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [BLW94] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the learnability of real-valued functions. In Proceedings of the seventh annual conference on Computational learning theory, pages 299\u2013310, 1994. [BNS13] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. In Prasad Raghavendra, Sofya Raskhodnikova, Klaus Jansen, and Jos\u00e9 D. P. Rolim, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 363\u2013378, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. [BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pages 464\u2013473. IEEE, 2014.   \n[BTGT18] Raef Bassily, Om Thakkar, and Abhradeep Guha Thakurta. Model-agnostic private learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [BUV14] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. In Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, page 1\u201310, New York, NY, USA, 2014. Association for Computing Machinery.   \n[BWZK22] Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Differentially private bias-term only fine-tuning of foundation models. In NeurIPS 2022 Workshop on Trustworthy and Socially Responsible Machine Learning (TSRML), 2022. [CH11] Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning. In Sham M. Kakade and Ulrike von Luxburg, editors, Proceedings of the 24th Annual Conference on Learning Theory, volume 19 of Proceedings of Machine Learning Research, pages 155\u2013186, Budapest, Hungary, 09\u201311 Jun 2011. PMLR.   \n[CWZ21] T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. The Annals of Statistics, 49(5):2825 \u2013 2850, 2021.   \n[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265\u2013284. Springer, 2006.   \n$[{\\mathrm{DSS}}^{+}15]$ Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 650\u2013669, 2015. [Duc23] John Duchi. Lecture notes on statistics and information theory, May 2023.   \n[FGV17] Vitaly Feldman, Cristobal Guzman, and Santosh Vempala. Statistical query algorithms for mean vector estimation and stochastic convex optimization. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1265\u20131277. SIAM, 2017. [FS17] Vitaly Feldman and Thomas Steinke. Generalization for adaptively-chosen estimators via stable median. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 728\u2013757. PMLR, 07\u201310 Jul 2017.   \n$[\\mathrm{GHN}^{+}23]$ Arun Ganesh, Mahdi Haghifam, Milad Nasr, Sewoong Oh, Thomas Steinke, Om Thakkar, Abhradeep Thakurta, and Lun Wang. Why is public pretraining necessary for private model training? In International Conference on Machine Learning, 2023.   \n[GLL22] Sivakanth Gopi, Yin Tat Lee, and Daogao Liu. Private convex optimization via exponential mechanism. In Conference on Learning Theory, pages 1948\u20131989. PMLR, 2022. [GRS18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.   \n[GTU23] Arun Ganesh, Abhradeep Thakurta, and Jalaj Upadhyay. Universality of langevin diffusion for private optimization, with applications to sampling from rashomon sets. In The Thirty Sixth Annual Conference on Learning Theory, pages 1730\u20131773. PMLR, 2023.   \n[KDRT21] Peter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension independent private erm with adagrad rates via publicly estimated subspaces. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 2717\u20132746. PMLR, 15\u201319 Aug 2021. [KST08] Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. Advances in neural information processing systems, 21, 2008. [KU20] Gautam Kamath and Jonathan Ullman. A primer on private statistics. arXiv preprint arXiv:2005.00010, 2020.   \n[LLHR23] Andrew Lowy, Zeman Li, Tianjian Huang, and Meisam Razaviyayn. Optimal differentially private learning with public data. arXiv preprint arXiv:2306.15056v1, 2023.   \n[LLHR24] Andrew Lowy, Zeman Li, Tianjian Huang, and Meisam Razaviyayn. Optimal differentially private model training with public data. arXiv preprint arXiv:2306.15056v2, 2024.   \n[LTLH22] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In International Conference on Learning Representations, 2022. [LW19] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation, 2019. [MT07] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 94\u2013103. IEEE, 2007.   \n[NMT $^{+}23$ ] Milad Nasr, Saeed Mahloujifar, Xinyu Tang, Prateek Mittal, and Amir Houmansadr. Effectively using public data in privacy preserving machine learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 25718\u201325732. PMLR, 23\u201329 Jul 2023.   \n$[\\mathrm{PAE^{+}}17]$ Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised knowledge transfer for deep learning from private training data. In International Conference on Learning Representations, 2017.   \n[PHYS24] Francesco Pinto, Yaxi Hu, Fanny Yang, and Amartya Sanyal. PILLAR: How to make semi-private learning more effective. In 2nd IEEE Conference on Secure and Trustworthy Machine Learning, 2024.   \n$[\\mathrm{PSM^{+}}18]$ ] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and \u00dalfar Erlingsson. Scalable private learning with PATE. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n$[\\mathbf{PV}\\mathbf{X}^{+}23]$ Natalia Ponomareva, Sergei Vassilvitskii, Zheng Xu, Brendan McMahan, Alexey Kurakin, and Chiyaun Zhang. How to dp-fy ml: A practical tutorial to machine learning with differential privacy. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201923, page 5823\u20135824, New York, NY, USA, 2023. Association for Computing Machinery. [RS12] Alexander Rakhlin and Karthik Sridharan. Statistical learning theory and sequential prediction. Lecture Notes in University of Pennsyvania, 44, 2012.   \n$[{\\bf S C Z}^{+}20]$ Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun. FedED: Federated learning via ensemble distillation for medical relation extraction. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2118\u20132128, Online, November 2020. Association for Computational Linguistics. [Sel23] Mark Sellke. On size-independent sample complexity of relu networks. arXiv preprint arXiv:2306.01992, 2023.   \n[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[SST10] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth loss. arXiv preprint arXiv:1009.3896, 2010. ", "page_idx": 13}, {"type": "text", "text": "[SSTT21] Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimensionality in unconstrained private glms. In International Conference on Artificial Intelligence and Statistics, pages 2638\u20132646. PMLR, 2021. [SU15] Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. Journal of Privacy and Confidentiality, 7, 01 2015. [VC71] N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264, 1971. [Ver18] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n$[\\mathbf{Y}\\mathbf{N}\\mathbf{B}^{+}22\\mathbf{a}]$ Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially private fine-tuning of language models. In International Conference on Learning Representations, 2022.   \n$[\\mathbf{Y}\\mathbf{N}\\mathbf{B}^{+}22\\mathbf{b}]$ ] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially private fine-tuning of language models. In International Conference on Learning Representations, 2022. [YZCL21] Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. In International Conference on Learning Representations, 2021. [ZWB21] Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd with gradient subspace identification. In International Conference on Learning Representations, 2021. ", "page_idx": 13}, {"type": "text", "text": "A Additional Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 8. [RS12, Theorem 12.7] For $\\mathcal{H}\\subseteq[-R,R]^{\\chi}$ , $m\\in\\mathbb{N}$ , $p\\geq1$ , $0<\\alpha\\le R_{*}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{N}_{2}(\\mathcal{H},\\alpha,m)\\leq\\left(\\frac{2R}{\\alpha}\\right)^{C\\!f\\!a t_{c\\alpha}(\\mathcal{H})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Further, for any $\\tau\\in(0,1)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\left(\\ensuremath{N_{\\infty}}(\\mathcal{H},\\alpha,m)\\right)\\le C^{\\prime}f a t(\\mathcal{H},c^{\\prime}\\tau\\alpha)\\log\\left(\\frac{R m}{f a t(\\mathcal{H},c^{\\prime}\\tau\\alpha)\\alpha}\\right)l o g^{\\tau}\\left(\\frac{m}{f a t(\\mathcal{H},c^{\\prime}\\tau\\alpha)}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c,c^{\\prime},C$ and $C^{\\prime}$ are absolute constants. ", "page_idx": 14}, {"type": "text", "text": "Theorem 9. [SST10, Lemma $A.3J$ For any hypothesis class $\\mathcal{H}$ , any sample size m and any $\\alpha>$ $\\Re_{m}(\\mathcal{H})$ , we have that, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf\\!a t_{\\alpha}(\\mathcal{H})\\leq\\frac{4m\\Re_{m}(\\mathcal{H})^{2}}{\\alpha^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem 10. [GRS18, Theorem $I J$ Let $M\\,\\in\\,\\mathbb{N}$ and $\\left(R_{j}\\right)_{j=1}^{M}$ be a sequence of scalars. The Rademacher complexity of the class of depth $M$ neural networks with 1-Lipschitz, positivehomogeneous activation function, $\\mathcal{H}$ , with weights $\\|W_{j}\\|_{F}\\le R_{j}$ is bounded as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Re_{m}(\\mathcal{H})\\leq\\frac{\\|\\mathcal{X}\\|\\,(\\sqrt{2\\log{(2)}\\,M}+1)\\prod_{j=1}^{M}R_{j}}{\\sqrt{m}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. $I D S S^{+}I5$ , Implied by Lemmas 5 and 14] Let $f:\\left\\{\\pm1\\right\\}^{n}\\mapsto\\mathbb{R}$ and define $g:[-1,1]\\to\\mathbb{R}$ as $g(p)\\,=\\,_{S\\sim\\mathcal{D}_{p}^{n}}[f(S)]$ , where $\\mathcal{D}_{p}$ is as defined in Appendix $B$ . Then for $a,b\\,\\in\\,\\mathbb{R},\\;b\\,>\\,a,$ , and $\\mu\\sim\\mathsf{U n i f}([a,b])$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu,S}{\\mathbb{E}}\\left[f(S)\\cdot\\underset{x\\in S}{\\sum}(x-\\mu)\\right]\\right]=\\underset{\\mu}{\\mathbb{E}}\\left[g^{\\prime}(\\mu)(1-\\mu^{2})\\right]}\\\\ &{=1-\\underset{\\mu}{\\mathbb{E}}\\left[\\mu^{2}\\right]+(g(b)-b)(1-b^{2})\\frac{1}{|b-a|}-(g(a)-a)(1-a^{2})\\frac{1}{|b-a|}+2\\underset{\\mu}{\\mathbb{E}}\\left[(g(\\mu)-\\mu)\\mu\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3. [FS17, Lemma A.1] Fix \u00b5, \u03f5, \u03b4, \u2206\u2208R. Let $X$ and $Y$ be random variables supported on $[\\mu-\\Delta,\\mu+\\Delta]$ . Suppose that $X$ and $Y$ are $(\\epsilon,\\delta)$ -indistinguishable, that is for any $\\mathcal{E}\\subseteq\\mathbb{R},$ , $e^{-\\epsilon}(\\mathbb{P}\\left[X\\in\\mathcal{E}\\right]-\\bar{\\delta})\\leq\\mathbb{P}\\left[Y\\in\\mathcal{E}\\right]\\leq e^{\\epsilon}\\mathbb{P}\\left[X\\in\\mathcal{E}\\right]+\\delta$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}\\left[X\\right]-\\mathbb{E}\\left[Y\\right]|\\leq(e^{\\epsilon}-1)\\mathbb{E}\\left[|X-\\mu|\\right]+2\\delta\\Delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Missing proofs from Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Lower Bounds for Mean Estimation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As stated previously, our SCO result follows primarily from new lower bound for PA-DP mean estimation. Here, we consider the setting where $\\mathcal{D}$ is supported on the $\\ell_{2}$ ball of radius $R>0$ . We define the mean of $\\mathcal{D}$ as $\\mu(D)$ . Our lower bound for PA-DP mean estimation follows much the same form as our SCO bound. ", "page_idx": 14}, {"type": "text", "text": "Theorem 11. Let $\\begin{array}{r}{\\delta\\leq\\frac{1}{16n d}}\\end{array}$ , $\\epsilon\\leq1$ . For any $(\\epsilon,\\delta)$ -PA-DP algorithm, there exists a distribution $\\mathcal{D}$ such that E $\\begin{array}{r}{\\colon[\\lVert A(S_{p u b},S_{p r i v}^{\\cdots\\cdots}-\\mu(\\mathcal{D})\\rVert]=\\Omega\\left(R\\cdot\\Psi(n_{p u b},n,\\epsilon,\\delta)\\right)}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "We present the full proof momentarily and provide a more detailed discussion on the challenges of establishing this lower bound in Appendix B.2. We highlight key ideas here. As with many other lower bounds in differential privacy, we leverage a construct known as fingerprinting codes [BUV14, $\\mathrm{DSS}^{+}15]$ . A key aspect of our analysis is showing that fingerprinting distributions can be used to recover the optimal non-private lower bound for mean estimation. This allows us to create a problem which is \u201chard\u201d both privately and non-privately. The analysis works by first showing that any sufficiently accurate algorithm must strongly correlate with the sampled datapoints. Next, we show upper bounds on how strongly the output of the algorithm correlates with the sampled dataset. The method for upper bounding this correlation varies depending on whether a given datapoint is considered public or private. Combining these upper and lower bounds on correlation yields the claimed result. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "To obtain the $\\sqrt{\\log\\left(1/\\delta\\right)}$ factor term in the lower bound, we use similar ideas to those in [SU15, CWZ21]. However, the introduction of public data leads to complications in prior methods. As such, we show that by analyzing the correlation of the coordinate wise clipping of the algorithms output, we are able to get bounds that appropriately scale with the accuracy. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 11 Before proceeding, we introduce the so-called fingerprinting distribution which will be the basis of our hard instance for mean estimation [BUV14, $\\bar{\\mathrm{DSS}}^{+}15]$ . Towards this end, for any vector $\\mu\\in[-1,1]^{d}$ we define $\\mathcal{D}_{\\mu}$ as the product distribution where, for any $j\\in[d]$ , a sample has its $j^{;}$ th coordinate as 1 with probability $(1+\\mu_{j})/2$ and as $-1$ with probability $(1-\\mu_{j})/2$ . As shorthand, we denote $\\scriptstyle{\\frac{R}{\\sqrt{d}}}D_{\\mu}$ as the distribution which samples a vector from $\\mathcal{D}_{\\mu}$ and then scales it by\u221aRd. For notational convenience, for a set $\\mathcal{E}$ , we will also use Unif $(\\mathcal{E})$ to denote the uniform distribution over elements of the set. ", "page_idx": 15}, {"type": "text", "text": "The theorem follows from two theorems which have different restrictions on the problem parameters. In addition to the following two theorems, Theorem 11 incorporates the classic $\\sqrt{\\frac{R}{\\sqrt{n}}}$ statistical lower bound that holds even non-privately. The first theorem we present holds for a larger range of parameters but does not achieve the dependence on $\\log\\left(1/\\delta\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 12. Let $\\epsilon\\mathrm{~>~0~}$ , $\\delta\\ \\leq\\ {\\frac{1}{16n}}$ and $\\boldsymbol{\\mathcal{A}}$ be an $(\\epsilon,\\delta)$ -PA-DP algorithm. For any setting of $\\begin{array}{r}{\\operatorname*{min}\\left\\{\\frac{\\sqrt{d}}{n_{p r i\\nu}\\epsilon},\\frac{1}{\\sqrt{n_{p u b}}}\\right\\}\\leq M\\leq1}\\end{array}$ , if \u00b5 \u223cUnif([\u2212M, M]d) and $\\begin{array}{r}{(S_{p u b},S_{p r i\\nu})\\sim\\frac{R}{\\sqrt{d}}D_{\\mu}^{n}}\\end{array}$ it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underset{A,S,\\mu}{\\mathbb{E}}\\left[\\|A(S_{p u b},S_{p r i v})-\\mu(D)\\|\\right]=\\Omega\\left(R\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{p u b}}},\\frac{\\sqrt{d}}{n(e^{\\epsilon}-1)}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In application to Theorem 11, we use $(e^{\\epsilon}-1)\\leq2\\epsilon$ whenever $\\epsilon\\leq1$ . The second theorem requires $d\\geq n\\epsilon$ but has the benefit of scaling with $\\log\\left(1/\\delta\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 13. Let $\\textstyle\\delta\\leq{\\frac{1}{3d n}}$ , $\\epsilon\\leq1$ , $d\\geq120^{2}n\\epsilon$ , and $\\begin{array}{r}{n_{p u b}\\,\\leq\\,\\frac{n\\epsilon}{120^{2}\\log\\left(1/[\\sqrt{n d}\\delta]\\right)}}\\end{array}$ , and $\\boldsymbol{\\mathcal{A}}$ an $(\\epsilon,\\delta)$ -PADP algorithm. Then there exists $M>0$ such for $\\mu\\sim\\mathsf{U n i f}([-M,M]^{d})$ and $\\begin{array}{r}{(S_{p r i\\nu},S_{p u b})\\sim\\frac{R}{\\sqrt{d}}D_{\\mu}^{n}\\,i t}\\end{array}$ holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underset{A,S,\\mu}{\\mathbb{E}}\\left[\\lVert A(S_{p u b},S_{p r i v})-\\mu(\\mathcal{D})\\rVert\\right]=\\Omega\\left(R\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{p u b}}},\\frac{\\sqrt{d\\log\\left(1/\\delta\\right)}}{n\\epsilon}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A crucial part of the analysis is leveraging the so called fingerprinting lemma, which roughly states that any accurate algorithm given a dataset sampled from $\\mathcal{D}_{\\mu}$ must strongly correlate with vectors in the dataset. Particularly pertinent to our analysis is achieving such a correlation even when the components of the mean $\\mu$ are much smaller than 1. Towards this end, we leverage the robust distribution framework of $[\\mathrm{DSS}^{+}15]$ to achieve the following version of the fingerprinting lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4 (Fingerprinting Lemma). Let $M\\in[0,1]$ and $\\mu$ be sampled uniformly from $[-M,M]^{d}$ . Let $\\boldsymbol{\\mathcal{A}}$ satisfy $\\underset{S\\sim\\mathcal{D}_{\\mu}^{n}}{\\mathbb{E}}\\left[\\|A(S)-\\mu\\|\\right]\\leq\\alpha$ (for any $\\mu\\in[-1,1]^{d},$ ). Then one has ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underset{A,S,\\mu}{\\mathbb{E}}\\left[\\sum_{i=1}^{n}\\left\\langle A(S),x_{i}-\\mu\\right\\rangle\\right]\\geq\\frac{2d}{3}-\\frac{\\alpha\\sqrt{d}}{M}-2M\\sqrt{d}\\alpha.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In the following we treat $\\boldsymbol{\\mathcal{A}}$ as a deterministic function and bound $\\underset{S,\\mu}{\\mathbb{E}}\\left[\\sum_{i=1}^{n}\\left\\langle A(S),x_{i}-\\mu\\right\\rangle\\right]$ . This is sufficient to bound $\\begin{array}{r}{{\\underset{A,S,\\mu}{\\mathbb{E}}}\\left[\\sum_{i=1}^{n}\\left\\langle A(S),x_{i}-\\mu\\right\\rangle\\right]}\\end{array}$ for randomized $\\boldsymbol{\\mathcal{A}}$ , since the analysis holds for ", "page_idx": 15}, {"type": "text", "text": "any function (i.e. the distribution does not depend on $\\mathcal{A}$ ). Further, we start with the one dimensional case such that $\\mu\\,\\in\\,\\mathbb R$ . Define $g(\\mu)\\,=\\,_{S\\sim\\mathcal{D}_{\\mu}^{n}}[A(S)]$ . We start by applying results developed in $[{\\mathrm{DSS}}^{+}15]$ ", "page_idx": 16}, {"type": "text", "text": ", ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\mathbb{E}}{S,\\mu}\\displaystyle\\left[A(S)\\sum_{i=1}^{n}(x_{i}-\\mu)\\right]\\frac{(i)}{=\\mu}\\,\\mathbb{E}\\left[g^{\\prime}(\\mu)(1-\\mu^{2})\\right]}&{}\\\\ {\\overset{(i i)}{\\geq}1-\\mathbb{E}\\left[\\mu^{2}\\right]+2\\mathbb{E}\\left[(g(\\mu)-\\mu)\\mu\\right]-\\frac{|g(-M)+M|+|g(M)-M|}{2M}}&{}\\\\ {\\geq2/3+2\\mathbb{E}\\left[(g(\\mu)-\\mu)\\mu\\right]-\\frac{|g(-M)+M|+|g(M)-M|}{2M}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Above, $(i)$ comes from $[{\\mathrm{DSS}}^{+}15$ , Lemma 5] and $(i i)$ comes from $[{\\mathrm{DSS}}^{+}15$ , Lemma 14], which we have collectively restated in Lemma 2. We now have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}}{s,\\mu}\\left[A(S)\\displaystyle\\sum_{i=1}^{n}(x_{i}-\\mu)\\right]\\geq2/3+\\frac{|g(-M)+M|+|g(M)-M|}{2M}+2\\mathbb{E}\\left[(g(\\mu)-\\mu)\\mu\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq2/3+\\frac{|g(-M)-M|+|g(M)-M|}{2M}-2\\mathbb{E}\\left[|g(\\mu)-\\mu|\\cdot|\\mu|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq2/3-\\frac{|\\mathbb{E}_{S\\sim\\mathcal{D}_{-M}}[A(S)]+M|+|\\mathbb{E}_{S\\sim\\mathcal{D}_{M}}[A(S)]-M|}{2M}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\,2M\\mathbb{E}\\left[\\left|_{S\\sim\\mathcal{D}_{\\mu}}[A(S)]-\\mu\\right|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Above we use the fact that $|\\mu|\\le M$ and the definition of $g$ . ", "page_idx": 16}, {"type": "text", "text": "We can now extend the above analysis to higher dimensions. For $\\mu\\in\\mathbb{R}^{d}$ , the above holds for each $\\mu_{j},j\\in[d]$ . For convenience define $\\bar{M}=(\\check{M},\\cdot\\cdot\\cdot,M)\\in\\mathbb{R}^{d}$ . Summing over $d$ dimensions we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}}{S_{\\mu}}\\left[\\left\\langle A(S),\\displaystyle\\sum_{i=1}^{n}(x_{i}-\\mu)\\right\\rangle\\right]}\\\\ &{\\geq\\frac{2d}{3}-\\frac{1}{2M}\\Big\\|_{S\\sim\\mathbb{P}_{-\\infty}}\\left[A(S)\\right]+\\bar{M}\\Big\\|_{1}-\\frac{1}{2M}\\Big\\|_{S\\sim\\mathbb{P}_{\\mathcal{M}}}\\left[A(S)\\right]-\\bar{M}\\Big\\|_{1}-2M\\mathbb{E}\\left[\\Big\\|_{S\\sim\\mathbb{P}_{\\mu}}\\left[A(S)\\right]-\\mu\\right.}\\\\ &{\\geq\\frac{2d}{3}-\\frac{1}{2M}\\kappa_{\\mathcal{S}\\sim\\bar{P}_{-\\infty}}\\left[\\|A(S)+\\bar{M}\\|_{1}\\right]-\\frac{1}{2M}\\kappa_{\\mathcal{S}\\sim\\bar{P}_{\\mathcal{M}}}\\left[\\|A(S)-\\bar{M}\\|_{1}\\right]-2M\\mathbb{E}_{\\mathcal{M}_{\\mu}}\\left[\\|A(S)-\\mu\\|_{1}\\right]}\\\\ &{\\geq\\frac{2d}{3}-\\frac{\\sqrt{d}}{2M}\\kappa_{\\mathcal{S}\\sim\\bar{P}_{-\\infty}}\\left[\\|A(S)+\\bar{M}\\|_{2}\\right]-\\frac{\\sqrt{d}}{2M}\\kappa_{\\mathcal{S}\\sim\\bar{P}_{\\mathcal{M}}}\\left[\\|A(S)-\\bar{M}\\|_{2}\\right]-2\\sqrt{d}M\\mathbb{E}_{\\mathcal{S},\\mu}^{\\mathbb{E}}\\left[\\|A(S)-\\mu\\|_{2}\\right]}\\\\ &{\\geq\\frac{2d}{3}-\\frac{\\alpha\\sqrt{d}}{M}-2M\\sqrt{d}\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This proves the claim. ", "page_idx": 16}, {"type": "text", "text": "We now turn towards proving Theorems 12 and 13. We start with the simpler proof of Theorem 12.   \nProof of Theorem 12.\u221a For our proof we will use a dataset of vectors in $\\{\\pm1\\}^{d}$ , and as such the $\\ell_{2}$ bound on the data is d. The final result will follow from rescaling by\u221aRd.   \nLet $S=\\{x_{1},x_{2},\\ldots x_{n}\\}=(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\sim{\\mathcal{D}}_{\\mu}^{n}$ be the concatenation of the public and private datasets.   \nWe also define $\\alpha=\\mathbb{E}\\left[\\|A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu\\|\\right]$ for notational convenience. ", "page_idx": 16}, {"type": "text", "text": "Define the following statistics, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{i}=\\left\\langle A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu,x_{i}-\\mu\\right\\rangle}\\\\ &{Z_{i}^{\\prime}=\\left\\langle A(S_{\\mathrm{pub}},S_{\\sim i})-\\mu,x_{i}-\\mu\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $S_{\\sim i}$ is the dataset formed by replacing $i$ -th data point of $S_{\\mathrm{priv}}$ with $x_{i}^{\\prime}\\sim\\mathcal{D}_{\\mu}$ . We have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{A},S,\\mu}\\Big[\\sum_{i=1}^{n}Z_{i}\\Big]=\\mathbb{E}\\Big[\\sum_{i=1}^{n_{\\mathrm{pub}}}Z_{i}\\Big]+\\mathbb{E}\\Big[\\sum_{i=n_{\\mathrm{pub}}+1}^{n}Z_{i}\\Big].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The lower bound proceeds by providing upper and lower bounds on the above sum. We first have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\langle A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu,\\sum_{i=1}^{n_{\\mathrm{pab}}}(x_{i}-\\mu)\\right\\rangle\\right]\\leq\\sqrt{\\mathbb{E}[\\|A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu\\|^{2}]\\mathbb{E}\\left\\|\\sum_{i=1}^{n_{\\mathrm{pab}}}(x_{i}-\\mu)\\right\\|^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality used Cauchy-Schwartz. ", "page_idx": 17}, {"type": "text", "text": "For the second term in Equation (2), we utilize differential privacy. Specifically, [FS17, Lemma A.1], restated in Lemma 3, gives that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\displaystyle\\sum_{i=n_{\\mathrm{pub}}+1}^{n}Z_{i}\\Big]\\leq\\displaystyle\\sum_{i=n_{\\mathrm{pub}}+1}^{n}\\left(\\mathbb{E}[Z_{i}^{\\prime}]+2(e^{\\epsilon}-1)\\sqrt{\\mathrm{Var}(Z_{i}^{\\prime})}+8\\delta d\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq4n_{\\mathrm{priv}}(e^{\\epsilon}-1)\\alpha+8n_{\\mathrm{priv}}\\delta d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above we use that $\\mathrm{Var}(Z_{i}^{\\prime})\\leq4\\alpha^{2}$ since $\\|x_{i}-\\mu\\|_{\\infty}\\leq4$ . Plugging the above two in Equation (2) yields, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{i=1}^{n}Z_{i}\\Big]\\leq(4n_{\\mathrm{priv}}(e^{\\epsilon}-1)\\alpha+8n_{\\mathrm{priv}}\\delta d)+\\alpha\\sqrt{d n_{\\mathrm{pub}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now use the fingerprinting lemma, Lemma 4, to lower bound the correlation. In this regard, note $\\begin{array}{r}{\\mathbb{\\bar{E}}_{\\boldsymbol{s},\\mu}\\left[\\langle\\mu,\\sum_{i=1}^{n}(\\boldsymbol{x}_{i}-\\bar{\\mu})\\rangle\\right]=0}\\end{array}$ . Thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{i=1}^{n}Z_{i}\\Big]=\\mathbb{E}\\left[\\left\\langle A(S_{\\mathrm{pub}},S_{\\mathrm{priv}}),\\sum_{i=1}^{n}x_{i}-\\mu\\right\\rangle\\right]\\geq\\frac{2d}{3}-\\frac{\\alpha\\sqrt{d}}{2M}-2M\\sqrt{d}\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging the obtained upper bound on the left hand side gives us, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4n_{\\mathfrak{p w}}(e^{\\epsilon}-1)\\alpha+8n_{\\mathfrak{p w}}\\delta d+\\alpha\\sqrt{d n_{\\mathfrak{p w}}}\\geq\\frac{2d}{3}-\\frac{\\alpha\\sqrt{d}}{M}-2M\\sqrt{d}\\alpha}\\\\ &{\\implies4n_{\\mathfrak{p w}}(e^{\\epsilon}-1)\\alpha+\\alpha\\sqrt{d n_{\\mathfrak{p w}}}\\geq\\frac{d}{6}-\\frac{\\alpha\\sqrt{d}}{M}-2M\\sqrt{d}\\alpha}\\\\ &{\\implies\\alpha\\left(4n_{\\mathfrak{p w}}(e^{\\epsilon}-1)+\\sqrt{d n_{\\mathfrak{p w}}}+\\frac{\\sqrt{d}}{M}+2M\\sqrt{d}\\right)\\geq\\frac{d}{6}}\\\\ &{\\implies\\alpha\\geq\\frac{1}{24}\\operatorname*{min}\\left\\{\\frac{d}{n_{\\mathfrak{p w}}(e^{\\epsilon}-1)},\\frac{\\sqrt{d}}{\\sqrt{n_{\\mathfrak{p w}}}},M\\sqrt{d},\\frac{\\sqrt{d}}{M}\\right\\}}\\\\ &{\\implies\\alpha\\geq\\frac{1}{24}\\operatorname*{min}\\left\\{\\frac{d}{n_{\\mathfrak{p w}}(e^{\\epsilon}-1)},\\frac{\\sqrt{d}}{\\sqrt{n_{\\mathfrak{p w}}}},M\\sqrt{d}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above the first implication uses the assumption that \u03b4 \u2264 16n1priv . The last implication uses the fact that $\\begin{array}{r}{M\\sqrt{d}\\leq\\frac{\\sqrt{d}}{M}}\\end{array}$ since $M\\leq1$ . Rescaling by a $\\textstyle{\\frac{R}{\\sqrt{d}}}$ factor yields the bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{A,S,\\mu}{\\mathbb{E}}\\left[\\|A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu\\|\\right]\\geq\\frac{R}{24}\\operatorname*{min}\\left\\{\\frac{\\sqrt{d}}{n_{\\mathrm{priv}}(e^{\\epsilon}-1)},\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},M\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Observe that any setting of $\\begin{array}{r}{M\\,\\geq\\,\\operatorname*{min}\\left\\lbrace\\frac{\\sqrt{d}}{n_{\\mathrm{priv}}\\epsilon},\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}}\\right\\rbrace}\\end{array}$ realizes the bound claimed in the theorem statement. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "We now turn towards achieving a dependence on $\\delta$ to prove Theorem 13. To do this, we leverage the the idea of filling a dataset with copies of each fingerprinting code seen in previous work [SU15, CWZ21]. However, in our case the introduction of public data makes this argument more delicate and leads to modified techniques for upper bounding the correlation statistics. See our discussion in Section B.2 for more details on why this is necessary. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 13. Let $\\alpha=\\mathbb{E}\\,[\\|A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\mu\\|],$ $\\begin{array}{r}{\\alpha^{*}=\\frac{1}{125}\\operatorname*{min}\\left\\{\\frac{d\\sqrt{\\log(1/\\delta)}}{n\\epsilon},\\sqrt{\\frac{d}{n_{\\mathrm{pub}}}}\\right\\}}\\end{array}$ and assume by way of contradiction that $\\alpha<\\alpha^{*}$ . Let $\\begin{array}{r}{k\\,=\\,\\frac{1}{3\\epsilon}\\log\\left(1/[\\sqrt{d n}\\delta]\\right)}\\end{array}$ . Let $\\textstyle m\\,=\\,{\\frac{n}{k}}$ . We set $\\begin{array}{r}{M=\\frac{4}{\\sqrt{d}}(\\alpha^{*}+\\sqrt{\\frac{d}{m}})}\\end{array}$ . Let $\\mu\\sim\\mathsf{U n i f}([-M,M]^{d})$ , $S_{z}=\\{z_{1},...,z_{m}\\}\\sim\\mathcal{D}_{\\mu}$ . Sample $S_{\\mathrm{priv}},S_{\\mathrm{pub}}\\sim$ Unif $\\left(\\{z_{1},...,z_{m}\\}\\right)$ and denote the combined dataset as $S=\\{x_{1},\\ldots,x_{n}\\}=(S_{\\mathrm{pub}},S_{\\mathrm{priv}})$ . Note that as in the proof of Theorem 12, we are starting by showing a lower bound for the case where the data is drawn from $\\mathcal{D}_{\\mu}$ instead o f\u221aRdD\u00b5, and will rescale at the end of the proof. ", "page_idx": 18}, {"type": "text", "text": "To prove our lower bound, we will provide upper and lower bounds on correlation statistics w.r.t. the intermediate dataset $S_{z}$ . We will also introduce a clipping procedure which helps better control the upper bound on correlation. In this regard, for each $j\\in[m]$ define $Z_{j}=\\langle\\lfloor A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\rfloor_{M},z_{j}-\\mu\\rangle$ , where $\\lfloor v\\rfloor_{M}$ denotes the operation of clipping every element of $v$ to $[-M,M]$ . In the following, we will provide upper and lower bounds on $\\mathbb{E}\\left[\\sum_{j=1}^{m}Z_{i}\\right]$ and use this to show that $\\alpha\\leq\\alpha^{*}$ implies a contradiction. ", "page_idx": 18}, {"type": "text", "text": "Lower Bound on Correlation We now want to lower bound $\\mathbb{E}\\left[\\sum_{j=1}^{m}Z_{j}\\right]$ . Towards this end, we can apply fingerprinting lemma, Lemma 4, to the algorithm which outputs the clipping. For $\\widehat{\\alpha}>0$ , if $\\underset{A,S}{\\mathbb{E}}\\left[\\|\\bar{[}\\bar{A}(S_{\\mathrm{pub}},\\bar{S}_{\\mathrm{priv}})\\|_{M}^{}-\\mu\\|\\right]\\leq\\widehat{\\alpha}$ , then this yields, ", "page_idx": 18}, {"type": "equation", "text": "$$\n_{A,S,\\mu}\\left[\\sum_{j=1}^{m}Z_{i}\\right]\\geq\\frac{2d}{3}-\\frac{\\widehat{\\alpha}\\sqrt{d}}{M}-2M\\sqrt{d}\\widehat{\\alpha}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now observe that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\left|\\left\\langle A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\right\\rvert\\boldsymbol{M}-\\boldsymbol{\\mu}\\right\\rVert\\right]\\leq\\mathbb{E}\\left[\\left\\lVert A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\boldsymbol{\\mu}\\right\\rVert\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\lVert A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})-\\frac{1}{m}\\sum_{z\\in S_{z}}z\\right\\rVert\\right]+\\mathbb{E}\\left[\\left\\lVert\\frac{1}{m}\\sum_{z\\in S_{z}}z-\\boldsymbol{\\mu}\\right\\rVert\\right]}\\\\ {\\leq\\alpha^{*}+\\sqrt{\\frac{d}{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the last step we use the assumed contra\u221adiction that $\\alpha\\leq\\alpha^{*}$ . Thus it suffices to set $\\begin{array}{r}{\\widehat{\\alpha}=\\alpha^{*}+\\sqrt{\\frac{d}{m}}}\\end{array}$ . Now by the setting $\\begin{array}{r}{M=\\frac{4}{\\sqrt{d}}\\widehat{\\alpha}}\\end{array}$ and $\\widehat{\\alpha}\\leq\\frac{\\sqrt{d}}{12}$ , Eqn. (3) implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{A,S,\\mu}{\\mathbb{E}}\\left[\\sum_{j=1}^{m}Z_{i}\\right]\\geq\\frac{2d}{3}-\\frac{d}{4}-8\\widehat{\\alpha}^{2}\\geq\\frac{d}{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounding the Number of Copies in the Dataset We now turn towards the more involved process of upper bounding $\\mathbb{E}_{\\boldsymbol{S},\\mu}\\left[\\sum_{i=1}^{n}\\bar{Z}_{i}\\right]$ . To do this however, it will be first helpful to show that no datapoint in $S_{z}$ is copied into $S$ too many times. ", "page_idx": 18}, {"type": "text", "text": "The first step is showing that no point is copied too many times into $S$ . For $j\\,\\in\\,[m]$ , let $\\mathcal{Z}_{j}\\;=\\;$ $\\{i\\in[n]:x_{i}\\overline{{=}}z_{j}\\}$ . Observe ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}\\big[\\exists j\\in[m]:|{\\mathcal Z}_{j}|\\ge(\\tau+1)k\\big]\\le\\displaystyle\\sum_{j=1}^{m}\\mathbb{P}\\big[|{\\mathcal Z}_{j}|\\ge(\\tau+1)k\\big]}}\\\\ &{\\le m\\exp\\left(-\\frac{3\\tau^{2}n}{4m(1-1/n)}\\right)}\\\\ &{\\le m\\exp\\left(-\\frac{3\\tau^{2}\\log{(1/\\delta)}}{8\\epsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second inequality follows from Bernstein\u2019s inequality for the sum of $n$ Bernoulli random variables with mean $1/m$ and the fact that $\\begin{array}{r}{\\mathbb{E}\\left[|Z_{j}|\\right]=\\frac{n}{m}=k}\\end{array}$ . Set $\\begin{array}{r}{\\tau=\\sqrt{\\frac{8\\epsilon\\log(d m)}{3\\log(1/\\delta)}}}\\end{array}$ and note since $\\epsilon\\leq1$ and $\\log\\left(d m\\right)\\leq\\log\\left(d n\\right)\\leq\\log\\left(1/\\delta\\right)$ (since $\\begin{array}{r}{\\delta\\leq\\frac{1}{d n}.}\\end{array}$ ), we have that $\\tau\\leq2$ . Thus, denoting $E$ as the event where no point in $S_{z}$ is copied into $S$ more than $3k$ times, we establish ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[E^{c}\\right]=\\mathbb{P}\\left[\\exists j\\in[m]:|\\mathcal{Z}_{j}|\\ge3k\\right]\\le\\frac{1}{d}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Upper Bound on Correlation Under our model, we assume that $\\boldsymbol{\\mathcal{A}}$ must treat all data in $S_{\\mathrm{priv}}$ as private. We will in fact only need to use the privacy property for a subset of samples in $S_{\\mathrm{priv}}$ to prove the correlation upper bound. Let $Z_{\\mathrm{priv}}\\subseteq[m]$ denote the set of indices s.t. $j\\in\\mathcal{T}_{\\mathrm{priv}}$ if every copy of $z_{j}$ sampled into the overall dataset is in the private dataset $S_{\\mathrm{priv}}$ ; that is ${\\mathcal{Z}}_{\\mathrm{priv}}=\\{j:(\\forall x\\in S_{\\mathrm{pub}})\\;{\\dot{x}}\\neq z_{j}\\}$ . Let ${\\mathcal{Z}}_{\\mathrm{pub}}=[m]\\setminus{\\mathcal{Z}}_{\\mathrm{priv}}$ . Observe that $\\mathcal{T}_{\\mathrm{priv}}$ may contain indices for points in $S_{z}$ which are never sampled into $S$ . We will see this does not affect our analysis. ", "page_idx": 19}, {"type": "text", "text": "We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{j=1}^{m}Z_{i}\\right]=\\mathbb{E}\\left[\\sum_{j\\in\\mathbb{Z}_{\\mathrm{pub}}}\\langle|\\boldsymbol{A}(S_{\\mathtt{p u b}},S_{\\mathtt{p r i v}})|_{\\cal M},z_{j}-\\mu\\rangle+\\sum_{j\\in\\mathbb{Z}_{\\mathtt{p u b}}}\\langle|\\boldsymbol{A}(S_{\\mathtt{p u b}},S_{\\mathtt{p r i v}})|_{\\cal M},z_{j}-\\mu\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The first term on the RHS can be bounded using the privacy property of $\\boldsymbol{\\mathcal{A}}$ . For any fixed $j\\in\\mathcal{Z}_{\\mathrm{priv}}$ , let $S_{\\mathrm{priv}}^{\\prime}$ denote the dataset which replaces every instance of $z_{j}$ in $S_{\\mathrm{priv}}$ with a copied fresh sample from $\\dot{\\mathcal{D}_{\\mu}}$ . By the above analysis, conditional on the event $E$ , at most $3k$ such points need to be replaced conditional on the event $E$ . Since $A(S_{\\mathrm{pub}},S_{\\mathrm{priv}}^{\\prime})$ is independent of $z_{j}$ , by the Chernoff Hoeffding bound, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\big\\langle\\lfloor A(S_{\\mathrm{pub}},S_{\\mathrm{priv}}^{\\prime})\\big\\rfloor_{M},z_{j}-\\mu\\big\\rangle\\ge\\tau\\mid E\\right]\\le\\exp\\left(-\\frac{\\tau^{2}}{8d M^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\boldsymbol{\\mathcal{A}}$ satisfies $k$ -group privacy with parameters $\\widehat{\\epsilon}\\le3k\\epsilon$ and $\\widehat{\\delta}=e^{3k\\epsilon}\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\langle\\lfloor A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\rfloor_{M},z_{j}-\\mu\\rangle\\ge\\tau\\mid E\\right]\\le\\exp\\left(\\widehat{\\epsilon}-\\frac{\\tau^{2}}{2d M^{2}}\\right)+\\widehat{\\delta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Setting $\\tau=M\\sqrt{d\\log\\left(1/\\delta\\right)}$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[Z_{j}\\mid E\\right]\\leq\\tau+2d\\mathbb{P}\\left[Z_{j}\\geq\\tau\\mid E\\right]}\\\\ &{\\qquad\\qquad\\leq M\\sqrt{d\\log\\left(1/\\delta\\right)}+2d e^{\\widehat{\\epsilon}}\\delta+\\widehat{\\delta}}\\\\ &{\\qquad\\qquad\\leq M\\sqrt{d\\log\\left(1/\\delta\\right)}+3d e^{3k\\epsilon}\\delta\\leq4M\\sqrt{d\\log\\left(1/\\delta\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last inequality comes from the setting of $\\begin{array}{r}{k\\,=\\,\\frac{1}{3\\epsilon}\\log\\left(\\frac{1}{\\sqrt{d n}\\delta}\\right)}\\end{array}$ and the fact that $\\textstyle M\\,\\geq\\,{\\frac{1}{\\sqrt{n}}}$ . Repeating this argument for each $j\\in\\mathcal{T}$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{j\\in\\mathbb{Z}_{\\mathtt{p r i v}}}Z_{j}\\Big]\\leq\\mathbb{E}\\Big[\\sum_{j\\in\\mathbb{Z}_{\\mathtt{p r i v}}}Z_{j}\\Bigm|E\\Big]\\mathbb{P}\\left[E\\right]+m M d\\mathbb{P}\\left[E^{c}\\right]\\leq5m M\\sqrt{d\\log\\left(1/\\delta\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last inequality uses the bound established on each $\\mathbb{E}\\left[Z_{j}\\mid E\\right]$ , $j\\in\\mathcal{Z}_{\\mathrm{priv}}$ , above and the bound on $\\mathbb{P}\\left[E^{c}\\right]$ from Eqn. (4). ", "page_idx": 20}, {"type": "text", "text": "To bound the correlation over the remaining vectors, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{j\\in\\mathbb{Z}_{\\mathrm{pub}}}Z_{j}\\right]\\leq\\sqrt{\\mathbb{E}[\\|\\left\\lfloor A(S_{\\mathrm{pub}},S_{\\mathrm{priv}})\\rfloor_{M}\\|^{2}\\right\\rfloor\\mathbb{E}\\Big[\\Big\\|\\sum_{j\\in\\mathbb{Z}_{\\mathrm{pub}}}(z_{j}-\\mu)\\Big\\|^{2}\\Big]}\\leq2M d\\sqrt{n_{\\mathrm{pub}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Above we have used the fact that $|Z_{\\mathrm{pub}}|\\le n_{\\mathrm{pub}}$ because $i\\in\\mathcal{Z}_{\\mathrm{pub}}$ only if at least one copy of $z_{i}$ is sampled into $S_{\\mathrm{pub}}$ . Combining the above we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{j=1}^{m}Z_{i}\\right]\\leq5m M\\sqrt{d\\log\\left(1/\\delta\\right)}+5M d\\sqrt{n_{\\mathrm{pub}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Bounds: The previously derived lower bound in Eqn. (3) establishes that $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{j\\in{\\mathcal Z}_{\\mathrm{priv}}}Z_{j}+\\sum_{j\\in{\\mathcal Z}_{\\mathrm{pub}}}Z_{j}\\right]\\ \\stackrel{\\cdot}{\\geq}\\ \\frac{d}{3}}\\end{array}$ . Using the above derived upper bounds we have the following manipulations, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{M\\sqrt{d}\\left(m\\sqrt{\\log\\left(1/\\delta\\right)}+\\sqrt{d n_{\\mathrm{pub}}}\\right)\\geq\\frac{d}{15}\\,}\\\\ &{\\iff}&{(\\alpha^{*}+\\sqrt{d/m})\\left(m\\sqrt{\\log\\left(1/\\delta\\right)}+\\sqrt{d n_{\\mathrm{pub}}}\\right)\\geq\\frac{d}{60}\\,}\\\\ &{\\iff}&{m\\alpha\\sqrt{\\log\\left(1/\\delta\\right)}+\\alpha^{*}\\sqrt{d n_{\\mathrm{pub}}}\\geq\\frac{d}{60}-\\sqrt{d\\log\\left(1/\\delta\\right)m}-d\\sqrt{\\frac{n_{\\mathrm{pub}}}{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second line above uses that $\\begin{array}{r}{M=\\frac{4}{\\sqrt{d}}(\\alpha^{*}+\\sqrt{\\frac{d}{m}})}\\end{array}$ . Under the condition that $\\begin{array}{r l r}{n_{\\mathrm{pub}}\\,\\le\\,\\frac{m}{120^{2}}\\,\\equiv}\\end{array}$ $\\begin{array}{r}{n_{\\mathrm{pub}}\\,\\leq\\,\\frac{3n\\epsilon}{120^{2}\\log\\left(1/[\\sqrt{n d}\\delta]\\right)}}\\end{array}$ , which is satisfied under by assumption in the theorem statement, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nm\\alpha\\sqrt{\\log{\\left(1/\\delta\\right)}}+\\alpha^{*}\\sqrt{d n_{\\sf p u b}}\\geq\\frac{d}{120}-\\sqrt{d\\log{\\left(1/\\delta\\right)}m}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now applying the assumption $\\begin{array}{r}{d\\geq120^{2}n\\epsilon\\implies m\\leq\\frac{d}{120^{2}\\log(1/\\delta)}}\\end{array}$ we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\alpha^{*}\\sqrt{\\log\\left(1/\\delta\\right)}+\\alpha^{*}\\sqrt{d n_{\\mathrm{pub}}}\\geq\\frac{d}{120}}\\\\ {\\alpha^{*}\\geq\\frac{1}{120}\\operatorname*{min}\\left\\{\\frac{d\\sqrt{\\log\\left(1/\\delta\\right)}}{n\\epsilon},\\sqrt{\\frac{d}{n_{\\mathrm{pub}}}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This establishes a contradiction, and thus \u03b1 \u2265\u03b1\u2217= 1125 min d long\u03f5(1/\u03b4), . Rescaling by $\\textstyle{\\frac{R}{\\sqrt{d}}}$ then yields the claimed result. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.2 Discussion of Lower Bound Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We here provide more details on why the particular lower bound techniques we present were chosen. Our aim for the following discussion is to elucidate some of the subtleties of leveraging the fingerprinting code framework when public data is present, with the hope that it will aid future work on the characterization of PA-DP problems. ", "page_idx": 20}, {"type": "text", "text": "One crucial challenge in developing the mean estimation lower bounds in Appendix B.1 is ensuring that the correlation sum, traditionally defined as $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{x\\in S}\\left\\langle A(S),x-\\mu\\right\\rangle\\right]}\\end{array}$ , scales with the accuracy, $\\alpha$ , of the algorithm. Previous work, such as [CWZ21], achieves this by setting the underlying distribution, $\\mathcal{D}$ , to be a mixture distribution which, for some $p\\,=\\,o(1)$ , samples a 0 vector with probability $(1-p)$ and samples from the non-trivial distribution, $\\mathcal{D}_{\\mu}$ , with probability $p$ . However, now the variance satisfies $\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\left\\Vert x-\\mathbb{E}\\left[x\\right]\\right\\Vert^{2}\\right]\\le2p R^{2}$ meaning that when public data is present it holds that $\\begin{array}{r}{\\mathbb{E}\\left[\\|\\frac{1}{n_{\\mathrm{pub}}}\\,\\sum_{x\\in S_{\\mathrm{pub}}}x-\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,[x]\\,\\|\\right]\\,\\leq\\,\\frac{2p R}{\\sqrt{n_{\\mathrm{pub}}}}\\,=\\,o\\big(\\frac{R}{\\sqrt{n_{\\mathrm{pub}}}}\\big)}\\end{array}$ , and one cannot hope to achieve the desired lower bound. Alternatively, by instead analyzing the sum E $\\begin{array}{r}{\\left[\\sum_{x\\in S}\\left\\langle A(S)-\\mu,x-\\mu\\right\\rangle\\right]}\\end{array}$ , as seen for example in [KU20], we are able to avoid sampling from a mixture distribution. Further, by leveraging the flexibility of the strong distribution framework from $[{\\mathrm{DSS}}^{+}15]$ , we are able to still ensure $\\|\\mu(\\mathcal \u1e0a D \u1e0c )\\|=o(1)$ , as needed for the SCO reduction; see Section B.3. These techniques lead to the result in Theorem 12. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Unfortunately, with regards to obtaining the $\\sqrt{\\log\\left(1/\\delta\\right)}$ improvement in Theorem 13, the property $\\mathbb{E}\\left[\\lVert A(S)-\\dot{\\mu}\\rVert\\right]\\leq\\alpha$ does little to help establish the needed tail bound; see Eqn. (5). By clipping the components of ${\\mathcal{A}}(S)$ to to the range $[-O(\\alpha),O(\\alpha)]$ , we are able to able to obtain the desired concentration. Unfortunately, this clipping technique in combination with the intermediate distribution, Unif(Sz), leads to the restrictions that d \u2265n\u03f5 and npub \u2264 log(1/n[nd\u03b4]). These restrictions occur because of the need for the \u201cadditional error\u201d introduced by the intermediate distribution to be negligible. To see this, observe the intermediate distribution leads to $\\begin{array}{r}{\\mathbb{E}\\left[\\|A(S)-\\mu\\|\\right]\\geq\\frac{1}{\\sqrt{m}}}\\end{array}$ since ${\\mathcal{A}}(S)$ depends on only $m$ vectors from $\\mathcal{D}_{\\mu}$ , and the analysis in the proof of Theorem 12 (with $n_{\\mathrm{priv}}=0$ ) shows us that even non-private algorithms cannot do better on this distribution. We remark that [CWZ21] avoids this issue, and hence the restriction on $d$ and $n_{\\mathrm{pub}}$ , because of the fact that one only actually needs $\\|\\mathbb{E}\\left[A(S)\\right]-\\mu\\|\\leq\\alpha$ for the fingerprinting lemma to hold, and $\\|\\mathbb{E}\\left[A(S)\\right]-\\mu\\|\\le\\mathbb{E}\\left[\\|A(S)-\\mu\\|\\right]$ . However, after clipping it is possible that $\\|\\mathbb{E}\\left[\\left\\lfloor A(S)\\right\\rfloor_{M}\\right]-\\mu\\|\\geq\\left\\|\\mathbb{E}\\left[A(S)\\right]-\\mu\\right\\|$ . ", "page_idx": 21}, {"type": "text", "text": "B.3 Missing proofs from Section 3.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . We use the instance in [BST14], $\\begin{array}{r l r}{\\ell(w;x)}&{{}=}&{G\\left\\langle w,x\\right\\rangle}\\end{array}$ and $\\mathcal{W}\\quad=$ $\\left\\{x\\in\\mathbb{R}^{d}:\\|x\\|\\leq1\\right\\}$ . By a standard rescaling argument, we only need to consider $G=D=1$ . We will consider the re-scaled data distribution used in Theorem 12, where $\\{z_{1},...,z_{n}\\}\\stackrel{i.i.d.}{\\sim}\\mathcal{D}_{\\mu}$ and the dataset $S$ has $\\begin{array}{r}{x_{j}=\\frac{1}{\\sqrt{d}}z_{j}}\\end{array}$ for $j\\in n$ . Here $\\mu\\sim\\mathsf{U n i f}([-M,M]^{d})$ where $M$ will be chosen later. ", "page_idx": 21}, {"type": "text", "text": "First note by Lemma 5 we have that $\\begin{array}{r}{\\mathbb{P}[|\\|\\mu\\|-\\sqrt{\\frac{2}{3}}M|\\ge\\frac{M}{256}]\\le\\frac{1}{512}}\\end{array}$ so long as $d$ is larger than some constant. Define this event as $E$ and $E^{\\prime}$ its complement. Thus we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})\\right]=\\mathbb{E}\\left[L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})|E\\right]\\mathbb{P}[E]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\left.\\mathbb{E}\\left[L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})|E^{\\prime}\\right]\\mathbb{P}[E^{\\prime}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\ge\\frac{1}{2}\\mathbb{E}\\left[L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})|E\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus it suffices to lower bound the conditional excess risk. ", "page_idx": 21}, {"type": "text", "text": "The optimal solution under the aforementioned loss is $\\begin{array}{r}{w^{*}=-\\frac{\\mu}{\\|\\mu\\|}}\\end{array}$ , since the constraint set is a ball of radius 1. We can see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})=\\langle\\mathcal{A}(S),\\mu\\rangle-\\left\\langle-\\frac{\\mu}{\\|\\mu\\|},\\mu\\right\\rangle}\\\\ &{\\quad=\\|\\mu\\|\\left(1-\\langle A(S),w^{*}\\rangle\\right)}\\\\ &{\\quad=\\|\\mu\\|\\left(1-\\frac{1}{2}\\|A(S)\\|^{2}-\\frac{1}{2}\\|w^{*}\\|^{2}+\\frac{1}{2}\\|A(S)-w^{*}\\|^{2}\\right)}\\\\ &{\\quad\\geq\\frac{1}{2}\\left\\|\\mu\\right\\|\\left\\|A(S)-w^{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will now lower bound $\\|A(S)-w^{*}\\|$ by using the lower bound for mean estimation developed in Theorem 11. Let the mean estimate candidate is $\\begin{array}{r}{\\bar{\\mu}(S)=\\bar{\\mu}=-\\sqrt{\\frac{2}{3}}M A(S)}\\end{array}$ . Under the event $E$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}=\\left\\|-\\sqrt{2/3}M A(S)-\\mu\\right\\|^{2}}\\\\ {\\qquad=\\left\\|-\\left\\|\\mu\\right\\|A(S)-\\mu+(\\sqrt{2/3}M-\\left\\|\\mu\\right\\|)A(S)\\right\\|^{2}}\\\\ {\\qquad\\leq2M^{2}\\left\\|A(S)-w^{*}\\right\\|^{2}+\\frac{M^{2}}{50}}\\\\ {\\qquad\\implies\\left\\|A(S)-w^{*}\\right\\|^{2}\\geq\\frac{\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}}{2M^{2}}-\\frac{1}{512}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The above follows from the definition of $w^{*}$ and since the algorithm\u2019s output is considered in a ball of radius 1, so $\\|A(S)\\|\\leq1$ . ", "page_idx": 22}, {"type": "text", "text": "Combining the above inequalities (6) and (7) then taking expectation we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[L(\\mathcal{A}(S);\\mathcal{D})-L(w^{*};\\mathcal{D})|E\\right]\\geq\\mathbb{E}\\left[\\frac{1}{4}\\left\\|\\mu\\right\\|\\left(\\frac{\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}}{M^{2}}-\\frac{1}{512}\\right)\\bigg|E\\right]}\\\\ {\\geq\\frac{M}{1024}\\left(\\frac{\\mathbb{E}\\left[\\|\\bar{\\mu}-\\mu\\|^{2}|E\\right]}{2M^{2}}-\\frac{1}{512}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To bound $\\mathbb{E}\\left[\\|\\bar{\\boldsymbol{\\mu}}-\\boldsymbol{\\mu}\\|^{2}\\boldsymbol{\\vert E\\right]}$ , observe ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}]=\\mathbb{E}[\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}|E]\\mathbb{P}[E]+\\mathbb{E}_{\\mu,S}[\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}|E^{\\prime}]\\mathbb{P}[E^{\\prime}]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[\\left\\|\\bar{\\mu}-\\mu\\right\\|^{2}|E]+4M^{2}\\mathbb{P}[E^{\\prime}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rearranging we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\left\\Vert\\bar{\\mu}-\\mu\\right\\Vert^{2}|E]\\geq\\mathbb{E}[\\left\\Vert\\bar{\\mu}-\\mu\\right\\Vert^{2}]-\\frac{M^{2}}{128}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will finish the bound by applying either Theorem 12 or Theorem 13. ", "page_idx": 22}, {"type": "text", "text": "Via Theorem 12: Set $\\begin{array}{r}{M=\\operatorname*{min}\\left\\{\\frac{\\sqrt{d}}{8n_{\\mathrm{priv}}},\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}}\\right\\}}\\end{array}$ . Under this setting of $M$ , Theorem 12 implies that the lower bound on mean estimate distance satisfies $\\begin{array}{r}{\\mathbb{E}\\left[\\lVert\\bar{\\boldsymbol{\\mu}}-\\boldsymbol{\\mu}\\rVert\\right]\\geq\\frac{M}{8}}\\end{array}$ , and thus $\\mathbb{E}[\\left\\Vert\\bar{\\mu}-\\mu\\right\\Vert^{2}\\left|E\\right|\\geq$ $\\frac{M^{2}}{128}$ by Eqn. (9) above. Plugging into Eqn. (8) we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[L(\\boldsymbol{A}(\\boldsymbol{S});\\mathcal{D})-L(\\boldsymbol{w}^{*};\\mathcal{D})\\right]=\\Omega\\left(\\boldsymbol{M}\\right)=\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{\\sqrt{d}}{n_{\\mathrm{priv}}},\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Via Theorem 13: In Theorem 13, the setting of $M$ used is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M=4\\Bigg(\\frac{1}{125}\\operatorname*{min}\\left\\{\\frac{\\sqrt{d\\log\\left(1/\\delta\\right)}}{n\\epsilon},\\sqrt{\\frac{1}{n_{\\mathrm{pub}}}}\\right\\}+\\sqrt{\\frac{\\log\\left(1/\\left[\\sqrt{d n}\\delta\\right]\\right)}{n\\epsilon}}\\Bigg)}\\\\ &{\\quad\\leq\\frac{1}{30}\\operatorname*{min}\\left\\{\\frac{\\sqrt{d\\log\\left(1/\\delta\\right)}}{n\\epsilon},\\sqrt{\\frac{1}{n_{\\mathrm{pub}}}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The inequality holds under the conditions d \u22651202n\u03f5 and npub \u22641202 log(1n/[\u221and\u03b4]). Thus we have under this setting of $M$ that $\\begin{array}{r}{\\mathbb{E}\\left[\\|\\bar{\\boldsymbol{\\mu}}-\\boldsymbol{\\mu}\\|\\right]\\geq\\frac{M}{8}}\\end{array}$ . Applying Eqns. (9) and (8) as in the previous case we have (providing the above conditions on $d$ and $n_{\\mathrm{pub}}$ hold) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[L(A(S);\\mathcal{D})-L(w^{*};\\mathcal{D})\\right]=\\Omega\\left(M\\right)=\\Omega\\left(\\operatorname*{min}\\left\\{\\frac{\\sqrt{d\\log\\left(1/\\delta\\right)}}{n\\epsilon},\\sqrt{\\frac{1}{n_{\\mathrm{pub}}}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 5. For z \u223cUnif([\u22121, 1]d), we have that \u2225z\u2225\u2208 \u221a23d \u00b1 3 ln2 (1/\u03b3), with probability at least 1 \u2212\u03b3. ", "page_idx": 23}, {"type": "text", "text": "Proof. This follows from standard concentration of norm results. We have that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|z\\right\\|^{2}=d\\mathbb{E}z_{1}^{2}=\\frac{2d}{3}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As in [Ver18, proof of Theorem 3.1.1], we use the simple fact that $|x-1|>\\delta\\implies\\left|x^{2}-1\\right|>$ $\\operatorname*{max}(\\delta,\\delta^{2})$ for any $x,\\delta\\geq0$ , to get, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|\\left|z\\right|\\right|-\\frac{\\sqrt{2d}}{\\sqrt{3}}\\right|>\\frac{\\sqrt{2d}\\delta}{\\sqrt{3}}\\right)=\\mathbb{P}\\left(\\left|\\frac{\\sqrt{3}\\left|z\\right|}{\\sqrt{2d}}-1\\right|>\\delta\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|\\frac{3\\left|z\\right|}{2d}-1\\right|>\\operatorname*{max}(\\delta,\\delta^{2})\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|\\frac{1}{d}\\sum_{i=1}^{d}z_{i}^{2}-\\frac{2}{3}\\right|>\\operatorname*{max}\\left((2/3)\\delta,((2/3)\\delta)^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We substitute $\\begin{array}{r}{\\bar{\\delta}=\\frac{2\\delta}{3}}\\end{array}$ and apply Bernstein\u2019s inequality for i.i.d sub-exponential random variables $z_{i}^{2}$ . Since, $z_{i}\\in[-1,1]$ , the sub-exponential norm $\\leq1$ . Applying Corollary 2.8.3 from [Ver18], we get that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{d}\\sum_{i=1}^{d}z_{i}^{2}-\\frac{2}{3}\\right|>\\operatorname*{max}\\left(\\bar{\\delta},(\\bar{\\delta})^{2}\\right)\\right)\\le\\exp\\left(-2\\bar{\\delta}^{2}d\\right)=\\exp\\left(-8\\delta^{2}d/9\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This gives us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\lVert z\\rVert-\\frac{\\sqrt{2d}}{\\sqrt{3}}\\right|>\\frac{\\sqrt{2d}\\delta}{\\sqrt{3}}\\right)\\le\\exp\\left(-8\\delta^{2}d/9\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, with probability, at least 1 \u2212\u03b3, we have that \u2225z\u2225\u2208 \u221a23d \u00b1 3 ln2 (1/\u03b3), which completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2. We use the squared loss instance as in [BST14, Section 5.2]; $\\begin{array}{r l}{\\ell(w;z)\\;=}&{{}}\\end{array}$ $\\frac{\\lambda}{2}\\left\\|w-z\\right\\|^{2}$ , with $\\begin{array}{r}{\\|z\\|\\le\\frac{G}{2\\lambda}}\\end{array}$ . The loss is $G$ -Lipschitz and $\\lambda$ strongly convex on the domain of unit ball at zero of radius 2G\u03bb. Given a datasets $S=\\{z_{1},z_{2},\\ldots,z_{n}\\}$ , the population risk minimizer is simply the population mean $\\mu(\\mathcal{D})$ . Further, it is straightforward to verify that the excess population risk a re-scaling of the mean estimation error ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[L(A(S))-\\operatorname*{min}_{w}L(w)]=\\frac{\\lambda}{2}\\mathbb{E}\\left\\|A(S)-\\mu(\\mathcal{D})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting the mean estimation lower bounds, Theorem 11, completes the proof. ", "page_idx": 23}, {"type": "text", "text": "C Missing Proofs from Section 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Define the orthogonal projection matrix $P_{X_{\\mathrm{pub}}}\\,=\\,U U^{\\top}$ . Note that the feature vectors in $\\tilde{S}_{\\mathrm{priv}}=$ $\\left\\{(U^{\\top}x_{i},y_{i})\\right\\}_{i=1}^{n_{\\mathrm{priv}}}$ are bounded. In particular $\\left\\|U^{\\top}x\\right\\|^{2}=x^{\\top}(U U^{\\top})x=x^{\\top}P_{X_{\\mathrm{pub}}}x\\leq\\left\\|x\\right\\|^{2}$ , since $P_{X_{\\mathrm{pub}}}$ is an orthogonal projection onto $\\operatorname{span}(\\mathcal{W}\\cap S_{\\mathrm{pub}})$ . Further, since $\\tilde{w}\\in\\tilde{\\mathcal{W}}$ , we have that there exists $\\mathring{w}\\in\\mathcal{W}$ such that $\\tilde{w}=U^{\\top}\\tilde{w}$ . Finally, $\\widehat{w}=U\\tilde{w}=U U^{\\top}\\mathring{w}=P_{X_{\\mathrm{pub}}}\\mathring{w}\\in\\mathcal{W}$ since the range of $P_{X_{\\mathrm{pub}}}\\subseteq\\mathcal{W}$ . ", "page_idx": 23}, {"type": "text", "text": "The privacy guarantee follows from the privacy guarantee of sub-routine $\\tilde{\\mathcal A}$ . For utility, we define $w^{\\ast}\\in$ arg $\\scriptstyle\\operatorname*{min}_{w\\in\\mathcal{W}}L(w;\\mathcal{D})$ and $\\tilde{w}^{*}\\,\\in\\,\\arg\\operatorname*{min}_{w\\in\\tilde{\\mathcal{W}}}L(w;U^{\\top}\\mathcal{D})$ , where $U^{\\top}\\mathcal{D}$ denotes the distribution which first samples from $\\mathcal{D}$ then project using $U^{\\top}$ . Let $\\mathring{w}^{\\ast}\\in\\mathcal{W}$ such that $\\mathring{w}^{\\ast}=U\\tilde{w}^{\\ast}$ . ", "page_idx": 24}, {"type": "text", "text": "Note that from the GLM structure, $L(\\tilde{w}^{*};U^{\\top}\\mathcal{D})=L(\\mathring{w}^{*};\\mathcal{D})$ . We have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{w};\\mathcal{D})-L(w^{*};\\mathcal{D})=L(\\widehat{w};\\mathcal{D})-\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})+L(\\widehat{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{\\qquad\\qquad\\qquad+\\widehat{L}(\\widetilde{w}^{*};S_{\\mathrm{priv}})-L(\\widehat{w}^{*};\\mathcal{D})+\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})-\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{O}\\left(G\\mathfrak{R}_{n_{\\mathrm{priv}}}(\\mathcal{H})+\\displaystyle\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathrm{priv}}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad+L(\\widehat{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})+\\widehat{L}(\\widetilde{w};\\widetilde{S}_{\\mathrm{priv}})-\\displaystyle\\operatorname*{min}_{w\\in\\widehat{\\mathcal{N}}}\\widehat{L}(w;\\widetilde{S}_{\\mathrm{priv}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability at least $1-\\beta/4$ . In the above, we control the generalization gap via uniform convergence and concentration for the fixed $\\mathring{w}^{\\ast}$ with respect to $S_{\\mathrm{priv}}$ . ", "page_idx": 24}, {"type": "text", "text": "The last term $\\begin{array}{r}{\\widehat{L}(\\tilde{w};\\tilde{S}_{\\mathrm{priv}})\\!-\\!\\operatorname*{min}_{w\\in\\tilde{\\mathcal{W}}}\\widehat{L}(w;\\tilde{S}_{\\mathrm{priv}})}\\end{array}$ is bounded by the guarantee of the private sub-routine with probability at least $1-\\beta/4$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\left.\\gamma}(\\tilde{A}(\\tilde{S}_{\\mathrm{piv}});\\tilde{S}_{\\mathrm{piv}})-\\operatorname*{min}_{w\\in\\tilde{\\mathcal{W}}}\\tilde{L}(\\tilde{A}(\\tilde{S}_{\\mathrm{piv}});\\tilde{S}_{\\mathrm{piv}})=\\tilde{O}\\left(G D\\,\\|\\,\\mathcal{X}\\|\\left(\\frac{\\sqrt{n_{\\mathrm{pub}}\\log\\left(1/\\delta\\right)}+\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathrm{piv}}\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, for any $\\bar{w}^{*}$ such that $\\bar{w}^{*}\\in U\\tilde{\\mathcal{W}}$ , with probability at least $1-\\beta/2$ , from $G$ -Lipschitznes, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nL(\\bar{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})\\leq L(\\bar{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})\\leq G\\left\\|\\bar{w}^{*}-w^{*}\\right\\|_{2,\\mathcal{D}_{\\mathcal{X}}}\\leq G\\alpha,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows essentially from Lemma 6 and Lemma 1 for $\\begin{array}{r l}{n_{\\mathrm{pub}}}&{{}=}\\end{array}$ $\\begin{array}{r}{O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(n_{\\mathrm{pub}})\\mathfrak{R}_{n_{\\mathrm{pub}}}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\end{array}$ . To elaborate, the first step holds since $\\mathring{w}^{\\ast}=U\\tilde{w}^{\\ast}$ and $\\tilde{w}^{*}$ is the the minimizer of risk over $\\tilde{\\mathcal{W}}$ . Now, Lemma 1 guarantees that for any $w^{\\ast}\\in\\mathcal{W}$ , there exists a $w^{*}$ in its $\\alpha$ -cover with respect to $\\lVert\\cdot\\rVert_{2,X_{\\mathrm{pub}}}$ , with $\\|w^{*}-\\bar{w}^{*}\\|_{2,\\mathcal{D}_{\\mathcal{X}}}\\le\\alpha$ . To argue why span $(X_{\\mathrm{pub}})$ is an $\\alpha$ -cover, from Lemma 6, we have that from any $\\alpha$ -cover $\\bar{\\mathcal{W}}$ , of $\\mathcal{W}$ w.r.t. $\\left\\Vert\\cdot\\right\\Vert_{2,X_{\\mathrm{pub}}}$ , we can remove elements which do not lie in span $\\left(S_{\\mathrm{pub}}\\right)$ and still have an $\\alpha$ -cover. Hence, the superset used in Algorithm 1, which essentially is, $\\bar{\\mathcal{W}}=P_{X_{\\mathrm{pub}}}\\mathcal{W}$ , is indeed an $\\alpha$ -cover. ", "page_idx": 24}, {"type": "text", "text": "The $n_{\\mathrm{pub}}$ we get is, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{\\mathrm{pub}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log{(1/\\beta)}}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left(D^{2}\\left\\|\\boldsymbol{\\mathcal{X}}\\right\\|^{2}\\operatorname*{max}\\left(\\frac{\\log{(2/\\beta)}}{\\alpha^{2}},\\frac{1}{\\alpha^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the above, we plug in the Rademacher complexity of bounded linear predictor, $\\Re_{m}(\\mathcal{H})=$ $\\Theta\\left({\\frac{D\\|{\\boldsymbol{\\mathcal{X}}}\\|}{m}}\\right)$ . Plugging the above in Equation (10), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\hat{w};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{=O\\left(\\frac{G D\\|\\mathcal{X}\\|}{\\sqrt{n_{\\mathtt{p r i v}}}}+\\frac{G D\\|\\mathcal{X}\\|\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}+\\frac{B\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}\\right)}\\\\ &{+O\\left(G D\\|\\mathcal{X}\\|\\left(\\frac{\\sqrt{n_{\\mathtt{p h}}\\log(1/\\beta)}+\\sqrt{\\log(4/\\beta)}}{n_{\\mathtt{p h i v}}\\epsilon}\\right)\\right)+G\\alpha}\\\\ &{=O\\left(\\frac{G D\\|\\mathcal{X}\\|\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}+G D^{2}\\|\\mathcal{X}\\|^{2}\\left(\\frac{\\sqrt{\\log(2/\\beta)+\\log(1/\\delta)}}{\\alpha n_{\\mathtt{p r i v}}\\epsilon}\\right)+\\frac{B\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}\\right)+G\\alpha}\\\\ &{=O\\left(G D\\|\\mathcal{X}\\|\\left(\\frac{\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}+\\left(\\frac{(\\log(2/\\beta)+\\log(1/\\delta))^{1/4}}{\\sqrt{n_{\\mathtt{p r i v}}}\\epsilon}\\right)\\right)+\\frac{B\\sqrt{\\log(4/\\beta)}}{\\sqrt{n_{\\mathtt{p r i v}}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the above follows by setting $\\begin{array}{r}{\\alpha=\\frac{D\\|\\mathcal{X}\\|(\\log(1/\\delta)+\\log(2/\\beta))^{1/4}}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}}\\end{array}$ . This yields the claimed rate. The resulting public sample complexity is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{\\mathrm{pub}}=\\tilde{O}\\left(D^{2}\\left\\|\\mathcal{X}\\right\\|^{2}\\operatorname*{max}\\left(\\frac{\\log{(2/\\beta)}}{\\alpha^{2}},\\frac{1}{\\alpha^{2}}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left(\\frac{n_{\\mathrm{priv}}\\epsilon}{(\\log{(2/\\beta)}+\\log{(1/\\delta)})^{1/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma 6. Let $\\tilde{\\mathcal{H}}$ be a $\\alpha$ -cover of $\\mathcal{H}$ with respect $\\lVert\\cdot\\rVert_{2,X_{p u b}}$ . Then, $\\bar{\\mathcal{H}}=\\tilde{\\mathcal{H}}\\cap s p a n(X_{p u b})$ is also an $\\alpha$ -cover. ", "page_idx": 25}, {"type": "text", "text": "Proof. Given two $h_{1},h_{2}\\in\\mathcal{H}$ , we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|h_{1}-h_{2}\\|_{2,X_{\\mathrm{pab}}}=\\sqrt{\\frac{1}{n_{\\mathrm{pub}}}\\sum_{i=1}^{n_{\\mathrm{pab}}}(h_{1}(x_{i})-h_{2}(x_{i}))^{2}}=\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}}\\sqrt{(w_{1}-w_{2})^{\\top}X_{\\mathrm{pub}}^{\\top}X_{\\mathrm{pub}}(w_{1}-w_{2})}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $w_{1}$ and $w_{2}$ are the vectors corresponding to linear functions $h_{1}$ and $h_{2}$ and $X_{\\mathrm{pub}}$ denote the matrix of public feature vectors. ", "page_idx": 25}, {"type": "text", "text": "Given any $h\\in\\mathcal H$ , let $\\tilde{h}$ denote the element closest to it in the cover $\\tilde{\\mathcal{H}}$ ; we have, $\\|h-\\tilde{h}\\|_{2,X_{\\mathrm{pub}}}\\leq\\alpha$ . Consider the singular value decomposition, $\\begin{array}{r}{X_{\\mathrm{pub}}=V\\Sigma U^{\\top}}\\end{array}$ , where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix. Define $\\bar{h}=P_{X_{\\mathrm{pub}}}(\\tilde{h})=U U^{\\top}\\tilde{h}$ . Note that $U$ is an orthogonal projection onto span $(X_{\\mathrm{pub}})$ and $P_{X_{\\mathrm{pub}}}$ is the corresponding projection matrix. We have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|h-\\bar{h}\\|_{2,X_{p\\mathsf{p h}}}^{2}=\\frac{1}{n_{\\mathrm{ph}}\\delta}(h-\\bar{h})^{\\top}X_{\\mathrm{ph}}^{\\top}X_{\\mathsf{p h}}(h-\\bar{h})}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{n_{\\mathrm{ph}}\\delta}(h-P_{X_{p\\mathsf{p h}}}(\\bar{h}))^{\\top}U\\Sigma^{2}U^{\\top}(h-P_{X_{p\\mathsf{p h}}}(\\bar{h}))}\\\\ &{\\qquad\\qquad=\\frac{1}{n_{\\mathrm{ph}}\\delta}(h-\\bar{h})^{\\top}U(U^{\\top}U)\\Sigma^{2}(U^{\\top}U)U^{\\top}(h-\\bar{h})}\\\\ &{\\qquad=\\frac{1}{n_{\\mathrm{ph}}\\delta}(h-\\bar{h})^{\\top}U\\Sigma^{2}U^{\\top}(h-\\bar{h})}\\\\ &{\\qquad=\\frac{1}{n_{\\mathrm{ph}}\\delta}(h-\\bar{h})^{\\top}X_{\\mathsf{p h}}^{\\top}X_{\\mathsf{p h}}(h-\\tilde{h})}\\\\ &{\\qquad\\qquad\\leq\\alpha^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since by construction $\\bar{h}$ also lies in span $\\left(X_{\\mathrm{pub}}\\right)$ , this proves the claim. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof of Theorem 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We state the complete version of this theorem and then present its proof. ", "page_idx": 25}, {"type": "text", "text": "Theorem 14. Let $\\epsilon\\mathrm{~\\ensuremath~{~>~}~}0,\\delta\\mathrm{~\\ensuremath~{~>~}~}0$ and $\\epsilon\\mathrm{~\\,~\\hfil~\\displaystyle~\\leq~\\,~}\\log{(1/\\delta)}$ . For a $G$ -Lipschitz, $B$ -bounded non-negative $H$ -smooth loss function, Algorithm $^{\\,l}$ satisfies $(\\epsilon,\\delta)$ -DP. If the private subroutine $\\tilde{\\mathcal A}$ guarantees Equation (1) with probability at least $1\\:-\\:\\beta$ , then with $\\begin{array}{r l}{n_{p u b}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\tilde{O}\\left(\\frac{(H D\\|\\mathcal{X}\\|)^{2/3}(n_{p r i v}\\epsilon)^{2/3}}{G^{2/3}(\\log(1/\\delta))^{1/3}}+\\frac{\\sqrt{H}n_{p r i v}\\epsilon\\sqrt{L(w^{*};D)}}{G\\sqrt{\\log(1/\\delta))}}\\right)}\\end{array}$ , with probability at least $1-\\beta,\\,L(\\widehat{w};D)-L(w^{*};D)$ is at most $\\begin{array}{r l}&{\\frac{5}{3}\\left(\\left(\\displaystyle\\frac{\\sqrt{H}D\\left\\|\\boldsymbol{X}\\right\\|}{\\sqrt{n_{p r i v}}\\epsilon}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}}\\right)\\sqrt{L(w^{*};\\mathcal{D})}+\\frac{H\\left\\|\\boldsymbol{X}\\right\\|^{2}D^{2}}{n_{p r i v}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}+\\frac{G D\\left\\|\\boldsymbol{X}\\right\\|\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{p r i v}\\epsilon}\\right)}\\\\ &{+\\,\\tilde{O}\\left(\\left(\\frac{\\sqrt{H}D^{2}\\left\\|\\boldsymbol{X}\\right\\|^{2}G\\sqrt{\\log\\left(1/\\delta\\right)}}{n_{p r i v}\\epsilon}\\right)^{2/3}+\\frac{H^{1/4}D\\left\\|\\boldsymbol{X}\\right\\|\\sqrt{G}\\left(\\log\\left(1/\\delta\\right)\\right)^{1/4}L(w^{*};\\mathcal{D})^{1/4}}{\\sqrt{n_{p r i v}}\\epsilon}\\right)}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Further, with $\\begin{array}{r}{n_{p u b}\\,=\\,\\tilde{O}\\left(\\frac{(H D\\,\\|\\boldsymbol{\\chi}\\|)^{2/3}\\,(n_{p r i\\nu})^{2/3}}{G^{2/3}(\\log(1/\\delta))^{1/3}}+\\frac{\\sqrt{H}n_{p r i\\nu}\\epsilon\\sqrt{\\widehat{L}(\\widehat{w}^{*};S_{p r i\\nu})}}{G\\sqrt{\\log(1/\\delta))}}\\right)}\\end{array}$ with probability at least $1-\\beta,$ , for any $\\bar{w}\\in\\mathcal{W}$ , $L(\\widehat{w};D)-\\widehat{L}(\\widehat{w}^{*};S_{p r i\\nu})$ is at most ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Im}{3}\\left(\\left(\\frac{\\sqrt{H}D\\,\\|\\boldsymbol{X}\\|}{\\sqrt{n_{p r i v}}\\epsilon}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}}\\right)\\sqrt{\\widehat{L}(\\hat{w}^{*};S_{p r i v})}+\\frac{H\\,\\|\\boldsymbol{X}\\|^{2}\\,D^{2}}{n_{p r i v}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}+\\frac{G D\\,\\|\\boldsymbol{X}\\|\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{p r i v}\\epsilon}\\right.}\\\\ &{+\\left.\\tilde{O}\\left(\\left(\\frac{\\sqrt{H}D^{2}\\,\\|\\boldsymbol{X}\\|^{2}\\,G\\sqrt{\\log\\left(1/\\delta\\right)}}{n_{p r i v}\\epsilon}\\right)^{2/3}+\\frac{H^{1/4}D\\,\\|\\boldsymbol{X}\\|\\sqrt{G}\\,\\left(\\log\\left(1/\\delta\\right)\\right)^{1/4}\\,\\widehat{L}\\left(\\overline{{w}};S_{p r i v}\\right)^{1/4}}{\\sqrt{n_{p r i v}\\epsilon}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $w^{*}$ and $\\widehat{w}^{*}$ are population and empirical minimizers with respect to $\\mathcal{D}$ and $S_{p r i v}$ respectively. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem $I4.$ . The privacy guarantee follows from the privacy guarantee of sub-routine $\\tilde{\\mathcal A}$ . The proof of the utility guarantee proceeds similar to that of Theorem 3. We define $w^{*}\\in$ ar $\\bar{\\operatorname{g\\,min}_{w\\in\\mathcal{W}}}\\,L(w;\\mathcal{D})$ and $\\tilde{\\boldsymbol{w}}^{*}\\in\\arg\\operatorname*{min}_{\\boldsymbol{w}\\in\\tilde{\\boldsymbol{W}}}L(\\boldsymbol{w};\\boldsymbol{U}^{\\top}\\mathcal{D})$ . Let $\\mathring{w}^{\\ast}\\in\\mathcal{W}$ such that $\\mathring{w}^{\\ast}\\,=\\,U\\tilde{w}^{\\ast}$ From the GLM structure, $L(\\tilde{w}^{*};U^{\\top}\\mathcal{D})=L(\\mathring{w}^{*};\\mathcal{D})$ . We have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{w};{\\mathcal D})-L(w^{*};{\\mathcal D})=L(\\widehat{w};{\\mathcal D})-\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})+\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})-L(w^{*};{\\mathcal D})}\\\\ &{\\qquad\\qquad\\qquad\\leq L(\\widehat{w};{\\mathcal D})-\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})+\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})-L((\\widehat{w}^{*};{\\mathcal D})}\\\\ &{\\qquad\\qquad\\qquad+\\,L(\\widehat{w}^{*};{\\mathcal D})-L(w^{*};{\\mathcal D})+\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})-\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left|L(\\widehat{w};{\\mathcal D})-\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})\\right|+\\left|L(\\widehat{w}^{*};{\\mathcal D})-\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})\\right|}\\\\ &{\\qquad\\qquad\\qquad+\\,L(\\widehat{w}^{*};{\\mathcal D})-L(w^{*};{\\mathcal D})+\\widehat{L}(\\widehat{w};\\widetilde{S}_{\\mathrm{priv}})-\\underset{w\\in\\widehat{\\mathcal N}}{\\operatorname*{min}}\\,\\widehat{L}(w;\\widetilde{S}_{\\mathrm{priv}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The last term $\\begin{array}{r}{\\widehat{L}(\\tilde{w};\\tilde{S}_{\\mathrm{priv}})\\--\\operatorname*{min}_{w\\in\\tilde{\\mathcal{W}}}\\widehat{L}(w;\\tilde{S}_{\\mathrm{priv}})}\\end{array}$ is bounded by the guarantees of the private subroutine with probability at least $1-\\beta/4$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{L}(\\widehat{w};\\tilde{S}_{\\mathrm{priv}})-\\operatorname*{min}_{w\\in\\tilde{W}}\\widehat{L}(w;\\tilde{S}_{\\mathrm{priv}})=\\tilde{O}\\left(G D\\,\\|\\chi\\|\\left(\\frac{\\sqrt{n_{\\mathrm{pub}}\\log\\left(1/\\delta\\right)}+\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathrm{priv}}\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To bound the term ${\\cal L}(\\mathring{w}^{\\ast};D)-{\\cal L}(w^{\\ast};D)$ in Equation (11), we apply smoothness to get, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\hat{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{\\le L(\\bar{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{\\le{\\mathbb E}\\left[\\left\\langle\\phi_{y}^{*}(\\langle w^{*},x\\rangle),\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle\\right\\rangle+\\displaystyle{\\frac{H}{2}}\\,{|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}}\\right]}\\\\ &{\\le{\\mathbb E}\\left[\\left|\\phi_{y}^{*}(\\langle w^{*},x\\rangle)\\right||\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|+\\displaystyle{\\frac{H}{2}}\\,{|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}}\\right]}\\\\ &{\\le{\\sqrt{{\\mathbb E}\\left|\\phi_{y}^{*}(\\langle w^{*},x\\rangle)\\right|^{2}}}{\\sqrt{{\\mathbb E}_{x\\sim{\\mathcal D}_{x}}\\,{|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}}}}+\\displaystyle{\\frac{H}{2}}{\\mathbb E}_{x\\sim{\\mathcal D}_{x}}\\,{|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}}}\\\\ &{\\le2{\\sqrt{H{\\mathbb E}_{x\\sim\\mathcal{D}}\\phi_{y}(\\langle w^{*},x\\rangle)}}{\\sqrt{{\\mathbb E}\\left|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle\\right|^{2}}}+\\displaystyle{\\frac{H}{2}}{\\mathbb E}_{x\\sim{\\mathcal D}_{x}}\\,{|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}}}\\\\ &{\\le2{\\sqrt{H L(w^{*};\\mathcal{D})}}\\alpha+H\\alpha^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the above holds for any $\\begin{array}{r l r}{\\bar{w}^{*}}&{{}\\in}&{\\mathcal{H}}\\end{array}$ such that $\\bar{w}^{*}\\;\\;\\in\\;\\;U\\tilde{\\mathcal{W}}$ by optimality of $\\tilde{w}^{*}$ in $\\tilde{\\mathcal{W}}$ . The second inequality holds from $H$ -smoothness, the third and fourth from CauchySchwarz, the fifth from self-bounding property of smooth non-negative losses (Lemma 4.1 in [SST10]). The final step holds with probability $1\\,-\\,\\beta/2$ from Lemma 1 with $\\mathrm{\\Delta}n_{\\mathrm{pub}}\\,=$ $\\begin{array}{r}{O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\end{array}$ together with that since $\\tilde{\\mathcal{W}}$ is an $\\alpha$ -cover of $\\mathcal{H}$ , together with Lemma 6 which shows that $\\tilde{\\mathcal{W}}$ is a valid $\\alpha$ -cover of $\\mathcal{W}$ . Therefore, there exists $\\bar{h}^{*}\\in\\tilde{\\mathcal{H}}$ with $\\left\\|\\bar{h}^{*}-h^{*}\\right\\|_{2,S_{\\mathrm{pub}}}\\leq\\alpha$ . ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Further, applying AM-GM inequality, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nL(\\mathring{w}^{\\ast};D)\\leq2L(w^{\\ast};D)+2H\\alpha^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first two terms in Equation (11) are bound via uniform convergence for smooth non-negative losses, (Theorem 1 in [SST10]) and Bernstein\u2019s inequality as follows; with probability at least $1\\!-\\!\\beta/4$ , we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\Delta\\xi;D)-\\bar{L}(\\xi,\\zeta;S_{m})\\Bigg\\{-\\bar{L}(\\xi^{*}(X;\\zeta))-\\bar{L}(\\xi^{*};S_{m})\\Bigg\\}}\\\\ &{=\\bar{Q}\\Bigg(\\sqrt{H^{3}\\alpha_{m}(\\theta,\\zeta)}-(\\frac{\\Delta\\xi}{\\Delta\\xi})\\frac{\\Delta\\xi(X;\\zeta)}{\\Delta\\theta_{m}^{*}}\\Bigg)\\Bigg(\\sqrt{\\bar{L}(\\xi;S_{m},\\zeta)}+\\sqrt{\\zeta(\\xi^{*};\\mathcal{D})}\\Bigg)}\\\\ &{\\quad+\\bar{Q}\\Bigg(\\frac{H^{3}\\alpha_{m}^{*}}{\\sqrt{H^{3}\\alpha_{m}^{*}}}\\big(\\Delta\\frac{H_{\\infty}(\\xi^{*};S_{m})}{H_{\\infty}(\\theta,\\zeta)}\\big)}\\\\ &{\\quad+\\bar{Q}\\Bigg(\\frac{H^{3}\\alpha_{m}^{*}}{\\sqrt{H^{3}\\alpha_{m}^{*}}}+\\sqrt{\\frac{\\Delta\\xi}{H_{\\infty}(\\theta;\\zeta)}}\\Bigg)\\Bigg(\\sqrt{\\bar{L}(\\xi;\\mathcal{D}_{m})}+\\sqrt{\\zeta(\\xi^{*};\\mathcal{D})}\\Bigg)}\\\\ &{\\quad+\\bar{Q}\\Bigg(\\frac{H^{3}\\alpha_{m}^{*}\\Delta\\xi^{*}}{\\sqrt{H^{3}\\alpha_{m}^{*}}}+\\frac{H_{\\infty}(\\xi^{*};\\mathcal{D}_{m})}{H_{\\infty}(\\theta,\\zeta)}\\Bigg)+\\bar{Q}\\Bigg(\\Delta p;\\mathrm{l}\\Big(\\frac{\\sqrt{H_{\\infty}\\log\\left(1/\\frac{\\delta}{\\Delta}\\right)}+\\sqrt{\\alpha_{m}^{*}(\\xi^{*};\\mathcal{D})}}{\\Delta\\theta_{m}^{*}}\\Big)\\Bigg)}\\\\ &{\\quad+\\bar{Q}\\Bigg(\\frac{\\sqrt{H_{\\infty}\\log\\left(1/\\frac{\\delta}{\\Delta}\\right)}}{\\sqrt{\\alpha_{m}^{*}}\\sqrt{\\alpha_{m}^{*}}}+\\sqrt{\\frac{\\Delta\\xi}{\\sqrt{H_{\\infty}\\log\\left(1/\\frac{\\delta}{\\Delta}\\right)}}}\\Bigg)\\Bigg]\\times\\bar{L}(\\xi;\\mathcal{D}_{m})}\\\\ &{\\quad+\\bar{Q}\\Bigg(\\frac{H^{3}\\alpha_{m}^{*}\\Delta\\xi^{*}}{\\sqrt{\\alpha_{m}^{*}}}+\\frac{H_{\\infty} \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second equality follows fr\u221aom Equation (12), instantiating the Rademacher complexity of linear pre\u221adictors, concavity of $x\\mapsto{\\sqrt{x}}$ and AM-GM inequality. The third equality follows concavity of $x\\mapsto{\\sqrt{x}}$ and Bernstein\u2019s inequality, the fourth follows from Equation (14) and AM-GM inequality. Plugging the above, Equation (13) and Equation (12) into Equation (11), we get that with $\\begin{array}{r}{\\overbrace{n_{\\mathrm{pub}}}^{\\substack{\\mathrm{c}\\,\\mathrm{c}\\,\\mathrm{e}}}=\\overbrace{O\\left(\\operatorname*{max}\\left(\\frac{\\Vert X\\Vert^{2}\\bar{D}^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(n_{\\mathrm{pub}})\\mathfrak{R}_{n_{\\mathrm{pub}}}^{2}(\\mathcal{H})\\right\\}\\right)}^{\\substack{\\mathrm{c}\\,\\mathrm{c}\\,\\mathrm{e}}}\\right)}\\end{array}$ , the following holds with probability at least $1-\\beta$ , ", "page_idx": 27}, {"type": "text", "text": "1 $\\begin{array}{r l}&{\\boldsymbol{\\mathcal{L}}(\\vec{w};\\mathcal{D})-\\boldsymbol{L}({\\boldsymbol w}^{*};\\mathcal{D})}\\\\ &{=\\bar{\\mathcal{O}}\\left(\\frac{\\sqrt{H}D\\|\\boldsymbol{X}\\|}{\\sqrt{n_{\\mathtt{p p w}}\\epsilon}}+\\sqrt{H}\\alpha+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathtt{p p w}}}}\\right)\\sqrt{\\boldsymbol{L}({\\boldsymbol w}^{*};\\mathcal{D})}}\\\\ &{+\\,\\bar{\\mathcal{O}}\\left(\\frac{H\\|\\boldsymbol{X}\\|^{2}D^{2}}{n_{\\mathtt{p p w}}\\epsilon}+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathtt{p p w}}}\\right)+O\\left(G D\\|\\boldsymbol{X}\\|\\left(\\frac{\\sqrt{n_{\\mathtt{p b}}\\log\\left(1/\\delta\\right)}+\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathtt{p p w}}\\epsilon}\\right)\\right)}\\\\ &{=\\bar{\\mathcal{O}}\\left(\\frac{\\sqrt{H}D\\|\\boldsymbol{X}\\|}{\\sqrt{n_{\\mathtt{p p w}}\\epsilon}}+\\frac{\\sqrt{H}D\\|\\boldsymbol{X}\\|}{\\sqrt{n_{\\mathtt{p p b}}}}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathtt{p p b}}}}\\right)\\sqrt{\\boldsymbol{L}({\\boldsymbol w}^{*};\\mathcal{D})}}\\\\ &{+\\,\\bar{\\mathcal{O}}\\left(\\frac{H\\|\\boldsymbol{X}\\|^{2}D^{2}}{n_{\\mathtt{p p b}}\\epsilon}+\\frac{H D^{2}\\|\\boldsymbol{X}\\|^{2}}{n_{\\mathtt{p p b}}}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathtt{p p b}}}\\right)+O\\left(G D\\|\\boldsymbol{X}\\|\\left(\\frac{\\sqrt{n_{\\mathtt{p b}}\\log\\left(1/\\delta\\right)}+\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathtt{p p b}}\\epsilon}+\\frac{\\sqrt{n_{\\mathtt{p p}}\\left(1/\\beta\\right)}+\\sqrt{\\log\\left(8/\\beta\\right)}}{n_{\\mathtt{p p b}}\\epsilon}\\right)\\right)}\\end{array}$ \u03b2) (16)   \n$\\begin{array}{r l}&{=\\tilde{O}\\left(\\frac{\\sqrt{H}D\\,\\|\\boldsymbol{X}\\|}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}}\\right)\\sqrt{L(w^{*};\\mathcal{D})}}\\\\ &{+\\,\\tilde{O}\\left(\\frac{H\\,\\|\\boldsymbol{X}\\|^{2}\\,D^{2}}{n_{\\mathrm{priv}}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}\\right)+O\\left(\\frac{G D\\,\\|\\boldsymbol{X}\\|\\,\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathrm{priv}}\\epsilon}\\right)}\\\\ &{+\\,O\\left(\\left(\\frac{\\sqrt{H}D^{2}\\,\\|\\boldsymbol{X}\\|^{2}\\,G\\sqrt{\\log\\left(1/\\delta\\right)}}{n_{\\mathrm{priv}}\\epsilon}\\right)^{2/3}+\\frac{H^{1/4}D\\,\\|\\boldsymbol{X}\\|\\,\\sqrt{G}\\,\\left(\\log\\left(1/\\delta\\right)\\right)^{1/4}\\,L\\left(w^{*};\\mathcal{D}\\right)^{1/4}}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}\\right)}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nn_{\\mathrm{pub}}=\\tilde{O}\\left(\\frac{(H D\\,\\|\\chi\\|)^{2/3}(n_{\\mathrm{priv}}\\epsilon)^{2/3}}{G^{2/3}(\\log{(1/\\delta)})^{1/3}}+\\frac{\\sqrt{H}n_{\\mathrm{priv}}\\epsilon\\sqrt{L(w^{*};\\mathcal{D})}}{G\\sqrt{\\log{(1/\\delta)})}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This completes the first part of the theorem. For thee second part, we start from Equation (16), ", "page_idx": 29}, {"type": "image", "img_path": "j14wStqZni/tmp/cf7ee15730cb093467eff097678bfe75201f86c7f4f8398e56f0231e556cf976.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "where the second inequality holds form optimality of $w*$ , the third from uniform convergence, Theorem 1 in [SST10] and AM-GM inequality, and the last by plugging in the following public sample complexity. ", "page_idx": 29}, {"type": "equation", "text": "$$\nn_{\\mathrm{pub}}=\\tilde{O}\\left(\\frac{(H D\\,\\|\\mathcal{X}\\|)^{2/3}(n_{\\mathrm{priv}}\\epsilon)^{2/3}}{G^{2/3}(\\log{(1/\\delta)})^{1/3}}+\\frac{\\sqrt{H}n_{\\mathrm{priv}}\\epsilon\\sqrt{\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})}}{G\\sqrt{\\log{(1/\\delta)})}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This completes the proof. ", "page_idx": 29}, {"type": "text", "text": "Theorem 15. In the setting of Theorem $^{l4}$ with the additional assumption that the global minimizer of risk L, w\u2217lies in W, we get that with npub = O\u02dc (HGD2\u2225/3X(\u2225lo)2g/(31(/n\u03b4p)r)iv1\u03f5/)32/3 , with probability at least ", "page_idx": 29}, {"type": "text", "text": "$1-\\beta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\boldsymbol{\\widehat{w}};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{=\\tilde{O}\\left(\\frac{\\sqrt{H}D\\,\\|\\boldsymbol{\\mathcal{X}}\\|}{\\sqrt{n p_{r i v}\\epsilon}}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}}\\right)\\sqrt{L(w^{*};\\mathcal{D})}+\\tilde{O}\\left(\\frac{H\\,\\|\\boldsymbol{\\mathcal{X}}\\|^{2}\\,D^{2}}{n_{p r i v}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}\\right)}\\\\ &{+\\,O\\left(\\frac{G D\\,\\|\\boldsymbol{\\mathcal{X}}\\|\\,\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{p r i v}\\epsilon}\\right)+O\\left(\\left(\\frac{\\sqrt{H}D^{2}\\,\\|\\boldsymbol{\\mathcal{X}}\\|^{2}\\,G\\sqrt{\\log\\left(1/\\delta\\right)}}{n_{p r i v}\\epsilon}\\right)^{2/3}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Further, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{w};{\\mathcal D})-\\widehat{L}(\\widehat{w}^{*};S_{p r i v})}\\\\ &{=\\tilde{O}\\left(\\frac{\\sqrt{H}D\\|{\\mathcal X}\\|}{\\sqrt{n_{p r i v}\\epsilon}}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}}\\right)\\sqrt{\\widehat{L}(\\widehat{w}^{*};S_{p r i v})}+\\tilde{O}\\left(\\frac{H\\|{\\mathcal X}\\|^{2}D^{2}}{n_{p r i v}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}\\right)}\\\\ &{+\\,O\\left(\\frac{G D\\,\\|{\\mathcal X}\\|\\,\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{p r i v}\\epsilon}\\right)+O\\left(\\left(\\frac{\\sqrt{H}D^{2}\\,\\|{\\mathcal X}\\|^{2}\\,G\\sqrt{\\log\\left(1/\\delta\\right)}}{n_{p r i v}\\epsilon}\\right)^{2/3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $w^{*}$ and $\\widehat{w}^{*}$ are population and empirical minimizers with respect to $\\mathcal{D}$ and $S_{p r i\\nu}$ respectively. ", "page_idx": 30}, {"type": "text", "text": "Proof. The proof is almost identical to that of Theorem 15. We repeat the steps pointing out the differences and how the expressions change. We continue till Equation (12). Next, we apply smoothness which results in the key difference between the analyses, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\bar{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{\\leq L(\\bar{w}^{*};\\mathcal{D})-L(w^{*};\\mathcal{D})}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\langle\\phi_{y}^{\\prime}(\\langle w^{*},x\\rangle),\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle\\right\\rangle+\\displaystyle\\frac{H}{2}\\left|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle\\right|^{2}\\right]}\\\\ &{\\leq\\left\\langle\\mathbb{E}\\left[\\phi_{y}^{\\prime}(\\langle w^{*},x\\rangle)x\\right],\\bar{w}^{*}-w^{*}\\right\\rangle+\\displaystyle\\frac{H}{2}\\mathbb{E}\\left[|\\langle\\bar{w}^{*},x\\rangle-\\langle w^{*},x\\rangle|^{2}\\right]}\\\\ &{\\leq\\langle\\nabla L(w^{*};\\mathcal{D}),\\bar{w}^{*}-w^{*}\\rangle+H\\alpha^{2}}\\\\ &{=H\\alpha^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where last equality uses the fact that $\\nabla L(w^{*};\\mathcal{D})\\,=\\,0$ since $w^{*}$ is the unconstrained minimizer. Continuing, we get, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{w};{D})-\\widehat{L}(\\widehat{w};S_{\\mathrm{priv}})\\Big|+\\Big|L(\\widehat{w}^{*};{D})-\\widehat{L}(\\widehat{w}^{*};S_{\\mathrm{priv}})\\Big|}\\\\ &{=\\tilde{O}\\left(\\frac{\\sqrt{H}D\\,\\|\\mathcal{X}\\|}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}}\\right)\\sqrt{L(w^{*};{D})}}\\\\ &{+\\,\\tilde{O}\\left(\\frac{H\\,\\|\\mathcal{X}\\|^{2}\\,D^{2}}{n_{\\mathrm{priv}}\\epsilon}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}\\right)+O\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{\\sqrt{n_{\\mathrm{pub}}\\log\\left(1/\\delta\\right)}+H\\alpha^{2}+\\sqrt{\\log\\left(4/\\beta\\right)}}{n_{\\mathrm{priv}}\\epsilon}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This yields, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.\\left\\langle{{\\bar{\\Phi}};{\\bar{y}}}\\right\\rangle-\\bar{L}({\\bar{\\Psi}}^{*})\\right\\rangle}\\\\ &{=\\bar{\\partial}\\left(\\frac{\\sqrt{H_{D}}\\left|{\\bar{y}}\\right|}{\\sqrt{\\eta_{\\mathrm{m}}\\omega_{c}}}+\\sqrt{\\frac{\\beta\\log\\left({\\bar{y}}/{\\tilde{D}}\\right)}{\\eta_{\\mathrm{m}}}}\\right)\\sqrt{\\bar{L}({\\bar{w}}^{*};{\\bar{D}})}}\\\\ &{+\\bar{O}\\left(\\frac{\\bar{H}\\left|{\\bar{X}}\\right|^{2}\\bar{D}^{2}}{\\eta_{\\mathrm{m}}\\omega_{c}}+H\\alpha^{2}+\\frac{\\bar{B}\\log\\left({\\bar{y}}/{\\tilde{D}}\\right)}{\\eta_{\\mathrm{m}}\\omega_{c}}\\right)+O\\left(G D\\left|{\\bar{x}}\\right|\\left(\\frac{\\sqrt{\\eta_{\\mathrm{m}}\\log\\left(1/\\tilde{\\delta}\\right)}+\\sqrt{\\log\\left(4/\\tilde{\\beta}\\right)}}{\\eta_{\\mathrm{m}}\\omega_{c}}\\right)\\right)}\\\\ &{=\\bar{\\partial}\\left(\\frac{\\sqrt{H_{D}}\\left|{\\bar{y}}\\right|}{\\sqrt{\\eta_{\\mathrm{m}}\\omega_{c}}}+\\sqrt{\\frac{\\sqrt{\\eta_{\\mathrm{m}}\\log\\left({\\bar{x}}/{\\tilde{D}}\\right)}}{\\eta_{\\mathrm{m}}\\omega_{c}}}\\right)\\sqrt{\\bar{L}({\\bar{w}}^{*};{\\bar{D}})}}\\\\ &{+\\bar{O}\\left(\\frac{\\sqrt{H_{D}}\\left|{\\bar{X}}\\right|^{2}\\bar{D}^{2}}{\\eta_{\\mathrm{m}}\\omega_{c}}+\\frac{H D^{2}\\left|{\\bar{y}}\\right|^{2}}{\\eta_{\\mathrm{m}}\\varphi_{\\mathrm{m}}}+\\frac{\\bar{B}\\log\\left({\\bar{x}}/{\\tilde{D}}\\right)}{\\eta_{\\mathrm{m}}\\varphi_{\\mathrm{m}}}\\right)+O\\left(G D\\left|{\\bar{X}}\\right|\\left(\\frac{\\sqrt{\\eta_{\\mathrm{m}}\\log\\left(1/\\tilde{\\delta}\\right)}+\\sqrt{\\log\\left(4/\\tilde{\\beta}\\right)}}{\\eta_{\\mathrm{m}}\\varphi_{\\mathrm{m}}}\\right)\\right.}\\\\ &{-\\bar{O}\\left(\\frac{\\sqrt{H_{D}}\\left|{\\bar{X}}\\right|}{\\sqrt{\\eta_{ \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The public sample complexity is, ", "page_idx": 31}, {"type": "equation", "text": "$$\nn_{\\mathrm{pub}}=\\tilde{O}\\left(\\frac{(H D\\,\\|\\mathcal{X}\\|)^{2/3}(n_{\\mathrm{priv}}\\epsilon)^{2/3}}{G^{2/3}(\\log{(1/\\delta)})^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The second part follows similarly. ", "page_idx": 31}, {"type": "text", "text": "C.3 Lower bounds ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "C.3.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To establish the GD\u221a\u2225X\u2225 term in the lower bound, we consider a one-dimenisonal problem where the loss $\\phi_{y}(\\widehat{y})=-G y\\widehat{y}$ and marginal distribution $D_{x}$ as the point distribution on $\\lVert\\mathcal{X}\\rVert$ such that the overall loss i s $\\underset{x,y}{\\mathbb{E}}\\left[\\ell(w,\\bar{(x,y)})\\right]=\\underset{y}{\\mathbb{E}}\\left[y\\cdot w\\|\\mathcal{X}\\|G\\right]$ . We further set $\\mathcal{W}=[-D,D]$ and consider $\\mathcal{D}_{y}$ to be the distribution which as 1 with probability $\\mathbb{P}\\left[y=1\\right]=(1+\\mu)/2$ and $\\mathbb{P}\\left[y=1\\right]=(1-\\mu)/2$ for some $\\mu\\in[-1,1]$ . Note the minimizer $\\begin{array}{r}{w^{*}=D\\frac{\\mu}{|\\mu|}}\\end{array}$ achieves population risk $-\\mu G D\\|\\mathcal{X}\\|$ . Classic results in information theory establish if $\\mu$ is sampled uniformly from $\\scriptstyle\\{\\pm{\\frac{1}{\\sqrt{6n}}}\\}$ , no algorithm can estimate the sign of $\\mu$ with probability better than $1/2$ (see [Duc23, Section 8.3]). Thus it must be that for any algorithm $\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol{A},\\boldsymbol{S}}\\left[L(\\boldsymbol{A}(\\boldsymbol{S});\\mathcal{D})-\\operatorname*{min}_{\\boldsymbol{w}\\in\\mathbb{R}^{d}}L(\\boldsymbol{w};\\mathcal{D})\\right]=\\Omega\\left(\\frac{G D\\|\\boldsymbol{\\mathcal{X}}\\|}{\\sqrt{n}}\\right),}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "The $\\begin{array}{r}{G D\\left\\|\\mathcal{X}\\right\\|\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n\\epsilon}},\\frac{\\sqrt{d}}{n\\epsilon}\\right\\}}\\end{array}$ term in the lower bound is essentially a corollary of $[\\mathbf{ABG}^{+}22$ , Theorem 6]. We provide further remarks here. The loss function used is, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\ell(w;(x,y))=\\phi_{y}(\\langle w,x\\rangle)=|y-\\langle w,x\\rangle|\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Define $d^{\\prime}:=\\operatorname*{min}(d,n\\epsilon)$ and $\\begin{array}{r}{p:=\\operatorname*{min}\\left(1,\\frac{d^{\\prime}}{n\\epsilon}\\right)}\\end{array}$ . The (known) marginal distribution $\\mathcal{D}_{\\mathcal{X}}$ is described as: with probability $1\\!-\\!p,x={\\vec{0}}$ , otherwise, $\\boldsymbol{x}\\sim\\operatorname{Unif}\\left(\\left\\|\\boldsymbol{\\mathcal{X}}\\right\\|\\left\\{e_{j}\\right\\}_{j=1}^{d^{\\prime}}\\right)$ where $e_{j}$ \u2019s are canonical basis vectors. The (unknown) conditional distribution of the response $y$ is as follows. Sample a \u201cfingerprinting code\u201d, $z^{\\prime}\\in\\left\\{0,1\\right\\}^{d^{\\prime}}$ with mean $\\mu^{\\prime}\\in[0,1]^{d^{\\prime}}$ where each co-ordinate $\\mu_{j}^{\\prime}\\sim\\mathrm{Beta}(0.0625,0.0625)$ i.i.d. Embed $z^{\\prime}$ in $d$ dimensions as $z$ and let $\\mu$ be the corresponding mean vector. Finally, define $\\begin{array}{r}{y\\,=\\,\\frac{D\\langle z,x\\rangle}{\\sqrt{d^{\\prime}}}}\\end{array}$ D\u27e8\u221azd,\u2032x \u27e9. The proof in [ABG+22, Theorem 6] then proceeds by lower bounding the loss by bounding the ability of any differential private algorithm to estimate the fingerprinting code $z$ . ", "page_idx": 31}, {"type": "text", "text": "Since the rank of $\\underbrace{\\mathbb{E}}_{x\\sim\\mathcal{D}_{x}}\\left[x x^{\\top}\\right]=d^{\\prime}$ , the result $[\\mathbf{ABG}^{+}22$ , Theorem 6] then yields a lower bound on the unconstrained excess risk, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A,S}\\left[L(\\boldsymbol{A}(S);\\mathcal{D})-\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}L(w;\\mathcal{D})\\right]=\\Omega\\left(G D\\left\\|\\boldsymbol{\\mathcal{X}}\\right\\|\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n\\epsilon}},\\frac{\\sqrt{d}}{n\\epsilon}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "but also guarantees that the global minimizer has norm at most $D$ . Thus, we achieve the same lower bound for $\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol{A},\\boldsymbol{S}}\\left[L(\\boldsymbol{A}(\\boldsymbol{S});\\bar{\\mathcal{D}})-\\operatorname*{min}_{w\\in\\mathcal{W}}L(w;\\mathcal{D})\\right]}\\end{array}$ by setting $\\mathcal{W}$ to be the ball of radius $D$ . ", "page_idx": 32}, {"type": "text", "text": "C.3.2 Proof of Theorem 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The proof uses the lower bound instance in the DP-SCO lower bound with public data, Theorem 1. We consider the case where $\\mathcal{D}_{y}$ is the point distribution on 1. Then for any $y\\in\\mathcal{V}$ , $\\mathcal{V}=\\{1\\}$ , the loss function is then $\\ell(w;(x,y))=y\\left\\langle w,x\\right\\rangle=\\left\\langle w,x\\right\\rangle$ , as in Theorem 1. Hence, a labeled and unlabeled sample have the same information. We also set $\\mathcal{W}$ to be the ball of radius $D$ . ", "page_idx": 32}, {"type": "text", "text": "Assume by contradiction there exists an $(\\epsilon,\\delta)$ -PA-DP algorithm, $\\boldsymbol{\\mathcal{A}}$ , which achieves rate $\\begin{array}{r}{O\\left(G D\\|\\mathcal{X}\\|(\\frac{1}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{\\sqrt{\\log(1/\\delta)}}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}})\\right)}\\end{array}$ with $o\\big(n_{\\mathrm{priv}}\\epsilon/\\log\\left(1/\\delta\\right)\\big)$ public samples. Since $\\begin{array}{r l r}{n_{\\mathrm{pub}}}&{{}=}&{o(n_{\\mathrm{priv}}\\epsilon/\\log\\left(1/\\delta\\right))}\\end{array}$ and $\\begin{array}{r l r}{\\dot{d}}&{{}=}&{\\omega(n\\epsilon)}\\end{array}$ , Theorem 1 gives a lower bound on $\\begin{array}{r}{\\mathbb{E}\\left[A(X_{\\mathrm{pub}},S_{\\mathrm{priv}};\\mathcal{D})-\\operatorname*{min}_{w\\in\\mathcal{W}}\\left\\{L(w;\\mathcal{D})\\right\\}\\right]}\\end{array}$ of ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Omega\\left(G D\\|\\mathcal{X}\\|\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{n_{\\mathrm{pub}}}},\\frac{\\sqrt{d\\log\\left(1/\\delta\\right)}}{n_{\\mathrm{priv}}\\epsilon}\\right\\}\\right)=\\omega\\left(G D\\|\\mathcal{X}\\|\\frac{\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{n_{\\mathrm{priv}}\\epsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\epsilon\\leq1$ , this is a contradiction. ", "page_idx": 32}, {"type": "text", "text": "D Missing proofs for Section 4.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "D.1 Proof of Theorem 7 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. The privacy proof follows from the guarantee of exponential mechanism [MT07]. In particular, the sensitivity of the score function is at most ${\\frac{2}{n_{\\mathrm{priv}}}}\\operatorname*{min}\\left(B,G R\\right)$ where the first follows from the loss bound of $B$ and the second from the Lipschitzness and bound on predictors. Let $h^{*}\\in\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}L(h;\\mathcal{D})$ and $\\tilde{h}^{*}\\in\\mathrm{arg}\\,\\mathrm{min}_{h\\in\\tilde{\\mathcal{H}}}\\,L(\\bar{h};\\bar{\\mathcal{D}})$ . From standard analysis based on uniform convergence, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L(\\widehat{h};\\mathcal{D})-L(h^{*};\\mathcal{D})=L(\\widehat{h};\\mathcal{D})-L(\\widehat{h};S_{\\mathrm{pres}})+\\widehat{L}(\\widehat{h};S_{\\mathrm{pres}})-L(h^{*};\\mathcal{D})}&{}\\\\ {\\leq\\underset{h\\in\\mathcal{H}}{\\operatorname*{sup}}\\left(L(h;\\mathcal{D})-\\widehat{L}(h;S_{\\mathrm{pres}})\\right)+L(\\widehat{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})}&{}\\\\ {+\\widehat{L}(\\widehat{h}^{*};S_{\\mathrm{pens}})-L(\\widehat{h}^{*};\\mathcal{D})+\\widehat{L}(\\widehat{h};S_{\\mathrm{pres}})-\\widehat{L}(\\widehat{h}^{*};S_{\\mathrm{pres}})}&{}\\\\ {\\leq2\\underset{h\\in\\mathcal{H}}{\\operatorname*{sup}}\\left|L(h;\\mathcal{D})-\\widehat{L}(h;S_{\\mathrm{pres}})\\right|+L(\\widehat{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})}&{}\\\\ {+\\widehat{L}(\\widehat{h};S_{\\mathrm{pens}})-\\underset{h\\in\\mathcal{H}}{\\operatorname*{min}}\\widehat{L}(h;S_{\\mathrm{pens}})}&{}\\\\ {\\leq2G\\widehat{\\Phi}_{{\\mathrm{spens}}}(\\mathcal{R})+O\\left(\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{\\eta_{\\mathrm{pres}}}}\\right)}&{}\\\\ {+O\\left(\\frac{\\operatorname*{min}\\left(B,G R\\right)\\left(\\log\\left(\\left|\\widehat{H}\\right|\\right)+\\log\\left(4/\\beta\\right)\\right)}{n\\eta_{\\mathrm{ept}}\\epsilon}\\right)+L(\\widehat{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the above holds with probability at least $1-\\beta/2$ and follows from guarantee of exponential mechanism [MT07] and uniform convergence ([SSBD14], see Theorem 16). We further have that $\\log\\Big(|\\tilde{\\mathcal{H}}|\\Big)=\\tilde{O}(\\mathrm{fat}_{c\\alpha}(\\mathcal{H}))$ from Lemma 7. ", "page_idx": 32}, {"type": "text", "text": "For the ${\\cal L}(\\tilde{h}^{*};{\\cal D})-{\\cal L}(h^{*};{\\cal D})$ term, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\tilde{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})\\leq L(\\bar{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})}\\\\ &{\\leq G\\mathbb{E}\\big|\\bar{h}^{*}(x)-h^{*}(x)\\big|}\\\\ &{\\leq G\\sqrt{\\mathbb{E}\\big|\\bar{h}^{*}(x)-h^{*}(x)\\big|^{2}}}\\\\ &{\\leq2G\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first step holds for any $\\begin{array}{r l r}{\\bar{h}^{*}}&{{}\\in}&{\\tilde{\\mathcal{H}}}\\end{array}$ by optimaility of $\\tilde{h}^{*}$ over $\\tilde{\\mathcal{H}}$ , the second holds from the $G$ -Lipschitzness of the loss function, the third from Jensen\u2019s inequality. The final step holds with probability $1\\:-\\:\\beta/2$ from Lemma 1 with $\\begin{array}{r l}{n_{\\mathrm{pub}}}&{{}=}\\end{array}$ $\\begin{array}{r}{O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\end{array}$ together with the fact that since $\\tilde{\\mathcal{H}}$ is an \u03b1-cover of H, hence there exists \u00afh\u2217\u2208H\u02dc with  \u00afh\u2217\u2212h\u2217  2,Xpub \u2264\u03b1. ", "page_idx": 33}, {"type": "text", "text": "Plugging the above in Equation (19), we get with probability at least $1-\\beta$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-L(h^{*};\\mathcal{D})\\leq2G\\mathfrak{R}_{n_{\\mathrm{priv}}}(\\mathcal{H})+O\\left(\\frac{B\\sqrt{\\log{(4/\\beta)}}}{\\sqrt{n_{\\mathrm{priv}}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\,O\\left(\\frac{\\operatorname*{min}{(B,G R)}(\\mathrm{fat}_{c\\alpha}(\\mathcal{H})+\\log{(4/\\beta)})}{n_{\\mathrm{priv}}\\epsilon}\\right)+2G\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 33}, {"type": "text", "text": "Theorem 16. [SSBD14] Let $\\mathcal{H}\\subseteq[-R,R]^{\\chi}$ . For any $G$ -Lipschitz, $B$ -bounded loss function, any probability distribution $\\mathcal{D}$ over $\\mathcal X\\times\\mathcal X$ , given m i.i.d. samples from $S$ , with probability at least $1-\\beta$ , the following holds for all $h\\in\\mathcal H$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|L(h;\\mathcal{D})-\\widehat{L}(h;S)\\right|\\leq2G\\Re_{m}(\\mathcal{H})+O\\left(B\\sqrt{\\frac{\\log{(4/\\beta)}}{m}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. This is a classical result in learning theory which follows directly Theorem 26.5 in [SSBD14] together with the contraction lemma (Lemma 26.9 in [SSBD14]) for Lipschitz losses. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Lemma 7. Let $\\tilde{\\mathcal{H}}$ be an $\\alpha$ -cover of $\\mathcal{H}$ with respect to $\\lVert\\cdot\\rVert_{2,X_{p u b}}$ . The size of $\\tilde{\\mathcal{H}}$ is bounded as, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log\\left(\\left|\\tilde{\\mathcal{H}}\\right|\\right)\\leq f\\!a t_{c\\alpha}(\\mathcal{H})\\log\\left(\\frac{2R}{\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where c is an absolute constant. ", "page_idx": 33}, {"type": "text", "text": "Proof. This follows directly from Theorem 8, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log\\left(\\left|\\tilde{\\mathcal{H}}\\right|\\right)=\\mathcal{N}_{2}\\left(\\mathcal{H},\\alpha,S_{\\mathrm{pub}}\\right)\\leq\\mathcal{N}_{2}\\left(\\mathcal{H},\\alpha,n_{\\mathrm{pub}}\\right)\\leq\\mathrm{fat}_{c\\alpha}(\\mathcal{H})\\log\\left(\\frac{2R}{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma 1 For $h\\in\\mathcal H$ , let $\\begin{array}{r}{\\tilde{h}\\in\\arg\\operatorname*{min}_{\\bar{h}\\in\\tilde{\\mathcal{H}}}\\|\\bar{h}-h\\|_{2,X_{\\mathrm{pub}}}}\\end{array}$ . Since $\\tilde{\\mathcal{H}}$ is an $\\tau$ -cover, this gives us that $\\|h-\\tilde{h}\\|_{2,X_{\\mathrm{pub}}}\\leq\\tau$ . We have, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|h-\\tilde{h}\\|_{2,\\mathcal{D}_{x}}^{2}=\\mathbb{E}\\left|h(x)-\\tilde{h}(x)\\right|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left|h(x)-\\tilde{h}(x)\\right|^{2}-\\frac{1}{n_{\\mathrm{pub}}}\\sum_{x\\in S_{\\mathrm{pub}}}\\left|h(x)-\\tilde{h}(x)\\right|^{2}+\\frac{1}{n_{\\mathrm{pub}}}\\sum_{x\\in S_{\\mathrm{pub}}}\\left|h(x)-\\tilde{h}(x)\\right|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{\\tilde{h}\\in\\mathcal{H}}{\\operatorname*{sup}}\\left(\\mathbb{E}\\left|h(x)-\\bar{h}(x)\\right|^{2}-\\frac{1}{n_{\\mathrm{pub}}}\\sum_{x\\in S_{\\mathrm{pub}}}\\left|h(x)-\\bar{h}(x)\\right|^{2}\\right)+\\tau^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The first term above can be seen as uniform deviation between the empirical and population risk, of another prediction problem, with squared loss, in the the realizable setting (with the responses generated by $h$ ). The squared loss is $\\frac{1}{2}$ -smooth and non-negative, so we can apply result of Theorem 1 in [SST10] instantiated in the realizable setting, which gives us that with probability at least $1-\\beta$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|h-\\tilde{h}\\|_{2.{\\mathcal D}_{\\mathcal X}}^{2}=O\\left(\\log^{3}(n_{\\mathrm{pub}})\\mathfrak R_{n_{\\mathrm{pub}}}^{2}(\\mathcal H)+\\frac{R^{2}\\log{(1/\\beta)}}{n_{\\mathrm{pub}}}\\right)+\\tau^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Choosing $n_{\\mathrm{pub}}$ such that $\\begin{array}{r}{n_{\\mathrm{pub}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(1/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\end{array}$ , we get the claimed result. ", "page_idx": 34}, {"type": "text", "text": "D.2 Optimistic rates with smooth non-negative losses ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Algorithm 2 achieves optimistic rates on risk depending on realizability/interpolation conditions, that is, whenever $L(h^{*};{\\mathcal{D}})$ or $\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})$ is small. ", "page_idx": 34}, {"type": "text", "text": "Theorem 17. Algorithm 2 with $\\begin{array}{r l r}{\\gamma}&{{}=}&{\\frac{2B}{n_{p r i\\nu}\\epsilon}}\\end{array}$ satisfies $\\epsilon$ -PA-DP. For $\\begin{array}{r l r l}{n_{p u b}}&{{}}&{=}\\end{array}$ $\\begin{array}{r}{O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:l o g^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)\\ <\\ \\infty}\\end{array}$ and any $\\alpha~>~0,$ , with probability at least $1-\\beta$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-L(h^{*};\\mathcal{D})=\\tilde{O}\\left(\\sqrt{H}\\mathfrak{R}_{n_{p r i v}}(\\mathcal{H})+\\sqrt{H}\\alpha+\\sqrt{\\frac{B\\log{(8/\\delta)}}{n_{p r i v}}}\\right)\\sqrt{L(h^{*};\\mathcal{D})}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\tilde{O}\\left(H\\mathfrak{R}_{n_{p r i v}}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log{(8/\\beta)}}{n_{p r i v}}+\\frac{B(f a t_{c\\alpha}(\\mathcal{H})+\\log{(4/\\beta)})}{n_{p r i v}\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-\\widehat{L}(\\widehat{h}^{*};S_{p r i v})\\leq\\tilde{O}\\left(\\sqrt{H}\\mathfrak{R}_{n_{p r i v}}(\\mathcal{H})+\\sqrt{H}\\alpha+\\sqrt{\\frac{B\\log\\left(8/\\delta\\right)}{n_{p r i v}}}\\right)\\sqrt{\\widehat{L}(\\widehat{h}^{*};S_{p r i v})}}\\\\ &{\\qquad\\qquad\\qquad+\\tilde{O}\\left(H\\mathfrak{R}_{n_{p r i v}}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{p r i v}}+\\frac{B(f a t_{c\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right))}{n_{p r i v}\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $h^{*}$ and h\u2217are population and empirical minimizers with respect to $\\mathcal{D}$ and $S_{p r i\\nu}$ respectively, and $c$ is an absolute constant. ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem $I7.$ . The privacy proof is the same as that of Theorem 7. For utility, let $h^{*}\\ \\in$ ar $\\scriptstyle\\operatorname{g}\\operatorname*{min}_{h\\in{\\mathcal{H}}}{\\cal L}(h;{\\mathcal{D}})$ and $\\begin{array}{r}{\\tilde{h}^{*}\\in\\arg\\operatorname*{min}_{h\\in\\tilde{\\mathcal{H}}}L(h;\\mathcal{D})}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "We start with the proof of the first part of the theorem. We have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-L(h^{*};\\mathcal{D})=L(\\widehat{h};\\mathcal{D})-\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})+\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})-L(h^{*};\\mathcal{D})}\\\\ &{\\qquad\\qquad\\qquad\\leq L(\\widehat{h};\\mathcal{D})-\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})+\\widehat{L}(\\widetilde{h}^{*};S_{\\mathrm{priv}})-L(\\widetilde{h}^{*};\\mathcal{D})}\\\\ &{\\qquad\\qquad+\\,L(\\widetilde{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})+\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})-\\widehat{L}(\\widetilde{h}^{*};S_{\\mathrm{priv}})}\\\\ &{\\qquad\\qquad\\leq\\left|L(\\widehat{h};\\mathcal{D})-\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})\\right|+\\left|L(\\widetilde{h}^{*};\\mathcal{D})-\\widehat{L}(\\widetilde{h}^{*};S_{\\mathrm{priv}})\\right|}\\\\ &{\\qquad\\qquad+\\,L(\\widetilde{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})+\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})-\\widehat{L}(\\widetilde{h}^{*};S_{\\mathrm{priv}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "From the guarantee of exponential mechanism together with $\\log\\Big(\\Big|\\tilde{\\mathcal{H}}\\Big|\\Big)\\;=\\;\\tilde{O}(\\mathrm{fat}_{c\\alpha}(\\mathcal{H}))$ from Lemma 7, we have that with probability at least $1-\\beta/4$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})-\\widehat{L}(\\widetilde{h}^{*};S_{\\mathrm{priv}})\\le\\widehat{L}(\\widehat{h};S_{\\mathrm{priv}})-\\displaystyle\\operatorname*{min}_{h\\in\\widehat{\\mathcal{H}}}\\widehat{L}(h;S_{\\mathrm{priv}})=O\\left(\\frac{B(\\log\\left(\\left|\\widetilde{\\mathcal{H}}\\right|\\right)+\\log\\left(4/\\beta\\right))}{n_{\\mathrm{priv}}\\epsilon}\\right)}&{{}}&{}\\\\ {=\\tilde{O}\\left(\\frac{B\\left(\\mathrm{fat}_{c\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right)\\right)}{n_{\\mathrm{priv}}\\epsilon}\\right)}&{{}}&{(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the ${\\cal L}(\\tilde{h}^{*};{\\cal D})-{\\cal L}(h^{*};{\\cal D})$ term in Equation (20), we apply smoothness to get, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\langle\\hat{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})\\leq L(\\hat{h}^{*};\\mathcal{D})-L(h^{*};\\mathcal{D})}}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\langle\\phi_{y}^{*}(h^{*}(x)),\\bar{h}^{*}(x)-h^{*}(x)\\right\\rangle+\\frac{H}{2}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left|\\phi_{y}^{*}(h^{*}(x))\\right|\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|+\\frac{H}{2}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|^{2}\\right]}\\\\ &{\\leq\\sqrt{\\mathbb{E}\\left|\\phi_{y}^{*}(h^{*}(x))\\right|^{2}}\\sqrt{\\mathbb{E}_{x\\sim\\mathcal{D}x}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|^{2}}+\\frac{H}{2}\\mathbb{E}_{x\\sim\\mathcal{D}x}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|}\\\\ &{\\leq2\\sqrt{H\\mathbb{E}_{x\\sim\\mathcal{D}}\\phi_{y}(h^{*}(x))}\\sqrt{\\mathbb{E}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|^{2}}+\\frac{H}{2}\\mathbb{E}_{x\\sim\\mathcal{D}x}\\left|\\bar{h}^{*}(x)-h^{*}(x)\\right|^{2}}\\\\ &{\\leq2\\sqrt{H L(h^{*};\\mathcal{D})}\\alpha+H\\alpha^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the above holds for any $\\bar{h}^{*}\\,\\in\\,\\tilde{\\mathcal{H}}$ . The second inequality holds from $H$ -smoothness, the third and fourth from Cauchy-Schwarz, the fifth from self-bounding property of smooth nonnegative losses [SST10]. The final step holds with probability $1\\bar{-\\beta^{\\prime}}2$ from Lemma 1 with $\\begin{array}{r}{\\dot{n_{\\mathrm{pub}}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log(2/\\beta)}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{\\dot{H}})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\end{array}$ together with the fact that since H\u02dc is an \u03b1-cover of H, so there exists \u00afh\u2217\u2208H\u02dc with  \u00afh\u2217\u2212h\u2217  2,Xpub \u2264\u03b1. ", "page_idx": 35}, {"type": "text", "text": "An application of AM-GM inequality further yields, ", "page_idx": 35}, {"type": "equation", "text": "$$\nL(\\tilde{h}^{*};{\\cal D})\\leq2L(h^{*};{\\cal D})+2H\\alpha^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The first two terms in Equation (20) are bound using uniform convergence for smooth non-negative losses, Theorem 1 in [SST10] and Bernstein\u2019s inequality as follows; with probability at least $1-\\beta/4$ , we have, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\hat{x};\\hat{x})-\\hat{L}(\\hat{x},\\hat{x}_{m})+\\left|\\hat{L}(\\hat{x};\\hat{x}_{m})-\\hat{L}(\\hat{x};\\hat{x}_{m})\\right|}\\\\ &{=\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\sqrt{\\frac{2M\\cos(\\theta)}{n\\hbar\\theta_{m}}}\\right)\\left(\\sqrt{\\hat{L}(\\hat{x};\\hat{x}_{m})}+\\sqrt{L(\\hat{x};\\hat{x}_{m})}\\right)}\\\\ &{+\\partial\\left(\\sqrt{\\alpha}\\sin(\\theta_{m})^{2}+\\frac{B\\cos(\\theta)}{n\\hbar\\theta_{m}}\\right)}\\\\ &{=\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\sqrt{\\frac{2M\\cos(\\theta)}{n\\hbar\\theta_{m}}}\\right)\\left(\\sqrt{\\hat{L}(\\hat{x};\\hat{x}_{m})}+\\sqrt{L(\\hat{x};\\hat{x}_{m})}\\right)}\\\\ &{+\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\frac{B\\cos(\\theta)}{n\\hbar\\theta_{m}}\\right)+\\partial\\left(\\frac{B(\\theta_{m};\\hat{x}_{m})+B\\cos(\\theta_{m})}{n\\hbar\\theta_{m}}\\right)}\\\\ &{=\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\sqrt{\\frac{2M\\cos(\\theta_{m})}{n\\hbar\\theta_{m}}}\\right)\\sqrt{\\hat{L}(\\hat{x};\\hat{x}_{m})}}\\\\ &{+\\partial\\left(\\sqrt{\\alpha}\\frac{\\partial(\\hat{x})}{\\partial\\sin(\\theta_{m})}+\\frac{B\\cos(\\theta_{m})}{n\\hbar\\theta_{m}}\\right)+\\partial\\left(\\frac{B(\\theta_{m};\\hat{x}_{m})+B\\cos(\\theta_{m})}{n\\hbar\\theta_{m}}\\right)}\\\\ &{=\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\sqrt{\\frac{2M\\cos(\\theta_{m})}{n\\hbar\\theta_{m}}}\\right)\\sqrt{\\hat{L}(\\hat{x};\\hat{x}_{m})}}\\\\ &{=\\partial\\left(\\sqrt{H^{3}w_{m}(t)}+\\sqrt{\\frac{\\hbar\\theta_{m}}{n\\hbar\\theta_{m}}}\\right)\\sqrt{\\hat{L}(\\hat{x};\\hat{x}_{m})}}\\\\ &{+\\partial\\left(\\sqrt{\\alpha}\\frac{\\partial(\\hat{x};\\hat{x}_{m})+B\\cos(\\\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second equality follows from Equati\u221aon (21), concavity of $x\\mapsto{\\sqrt{x}}$ and AM-GM inequality. The third equality follows concavity of $x\\mapsto{\\sqrt{x}}$ and Bernstein\u2019s inequality, the fourth follows from Equation (23) and AM-GM inequality. ", "page_idx": 35}, {"type": "text", "text": "Plugging the above, Equation (22) and Equation (21) into Equation (20) yields the following with probability at least $1-\\beta$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-L(h^{*};\\mathcal{D})}\\\\ &{=\\tilde{O}\\left(\\sqrt{H}\\mathfrak{R}_{n_{\\mathrm{priv}}}(\\mathcal{H})+\\sqrt{H}\\alpha+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}}\\right)\\sqrt{L(h^{*};\\mathcal{D})}}\\\\ &{+\\,\\tilde{O}\\left(H\\mathfrak{R}_{n_{\\mathrm{priv}}}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathrm{priv}}}+\\frac{B\\left(\\operatorname{fat}_{c\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right)\\right)}{n_{\\mathrm{priv}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This completes the first part of the theorem. For the second part, we proceed from Equation (25) onwards ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{L(\\widehat{h};D)\\leq L(h^{*};D)+\\bar{O}\\left(\\sqrt{H\\mathfrak{R}_{n\\varphi}(M)+\\sqrt{H}\\alpha}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}}\\right)\\sqrt{L(h^{*};D)}}}\\\\ &{+\\,\\bar{O}\\left(H\\mathfrak{R}_{n\\varphi}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}+\\frac{B(\\tan_{\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right))}{n_{\\mathfrak{p h}}\\epsilon}\\right)}\\\\ &{\\leq L(\\widehat{h}^{*};D)+\\bar{O}\\left(\\sqrt{H\\mathfrak{R}_{n\\varphi}(M)+\\sqrt{H}\\alpha}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}}\\right)\\sqrt{L(\\widehat{h}^{*};D)}}\\\\ &{+\\,\\bar{O}\\left(H\\mathfrak{R}_{n\\varphi}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}+\\frac{B(\\tan_{\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right))}{n_{\\mathfrak{p h}}\\epsilon}\\right)}\\\\ &{\\leq\\widehat{L}(\\widehat{h}^{*};S_{n\\varphi})+\\bar{O}\\left(\\sqrt{H\\mathfrak{R}_{n\\varphi}(\\mathcal{H})+\\sqrt{H}\\alpha}+\\sqrt{\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}}\\right)\\sqrt{\\widehat{L}(\\widehat{h}^{*};S_{n\\varphi})}}\\\\ &{+\\,\\bar{O}\\left(H\\mathfrak{R}_{n\\varphi}^{2}(\\mathcal{H})+H\\alpha^{2}+\\frac{B\\log\\left(8/\\beta\\right)}{n_{\\mathfrak{p h}}}+\\frac{B(\\tan_{\\alpha}(\\mathcal{H})+\\log\\left(4/\\beta\\right))}{n_{\\mathfrak{p h}}\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the second inequality follows form optimality of $h^{*}$ , the third from uniform convergence, Theorem 1 in [SST10] and AM-GM inequality. This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "D.3 Proof of Corollary 2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We use the result from [GRS18], restated as Theorem 10. Further, note that range bound on the hypothesis class is simply $\\begin{array}{r}{R\\,\\leq\\,\\|\\mathcal{X}\\|\\prod_{j=1}^{d}R_{j}}\\end{array}$ . Instantiating our general result Theorem 7 with the above together with the relation between fat-shattering dimension and Rademacher complexity (Theorem 9) yields the following excess risk bound, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-\\underset{h\\in\\mathcal{H}}{\\mathrm{min}}L(h;\\mathcal{D})}\\\\ &{=O\\left(\\frac{G\\left\\|\\mathcal{X}\\right\\|\\left(\\sqrt{2\\log\\left(2\\right)M}+1\\right)\\prod_{j=1}^{M}R_{j}}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{B\\log\\left(4/\\beta\\right)}{n_{\\mathrm{priv}}\\epsilon}\\right)}\\\\ &{+\\,\\widetilde{O}\\left(\\left(\\frac{B G^{2}(\\sqrt{2\\log\\left(2\\right)M}+1)^{2}\\left\\|\\mathcal{X}\\right\\|^{2}\\left(\\prod_{j=1}^{M}R_{j}\\right)^{2}}{n_{\\mathrm{priv}}\\epsilon}\\right)^{1/3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where in the above, we set $\\begin{array}{r}{\\alpha=\\bigg(\\frac{B(\\sqrt{2\\log(2)M}+1)^{2}\\|\\mathcal{X}\\|^{2}(\\prod_{j=1}^{M}R_{j})^{2}}{G n_{\\mathrm{priv}}\\epsilon}\\bigg)^{1/3}.}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "The number of public samples then is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{\\mathrm{pub}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log{(2/\\beta)}}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left(\\|\\mathcal{X}\\|^{2}(\\displaystyle\\prod_{j=1}^{M}R_{j})^{2}\\operatorname*{max}\\left(\\frac{\\log{(2/\\beta)}}{\\alpha^{2}},\\frac{M}{\\alpha^{2}}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left((\\|\\mathcal{X}\\|(\\displaystyle\\prod_{j=1}^{M}R_{j}))^{2/3}(n_{\\mathrm{piv}}\\epsilon)^{2/3}M^{1/3}\\log{(2/\\beta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "D.4 Proof of Corollary 3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Note that $R\\leq D\\left\\|{\\mathcal{X}}\\right\\|$ . Further, we have [KST08, FGV17], ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Re_{m}(\\mathcal{H})=O\\left(\\frac{D\\left\\lVert\\boldsymbol{\\mathcal{X}}\\right\\rVert}{m^{1/r}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, we have from Theorem 9, for any $\\alpha>\\Re_{m}(\\mathcal{H})$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{fat}_{\\alpha}(\\mathcal{H})\\leq\\frac{4m\\Re_{m}^{2}(\\mathcal{H})}{\\alpha^{2}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Choose $m=n_{\\mathrm{pub}}$ and $\\alpha=\\left(\\log\\left(n_{\\mathrm{pub}}\\right)\\right)^{3/2}\\mathfrak{R}_{n_{\\mathrm{pub}}}(\\mathcal{H})$ , to get that f $\\mathfrak{u}_{\\alpha}(\\mathcal{H})=\\tilde{O}\\left(n_{\\mathrm{pub}}\\right)$ . Plugging this in Theorem 7, we get, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\because}{\\underset{w\\in\\mathcal{W}}{\\sum}}-\\underset{w\\in\\mathcal{W}}{\\operatorname*{min}}\\,L(w;\\mathcal{D})=O\\left(\\frac{G D\\,\\|\\mathcal{X}\\|}{n_{\\mathtt{p h}}\\mathrm{\\\"=}}+\\frac{G D\\,\\|\\mathcal{X}\\|\\mathrm{~\\mathcal{N}\\mathrm{og}~}(4/\\beta)}{\\sqrt{n_{\\mathtt{p h}}}}+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathtt{p h}}}}\\right)}\\\\ &{\\phantom{\\overset{\\quad,}{\\quad\\quad\\quad}}+\\Tilde{O}\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{n_{\\mathtt{p h}}}{n_{\\mathtt{p h}}\\mathrm{\\\"=}}+\\frac{1}{n_{\\mathtt{p h}}\\mathrm{\\\"=}}+\\frac{\\log\\left(4/\\beta\\right)}{n_{\\mathtt{p h}}\\mathrm{\\'=}}\\right)\\right)}\\\\ &{\\phantom{\\overset{\\quad,}{\\quad\\quad\\quad}}=\\Tilde{O}\\left(G D\\,\\|\\mathcal{X}\\|\\left(\\frac{1}{n_{\\mathtt{p h}}\\mathrm{\\\"=}}+\\frac{\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathtt{p h}}\\mathrm{\\\"=}}}+\\frac{\\log\\left(2/\\beta\\right)}{\\left(n_{\\mathtt{p h}}\\mathrm{\\'=}\\right)^{\\frac{1}{\\mathrm{+}}}}+\\frac{\\log\\left(4/\\beta\\right)}{n_{\\mathtt{p h}}\\mathrm{\\'=}}\\right)\\right)}\\\\ &{\\phantom{\\overset{\\quad,}{\\quad\\quad\\quad}}+O\\left(\\frac{B\\,\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathtt{p h}}\\mathrm{\\\"=}}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where in the last step, we plug in $\\begin{array}{r l r}{n_{\\mathrm{pub}}}&{=}&{O\\left((n_{\\mathrm{priv}}\\epsilon)^{r/(1+r)}\\log\\left(2/\\beta\\right)\\right)}\\end{array}$ , yielding $\\alpha\\quad=$ $\\begin{array}{r}{O\\left(\\frac{D\\|\\mathcal{X}\\|}{\\left(n_{\\mathrm{priv}}\\epsilon\\right)^{\\frac{1}{r+1}}}\\right)}\\end{array}$ . The number of public samples simplifies as, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{\\mathrm{pub}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log{(2/\\beta)}}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left((n_{\\mathrm{priv}}\\epsilon)^{\\frac{2}{r+1}}\\log{(2/\\beta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which is satisfied from our choice since $r\\geq2$ . ", "page_idx": 37}, {"type": "text", "text": "D.5 Additional Results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Corollary 4. In the setting of Theorem 7 together with $\\begin{array}{r l r}{\\mathcal{X}}&{{}=}&{\\left\\{{\\boldsymbol{x}}\\in\\mathbb{R}^{d}:\\|{\\boldsymbol{x}}\\|\\leq\\|{\\boldsymbol{\\mathcal{X}}}\\|\\right\\}}\\end{array}$ and $\\begin{array}{r l r}{\\mathcal{H}}&{=}&{\\left\\{x\\mapsto\\left\\langle w,x\\right\\rangle,x\\in\\mathcal{X},\\|w\\|\\leq D\\right\\}}\\end{array}$ . With $\\begin{array}{r l r}{\\alpha}&{{}=}&{\\left(\\frac{D^{2}\\|\\boldsymbol{\\mathcal{X}}\\|^{2}}{n_{p r i\\nu}\\epsilon}\\right)^{1/3}}\\end{array}$ and $\\begin{array}{r l}{n_{p u b}}&{{}=}\\end{array}$ $\\tilde{O}\\left((D\\,\\|\\boldsymbol{\\mathcal{X}}\\|)^{2/3}(n_{p r i\\nu}\\epsilon)^{2/3}\\log\\left(2/\\beta\\right)\\right)$ , with probability at least $1-\\beta$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\therefore({\\widehat{h}};{\\mathcal{D}})-\\operatorname*{min}_{h\\in{\\mathcal{H}}}L(h;{\\mathcal{D}})={\\tilde{O}}\\left({\\frac{G D\\left\\|{\\boldsymbol{X}}\\right\\|}{\\sqrt{n_{p r i v}}}}+G\\left({\\frac{D^{2}\\left\\|{\\boldsymbol{X}}\\right\\|^{2}}{n_{p r i v}}}\\right)^{1/3}+{\\frac{B{\\sqrt{\\log\\left(4/\\beta\\right)}}}{\\sqrt{n_{p r i v}}}}+{\\frac{B\\log\\left(4/\\beta\\right)}{n_{p r i v}\\epsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. In this setting, we have that $R\\leq D\\left\\|{\\mathcal{X}}\\right\\|$ . Further, it is known [KST08], ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathsf{f a t}_{\\alpha}(\\mathcal{H})=O\\left(\\frac{D^{2}\\left\\|\\mathcal{X}\\right\\|^{2}}{\\alpha^{2}}\\right),\\qquad\\Re_{n}(\\mathcal{H})=O\\left(\\frac{D\\left\\|\\mathcal{X}\\right\\|}{\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Plugging this in Theorem 7, we get, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\widehat{h};\\mathcal{D})-\\underset{h\\in\\mathcal{H}}{\\mathrm{min}}\\,L(h;\\mathcal{D})}\\\\ &{=O\\left(\\frac{G D\\,\\|\\mathcal{X}\\|}{\\sqrt{n_{\\mathrm{priv}}}}\\right)+O\\left(\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathrm{priv}}}}\\right)+\\tilde{O}\\left(\\frac{\\operatorname*{min}\\left(B,G D\\,\\|\\mathcal{X}\\|\\right)D\\,\\|\\mathcal{X}\\|}{\\alpha^{2}n_{\\mathrm{priv}}\\epsilon}\\right)}\\\\ &{+\\,2G\\alpha+O\\left(\\frac{B\\log\\left(4/\\beta\\right)}{n_{\\mathrm{priv}}\\epsilon}\\right)}\\\\ &{=\\tilde{O}\\left(\\frac{G D\\,\\|\\mathcal{X}\\|}{\\sqrt{n_{\\mathrm{priv}}}}+G\\left(\\frac{D^{2}\\,\\|\\mathcal{X}\\|^{2}}{n_{\\mathrm{priv}}\\epsilon}\\right)^{1/3}+\\frac{B\\sqrt{\\log\\left(4/\\beta\\right)}}{\\sqrt{n_{\\mathrm{priv}}}}+\\frac{B\\log\\left(4/\\beta\\right)}{n_{\\mathrm{priv}}\\epsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where in the last step, we plug in $\\begin{array}{r}{\\alpha=\\left(\\frac{D^{2}\\|\\mathcal{X}\\|^{2}}{n_{\\mathrm{priv}}\\epsilon}\\right)^{1/3}}\\end{array}$ . The number of public samples simplifies as, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{\\mathrm{pub}}=O\\left(\\operatorname*{max}\\left(\\frac{R^{2}\\log{(2/\\beta)}}{\\alpha^{2}},\\operatorname*{min}\\left\\{m:\\log^{3}(m)\\Re_{m}^{2}(\\mathcal{H})\\leq\\alpha^{2}\\right\\}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left(D^{2}\\left\\|\\boldsymbol{\\mathcal{X}}\\right\\|^{2}\\operatorname*{max}\\left(\\frac{\\log{(2/\\beta)}}{\\alpha^{2}},\\frac{1}{\\alpha^{2}}\\right)\\right)}\\\\ &{\\qquad=\\tilde{O}\\left((D\\left\\|\\boldsymbol{\\mathcal{X}}\\right\\|)^{2/3}(n_{\\mathrm{priv}}\\epsilon)^{2/3}\\log{(2/\\beta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Claims made are given in Sections 3 and 4 proofs are provided in the Appendix Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The specific assumptions made are detailed in the preliminaries and theorem statements. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Statements are proved or cite a relevant reference. Many of these proofs can be found in the appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: There are no experimental results. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: There is no associated code. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: There are no experiments. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: There are no experiments. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: There are no experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The theoretical nature of the results means there are minimal ethical concerns. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The theoretical nature of the work means that any societal impact would be very indirect. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: No such assets are used as a part of this research. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: No such assets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 43}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: No such assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]