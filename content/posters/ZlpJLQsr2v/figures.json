[{"figure_path": "ZlpJLQsr2v/figures/figures_1_1.jpg", "caption": "Figure 1: Schematic of motion modeling paradigms in video frame interpolation. (a) A na\u00efve linear combination of bidirectional flows Fo\u21921, F1\u21920 (i.e., flows between input frames) may lead to ambiguous and coarse motion estimation due to strong overlapped and linear assumptions. (b) A time-condition-based modeling approach may predict suboptimal bilateral flows Ft\u21920, Ft\u21921 (i.e., flows between estimated and input frames), capturing spatiotemporal changes for moving objects ineffectively. (c) Our generalizable implicit motion modeling properly represents spatiotemporal dynamics across videos and predict better bilateral flows via an adaptive coordinate-based neural network.", "description": "This figure compares three different motion modeling paradigms in video frame interpolation. (a) shows a naive linear combination of bidirectional flows, which is prone to errors due to strong overlapped and linear assumptions. (b) shows a time-conditioned approach that directly predicts bilateral flows for specific timestamps, but it lacks the ability to effectively model spatiotemporal dynamics. (c) introduces the proposed generalizable implicit motion modeling approach, which uses an adaptive coordinate-based neural network to model spatiotemporal dynamics more accurately and predict better bilateral flows.", "section": "1 Introduction"}, {"figure_path": "ZlpJLQsr2v/figures/figures_3_1.jpg", "caption": "Figure 2: Our GIMM first transforms initial bidirectional flows Fo\u21921, F1\u21920 as normalized flows Vo, V\u2081. The motion encoder extracts motion features Ko, K\u2081 from Vo, V\u2081, respectively. Ko, K\u2081 are then forward warped at a given timestep t using bidirectional flows to obtain the warped features Kt0, Kt 1. We pass the warped and initial motion features into a Latent Refiner that outputs motion latent Lt, representing motion information at t. Conditioned on Lt(x, y), the coordinate-based network ge predicts the corresponding normalized flow Vt with 3D coordinates x = (x, y, t). For interpolation usage, Vt is then transferred into bilateral flows Ft\u21920, Ft 1 through denormalization.", "description": "This figure illustrates the architecture of the Generalizable Implicit Motion Modeling (GIMM) module.  It shows how bidirectional optical flows are normalized, encoded into motion features, and then used to generate implicit motion representations. These representations are then used by a coordinate-based network to predict normalized flows at arbitrary timesteps, which are finally denormalized to obtain bilateral flows.", "section": "3.1 Generalizable Implicit Motion Modeling"}, {"figure_path": "ZlpJLQsr2v/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparisons of different motion modeling methods on SNU-FILM-arb-Hard. All the results are predicted at t = 0.75, and ground truth flows are obtained by FlowFormer [19].", "description": "This figure presents a qualitative comparison of various motion modeling methods' performance on the SNU-FILM-arb-Hard dataset.  The methods compared include Linear, Forward Warp, End-to-End, BMBC, and the proposed GIMM-VFI-R. Each method's output is shown alongside the ground truth optical flow, illustrating the differences in motion prediction accuracy.  The results displayed are for a specific timestep (t=0.75).  The figure aims to visually demonstrate GIMM-VFI-R's superior performance in accurately modeling complex motions.", "section": "4.1 Motion Modeling"}, {"figure_path": "ZlpJLQsr2v/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative comparisons of arbitrary-timestep interpolation on XTest-2K [46]. Positions pointed by the yellow arrow indicate the distinct performance of our method.", "description": "This figure compares the performance of different video frame interpolation methods on the XTest-2K benchmark at two different timesteps (0.25 and 0.75).  The methods compared are CURE, EMA-VFI, GIMM-VFI-R, GIMM-VFI-F, and a ground truth.  Each row shows a different video sequence, highlighting the interpolation results for various complex motions. The yellow arrows indicate specific areas where the proposed GIMM-VFI method demonstrates superior performance in handling challenging motion scenarios and preserving details.", "section": "4.4 Arbitrary-timestep Video Frame Interpolation"}, {"figure_path": "ZlpJLQsr2v/figures/figures_13_1.jpg", "caption": "Figure 2: Our GIMM first transforms initial bidirectional flows Fo\u21921, F1\u21920 as normalized flows Vo, V\u2081. The motion encoder extracts motion features Ko, K\u2081 from Vo, V\u2081, respectively. Ko, K\u2081 are then forward warped at a given timestep t using bidirectional flows to obtain the warped features Kt0, Kt 1. We pass the warped and initial motion features into a Latent Refiner that outputs motion latent Lt, representing motion information at t. Conditioned on Lt(x, y), the coordinate-based network ge predicts the corresponding normalized flow V\u0142 with 3D coordinates x = (x, y, t). For interpolation usage, V\u2081 is then transferred into bilateral flows Ft\u21920, Ft 1 through denormalization.", "description": "This figure illustrates the architecture of the Generalizable Implicit Motion Modeling (GIMM) module. It shows how initial bidirectional flows are normalized, motion features are extracted and warped, and a latent motion representation is generated. This latent representation, along with spatiotemporal coordinates, is fed to a coordinate-based network to predict normalized flows, which are then denormalized to produce bilateral flows for video frame interpolation.", "section": "3.1 Generalizable Implicit Motion Modeling"}, {"figure_path": "ZlpJLQsr2v/figures/figures_14_1.jpg", "caption": "Figure 7: Overview of the frame synthesis module. The frame synthesis module conducts multi-scale flow refinements and predicts warping masks for interpolation, including the intermediate warping mask M predicted by the initial decoder. It integrates the images and their pyramid features A, A\u2081|l \u2208 1,2 during decoding, updating predictions with correlation information from the Bidirectional Correlation Volume, which is constructed with correlation features (Co, C1). Ultimately, we use the outputs from the Final Decoder to perform the final interpolation of It through the Multi-field Refinement block. This block is adapted from AMT-G [29]. Modified components are highlighted in green for clarity.", "description": "This figure shows the architecture of the frame synthesis module, which takes the predicted bilateral flows, context features, and correlation features as input to generate the final interpolated frame. It consists of an initial decoder, an update block, a final decoder, and a multi-field refinement block. The initial decoder predicts an intermediate warping mask, which is then refined by the update block and the final decoder. The multi-field refinement block combines the final warped images to generate the final interpolated frame. The module is based on AMT-G [29], with some modifications highlighted in green.", "section": "3.2 Integrating GIMM with Frame Interpolation"}, {"figure_path": "ZlpJLQsr2v/figures/figures_14_2.jpg", "caption": "Figure 8: The network architectures of the modified decoders within the frame synthesis module. Unless specifically noted, each convolutional layer is activated by a PReLU function. The Deeper IFRBlock (highlighted in red) is based on the decoder architecture (IFRBlock) introduced in IFR-Net [26]. In particular, we enhance the original IFRBlock by adding two additional residual blocks to create the Deeper IFRBlock.", "description": "This figure details the network architectures of the modified decoders within the frame synthesis module.  It highlights the use of a modified IFRBlock (Deeper IFRBlock), which is based on the IFRBlock from IFR-Net [26] but enhanced with two additional residual blocks.  The figure shows the structures of both the Initial Decoder and the Final Decoder, showing the convolutional layers, activation functions (PReLU), and connections between different components within the decoder modules.", "section": "3.2 Integrating GIMM with Frame Interpolation"}, {"figure_path": "ZlpJLQsr2v/figures/figures_16_1.jpg", "caption": "Figure 9: Visual comparisons for 6X motion modeling on the Vimeo-septuplet-flow. We compare GIMM with its model variants mentioned in the ablation study of the main text.", "description": "This figure provides a qualitative comparison of the GIMM model's performance in 6X motion modeling on the Vimeo-septuplet-flow dataset. It compares the results of the full GIMM model against several ablation variants, each of which removes or modifies a specific component of the GIMM framework. These ablation studies investigate the effects of removing different architectural components such as the forward warping, the implicit model, latent refinement, and spatial coordinates, allowing for an assessment of their relative contributions to the model's effectiveness.", "section": "7.5 Qualitative Results of GIMM Motion Modeling"}]