[{"figure_path": "0lBx844upd/tables/tables_4_1.jpg", "caption": "Table 1: Performance analysis of pruning the \"self_attn.k_proj\" layer in the first block of the OPT-13B model at various sparsity levels. (Left) Relative reconstruction error ||XW \u2013 XW||/||XW|| of the optimal weights W constrained to the support determined by each pruning method. (Right) Comparison of time and reconstruction error for three scenarios, all using magnitude pruning to determine the support and then: (i) no post-processing (w/o pp.), (ii) refining the weights with ALPS, and (iii) refining the weights optimally with backsolve.", "description": "This table presents a comparison of different pruning methods' performance on a single layer of the OPT-13B model at different sparsity levels. The left side shows the relative reconstruction error achieved by each method when using the optimal weights constrained to the support determined by each method. The right side compares the time taken and reconstruction error of three different approaches: no post-processing, refining weights using ALPS, and optimal backsolve, all while using magnitude pruning to determine support.", "section": "4.1 Reconstruction error on a single layer"}, {"figure_path": "0lBx844upd/tables/tables_7_1.jpg", "caption": "Table 1: Performance analysis of pruning the \"self_attn.k_proj\" layer in the first block of the OPT-13B model at various sparsity levels. (Left) Relative reconstruction error ||XW \u2013 XW||/||XW|| of the optimal weights W constrained to the support determined by each pruning method. (Right) Comparison of time and reconstruction error for three scenarios, all using magnitude pruning to determine the support and then: (i) no post-processing (w/o pp.), (ii) refining the weights with ALPS, and (iii) refining the weights optimally with backsolve.", "description": "This table presents a comparison of different pruning methods' performance on a single layer of the OPT-13B model at various sparsity levels.  The left side shows the relative reconstruction error achieved by each method, while the right side compares the runtime and reconstruction error with and without post-processing using the ALPS method and a standard backsolve method.", "section": "4.1 Reconstruction error on a single layer"}, {"figure_path": "0lBx844upd/tables/tables_7_2.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table compares the performance of different one-shot unstructured pruning methods (MP, SparseGPT, Wanda, DSnoT, and ALPS) on various OPT models (1.3B to 30B parameters) at 70% sparsity.  The performance is evaluated using perplexity scores on WikiText2, PTB, and C4 datasets, as well as zero-shot performance on five benchmark tasks (MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge). Lower perplexity scores and higher zero-shot accuracy scores indicate better performance.  The mean and standard deviation are reported for each metric.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_9_1.jpg", "caption": "Table 3: Performance analysis for one-shot pruning of OPT-30B and LLaMA2-13B at 2: 4 and 4: 8 sparsity patterns. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values correspond to better performance, and \u2191 denotes higher values correspond to better performance.", "description": "This table compares the performance of different one-shot pruning methods (MP, Wanda, SparseGPT, DSnoT, and ALPS) on OPT-30B and LLaMA2-13B models with N:M sparsity patterns (2:4 and 4:8).  The results are evaluated across multiple metrics: WikiText2 perplexity (lower is better), PTB perplexity (lower is better), C4 perplexity (lower is better), PIQA accuracy (higher is better), ARC-Easy accuracy (higher is better), and ARC-Challenge accuracy (higher is better).  Each method is run five times, and the mean and standard deviation are reported.", "section": "4.3 N:M sparsity"}, {"figure_path": "0lBx844upd/tables/tables_18_1.jpg", "caption": "Table 5: The rate of change of the support (of weights) between consecutive iterations, comparing ALPS with ADMM using a fixed penalty parameter p.", "description": "This table compares the rate of change in the support (indices of non-zero weights) between consecutive iterations for three methods: ALPS and ADMM with two different fixed penalty parameters (p = 0.3 and p = 3).  It demonstrates that ALPS, with its adaptive penalty parameter scheme, converges rapidly while maintaining high solution quality, unlike ADMM with fixed parameters, which either converges slowly or converges to a poor solution.  The \"Supp change / Iter\" represents the percentage change in the support between iterations.", "section": "B.2.1 The importance of the p update scheme"}, {"figure_path": "0lBx844upd/tables/tables_19_1.jpg", "caption": "Table 6: Runtime (in seconds) comparison for one-shot unstructured pruning of OPT models and LLaMA models. Here, runtime includes input activation generation and model pruning.", "description": "This table compares the runtime in seconds for different one-shot unstructured pruning methods across various OPT and LLaMA models.  The runtime includes the time taken for input activation generation and the model pruning process itself.  The table shows a significant increase in runtime for ALPS compared to other methods, which can be attributed to ALPS using a more advanced optimization-based approach for pruning.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_19_2.jpg", "caption": "Table 7: Relative reconstruction error ||XW \u2013 XW||/||XW|| comparison between ALPS and ADMM-Grad across different sparsity levels.", "description": "This table compares the performance of ALPS and ADMM-Grad in terms of relative reconstruction error for a single layer in an OPT-13B model at different sparsity levels (0.4 to 0.9).  It demonstrates the superior performance of ALPS in approximating the dense model's output, particularly at higher sparsity levels.", "section": "4.1 Reconstruction error on a single layer"}, {"figure_path": "0lBx844upd/tables/tables_19_3.jpg", "caption": "Table 7: Relative reconstruction error ||XW \u2013 XW||/||XW|| comparison between ALPS and ADMM-Grad across different sparsity levels.", "description": "This table compares the performance of ALPS and ADMM-Grad in terms of relative reconstruction error at various sparsity levels. The relative reconstruction error measures how well the pruned model approximates the output of the original dense model. Lower values indicate better performance.", "section": "4.1 Reconstruction error on a single layer"}, {"figure_path": "0lBx844upd/tables/tables_20_1.jpg", "caption": "Table 9: Performance analysis for one-shot unstructured pruning of LLaMA-3 8B models at various sparsity levels using MMLU benchmark.", "description": "This table presents the performance of different one-shot unstructured pruning methods (MP, Wanda, SparseGPT, DSnoT, and ALPS) on the LLaMA3-8B model at various sparsity levels (0.4, 0.5, 0.6, 0.7, 2:4, and 4:8). The performance is measured using the MMLU benchmark, and the table shows the mean accuracy across all MMLU categories for each method and sparsity level.  The results demonstrate that ALPS outperforms other methods, especially at high sparsity levels, and further validate its effectiveness in producing high-performance pruned models.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_21_1.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table compares the performance of different one-shot unstructured pruning methods (MP, SparseGPT, Wanda, DSnoT, and ALPS) on OPT models with varying sizes (1.3B to 30B parameters) at 70% sparsity.  The evaluation metrics include perplexity on three datasets (WikiText2, PTB, C4) and zero-shot performance on five tasks (MMLU, PIQA, LAMBADA, ARC-Easy, ARC-Challenge). Lower perplexity scores and higher accuracy scores indicate better performance. The results show the mean and standard deviation of each metric across five independent runs for each method.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_21_2.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table compares the performance of different one-shot unstructured pruning methods on OPT language models of varying sizes (1.3B to 30B parameters) at a 70% sparsity level.  The metrics used to evaluate the performance are perplexity scores on the WikiText2, PTB, and C4 datasets, and zero-shot accuracy scores on five tasks: MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge.  Lower perplexity and higher accuracy scores indicate better performance.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_22_1.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table presents the performance comparison of various one-shot unstructured pruning methods on OPT models with 70% sparsity.  The metrics used are perplexity (lower is better) on WikiText2, PTB, and C4 datasets, and accuracy (higher is better) on five zero-shot benchmark tasks: MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge. Each method was run five times, and the table shows mean and standard deviation values for each metric and dataset.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_22_2.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table compares the performance of different one-shot unstructured pruning methods on OPT models with various sizes (1.3B to 30B parameters) at a sparsity level of 70%.  It evaluates five different methods (MP, SparseGPT, Wanda, DSnoT, and ALPS) across multiple metrics including perplexity on WikiText2, PTB, and C4 datasets, and zero-shot performance on five benchmark tasks (MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge).  Lower perplexity values and higher accuracy scores indicate better model performance.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_23_1.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table presents the results of an experiment comparing different one-shot unstructured pruning methods on OPT language models with 70% sparsity.  The methods are evaluated using multiple metrics, including perplexity on three different datasets (WikiText2, Penn Treebank, and C4) and zero-shot performance across five tasks (MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge).  The mean and standard deviation of the results across five runs are shown for each method.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_23_2.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table presents the results of comparing several one-shot unstructured pruning methods for large language models (LLMs).  The models used are from the OPT and LLAMA families, and the evaluation metrics are perplexity scores on WikiText2, PTB, and C4 datasets, along with zero-shot performance scores across five different tasks (MMLU, PIQA, LAMBADA, ARC-Easy, ARC-Challenge). The table shows the mean and standard deviation for each method, across five runs, allowing for a statistical comparison of performance.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_24_1.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table presents the results of five different one-shot unstructured pruning methods (MP, SparseGPT, Wanda, DSnoT, and ALPS) applied to OPT models of varying sizes (1.3B to 30B parameters).  The models were pruned to 70% sparsity. The table shows the mean and standard deviation of the performance across five runs for each method on several metrics: WikiText2 perplexity, PTB perplexity, C4 perplexity, LAMBADA accuracy, PIQA accuracy, ARC-Easy accuracy, and ARC-Challenge accuracy. Lower values are better for perplexity, and higher values are better for accuracy.", "section": "4.2 Pruning OPT and LLAMA models"}, {"figure_path": "0lBx844upd/tables/tables_24_2.jpg", "caption": "Table 2: Performance analysis for one-shot unstructured pruning of OPT models (1.3B ~ 30B) at 70% sparsity. We run each method five times and report the mean and standard deviation of each performance criterion. Here, \u2193 denotes lower values corresponding to better performance, and \u2191 denotes higher values corresponding to better performance.", "description": "This table presents the results of five different one-shot unstructured pruning methods applied to OPT language models of varying sizes (1.3B to 30B parameters).  The models were pruned to 70% sparsity. The table shows the mean and standard deviation of perplexity scores on three datasets (WikiText2, PTB, and C4) and zero-shot performance on five benchmark tasks (MMLU, PIQA, LAMBADA, ARC-Easy, and ARC-Challenge). Lower perplexity scores and higher accuracy scores indicate better performance.", "section": "4.2 Pruning OPT and LLAMA models"}]