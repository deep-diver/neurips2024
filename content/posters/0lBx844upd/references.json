{"references": [{"fullname_first_author": "Stephen Boyd", "paper_title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "publication_date": "2011-01-01", "reason": "This paper introduces the ADMM algorithm which is a core component of ALPS and a central method of the paper"}, {"fullname_first_author": "Elias Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "publication_date": "2023-01-01", "reason": "This paper proposes SparseGPT, a state-of-the-art one-shot LLM pruning method that ALPS is compared against."}, {"fullname_first_author": "Mingjie Sun", "paper_title": "A simple and effective pruning approach for large language models", "publication_date": "2023-01-01", "reason": "ALPS is compared to Wanda, another state-of-the-art method for one-shot LLM pruning, proposed in this paper."}, {"fullname_first_author": "Song Han", "paper_title": "Learning both weights and connections for efficient neural network", "publication_date": "2015-01-01", "reason": "This paper is foundational to the field of network pruning, providing a baseline method against which ALPS is measured."}, {"fullname_first_author": "Yuxin Zhang", "paper_title": "Dynamic sparse no training: Training-free fine-tuning for sparse LLMs", "publication_date": "2023-01-01", "reason": "This paper proposes DSnoT, yet another state-of-the-art one-shot LLM pruning method that ALPS improves upon."}]}