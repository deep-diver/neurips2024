[{"type": "text", "text": "Distributed Least Squares in Small Space via Sketching and Bias Reduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sachin Garg Computer Science & Engineering University of Michigan sachg@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Kevin Tan   \nDepartment of Statistics   \nUniversity of Pennsylvania   \nkevtan@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Micha\u0142 Derezin\u00b4ski Computer Science & Engineering University of Michigan derezin@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression. We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error. In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data. This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches. Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity. The techniques include new higher-moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matrix sketching is a powerful collection of randomized techniques for compressing large data matrices, developed over a long line of works as part of Randomized Numerical Linear Algebra [RandNLA, e.g., 45, 26, 37, 39, 21]. Sketching can be used to reduce the large dimension $n$ of a data matrix $\\mathbf{A}\\in\\mathbb{R}^{\\overline{{n}}\\times d}$ by applying a random sketching matrix (operator) $\\mathbf{S}\\in\\mathbb{R}^{\\bar{m}\\times n}$ to obtain the sketch $\\tilde{\\mathbf{A}}=\\mathbf{S}\\mathbf{A}\\in\\mathbb{R}^{m\\times d}$ where $m\\ll n$ . For example, sketching can be used to approximate the solution to the least squares problem, $\\mathbf{x}^{*}=\\mathrm{argmin}_{\\mathbf{x}}\\,L(\\mathbf{x})$ where $\\mathbf{\\check{\\boldsymbol{L}}(\\mathbf{x})}=\\|\\mathbf{Ax}-\\mathbf{\\check{b}}\\|^{2}$ , by using a sketched estimator $\\tilde{\\mathbf{x}}=\\mathrm{argmin}_{\\mathbf{x}}\\,\\|\\tilde{\\mathbf{A}}\\mathbf{x}-\\tilde{\\mathbf{b}}\\|^{2}$ , where $\\tilde{\\mathbf{A}}=\\mathbf{S}\\mathbf{A}$ and $\\tilde{\\mathbf{b}}=\\mathbf{S}\\mathbf{b}$ . ", "page_idx": 0}, {"type": "text", "text": "Perhaps the simplest form of sketching is subsampling, where the sketching operator S selects a random sample of the rows of matrix A. However, the real advantage of sketching as a framework emerges as we consider more complex operators S, such as sub-Gaussian matrices [1], randomized Hadamard transforms [3], and sparse random matrices [12]. These approaches ensure higher quality and more robust compression of the data matrix, e.g., leading to provable $\\epsilon$ -approximation guarantees for the estimate $\\tilde{\\bf x}$ in the least squares task, i.e., $L(\\tilde{\\mathbf{x}})\\leq\\grave{(1+\\epsilon)}L(\\mathbf{x}^{*})$ . Nevertheless, there are fundamental limitations to how far we can compress a data matrix using sketching while ensuring an $\\epsilon$ -approximation. These limitations pose a challenge particularly in space-limited computing environments, such as for streaming algorithms where we observe the matrix A, say, one row at a time, and we have limited space for storing the sketch [11]. ", "page_idx": 0}, {"type": "image", "img_path": "rkuVYosT2c/tmp/20c9debdc79650597726b68a4e4c516ec20280c90d2ed7e36e686c8c0217a60d.jpg", "img_caption": ["Figure 1: Illustration of the leverage score sparsification algorithm used in Theorem 1. Each row of the sketch mixes $\\tilde{O}(1/\\epsilon)$ leverage score samples from A. Remarkably, the $\\epsilon$ -error guarantee of the subsampled estimator is retained as $\\epsilon$ -bias of the sketched estimator. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "One strategy for overcoming the fundamental limitations of sketching as a compression tool is to look beyond the single approximation guarantee provided by a sketching-based estimator $\\tilde{\\bf x}$ , and consider how its broader statistical properties can be leveraged in a given computing environment. To that end, many recent works have demonstrated both theoretically and empirically that sketching-based estimators often exhibit not only approximation robustness but also statistical robustness, for instance enjoying sharp confidence intervals, effectiveness of statistical inference tools such as bootstrap and cross-validation, as well as accuracy boosting techniques such as distributed averaging [e.g., 35, 25, 32, 33]. Yet, these results have had limited impact on the traditional computational complexity analysis in RandNLA and sketching literature, as many of them either impose additional assumptions, focus on sharpening the constant factors, or require using more expensive sketching techniques. In this work, we demonstrate that the statistical properties of sketching-based estimators can in fact have a substantial impact on the computational trade-offs that arise in RandNLA. ", "page_idx": 1}, {"type": "text", "text": "Our key motivating example is the above mentioned least squares regression task. It is well understood that for an $n\\times d$ least squares task, to recover an $\\epsilon$ -approximate solution out of an $m\\times d$ sketch, we need sketch size at least $\\bar{m}=\\Omega(d/\\epsilon)$ . This has been formalized in the streaming setting with a lower bound of $\\Omega(\\epsilon^{-1}d^{2}\\log(n d))$ bits of space required, when all of the input numbers use $O(\\log(n d))$ bits of precision [11]. One setting where this can be circumvented is in the distributed computing model where the bits can be spread out across many machines, so that the per-machine space can be smaller. Here, one could for instance hope that we can maintain small $O(d)\\times d$ sketches in $q=O(1/\\epsilon)$ machines and then combine their estimates to recover an $\\epsilon$ -approximate solution. A simple and attractive approach is to average the estimates $\\tilde{\\mathbf{x}}_{i}$ produced by the individual machines, returning $\\begin{array}{r}{\\hat{{\\bf x}}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{{\\bf x}}_{i}}\\end{array}$ , as this only requires each machine to communicate $O(d\\log(n d))$ bits of information about its sketch. This approach requires the sketching-based estimates $\\tilde{\\mathbf{x}}_{i}$ to have sufficiently small bias for the averaging scheme to be effective. While this has been demonstrated empirically in many cases, existing theoretical results still require relatively expensive sketching methods to recover low-bias estimators, leading to an unfortunate trade-off in the distributed averaging scheme between the time and space complexity required. ", "page_idx": 1}, {"type": "text", "text": "In this work, we address the time-space trade-off in distributed averaging of sketching-based estimators, by giving a sharp characterization of how their bias depends on the sparsity of the sketching matrix. Remarkably, we show that in the distributed streaming environment one can compress the data down to the minimum size of $O(d^{2}\\log(n d))$ bits at no extra computational cost, while still being able to recover an $\\epsilon_{}$ -approximate solution for least squares and related problems. Importantly, our results require the sketching matrix to be slightly denser than is necessary for obtaining approximation guarantees on a single estimate, and thus, cannot be recovered by standard RandNLA sampling methods such as approximate leverage score sampling [27]. ", "page_idx": 1}, {"type": "text", "text": "Before we state the full result in the distributed setting, we give our main technical contribution. which is the following efficient construction of a low-bias least squares estimator in a single pass using only $O(d^{2}\\log(\\overline{{n}}d))$ bits of space, assuming all numbers use $O(\\log(n d))$ bits of precision. Below, $\\gamma>0$ denotes an arbitrarily small constant. ", "page_idx": 1}, {"type": "table", "img_path": "rkuVYosT2c/tmp/1f4dd584636e87dd5a7137e70e203bd35ca18afe3d6be4391e5a5366b4847905.jpg", "table_caption": ["This work (Thm. 1) Leverage Score Sparsification $\\overline{{\\mathrm{nnz}(\\mathbf{A})+\\tilde{O}(d^{2}/\\epsilon)}}$ "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of time complexities and parallel passes over the data required for different methods to obtain a $(1+\\epsilon)$ -approximation in $O(d^{2}\\,\\dot{\\log}(n d))$ bits of space for an $n\\times d$ least squares problem $(\\mathbf{A},\\mathbf{b})$ , given a preconditioner $\\mathbf{P}$ such that $\\kappa(\\mathbf{AP})=O(1)$ (see Section 3 for our computational model). We include the fully sequential Weighted Mb-SGD as a reference. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. Given streaming access to $\\textbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\textbf{b}\\in\\mathbb{R}^{n}$ , and direct access to a preconditioner matrix $\\textbf{P}\\in\\mathbb{R}^{d\\times d}$ such that $\\kappa(\\mathbf{A}\\mathbf{P})~\\leq~\\alpha,$ , within a single pass over $(\\mathbf{A},\\mathbf{b})$ , in $O(\\gamma^{-1}\\mathrm{nnz}(\\mathbf{A})+\\epsilon^{-1}\\alpha d^{2+\\gamma}\\mathrm{polylog}(d))$ time and ${\\cal O}(d^{2}\\log(n d))$ bits of space, we can construct $a$ randomized estimator \u02dcx for the least squares solution $\\mathbf{x}^{*}=\\mathrm{argmin}_{\\mathbf{x}}\\,\\|\\bar{\\mathbf{A}}\\bar{\\mathbf{x}}-\\mathbf{b}\\|^{2}$ such that: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\mathrm{Bias})}&{\\left\\|\\mathbf{A}\\mathbb{E}[\\tilde{\\mathbf{x}}]-\\mathbf{b}\\right\\|^{2}\\leq(1+\\epsilon)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2},}\\\\ {(\\mathrm{Variance})}&{\\mathbb{E}\\big[\\|\\mathbf{A}\\tilde{\\mathbf{x}}-\\mathbf{b}\\|^{2}\\big]\\leq2\\,\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our estimator $\\tilde{\\bf x}$ is constructed at the end of the data pass from a sketch $(\\tilde{\\mathbf{A}},\\tilde{\\mathbf{b}})$ where $\\tilde{\\mathbf{A}}=\\mathbf{S}\\mathbf{A}$ and $\\tilde{\\mathbf{b}}=\\mathbf{S}\\mathbf{b}$ , by minimizing $\\|\\tilde{\\mathbf{A}}\\mathbf{x}-\\tilde{\\mathbf{b}}\\|^{2}$ using preconditioned conjugate gradient. Here, S is a carefully constructed sparse sketching matrix which is inspired by the so-called leverage score sparsified (LESS) embeddings [18]. Leverage scores represent the relative importances of the rows of A which are commonly used for subsampling in least squares (see Definition 1), and their estimates can be easily obtained in a single pass by using the preconditioner matrix $\\mathbf{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Our time complexity bound of $\\tilde{O}(\\mathrm{nnz}(\\mathbf{A})+d^{2}/\\epsilon)$ matches the time it would take (for a single machine) to subsample $\\tilde{O}(d/\\epsilon)$ rows of $\\mathbf{A}$ according to the approximate leverage scores and produce an estimator $\\tilde{\\bf x}$ that achieves the $\\epsilon_{}$ -error bound $\\|\\bar{\\mathbf{A}^{\\tilde{\\mathbf{x}}}}-\\mathbf{b}\\|^{2}\\overset{\\cdot}{\\leq}(1+\\epsilon)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}$ . However, this strategy requires either maintaining $\\tilde{O}(d^{2}/\\epsilon)$ bits of space for the sketch, or computing $\\tilde{\\bf x}$ directly along the way, blowing up the runtime to ${\\tilde{O}}(d^{\\omega}/\\epsilon)$ . Since approximate leverage score sampling leads to significant least squares bias, averaging can only improve this to ${\\tilde{O}}(d^{\\omega}/{\\sqrt{\\epsilon}})$ (see Table 1). An alternate strategy would be to combine leverage score sampling with a preconditioned mini-batch stochastic gradient descent (Weighted Mb-SGD), with mini-batches chosen so that they fit in $\\tilde{O}(d^{2})$ space. This achieves the same time and space complexity as our method, but due to the streaming access to A and the sequential nature of SGD, it requires $O(1/\\epsilon)$ data passes. ", "page_idx": 2}, {"type": "text", "text": "Instead, our algorithm essentially mixes an $\\tilde{O}(d/\\epsilon)$ size leverage score sample into an $O(d)$ size sketch, merging $\\tilde{O}(1/\\epsilon)$ rows of A into a single row of the sketch (see Figure 1). This results in better data compression compared to direct leverage score sampling, with only $\\tilde{O}(d^{2})$ bits of space, while retaining the same $\\tilde{O}(\\mathrm{nnz}(\\mathbf{A})+d^{2}/\\epsilon)$ runtime complexity as the above approaches and requiring only a single data pass. The resulting estimator $\\tilde{\\bf x}$ can no longer recover the $\\epsilon$ -error bound, but remarkably, its expectation $\\mathbb{E}[\\tilde{\\mathbf{x}}]$ still does. To turn this into an improved estimator in a distributed model, we can simply average $\\bar{q}=1/\\epsilon$ such estimators, i.e., q1 iq=1 \u02dcxi, obtaining $\\mathbb{E}\\|\\mathbf{A}\\hat{\\mathbf{x}}-\\mathbf{b}\\|^{2}\\leq(1+2\\epsilon)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}$ . As shown in Table 1, ours is the first result in this model to achieve $\\tilde{O}(d^{2})$ space in a single pass and faster than current matrix multiplication time $O(d^{\\omega})$ . ", "page_idx": 2}, {"type": "text", "text": "Finally, by incorporating a preconditioning scheme, we illustrate how our construction can be used to design the first algorithm that solves least squares in current matrix multiplication time, constant parallel passes and $O(d^{2}\\log(n d))$ bits of space. We note that the $O(d^{\\omega})$ cost comes only from the worst-case complexity of constructing the preconditioner $\\mathbf{P}$ , which can often be accelerated in practice. The computational model used in Theorem 2 is described in detail in Section 3. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2. Given $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{b}\\in\\mathbb{R}^{n}$ in the parallel computing model, using two parallel passes with $q$ machines, we can compute \u02dcx such that with probability 0.9 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}\\tilde{\\mathbf{x}}-\\mathbf{b}\\|\\leq\\left(1+\\epsilon+O(1/q)\\right)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in $O(\\gamma^{-1}\\mathrm{nnz}(\\mathbf{A})+d^{\\omega}+\\epsilon^{-1}d^{2+\\gamma}\\mathrm{polylog}(d))$ time, $O(d^{2}\\log(n d))$ bits of space and $O(d\\log(n d))$ bits of communication. In particular, choosing $q=1/\\epsilon$ we recover an $O(\\epsilon)$ -approximation. ", "page_idx": 3}, {"type": "text", "text": "Remark 2. While the parallel computing model in Theorem 2 assumes that all machines have streaming access to the entire data matrix, this result can be easily extended to the setting where A has been randomly down-sampled or partitioned into separate size n chunks $\\mathbf{A}_{1},...,\\mathbf{A}_{q}$ , and each machine constructs an estimate $\\tilde{\\mathbf{x}}_{i}$ based on a sketch of its own chunk. Then, with the same computational guarantees as in Theorem 2, the averaged estimator $\\begin{array}{r}{\\tilde{\\mathbf{x}}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{x}}_{i}}\\end{array}$ with probability 0.9 enjoys a guarantee of: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}\\tilde{\\mathbf{x}}-\\mathbf{b}\\|\\leq\\left(1+O(\\epsilon+1/q+B_{\\mathrm{chunk}})\\right)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B_{\\mathrm{chunk}}$ is the bias that would be incurred if we solved each chunk exactly and averaged those solutions. Using existing guarantees for uniform down-sampling of least squares [Theorem 20, 43], one can bound this bias as $\\begin{array}{r}{B_{\\mathrm{chunk}}=\\tilde{O}\\!\\left(\\frac{\\mu}{q n}+(\\frac{\\mu}{n})^{2}\\right)}\\end{array}$ , where $\\mu$ is the coherence of the data matrix and n is the chunk size. For sufficiently large chunks, this bias is negligible compared to the error \u03f5. We prove the above high probability guarantee in Theorem $^{6}$ in Appendix $E$ . ", "page_idx": 3}, {"type": "text", "text": "Further applications. Our least squares analysis can be extended to other settings where prior works [e.g., 17, 18, 16, 22] have analyzed randomized estimators based on sparse sketching via techniques from asymptotic random matrix theory. The primary and most direct application involves correcting inversion bias in the so-called sketched inverse covariance estimate $(\\tilde{\\mathbf{A}}^{\\dagger}\\tilde{\\mathbf{A}})^{-1}$ , which was the motivating task of [18], with applications including distributed second-order optimization and statistical uncertainty quantification, where quantities like $(\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}})^{-1}\\mathbf{x}$ are approximated. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3 (informal Theorem 5). Given $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and its LESS embedding S with sketch size $m\\geq C d$ and s non-zeros per row, the inverse covariance sketch $(\\frac{m}{m-d}{\\bf A}^{\\top}{\\bf S}^{\\top}{\\bf S}{\\bf A})^{-1}$ is an $(\\epsilon,\\delta)$ - unbiased estimator of $(\\mathbf{A}^{\\top}\\mathbf{A})^{-1}$ (see Definition 3) for $\\begin{array}{r}{\\epsilon=\\tilde{O}\\big((1+\\sqrt{d/s})\\frac{\\sqrt{d}}{m}\\big)}\\end{array}$ and $\\delta=1/\\mathrm{poly}(d)$ . ", "page_idx": 3}, {"type": "text", "text": "This result should be compared with $\\begin{array}{r}{\\epsilon=\\tilde{O}\\big((1+d/s)\\frac{\\sqrt{d}}{m}\\big)}\\end{array}$ obtained by [18]. Thus, we get a direct improvement for very sparse sketches, i.e., $s=o(d)$ . This can be immediately translated into an improved local convergence guarantee for Distributed Newton Sketch which is a second-order convex minimization algorithm used in settings where the Hessian matrix can be expressed as ${\\bf A}^{\\top}{\\bf A}$ for a tall matrix A, e.g., in generalized linear models like logistic regression. In this method, following Corollary 16 of [18] as well as related results [16, 44, 19], we use sketching and averaging to estimate a Newton step: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{H}}_{i}^{-1}\\mathbf{g}_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf g}_{t}$ is the gradient at $\\mathbf{x}_{t}$ and $\\tilde{\\mathbf{H}}_{1},...,\\tilde{\\mathbf{H}}_{q}$ are Hessian sketches constructed by independent machines. The following corollary, which is a direct improvement over Corollary 16 of [18], shows that Distributed Newton Sketch on a generalized linear model task can achieve a fast local convergence rate of the form f(xt+1)\u2212\u2212f(\u2217x\u2217) with $O(d^{\\omega})$ time per iteration (see Appendix B.1 for details). ", "page_idx": 3}, {"type": "text", "text": "Corollary 1 (Distributed Newton Sketch). Consider $\\begin{array}{r}{f(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{i}(\\mathbf{x}^{\\top}\\phi_{i})+\\frac{\\lambda}{2}\\|\\mathbf{x}\\|^{2}}\\end{array}$ , where $\\ell_{i}$ are convex twice continuously differentiable functions, such that $f$ has a Lipschitz Hessian, $\\lambda>0$ , and $\\phi_{i}^{\\top}$ is the ith row of an $n\\times d$ data matrix $\\Phi$ . Given $\\epsilon>0$ , there is a neighborhood $U_{\\epsilon}$ around the minimizer $\\mathbf{x}^{*}=\\mathrm{argmin}_{\\mathbf{x}}\\:f(\\mathbf{x})$ such that, for any $\\mathbf{x}_{t}\\in U_{\\epsilon}$ , using two parallel passes with $\\tilde{O}(1/\\epsilon)$ machines, we can compute a Distributed Newton Sketch update $\\mathbf x_{t+1}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{t+1})-f(\\mathbf{x}^{*})\\leq\\epsilon\\cdot\\big[f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*})\\big],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in $O(\\gamma^{-1}\\mathrm{nnz}(\\Phi)+d^{\\omega}+\\epsilon^{-1}d^{2+\\gamma}\\mathrm{polylog}(d))$ time, $O(d^{2}\\log(n d))$ bits of space and $O(d\\log(n d))$ bits of communication. ", "page_idx": 3}, {"type": "text", "text": "Our Techniques. At the core of our analysis are techniques inspired by asymptotic random matrix theory (RMT) in the proportional limit [e.g., see 6]. Here, in order to establish the limiting spectral distribution (such as the Marchenko-Pastur law) of a random matrix $\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}}$ whose dimensions diverge to infinity, one aims to show the convergence of the Stieltjes transform of its resolvent matrix $(\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}}-\\acute{z}\\mathbf{I})^{-1}$ . Recently, [18] showed that these techniques can be adapted to sparse sketching matrices (via leverage score sparsification) in order to characterize the bias of the sketched inverse covariance $(\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}})^{-1}$ , where $\\mathbf{\\tilde{A}}=\\mathbf{SA}$ . ", "page_idx": 4}, {"type": "text", "text": "Our main contribution is two-fold. First, we show that a similar argument can also be applied to analyze the bias of the least squares estimator, $\\tilde{\\mathbf{x}}=(\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}})^{-1}\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{b}}$ . Unlike the inverse covariance, this estimator no longer takes the form of a resolvent matrix, but its bias is also associated with the inverse, which means that we can use a leave-one-out argument to characterize the effect of removing a single row of the sketch on the estimation bias. Our second main contribution is to improve the sharpness of the bounds relative to the sparsity of the sketching matrix by combining a careful application of H\u00f6lder\u2019s inequality with a higher moments analysis of the restricted BaiSilverstein inequality for quadratic forms. Those improvements are not only applicable to the least squares analysis, but also to all existing RMT-style results for LESS embeddings, including the aforementioned inverse covariance estimation, as well as applications in stochastic optimization, resulting in the sketching cost of LESS embeddings dropping below matrix multiplication time. ", "page_idx": 4}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Randomized numerical linear algebra. RandNLA sketching techniques have been developed over a long line of works, starting from fast least squares approximations of [42]; for an overview, see [45, 26, 37, 39, 21] among others. Since then, these methods have been used in designing fast algorithms not only for least squares but also many other fundamental problems in numerical linear algebra and optimization including low-rank approximation [13, 34], $l_{p}$ regression [14], solving linear systems [28, 24] and more. Using sparse random matrices for matrix sketching also has a long history, including data-oblivious sketching methods such as CountSketch [12], OSNAP [40], and more [38]. Leverage score sparsification (LESS) was introduced by [18] as a data-dependent sparse sketching method to enable RMT-style analysis for sketching (see below). ", "page_idx": 4}, {"type": "text", "text": "Unbiased estimators for least squares. To put our results in a proper context, let us consider other approaches for producing near-unbiased estimators for least squares, see also Table 1. First, a well known folklore result states that the least squares estimator computed from a dense Gaussian sketching matrix is unbiased. The bias of other sketching methods, including le\u221average score sampling and OSNAP, has been studied by [43], showing that these methods need a $\\sqrt{\\epsilon}$ -error guarantee to achieve an $\\epsilon$ -bias which leads to little improvement unless $\\epsilon$ is extremely small and the sketch size is sufficiently large. Another approach of constructing unbiased estimators for least squares, first proposed by [23], is based on subsampling with a non-i.i.d. importance sampling distribution based on Determinantal Point Processes [DPPs, 31, 20]. However, despite significant efforts [30, 7, 5], sampling from DPPs remains quite expensive: the fastest known algorithm requires running a Markov chain for $\\mathrm{polylog}(n/\\epsilon)$ many steps, each of which requires a separate data pass and takes $O(d^{\\omega})$ time. Other approaches have also been considered which provide partial bias reduction for i.i.d. RandNLA subsampling schemes in various regimes that are are either much more expensive or not directly comparable to ours [2, 44]. ", "page_idx": 4}, {"type": "text", "text": "Statistical and RMT analysis of sketching. Recently, there has been significant interest in statistical and random matrix theory (RMT) analysis of matrix sketching; see [21] for an overview. These approaches include both asymptotic analysis via limiting spectral distributions and deterministic equivalents [35, 25, 32, 33], and non-asymptotic analysis under statistical assumptions [36, 41, 4]. A number of works have shown that the RMT-style techniques based on deterministic equivalents can be made rigorously non-asymptotic for certain sketching methods such as dense sub-Gaussian [17], LESS matrices [16, 18], and other sparse matrices [9], which has been applied to low-rank approximation, fast subspace embeddings and stochastic optimization. Our new analysis can be viewed as a general strategy for directly improving the sparsity required by LESS embeddings (and thereby, the sketching time complexity) in many of these applications, specifically those that rely on analysis inspired by the calculus of deterministic equivalents via generalized Stieltjes transforms. ", "page_idx": 4}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Notations. In all our results, we use lowercase letters to denote scalars, lowercase boldface for vectors, and uppercase boldface for matrices. The norm $\\|\\cdot\\|$ denotes the spectral norm for matrices and the Euclidean norm for vectors, whereas $\\|\\cdot\\|_{F}$ denotes the Frobenius norm for matrices. We use $\\preceq$ to denote the p.s.d. ordering of matrices. ", "page_idx": 5}, {"type": "text", "text": "Computational model. We next clarify the computational model that is used in Theorem 2. We consider a central data server storing $\\left(\\mathbf{A},\\mathbf{b}\\right)$ , and $q$ machines. The $j$ th machine has a handle $\\operatorname{Stream}(j)$ , which can be used to open a stream and to read the next row/label pair $\\left(\\mathbf{a}_{i},b_{i}\\right)$ in the stream. After a full pass, the machine can re-open the handle and begin another pass over the data. The machines can operate their streams entirely asynchronously, and each has its own limited local storage space, e.g., in Theorem 2 we use $O(d^{2}\\,\\mathrm{{log}}(n d))$ bits of space per machine. At the end, they can communicate some information back to the server, e.g., in Theorem 2, they communicate their final estimate vectors $\\tilde{\\mathbf{x}}_{j}$ , using $O(d\\log(n d))$ bits of communication. Then, the server computes the final estimate, in our case via averaging, q1 iq=1 \u02dcxi, which can be done either directly or via a map-reduce type architecture. ", "page_idx": 5}, {"type": "text", "text": "We define the parallel passes required by such an algorithm as the maximum number of times the stream is opened by any single machine. We analogously define time/space/communication costs by taking a maximum over the costs required by any single machine (for communication, this refers only to the number of bits sent from the machine back to the server). ", "page_idx": 5}, {"type": "text", "text": "Definitions and useful lemmas. In our framework, we construct a sparse sketching matrix S where sparsification is achieved using a probability distribution over rows of data matrix A, that is proportional to the leverage scores of A. The next definition [following, e.g., 9] provides the explicit definition of exact and approximate leverage scores for our setting. ", "page_idx": 5}, {"type": "text", "text": "Definition 1 $(\\beta_{1},\\beta_{2})$ -approximate leverage scores). Fix a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and consider matrix $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ with orthonormal columns spanning the column space of A. Then, the leverage scores $l_{i},1\\leq i\\leq n$ are defined as the row norms squared of U, i.e., $\\bar{l}_{i}=\\Vert\\bar{\\mathbf{u}}_{i}\\Vert^{2}$ , where $\\mathbf{u}_{i}^{\\top}$ is the ith row of U. Furthermore, consider fixed $\\beta_{1},\\beta_{2}>1$ . Then $\\tilde{l_{i}}$ are called $(\\beta_{1},\\beta_{2})$ -approximate leverage scores for A if the following holds for all $i$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{l_{i}}{\\beta_{1}}\\leq\\tilde{l}_{i}\\ a n d\\ \\sum_{i=1}^{n}\\tilde{l}_{i}\\leq\\beta_{2}\\cdot d.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The approximate leverage scores can be computed by first constructing a preconditioner matrix $\\mathbf{P}\\in\\dot{\\mathbb{R}}^{\\hat{d}\\times d}$ such that $\\kappa(\\mathbf{{A}\\bar{P}})=O(1)$ , which takes $O(\\mathrm{ninz}(\\mathbf{A})+d^{\\omega})$ in a single pass, and then relying on the following norm approximation scheme. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Based on Lemma 7.2 from [10]). Given $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{P}\\in\\mathbb{R}^{d\\times d}$ , using a single pass over A in time $O(\\gamma^{-1}(\\mathrm{nnz}(\\mathbf{A})+d^{2}))$ for small constant $\\gamma>0$ , we can compute estimates $\\tilde{\\tilde{l}_{1}},...,\\tilde{l}_{n}$ such that with probability $\\ge0.95$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nn^{-\\gamma}\\|e_{i}^{\\top}\\mathbf{A}\\mathbf{P}\\|^{2}\\leq\\tilde{l}_{i}\\leq O(\\log(n))\\|\\mathbf{e}_{i}^{\\top}\\mathbf{A}\\mathbf{P}\\|^{2}\\quad\\forall i\\qquad a n d\\qquad\\sum_{i}\\tilde{l}_{i}\\leq O(1)\\cdot\\|\\mathbf{A}\\mathbf{P}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the next definition, we give the sparse sketching strategy used in our analysis. This approach is similar to the original leverage score sparsification proposed by [18], except: 1) we adapted it so that it can be implemented effectively in a single pass, and 2) we use it in a much sparser regime (fewer non-zeros per row). ", "page_idx": 5}, {"type": "text", "text": "Definition 2 $\\left(s,\\beta_{1},\\beta_{2}\\right)$ -LESS embedding). Fix a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and some $s\\geq0$ . Let the tuple $(\\widetilde{l}_{1},\\cdot\\cdot\\cdot\\,,\\widetilde{l}_{n})$ denote $(\\beta_{1},\\beta_{2})$ -approximate leverage scores for A. Let $\\begin{array}{r}{p_{i}=\\operatorname*{min}\\{1,\\frac{s\\beta_{1}\\tilde{l}_{i}}{d}\\}}\\end{array}$ . We define a $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier $\\xi$ as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\xi}=\\bigg(\\frac{b_{1}}{\\sqrt{p_{1}}},\\cdot\\cdot\\cdot\\mathrm{\\nabla},\\frac{b_{n}}{\\sqrt{p_{n}}}\\bigg)\\quad w h e r e\\quad b_{i}\\sim\\mathrm{Bernoulli}(p_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, we define the $(s,\\beta_{1},\\beta_{2})$ -leverage score sparsified (LESS) embedding of size m as matrix $\\mathbf{S}\\in\\mathbb{R}^{m\\times d}$ with i.i.d. rows \u221a1mxi such that xi = diag(\u03bei)yi where \u03bei denotes a randomly generated $(\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier and $\\mathbf{y}_{i}\\in\\mathbb{R}^{n}$ consist of random $\\pm1$ entries. ", "page_idx": 5}, {"type": "text", "text": "A key property of a sketching matrix is the subspace embedding property, defined below. It was recently shown by [9] that LESS embeddings require only polylogarithmically many non-zeros per row of $\\mathbf{S}$ to prove that $\\mathbf{S}$ is a subspace embedding for the data matrix A with the optimal $\\begin{array}{r}{m=\\bar{O(d)}}\\end{array}$ sketching dimension. For our main results, it is sufficient to use $\\eta=O(1)$ below. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Subspace embedding for LESS, Theorem 1.3, [9]). $F i x\\,\\eta,\\delta>0$ . Consider $\\beta_{1},\\beta_{2}>1$ and $a$ full rank matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ . Then for a $(\\beta_{1},\\beta_{2})$ -leverage score sparsified embedding $\\mathbf{S}\\in\\mathbb{R}^{m\\times n}$ with $s\\geq O(\\log^{4}(d/\\delta)/\\eta^{4})$ and $m=\\mathcal{O}((d+\\log{1/\\delta})/\\eta^{2})$ , with probability $1-\\delta$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\frac{1}{1+\\eta}}\\cdot\\mathbf{A}^{\\top}\\mathbf{A}\\preceq\\mathbf{A}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{A}\\preceq(1+\\eta)\\cdot\\mathbf{A}^{\\top}\\mathbf{A}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Least Squares Bias Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we provide an outline of the bias analysis for the sketched least squares estimator constructed using a LESS embedding, leading to the proofs of our main results, Theorems 1 and 2. In particular, we prove the following main technical result (detailed proof in Appendix C). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Bias of LESS-sketched least squares). Fix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and let $\\mathbf{S}$ be an $(s,\\beta_{1},\\beta_{2})$ -LESS embedding of size m for A. Let S satisfy (1) with $\\begin{array}{r}{\\eta=\\frac{1}{2}}\\end{array}$ and probability $1-\\delta$ where $\\delta<\\textstyle{\\frac{1}{m^{4}}}$ . Then there exists an event with probability at least $1-\\delta$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})=\\mathcal{O}\\left(\\frac{d}{m^{2}}\\left(1+\\frac{d}{s}\\right)\\log^{9}(n/\\delta)\\right)\\cdot L(\\mathbf{x}^{*}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 3. Thus, the bias $L(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})$ of the LESS estimator using $O(\\beta_{1}\\beta_{2}s)=\\tilde{O}(s)$ nonzeros per row of $\\mathbf{S}$ is of the order $\\begin{array}{r}{\\tilde{O}(\\frac{d^{2}}{s m^{2}}+\\frac{d}{m^{2}})\\cdot L(\\mathbf{x}^{*})}\\end{array}$ . By comparison, the standard expe\u2217cted loss bound which holds for sketched least squares (including this estimator) is $\\mathbb{E}[L(\\tilde{\\mathbf{x}})]-L(\\mathbf{x}^{*})\\leq$ $\\tilde{O}(\\frac{d}{m})L(\\mathbf{x}^{*})$ , and the best known bound on the bias of most standard sketched estimators (e.g., leverage score sampling) is $\\tilde{O}(\\frac{d^{2}}{m^{2}})L(\\mathbf{x}^{*})$ , given by [43]. So, our result recovers the standard bias bound for $s=1$ and improves on it for $s\\gg1$ by a factor of $\\operatorname*{min}\\{s,d\\}$ . At the end of the section, we discuss how to deal with the lower order term $\\tilde{O}(\\frac{\\dot{d}}{m^{2}})$ to reduce the bias further. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. Using a standard argument, we can replace the matrix A with the matrix $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ consisting of orthonormal columns spanning the column space of $\\mathbf{A}$ , and assume that $n=\\mathrm{poly}(d)$ . Let S be an $(s,\\beta_{1},\\beta_{2})$ -LESS embedding for $\\mathbf{U}$ . Also, let $\\mathbf{b}\\in\\mathbb{R}^{n}$ be a vector of responses/labels corresponding to $n$ rows in $\\mathbf{U}$ . Let $\\tilde{\\mathbf{x}}=\\operatorname{argmin}_{\\mathbf{x}}\\|\\mathbf{SUx}-\\mathbf{Sb}\\|^{2}$ . Furthermore for any $\\textbf{x}\\in\\mathbb{R}^{d}$ we can find the loss at $\\mathbf{x}$ as $L(\\mathbf{x})\\,=\\,\\|\\mathbf{Ux}\\,\\overset{\\,^{\\,^{\\,}}}{-}\\,\\mathbf{b}\\|^{2}$ . Additionally, we use $\\mathbf{r}$ to denote the residual $\\ensuremath{\\mathbf{b}}-\\ensuremath{\\mathbf{U}}\\ensuremath{\\mathbf{x}}^{*}$ . We also define $\\mathbf{Q}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{U})^{-1}$ as the sketched inverse covariance matrix with scaling $\\textstyle\\gamma={\\frac{m}{m-d}}$ representing the standard correction accounting for inversion bias. We condition on the high probability event $\\mathcal{E}$ guaranteed in Lemma 2 and consider $L(\\mathbb{E}_{\\mathcal{E}}(\\tilde{\\mathbf{x}}))\\!-\\!L(\\mathbf{x}^{*})$ . By Pythagorean theorem, we have $L(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})=\\|\\mathbf{U}(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-\\mathbf{U}\\mathbf{x}^{*}\\|^{2}$ . Note that by the normal equations we have $\\tilde{\\mathbf{x}}=(\\mathbf{U}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{b}=\\gamma\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{b}$ , and also $\\begin{array}{r}{\\mathbf{S}^{\\top}\\mathbf{S}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}}\\end{array}$ m1 i=1 xixi\u22a4 . These two facts lead to writing the bias as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})=\\left\\lVert\\gamma\\cdot\\mathbb{E}_{\\mathcal{E}}[\\mathbf{QU}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]\\right\\rVert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using a leave-one-out technique , we replace $\\mathbf{Q}$ with $\\mathbf{Q}_{-i}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i}^{\\top}\\mathbf{S}_{-i}\\mathbf{U})^{-1}$ , where $\\mathbf{S}_{-i}$ denotes matrix S without the $i$ th row, by noting that $\\begin{array}{r}{\\mathbf{Q}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i}^{\\top}\\mathbf{S}_{-i}\\mathbf{U}+\\frac{\\gamma}{m}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U})^{-1}}\\end{array}$ and applying the Sherman-Morrison formula. This leads to the following relation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\mathbb{E}_{\\mathcal{E}}[\\widetilde\\mathbf{x}])-L(\\mathbf{x}^{*})\\leq2\\underbrace{\\|\\mathbb{E}_{\\mathcal{E}}\\left[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\|^{2}}_{\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}}+2\\underbrace{\\left\\|\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\right\\|^{2}}_{\\|\\mathbf{Z}_{2}\\mathbf{r}\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma_{i}\\,=\\,1+\\frac{\\gamma}{m}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}\\end{array}$ . Due to the subspace embedding assumption and assuming $m$ large enough, we have $\\|\\mathbf{Q}\\|=O(1)$ and also $\\|\\mathbf{Q}_{-i}\\|=O(1)$ . The first term $\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}$ is quite straightforward to bound since, if not for the conditioning on the high probability event $\\mathcal{E}$ , we would have $\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]=\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{r}]=\\mathbf{0}$ , which follows from $\\mathbf{\\bar{U}}^{\\top}(\\mathbf{b}-\\mathbf{U}\\dot{\\mathbf{x}}^{*})=\\mathbf{0}$ . We get an upper bound on $\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}$ as $\\begin{array}{r}{O\\left(\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}+\\frac{d}{m^{2}}\\right)\\cdot\\|\\mathbf{r}\\|^{2}}\\end{array}$ , which is sufficient for us. ", "page_idx": 6}, {"type": "text", "text": "The central novelty of our analysis lies in bounding $\\left\\|\\mathbf{Z}_{2}\\mathbf{r}\\right\\|^{2}$ for $(s,\\beta_{1},\\beta_{2})$ -LESS embeddings, which is the dominant term. Our key observation is that, when examining a random variable of the form $\\mathbf{x}_{i}^{\\top}\\mathbf{v}$ for some vector $\\mathbf{v}$ , the dependence on the sparsity of row $\\mathbf{x}_{i}$ only arises when considering moments higher than $2+{\\frac{1}{O(\\log(n))}}$ , because otherwise we can simply rely on the fact that $\\mathbb{E}[{\\mathbf{x}}_{i}{\\mathbf{x}}_{i}^{\\top}]={\\mathbf{I}}$ . Thus, when decomposing $\\left\\|\\mathbf{Z}_{2}\\mathbf{r}\\right\\|^{2}$ , we must carefully separate the contribution of near-second moments vs the contribution of higher moments to the overall bound. ", "page_idx": 7}, {"type": "text", "text": "To obtain this separation, we start by applying H\u00f6lder\u2019s inequality on $\\|\\mathbf{Z}_{2}\\mathbf{r}\\|$ with $p=O(\\log(n))$ and $\\begin{array}{r}{q=1+\\frac{\\mathbf{\\hat{1}}}{O(\\log(n))}}\\end{array}$ to get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}_{2}\\mathbf{r}\\|\\leq\\left(\\mathbb{E}_{\\mathcal{E}}[\\left|\\frac{\\gamma}{\\gamma_{i}}-1\\right|^{p}]\\right)^{1/p}\\cdot\\left(\\operatorname*{sup}_{\\|\\mathbf{v}\\|=1}\\mathbb{E}_{\\mathcal{E}}\\left[\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]^{q}\\right)^{1/q}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore applying Cauchy-Schwarz inequality on the second term leads to ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{Z}_{2}\\mathbf{r}\\|\\leq\\left(\\mathbb{E}_{\\mathcal{E}}[\\left|\\frac{\\gamma}{\\gamma_{i}}-1\\right|^{p}]\\right)^{1/p}\\cdot\\left(\\mathbb{E}_{\\mathcal{E}}\\left\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\|^{2q}\\right)^{1/2q}\\cdot\\left(\\mathbb{E}_{\\mathcal{E}}\\left\\|\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right\\|^{2q}\\right)^{1/2q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Unlike [18], we exploit the fact that $\\left\\|\\mathbf{x}_{i}\\right\\|^{1/O(\\log(n))}=O(1)$ and get a constant upper bound on $(\\mathbb{E}_{\\mathcal{E}}\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\|^{2q})$ . However, this results in a much more careful argument, requiring now an upper bound on $\\begin{array}{r}{(\\mathbb{E}_{\\mathcal{E}}[\\left|\\frac{\\gamma}{\\gamma_{i}}-1\\right|^{p}])^{1/p}}\\end{array}$ for $p=O(\\log(n))$ . First, we observe that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{\\varepsilon}[\\left|\\frac{\\gamma}{\\gamma_{i}}-1\\right|^{p}]\\right)^{1/p}\\leq|\\gamma-\\bar{\\gamma}|+(\\mathbb{E}_{\\varepsilon}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right])^{1/p}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\gamma}=1+\\frac{\\gamma}{m}\\mathbb{E}_{\\mathcal{E}}\\left(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right)}\\end{array}$ . In particular, for the second term, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right])^{1/p}\\leq\\left(\\frac{\\gamma}{m}\\right)\\cdot\\left[\\left(\\mathbb{E}_{\\mathcal{E}}\\left[(\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbf{x}_{i}^{\\top}\\bar{\\mathbf{U}}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i})^{p}\\right]\\right)^{1/p}+\\left(\\mathbb{E}_{\\mathcal{E}}\\left[\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}}\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbf{x}_{i}^{\\top}\\bar{\\mathbf{U}}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right]\\right)^{1/p}\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To bound the first of these two terms, we prove a new version of the Restricted Bai-Silverstein inequality (Lemma 3) for $(s,\\beta_{1},\\beta_{2})$ -LESS embeddings. Unlike [18], we provide a proof with any $p$ and any $(\\beta_{1},\\beta_{2})$ values. Furthermore, utilizing the subspace embedding guarantee from Lemma 2, we prove a much more general result where the number of non-zeros in the approximate leverage score sparsifier $\\xi$ can be much smaller than $d$ (proof in Appendix D). ", "page_idx": 7}, {"type": "text", "text": "Lemma 3 (Restricted Bai-Silverstein for $(s,\\beta_{1},\\beta_{2})$ -LESS embeddings). Let $p\\in\\mathbb N$ be fixed and $\\textbf{U}\\in\\mathbb{R}^{n\\times d}$ be such that $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{I}.$ . Let $\\mathbf{x}_{i}\\,=\\,\\mathrm{diag}(\\pmb{\\xi})\\mathbf{y}_{i}$ where $\\mathbf{y}_{i}\\,\\in\\,\\mathbb{R}^{n}$ has independent $\\pm1$ entries and $\\xi$ is an $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier for U. Then for any matrix with $0\\preceq\\mathbf{C}\\preceq{\\mathcal{O}}(1)\\cdot\\mathbf{I}$ and any $\\delta>0$ we have for an absolute constant $c>0$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{C})-\\mathbf{x}_{i}^{\\top}\\mathbf{UCU}^{\\top}\\mathbf{x}_{i}\\right]^{p}\\right)^{1/p}<c\\cdot\\sqrt{d}p^{3}\\cdot\\left(1+\\sqrt{\\frac{d p\\log(d/\\delta)}{s}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using Lemma 3, we upper bound the first term squared in (3) as $\\begin{array}{r}{\\tilde{O}\\left(\\frac{d}{m^{2}}\\left(1+\\frac{d}{s}\\right)\\right)}\\end{array}$ . Moreover, also using Lemma 3, we get a matching upper bound on $|\\gamma-\\bar{\\gamma}|$ . We then design a martingale concentration argument to prove a high probability upper bound on the last remaining term, $\\boxed{\\mathrm{tr}(\\mathbf{Q}_{-i}^{\\bar{}})\\!-\\!\\mathbb{E}_{\\mathcal{E}}\\mathrm{tr}(\\mathbf{Q}_{-i})}$ , which implies the desired moment bound (proof in Appendix B), concluding the proof of Theorem 4. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4. For given $\\delta>0$ and matrix $\\mathbf{Q}_{-i}$ we have with probability $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n|\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}\\mathrm{tr}(\\mathbf{Q}_{-i})|\\leq c^{\\prime}\\gamma\\cdot\\frac{d}{\\sqrt{m}}\\log^{4.5}(m/\\delta).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Completing the proof of Theorem 1. First, suppose that $\\epsilon\\;\\geq\\;O(\\mathrm{polylog}(d)/d)$ so that the bias bound can be achieved from Theorem 4. Our implementation is mainly based on the online construction of approximate leverage scores, given the preconditioner $\\mathbf{P}$ , using Lemma 1. Briefly, this construction proceeds by first sketching $\\mathbf{P}$ using a $\\bar{d}\\times O(1/\\gamma)$ Gaussian matrix $\\mathbf{G}$ to produce the matrix $\\tilde{\\mathbf{P}}=\\mathbf{P}\\mathbf{G}$ , and then, for each observed row ${\\bf a}_{i}$ of $\\mathbf{A}$ , we compute $\\tilde{l}_{i}=\\|\\mathbf{a}_{i}^{\\top}\\tilde{\\mathbf{P}}\\|^{2}$ . Assuming without loss of generality that $d=\\mathrm{poly}(n)$ and adjusting $\\gamma$ , the estimates satisfy $\\dot{\\beta_{1}}\\beta_{2}=O(\\alpha d^{\\gamma})$ . ", "page_idx": 7}, {"type": "text", "text": "Next, we sample the non-zero entries of S corresponding to the observed row ${\\bf a}_{i}$ , i.e., the $i$ -th column of S. Note that for this we only need to know the single leverage score estimate $\\tilde{l}_{i}$ . Crucially for our analysis, the entries of this column need to be sampled i.i.d., which can be done in time proportional to the number of non-zeros in that column by first sampling a corresponding Binomial distribution to determine how many non-zeros we need, then picking a random subset of that size, and then sampling the random $\\pm1$ values. Altogether, the cost of constructing the sketch is $O(\\gamma^{-1}\\mathrm{nnz}(\\mathbf{A})\\!+\\!\\beta_{1}\\beta_{2}s\\dot{d}^{2})\\stackrel{\\mathrm{~=~}}{=}$ $O(\\gamma^{-1}\\mathrm{nnz}(\\mathbf{A})\\!+\\!\\alpha\\epsilon^{-1}d^{2+}\\bar{\\gamma}\\mathrm{polylog}(d))$ by setting $s=O(\\mathrm{polylog}(d)/\\epsilon)$ . Finally, once we construct the sketch, at the end of the pass we can run conjugate gradient preconditioned with $\\mathbf{P}$ on the sketched problem, which takes $\\tilde{O}(\\alpha\\bar{d}^{2})$ . ", "page_idx": 8}, {"type": "text", "text": "We note that in the (somewhat artificial) regime where we require extremely small bias, i.e., $\\epsilon=$ $o(\\mathrm{polylog}(d)/d)$ , the bound claimed in\u221a Theorem 1 can still be obtained, since in this case for small enough $\\gamma$ we have $d^{2+\\gamma}/\\epsilon\\,=\\,{\\cal O}(d^{\\omega}/\\sqrt{\\epsilon})$ with $\\omega\\,<\\,2.5$ , so we can rely on direct leverage score sampling (which corresponds to $s\\,=\\,1$ ), and instead of maintaining the sketch, we compute the estimator $\\tilde{\\mathbf{x}}=(\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}})^{-1}\\tilde{\\mathbf{A}}^{\\top}\\mathbf{b}$ directly along the way. This involves performing a separate $d\\times d$ matrix multiplication after collecting each $d$ leverage score samples, to gradually compute $\\tilde{\\mathbf{A}}^{\\top}\\tilde{\\mathbf{A}}$ , and then inverting the matrix at the end. From Theorem 4, we see that it suffices to set sketch size $m=\\tilde{O}(d/\\sqrt{\\epsilon})$ , which leads to the desired runtime. ", "page_idx": 8}, {"type": "text", "text": "Completing the proof of Theorem 2. Here, we use a slightly modified variant of Lemma 2, given as Theorem 1.4 in [9], which shows that using a single pass we can compute a sketch $\\tilde{\\mathbf A}$ in time $O(\\mathrm{nnz}(\\mathbf{A})+d^{\\omega})$ , which satisfies the subspace embedding property (1) with $\\begin{array}{r}{\\eta=\\frac{1}{2}}\\end{array}$ . Then, we can perform the QR decomposition $\\tilde{\\mathbf{A}}=\\mathbf{Q}\\mathbf{R}$ and set ${\\bf P}={\\bf R}^{-1}$ in additional time $O(d^{\\omega})$ to obtain the desired preconditioner. Next, we use Theorem 1 to construct $q$ i.i.d. estimators $\\tilde{\\mathbf{x}}_{i}$ in a second parallel pass, and finally, the estimators are aggregated to compute $\\begin{array}{r}{\\hat{\\mathbf{x}}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{x}}_{i}}\\end{array}$ , which satisfies $\\mathbb{E}\\|\\mathbf{A}\\hat{\\mathbf{x}}-\\mathbf{b}\\|^{2}\\leq\\left(1+\\epsilon+O(1/q)\\right)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}$ . Applying Markov\u2019s inequality concludes the proof. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we illustrate empirically how our results point to a practical free lunch phenomenon in distributed averaging of sketching-based estimators. As mentioned in Section 1, our construction from Theorem 1 essentially works by taking a subsample of the data and then mixing groups of those rows together to produce an even smaller sketch (see Figure 1). According to our theory, while the small sketch does not recover the same $\\epsilon_{\\mathrm{:}}$ -small error as the larger subsample, it does recover an $\\epsilon$ -small bias. Moreover, this happens without incurring any additional computational cost, as the cost of the sketching is proportional to the cost of simply reading the subsampled rows. This suggests that we can use sparse sketching to compress a data subsample down to a small size while retaining the least squares performance in a distributed averaging environment. ", "page_idx": 8}, {"type": "text", "text": "To verify this, we evaluate the effectiveness of distributed averaging of sketched least squares estimators on several benchmark datasets. Specifically, we visualize the relative error of the averaged sketch-and-solve estimator $\\begin{array}{r}{\\frac{L(\\hat{\\mathbf{x}})-L(\\mathbf{x}^{*})}{L(\\mathbf{x}^{*})}}\\end{array}$ , against the number of machines $q$ used to generate the estimate $\\begin{array}{r}{\\hat{\\mathbf{x}}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{x}}_{i}}\\end{array}$ . Each estimate $\\tilde{\\mathbf{x}}_{i}$ is constructed with the same sparsification strategy used by LESS, except that instead of sparsifying the sketch with leverage scores, we instead sparsify them with uniform probabilities (which is often sufficient in practice). Following [16], we call the resulting method LESSUniform. Within each dataset, we perform four simulations, designed so that the total sketching cost stays the same for all four test cases, by simultaneously changing sketch size and sparsity. Concretely, we vary these so that the product (sketch $\\mathrm{size}\\times\\mathrm{nnz}$ per row) stays the same in each case, so as to ensure that the total cost of sketching is fixed in each plot. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2, on the X-axis we plot the number $q$ of estimators being averaged, so that the bias of a single estimator appears on the right-hand side of the plot (large $q$ ), whereas the variance (error) appears on the left-hand side ${\\mathit{\\Gamma}}_{q}=1{\\mathit{\\Gamma}}_{}$ ). In each plot, the line with nnz per $\\mathrm{row}=1$ (and large sketch size) corresponds to uniform subsampling, whereas the remaining ones are sketches produced by compressing that subsample. The plot shows that decreasing the sketch size (i.e., compressing the sample) does increase the error of a single estimator (as expected), however it also shows that the bias of these estimators remains essentially unchanged regardless of the sketch size (since all lines meet as $q\\rightarrow\\infty$ ), confirming that suitable sparse sketches that \u201ccompress\u201d rows of data can preserve near-unbiasedness without increasing the cost. ", "page_idx": 8}, {"type": "image", "img_path": "rkuVYosT2c/tmp/64186960b0bec27d08ac3e7c310e49b63f5a9b91cbab8293b01ae50de54e2dc7.jpg", "img_caption": ["Figure 2: Distributed averaging experiment on YearPredictionMSD and Abalone datasets [8], showing that sparse sketching can be used to compress the data while preserving near-unbiasedness without increasing the estimation cost (see Appendix F for results on the Boston dataset). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "This phenomenon may not occur for all sketching methods. Figure 4 within Appendix F showcases a few more interesting results. We first further demonstrate that suitable sketches that \u201ccompress\u201d rows of data A into a single row of $\\tilde{\\mathbf A}$ can preserve near-unbiasedness without increasing the cost. In particular, this desirable phenomenon that LESSUniform enjoys also extends to LESS proper, as well as the Gaussian and Subgaussian (Rademacher) sketches. In fact, we observe that LESS enjoys similar desirable performance as the Gaussian and Subgaussian sketches and virtually no least squares bias, while retaining the computational speedups of sparse sketching, suggesting that it attains the best of both worlds. ", "page_idx": 9}, {"type": "text", "text": "However, when we decrease the number of subsamples within leverage score subsampling, the bias introduced by subsampling increases as expected. This happens as the number of subsamples is reduced without increasing the amount of \u201ccompression\u201d as one would with LESS or LESSUniform.1 We also show that the subsampled randomized Hadamard transform (SRHT) can exhibit some amount of least squares bias as the sketch size decreases. The numerical results shown for the bias introduced by leverage score subsampling and the SRHT complement the lower bounds established in [18]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gave a new sparse sketching method that, using two passes over the data, produces a nearlyunbiased least squares estimator, which can be used to improve upon the space-time trade-offs of solving least squares in parallel or distributed environments via simple averaging. In particular, our algorithm is the first to require only $O(d^{2}\\log(n d))$ bits of space and current matrix multiplication time $O(d^{\\omega})$ while obtaining an $\\epsilon=o(1)$ approximation in few passes. Our techniques are of broader interest to sketching-based optimization algorithms, including Distributed Newton Sketch. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF CAREER CCF-2338655. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of computer and System Sciences, 66(4):671\u2013687, 2003. [2] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time. The Journal of Machine Learning Research, 18(1):4148\u20134187, 2017.   \n[3] Nir Ailon and Bernard Chazelle. The fast Johnson\u2013Lindenstrauss transform and approximate nearest neighbors. SIAM Journal on computing, 39(1):302\u2013322, 2009.   \n[4] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 775\u2013783, 2015.   \n[5] Nima Anari, Yang P Liu, and Thuy-Duong Vuong. Optimal sublinear sampling of spanning trees and determinantal point processes via average-case entropic independence. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 123\u2013134. IEEE, 2022.   \n[6] Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices, volume 20. Springer, 2010.   \n[7] Daniele Calandriello, Michal Derezinski, and Michal Valko. Sampling from a k-dpp without looking at all items. Advances in Neural Information Processing Systems, 33:6889\u20136899, 2020.   \n[8] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 2011.   \n[9] Shabarish Chenakkod, Micha\u0142 Derezin\u00b4ski, Xiaoyu Dong, and Mark Rudelson. Optimal embedding dimension for sparse subspace embeddings. In 56th Annual ACM Symposium on Theory of Computing, 2024.   \n[10] Nadiia Chepurko, Kenneth L Clarkson, Praneeth Kacham, and David P Woodruff. Near-optimal algorithms for linear algebra in the current matrix multiplication time. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 3043\u20133068. SIAM, 2022.   \n[11] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205\u2013214, 2009.   \n[12] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the forty-ffith annual ACM symposium on Theory of Computing, pages 81\u201390, 2013.   \n[13] Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1758\u20131777. SIAM, 2017.   \n[14] Michael B Cohen and Richard Peng. Lp row sampling by lewis weights. In Proceedings of the symposium on Theory of computing, pages 183\u2013192, 2015.   \n[15] Micha\u0142 Derezin\u00b4ski. Stochastic variance-reduced newton: Accelerating finite-sum minimization with large batches. arXiv preprint arXiv:2206.02702, 2022.   \n[16] Micha\u0142 Derezin\u00b4ski, Jonathan Lacotte, Mert Pilanci, and Michael W Mahoney. Newton-LESS: Sparsification without trade-offs for the sketched newton update. Advances in Neural Information Processing Systems, 34:2835\u20132847, 2021.   \n[17] Micha\u0142 Derezin\u00b4ski, Feynman T Liang, Zhenyu Liao, and Michael W Mahoney. Precise expressions for random projections: Low-rank approximation and randomized newton. Advances in Neural Information Processing Systems, 33, 2020.   \n[18] Micha\u0142 Derezin\u00b4ski, Zhenyu Liao, Edgar Dobriban, and Michael Mahoney. Sparse sketches with small inversion bias. In Conference on Learning Theory, pages 1467\u20131510. PMLR, 2021.   \n[19] Micha\u0142 Derezi\u00b4nski and Michael W Mahoney. Distributed estimation of the inverse Hessian by determinantal averaging. In Advances in Neural Information Processing Systems 32, pages 11401\u201311411. 2019.   \n[20] Micha\u0142 Derezi\u00b4nski and Michael W Mahoney. Determinantal point processes in randomized numerical linear algebra. Notices of the American Mathematical Society, 68(1):34\u201345, 2021.   \n[21] Micha\u0142 Derezin\u00b4ski and Michael W Mahoney. Recent and upcoming developments in randomized numerical linear algebra for machine learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6470\u20136479, 2024.   \n[22] Micha\u0142 Derezi\u00b4nski and Elizaveta Rebrova. Sharp analysis of sketch-and-project methods via a connection to randomized singular value decomposition. SIAM Journal on Mathematics of Data Science, 6(1):127\u2013153, 2024.   \n[23] Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Unbiased estimates for linear regression via volume sampling. In Advances in Neural Information Processing Systems 30, pages 3087\u20133096, 2017.   \n[24] Micha\u0142 Derezin\u00b4ski and Jiaming Yang. Solving dense linear systems faster than via preconditioning. In 56th Annual ACM Symposium on Theory of Computing, 2024.   \n[25] Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares regression. Advances in Neural Information Processing Systems, 32, 2019.   \n[26] Petros Drineas and Michael W Mahoney. Randnla: randomized numerical linear algebra. Communications of the ACM, 59(6):80\u201390, 2016.   \n[27] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Sampling algorithms for $\\ell_{2}$ regression and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, pages 1127\u20131136, 2006.   \n[28] Robert M Gower and Peter Richt\u00e1rik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660\u20131690, 2015.   \n[29] William B Johnson, Gideon Schechtman, and Joel Zinn. Best constants in moment inequalities for linear combinations of independent and exchangeable random variables. The Annals of Probability, pages 234\u2013253, 1985.   \n[30] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In Proceedings of the 28th International Conference on Machine Learning, pages 1193\u20131200, June 2011.   \n[31] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publishers Inc., Hanover, MA, USA, 2012.   \n[32] Jonathan Lacotte, Sifan Liu, Edgar Dobriban, and Mert Pilanci. Optimal iterative sketching methods with the subsampled randomized Hadamard transform. Advances in Neural Information Processing Systems, 33:9725\u20139735, 2020.   \n[33] Daniel LeJeune, Pratik Patil, Hamid Javadi, Richard G Baraniuk, and Ryan J Tibshirani. Asymptotics of the sketched pseudoinverse. arXiv preprint arXiv:2211.03751, 2022.   \n[34] Yi Li and David Woodruff. Input-sparsity low rank approximation in schatten norm. In International Conference on Machine Learning, pages 6001\u20136009. PMLR, 2020.   \n[35] Miles E Lopes, Shusen Wang, and Michael W Mahoney. A bootstrap method for error estimation in randomized matrix multiplication. The Journal of Machine Learning Research, 20(1):1434\u2013 1473, 2019.   \n[36] Ping Ma, Yongkai Chen, Xinlian Zhang, Xin Xing, Jingyi Ma, and Michael W Mahoney. Asymptotic analysis of sampling estimators for randomized numerical linear algebra algorithms. The Journal of Machine Learning Research, 23(1):7970\u20138014, 2022.   \n[37] Per-Gunnar Martinsson and Joel A Tropp. Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica, 29:403\u2013572, 2020.   \n[38] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression. In Proceedings of the Symposium on Theory of Computing, STOC \u201913, pages 91\u2013100, 2013.   \n[39] R. Murray, J. Demmel, M. W. Mahoney, N. B. Erichson, M. Melnichenko, O. A. Malik, L. Grigori, M. Derezin\u00b4ski, M. E. Lopes, T. Liang, and H. Luo. Randomized Numerical Linear Algebra \u2013 a perspective on the field with an eye to software. Technical Report arXiv preprint arXiv:2302.11474, 2023.   \n[40] Jelani Nelson and Huy L Nguy\u00ean. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science, pages 117\u2013126. IEEE, 2013.   \n[41] G. Raskutti and M. W. Mahoney. A statistical perspective on randomized sketching for ordinary least-squares. Journal of Machine Learning Research, 17(214):1\u201331, 2016.   \n[42] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS\u201906), pages 143\u2013152. IEEE, 2006.   \n[43] S. Wang, A. Gittens, and M. W. Mahoney. Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging. Journal of Machine Learning Research, 18(218):1\u2013 50, 2018.   \n[44] Shusen Wang, Fred Roosta, Peng Xu, and Michael W Mahoney. GIANT: globally improved approximate newton method for distributed optimization. Advances in Neural Information Processing Systems, 31:2332\u20132342, 2018.   \n[45] David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends\u00ae in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Detailed preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We start by providing several classical results, used in our analysis. The following formula provides a way to compute the inverse of matrix A after a rank-1 update, given the inverse before the update. ", "page_idx": 13}, {"type": "text", "text": "Lemma 5 (Sherman-Morrison formula). For an invertible matrix $\\mathbf{A}\\in\\mathbb{R}^{d\\times d}$ and vector $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{d}$ , $\\mathbf{A}+\\mathbf{u}\\mathbf{v}^{\\top}$ is invertible if and only $i f\\,1+\\mathbf{v}^{\\top}\\mathbf{A}^{-1}\\mathbf{u}\\neq0.$ . If this holds then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\mathbf{A}+\\mathbf{u}\\mathbf{v}^{\\top})^{-1}=\\mathbf{A}^{-1}-\\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^{\\top}\\mathbf{A}^{-1}}{1+\\mathbf{v}^{\\top}\\mathbf{A}^{-1}\\mathbf{u}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\mathbf{A}+\\mathbf{u}\\mathbf{v}^{\\top})^{-1}\\mathbf{u}=\\frac{\\mathbf{A}^{-1}\\mathbf{u}}{1+\\mathbf{v}^{\\top}\\mathbf{A}^{-1}\\mathbf{u}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following inequality provides a crucial tool for writing expectation of the product of two random variables as the product of higher individual moments. ", "page_idx": 13}, {"type": "text", "text": "Lemma 6 (H\u00f6lder\u2019s inequality). For real-valued random variables $X$ and $Y$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[|X Y|]\\le(\\mathbb{E}[|X|^{p}])^{1/p}\\cdot(\\mathbb{E}[|Y|^{q}])^{1/q}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $p,q>0$ are H\u00f6lder\u2019s conjugates, i.e. $\\textstyle{\\frac{1}{p}}+{\\frac{1}{q}}=1$ ", "page_idx": 13}, {"type": "text", "text": "The following technical lemmas provide concentration results for the sum of random quantities. We collect these results here and then refer to them while using in our analysis. ", "page_idx": 13}, {"type": "text", "text": "Lemma 7 (Matrix Chernoff Inequality). For $i=1,2,\\cdots\\,,n$ consider a sequence $\\mathbf{Z}_{i}$ of $d\\times d$ positive semi-definite random matrices such that $\\begin{array}{r}{\\mathbb{E}[\\frac{1}{n}\\sum_{i}\\mathbf{Z}_{i}]=\\mathbf{I}_{d}}\\end{array}$ and $\\|\\mathbf{Z}_{i}\\|\\leq R.$ Then for any $\\epsilon>0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\lambda_{\\operatorname*{max}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{Z}_{i}\\right)\\geq(1+\\epsilon)\\right)\\leq d\\cdot\\exp\\left(-\\frac{n\\epsilon^{2}}{(2+\\epsilon)R}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\lambda_{m a x}$ denotes taking the maximum eigenvalue. ", "page_idx": 13}, {"type": "text", "text": "Lemma 8 (Azuma\u2019s inequality). If $\\{Y_{0},Y_{1},Y_{2},\\cdot\\cdot\\cdot\\}$ is a martingale with $|Y_{j}-Y_{j-1}|\\leq c_{j}$ then for any $m,\\lambda>0$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|Y_{m}-Y_{0}\\right|\\geq\\lambda\\right)\\leq2\\cdot\\exp\\left(-{\\frac{\\lambda^{2}}{2\\sum_{j=1}^{m}c_{j}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 9 (Rosenthal\u2019s inequality ([29], Theorem 2.5 and Corollary 2.6)). Let $1\\leq p<\\infty$ and $X_{1},X_{2},\\cdot\\cdot\\cdot,X_{n}$ are nonnegative, independent random variables with finite $p^{t h}$ moments then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\sum_{i}X_{i}\\right]^{p}\\right)^{1/p}\\leq\\frac{2p}{\\log(p)}\\cdot\\operatorname*{max}\\left\\{\\sum_{i}\\mathbb{E}[X_{i}],\\left(\\sum_{i}\\mathbb{E}[X_{i}^{p}]\\right)^{1/p}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, for mean-zero independent and symmetric random variables we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\sum_{i}X_{i}\\right]^{p}\\right)^{1/p}\\leq\\frac{2p}{\\sqrt{\\log(p)}}\\cdot\\operatorname*{max}\\left\\{\\left(\\sum_{i}\\mathbb{E}[X_{i}^{2}]\\right)^{1/2},\\left(\\sum_{i}\\mathbb{E}[X_{i}^{p}]\\right)^{1/p}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 10 (Bai-Silverstein\u2019s Inequality Lemma B.26 from [6]). Let $\\mathbf{B}$ be a $d\\times d$ be a fixed matrix and $\\mathbf{x}$ be a random vector of independent entries. Let $\\mathbb{E}[x_{i}]=0$ and $\\mathbb{E}[x_{i}^{2}]=1,\\!a n d\\,\\bar{\\mathbb{E}}|x_{j}|^{l}\\leq\\nu_{l}$ Then for any $p\\geq1$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}|{\\mathbf{x}}^{\\top}\\mathbf{B}{\\mathbf{x}}-\\operatorname{tr}(\\mathbf{B})|^{p}\\leq(2p)^{p}\\cdot\\left((\\nu_{4}\\mathrm{tr}(\\mathbf{B}\\mathbf{B}^{\\top}))^{p/2}+\\nu_{2p}\\mathrm{tr}(\\mathbf{B}\\mathbf{B}^{\\top})^{p/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Inversion bias analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the notion of an $(\\epsilon,\\delta)$ unbiased estimator of $\\mathbf{A}$ as defined in [18]. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 ( $[\\epsilon,\\delta)$ -unbiased estimator). For $\\epsilon,\\delta>0,$ , a random positive definite matrix $\\mathbf{B}\\in\\mathbb{R}^{d\\times d}$ is called an $(\\epsilon,\\delta)$ unbiased estimator of A if there exists an event $\\mathcal{E}$ with $\\operatorname*{Pr}({\\mathcal{E}})\\geq1-\\delta$ such that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\epsilon}\\mathbf{A}\\preceq\\mathbb{E}_{\\mathcal{E}}[\\mathbf{B}]\\preceq(1+\\epsilon)\\mathbf{A}\\quad a n d,\\quad\\mathbf{B}\\preceq O(1)\\cdot\\mathbf{A},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "when conditioned on the event $\\mathcal{E}$ . ", "page_idx": 14}, {"type": "text", "text": "In this section, we give a formal statement and proof for Theorem 3, which is then used in the proof of Theorem 4. We replace A with $\\mathbf{U}$ such that $\\mathbf{U}$ consists of $d$ orthonormal columns spanning the column space of $\\mathbf{A}$ . Here $\\mathbf{S}_{m}\\in\\mathbb{R}^{m\\times n}$ denotes a LESS sketching matrix with independent rows $\\scriptstyle{\\frac{1}{\\sqrt{m}}}\\mathbf{x}^{\\top}$ , $\\mathbf{x}^{\\top}\\,=\\,\\mathbf{y}^{\\top}\\,\\cdot\\,\\mathrm{diag}(\\pmb{\\xi})$ where $\\mathbf{y}$ consists of $\\pm1$ Rademacher entries and $\\xi$ is an $(s,\\beta_{1},\\beta_{2})$ - approximate leverage score sparsifier. Note that $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]=\\mathbf{I}_{n}$ . We assume that the sketching matrix $\\mathbf{S}_{m}$ consists of $m\\ge10d$ i.i.d rows and 3 divides $m$ . Also, we assume that the $\\mathbf{S}_{m}$ satisfies the subspace embedding condition for $\\mathbf{U}$ (Theorem 2) with $\\begin{array}{r}{\\eta=\\frac{1}{2}}\\end{array}$ . Let $\\mathbf{Q}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}$ where $\\textstyle\\gamma={\\frac{m}{m-d}}$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 5 (Small inversion bias for $(s,\\beta_{1},\\beta_{2})$ -LESS embeddings). Let $\\delta>0$ satisfy $\\delta<\\frac{1}{m^{4}}$ and $m\\geq O(d)$ . Let $\\mathbf{S}_{m}\\in\\mathbb{R}^{m\\times n}$ be an $(s,\\beta_{1},\\beta_{2})$ -LESS embedding for data matrix $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ such that $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{I}.$ . Then there exists an event $\\mathcal{E}$ with $\\operatorname*{Pr}({\\mathcal{E}})\\geq1-\\delta$ such that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{1+\\epsilon}}\\cdot\\mathbf{I}\\preceq\\mathbb{E}_{\\varepsilon}[\\mathbf{Q}]\\preceq(1+\\epsilon)\\cdot\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\mathbf{S}_{-i}$ denote $\\mathbf{S}_{m}$ without the $i^{t h}$ row, and $\\mathbf{S}_{-i j}$ denote $\\mathbf{S}_{m}$ with the $i^{t h}$ and $j^{t h}$ rows removed. Let $\\mathbf{Q}_{-i}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i}^{\\top}\\mathbf{S}_{-i}\\mathbf{U})^{-1}$ and $\\mathbf{Q}_{-i j}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i j}^{\\top}\\mathbf{S}_{-i j}\\mathbf{U})^{-1}$ . We proceed with the same proof strategy as adopted in [18]. We define the events $\\mathscr{E}_{j}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{E}_{j}=\\frac{3}{m}\\mathbf{U}^{\\top}\\left(\\sum_{i=t(j-1)+1}^{t j}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\right)\\mathbf{U}\\succeq\\frac{1}{2}\\mathbf{I},\\ j=1,2,3\\quad,\\mathcal{E}=\\wedge_{j=1}^{3}\\mathcal{E}_{j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that event $\\mathcal{E}_{j}$ means that the sketching matrix with just $(1/3)^{r d}$ rows (scaled to maintain unbiasedness of the sketch) from $\\mathbf{S}_{m}$ satisfies a lower spectral approximation of $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{I}.$ . Also we notice that events $\\mathcal{E}_{1},\\mathcal{E}_{2},\\mathcal{E}_{3}$ are independent, and for any pair $(i,j)$ there exists at least one event $\\mathcal{E}_{k}$ $,\\;k\\in\\{1,2,3\\}$ such that $\\mathcal{E}_{k}$ is independent of both $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ . Furthermore conditioned on $\\mathcal{E}_{k}$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{-i}\\preceq6\\cdot\\mathbf{I}_{d}\\;\\;\\mathrm{and}\\;\\;\\mathbf{Q}_{-i j}\\preceq6\\cdot\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that as guaranteed in Theorem 2, we have $\\operatorname*{Pr}({\\mathcal{E}}_{k})\\geq1-\\delta^{\\prime}$ for all $k$ and therefore $\\operatorname*{Pr}({\\mathcal{E}})\\geq1-\\delta$ with $\\delta^{\\prime}=\\delta/3$ . Let $\\mathbb{E}_{\\mathcal{E}}$ denote the expectation conditioned on the event $\\mathcal{E}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{I}-\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}]=-\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}]+\\gamma\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U}]}\\\\ &{\\quad\\quad\\quad=-\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}]+\\gamma\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U}]}\\\\ &{\\quad\\quad\\quad=-\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}]+\\gamma\\mathbb{E}_{\\boldsymbol\\varepsilon}\\big[\\frac{\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{X}_{i}^{\\top}\\mathbf{U}}{1+\\frac{\\gamma}{m}\\mathbf{X}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}\\big]}\\\\ &{\\quad\\quad\\quad=\\underbrace{\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}(\\mathbf{x}_{i}\\mathbf{X}_{i}^{\\top}-\\mathbf{I})\\mathbf{U}]}_{\\mathbf{Z}_{0}}+\\underbrace{\\mathbb{E}_{\\boldsymbol\\varepsilon}[\\mathbf{Q}_{-i}-\\mathbf{Q}]}_{\\mathbf{Z}_{1}}+\\underbrace{\\mathbb{E}_{\\boldsymbol\\varepsilon}\\big[\\big(\\frac{\\gamma}{\\gamma_{i}}-1\\big)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{X}_{i}^{\\top}\\mathbf{U}\\big]}_{\\mathbf{Z}_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma_{i}\\ =\\ 1\\,+\\,\\frac{\\gamma}{m}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}\\end{array}$ . The second equality follows by noting that ${\\bf S}_{m}^{\\top}{\\bf S}_{m}\\;\\;=\\;$ $\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}$ and using linearity of expectation. The third equality holds due to the application of Sherman Morrison\u2019s (Lemma 5) formula on $\\begin{array}{r}{\\mathbf{Q}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i}^{\\top}\\mathbf{S}_{-i}\\mathbf{U}+\\frac{\\gamma}{m}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U})^{-1}}\\end{array}$ . We start by upper bounding $\\mathbf{Z}_{\\mathrm{0}}$ and use the following from [18]. ", "page_idx": 14}, {"type": "text", "text": "Lemma 11 (Upper bound on $\\|\\mathbf{Z}_{0}\\|,$ ). For any $k>0$ we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}_{\\mathcal{E}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}(\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}-\\mathbf{I})\\mathbf{U}]\\right\\|\\le12\\left(k\\delta^{\\prime}+\\int_{k}^{\\infty}\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\ge x)d x\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Chebyshev\u2019s inequality we have, $\\begin{array}{r}{\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\geq x)\\leq\\frac{\\operatorname{Var}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i})}{x^{2}}}\\end{array}$ . Using Restricted Bai-Silverstein inequality for $(s,\\beta_{1},\\beta_{2})$ -approximate LESS embeddings i.e., Lemma 3 (proved in Lemma 16) with p = 2, we have Var(xi\u22a4 UU\u22a4xi) \u2264cd\u00b7 1 + d logs(d/\u03b4) for some absolute constant $c$ . Let $k=m^{2}$ and $\\delta^{\\prime}<{\\textstyle\\frac{1}{m^{4}}}$ we get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}_{0}\\|={\\mathcal{O}}\\left({\\frac{1}{m^{2}}}+{\\frac{d}{m^{2}}}+{\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}}\\right)={\\mathcal{O}}\\left({\\frac{d}{m^{2}}}+{\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We use the bound on the term $\\|\\mathbf{Z}_{1}\\|$ directly from [18], provided below as a Lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 12 (Upper bound on $\\|\\mathbf{Z}_{1}\\|$ , [18]). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}_{\\varepsilon^{\\prime}}[\\mathbf{Q}_{-i}-\\mathbf{Q}]\\|=\\mathcal{O}(1/m).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It remains to upper bound $\\left\\|\\mathbf{Z}_{2}\\right\\|$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{Z}_{2}\\|=\\|\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\right]\\|\\leq\\underset{\\|\\mathbf{v}\\|=1,\\ \\|\\mathbf{z}\\|=1}{\\operatorname*{sup}}\\mathbb{E}_{\\mathcal{E}}[\\left|\\frac{\\gamma}{\\gamma_{i}}-1\\right|\\cdot|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{z}|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying H\u00f6lder\u2019s inequality with $p={\\mathcal{O}}(\\log(n))$ and $\\begin{array}{r}{q=\\frac{p}{p-1}=1+\\Delta}\\end{array}$ for $\\begin{array}{r}{\\Delta=\\frac{1}{\\mathcal{O}(\\log(n))}}\\end{array}$ O(log(n)), we get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{Z}_{2}\\|\\leq\\underset{\\|\\mathbf{v}\\|=1,\\;\\|\\mathbf{z}\\|=1}{\\operatorname*{sup}}\\left(\\mathbb{E}_{\\mathcal{E}}\\big[|\\frac{\\mathcal{I}}{\\gamma_{i}}-1|^{p}\\big]\\right)^{1/p}\\cdot\\big(\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{z}|^{q}\\big]\\big)^{1/q}}\\\\ &{\\qquad\\leq\\bigg(\\mathbb{E}_{\\mathcal{E}}\\big[|\\frac{\\mathcal{I}}{\\gamma_{i}}-1|^{p}\\big]\\bigg)^{1/p}\\cdot\\underset{\\underset{\\mathcal{O}(1)}{\\operatorname*{sup}}}{\\operatorname*{sup}}\\left(\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2q}\\big]\\right)^{1/2q}\\cdot\\underset{\\mathcal{O}(1)}{\\operatorname*{sup}}\\;\\frac{\\big(\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{z}|^{2q}\\big]\\big)^{1/2q}}{\\mathcal{O}(1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used Cauchy-Schwarz inequality on $\\mathbb{E}_{\\mathcal{E}}\\left[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{z}|^{q}\\right]$ . Now note that $\\mathbf{x}_{i}=$ $\\mathrm{diag}(\\boldsymbol{\\xi})\\cdot\\mathbf{y}_{i}$ , we have $\\|\\mathbf{x}_{i}\\|\\,=\\,\\mathrm{poly}(n)$ and therefore $\\|\\mathbf{x}_{i}\\|^{\\Delta}\\,=\\,\\mathcal{O}(1)$ . We now show the terms involving exponents depending on $q$ are $\\mathcal{O}(1)$ as highlighted in the inequality (6). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\small\\left(\\underset{\\mathrm{\\tiny\\{\\|\\mathbf{v}\\|=1}}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2q}\\big]\\right)^{1/2q}\\leq\\left(\\underset{\\mathrm{\\tiny\\{\\|\\mathbf{v}\\|=1}}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2}\\cdot\\big|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2\\Delta}\\big]\\right)^{1/2q}}\\\\ &{\\leq\\left(\\underset{\\mathrm{\\tiny\\{\\|\\mathbf{v}\\|=1}}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathcal{E}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2}\\cdot\\big\\|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\|^{2\\Delta}\\cdot\\|\\mathbf{x}_{i}\\|^{2\\Delta}\\big]\\right)^{1/2q}}\\\\ &{\\leq\\mathcal{O}(1)\\cdot\\big(\\mathbb{E}_{\\mathcal{E}}\\big[\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\|^{2}\\cdot\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\|^{2\\Delta}\\big]\\big)^{1/2q}}\\\\ &{\\leq\\mathcal{O}(1)\\cdot\\left(2\\cdot\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\big[\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\|^{2}\\cdot\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\|^{2\\Delta}\\big]\\right)^{1/2q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here ${\\mathcal{E}}^{\\prime}$ is an event independent of $\\mathbf{x}_{i}$ . Without loss of generality, we can assume that $\\mathcal{E}^{'}=\\mathcal{E}_{1}\\wedge\\mathcal{E}_{2}$ . We first condition on $\\mathbf{Q}_{-i}$ and take expectation over $\\mathbf{x}_{i}$ . Also note that $\\mathbf{Q}_{-i}$ and $\\mathbf{x}_{i}$ are independent and event $\\mathcal{E}^{'}$ is independent of $\\mathbf{x}_{i}$ , and furthermore $\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}]=\\mathbb{E}[\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}]=1$ . We get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\operatorname*{sup}_{\\|\\mathbf{v}\\|=1}\\mathbb{E}_{\\mathcal{E}}\\left[\\left|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right|^{2q}\\right]\\right)^{1/2q}\\leq\\mathcal{O}(1)\\cdot\\left(\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\|^{2q}]\\right)^{1/2q}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we use that conditioned on $\\mathcal{E}^{'},\\,\\Vert\\mathbf{Q}_{-i}\\Vert\\leq6$ and $\\|\\mathbf{U}^{\\top}\\|=1$ , we get, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\operatorname*{sup}_{\\|\\mathbf{v}\\|=1}\\mathbb{E}_{\\boldsymbol{\\varepsilon}}\\big[|\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}|^{2q}\\big]\\right)^{1/2q}=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, using $\\|\\mathbf{x}_{i}\\|^{\\Delta}=\\mathcal{O}(1)$ and $\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}]=\\mathbb{E}[\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}]=1,$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\|\\mathbf{z}\\|=1}\\left(\\mathbb{E}_{\\mathcal{E}}\\left[|\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{z}|^{2q}\\right]\\right)^{1/2q}=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we prove an upper bound on $\\begin{array}{r}{\\left(\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)^{p}\\right]\\right)^{1/p}}\\end{array}$ . Without loss of generality, we assume that $p$ is even. We have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)^{p}\\right]\\le2\\cdot\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)^{p}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathcal{E}^{'}$ is event independent of $\\mathbf{x}_{i}$ . The above can be upper bounded as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)^{p}\\right]\\right)^{1/p}\\leq\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\left(\\gamma-\\gamma_{i}\\right)^{p}\\right]\\right)^{1/p}=\\left(\\mathbb{E}_{\\varepsilon}\\left[\\left(\\gamma-\\bar{\\gamma}+\\bar{\\gamma}-\\gamma_{i}\\right)^{p}\\right]\\right)^{1/p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\vert\\gamma-\\bar{\\gamma}\\vert+\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\left(\\gamma_{i}-\\bar{\\gamma}\\right)^{p}\\right]\\right)^{1/p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\gamma}=1+\\frac{\\gamma}{m}\\mathbb{E}_{\\mathcal{E}^{\\prime}}}\\end{array}$ $(\\mathbf{x}_{i}^{\\top}\\mathbf{UQ}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i})$ . As ${\\mathcal{E}}^{\\prime}$ is independent of $\\mathbf{x}_{i}$ and $\\mathbf{Q}_{-i}$ is independent of $\\mathbf{x}_{i}$ , we get $\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left(\\mathbf{x}_{i}^{\\top}\\ddot{\\mathbf{U}}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right)=\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})$ . Therefore, $\\begin{array}{r}{\\bar{\\gamma}=1+\\frac{\\gamma}{m}\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})}\\end{array}$ . We now aim to upper bound $\\big(\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right]\\big)^{1/p}$ as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right])^{1/p}\\leq\\left(\\frac{\\gamma}{m}\\right)\\cdot\\left[\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[(\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i})^{p}\\right]\\right)^{1/p}+\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\varepsilon^{\\prime}}\\operatorname{tr}(\\mathbf{Q}_{-i})\\right]\\right)\\right)\\cdot\\mathbf{u}_{i}^{\\top}=\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\varepsilon^{\\prime}}\\operatorname{tr}(\\mathbf{Q}_{-i})\\right]\\right)\\cdot\\mathbf{u}_{i}^{\\top}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using our new Restricted Bai-Silverstein inequality from Lemma 3 (restated as Lemma 16 and proven in Appendix D), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[(\\operatorname{tr}(\\mathbf{Q}_{-i})-\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i})^{p}\\right]\\right)^{1/p}<c\\cdot p^{3}\\sqrt{d}\\cdot\\left(1+\\sqrt{\\frac{d p\\log(d/\\delta)}{s}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now consider $(\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})\\right]^{p})^{1/p}$ . In Lemma 4 (restated below as Lemma 13), we show that $\\begin{array}{r}{|\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})|\\leq\\frac{c^{\\prime}\\gamma}{\\sqrt{m}}\\!\\cdot\\!d\\log^{4.5}(m/\\delta)}\\end{array}$ with probability at least $1-\\delta$ . Conditioned on this high-probability event we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\big(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\varepsilon^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})\\right]^{p}\\big)^{1/p}\\leq\\frac{c^{\\prime}\\gamma}{\\sqrt{m}}\\cdot d\\log^{4.5}(m/\\delta)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for an absolute constant $c^{\\prime}>0$ . Therefore we get with probability at least $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right])^{1/p}\\leq\\frac{\\gamma}{m}\\left[c\\cdot p^{3}\\sqrt{d}\\cdot\\left(1+\\sqrt{\\frac{d p\\log(d/\\delta)}{s}}\\right)+\\frac{c^{\\prime}\\gamma}{\\sqrt{m}}\\cdot d\\log^{4.5}(m/\\delta)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $m>d$ , we get $\\begin{array}{r}{\\left(\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[(\\gamma_{i}-\\bar{\\gamma})^{p}\\right]\\right)^{1/p}=\\mathcal{O}\\left(\\frac{\\sqrt{d}\\log^{4.5}(n/\\delta)}{m}\\cdot\\left(1+\\sqrt{\\frac{d}{s}}\\right)\\right)}\\end{array}$ . Also using the analysis in [18] for upper bounding $|\\gamma-\\bar{\\gamma}|$ , we get a matching upper bound on $|\\gamma-\\bar{\\gamma}|$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\gamma-\\bar{\\gamma}|=\\mathcal{O}\\left(\\frac{\\sqrt{d}\\log^{4.5}(n/\\delta)}{m}\\cdot\\left(1+\\sqrt{\\frac{d}{s}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting these bounds in (7) and then in (6) we get, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}_{2}\\|=\\mathcal{O}\\left(\\frac{\\sqrt{d}\\log^{4.5}(n/\\delta)}{m}\\cdot\\left(1+\\sqrt{\\frac{d}{s}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the upper bounds for $\\mathbf{Z}_{0},\\mathbf{Z}_{1}$ and $\\mathbf{Z}_{2}$ using relations (4,5,8) we conclude our proof. ", "page_idx": 16}, {"type": "text", "text": "We now provide the proof of Lemma 4, which we restate in the following Lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 13. For given $\\delta>0$ and matrix $\\mathbf{Q}_{-i}$ we have with probability $1-\\delta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})|\\leq\\frac{c^{\\prime}\\gamma}{\\sqrt{m}}\\cdot d\\log^{4.5}(m/\\delta)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for an absolute constant $c^{\\prime}>0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Writing $\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})$ as a finite sum, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\varepsilon^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})=\\sum_{j=1}^{m}\\mathbb{E}_{\\varepsilon^{\\prime},j}\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\varepsilon^{\\prime},j-1}\\mathrm{tr}(\\mathbf{Q}_{-i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denoting $X_{j}=\\mathbb{E}_{\\varepsilon^{\\prime},j}\\mathrm{tr}(\\mathbf{Q}_{-i})$ with $X_{0}=\\mathbb{E}_{\\varepsilon^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})$ , we have the following formulation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})=\\sum_{j=1}^{m}X_{j}-X_{j-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\mathbb{E}_{\\mathcal{E}^{\\prime},j-1}[X_{j}]\\ =\\ X_{j-1}$ . The random sequence $X_{j}$ forms a martingale and $\\mathrm{tr}({\\bf Q}_{-i})\\mathrm{~-~}$ $\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\mathrm{tr}(\\mathbf{Q}_{-i})=X_{m}-X_{0}$ . We find an upper bound on $|X_{j}-X_{j-1}|$ . To achieve that we note, $\\begin{array}{r}{\\mathrm{V}_{j}-X_{j-1}=\\mathbb{E}_{\\xi^{\\prime},j}\\mathrm{tr}(\\mathbf{Q}_{-i})-\\mathbb{E}_{\\xi^{\\prime},j-1}\\mathrm{tr}(\\mathbf{Q}_{-i})=-(\\mathbb{E}_{\\xi^{\\prime},j}-\\mathbb{E}_{\\xi^{\\prime},j-1})(\\mathrm{tr}(\\mathbf{Q}_{-i j}-\\mathbf{Q}_{-i})-\\mathrm{tr}(\\mathbf{Q}_{-i j}))}\\end{array}$ ). Therefore with $\\psi_{j}=(\\mathbb{E}_{\\mathcal{E}^{\\prime},j}-\\mathbb{E}_{\\mathcal{E}^{\\prime},j-1})\\mathrm{tr}(\\mathbf{Q}_{-i j}-\\mathbf{Q}_{-i})$ and $\\chi_{j}=-(\\mathbb{E}_{\\mathcal{E}^{\\prime},j}-\\mathbb{E}_{\\mathcal{E}^{\\prime},j-1})\\mathrm{tr}(\\mathbf{Q}_{-i j})$ , we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n|X_{j}-X_{j-1}|\\leq|\\psi_{j}+\\chi_{j}|\\leq|\\psi_{j}|+|\\chi_{j}|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From [18], we have $\\textstyle|\\chi_{j}|\\leq{\\frac{1}{m}}$ . We now prove an upper bound on $\\psi_{j}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\mathrm{tr}(\\mathbf Q_{-i j})-\\mathrm{tr}(\\mathbf Q_{-i})=\\mathrm{tr}\\left(\\frac{\\frac{\\gamma}{m}{\\mathbf Q}_{-i j}{\\mathbf U}^{\\top}\\mathbf x_{j}{\\mathbf x}_{j}^{\\top}{\\mathbf U}{\\mathbf Q}_{-i j}{\\mathbf U}^{\\top}{\\mathbf U}}{1+\\frac{\\gamma}{m}{\\mathbf x}_{j}^{\\top}{\\mathbf U}{\\mathbf Q}_{-i j}{\\mathbf U}^{\\top}\\mathbf x_{j}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\frac{\\gamma}{m}{\\mathbf X}_{j}^{\\top}\\left({\\mathbf U}{\\mathbf Q}_{-i j}{\\mathbf U}^{\\top}\\right)^{2}\\mathbf x_{j}}{1+\\frac{\\gamma}{m}{\\mathbf x}_{j}^{\\top}{\\mathbf U}{\\mathbf Q}_{-i j}{\\mathbf U}^{\\top}\\mathbf x_{j}}\\leq\\frac{\\gamma}{m}{\\mathbf x}_{j}^{\\top}{\\mathbf U}{\\mathbf Q}_{-i j}^{2}{\\mathbf U}^{\\top}\\mathbf x_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now look at the term $\\mathbf{x}_{j}^{\\top}\\mathbf{UQ}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}$ . For any $a>0$ and any $k>0$ , by Markov\u2019s inequality we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{s}_{\\mathbf{\\Theta}}\\operatorname{\\mathrm{\\mathrm{P}}}\\left(\\mathbf{x}_{j}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}\\geq a\\right)\\leq\\frac{\\operatorname{\\mathbb{E}}[|\\mathbf{x}_{j}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}|^{k}]}{a^{k}}}\\\\ &{\\phantom{\\leq}\\leq\\frac{2^{k-1}\\cdot\\operatorname{\\mathbb{E}}[|\\mathbf{x}_{j}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}-\\operatorname{tr}(\\mathbf{Q}_{-i j}^{2})|^{k}}{a^{k}}+\\frac{2^{k-1}\\cdot\\operatorname{\\mathbb{E}}[(\\operatorname{tr}(\\mathbf{Q}_{-i j}^{2}))^{k}]}{a^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let ${\\mathcal{E}}_{1}$ be an event independent of both $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ and have probability at least $1-\\delta^{\\prime}$ . Therefore we have $\\mathbb{E}[\\cdot]\\leq2\\cdot\\mathbb{E}_{\\varepsilon_{1}}[\\cdot]$ . We get, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left({\\bf x}_{j}^{\\top}{\\bf U}{\\bf Q}_{-i j}^{2}{\\bf U}^{\\top}{\\bf x}_{j}\\geq a\\right)\\leq\\frac{2^{k}\\cdot\\mathbb{E}_{\\mathcal{E}_{1}}\\left[\\left|{\\bf x}_{j}^{\\top}{\\bf U}{\\bf Q}_{-i j}^{2}{\\bf U}^{\\top}{\\bf x}_{j}-\\mathrm{tr}({\\bf Q}_{-i j}^{2})\\right|^{k}\\right.}{a^{k}}+\\frac{2^{k}\\cdot\\mathbb{E}_{\\mathcal{E}_{1}}\\left[\\left(\\mathrm{tr}({\\bf Q}_{-i j}^{2})\\right)^{k}\\right]}{a^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now upper bound both terms on the right-hand side separately. Considering the term $\\mathbb{E}_{\\mathcal{E}_{1}}[|\\mathbf{x}_{j}^{\\top}\\mathbf{U}\\dot{\\mathbf{Q}}_{-i j}^{\\bar{2}}\\mathbf{U}^{\\top}\\mathbf{x}_{j}-\\mathrm{tr}(\\mathbf{Q}_{-i j}^{2})|^{k}$ and using Lemma 3 we get, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}_{1}}[|\\mathbf{x}_{j}^{\\top}\\mathbf{UQ}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}-\\mathrm{tr}(\\mathbf{Q}_{-i j}^{2})|^{k}]\\leq c^{k}\\cdot k^{3k}\\cdot\\left(\\frac{d^{2}k\\log(d/\\delta)}{s}+d\\right)^{k/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now considering the second term in (9), i.e., $\\mathbb{E}_{\\mathcal{E}_{1}}[(\\mathrm{tr}(\\mathbf{Q}_{-i j}^{2}))^{k}]$ . We use that conditioned on ${\\mathcal{E}}_{1}$ , we have $\\mathbf{Q}_{-i j}\\preceq6\\mathbf{I}_{d}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}_{1}}[(\\operatorname{tr}(\\mathbf{Q}_{-i j}^{2}))^{k}]\\leq6^{k}\\cdot d^{k}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting (10) and (11) in (9), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\mathbf{x}}_{j}^{\\top}{\\mathbf{UQ}}_{-i j}^{2}{\\mathbf{U}}^{\\top}{\\mathbf{x}}_{j}\\geq a\\right)\\leq\\frac{2^{k}\\cdot c^{k}\\cdot k^{3k}\\cdot\\left(\\frac{d^{2}k\\log(d/\\delta))}{s}+d\\right)^{k/2}}{a^{k}}+\\frac{2^{k}\\cdot6^{k}\\cdot d^{k}}{a^{k}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\bf x}_{j}^{\\top}{\\bf U Q}_{-i j}^{2}{\\bf U}^{\\top}{\\bf x}_{j}\\geq a\\right)\\leq\\frac{2^{k}\\cdot c^{k}\\cdot k^{3k}\\cdot d^{k}\\cdot(k\\log(d/\\delta))^{k/2}}{a^{k}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some potentially different constant $c$ . Consider $\\begin{array}{r}{k=\\left\\lceil\\frac{\\log\\left(m/\\delta\\right)}{\\log\\left(2\\right)}\\right\\rceil}\\end{array}$ and $a=4\\cdot c\\cdot k^{3}\\cdot d\\cdot{\\sqrt{k\\log(d/\\delta)}}$ and we have with probability at least $1-\\delta/m$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{x}_{j}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i j}^{2}\\mathbf{U}^{\\top}\\mathbf{x}_{j}\\leq4\\cdot c\\cdot k^{3}\\cdot d\\cdot{\\sqrt{\\log(k d/\\delta)}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies that for an absolute constant $c^{\\prime}$ we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mathrm{tr}(\\mathbf{Q}_{-i j})-\\mathrm{tr}(\\mathbf{Q}_{-i})|\\leq c^{\\prime}\\cdot\\frac{\\gamma}{m}\\cdot d\\cdot\\log^{3.5}(m/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore we now have an upper bound for $|\\psi_{j}|$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\psi_{j}|=|\\mathbb{E}_{\\boldsymbol{\\varepsilon}^{\\prime},j}-\\mathbb{E}_{\\boldsymbol{\\varepsilon}^{\\prime},j-1}(\\mathrm{tr}(\\mathbf{Q}_{-i j}-\\mathbf{Q}_{-i}))|\\leq2c^{\\prime}\\cdot\\frac{\\gamma}{m}\\cdot d\\cdot\\log^{3.5}(m/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This means for all $j$ , we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n|X_{j}-X_{j-1}|\\leq4c^{\\prime}\\cdot\\frac{\\gamma}{m}\\cdot d\\cdot\\log^{3.5}(m/\\delta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability at least $1-\\delta$ . Consider $c_{j}=4c^{\\prime}\\cdot\\frac{\\gamma}{m}\\cdot d\\cdot\\log^{3.5}(m/\\delta)$ . Then $\\begin{array}{r}{\\sum_{j=1}^{m}c_{j}^{2}=\\frac{\\gamma^{2}}{m}\\cdot16c^{\\prime2}}\\end{array}$ \u00b7 $d^{2}\\log^{7}(m/\\delta)$ . Applying Azuma\u2019s inequality (Lemma 8) with $\\textstyle\\lambda={\\frac{\\gamma}{\\sqrt{m}}}\\cdot4c^{\\prime}\\cdot d\\cdot\\log^{3.5}(m/\\delta)$ . We get with probability at least $1-\\delta$ and for potentially different absolute constant $c^{\\prime}>0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n|X_{m}-X_{0}|\\leq{\\frac{c^{\\prime}\\gamma}{\\sqrt{m}}}\\cdot d\\log^{4.5}(m/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This concludes our proof. ", "page_idx": 18}, {"type": "text", "text": "B.1 Proof of Corollary 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider the following Distributed Newton Sketch method: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\mathbf{p}_{t},\\qquad\\mathbf{p}_{t}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{H}}_{i}^{-1}\\mathbf{g}_{t},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{g}_{t}\\;=\\;\\nabla f(\\mathbf{x}_{t})$ and $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{i}\\;=\\;\\frac{m}{m-d}\\mathbf{A}_{t}^{\\top}\\mathbf{S}_{i}^{\\top}\\mathbf{S}_{i}\\mathbf{A}_{t}}\\end{array}$ is a sketched estimate of the Hessian $\\mathbf{H}_{t}\\ =$ $\\nabla^{2}f(\\mathbf{x}_{t})$ , when expressed as the matrix product $\\mathbf{H}_{t}=\\mathbf{A}_{t}^{\\top}\\mathbf{A}_{t}$ for the appropriately chosen $n\\times d$ matrix ${\\bf A}_{t}$ (as can be done for any generalized linear model). This update can be viewed as an approximate \u221aNewton step with the Hessian inverse estimate $\\begin{array}{r}{\\hat{\\mathbf{H}}_{t}^{-1}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{H}}_{i}^{-1}}\\end{array}$ . As long as each $\\tilde{\\mathbf{H}}_{t}^{-1}$ is an $(\\sqrt{\\epsilon},\\delta/2q)$ -unbiased estimator of $\\mathbf{H}_{t}^{-1}$ , then using Lemma 34 from [18] we get that the averaged Hessian inverse with $q=\\tilde{O}(1/\\epsilon)$ satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\tilde{\\epsilon}}\\mathbf{H}_{t}^{-1}\\preceq\\hat{\\mathbf{H}}_{t}^{-1}\\preceq(1+\\tilde{\\epsilon})\\mathbf{H}_{t}^{-1},\\quad\\tilde{\\epsilon}=\\sqrt{\\epsilon}+\\tilde{O}(1/\\sqrt{q})=O(\\sqrt{\\epsilon}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using standard approximate Newton analysis (e.g., see Lemmas 1 and 3, and the related discussion in [15]), this implies that when $\\mathbf{x}_{t}$ is in a sufficiently small neighborhood around $\\mathbf{x}^{*}$ (determined solely by the strong convexity and Lipschitz constants of the Hessian of $f$ ), we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{f(\\mathbf{x}_{t+1})-f(\\mathbf{x}^{*})}{f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*})}=O(\\tilde{\\epsilon}^{2})=O(\\epsilon).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Adjusting the constants appropriately, relying on Theorem 3 for constructing $\\tilde{\\mathbf{H}}_{i}$ , and following the complexity analysis from Theorem 2, we recover the claim. ", "page_idx": 18}, {"type": "text", "text": "C Least squares bias analysis: Proof of Theorem 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we aim to prove Theorem 4. Let $\\mathbf{x}^{*}=\\mathrm{argmin}_{\\mathbf{x}}\\,\\|\\mathbf{U}\\mathbf{x}-\\mathbf{b}\\|^{2}$ where $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ is the data matrix containing $n$ data points and $\\mathbf{b}\\in\\mathbb{R}^{n}$ is a vector containing labels corresponding to $n$ data points. We adopt the same notations as used in the proof of Theorem 5. Let $\\begin{array}{r}{\\tilde{\\mathbf{x}}=\\mathrm{argmin}_{\\mathbf{x}}\\,\\|\\mathbf{S}_{m}\\mathbf{U}\\mathbf{x}-\\mathbf{\\Lambda}}\\end{array}$ $\\mathbf{S}_{m}\\mathbf{b}\\|^{2}$ . Furthermore for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ we can find the loss at $\\mathbf{x}$ as $L(\\mathbf{x})=\\|\\mathbf{Ux}-\\mathbf{b}\\|^{2}$ . Additionally, we use $\\mathbf{r}$ to denote the residual $\\mathbf{b}-\\mathbf{U}\\mathbf{x}^{*}$ . We aim to provide an upper bound on the bias introduced due to this sketch and solve paradigm, i.e. $L(\\mathbb{E}(\\tilde{\\mathbf{x}}))-L(\\mathbf{x}^{*})$ . Similar to Theorem 4 we condition on the high probability event $\\mathcal{E}$ and consider $L(\\mathbb{E}_{\\mathcal{E}}(\\tilde{\\mathbf{x}}))-L(\\mathbf{x}^{*})$ . By Pythagorean theorem, we have $L(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})=\\|\\mathbf{U}(\\mathbb{E}_{\\mathcal{E}}[\\tilde{\\mathbf{x}}])-\\mathbf{U}\\mathbf{x}^{*}\\|^{2}$ . Also, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal L}(\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\tilde{\\mathbf{x}}])-{\\cal L}(\\mathbf{x}^{*})=\\|\\mathbf{U}(\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\tilde{\\mathbf{x}}])-\\mathbf{U}\\mathbf{x}^{*}\\|^{2}=\\|\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\tilde{\\mathbf{x}}]-\\mathbf{x}^{*}\\|^{2}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~=\\left\\|\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{b}]-\\mathbf{U}^{\\top}\\mathbf{b}\\right\\|^{2}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~=\\left\\|\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}](\\mathbf{b}-\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{b})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\mathbf{b}-\\mathbf{UU}^{\\top}\\mathbf{b}=\\mathbf{b}-\\mathbf{Ux}^{*}=\\mathbf{r}$ . We get, ", "page_idx": 19}, {"type": "equation", "text": "$$\nL(\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\tilde{\\mathbf{x}}])-L(\\mathbf{x}^{*})=\\left\\|\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}]\\mathbf{r}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consider $\\mathbf{Q}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}$ , $\\begin{array}{r}{\\mathbf{Q}_{-i}=(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{-i}^{\\top}\\mathbf{S}_{-i}\\mathbf{U})^{-1}\\mathrm{and}\\;\\gamma=\\frac{m}{m-d},}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{r}]=\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\gamma(\\gamma\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{r}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{r}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\mathbb{E}_{\\boldsymbol{\\mathcal{E}}}[\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used linearity of expectation in the last line combined with $\\begin{array}{r}{\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\;=\\;\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}}\\end{array}$ Using She\u03b3rman-Morrison formula (Lemma 5) we have $\\begin{array}{r}{\\mathbf{Q}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\;=\\;\\frac{\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}{1+\\frac{\\gamma}{m}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}}\\end{array}$ . Denote $\\begin{array}{r}{\\gamma_{i}=1+\\frac{\\gamma}{m}\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}}\\end{array}$ and substitute we get, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{r}]=\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol\\varepsilon}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{r}]=\\mathbb{E}_{\\boldsymbol\\varepsilon}\\left[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]+\\mathbb{E}_{\\boldsymbol\\varepsilon}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So we get the following decomposition: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\cal L(\\mathbb{E}_{\\boldsymbol\\mathcal{E}}[\\tilde{\\mathbf{x}}])-\\cal L(\\mathbf{x}^{*})=\\left\\|\\mathbb{E}_{\\boldsymbol\\mathcal\\mathcal{E}}[(\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}\\mathbf{U})^{-1}\\mathbf{U}^{\\top}\\mathbf{S}_{m}^{\\top}\\mathbf{S}_{m}]\\mathbf{r}\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\left\\|\\mathbb{E}_{\\boldsymbol\\mathcal\\mathcal{E}}\\left[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]+\\mathbb{E}_{\\boldsymbol\\mathcal\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq2\\underbrace{\\left\\|\\mathbb{E}_{\\boldsymbol\\mathcal{E}}\\left[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\right\\|^{2}}_{\\left\\|\\mathbf{Z}_{0}\\mathbf{r}\\right\\|^{2}}+2\\underbrace{\\left\\|\\mathbb{E}_{\\boldsymbol\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\right\\|^{2}}_{\\left\\|\\mathbf{Z}_{2}\\mathbf{r}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that a similar decomposition was considered in the proof of Theorem 5 (see Appendix B) with slightly different $\\mathbf{Z}_{0}$ and $\\mathbf{Z}_{2}$ . We first bound $\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}$ in the following argument. Without loss of generality, we assume that events ${\\mathcal{E}}_{1}$ and $\\mathcal{E}_{2}$ are independent of $\\mathbf{x}_{i}$ and $\\mathcal{E}^{'}=\\mathcal{E}_{1}\\wedge\\mathcal{E}_{2}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Z}_{0}\\mathbf{r}=\\mathbb{E}_{\\mathcal{E}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]=\\frac{\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{\\mathcal{E}}]}{\\operatorname*{Pr}(\\mathcal{E})}}\\\\ &{\\quad\\quad=\\frac{\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{\\mathcal{E}_{1}}\\cdot\\mathbf{1}_{\\mathcal{E}_{2}}\\cdot\\left(1-\\mathbf{1}_{-\\mathcal{E}_{3}}\\right)]}{\\operatorname*{Pr}(\\mathcal{E})}}\\\\ &{\\quad\\quad=\\frac{\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{\\mathcal{E}_{1}}\\cdot\\mathbf{1}_{\\mathcal{E}_{2}}]}{\\operatorname*{Pr}(\\mathcal{E})}-\\frac{\\mathbb{E}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{\\mathcal{E}_{1}}\\cdot\\mathbf{1}_{\\mathcal{E}_{2}}\\cdot\\mathbf{1}_{-\\mathcal{E}_{3}})]}{\\operatorname*{Pr}(\\mathcal{E})}}\\\\ &{\\quad\\quad=\\frac{1}{1-\\delta^{\\prime}}\\left(\\mathbb{E}_{\\varepsilon^{\\prime}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]-\\mathbb{E}_{\\varepsilon^{\\prime}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{-\\mathcal{E}_{3}})\\right]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now note that in the first term $\\mathbf{Q}_{-i}$ is independent of $\\mathbf{x}_{i}$ and $\\mathbf{x}_{i}$ is also independent of the event $\\mathcal{E}^{'}$ . Using this with the fact that $\\mathbb{E}[\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}]=\\mathbf{I}$ we get $\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]=\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{r}]=0$ , since $\\mathbf{U}^{\\top}\\mathbf{r}=0$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{Z}_{0}\\mathbf{r}\\right\\|^{2}\\leq2\\left\\|\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{-\\mathcal{E}_{3}}]\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq72\\|\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\cdot\\mathbf{1}_{-\\mathcal{E}_{3}}]\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last inequality holds because conditioned on $\\mathcal{E}^{'}$ , we know that $\\|\\mathbf{Q}_{-i}\\|\\leq6$ . Using CauchySchwarz inequality we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbf{Z}_{0}\\mathbf{r}\\right\\|^{2}\\leq72\\left(\\sqrt{\\mathbb{E}_{\\xi^{\\prime}}[\\mathbf{x}_{i}^{\\top}\\mathbf{UU}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{-\\varepsilon_{3}}]}\\cdot\\sqrt{\\mathbb{E}_{\\xi^{\\prime}}[\\mathbf{r}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}]}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\mathbb{E}_{\\mathcal{E}^{\\prime}}[{\\bf r}^{\\top}{\\bf x}_{i}{\\bf x}_{i}^{\\top}{\\bf r}]=\\left\\|{\\bf r}\\right\\|^{2}$ . Also, we can bound $\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\neg\\mathcal{E}_{3}}]$ as following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{T}_{\\mathcal{E}^{\\prime}}[\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\mathcal{E}_{3}}]=\\displaystyle\\int_{0}^{\\infty}\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\mathcal{-E}_{3}}>x)d x}\\\\ &{\\displaystyle=\\int_{0}^{y}\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\mathcal{-E}_{3}}>x)d x+\\displaystyle\\int_{y}^{\\infty}\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\mathcal{-E}_{3}}>x)d x}\\\\ &{\\displaystyle\\leq y\\delta^{\\prime}+\\displaystyle\\int_{y}^{\\infty}\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}>x)d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Chebyshev\u2019s inequality, $\\begin{array}{r}{\\operatorname*{Pr}(\\mathbf{x}_{i}^{\\top}{\\mathbf U}{\\mathbf U}^{\\top}\\mathbf{x}_{i}\\geq x)\\leq\\frac{\\operatorname{Var}(\\mathbf{x}_{i}^{\\top}{\\mathbf U}{\\mathbf U}^{\\top}\\mathbf{x}_{i})}{x^{2}}}\\end{array}$ . By Restricted Bai-Silverstein, Lemma 3 with $p=2$ , we have $\\begin{array}{r}{\\mathrm{Var}(\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i})\\leq c\\cdot\\left(d+\\frac{d^{2}\\log(d/\\delta)}{s}\\right)}\\end{array}$ d2logs(d/\u03b4) for some absolute constant c. Let $y=m^{2}$ and $\\begin{array}{r}{\\delta^{\\prime}<\\frac{1}{m^{4}}}\\end{array}$ we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{x}_{i}^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\cdot\\mathbf{1}_{\\neg\\mathcal{E}_{3}}]=\\mathcal{O}\\left(\\frac{1}{m^{2}}+\\frac{d}{m^{2}}+\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}\\right)=\\mathcal{O}\\left(\\frac{d}{m^{2}}+\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This finishes upper bounding $\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}_{0}\\mathbf{r}\\|^{2}={\\mathcal{O}}\\left({\\frac{d}{m^{2}}}+{\\frac{d^{2}\\log(d/\\delta)}{s m^{2}}}\\right)\\cdot\\|\\mathbf{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we proceed with $\\|\\mathbf{Z}_{2}\\mathbf{r}\\|^{2}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{Z}_{2}\\mathbf{r}\\|=\\left\\|\\mathbb{E}_{\\mathcal{E}}\\left[\\left(\\frac{\\gamma}{\\gamma_{i}}-1\\right)\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right]\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying H\u00f6lder\u2019s inequality with $p={\\mathcal{O}}(\\log(n))$ and $q=1+\\Delta$ where $\\begin{array}{r}{\\Delta=\\frac{1}{\\mathcal{O}(\\log(n))}}\\end{array}$ O(log(n)), we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Z}_{2}\\mathbf{r}\\|\\leq\\bigg(\\mathbb{E}_{\\mathcal{E}}\\big|\\frac{\\gamma}{\\gamma_{i}}-1\\big|^{p}\\bigg)^{1/p}\\cdot\\bigg(\\underset{\\Vert\\mathbf{v}\\Vert=1}{\\operatorname*{sup}}\\mathbb{E}_{\\mathcal{E}}\\big[\\mathbf{v}^{\\top}\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\big]^{q}\\bigg)^{1/q}}\\\\ &{\\qquad\\leq\\bigg(\\mathbb{E}_{\\mathcal{E}}\\big|\\frac{\\gamma}{\\gamma_{i}}-1\\big|^{p}\\bigg)^{1/p}\\cdot\\bigg(\\mathbb{E}_{\\mathcal{E}}\\left\\Vert\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\Vert^{2q}\\bigg)^{1/2q}\\cdot\\bigg(\\mathbb{E}_{\\mathcal{E}}\\left\\Vert\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right\\Vert^{2q}\\bigg)^{1/2q}}\\\\ &{\\qquad\\leq\\bigg(\\mathbb{E}_{\\mathcal{E}}\\big|\\frac{\\gamma}{\\gamma_{i}}-1\\big|^{p}\\bigg)^{1/p}\\cdot\\bigg(2\\cdot\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\left\\Vert\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\Vert^{2}\\cdot\\left\\Vert\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\Vert^{2\\Delta}]\\bigg)^{1/2q}\\cdot\\bigg(2\\cdot\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\left\\Vert\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right\\Vert^{2}\\cdot\\left\\Vert\\mathbf{x}\\right\\Vert^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\left\\|\\mathbf{x}_{i}\\right\\|\\;=\\;\\mathrm{poly}(n)$ , we have $\\left\\|\\mathbf{x}_{i}\\right\\|^{2\\Delta}\\ =\\ \\mathcal{O}(1)$ and therefore we have $\\begin{array}{r l}{\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\|^{2\\Delta}}&{=}\\end{array}$ $\\mathcal{O}(1)\\;\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\|^{2\\Delta}$ . Also $\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\|^{2}\\,=\\,\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left\\|\\mathbf{Q}_{-i}\\right\\|^{2}$ . Using that conditioned on $\\mathcal{E}^{'}$ we have $\\|\\mathbf{Q}_{-i}\\|\\leq6$ , we get $\\left(\\mathbb{E}_{\\mathcal{E}}\\left\\|\\mathbf{Q}_{-i}\\mathbf{U}^{\\top}\\mathbf{x}_{i}\\right\\|^{2q}\\right)^{1/2q}=\\mathcal{O}(1)$ . Similarly using $\\left\\|\\mathbf{x}_{i}\\right\\|^{2\\Delta}=\\mathcal{O}(1)$ we get $\\left(\\mathbb{E}_{\\mathcal{E}}\\left\\|\\mathbf{x}_{i}^{\\top}\\mathbf{r}\\right\\|^{2q}\\right)^{1/2q}=\\mathcal{O}(1)\\left\\|\\mathbf{r}\\right\\|$ . This gives us: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{Z}_{2}\\mathbf{r}\\right\\|\\leq\\mathcal{O}(1)\\cdot\\left(\\mathbb{E}_{\\mathcal{E}}{\\big|}\\frac{\\gamma}{\\gamma_{i}}-1{\\big|}^{p}\\right)^{1/p}\\cdot\\left\\|\\mathbf{r}\\right\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now using (8) from the proof of Theorem 5, we get, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{\\varepsilon}{\\big|}{\\frac{\\gamma}{\\gamma_{i}}}-1{\\big|}^{p}\\right)^{1/p}={\\mathcal{O}}\\left({\\frac{{\\sqrt{d}}\\log^{4.5}(n/\\delta)}{m}}\\cdot\\left(1+{\\sqrt{\\frac{d}{s}}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally the bound for $\\|\\mathbf{Z}_{2}\\mathbf{r}\\|^{2}$ follows as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z_{2}r}\\|^{2}=\\mathcal{O}\\left(\\frac{d\\log^{9}(n/\\delta)}{m^{2}}\\cdot\\left(1+\\frac{d}{s}\\right)\\right)\\cdot\\|\\mathbf{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining (13) and (14) we conclude our proof. ", "page_idx": 21}, {"type": "text", "text": "D Higher-Moment Restricted Bai-Silvestein (Lemma 3) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we prove Lemma 3. We need the following two auxiliary lemmas to derive the main theorem of this section. The first lemma uses Matrix-Chernoff concentration inequality to upper bound the spectral norm of $\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}$ where $\\xi$ is a $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier. ", "page_idx": 21}, {"type": "text", "text": "Lemma 14 (Spectral norm bound with leverage score sparsifier). Let $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ has orthonormal columns. Let $\\xi$ be a $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier for U, and denote ${\\bf U}_{\\xi}=$ $\\mathrm{diag}(\\pmb{\\xi})\\mathbf{U}$ . Then for any $\\delta>0$ we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\left\\|\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\right\\|\\geq\\left(1+\\frac{3d\\log(d/\\delta)}{s}\\right)\\right)\\leq\\delta\\quad i f s<d,}\\\\ &{\\operatorname*{Pr}\\left(\\left\\|\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\right\\|\\geq(1+3\\log(d/\\delta))\\right)\\leq\\delta\\quad i f s\\geq d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Writing $\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}$ as a sum of matrices we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}=\\sum_{i=1}^{n}\\xi_{i}^{2}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top}=\\sum_{i=1}^{n}\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top}=\\sum_{i=1}^{n}\\mathbf{Z}_{i}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{p_{i}\\,=\\,\\operatorname*{min}\\{1,\\frac{s\\beta_{1}\\tilde{l}_{i}}{d}\\}}\\end{array}$ and $\\begin{array}{r}{{\\bf Z}_{i}\\,=\\,\\frac{b_{i}}{p_{i}}{\\bf u}_{i}{\\bf u}_{i}^{\\top}}\\end{array}$ . Note that $\\mathbf{Z}_{i}^{\\prime}s$ are independent random variables and $\\mathbb{E}[\\mathbf{Z}_{i}]\\,=\\,{\\mathbf{u}}_{i}\\mathbf{u}_{i}^{\\top}$ . Also $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{E}\\mathbf{Z}_{i}\\,=\\,\\mathbf{U}^{\\top}\\mathbf{U}\\,=\\,\\mathbf{I}_{d}}\\end{array}$ . If $p_{i}\\,=\\,1\\,$ then $\\mathbf{Z}_{i}\\,=\\,{\\mathbf{u}}_{i}\\mathbf{u}_{i}^{\\top}$ and therefore $\\left\\|\\mathbf{Z}_{i}\\right\\|\\,=\\,\\left\\|\\mathbf{u}_{i}\\right\\|^{2}\\,\\leq\\,1$ . If $p_{i}~<~1$ , we have $\\begin{array}{r}{\\|\\mathbf{Z}_{i}\\|\\,\\le\\,\\frac{1}{p_{i}}\\,\\|\\mathbf{u}_{i}\\|^{2}\\,=\\,\\frac{d}{s\\beta_{1}\\tilde{l}_{i}}\\,\\cdot\\,l_{i}}\\end{array}$ . As $l_{i}\\,\\leq\\,\\beta_{1}{\\tilde{l}}_{i}$ , we get $\\begin{array}{r}{\\|\\mathbf{Z}_{i}\\|\\leq\\frac{d}{s}}\\end{array}$ . Therefore $\\|\\mathbf{Z}_{i}\\|\\leq\\operatorname*{max}\\lbrace1,\\frac{d}{s}\\rbrace$ for all $i$ . Denote $R=\\operatorname*{max}\\{1,\\frac{d}{s}\\}$ . We use Matrix Chernoff (Lemma $^7$ ) to upper bound the largest eigenvalue of $\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}$ . For any $\\epsilon>0$ , we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{n}\\mathbf{Z}_{i}\\right)\\geq(1+\\epsilon)\\right)\\leq d\\cdot\\exp\\left(-\\frac{\\epsilon^{2}}{(2+\\epsilon)R}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With $R=\\operatorname*{max}\\{1,\\frac{d}{s}\\}$ and depending on the case whether $s\\leq d$ or $s>d$ we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\displaystyle\\left(\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{n}\\mathbf{Z}_{i}\\right)\\geq\\left(1+\\frac{3d\\log(d/\\delta)}{s}\\right)\\right)\\leq\\delta\\quad\\mathrm{if}\\;s\\leq d,}\\\\ &{\\operatorname*{Pr}\\displaystyle\\left(\\lambda_{\\operatorname*{max}}\\left(\\sum_{i=1}^{n}\\mathbf{Z}_{i}\\right)\\geq\\left(1+3\\log(d/\\delta)\\right)\\right)\\leq\\delta\\;\\mathrm{if}\\;s>d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\mathcal{A}_{\\xi}$ denote the event $\\begin{array}{r}{\\left\\|\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\right\\|\\le1+\\frac{3d\\log(d/\\delta)}{s}}\\end{array}$ , holding with probability at least $1-\\delta$ , for small $\\delta>0$ . In the next result we upper bound the higher moments of the trace of $\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}$ for any matrix $\\mathbf{C}\\preceq{\\mathcal{O}}(1)\\cdot\\mathbf{I}$ . We first prove the upper bound in the case when the high probability event $\\mathcal{A}_{\\xi}$ does not occur. ", "page_idx": 21}, {"type": "text", "text": "Lemma 15 (Trace moment bound over small probability event). Let $k\\in\\mathbb{N}$ be fixed. Let $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ have orthonormal columns. Let $\\xi$ be a $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier for U. Let $\\mathbf{U}_{\\xi}=\\mathrm{diag}(\\xi)\\mathbf{U}.$ . Also let $\\begin{array}{r}{\\operatorname*{Pr}(A_{\\xi})\\geq1-\\frac{1}{(12d)^{4k}}}\\end{array}$ (121d)4k and event E be independent of the sparsifier \u03be. Then we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}|\\neg{A}_{\\xi}\\right]\\le(4k)^{4k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any fixed matrix $\\mathbf{C}$ such that $\\mathbf{0}\\preceq\\mathbf{C}\\preceq6\\mathbf{I}.$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol{\\varepsilon}^{\\prime}}\\left[(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}|\\!\\!\\setminus\\!\\!\\mathcal{A}_{\\xi}\\right]=\\displaystyle\\int_{0}^{\\infty}\\mathrm{Pr}((\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}\\cdot\\mathbf{1}_{\\neg\\!\\!\\mathcal{A}_{\\xi}}\\geq x|\\boldsymbol{\\varepsilon}^{\\prime})d x}\\\\ {=\\displaystyle\\int_{0}^{\\infty}\\mathrm{Pr}((\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}\\cdot\\mathbf{1}_{\\neg\\!\\!\\mathcal{A}_{\\xi}}\\geq x)d x.\\ ~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last equality holds because $\\mathcal{E}^{'}$ is independent of $\\xi$ . Consider some fixed $y>0$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\varepsilon^{\\prime}}\\left[(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}|\\neg{A}_{\\xi}\\right]\\leq\\displaystyle\\int_{0}^{y}\\mathrm{Pr}((\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{2k}\\cdot\\mathbf{1}_{\\neg A_{\\xi}}\\geq x)d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\int_{y}^{\\infty}\\mathrm{Pr}((\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{2k}\\cdot\\mathbf{1}_{\\neg A_{\\xi}}\\geq x)d x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq y\\cdot\\delta+\\mathbb{E}[(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{4k}]\\cdot\\displaystyle\\int_{y}^{\\infty}\\frac{1}{x^{2}}d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last inequality holds because by Chebyshev\u2019s inequality $\\operatorname*{Pr}((\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{2k}\\ \\ \\geq\\ x)\\ \\ \\leq$ E[(tr(U\u03beC2U\u03be\u22a4 ))4k]. Also note that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{4k}=\\left(\\sum_{i=1}^{n}\\xi_{i}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{4k}=\\left(\\sum_{i=1}^{n}\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{4k}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For $1\\leq i\\leq n$ , let $R_{i}$ be random variables denoting $\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}$ . Then note that $R_{i}$ are independent random variables with $\\mathbb{E}[R_{i}]\\,=\\,{\\bf u}_{i}^{\\top}{\\bf C}{\\bf u}_{i}$ . Also $R_{i}$ are non-negative random variables with finite $(4k)^{t h}$ moment. Using Rosenthal\u2019s inequality (Lemma 9) we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n}\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{4k}\\right]\\leq2^{4k}\\cdot(4k)^{4k}\\cdot\\left[\\sum_{i=1}^{n}\\mathbb{E}[R_{i}^{4k}]+\\left(\\sum_{i=1}^{n}\\mathbb{E}[R_{i}]\\right)^{4k}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{E}[R_{i}]=\\operatorname{tr}(\\mathbf{U}\\mathbf{C}\\mathbf{U}^{\\top})}\\end{array}$ and $\\mathbb{E}[R_{i}^{4k}]$ can be found as follows: if $p_{i}=1$ then $R_{i}={\\bf u}_{i}^{\\top}{\\bf C}{\\bf u}_{i}$ and t herefore $R_{i}^{4k}=(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{4k}\\leq\\|\\mathbf{u}_{i}\\|^{2(4k-1)}\\,\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{4k}\\mathbf{u}_{i}\\leq\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{4k}\\mathbf{u}_{i}.$ , if $p_{i}<1$ , we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{i}^{4k}]=p_{i}\\cdot\\frac{1}{p_{i}^{4k}}(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{4k}=\\frac{d^{4k-1}}{s^{4k-1}}\\cdot\\frac{1}{(\\beta_{1}\\widetilde{l}_{i})^{4k-1}}(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{4k}}\\\\ &{\\qquad\\quad\\leq\\frac{d^{4k-1}}{s^{4k-1}}\\cdot\\frac{1}{(\\beta_{1}\\widetilde{l}_{i})^{4k-1}}\\cdot\\|\\mathbf{u}_{i}\\|^{2(4k-1)}\\,\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{4k}\\mathbf{u}_{i}}\\\\ &{\\qquad\\quad=\\frac{d^{4k-1}}{s^{4k-1}}\\cdot\\frac{1}{(\\beta_{1}\\widetilde{l}_{i})^{4k-1}}\\cdot l_{i}^{4k-1}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{4k}\\mathbf{u}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now using $l_{i}\\leq\\beta_{1}\\tilde{l}_{i}$ we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{i}^{4k}]\\leq\\frac{d^{4k-1}}{s^{4k-1}}\\cdot\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{4k}\\mathbf{u}_{i}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n}\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{4k}\\right]\\leq2^{4k}\\cdot(4k)^{4k}\\cdot\\left[\\operatorname*{max}\\left(1,\\frac{d^{4k-1}}{s^{4k-1}}\\right)\\cdot\\mathrm{tr}(\\mathbf{C}^{4k})+(\\mathrm{tr}(\\mathbf{C}))^{4k}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the above inequality along with using $\\mathbf{C}\\preceq6\\mathbf{I}$ we get, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\sum_{t=1}^{s}\\frac{1}{s p_{i_{t}}}\\mathbf{u}_{i_{t}}^{\\top}\\mathbf{C}\\mathbf{u}_{i_{t}}\\right)^{4k}\\right]\\leq2^{4k}\\cdot(4k)^{4k}\\cdot6^{4k}\\cdot d^{4k}=(12)^{4k}\\cdot(4k)^{4k}\\cdot d^{4k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting the above bound in (15), it follows that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\varepsilon^{\\prime}}\\left[(\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}))^{k}|\\lnot\\lnot A_{\\xi}\\right]\\le y\\cdot\\delta+(12)^{4k}\\cdot(4k)^{4k}\\cdot d^{4k}\\cdot\\frac{1}{y}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $y>(12d)^{4k}$ and $\\begin{array}{r}{\\delta<\\frac{1}{y}}\\end{array}$ , we get the desired result. ", "page_idx": 23}, {"type": "text", "text": "We are now ready to prove the main result of this section. The following result, which is central to our analysis, upper bounds the high moments of a deviation of a quadratic form from its mean. ", "page_idx": 23}, {"type": "text", "text": "Lemma 16 (Restricted Bai-Silverstein for $(s,\\beta_{1},\\beta_{2})$ -LESS embedding). Let $p\\in\\mathbb N$ be fixed and $\\mathbf{U}\\in\\mathbb{R}^{n\\times d}$ have orthonormal columns. Let $\\mathbf{x}_{i}=\\mathrm{diag}(\\boldsymbol{\\xi})\\mathbf{y}_{i}$ where $\\mathbf{y}_{i}\\,\\in\\,\\mathbb{R}^{n}$ has independent $\\pm1$ entries and $\\xi$ is a $(s,\\beta_{1},\\beta_{2})$ -approximate leverage score sparsifier for U. Let $\\mathbf{U}_{\\xi}=\\mathrm{diag}(\\xi)\\mathbf{U}$ . Then for any matrix $\\mathbf{0}\\preceq\\mathbf{C}\\preceq6\\mathbf{I}$ and any $\\delta>0$ we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{C})-\\mathbf{x}_{i}^{\\top}\\mathbf{UCU^{\\top}x}_{i}\\right]^{p}\\right)^{1/p}<c\\cdot p^{3}\\sqrt{d}\\cdot\\left(1+\\sqrt{\\frac{d p\\log(d/\\delta)}{s}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for an absolute constant $c>0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\mathbf{x}_{i}=\\mathrm{diag}(\\pmb{\\xi})\\mathbf{y}_{i}$ where $\\mathbf{y}_{i}$ is vector of Rademacher $\\pm1$ entries. Denote $\\mathbf{U}_{\\xi}=\\mathrm{diag}(\\xi)\\mathbf{U}$ \u6b63 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left[\\left(\\operatorname{tr}(\\mathbf{C})-\\mathbf{x}_{i}^{\\top}\\mathbf{UCU}^{\\top}\\mathbf{x}_{i}\\right)^{p}\\right]=\\mathbb{E}\\left[\\left(\\operatorname{tr}(\\mathbf{C})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right)^{p}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\left(\\operatorname{tr}(\\mathbf{C})-\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top})+\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right)^{p}\\right]}\\\\ &{\\phantom{=}\\le2^{p-1}\\left(\\underbrace{\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{C})-\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top})\\right]^{p}}_{T_{1}}+\\underbrace{\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{CU}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right]^{p}}_{T_{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "First, consider $T_{1}$ , substitute $\\begin{array}{r}{\\xi_{i}^{2}=\\frac{b_{i}}{p_{i}}}\\end{array}$ , and assume exponent $p$ to be even. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{tr}(\\mathbf{C})-\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\right]^{p}=\\mathbb{E}\\left[\\mathrm{tr}(\\mathbf{U}\\mathbf{C}\\mathbf{U}^{\\top})-\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\right]^{p}}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}(\\xi_{i}^{2}-1)\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{p}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}(\\frac{b_{i}}{p_{i}}-1)\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}\\right)^{p}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $1\\,\\leq\\,i\\,\\leq\\,n$ consider random variables $R_{i}$ where $\\begin{array}{r}{R_{i}\\ =\\ \\frac{b_{i}}{p_{i}}{\\bf u}_{i}^{\\top}{\\bf C}{\\bf u}_{i}}\\end{array}$ . Furthermore let $Y_{i}~=$ $R_{i}-{\\bf u}_{i}^{\\top}{\\bf C}{\\bf u}_{i}$ . Then $\\begin{array}{r}{\\sum_{i=1}^{n}Y_{i}=\\sum_{i=1}^{n}\\big(\\frac{b_{i}}{p_{i}}-1\\big)\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i}}\\end{array}$ and $\\mathbb{E}[Y_{i}]\\,=\\,0$ . $Y_{i}$ are independent mean zero random variables with finite $p^{t h}$ moments and therefore we can use Rosenthal\u2019s inequality (for symmetric random variables, Lemma 9) to get, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n}Y_{i}\\right]^{p}<A(p)\\left(\\underbrace{\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}]^{p}}_{T_{1}^{1}}+\\underbrace{\\left(\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}]^{2}\\right)^{p/2}}_{T_{1}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $A(p)$ is a constant depending on $p$ . We bound $T_{1}^{1}$ and $T_{1}^{2}$ separately, starting with $T_{1}^{1}$ as, ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{1}^{1}=\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}]^{p}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that $Y_{i}\\,=\\,R_{i}\\,-\\,{\\bf u}_{i}^{\\top}{\\bf C}{\\bf u}_{i}$ . $R_{i}$ is always non-negative because $\\mathbf{C}$ is a positive semi-definite matrix. Therefore we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}(Y_{i})^{p}\\leq\\mathbb{E}(R_{i})^{p}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We find the $p^{t h}$ moment of $R_{i},\\mathbb{E}(R_{i})^{p}=(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{p}$ if $p_{i}=1$ . If $p_{i}<1$ then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Xi(R_{i})^{p}=p_{i}^{p-1}(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{p}=\\frac{d^{p-1}}{s^{p-1}}\\cdot\\frac{1}{(\\beta_{1}\\tilde{l}_{i})^{p-1}}(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{p}\\leq\\frac{d^{p-1}}{s^{p-1}}\\cdot\\frac{1}{(\\beta_{1}\\tilde{l}_{i})^{p-1}}\\cdot\\|\\mathbf{u}_{i}\\|^{2(p-1)}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{p}\\mathbf{u}_{i}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\frac{d^{p-1}}{s^{p-1}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{p}\\mathbf{u}_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the last inequality we use $\\|\\mathbf{u}_{i}\\|^{2(p-1)}=l_{i}^{p-1}\\le(\\beta_{1}\\tilde{l}_{i})^{p-1}$ . Summing over $i$ from 1 to $n$ we get an upper bound for $T_{1}^{\\tilde{1}}$ as the following: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{E}(R_{i})^{p}\\leq\\sum_{i=1}^{n}\\mathbb{E}(R_{i})^{p}\\leq\\operatorname*{max}\\left(1,{\\frac{d^{p-1}}{s^{p-1}}}\\right)\\cdot\\operatorname{tr}(\\mathbf{UC}^{p}\\mathbf{U}^{\\top}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we upper bound $T_{1}^{2}$ . Note that $\\mathbb{E}(Y_{i})^{2}=\\mathbb{E}[R_{i}]^{2}=(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{2}$ if $p_{i}=1$ and if $p_{i}<1$ we have $\\begin{array}{r}{\\mathbb{E}[R_{i}]^{2}=\\frac{1}{p_{i}}\\cdot(\\mathbf{u}_{i}^{\\top}\\mathbf{C}\\mathbf{u}_{i})^{2}\\leq\\frac{d}{s}\\cdot\\frac{1}{\\beta_{1}\\tilde{l}_{i}}\\left\\|\\mathbf{u}_{i}\\right\\|^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{2}\\mathbf{u}_{i}\\leq\\frac{d}{s}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{2}\\mathbf{u}_{i}}\\end{array}$ . Summing over $i$ from 1 to $n$ we get an upper bound for $T_{1}^{2}$ as, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}\\mathbb{E}(Y_{i})^{2}\\right)^{p/2}\\leq\\left(\\operatorname*{max}\\left(1,\\frac{d}{s}\\right)\\operatorname{tr}(\\mathbf{UC^{2}U^{\\top}})\\right)^{p/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting (18) and (19) in (17) we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\natural\\left[\\mathrm{tr}(\\mathbf{C})-\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\right]^{p}\\leq A(p)\\cdot\\left(\\operatorname*{max}\\left(1,\\frac{d^{p-1}}{s^{p-1}}\\right)\\cdot\\mathrm{tr}(\\mathbf{UC}^{p}\\mathbf{U}^{\\top})+\\left(\\operatorname*{max}\\left(1,\\frac{d}{s}\\right)\\mathrm{tr}(\\mathbf{UC}^{2}\\mathbf{U}^{\\top})\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we aim to upper bound the term $T_{2}$ in (16) i.e., $\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right]^{p}$ . First, we condition over $\\xi$ and take expectation over $\\mathbf{y}_{i}$ . This requires using standard Bai-Silverstein inequality (Lemma 10), we get, ", "page_idx": 24}, {"type": "text", "text": "$\\boldsymbol{\\uptau}\\left[\\operatorname{tr}({\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}})-{\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{y}_{i}}\\right]^{p}\\leq B(p)\\cdot\\mathbb{E}\\left[\\left(\\nu_{4}\\mathrm{tr}({\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}}{\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}})\\right)^{p/2}+\\nu_{2p}\\mathrm{tr}({\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}}{\\mathbf{U}_{\\xi}}{\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}})\\right],$ CU\u03be\u22a4 )p/2 where $B(p)$ is a constant depending on $p$ . Since $\\mathbf{y}_{i}$ consists of $\\pm1$ entries we have $\\nu_{4},\\nu_{2p}\\leq1$ . Also using $\\operatorname{tr}(\\mathbf{A}\\mathbf{B})\\leq\\operatorname{tr}(\\mathbf{A})\\operatorname{tr}(\\mathbf{B})$ and considering the high probability event $\\mathcal{A}_{\\xi}$ capturing $\\left\\|\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\right\\|\\leq$ $\\begin{array}{r}{\\left(1+\\operatorname*{max}\\left(\\frac{3d\\log(d/\\delta)}{s},3\\log(d/\\delta)\\right)\\right)}\\end{array}$ . We get the following, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}\\left[\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right]^{p}\\leq2B(p)\\cdot\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\right)^{p/2}}\\\\ &{=2B(p)\\cdot\\left[\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\cdot\\mathbf{1}_{A_{\\xi}}\\right)^{p/2}\\right]+2B(p)\\cdot\\left[\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})\\cdot\\mathbf{1}_{\\neg A_{\\xi}}\\right)^{p/2}\\right]}\\\\ &{\\leq2B(p)\\cdot\\left(1+\\operatorname*{max}\\left(\\frac{3d\\log(d/\\delta)}{s},3\\log(d/\\delta)\\right)\\right)^{p/2}\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}^{2}\\mathbf{U}_{\\xi}^{\\top})\\right)^{p/2}+2B(p)\\cdot(2p)^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first term in the last inequality follows from the Matrix-Chernoff (Lemma 7) and the second term follows from Lemma 15 by considering $k=p/2$ (assuming that $\\delta$ is small enough so that Lemma 15 is satisfied). We now upper-bound $\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}^{2}\\mathbf{U}_{\\xi}^{\\top})\\right)^{p/2}$ using Rosenthal\u2019s inequality for uncentered (non-symmetric) random variables, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}^{2}\\mathbf{U}_{\\xi}^{\\top})=\\sum_{i=1}^{n}\\xi_{i}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{2}\\mathbf{u}_{i}=\\sum_{i=1}^{n}\\frac{b_{i}}{p_{i}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{2}\\mathbf{u}_{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $1\\leq i\\leq n$ consider independent random variables $\\begin{array}{r}{R_{i}^{\\prime}=\\frac{b_{i}}{p_{i}}{\\bf u}_{i}^{\\top}{\\bf C}^{2}{\\bf u}_{i}}\\end{array}$ . We have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}R_{i}^{\\prime}=\\operatorname{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}^{2}\\mathbf{U}_{\\xi}^{\\top}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here $R_{i}^{\\prime}$ are positive random variables with finite $(p/2)^{t h}$ moment. Using Rosenthal\u2019s inequality we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\sum_{i=1}^{n}R_{i}^{\\prime}\\right)^{p/2}\\leq C(p/2)\\cdot\\left[\\underbrace{\\sum_{i=1}^{n}\\mathbb{E}(R_{i}^{\\prime})^{p/2}}_{T_{2}^{1}}+\\underbrace{\\left(\\sum_{i=1}^{n}\\mathbb{E}R_{i}^{\\prime}\\right)^{p/2}}_{T_{2}^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is straightforward to upper bound $\\mathbb{E}(R_{i}^{\\prime})^{p/2}$ as, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(R_{i}^{\\prime})^{p/2}=(\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{2}\\mathbf{u}_{i})^{p/2}\\mathrm{~if~}\\,p_{i}=1,}\\\\ {\\le\\frac{d^{\\frac{p}{2}-1}}{s^{\\frac{p}{2}-1}}\\mathbf{u}_{i}^{\\top}\\mathbf{C}^{p}\\mathbf{u}_{i}\\mathrm{~if~}\\,p_{i}<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Summing over $i$ from 1 to $n$ we get upper bound for $T_{2}^{1}$ as, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{E}(R_{n}^{\\prime})^{p/2}\\leq\\operatorname*{max}\\left(1,{\\frac{d^{{\\frac{p}{2}}-1}}{s^{{\\frac{p}{2}}-1}}}\\right)\\operatorname{tr}(\\mathbf{U}\\mathbf{C}^{p}\\mathbf{U}^{\\top}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we consider $T_{2}^{2}$ . It is simply given as, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}\\mathbb{E}R_{j}^{\\prime}\\right)^{p/2}=\\left(\\mathrm{tr}(\\mathbf{UC}^{2}\\mathbf{U}^{\\top})\\right)^{p/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (23) and (24) and substituting in (22) we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}^{2}\\mathbf{U}_{\\xi}^{\\top})\\right)^{p/2}\\le C(p/2)\\cdot\\left(\\operatorname*{max}\\left(1,\\frac{d^{\\frac{p}{2}-1}}{s^{\\frac{p}{2}-1}}\\right)\\mathrm{tr}(\\mathbf{UC}^{p}\\mathbf{U}^{\\top})+\\left(\\mathrm{tr}(\\mathbf{UC}^{2}\\mathbf{U}^{\\top})\\right)^{p/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting (25) in (21) and let $w=\\operatorname*{max}(1,d/s)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathrm{tr}(\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top})-\\mathbf{y}_{i}^{\\top}\\mathbf{U}_{\\xi}\\mathbf{C}\\mathbf{U}_{\\xi}^{\\top}\\mathbf{y}_{i}\\right]^{p}\\leq2B(p)\\cdot(2p)^{2p}}\\\\ &{+2B(p)C(p/2)\\cdot(1+3w\\log(d/\\delta))^{p/2}\\cdot\\left[w^{\\frac{p}{2}-1}\\mathrm{tr}(\\mathbf{UC}^{p}\\mathbf{U}^{\\top})+\\left(\\mathrm{tr}(\\mathbf{UC}^{2}\\mathbf{U}^{\\top})\\right)^{p/2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the bounds for $T_{1}$ (20) and $T_{2}$ (26) substituting in (16), and noting that $\\operatorname{tr}(\\mathbf{U}\\mathbf{C}^{k}\\mathbf{U}^{\\top})=$ $\\mathrm{tr}(\\mathbf{C}^{k})$ for any $k$ , we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\mathrm{tr}(\\mathbf{C})-\\mathbf{x}_{i}^{\\top}\\mathbf{UCU}^{\\top}\\mathbf{x}_{i}\\right)^{p}\\right]\\leq2^{p-1}A(p)\\cdot\\left(w^{p-1}\\cdot\\mathrm{tr}(\\mathbf{C}^{p})+\\left(w\\cdot\\mathrm{tr}(\\mathbf{C}^{2})\\right)^{p/2}\\right)}\\\\ &{\\qquad\\qquad+2^{p}B(p)C(p/2)\\cdot\\left(1+3w\\log(d/\\delta)\\right)^{p/2}\\cdot\\left[w^{\\frac{p}{2}-1}\\mathrm{tr}(\\mathbf{C}^{p})+\\left(\\mathrm{tr}(\\mathbf{C}^{2})\\right)^{p/2}\\right]}\\\\ &{\\qquad\\qquad+2^{p}B(p)(2p)^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we specify the various constants depending on $p$ . We have $A(p)\\ \\leq\\ (2p)^{p},B(p)\\ \\leq$ $(2p)^{p},C(p/2)\\,\\leq\\,p^{p/2}$ . Also we use $\\operatorname{tr}(\\mathbf{C}^{k})\\,\\leq\\,6^{k}\\cdot d$ since $\\mathbf{C}\\,\\preceq\\,6\\mathbf{I}$ . This implies for an absolute constant $c$ we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{C})-\\mathbf{x}_{i}^{\\top}\\mathbf{UCU^{\\top}x}_{i}\\right]^{p}\\right)^{1/p}\\leq c\\cdot p^{3}\\sqrt{d}\\cdot\\left(1+\\sqrt{\\frac{d p\\log(d/\\delta)}{s}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\delta>0$ is now arbitrary. ", "page_idx": 25}, {"type": "text", "text": "E Application to distributed settings with partitioned data ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we extend our main result to settings where the dataset is partitioned across multiple machines. We illustrate this in the distributed setting considered in [43]. Let $\\mathbf{A}\\in\\mathbb{R}^{N\\times d}$ and $\\mathbf{b}\\in\\dot{\\mathbb{R}}^{N}$ with $\\operatorname{rank}(\\mathbf{A})=d$ . In practice, we randomly shuffle the rows of A and $\\mathbf{b}$ , and partition the data uniformly across $q$ machines, with every machine getting chunks of size $n$ denoted by $(\\mathbf{A}_{i},\\mathbf{b}_{i})$ . In this distributed setup, $\\mathbf{A}_{i}$ can be considered a uniformly subsampled sketch of $\\mathbf{A}$ with sketch size $n$ . However, similar to the analysis in [43], in the following analysis we assume that each machine constructs its sketch $(\\mathbf{A}_{i},\\mathbf{b}_{i})$ by uniformly sampling rows from A and b with replacement and independently from other machines. This setting is not ideally a partition but we believe similar results can also be shown for the setting where we partition the dataset after reshuffling the rows. Let $\\begin{array}{r}{\\mu=N\\operatorname*{max}_{i=1}^{n}\\ell_{i}(\\mathbf{A})}\\end{array}$ denote the matrix coherence of A. Let $\\mathbf{S}\\in\\mathbb{R}^{n\\times N}$ be a uniform sampling matrix with size $n$ , where we will assume that S also scales the samples by $\\sqrt{N/n}$ , which implies that $\\mathbb{E}[\\mathbf{S}^{\\top}\\mathbf{S}]=\\mathbf{I}$ . Then the following holds: ", "page_idx": 26}, {"type": "text", "text": "Lemma 17 (Subspace embedding using uniform subsampling; based on Theorem 12 in [43]). Let $\\textbf{S}\\in\\mathbb{R}^{n\\times N}$ be a uniform sampling matrix of size $n$ where $n\\,\\geq\\,O(\\mu\\log(d/\\delta))$ . Then with probability $1-\\delta$ we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{1}{1+\\eta}}\\cdot\\mathbf{A}^{\\top}\\mathbf{A}\\preceq\\mathbf{A}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{A}\\preceq(1+\\eta)\\cdot\\mathbf{A}^{\\top}\\mathbf{A}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\eta=O(\\sqrt{\\mu\\log(d/\\delta)/n})$ . ", "page_idx": 26}, {"type": "text", "text": "Let $\\mathbf{x}^{*}=\\mathbf{A}^{\\dagger}\\mathbf{b}$ denote the minimizer of least squares loss $\\|\\mathbf{Ax}-\\mathbf{b}\\|^{2}$ and $\\mathbf{x}_{i}^{*}=\\mathbf{A}_{i}^{\\dag}\\mathbf{b}_{i}$ denote the solution to the sketched least squares problem at $i^{t h}$ machine. Here ${\\bf A}_{i}={\\bf S}_{i}{\\bf A}$ and $\\mathbf{b}_{i}=\\mathbf{S}_{i}\\mathbf{b}$ , where $\\mathbf{S}_{i}$ denotes the uniform subsampling matrix for $i^{t h}$ machine. We can use the model averaging result from [43] to prove the following result. ", "page_idx": 26}, {"type": "text", "text": "Lemma 18 (Model averaging result; adapted from Theorem 20 in [43]). Let there be q machines and each machine constructs its sketch $(\\mathbf{A}_{i},\\mathbf{b}_{i})$ independently by uniformly sampling n rows from A and b, where $n=O(\\mu\\log(d q/\\delta))$ . Then with probability 0.99, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{A}\\bar{\\mathbf{x}}^{*}-\\mathbf{b}\\right\\|^{2}\\leq\\left(1+c\\Big(\\frac{\\mu\\log(d/\\delta)}{q n}+\\frac{\\mu^{2}\\log^{2}(d/\\delta)}{n^{2}}\\Big)\\right)\\cdot\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{x}}^{*}=\\frac{1}{q}\\sum_{i=1}^{q}\\mathbf{x}_{i}^{*}}\\end{array}$ and $c>0$ is an absolute constant2. ", "page_idx": 26}, {"type": "text", "text": "Since computing $\\mathbf{x}_{i}^{*}$ at any machine could be potentially expensive due to large $n$ , we instead find $\\tilde{\\mathbf{x}}_{i}$ at $i^{t h}$ machine using the sketching procedure from Theorem 2. Note that this requires applying our debiasing techniques to the least squares problem min $\\lVert{\\bf A}_{i}{\\bf x}-{\\bf b}_{i}\\rVert^{2}$ at every machine. To that end, let $\\begin{array}{r}{\\bar{\\mathbf{x}}=\\frac{\\bar{1}}{q}\\sum_{i=1}^{q}\\bar{\\tilde{\\mathbf{x}}}_{i}}\\end{array}$ and consider $\\bar{\\mathbb{E}_{\\mathcal{P}}}\\|\\mathbf{A}\\bar{\\mathbf{x}}-\\mathbf{b}\\|^{2}$ , where $\\mathbb{E}_{\\mathcal{P}}$ means conditional expectation given the sketches $(\\mathbf{A}_{i},\\mathbf{b}_{i})$ at every machine. The following theorem states that if the bias from partitioning is much smaller than the sketching error in each machine, then our approach can be successfully used to reduce the bias of the sketching estimate so that averaging will work as desired. ", "page_idx": 26}, {"type": "text", "text": "Theorem 6. Let there be q machines and each machine constructs its sketch $(\\mathbf{A}_{i},\\mathbf{b}_{i})$ independently by uniformly sampling $n$ rows from A and b, where $n\\geq O(\\mu\\log(d q/\\delta))$ . Also, let $i^{t h}$ machine construct $\\tilde{\\mathbf{x}}_{i}$ via locally sketching $\\mathbf{A}_{i}$ and ${\\bf b}_{i}$ such that $\\tilde{\\mathbf{x}}_{i}$ satisfies Theorem $^{\\,l}$ (with $\\mathbf{A}_{i}$ and ${\\bf b}_{i}$ ) at $i^{t h}$ machine. Then, the estimator q1 iq=1 \u02dcxi averaged across the machines satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}\\bar{\\mathbf{x}}-\\mathbf{b}\\|^{2}\\leq\\bigg(1+c^{\\prime}\\Big(\\epsilon+\\frac{1}{q}+\\frac{\\mu\\log(d/\\delta)}{q n}+\\frac{\\mu^{2}\\log^{2}(d/\\delta)}{n^{2}}\\Big)\\bigg)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with probability at least 0.9 and an absolute constant $c^{\\prime}>0$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. By the Pythagorean theorem we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}\\bar{\\mathbf{x}}-\\mathbf{b}\\|^{2}=\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}+\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbf{x}^{*})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We proceed to upper bound $\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbf{x}^{*})\\|^{2}$ as, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbf{x}^{*})\\|^{2}\\leq2\\left(\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\bar{\\mathbf{x}}^{*})\\|^{2}+\\|\\mathbf{A}(\\bar{\\mathbf{x}}^{*}-\\mathbf{x}^{*})\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We use Lemma 18 to upper bound the second term and get, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbf{x}^{*})\\|^{2}\\leq2\\left(\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\bar{\\mathbf{x}}^{*})\\|^{2}+c\\Big(\\frac{\\mu\\log(d/\\delta_{1})}{q n}+\\frac{\\mu^{2}\\log^{2}(d/\\delta_{1})}{n^{2}}\\Big)\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\bar{\\mathbf{x}}^{*})\\|^{2}=\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbb{E}_{\\mathcal{P}}[\\bar{\\mathbf{x}}]+\\mathbb{E}_{\\mathcal{P}}[\\bar{\\mathbf{x}}]-\\bar{\\mathbf{x}}^{*}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathcal{P}}\\|\\mathbf{A}(\\bar{\\mathbf{x}}-\\mathbb{E}_{\\mathcal{P}}[\\bar{\\mathbf{x}}])\\|^{2}+\\|\\mathbf{A}(\\mathbb{E}_{\\mathcal{P}}[\\bar{\\mathbf{x}}]-\\bar{\\mathbf{x}}^{*})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We upper bound both terms in (30) separately. Proceeding with the first term we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{P}|\\mathbf{A}(\\mathbf{x}-\\mathbb{E}_{P}|\\mathbf{A})|^{2}=\\mathbb{E}_{P}\\Big|\\mathbf{A}\\Big(\\frac{1}{Q}\\frac{\\sqrt{\\mathbf{A}}}{\\underset{i=1}{\\overset{n}{\\sum}}}(\\mathbf{A}_{i}-\\mathbb{E}_{P}|\\mathbf{A}_{i}|)\\Big)\\Big|^{2}}\\\\ &{=\\frac{1}{Q^{2}}\\cdot\\mathbb{E}_{P}\\Big|\\mathbf{A}\\Big(\\frac{\\sqrt{\\mathbf{A}}}{\\underset{i=1}{\\overset{n}{\\sum}}}(\\mathbf{\\hat{x}}_{i}-\\mathbb{E}_{P}|\\mathbf{\\hat{x}}_{i}|)\\Big)\\Big|^{2}}\\\\ &{=\\frac{1}{Q^{2}}\\cdot\\sum_{i=1}^{Q}\\mathbb{E}_{P}|\\mathbf{A}(\\mathbf{\\hat{x}}_{i}-\\mathbb{E}_{P}|\\mathbf{\\hat{x}}_{i}|)|^{2}}\\\\ &{\\leq\\frac{1}{Q^{2}}\\frac{\\sqrt{\\mathbf{A}}}{Q^{2}}\\cdot\\frac{\\mathbf{w}}{\\underset{i=1}{\\overset{n}{\\sum}}}\\mathbb{E}_{P}|\\mathbf{A}_{i}(\\mathbf{\\hat{x}}_{i}-\\mathbb{E}_{P}|\\mathbf{\\hat{x}}_{i}|)|^{2}}\\\\ &{\\leq\\frac{2(1+\\eta)}{Q^{2}}\\cdot\\frac{\\sqrt{\\mathbf{A}}}{\\underset{i=1}{\\overset{n}{\\sum}}}\\mathbb{E}_{P}\\Big(\\mathbf{\\hat{L}}_{\\mathbf{A}}(\\mathbf{\\hat{x}}_{i}-\\mathbf{x}_{i}^{\\prime}|)|^{2}+\\|\\mathbf{A}_{i}(\\mathbb{E}_{P}|\\mathbf{\\hat{x}}_{i}|-\\mathbf{x}_{i}^{\\prime}|)|^{2}\\Big)}\\\\ &{\\leq\\frac{2(1+\\eta)}{Q^{2}}\\cdot\\frac{\\sqrt{\\mathbf{A}}}{\\underset{i=1}{\\overset{n}{\\sum}}}\\mathbb{E}_{P}\\Big|\\mathbf{A}_{i}\\mathbf{x}_{i}^{\\prime}-\\mathbf{b}_{i}^{\\prime}\\|^{2}}\\\\ &{\\leq\\frac{2(1+\\eta)}{Q^{2}}\\cdot\\frac{\\sqrt{\\mathbf{B}}}{\\underset{i=1}{\\overset{n}{\\sum}}}\\int\\mathbb{E}_{P}|\\mathbf{A}_{i}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The equality (31) holds as all $\\tilde{\\mathbf{x}}_{i}^{\\prime}s$ are independent. The inequality (32) holds due to the subspace embedding property (Lemma 17). The inequality (33) holds due to the variance bound in Theorem 1. The last inequality holds because $\\mathbf{x}_{i}^{*}$ minimizes the least squares loss $\\lVert{\\bf A}_{i}{\\bf x}-{\\bf b}_{i}\\rVert^{2}$ at $i^{t h}$ machine. We now upper bound the second term in (30) as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{A}(\\mathbb{E}_{P}[\\hat{\\mathbf{x}}]-\\hat{\\mathbf{x}}^{*})\\|=\\Big\\|\\mathbf{A}\\Big(\\frac{1}{q}\\frac{q}{i-1}\\big(\\mathbb{E}_{P}[\\hat{\\mathbf{x}}_{i}]-\\mathbf{x}_{i}^{*}\\big)\\Big)\\Big\\|}&{}\\\\ {\\le\\frac{1}{q}\\cdot\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}(\\mathbb{E}_{P}[\\hat{\\mathbf{x}}_{i}]-\\mathbf{x}_{i}^{*})\\|}&{}\\\\ {\\le\\frac{\\sqrt{1+\\eta}}{q}\\cdot\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}(\\mathbb{E}_{P}[\\hat{\\mathbf{x}}_{i}]-\\mathbf{x}_{i}^{*})\\|}&{}\\\\ {\\le\\frac{\\sqrt{\\epsilon(1+\\eta)}}{q}\\cdot\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}_{i}^{*}-\\mathbf{b}_{i}\\|}&{}\\\\ {\\le\\frac{\\sqrt{\\epsilon(1+\\eta)}}{q}\\cdot\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}_{i}^{*}-\\mathbf{b}_{i}\\|}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The inequality (35) holds due to the subspace embedding property (Lemma 17) and inequality (36) holds due to the bias bound in Theorem 1. Therefore we get, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}(\\mathbb{E}_{\\mathcal{P}}[\\bar{\\mathbf{x}}]-\\bar{\\mathbf{x}}^{*})\\|^{2}\\leq\\frac{\\epsilon(1+\\eta)}{q^{2}}\\Big(\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|\\Big)^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining (28, 29, 34, 37) and assuming $\\eta<1$ we get, $\\begin{array}{r l r}&{\\mathbb{\\bar{\\tau}}_{P}\\|\\mathbf{A}\\bar{\\mathbf{x}}-\\mathbf{b}\\|^{2}\\leq\\Big(1+2c\\Big(\\displaystyle\\frac{\\mu\\log(d/\\delta_{1})}{q n}+\\displaystyle\\frac{\\mu^{2}\\log^{2}(d/\\delta_{1})}{n^{2}}\\Big)\\Big)\\cdot\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}+\\displaystyle\\frac{8}{q^{2}}\\cdot\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|}&\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{4\\epsilon}{q^{2}}\\Big(\\displaystyle\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|\\Big)^{2}.}&{\\displaystyle(38)}\\end{array}$ As $\\begin{array}{r}{\\mathbb{E}\\!\\left(\\frac{1}{q}\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|^{2}\\right)=\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}}\\end{array}$ , we just use Markov\u2019s inequality to upperbound the second term in (38) by $\\frac{160}{q}\\|\\mathbf{Ax^{*}-b}\\|^{2}$ with probability 0.95. For the last term, we have $\\sum_{i=1}^{q}\\left\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\right\\|\\right)^{2}=q\\cdot\\frac{1}{q}\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|^{2}+(q^{2}-q)\\cdot\\frac{1}{q^{2}-q}\\sum_{i,j,i\\neq j}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|\\cdot\\|\\mathbf{A}_{j}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|.$ bj Again $\\begin{array}{r}{\\frac{1}{q}\\sum_{i=1}^{q}\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|^{2}\\leq20\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}}\\end{array}$ with probability 0.95. The last term is an average of $\\;q^{2}-q$ random variables and we can provide uniform bound for the expectation of all of them using the independence among different machines. We have $\\mathbb{E}[\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|\\cdot\\|\\mathbf{A}_{j}\\mathbf{x}^{*}-\\mathbf{b}_{j}\\|]\\le$ $\\sqrt{\\mathbb{E}[\\|\\mathbf{A}_{i}\\mathbf{x}^{*}-\\mathbf{b}_{i}\\|^{2}]}\\cdot\\sqrt{\\mathbb{E}[\\|\\mathbf{A}_{j}\\mathbf{x}^{*}-\\mathbf{b}_{j}\\|^{2}]}=\\|\\mathbf{A}\\mathbf{x}^{*}-\\mathbf{b}\\|^{2}$ . Again using Markov\u2019s inequality we can upper bound the average of these $\\;q^{2}-q$ random variables with 20 times their expectation with probability 0.95, finishing the proof of the theorem. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "F Further experimental details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here, we provide a small set of numerical experiments to empirically examine the relative error of distributed averaging estimates from individual machines to return an estimator $\\begin{array}{r}{\\hat{\\mathbf{x}}\\,=\\,\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{x}}_{i}}\\end{array}$ First, we show that when the sketching-based estimates $\\tilde{\\mathbf{x}}_{i}$ do have a non-negligible bias, so that the distributed averaging estimator $\\hat{\\bf x}$ remains inconsistent in the number of machines \u2013 even with an unlimited number of machines, as long as the space on each machine is limited, the averaged estimator\u2019s performance will be limited by the bias of the individual estimates. Second, we show that one can use sketching to compress a data subsample at no extra computational cost, without increasing its bias, which we refer to as the free lunch in distributed averaging via sketching. ", "page_idx": 28}, {"type": "text", "text": "We examine three benchmark regression datasets, Abalone (4177 rows, 8 features), Boston (506 rows, 13 features), and YearPredictionMSD (truncated to the first 2500 rows, with 90 features), from the libsvm repository from [8]. All the experiments were run on an i9-13900k processor with 128GB of RAM and an NVIDIA RTX 3090 GPU. We repeat the experiment 100 times, with the shaded region representing the standard error. We visualize the relative error of the averaged sketchand-solve estimator $\\frac{L(\\hat{\\mathbf{x}}){-}L(\\mathbf{x}^{*})}{L(\\mathbf{x}^{*})}$ , against the number of machines $q$ used to generate the estimate $\\begin{array}{r}{\\hat{\\mathbf{x}}=\\frac{1}{q}\\sum_{i=1}^{q}\\tilde{\\mathbf{x}}_{i}.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Each estimate $\\tilde{\\mathbf{x}}_{i}$ was constructed with the same sparsification strategy used by LESS, except that instead of sparsifying the sketch with leverage scores, we instead sparsify them with uniform probabilities. Following [16], we call the resulting method LESSUniform. Within each dataset, we perform four simulations, each with different sketch sizes and different numbers of nonzero entries per row. We vary these so that the product (sketch size $\\times\\,\\mathfrak{n n z}$ per row) stays the same, so as to ensure that the total cost of sketching is fixed in each plot. ", "page_idx": 28}, {"type": "text", "text": "As expected, decreasing the sketch size while increasing the number of nonzeros per row (effectively increasing the amount of \u201ccompression\u201d occurring here by sparse sketching) increases the error in all three datasets. However, remarkably, increasing the amount of \u201ccompression\u201d does not seem to increase the amount of bias. We can therefore conclude that sparse sketches preserve nearunbiasedness, while enabling us to reduce the sketch size from subsampling without incurring any additional computational cost. The increase in error can be mitigated in a distributed setting by increasing the number of estimates/machines. ", "page_idx": 28}, {"type": "text", "text": "F.1 Comparison with other sketching estimators ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To further illustrate the phenomenon that suitable sketched least squares estimators enjoy small bias, we provide a further set of numerical experiments below in Figure 4 for the Boston and Abalone datasets. Recall that in Figures 2 and 3, we show that increasing the amount of \u201ccompression\u201d of the sample, by reducing the sketch size when using sparse sketching matrices with uniform probabilities, does not increase the bias of the sketched estimator. Figure 4 further demonstrates that Gaussian and Subgaussian sketches, which are computationally much more expensive than our proposed sparse sketches, exhibit virtually no bias in sketched least squares (as expected). A similar conclusion can be made for the proper LESS method, which uses leverage score estimates, and is therefore more expensive than the LESSUniform method we use in other experiments, but still potentially much cheaper than Subgaussian sketches. ", "page_idx": 28}, {"type": "image", "img_path": "rkuVYosT2c/tmp/4cedaea82a915f88dfb407e56e84acb0382a003323de9c5de08c0902e53a9dba.jpg", "img_caption": ["Figure 3: Comparison of the relative error of the distributed averaging estimator of sketch-andsolve least squares estimates where the sketches are constructed with sparse sketching matrices with uniform probabilities (LESSUniform) on libsvm dataset Boston (see Figure 2 for results on YearPredictionMSD and Abalone). For each dataset, the computational cost of sketching is the same in all four parameter settings. Remarkably, sketching to a smaller size appears to preserve near-unbiasedness without incurring any additional computational cost. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "However, the unbiasedness does not hold for all sparse sketches, as we demonstrate below in Figure 4. At a high level, we show that sketches constructed using leverage score subsampling and the SRHT can exhibit a non-negligible level of least squares bias. ", "page_idx": 29}, {"type": "text", "text": "Gaussian and Subgaussian sketches, as we see in the figure, enjoy near-unbiasedness while not introducing additional bias as the sketch size decreases. This does not hold for leverage score subsampling. When we decrease the number of subsamples within leverage score subsampling, the bias introduced by subsampling increases as suggested by the lower bound in Theorem 10 of [18]. Intuitively, this happens as the number of subsamples is reduced without increasing the amount of \u201ccompression\u201d as one would with LESS or LESSUniform. ", "page_idx": 29}, {"type": "text", "text": "We also show that the subsampled randomized Hadamard transform (SRHT) also introduces increased bias as the sketch size decreases. This complements the lower bound established for the failure of SRHT and other data-oblivious sparse embeddings like CountSketch to satisfy the restricted Bai-Silverstein property that is a structural condition for near-unbiasedness sketches [18]. ", "page_idx": 29}, {"type": "text", "text": "For completeness, we also demonstrate the desirable performance of LESS proper. Figure 4 demonstrates that the desirable phenomenon that LESSUniform enjoys also extends to LESS proper. In fact, we observe that LESS enjoys similar desirable performance as the Gaussian and Subgaussian sketches and does not increase (in fact, it has minimal) least squares bias, while enjoying the computational speedups that sparse sketches also enjoy, suggesting a best-of-both-worlds property. ", "page_idx": 29}, {"type": "image", "img_path": "rkuVYosT2c/tmp/043d2417af5bb4de8efb70004b29fe0bad5e1986ae49b86a8ed84ed98cf2535b.jpg", "img_caption": ["Figure 4: Distributed averaging experiment for the Boston and Abalone datasets using five different sketching techniques: Leverage Score Subsampling, Subsampled Randomized Hadamard Transform (SRHT), Gaussian sketches, Subgaussian sketches, and LESS. Both Gaussian and Subgaussian sketches exhibit no observable least squares bias, whereas Leverage Score Subsampling and SRHT exhibit a small but measurable level of bias. LESS enjoys the similar high performance as the Gaussian and Subgaussian sketches and does not increase (in fact, it has minimal) least squares bias, but as a sparse sketch also allows for the significant improvements in runtime that sparse sketches enjoy, demonstrating that it enjoys the best-of-both-worlds. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper provides concise mathematical proofs for the claims made in the abstract. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The results proved in the paper hold in a very general setting, without any limitations or requiring strong assumptions. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All theoretical results have been proved. Most of the formal proofs are part of the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The experiments demonstrate the results for solving a few least-squares problems using the Matrix sketching technique described in the paper. All the experimental results are easily reproducible. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: A supplementary zipfile containing the code is provided to reproduce the experimental results. One of the experiments requires the file \u2018YearPredictionMSD.txt\u2019 file, which can be obtained at (https://archive.ics.uci.edu/dataset/ 203/yearpredictionmsd). ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper describes the parameters e.g. sketch size, sparsity of sketch (nnz per row), and number of estimates(number of machines) along with dataset details to understand the results. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The shaded areas in our plots denote the standard error obtained after averaging the result over 100 trials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All experiments were run on an i9-13900k with 128GB of RAM and a NVIDIA RTX 3090 GPU. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work aligns with the NeurIPS code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our theoretical work provides a Matrix sketching algorithm, recovering a nearly unbiased least squares estimator. Solving linear systems approximately is an important component of a large number of algorithms from varied domains. Our work can thus provide a way to recover approximation solutions with a smaller bias. This does not lead to any negative societal impact. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This work does not release any data or model, and carries no risk of misuse. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We test our theoretical results on a few publicly available datasets that have been cited in the paper as well. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No new assets are released in this work. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]