[{"figure_path": "bzuQtVDxv0/figures/figures_1_1.jpg", "caption": "Figure 1: We propose an approach to convert a video into a Video Gaussian Representation (VGR), which can be used for versatile video processing tasks conveniently.", "description": "This figure illustrates the overall pipeline of the proposed approach. A video is converted into a Video Gaussian Representation (VGR), which is then used for various downstream video processing tasks, including tracking, depth prediction, stereoscopic view synthesis, and video editing. The VGR serves as a convenient and versatile intermediate representation that facilitates these tasks.", "section": "1 Introduction"}, {"figure_path": "bzuQtVDxv0/figures/figures_2_1.jpg", "caption": "Figure 2: Pipeline of our approach. Given a video, we represent its intricate 3D content using video Gaussians in the camera coordinate space. By associating them with motion parameters, we enable video Gaussians to capture the video dynamics. These video Gaussians are supervised by RGB image frames and 2D priors such as optical flow, depth, and label masks. This representation makes it convenient for users to perform various editing tasks on the video.", "description": "This figure illustrates the overall pipeline of the proposed method. The input is a video sequence. The video is converted to a 3D representation using video Gaussians in a camera coordinate space. Each Gaussian is associated with motion parameters to capture video dynamics.  The representation is supervised using RGB frames and additional 2D priors such as optical flow, depth, and masks. This allows convenient video editing and processing operations, including painting, adding, removing objects and novel view and stereoscopic video interpolations.", "section": "4 Method"}, {"figure_path": "bzuQtVDxv0/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparison of video reconstruction using our method and SOTA methods.", "description": "This figure shows a qualitative comparison of video reconstruction results obtained using the proposed method and three state-of-the-art (SOTA) methods: CoDeF, Omnimotion, and Ground Truth (GT). The figure presents two example video sequences side-by-side. In each example, the results from each method are shown in separate frames, allowing for a visual comparison of the reconstruction quality. This comparison highlights the ability of the proposed method to generate more realistic and detailed video reconstructions compared to existing methods.", "section": "6 Experiments"}, {"figure_path": "bzuQtVDxv0/figures/figures_6_2.jpg", "caption": "Figure 11: Tracking results visualization shows our method outperforms CoDeF [33] and 4DGS [51], especially in handling large-scale view changes and scene motions.", "description": "This figure shows a comparison of dense tracking results between the proposed method and two state-of-the-art methods (CoDeF and 4DGS) across three different video clips.  Each clip presents unique challenges for tracking, including large-scale changes in viewpoint and fast object motion.  The visualization highlights the superior performance of the proposed method, especially in handling these complex motion patterns.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparison of video depth and features generated by our method and SOTA single-frame estimation methods. Our method yields more consistent estimations.", "description": "This figure compares the video depth and features generated by the proposed method with those from state-of-the-art (SOTA) single-frame estimation methods.  It visually demonstrates that the proposed method produces more consistent depth and feature estimations across frames compared to the SOTA methods. The improved consistency is highlighted by the visual differences in the depth and feature maps generated by each approach. Red boxes highlight regions where the difference is most apparent.", "section": "6.1 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_8_1.jpg", "caption": "Figure 6: Appearance editing results using the 2D prompt editing method [60].", "description": "This figure shows the results of appearance editing on a video using a 2D prompt editing method. The top row displays the original video frames. The middle row shows the results of applying a sketch-like style transfer, and the bottom row shows the results of applying a cartoon-like style transfer. The results demonstrate that the proposed method is able to effectively transfer appearance styles to videos.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_8_2.jpg", "caption": "Figure 1: We propose an approach to convert a video into a Video Gaussian Representation (VGR), which can be used for versatile video processing tasks conveniently.", "description": "This figure demonstrates the proposed method of converting a video into a Video Gaussian Representation (VGR).  It shows the input video and several downstream applications that benefit from this representation, such as tracking, consistent depth prediction, stereoscopic video synthesis, and video editing. The VGR acts as an intermediary step to enable convenient and versatile video processing. ", "section": "1 Introduction"}, {"figure_path": "bzuQtVDxv0/figures/figures_9_1.jpg", "caption": "Figure 8: Stereo view synthesis. One original frame is visualized in the first column for comparison.", "description": "This figure shows the results of stereoscopic video creation using the proposed Video Gaussian Representation (VGR).  The first column displays a single frame from the original video.  The remaining columns present synthesized stereoscopic views generated by slightly translating the video Gaussians horizontally, simulating the interocular distance. This demonstrates the 3D capabilities of the VGR, enabling the creation of novel viewpoints and stereoscopic videos.", "section": "6.3 Limitations"}, {"figure_path": "bzuQtVDxv0/figures/figures_9_2.jpg", "caption": "Figure 9: Our method may underperform in two scenarios. (1) Without camera estimation, it has to learn large camera rotations as scene motions, causing distant background blur (left). (2) It fails to track fast-moving transients, as photometric loss is insufficient for motion fitting (right).", "description": "This figure shows two scenarios where the proposed method, which does not estimate camera poses, may underperform. In the first scenario (left), the model must learn large camera rotations as scene motions, which results in blurry backgrounds.  In the second scenario (right), it fails to track fast-moving objects because the photometric loss is insufficient for accurate motion fitting. The figure visually compares the ground truth video frames with the results generated by the proposed method, highlighting the limitations in handling complex motion scenarios.", "section": "6.3 Limitations"}, {"figure_path": "bzuQtVDxv0/figures/figures_14_1.jpg", "caption": "Figure 10: Video reconstruction compared with RoDynRF [23] and 4DGS [51]. Our method has higher PSNR and better visual quality.", "description": "This figure compares the video reconstruction quality of the proposed method with two state-of-the-art methods, RoDynRF and 4DGS.  Two video examples are shown: a person wakeboarding and a motorcycle doing a wheelie. For each example, the figure displays two frames (t1 and t2) reconstructed by each method.  The Peak Signal-to-Noise Ratio (PSNR) values are provided for each reconstruction to quantify the visual quality. The results demonstrate that the proposed method achieves significantly higher PSNR values, indicating superior visual quality compared to the other two methods.", "section": "6.1 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_15_1.jpg", "caption": "Figure 11: Tracking results visualization shows our method outperforms CoDeF [?] and 4DGS [? ], especially in handling large-scale view changes and scene motions.", "description": "This figure compares the dense tracking results of three different methods: CoDeF, 4DGS, and the proposed method.  The results are shown across several frames of a video depicting a person performing parkour-like movements.  The visualization highlights the ability of each method to accurately track the person's movement through significant changes in viewpoint and the challenges of dealing with complex motion. The proposed method showcases better performance in maintaining consistent and accurate tracking across the frames compared to the other two methods.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_15_2.jpg", "caption": "Figure 12: The depth prior (w/ depth) ensures video Gaussians conform to a realistic layout, while rigidity regularization (w/ rigid) eliminates floaters.", "description": "This figure in the paper demonstrates the effects of depth and rigidity regularization on the generated video.  The top row shows depth maps, and the bottom row shows the corresponding RGB frames.  The leftmost column (w/o rigid) shows the result without rigidity regularization \u2013 notice the unrealistic floating structures. The second column (w/ rigid) shows the improved result with rigidity regularization, which eliminates the floating structures.  The third and fourth columns show the effect of the depth prior (w/ depth vs w/o depth). The improved consistency and realistic representation of depth in the image on the rightmost column highlight the benefit of using the depth prior.", "section": "A.3 Ablation Study"}, {"figure_path": "bzuQtVDxv0/figures/figures_15_3.jpg", "caption": "Figure 13: Ablation for camera model / L2 depth loss", "description": "This figure shows the ablation study results for different camera models (orthographic vs. perspective) and depth loss functions (scale-and-shift-trimmed loss vs. L2 loss).  It visually demonstrates the impact of these choices on the quality of the video reconstruction, highlighting the importance of the chosen camera model and depth loss in maintaining the 3D structure and visual fidelity of the representation.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_16_1.jpg", "caption": "Figure 14: Ablation study on hyperparameters n and l reveals optimal quality at n = 8, l = 8.", "description": "This figure shows an ablation study on the hyperparameters n and l used in the paper's method for representing the 3D motion of Gaussian points. The study is conducted by testing different combinations of n and l values and comparing the results to the ground truth (GT). Each row of the image shows a sequence of frames reconstructed with different n and l combinations, with a red bounding box highlighting the area of interest. The results show that the optimal quality is achieved when n = 8 and l = 8, indicating that the model benefits from both polynomial and Fourier bases in representing the smooth trajectories of objects. However, using only polynomial bases (n=0, l=8) or only Fourier bases (n=8, l=0) significantly degrades the result quality.", "section": "6.3 Ablation Study"}, {"figure_path": "bzuQtVDxv0/figures/figures_17_1.jpg", "caption": "Figure 15: Segmentation comparison reveals that our lifted SAM features outperform per-frame SAM segmentation, delivering higher resolution feature maps and superior IoU.", "description": "This figure compares the segmentation results obtained using the proposed method's lifted SAM features against those obtained using per-frame SAM features. The results demonstrate that the proposed method's lifted SAM features yield higher-resolution feature maps and achieve a significantly superior Intersection over Union (IoU) score, indicating improved segmentation accuracy and detail.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_17_2.jpg", "caption": "Figure 16: Video interpolation results. Please refer to the supplementary video for better visualization.", "description": "This figure shows the video interpolation results for two example video clips. The top row shows a kangaroo jumping over a road, and the bottom row shows a train moving around a circular track.  In both cases, intermediate frames are generated at t+0.25, t+0.5, t+0.75 between the original frames (t) and the final frame (t+1), demonstrating the smooth interpolation achieved by the proposed method.", "section": "5 Video Processing Applications"}, {"figure_path": "bzuQtVDxv0/figures/figures_18_1.jpg", "caption": "Figure 17: Segmentation and editing results are displayed from left to right: multi-object segmentation and video editing across two frames. The first row illustrates geometry editing where two individuals are copied and repositioned to lie on the ground and stand aside. The second shows the appearance editing with \"2024\" and \"NeurIPS\" painted on two fish.", "description": "This figure demonstrates the capabilities of the proposed Video Gaussian Representation (VGR) for multi-object editing in videos.  The top row shows geometry editing: two individuals are selected and their positions and poses are modified. The bottom row showcases appearance editing: text is added to the video.  This illustrates the model's ability to handle complex editing tasks that require understanding and manipulating both appearance and motion of multiple objects.", "section": "5 Video Processing Applications"}]