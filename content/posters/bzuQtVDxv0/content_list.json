[{"type": "text", "text": "Splatter a Video: Video Gaussian Representation for Versatile Processing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang-Tian Sun1\u2217 Yi-Hua Huang1\u2217 Lin Ma Xiaoyang Lyu1 Yan-Pei Cao2 Xiaojuan $\\mathbf{Q^{i^{1\\dagger}}}$ ", "page_idx": 0}, {"type": "text", "text": "1The University of Hong Kong 2 VAST ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Video representation is a long-standing problem that is crucial for various downstream tasks, such as tracking, depth prediction, segmentation, view synthesis, and editing. However, current methods either struggle to model complex motions due to the absence of 3D structure or rely on implicit 3D representations that are ill-suited for manipulation tasks. To address these challenges, we introduce a novel explicit 3D representation\u2014video Gaussian representation\u2014that embeds a video into 3D Gaussians. Our proposed representation models video appearance in a 3D canonical space using explicit Gaussians as proxies and associates each Gaussian with 3D motions for video motion. This approach offers a more intrinsic and explicit representation than layered atlas or volumetric pixel matrices. To obtain such a representation, we distill 2D priors, such as optical flow and depth, from foundation models to regularize learning in this ill-posed setting. Extensive applications demonstrate the versatility of our new video representation. It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Video processing, which encompasses a variety of tasks such as video editing, can enable numerous applications in fields like social media, filmmaking, and advertising [2, 50]. A video can be viewed as a collection of spatiotemporal pixels. However, processing a video directly in its pixel space, while maintaining temporal consistency, poses challenges due to the inherent complexities associated with appearance, motion, occlusions, and noise in the video data [14, 27]. Consequently, a robust video representation capable of abstracting and disentangling appearance and motion is crucial for facilitating various applications and overcoming these challenges. ", "page_idx": 0}, {"type": "text", "text": "Existing research on video representation for processing has primarily focused on 2D/2.5D techniques, employing methods such as optical flow and tracking to associate pixels across frames [47, 14, 58]. These approaches often involve learning a canonical image [12, 33, 49, 30] or a layered atlas with persistent motion patterns [14, 24, 4, 9] to facilitate editing and then use optical flow or tracks to propagate edits throughout a video. The most recent work [33] utilizes hash grids combined with implicit functions to embed a video into a learned canonical image for appearance and a deformation field for motion. Despite achieving promising results in appearance editing tasks, these methods struggle to handle occlusions of objects (see Fig. 3), leading to erroneous propagation. Although layered 2.5D representation [14, 24, 4, 9] can mitigate this issue, they still face challenges with complex self-occlusions within a layer. Moreover, these techniques have limited or no capability in addressing processing tasks that require 3D information, such as video representation with complex occlusions, consistent depth prediction, and stereoscopic video generation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/964dd4f45387803caee82e1c631fbe9f8ebf42310099873f56451efdd55ee80f.jpg", "img_caption": ["Figure 1: We propose an approach to convert a video into a Video Gaussian Representation (VGR), which can be used for versatile video processing tasks conveniently. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Drawing inspiration from the fact that a video is essentially a projection of the dynamic 3D world onto the 2D image plane at different moments, we pose the question: is it possible to represent a video in its intrinsic 3D form? By doing so, we could potentially bypass the limitations of 2D representations, such as occlusions, reduce the complexity of motion modeling, and support processing tasks that require 3D information. Recent work [48] has explored 3D representations, which employ an implicit radiance field to model a canonical 3D space and leverage a bi-directional mapping network for associating 2D pixels with 3D representations. While this approach demonstrates promising performance in dense tracking, it falls short in faithfully representing video appearance, making it incapable of performing video processing tasks that require generating new videos, such as video editing. Moreover, its implicit nature limits its applicability to a variety of video processing tasks that require explicit content or motion manipulations, such as the removal or addition of objects and adjustments to the motion patterns of objects. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel explicit video Gaussian representation (VGR) based on 3D Gaussians [16]. Our core idea revolves around utilizing Gaussians in a canonical 3D space to model video appearance while associating each Gaussian with time-dependent 3D motion attributes to control its locations at different time steps for video motion. This 3D representation can then be employed to process and render videos effectively. The subsequent challenge lies in how to map a video onto such a 3D Gaussian representation. This is inherently difficult due to the loss of essential 3D information during 3D-to-2D projection, as well as the entanglement of motion and appearance in videos. However, recent advancements in large models have facilitated the acquisition of high-quality monocular priors from images and videos, such as optical flow [44, 11] and monocular depth [54, 15, 53]. While these 2D priors may not be perfect, they can serve as regularization for learning through knowledge distillation. Consequently, we propose leveraging these 2D priors in conjunction with our 3D motion regularization for learning. By doing so, we effectively lift 2D information\u2013 such as pixels, depth, and optical flow\u2013into a unified and compact 3D representation. ", "page_idx": 1}, {"type": "text", "text": "Upon learning, our video Gaussian representation can be used to support versatile video processing tasks, as shown in Fig. 1. Here, we showcase its efficacy in 7 video-processing tasks: Specifically, it can be used to obtain 1) dense tracking and 2) improve the consistency of monocular 2D prior across frames, leading to better video depth and feature consistency. Secondly, our representation facilitates a range of video editing tasks, including 3) geometry editing and 4) appearance editing. Thirdly, it also proves useful in video interpolation, allowing for 5) the generation of smooth transitions between frames. Finally, as our representation is inherently 3D, it opens up additional possibilities, such as 6) novel view synthesis (to a certain extent) and 7) the creation of stereoscopic videos. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "As our method utilizes dynamic 3D Gaussians to represent videos and supports versatile video processing, this section introduces related works on video editing, tracking, and dynamic Gaussian splatting. We briefly cover the most relevant works. For additional references, see Sec.A.7. ", "page_idx": 1}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/6c2f634e1cc663ed24af0826298e977524eea9baab84f83440afc1f9d92afda8.jpg", "img_caption": ["Figure 2: Pipeline of our approach. Given a video, we represent its intricate 3D content using video Gaussians in the camera coordinate space. By associating them with motion parameters, we enable video Gaussians to capture the video dynamics. These video Gaussians are supervised by RGB image frames and 2D priors such as optical flow, depth, and label masks. This representation makes it convenient for users to perform various editing tasks on the video. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Video Editing Decomposing videos into layered representations facilitates advanced video editing techniques. Kasten et al. [14] introduced layered neural atlases, enabling efficient video propagation and editing. Further advancements include deformable sprites [58], bi-directional warping fields [9], and innovations in rendering lighting and color details [4]. CoDeF [33] and GenDeF [49] focus on multi-resolution hash grids and shallow MLPs for frame-by-frame deformations. Latent diffusion models [38] and methodologies like ControlVideo [61], MaskINT [28], and VidToMe [20] have also been employed for data-driven video editing. ", "page_idx": 2}, {"type": "text", "text": "Video Tracking Video tracking captures physical motion within video sequences. PIPs [8] and TAPIR [6] offer foundational approaches, while CoTracker [13] uses a sliding-window transformer for tracking. OminiMotion [48] and MFT [30] employ neural radiance fields and optical flow fields for dense tracking. State-of-the-art methods like RAFT [44] and FlowFormer [11] provide accurate flow estimations but struggle with long-term correspondences. ", "page_idx": 2}, {"type": "text", "text": "Dynamic Gaussian Splatting Gaussian Splatting [16] enhances rendering in radiance fields and has been extended to dynamic scenes [26, 57, 51]. Methods like SC-GS [10] and 3DGStream [43] offer novel approaches for scene dynamics. Our method targets monocular video representation, eliminating the need for camera pose estimations and facilitating robust long-term tracking and editing in dynamic scenes. ", "page_idx": 2}, {"type": "text", "text": "3 3D Gaussian Splatting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Gaussian splatting [16] models 3D scenes using Gaussians learned from multiview images. Each Gaussian, $G$ , is defined by a center $\\mu$ and a covariance matrix $\\Sigma$ : $G(x)=\\exp\\left(-{\\textstyle{\\frac{1}{2}}}(x-\\mu)^{\\vee}\\Sigma^{-1}(x-\\mu)\\right)$ . Here, $\\Sigma$ is decomposed into $R S S^{T}R^{T}$ for optimization, with $R$ as a rotation matrix parameterized by a quaternion $q$ and $S$ as a scaling matrix parameterized by a vector s. Each Gaussian also has an opacity $\\alpha$ and spherical harmonic $(S\\mathcal{H})$ coefficients $s h$ . Then 3D Gaussians can be formulated as: $\\mathcal{G}=\\{G_{j}:\\mu_{j},q_{j},s_{j},\\alpha_{j},s h_{j}\\}$ . Rendering is done via: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC(u)=\\sum_{i\\in N}T_{i}\\sigma_{i}S\\mathcal{H}(s h_{i},v_{i}),T_{i}=\\Pi_{j=1}^{i-1}(1-\\sigma_{j}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma_{i}$ is calculated by projecting Gaussian $G_{i}$ at the rendering pixel and $v$ is the direction from view point to the Gaussian. Optimizing parameters $\\left\\{{G_{j}}:{\\mu_{j}},{q_{j}},s_{j},{\\alpha_{j}},s h_{j}\\right\\}$ and adjusting densities allows for high-quality, real-time image synthesis. For a more detailed introduction to Gaussian Splatting, please refer to Sec. A.8. We extend 3D Gaussians to represent a video by adding attributes to Gaussians for versatile processing. ", "page_idx": 2}, {"type": "text", "text": "Given a video, our goal is to use 3D Gaussians in a canonical space to represent its appearance and associate Gaussians with 3D motions for video dynamics. To facilitate this mapping, we incorporate 2D priors extracted from existing 2D models and apply 3D motion regularization. This representation allows us to efficiently perform various downstream applications. The pipeline of our method is depicted in Fig. 2. In the following, we elaborate on the video Gaussian representation in Sec.4.1. Then, we discuss the learning objectives and optimization details in Sec.4.2 and Sec. 4.3, respectively. ", "page_idx": 3}, {"type": "text", "text": "4.1 Video Gaussian Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Camera Coordinate Space Instead of utilizing an absolute 3D world coordinate system, we opt for the orthographic camera coordinate system to model a video\u2019s 3D structure, as demonstrated in Omnimotion [48]. In this space, the video\u2019s width, height, and depth correspond to the $X,Y$ , and $Z$ axes, respectively. This enables us to circumvent the challenges associated with estimating camera poses or disentangling camera motion from scene dynamics, which can be not only time-consuming [40, 41] but also prone to failure in casually captured monocular videos with dynamic objects [34, 59]. By modeling the scene as dynamic 3D Gaussians in the camera coordinate space, we intertwine camera motion with object motion and treat them as the same type of motion, eliminating the need for camera calibration. During the rendering process, the 3D Gaussians in the camera coordinate space are rasterized into images from an identity pose camera. This approach simplifies the representation of dynamics and avoids the challenges of estimating camera pose from monocular casual videos. ", "page_idx": 3}, {"type": "text", "text": "Video Gaussians Given a video $\\mathcal{V}\\,=\\,\\{I_{1},I_{2},\\ldots,I_{n}\\}$ consisting of $n$ frames, our video Gaussian representation transforms it into a set of dynamic 3D Gaussians, parameterized as $\\textit{g}=$ $\\left\\{G_{1},\\bar{G_{2}},\\dots,G_{m}\\right\\}$ , to simultaneously represent the appearance and motion dynamics of the video. Each Gaussian is characterized by its position $\\mu$ , rotation quaternion $q$ , scale $s$ , spherical harmonics $(S\\mathcal{H})$ coefficients of appearance $s h$ , and opacity $\\alpha$ . In addition to these fundamental Gaussian properties for appearance, dynamic attributes $p$ , segmentation labels $m$ , and image features $f$ from any 2D base models (e.g., DINOv2 [31] and SAM [17]) can also be associated with 3D Gaussians to depict the video\u2019s scene content. Consequently, a Gaussian can be expressed as $G=(\\mu,q,s,\\alpha,s h,p,m,f)$ . To learn these properties from a video, we enhance the differentiable 3D Gaussian renderer to render additional attributes beyond simple color, which we denote as $\\mathcal{R}(\\mu,q,s,\\alpha,x)$ , where $x$ represents the specific attribute to be rendered. The rendering function $\\mathcal{R}$ follows the same procedure as color rendering in the original Gaussian Splatting method [16]. ", "page_idx": 3}, {"type": "text", "text": "Gaussian Dynamics When parameterizing motion, there is a trade-off between incorporating more regularization from motion priors and achieving high fitting capability [46]. In line with recent popular methods [21, 18], we employ a flexible set of hybrid bases comprising polynomials [22] and Fourier series [1] to model smooth 3D trajectories. Specifically, we assign learnable polynomial and Fourier coefficients to each Gaussian, denoted as $p=\\{p_{p}^{n}\\}\\cup\\dot{\\{p_{\\mathrm{sin}}^{l},p_{\\mathrm{cos}}^{l}\\}}$ , respectively. Here, $n$ and $l$ represent the order of coefficients. The position of a Gaussian at time $t$ can then be determined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu(t)=\\mu_{0}+\\sum_{n=0}^{N}p_{p}^{n}t^{n}+\\sum_{l=0}^{L}(p_{\\mathrm{sin}}^{l}\\cos(l t)+p_{\\mathrm{cos}}^{l}\\sin(l t)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Polynomial bases $\\{t^{n}\\}$ are effective in modeling overall trends and local non-periodic variations in motion trajectories and are widely used in curve representation, such as in Bezier and B-spline curves [32, 7]. Fourier bases $\\{\\cos(l t),\\sin(l t)\\}$ offer a frequency domain parameterization of curves, making them suitable for ftiting smooth movements [1], and excel in capturing periodic motion components. The combination of these two bases leverages the strengths of both, providing comprehensive modeling, enhanced flexibility and accuracy, reduced overftiting, and robustness to noise. This equips Gaussians with the adaptability to fit various types of trajectories by adjusting the corresponding learnable coefficients. It is important to note that for each Gaussian, the associated parameters $p=\\{p_{p}^{n}\\}\\cup\\{p_{\\mathrm{sin}}^{l},p_{\\mathrm{cos}}^{l}\\}$ are learned from the video by optimizing the learning objective as described in Sec. 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.2 2D Monocular Priors and 3D Motion Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Learning video Gaussians in the camera coordinate space to achieve consistency with real-world content using photometric loss is challenging and often ill-posed. There are multiple solutions for video Gaussians to fit the observed 2D projections. For instance, relative depth orders among scene objects can be ambiguous without occlusion cues. Moreover, different Gaussians may sequentially represent the same object, and their motion may not precisely match the object\u2019s actual motion. Therefore, regularization is required during the training process. ", "page_idx": 4}, {"type": "text", "text": "Thanks to advancements in 2D visual understanding methods, monocular 2D priors such as optical flow [44, 11] and depth estimation [54, 15, 53] are now accessible. Although not perfect, these priors can provide crucial cues to regularize learning. To stabilize our method\u2019s training and ensure a real-world consistent solution, we supervise the video Gaussians using priors from the estimated flow obtained from RAFT [44] and the estimated depth derived from Marigold [15]. ", "page_idx": 4}, {"type": "text", "text": "Flow Distillation Optical flow represents the 2D projection of 3D motion. Flow distillation serves to regularize the 2D projections of 3D Gaussian motions. To guarantee that the motion of video Gaussians aligns with the estimated optical flow, we project the 3D motion of Gaussians $(\\mu(t_{2})-$ $\\mu(t_{1}))$ between frames $t_{1}$ and $t_{2}$ onto the 2D image plane and regularize it using the estimated optical flow: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{flow}}=\\mathbb{E}_{(t_{1},t_{2})}\\left(\\|\\mathcal{R}(\\mu(t_{1}),q,s,\\alpha,\\pi(\\mu(t_{2}))-\\pi(\\mu(t_{1})))-\\|\\mathrm{ow}_{t_{1}\\to t_{2}}\\|_{1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\pi$ denotes the projection function that maps camera coordinates to image coordinates after projection, and $\\mathrm{flow}_{t_{1}\\rightarrow t_{2}}$ represents the optical flow estimated by RAFT [44] from $t_{1}$ to $t_{2}$ . This prior aids video Gaussians in learning the scene flow by ensuring that the 2D projection of their 3D motion on the XY-plane is consistent with the optical flow instead of relying on relative depth changes along the $Z_{\\cdot}$ -axis to fit frame colors. ", "page_idx": 4}, {"type": "text", "text": "Depth Distillation Monocular depth estimation provides the per-frame depth of a video. Although these estimates may be inconsistent across long-range frames, they offer valuable cues for regularizing the scene geometry. As a result, we utilize depth maps estimated by Marigold [15] to ensure a reasonable geometry for our video Gaussians. We employ the scale- and shift-trimmed loss proposed in MiDaS [37]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{depth}}=\\mathbb{E}_{t}\\left(\\Vert\\tau(D^{t})-\\tau(\\hat{D^{t}})\\Vert^{2}\\right),\\tau(D^{t})=(D^{t}-t(D^{t}))/\\Vert D^{t}-t(D^{t})\\vert,t(D^{t})=\\mathrm{median}(D^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D^{t}$ is the rendered depth of 3D Gaussians at time $t$ , and $\\hat{D}^{t}$ is the corresponding predicted depth. It is worth noting that, thanks to our 3D representation, our approach can, in turn, refine the inconsistent monocular depth estimations and yield consistent depth predictions for a video. ", "page_idx": 4}, {"type": "text", "text": "In sum, flow distillation regularizes the projected 3D Gaussian motion on the 2D image plane, corresponding to the X-Y axes in the camera coordinate space. Meanwhile, depth distillation regularizes the relative video Gaussian positions corresponding to the ${\\cal Z}_{}$ -axis in the camera coordinate space. Together, they offer comprehensive 3D supervision and complement each other, effectively regularizing the learning of 3D motion for video Gaussians. ", "page_idx": 4}, {"type": "text", "text": "3D Motion Regularization In addition to depth and flow distillation, we employ local rigidity regularization to prevent Gaussians from overftiting the rendering targets through non-rigid motions [10, 26]. This approach encourages the 3D motion of individual Gaussians to be as locally rigid as possible [42]. As a result, Gaussians form locally rigid structures, aligning with real-world dynamics. To constrain the local rigidity of a Gaussian $G_{i}$ from time $t_{1}$ to $t_{2}$ , we first identify the $K$ nearest neighboring Gaussians $G_{k}(k\\in\\mathcal N_{i})$ using its 3D position at $t_{1}$ . Then, we apply the rigid loss to ensure that the edges between them $(\\mu_{i}(t_{1})-\\mu_{k}(t_{1}))$ adhere to a rigid transformation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{arap}}=\\mathbb{E}_{(i,t_{1},t_{2})}\\left(\\sum_{k\\in\\mathcal{N}_{i}}||(\\mu_{i}(t_{1})-\\mu_{k}(t_{1}))-\\hat{R}_{i}(\\mu_{i}(t_{2})-\\mu_{k}(t_{2}))||^{2}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $R$ is the estimated rigid rotation transformation given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{R}_{i}=\\mathop{\\mathrm{arg\\,min}}_{R\\in{\\bf S O}(3)}\\sum_{k\\in\\mathcal{N}_{i}}\\|\\mu_{i}(t_{1})-\\mu_{k}(t_{1}))-R(\\mu_{i}(t_{2})-\\mu_{k}(t_{2}))\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.3 Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In addition to 2D priors and 3D regularization for learning 3D motion and geometry, we also incorporate a color rendering loss for appearance learning. Furthermore, we introduce an optional mask loss to facilitate the separation of background and foreground, which is particularly useful for editing applications. ", "page_idx": 5}, {"type": "text", "text": "Color Rendering Loss Video Gaussian representation also learns to fit the color of video frames $\\{I_{g t}^{t}\\}$ as in novel view synthesis methods [16, 29] with the rendering loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{render}}=\\mathbb{E}_{t}\\left(||\\mathcal{R}(\\mu(t),q,s,\\alpha,S\\mathcal{H}(s h,v))-I_{g t}^{t}||\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Mask Loss Segmentation labels serve as a crucial attribute for pixels, enabling the identification of groups of pixels belonging to foreground objects. In our experiments, we separate pixels into foreground and background components by segmenting each frame and extracting the foreground mask ${\\mathcal{M}}^{t}$ . This mask is subsequently lifted to Gaussian, where it is associated with a learnable label attribute $m\\in\\{0,1\\}$ . The label attributes of Gaussians are supervised by the image segmentation results: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{label}}=\\mathbb{E}_{t}\\left(||\\mathcal{R}(\\mu(t),q,s,\\alpha,m)-\\mathcal{M}^{t}||_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With the segmentation label, we can divide Gaussians into different parts and constrain their motion respectively, as shown in Eq. 5. Our approach can also manipulate (remove/duplicate) and edit specific objects in a video, as shown in $\\mathrm{Fig}\\ 7$ . ", "page_idx": 5}, {"type": "text", "text": "Total Learning Objective The total learning objective is the weighted sum of all the losses: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{\\mathrm{render}}\\mathcal{L}_{\\mathrm{render}}+\\lambda_{\\mathrm{depth}}\\mathcal{L}_{\\mathrm{depth}}+\\lambda_{\\mathrm{flow}}\\mathcal{L}_{\\mathrm{flow}}+\\lambda_{\\mathrm{arap}}\\mathcal{L}_{\\mathrm{arap}}+\\lambda_{\\mathrm{label}}\\mathcal{L}_{\\mathrm{label}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Adaptive Density Control We initialize the video Gaussians by uniformly sampling points in the camera coordinate space of the first frame, and apply a similar density control strategy as in vanilla Gaussian Splatting [16]. For more details, please refer to Sec. A.1. ", "page_idx": 5}, {"type": "text", "text": "5 Video Processing Applications ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With our video Gaussian representation, we can perform various video processing tasks, including 1) dense tracking, 2) consistent depth/feature prediction, 3) geometry editing, 4) appearance editing, 5) frame interpolation, 6) novel view synthesis, and 7) stereoscopic video creation. In this section, we detail these applications, highlighting the versatility of video Gaussians. ", "page_idx": 5}, {"type": "text", "text": "Dense Tracking Since the scene motion is captured by the dynamics of video Gaussians, we can project these dynamics onto the image plane as UV flow and rasterize the attributes as flow maps. This method handles both short and long-frame gaps effectively. The pixel flow map $d U_{t_{1}\\to t_{2}}$ from $t_{1}$ to $t_{2}$ is calculated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nd U_{t_{1}\\to t_{2}}=\\mathcal{R}(\\mu(t_{1}),q,s,\\alpha,\\pi(\\mu(t_{2}))-\\pi(\\mu(t_{1}))).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The rendered dense flow map provides pixel correspondences, facilitating tracking across frames. ", "page_idx": 5}, {"type": "text", "text": "Consistent Depth/Feature Prediction Video Gaussians, supervised by monocular depth priors for each frame, conform to a reasonable geometry layout, providing consistent depth predictions across frames. Similarly, other image features can be distilled into video Gaussians; unifying per-frame features into a consistent 3D form. To distill image features (e.g., SAM [17] or DINOv2 [31]), we associate each video Gaussian with a feature attribute $f$ and rasterize them to match the feature map $\\{\\mathcal{F}_{g t}^{t}\\}$ from 2D models: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{feature}}=\\mathbb{E}_{t}\\left(||\\mathcal{R}(\\mu(t),q,s,\\alpha,f)-\\mathcal{F}_{g t}^{t}||_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Optimizing video Gaussians with $\\mathcal{L}_{\\mathrm{feature}}$ unifies frame-wise 2D features in a 3D form, enabling the rendering of view-consistent feature maps $\\{\\mathcal{F}_{t}\\}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}=\\mathcal{R}(\\mu(t),q,s,\\alpha,f).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Consistent feature prediction is crucial for applications like video segmentation and re-identification. ", "page_idx": 5}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/52b491a5a952f0f1a8a0114bbc012f73fdd1b8972efe4a8b20caf1f56e03dadf.jpg", "img_caption": ["Figure 3: Qualitative comparison of video reconstruction using our method and SOTA methods. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/3328cfe892acc18c4a8992b9321f33bef578ce271893914c552caa831d069c06.jpg", "img_caption": ["Figure 4: Dense tracking results on diverse complex motion patterns. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Geometry Editing In the unified 3D space, geometry editing is straightforward. By distilling segmentation labels into video Gaussians, we can select Gaussians of the target identity and transform their positions $\\mu$ , quaternions $q$ , and scales $s$ for translation, resizing, and rotation. Adjusting their opacities changes the transparency of the edited objects. It also facilitates easy object removal within a video and supports object copying both between and within videos. ", "page_idx": 6}, {"type": "text", "text": "Appearance Editing Appearance editing with video Gaussians can also be easily achieved. Users can select a specific frame $t$ and perform painting, recoloring, or stylization. We fix all attributes except the $S\\mathcal{H}$ coefficients representing Gaussian appearance and optimize them to fit the edited image $I_{\\mathrm{edit}}^{t}$ using: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{edit}}=||\\mathcal{R}(\\mu(t),q,s,\\alpha,S\\mathcal{H}(s h,v))-I_{\\mathrm{edit}}^{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The edited results can propagate throughout the video, maintaining temporal consistency. ", "page_idx": 6}, {"type": "text", "text": "Frame Interpolation The learned smooth trajectories of video Gaussians enable interpolation of scene dynamics at any up-sampling rate. Interpolated Gaussians\u2019 dynamic attributes can render interpolated video frames. By re-mapping the timestep values $\\{t\\}\\to\\{t^{\\prime}\\}$ with an arbitrary continuous function, we can freely adjust the video playback speed. ", "page_idx": 6}, {"type": "text", "text": "Novel View Synthesis Applying a global rigid transformation $\\tau\\ \\in\\ S\\mathcal{E}(3)$ to video Gaussians allows for camera position adjustments. The rendering results of transformed Gaussians $\\mathcal{R}(\\mathcal{T}(\\mu(t)),\\mathcal{T}(q),s,\\alpha,S\\dot{\\mathcal{H}}(s h,v))$ provide synthesized views from different perspectives. ", "page_idx": 6}, {"type": "text", "text": "Stereoscopic Video Creation Similar to the novel view synthesis application, we can achieve stereoscopic frames by slightly translating video Gaussians horizontally by a fixed distance, representing the interocular distance. This application is crucial in filmmaking and gaming. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Evaluation We conducted experiments on the DAVIS dataset [36] as well as some videos used by Omnimotion [48] and CoDeF [33]. Our approach is evaluated based on two criteria: 1) reconstructed video quality and 2) downstream video processing tasks. In addition to general video representation methods Deformable Sprites [58], Omnimotion [48] and CoDeF [33], we also compare with dynamic ", "page_idx": 6}, {"type": "table", "img_path": "bzuQtVDxv0/tmp/b1adf53ae3022a13d89630513e4c51cd5872c19cbb3b08900b404070dd61e10b.jpg", "table_caption": ["Table 1: Comparison with existing methods on Tap-Vid benchmark (DAVIS) "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/56489baa5644b4a511f62287af2e309eacc1a270ee2ef22f319f6525a9f11428.jpg", "img_caption": ["Figure 5: Qualitative comparison of video depth and features generated by our method and SOTA single-frame estimation methods. Our method yields more consistent estimations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "NeRF/3DGS methods, namely 4DGS [51] and RoDynRF [23]. Note that for 4DGS, we estimate camera poses using monocular depth estimation method Unidepth [35] and DROID-SLAM [45]. Despite these efforts, the performance remains unsatisfactory, further highlighting the challenges of accurate camera pose estimation in causal dynamic videos. In contrast, our approach demonstrates the capability to handle more complex motions and achieves significantly higher reconstruction quality. For downstream tasks, our method also shows comparable performance to those specifically designed for these tasks. ", "page_idx": 7}, {"type": "text", "text": "Video Reconstruction To demonstrate our method\u2019s fitting ability for casual videos, we compare it with Omnimotion [48] and CoDeF [33]. Omnimotion tends to render blurred results due to the smooth bias of the MLP when modeling the canonical space, while CoDeF struggles with complex motions due to the limited representation ability of the 2D canonical image. We report the rendering quality metrics and visualizations on the Tap-Vid DAVIS dataset [36] in Table 1 and Figure 3. More comparison with RyDynRF [23] and 4DGS [51] are provided in the supplementary materials. ", "page_idx": 7}, {"type": "text", "text": "6.1 Video Processing Applications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dense Tracking. Our approach enables dense tracking by projecting the dynamics of Gaussians onto 2D image planes to obtain correspondences. Tracking results are visualized in Fig. 4 and evaluated in Table 1. Despite Omnimotion\u2019s specialization in tracking, our approach supports a wider array of video processing tasks with higher computational and training efficiencies. It achieves comparable results with better reconstruction quality using fewer resources. Tracking performance comparisons with similar-cost methods (CoDeF / 4DGS) are shown in the supplementary materials 11, highlighting our superior outcomes. ", "page_idx": 7}, {"type": "text", "text": "Consistent Depth / Feature Generation. We present the results of consistent video depth and features (using SAM [17]) in Fig. 5, compared to per-frame prediction. Due to the unified 3D representation of video frames, the predicted depth and features exhibit significantly better consistency than those obtained from monocular predictions. We also evaluate the effectiveness of consistent SAM feature in the segmentation task in the supplementary materials 15. We recommend that readers watch the supplemental videos for better illustrations. ", "page_idx": 7}, {"type": "text", "text": "Geometry Editing By manipulating the Gaussians associated with specific labels, we can achieve geometric editing of target identities, as demonstrated in Fig. 7. By deleting foreground Gaussians, we can remove foreground elements and render a clean background. Our approach also supports geometric edits such as duplicating, resizing, and translating. Additionally, the motion of these elements can be adjusted by setting different motion attributes. ", "page_idx": 7}, {"type": "text", "text": "Appearance Editing Users can edit the appearance in a specific frame by drawing, stylizing, or recoloring, and these edits will be propagated across the entire video with cross-frame consistency. In ", "page_idx": 7}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/601198922861c48836d5727e7ba801c04fea000847aa3f3ff74db283f1d36255.jpg", "img_caption": ["Fig.6, we demonstrate appearance editing using ControlNet[60]. Appearance editing is user-friendly in our representation, as it only requires single-frame editing. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/559ebc666928da93be7129ebfd51a6fb8a38f30cf0936955b42b4885f6fb2275.jpg", "img_caption": ["Figure 7: Geometry editing results including object deleting, resizing, copying, and translating. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Novel View Synthesis & Stereoscopic Video Creation Benefitting from depth regularization, the 3D Gaussians maintain a meaningful 3D structure, even from a monocular video. This facilitates novel view synthesis tasks, with examples provided in the supplemental video. Stereoscopic videos can also be produced, as shown in Fig. 8. ", "page_idx": 8}, {"type": "text", "text": "6.2 Ablation Study. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform ablation studies to validate the importance of the proposed modules, including camera model (perspective/orthographic), flow loss, depth loss (L2/scale-shift invariant). The results are reported in Table 2. ", "page_idx": 8}, {"type": "table", "img_path": "bzuQtVDxv0/tmp/a1da091f54f62b56dedabf6ca0deb1007a633e2c9a6e5363c790d44bf42eb994.jpg", "table_caption": ["Table 2: Ablation of each module in our framework. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Using a predefined pinhole camera intrinsic led to unstable optimization, resulting in artifacts in both geometry and appearance. This instability likely stems from the rasterization process, where the denominators of Gaussians\u2019 screen coordinates UV include depth, complicating the gradients. Replacing the shift- and scale-invariant depth loss with absolute L2 loss degrades performance, as monocular depth cues are ambiguous regarding scale and shift. ", "page_idx": 8}, {"type": "text", "text": "Moreover, the depth prior is crucial for maintaining the 3D structure of Gaussians. Without the depth prior, Gaussians collapse into a flat 2D plane, hindering novel view synthesis and resembling 2D-layer methods. ", "page_idx": 9}, {"type": "text", "text": "We also demonstrate the significance of motion regularization (rigid loss) and the selection of motion coefficients n and l, which effectively suppress unorganized Gaussian motion. Please refer to Sec. A.3 for more details. ", "page_idx": 9}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/935b728ff6c0c1fa73ce592be4a0865e53550f559e129dbf41117331adf1bbe0.jpg", "img_caption": ["Figure 8: Stereo view synthesis. One original frame is visualized in the first column for comparison. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6.3 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although achieving satisfying performance, there are still some limitations to be enhanced. First, our approach suffers from significant changes in the scene, since large deformation is hard to optimize. Initializing the scene with dynamic point clouds might alleviate this problem. In addition, our approach still relies on existing correspondence estimation methods (e.g., RAFT), which might fail when processing rapid and highly non-rigid motion. Extending this representation to more general scenarios is still worth exploring. We have illustrated two scenarios in Fig 9. ", "page_idx": 9}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/64801f73461ae5aa4346f96b6f250a6e7648fbd15bc84b041ea3cc2d36d12fd6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 9: Our method may underperform in two scenarios. (1) Without camera estimation, it has to learn large camera rotations as scene motions, causing distant background blur (left). (2) It fails to track fast-moving transients, as photometric loss is insufficient for motion fitting (right). ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced a novel explicit video Gaussian representation (VGR) based on 3D Gaussians to address the challenges of video processing. By modeling video appearance in a canonical 3D space and associating each Gaussian with time-dependent 3D motion attributes, our approach effectively handles complex motions and occlusions. Leveraging recent advancements in monocular priors, such as optical flow and depth, we lift 2D information into a compact 3D representation, facilitating a wide range of video-processing tasks. Our VGR method demonstrates efficacy in dense tracking, improving monocular 2D priors, video editing, interpolation, novel view synthesis, and stereoscopic video creation, providing a robust and versatile framework for sophisticated video processing applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work has been supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), and RGC Matching Fund Scheme (RMGS). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust. We would like to thank Ziyi Yang for the insightful discussion and generous help. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade. Trajectory space: A dual representation for nonrigid structure from motion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(7):1442\u20131456, 2010.   \n[2] Alan C Bovik. Handbook of image and video processing. Academic press, 2010.   \n[3] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. 2023.   \n[4] Cheng-Hung Chan, Cheng-Yang Yuan, Cheng Sun, and Hwann-Tzong Chen. Hashing neural video decomposition with multiplicative residuals in space-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7743\u20137753, 2023.   \n[5] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: A benchmark for tracking any point in a video. Advances in Neural Information Processing Systems, 35:13610\u201313626, 2022.   \n[6] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10061\u201310072, 2023.   \n[7] William J Gordon and Richard F Riesenfeld. B-spline curves and surfaces. In Computer aided geometric design, pages 95\u2013126. Elsevier, 1974.   \n[8] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 59\u201375. Springer, 2022.   \n[9] Jiahui Huang, Leonid Sigal, Kwang Moo Yi, Oliver Wang, and Joon-Young Lee. Inve: Interactive neural video editing. arXiv preprint arXiv:2307.07663, 2023.   \n[10] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparsecontrolled gaussian splatting for editable dynamic scenes. arXiv preprint arXiv:2312.14937, 2023.   \n[11] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In European conference on computer vision, pages 668\u2013685. Springer, 2022.   \n[12] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. Advances in neural information processing systems, 33:19545\u201319560, 2020.   \n[13] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023.   \n[14] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):1\u201312, 2021.   \n[15] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023.   \n[18] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting. arXiv preprint arXiv:2312.00112, 2023.   \n[19] Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, and Dong Xu. A video is worth 256 bases: Spatial-temporal expectation-maximization inversion for zero-shot video editing. arXiv preprint arXiv:2312.05856, 2023.   \n[20] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. arXiv preprint arXiv:2312.10656, 2023.   \n[21] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4273\u20134284, 2023.   \n[22] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. arXiv:2312.03431, 2023.   \n[23] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[24] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. arXiv preprint arXiv:2009.07833, 2020.   \n[25] Tao Lu, Yu Mulin, Xu Linning, Xiangli Yuanbo, Wang Limin, Lin Dahua, and Dai. Bo. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering. Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[26] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024.   \n[27] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 39(4):71\u20131, 2020.   \n[28] Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu, Wenliang Zhao, Lior Shapira, and Xiaohui Xie. Maskint: Video editing via interpolative non-autoregressive masked transformers. arXiv preprint arXiv:2312.12468, 2023.   \n[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[30] Michal Neoral, Jon\u00e1\u0161 \u0160er\\`ych, and Ji\u02c7r\u00ed Matas. Mft: Long-term tracking of every pixel. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6837\u20136847, 2024.   \n[31] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023.   \n[32] Halil Oru\u00e7 and George M Phillips. q-bernstein polynomials and b\u00e9zier curves. Journal of Computational and Applied Mathematics, 151(1):1\u201312, 2003.   \n[33] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. arXiv preprint arXiv:2308.07926, 2023.   \n[34] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40(6):1\u201312, 2021.   \n[35] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[36] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.   \n[37] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022.   \n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[39] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. International journal of computer vision, 80:72\u201391, 2008.   \n[40] Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[41] Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016.   \n[42] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, pages 109\u2013116. Citeseer, 2007.   \n[43] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic free-viewpoint videos. arXiv preprint arXiv:2403.01444, 2024.   \n[44] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020.   \n[45] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In Neural Information Processing Systems, 2021.   \n[46] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields for dynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021.   \n[47] John YA Wang and Edward H Adelson. Representing moving images with layers. IEEE transactions on image processing, 3(5):625\u2013638, 1994.   \n[48] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19795\u201319806, 2023.   \n[49] Wen Wang, Kecheng Zheng, Qiuyu Wang, Hao Chen, Zifan Shi, Ceyuan Yang, Yujun Shen, and Chunhua Shen. Gendef: Learning generative deformation field for video generation. arXiv preprint arXiv:2312.04561, 2023.   \n[50] Yao Wang, J\u00f6rn Ostermann, and Ya-Qin Zhang. Video processing and communications, volume 1. Prentice hall Upper Saddle River, NJ, 2002.   \n[51] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023.   \n[52] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. arXiv preprint arXiv:2404.04319, 2024.   \n[53] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024.   \n[54] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. arXiv preprint arXiv:2401.10891, 2024.   \n[55] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv 2310.10642, 2023.   \n[56] Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and Xiaogang Jin. Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting. arXiv preprint arXiv:2402.15870, 2024.   \n[57] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023.   \n[58] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised video decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2657\u20132666, 2022.   \n[59] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. Ds-slam: A semantic visual slam towards dynamic environments. In 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 1168\u20131174. IEEE, 2018.   \n[60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[61] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023.   \n[62] Youyuan Zhang, Xuan Ju, and James J Clark. Fastvideoedit: Leveraging consistency models for efficient text-to-video editing. arXiv preprint arXiv:2403.06269, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / Supplemental Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Typically, we use a video clip of about 50-100 frames and train the system iteratively for 20,000 steps. The training duration is approximately 15-20 minutes on an NVIDIA 3090 GPU. The Gaussians are initialized as 10,0000 points randomly sampled in a $[-1,1]\\times[-1,1]\\times[0,1]$ box. We use an orthographic camera for rendering for simplicity, which is fixed at the origin. We also modify the rasterization pipeline of 3DGS to support the orthographic projection by replacing the $J$ in EWA projection with $\\left[{\\begin{array}{c c c}{W/2}&{0}&{0}\\\\ {0}&{H/2}&{0}\\end{array}}\\right]\\}$ where $W$ and $H$ are the resolution of the image. For each attribute attached to Gaussians, we set different learning parameters and annealing strategies, list in Tab 3. Note that the dynamics of Gaussians\u2019 rotation is also modelled in the same way as position. During training, the number of video Gaussians is adaptively adjusted as in vanilla Gaussian Splatting [16]. Every 100 steps, Gaussians with an accumulated gradient scale of positions above a threshold will be densified. Based on their projected size, they will be either split or cloned. Concurrently, Gaussians with opacities below a threshold will be pruned. To avoid floaters, the opacity of Gaussians is reset to 0.01 every 3000 steps. After optimization, there are around $10^{5}-10^{6}$ 3D Gaussian for a video containing $\\mathrm{i\\bar{0}^{7}-10^{8}}$ pixels (resolution $\\times$ frame number). ", "page_idx": 14}, {"type": "text", "text": "The loss weights for render, depth, flow, motion regularization, and label are set to $\\lambda_{r e n d e r}=5.0$ , $\\lambda_{d e p t h}=1.0$ , $\\lambda_{f l o w}=2.0$ , $\\lambda_{a r a p}=0.1$ , and $\\lambda_{l a b e l}=1.0$ . ", "page_idx": 14}, {"type": "table", "img_path": "bzuQtVDxv0/tmp/7411868bde025ad5ece760aa6ec09066b5fe684077cca66ce08db13f60612daf.jpg", "table_caption": ["Table 3: Gaussian attributes table "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 More Visual Comparison ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have visualized the comparison with 4DGS [51] and RoDynRF [23] in Fig 10. We also visualized the tracking comparison in Fig 11. Note that due to the inaccurate camera pose for the wild scene, the performance of 4DGS is very limited. ", "page_idx": 14}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/ba7e4aadfb178629811df471743764caad855bb6501f5b5730d0cb51912381b3.jpg", "img_caption": ["Figure 10: Video reconstruction compared with RoDynRF [23] and 4DGS [51]. Our method has higher PSNR and better visual quality. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/36207232e2dcad5337809392e17113681603b9d2724b81efa4c98423beecfec7.jpg", "img_caption": ["CoDeF [33] ", "Figure 11: Tracking results visualization shows our method outperforms CoDeF [? ] and 4DGS [? ], especially in handling large-scale view changes and scene motions. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Depth Regularization. Without the depth prior, Gaussians collapse into a 2D plane. Although overftiting ability remains largely unaffected, the 3D structure is lost, and novel view synthesis is no longer possible, as shown in the right part of Fig. 12. Our approach then resembles 2D layer-based methods. ", "page_idx": 15}, {"type": "text", "text": "Rigid Loss. Without the rigid motion constraint, undesirable floaters appear, degrading rendering quality and reducing reconstruction PSNR by 1.51 dB, as illustrated in the left part of Fig. 12. ", "page_idx": 15}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/0129deddf9c119ca2441b68f404424a5ec12ba3f68dfe388757cdf2f5bb89253.jpg", "img_caption": ["Figure 12: The depth prior (w/ depth) ensures video Gaussians conform to a realistic layout, while rigidity regularization (w/ rigid) eliminates floaters. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We also conducted ablation studies on the selection of camera models and depth loss formats as shown in Fig 13. ", "page_idx": 15}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/e187b726ba86b90d5cc3fe567f8ca9a4408a443d6fa1d1946e6e7550eddfa571.jpg", "img_caption": ["Figure 13: Ablation for camera model / L2 depth loss "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We also evaluated the effect of different motion representation parameters $[\\mathfrak{n}/\\mathfrak{l}$ in Eq 2), reported in Table 4 and Fig. 14. ", "page_idx": 15}, {"type": "table", "img_path": "bzuQtVDxv0/tmp/59da9123ed3996215f9ce5c0e6a0b7b4a8598284df1427bcc5f3080670e726a0.jpg", "table_caption": ["Table 4: Ablation of n/l choice. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/7de5369369bef69686e22b1fae02ff68be29ae3242c9825e62d677bd2987a61d.jpg", "img_caption": ["Figure 14: Ablation study on hyperparameters $n$ and $l$ reveals optimal quality at $n=l=8$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.4 Consistent SAM Feature Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "By adding SAM features to Gaussian points, our method can obtain consistent SAM features. Compared with SAM extracted from per single frame, this feature has better consistency for moving objects and removes the requirements of passing through the SAM image encoder, hence obtaining more accurate video segmentation results at a faster speed, as reported in the Table 5 and Fig. 15. ", "page_idx": 16}, {"type": "table", "img_path": "bzuQtVDxv0/tmp/310a602a79d57a1f93975fb0a63a766382591d009bc56b43854bd21cd46270b2.jpg", "table_caption": ["Table 5: Comparision of per-frame SAM feature and ours. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.5 Video Interpolation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Thanks to the continuous parameterization of dynamics, our approach can interpolate video frames over time. We present the interpolation results in Fig. 16. Our method supports any video interpolation using an arbitrary continuous time re-mapping function at any frame rate. ", "page_idx": 16}, {"type": "text", "text": "A.6 Multi-object Editing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "By adding a multi-channel mask attribute to each Gaussian point, our method can achieve separate editing of multiple objects. We visualize an example of multi-object geometry / appearance editing in Fig. 17. ", "page_idx": 16}, {"type": "text", "text": "A.7 Expanded Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Video Editing Decomposing videos into layered representations facilitates advanced video editing techniques. Kasten et al.[14] introduced layered neural atlases that decompose an image into textured layers and learn a corresponding deformation field, thereby enabling efficient video propagation and editing. Subsequent advancements have introduced more sophisticated models. Ye et al.[58] developed deformable sprites, segregating videos into distinct motion groups, each driven by an MLP-based representation. Huang et al.[9] proposed employing a bi-directional warping field to support extensive video tracking and editing capabilities over longer durations. Recent innovations have also focused on enhancing the rendering of lighting and color details. Chan et al.[4] extended this approach by incorporating additional layers and introducing residual color maps, enhancing the representation of illumination effects within the video. The most current development in this area is CoDeF [33], which leverages a multi-resolution hash grid and a shallow MLP to model frame-by-frame deformations relative to a canonical image. This approach allows for editing in the canonical space, with changes effectively propagated across the entire video. GenDeF [49] uses a similar representation to generate controllable videos. ", "page_idx": 16}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/45392f43ea54fe47f9fdd386759ced92c18e57966ff8d85f05fbbba103c9343c.jpg", "img_caption": ["Figure 15: Segmentation comparison reveals that our lifted SAM features outperform per-frame SAM segmentation, delivering higher resolution feature maps and superior IoU. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/5824c2c39610c7cfdfb6b23ec00c9f8dba9eca017657f4f57561e45f9bc1eb49.jpg", "img_caption": ["Figure 16: Video interpolation results. Please refer to the supplementary video for better visualization. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Several studies have exploited the generative capabilities of latent diffusion models[38] for datadriven video editing. ControlVideo[61] adopts the methodology of ControlNet [60], integrating control signals into the network during the video reconstruction process to guide editing. Employing a related technique to manage control signals, MaskINT[28] utilizes frame interpolation to generate edited videos from specifically edited keyframes. In contrast, VidToMe[20] implements a token merging approach to incorporate control signals into the editing process. Additionally, certain research efforts [19, 62] have explored using inversion solutions to achieve video editing. ", "page_idx": 17}, {"type": "text", "text": "Video Tracking. Video tracking is essential for capturing the physical motion of each point within a video sequence[39]. PIPs[8] track motion within fixed-size windows and include an occlusion branch, though they lack the ability to re-detect targets following prolonged occlusions. Building on the temporal processing concepts from PIPs, TAPIR [6] introduces TAP-Net[5], which precisely locates per-frame points. CoTracker[13] advances this by tracking individual query points using a sliding-window transformer approach. OminiMotion[48] pioneers the use of neural radiance fields[29] to model scene flow in NDC space. Its bijection network, which represents scene flow, is optimized for photometric consistency across frames, thereby enabling dense tracking. MFT[30] employs a sequential and dense point tracking methodology using optical flow fields computed across varying time spans. SpatialTracker[52] transforms each frame into a triplane and estimates trajectories by iteratively predicting movements with a transformer, facilitating 2D tracking within a 3D space. While state-of-the-art optical flow methods such as RAFT[44] and FlowFormer[11] provide accurate flow estimations for consecutive frames, they struggle with maintaining long-term frame correspondences. ", "page_idx": 17}, {"type": "text", "text": "Gaussian Splatting Gaussian Splatting [16] has emerged as a potent method for enhancing rendering quality and speed in radiance fields. Following that, Lu et al. [25] further organize the Gaussians distribution by introducing anchor points, and Yang et al. [56] enrich its ftiting ability in the specular setting with anisotropic sphere gaussian. These approaches have been extended to dynamic scenes in various recent studies. Luiten et al.[26] utilize frame-by-frame training, making it well-suited for multi-view scenes. Yang et al.[57] advance this by segmenting scenes into 3D Gaussians coupled with a deformation field, particularly for monocular scenes. Building upon this work, Wu et al.[51] have replaced the traditional MLP with multi-resolution hex-planes [3] and a shallow MLP. Additionally, Yang et al.[55] integrate time as an additional dimension in their 4D Gaussian model. SC-GS[10] introduces a novel approach using sparse control points to learn a spatially compact representation of scene dynamics. 3DGStream [43] offers a high-quality free viewpoint video (FVV) stream of dynamic scenes generated in real-time, though it necessitates multi-view video streams as input. Gaussian-Flow [22] hybrid the basis of polynomial and Fourier to represent the Gaussian motion. These methods typically rely on pre-estimated camera poses. Our approach specifically targets monocular video representation, obviating the need for camera pose estimations. This facilitates more robust long-term tracking and editing capabilities in dynamic scenes. ", "page_idx": 17}, {"type": "image", "img_path": "bzuQtVDxv0/tmp/be6b411ce0c59f04f7eddc8e08061e3868a64aae1c9850a877555616cf3d8083.jpg", "img_caption": ["Figure 17: Segmentation and editing results are displayed from left to right: multi-object segmentation and video editing across two frames. The first row illustrates geometry editing where two individuals are copied and repositioned to lie on the ground and stand aside. The second shows the appearance editing with $\"2024\"$ and \"NeurIPS\" painted on two fish. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "A.8 Detailed Introduction to 3D Gaussian Splatting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Gaussian splatting [16] models 3D scenes using 3D Gaussians by learning from posed multiview images. Each Gaussian, denoted as $G$ , is defined by a central point $\\mu$ and a covariance matrix $\\Sigma$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nG(x)=\\exp{(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The covariance matrix $\\Sigma$ undergoes decomposition into $R S S^{T}R^{T}$ for efficient optimization. Here, $R$ represents a rotation matrix, parameterized by a quaternion $q$ from $\\mathbf{SO}(3)$ , and $S$ is a scaling matrix defined by a positive 3D vector $s$ . Additionally, each Gaussian is assigned an opacity value $\\alpha$ to modulate its rendering impact and is equipped with spherical harmonic (SH) coefficients $s h$ for capturing view-dependent effects. The collection of Gaussians is represented as ${\\mathcal{G}}\\,=\\,\\{G_{j}\\,:$ $\\mu_{j},q_{j},s_{j},\\alpha_{j},s h_{j}\\}$ . Rendering is achieved through the equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\nC(u)=\\sum_{i\\in N}T_{i}\\sigma_{i}S\\mathcal{H}(s h_{i},v_{i}),\\mathrm{~where~}T_{i}=\\Pi_{j=1}^{i-1}(1-\\sigma_{j}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $S\\mathcal{H}$ denotes the spherical harmonic function and $v_{i}$ the viewing direction. The value of $\\sigma_{i}$ is determined by evaluating the corresponding projection of Gaussian $G_{i}$ at pixel $u$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma_{i}=\\alpha_{i}\\exp(-\\frac{1}{2}(u-\\mu_{i}^{\\prime})^{T}\\Sigma_{i}^{\\prime}(u-\\mu_{i}^{\\prime})),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mu_{i}^{\\prime}$ and $\\Sigma_{i}^{\\prime}$ represent the projected 2D center and covariance matrix of Gaussian $G_{i}$ , respectively. By optimizing the Gaussian parameters $\\left\\{{G_{j}}:{\\mu_{j}},{q_{j}},s_{j},{\\alpha_{j}},s h_{j}\\right\\}$ and dynamically adjusting Gaussian densities, high-quality and real-time image synthesis is facilitated. However, vanilla Gaussian splatting can only be used to represent a static scene. In this paper, we integrate this representation with video by assigning additional attributes to each Gaussian, enabling more versatile video processing. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper\u2019s contributions and scope are reflected. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Discussed in Sec. 6.3 ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Full implementation details are provided in Sec. A.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Code and data are not provided for now but will be released to the public. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Full implementation details are provided in Sec. A.1. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Results in tables are averaged over 3 runs with different random seeds. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Provided in Sec. A.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The statement about the impacts of the work to the society is provided in Sec. 1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not pose such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The assets are cited and corresponding licenses are respected. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]