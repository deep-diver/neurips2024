[{"type": "text", "text": "On Tractable $\\Phi$ -Equilibria in Non-Concave Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Cai Constantinos Daskalakis Haipeng Luo Yale University MIT CSAIL University of Southern California yang.cai@yale.edu costis@csail.mit.edu haipengl@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Chen-Yu Wei Weiqiang Zheng University of Virginia Yale University chenyu.wei@virginia.edu weiqiang.zheng@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent\u2019s utility is concave in their own strategy, this is not the case when utilities are non-concave \u2013 a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents\u2019 utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of $\\Phi$ -equilibria introduced by Greenwald and Jafari [GJ03], which is guaranteed to exist for an arbitrary set of strategy modifications $\\Phi$ even in nonconcave games [SL07]. However, the tractability of $\\Phi$ -equilibria in such games remains elusive. In this paper, we initiate the study of tractable $\\Phi$ -equilibria in nonconcave games and examine several natural families of strategy modifications. We show that when $\\Phi$ is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding $\\Phi$ -equilibria. Additionally, we explore cases where $\\Phi$ is infinite but consists of local modifications, showing that Online Gradient Descent can efficiently approximate $\\Phi$ -equilibria in non-trivial regimes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Von Neumann\u2019s celebrated minimax theorem establishes the existence of Nash equilibrium in all two-player zero-sum games where the players\u2019 utilities are continuous as well as concave in their own strategy [Neu28].1 This assumption that players\u2019 utilities are concave, or quasi-concave, in their own strategies has been a cornerstone for the development of equilibrium theory in Economics, Game Theory, and a host of other theoretical and applied fields that make use of equilibrium concepts. In particular, (quasi-)concavity is key for showing the existence of many types of equilibrium, from generalizations of min-max equilibrium [Fan53; Sio58] to competitive equilibrium in exchange economies [AD54; McK54], mixed Nash equilibrium in finite normal-form games [Nas50], and, more generally, Nash equilibrium in (quasi-)concave games [Deb52; Ros65]. ", "page_idx": 0}, {"type": "text", "text": "Table 1: A comparison between different solution concepts in multi-player non-concave games. We include definitions of Nash equilibrium, mixed Nash equilibrium, (coarse) correlated equilibrium, strict local Nash equilibrium, and second-order local Nash equilibrium in Appendix B. We also give a detailed discussion on the existence and complexity of these solution concepts in Appendix B. ", "page_idx": 1}, {"type": "table", "img_path": "3CtTMF5zzM/tmp/93db63c8e6ed818e926afd0c02f9bc0f073fabdf8c83d19df6c157011280a2e7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Not only are equilibria guaranteed to exist in concave games, but it is also well-established\u2014thanks to a long line of work at the interface of game theory, learning and optimization whose origins can be traced to Dantzig\u2019s work on linear programming [Geo63], Brown and Robinson\u2019s work on fictitious play [Bro51; Rob51], Blackwell\u2019s approachability theorem [Bla56] and Hannan\u2019s consistency theory [Han57]\u2014that several solution concepts are efficiently computable both centrally and via decentralized learning dynamics. For instance, it is well-known that the learning dynamics produced when the players of a game iteratively update their strategies using no-regret learning algorithms, such as online gradient descent, is guaranteed to converge to Nash equilibrium in twoplayer zero-sum concave games, and to coarse correlated equilibrium in multi-player general-sum concave games [CL06]. The existence of such simple decentralized dynamics further justifies using these solution concepts to predict the outcome of real-life multi-agent interactions where agents deploy strategies, obtain feedback, and use that feedback to update their strategies. ", "page_idx": 1}, {"type": "text", "text": "While (quasi-)concave utilities have been instrumental in the development of equilibrium theory, as described above, they are also too restrictive an assumption. Several modern applications and outstanding challenges in Machine Learning, from training Generative Adversarial Networks (GANs) to Multi-Agent Reinforcement Learning (MARL) as well as generic multi-agent Deep Learning settings where the agents\u2019 strategies are parameterized by deep neural networks or their utilities are computed by deep neural networks, or both, give rise to games where the agents\u2019 utilities are non-concave in their own strategies. We call these games non-concave, following [Das22]. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, classical equilibrium theory quickly hits a wall in non-concave games. First, Nash equilibria are no longer guaranteed to exist. Second, while mixed Nash, correlated and coarse correlated equilibria do exist\u2014under convexity and compactness of the strategy sets [Gli52], which we have been assuming all along in our discussion so far, they have infinite support, in general [Kar14]. Finally, they are computationally intractable; so, a fortiori, they are also intractable to attain via decentralized learning dynamics. ", "page_idx": 1}, {"type": "text", "text": "In view of the importance of non-concave games in emerging ML applications and the afore-described state-of-affairs, our investigation is motivated by the following broad and largely open question: ", "page_idx": 1}, {"type": "text", "text": "Question from [Das22]: Is there a theory of non-concave games? What solution concepts are meaningful, universal, and tractable? ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study Daskalakis\u2019 question through the lens of the classical solution concept of $\\Phi$ -equilibria introduced by Greenwald and Jafari [GJ03]. This concept is guaranteed to exist for virtually any set of strategy modifications $\\Phi$ , even in non-concave games, as demonstrated by Stoltz and Lugosi [SL07].2 However, the tractability of $\\Phi$ -equilibria in such games remains elusive. In this paper, we initiate the study of tractable $\\Phi$ -equilibria in non-concave games and examine several natural families of strategy modifications. ", "page_idx": 1}, {"type": "text", "text": "Figure 1: The relationship between different solution concepts in non-concave games. An arrow from one solution concept to another means the former is contained in the latter. The dashed arrow from $\\operatorname{Conv}(\\Phi(\\delta))$ -equilibria to $\\Phi_{\\mathrm{Finite}}$ -equilibria means the former is contained in the latter when $\\Phi(\\delta)=\\Phi_{\\mathrm{Finite}}$ . ", "page_idx": 2}, {"type": "image", "img_path": "3CtTMF5zzM/tmp/a8ef745908f50df2939edad48764cc0c408016f56e3a1a4eb8ec949fe75a9f3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "$\\Phi$ -Equilibrium. The concept of $\\Phi$ -equilibrium generalizes (coarse) correlated equilibrium. A $\\Phi$ - equilibrium is a joint distribution over $\\Pi_{i=1}^{n}\\mathcal{X}_{i}$ , the Cartesian product of all players\u2019 strategy sets, and is defined in terms of a set, $\\Phi^{\\mathcal{X}_{i}}$ , of strategy modifications, for each player $i$ . The set $\\Phi^{\\mathcal{X}_{i}}$ contains functions mapping $\\mathbf{\\mathcal{X}}_{i}$ to itself. A joint distribution over strategy profiles qualifies as a $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ -equilibrium if no player $i$ can increase their expected utility by using any strategy modification function, $\\phi_{i}\\,\\in\\,\\Phi^{\\mathcal{X}_{i}}$ , on the strategy sampled from the joint distribution. The larger the set $\\Phi$ , the stronger the incentive guarantee offered by the $\\Phi$ -equilibrium. For example, if $\\breve{\\Phi}^{\\mathcal{X}_{i}}$ contains all constant functions, the corresponding $\\Phi$ -equilibrium coincides with the notion of coarse correlated equilibrium. Throughout the paper, we also consider $\\varepsilon,$ -approximate $\\Phi$ -equilibria, where no player can gain more than $\\varepsilon$ by deviating using any function from $\\mathbf{\\dot{\\Phi}}^{\\mathcal{X}_{i}}$ . We study several families of $\\Phi$ and illustrate their relationships in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "Finite Set of Global Deviations. The first case we consider is when each player $i$ \u2019s set of strategy modifications, $\\Phi^{\\mathcal{X}_{i}}$ , contains a finite number of arbitrary functions mapping $\\mathcal{X}_{i}$ to itself. As shown in [GJ03], if there exists an online learning algorithm where each player $i$ is guaranteed to have sublinear $\\Phi^{\\mathcal{X}_{i}}$ -regret, the empirical distribution of joint strategies played converges to a $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ -equilibrium. Gordon, Greenwald, and Marks [GGM08] consider $\\Phi$ -regret minimization but for concave reward functions, and their results, therefore, do not apply to non-concave games. Stoltz and Lugosi [SL07] provide an algorithm that achieves no $\\Phi^{\\mathcal{X}_{i}}$ -regret in non-concave games; however, their algorithm requires a fixed-point computation per step, making it computationally inefficient.3 Our first contribution is to provide an efficient randomized algorithm that achieves no $\\Phi^{\\mathcal{X}_{i}}$ -regret for each player $i$ with high probability. ", "page_idx": 2}, {"type": "text", "text": "Contribution 1: Let $\\mathcal{X}$ be a strategy set (not necessarily compact or convex), and $\\Psi$ an arbitrary finite set of strategy modification functions for $\\mathcal{X}$ . We design a randomized online learning algorithm that achieves $O\\left({\\sqrt{T\\log|\\Psi|}}\\right)$ $\\Psi$ -regret, with high probability, for arbitrary bounded reward functions on $\\mathcal{X}$ (Theorem 2). The algorithm operates in time ${\\sqrt{T}}|\\Psi|$ per iteration. If every player in a non-concave game adopts this algorithm, the empirical distribution of strategy profiles played forms an $\\varepsilon$ -approximate $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\chi_{i}}$ -equilibrium, with high probability, for any $\\varepsilon>0$ , after poly $\\left(\\textstyle{\\frac{1}{\\varepsilon}},\\log\\left(\\operatorname*{max}_{i}\\vert\\Phi^{\\mathcal{X}_{i}}\\vert\\right),\\log^{\\stackrel{\\cdot}{n}}\\right)$ iterations. ", "page_idx": 2}, {"type": "text", "text": "If players have infinitely many global strategy modifications, we can extend Algorithm 1 by discretizing the set of strategy modifications under mild assumptions, such as the modifications being Lipschitz (Corollary 1). The empirical distribution of the strategy profiles still converges to the corresponding $\\Phi$ -equilibrium, but at a much slower rate of $O(T^{-\\frac{1}{d+2}})$ , where $d$ is the dimension of the set of strategies. Additionally, the algorithm requires exponential time in the dimension per iteration, making it inefficient. This inefficiency is unavoidable, as the problem remains intractable even when $\\Phi$ contains only constant functions. ", "page_idx": 2}, {"type": "text", "text": "To address the limitations associated with infinitely large global strategy modifications, a natural approach is to focus on local deviations instead. The corresponding $\\Phi$ -equilibrium will guarantee local stability. The study of local equilibrium concepts in non-concave games has received significant attention in recent years\u2014see e.g., [RBS16; HSZ17; DP18; JNJ20; DSZ21]. However, these solution concepts either are not guaranteed to exist, are restricted to sequential two-player zero-sum games [MV21], only establish local convergence guarantees for learning dynamics\u2014see e.g., [DP18; WZB20; FCR20], only establish asymptotic convergence guarantees\u2014see e.g., $[\\mathrm{Das}{+}23]$ , or involve non-standard solution concepts where local stability is not with respect to a distribution over strategy profiles [HSZ17]. ", "page_idx": 3}, {"type": "text", "text": "We study the tractability of $\\Phi$ -equilibrium with infinitely large $\\Phi$ sets that consist solely of local strategy modifications. These local solution concepts are guaranteed to exist in general multi-player non-concave games. Specifically, we focus on the following three families of natural deviations. ", "page_idx": 3}, {"type": "text", "text": "- Projection based Local Deviations: Each player $i$ \u2019s set of strategy modifications, denoted by $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}_{i}}(\\delta)$ , contains all deviations that attempt a small step from their input in a fixed direction and project if necessary, namely are of the form $\\phi_{v}(x)=\\Pi_{\\mathcal{X}_{i}}[x-v]$ , where $\\lVert v\\rVert\\leq\\delta$ and $\\Pi_{\\mathcal{X}_{i}}$ stands for the $\\ell_{2}$ -projection onto $\\mathbf{\\mathcal{X}}_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "- Convex Combination of Finitely Many Local Deviations: Each player $i$ \u2019s set of strategy modifications, denoted by $\\operatorname{Conv}(\\Phi^{\\mathcal{X}_{i}}(\\boldsymbol{\\delta}))$ , contains all deviations that can be represented as a convex combination of a finite set of $\\delta$ -local strategy modifications, i.e., $\\|\\phi(x)\\!-\\!x\\|\\stackrel{\\cdot}{\\leq}\\delta$ for all $\\phi\\in\\Phi^{\\mathcal{X}_{i}}(\\delta)$ . ", "page_idx": 3}, {"type": "text", "text": "- Interpolation based Local Deviations: each player $i$ \u2019s set of local strategy modifications, denoted by $\\Phi_{\\mathrm{Int}}^{\\bar{\\mathcal{X}}_{i}}(\\delta)$ , that contains all deviations that interpolate between the input strategy and another strategy in $\\mathcal{X}_{i}$ . Formally, each element $\\phi_{\\lambda,x^{\\prime}}(x)$ of $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}_{i}}(\\delta)$ can be represented as $(1-\\lambda)x+\\lambda x^{\\prime}$ for some $x^{\\prime}\\in\\mathcal{X}_{i}$ and $\\dot{\\lambda}\\le\\delta/D_{{\\lambda_{i}}}$ $D_{\\mathcal{X}_{i}}$ is the diameter of $\\mathcal{X}_{i}$ ). ", "page_idx": 3}, {"type": "text", "text": "For our three families of local strategy modifications, we explore the tractability of $\\Phi$ -equilibrium within a regime we term the first-order stationary regime, where $\\varepsilon=\\Omega(\\delta^{2})^{4}$ , with $\\delta$ representing the maximum deviation allowed for a player. An $\\varepsilon,$ -approximate $\\Phi$ -equilibrium in this regime ensures firstorder stability. This regime is particularly interesting for two reasons: (i) Daskalakis, Skoulakis, and Zampetakis [DSZ21] have demonstrated that computing an $\\varepsilon$ -approximate $\\delta$ -local Nash equilibrium in this regime is intractable.5 This poses an intriguing question: can correlating the players\u2019 strategies, as in a $\\Phi$ -equilibrium, potentially make the problem tractable? (ii) Extending our algorithm, initially designed for finite sets of strategy modifications, to these three sets of local deviations results in inefficiency; specifically, the running time becomes exponential in one of the problem\u2019s natural parameters. Designing efficient algorithms for this regime thus presents challenges. Despite these, we show the following: ", "page_idx": 3}, {"type": "text", "text": "Contribution 2: For any $\\delta>0$ , for each of the three families of infinite $\\delta$ -local strategy modifications mentioned above, there exists an efficient uncoupled learning algorithm that converges to an $\\varepsilon$ -approximate $\\Phi$ -equilibrium of the non-concave game in the first-order stationary regime, i.e., $\\varepsilon=\\dot{\\Omega}(\\delta^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "We present our results for the projection-based local deviation in Theorem 3 and Theorem 9. Our result for the convex combination of local deviations can be found in Theorem 4. Theorem 5 contains our result for the interpolation-based local deviations. Similar to the finite case, our algorithms build on the connection between $\\Phi$ -regret minimization and $\\Phi$ -equilibrium. Given that our strategy modifications are non-standard, it is a priori unclear how to minimize the corresponding $\\Phi$ -regret. For instance, to our knowledge, no algorithm is known to minimize $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret even when the reward functions are concave, and provably $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is incomparable to external regret (Examples 3 and 4). However, via a novel analysis, we show that Online Gradient Descent (GD) and Optimistic Gradient (OG) achieve a near-optimal $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret guarantee (Theorem 3 and Theorem 8). Our results provide efficient uncoupled algorithms to compute $\\varepsilon$ -approximate $\\Phi(\\delta)$ -equilibria in the first-order stationary regime $\\varepsilon=\\Omega(\\delta^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "Further related work is discussed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A ball of radius $r>0$ centered at $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is denoted by $B_{d}(x,r):=\\{x^{\\prime}\\in\\mathbb{R}^{d}:\\|x-x^{\\prime}\\|\\leq r\\}$ . We use $\\Vert\\cdot\\Vert$ for $\\ell_{2}$ norm throughout. We also write $B_{d}(\\delta)$ for a ball centered at the origin with radius $\\delta$ . For $a\\in\\mathbb R$ , we use $[a]^{+}$ to denote $\\operatorname*{max}\\{0,a\\}$ . We denote $D_{\\mathcal{X}}$ as the diameter of a set $\\mathcal{X}$ . ", "page_idx": 4}, {"type": "text", "text": "Continuous / Smooth Games. An $n$ -player continuous game has a set of $n$ players $[n]\\;:=$ $\\{1,2,\\ldots,n\\}$ . Each player $i\\,\\in\\,[n]$ has a nonempty convex and compact strategy set $\\mathcal{X}_{i}\\ \\subseteq\\ \\mathbb{R}^{d_{i}}$ . For a joint strategy proflie $\\begin{array}{r}{x=(\\dot{x_{i}},\\dot{x}_{-i})\\in\\prod_{j=1}^{n}\\bar{\\chi}_{j}^{\\textit{j}}}\\end{array}$ , the reward of player $i$ is determined by a utility function $\\begin{array}{r}{u_{i}:\\prod_{j=1}^{n}\\mathcal{X}_{j}\\to[0,1]}\\end{array}$ . We denote by $\\textstyle d=\\sum_{i=1}^{n}d_{i}$ the dimensionality of the game and assume $\\operatorname*{max}_{i\\in[n]}\\{D_{\\mathcal{X}_{i}}\\}\\leq D$ . A smooth game is a continuous game whose utility functions further satisfy the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Smooth Games). The utility function $u_{i}(x_{i},x_{-i})$ for any player $i\\in[n]$ is differentiable and satisfies ", "page_idx": 4}, {"type": "text", "text": "1. ( $\\boldsymbol{G}$ -Lipschitzness) $\\|\\nabla_{x_{i}}u_{i}(x)\\|\\leq G$ for all i and $\\textstyle x\\in\\prod_{j=1}^{n}{\\mathcal{X}}_{j}$ ; ", "page_idx": 4}, {"type": "text", "text": "2. ( $L$ -smoothness) there exists $L_{i}>0$ such that $\\|\\nabla_{x_{i}}u_{i}(x_{i},x_{-i})-\\nabla_{x_{i}}u_{i}(x_{i}^{\\prime},x_{-i})\\|\\leq L_{i}\\|x_{i}-x_{i}^{\\prime}\\|$ for all $x_{i},x_{i}^{\\prime}\\in\\mathcal{X}_{i}$ and $x_{-i}\\in\\Pi_{j\\neq i}\\chi_{j}$ . We denote $L=\\operatorname*{max}_{i}L_{i}$ as the smoothness of the game. ", "page_idx": 4}, {"type": "text", "text": "Crucially, we make no assumption on the concavity of $u_{i}(x_{i},x_{-i})$ . ", "page_idx": 4}, {"type": "text", "text": "$\\Phi$ -equilibrium and $\\Phi$ -regret. Below we formally introduce the concept of $\\Phi$ -equilibrium and its relationship with online learning and $\\Phi$ -regret minimization. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 ( $\\Phi$ -equilibrium [GJ03; SL07]). In a continuous game, a distribution $\\sigma$ over joint strategy proflies $\\Pi_{i=1}^{n}\\mathcal{X}_{i}$ is an $\\varepsilon$ -approximate $\\Phi$ -equilibrium for some $\\varepsilon\\ge0$ and a proflie of strategy modification sets $\\Phi=\\Pi_{i=1}^{n}\\Phi_{i}$ if and only if for all player $\\in[n],\\operatorname*{max}_{\\phi\\in\\Phi_{i}}\\mathbb{E}_{x\\sim\\sigma}[u_{i}(\\phi(x_{i}),x_{-i})]\\leq$ $\\mathbb{E}_{x\\sim\\sigma}[u_{i}(x)]+\\varepsilon$ . When $\\varepsilon=0$ , we call $\\sigma$ a $\\Phi$ -equilibrium. ", "page_idx": 4}, {"type": "text", "text": "We consider the standard online learning setting: at each day $t\\in[T]$ , the learner chooses an action $x^{t}$ from a nonempty convex compact set $\\mathcal{X}\\subseteq\\mathbb{R}^{m}$ and the adversary chooses a possibly non-convex loss function $f^{t}:\\bar{\\boldsymbol{\\chi}}\\rightarrow\\mathbb{R}$ , then the learner suffers a loss $f^{t}(x^{t})$ and receives feedback. In this paper, we focus on two feedback models: (1) the player receives an oracle for $f^{t}(\\cdot)$ ; (2) the player receives only the gradient $\\nabla f^{t}(x^{t})$ . The classic goal of an online learning algorithm is to minimize the external regret defined as $\\begin{array}{r}{\\mathrm{Reg}^{T}:=\\operatorname*{max}_{x\\in\\mathcal{X}}\\sum_{t=1}^{T}(f^{t}(x^{t})-f^{t}(x))}\\end{array}$ . An algorithm is called no-regret if its external regret is sublinear in $T$ . The notion of $\\Phi$ -regret generalizes external regret by allowing more general strategy modifications. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 ( $\\Phi$ -regret). Let $\\Phi$ be a set of strategy modification functions $\\{\\phi:\\mathcal{X}\\to\\mathcal{X}\\}$ . For $T\\geq1$ , the $\\Phi$ -regret of an online learning algorithm is $\\begin{array}{r}{\\mathrm{Reg}_{\\Phi}^{T}:=\\operatorname*{max}_{\\phi\\in\\Phi}\\sum_{t=1}^{T}\\big(f^{t}(x^{t})-f^{t}(\\phi(x^{t}))\\big).A n}\\end{array}$ algorithm is called no $\\Phi$ -regret $i f$ its $\\Phi$ -regret is sublinear in $T$ . ", "page_idx": 4}, {"type": "text", "text": "Many classic notions of regret can be interpreted as $\\Phi$ -regret. For example, the external regret is $\\Phi_{\\mathrm{ext}}$ -regret where $\\Phi_{\\mathrm{ext}}$ contains all constant strategy modifications $\\phi_{x^{*}}(x)=x^{*}$ for all $x^{*}\\in\\mathcal{X}$ . The swap regret on simplex $\\Delta^{m}$ is $\\Phi_{\\mathrm{swap}}$ -regret where $\\Phi_{\\mathrm{swap}}$ contains all linear transformations $\\phi:\\Delta^{m}\\rightarrow\\Delta^{m}$ . A fundamental result for learning in games is that no- $\\Phi$ -regret learning dynamics in games converge to an approximate $\\Phi$ -equilibrium [GJ03]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 ([GJ03]). If each player i\u2019s $\\Phi_{i}$ -regret is upper bounded by $\\mathrm{Reg}_{\\Phi_{i}}^{T}$ , then their empirical distribution of strategy profiles played is an $(\\operatorname*{max}_{i\\in[n]}\\mathrm{{Reg}}_{\\Phi_{i}}^{T}/T)$ -approximate $\\Phi$ -equilibrium. ", "page_idx": 4}, {"type": "text", "text": "3 Tractable $\\Phi$ -Equilibrium for Finite $\\Phi$ via Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we revisit the problem of computing and learning an $\\Phi$ -equilibrium in non-concave games when each player\u2019s set of strategy modifications $\\Phi^{\\mathcal{X}_{i}}$ is finite. ", "page_idx": 4}, {"type": "text", "text": "The pioneering work of Stoltz and Lugosi [SL07] gives a no- $\\Phi$ -regret algorithm for this case where each player chooses a distribution over strategies in each round. This result also implies convergence to $\\Phi$ -equilibrium. However, the algorithm by Stoltz and Lugosi [SL07] is not computationally efficient. In each iteration, their algorithm requires computing a distribution that is stationary under a transformation that can be represented as a mixture of the modifications in $\\Phi$ . The existence of such a stationary distribution is guaranteed by the Schauder-Cauty fixed-point theorem [Cau01], but the distribution might require exponential support and be intractable to find. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Our main result in this section is an efficient $\\Phi$ -regret minimization algorithm (Algorithm 1) that circumvents the step of the exact computation of a stationary distribution. Consequently, our algorithm also ensures efficient convergence to a $\\Phi$ -equilibrium when adopted by all players. ", "page_idx": 5}, {"type": "table", "img_path": "3CtTMF5zzM/tmp/cc5e2b97012c8c5410a7794d5f91255dde30ed2a95340441512052a1525dbee8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "3CtTMF5zzM/tmp/8021e3c1fe580502e1d2d3bce9b49a77c6bf79c035306c1b83c4aee010b2c042.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $\\mathcal{X}$ be a strategy set (not necessarily compact or convex), $\\Phi$ be an arbitrary finite set of strategy modifications over $\\mathcal{X}$ , and $u^{1}(\\cdot),\\ldots,u^{T}(\\cdot)$ be an arbitrary sequence of possibly non-concave reward functions from $\\mathcal{X}$ to $[0,1]$ . If we instantiate Algorithm $I$ with $\\Re_{\\Phi}$ being the Hedge algorithm over $\\Phi$ and $h={\\sqrt{T}}$ , the algorithm guarantees that, with probability at least $1-\\beta_{i}$ , it produces $a$ sequence of strategies $x^{1},\\ldots,x^{T}$ with $\\Phi$ -regret at most $\\begin{array}{r}{\\operatorname*{max}_{\\phi\\in\\Phi}\\sum_{t=1}^{T}u^{t}(\\phi(x^{t}))-\\sum_{t=1}^{T}u^{t}(x^{t})\\leq}\\end{array}$ $8\\sqrt{T(\\log|\\Phi|+\\log(1/\\beta))}$ . Moreover, the algorithm runs in time ${\\cal O}(\\sqrt{T}|\\Phi|)$ per iteration. ", "page_idx": 5}, {"type": "text", "text": "If all players in a non-concave continuous game employ Algorithm 1, then with probability at least $1-\\beta$ , for any $\\varepsilon>0$ , the empirical distribution of strategy profiles played forms an $\\varepsilon$ -approximate $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ -equilibrium, after poly $\\left(\\textstyle{\\frac{1}{\\varepsilon}},\\log\\left(\\operatorname*{max}_{i}\\vert\\Phi^{{\\mathcal{X}}_{i}}\\vert\\right),\\log{\\frac{n}{\\beta}}\\right)$ iterations. ", "page_idx": 5}, {"type": "text", "text": "High-level ideas. We adopt the framework in [SL07]. The framework contains two steps in each iteration $t$ : (1) the learner runs a no-external-regret algorithm over $\\Phi$ which outputs $p^{t}\\in\\Delta(\\Phi)$ in each iteration $t$ ; (2) the learner chooses a stationary distribution $\\begin{array}{r}{\\mu^{t}=\\sum_{\\phi\\in\\Phi}\\bar{p^{t}}\\phi(\\bar{\\mu^{t}})}\\end{array}$ , where we slightly abuse notation to use $\\phi(\\mu^{t})$ to denote the image measure of $\\mu$ by $\\phi$ . However, how to compute the stationary distribution $\\mu^{t}$ efficiently is unclear. We essentially provide a computationally efficient way to carry out step (2) without computing this stationary distribution. ", "page_idx": 5}, {"type": "text", "text": "\u2022 We first construct an $\\varepsilon$ -approximate stationary distribution by recursively applying strategy modifications from $\\Phi$ . The constructed distribution can be viewed as a tree. Our construction is inspired by the recent work of Zhang, Anagnostides, Farina, and Sandholm $[Z\\mathrm{ha}+24]$ for concave games. The main difference here is that for non-concave games, the distribution needs to be approximately stationary with respect to a mixture of strategy modifications rather than a single one as in concave games. Consequen\u221atly, this leads to an approximate stationary distribution with prohibitively high support size $\\bigl(\\bigl|\\bar{\\Phi}\\bigr|\\bigr)^{\\sqrt{T}}$ , as opposed to $\\sqrt{T}$ in $[Z\\mathrm{ha}+24]$ for concave games. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Despite the exponentially large support size of the distribution,\u221a we utilize its tree structure to design a simple and efficient sampling procedure that runs in time $\\sqrt{T}$ . Equipped with such a sampling procedure, we provide an efficient randomized algorithm that generates a sequence of strategies so that, with high probability, the $\\Phi$ -regret for this sequence of strategies is at most $O(\\sqrt{T\\log{|\\Phi|}})$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We defer the full proof of Theorem 2 to Section 3.1. An extension of Theorem 2 to infinite $\\Phi$ holds when the rewards $\\{u^{t}\\}_{t\\in[T]}$ are $G$ -Lipschitz and $\\Phi$ admits an $\\alpha$ -cover with size $N(\\alpha)$ . In particular, when $\\Phi$ is the set of all $M$ -Lipschitz functions over $[0,1]^{d}$ , $\\Phi$ admits an $\\alpha$ -cover with $\\log N(\\alpha)$ of the order $(1/\\alpha)^{d}$ [SL07]. In this case, we have ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. There is a randomized algorithm such that, with probability at least $1-\\beta$ , the $\\Phi$ -regret is bounded by $c\\cdot T^{\\frac{d+1}{d+2}}\\cdot\\log(1/\\beta)$ , where c only depends on $G$ and $M$ . The algorithm runs in time $\\operatorname{poly}(T,N(T^{-1/(d+2)}))$ . ", "page_idx": 6}, {"type": "text", "text": "3.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For a distribution $\\mu\\in\\Delta(\\mathcal{X})$ over strategy space $\\mathcal{X}$ , we slightly abuse notation and define its expected utility as $u^{t}(\\mu):=\\mathbb{E}_{x\\sim\\mu}[u^{t}(x)]\\in[0,1]$ . We define $\\phi(\\mu)$ the image of $\\mu$ under transformation $\\phi$ . In each iteration $t$ , the learner chooses their strategy $x^{t}\\in\\mathscr{X}$ according to the distribution $\\mu^{t}$ . For a sequence of strategies $\\{x^{t}\\}_{t\\in[T]}$ , the $\\Phi$ -regret is $\\begin{array}{r}{\\mathrm{Reg}_{\\Phi}^{T}:=\\operatorname*{max}_{\\phi\\in\\Phi}\\left\\{\\sum_{t=1}^{T}\\left(u^{t}(\\phi(x^{t}))-u^{t}(x^{t})\\right)\\right\\}.}\\end{array}$ . Algorithm 1 uses an external regret minimization algorithm $R_{\\Phi}$ over $\\Phi$ which outputs a distribution $p^{t}\\in\\Delta(\\Phi)$ . We can then decompose the $\\Phi$ -regret into two parts. ", "page_idx": 6}, {"type": "image", "img_path": "3CtTMF5zzM/tmp/f65f19e5053471f7fe78a549ecb9cf7e54d93a8d11d318abcb6e775b10afd175.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "I: Bounding the external regret over $\\Phi$ . The external regret over $\\Phi$ can be bounded directly. This is equivalent to an online expert problem: in each iteration $t$ , the external regret minimizer $\\Re_{\\Phi}$ chooses $p^{t}\\in\\Delta(\\Phi)$ and the adversary then determines the utility of each expert $\\phi\\in\\Phi$ as $u^{t}(\\phi({x^{t}}))$ . We choose the external regret minimizer $\\Re_{\\Phi}$ to be the Hedge algorithm [FS99]. Then we have $\\begin{array}{r}{\\operatorname*{max}_{\\phi\\in\\Phi}\\left\\{\\sum_{t=1}^{T}u^{t}(\\phi(x^{t}))-\\mathbb{E}_{\\phi^{\\prime}\\sim p^{t}}[u^{t}(\\phi^{\\prime}(x^{t}))]\\right\\}\\leq2\\sqrt{T\\log|\\Phi|}}\\end{array}$ (Theorem 6), where we use the fact that the utility function $u^{t}$ is bounded in $[0,1]$ . ", "page_idx": 6}, {"type": "text", "text": "II: Bounding error due to sampling from an approximate stationary distribution. We first define a distribution $\\mu^{t}$ over $\\mathcal{X}$ using a complete $|\\Phi|.$ -ary tree with depth $h$ . The root of this tree is an arbitrary strategy $x_{\\mathrm{root}}\\in\\mathcal X$ . Each internal node $x$ has exactly $|\\Phi|$ children, denoted as $\\{\\phi(x)\\}_{\\phi\\in\\Phi}$ . The distribution $\\mu^{t}$ is supported on the nodes of this tree. Next, we define the probability for each node under the distribution $\\mu^{t}$ . The root node $x_{\\mathrm{root}}$ receives probability $\\frac{1}{h}$ . The probability of other nodes is defined in a recursive manner. For every node $\\boldsymbol{x}=\\boldsymbol{\\phi}(\\boldsymbol{x}_{p})$ where $x_{p}$ is its parent, $x$ receives probability $\\operatorname*{Pr}_{\\mu^{t}}[x]=\\operatorname*{Pr}_{\\mu^{t}}[x_{p}]\\cdot p^{t}(\\phi)$ . It is then clear that the total probability of the children of a node $x_{p}$ is exactly $\\operatorname{Pr}_{\\mu^{t}}[x\\,$ is $x_{p}$ \u2019s $\\begin{array}{r}{\\mathrm{`hild]}=\\sum_{\\phi}\\mathrm{Pr}[x_{p}]\\cdot p^{t}(\\phi)=\\mathrm{Pr}[x_{p}]}\\end{array}$ . Denote the set of nodes in depth $k$ as $N_{k}$ . We have $\\mathrm{Pr}_{\\mu^{t}}[x\\in N_{k}]=\\frac{1}{h}$ for every depth $1\\leq k\\leq h$ . Thus the distribution $\\mu^{t}$ supports o n ||\u03a6\u03a6||\u2212\u221211 points and is well-defined. By the construction above, we know xt output by Algorithm 2 is a sample from $\\mu^{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Now we show that the approximation error of $\\mu^{t}$ is bounded by $O(\\frac{1}{h})$ . We can evaluate the approximation error of $\\mu^{t}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\phi\\sim p^{t}}\\big[u^{t}(\\phi(\\mu^{t}))\\big]-u^{t}(\\mu^{t})=\\mathbb{E}_{\\phi\\sim p^{t}}\\Bigg[\\sum_{k=1}^{h}\\sum_{x\\in N_{k}}\\operatorname*{Pr}[x]u^{t}(\\phi(x))\\Bigg]-\\Bigg[\\sum_{k=1}^{h}\\sum_{x\\in N_{k}}\\operatorname*{Pr}[x]u^{t}(x)\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{k=1}^{h}\\sum_{x\\in N_{k}}\\Bigg(\\mathbb{E}_{\\phi\\sim p^{t}}\\bigg[\\mathrm{Pr}[x]u^{t}(\\phi(x))\\bigg]-\\mathrm{Pr}[x]u^{t}(x)\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We recall that for a node $\\boldsymbol{x}=\\phi(\\boldsymbol{x}_{p})$ with $x_{p}$ being its parent, we have $\\operatorname*{Pr}_{\\mu^{t}}[x]=\\operatorname*{Pr}_{\\mu^{t}}[x_{p}]\\cdot p^{t}(\\phi)$ . Thus for any $1\\leq k\\leq h-1$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\forall\\:K}\\left(\\mathbb{E}_{\\phi\\sim p^{t}}\\bigg[\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(\\phi(x))\\bigg]-\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(x)\\right)=\\sum_{x\\in N_{k}}\\left(\\sum_{\\phi\\in\\Phi}p^{t}(\\phi)\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(\\phi(x))-\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(x)\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the above equality, we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\hat{Z}}_{\\phi\\sim p^{t}}\\big[u^{t}(\\phi(\\mu^{t}))\\big]-u^{t}(\\mu^{t})}\\\\ &{=\\displaystyle\\sum_{k=1}^{h-1}\\sum_{\\substack{x\\in N_{k+1}}}\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(x)+\\displaystyle\\sum_{x\\in N_{h}}\\sum_{\\phi\\in\\Phi}p^{t}(\\phi)\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(\\phi(x))-\\displaystyle\\sum_{k=2\\,x\\in N_{k}}^{h}\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(x)-\\operatorname*{Pr}_{\\mu^{t}}[x_{\\mathrm{rool}}]u^{t}(x_{\\mathrm{rool}})}\\\\ &{=\\displaystyle\\sum_{x\\in N_{h}}\\sum_{\\phi\\in\\Phi}p^{t}(\\phi)\\operatorname*{Pr}_{\\mu^{t}}[x]u^{t}(\\phi(x))-\\operatorname*{Pr}_{\\mu^{t}}[x_{\\mathrm{rool}}]u^{t}(x_{\\mathrm{rool}})\\leq\\frac{1}{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where in the last inequality we use the fact that $\\textstyle\\sum_{x\\in N_{k}}\\operatorname*{Pr}_{\\mu^{t}}[x]\\,=\\,{\\frac{1}{h}}$ for all $1\\leq k\\leq h$ and the utility function $u^{t}$ is bounded in $[0,1]$ . Therefore, for $x^{t}\\sim\\mu^{t}$ , the sequence of random variables $\\begin{array}{r}{\\{\\sum_{t=1}^{\\tau}\\big(\\mathbb{E}_{\\phi\\sim p^{t}}[u^{t}(\\phi(x^{t}))]-u^{t}(\\bar{x^{t}})-\\bar{\\frac{1}{h}}\\big)\\}_{\\tau\\geq1}}\\end{array}$ is a super-martingale. Thanks to the boundedness of the utility function, we can apply the Hoeffding-Azuma Inequality and get for any $\\varepsilon>0$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}\\left(\\mathbb{E}_{\\phi\\sim p^{t}}\\big[u^{t}(\\phi(x^{t}))\\big]-u^{t}(x^{t})-\\frac{1}{h}\\right)\\geq\\varepsilon\\right]\\leq\\exp\\left(-\\frac{\\varepsilon^{2}}{8T}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Combining the bounds for I and $\\mathrm{II}$ with $\\varepsilon=\\sqrt{8T\\log(1/\\beta)}$ and $h={\\sqrt{T}}$ , we have, with probability $1-\\beta$ , that $\\begin{array}{r}{\\mathrm{Reg}_{\\Phi}^{T}\\leq2\\sqrt{T\\log|\\Phi|}+\\frac{T}{h}+\\sqrt{8T\\log(1/\\beta)}\\leq8\\sqrt{T(\\log|\\Phi|+\\log(1/\\beta))}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Convergence to $\\Phi$ -equilibrium. If all players in a non-concave continuous game employ Algorithm 1, then we know for each player $i$ , with probability $\\textstyle1-{\\frac{\\beta}{n}}$ , its $\\Phi^{\\mathcal{X}_{i}}$ -regret is upper bounded by $8\\sqrt{T(\\log|\\Phi^{\\mathcal{X}_{i}}|+\\log(n/\\beta))}$ . By a union bound over all $n$ players, we get with probability $1-\\beta$ , every player $i$ \u2019s $\\Phi^{\\mathcal{X}_{i}}$ -regret is upper bounded by $8\\sqrt{T(\\log|\\Phi^{\\mathcal{X}_{i}}|+\\log(n/\\beta))}$ . Now by Theorem 1, we know the empirical distribution of strategy profiles played forms an $\\varepsilon$ -approximate $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ -equilibrium, as long as $T\\geq64(\\log(\\operatorname*{max}_{i}|\\dot{\\Phi}^{\\mathcal{X}_{i}}|)+\\mathrm{\\dot{log}}(n/\\beta))/\\varepsilon^{2}$ iterations. ", "page_idx": 7}, {"type": "text", "text": "4 Approximate $\\Phi$ -Equilibria under Infinite Local Strategy Modifications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section studies $\\Phi$ -equilibrium when $|\\Phi|$ is infinite. It is, in general, computationally hard to compute a $\\Phi$ -equilibrium even if $\\Phi$ contains all constant deviations. Instead, we focus on $\\Phi$ that consists solely of local strategy modifications. We introduce several natural classes of local strategy modifications and provide efficient online learning algorithms that converge to $\\varepsilon$ -approximate $\\Phi$ - equilibrium in the first-order stationary regime where $\\bar{\\varepsilon^{\\mathrm{~\\,~}}}=\\Omega(\\delta^{2}L)$ . These approximate $\\Phi$ -equilibria guarantee first-order stability. ", "page_idx": 7}, {"type": "text", "text": "Definition 3 ( $\\delta$ -local strategy modification). For each agent $i$ , we call a set of strategy modifications $\\Phi^{\\mathcal{X}_{i}}$ $\\delta$ -local $i f.$ for all $x\\in\\mathcal{X}_{i}$ and $\\phi_{i}\\in\\Phi^{\\vec{x}_{i}}$ , $\\|\\phi_{i}(x)-x\\|\\leq\\delta$ . We use notation $\\Phi^{\\vec{X_{i}}}(\\delta)$ to denote $a$ $\\delta$ -local strategy modification set for agent $i$ . We also use $\\Phi(\\delta)=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}(\\delta)$ to denote a profile of $\\delta$ -local strategy modification sets. ", "page_idx": 7}, {"type": "text", "text": "Below we present a useful reduction from computing an $\\varepsilon$ -approximate $\\Phi(\\delta)$ -equilibrium in nonconcave smooth games to $\\Phi^{\\mathcal{X}_{i}}(\\delta)$ -regret minimization against convex losses for any $\\begin{array}{r}{\\varepsilon\\ge\\frac{\\delta^{2}L}{2}}\\end{array}$ \u03b4L. The key observation here is that the $L$ -smoothness of the utility function permits, within a $\\delta$ -neighborhood, a $\\frac{\\delta^{2}L}{2}$ -approximation using a linear function. We defer the proof to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Lemma 1 (No $\\Phi(\\delta)$ -Regret for Convex Losses to Approximate $\\Phi(\\delta)$ -Equilibrium in Non-Concave Games). For any $T\\geq1$ and $\\delta>0,$ , let $\\boldsymbol{\\mathcal{A}}$ be an algorithm that guarantees to achieve no more than $\\operatorname{Reg}_{\\Phi^{\\boldsymbol{x}_{i}}(\\boldsymbol{\\delta})}^{T}\\Phi^{\\mathcal{X}_{i}}(\\boldsymbol{\\hat{\\delta}})$ -regret for convex loss functions for each agent $i\\in[n]$ . Then ", "page_idx": 7}, {"type": "text", "text": "1. The $\\Phi^{\\mathcal{X}_{i}}(\\delta)$ -regret of $\\boldsymbol{\\mathcal{A}}$ for non-convex and $L$ -smooth losses is at most $\\begin{array}{r}{\\mathrm{Reg}_{\\Phi^{\\lambda_{i}}\\left(\\delta\\right)}^{T}+\\frac{\\delta^{2}L T}{2},}\\end{array}$ $\\forall i$ . ", "page_idx": 8}, {"type": "text", "text": "2. When every agent employs $\\boldsymbol{\\mathcal{A}}$ in a non-concave $L$ -smooth game, their empirical distribution of the joint strategies played converges to a $\\scriptstyle\\left(\\operatorname*{max}_{i\\,\\in\\,[n]}\\left\\{\\mathrm{Reg}_{\\Phi}^{T}x_{i\\,(\\,\\delta)}\\right\\}\\right/T\\,+\\,{\\frac{\\delta^{2}L}{2}}\\right)$ -approximate $\\Phi(\\delta)$ -equilibrium. ", "page_idx": 8}, {"type": "text", "text": "4.1 Projection-Based Local Strategy Modifications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we study a set of local strategy modifications based on projection. Specifically, the set $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ encompasses all deviations that essentially add a fixed displacement vector $v$ to the input strategy and project back to the feasible set: $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta):=\\{\\phi_{\\mathrm{Proj},v}(x)=\\Pi_{\\mathcal{X}}[x-v]:v\\in$ $B_{d}(\\delta)\\}$ . It is clear that $\\|\\phi_{v}(x)-x\\|\\,\\leq\\,\\|v\\|\\,\\leq\\,\\delta$ . The induced $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}:=$ $\\begin{array}{r}{\\operatorname*{max}_{v\\in B_{d}(\\delta)}\\sum_{t=1}^{T}\\big(f^{t}(x^{t})-f^{t}(\\Pi_{\\mathcal{X}}[x^{t}-v])\\big)}\\end{array}$ . We also define $\\Phi_{\\mathrm{Proj}}(\\delta)=\\Pi_{i=1}^{n}\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}_{i}}(\\delta)$ ", "page_idx": 8}, {"type": "text", "text": "By Lemma 1, to achieve convergence to an approximate $\\Phi_{\\mathrm{Proj}}(\\delta)$ -equilibrium, it suffices to minimize $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret against convex losses. However, to our knowledge, there does not exist an algorithm that minimize $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret even in the convex case. In fact, external regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret are provably incomparable: a sequence of actions may suffer high ${\\mathrm{Reg}}^{T}$ but low $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}$ (Example 3) or vise versa (Example 4). At a high level, the external regret competes against a fixed action, whereas $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is more akin to the notion of dynamic regret, competing with a sequence of varying actions. Despite this, surprisingly, we show that classical algorithms like Online Gradient Descent (GD) and Optimistic Gradient (OG), known for minimizing external regret, also attain near-optimal $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret. We defer the examples and missing proofs to Appendix E. ", "page_idx": 8}, {"type": "text", "text": "$\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -Regret Minimization in the Adversarial Setting. We show that (GD) enjoys an $O(\\dot{G_{\\sqrt{\\delta D_{\\mathcal{X}}T}}})\\ \\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret despite the difference between the external regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ - regret. First, let us recall the update rule of GD: given initial point $x^{1}\\in\\chi$ and step size $\\eta>0$ , GD updates in each iteration $t$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\boldsymbol{x}}^{t+1}=\\Pi_{\\mathcal{X}}[{\\boldsymbol{x}}-\\eta\\nabla f^{t}({\\boldsymbol{x}}^{t})].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The key step in our analysis for GD is simple but novel and general (See Appendix E.2). We extend the analysis to the Optimistic Gradient (OG) algorithm in Appendix E.4. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. Let $\\delta>0$ and $T\\in\\mathbb{N}$ . For any convex and $G$ -Lipschitz loss functions $\\{f^{t}:\\mathcal{X}\\to\\right.$ $\\mathbb{R}\\}_{t\\in[T]}$ , the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret\u221a of (GD) with step size $\\eta>0$ is $\\begin{array}{r}{\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\leq\\frac{\\delta^{2}}{2\\eta}+\\frac{\\eta}{2}G^{2}T+\\frac{\\delta D_{X}}{\\eta}}\\end{array}$ . We can choose \u03b7 optimally as \u03b4(\u03b4\u221a+DX ) and attain $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\,\\leq\\,2G\\sqrt{\\delta(\\delta+D_{\\mathcal{X}})T}$ . For any $\\delta>0$ and any $\\varepsilon>0$ , when all player employ $G D$ in a smooth game, their empirical distribution of played strategy proflies converges to an $\\left(\\varepsilon+\\frac{\\delta^{2}L}{2}\\right)$ -approximate $\\Phi_{\\mathrm{Proj}}(\\delta)$ -equilibrium in ${\\cal O}(1/\\varepsilon^{2})$ iterations. ", "page_idx": 8}, {"type": "text", "text": "Remark 1. The $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret can also be viewed as the dynamic regret $I Z i n O3J$ with changing comparators $\\{p^{t}:=\\Pi_{\\mathcal{X}}[x-v]\\}$ . However, our analysis does not follow from standard $\\begin{array}{r}{O(\\frac{(1+P_{T})}{\\eta}+}\\end{array}$ $\\eta T)$ dynamic regret bound of $G D$ [Zin03] since $P_{T}$ , defined as $\\textstyle\\sum_{t=2}^{T}\\|p^{t}-p^{t-1}\\|$ , can be $\\Omega(\\eta T)$ . ", "page_idx": 8}, {"type": "text", "text": "Lower bounds for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret. We complement our upper bound with two lower bounds for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret minimization. The first one is an $\\Omega(\\delta G\\sqrt{T})$ lower bound for any online learning algorithms against linear loss functions (Theorem 7). The second one is an $\\Omega(\\delta^{2}L T)$ lower bound for any algorithm that satisfies the linear span assumption (Proposition 1), which holds for GD and OG against $L$ -smooth non-convex losses. Combining with Lemma 1, this lower bound suggests that GD attains nearly optimal $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret, even in the non-convex setting, among a natural family of gradient-based algorithms. We defer the theorem statements and detailed discussion to Appendix E.3. ", "page_idx": 8}, {"type": "text", "text": "Improved $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -Regret in the Game Setting. We complement the $\\Omega({\\sqrt{T}})$ lower bound in the adversarial setting by considering the game setting where players interact with each other using the same algorithm setting, which has been extensively studied for concave games $[\\mathrm{Syr}+15$ ; CP20; DFG21; Ana $\\boldsymbol{+222}$ ; Ana $^{+22}\\mathrm{b}$ ; Far+22a]. We prove an improved $O(T^{\\frac{1}{4}})$ individual $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret bound for OG (Theorem 9). We defer the details to Appendix E.4 ", "page_idx": 8}, {"type": "text", "text": "4.2 Convex Combination of Finite Local Strategy Modifications ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section considers $\\mathrm{Conv}(\\Phi)$ where $\\Phi$ is a finite set of local strategy modifications. The set of infinite strategy modifications $\\mathrm{Conv}(\\Phi)$ is defined as $\\begin{array}{r}{\\mathrm{Conv}(\\Phi)=\\{\\phi_{p}\\bar{(x)}=\\sum_{\\phi\\in\\Phi}p(\\phi)\\phi(x):p\\in}\\end{array}$ $\\Delta(\\Phi)\\}$ . Our main result is an efficient algorithm (Algorithm 3) that guarantees convergence to an $\\varepsilon$ -approximate $\\mathrm{Conv}(\\Phi)$ -equilibrium in a smooth game satisfying Assumption 1 for any $\\varepsilon>\\delta^{2}L$ . Due to space constraints, we defer Algorithm 3 and the proof to Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Theorem 4. Let $\\mathcal{X}$ be a convex and compact set, $\\Phi$ be an arbitrary finite set of $\\delta$ -local strategy modification functions for $\\mathcal{X}$ , and $u^{1}(\\cdot),\\bar{\\cdot}\\cdot\\cdot,u^{T}(\\cdot)$ be a sequence of $G$ -Lipschitz and $L$ -smooth but possibly non-concave reward functions from $\\mathcal{X}$ to $[0,1]$ . If we instantiate Algorithm 3 with $\\Re_{\\Phi}$ being the Hedge algorithm over $\\Delta(\\Phi)$ and $K\\,=\\,{\\sqrt{T}}$ , the algorithm guarantees that, with probabi\u221ality at least $1-\\beta$ , it produces a sequence of strategies $\\bar{x}^{1},\\bar{\\ldots},x^{T}$ wit\u221ah $\\mathrm{Conv}(\\Phi)$ -regret at most $8{\\sqrt{T}}(G\\delta{\\sqrt{\\log|\\Phi|}}+{\\sqrt{\\log(1/\\beta)}})+\\delta^{2}L T$ . The algorithm runs in time $\\sqrt{T}|\\Phi|$ per iteration. ", "page_idx": 9}, {"type": "text", "text": "If all players in a non-concave smooth game employ Algorithm 3, then with probability $1-\\beta_{i}$ , for any $\\varepsilon>0$ , the empirical distribution of strategy profiles played forms an $\\left(\\varepsilon+\\delta^{2}L\\right)$ -approximate $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ -equilibrium\u201e after poly $\\begin{array}{r}{\\left(\\frac{1}{\\varepsilon},G,\\log\\left(\\operatorname*{max}_{i}|\\Phi^{\\mathcal{X}_{i}}|\\right),\\log\\frac{n}{\\beta}\\right)}\\end{array}$ iterations. ", "page_idx": 9}, {"type": "text", "text": "4.3 Interpolation-Based Local Strategy Modifications ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce a natural set of local strategy modifications and the corresponding local equilibrium notion. Given any set of (possibly non-local) strategy modifications $\\Psi=\\{\\psi:\\mathcal{X}\\rightarrow\\mathcal{X}\\}$ , we define a set of local strategy modifications as follows: for $\\delta\\leq D_{\\mathcal{X}}$ and $\\lambda\\in[0,1]$ , each strategy modification $\\phi_{\\lambda,\\psi}$ interpolates the input strategy $x$ with the modified strategy $\\psi(x)$ : formally, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{Int},\\Psi}^{\\chi}(\\delta):=\\left\\{\\phi_{\\lambda,\\psi}(x):=(1-\\lambda)x+\\lambda\\psi(x):\\psi\\in\\Psi,\\lambda\\leq\\delta/D_{X}\\right\\}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Note that for any $\\psi\\ \\in\\ \\Psi$ and $\\begin{array}{l}{\\lambda\\ \\leq\\ \\frac{\\delta}{D_{\\mathcal{X}}}}\\end{array}$ , we have $\\|\\phi_{\\lambda,\\psi}(x)-x\\|\\ =\\ \\lambda\\|x-\\psi(x)\\|\\ \\leq\\ \\delta,$ , respecting the locality constraint. The induced $\\Phi_{\\mathrm{Int},\\Psi}^{\\mathcal{X}}(\\delta)$ -regret can be written as $\\mathrm{Reg}_{\\mathrm{Int},\\Psi,\\delta}^{T}\\,:=$ $\\begin{array}{r}{\\operatorname*{max}_{\\psi\\in\\Psi,\\lambda\\leq\\frac{\\delta}{D_{\\chi}}}\\sum_{t=1}^{T}\\big(f^{t}(x^{t})-f^{t}((1-\\lambda)x^{t}+\\lambda\\psi(x^{t}))\\big).}\\end{array}$ . To guarantee convergence to the corresponding $\\Phi$ -equilibrium, it suffices to minimize $\\Phi_{\\mathrm{Int},\\Psi}^{\\mathcal{X}}(\\delta)$ -regret against convex losses, which we show further reduces to $\\Psi$ -regret minimization against convex losses (Theorem 10 in Appendix G). ", "page_idx": 9}, {"type": "text", "text": "CCE-like Instantiation. In the special case where $\\Psi$ contains only constant strategy modifications (i.e., $\\psi(x)=x^{*}$ for all $x^{\\th}$ ), we get a coarse correlated equilibrium (CCE)-like instantiation of local equilibrium, which limits the gain by interpolating with any fixed strategy. We denote the resulting set of local strategy modification simply as $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\bar{\\delta})$ . We can apply any no-external regret algorithm for efficient $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -regret minimization and computation of $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}(\\delta)$ -equilibrium in the first-order stationary regime as summarized in Theorem 5. We also discuss faster convergence rates in the game setting in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5. For the Online G\u221aradient Descent algorithm (GD) [Zin03] with step size $\\begin{array}{r}{\\eta=\\frac{D_{\\chi}}{G\\sqrt{T}}}\\end{array}$ , its $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -trheeg i sa lagt ormitohstm $2\\delta G\\sqrt{T}$ .o oFtuh rtghaemrem, otrhee,i rf oer mapniry $\\delta>0$ s tarnibd uatinoyn $\\begin{array}{r}{\\varepsilon>\\frac{\\delta^{2}L}{2}}\\end{array}$ ,d  wsthreant eaglyl  pplraoyfeilerss $G D$ converges to an $\\begin{array}{r}{\\left(\\varepsilon+\\frac{\\delta^{2}L}{2}\\right)}\\end{array}$ -approximate $\\Phi_{\\mathrm{Int}}(\\delta)$ -equilibrium in ${\\cal O}(1/\\varepsilon^{2})$ iterations. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lower Bound in the Global Regime When $\\delta$ equals the diameter of our strategy set, it is NP-hard to compute an $\\varepsilon$ -approximate $\\Phi(\\delta)$ -equilibrium (for $\\Phi(\\delta)=\\Phi_{\\mathrm{Proj}}(\\delta),\\Phi_{\\mathrm{Int}}(\\delta))$ , even when $\\varepsilon=\\Theta(1)$ and $G,L=O(\\mathrm{poly}(d))$ . Moreover, given black-box access to value and gradient queries, finding such equilibria requires exponentially many queries in at least one of the parameters $d,G,L,1/\\varepsilon$ . These results are presented as Theorem 12 and Theorem 13 in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "More Efficient $\\Phi$ -Equilibria We include the discussion of another natural class of local strategy modifications that is based on beam search, where GD suffers linear regret to Appendix H. This result shows that even for simple local strategy modification sets $\\Phi(\\delta)$ , the landscape of efficient local $\\Phi(\\delta)$ -regret minimization is already quite rich and many questions remain open. A fruitful future direction is to identify more classes of $\\Phi$ that admit efficient regret minimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers for their constructive comments that improves the paper. Y.C. acknowledge the support from the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. W.Z. was supported by the NSF Awards CCF-1942583 (CAREER), CCF-2342642, and a Research Fellowship from the Center for Algorithms, Data, and Market Design at Yale (CADMY). H.L. was supported by NSF award IIS-1943607. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[AD54] Kenneth J Arrow and Gerard Debreu. \u201cExistence of an equilibrium for a competitive economy\u201d. In: Econometrica (1954), pp. 265\u2013290.   \n[AFS23] Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. \u201cNear-Optimal PhiRegret Learning in Extensive-Form Games\u201d. In: International Conference on Machine Learning (ICML). 2023.   \n[AGH19] Naman Agarwal, Alon Gonen, and Elad Hazan. \u201cLearning in non-convex games with an optimization oracle\u201d. In: Conference on Learning Theory. PMLR. 2019, pp. 18\u201329.   \n[AHK12] Sanjeev Arora, Elad Hazan, and Satyen Kale. \u201cThe multiplicative weights update method: a meta-algorithm and applications\u201d. In: Theory of computing 8.1 (2012), pp. 121\u2013164.   \n[ALW21] Jacob Abernethy, Kevin A Lai, and Andre Wibisono. \u201cLast-iterate convergence rates for min-max optimization: Convergence of hamiltonian gradient descent and consensus optimization\u201d. In: Algorithmic Learning Theory. PMLR. 2021, pp. 3\u201347.   \n[Ana+22a] Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Noah Golowich, and Tuomas Sandholm. \u201cNear-optimal no-regret learning for correlated equilibria in multi-player general-sum games\u201d. In: Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing (STOC). 2022.   \n[Ana+22b] Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng Luo, and Tuomas Sandholm. \u201cUncoupled Learning Dynamics with ${\\cal O}(\\log T)$ Swap Regret in Multiplayer Games\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.   \n[Aue $+02$ ] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. \u201cThe nonstochastic multiarmed bandit problem\u201d. In: SIAM journal on computing 32.1 (2002), pp. 48\u201377.   \n[AZ22] Amir Ali Ahmadi and Jeffrey Zhang. \u201cOn the complexity of finding a local minimizer of a quadratic function over a polytope\u201d. In: Mathematical Programming 195.1 (2022), pp. 783\u2013792.   \n[AZF19] Sergul Aydore, Tianhao Zhu, and Dean P Foster. \u201cDynamic local regret for non-convex online forecasting\u201d. In: Advances in neural information processing systems 32 (2019).   \n[Bai $+22$ ] Yu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. \u201cEfficient Phi-Regret Minimization in Extensive-Form Games via Online Mirror Descent\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 22313\u201322325.   \n[Ber+23] Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Francesco Trov\u00f2, and Nicola Gatti. \u201cConstrained Phi-Equilibria\u201d. In: International Conference on Machine Learning. 2023.   \n[Bla56] David Blackwell. \u201cAn analog of the minimax theorem for vector payoffs.\u201d In: Pacific Journal of Mathematics 6.1 (Jan. 1956). Publisher: Pacific Journal of Mathematics, A Non-profit Corporation, pp. 1\u20138.   \n[Bro51] George W Brown. \u201cIterative solution of games by fictitious play\u201d. In: Act. Anal. Prod Allocation 13.1 (1951), p. 374.   \n[Bub+15] S\u00e9bastien Bubeck et al. \u201cConvex optimization: Algorithms and complexity\u201d. In: Foundations and Trends\u00ae in Machine Learning 8.3-4 (2015), pp. 231\u2013357.   \n[Cau01] Robert Cauty. \u201cSolution du probl\u00e8me de point fixe de Schauder\u201d. en. In: Fundamenta Mathematicae 170 (2001). Publisher: Instytut Matematyczny Polskiej Akademii Nauk, pp. 231\u2013246.   \n[CDT09] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. \u201cSettling the complexity of computing two-player Nash equilibria\u201d. In: Journal of the ACM (JACM) 56.3 (2009), pp. 1\u201357.   \n[CL06] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[CP20] Xi Chen and Binghui Peng. \u201cHedging in games: Faster convergence of external and swap regrets\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 18990\u201318999.   \n[CZ23] Yang Cai and Weiqiang Zheng. \u201cAccelerated Single-Call Methods for Constrained Min-Max Optimization\u201d. In: International Conference on Learning Representations (ICLR) (2023).   \n$[\\mathrm{Dag}+24]$ Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. \u201cFrom External to Swap Regret 2.0: An Efficient Reduction for Large Action Spaces\u201d. In: Proceedings of the 56th Annual ACM Symposium on Theory of Computing. 2024, pp. 1216\u20131222.   \n[Das+23] Constantinos Daskalakis, Noah Golowich, Stratis Skoulakis, and Emmanouil Zampetakis. \u201cSTay-ON-the-Ridge: Guaranteed Convergence to Local Minimax Equilibrium in Nonconvex-Nonconcave Games\u201d. In: The Thirty Sixth Annual Conference on Learning Theory. PMLR. 2023, pp. 5146\u20135198.   \n[Das22] Constantinos Daskalakis. \u201cNon-Concave Games: A Challenge for Game Theory\u2019s Next 100 Years\u201d. In: Cowles Preprints (2022).   \n[DDJ21] Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. \u201cEfficient methods for structured nonconvex-nonconcave min-max optimization\u201d. In: International Conference on Artificial Intelligence and Statistics (2021).   \n[Deb52] Gerard Debreu. \u201cA social equilibrium existence theorem\u201d. In: Proceedings of the National Academy of Sciences 38.10 (1952), pp. 886\u2013893.   \n[DFG21] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. \u201cNear-optimal noregret learning in general games\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) (2021).   \n[DGP09] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. \u201cThe complexity of computing a Nash equilibrium\u201d. In: Communications of the ACM 52.2 (2009), pp. 89\u201397.   \n[DP18] Constantinos Daskalakis and Ioannis Panageas. \u201cThe limit points of (optimistic) gradient descent in min-max optimization\u201d. In: the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS). 2018.   \n[DSZ21] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. \u201cThe complexity of constrained min-max optimization\u201d. In: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC). 2021.   \n[Fan53] Ky Fan. \u201cMinimax theorems\u201d. In: Proceedings of the National Academy of Sciences 39.1 (1953), pp. 42\u201347.   \n[Far+22a] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. \u201cNear-optimal no-regret learning dynamics for general convex games\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 39076\u2013 39089.   \n$[\\mathrm{Far}+22\\mathrm{b}]$ Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. \u201cSimple Uncoupled No-regret Learning Dynamics for Extensive-form Correlated Equilibrium\u201d. In: Journal of the ACM 69.6 (2022).   \n[FCR20] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. \u201cImplicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 3133\u2013 3144.   \n[FR21] Tanner Fiez and Lillian J Ratliff. \u201cLocal convergence analysis of gradient descent ascent with finite timescale separation\u201d. In: Proceedings of the International Conference on Learning Representation. 2021.   \n[FS99] Yoav Freund and Robert E. Schapire. \u201cAdaptive Game Playing Using Multiplicative Weights\u201d. In: Games and Economic Behavior 29 (1999), pp. 79\u2013103.   \n[Geo63] George B. Dantzig. Linear Programming and Extensions. Princeton University Press, 1963.   \n[GGM08] Geoffrey J Gordon, Amy Greenwald, and Casey Marks. \u201cNo-regret learning in convex games\u201d. In: Proceedings of the 25th international conference on Machine learning. 2008, pp. 360\u2013367.   \n[GJ03] Amy Greenwald and Amir Jafari. \u201cA general class of no-regret learning algorithms and game-theoretic equilibria\u201d. In: Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings. Springer. 2003, pp. 2\u201312.   \n[Gli52] Irving L Glicksberg. \u201cA further generalization of the Kakutani fixed theorem, with application to Nash equilibrium points\u201d. In: Proceedings of the American Mathematical Society 3.1 (1952), pp. 170\u2013174.   \n[GLZ18] Xiand Gao, Xiaobo Li, and Shuzhong Zhang. \u201cOnline learning with non-convex losses and non-stationary regret\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2018, pp. 235\u2013243.   \n[GZL23] Ziwei Guan, Yi Zhou, and Yingbin Liang. \u201cOnline Nonconvex Optimization with Limited Instantaneous Oracle Feedback\u201d. In: The Thirty Sixth Annual Conference on Learning Theory. PMLR. 2023, pp. 3328\u20133355.   \n[Han57] James Hannan. \u201cApproximation to Bayes risk in repeated play\u201d. In: Contributions to the Theory of Games 3 (1957), pp. 97\u2013139.   \n$[\\mathrm{Hel}+20]$ Am\u00e9lie H\u00e9liou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. \u201cOnline non-convex optimization with imperfect feedback\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 17224\u201317235.   \n[HMC21a] Nadav Hallak, Panayotis Mertikopoulos, and Volkan Cevher. \u201cRegret minimization in stochastic non-convex learning via a proximal-gradient approach\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 4008\u20134017.   \n[HMC21b] Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. \u201cThe limits of min-max optimization algorithms: Convergence to spurious non-critical sets\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 4337\u20134348.   \n[HSZ17] Elad Hazan, Karan Singh, and Cyril Zhang. \u201cEfficient regret minimization in nonconvex games\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 1433\u20131441.   \n[JNJ20] Chi Jin, Praneeth Netrapalli, and Michael Jordan. \u201cWhat is local optimality in nonconvex-nonconcave minimax optimization?\u201d In: International conference on machine learning (ICML). PMLR. 2020, pp. 4880\u20134889.   \n[Kar14] Samuel Karlin. Mathematical Methods and Theory in Games, Programming, and Economics: Volume 2: The Theory of Infinite Games. Elsevier, 2014.   \n[Kar59] Samuel Karlin. Mathematical methods and theory in games, programming, and economics: Volume II: the theory of infinite games. Vol. 2. Addision-Wesley, 1959.   \n$[\\mathrm{Kri}{+}15]$ Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. \u201cThe hedge algorithm on a continuum\u201d. In: International Conference on Machine Learning. PMLR. 2015, pp. 824\u2013832.   \n[McK54] Lionel McKenzie. \u201cOn equilibrium in Graham\u2019s model of world trade and other competitive systems\u201d. In: Econometrica (1954), pp. 147\u2013161.   \n[MK87] Katta G. Murty and Santosh N. Kabadi. \u201cSome NP-complete problems in quadratic and nonlinear programming\u201d. en. In: Mathematical Programming 39.2 (June 1987), pp. 117\u2013129.   \n[MM10] Odalric-Ambrym Maillard and R\u00e9mi Munos. \u201cOnline learning in adversarial lipschitz environments\u201d. In: Joint european conference on machine learning and knowledge discovery in databases. Springer. 2010, pp. 305\u2013320.   \n[Mor+21a] Dustin Morrill, Ryan D\u2019Orazio, Reca Sarfati, Marc Lanctot, James R Wright, Amy R Greenwald, and Michael Bowling. \u201cHindsight and sequential rationality of correlated play\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 6. 2021, pp. 5584\u20135594.   \n[Mor+21b] Dustin Morrill, Ryan D\u2019Orazio, Marc Lanctot, James R Wright, Michael Bowling, and Amy R Greenwald. \u201cEfficient deviation types and learning for hindsight rationality in extensive-form games\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 7818\u20137828.   \n[MRS20] Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. \u201cOn gradient-based learning in continuous games\u201d. In: SIAM Journal on Mathematics of Data Science 2.1 (2020), pp. 103\u2013131.   \n[MV21] Oren Mangoubi and Nisheeth K Vishnoi. \u201cGreedy adversarial equilibrium: an efficient alternative to nonconvex-nonconcave min-max optimization\u201d. In: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021, pp. 896\u2013909.   \n[MZ19] Panayotis Mertikopoulos and Zhengyuan Zhou. \u201cLearning in games with continuous action sets and unknown payoff functions\u201d. In: Mathematical Programming 173 (2019), pp. 465\u2013507.   \n[Nas50] John F Nash Jr. \u201cEquilibrium points in n-person games\u201d. In: Proceedings of the national academy of sciences 36.1 (1950), pp. 48\u201349.   \n[Neu28] J v. Neumann. \u201cZur theorie der gesellschaftsspiele\u201d. In: Mathematische annalen 100.1 (1928), pp. 295\u2013320.   \n$[\\mathrm{Pet}+22]$ Thomas Pethick, Puya Latafat, Panagiotis Patrinos, Olivier Fercoq, and Volkan Cevher\u00e5. \u201cEscaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems\u201d. In: International Conference on Learning Representations (ICLR). 2022.   \n$[\\mathrm{Pil}+22]$ Georgios Piliouras, Mark Rowland, Shayegan Omidshafiei, Romuald Elie, Daniel Hennes, Jerome Connor, and Karl Tuyls. \u201cEvolutionary dynamics and phi-regret minimization in games\u201d. In: Journal of Artificial Intelligence Research 74 (2022), pp. 1125\u2013 1158.   \n[PR24] Binghui Peng and Aviad Rubinstein. \u201cFast swap regret minimization and applications to approximate correlated equilibria\u201d. In: Proceedings of the 56th Annual ACM Symposium on Theory of Computing. 2024, pp. 1223\u20131234.   \n[RBS16] Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. \u201cOn the characterization of local Nash equilibria in continuous games\u201d. In: IEEE transactions on automatic control 61.8 (2016), pp. 2301\u20132307.   \n[Rob51] Julia Robinson. \u201cAn iterative method of solving a game\u201d. In: Annals of mathematics (1951), pp. 296\u2013301.   \n[Ros65] J Ben Rosen. \u201cExistence and uniqueness of equilibrium points for concave n-person games\u201d. In: Econometrica (1965), pp. 520\u2013534.   \n[RS13] Sasha Rakhlin and Karthik Sridharan. \u201cOptimization, learning, and games with predictable sequences\u201d. In: Advances in Neural Information Processing Systems (2013).   \n[RST11] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. \u201cOnline learning: Beyond regret\u201d. In: Proceedings of the 24th Annual Conference on Learning Theory. JMLR Workshop and Conference Proceedings. 2011, pp. 559\u2013594.   \n[Sha24] Dravyansh Sharma. \u201cNo Internal Regret with Non-convex Loss Functions\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 13. 2024, pp. 14919\u2013 14927.   \n[Sio58] Maurice Sion. \u201cOn general minimax theorems.\u201d In: Pacific J. Math. 8.4 (1958), pp. 171\u2013 176.   \n[SL07] Gilles Stoltz and G\u00e1bor Lugosi. \u201cLearning correlated equilibria in games with compact sets of strategies\u201d. In: Games and Economic Behavior 59.1 (2007), pp. 187\u2013208.   \n[SMB22] Ziang Song, Song Mei, and Yu Bai. \u201cSample-efficient learning of correlated equilibria in extensive-form games\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 4099\u20134110.   \n[SN20] Arun Sai Suggala and Praneeth Netrapalli. \u201cOnline non-convex learning: Following the perturbed leader is optimal\u201d. In: Algorithmic Learning Theory. PMLR. 2020, pp. 845\u2013 861.   \n[Syr+15] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. \u201cFast convergence of regularized learning in games\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) (2015).   \n[VF08] Bernhard Von Stengel and Fran\u00e7oise Forges. \u201cExtensive-form correlated equilibrium: Definition and computational complexity\u201d. In: Mathematics of Operations Research 33.4 (2008), pp. 1002\u20131022.   \n[WZB20] Yuanhao Wang, Guodong Zhang, and Jimmy Ba. \u201cOn Solving Minimax Optimization Locally: A Follow-the-Ridge Approach\u201d. In: International Conference on Learning Representations (ICLR). 2020.   \n$[Z\\mathrm{ha}+24]$ Brian Hu Zhang, Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. \u201cEfficient $\\Phi$ -Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games\u201d. In: Advances in Neural Information Processing Systems. 2024.   \n[Zin03] Martin Zinkevich. \u201cOnline convex programming and generalized infinitesimal gradient ascent\u201d. In: Proceedings of the 20th international conference on machine learning (ICML). 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 Introduction   \n1.1 Contributions 2   \nPreliminaries 5   \n3 Tractable $\\Phi$ -Equilibrium for Finite $\\Phi$ via Sampling 5   \n3.1 Proof of Theorem 2 7   \nApproximate $\\Phi$ -Equilibria under Infinite Local Strategy Modifications 8   \n4.1 Projection-Based Local Strategy Modifications . . 9   \n4.2 Convex Combination of Finite Local Strategy Modifications 10   \n4.3 Interpolation-Based Local Strategy Modifications . 10   \n5 Discussion and Future Directions 10   \nA Related Work 16   \nB Additional Preliminaries: Solution Concepts in Non-Concave Games 17   \nC Rerget Bound for Hedge 19   \nD Proof of Lemma 1 19   \nE Missing Details in Section 4.1 19   \nE.1 Differences between External Regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}$ -regret . . . 19   \nE.2 Proof of Theorem 3 . 20   \nE.3 Lower bounds for \u03a6PXroj-Regret . . 21   \nE.3.1 Proof of Theorem 7 . 21   \nE.3.2 Proof of Proposition 1 22   \nE.4 Improved $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -Regret in the Game Setting and Proof of Theorem 9 22   \nE.4.1 Proof of Theorem 8 . 23   \nE.4.2 Proof of Theorem 9 . 23   \nF Missing Details in Section 4.2 24   \nF.1 Proof of Theorem 4 24   \nG Missing details in Section 4.3 26   \nH Beam-Search Local Strategy Modifications and Local Equilibria 27   \nH.1 Proof of Theorem 11 28   \nHardness in the Global Regime 28   \nI.1 Proof of Theorem 13 29 ", "page_idx": 15}, {"type": "text", "text": "J Removing the $D$ dependence for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}$ -regret ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "J.1 One-Dimensional Case 30   \nJ.2 $d$ -Dimensional Box Case 31 ", "page_idx": 16}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Non-Concave Games. An important special case of multi-player games are two-player zero-sum games, which are defined in terms of some function $f:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ that one of the two players say the one choosing $x\\in\\mathscr{X}$ , wants to minimize, while the other player, the one choosing $y\\in\\mathcal{V}$ , wants to maximize. Finding Nash equilibrium in such games is tractable in the convex-concave setting, i.e. when $f(x,y)$ is convex with respect to the minimizing player\u2019s strategy, $x$ , and concave with respect to the maximizing player\u2019s strategy, $y$ , but it is computationally intractable in the general nonconvex-nonconcave setting. Namely, a Nash equilibrium may not exist, and it is NP-hard to determine if one exists and, if so, find it. Moreover, in this case, stable limit points of gradientbased dynamics are not necessarily Nash equilibria, not even local Nash equilibria [DP18; MRS20]. Moreover, there are examples including the \u201cPolar Game\u201d $[\\mathrm{Pet}+22]$ and the \u201cForsaken Matching Pennies\u201d [HMC21b] showing that for GD / OG and many other no-regret learning algorithms in nonconvex-nonconcave min-max optimization, the last-iterate does not converge and even the averageiterate fails to be a stationary point. We emphasize that the convergence guarantees we provide for GD / OG in Section 4.1 and Section 4.3 holds for the empirical distribution of play, not the average-iterate or the last-iterate. ", "page_idx": 16}, {"type": "text", "text": "A line of work focuses on computing Nash equilibrium under additional structure in the game. This encompasses settings where the game satisfies the (weak) Minty variational inequality [MZ19; DDJ21; $\\mathrm{Pet}{+22}$ ; CZ23], or is sufficiently close to being bilinear [ALW21]. However, the study of universal solution concepts in the nonconvex-nonconcave setting is sparse. Daskalakis, Skoulakis, and Zampetakis [DSZ21] proved the existence and computational hardness of local Nash equilibrium. In a more recent work, $[\\mathrm{Das}{+}23]$ proposes second-order algorithms with asymptotic convergence to local Nash equilibrium. Several works study sequential two-player zero-sum games with additional assumptions about the player who goes second. They propose equilibrium concepts such as local minimax points [JNJ20], differentiable Stackelberg equilibrium [FCR20], and greedy adversarial equilibrium [MV21]. Notably, local minimax points are stable limit points of Gradient-DescentAscent (GDA) dynamics [JNJ20; WZB20; FR21] while greedy adversarial equilibrium can be computed efficiently using second-order algorithms in the unconstrained setting [MV21]. In contrast to these studies, we focus on the more general case of multi-player non-concave games. ", "page_idx": 16}, {"type": "text", "text": "Local Equilibrium. To address the limitations associated with classical, global equilibrium concepts, a natural approach is to focus on developing equilibrium concepts that guarantee local stability instead. One definition of interest is the strict local Nash equilibrium, wherein each player\u2019s strategy corresponds to a local maximizer of their utility function, given the other players\u2019 strategies. Unfortunately, a strict local Nash equilibrium may not always exist, as demonstrated in Example 1. Furthermore, a weaker notion\u2014the second-order local Nash equilibrium, where each player has no incentive to deviate based on the second-order Taylor expansion estimate of their utility, is also not guaranteed to exist as illustrated in Example 1. What\u2019s more, it is NP-hard to check whether a given strategy proflie is a strict local Nash equilibrium or a second-order local Nash equilibrium, as implied by the result of Murty and Kabadi [MK87] and Ahmadi and Zhang [AZ22].6 Finally, one can consider local Nash equilibrium, a first-order stationary solution, which is guaranteed to exist [DSZ21]. Unlike non-convex optimization, where targeting first-order local optima sidesteps the intractability of global optima, this first-order local Nash equilibrium has been recently shown to be intractable, even in two-player zero-sum non-concave games with joint feasibility constraints [DSZ21].7 See Table 1 for a summary of solution concepts in non-concave games. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Online Learning with Non-Convex Losses. A line of work has studied online learning against non-convex losses. To circumvent the computational intractability of this problem, various approaches have been pursued: some works assume a restricted set of non-convex loss functions [GLZ18], while others assume access to a sampling oracle [MM10; $\\mathrm{Kri}{+15}$ ] or access to an offline optimization oracle [AGH19; SN20; $\\mathrm{Hel}+20_{-}$ or a weaker notion of regret [HSZ17; AZF19; HMC21a; GZL23]. The work most closely related to ours is [HSZ17]. The authors propose a notion of $w$ -smoothed local regret against non-convex losses, and they also define a local equilibrium concept for non-concave games. They use the idea of smoothing to average the loss functions in the previous $w$ iterations and design algorithms with optimal $w$ -smoothed local regret. The concept of regret they introduce suggests a local equilibrium concept. However, their local equilibrium concept is non-standard in that its local stability is not with respect to a distribution over strategy profiles sampled by this equilibrium concept. Moreover, the path to attaining this local equilibrium through decentralized learning dynamics remains unclear. The algorithms provided in [HSZ17; GZL23] require that every agent $i$ experiences (over several rounds) the average utility function of the previous $w$ iterates, denoted as $\\begin{array}{r}{F_{i,w}^{t}:=\\frac{1}{w}\\sum_{\\ell=0}^{w-1}u_{i}^{t-\\ell}(\\cdot,x_{-i}^{t-\\ell})}\\end{array}$ . Implementing this imposes a significant coordination burden on the agents. In contrast, we focus on a natural concept of $\\Phi(\\delta)$ -equilibrium, which is incomparable to that of [HSZ17], and we also show that efficient convergence to this concept is achieved via decentralized gradient-based learning dynamics. ", "page_idx": 17}, {"type": "text", "text": "$\\Phi$ -regret and $\\Phi$ -equilibrium. The concept of $\\Phi$ -regret and the associated $\\Phi$ -equilibrium is introduced by Greenwald and Jafari [GJ03] and has been broadly investigated in the context of concave games [GJ03; SL07; GGM08; RST11; $\\mathrm{Pil}+22$ ; Ber $+23$ ; $\\mathrm{Dag}{+}24$ ; PR24] and extensiveform games [VF08; Mor+21a; Mor+21b; $\\mathrm{Far}{+}22\\mathrm{b}$ ; Ba $^{+22}$ ; SMB22; AFS23; $Z\\mathrm{ha}{+24}]$ . The work of [Sha24] studies internal regret minimization against non-convex losses. To our knowledge, no efficient algorithm exists for the classes of $\\Phi$ -equilibria we consider for non-concave games. Specifically, all of these algorithms, when applied to compute a $\\left(\\varepsilon,\\Phi(\\delta)\\right)$ -equilibrium for a general $\\delta$ -local strategy modification set $\\Phi(\\delta)$ (using Lemma 1), require running time exponential in either $1/\\varepsilon$ or the dimension $d$ . In contrast, we show that for several natural choices of $\\Phi(\\delta)$ , $\\varepsilon$ -approximate $\\Phi(\\delta)$ -equilibrium can be computed efficiently, i.e. polynomial in $1/\\varepsilon$ and $d$ , using simple algorithms. ", "page_idx": 17}, {"type": "text", "text": "B Additional Preliminaries: Solution Concepts in Non-Concave Games ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present definitions of several solution concepts in the literature as well as the existence and computational complexity of each solution concept. ", "page_idx": 17}, {"type": "text", "text": "Definition 4 (Nash Equilibrium). In a continuous game, a strategy profile $\\textstyle x\\in\\prod_{j=1}^{n}{\\mathcal{X}}_{j}$ is a Nash equilibrium (NE) if and only if for every player $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{i}(x_{i}^{\\prime},x_{-i})\\leq u_{i}(x),\\forall x_{i}^{\\prime}\\in\\mathcal{X}_{i}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition 5 (Mixed Nash Equilibrium). In a continuous game, a mixed strategy profile $p~\\in$ $\\textstyle\\prod_{j=1}^{n}\\Delta(\\mathcal{X}_{j})$ (here we denote $\\Delta(\\mathcal X_{i})$ as the set of probability measures over $\\mathbf{\\mathcal{X}}_{i}$ ) is $a$ mixed Nash equilibrium (MNE) if and only if for every player $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{i}(p_{i}^{\\prime},p_{-i})\\leq u_{i}(p),\\forall p_{i}^{\\prime}\\in\\Delta(\\mathcal{X}_{i})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition 6 ((Coarse) Correlated Equilibrium). In a continuous game, a distribution $\\sigma$ over joint strategy profiles $\\Pi_{i=1}^{n}\\chi_{i}$ is $a$ correlated equilibrium (CE) if and only if for all player $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{i}:\\mathcal{X}_{i}\\to\\mathcal{X}_{i}}\\mathbb{E}_{x\\sim\\sigma}[u_{i}(\\phi_{i}(x_{i}),x_{-i})]\\le\\mathbb{E}_{x\\sim\\sigma}[u_{i}(x)].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, a distribution $\\sigma$ over joint strategy profiles $\\Pi_{i=1}^{n}\\chi_{i}$ is $a$ coarse correlated equilibrium (CCE) if and only if for all player $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x_{i}^{\\prime}\\in\\mathcal{X}_{i}}\\mathbb{E}_{x\\sim\\sigma}[u_{i}(x_{i}^{\\prime},x_{-i})]\\le\\mathbb{E}_{x\\sim\\sigma}[u_{i}(x)].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition 7 (Strict Local Nash Equilibrium). In a continuous game, a strategy proflie $\\textstyle x\\in\\prod_{j=1}^{n}{\\mathcal{X}}_{j}$ is $a$ strict local Nash equilibrium if and only if for every player $i\\in[n]$ , there exists $\\delta>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{i}(x_{i}^{\\prime},x_{-i})\\leq u_{i}(x),\\forall x_{i}^{\\prime}\\in B_{d_{i}}(x_{i},\\delta)\\cap\\mathcal{X}_{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition 8 (Second-order Local Nash Equilibrium). Consider a continuous game where each utility function $u_{i}(x_{i},x_{-i})$ is twice-differentiable with respect to $x_{i}$ for any fixed $x_{-i}$ . $A$ strategy profile $\\textstyle x\\in\\prod_{j=1}^{n}{\\dot{X}}_{j}$ is $a$ second-order local Nash equilibrium if and only if for every player $i\\in[n]$ , $x_{i}$ maximizes the second-order Taylor expansion of its utility functions at $x_{i}$ , or formally, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{x_{i}}u_{i}(x),x_{i}^{\\prime}-x_{i}\\rangle+(x_{i}^{\\prime}-x_{i})^{\\top}\\nabla_{x_{i}}^{2}u_{i}(x)(x_{i}^{\\prime}-x_{i})\\leq0,\\forall x_{i}^{\\prime}\\in\\mathcal{X}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Existence Mixed Nash equilibria exist in continuous games, thus smooth games [Deb52; Gli52; Fan53]. By definition, an MNE is also a CE and a CCE. This also proves the existence of CE and CCE. In contrast, strict local Nash equilibria, second-order Nash equilibria, or (pure) Nash equilibria may not exist in a smooth non-concave game, as we show in the following example. ", "page_idx": 18}, {"type": "text", "text": "Example 1. Consider a two-player zero-sum non-concave game: the action sets are $\\mathcal{X}_{1}=\\mathcal{X}_{2}=$ $[-1,1]$ and the utility functions are $u_{1}(x_{1},x_{2})=-u_{2}(x_{1},\\bar{x_{2}})=(x_{1}-x_{2})^{2}$ . Let $x=(x_{1},x_{2})\\in$ $\\mathcal{X}_{1}\\times\\mathcal{X}_{2}$ be any strategy proflie: if $x_{1}=x_{2}$ , then player $^{\\,l}$ is not at a local maximizer; if $x_{1}\\neq x_{2}$ , then player 2 is not at a local maximizer. Thus $x$ is not a strict local Nash equilibrium. Since the utility function is quadratic, we conclude that the game also has no second-order local Nash equilibrium. ", "page_idx": 18}, {"type": "text", "text": "Computational Complexity Consider a single-player smooth non-concave game with a quadratic utility function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$ . The problem of finding a local maximizer of $f$ can be reduced to the problem of computing a NE, a MNE, a CE, a CCE, a strict local Nash equilibrium, or a second-order local Nash equilibrium. Since computing a local maximizer or checking if a given point is a local maximizer is NP-hard [MK87], we know that the computational complexities of NE, MNE, CE, CCE, strict local Nash equilibria, and second-order local Nash equilibria are all NP-hard. ", "page_idx": 18}, {"type": "text", "text": "Representation Complexity Karlin [Kar59] present a two-player zero-sum non-concave game whose unique MNE has infinite support. Since in a two-player zero-sum game, the marginal distribution of a CE or a CCE is an MNE, it also implies that the representation complexity of any CE or CCE is infinite. We present the example in Karlin [Kar59] here for completeness and also prove that the game is Lipschitz and smooth. ", "page_idx": 18}, {"type": "text", "text": "Example 2 ([Kar59, Chapter 7.1, Example 3]). We consider a two-player zero-sum game with action sets $\\mathcal{X}_{1}=\\mathcal{X}_{2}=[0,1]$ . Let p and $q$ be two distributions over [0, 1]. The only requirement for p and $q$ is that their cumulative distribution functions are not finite-step functions. For example, we can take $p=q$ to be the uniform distribution. ", "page_idx": 18}, {"type": "text", "text": "Let $\\mu_{n}$ and $\\nu_{n}$ denote the $n$ -th moments of $p$ and $q_{:}$ , respectively. Define the utility function ", "page_idx": 18}, {"type": "equation", "text": "$$\nu(x,y)=u_{1}(x,y)=-u_{2}(x,y)=\\sum_{n=0}^{\\infty}\\frac{1}{2^{n}}(x^{n}-\\mu_{n})(y^{n}-\\nu_{n}),\\quad0\\leq x,y\\leq1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Claim 1. The game in Example 2 is 2-Lipschitz and 6-smooth, and $(p,q)$ is its unique (mixed) Nash equilibrium. ", "page_idx": 18}, {"type": "text", "text": "Proof. Fix any $y\\in[0,1]$ , since $\\begin{array}{r}{|\\frac{1}{2^{n}}(y^{n}-\\nu_{n})n x^{n-1}|\\,\\leq\\,\\frac{n}{2^{n}}}\\end{array}$ , the series of $\\nabla_{x}u(x,y)$ is uniformly convergent. We have $\\begin{array}{r}{|\\nabla_{x}u(x,y)|\\leq\\sum_{n=0}^{\\infty}\\frac{n}{2^{n}}\\leq2,\\quad y\\in[0,1]}\\end{array}$ . Similarly, we have $|\\nabla_{x}^{2}u(x,y)|\\leq$ $\\textstyle\\sum_{n=0}^{\\infty}{\\frac{n^{2}}{2^{n}}}\\leq6$ for all $y\\in[0,1]$ . By symmetry, we also have $|\\nabla_{y}(x,y)|\\le2$ and $|\\nabla_{y}^{2}(x,y)|\\leq6$ for all $x,y\\in[0,1]$ . Thus, the game is 2-Lispchitz and 6-smooth. ", "page_idx": 18}, {"type": "text", "text": "Since $\\begin{array}{r}{|\\frac{1}{2^{n}}(x^{n}-\\mu_{n})(y^{n}-\\nu_{n})|\\leq\\frac{1}{2^{n}}}\\end{array}$ , the series of $u(x,y)$ is absolutely and uniformly convergent. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\int_{0}^{1}u(x,y)\\mathrm{d}F_{p}(x)=\\sum_{n=0}^{\\infty}\\frac{1}{2^{n}}(y^{n}-\\nu_{n})\\int_{0}^{1}(x^{n}-\\mu_{n})\\mathrm{d}F_{p}(x)\\equiv0,}}\\\\ {\\displaystyle{\\int_{0}^{1}u(x,y)F_{q}(y)=\\sum_{n=0}^{\\infty}\\frac{1}{2^{n}}(x^{n}-\\mu_{n})\\int_{0}^{1}(y^{n}-\\nu_{n})\\mathrm{d}F_{q}(y)\\equiv0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, $(p,q)$ is a mixed Nash equilibrium, and the value of the game is 0. Suppose $(p^{\\prime},q^{\\prime})$ is also a mixed Nash equilibrium. Then $(p,q^{\\prime})$ is a mixed Nash equilibrium. Note that $p$ supports on every point in $[0,1]$ . As a consequence, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n0\\equiv\\int_{0}^{1}u(x,y)\\mathrm{d}F_{q^{\\prime}}(y)=\\sum_{n=0}^{\\infty}\\frac{1}{2^{n}}(x^{n}-\\mu_{n})(\\nu_{n}^{\\prime}-\\nu_{n})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $x\\in[0,1]$ , where $\\nu_{n}^{\\prime}$ is the $n$ -th moment of $q^{\\prime}$ . Since the series vanished identically, the coefficients of each power of $x$ must vanish. Thus $\\nu_{n}^{\\prime}=\\nu_{n}$ and $q^{\\prime}=q$ . Similarly, we have $\\bar{p^{\\prime}}=p$ , and the mixed Nash equilibrium is unique. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Rerget Bound for Hedge ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 6 ([AHK12]). In an $N$ -expert problem, assume all the rewards are bounded, i.e., $u^{t}\\in$ [\u2212M, M]N, then the Hedge algorithm with step size \u03b7 = min{ M1 , Mlo\u221ag TN } has regret ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p\\in\\Delta(N)}\\sum_{t=1}^{T}\\left\\langle u^{t},p\\right\\rangle-\\sum_{t=1}^{T}\\left\\langle u^{t},p^{t}\\right\\rangle\\leq2M\\sqrt{T\\log N}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D Proof of Lemma 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let $\\{f^{t}\\}_{t\\in[T]}$ be a sequence of non-convex $L$ -smooth loss functions satisfying Assumption 1. Let $\\{x^{t}\\}_{t\\in[T]}$ be the iterates produced by $\\boldsymbol{\\mathcal{A}}$ against $\\{f^{t}\\}_{t\\in[T]}$ . Then $\\{x^{t}\\}_{t\\in[T]}$ is also the iterates produced by $\\boldsymbol{\\mathcal{A}}$ against a sequence of linear loss functions $\\{\\langle\\nabla f^{t}(x^{t}),\\cdot\\rangle\\}$ . For the latter, we know ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi\\in\\Phi^{\\mathcal{X}}(\\delta)}\\sum_{t=1}^{T}\\left\\langle\\nabla f^{t}(x^{t}),x^{t}-\\phi(x^{t})\\right\\rangle\\leq\\mathrm{Reg}_{\\Phi^{\\mathcal{X}}(\\delta)}^{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then using $L$ -smoothness of $\\{f^{t}\\}$ and the fact that $\\|\\phi(x)-x\\|\\leq\\delta$ for all $\\phi\\in\\Phi(\\delta)$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\phi\\in\\Phi^{\\mathcal{X}}(\\delta)}\\displaystyle\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(\\phi(x^{t}))\\leq\\displaystyle\\operatorname*{max}_{\\phi\\in\\Phi^{\\mathcal{X}}(\\delta)}\\displaystyle\\sum_{t=1}^{T}\\left(\\left\\langle\\nabla f^{t}(x^{t}),x^{t}-\\phi(x^{t})\\right\\rangle+\\frac{L}{2}\\|x^{t}-\\phi(x^{t})\\|^{2}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathrm{Reg}_{\\Phi^{\\mathcal{X}}(\\delta)}^{T}+\\displaystyle\\frac{\\delta^{2}L T}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the proof of the first part. ", "page_idx": 19}, {"type": "text", "text": "Let each player $i\\in[n]$ employ algorithm $\\boldsymbol{\\mathcal{A}}$ in a smooth game independently and produces iterates $\\{x^{t}\\}$ . The averaged joint strategy profile $\\sigma^{T}$ that chooses $x^{t}$ uniformly at random from $t\\,\\in\\,[T]$ satisfies for any player $i\\in[n]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi\\in\\Phi^{\\mathcal{X}_{i}}(\\delta)}{\\operatorname*{max}}\\mathbb{E}_{x\\sim\\sigma}\\big[u_{i}(\\phi(x_{i}),x_{-i})\\big]-\\mathbb{E}_{x\\sim\\sigma}\\big[u_{i}(x)\\big]}\\\\ &{=\\underset{\\phi\\in\\Phi^{\\mathcal{X}_{i}}(\\delta)}{\\operatorname*{max}}\\frac{1}{T}\\sum_{t=1}^{T}\\big(u_{i}\\big(\\phi(x_{i}^{t}),x_{-i}^{t}\\big)-u_{i}(x^{t})\\big)}\\\\ &{\\leq\\frac{\\mathrm{Re}\\mathrm{g}_{\\Phi^{\\mathcal{X}_{i}}(\\delta)}^{T}}{T}+\\frac{\\delta^{2}L}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus $\\sigma^{T}$ is a $\\begin{array}{r}{(\\operatorname*{max}_{i\\in[n]}\\{\\mathrm{Reg}_{\\Phi^{\\mathcal{X}_{i}}(\\delta)}^{T}\\}\\cdot T^{-1}+\\frac{\\delta^{2}L}{2})}\\end{array}$ -approximate $\\Phi(\\delta))$ -equilibrium. This completes the proof of the second part. ", "page_idx": 19}, {"type": "text", "text": "E Missing Details in Section 4.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Differences between External Regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}$ -regret ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the following two examples, we show that $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is incomparable with external regret for convex loss functions . A sequence of actions may suffer high ${\\mathrm{Reg}}^{T}$ but low ${\\mathrm{Reg}}_{\\mathrm{Proj},\\delta}^{T}$ (Example 3), and vise versa (Example 4). ", "page_idx": 19}, {"type": "text", "text": "Example 3. Let $f^{1}(x)\\,=\\,f^{2}(x)\\,=\\,|x|$ for $x\\,\\in\\,\\mathcal{X}\\,=\\,[-1,1]$ . Then the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret of the sequence $\\{x^{1}\\,=\\,{\\textstyle{\\frac{1}{2}}},x^{2}\\,=\\,-{\\textstyle{\\frac{1}{2}}}\\}$ for any $\\delta\\,\\in\\,(0,{\\frac{1}{2}})$ is 0. However, the external regret of the same sequence is 1. By repeating the construction for $\\textstyle{\\frac{T}{2}}$ times, we conclude that there exists a sequence of actions with $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}=0$ and $\\begin{array}{r}{\\mathrm{Reg}^{T}=\\frac{T}{2}}\\end{array}$ for all $T\\geq2$ . ", "page_idx": 20}, {"type": "text", "text": "Example 4. Let $f^{1}(x)=-2x$ and $f^{2}(x)=x$ for $x\\in\\mathcal{X}=[-1,1]$ . Then the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret of the sequence $\\{x^{1}={\\textstyle{\\frac{1}{2}}},x^{2}=0\\}$ for any $\\delta\\in(0,\\frac{1}{2})$ is $\\delta$ . However, the external regret of the same sequence is 0. By repeating the construction for $\\textstyle{\\frac{T}{2}}$ times, we conclude that there exists a sequence of actions with RegTProj,\u03b4 = \u03b42T and $\\mathrm{Reg}^{T}=0$ for all $T\\geq2$ . ", "page_idx": 20}, {"type": "text", "text": "At a high level, the external regret competes against a fixed action, whereas $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is more akin to the notion of dynamic regret, competing with a sequence of varying actions. When the environment is stationary, i.e., $f^{t}=f$ (Example 3), a sequence of actions that are far away from the global minimum must suffer high regret, but may produce low $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret since the change to the cumulative loss caused by a fixed-direction deviation could be neutralized across different actions in the sequence. In contrast, in a non-stationary (dynamic) environment (Example 4), every fixed action performs poorly, and a sequence of actions could suffer low regret against a fixed action but the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\bar{\\delta})$ -regret that competes with a fixed-direction deviation could be large. Nevertheless, despite these differences between the two notions of regret as shown above, they are compatible for convex loss functions: our main results in this section provide algorithms that minimize external regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret simultaneously. ", "page_idx": 20}, {"type": "text", "text": "E.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Let us denote $v\\in B_{d}(\\delta)$ a fixed deviation and define $p^{t}=\\Pi_{\\mathcal{X}}[x^{t}-v]$ . By standard analysis of GD [Zin03] (see also the proof of $[\\mathrm{Bub}+15$ , Theorem 3.2] ), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\big(f^{t}(x^{t})-f^{t}(p^{t})\\big)\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta}\\left(\\left\\|x^{t}-p^{t}\\right\\|^{2}-\\left\\|x^{t+1}-p^{t}\\right\\|^{2}+\\eta^{2}\\right\\|\\nabla f^{t}(x^{t})\\right\\|^{2}\\!\\right)}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T-1}\\frac{1}{2\\eta}\\left(\\left\\|x^{t+1}-p^{t+1}\\right\\|^{2}-\\left\\|x^{t+1}-p^{t}\\right\\|^{2}\\right)+\\frac{\\delta^{2}}{2\\eta}+\\frac{\\eta}{2}G^{2}T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step uses $\\|x^{1}-p^{1}\\|\\leq\\delta$ and $\\|\\nabla f^{t}(x^{t})\\|\\leq G$ . Here the terms $\\left\\|{\\boldsymbol{x}}^{t+1}-{\\boldsymbol{p}}^{t+1}\\right\\|^{2}-$ $\\left\\|\\boldsymbol{x}^{t+1}-\\boldsymbol{p}^{t}\\right\\|^{2}$ do not telescope, and we further relax them in the following key step. ", "page_idx": 20}, {"type": "text", "text": "Key Step: We relax the first term as: ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|x^{t+1}-p^{t+1}\\right|\\right|^{2}-\\left\\|x^{t+1}-p^{t}\\right\\|^{2}=\\left\\langle p^{t}-p^{t+1},2x^{t+1}-p^{t}-p^{t+1}\\right\\rangle}&{}\\\\ {=\\left\\langle p^{t}-p^{t+1},2x^{t+1}-2p^{t+1}\\right\\rangle-\\left\\|p^{t}-p^{t+1}\\right\\|^{2}}&{}\\\\ {=2\\big\\langle p^{t}-p^{t+1},v\\big\\rangle+2\\big\\langle p^{t}-p^{t+1},x^{t+1}-v-p^{t+1}\\big\\rangle-\\left\\|p^{t}-p^{t+1}\\right\\|}&{}\\\\ {\\leq2\\big\\langle p^{t}-p^{t+1},v\\big\\rangle-\\left\\|p^{t}-p^{t+1}\\right\\|^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last inequality we use the fact that $p^{t+1}$ is the projection of ${\\boldsymbol{x}}^{t+1}-{\\boldsymbol{v}}$ onto $\\mathcal{X}$ and $p^{t}$ is in $\\mathcal{X}$ . Now we get a telescoping term $2\\langle p^{t}-p^{t+1},u\\rangle$ and a negative term $-\\left\\|p^{t}-p^{t+1}\\right\\|^{2}$ . The negative term is useful for improving the regret analysis in the game setting, but we ignore it for now. Combining the two inequalities above, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left(f^{t}(x^{t})-f^{t}(p^{t})\\right)\\leq\\frac{\\delta^{2}}{2\\eta}+\\frac{\\eta}{2}G^{2}T+\\frac{1}{\\eta}\\sum_{t=1}^{T-1}\\left\\langle p^{t}-p^{t+1},v\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{\\delta^{2}}{2\\eta}+\\frac{\\eta}{2}G^{2}T+\\frac{1}{\\eta}\\langle p^{1}-p^{T},v\\rangle\\leq\\frac{\\delta^{2}}{2\\eta}+\\frac{\\eta}{2}G^{2}T+\\frac{\\delta D_{\\mathcal{X}}}{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the above holds for any $v$ with $\\lVert v\\rVert\\leq\\delta$ , it also upper bounds ${\\mathrm{Reg}}_{\\mathrm{Proj},\\delta}^{T}$ . ", "page_idx": 20}, {"type": "text", "text": "E.3 Lower bounds for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}$ -Regret ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 7 (Lower bound for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret against convex losses). For any $T\\geq1$ , $D_{\\mathcal{X}}>0$ $0\\ <\\ \\delta\\ \\leq\\ D_{\\mathcal{X}}$ , and $G~\\ge~0$ , there exists a distribution $\\mathcal{D}$ on $G$ -Lipschitz linear loss functions $f^{1},\\ldots,f^{T}$ over $\\mathcal{X}=[-D_{\\mathcal{X}},D_{\\mathcal{X}}]$ such that for any online algorithm, its $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret on the loss sequence satisfies $\\mathbb{E}_{\\mathcal{D}}[\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}]=\\Omega(\\delta G\\sqrt{T})$ . ", "page_idx": 21}, {"type": "text", "text": "Remark 2. A keen reader may notice that the $\\Omega(G\\delta\\sqrt{T})$ lower bound in Theorem 7 does not match the $O(G\\sqrt{\\delta D_{\\mathcal{X}}T})$ upper bound in Theorem $3$ , especially when $D_{\\mathcal{X}}\\gg\\delta$ . A natural question is: which of them is tight? We conjecture that the lower bound is tight. In fact,\u221a for the special case where the feasible set $\\mathcal{X}$ is a box, we obtain a $D_{\\mathcal{X}}$ -independent bound $O(d^{\\frac{1}{4}}G\\delta\\sqrt{T})$ using a modified version of $G D$ , which is tight when $d=1$ . See Appendix $J$ for a detailed discussion. ", "page_idx": 21}, {"type": "text", "text": "This lower bound suggests that GD achieves near-optimal $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret for convex losses. For $L$ -smooth non-convex loss functions, we provide another $\\Omega(\\delta^{2}L T)$ lower bound for algorithms that satisfy the linear span assumption. The linear span assumption states that the algorithm produces $\\begin{array}{r}{x^{t+1}\\overset{*}{\\in}\\{\\Pi_{{\\mathcal{X}}}[\\sum_{i\\in[t]}a_{i}\\cdot x^{i}\\!+\\!\\dot{b_{i}}\\cdot\\nabla f^{i}(x^{i})]:a_{i},\\dot{b_{i}}\\in\\mathbb{R},\\forall i\\in\\mathsf{\\dot{[}t]}\\}}\\end{array}$ as essentially the linear combination of the previous iterates and their gradients. Many online algorithms such as online gradient descent and optimistic gradient satisfy the linear span assumption. Combining with Lemma 1, this lower bound suggests that GD attains nearly optimal $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)^{\\bar{\\phantom{}}}$ -regret, even in the non-convex setting, among a natural family of gradient-based algorithms. ", "page_idx": 21}, {"type": "text", "text": "Proposition 1 (Lower bound for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret against non-convex losses). For any $T\\ \\geq\\ 1$ , $\\delta\\in(0,1).$ , and $L\\geq0$ , there exists a sequence of $L$ -Lipschitz and $L$ -smooth non-convex loss functions $f^{1},\\ldots,f^{T}$ on $\\mathcal{X}=[-1,1]$ such that for any algorithm that satisfies the linear span assumption, its $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret on the loss sequence is $\\begin{array}{r}{\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\geq\\frac{\\delta^{2}L T}{2}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "E.3.1 Proof of Theorem 7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our proof technique comes from the standard one used in multi-armed bandits $[\\mathrm{Aue}{+}02$ , Theorem 5.1]. Suppose that $f^{t}(x)=g^{t}x$ . We construct two possible environments. In the first environment, $g^{t}=G$ with probability $\\textstyle{\\frac{1+\\varepsilon}{2}}$ and $g^{t}=-G$ with probability $\\frac{1\\!-\\!\\varepsilon}{2}$ ; in the second environment, $g^{t}=G$ with probability $\\scriptstyle{\\frac{1-\\varepsilon}{2}}$ and $g^{t}=-G$ with probability $\\textstyle{\\frac{1+\\varepsilon}{2}}$ . We use $\\mathbb{E}_{i}$ and $\\mathbb{P}_{i}$ to denote the expectation and probability measure under environment $i$ , respectively, for $i\\,=\\,1,2$ . Suppose that the true environment is uniformly chosen from one of\u221a these two environments. Below, we show that the expected regret of the learner is at least $\\Omega(\\delta G\\sqrt{T})$ . ", "page_idx": 21}, {"type": "text", "text": "Define $\\begin{array}{r}{N_{+}\\,=\\,\\sum_{t=1}^{T}\\mathbb{I}\\{x^{t}\\,\\geq\\,0\\}}\\end{array}$ be the number of times $x^{t}$ is non-negative, and define $f^{1:T}\\;=$ $(f^{1},\\ldots,f^{T})$ . Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{1}[N_{+}]-\\mathbb{E}_{2}[N_{+}]\\right|=\\left|\\displaystyle\\sum_{j\\ge\\nu}\\left(\\mathbb{P}_{1}(f^{1:T})\\mathbb{E}\\left[N_{+}\\mid f^{1:T}\\right]-\\mathbb{P}_{2}(f^{1:T})\\mathbb{E}\\left[N_{+}\\mid f^{1:T}\\right]\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(enumerat~all~possible~sequences~conding~bounded~}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq T\\displaystyle\\sum_{j:\\nu=\\nu}\\left|\\mathbb{P}_{1}(f^{1:T})-\\mathbb{P}_{2}(f^{1:T})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=T\\|\\mathbb{P}_{1}-\\mathbb{P}_{2}\\|_{\\Gamma\\times\\Gamma}}\\\\ &{\\qquad\\qquad\\qquad\\leq T\\sqrt{(2\\ln2)\\mathrm{KL}}\\mathbb{E}(\\mathbb{P}_{1},\\mathbb{P}_{2})}\\\\ &{\\qquad\\qquad\\qquad=T\\sqrt{(2\\ln2)T\\cdot\\mathrm{i\\!~KL}}\\left(\\mathbf{Remoli}\\left(\\frac{1+\\varepsilon}{2}\\right),8\\mathrm{emouli}\\left(\\frac{1-\\varepsilon}{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad=T\\sqrt{(2\\ln2)T\\varepsilon}\\ln\\frac{1+\\varepsilon}{1-\\varepsilon}\\leq T\\sqrt{(4\\ln2)T\\varepsilon^{2}}.}\\end{array}\n$$$f^{1:T}$ ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the first environment, we consider the regret with respect to $v=\\delta$ . Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{1}\\left[{\\mathrm{Reg}}_{\\mathrm{Proj},\\delta}^{T}\\right]\\geq\\mathbb{E}_{1}\\left[\\displaystyle\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(\\Pi_{\\mathcal{X}}[x^{t}-\\delta])\\right]=\\mathbb{E}_{1}\\left[\\displaystyle\\sum_{t=1}^{T}g^{t}(x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-\\delta])\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{1}\\left[\\displaystyle\\sum_{t=1}^{T}\\varepsilon G(x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-\\delta])\\right]\\geq\\varepsilon\\delta G\\mathbb{E}_{1}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}\\{x^{t}\\geq0\\}\\right]=\\varepsilon\\delta G\\mathbb{E}_{1}\\left[N_{+}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the last inequality we use the fact that if $x^{t}\\geq0$ then $\\begin{array}{r}{x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-\\delta]=x^{t}-(x^{t}-\\delta)=\\delta}\\end{array}$ because $D\\geq\\delta$ . In the second environment, we consider the regret with respect to $v=-\\delta$ . Then similarly, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Sigma}_{2}\\left[\\mathrm{Re}_{\\mathrm{g}_{\\mathrm{Proj},\\delta}}^{T}\\right]\\geq\\mathbb{E}_{2}\\left[\\displaystyle\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(\\Pi_{X}[x^{t}+\\delta])\\right]=\\mathbb{E}_{2}\\left[\\displaystyle\\sum_{t=1}^{T}g^{t}(x^{t}-\\Pi_{X}[x^{t}+\\delta])\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{2}\\left[\\displaystyle\\sum_{t=1}^{T}-\\varepsilon G(x^{t}-\\Pi_{X}[x^{t}+\\delta])\\right]\\geq\\varepsilon\\delta G\\mathbb{E}_{2}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}\\{x^{t}<0\\}\\right]=\\varepsilon\\delta G\\left(T-\\mathbb{E}_{2}\\left[N_{+}[x^{t}<\\delta]\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summing up the two inequalities, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\left(\\mathbb{E}_{1}\\left[\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\right]+\\mathbb{E}_{2}\\left[\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\right]\\right)\\geq\\frac{1}{2}\\left(\\varepsilon\\delta G T+\\varepsilon\\delta G(\\mathbb{E}_{1}[N_{+}]-\\mathbb{E}_{2}[N_{+}])\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\left(\\varepsilon\\delta G T-\\varepsilon\\delta G T\\varepsilon\\sqrt{(4\\ln2)T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Choosing \u221a(16 1ln 2)T , we can lower bound the last expression by \u2126(\u03b4G T). The theorem is proven by noticing that $\\begin{array}{r}{\\frac{1}{2}\\left(\\mathbb{E}_{1}\\left[\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\right]+\\mathbb{E}_{2}\\left[\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\right]\\right)}\\end{array}$ is the expected regret of the learner. ", "page_idx": 22}, {"type": "text", "text": "E.3.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Consider $f:[-1,1]\\to\\mathbb{R}$ such that $\\textstyle f(x)=-{\\frac{L}{2}}x^{2}$ and let $f^{t}=f$ for all $t\\in[T]$ . Then any first-order methods that satisfy the linear span assumption with initial point $x^{1}=0$ will produce $x^{t}=0$ for all $t\\in[T]$ . The $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret is thus $\\begin{array}{r}{\\sum_{t=1}^{T}(f(0)-f(\\delta))=\\frac{\\delta^{2}L T}{2}}\\end{array}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "E.4 Improved $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -Regret in the Game Setting and Proof of Theorem 9 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Any online algorithm suffers an $\\Omega({\\sqrt{T}})\\;\\Phi_{\\mathrm{Proj}}^{\\chi}(\\delta)$ -regret even against linear loss functions by Theorem 7. This lower bound, however, holds only in the adversarial setting. In this section, we show an improved $O(T^{\\frac{1}{4}})$ individual $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret bound under a slightly stronger smoothness assumption (Assumption 2) in the game setting, where players interact with each other using the same algorithm, previous results show improved external regret $[\\mathrm{Syr}+15$ ; CP20; DFG21; $\\mathrm{Ana}{+}22\\mathrm{a}$ ; Ana $^{+22}\\mathrm{b}$ ; $\\mathrm{Far}{+}22\\mathrm{a}]$ . This assumption is naturally satisfied by finite normal-form games and is also made for results about concave games $[\\mathrm{Far}{+}22\\mathrm{a}]$ . ", "page_idx": 22}, {"type": "text", "text": "Assumption 2. For any player $i~\\in~[n]$ , the utility $u_{i}(x)$ satisfies $\\|\\nabla_{x_{i}}u_{i}(x)-\\nabla_{x_{i}}u_{i}(x^{\\prime})\\|\\ \\leq$ $L\\|x-x^{\\prime}\\|$ for all $x,x^{\\prime}\\in\\mathcal{X}$ . ", "page_idx": 22}, {"type": "text", "text": "We study the Optimistic Gradient (OG) algorithm [RS13], an optimistic variant of GD that has been shown to have improved individual external regret guarantee in the game setting $[\\mathrm{Syr}+15]$ . The OG algorithm initializes $w^{0}\\in\\chi$ arbitrarily and $\\bar{g}^{0}=0$ . In each step $t\\geq1$ , the algorithm plays $x^{t}$ , receives feedback $g^{t}:=\\nabla f^{t}(x^{t})$ , and updates $w^{t}$ , as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx^{t}=\\Pi_{\\mathcal{X}}\\big[w^{t-1}-\\eta g^{t-1}\\big],\\quad w^{t}=\\Pi_{\\mathcal{X}}\\big[w^{t-1}-\\eta g^{t}\\big].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We show that OG has $O(\\sqrt{T})\\;\\Phi_{\\mathrm{Proj}}^{\\chi}(\\delta)$ -regret in the adversarial setting and fast $O(T^{1/4})\\;\\Phi_{\\mathrm{Proj}}^{\\chi}(\\delta).$ regret and convergence to approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium in games. ", "page_idx": 22}, {"type": "text", "text": "Theorem 8 (Adversarial Regret Bound for OG). Let $\\delta>0$ and $T\\in\\mathbb N$ . For convex and $G$ -Lipschitz loss functions $\\{f^{t}:\\mathcal{X}\\to\\mathbb{R}\\}_{t\\in[T]}.$ , the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret of (OG) with step size $\\eta>0$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\leq\\frac{\\delta D_{X}}{\\eta}+\\eta\\sum_{t=1}^{T}\\big\\|g^{t}-g^{t-1}\\big\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Choosing step size $\\begin{array}{r}{\\eta=\\frac{\\sqrt{\\delta D_{\\mathcal{X}}}}{2G\\sqrt{T}}}\\end{array}$ , we have $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}\\leq4G\\sqrt{\\delta D_{\\mathcal{X}}T}$ ", "page_idx": 23}, {"type": "text", "text": "Theorem 9 (Improved Individual $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -Regret of OG in the Game Setting). In a $G$ -Lipschitz $L$ -smooth (in the sense of Assumption 2) game, when all players employ $O G$ with step size $\\eta>0$ , then for each player i, \u03b4 > 0, and T \u22651, their individual \u03a6PXrioj(\u03b4)-regret denoted as RegTP,rioj,\u03b4 is $\\begin{array}{r}{\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T,i}\\leq\\frac{\\delta D}{\\eta}+\\eta G^{2}+3n L^{2}G^{2}\\eta^{3}T}\\end{array}$ . Choosing $\\eta=\\operatorname*{min}\\{(\\delta\\bar{D}/(n L^{2}G^{2}T))^{\\frac{1}{4}},(\\delta D)^{\\frac{1}{2}}/G\\}$ , we have $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T,i}\\,\\stackrel{<}{=}\\,4(\\delta D)^{\\frac{3}{4}}(n L^{2}G^{2}T)^{\\frac{1}{4}}+2\\sqrt{\\delta D}G$ . Furthermore, for any $\\delta\\,>\\,0$ and any $\\varepsilon>0$ , their empirical distribution of played strategy profiles converges to an $\\big(\\varepsilon\\,+\\,\\frac{\\delta^{2}L}{2}\\big)$ -approximate $\\Phi_{\\mathrm{Proj}}(\\delta)$ -equilibrium in $O(1/\\varepsilon^{\\frac{4}{3}})$ iterations. ", "page_idx": 23}, {"type": "text", "text": "E.4.1 Proof of Theorem 8 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. Fix any deviation $v$ that is bounded by $\\delta$ . Let us define $p^{0}\\,=\\,w^{0}$ and $p^{t}\\,=\\,\\Pi_{\\mathcal{X}}[x^{t}\\,-\\,v]$ . Following standard analysis of OG [RS13], we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(p^{t})\\leq\\displaystyle\\sum_{t=1}^{T}\\left\\langle\\nabla f^{t}(x^{t}),x^{t}-p^{t}\\right\\rangle}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2\\eta}\\Big(\\big\\|w^{t-1}-p^{t}\\big\\|^{2}-\\big\\|w^{t}-p^{t}\\big\\|^{2}\\Big)+\\eta\\big\\|g^{t}-g^{t-1}\\big\\|^{2}-\\frac{1}{2\\eta}\\Big(\\big\\|x^{t}-w^{t}\\big\\|^{2}+\\big\\|x^{t}-w^{t-1}\\big\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\left(\\frac{1}{2\\eta}\\big\\|w^{t-1}-p^{t}\\big\\|^{2}-\\frac{1}{2\\eta}\\big\\|w^{t-1}-p^{t-1}\\big\\|^{2}+\\eta\\big\\|g^{t}-g^{t-1}\\big\\|^{2}-\\frac{1}{2\\eta}\\big\\|x^{t}-w^{t-1}\\big\\|^{2}\\right)\\qquad(3\\sigma-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we apply a similar analysis from Theorem 3 to upper bound the term $\\left\\|w^{t-1}-p^{t}\\right\\|^{2}-$ $\\left\\|\\boldsymbol{w}^{t-1}-\\boldsymbol{p}^{t-1}\\right\\|^{2}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert w^{t-1}-p^{t}\\right\\Vert^{2}-\\left\\Vert w^{t-1}-p^{t-1}\\right\\Vert^{2}}\\\\ &{=\\left\\langle p^{t-1}-p^{t},2w^{t-1}-p^{t-1}-p^{t}\\right\\rangle}\\\\ &{=\\left\\langle p^{t-1}-p^{t},2w^{t-1}-2p^{t}\\right\\rangle-\\left\\Vert p^{t}-p^{t-1}\\right\\Vert^{2}}\\\\ &{=2\\bigl\\langle p^{t-1}-p^{t},v\\bigr\\rangle+2\\bigl\\langle p^{t-1}-p^{t},w^{t-1}-v-p^{t}\\bigr\\rangle-\\left\\Vert p^{t}-p^{t-1}\\right\\Vert^{2}}\\\\ &{=2\\bigl\\langle p^{t-1}-p^{t},v\\bigr\\rangle+2\\bigl\\langle p^{t-1}-p^{t},x^{t}-v-p^{t}\\bigr\\rangle+2\\bigl\\langle p^{t-1}-p^{t},w^{t-1}-x^{t}\\bigr\\rangle-\\left\\Vert p^{t}-p^{t-1}\\right\\Vert^{2}}\\\\ &{\\leq2\\bigl\\langle p^{t-1}-p^{t},v\\bigr\\rangle+\\left\\Vert x^{t}-w^{t-1}\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last-inequality we use $\\left\\langle p^{t-1}-p^{t},x^{t}-u-p^{t}\\right\\rangle\\leq0$ since $p^{t}=\\Pi_{\\mathcal{X}}[x^{t}-v]$ and $\\mathcal{X}$ is a compact convex set; we also use $2\\langle{\\dot{a}},b\\rangle-b^{2}\\leq a^{2}$ . In the analysis above, unlike the analysis of GD where we drop the negative term $-\\left\\|p^{t}-p^{t-1}\\right\\|^{2}$ , we use $-\\left\\|p^{t}-p^{t-1}\\right\\|^{2}$ to get a term $\\left\\|\\boldsymbol{x}^{t}-\\boldsymbol{w}^{t-1}\\right\\|^{2}$ which can be canceled by the last term in (3). ", "page_idx": 23}, {"type": "text", "text": "Now we combine the above two inequalities. Since the term $\\left\\|\\boldsymbol{x}^{t}-\\boldsymbol{w}^{t-1}\\right\\|^{2}$ cancels out and $2\\langle p^{t-1}-p^{t},v\\rangle$ telescopes, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(p^{t})\\leq\\frac{\\langle p^{0}-p^{T},u\\rangle}{\\eta}+\\sum_{t=1}^{T}\\eta\\big\\Vert g^{t}-g^{t-1}\\big\\Vert^{2}\\leq\\frac{\\delta D_{\\mathcal{X}}}{\\eta}+\\eta\\sum_{t=1}^{T}\\big\\Vert g^{t}-g^{t-1}\\big\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.4.2 Proof of Theorem 9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the analysis of Theorem 8 for the adversarial setting, the term $\\left\\|g^{t}-g^{t-1}\\right\\|^{2}$ can be as large as $4G^{2}$ . In the game setting where every player $i$ employs OG, $g_{i}^{t}$ ,i.e., $-\\nabla_{x_{i}}u_{i}(x)$ , depends on other players\u2019 action $x_{-i}^{t}$ . Note that the change of the players\u2019 actions $\\left\\|\\boldsymbol{x}^{t}-\\boldsymbol{x}^{t-1}\\right\\|^{2}$ is only ${\\cal O}(\\eta^{2})$ . Such stability of the updates leads to an improved upper bound on $\\left|\\left|g_{i}^{t}-g_{i}^{t-1}\\right|\\right|^{2}$ and hence also an improved $O(T^{\\frac{1}{4}})$ $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret for the player. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let us fix any player $i\\in[n]$ in the smooth game. In every step $t$ , player $i$ \u2019s loss function $f^{t}:\\mathcal{X}_{i}\\to\\mathbb{R}$ is $\\langle-{\\bar{\\nabla}}_{x_{i}}{u_{i}}\\bar{(}x^{t}),\\cdot\\rangle$ determined by their utility function $u_{i}$ and all players\u2019 actions $x^{t}$ . Therefore, their gradient feedback is $g^{t}=-\\nabla_{x_{i}}u_{i}(x^{t})$ . For all $t\\geq2$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|g^{t}-g^{t-1}\\right\\|^{2}=\\left\\|\\nabla u_{i}(x^{t})-\\nabla u_{i}(x^{t-1})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq L^{2}\\big\\|x^{t}-x^{t-1}\\big\\|^{2}}\\\\ &{\\qquad\\qquad=L^{2}\\displaystyle\\sum_{i=1}^{n}\\left\\|x_{i}^{t}-x_{i}^{t-1}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq3L^{2}\\displaystyle\\sum_{i=1}^{n}\\Big(\\big\\|x_{i}^{t}-w_{i}^{t}\\big\\|^{2}+\\big\\|w_{i}^{t}-w_{i}^{t-1}\\big\\|^{2}+\\big\\|w_{i}^{t-1}-x_{i}^{t-1}\\big\\|^{2}\\Big)}\\\\ &{\\qquad\\qquad\\leq3n L^{2}\\eta^{2}G^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use $L$ -smoothness of the utility function $u_{i}$ in the first inequality; we use the update rule of OG and the fact that gradients are bounded by $G$ in the last inequality. ", "page_idx": 24}, {"type": "text", "text": "Applying the above inequality to the regret bound obtained in Theorem 8, the individual $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ - regret of player $i$ is upper bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T,i}\\leq\\frac{\\delta D}{\\eta}+\\eta G^{2}+3n L^{2}G^{2}\\eta^{3}T.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C\u221ahoosing $\\eta=\\operatorname*{min}\\{(\\delta D/(n L^{2}G^{2}T))^{\\frac{1}{4}},(\\delta D)^{\\frac{1}{2}}/G\\}$ , we have $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T,i}\\leq4(\\delta D)^{\\frac{3}{4}}(n L^{2}G^{2}T)^{\\frac{1}{4}}+$ . Using Lemma 1, we have the empirical distribution of played strategy proflies converge to an $\\left(\\varepsilon+\\frac{\\delta^{2}L}{2}\\right)$ -approximate $\\Phi_{\\mathrm{Proj}}(\\delta))$ -equilibrium in $O(1/\\varepsilon^{\\frac{4}{3}})$ iterations. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "F Missing Details in Section 4.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Algorithm 3: $\\mathrm{Conv}(\\Phi)$ -regret minimization for Lipschitz smooth non-concave rewards Input: $x_{1}\\in\\mathcal{X}$ , $K\\ge2$ , a no-external-regret algorithm $\\Re_{\\Phi}$ against linear reward over $\\Delta(\\Phi)$ Output: A $\\mathrm{Conv}(\\Phi)$ -regret minimization algorithm over $\\mathcal{X}$   \n1 function NEXTSTRATEGY()   \n2 $\\boldsymbol{p}^{t}\\gets\\Re_{\\Phi}$ .NEXTSTRATEGY(). Note that $p_{t}$ is a distribution over $\\Phi$ .   \n3 $x_{k}\\leftarrow\\phi_{p^{t}}(x_{k-1})$ , for all $2\\le k\\le K$   \n4 return $x^{t}\\gets$ uniformly at random from $\\{x_{1},\\ldots,x_{K}\\}$ .   \n5 function OBSERVEREWARD $\\left(\\nabla_{x}u^{t}(x^{t})\\right)$   \n6 $u_{\\Phi}^{t}(\\cdot)\\gets\\mathbf{a}$ linear reward over $\\Delta(\\Phi)$ with $u_{\\Phi}^{t}(\\phi)=\\langle\\nabla_{x}u^{t}(x^{t}),\\phi(x^{t})-x^{t}\\rangle$ for all $\\phi\\in\\Phi$ . $\\Re_{\\Phi}$ .OBSERVEREWARD $(u_{\\Phi}^{t}(\\cdot))$ . ", "page_idx": 24}, {"type": "text", "text": "F.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof Sketch. We adopt the framework in [SL07; GGM08] (as described in Section 3) with two main modifications. First, we utilize the $L$ -smoothness of the utilities to transform the problem of external regret over $\\Delta(\\Phi)$ against non-concave rewards into a linear optimization problem. Second, we use the technique of \u201cfixed point in expectation\" $[Z\\mathrm{ha}+24]$ to circumvent the intractable problem of finding a fixed point. ", "page_idx": 24}, {"type": "text", "text": "Proof. For a sequence of strategies $\\{x^{t}\\}_{t\\in[T]}$ , its $\\mathrm{Conv}(\\Phi)$ -regret is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{\\mathrm{Conv}(\\Phi)}^{T}=\\underset{\\phi\\in\\mathrm{Conv}(\\Phi)}{\\operatorname*{max}}\\left\\{\\displaystyle\\sum_{t=1}^{T}\\left(u^{t}(\\phi(x^{t}))-u^{t}(x^{t})\\right)\\right\\}}\\\\ &{\\qquad\\qquad=\\underset{p\\in\\Delta(\\Phi)}{\\operatorname*{max}}\\left\\{\\displaystyle\\sum_{t=1}^{T}u^{t}(\\phi_{p}(x^{t}))-u^{t}(\\phi_{p^{t}}(x^{t}))\\right\\}_{,t=1}+\\displaystyle\\sum_{t=1}^{T}u^{t}(\\phi_{p^{t}}(x^{t}))-u^{t}(x^{t})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "I I: approximation  e rror of fixed poin t ", "page_idx": 24}, {"type": "text", "text": "Bounding External Regret over $\\Delta(\\Phi)$ We can define a new reward function $f^{t}(p):=u^{t}(\\phi_{p}(x^{t}))$ over $p\\in\\Delta(\\Phi)$ . Since $\\bar{u^{t}}$ is non-concave, the reward $f^{t}$ is also non-concave and it is computational intractable to minimize external regret. We use locality to avoid computational barrier. Here we use the fact that $\\Phi=\\Phi(\\delta)$ contains only $\\delta$ -local strategy modifications. Then by $L$ -smoothness of $u^{t}$ , we know for any $p\\in\\Delta(\\Phi)$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n|u^{t}(\\phi_{p}(x^{t})-u^{t}(x^{t})-\\langle\\nabla u^{t}(x^{t}),\\phi_{p}(x^{t})-x^{t}\\rangle)|\\leq\\frac{L}{2}\\big\\|\\phi_{p}(x^{t})-x^{t}\\big\\|^{2}\\leq\\frac{\\delta^{2}L}{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we can approximate the non-concave optimization problem by a linear optimization problem over $\\Delta(\\Phi)$ with only second-order error $\\frac{\\delta^{2}L}{2}$ . Here we use the notation $a=b\\pm c$ to mean $b-c\\leq$ $a\\leq b+c$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{t}(\\phi_{p}(x^{t})-u^{t}(x^{t})=\\langle\\nabla u^{t}(x^{t}),\\phi_{p}(x^{t})-x^{t}\\rangle\\pm\\displaystyle\\frac{\\delta^{2}L}{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left\\langle\\nabla u^{t}(x^{t}),\\displaystyle\\sum_{\\phi\\in\\Phi}p(\\phi)\\phi(x^{t})-x^{t}\\right\\rangle\\pm\\displaystyle\\frac{\\delta^{2}L}{2}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{\\phi\\in\\Phi}p(\\phi)\\langle\\nabla u^{t}(x^{t}),\\phi(x^{t})-x^{t}\\rangle\\pm\\displaystyle\\frac{\\delta^{2}L}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can then instantiate the external regret $\\Re_{\\Phi}$ as the Hedge algorithm over reward $f^{t}(p)\\;=\\;$ $\\begin{array}{r}{\\sum_{\\phi\\in\\Phi}p(\\phi)\\langle\\nabla u^{t}(x^{t}),\\phi(x^{t})-x^{t}\\rangle}\\end{array}$ and get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p\\in\\Delta(\\Phi)}{\\operatorname*{max}}\\left\\{\\displaystyle\\sum_{t=1}^{T}u^{t}(\\phi_{p}(x^{t}))-u^{t}(\\phi_{p^{t}}(x^{t}))\\right\\}}\\\\ &{\\leq\\underset{p\\in\\Delta(\\Phi)}{\\operatorname*{max}}\\left\\{\\displaystyle\\sum_{t=1}^{T}\\sum_{\\phi\\in\\Phi}(p(\\phi)-p^{t}(\\phi))\\langle\\nabla u^{t}(x^{t}),\\phi(x^{t})-x^{t}\\rangle)\\right\\}+\\delta^{2}L T}\\\\ &{\\leq2G\\delta\\sqrt{T\\log\\left|\\Phi\\right|}+\\delta^{2}L T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use the fact that $\\langle\\nabla u^{t}(x^{t}),\\phi(x^{t})-x^{t}\\rangle\\leq\\|\\nabla u^{t}(x^{t})\\|\\cdot\\|\\phi(x^{t})-x^{t}\\|\\leq G\\delta.$ ", "page_idx": 25}, {"type": "text", "text": "Bounding error due to sampling from a fixed point in expectation We choose $x_{1}$ as an arbitrary point in $\\mathcal{X}$ . Then we recursively apply $\\phi_{p^{t}}$ to get ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{k}=\\phi_{p^{t}}(x_{k-1})=\\sum_{\\phi\\in\\Phi}p^{t}(\\phi)\\phi(x_{k-1}),\\forall2\\leq k\\leq K.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We denote $\\mu^{t}=\\mathrm{Uniform}\\{x_{k}:1\\leq k\\leq K\\}$ . Then the strategy $x^{t}\\sim\\mu^{t}$ is sampled from $\\mu^{t}$ . We have that $\\mu^{t}$ is an approximate fixed-point in expectation / stationary distribution in the sense that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mu^{t}}\\big[u^{t}(\\phi_{p^{t}}(x^{t}))-u^{t}(x^{t})\\big]=\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}u^{t}(\\phi_{p^{t}}(x_{k})-u^{t}(x_{k}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{K}\\big(u^{t}(\\phi_{p^{t}}(x_{K}))-u^{t}(x_{1})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thanks to the boundedness of $u^{t}$ , we can use Hoeffding-Azuma\u2019s inequality to conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{t=1}^{T}\\left(u^{t}(\\phi_{p^{t}}(x^{t}))-u^{t}(x^{t})-\\frac{1}{K}\\right)\\geq\\epsilon\\right]\\leq\\exp{\\left(-\\frac{\\varepsilon^{2}}{8T}\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any $\\varepsilon>0$ . Combining the above with $\\varepsilon=\\sqrt{8T\\log(1/\\beta)}$ and $K=\\sqrt{T}$ , we get with probability at least $1-\\beta$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{\\mathrm{Conv}(\\Phi)}^{T}\\le2G\\delta\\sqrt{T\\log|\\Phi|}+\\delta^{2}L T+\\sqrt{T}+\\sqrt{8T\\log(1/\\beta)}}\\\\ &{\\qquad\\qquad\\qquad\\leq8\\sqrt{T}\\Big(G\\delta\\sqrt{\\log|\\Phi|}+\\sqrt{\\log(1/\\beta)}\\Big)+\\delta^{2}L T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Convergence to $\\Phi$ -equilibrium If all players in a non-concave continuous game employ Algorithm 1, then we know for each player $i$ , with probability $\\textstyle1-{\\frac{\\beta}{n}}$ , its $\\Phi^{\\mathcal{X}_{i}}$ -regret is upper bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n8\\sqrt{T}\\bigg(G\\delta\\sqrt{\\log{|\\Phi^{\\mathcal{X}_{i}}|}}+\\sqrt{\\log(n/\\beta)}\\bigg)+\\delta^{2}L T.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By a union bound o\u221aver all $n$ players, we get with probability $1-\\beta$ , every player $i$ \u2019s $\\Phi^{\\mathcal{X}_{i}}$ -regret is upper bounded by $8{\\sqrt{T}}(G\\delta{\\sqrt{\\log|\\Phi^{+}{\\mathcal{X}}_{i}|}}+{\\sqrt{\\log(n/\\beta)}})+\\delta^{2}L T$ . Now by Theorem 1, we know the empirical distribution of strategy profiles played forms an $(\\varepsilon+\\delta^{2}L)$ -approximate $\\Phi=\\Pi_{i=1}^{n}\\Phi^{\\mathcal{X}_{i}}$ - equilibrium, as long as $\\begin{array}{r}{T\\ge\\frac{128\\left(G^{2}\\delta^{2}\\log|\\Phi^{\\mathcal{X}_{i}}|+\\log(n/\\beta)\\right)}{\\varepsilon^{2}}}\\end{array}$ iterations. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "G Missing details in Section 4.3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We introduce a natural set of local strategy modifications and the corresponding local equilibrium notion. Given any set of (possibly non-local) strategy modifications $\\Psi=\\{\\psi:\\mathcal{X}\\rightarrow\\mathcal{X}\\}$ , we define a set of local strategy modifications as follows: for $\\delta\\leq D_{\\mathcal{X}}$ and $\\lambda\\in[0,1]$ , each strategy modification $\\phi_{\\lambda,\\psi}$ interpolates the input strategy $x$ with the modified strategy $\\psi(x)$ : formally, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{Int},\\Psi}^{\\chi}(\\delta):=\\left\\{\\phi_{\\lambda,\\psi}(x):=(1-\\lambda)x+\\lambda\\psi(x):\\psi\\in\\Psi,\\lambda\\leq\\delta/D_{X}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that for any $\\psi\\ \\in\\ \\Psi$ and $\\begin{array}{l}{\\lambda\\ \\leq\\ \\frac{\\delta}{D_{\\mathcal{X}}}}\\end{array}$ , we have $\\|\\phi_{\\lambda,\\psi}(x)-x\\|\\ =\\ \\lambda\\|x-\\psi(x)\\|\\ \\leq\\ \\delta$ , respecting the locality constraint. The induced $\\Phi_{\\mathrm{Int},\\Psi}^{\\mathcal{X}}(\\delta)$ -regret can be written as $\\mathrm{Reg}_{\\mathrm{Int},\\Psi,\\delta}^{T}\\ :=$ $\\begin{array}{r l}&{\\operatorname*{max}_{\\psi\\in\\Psi,\\lambda\\leq\\frac{\\delta}{D_{\\mathcal{X}}}}\\sum_{t=1}^{T}\\left(f^{t}(x^{t})-f^{t}((1-\\lambda)x^{t}+\\lambda\\psi(x^{t}))\\right)}\\end{array}$ . We now define the corresponding $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibrium. ", "page_idx": 26}, {"type": "text", "text": "Definition 9. Define $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)=\\Pi_{j=1}^{n}\\Phi_{\\mathrm{Int},\\Psi_{j}}^{\\mathcal{X}_{j}}(\\delta)$ . In a continuous game, a distribution $\\sigma$ over strategy profiles is an ( $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibrium if and only if for all player $i\\in[n],$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\psi\\in\\Psi_{i},\\lambda\\leq\\delta/D_{\\mathcal{X}_{i}}}\\mathbb{E}_{x\\sim\\sigma}[u_{i}((1-\\lambda)x_{i}+\\lambda\\psi(x_{i}),x_{-i})]\\leq\\mathbb{E}_{x\\sim\\sigma}[u_{i}(x)]+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Intuitively speaking, when a correlation device recommends strategies to players according to an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibrium, no player can increase their utility by more than $\\varepsilon$ through a local deviation by interpolating with a (possibly global) strategy modification $\\psi\\in\\Psi$ . The richness of $\\Psi$ determines the incentive guarantee provided by an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibriumas well as its computational complexity. When we choose $\\Psi$ to be the set of all possible strategy modifications, the corresponding notion of local equilibrium\u2014limiting the gain of a player by interpolating with any strategy\u2014resembles that of a correlated equilibrium. ", "page_idx": 26}, {"type": "text", "text": "Computation of $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -Equilibrium. By Lemma 1, we know computing an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibrium reduces to minimizing $\\Phi_{\\mathrm{Int},\\Psi}^{\\mathcal{X}}(\\delta)$ -regret against convex loss functions. We show that minimizing \u03a6IXnt,\u03a8(\u03b4)-regret against convex loss functions further reduces to $\\Psi$ -regret minimization against linear loss functions. ", "page_idx": 26}, {"type": "text", "text": "Theorem 10. Let $\\boldsymbol{\\mathcal{A}}$ be an algorithm with $\\Psi$ -regret ${\\mathrm{Reg}}_{\\Psi}^{T}(G,D_{\\mathcal{X}})$ for linear and $G$ -Lipschitz loss functions over $\\mathcal{X}$ . Then, for any $\\delta>0$ , the $\\Phi_{\\mathrm{Int},\\Psi}^{\\mathcal{X}^{-}}(\\delta)$ -regret of $\\boldsymbol{\\mathcal{A}}$ for convex and $G$ -Lipschitz loss functions over $\\mathcal{X}$ is at most $\\begin{array}{r}{\\frac{\\delta}{D x}\\cdot\\left[\\mathrm{Reg}_{\\Psi}^{T}(G,D_{\\mathcal{X}})\\right]^{+}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. By definition and convexity of $f^{t}$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi\\in\\Phi_{\\mathrm{Int},\\psi}^{x}(\\delta)}{\\operatorname*{max}}\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}(\\phi(x^{t}))=\\underset{\\psi\\in\\Psi,\\lambda\\leq\\frac{\\delta}{D_{\\mathcal{X}}}}{\\operatorname*{max}}\\sum_{t=1}^{T}f^{t}(x^{t})-f^{t}((1-\\lambda)x^{t}+\\lambda\\psi(x^{t}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\delta}{D_{\\mathcal{X}}}\\left[\\underset{\\psi\\in\\Psi}{\\operatorname*{max}}\\sum_{t=1}^{T}\\left\\langle\\nabla f^{t}(x^{t}),x^{t}-\\psi(x^{t})\\right\\rangle\\right]^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "3CtTMF5zzM/tmp/9ff2b3c2b7a0c6ac6e2a91987859715ab461f65b1e7219034b82c3ad0bde4c5a.jpg", "img_caption": ["Figure 2: Illustration of $\\phi_{\\mathrm{Proj},v}(x)$ and $\\phi_{\\mathrm{Beam},v}(x)$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Note that when $f^{t}$ is linear, the reduction is without loss. Thus, any worst-case $\\Omega(r(T))$ -lower bound faodr $\\Psi$ t-sr eegfrfeict iiemntp e-sr eag $\\Omega\\big(\\frac{\\delta}{D_{\\mathcal{X}}}\\cdot r(T)\\big)$ i loon waelrg obroiuthndm fs osr $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ a-pr etgrraent.s foMromreatoivoenr,s  foovr ear ntyh es est $\\Psi$ ptlheaxt $\\Psi$ and more generally any set such that (i) all modifications in the set can be represented as linear transformations in some finite-dimensional space and (ii) fixed point computation can be carried out efficiently for any linear transformations [GGM08], we also get an efficient algorithm for computing an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int},\\Psi}(\\delta)$ -equilibrium in the first-order stationary regime. ", "page_idx": 27}, {"type": "text", "text": "CCE-like Instantiation In the special case where $\\Psi$ contains only constant strategy modifications (i.e. $\\psi(x)=x^{*}$ for all $x$ ), we get a coarse correlated equilibrium (CCE)-like instantiation of local equilibrium, which limits the gain by interpolating with any fixed strategy. We denote the resulting set of local strategy modification simply as $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}$ . We can apply any no-external regret algorithm for efficient $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}$ -regret minimization and computation of $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}(\\delta)$ -equilibrium in the first-order stationary regime as summarized in Theorem 5. ", "page_idx": 27}, {"type": "text", "text": "The above $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -regret bound of $O({\\sqrt{T}})$ is derived for the adversarial setting. In the game setting, where each player employs the same algorithm, players may have substantially lower external regret $[\\mathrm{Syr}+15$ ; CP20; DFG21; Ana $+22\\mathrm{a}$ ; $\\mathrm{Ana}{+}22\\mathrm{b}$ ; $\\mathrm{Far}{+}22\\mathrm{a}]$ but we need a slightly stronger smoothness assumption than Assumption 1. This assumption is naturally satisfied by finite normalform games and is also made for results about concave games $[\\mathrm{Far}{+}22\\mathrm{a}]$ . Using Assumption 2 and Lemma 1, the no-regret learning dynamics of $[\\mathrm{Far}{+}22\\mathrm{a}]$ that guarantees ${\\cal O}(\\log T)$ individual external regret in concave games can be applied to smooth non-concave games so that the individual $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -regret of each pla2yer is at most $O(\\log T)+{\\frac{\\delta^{2}L T}{2}}$ . This gives an algorithm with faster $\\tilde{O}(1/\\varepsilon)$ convergence to an $\\big(\\varepsilon+\\frac{\\delta^{2}L}{2}\\big)$ -approximate $\\Phi_{\\mathrm{Int}}(\\delta)$ -equilibrium than GD. ", "page_idx": 27}, {"type": "text", "text": "H Beam-Search Local Strategy Modifications and Local Equilibria ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Section 4.1 and Section 4.3, we have shown that GD achieves near-optimal performance for both $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -regret and $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret. In this section, we introduce another natural set of local strategy modifications, $\\Phi_{\\mathrm{Beam}}^{\\mathcal{X}}(\\delta)$ , which is similar to $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ . Specifically, the set $\\Phi_{\\mathrm{Beam}}^{\\mathcal{X}}(\\delta)$ contains deviations that try to move as far as possible in a fixed direction (see Figure 2 for an illustration of the difference between $\\phi_{\\mathrm{Beam},v}(x)$ and $\\phi_{\\mathrm{Proj},v}(x))$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{Beam}}^{\\mathcal{X}}(\\delta):=\\{\\phi_{\\mathrm{Beam},v}(x)=x-\\lambda^{*}v:v\\in B_{d}(\\delta),\\lambda^{*}=\\operatorname*{max}\\{\\lambda:x-\\lambda v\\in\\mathcal{X},\\lambda\\in[0,1]\\}\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It is clear that $\\|\\phi_{\\mathrm{Beam},v}(x)-x\\|\\leq\\|v\\|\\leq\\delta$ . We can similarly derive the notion of $\\Phi_{\\mathrm{Beam}}^{\\mathcal{X}}$ -regret and $(\\varepsilon,\\Phi_{\\mathrm{Beam}}(\\delta))$ -equilibrium. Surprisingly, we show that GD suffers linear \u03a6BXeam(\u03b4)-regret (proof deferred to Appendix H.1). ", "page_idx": 27}, {"type": "text", "text": "Theorem 11. For any $\\delta,\\eta\\ <\\ \\textstyle{\\frac{1}{2}}$ and $T\\ \\geq\\ 1$ , there exists a sequence of linear loss functions $\\{f^{t}:\\mathcal{X}\\subseteq[0,1]^{2}\\to\\mathbb{R}\\}_{t\\in[T]}$ such that $G D$ with step size $\\eta$ suffers $\\Omega(\\delta T)\\;\\Phi_{\\mathrm{Beam}}^{\\mathcal{X}}(\\delta)$ -regret. ", "page_idx": 27}, {"type": "text", "text": "H.1 Proof of Theorem $\\bf{11}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let $\\mathcal{X}\\subset\\mathbb{R}^{2}$ be a triangle region with vertices $A\\,=\\,(0,0),\\,B\\,=\\,(1,1),\\,C\\,=\\,(\\delta,0)$ . Consider $v=(-\\delta,0)$ . The initial point is $x_{1}=(0,0)$ . ", "page_idx": 28}, {"type": "text", "text": "The adversary will choose $\\ell_{t}$ adaptively so that $x_{t}$ remains on the boundary of $\\mathcal{X}$ and cycles clockwise (i.e., $A\\rightarrow\\cdot\\cdot\\cdot\\rightarrow B\\rightarrow\\cdot\\cdot\\cdot\\rightarrow C\\rightarrow\\cdot\\cdot\\cdot\\rightarrow A\\rightarrow\\cdot\\cdot\\cdot\\cdot\\rightarrow$ . To achieve this, the adversary will repeat the following three phases: ", "page_idx": 28}, {"type": "text", "text": "1. Keep choosing $\\ell_{t}=u_{\\overrightarrow{B A}}\\left(u_{\\overrightarrow{B A}}\\right)$ denotes the unit vector in the direction of $\\overrightarrow{B A}$ ) until $x_{t+1}$ reaches $B$ .   \n2. Keep choosing $\\ell_{t}=u_{\\overrightarrow{C B}}$ until $x_{t+1}$ reaches $C$ .   \n3. Keep choosing $\\ell_{t}=u_{\\overrightarrow{A C}}$ until $x_{t+1}$ reaches $A$ . ", "page_idx": 28}, {"type": "text", "text": "In Phase 1, $x_{t}\\in\\overline{{A B}}$ . By the choice of $v=(-\\delta,0)$ , we have $\\boldsymbol{x}_{t}-\\phi_{v}(\\boldsymbol{x}_{t})=(-\\delta(1-\\boldsymbol{x}_{t,1}),0)$ , and the instantaneous regret is $\\frac{\\delta(1\\!-\\!x_{t,1})}{\\sqrt{2}}\\geq0$ . ", "page_idx": 28}, {"type": "text", "text": "In Phase 2, $x_{t}\\,\\in\\,\\overline{{B C}}$ . By the choice of $\\boldsymbol{v}\\,=\\,(\\!-\\delta,0)$ , we have $x_{t}\\,-\\,\\phi_{v}(x_{t})\\,=\\,(0,0)$ , and the instantaneous regret is 0. ", "page_idx": 28}, {"type": "text", "text": "In Phase 3, $x_{t}\\in\\overline{{C A}}$ . By the choice of $v=(-\\delta,0)$ , we have $x_{t}-\\phi_{v}(x_{t})=(-\\delta+x_{t,1},0)$ , and the instantaneous regret is $-\\delta+x_{t,1}\\leq0$ . ", "page_idx": 28}, {"type": "text", "text": "In each cycle, the number of rounds in Phase 1 is of order $\\Theta({\\frac{\\sqrt{2}}{\\eta}})$ , the number of rounds in Phase 2 is between $\\begin{array}{r}{O(\\frac{1}{\\eta})}\\end{array}$ and $\\begin{array}{r}{O(\\frac{\\sqrt{2}}{\\eta})}\\end{array}$ , the number of rounds in Phase 3 is of order $\\Theta({\\frac{\\delta}{\\eta}})$ . ", "page_idx": 28}, {"type": "text", "text": "Therefore, the cumulative regret in each cycle is roughly ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{\\sqrt{2}}{\\eta}}\\times{\\frac{0.5\\delta}{\\sqrt{2}}}+0+{\\frac{\\delta}{\\eta}}\\left(-0.5\\delta\\right)={\\frac{0.5\\delta-0.5\\delta^{2}}{\\eta}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\frac{T}{\\frac{\\sqrt{2}}{\\eta}\\!+\\!\\frac{\\sqrt{2}}{\\eta}\\!+\\!\\frac{\\delta}{\\eta}}=\\Theta(\\eta T)$ . Overall, the cumulative regret is at least $\\begin{array}{r}{\\frac{0.5\\delta-0.5\\delta^{2}}{\\eta}\\times\\Theta(\\eta T)=\\Theta(\\delta T)}\\end{array}$ as long as $\\delta<0.5$ . ", "page_idx": 28}, {"type": "text", "text": "I Hardness in the Global Regime ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the first-order stationary regime $\\delta\\leq\\sqrt{2\\varepsilon/L},\\left(\\varepsilon,\\delta\\right)$ -local Nash equilibrium is intractable, and we have shown polynomial-time algorithms for computing the weaker notions of $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}(\\delta))$ - equilibrium and $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}(\\delta))$ -equilibrium. A natural question is whether correlation enables efficient computation of $\\varepsilon$ -approximate $\\Phi(\\delta)_{,}$ )-equilibrium when $\\delta$ is in the global regime, i.e., $\\delta=\\Omega(\\sqrt{d})$ . In this section, we prove both computational hardness and a query complexity lower bound for both notions in the global regime ", "page_idx": 28}, {"type": "text", "text": "To prove the lower bound results, we only require a single-player game. The problem of computing an $\\varepsilon$ -approximate $\\Phi(\\delta)$ -equilibrium becomes: given scalars $\\varepsilon,\\delta,G,L>0$ and a polynomial-time Turing machine $\\mathcal{C}_{f}$ evaluating a $G$ -Lipschitz and $L$ -smooth function $f:[0,1]^{d}\\rightarrow[0,1]$ and its gradient $\\nabla f:[0,1]^{d}\\rightarrow\\mathbb{R}^{d}$ , we are asked to output a distribution $\\sigma$ that is an $\\varepsilon,$ -approximate $\\Phi(\\delta)$ -equilibrium or $\\perp$ if such equilibrium does not exist. ", "page_idx": 28}, {"type": "text", "text": "Hardness of finding $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -equilibria in the global regime When $\\delta\\,=\\,\\sqrt{d}$ , which equals to the diameter $D$ of $[0,1]^{d}$ , then the problem of finding an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ - equilibrium is equivalent to finding a $(\\varepsilon,\\delta)$ -local minimum of $f$ : assume $\\sigma$ is an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -equilibrium of $f$ , then there exists $x\\in[0,1]^{d}$ in the support of $\\sigma$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x)-\\operatorname*{min}_{x^{*}\\in[0,1]^{d}\\cap B_{d}(x^{*},\\delta)}f(x^{*})\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then hardness of finding an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -equilibrium follows from hardness of finding a $(\\varepsilon,\\delta)$ -local minimum of $f$ [DSZ21]. The following Theorem is a corollary of Theorem 10.3 and 10.4 in [DSZ21]. ", "page_idx": 28}, {"type": "text", "text": "Theorem 12 (Hardness of finding $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -equilibria in the global regime). In the worst case, the following two holds. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Computing an $\\varepsilon$ -appr\u221aoximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ -equilibrium for a game on $\\mathcal{X}=[0,1]^{d}$ with $G={\\sqrt{d}},$ , $L=d$ , $\\begin{array}{r}{\\varepsilon\\leq\\frac{1}{24}}\\end{array}$ , $\\delta=\\sqrt{d}$ is NP-hard. \u2022 $\\Omega(2^{d}/d)$ value/gradient queries are needed to determine an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Int}}^{\\mathcal{X}}(\\delta)$ - equilibrium for a game on $\\mathcal{X}=[0,1]^{d}$ with $G=\\Theta(d^{15})$ , $L=\\Theta(d^{22})$ , $\\varepsilon<1$ , $\\delta=\\sqrt{d}$ . ", "page_idx": 29}, {"type": "text", "text": "Hardness of finding $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibria in the global regime ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem 13 (Hardness of of finding $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibria in the global regime). In the worst case, the following two holds. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Computing an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium for a game on $\\mathcal{X}=[0,1]^{d}$ with $G=$ $\\Theta(d^{15})$ , $L=\\Theta(d^{22})$ , $\\varepsilon<1$ , $\\delta=\\sqrt{d}$ is NP-hard. \u2022 $\\Omega(2^{d}/d)$ value/gradient queries are needed to determine an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ - equilibrium for a game on $\\mathcal{X}=[0,1]^{d}$ with $G=\\Theta(d^{15})$ , $L=\\Theta(d^{22})$ , $\\varepsilon<1$ , $\\delta=\\sqrt{d}$ . ", "page_idx": 29}, {"type": "text", "text": "The hardness of computing $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium also implies a lower bound on $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret in the global regime. ", "page_idx": 29}, {"type": "text", "text": "Corollary 2 (Lower bound of $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret against non-convex functions). In the worst case, the $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -regret of any online algorit\u221ahm is at least $\\Omega(2^{d}/d,T)$ even for loss functions $f:[0,1]^{d}\\rightarrow$ $[0,1]$ with $G,L=\\mathrm{poly}(d)$ and $\\delta=\\sqrt{d}$ . ", "page_idx": 29}, {"type": "text", "text": "The proofs of Theorem 13 and Corollary 2 can be found in the next two sections. ", "page_idx": 29}, {"type": "text", "text": "I.1 Proof of Theorem 13 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We will reduce the problem of finding an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium in smooth games to finding a satisfying assignment of a boolean function, which is NP-complete. ", "page_idx": 29}, {"type": "text", "text": "Fact 1. Given only black-box access to a boolean formula $\\phi:\\{0,1\\}^{d}\\to\\{0,1\\}$ , at least $\\Omega(2^{d})$ queries are needed in order to determine whether $\\phi$ admits a satisfying assignment $x^{*}$ such that $\\phi(x^{*})=1$ . The term black-box access refers to the fact that the clauses of the formula are not given, and the only way to determine whether a specific boolean assignment is satisfying is by querying the specific binary string. Moreover, the problem of finding a satisfying assignment of a general boolean function is NP-hard. ", "page_idx": 29}, {"type": "text", "text": "We revisit the construction of the hard instance in the proof of [DSZ21, Theorem 10.4] and use its specific structures. Given black-box access to a boolean formula $\\phi$ as described in Fact 1, following [DSZ21], we construct the function $f_{\\phi}(x):[0,1]^{d}\\to[0,1]$ as follows: ", "page_idx": 29}, {"type": "text", "text": "1. for each corner $v\\in V=\\{0,1\\}^{d}$ of the $[0,1]^{d}$ hypercube, we set $f_{\\phi}(x)=1-\\phi(x).$ ", "page_idx": 29}, {"type": "text", "text": "2. for the rest of the points $x\\in[0,1]^{d}/V$ , we set $\\begin{array}{r}{f_{\\phi}(x)=\\sum_{v\\in V}P_{v}(x)\\cdot f_{\\phi}(v)}\\end{array}$ where $P_{v}(x)$ are non-negative coefficients defined in [DSZ21, Definition 8.9]. ", "page_idx": 29}, {"type": "text", "text": "The function $f_{\\phi}$ satisfies the following properties: ", "page_idx": 29}, {"type": "text", "text": "1. if $\\phi$ is not satisfiable, then $f_{\\phi}(x)=1$ for all $x\\in[0,1]^{d}$ since $f_{\\phi}(v)=1$ for all $v\\in V$ ; if $\\phi$ has a satisfying assignment $v^{*}$ , then $f_{\\phi}(v^{*})=0$ .   \n2. $f_{\\phi}$ is $\\Theta(d^{12})$ -Lipschitz and $\\Theta(d^{25})$ -smooth.   \n3. for any point $x\\in[0,1]^{d}$ , the set $V(x):=\\{v\\in V:P_{v}(x)\\neq0\\}$ has cardinality at most $d+1$ while $\\begin{array}{r}{\\sum_{v\\in V}P_{v}(x)=1}\\end{array}$ ; any value $/$ gradient query of $f_{\\phi}$ can be simulated by $d+1$ queries on $\\phi$ . ", "page_idx": 29}, {"type": "text", "text": "In the case there exists a satisfying argument $v^{*}$ , then $f_{\\phi}(v^{*})=0$ . Def\u221aine the deviation $e$ so that $e[i]=1$ if $v^{*}[i]=0$ and $e[i]=-1$ if $v^{*}[i]=1$ . It is clear that $\\|e\\|={\\sqrt{d}}=\\delta$ . By properties of projection on $[0,1]^{d}$ , for any $x\\in[0,1]^{d}$ , we have $\\Pi_{[0,1]^{n}}[x-v]\\,=\\,v^{*}$ . Then any $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium $\\sigma$ must include some $x^{*}\\in\\mathcal{X}$ with $f_{\\phi}(x^{*})<1$ in the support, since $\\varepsilon<1$ . In case there exists an algorithm $\\boldsymbol{\\mathcal{A}}$ that computes an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium, $\\boldsymbol{\\mathcal{A}}$ must have queried some $x^{*}$ with $f_{\\phi}(x^{*})<1$ . Since $\\begin{array}{r}{f_{\\phi}(x^{*})=\\sum_{v\\in V(x^{*})}P_{v}(x^{*})f_{\\phi}(v)<1}\\end{array}$ , there exists $\\hat{v}\\in V(x^{*})$ such that $f_{\\phi}(\\hat{v})=0$ . Since $|V(x^{*})|\\leq d+1$ , it takes addition $d+1$ queries to find $\\hat{v}$ with $f_{\\phi}(\\hat{v})=0$ . By Fact 1 and the fact that we can simulate every value/gradient query of $f_{\\phi}$ by $d+1$ queries on $\\phi$ , $\\boldsymbol{\\mathcal{A}}$ makes at least $\\Omega(2^{d}/d)$ value/gradient queries. ", "page_idx": 30}, {"type": "text", "text": "Suppose there exists an alg\u221aorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ that outputs an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium $\\sigma$ in time $T(B)$ for $\\varepsilon<1$ and $\\delta=\\sqrt{d}$ . We construct another algorithm $\\mathcal{C}$ for SAT that terminates in time $T(B)\\cdot\\mathrm{poly}(d).\\,C$ : (1) given a boolean formula $\\phi$ , construct $f_{\\phi}$ as described above; (2) run $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and get output $\\sigma$ (3) check the support of $\\sigma$ to find $v\\in\\{0,1\\}^{d}$ such that $f_{\\phi}(v)=0$ ; (3) if finds $v\\in\\{0,1\\}^{d}$ such that $f_{\\phi}(v)=0$ , then $\\phi$ is satisfiable, otherwise $\\phi$ is not satisfiable. Since we can evaluate $f_{\\phi}$ and $\\nabla f_{\\phi}$ in $\\mathrm{poly}(d)$ time and the support of $\\sigma$ is smaller than $T(B)$ , the algorithm $\\mathcal{C}$ terminates in time $O(T(B)\\cdot\\mathrm{poly}(d))$ . The above gives a polynomial time reduction from SAT to finding an $\\varepsilon$ -approximate $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}(\\delta)$ -equilibrium and proves the NP-hardness of the latter problem. ", "page_idx": 30}, {"type": "text", "text": "I.2 Proof of Corollary 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Let $\\phi:\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$ be a boolean formula and define $f_{\\phi}:[0,1]^{d}\\to[0,1]$ the same as that in Theorem 13. We know $f_{\\phi}$ is $\\Theta(\\mathrm{poly}(d))$ -Lipschitz and $\\Theta(\\mathrm{poly}(d))$ -smooth. Now we let the adversary pick $f_{\\phi}$ each time. For any $T\\leq O(2^{d}/d)$ , in case there exists an online learning algorithm with $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}<\\frac{T}{2}$ , then $\\begin{array}{r}{\\sigma:=\\frac{1}{T}\\sum_{t=1}^{T}1_{x^{t}}}\\end{array}$ is an $\\textstyle{\\big(}{\\frac{1}{2}},\\delta{\\big)}$ -equilibrium. Applying Theorem 13 and the fact that in this case, $\\mathrm{Reg}_{\\mathrm{Proj},\\delta}^{T}$ is non-decreasing with respect to $T$ concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "J Removing the $D$ dependence for $\\Phi_{\\mathrm{Proj}}^{\\mathcal{X}}$ -regret ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For the regime $\\delta\\leq D_{\\mathcal{X}}$ which we are more interested in, the lower bound in Theorem 7 is $\\Omega(G\\delta\\sqrt{T})$ while the upper bound in Theorem 3 is $O(G\\sqrt{\\delta D_{X}T})$ . They are not tight especially when $D_{\\mathcal{X}}\\gg\\delta$ . A natural question is: which of them is the tight bound? We conjecture that the lower bound is tight. In fact, for the special case\u221a where the feasible set $\\mathcal{X}$ is a box, we have a way to obtain a $D_{\\mathcal{X}}$ -independent bound $O(d^{\\frac{1}{4}}G\\delta\\sqrt{T})$ , which is tight when $d\\,=\\,1$ . Below, we first describe the improved strategy in 1-dimension. Then we show how to extend it to the $d_{\\cdot}$ -dimensional box setting. ", "page_idx": 30}, {"type": "text", "text": "J.1 One-Dimensional Case ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In one-dimension, we assume that $\\textstyle{\\mathcal{X}}=[a,b]$ for some $b-a\\geq2\\delta$ (if $b-a\\leq2\\delta$ , then our original bound in Theorem 3 is already of order $G\\delta\\sqrt{T})$ . We first investigate the case where $f^{t}(x)$ is a linear function, i.e., $f^{t}(x)=g^{t}x$ for some $g^{t}\\in[-G,G]$ . The key idea is that we will only select $x^{t}$ from the two intervals $[a,a+\\delta]$ and $[b-\\delta,b]$ , and never play $\\dot{\\boldsymbol{x}}^{t}\\in\\left(a+\\delta,b-\\delta\\right)$ . To achieve so, we concatenate these two intervals, and run an algorithm in this region whose diameter is only $2\\delta$ . The key property we would like to show is that the regret is preserved in this modified problem. ", "page_idx": 30}, {"type": "text", "text": "More precisely, given the original feasible set $\\textstyle{\\mathcal{X}}=[a,b]$ , we create a new feasible set $\\mathcal{V}=[-\\delta,\\delta]$ and apply our algorithm GD in this new feasible set. The loss function is kept as $f^{t}(x)\\,=\\,g^{t}x$ . Whenever the algorithm for $\\boldsymbol{\\wp}$ outputs $y^{t}\\in[-\\delta,0]$ , we play $x^{t}=y^{t}+a+\\dot{\\delta}$ in $\\mathcal{X}$ ; whenever it outputs $y^{t}\\in(0,\\bar{\\delta}]$ , we play $x^{t}=y^{\\bar{t}}+b^{\\underline{{\\,}}}-\\delta$ . Below we show that the regret is the same in these two problems. Notice that when $y^{t}\\leq0$ , we have for any $v\\in[-\\delta,\\delta]$ , ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{x^{t}-\\Pi_{X}[x^{t}-v]=x^{t}-\\operatorname*{max}\\left(\\operatorname*{min}\\left(x^{t}-v,b\\right),a\\right)}\\\\ {=x^{t}-\\operatorname*{max}\\left(x^{t}-v,a\\right)}\\\\ {(x^{t}-v=y^{t}+a+\\delta-v\\leq a+2\\delta\\leq b\\mathrm{~always~holds})}\\\\ {=y^{t}+a+\\delta-\\operatorname*{max}\\left(y^{t}+a+\\delta-v,a\\right)}\\\\ {=y^{t}-\\operatorname*{max}\\left(y^{t}-v,-\\delta\\right)}\\\\ {=y^{t}-\\operatorname*{max}\\left(\\operatorname*{min}\\left(y^{t}-v,\\delta\\right),-\\delta\\right)}&{(y^{t}-v\\leq\\delta\\mathrm{~always~holds})}\\\\ {=y^{t}-\\Pi_{Y}[y^{t}-v]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, when $y^{t}>0$ , we can follow the same calculation and prove $x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-v]=y^{t}-$ $\\Pi_{\\mathcal{Y}}[y^{t}-\\boldsymbol{v}]$ . Thus, the regret in the two problems: ", "page_idx": 31}, {"type": "equation", "text": "$$\ng^{t}\\left(x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-v]\\right)\\quad\\mathrm{and}\\quad g^{t}\\left(y^{t}-\\Pi_{\\mathcal{Y}}[y^{t}-v]\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "are exactly the same for any $v$ . Finally, observe that the diameter of $\\boldsymbol{\\wp}$ is only of order $O(\\delta)$ . Thus, the upper bound in Theorem 3 would give us an upper bound of $O(G{\\sqrt{\\delta\\cdot\\delta T}})=O(G\\delta{\\sqrt{T}})$ . ", "page_idx": 31}, {"type": "text", "text": "For convex $f^{t}$ , we run the algorithm above with $g^{t}=\\nabla f^{t}(x^{t})$ . Then by convexity we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nf^{t}(x^{t})-f^{t}(\\Pi_{\\mathcal{X}}[x^{t}-v])\\leq g^{t}(x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-v])=g^{t}(y^{t}-\\Pi_{\\mathcal{Y}}[y^{t}-v]),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "so the regret in the modified problem (which is $O(G\\delta\\sqrt{T}))$ still serves as a regret upper bound for the original problem. ", "page_idx": 31}, {"type": "text", "text": "J.2 $d$ -Dimensional Box Case ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "A $d$ -dimensional box is of the form $\\mathcal{X}=[a_{1},b_{1}]\\times[a_{2},b_{2}]\\times\\cdot\\cdot\\cdot\\times[a_{d},b_{d}]$ . The box case is easy to deal with because we can decompose the regret into individual components in each dimension. Namely, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{t}(x^{t})-f^{t}(\\Pi_{\\mathcal{X}}[x^{t}-v])\\le\\nabla f^{t}(x^{t})^{\\top}\\left(x^{t}-\\Pi_{\\mathcal{X}}[x^{t}-v]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{d}g_{i}^{t}\\left(x_{i}^{t}-\\Pi_{\\mathcal{X}_{i}}[x_{i}^{t}-v_{i}]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we define $\\mathcal{X}_{i}=[a_{i},b_{i}]$ , $g^{t}=\\nabla f^{t}(x^{t})$ , and use subscript $i$ to indicate the $i$ -th component of a vector. The last equality above is guaranteed by the box structure. This decomposition allows as to view the problem as $d$ independent 1-dimensional problems. ", "page_idx": 31}, {"type": "text", "text": "Now we follow the strategy described in Section J.1 to deal with individual dimensions (if $b_{i}\\!-\\!a_{i}<2\\delta$ then we do not modify $\\mathcal{X}_{i}$ ; otherwise, we shrink $\\mathcal{X}_{i}$ to be of length 2\u03b4). Applying the analysis of Theorem 3 to each dimension, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\sum_{i=1}^{d}g_{i}^{t}\\left(x_{i}^{t}-\\Pi_{\\mathcal{X}_{i}}[x_{i}^{t}-v_{i}]\\right)}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}\\left(\\frac{v_{i}^{2}}{2\\eta}+\\frac{\\eta}{2}\\sum_{t=1}^{T}(g_{i}^{t})^{2}+\\frac{|v_{i}|\\times2\\delta}{\\eta}\\right)\\mathrm{~(the~diameter~in~each~dimension~is~now~bounded~by~2\\delta~}}\\\\ &{\\le O\\left(\\frac{\\delta\\sum_{i=1}^{d}|v_{i}|}{\\eta}+\\eta G^{2}T\\right)}\\\\ &{\\le O\\left(\\frac{\\delta^{2}\\sqrt{d}}{\\eta}+\\eta G^{2}T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Choosing the optimal $\\begin{array}{r}{\\eta=\\frac{d^{\\frac{1}{4}}\\delta}{G\\sqrt{T}}}\\end{array}$ , we get the regret upper bound of order $O\\left(d^{\\frac{1}{4}}G\\delta\\sqrt{T}\\right)$ . ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 32}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 32}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 32}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 32}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 32}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We state the problem setting and our main contributions in the abstract and introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our results provide efficient uncoupled algorithms to compute $\\varepsilon$ -approximate $\\Phi$ -equilibria in the first-order stationary regime. We leave the computational complexity of finding $\\varepsilon$ -approximate $\\Phi$ -equilibria beyond the first-order stationary regime as an open question. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We provide assumptions and proofs for our theoretical results in the main body and the appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not contain experimental results. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not contain experimental results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 34}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not contain experimental results. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not contain experimental results. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not contain experimental results. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and the current paper conforms the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This is a purely theoretical paper and we do not see any immediate societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This is a purely theoretical paper ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This is a purely theoretical paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This is a purely theoretical paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not contain experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not contain experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]