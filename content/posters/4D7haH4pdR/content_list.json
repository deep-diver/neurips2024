[{"type": "text", "text": "Bias Detection via Signaling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiling Chen Tao Lin Ariel D. Procaccia Harvard University Harvard University Harvard University yiling@seas.harvard.edu tlin@g.harvard.edu arielpro@g.harvard.edu Aaditya Ramdas Itai Shapira Carnegie Mellon University Harvard University aramdas@cmu.edu itaishapira@g.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent\u2019s beliefs directly, we take an approach inspired by information design. Specifically, we measure an agent\u2019s bias by designing a signaling scheme and observing the actions the agent takes in response to different signals, assuming that the agent maximizes their own expected utility. Our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A bag contains two coins that look and feel identical, but one is a fair coin that, on a flip, comes up heads with probability 0.5, and the other is an unfair coin with probability 0.9 of heads. You reach into the bag, grab one of the coins and flip it once; it lands on heads. Since you are (hopefully) familiar with Bayes\u2019 rule, you conclude that the probability you are holding the fair coin is $\\approx0.36$ . Now suppose you are offered the following deal: if you pay $\\mathbb{S}1$ , you get to flip the same coin again, and if it comes up heads, you will receive $\\mathbb{S}1.4$ . Since you now believe that the probability of heads is 0.76, you take the deal (assuming you are risk neutral) and earn 6 cents in expectation. ", "page_idx": 0}, {"type": "text", "text": "If, by contrast, another risk-neutral person in the same situation decides to decline the same deal, they must believe that the probability they are holding the fair coin is greater than 0.47. That is, their belief is still very close to the prior of 0.5. We think of such a person as being biased, in the sense that they are unwilling to significantly update their beliefs, despite evidence to the contrary. ", "page_idx": 0}, {"type": "text", "text": "Of course, failing to update one\u2019s beliefs about coin flips is not the end of the world. But this example serves to illustrate a broader phenomenon that, in our view, is both important and ubiquitous. In particular, the \u201cstickiness\u201d of prior beliefs in the face of evidence plays a role in politics \u2014 think of the controversy over Russian collusion in the 2016 US presidential election or the existence of weapons of mass destruction in Iraq in 2003. It is also prevalent in science, as exemplified by the polarized debate over the origins of the Covid pandemic [3]. ", "page_idx": 0}, {"type": "text", "text": "Our goal in this paper is to develop algorithms that are able to detect bias in the form of non-Bayesian updating of beliefs. To our knowledge, we are the first to formalize and analytically address this problem, and we aim to build an initial framework that future work would build on. In the long term, we believe such algorithms could have many applications, including understanding to what degree the foregoing type of bias contributes to disagreement and polarization, and discounting the opinions of biased agents to improve collective decision making. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our approach. The first question we need to answer is how to quantify bias. In this first investigation, we adopt a linear model of bias that was proposed and used as a general belief updating model in economics [8, 10, 5, 18]. If the prior is $\\mu_{0}$ and the correct Bayesian posterior upon receiving a signal (or evidence) $s$ is denoted $\\mu_{s}$ , we posit that an agent with bias $\\bar{w}\\in[0,1]$ adopts the belief $w\\mu_{0}+(1-w)\\mu_{s}$ . At the extremes, an agent with bias $w=0$ performs perfect Bayesian updating and an agent with bias $w=1$ cannot be convinced to budge from the prior. ", "page_idx": 1}, {"type": "text", "text": "The bigger conceptual question is how we can infer an agent\u2019s bias. To address it, we take an approach that is inspired by the literature on information design [13]. In our context, suppose that we (the principal) and the agent have asymmetric information: while both share a common (say public) prior about the state of the world, the principal knows the true (realized) state of the world, but the agent does not. The principal publicly commits to a (randomized) signaling scheme that specifies the probability of sending each possible signal given each possible realized state of the world. Given their knowledge of the latter, the principal draws a signal from the specified distribution and sends it to the agent. Upon receiving such a signal, the agent updates their beliefs about the state of the world (from the common prior) and then takes an action that maximizes their expected utility according to a given utility function. Similarly to the example we started with, it is the action taken by the agent that can (indirectly) reveal their degree of bias. ", "page_idx": 1}, {"type": "text", "text": "Note that the problem of estimating the exact level of bias reduces to the problem of detecting whether the agent\u2019s bias is above or below some threshold. Indeed, to estimate the level of bias to an accuracy of \u03f5, $\\log(1/\\epsilon)$ such threshold queries suffice by using binary search. The challenge, then, is to design signaling schemes that test whether bias is above or below a given threshold in the most efficient way, that is, using a minimum number of signals in expectation. ", "page_idx": 1}, {"type": "text", "text": "Our results. We design a polynomial-time algorithm that computes optimal signaling schemes, in Section 4. We first show that constant algorithms, which repeatedly use the same signaling scheme, are as powerful as adaptive algorithms, which can vary the scheme over time based on historical data (Lemma 4.1); we can therefore restrict our attention to constant algorithms. In Lemma 4.5, we establish a version of the revelation principle for the bias detection problem, which asserts that optimal signaling schemes need only use signals that can be interpreted as action recommendations. Finally, building on these insights, we show that the optimal solution to our problem is obtained by solving a \u201csmall\u201d linear program (Algorithm 1 and Theorem 4.6). ", "page_idx": 1}, {"type": "text", "text": "In Section 5, we present a geometric characterization of optimal signaling schemes (Theorem 5.2), which sheds additional light on the performance of the algorithm. In particular, the characterization provides sufficient and necessary conditions for the testability of bias, and also identifies cases where only a single sample is needed for this task. ", "page_idx": 1}, {"type": "text", "text": "Related work. There is a significant body of experimental work in the social sciences aiming to explain the failure of partisans to reach similar beliefs on factual questions where there is a large amount of publicly available evidence. The fact that biased belief updating occurs is undisputed (to our knowledge), and the focus is on understanding the factors that play a role. In particular, a prominent line of work supports the (perhaps counterintuitive) hypothesis that the more cognitively sophisticated a partisan is, the more politically biased is their belief update process [16, 17, 12, 11]. These results are challenged by more recent work by Tappin et al. [19], who found that greater analytical thinking is associated with belief updates that are less biased, using an experimental design that explicitly measures the proximity of belief updates to a correct Bayesian posterior. While these studies provide empirical underpinnings for our theoretical model, their research questions are orthogonal to ours: we aim to measure the magnitude of bias regardless of its source. ", "page_idx": 1}, {"type": "text", "text": "Classical work in information design [4, 13] studies how a principal can strategically provide information to induce an agent to take actions that are beneficial for the principal, assuming a perfectly Bayesian agent. Various relaxations of the perfectly Bayesian assumption have been investigated [1, 10, 7, 5, 9, 21, 14]. The work by de Clippel and Zhang [5] is close to us, which studies biased belief update models including the linear model. However, their goal is to maximize the principal\u2019s utility with the agent\u2019s bias fully known. In our problem the agent\u2019s bias level is unknown, and the principal\u2019s goal is to infer the agent\u2019s bias level instead of maximizing their own utility. Tang and ", "page_idx": 1}, {"type": "text", "text": "Ho [18] present real-world experiments showing that human belief updates are close to a linear bias model, which supports our theoretical assumption. ", "page_idx": 2}, {"type": "text", "text": "2 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Biased agent. Consider a standard Bayesian setting: the relevant state of the world is $\\theta\\:\\in\\:\\Theta$ , distributed according to some known prior distribution $\\mu_{0}$ . If an agent were perfectly Bayesian, when receiving some new information (\u201csignal\u201d) $s$ and with the knowledge of the conditional distributions $P(s{\\bar{|}}\\theta)$ for all $\\theta$ , they would update their belief about the state of the world according to Bayes\u2019 Rule: $\\begin{array}{r}{\\mu_{s}(\\theta)=P(\\theta|s)=\\frac{\\mu_{0}(\\theta)P(s|\\theta)}{P(s)}}\\end{array}$ . We refer to $\\mu_{s}$ as the true posterior belief induced by $s$ . Being biased, the agent\u2019s belief after seeing $s$ , denoted $\\nu_{s}$ , is a convex combination of $\\mu_{s}$ and $\\mu_{0}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nu_{s}=w\\mu_{0}+(1-w)\\mu_{s},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $w\\,\\in\\,[0,1]$ is the unknown bias level, capturing the agent\u2019s inclination to retain their prior belief in the presence of new information. This linear model was proposed and adopted in economics for non-Bayesian belief updating [8, 10], in order to capture people\u2019s conservatism in processing new information and their tendency to protect their beliefs [20]. ", "page_idx": 2}, {"type": "text", "text": "The agent can choose an action from a finite set $A$ and has a state-dependent utility function $U:$ $A\\times{\\bar{\\Theta}}\\,\\rightarrow\\,\\mathbb{R}$ . They receive utility $U(a,\\theta)$ when taking action $a$ in state $\\theta$ . The agent will act according to their (biased) belief $\\nu_{s}$ and choose an action $a$ that maximizes their expected utility: ", "page_idx": 2}, {"type": "equation", "text": "$$\na\\in\\underset{a\\in A}{\\arg\\operatorname*{max}}~\\mathbb{E}_{\\theta\\sim\\nu_{s}}[U(a,\\theta)]=\\underset{a\\in A}{\\arg\\operatorname*{max}}~\\sum_{\\theta\\in\\Theta}\\nu_{s}(\\theta)U(a,\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the absence of any additional information, the agent operates based on the prior belief $\\mu_{0}$ and will select an action deemed optimal with respect to $\\mu_{0}$ . We introduce the following mild assumption to ensure the uniqueness of this action: ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. There is a unique action that maximizes the expected utility based on the prior belief $\\begin{array}{r}{\\dot{\\mu_{0}}\\colon|\\operatorname*{arg\\,max}_{a\\in A}\\{\\sum_{\\theta\\in\\Theta}\\bar{\\mu}_{0}(\\theta)U(a,\\theta)\\}|=1.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "This assumption will be made throughout the paper. We denote the unique optimal action on the prior belief as $\\begin{array}{r}{a_{0}=\\arg\\operatorname*{max}_{a\\in A}\\{\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)U(a,\\theta)\\}}\\end{array}$ , and call it the default action. ", "page_idx": 2}, {"type": "text", "text": "Bias detection. The principal, who knows the prior $\\mu_{0}$ and the agent\u2019s utility function $U$ , seeks to infer the agent\u2019s bias level from their action as efficiently as possible. The principal has an informational advantage \u2014 they observe the independent realizations of the state of the world at each time step. In other words, the principal knows $\\theta_{t}$ , an independent sample drawn according to $\\mu_{0}$ at time $t$ . The principal wants to design signaling schemes to strategically reveal information about $\\theta_{t}$ to the agent, hoping to influence the agent\u2019s biased belief in a way that the agent\u2019s chosen actions reveal information about their bias level. Specifically, with a finite signal space $S$ , the principal can commit to a signaling scheme $\\pi_{t}:\\Theta\\to\\Delta(S)$ at time $t$ , where $\\pi_{t}(s|\\theta)$ specifies the probability of sending signal $s$ in state $\\theta$ at time $t$ . After seeing a signal $s_{t}$ , drawn according to $\\pi_{t}(s|\\theta_{t})$ at time $t$ , the agent takes action $a_{t}$ that is optimal for their biased belief $\\nu_{s_{t}}$ . The principal infers information about bias $w$ from the history of signaling schemes, realized states, realized signals, and agent actions $\\mathcal{H}_{t}=\\left\\{(\\pi_{1},\\theta_{1},s_{1},a_{1}),\\dots,(\\pi_{t},\\theta_{t},s_{t},a_{t})\\right\\}$ . We denote by $\\Pi$ an adaptive algorithm that the principal uses to decide on the signaling scheme at time $t+1$ based on history $\\mathcal{H}_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "Given a threshold $\\tau\\in(0,1)$ , the principal wants to design $\\Pi$ to answer the following question: ", "page_idx": 2}, {"type": "text", "text": "As noted earlier, by answering the above threshold question, one can also estimate the bias level $w$ within accuracy $\\epsilon$ by performing $\\log(1/\\epsilon)$ iterations of binary search. This effectively reduces the broader task of estimating $w$ to a sequence of targeted threshold checks. By employing an adaptive signaling scheme, this approach lets us approximate $w$ to any desired precision, providing an efficient solution to the bias estimation problem. ", "page_idx": 2}, {"type": "text", "text": "An algorithm $\\Pi$ for the above question terminates as soon as it can output a deterministic answer. The number of time steps for $\\Pi$ to terminate, denoted by $T_{\\tau}(\\Pi,w)$ , is a random variable. The sample complexity of $\\Pi$ is defined to be the expected termination time in the worst case over $w\\in[0,1]$ : ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (sample complexity). The (worst-case) sample complexity of $\\Pi$ is defined $a s^{2}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{\\tau}(\\Pi)=\\operatorname*{max}_{w\\in[0,1]}\\mathbb{E}[T_{\\tau}(\\Pi,w)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our objective is to develop an algorithm $\\Pi$ that can determine whether $w~\\geq~\\tau$ or $w~\\leq~\\tau$ with minimal sample complexity. Specifically, we want to solve the following minimax problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Pi}\\operatorname*{max}_{w\\in[0,1]}\\mathbb{E}[T_{\\tau}(\\Pi,w)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We say that an algorithm $\\Pi$ is constant if it keeps using the same signaling scheme repeatedly until termination. Constant algorithms are a special case of non-adaptive algorithms, which may vary the signaling schemes over time but remain independent of historical data. ", "page_idx": 3}, {"type": "text", "text": "Preliminaries. We now introduce the well-known splitting lemma from the information design literature [2, 13, 15]. It relates a signaling scheme with a set of induced true posteriors for a Bayesian agent and a distribution over the set of true posteriors. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1 (Splitting Lemma, e.g., [13]). Let $\\pi$ be a signaling scheme where each signal $s\\ \\in$ $S$ is sent with unconditional probability $\\begin{array}{r}{\\pi(s)\\;=\\;\\sum_{\\theta\\in\\Theta}\\bar{\\mu}_{0}(\\theta)\\bar{\\pi}(s|\\theta)}\\end{array}$ and induces true posterior $\\mu_{s}$ . Then, the prior $\\mu_{0}$ equals the convex combination of $\\{\\mu_{s}\\}_{s\\in S}$ with weights $\\{\\pi(s)\\}_{s\\in S}$ : $\\mu_{0}\\,=$ $\\textstyle\\sum_{s\\in S}\\pi(s)\\mu_{s}$ . Conversely, if the prior can be expressed as a convex combination of distributions $\\mu_{s}^{\\prime}\\in\\Delta(\\Theta)$ : $\\begin{array}{r}{\\mu_{0}=\\sum_{s\\in S}p_{s}\\mu_{s}^{\\prime}}\\end{array}$ , where $\\begin{array}{r}{p_{s}\\geq0,\\sum_{s\\in S}p_{s}=1,}\\end{array}$ , then there exists a signaling scheme $\\pi$ where each signal $s$ is sent with unconditional probability $\\pi(s)=p_{s}$ and induces posterior $\\mu_{s}^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "The splitting lemma is also referred as the Bayesian consistency condition. It allows one to think about choosing a signaling scheme as choosing a set of true posteriors, $\\{\\mu_{s}\\}_{s\\in S}$ , and a distribution over the set, $\\bar{\\{\\pi(s)\\}_{s\\in S}}$ , in a Bayesian consistent way. ", "page_idx": 3}, {"type": "text", "text": "3 Warm-Up: A Two-State, Two-Action Example ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "How can the principal design a signaling scheme to learn the agent\u2019s bias level? We use a simple two-state, two-action example to demonstrate how inducing a specific true posterior belief will allow the principal to determine whether $w\\geq\\tau$ or $w\\leq\\tau$ . ", "page_idx": 3}, {"type": "text", "text": "The two states of the world are represented as $\\{\\mathrm{Good},\\,\\mathrm{Bad}\\}$ . The agent has two possible actions: Active and Passive. Taking the Passive action always yields a utility of 0, independently of the state. For the Active action, the utility is $a$ if the state is Good and $-b$ otherwise; $a,b>0$ . We use the probability of the Good state to represent a belief, so the prior is a number $\\mu_{0}\\in[0,1]$ , which is only a slight abuse of notation. With belief $\\mu\\in[0,1]$ for the Good state (and $1-\\mu$ for the Bad state), the agent\u2019s expected utility for choosing the Active action is $a\\mu-b(1-\\mu)=(a+b)\\mu-b$ . Thus, the Active action is better than the Passive action (so the agent will take Active) if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(a+b)\\mu-b>0\\quad\\iff\\quad\\mu>\\frac{b}{a+b}=:\\mu^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Conversely, the Passive action is better if $\\mu<\\mu^{*}$ . Here, $\\begin{array}{r}{\\mu^{*}=\\frac{b}{a+b}}\\end{array}$ is an indifference belief where the agent is indifferent between the two actions. We assume that the prior $\\mu_{0}$ satisfies $0<\\mu_{0}<\\mu^{*}$ , so the agent chooses the Passive action by default. ", "page_idx": 3}, {"type": "text", "text": "Consider the following constant signaling scheme $\\pi_{\\tau}$ with two signals $\\{G,B\\}$ : ", "page_idx": 3}, {"type": "text", "text": "\u2022 If the state is Good, send signal $G$ with probability one.   \n\u2022 If the state is Bad, send signal B with probability(\u00b5\u2217\u2212\u03c4\u00b5\u00b50\u2212)\u00b5(01\u2212\u00b50) and signal $G$ with the complement probability. ", "page_idx": 3}, {"type": "text", "text": "We will show that, by repeatedly using $\\pi_{\\tau}$ , we can test whether the agent\u2019s bias $w$ is $\\leq\\tau$ or $\\geq\\tau$ . By Bayes\u2019 Rule, the true posterior beliefs (for the Good state) associated with the two signals are $\\mu_{B}=0$ (i.e., on receiving $B$ , the agent knows the state is Bad for sure) and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{G}=P(\\operatorname{Good}|G)=\\frac{\\mu_{0}\\cdot\\pi_{\\tau}(G|\\operatorname{Good})}{\\mu_{0}\\cdot\\pi_{\\tau}(G|\\operatorname{Good})+(1-\\mu_{0})\\cdot\\pi_{\\tau}(G|\\operatorname{Bad})}=\\frac{\\mu^{*}-\\tau\\mu_{0}}{1-\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notably, the posterior $\\mu_{G}$ satisfies the following property: if the agent\u2019s bias level $w$ is exactly equal to $\\tau$ , then the agent\u2019s biased belief is equal to the indifference belief: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{when}\\;w=\\tau,\\qquad\\nu_{G}=\\tau\\mu_{0}+(1-\\tau)\\mu_{G}=\\mu^{*}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also note the inequality $\\mu_{0}<\\mu^{*}<\\mu_{G}$ . As a result, if the agent\u2019s bias level $w$ is greater than $\\tau$ , then the biased belief will be smaller than $\\mu^{*}$ , and otherwise the opposite is true: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{for}\\ w>\\tau,\\quad w\\mu_{0}+(1-w)\\mu_{G}<\\mu^{*};\\qquad\\mathrm{for}\\ w<\\tau,\\quad w\\mu_{0}+(1-w)\\mu_{G}>\\mu^{*}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By Equation (1), this means that the agent will take the Passive action if $w>\\tau$ , and the Active action if $w<\\tau$ (on receiving $G$ ). Therefore, by observing which action is taken by the agent when signal $G$ is sent, we can immediately tell whether $w\\leq\\tau$ or $w\\geq\\tau$ . This leads to the following: ", "page_idx": 4}, {"type": "text", "text": "cTohnesotarentm s i3g.n1.a liInng t hsec htewmoe- s e,c atnw ot-eastc tiwohne tehxear mtphlee , afgoern ta\u2019sn yb tiharse osladt $\\scriptstyle\\tau\\ \\in\\ [0,\\,{\\frac{1-\\mu^{*}}{1-\\mu_{0}}}],$ o, rt e: $\\pi_{\\tau}$ $w$ $w~\\leq~\\tau$ $w~\\geq~\\tau$ specifically, whenever the signal $G$ is sent, ", "page_idx": 4}, {"type": "text", "text": "\u2022 if the agent takes action Active, then $w\\leq\\tau$ , \u2022 if the agent takes action Passive, then $w\\geq\\tau$ . ", "page_idx": 4}, {"type": "text", "text": "The sample complexity of this scheme is \u00b5\u00b50\u2217(1\u2212\u2212\u00b5\u03c40) + 1, which increases with \u03c4. ", "page_idx": 4}, {"type": "text", "text": "Proof. The range $\\begin{array}{r}{\\tau\\,\\in\\,[0,\\frac{1-\\mu^{*}}{1-\\mu_{0}}]}\\end{array}$ ensures that the probability $\\begin{array}{r}{\\pi_{\\tau}(B|\\mathrm{Bad})\\,=\\,\\frac{\\mu^{*}-\\mu_{0}}{(\\mu^{*}-\\tau\\mu_{0})(1-\\mu_{0})}}\\end{array}$ is in . The two items in the theorem follow from the argument before the theorem statement. The sample complexity is equal to the expected number of time steps until a $G$ signal is sent, which is a geometric random variable with success probability $P(G)\\,=\\,\\mu_{0}\\pi_{\\tau}(G|\\mathrm{Good})+(1\\,-$ \u00b50)\u03c0\u03c4(G|Bad) = \u00b5\u00b50\u2217(\u22121\u03c4\u2212\u00b5\u03c40) . So the sample complexity is equal to the meanP (1G) =\u00b5\u00b50\u2217(1\u2212\u2212\u00b5\u03c40) +1. ", "page_idx": 4}, {"type": "text", "text": "The main intuition behind this result is that in order to test whether $w\\geq\\tau$ or $w\\leq\\tau$ , we design a signaling scheme where certain signals induce posteriors that make the agent indifferent between two actions if the agent\u2019s bias level is exactly $\\tau$ . Then, the action actually taken by the agent will directly reveal whether $w\\geq\\tau$ or $w\\leq\\tau$ . Such signals are useful signals, but not all signals are necessarily useful. The sample complexity is then determined by the total probability of useful signals. This intuition will carry over to computing the optimal signaling scheme for the general case in Section 4. ", "page_idx": 4}, {"type": "text", "text": "Finally, we remark that using the constant signaling scheme $\\pi_{\\tau}$ constructed above to test $w\\geq\\tau$ or $w\\leq\\tau$ is in fact the optimal adaptive algorithm, according to the results we will present in Section 4. So, the minimal sample complexity to test whether $w\\geq\\tau$ or $w\\leq\\tau$ in this two-state, two-action example is exactly $\\begin{array}{r}{\\frac{\\mu^{*}-\\mu_{0}}{\\mu_{0}(1-\\tau)}+1}\\end{array}$ as shown in Theorem 3.1. ", "page_idx": 4}, {"type": "text", "text": "4 Computing the Optimal Signaling Scheme in the General Case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we generalize the initial observations from the previous section to the case with any number of actions and states and general utility function $U$ . We will show how to compute the optimal algorithm (signaling scheme) to test the agent\u2019s bias level. There are three key ingredients. First, we prove that we can use a constant signaling scheme. Second, we develop a \u201crevelation principle\u201d to further simplify the space of signaling schemes. Building on these two steps, we show that the optimal signaling scheme can be computed by a linear program. ", "page_idx": 4}, {"type": "text", "text": "4.1 Optimality of Constant Signaling Schemes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we show that adaptive algorithms are no better than constant algorithms for the problem of testing whether $w\\geq\\tau$ or $w\\leq\\tau$ . Therefore, to find the algorithm with minimal sample complexity, we only need to consider constant algorithms/signaling schemes. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.1. Fix $\\tau\\,\\in\\,(0,1)$ . For the problem of testing whether $w\\ \\geq\\ \\tau$ or $w\\ \\leq\\ \\tau$ , the sample complexity of any adaptive algorithm is at least that of the optimal constant algorithm (i.e., using $a$ fixed signaling scheme repeatedly). ", "page_idx": 5}, {"type": "text", "text": "To prove this lemma, we introduce some notations. For any action $a\\in A\\setminus\\{a_{0}\\}$ , define vector ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{a}=(c_{a,\\theta})_{\\theta\\in\\Theta}=\\left(U(a_{0},\\theta)-U(a,\\theta)\\right)_{\\theta\\in\\Theta}\\in\\mathbb{R}^{|\\Theta|},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "whose components are the utility differences between the default action $a_{0}$ and any other action $a$ at different states $\\theta\\in\\Theta$ . Let $R_{a_{0}}\\subseteq\\Delta(\\Theta)$ be the region of beliefs under which the agent strictly prefers $a_{0}$ over any other action: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{a_{0}}=\\big\\{\\mu\\in\\Delta(\\Theta)\\mid\\forall a\\in A\\setminus\\{a_{0}\\},\\,c_{a}^{\\top}\\mu>0\\big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is the intersection of $|A|-1$ open halfspaces with the probability simplex $\\Delta(\\Theta)$ . As the agent strictly prefers $a_{0}$ at the prior $\\mu_{0}$ , we have $\\mu_{0}\\in R_{a_{0}}$ . The boundary of this region, $\\partial R_{a_{0}}$ , is the set of beliefs where the agent is indifferent between $a_{0}$ and at least one other action $a\\in A\\setminus\\{a_{0}\\}$ and $a_{0}$ and $a$ are both (weakly) better than any other action: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\partial R_{a_{0}}=\\left\\{\\mu\\in\\Delta(\\Theta)\\mid\\exists a\\in A\\setminus\\{a_{0}\\},c_{a}^{\\top}\\mu=0\\mathrm{~and~}\\forall a^{\\prime}\\in A\\setminus\\{a_{0}\\},c_{a^{\\prime}}^{\\top}\\mu\\geq0\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lastly, the exterior of $R_{a_{0}}$ , denoted as $\\mathrm{ext}R_{a_{0}}$ , comprises the set of beliefs where the agent strictly prefers not to choose $a_{0}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{ext}R_{a_{0}}=\\Delta(\\Theta)\\setminus\\left(R_{a_{0}}\\cup\\partial R_{a_{0}}\\right)=\\left\\{\\mu\\in\\Delta(\\Theta)\\mid\\exists a\\in A\\setminus\\{a_{0}\\},\\,c_{a}^{\\top}\\mu<0\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given a signaling scheme $\\pi$ , we classify its signals into three types based on the location of the biased belief associated with the signal with respect to the region $R_{a_{0}}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1. Let $\\tau\\in(0,1)$ be a parameter. Let $s\\,\\in\\,S$ be a signal from a signaling scheme $\\pi$ , with associated true posterior $\\mu_{s}$ and $\\tau$ -biased posterior $\\mu_{s}^{\\tau}=\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ . We say s is ", "page_idx": 5}, {"type": "text", "text": "\u2022 an internal signal if \u00b5\u03c4s \u2208Ra0;   \n\u2022 a boundary signal if $\\mu_{s}^{\\tau}\\in\\partial R_{a_{0}}$ ;   \n\u2022 an external signal if $\\mu_{s}^{\\tau}\\in\\mathrm{ext}R_{a_{0}}$ . ", "page_idx": 5}, {"type": "text", "text": "The above classification helps to formalize the idea of whether a signal is \u201cuseful\u201d for bias detection. A boundary signal is useful because the action taken by the agent after receiving a boundary signal immediately tells whether $w\\geq\\tau$ or $w\\leq\\tau$ : ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2. When a boundary signal is realized, the agent\u2019s action immediately reveals whether $w\\geq\\tau$ or $w\\leq\\tau$ . Specifically, if the agent chooses action $a_{0}$ , then $w\\geq\\tau$ ; otherwise, $w\\leq\\tau$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. If the agent\u2019s bias level satisfies $w<\\tau$ , then the biased belief $\\nu_{s}=w\\mu_{0}+(1-w)\\mu_{s}$ must be inside $R_{a_{0}}$ (because $\\mu_{s}^{\\tau}=\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ is on the boundary of $R_{a_{0}}$ and $\\mu_{0}\\in R_{a_{0}})$ , so the agent strictly prefers the default action $a_{0}$ . If $w>\\tau$ , then the biased belief $\\nu_{s}$ is outside of $R_{a_{0}}$ , so the agent will not take action $a_{0}$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "An external signal might also be useful in revealing whether $w\\geq\\tau$ or $w\\leq\\tau$ if the agent is indifferent between some actions $a_{1},a_{2}$ other than $a_{0}$ at the $\\tau$ -biased belief $\\mu_{s}^{\\tau}$ . However, the following lemma shows that, in such cases, we can always modify the signaling scheme to turn the external signal into a boundary signal. This modification will increase the total probability of useful signals and hence reduce the sample complexity. The proof of this lemma is in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.3. Suppose \u03a0 is an adaptive algorithm that uses signaling schemes with internal, boundary, and external signals. Then, there exists another adaptive algorithm $\\Pi^{\\prime}$ with equal or lower sample complexity that employs only signaling schemes with internal and boundary signals. ", "page_idx": 5}, {"type": "text", "text": "An internal signal, on the other hand, is not useful for testing $w\\geq\\tau$ or $w\\leq\\tau$ , for the following reason. For an internal signal, the biased belief with bias level $\\tau$ , $\\mu_{s}^{\\tau}$ , lies inside $R_{a_{0}}$ . Since $R_{a_{0}}$ is an open region, there must exist a small number $\\varepsilon>0$ such that when the agent has bias level $w=\\tau+\\varepsilon$ or $\\tau-\\varepsilon$ , the biased belief with bias level $w$ , $w\\mu_{0}+(1-w)\\mu_{s}$ , is also inside the region $R_{a_{0}}$ , so the agent will take action $a_{0}$ . As the agent takes $a_{0}$ under both $w=\\tau+\\varepsilon$ and $\\tau-\\varepsilon$ , we cannot distinguish these two cases, so this signal is not helpful in determining $w\\geq\\tau$ or $w\\leq\\tau$ . The following lemma formalizes the idea that internal signals are not useful: ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4. To test whether $w\\geq\\tau$ or $w\\leq\\tau$ , any adaptive algorithm that uses signaling schemes with boundary and internal signals cannot terminate until a boundary signal is sent. ", "page_idx": 6}, {"type": "text", "text": "Proof of Lemma 4.1. By Lemma 4.3, the optimal adaptive algorithm only uses signaling schemes with boundary and internal signals. By Lemma 4.4, the algorithm cannot terminate until a boundary signal is sent. By Lemma 4.2, the algorithm terminates when a boundary signal is sent. We conclude that the termination time of any adaptive algorithm cannot be better than the constant algorithm that keeps using the signaling scheme that maximizes the total probability of boundary signals. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4.2 Revelation Principle ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To compute the optimal constant signaling scheme, we need another technique that is similar to the revelation principle in the information design literature [13, 6]. The revelation principle says that, in some information design problems, it is without loss of generality to consider only \u201cdirect\u201d signaling schemes where signals are recommendations of actions for the agent, that is, the signal space is $S\\,=\\,A$ , and when the principal sends signal $a$ , it should be optimal for the agent to take action $a$ given the posterior belief induced by signal $a$ . ", "page_idx": 6}, {"type": "text", "text": "Unlike classical information design problems where the agent is unbiased, our problem involves a biased agent, so we need a different revelation principle: the signals are still action recommendations, but when the principal sends signal $a$ , action $a$ is optimal for an agent with bias level exactly $\\tau$ ; moreover, if $a\\neq a_{0}$ , then an agent with bias level $\\tau$ will be indifferent between $a$ and $a_{0}$ . This insight is formalized in the following lemma: ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.5 (revelation principle for bias detection). Let $\\pi$ be an arbitrary signaling scheme that can test $w\\geq\\tau$ or $w\\leq\\tau$ . Then, there exists another signaling scheme $\\pi^{\\prime}$ that can do so with signal space $S=A$ such that: ", "page_idx": 6}, {"type": "text", "text": "(1) Given signal $a\\in A$ , action a is an optimal action for any agent with bias level $w=\\tau$ .   \n(2) Given signal $a\\,\\in\\,A\\setminus\\{a_{0}\\}$ , actions a and $a_{0}$ are both optimal for any agent with bias level $w=\\tau$ . As a corollary, if the agent\u2019s bias level $w<\\tau$ , then the agent strictly prefers a over $a_{0}$ ; and if $w>\\tau,$ , then the agent strictly prefers $a_{0}$ over any other actions.   \n(3) The sample complexity satisfies $T_{\\tau}(\\pi^{\\prime})\\leq T_{\\tau}(\\pi)$ . ", "page_idx": 6}, {"type": "text", "text": "In the above signaling scheme $\\pi^{\\prime}$ , every $a\\in A\\setminus\\{a_{0}\\}$ is a boundary signal (Definition 4.1), which is useful for testing bias: given signal $a\\in A\\setminus\\{{\\dot{a}}_{0}\\}$ , if the agent takes action $a_{0}$ , then it must be $w\\geq\\tau$ ; otherwise $w\\leq\\tau$ . The signal $a_{0}$ is internal and not useful for determining $w\\geq\\tau$ or $w\\leq\\tau$ . So, the sample complexity of $\\pi^{\\prime}$ is equal to the expected time steps until a signal in $A\\setminus\\{a_{0}\\}$ is sent. ", "page_idx": 6}, {"type": "text", "text": "The idea behind Lemma 4.5 is combination of signals. Suppose there is a signaling scheme that can determine whether $w\\geq\\tau$ or $w\\leq\\tau$ with a signal space larger than $A$ . There must exist two signals $s$ and $s^{\\prime}$ under which the agent is indifferent between $a_{0}$ and some action $a\\neq a_{0}$ if the agent\u2019s bias level is exactly $\\tau$ . We can then combine the two signals into a single signal $s^{\\prime\\prime}$ under which the agent remains indifferent between $a_{0}$ and $a$ , yielding a new signaling scheme with a smaller signal space. Repeating this can reduce the signal space to size $|A|$ . See Appendix A.3 for the full proof. ", "page_idx": 6}, {"type": "text", "text": "4.3 Algorithm for Computing the Optimal Signaling Scheme ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we present an algorithm to compute the optimal (minimal sample complexity) signaling scheme to test whether $w\\geq\\tau$ or $w\\leq\\tau$ . The revelation principle in the previous subsection ensures that we only need a direct signaling scheme where signals are action recommendations. The optimal direct signaling scheme turns out to be solvable by a linear program, detailed in Algorithm 1. In the linear program, the constraint in Equation (5) ensures that whenever the principal recommends action $a\\in A$ , it is optimal for an agent with bias level $\\tau$ to take action $a$ ; this satisfies condition (1) in the revelation principle (Lemma 4.5). The indifference constraint (Equation (5)) ensures that when the recommended action $a$ is not $a_{0}$ , an agent with bias level $\\tau$ is indifferent between $a$ and $a_{0}$ ; this satisfies condition (2) in the revelation principle. The objective (Equation (4)) is to maximize the probability of useful signals (those in $A\\,\\bar{\\backslash}\\,\\{a_{0}\\bar{\\})$ , hence minimize the sample complexity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Algorithm 1 finds a constant signaling scheme for testing $w\\;\\geq\\;\\tau\\;o r\\leq\\;\\tau$ that is optimal among all adaptive signaling schemes. The sample complexity of the optimal signaling scheme is $1/p^{*}$ , where $p^{*}$ is the optimal objective value in Equation (4). ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1: Linear program to compute the optimal signaling scheme ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input : prior $\\mu_{0}$ , utility function $U$ , and the parameter $\\tau\\in(0,1)$ Variable: signaling scheme $\\pi$ , consisting of conditional probabilities $\\pi(a|\\theta)$ for $a\\in A,\\theta\\in\\Theta$ Denote $\\Delta U(a,a^{\\prime},\\theta)=U(a,\\theta)-U(a^{\\prime},\\theta)$ . Solve the following linear program: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{{Maximize}}\\:\\:\\sum_{\\begin{array}{c}{{a\\in A\\backslash\\{a_{0}\\}\\:\\theta\\in\\Theta}}\\end{array}}\\pi(a|\\theta)\\mu_{0}(\\theta)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "subject to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\cdot\\mu_{0}(\\theta)\\Big[(1-\\tau)\\Delta U(a,a^{\\prime},\\theta)+\\tau\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\Delta U(a,a^{\\prime},\\theta^{\\prime})\\Big]\\geq0;\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Indifference between $a$ and ${\\mathfrak{a}}{\\mathfrak{o}}\\colon\\forall a\\in A\\setminus\\{a_{0}\\}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\cdot\\mu_{0}(\\theta)\\Big[(1-\\tau)\\Delta U(a,a_{0},\\theta)+\\tau\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\Delta U(a,a_{0},\\theta^{\\prime})\\Big]=0;\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Probability distribution constraints: $\\forall\\theta\\in\\Theta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{a\\in A}\\pi(a|\\theta)=1\\quad{\\mathrm{~and~}}\\quad\\forall a\\in A,\\;\\pi(a|\\theta)\\geq0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the above optimal signaling scheme, whenever the principal recommends an action $a$ other than $a_{0}$ , the agent\u2019s action immediately reveals whether $w\\geq\\tau$ or $w\\leq\\tau$ : if the agent indeed follows the recommendation or takes any other action than $a_{0}$ , then the bias must be small $(w\\leq\\tau)$ ; if the agent takes $a_{0}$ instead, the bias must be large $w\\geq\\tau)$ ). Thus, the expected sample complexity is equal to the expected number of iterations until a signal in $A\\setminus\\{a_{0}\\}$ is sent, which is $1/p^{*3}$ . ", "page_idx": 7}, {"type": "text", "text": "The linear program in Algorithm 1 has a polynomial size in $|A|$ (the number of actions) and $|\\Theta|$ (the number of states), so it is a polynomial-time algorithm. The solution $p^{*}$ depends on the geometry of the problem instance and does not seem to have a closed-form expression. ", "page_idx": 7}, {"type": "text", "text": "The remainder of this section proves Theorem 4.6. The proof requires an additional lemma: ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.7. Given a signaling scheme $\\pi\\,=\\,(\\pi(a|\\theta))_{a\\in A,\\theta\\in\\Theta}$ and an agent\u2019s bias level $w$ , after signal a is sent, the agent strictly prefers action $a_{1}$ over $a_{2}$ under the biased belief if and only $i f$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\cdot\\mu_{0}(\\theta)\\Big[(1-w)\\Delta U(a_{1},a_{2},\\theta)+w\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\Delta U(a_{1},a_{2},\\theta^{\\prime})\\Big]>0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. The agent\u2019s biased belief under signal $a$ and bias level $w$ is given by $\\left(1\\ -\\right.$ \u03b8\u2032\u2208\u00b5\u03980 (\u00b5\u03b80)(\u03c0\u03b8(\u2032a)|\u03c0\u03b8()a|\u03b8\u2032) + w\u00b50(\u03b8), \u2200\u03b8 \u2208 \u0398. The condition for the agent to strictly prefer a1 over $a_{2}$ is that the expected utility under the biased belief when choosing $a_{1}$ is greater than that of $a_{2}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\left((1-w)\\frac{\\mu_{0}(\\theta)\\pi(a|\\theta)}{\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\pi(a|\\theta^{\\prime})}+w\\mu_{0}(\\theta)\\right)\\Delta U(a_{1},a_{2},\\theta)>0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Delta U(a_{1},a_{2},\\theta)=U(a_{1},\\theta)-U(a_{2},\\theta)$ . Multiplying by $\\begin{array}{r}{\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\pi(a|\\theta^{\\prime})}\\end{array}$ , we obtain: ", "page_idx": 7}, {"type": "equation", "text": "$$\n(1-w)\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\pi(a|\\theta)\\Delta U(a_{1},a_{2},\\theta)+w\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\pi(a|\\theta^{\\prime})\\Delta U(a_{1},a_{2},\\theta)>0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Factoring out the terms, this can be rewritten as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\mu_{0}(\\theta)\\bigg((1-w)\\Delta U(a_{1},a_{2},\\theta)+w\\sum_{\\theta^{\\prime}\\in\\Theta}\\mu_{0}(\\theta^{\\prime})\\Delta U(a_{1},a_{2},\\theta^{\\prime})\\bigg)>0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "4D7haH4pdR/tmp/418367a848c5ff12a3a112158f2c3c68f33eadfecae66ce990b0f40f17d873be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: The three qualitatively different cases for detecting the level of bias, each illustrated within a simplex over three states where $\\mu_{0}$ is the prior belief. Each point in the simplex corresponds to an optimal action for the agent. Green curves indicate indifference between the default action $a_{0}$ and another action under an unbiased belief. Orange curves are translated versions of these indifference curves; a posterior on these curves means the agent\u2019s biased belief (at bias level $\\tau$ ) aligns with the green curves. From (a) to (c), $\\tau$ increases, translating the orange curves further. In Figure 1a, $\\mu_{0}$ can be represented as a convex combination of points on the translated curves, allowing bias level detection with a single sample. In Figure 1b, only some signals are useful, requiring more than one sample in the worst case. In Figure 1c, the bias level cannot be tested against $\\tau$ . ", "page_idx": 8}, {"type": "text", "text": "This final expression is positive if and only if the agent to strictly prefer $a_{1}$ over $a_{2}$ ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 4.6. According to Lemma 4.1 (constant algorithms are optimal) and Lemma 4.5 (revelation principle), to find an optimal adaptive algorithm we only need to find the optimal constant signaling scheme that satisfies the conditions in Lemma 4.5. We verify that the signaling scheme computed from the linear program in Algorithm 1 satisfies the conditions in Lemma 4.5: ", "page_idx": 8}, {"type": "text", "text": "\u2022 The optimality constraint (Equation (5)) in the linear program, together with Lemma 4.7, ensures that: whenever signal $a\\in A$ is sent, action $a$ is weakly better than any other action for an agent with bias level $w=\\tau$ . This satisfies the first condition in Lemma 4.5. \u2022 The indifference constraint (Equation (5)), together with Lemma 4.7, ensures that: whenever $a\\in A\\setminus\\{a_{0}\\}$ is sent, the agent is indifferent between action $a$ and $a_{0}$ if the bias level $w=\\tau$ . Then, by the optimality constraint (Equation (5)), we have both $a$ and $a_{0}$ being optimal actions. This satisfies the second condition in Lemma 4.5. ", "page_idx": 8}, {"type": "text", "text": "We then argue that the solution of the linear program is the optimal signaling scheme that satisfies the conditions of Lemma 4.5. According to our argument after Lemma 4.5, only the signals in $A\\setminus\\{a_{0}\\}$ are useful signals, so the sample complexity is equal to the expected number of time steps until a signal in $A\\,\\backslash\\,\\bar{\\{a_{0}\\}}$ is sent. The probability that a signal in $A\\setminus\\{a_{0}\\}$ is sent at each time step is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\pi(a)=\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\pi(a|\\theta).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "bTehre  oef xtpiemcet esdt enpus misb ear  goefo tmiemteri cs treapnsd ios mt hvea riinavbelres)e. $\\frac{1}{\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\pi(a|\\theta)}$ i(zbees ctahues ep rtohbea bniulimty$\\begin{array}{r}{\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}^{}\\mu_{0}(\\bar{\\theta})\\pi(a|\\theta)}\\end{array}$ , so it minimizes the sample complexity. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "5 Geometric Characterization of the Testability of Bias ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To complement the algorithmic solution presented in the previous section, this section provides a geometric characterization of the bias detection problem. We identify the conditions under which testing whether $w\\geq\\tau$ or $w\\leq\\tau$ can be done in only one sample, in finite number of samples, or cannot be done at all (which is the scenario where the linear program in Algorithm 1 is infeasible). ", "page_idx": 8}, {"type": "text", "text": "By Assumption 2.1 ( $\\boldsymbol{a}_{0}$ is strictly better than other actions at prior $\\mu_{0}$ ), we have: ", "page_idx": 8}, {"type": "equation", "text": "$$\nc_{a}^{\\top}\\mu_{0}=\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\bigl(U(a_{0},\\theta)-U(a,\\theta)\\bigr)>0,\\quad\\forall a\\in A\\setminus\\{a_{0}\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $c_{a}$ is as defined in Equation (2). Define $I_{a}$ as the set of indifference beliefs between action $a$ and $a_{0}$ , which is the intersection of the hyperplane $\\{x|c_{a}^{\\top}x=0\\}$ and the probability simplex $\\Delta(\\Theta)$ : ", "page_idx": 9}, {"type": "equation", "text": "$$\nI_{a}:=\\{\\mu\\in\\Delta(\\Theta)\\mid c_{a}^{\\top}\\mu=0\\}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Given a parameter $\\tau\\in(0,1)$ , for which we want to test whether $w\\geq\\tau$ or $w\\leq\\tau$ , let ", "page_idx": 9}, {"type": "equation", "text": "$$\nI_{a,\\tau}=\\{\\mu\\in\\Delta(\\Theta)\\mid(1-\\tau)\\mu+\\tau\\mu_{0}\\in I_{a}\\}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "be the set of posterior beliefs for which, if the agent\u2019s bias level is exactly $\\tau$ , then the agent\u2019s biased belief will fall within the indifference set $I_{a}$ . ", "page_idx": 9}, {"type": "text", "text": "Lemma 5.1. $I_{a,\\tau}$ is equal to the intersection of the probability simplex $\\Delta(\\Theta)$ and a translation of the hyperplane $\\{x\\mid c_{a}^{\\top}x=0\\}$ : $\\begin{array}{r}{I_{a,\\tau}=\\left\\{\\mu\\in\\Delta(\\Theta)\\mid c_{a}^{\\top}\\mu=-\\frac{\\tau}{1-\\tau}c_{a}^{\\top}\\mu_{0}\\right\\}}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "The proof of this lemma is in Appendix B.1. With this representation of $I_{a,\\tau}$ in hand, we can now present a geometric characterization of the testability of bias. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.2 (geometric characterization). $F i x\\,\\tau\\in(0,1)$ . The problem of testing $w\\geq\\tau$ or $w\\leq\\tau$ ", "page_idx": 9}, {"type": "text", "text": "\u2022 Can be solved with a single sample (the sample complexity is $^{\\,I}$ ) if and only if the prior $\\mu_{0}$ is in the convex hull formed by the translated sets $I_{a,\\tau}$ for all non-default actions $a\\in A\\setminus\\{a_{0}\\}$ : i.e., \u00b50 \u2208ConvexHull $\\textstyle\\bigcup_{a\\in A\\setminus\\{a_{0}\\}}I_{a,\\tau}\\;)$ .   \n\u2022 Can be solved (with finite sample complexity) if and only if $I_{a,\\tau}\\neq\\emptyset$ for at least one $a\\in A\\backslash\\{a_{0}\\}$ .   \n\u2022 Cannot be solved if $I_{a,\\tau}=\\emptyset$ for all $a\\in A\\setminus\\{a_{0}\\}$ . ", "page_idx": 9}, {"type": "text", "text": "Figure 1 illustrates the three cases of Theorem 5.2. In the first case, the solution of the linear program in Algorithm 1 satisfies $\\begin{array}{r}{\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\mu_{0}(\\theta)=1}\\end{array}$ , meaning that useful signals are sent with probability 1, which allows us to tell whether $w\\geq\\tau$ or $w\\leq\\tau$ immediately. In the second case, the total probability of useful signals satisfies $\\begin{array}{r}{\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\mu_{0}(\\theta)\\,<\\,1}\\end{array}$ , so the sample complexity is more than 1. In the third case, the linear program in Algorithm 1 is not feasible, so $w\\ \\geq\\ \\tau$ or $w\\ \\leq\\ \\tau$ cannot be determined; importantly, this is not a limitation of our particular algorithm, but a general impossibility in our model. The proof of Theorem 5.2 is in Appendix B.2. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our approach has some limitations; here we discuss the two that we view as most significant. ", "page_idx": 9}, {"type": "text", "text": "First, we have assumed a linear model of bias. While the linear model is common in the literature [8, 10, 5, 18], we also consider a more general model of bias (in Appendix C): as the bias level $w$ increases from 0 to 1, the agent\u2019s belief changes from the true posterior $\\mu_{s}$ to the prior $\\mu_{0}$ according to some general continuous function $\\phi(\\mu_{0},\\mu_{s},w)$ . We show that, as long as the function $\\phi$ satisfies a certain single-crossing property (as $w$ increases, once the agent starts to prefer the default action $a_{0}$ , they will not change the preferred action anymore), our results regarding the optimality of constant signaling schemes and the geometric characterization still hold, while the revelation principle and the linear program algorithm no longer work because $\\phi$ is not linear. We consider it an interesting challenge to come up with more general models of bias that are still tractable, in the sense that one can efficiently design good signaling schemes with reasonable sample complexity bounds. ", "page_idx": 9}, {"type": "text", "text": "Second, we have assumed that the agent\u2019s prior is the same as the real prior from which states of the world are drawn. But what if the agent\u2019s prior is different? Our results directly extend to the case where the agent has a wrong, known prior. If the agent\u2019s prior is unknown, then our problem becomes significantly more challenging. More generally, the agent may have a private type that determines both their prior and utility and is unknown to the principal. We conjecture that testing the agent\u2019s bias in this case becomes impossible, because if different types consistently take \u201copposite\u201d actions, then the actions provide no information about the agent\u2019s bias. ", "page_idx": 9}, {"type": "text", "text": "Despite these limitations, we view our paper as making significant progress on a novel problem that seems fundamental. Our results suggest that practical algorithms for detecting bias in belief update are within reach and, in the long term, may lead to new insights on issues of societal importance. In particular, we anticipate future research in more complex situations such as combining decisions of many experts (human or AI) after measuring and accounting for their individual biases. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by the National Science Foundation under grants IIS-2147187, IIS-2229881 and CCF-2007080; and by the Office of Naval Research under grants N00014-20-1- 2488 and N00014-24-1-2704. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] R. Alonso and O. C\u00e2mara. Bayesian persuasion with heterogeneous priors. Journal of Economic Theory, 165:672\u2013706, 2016.   \n[2] R. J. Aumann, M. B. Maschler, and R. E. Stearns. Repeated Games with Incomplete Information. MIT Press, 1995.   \n[3] J. D. Bloom, Y. A. Chan, R. S. Baric, P. J. Bjorkman, S. Cobery, B. E. Deverman, D. N. Fisman, R. Gupta, A. Iwasaki, M. Lipsitch, R. Medzhitov, R. A. Neher, R. Nielsen, N. Patterson, T. Stearns, E. van Nimwegen, M. Worobey, and D. A. Relman. Investigate the origins of COVID-19. Science, 372(6543):694, 2021.   \n[4] V. P. Crawford and J. Sobel. Strategic information transmission. Econometrica, 50(6):1431\u2013 1451, 1982.   \n[5] G. de Clippel and X. Zhang. Non-Bayesian persuasion. Journal of Political Economy, 130 (10):2594\u20132642, 2022.   \n[6] S. Dughmi and H. Xu. Algorithmic Bayesian persuasion. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC), pages 412\u2013425, 2016.   \n[7] P. Dworczak and A. Pavan. Preparing for the worst but hoping for the best: Robust (Bayesian) persuasion. Econometrica, 90(5):2017\u20132051, 2022.   \n[8] L. G. Epstein, J. Noor, and A. Sandroni. Non-Bayesian learning. The B.E. Journal of Theoretical Economics, 10(1):732\u2013735, 2010.   \n[9] Yiding Feng, Chien-Ju Ho, and Wei Tang. Rationality-robust information design: Bayesian persuasion under quantal response. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 501\u2013546. SIAM, 2024.   \n[10] D. Hagmann and G. Loewenstein. Persuasion with motivated beliefs. Carnegie Mellon University technical report, 2017.   \n[11] D. M. Kahan. Ideology, motivated reasoning, and cognitive reflection. Judgment and Decision Making, 8(4):18, 2013.   \n[12] D. M. Kahan, E. Peters, M. Wittlin, P. Slovic, L. L. Ouellette, D. Braman, and G. Mandel. The polarizing impact of science literacy and numeracy on perceived climate change risks. Nature Climate Change, 2(10):732\u2013735, 2012.   \n[13] E. Kamenica and M. Gentzkow. Bayesian persuasion. American Economic Review, 101(6): 2590\u20132615, 2011.   \n[14] T. Lin and Y. Chen. Persuading a learning agent. arXiv preprint arXiv:2402.09721, 2024.   \n[15] J. Renault. Repeated games with incomplete information. In R. A. Meyers, editor, Encyclopedia of Complexity and Systems Science. Springer, 2012.   \n[16] C. S. Taber and M. Lodge. Motivated skepticism in the evaluation of political beliefs. American Journal of Political Science, 50(3):755\u2013769, 2006.   \n[17] C. S. Taber, D. Cann, and S. Kucsova. The motivated processing of political arguments. Political Behavior, 31(2):137\u2013155, 2009.   \n[18] Wei Tang and Chien-Ju Ho. On the Bayesian Rational Assumption in Information Design. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 9:120\u2013130, October 2021. ISSN 2769-1349, 2769-1330. doi: 10.1609/hcomp.v9i1.18945. URL https: //ojs.aaai.org/index.php/HCOMP/article/view/18945.   \n[19] B. M. Tappin, G. Pennycook, and D. G. Rand. Bayesian or biased? Analytic thinking and political belief updating. Cognition, 204:104375, 2020.   \n[20] E. Ward. Conservatism in human information processing. In D. Kahneman, P. Slovic, and A. Tversky, editors, Judgment Under Uncertainty: Heuristics and Biases. Cambridge University Press, 2007.   \n[21] K. Yang and H. Zhang. Computational aspects of Bayesian persuasion under approximate best response. arXiv preprint arXiv:2402.07426, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Justification: Limitations are discussed in Section 6. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: They are discussed in Section 1 and Section 6. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 16}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "A Missing Proofs from Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Suppose that, during its operation, $\\Pi$ selects a signaling scheme $\\pi$ that includes an external signal $s\\,\\in\\,S$ . By definition, for an external signal, the $\\tau$ -biased belief $\\mu_{s}^{\\tau}\\,=\\,\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ is in $\\mathrm{ext}R_{a_{0}}$ . This implies that the true posterior $\\mu_{s}$ , derived from the signaling scheme $\\pi$ and the prior $\\mu_{0}$ , also lies in $\\mathrm{ext}R_{a_{0}}$ . Consequently, the line segment connecting $\\mu_{s}$ and $\\mu_{0}$ , represented as $\\{(1-t)\\mu_{s}+t\\mu_{0}\\ |\\ t\\in[0,1]\\}$ , must intersect the boundary $\\partial R_{a_{0}}$ at some point. Denote this intersection by $\\mu^{*}=(1-t^{*})\\mu_{s}+t^{*}\\mu_{0}\\in\\partial R_{a_{0}}$ . ", "page_idx": 18}, {"type": "text", "text": "We will adjust the original signaling scheme $\\pi$ . To do so, define $\\tilde{\\mu}_{s}$ as the belief whose $\\tau$ -biased version equals $\\mu^{*}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\tilde{\\mu}_{s}=\\mu^{*}\\quad\\Longleftrightarrow\\quad\\tilde{\\mu}_{s}=\\frac{(t^{*}-\\tau)\\mu_{0}+(1-t^{*})\\mu_{s}}{1-\\tau}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Under the original signaling scheme $\\pi$ , according to the splitting lemma (Lemma 2.1), the prior $\\mu_{0}$ can be represented as a convex combination of $\\mu_{s}$ and the posteriors associated with other signals $s^{\\prime}\\in S\\setminus\\bar{\\{}s\\}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{0}=p_{s}\\mu_{s}+\\sum_{s^{\\prime}\\in S\\setminus\\{s\\}}p_{s^{\\prime}}\\mu_{s^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If we change $\\mu_{s}$ to $\\tilde{\\mu}_{s}$ , then we obtain a new convex combination (this is valid because $\\tilde{\\mu}_{s}$ is on the line segment from $\\mu_{s}$ to $\\mu_{0}$ ): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{0}=\\tilde{p}_{s}\\tilde{\\mu}_{s}+\\sum_{s^{\\prime}\\in S\\setminus\\{s\\}}\\tilde{p}_{s^{\\prime}}\\mu_{s^{\\prime}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{p}_{s}=\\frac{p_{s}}{1-t^{*}+t^{*}p_{s}}\\quad\\mathrm{and}\\quad\\forall s^{\\prime}\\in S\\setminus\\{s\\},\\ \\tilde{p}_{s^{\\prime}}=\\frac{1-t^{*}}{1-t^{*}+t^{*}p_{s}}p_{s^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, by the splitting lemma (Lemma 2.1), there exists a signaling scheme $\\pi^{\\prime}$ with $|S|$ signals where signal $s$ induces posterior $\\tilde{\\mu}_{s}$ and other signals $s^{\\prime}$ induces $\\mu_{s^{\\prime}}$ . Note that the $\\tau$ -biased version of $\\tilde{\\mu}_{s}$ satisfies $\\tau\\mu_{0}+(1-\\tau)\\tilde{\\mu}_{s}=\\mu^{*}\\in\\partial R_{a_{0}}$ , so $s$ is a boundary signal under signaling scheme $\\pi^{\\prime}$ . ", "page_idx": 18}, {"type": "text", "text": "Since $s$ is a boundary signal, we can immediately tell whether $w~\\geq~\\tau$ or $w~\\leq~\\tau$ according to Lemma 4.2 when $s$ is sent and end the algorithm. If any signal $s^{\\prime}$ other than $s$ is sent, the induced posterior $\\mu_{s}$ is the same as the posterior in the original signaling scheme $\\pi$ , so the agent will take the same action, and we can just follow the rest of the original algorithm $\\Pi$ . But we note that the probability of signal $s$ being sent under the new signaling scheme $\\pi^{\\prime}$ is larger than or equal to the probability under the original signaling scheme $\\pi$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{p}_{s}=\\frac{p_{s}}{1-t^{*}+t^{*}p_{s}}\\geq p_{s}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So, in expectation, we can end the algorithm faster by using $\\tilde{\\pi}$ than using $\\pi$ . Hence, by repeating the above procedure to replace all the signaling schemes in the original algorithm $\\Pi$ that use external signals, we obtain a new algorithm $\\Pi^{\\prime}$ that only uses boundary and internal signals with smaller or equal sample complexity. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.2 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\Pi$ be any adaptive algorithm using signaling schemes with boundary and internal signals. Let $\\mathcal{\\bar{H}}_{t}=\\left\\{(\\pi_{1},\\theta_{1},s_{1},a_{1}^{-}),\\ldots,\\bar{(\\pi_{t},\\theta_{t},s_{t},a_{t}^{-})}\\right\\}$ be any history that can happen during the execution of $\\Pi$ . If no boundary signal has been sent, then every realized signal $s_{k}$ is an internal signal in the respective signaling scheme $\\pi_{k}$ , with the $\\tau$ -biased posterior satisfying $\\mu_{s_{k}}^{\\tau}=\\tau\\mu_{0}+(1-\\tau)\\mu_{s_{k}}\\in$ $R_{a_{0}}$ . Because $R_{a_{0}}\\,=\\,\\left\\{\\mu\\,\\in\\,\\Delta(\\Theta)\\,\\mid\\,\\forall a\\,\\in\\,A\\,\\backslash\\,\\{a_{0}\\},\\,c_{a}^{\\top}\\mu\\,>\\,0\\right\\}$ is an open region, there must exist some $\\varepsilon_{k}>0$ such that the $\\ell_{1}$ -norm ball $B_{\\varepsilon_{k}}(\\mu_{s_{k}}^{\\tau})\\,=\\,\\{\\mu\\,\\in\\,\\dot{\\Delta}(\\Theta)\\,:\\,\\|\\mu-\\mu_{s_{k}}^{\\tau}\\|_{1}\\,\\leq\\,\\varepsilon_{k}\\}$ is a subset of $R_{a_{0}}$ . Let $\\varepsilon=\\operatorname*{min}_{k=1}^{t}\\varepsilon_{k}>0$ . Then $B_{\\varepsilon}(\\mu_{s_{k}}^{\\tau})\\subseteq R_{a_{0}}$ for every $k=1,\\hdots,t$ . Suppose the agent\u2019s bias level $w$ is in the range $\\left[\\tau-{\\frac{\\varepsilon}{2}},\\tau+{\\frac{\\varepsilon}{2}}\\right]$ . Then, for every signal $s_{k}$ , the agent\u2019s biased belief $\\nu_{s_{k}}=w\\mu_{0}+(1-w)\\mu_{s_{k}}$ satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nu_{s_{k}}-\\mu_{s_{k}}^{\\tau}\\|_{1}=\\|(w-\\tau)(\\mu_{0}-\\mu_{s_{k}})\\|_{1}\\leq|w-\\tau|\\cdot\\|\\mu_{0}-\\mu_{s_{k}}\\|_{1}\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This means ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{s_{k}}\\in B_{\\varepsilon}(\\mu_{s_{k}}^{\\tau})\\subseteq R_{a_{0}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, the agent should take action $a_{0}$ given signal $s_{k}$ . Note that this holds for every $k=1,\\hdots,t$ and any $\\begin{array}{r}{w\\in[\\tau-\\frac{\\varepsilon}{2},\\tau+\\frac{\\varepsilon}{2}]}\\end{array}$ . So we cannot determine whether $w\\geq\\tau$ or $w\\leq\\tau$ so far. We have to run the algorithm until a boundary signal is sent. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "A.3 Proof of Lemma 4.5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\pi$ be a signaling scheme that can test whether $w\\geq\\tau$ or $w\\leq\\tau$ . According to Lemma 4.3, $\\pi$ can be assumed to only use boundary and internal signals. Recall that a signal $s$ is boundary if the $\\tau$ -biased belief $\\mu_{s}^{\\tau}=\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ lies on the boundary set $\\partial R_{a_{0}}$ . For $a\\in A\\setminus\\{a_{0}\\}$ , let $B_{a}$ be the set of beliefs under which the agent is indifferent between $a$ and $a_{0}$ and $a$ and $a_{0}$ are both better than other actions: ", "page_idx": 19}, {"type": "equation", "text": "$$\nB_{a}=\\{\\mu\\in\\Delta(\\Theta)\\mid c_{a}^{\\top}\\mu=0\\,\\mathrm{~and~}\\,\\forall a^{\\prime}\\in A,c_{a^{\\prime}}^{\\top}\\mu\\geq0\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The boundary set $\\partial R_{a_{0}}$ can be written as the union of $B_{a}$ for $a\\in A\\setminus\\{a_{0}\\}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial R_{a_{0}}=\\bigcup_{a\\in A\\setminus\\{a_{0}\\}}B_{a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we classify the boundary signals into $|A|-1$ sets $\\{S_{a}\\}_{a\\in A\\setminus\\{a_{0}\\}}$ according to which $B_{a}$ sets their $\\tau$ -biased beliefs belong to: namely, the set $S_{a}$ contains boundary signals $s$ under which ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\in B_{a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We then combine the signals in $S_{a}$ . Specifically, consider the normalized weighted average of the true posterior beliefs associated with the signals in $S_{a}$ , denoted by $\\mu_{a}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu_{a}=\\sum_{s\\in S_{a}}\\frac{\\pi(s)}{\\sum_{s^{\\prime}\\in S_{a}}\\pi(s^{\\prime})}\\mu_{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the $\\tau$ -biased version of $\\mu_{a}$ is also in the set $B_{a}$ because $B_{a}$ is a convex set: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\mu_{a}=\\sum_{s\\in S_{a}}\\frac{\\pi(s)}{\\sum_{s^{\\prime}\\in S_{a}}\\pi(s^{\\prime})}\\Big(\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\Big)\\in B_{a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This means that if a signal $a$ induces true posterior $\\mu_{a}$ , then this signal is a boundary signal. ", "page_idx": 19}, {"type": "text", "text": "After defining $\\mu_{a}$ as above for every $a\\in A\\setminus\\{a_{0}\\}$ , let\u2019s consider the set of internal signals of the signaling scheme $\\pi$ , which we denote by $S_{I}$ . For each internal signal $s\\,\\in\\,S_{I}$ , the $\\tau$ -biased belief satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\in R_{a_{0}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similar to above, we combine all the signals in $S_{I}$ : define $\\mu_{a_{0}}$ to be the normalized weighted average of the posteriors associated with all internal signals: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu_{a_{0}}=\\sum_{s\\in S_{I}}\\frac{\\pi(s)}{\\sum_{s^{\\prime}\\in S_{I}}\\pi(s^{\\prime})}\\mu_{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the $\\tau$ -biased version of $\\mu_{a_{0}}$ must be in $R_{a_{0}}$ because $R_{a_{0}}$ is a convex set: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\mu_{a_{0}}=\\sum_{s\\in S_{I}}\\frac{\\pi(s)}{\\sum_{s^{\\prime}\\in S_{I}}\\pi(s^{\\prime})}\\Big(\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\Big)\\in R_{a_{0}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This means that, if a signal induces posterior $\\mu_{a_{0}}$ , then this signal is internal. ", "page_idx": 19}, {"type": "text", "text": "From the splitting lemma (Lemma 2.1), we know that the convex combination of the original posteriors $\\textstyle\\sum_{s\\in S}\\pi(s)\\mu_{s}$ is equal to the prior $\\mu_{0}$ . This means that the following convex combination of the new posteriors $\\{\\mu_{a}\\}_{a\\in A\\backslash a_{0}}$ and $\\mu_{a_{0}}$ is also equal to the prior: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{a\\in A\\backslash a_{0}}\\sum_{s^{\\prime}\\in S_{a}}\\pi(s^{\\prime})\\mu_{a}+\\sum_{s^{\\prime}\\in S_{I}}\\pi(s^{\\prime})\\mu_{a_{0}}=\\sum_{a\\in A\\backslash a_{0}}\\sum_{s\\in S_{a}}\\pi(s)\\mu_{s}+\\sum_{s\\in S_{I}}\\pi(s)\\mu_{s}=\\sum_{s\\in S}\\pi(s)\\mu_{s}=\\mu_{0}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the convex combination weight of $\\mu_{a}$ is $\\sum_{s^{\\prime}\\in S_{a}}\\pi(s^{\\prime})$ for every $a\\in A\\setminus\\{a\\}$ and the convex combination weight of $\\mu_{a_{0}}$ is $\\sum_{s^{\\prime}\\in S_{I}}\\pi(s^{\\prime})$ . One can easily verify that the weights sum to 1. Then, by the splitting lemma (Lemma 2.1), there exists a signaling scheme $\\pi^{\\prime}$ with signal space of size $|A|$ (so we simply denote the signal space by $A$ ) where each signal $a\\in A$ induces posterior $\\mu_{a}$ . We show that this new signaling scheme $\\pi^{\\prime}$ satisfies the properties in Lemma 4.5: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Signal $a_{0}$ induces posterior $\\mu_{a_{0}}$ whose $\\tau$ -biased version satisfies $\\tau\\mu_{0}+(1-\\tau)\\mu_{a_{0}}\\in R_{a_{0}}$ . So, given signal $a_{0}$ , action $a_{0}$ is the optimal action for an agent with bias level $\\tau$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 For each signal $a\\ \\in\\ A\\ \\backslash\\ \\{a_{0}\\}$ , the induced posterior $\\mu_{a}$ satisfies $\\tau\\mu_{0}+(1-\\tau)\\mu_{a}\\,\\in$ $B_{a}\\subseteq\\partial R_{a_{0}}$ . So, by the definition of $B_{a}$ , an agent with bias level $\\tau$ is indifferent between actions $a$ and $a_{0}$ and these two actions are better than other actions. Also, this signal is a boundary signal by Definition 4.1, which satisfies the following according to Lemma 4.2: if the agent\u2019s bias level $w<\\tau$ , then the agent strictly prefers $a$ over $a_{0}$ ; if $w>\\tau$ , then the agent strictly prefers $a_{0}$ over $a$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 The sample complexity of $\\pi^{\\prime}$ is the same as $\\pi$ because: (1) the sample complexity is equal to the inverse of the total probability of boundary signals (as a corollary of Lemma 4.4), and (2) the total probability of boundary signals of the two signaling schemes are the same: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\pi^{\\prime}(a)=\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\sum_{s^{\\prime}\\in S_{a}}\\pi(s^{\\prime})=\\sum_{s\\in\\cup_{a\\in A\\setminus\\{a_{0}\\}}S_{a}}\\pi(s).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So, $T_{\\tau}(\\pi^{\\prime})=T_{\\tau}(\\pi)$ . ", "page_idx": 20}, {"type": "text", "text": "B Missing Proofs from Section 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. For $\\mu\\in\\Delta(\\Theta)$ , by convexity of $\\Delta(\\Theta)$ , we have $(1-\\tau)\\mu+\\tau\\mu_{0}\\in\\Delta(\\Theta)$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu\\in I_{a,\\tau}\\iff(1-\\tau)\\mu+\\tau\\mu_{0}\\in I_{a}\\iff c_{a}^{\\top}((1-\\tau)\\mu+\\tau\\mu_{0})=0}\\\\ &{\\iff(1-\\tau)c_{a}^{\\top}\\mu+\\tau c_{a}^{\\top}\\mu_{0}=0}\\\\ &{\\iff c_{a}^{\\top}\\mu=-\\displaystyle\\frac{\\tau}{1-\\tau}c_{a}^{\\top}\\mu_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first prove the first part of Theorem 5.2, then prove the the second and third parts. ", "page_idx": 20}, {"type": "text", "text": "B.2.1 Proof of Part 1 of Theorem 5.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We want to prove that $w\\geq\\tau$ or $w\\leq\\tau$ can be tested with a single sample if and only if the prior $\\mu_{0}$ is in the convex hull formed by the translated sets $I_{a,\\tau}$ for all non-default actions $i\\in A\\setminus\\{a_{0}\\}$ : $\\mu_{0}\\in\\mathrm{ConvexHull}\\left(\\cup_{a\\in A\\setminus\\{a_{0}\\}}I_{a,\\tau}\\right)$ . ", "page_idx": 20}, {"type": "text", "text": "The \u201cif\u201d part. Suppose $\\mu_{0}\\in\\mathrm{ConvexHull}\\big(\\cup_{a\\in A\\setminus\\{a_{0}\\}}I_{a,\\tau}\\big)$ , namely, there exist a set of positive weights $\\{p_{s}\\}_{s\\in S}$ and a set of posterior beliefs $\\{\\mu_{s}\\}_{s\\in S}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu_{0}=\\sum_{s\\in S}p_{s}\\mu_{s},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where each $\\mu_{s}\\in I_{a,\\tau}$ for some $a\\in A\\setminus\\{a_{0}\\}$ . By definition, the $\\tau$ -biased belief $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ is in the indifference set $I_{a}$ . Recall the definition of the boundary set $\\partial R_{a_{0}}$ (Equation (3)), which is the set of beliefs under which the agent is indifferent between $a_{0}$ and some other action and these two actions are better any other actions. The $\\tau$ -biased belief $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\in I_{a}$ may or may not belong to $\\partial R_{a_{0}}$ , depending on whether $a$ and $a_{0}$ are better than any other actions: ", "page_idx": 20}, {"type": "text", "text": "\u2022 If $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\,\\in\\,\\partial R_{a_{0}}$ , then $s$ is a boundary signal (by Definition 4.1) and hence useful for testing whether $w\\geq\\tau$ or $w\\leq\\tau$ (Lemma 4.2). Denote $\\mu_{s}^{\\prime}=\\mu_{s}$ in this case. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\notin\\partial R_{a_{0}}$ , then there must exist some action $a^{\\prime}$ that is strictly better than $a$ and $a_{0}$ for the agent at the $\\tau$ -biased belief, hence $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\in\\mathrm{ext}R_{a_{0}}$ (so $s$ is an external signal). Then, according to the argument in Lemma 4.3, we can find another belief $\\mu_{s}^{\\prime}$ on the line segment between $\\mu_{s}$ and $\\mu_{0}$ such that the $\\tau$ -biased version of $\\mu_{s}^{\\prime}$ lies exactly on the boundary set $\\partial R_{a_{0}}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau\\mu_{0}+(1-\\tau)\\mu_{s}^{\\prime}\\in\\partial R_{a_{0}},\\quad\\mu_{s}^{\\prime}=t\\mu_{s}+(1-t)\\mu_{0}\\;\\mathrm{for~some}\\;t\\in[0,1].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "After the above discussion, we have found a $\\mu_{s}^{\\prime}$ that is either equal to $\\mu_{s}$ or on the line segment between $\\mu_{s}$ and $\\mu_{0}$ , for every $s\\in S$ . So, $\\mu_{0}$ can be written as a convex combination of $\\{\\mu_{s}^{\\prime}\\}_{s\\in S}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{0}=\\sum_{s\\in S}p_{s}^{\\prime}\\mu_{s}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, the $\\mu_{s}^{\\prime}$ defined above satisfies $\\tau\\mu_{0}\\,+\\,(1\\,-\\,\\tau)\\mu_{s}^{\\prime}\\,\\in\\,\\partial R_{a_{0}}$ . So, a signal inducing true posterior $\\mu_{s}^{\\prime}$ will be a boundary signal and useful for testing $w\\geq\\tau$ or $w\\leq\\tau$ (Lemma 4.2). Finally, by the splitting lemma (Lemma 2.1), we know that there must exist a signaling scheme $\\pi^{\\prime}$ with signal space $S$ where each signal $s\\in S$ indeed induces posterior $\\mu_{s}^{\\prime}$ . Such a signaling scheme sends useful (boundary) signals with probability 1. Hence, the sample complexity of it is 1. ", "page_idx": 21}, {"type": "text", "text": "The \u201conly if\u201d part. Suppose whether $w\\geq\\tau$ or $w\\leq\\tau$ can be tested with a single sample. This means that the optimal signaling scheme obtained from the linear program in Algorithm 1 must satisfy $\\begin{array}{r}{\\sum_{a\\in A\\backslash\\{a_{0}\\}}\\bar{\\pi}(a)\\,\\stackrel{*}{=}\\,\\sum_{a\\in A\\backslash\\{a_{0}\\}}\\sum_{\\theta\\in\\Theta}\\pi(a|\\theta)\\mu_{0}(\\theta)\\,=\\,1}\\end{array}$ , namely, the total probability of useful signals (signals in $A\\setminus\\{a_{0}\\})$ is 1. Then, by the splitting lemma, the prior $\\mu_{0}$ can be expressed as the convex combination ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{0}=\\sum_{a\\in A\\setminus\\{a_{0}\\}}\\pi(a)\\mu_{a}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\pi(a)\\,=\\,\\sum_{\\theta\\in\\Theta}\\mu_{0}(\\theta)\\pi(a|\\theta)}\\end{array}$ is the unconditional probability of signal $a$ and $\\mu_{a}$ is the true posterior induced by signal $a$ . Moreover, the indifference constraint (6) in the linear program ensures that the agent is indifferent between $a$ and $a_{0}$ upon receiving signal $a$ if the agent has bias level $\\tau$ : mathematically, $\\tau\\mu_{0}+(1-\\tau)\\mu_{a}\\in I_{a}$ . This means $\\mu_{a}\\in I_{a,\\tau}$ by definition. So, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{0}\\in\\mathrm{ConvexHull}\\Bigg(\\bigcup_{a\\in A\\setminus\\{a_{0}\\}}I_{a,\\tau}\\Bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.2.2 Proof of Parts 2 and 3 of Theorem 5.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first prove that, if whether $w\\geq\\tau$ or $w\\leq\\tau$ can be tested with finite sample complexity, then $I_{a,\\tau}\\neq\\emptyset$ for at least one $a\\in A\\setminus\\{a_{0}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "According to Lemma 4.1, if we can test whether $w\\geq\\tau$ or $w\\leq\\tau$ with finite sample complexity using adaptive algorithms, then we can do this using a constant signaling scheme. Lemma 4.3 further ensures that we can do this using a constant signaling scheme $\\pi$ with only boundary and internal signals. But according to Lemma 4.4, internal signals are not useful for testing $w\\geq\\tau$ or $w\\leq\\tau$ . So, the signaling scheme $\\pi$ must send some boundary signal $s$ with positive probability. Let $\\mu_{s}$ be the true posterior induced by $s$ . By the definition of boundary signal, $\\tau\\mu_{0}\\dot{+}\\,(1-\\tau)\\dot{\\mu}_{s}\\,\\in\\,\\dot{\\partial}R_{a_{0}}$ , implying that the agent is indifferent between $a_{0}$ and some action $a\\,\\in\\,A\\setminus\\{a_{0}\\}$ if their belief is $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}$ (and $a_{0}$ and $a$ are better than any other actions). This means $\\tau\\mu_{0}+(1-\\tau)\\mu_{s}\\in I_{a}$ , so $\\mu_{s}\\in I_{a,\\tau}$ by definition. Hence, $I_{a,\\tau}\\neq\\emptyset$ . ", "page_idx": 21}, {"type": "text", "text": "We then prove the opposite direction: if $I_{a,\\tau}\\neq\\emptyset$ for at least one $a\\in A\\setminus\\{a_{0}\\}$ , then whether $w\\geq\\tau$ or $w\\leq\\tau$ can be tested with finite sample complexity. ", "page_idx": 21}, {"type": "text", "text": "Let $a_{1}\\in A\\setminus\\{a_{0}\\}$ be an action for which $I_{a_{1},\\tau}\\neq\\emptyset$ . We claim that: ", "page_idx": 21}, {"type": "text", "text": "Claim B.1. There exists a state $\\theta_{1}\\,\\in\\,\\Theta$ for which the agent weakly prefers action $a_{1}$ over action $a_{0}$ if the true posterior is state $\\theta_{1}$ with probability $^{\\,I}$ and the agent has bias level $\\tau$ . In notation, let $e_{\\theta_{1}}\\,\\in\\,\\Delta(\\Theta)$ be the vector whose $\\theta_{1}t h$ component is 1 and other components are 0. The agent weakly prefers action $a_{1}$ over action $a_{0}$ under belief $\\tau\\mu_{0}+(1-\\tau)e_{\\theta_{1}}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Suppose on the contrary that no such state $\\theta_{1}$ exists. Then the agent strictly prefers $a_{0}$ over $a_{1}$ under belief $\\tau\\mu_{0}+(1-\\tau)\\dot{e}_{\\theta}$ for every state $\\theta\\in\\Theta$ . This implies that, for any belief $\\mu\\in\\Delta(\\Theta)$ , the agent should also strictly prefer $a_{0}$ over $a_{1}$ under the belief $\\tau\\mu_{0}+(1-\\tau)\\mu$ , due to linearity of the agent\u2019s utility with respect to the belief. The agent strictly preferring $a_{0}$ over $a_{1}$ implies $\\tau\\mu_{0}+(\\bar{1}-\\tau)\\mu\\notin I_{a}$ , so $\\mu$ cannot be in $I_{a,\\tau}$ by definition. This holds for any $\\mu\\,\\in\\,\\Delta(\\Theta)$ , so $I_{a,\\tau}=\\emptyset$ , a contradiction. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Let $\\theta_{1}$ be the state in the above claim. The prior $\\mu_{0}$ can be trivially written as the convex combination of $e_{\\theta_{1}}$ and $e_{\\theta}$ for other states $\\theta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{0}=\\mu_{0}(\\theta_{1})e_{\\theta_{1}}+\\sum_{\\theta\\in\\Theta\\backslash\\{\\theta_{1}\\}}\\mu_{0}(\\theta)e_{\\theta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the agent does not prefer $a_{0}$ under belief $\\tau\\mu_{0}+(1-\\tau)e_{\\theta_{1}}$ , the belief $\\tau\\mu_{0}+(1-\\tau)e_{\\theta_{1}}$ cannot be in the region $R_{a_{0}}$ . The prior $\\mu_{0}$ is in the region $R_{a_{0}}$ . Consider the line segment connecting $e_{\\theta_{1}}$ and the prior $\\mu_{0}$ . There must exist a point $\\mu^{\\prime}=t e_{\\theta_{1}}+(1-t)\\mu_{0}$ on the line segment such that the $\\tau$ -biased belief $\\tau\\mu_{0}+(1-\\tau)\\mu^{\\prime}$ lies exactly on the boundary of $R_{a_{0}}$ . Clearly, the prior can also be written as a convex combination of $\\mu^{\\prime}$ and $e_{\\theta}$ for $\\theta\\in\\Theta\\setminus\\{\\theta_{1}\\}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{0}=p^{\\prime}\\mu^{\\prime}+\\sum_{\\theta\\in\\Theta\\setminus\\{\\theta_{1}\\}}p_{\\theta}^{\\prime}e_{\\theta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then by the splitting lemma (Lemma 2.1), there exists a signaling scheme with $|\\Theta|$ signals where one signal induces posterior $\\mu^{\\prime}$ and the other signals induce posteriors $\\{e_{\\theta}\\}_{\\theta\\in\\Theta\\backslash\\{\\theta_{1}\\}}$ . In particular, the signal inducing $\\mu^{\\prime}$ is a boundary signal since $\\tau\\mu_{0}+(1-\\tau)\\mu^{\\prime}\\in\\partial R_{a_{0}}$ by construction. By Lemma 4.2, that signal is useful for testing $w\\ \\geq\\ \\tau$ or $w\\ \\leq\\ \\tau$ . When that signal is sent (which happens with positive probability $p^{\\prime}\\,>\\,0$ at each time step), we can tell $w\\ \\geq\\ \\tau$ or $w\\,\\leq\\,\\tau$ . This finishes the proof. ", "page_idx": 22}, {"type": "text", "text": "The two directions proved above together prove the parts 2 and 3 of Theorem 5.2. ", "page_idx": 22}, {"type": "text", "text": "C A More General Bias Model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We define a more general model of biased belief than the linear model. The agent\u2019s bias is captured by some function $\\varphi:\\Delta(\\Theta)\\times\\Delta(\\Theta)\\times[0,1]\\,\\to\\,\\Delta(\\Theta)$ . Given prior $\\mu_{0}\\in\\bar{\\Delta}(\\Theta)$ , true posterior $\\mu_{s}\\in\\Delta(\\Theta)$ , and bias level $w\\in[0,1]$ , the agent has biased belief $\\phi(\\mu_{0},\\mu_{s},w)$ . The linear model is the special case where ${\\phi}(\\mu_{0},\\mu_{s},\\bar{w})\\stackrel{.}{=}w{\\mu}_{0}{+}(1{-}w){\\mu}_{s}$ . We make the following natural assumptions on $\\phi$ : ", "page_idx": 22}, {"type": "text", "text": "Assumption C.1. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 $\\phi(\\mu_{0},\\mu_{s},0)=\\mu_{s}$ (no bias), $\\phi(\\mu_{0},\\mu_{s},1)=\\mu_{0}$ (full bias).   \n\u2022 $\\phi(\\mu_{0},\\mu_{s},w)$ is continuous in $\\mu_{0},\\mu_{s},w$ . ", "page_idx": 22}, {"type": "text", "text": "We then make some joint assumptions on the bias model $\\phi$ and the agent\u2019s utility function $U$ . Recall that the notation $\\bar{R_{a_{0}}^{\\ \u3001\\ \u3001}}=\\left\\{\\mu\\in\\mathbf{\\bar{\\Delta}}(\\Theta)\\mid\\forall a\\in A\\setminus\\{a_{0}\\},\\,c_{a}^{\\top}\\mu>0\\right\\}$ is the region of beliefs under which the agent strictly prefers action $a_{0}$ , $\\partial R_{a_{0}}$ is the boundary of $\\hat{R}_{a_{0}}$ , and $\\mathrm{ext}R_{a_{0}}$ is the exterior of $R_{a_{0}}$ where the agent strictly not prefers $a_{0}$ . ", "page_idx": 22}, {"type": "text", "text": "Assumption C.2. When receiving no information, the agent will take the default action: $\\forall\\mu_{0}\\ \\in$ $\\Delta(\\Theta),\\bar{\\forall}w\\in[0,1],$ , $\\phi(\\mu_{0},\\mu_{0},w)\\in R_{a_{0}}$ . ", "page_idx": 22}, {"type": "text", "text": "Definition C.1. We say that a posterior belief $\\mu\\ \\in\\ \\Delta(\\Theta)$ satisfies single-crossing if the curve $\\{\\phi(\\mu_{0},\\mu,w)\\,:\\,w\\,\\in\\,[0,1]\\}$ passes the boundary $\\partial R_{a_{0}}$ only once: namely, there exists $\\overline{{w}}\\,\\in\\,[0,1]$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\forall w\\in[0,\\overline{{w}}),}&{\\phi(\\mu_{0},\\mu,w)\\in\\mathrm{ext}R_{a_{0}};}\\\\ &{\\phi(\\mu_{0},\\mu,\\overline{{w}})\\in\\partial R_{a_{0}};}\\\\ {\\forall w\\in(\\overline{{w}},1],}&{\\phi(\\mu_{0},\\mu,w)\\in R_{a_{0}}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We assume that all posteriors outside $R_{a_{0}}$ satisfy single-crossing, and all posteriors inside $R_{a_{0}}$ do not cross the boundary when the bias level varies in $[0,1]$ : ", "page_idx": 22}, {"type": "text", "text": "Assumption C.3. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Any $\\mu\\notin R_{a_{0}}$ satisfies single-crossing. ", "page_idx": 23}, {"type": "text", "text": "Under the above general bias model with the stated assumptions, our results regarding the optimality of constant signaling schemes (Lemma 4.1) still holds. The geometric characterization of testability of bias (Theorem 5.2) holds after redefining some notations. Let $I_{a}=\\{\\mu\\in\\Delta(\\Theta)\\mid c_{a}^{\\top}\\mu=0\\}$ still be the set of beliefs where the agent is indifferent between actions $a$ and $a_{0}$ . Let $I_{a,\\tau}$ still be the set of posterior beliefs for which an agent with bias level $\\tau$ will be indifferent between $a$ and $a_{0}$ , but with a more general expression than the linear model: ", "page_idx": 23}, {"type": "equation", "text": "$$\nI_{a,\\tau}:=\\{\\mu\\in\\Delta(\\Theta)\\mid\\phi(\\mu_{0},\\mu,\\tau)\\in I_{a}\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With the above definition of $I_{a,\\tau}$ , Theorem 5.2 still holds. We omit the proofs because they are almost identical to the proofs for the linear model. ", "page_idx": 23}, {"type": "text", "text": "The revelation principle (Lemma 4.5) and the linear program algorithm for computing the optimal signaling scheme (Algorithm 1 and Theorem 4.6) do not apply to the general bias model because $\\phi$ is not linear. Designing an efficient algorithm to compute a good signaling scheme to test bias in a more general model is an interesting future direction. ", "page_idx": 23}]