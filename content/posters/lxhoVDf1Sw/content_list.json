[{"type": "text", "text": "Predictive Attractor Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ramy Mounir Sudeep Sarkar ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science and Engineering, University of South Florida, Tampa {ramy, sarkar}@usf.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequential memory, the ability to form and accurately recall a sequence of events or stimuli in the correct order, is a fundamental prerequisite for biological and artificial intelligence as it underpins numerous cognitive functions (e.g., language comprehension, planning, episodic memory formation, etc.) However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context. Inspired by biologically plausible neuroscience theories of cognition, we propose Predictive Attractor Models (PAM), a novel sequence memory architecture with desirable generative properties. PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input only once. Additionally, we find that PAM avoids catastrophic forgetting by uniquely representing past context through lateral inhibition in cortical minicolumns, which prevents new memories from overwriting previously learned knowledge. PAM generates future predictions by sampling from a union set of predicted possibilities; this generative ability is realized through an attractor model trained alongside the predictor. We show that PAM is trained with local computations through Hebbian plasticity rules in a biologically plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based learning, capacity scaling) are discussed throughout the paper. Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research. Illustration videos and code are available on our project page: https://ramymounir.com/publications/pam. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modeling the temporal associations between consecutive inputs in a sequence (i.e., sequential memory) enables biological agents to perform various cognitive functions, such as episodic memory formation [53, 64, 15], complex action planning [22] and translating between languages [3]. For example, playing a musical instrument requires remembering the sequence of notes in a piece of music; similarly, playing a game of chess requires simulating, planning, and executing a sequence of moves in a specific order. While the ability to form such memories of static, unrelated events has been extensively studied [56, 55, 63, 69, 66], the ability of biologically-plausible artificial networks to learn and recall temporally-dependent patterns has not been sufficiently explored in literature [62]. The task of sequential memory is considered challenging for models operating under biological constraints (i.e., local synaptic computations) for many reasons, including catastrophic forgetting, ambiguous context representations, multiple future possibilities, etc. ", "page_idx": 0}, {"type": "text", "text": "In addition to the biological constraint, we impose the following set of desirable characteristics as learning constraints on sequence memory models. ", "page_idx": 0}, {"type": "text", "text": "\u2022 The learning of one sequence does not overwrite the previously learned sequences. This property is defined under the continual learning framework as avoiding catastrophic forgetting and evaluated with the Backward Transfer (BWT) metric [38].   \n\u2022 The model should uniquely encode inputs based on their context in a sequence. Consider the sequence of letters \u201cEVER\u201d; the representation of \u201cE\u201d at position 1 should be different from \u201cE\u201d at position 3, thus resulting in different predictions: \u201cV\u201d and \u201cR\u201d. Moreover, the representation of \u201cE\u201d at position 3 in \u201cEVER\u201d should be different from \u201cE\u201d at position 3 in \u201cCLEVER\u201d. Therefore, positional encoding is not a valid solution.   \n\u2022 When presented with multiple valid, future possibilities, the model should learn to represent each possibility separately yet stochastically sample a single valid possibility. Consider the two sequences \u201cTHAT\u201d and \u201cTHEY\u201d; after seeing \u201cTH\u201d, the model should learn to generate either \u201cA\u201d or \u201cE\u201d, but not an average [37] or a union of both [25].   \n\u2022 The model should be capable of incrementally learning each transition without seeing the whole sequence or revisiting older sequence transitions that are previously learned. This property falls under online learning constraints, also called stream learning [45, 25].   \n\u2022 The learning algorithm should be tolerant and robust to significant input noise. A model should continuously clean the noisy inputs using learned priors and beliefs, thus performing future predictions based on the noise-free observations [25]. ", "page_idx": 1}, {"type": "text", "text": "We propose Predictive Attractor Models (PAM), which consists of a state prediction model and a generative attractor model. The predictor in PAM is inspired by the Hierarchical Temporal Memory (HTM) [25] learning algorithm, where a group of neurons in the same cortical minicolumn share the same receptive feedforward connection from the sensory input on their proximal dendrites. The depolarization of the voltage of any neuron in a single minicolumn (i.e., on distal dendrites) primes this depolarized neuron to fire first while inhibiting all the other neurons in the same minicolumn from firing (i.e., competitive learning). The choice of which neurons fire within the same minicolumn is based on the previously active neurons and their trainable synaptic weights to the depolarized neurons, which gives rise to a unique context representation for every input [20]. The sparsity of representations (discussed later in Section 3.2) allows for multiple possible predictions to be represented as a union of individual cell assemblies. The Attractor Model learns to disentangle possibilities by strengthening the synaptic weights between active neurons of input representations and inhibiting the other predicted possibilities from firing, effectively forming fixed point attractors during online learning. During recall, the model uses these learned conditional attractors to sample one of the valid predicted possibilities or uses the attractors as prior beliefs for removing noise from sensory observations. ", "page_idx": 1}, {"type": "text", "text": "PAM satisfies the above-listed constraints for a sequential memory model, whereas the current state-of-the-art models fail in all or many of the constraints, as shown in the experiments. Our contributions can be summarized as follows: (1) Present the novel PAM learning algorithm that can explicitly represent context in memory without backpropagation, avoid catastrophic forgetting, and perform stochastic generation of multiple future possibilities. (2) Perform extensive evaluation of PAM on multiple tasks (e.g., sequence capacity, sequence generation, catastrophic forgetting, noise robustness, etc.) and different data types (e.g., protein sequences, text, vision). (3) Formulate PAM and its learning rules as a State Space Model grounded in variational inference and the Free Energy Principle [17]. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Predictive Coding ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Predictive coding proposes a framework for the hierarchical processing of information. It was initially formulated as a time series compression algorithm to create a more efficient coding system [16, 46]. A few decades later, PC was used to model visual processing in the Retina [59, 27] as an inference model. In the seminal work of Rao and Ballard [52], PC was reformulated as a general computational model of the cortex. The main intuition is that the brain continuously predicts all perceptual inputs, resulting in a quantity of prediction error, which can be minimized by adjusting its neural activities and synaptic strengths. In-depth variational free energy derivation is provided in Appendix A.1. ", "page_idx": 1}, {"type": "text", "text": "PC defines two subgroups of neurons: value $_{z}$ and error. Each neuron contains a value node sending its prediction to the lower level $\\hat{z}_{l}=f_{l+1}(z_{l+1})$ through learnable function $f$ , and error node propagating its computed error to the higher level. The total prediction error is computed as $\\begin{array}{r}{\\epsilon=\\dot{\\sum_{l}}\\,\\vert\\vert(\\bar{z_{l}}-\\bar{\\hat{z}_{l}})\\vert\\vert_{2}^{2}}\\end{array}$ , which is minimized by first running the network value nodes to equilibrium through optimizing the value nodes $\\{z_{l}\\}_{l=0}^{L}$ . At equilibrium, the value nodes are fixed, and inferential optimization is performed by optimizing the functions {fl}lL=1. Both optimizations aim to minimize the same prediction error over all neurons. This propagation of error to equilibrium is shown to be equivalent to backpropagation but using only local computations [66]. The PC formulation has shown success in training on static and i.i.d data [66, 69, 57, 23]. More recently [62], Temporal Predictive Coding (tPC) has also shown some success in sequential memory tasks by modifying error formulation to account for a one-step synaptic delay through interneurons, thus modeling temporal associations between sequence inputs. In the experiments, we compare our model to tPC and its 2-layer variant [62]. A few PC-based models [47, 48] have shown promise for class-incremental continual learning tasks, other PC-inspired models, such as [39, 23, 24], have diverged from the biologically plausible constraints by training through backpropagation through time. ", "page_idx": 2}, {"type": "text", "text": "2.2 Fixed-Point Attractor Dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Attractor dynamics refer to mathematical models that describe the behavior of dynamical systems. In our review, we focus on fixed point attractors, specifically Hopfield Networks [26] (HN), which is an instance of associative memory models [32, 29, 30]. Consider, an ordered sequence of $T+1$ consecutive patterns $\\pmb{x}\\,=\\,[\\pmb{x}_{t}]_{t=1}^{T+1}$ , where $\\pmb{x}_{t}\\,\\in\\,\\{-1,1\\}^{N}$ . We refer to the Universal Hopfield Networks (UHN) framework [41] to describe all variants of HN architecture using a similarity (sim) function and a separation (sep) function, as shown in equation 1. This family of models can be viewed as a memory recall function, where a query $\\xi$ (i.e., noisy or incomplete pattern) is compared to the existing patterns to compute similarity scores using the \u201csim\u201d function. These scores are then used to weight the projection patterns after applying a \u201csep\u201d function to increase the separation between similarity scores. The classical HN uses symmetric weights to store the patterns; therefore, it cannot be used to model temporal associations in a sequence. The asymmetric Hopfield Network (AHN) [58] uses asymmetric weights to recall the next pattern in the sequence for a given query $\\xi$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\xi}=\\underbrace{P}_{\\mathrm{Projection}}\\cdot\\underbrace{\\operatorname*{sep}}_{\\mathrm{Separation}}(\\underbrace{\\sin(M,\\xi)}_{\\mathrm{Similarity}})=\\left\\{\\sum_{t=1}^{T}\\mathbf{x}_{t+1}\\mathrm{sep}\\left(\\sin(\\mathbf{x}_{t},\\xi)\\right)\\right.\\quad\\mathrm{Asymmetric~Weights}}_{\\mathrm{Erell~Xetsep}\\left(\\sin(\\mathbf{x}_{t},\\xi)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When a dot product \u201csim\u201d function and identity \u201csep\u201d function are used, we get the classical HN [26] and AHN [58]. A few variants have been proposed to increase the capacity of the model. Recently, [12] has extended AHN by using a polynomial (with degree $d$ ) or a softmax function (with temperature $\\beta_{.}$ ) as the \u201csep\u201d function. HN can also be applied to continuous dense patterns [34, 51, 12], and extended to sparse modern Hopfield models [28, 67]. ", "page_idx": 2}, {"type": "text", "text": "2.3 Predictive Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Predictive learning takes a more general form of minimizing the prediction error between two views of the same input to improve representations. Many backpropagation-based approaches to predictive learning have been proposed; most recently, JEPA [37] and its variants [8, 10, 9, 70], learn useful dense representations from images and videos using the predictive objective. Other models, such as [11, 21, 13] - to list a few, use a similar methodology of predicting distorted versions of the same input to learn good feature representations. Prediction-based approaches have also been used to segment videos into events temporally [2, 42] and spatially [44, 43, 1]. More recently, STREAMER [45] used a predictive learning approach to achieve hierarchical segmentation and representation learning from streaming egocentric videos, where a high prediction error is used as an event boundary. While these biologically implausible approaches show impressive results on their respective tasks, they still suffer from deep learning known limitations, such as catastrophic forgetting and the inability to generate multiple possibilities in regression-based predictive tasks. Hierarchical Temporal Memory (HTM) [25] is a predictive approach that is heavily inspired by neurobiology. HTM relies on lateral inhibition between neurons of the same minicolumn and sparsity of input representations (i.e., SDR) to learn temporal context and associations using only local Hebbian rules. ", "page_idx": 2}, {"type": "text", "text": "HTM can be applied to online tasks, such as anomaly detection [6] and object recognition [36], but it is currently incapable of generating future predictions in auto-regressive prediction tasks. ", "page_idx": 3}, {"type": "text", "text": "3 Predictive Attractor Models ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/24c39b660b05774faa4c3debdc80e4864c7bcbda06fd568edea1b0c654cef6dc.jpg", "img_caption": ["3.1 State Space Model (SSM) formulation "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: State Space Model. (Left): Dynamical system represented by first-order Markov chain of latent states $_{\\textit{z}}$ with transition function $f$ and an emission function $g$ which projects to the observation states $\\textbf{\\em x}$ . (Right): Gaussian form assumptions for the prior $\\hat{z}$ and posterior $_{z}$ latent states, and the Mixture of Gaussian model representing the conditional probability of multiple possibilities $p(\\pmb{x}|\\pmb{z})$ ", "page_idx": 3}, {"type": "text", "text": "PAM can be represented as a dynamical system with its structure depicted by a Bayesian probabilistic graphical [35, 31] model, more specifically, a State Space Model, where we can perform Bayesian inference on the latent variables and derive learning rules using Variational Inference. Formally, we define a state space model as a tuple $(\\mathcal{Z},\\mathcal{X},f,g)$ , where $\\mathcal{Z}$ is the latent state space, $\\mathcal{X}$ is the observation space, and $f$ and $g$ are the transition and emission functions respectively (similar to HMM [50]). We consider a Gaussian form with white Gaussian noise covariance $\\Sigma_{z}$ for the latent states. However, we assume a latent state $_{\\textit{z}}$ can generate multiple valid possibilities. Therefore, we model the conditional probability $p(\\pmb{x}_{t}|\\pmb{z}_{t})$ as a Multivariate Gaussian Mixture Model (GMM), where each mode is considered a possibility or a fixed-point attractor in an associative memory model. The GMM has $C$ components with means $g_{c}(z_{t})$ , covariances $\\Sigma_{c}$ and component weights $w_{c}$ . The SSM dynamics can be formally represented with the following equations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{t}|\\boldsymbol{z}_{t-1}\\sim\\mathcal{N}(f(\\boldsymbol{z}_{t-1}),\\boldsymbol{\\Sigma}_{z}),\\quad\\mathrm{and}\\quad\\boldsymbol{x}_{t}|\\boldsymbol{z}_{t}\\sim\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(g_{c}(\\boldsymbol{z}_{t}),\\boldsymbol{\\Sigma}_{c})\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathscr{z}_{t}\\in\\mathcal{Z}$ and $\\mathbf{\\boldsymbol{x}}_{t}\\in\\mathcal{X}$ . From the Bayesian inference viewpoint, we are interested in the posterior $p(z_{t}|\\mathbf{x}_{t},z_{t-1})$ . Since the functions $f$ and $g$ are non-linear, the computation of this posterior is intractable (unlike an LG-SSM, such as Kalman Filter [65]). Therefore, we utilize variational inference to approximate the posterior by assuming the surrogate posterior $q(z)$ has a Gaussian form, and minimize the Variational Free Energy (VFE) [18]. The minimization of VFE (in equation 3) minimizes the KL-divergence between the approximate posterior $q(z)$ and the true posterior $p(z_{t}|\\mathbf{x}_{t},z_{t-1})$ . Derivation 2 of Variational Free Energy is provided in appendix A.1 ", "page_idx": 3}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/0d93a6af516f7cd6bab3ffa4097d90fd1dd9d094ec846c04be95fc6132b7db42.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{E}_{q}\\equiv\\mathbb{E}_{z_{t}\\sim q(z_{t})}$ and $\\mathcal{H}_{q}$ is the Entropy of the approximate posterior $q(z)$ . The assumption of Gaussian forms for the latent and observable states can further simplify the negative log-likelihood terms (i.e., Latent State Accuracy and Observation Accuracy) to prediction errors. This learning objective encourages the approximate posterior $q(z)$ to assign a high probability to states that explain the observations well and follow the latent dynamics of the system. We minimize the prediction errors (i.e., learn the transition and emission functions) through Hebbian rules as shown in equations 7 and 8. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 1 Assume the likelihood $p(\\pmb{x}_{t}|\\pmb{z}_{t})$ in eqn $3$ represents multiple possibilities using a Gaussian Mixture Model (GMM) conditioned on the latent state $\\boldsymbol{z}_{t}$ , as shown in eqn 2. The maximization of such log-likelihood function (i.e.,\u2202\u2202x l $\\begin{array}{r}{\\frac{\\partial}{\\partial\\mathbf{x}}\\log p(\\mathbf{x}_{t}|\\boldsymbol{z}_{t}),}\\end{array}$ w.r.t a query observation state $\\textbf{\\em x}$ is equivalent to the Hopfield recall function (i.e., eqn $I$ ) with the means of the GMM representing the attractors of a Hopfield model. Formally, the weighted average of the GMM means (i.e., $\\{\\mu_{c}\\}_{c=1}^{C})$ , with the weights being a similarity measure, maximizes the log-likelihood of $\\textbf{\\em x}$ under the mixture model. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}=\\sum_{c=1}^{C}\\underbrace{\\frac{w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})\\cdot\\pmb{\\Sigma}_{c}^{-1}}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})\\cdot\\pmb{\\Sigma}_{c}^{-1}}}_{s i m i l a r i t y\\;s c o r e}\\cdot\\underbrace{\\pmb{\\mu}_{c}}_{p r o j e c t i o n}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "", "img_caption": ["Proof: See derivation 3 in appendix A.2 for the full proof. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Preliminaries and Notations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Sparse Distributed Representation (SDR) Inspired by the sparse coding principles observed in the brain, SDRs encode information using a small set of active neurons in high dimensional binary representation. We adopt SDRs as a more biologically plausible cell assembly representation [30]. SDRs have desirable characteristics, such as a low chance of false positives and collisions between multiple SDRs and high representational capacity [5] (More on SDRs in Appendix G). An SDR is parameterized by the total number of neurons $N$ and the number of active neurons $W$ . The ratio $S\\,=\\,W/N$ denotes the SDR sparsity. A 1-dimensional SDR $\\textbf{\\em x}$ can be indexed as $\\pmb{x}^{i}\\,\\in\\,\\{0,1\\}$ , whereas a 2-dimensional SDR $_{z}$ can be indexed as $z^{i j}\\in\\{0,1\\}$ . To identify the active neurons, we define the function $I:\\{0,1\\}^{N}\\mapsto\\mathbb{N}^{W}$ to represent the indices of the active neurons in an SDR $\\textbf{\\em x}$ as $I(\\pmb{x})=\\{i|\\pmb{x}^{i}=1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Context as Orthogonal Dimension We transform the high-order Markov dependencies between observation states into a first-order Markov chain of latent states by storing context information in those latent states. The latent states SDRs, $z\\in\\{0,1\\}^{N_{c}\\times N_{k}}$ , are represented with two orthogonal dimensions, where content information about the input is stored in one dimension with size $N_{c}$ , while context related information is stored in an orthogonal dimension with size $N_{k}$ . Therefore, the projection of the latent state $_{z}$ on the first dimension $(\\mathrm{i}.\\mathrm{e}.,\\downarrow z)$ removes all context information from the state. In contrast, adding context information to an observation state $\\textbf{\\em x}$ expands the dimensionality of the state (i.e., $\\uparrow\\alpha)$ such that context can be encoded without affecting its content. Competitive learning is enforced on the context dimension through lateral inhibition, effectively minimizing the collisions between contexts of multiple SDRs. We define a projection operator $\\downarrow$ : $\\left\\{0,1\\right\\}^{N_{c}\\times\\check{N}_{k}}\\mapsto\\{0,1\\}^{N_{c}}$ . Additionally, we define a projection operator $\\uparrow$ : $\\{0,\\dot{1}\\}^{\\check{N}_{c}}\\mapsto\\{0,\\dot{1}\\}^{N_{c}\\times\\dot{N_{k}}}$ for 1-dimensional SDRs (i.e., $\\textbf{\\em x}$ ) as shown in equation 5. A detailed list of notations is provided in the supplementary material (Table 1). ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\downarrow z)^{i}=\\left\\{\\begin{array}{l l}{{1}}&{{\\mathrm{if}\\;\\exists j\\;\\mathrm{s.t.}\\;z^{i j}=1,}}\\\\ {{0}}&{{\\mathrm{otherwise,}}}\\end{array}\\right.\\;\\;(\\uparrow x)^{i j}=\\left\\{\\begin{array}{l l}{{1}}&{{\\mathrm{if}\\;x^{i}=1,}}\\\\ {{0}}&{{\\mathrm{otherwise,}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Sequence Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a sequence of $T+1$ SDR patterns $[\\pmb{x}_{t}]_{t=1}^{T+1}$ , where $\\pmb{x}_{t}\\in\\{0,1\\}^{N_{c}}$ , the sequence can be learned by modeling the context-dependent transitions between consecutive inputs within the sequence. We define learnable weight parameters for transition and emission functions, $\\pmb{A}\\in\\mathbb{R}^{N_{c}N_{k}\\times\\mathbf{\\hat{N}}_{c}N_{k}}$ , $B\\in$ $\\mathbb{R}^{N_{c}\\times N_{c}}$ respectively. A single latent state transition is defined as $\\hat{\\boldsymbol{z}}_{t}=\\delta(\\mathbf{A}\\cdot\\boldsymbol{z}_{t-1})=\\delta(\\mathbf{a}_{t})$ , where $\\delta$ is a threshold function transforming the logits $\\mathbf{\\deltaa}_{t}$ to the predicted SDR state $\\hat{z}_{t}$ . The full sequence learning algorithm is provided in algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 : Sequence Learning. Given a sequence $\\textbf{\\em x}$ of $\\mathrm{T}{+1}$ patterns, this algorithm learns the Transition and Emission synaptic weights ( $\\mathbf{\\deltaA}$ and $_B$ ). Fixed start context $m({\\pmb a}_{0})$ is initialized for all learned sequences. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 : Sequence Generation. Given a noisy sequence (i.e., online), or the first input in a sequence (i.e., offline). The model uses the learned functions $\\pmb{A}$ and $_B$ to generate the full sequence. $\\sim$ denotes sampled from (eqn 9). ", "page_idx": 5}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/876b188424ca7fc5d032dc63530cf0d27be1040d79231ba2451f5e01110f1def.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Context Encoding through Competitive Learning Every observation $\\pmb{x}_{t}$ contains only content information about the input; we embed the observation with context by expanding the state with an orthogonal dimension (i.e., $\\uparrow\\pmb{x}_{t})$ , which activates all neurons in the minicolumns at the indices $I(\\pmb{x}_{t})$ . Then, for each active minicolumn, the neuron in a predictive state (i.e., higher than the prediction threshold) fires and inhibits all the other neurons in the same minicolumn from firing (i.e., lateral inhibition), as shown in Equation 6. If none - or more than one - of the neurons are in a predictive state, random Gaussian noise (\u03f5) acts as a tiebreaker to select a context neuron. We do not allow multiple neurons to fire in the same minicolumn, which is different from HTM [25], where multiple cells can fire in any minicolumn (e.g., bursting). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m({a}_{t})^{i j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;a_{t}^{i j}=\\operatorname*{max}(\\{\\delta(a_{t}^{i j})+\\epsilon\\}_{j=0}^{N_{k}}),}\\\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.\\quad z_{t}=(\\uparrow x_{t})\\cap m({a}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "State Transition Learning The transition between latent states is learned through local computations with Hebbian-based rules. We modify the synaptic weights $\\pmb{A}$ to model the transition between pre-synaptic neurons $z_{t-1}$ and post-synaptic neurons $\\boldsymbol{z}_{t}$ . Only the synapses with active pre-synaptic neurons are modified. The weights operate on context-dependant latent states (i.e., $z_{t-1}\\to z_{t})$ ); thus, the learning of one transition does not overwrite previously learned transition of different contexts, regardless of the input contents (i.e., $\\mathbf{\\nabla}x_{t-1}\\,\\mathrm{~}$ ). We use the learning constant coefficients $\\eta_{A}^{+}$ and $\\eta_{A}^{-}$ to independently control learning and forgetting behavior, as shown in equation 7. A lower $\\eta_{A}^{-}$ encourages learning multiple possibilities by slowing down the forgetting behavior. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta A=\\underbrace{\\eta_{A}^{+}\\cdot z_{t-1}\\cdot z_{t}^{-}}_{\\Delta A_{\\mathrm{increase}}}+\\underbrace{\\eta_{A}^{-}\\cdot z_{t-1}\\cdot\\left(1-z_{t}\\right)^{T}}_{\\Delta A_{\\mathrm{decrease}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Contrastive Attractors Formation The attractors are formed in an online manner by contrasting the input observation $\\pmb{x}_{t}$ to the predicted union set of possibilities $\\downarrow\\hat{z_{t}}$ . The goal is to learn excitatory synapses between active neurons of $\\pmb{x}_{t}$ , and bidirectional inhibitory synapses between $\\pmb{x}_{t}$ and the union set of predicted possibilities excluding the $\\pmb{x}_{t}$ possibility (i.e., $(\\downarrow\\hat{z}_{t})\\setminus x_{t}))$ ), as shown in equation 8. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta B=\\underbrace{\\eta_{B}^{+}\\cdot x_{t}\\cdot x_{t}}_{\\Delta B_{\\mathrm{incrase}}}+\\underbrace{\\eta_{B}^{-}\\cdot\\left[x_{t}\\cdot\\left(\\left(\\downarrow\\hat{z}_{t}\\right)\\setminus x_{t}\\right)^{T}+\\left(\\left(\\downarrow\\hat{z}_{t}\\right)\\setminus x_{t}\\right)\\cdot x_{t}^{T}\\right]}_{\\Delta B_{\\mathrm{decrase}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Sequence Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After learning one or multiple sequences using algorithm 1, we use algorithm 2 to generate sequences. First, we define two generative tasks: online and offline. In online sequence generation, a noisy version of the sequence is provided as input, and the model is expected to generate the original learned sequence. In offline sequence generation, the model is only provided with the first input, and it is expected to generate the entire sequence auto-regressively. For cases with equally valid future predictions (e.g., \u201ca\u201d and \u201ce\u201d after \u201cTH\u201d in \u201cTHAT\u201d and \u201cTHEY\u201d), the model is expected to stochastically generate either one of the possibilities (i.e., \u201cTHAT\u201d or \u201cTHEY\u201d). The online generation task is a more challenging extension of the online recall task in [62], where the noise-free inputs are provided, and the model only makes a 1-step prediction into the future. During offline sequence generation, the model randomly samples from the union set of predictions $\\downarrow\\hat{z}$ a single SDR with $W$ active neurons (equation 9) to initialize the iterative attractor procedure. $\\pi$ denotes a random permutation function. This random permutation function allows the model to randomly generate a different sequence with every generation. ", "page_idx": 5}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/9534d7b0bfb9f0dd85c8b25d7a7dad727f2a31b8a396d8b6ecec9c84a9d098ba.jpg", "img_caption": ["Figure 2: Sequence Generation. (Left): Offline generation by sampling a single possibility (i.e., attractor point) from a union of predicted possibilities. (Right): Online generation by removing noise from an observation using the prior beliefs about the observed state. Markov Blanket separates the agent\u2019s latent variables from the world\u2019s observable states. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{x}}^{i}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}i\\in\\{\\pi(I(\\downarrow\\hat{z}_{t}))^{w}\\}_{w=0}^{W},}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Evaluation and Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Metrics To evaluate the similarity of two SDRs, we use the Jaccard Index (i.e., IoU)1, which focuses on the active bits in sparse binary representations. Since the sparsity $S$ of the representations can change across experiments and methods, we normalize the IoU by the expected IoU (Derived in Theorem 2 in Appendix A.3) of two random SDRs at their specified sparsities. The normalized IoU is computed a s Io1U\u2212\u2212EE[I[oIoUU]] . We use the Backward Transfer [38] metric in evaluating catastrophic forgetting. Mean Squared Error (MSE) is used to compare images for vision datasets. ", "page_idx": 6}, {"type": "text", "text": "Datasets We perform evaluations on synthetic and real datasets. The synthetic datasets allow us to manually control variables (e.g., sequence length, correlation, noise, input size) to better understand the models\u2019 behavior across various settings. Additionally, we evaluate on real datasets of various types (e.g., protein sequences, text, vision) to benchmark PAM\u2019s performance relative to other models on more challenging and real sequences. For all vision experiments, we use an SDR autoencoder to learn a mapping between images and SDRs (Details on the autoencoder are provided in Appendix C). We run all experiments for 10 different seeds and report the mean and standard deviation in all the figures. More experimental details and results are provided in Appendices $\\mathrm{C}$ and E. ", "page_idx": 6}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/14fbc6506f215650e21186cf18e9e204f5bf8aa4c02c387047d189de54d6521f.jpg", "img_caption": ["Figure 3: Quantitative results on (A-B) Offline sequence capacity, (C) Noise robustness, and $({\\bf D})$ Time of sequence learning and recall. Qualitative results on highly correlated CIFAR sequence in $(\\mathbf{E})$ offilne and $(\\mathbf{F})$ online settings. The mean and standard deviation of 10 trials are reported for all plots. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We align our evaluation tasks with the desired characteristics of a biologically plausible sequence memory model, as listed in the introduction. We show that PAM outperforms current predictive coding and associative memory SoTA approaches on all tasks. Most importantly, PAM is capable of long context encoding, multiple possibilities generation, and learning continually and efficiently while avoiding catastrophic forgetting. These tasks pose numerous significant challenges to other methods. ", "page_idx": 7}, {"type": "text", "text": "Offilne Sequence Capacity We evaluate the models\u2019 capacity to learn long sequences by varying the input size $N_{c}$ , model parameters (e.g., $N_{k}$ ), and sequence correlation. The correlation is increased by reducing the number of unique patterns (i.e., vocab) used to create a sequence of length $T$ . Correlation is computed as $1.0~\\bar{-}~\\frac{\\bar{\\mathrm{vocab}}}{T}$ . In Figure $3\\textbf{A}$ , we vary the input size $N_{c}$ and ablate the models to find the maximum sequence length to be encoded and retrieved, in an offilne manner, with a Normalized IoU higher than 0.9. We set the number of active bits $W$ to be 5 unless otherwise specified. Results show that Hopfield Networks (HN) fail to learn with sparse representations; therefore, we use $W$ of $0.5N_{c}$ only for HN and normalize the IoU metric accordingly. PAM\u2019s capacity significantly increases with context neurons $N_{k}$ , as expected. HN\u2019s capacity also increases with the polynomial degree $d$ of its separation function; however, as shown in Figure $\\mathbf{\\nabla}3\\textbf{B}$ , the capacity sharply drops as correlation increases. PAM retains its capacity with increasing correlation, reflecting its ability to encode context in long sequences (i.e., high-order Markov memory). This context encoding property is also demonstrated in the qualitative CIFAR [33] results in Figure $3\\,\\mathbf{E}$ and $\\mathbf{F}$ , where a short sequence of images with high correlation is used. The model must uniquely encode the context to correctly predict at every step in the sequence. While PAM correctly predicts the full context, single-layer tPC learns to indefinitely alternate between the patterns, while two-layered tPC attempts to average its predictions. AHN shows similar low performance and failure mode as in [62]. ", "page_idx": 7}, {"type": "text", "text": "Catastrophic Forgetting To assess the model\u2019s performance in a continual learning setup, we sequentially train each model on multiple sequences and compute the Backward Transfer (BWT) [38] metric by reporting the average normalized IoU on previously learned sequences after learning a new one. In Figure $4\\,\\mathbf{A}$ , we report BWT for 50 synthetically generated sequences with varying correlations. AHN can avoid catastrophic forgetting when the patterns are not correlated, whereas tPC fails to retain previous knowledge regardless of the correlation value. PAM, with high enough context $N_{k}$ , does not overwrite or forget previously learned sequences after learning new ones but performs poorly when $N_{k}=1$ , as expected. We repeat the experiment on more challenging sequences from ProteinNet [7], which contains highly correlated $\\left(>0.9\\right)$ and long sequences (details in appendix). The results in Figure ${\\bf4\\nabla B}$ show a similar trend with PAM requiring more context neurons $N_{k}$ to handle the more challenging data. Qualitative results on moving-MNIST [60] in Figure 4 F further demonstrate the catastrophic forgetting challenge where the learning of the second sequence overwrites the learned sequence. PAM successfully retrieves the previously learned sequence while the other models fail. ", "page_idx": 7}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/6d110badcbc7317eb98588e7b64b77546aa563233cadaab3966e5436552bbe7f.jpg", "img_caption": ["Figure 4: Qualitative results on (A) synthetic and $({\\bf{B}})$ protein sequences backward transfer, and (C-D) multiple possibilities generation on text datasets. Qualitative results on $\\mathbf{\\tau}(\\mathbf{E})$ noise robustness on CLEVRER sequence, and $(\\mathbf{F})$ catastrophic forgetting on Moving MNIST dataset. highlights the first frame with significant error. The mean and standard deviation of 10 trials are reported for all plots. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Multiple Possibilities Generation In addition to accurately encoding input contexts, PAM is designed to represent multiple valid possibilities and sample a single possibility. We perform evaluation on a dataset of four-letter English words (details in appendix), which includes many possible future completions (e.g., \u201cth[is]\u201d, \u201cth[at]\u201d, \u201cth[em]\u201d, etc.) We train PAM on the list of letters sequentially (i.e., one word at a time); the other methods are trained in a simpler batched setup as in [62] because they suffer from catastrophic forgetting. This puts PAM at a disadvantage, but as shown in Figure 4 C, PAM still significantly outperforms the other methods in accurately generating valid words (high average IoU) in an offline manner. Both tPC and AHN fail to generate meaningful words when trained on sequences with multiple future possibilities. Figure 4 D further demonstrates the stochastic generative property of PAM. We show PAM\u2019s ability to recall more of the dataset as it repeats the generation process, whereas PC and AHN fail in the dataset recall task. ", "page_idx": 8}, {"type": "text", "text": "Online Noise Robustness The online generation ability of PAM shown in Figure 2 allows the model to use the learned attractors to clean the noisy observations before using them as inputs to the predictor. This step allows the model to use its prior belief about future observations to modify the noisy inputs. In Figure $\\textrm{\\scriptsize3C}$ , we evaluate the models\u2019 performances by changing a percentage of the active bits during online generation. PAM is able to completely replace the noisy input with its prior belief if it does not exist in the predicted set of possibilities $\\hat{z}$ . In contrast, the other methods use the noisy inputs, thus hindering their performances. We provide qualitative results on CLEVRER [68] in Figure $4\\,\\mathbf{E}$ ; PAM retrieves the original sequence despite getting noisy inputs ( $40\\%$ noise), and outperforms the other models. Interestingly, tPC performs reasonably well on this task despite the added noise. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Efficiency We report, in Figure $\\mathbf{\\nabla}3\\textbf{D}$ , the time each model requires to learn and recall a sequence. For this study, we use input size $N_{c}=100$ and vary the sequence length. PAM operates entirely on CPU. The results show that a single-layer tPC model requires more time than all PAM variants $(N_{k}\\leq24)$ . Additionally, a two-layered tPC requires two to three orders of magnitude more time than PAM or single-layered tPC, significantly limiting its scalability and practicality when applied to real data with long sequences. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "PAM requires that its representations be sparse and binary (i.e., SDRs) in order to represent multiple possibilities as a union of SDRs with minimal overlap. Therefore, PAM cannot be directly applied to images in the input space like Dense Hopfield Networks. We argue that the neocortex encodes sensory input into SDRs for processing and does not operate directly on the input in its dense representation. Methods that operate directly on dense representations (e.g., images) are arguably less biologically plausible as the neocortex uses sparse binary representations (i.e., cell assemblies with a small fraction of firing neurons) to store and manipulate information. This paper focuses on learning multiple sequences without catastrophic forgetting and stochastically generating multiple possibilities efficiently, and we assume the sequence is provided in SDR format. Additionally, methods that operate on the input directly face challenges when the input is naturally sparse (see Figure $3\\textbf{A}$ ). Therefore, it is useful to encode the input into a representation with fixed sparsity before applying sequential memory learning algorithms. In future work, we plan to investigate how to encode high dimensional complex inputs (e.g., images) in a compositional part-whole structure of SDRs where we can apply PAM at different levels of abstraction. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed PAM, a biologically plausible generative model inspired by neuroscience findings and theories of cognition. We demonstrate that PAM is capable of encoding unique contexts with tremendous scalable capacity that is not affected by sequence correlation or noise. PAM is a generative model; it can represent multiple possibilities as a union of SDRs (a property of sparsity) and sample single possibilities, thus predicting multiple steps in the future despite multiple possible continuations. We also show that PAM does not suffer from catastrophic forgetting as it learns multiple sequences. PAM is trained using local computations through Hebbian rules and runs efficiently on CPUs. Future directions include hierarchical sensory processing and higher-order sparse predictive models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by the US National Science Foundation Grants CNS 1513126 and IIS 1956050. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sathyanarayanan Aakur and Sudeep Sarkar. Action localization through continual predictive learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV 16, pages 300\u2013317. Springer, 2020.   \n[2] Sathyanarayanan N Aakur and Sudeep Sarkar. A perceptual prediction framework for self supervised event segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1197\u20131206, 2019.   \n[3] Valentin Afraimovich, Xue Gong, and Mikhail Rabinovich. Sequential memory: Binding dynamics. Chaos: An Interdisciplinary Journal of Nonlinear Science, 25(10), 2015.   \n[4] Subutai Ahmad and Jeff Hawkins. Properties of sparse distributed representations and their application to hierarchical temporal memory. arXiv preprint arXiv:1503.07469, 2015.   \n[5] Subutai Ahmad and Jeff Hawkins. How do neurons operate on sparse distributed representations? a mathematical theory of sparsity, neurons and active dendrites. arXiv preprint arXiv:1601.00720, 10, 2016.   \n[6] Subutai Ahmad, Alexander Lavin, Scott Purdy, and Zuha Agha. Unsupervised real-time anomaly detection for streaming data. Neurocomputing, 262:134\u2013147, 2017.   \n[7] Mohammed AlQuraishi. Proteinnet: a standardized data set for machine learning of protein structure. BMC bioinformatics, 20:1\u201310, 2019.   \n[8] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023.   \n[9] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024.   \n[10] Adrien Bardes, Jean Ponce, and Yann LeCun. Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of motion and content features. arXiv preprint arXiv:2307.12698, 2023.   \n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[12] Hamza Chaudhry, Jacob Zavatone-Veth, Dmitry Krotov, and Cengiz Pehlevan. Long sequence hopfield memory. Advances in Neural Information Processing Systems, 36, 2024.   \n[13] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n[14] Nikolaos Chrysanthidis, Florian Fiebig, and Anders Lansner. Introducing double bouquet cells into a modular cortical associative memory model. Journal of computational neuroscience, 47(2):223\u2013230, 2019.   \n[15] Martin A Conway. Episodic memories. Neuropsychologia, 47(11):2305\u20132313, 2009.   \n[16] Peter Elias. Predictive coding\u2013i. IRE transactions on information theory, 1(1):16\u201324, 1955.   \n[17] Karl Friston. The free-energy principle: a unified brain theory? Nature reviews neuroscience, 11(2):127\u2013 138, 2010.   \n[18] Karl Friston, J\u00e9r\u00e9mie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational free energy and the laplace approximation. Neuroimage, 34(1):220\u2013234, 2007.   \n[19] Karl J Friston, N Trujillo-Barreto, and Jean Daunizeau. Dem: a variational treatment of dynamic systems. Neuroimage, 41(3):849\u2013885, 2008.   \n[20] Dileep George, Rajeev V Rikhye, Nishad Gothoskar, J Swaroop Guntupalli, Antoine Dedieu, and Miguel L\u00e1zaro-Gredilla. Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps. Nature communications, 12(1):2392, 2021.   \n[21] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.   \n[22] Patrick Haggard. Planning of action sequences. Acta Psychologica, 99(2):201\u2013215, 1998.   \n[23] Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, and Zhongming Liu. Deep predictive coding network with local recurrent processing for object recognition. Advances in neural information processing systems, 31, 2018.   \n[24] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130, 2019.   \n[25] Jeff Hawkins and Subutai Ahmad. Why neurons have thousands of synapses, a theory of sequence memory in neocortex. Frontiers in neural circuits, 10:174222, 2016.   \n[26] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982.   \n[27] Toshihiko Hosoya, Stephen A Baccus, and Markus Meister. Dynamic predictive coding by the retina. Nature, 436(7047):71\u201377, 2005.   \n[28] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Hiroshi Kage. Implementing associative memories by echo state network for the applications of natural language processing. Machine Learning with Applications, 11:100449, 2023.   \n[30] Pentti Kanerva. Sparse distributed memory. MIT press, 1988.   \n[31] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.   \n[32] Bart Kosko. Bidirectional associative memories. IEEE Transactions on Systems, man, and Cybernetics, 18(1):49\u201360, 1988.   \n[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[34] Dmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition. Advances in neural information processing systems, 29, 2016.   \n[35] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.   \n[36] Niels Leadholm, Marcus Lewis, and Subutai Ahmad. Grid cell path integration for movement-based visual object recognition. arXiv preprint arXiv:2102.09076, 2021.   \n[37] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022.   \n[38] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.   \n[39] William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016.   \n[40] Henry Markram, Maria Toledo-Rodriguez, Yun Wang, Anirudh Gupta, Gilad Silberberg, and Caizhi Wu. Interneurons of the neocortical inhibitory system. Nature reviews neuroscience, 5(10):793\u2013807, 2004.   \n[41] Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Universal hopfield networks: A general framework for single-shot associative memory models. In International Conference on Machine Learning, pages 15561\u201315583. PMLR, 2022.   \n[42] Ramy Mounir, Sathyanarayanan Aakur, and Sudeep Sarkar. Self-supervised temporal event segmentation inspired by cognitive theories. In Advanced Methods and Deep Learning in Computer Vision, pages 405\u2013448. Elsevier, 2022.   \n[43] Ramy Mounir, Roman Gula, J\u00f6rn Theuerkauf, and Sudeep Sarkar. Spatio-temporal event segmentation for wildlife extended videos. In International Conference on Computer Vision and Image Processing, pages 48\u201359. Springer, 2021.   \n[44] Ramy Mounir, Ahmed Shahabaz, Roman Gula, J\u00f6rn Theuerkauf, and Sudeep Sarkar. Towards automated ethogramming: Cognitively-inspired event segmentation for streaming wildlife video monitoring. International journal of computer vision, 131(9):2267\u20132297, 2023.   \n[45] Ramy Mounir, Sujal Vijayaraghavan, and Sudeep Sarkar. Streamer: Streaming representation learning and event segmentation in a hierarchical manner. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] J O\u2019Neal. Entropy coding in speech and television differential pcm systems (corresp.). IEEE Transactions on Information Theory, 17(6):758\u2013761, 1971.   \n[47] Alex Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Lifelong neural predictive coding: Learning cumulatively online without forgetting. Advances in Neural Information Processing Systems, 35:5867\u2013 5881, 2022.   \n[48] Alexander Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Continual learning of recurrent neural networks by locally aligning distributed representations. IEEE transactions on neural networks and learning systems, 31(10):4267\u20134278, 2020.   \n[49] A Peters and C Sethares. The organization of double bouquet cells in monkey striate cortex. Journal of neurocytology, 26(12):779\u2013797, 1997.   \n[50] Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. ieee assp magazine, 3(1):4\u201316, 1986.   \n[51] Hubert Ramsauer, Bernhard Sch\u00e4f,l Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael K Kopp, et al. Hopfield networks is all you need. In International Conference on Learning Representations, 2020.   \n[52] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79\u201387, 1999.   \n[53] Edmund T Rolls. A computational theory of episodic memory formation in the hippocampus. Behavioural brain research, 215(2):180\u2013196, 2010.   \n[54] Tommaso Salvatori, Ankur Mali, Christopher L Buckley, Thomas Lukasiewicz, Rajesh PN Rao, Karl Friston, and Alexander Ororbia. Brain-inspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870, 2023.   \n[55] Tommaso Salvatori, Luca Pinchetti, Beren Millidge, Yuhang Song, Tianyi Bao, Rafal Bogacz, and Thomas Lukasiewicz. Learning on arbitrary graph topologies via predictive coding. Advances in neural information processing systems, 35:38232\u201338244, 2022.   \n[56] Tommaso Salvatori, Yuhang Song, Yujian Hong, Lei Sha, Simon Frieder, Zhenghua Xu, Rafal Bogacz, and Thomas Lukasiewicz. Associative memories via predictive coding. Advances in Neural Information Processing Systems, 34:3874\u20133886, 2021.   \n[57] Tommaso Salvatori, Yuhang Song, Beren Millidge, Zhenghua Xu, Lei Sha, Cornelius Emde, Rafal Bogacz, and Thomas Lukasiewicz. Incremental predictive coding: A parallel and fully automatic learning algorithm. arXiv preprint arXiv:2212.00720, 2022.   \n[58] Haim Sompolinsky and Ido Kanter. Temporal association in asymmetric neural networks. Physical review letters, 57(22):2861, 1986.   \n[59] Mandyam Veerambudi Srinivasan, Simon Barry Laughlin, and Andreas Dubs. Predictive coding: a fresh view of inhibition in the retina. Proceedings of the Royal Society of London. Series B. Biological Sciences, 216(1205):427\u2013459, 1982.   \n[60] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning, pages 843\u2013852. PMLR, 2015.   \n[61] Dan D Stettler, Aniruddha Das, Jean Bennett, and Charles D Gilbert. Lateral connectivity and contextual interactions in macaque primary visual cortex. Neuron, 36(4):739\u2013750, 2002.   \n[62] Mufeng Tang, Helen Barron, and Rafal Bogacz. Sequential memory with temporal predictive coding. Advances in Neural Information Processing Systems, 36, 2024.   \n[63] Mufeng Tang, Tommaso Salvatori, Beren Millidge, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Recurrent predictive coding models for associative memory employing covariance learning. PLoS computational biology, 19(4):e1010719, 2023.   \n[64] Endel Tulving et al. Episodic and semantic memory. Organization of memory, 1(381-403):1, 1972.   \n[65] Greg Welch, Gary Bishop, et al. An introduction to the kalman filter. 1995.   \n[66] James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):1229\u20131262, 2017.   \n[67] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. Stanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. arXiv preprint arXiv:2312.17346, 2023.   \n[68] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2019.   \n[69] Jinsoo Yoo and Frank Wood. Bayespcn: A continually learnable predictive coding associative memory. Advances in Neural Information Processing Systems, 35:29903\u201329914, 2022.   \n[70] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International conference on machine learning, pages 12310\u201312320. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Theorems and Derivations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Variational Free Energy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Predictive Coding Consider a hierarchical generative model with hidden states $\\{z\\}_{l=0}^{L}$ , where $l\\leq L$ denotes the level in the hierarchy. The conditional probability $p(z_{l}|\\boldsymbol{z}_{l+1})$ is assumed to be a multivariate Gaussian distribution with its mean calculated as a function $f_{l+1}$ of the higher-level hidden representation $z_{l+1}$ and covariance $\\Sigma_{l}$ as shown in equation 10. ", "page_idx": 14}, {"type": "equation", "text": "$$\np(z_{l}|z_{l+1})=\\mathcal{N}(f_{l+1}(z_{l+1}),\\Sigma_{l})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The goal is to calculate the posterior of hidden states given an observation $\\textbf{\\em x}$ , (i.e., $P(\\{z\\}_{l=0}^{L}|\\mathbf{\\boldsymbol{x}}))$ . Since the prediction function $f$ contains a non-linear activation, we cannot analytically compute the posterior and we have to approximate it with a surrogate posterior (i.e., $q(\\{z\\}_{l=0}^{\\bar{L}})$ by maximizing the Evidence Lower Bound (ELBO). We apply the mean field approximation to factorize this joint posterior probability into conditionally independent posteriors $\\{\\bar{q}(z_{l})\\}_{l=0}^{L}\\}$ , and apply the Laplace approximation to use Gaussian forms for the approximate distribution [19, 18, 54]. Through these approximations, we can maximize the ELBO, or equivalently minimize the Variational Free Energy, in equation 11. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbb{E}_{z\\sim q(z)}[\\log(\\frac{q(z)}{p(x,z)})]}_{\\mathrm{Variational~Free~Energy}}=\\underbrace{\\mathbb{E}_{z\\sim q(z)}[\\log(\\frac{q(z)}{p(z)})]}_{D_{K L}(q||p)}+\\underbrace{\\mathbb{E}_{z\\sim q(z)}[\\log(\\frac{1}{p(x|z)})]}_{\\mathrm{Accuracy}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The variational free energy can be reduced to minimizing the negative log-likelihood (Accuracy term), which is simply the prediction error when the likelihood is assumed to take a Gaussian Form. Therefore, minimizing the prediction error reduces to the sum of the squared prediction error of every neuron. ", "page_idx": 14}, {"type": "text", "text": "Derivation 1 Variational Free Energy derivation for the predictive coding objective function in equation $I I$ . We approximate the true posterior $p(z|x)$ with a surrogate posterior $q(z)$ . The objective is to minimize the reverse $K L$ divergence $D_{K L}(q(\\pmb{z})||p(\\pmb{z}|\\pmb{x}))$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{D}_{K L}(q(z)||p(z|x))=\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(z|x)}]}&{(K L\\,D i v e r g e n c e\\ d e f i n i t i o n)}\\\\ &{=\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{q(z)p(x)}{p(x|z)p(z)}]}&{(B a\\ y e s\\ T h e o r e m)}\\\\ &{=\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(x|z)p(z)}]+\\mathbb{E}_{z\\sim q(z)}[\\log p(x)]}&{(L i n e a r i t y\\ o f e x p e c t a t i o n s)}\\\\ {\\mathcal{D}_{K L}(q(z)||p(z|x))=\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(x|z)p(z)}]+\\log p(x)}&{(E i v i d e n c e\\ d o e s\\ n o t\\ d e p e n d o)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To minimize the $K L$ divergence, we can minimize the Variational Free energy instead because the Evidence term $(\\log p(x))$ is a constant negative term. The Variational Free Energy can be further simplified as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{L}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(x|z)p(z)}]=\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{1}{p(x|z)}]+\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(z)}]}&{(L i n e a r i t y\\ o f E x p e c t a t i o n s)}\\\\ {\\mathfrak{L}_{z\\sim q(z)}[\\log\\frac{q(z)}{p(x|z)p(z)}]=\\underbrace{\\mathbb{E}_{z\\sim q(z)}[\\log\\frac{1}{p(x|z)}]}_{=\\ Q_{z\\sim q(z)}[\\log\\frac{1}{p(x|z)}]}+D_{K L}(q(z)||p(z))}&{(K L\\,D i\\nu e r g e n c e\\ d e f i n i t i o n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We arrive at equation $I I$ . Minimizing the error term (i.e., negative log-likelihood) is equivalent to minimizing the Sum of Squared Error (SSE) when a Gaussian form is assumed for the likelihood $p(\\pmb{x}|\\pmb{z})$ . ", "page_idx": 15}, {"type": "text", "text": "Derivation 2 Variational Free Energy derivation for a State Space Model (SSM) in equation 3. Latent states are denoted with $_{\\textit{z}}$ , whereas observations are denoted with $\\textbf{\\em x}$ . We assume non-linear transition and emission function (i.e., $f$ and $g_{.}$ ); therefore a variational approximation is needed to approximate the true posterior $p(z_{t}|\\mathbf{\\boldsymbol{z}}_{t-1},\\mathbf{\\boldsymbol{x}}_{t})$ with a surrogate posterior $q(\\boldsymbol{z}_{t})$ . As in derivation 1, the goal is to minimize the divergence between the true posterior and the approximate posterior (i.e., $D_{K L}(q(\\boldsymbol{z}_{t})||p(\\boldsymbol{z}_{t}|\\boldsymbol{z}_{t-1},\\boldsymbol{x}_{t})),$ ). Note that, for notation brevity, $\\mathbb{E}_{q}\\equiv\\mathbb{E}_{z\\sim q(z_{t})}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{K L}(q(z_{t})||p(z_{t}|z_{t-1},x_{t}))=\\mathbb{E}_{q}[\\log\\frac{q(z_{t})}{p(z_{t}|z_{t-1},x_{t})}]}\\\\ &{\\phantom{D_{K L}(q(z_{t})||p(z_{t}|z_{t-1},x_{t}))=}=\\mathbb{E}_{q}[\\log\\frac{q(z_{t})p(x_{t}|z_{t-1})}{p(x_{t}|z_{t},z_{t-1})p(z_{t}|z_{t-1})}]}\\\\ &{\\phantom{D_{K L}(q(z_{t})||p(z_{t}|z_{t-1},x_{t}))=}=\\mathbb{E}_{q}[\\log\\frac{q(z_{t})p(x_{t}|z_{t-1})}{p(x_{t}|z_{t},z_{t+\\tau})p(z_{t}|z_{t-1})}]}\\\\ &{D_{K L}(q(z_{t})||p(z_{t}|z_{t-1},x_{t}))=\\mathbb{E}_{q}[\\log\\frac{q(z_{t})}{p(x_{t}|z_{t})p(z_{t}|z_{t-1})}]_{+}\\mathbb{E}_{q}[\\log p(x_{t}|z_{t-1})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Conditional Independence) ", "page_idx": 15}, {"type": "text", "text": "We can minimize the Variational Free Energy term, which reduces to the log-likelihood of two prediction error terms and the negative entropy of the approximate posterior $q(\\boldsymbol{z}_{t})$ as shown below. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q}[\\log\\frac{q(z_{t})}{p(x_{t}|z_{t})p(z_{t}|z_{t-1})}]=\\mathbb{E}_{q}[\\log\\frac{1}{p(z_{t}|z_{t-1})}]+\\mathbb{E}_{q}[\\log\\frac{1}{p(x_{t}|z_{t})}]-\\mathbb{E}_{q}[\\log\\frac{1}{q(z_{t})}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Minimizing the Variational Free Energy above forces $q(\\boldsymbol{z}_{t})$ to better approximate the true posterior. ", "page_idx": 15}, {"type": "text", "text": "A.2 Gaussian Mixture Model and Hopfield Recall ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Derivation 3 We derive theorem $I$ which states that the maximization of the log-likelihood of $\\left.p(\\pmb{x}_{t}|\\pmb{z}_{t})\\right.$ in the form of a Gaussian Mixture Model is equivalent to the recall function in Hopfield networks (i.e., eqn 1), where the means of the GMM (i.e., $\\{\\pmb{\\mu}_{c}\\}_{c=1}^{C})$ represents the attractors of a Hopfield model. To maximize the log-likelihood, we compute its partial derivative with respect to $\\textbf{\\em x}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial x}\\log p(x|z)=\\frac{\\partial}{\\partial x}[\\log\\sum_{e=1}^{C}\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})]}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\cdot\\frac{\\partial}{\\partial x}\\frac{C}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{c})}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\cdot\\frac{C}{c-1}w_{c}\\frac{\\partial}{\\partial x}\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\cdot\\frac{C}{c-1}w_{c}\\cdot[-\\Sigma_{c}^{-1}(x-\\mu_{e})]\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(x;\\mu_{e},\\Sigma_{e})}\\cdot\\frac{\\sum_{c=1}^{C}(x-\\mu_{e})}{\\sum_{c=1}^{C}w_{c}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By setting the partial derivative of the log-likelihood to 0, we can estimate the value of x, which maximizes the function $\\log p(x|z)$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\ }&{\\displaystyle\\frac{\\partial}{\\partial x}\\log p(\\pmb{x}|\\varepsilon)=0}\\\\ &{\\underbrace{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})\\cdot-\\Sigma_{c}^{-1}(\\pmb{x}-\\mu_{c})}_{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})}=0}\\\\ &{\\ }&{\\underbrace{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})\\cdot\\pmb{\\Sigma}_{c}^{-1}\\pmb{x}}_{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})}=\\frac{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})\\cdot\\pmb{\\Sigma}_{c}^{-1}\\mu_{c}}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}(\\pmb{x};\\mu_{c},\\pmb{\\Sigma}_{c})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we can rearrange the equation in terms of x and show that it is equivalent to the Hopfield recall function where the recall value $\\textbf{\\em x}$ equals a weighted average of the attractors (i.e., means of GMM $\\{\\pmb{\\mu}_{c}\\}_{c=1}^{C})$ , with the weights being a similarity score function. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb x}=\\frac{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}({\\pmb x};\\mu_{c},\\Sigma_{c})\\cdot\\Sigma_{c}^{-1}{\\pmb\\mu}_{c}}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}({\\pmb x};\\mu_{c},\\Sigma_{c})\\cdot\\Sigma_{c}^{-1}}}}\\\\ {{\\displaystyle{\\pmb x}=\\sum_{c=1}^{C}\\frac{w_{c}\\cdot\\mathcal{N}({\\pmb x};\\mu_{c},\\Sigma_{c})\\cdot\\Sigma_{c}^{-1}}{\\sum_{c=1}^{C}w_{c}\\cdot\\mathcal{N}({\\pmb x};\\mu_{c},\\Sigma_{c})\\cdot\\Sigma_{c}^{-1}}\\cdot\\underbrace{\\mu_{c}}_{p r o j e c t i o n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Expected IoU of Random SDRs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 2 Consider two SDRs with sparsity defined as random variables $p\\sim\\mathcal{U}(0,1)$ and $q\\sim$ $\\mathcal{U}(0,1)$ , the expected Jaccard Index (i.e., IoU) of the two random SDRs follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{p q}{p+q-p q}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof: Given the sparsity random variables of both SDRs (i.e., $p$ and $q$ ) and the SDR size $n$ , the number of active bits at the same location in both SDRs is equal to the joint probability of both SDRs being active multiplied by the SDR size (i.e., npq). The union of both SDRs is the total number of active bits minus the active bits in both SDRs, which is equal to $n p+n q-n p q$ . Therefore, the expected intersection over union is $\\begin{array}{r}{\\frac{n p q}{n p+n q-n p q}=\\frac{p q}{p+q-p q}}\\end{array}$ ", "page_idx": 16}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/8024acf4897b1336f972bb9ae3d5c148fceb63e808e4753c4132004715d4b7d7.jpg", "img_caption": ["Figure 5: Empirical Validation of Theorem 2 "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Empirical Validation: We perform empirical validation of the above theorem as shown in figure 5. The sparsity of the first SDR is fixed at 0.1 and 0.5. We vary the sparsity of the second SDR between 0.0 and 1.0 in steps of 0.1 and calculate the average IoU over a population of 1000 pairs of SDR for every setting. Empirical results agree with the derived formulation in theorem 2. ", "page_idx": 17}, {"type": "text", "text": "B Notations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The notations used in our paper are summarized in Table 1. ", "page_idx": 17}, {"type": "text", "text": "C Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Synthetic For synthetic experiments, we generate SDRs with the specified size $N_{c}$ and uniformly initialized active bits $W$ to match the required sparsity $S$ . In many of the experiments, $N_{c}$ is set to 100 with 5 active bits unless otherwise specified. For Hopfield experiments, we set the sparsity to $50\\%$ to improve its performance. ", "page_idx": 17}, {"type": "text", "text": "Protein Sequences We use the dataset ProteinNet 7 [7] to extract protein sequences. Each sequence consists of a chain of Amino Acids. In the dataset, there are only 20 different types of Amino Acids (i.e., vocabulary), creating long protein sequences with hundreds of Amino Acids. The dataset is reported in the fasta format, where each Amino Acid is represented with a single-letter code. We create a dictionary mapping from the Amino Acid types to random SDRs with $N_{c}=100$ and $W=5$ to train the models. When choosing the sequences, we ensure that the starting Amino Acid is unique for all the dataset sequences to avoid ambiguous predictions in the continual learning evaluation. A sample of the protein sequence is provided below: ", "page_idx": 17}, {"type": "text", "text": "MGAAASIQTTVNTLSERISSKLEQEANASAQTKCDIEIGNFYIRQNHGCN LTVKNMCSADADAQLDAVLSAATETYSGLTPEQKAYVPAMFTAALNIQTS VNTVVRDFENYVKQTCNSSAVVDNKLKIQNVIIDECYGAPGSPTNLEFIN TGSSKGNCAIKALMQLTTKATTQIAPKQVAGTGVQFYMIVIGVIILAALF MYYAKRMLFTSTNDKIKLILANKENVHWTTYMDTFFRTSPMVIATTDMQN ", "page_idx": 17}, {"type": "text", "text": "Text To evaluate the generative ability of PAM, we use a dataset of the most frequently used English words. For preprocessing, we extract one hundred 4-letter words from the dataset and create a mapping dictionary from all the unique letters in the dataset to random SDRs with $N_{c}=100$ and ", "page_idx": 17}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/2258c108b61ef966efb601ec9ea67f2c8b9ffaf869df4e2237ad7c79e488cec1.jpg", "table_caption": ["Symbol Description "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "$W=5$ (except for AHN; $W=0.5N_{c})$ . The dataset contains many words with ambiguous future possibilities. The selected words are provided below: ", "page_idx": 18}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/ab3cb46c737f4fce0771a10f519c019f323c12aff5d0579e1e5344b52c7da580.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Vision In our experiment, we evaluate on sequences extracted from Moving MNIST [60], CLEVRER [68] as well as synthetically generated sequences of CIFAR [33] images. In order to convert images to SDRs and SDRs back to images while encoding semantics into the SDRs, we design an SDR AutoEncoder. The goal is to force the bottleneck representation of the autoencoder to become a sparse binary representation, where two visually similar images would result in two SDRs with a high overlap of active neurons. We simply design a CNN autoencoder with a 3-layer CNN encoder and 3-layer CNN decoder and apply top K binarization operation on the bottleneck embedding during training. The full architecture of the SDR autoencoder is shown in Figure 6. ", "page_idx": 18}, {"type": "text", "text": "In practice, we use a weighted average of the SDR and Dense representation to allow gradients of the reconstruction loss to freely propagate into the encoder. The weight of the SDR (i.e., $\\alpha$ ) is gradually and linearly increased (from 0.0 to 1.0) with the number of training epochs. This gradual increase is fundamental to the training of the SDR Autoencoder as it smooths the loss landscape and allows the model to distribute the active bits on the full SDR. The total mse loss becomes $\\mathcal{L}_{e n c}+\\mathcal{L}_{r e c o n}$ . We use Adam optimizer with a learning rate of $1\\!\\times\\!10^{-4}$ . For Moving MNIST, we use a bottleneck embedding (i.e., $N_{c,}$ ) of size 100 with 5 active bits, whereas, for more complex datasets (i.e., CLEVRER, CIFAR), we use an SDR of size 200 with 10 active bits. We show examples of the autoencoder reconstruction with full binary SDR (i.e., $\\alpha=1$ ) for all three datasets in Figure 7. ", "page_idx": 18}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/6f31186cd576701739c34fa97a1166488ba877dce59447e29d0a5a1a5d2a67e1.jpg", "img_caption": ["Figure 6: Architecture of the SDR Autoencoder "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/c78ee745bb89ac753963d3e870a9306906376e882cb0c982d881088de9b8a0f5.jpg", "img_caption": ["Figure 7: Examples of Autoencoder reconstructions from SDRs for all three datasets "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we describe the implementation details and hyperparameters of each method. For each model, we optimize a single set of hyperparameters for all the experiments. ", "page_idx": 20}, {"type": "text", "text": "PAM The neurons in both, transition and emission, functions are fully connected. We do not assume any of the weight matrices are symmetric. All synaptic weights are initialized by sampling from a normal distribution with a mean of 0.0 and a standard deviation of 0.1. All $\\eta^{\\dot{+}}$ values in Equations 7 & 8 are set to 0.1. $\\eta_{B}^{-}$ is set to $-0.1$ , while $\\eta_{A}^{-}$ is set to 0.0 to avoid forgetting previous possibilities when learning new transitions; PAM learns a union of possibilities. The threshold for the $\\delta$ function is set as a function of the SDR sparsity. For the transition function, we use a threshold of $0.8W$ , where $W$ is the active number of bits in the latent state $(z)$ SDR. For the emission function, we use a threshold of $0.1W$ , where $W$ is the active number of bits in the observation state $(x)$ SDR. During offline generation we sample an initial $\\tilde{\\pmb{x}}$ from $\\downarrow\\hat{z}$ with $W=1$ active neurons. During generation, we set the maximum number of attractor iterations to 100 but stop iterating when the energy of the state converges to a local minimum. During sequence learning, we update $\\pmb{A}$ and $_B$ iteratively until the transition is learned before learning the next transition. This iterative weight update makes the model insensitive to the hyperparameter values $\\eta$ . Both $\\pmb{A}$ and $_B$ are always clamped in the range $[-1,1]$ . The states $_{\\textit{z}}$ are flattened into a single dimension before applying the learning rule in Equation 7. Binary representations (i.e., $\\{0,1\\}$ ) are used as inputs. ", "page_idx": 20}, {"type": "text", "text": "Temporal Predictive Coding For the tPC architecture, we use learning rate of 1e-4 for 800 learning iterations. When a 2-layer tPC model is used, the inference learning rate is set to 1e-2 for 400 inference iterations. Also, the hidden size is set to twice the input size. We found that these parameters work best for all of the experiments and allow the model to fully converge. Bipolar representations (i.e., $\\left\\{-1,\\,1\\right\\}$ ) are used as inputs. ", "page_idx": 20}, {"type": "text", "text": "Asymmetric Hopfield Network The Hopfield model does not require hyperparameters other than the ablated separation function. In many experiments, we use a polynomial separation function with degree $d$ set to 1 or 2. Bipolar representations (i.e., {-1, 1}) are used as inputs. ", "page_idx": 20}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we describe the setup of each figure in the main paper and provide additional quantitative and qualitative results for each task. All experiments are run for 10 different seeds/trials. We report the mean and standard deviation in all the figures and tables. ", "page_idx": 20}, {"type": "text", "text": "E.1 Sequence Capacity ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figure 3 A This experiment plots the maximum offline sequence length (i.e., sequence capacity, $T_{m a x})$ at different input sizes. The input size $N_{c}$ is varied from 10 to 100 while the number of active bits $W$ is fixed to 5. We compare variants of our model with $N_{k}$ set to 4 and 8 to temporal predictive coding (tPC) and Asymmetric Hopfield Network (AHN). We observe that AHN completely fails as the sparsity $S$ of the pattern decreases. Therefore, we also compare to AHN with the sparsity set to $50\\%$ (i.e., $W=0.5N_{c})$ ). For AHN models, we experiment with a polynomial exponential function with degrees 1 and 2, as recently proposed [12] and used for evaluation in recent papers [62]. All models in this experiment are set to recall/generate in an offilne manner, where only the first input is provided. PAM outperforms all other methods and has the potential to improve further by expanding the context neurons $N_{k}$ . The patterns in this experiment are uncorrelated, such that each pattern has active bits that are uniformly initialized. ", "page_idx": 20}, {"type": "text", "text": "Figure 3 B This experiment plots the effect of sequence correlation on the maximum offline capacity. The higher the correlation value, the more exact repetitions of patterns are available in the sequence. We enforce correlation by limiting the number of unique patterns (i.e., vocab) used to create the sequence. All patterns in this experiment are set to a size of $N_{c}=100$ and $W=5$ (except for AHN, which is set at $W=0.5N_{c}$ ). Results show that the capacity of all other methods sharply drops when correlation is introduced. PAM retains most of its original capacity. ", "page_idx": 20}, {"type": "text", "text": "Figure $^3$ E & F In this experiment, we provide a qualitative example of a short sequence $T=10$ ) with high correlation (0.8). The sequence is learned by all the methods; then we perform offilne $\\mathbf{\\tau}(\\mathbf{E})$ and online (F) recall on the sequence. We use the SDR autoencoder to create SDRs from these CIFAR images for training and recall. The SDRs have a size $N_{c}$ of 200 and $W=10$ . In the offline recall, only the first input is provided and the model auto-regressively generates the full sequence using its own predictions at every time step. In online recall, the models perform a single-step prediction and always use the ground truth input at every time step to perform predictions. Results show that only PAM can retain a context of correlated sequence and accurately predict into the future based on this context. ", "page_idx": 21}, {"type": "text", "text": "Figure 8 A In this experiment, we show the effect of scaling the model context memory beyond a simple $N_{k}=4$ and $N_{k}=8$ . We show that when using $N_{k}=16$ and $N_{k}=24$ , PAM can model much longer sequences. We vary the input size $N_{c}$ from 10 to 50 and report the offline sequence capacity of the model as ablations. ", "page_idx": 21}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/e9358539536ebabbb0f80d8599ea33ad6ca8e5b37335e563f345052f8e3f0f22.jpg", "img_caption": ["Figure 8: Additional sequence capacity experiments. A: scaling of the offilne sequence capacity with context memory size $N_{k}$ and input size $N_{c}$ . B: Online sequence capacity of various methods. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure ${\\bf8}$ B Similar to the experiment plotted in Figure $3\\textbf{A}$ , we report the sequence capacity with input size $N_{c}$ . However, this experiment evaluates the online generation capacity, where the model uses the correct pattern at every prediction time step instead of using its own prediction from the previous time step. Results show that PAM significantly increased in capacity (three times in some cases), whereas the other methods have not increased as much in modeling longer sequences. ", "page_idx": 21}, {"type": "text", "text": "Figure 9 We provide additional qualitative example on a different highly correlated sequence. The result shows a different failure mode for AHN, whereas PAM still performs well. ", "page_idx": 21}, {"type": "text", "text": "E.2 Catastrophic Forgetting ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Figure 4 A We benchmark the performance of difference models in the challenging continual learning setup. The models are expected to avoid catastrophic forgetting by not overwriting previously learned sequences. In this experiment, we use 50 sequences, each with size $N_{c}=100$ and length $T=10$ . We vary the correlation of the sequences from 0.0 to 0.5 and compute the backward transfer metric with the normalized IoU as the measure of similarity. Results show that AHN can avoid catastrophic forgetting when the sequence is uncorrelated but quickly drops in performance with correlation. tPC fails to retain learned sequences regardless of correlation. PAM performs well with more context neurons $N_{k}$ . When setting $N_{k}$ to 1, the model fails to retain its knowledge due to the decreased context modeling capability with a single context neuron. ", "page_idx": 21}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/0068dc64346e1f26e9aa801da9535024a8053cb56cd337d4a5efa33b46dfb94e.jpg", "img_caption": ["Figure 9: Additional qualitative example of correlated sequential memory with CIFAR images. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4 B In this experiment, we report the performance of the models on protein sequences. This is a more challenging setup due to the long sequence (a few hundred on average) with high correlation (only 20 unique Amino Acids). We show a similar trend where the other methods fail due to high correlation or sequence lengths. PAM outperforms the other methods when using context memory $N_{k}$ of 16 or 24. All Amino Acid types are converted to fixed and randomly initialized SDRs with $N_{c}=100$ . The sparsity is set similar to sequence capacity experiments (i.e., $W=5$ and $W=0.5N_{c})$ ). ", "page_idx": 22}, {"type": "text", "text": "Figure 4 F We provide qualitative results on a simple experiment with 2 sequences from moving MNIST. The models learn the first sequence and then learn the second sequence. The models are not allowed to train on the first sequence after they have trained on the second sequence. We then perform online generation on the first sequence with all models. We use the SDR autoencoder to generate SDRs for all images in the sequences; the SDRs have $N_{c}=100$ with $W=5$ (for all methods except AHN). The results show that PAM can recall the full sequence even after being trained on another sequence. Other methods fail in this simple task, even in online recall setup. ", "page_idx": 22}, {"type": "text", "text": "Figure $10$ We provide continual learning results similar to Figures 4 A & B; however, instead of offline generation, we perform the evaluation in online manner. ", "page_idx": 22}, {"type": "text", "text": "Figure 11 We provide additional qualitative results on Moving MNIST, as well as quantitative results averaged over 10 trials of MNIST sequence pairs. These quantitative results are reported as the Mean Squared Error of the reconstructed image. Results show that PAM reports the lowest error with a much smaller variance. ", "page_idx": 22}, {"type": "text", "text": "Tables 2, 3, 4, & 5 The Backward Transfer metric (BWT) to evaluate catastrophic forgetting is computed by taking the average of the performance on previously learned sequences after training on a new sequence. In addition to plotting this average in previous experiments, we provide the full tables for one of the experiments as an example. All tables show results on 10 sequences and $N_{c}=100$ . The BWT metric is calculated as the average of the similarity metric reported in these tables. ", "page_idx": 22}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/820043273071ec43323e184782f26a51d7490698ca1c0e571bd1c6fc76b165d0.jpg", "img_caption": ["Figure 10: Catastrophic forgetting experiments in an online manner. Results are shown on a synthetic dataset of SDR sequences with different correlations and on protein sequences. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/21d03c88bad5c017ad74d0f4ec6ea3978090029aaec7912eacda0b6688f48c8d.jpg", "img_caption": ["Figure 11: Additional qualitative catastrophic forgetting visualization on Moving MNIST and quantitative results on the reconstruction error of 10 Moving MNIST random examples "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 2: Catastophic forgetting experiment results on 10 sequences for PAM with $N_{k}=1$ . The table shows the mean normalized IoU and standard deviation of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the shown numbers. Results are averaged over 10 trials. ", "page_idx": 23}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/7bc3e484134e0fc3068846811a23902cf8806d5b2a92cc5aa719b0b43b7034cd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 3: Catastophic forgetting experiment results on 10 sequences for PAM with $N_{k}=4$ . The table shows the mean normalized IoU and standard deviation of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the shown numbers. Results are averaged over 10 trials. ", "page_idx": 24}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/113c4abe50087ab0374b390a58c22d96ac1fc877e92d1d323e172dacd6f6ed30.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 4: Catastophic forgetting experiment results on 10 sequences for tPC. The table shows the mean normalized IoU and standard deviation of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the shown numbers. Results are averaged over 10 trials. ", "page_idx": 24}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/7927195cbdaf33a20d87fa3fc4cfc502879e7b37c4ca5771f3f98cdbb8d1431e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 5: Catastophic forgetting experiment results on 10 sequences for AHN with $d\\,=\\,2$ and $W=0.5N_{c}$ . The table shows the mean normalized IoU and standard deviation of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the shown numbers. Results are averaged over 10 trials. ", "page_idx": 24}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/3fc8aac882bfb8b154d8bd706686c6e18464d5bbabc9da6b400b319f3c4439ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.3 Multiple Possibilities Generation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this task, we evaluate the models\u2019 ability to generate meaningful sequences and recall the full dataset despite being presented with multiple valid possibilities. Ideally, the models are expected to sample a single possibility if trained on sequences with ambiguous future continuations (equally valid possibilities). This is a challenging task for most biologically plausible (e.g., tPC, AHN, etc.) and implausible (e.g., transformers, RNNs, etc) models. Most approaches assume the existence of a full set of possibilities and transform the task from regression to classification (e.g., LLM). For vision tasks, some methods (VQ-VAE and its variants) cluster the dense representations to create this set of possibilities and perform classification. We do not assume the existence of a full set of possibilities but instead perform a true generative evaluation as a regression task. ", "page_idx": 24}, {"type": "text", "text": "Figure 4 C In this experiment, we compute the average normalized IoU of the generated words. The model\u2019s ability to generate a full sequence with high IoU means it can produce sharp single predictions despite being trained on multiple equally valid future predictions. As the number of words increases, the performance of other models decreases as they struggle to model ambiguous future predictions; however, PAM outperforms the other approaches by sampling from these possibilities. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Figure 4 D This experiment evaluates the ability of the models to recall the dataset words. We compute the recall as the number of valid unique words generated divided by the total number of words in the dataset. Since PAM is a generative stochastic model, the recall increases with every generation. The other methods are deterministic and, therefore, do not report an increase in dataset recall with more generations. The other methods completely fail to generate any meaningful words. We use an average IoU threshold of 0.9 to classify a generated word as correct, similar to sequence capacity experiments. ", "page_idx": 25}, {"type": "text", "text": "Figure 12 We provide qualitative results by showing the unique generated words by different models after 5 dataset generations. PAM $N_{k}\\,=\\,4$ generates some of the dataset words but also generates many wrong words. By increasing the context memory neurons to $N_{k}=8$ , the model generates many more correct words and reduces the false positives. The other methods cannot generate meaningful words. ", "page_idx": 25}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/507c3f45832008c9a5eba0c7cb14b6757c14ad2b012b48e2c92313d8585a5b3d.jpg", "img_caption": ["Figure 12: Qualitative results showing the generated words from PAM, tPC [62] and AHN [12] Words highlighted in green are available in the dataset (i.e., True positives). Words highlighted in red are not available in the dataset (i.e., False positives). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E.4 Noise Robustness ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Figure $^3$ C To evaluate for noise robustness, we plot each model\u2019s performance (Normalized IoU) with varying levels of noise added in an online generation setting. The noisy inputs are created by changing a percentage of the active neurons to different uniformly chosen neurons in the SDR. The noise is computed as a percentage for a fair comparison across different SDR sparsities (e.g., tPC vs. AHN). We show the results for sequences of lengths $T\\,=\\,200$ and no correlation. PAM has the ability to compare the noisy input representation to the learned attractors to recover the correct clean input. Therefore, even when the SDR is completely changed, PAM relies on its predictions and completely ignores the noisy input. During generation, PAM always generates (or corrects a noisy input) from within the predicted set of possibilities. The other approaches use the noisy inputs during recall, which affects their performance. ", "page_idx": 25}, {"type": "text", "text": "Figure 4 E We provide qualitative results on the CLEVRER dataset. The memories sequence is learned by all the models, then a noisy sequence is used during generation. We only add noise starting from the second pattern in the input sequence. The results show that tPC models are performing relatively well, yet they are still outperformed by PAM $N_{k}\\,=\\,8$ . We set $N_{c}\\,=\\,200$ in the SDR autoencoder to learn the SDRs used in this experiment. We use $40\\%$ noise in this experiment. ", "page_idx": 25}, {"type": "text", "text": "Figure 13 We perform additional experiments on varying the sequence lengths and the correlation in the sequence, all the other settings remain the same as in the experiment of Figure $3\\;\\mathbf{C}$ . The results show that with shorter sequences $(\\le200)$ , no noise, and no correlation, all the models recall the learned sequence well. When higher correlation is used, 2-layered tPC performs relatively well with short sequences (i.e., $T=10$ ) but fails with longer sequences (i.e., $T=100$ ). The Hopfield model fails more with correlation than sequence length. The added noise affects all reported methods except for PAM due to its ability to rely on its predictions and attractors to clean the noisy signal. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Figure 14 We provide an additional qualitative example with similar trend to Figure $4\\,\\mathbf{E}$ . We also provide quantitative results of CLEVRER averaged over 10 experiments. The mean squared error of the generated sequence for multiple models at different noise levels is reported. It is clear that PAM outperforms all methods, and a 2-layered tPC is the second best. ", "page_idx": 26}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/78dbb88462d371886ab295dcfa94326600b2fb676c3f837f9f0639d8fcea1c78.jpg", "img_caption": ["Figure 13: The effect of noise on online generation with varying sequence lengths and sequence correlations. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/afdbd230f5dfa07deec96501070ebb848eb8584571c7cdbacdac398aca412131.jpg", "img_caption": ["Figure 14: Additional qualitative example of online generation with noise on CLEVRER dataset, and quantitative results of reconstruction error over 10 CLEVRER examples at different noise levels. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.5 Efficiency ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Figure $^3$ D We compare the efficiency of the models and show that PAM is at least two orders of magnitude more efficient than tPC with 2 layers. A single layer tPC is almost equivalent to PAM with high context memory of $N_{k}=24$ . AHN is highly efficient as the model is not usually trained, but the recall equation is used instead. Therefore, we exclude AHN from the comparison. ", "page_idx": 27}, {"type": "text", "text": "E.6 Additional Results ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/28beebbb2b3284ab175cc4e7d18da7a4d2d3a0fbe15e64ded8494e5f9fcc413b.jpg", "img_caption": ["Figure 15: (A) Performance of online and offilne PAM $N_{c}=50$ , $N_{k}=4$ ) generation with sequence length. $({\\bf{B}})$ Performance of PAM $N_{c}=50$ , $N_{k}=4$ ) generation for different connection densities. (C) Backward transfer performance for PAM and 2-layer transformer on the protein sequence task. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 15 A We show the performance of PAM in an online and offline generation task as the sequence length increases. In offilne generation, incorrect predictions accumulate quickly, leading to a sharp drop in performance. However, online generation attempts to correct inaccurate predictions; therefore, performance smoothly degrades with sequence length. Note that both experiments (i.e., online and offline) use identical seeds/sequences for a fair comparison. ", "page_idx": 27}, {"type": "text", "text": "Figure 15 B We provide additional results on testing the effect of connection density. While neurons in PAM are currently densely connected (similar to Hopfield Network), we show that PAM can still perform relatively well in a sparser setup. ", "page_idx": 27}, {"type": "text", "text": "Figure $15$ C We provide additional results comparing PAM to a 2-layer transformer on continually learning protein sequences. Backward transfer results show low performance of transformers in remembering older sequences after training on new sequences. Table 6 supplements these results by showing detailed sequence-by-sequence performance values. Transformers excel at learning the current task (diagonal values) but suffer in remembering previously learned tasks (lower triangle values). ", "page_idx": 27}, {"type": "text", "text": "Table 6: Catastophic forgetting experiment results on 10 protein sequences for a 2-layer Transformer. The table shows the mean normalized IoU of previous learned sequences after training on new sequences. The Backward Transfer metric is the average of all the numbers in the lower triangle. ", "page_idx": 27}, {"type": "table", "img_path": "lxhoVDf1Sw/tmp/e0841a203c92619546a5f23d669accb992a017e369bb17a57d6ed54581e3b94c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Discussions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Biological Plausibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "PAM inherits assumptions from Hierarchical Temporal Memory (HTM) [25] that the sequence modeling takes place in Layer IV of the cortical column, where it receives both thalamocortical sensory driving input and contextual information. Sensory input arrives on the proximal dendrites of pyramidal cells, while contextual information reaches the basal dendrites, priming specific contexts by depolarizing neurons. Depolarized neurons then fire first and inhibit other neurons within the same minicolumn via lateral inhibition, a process assumed to be mediated by interneurons (possibly Double Bouquet Cells (DBCs)) [49]. HTM assumes interneurons [40] perform lateral inhibition during prediction but does not learn the synapses of the interneurons. Interneurons are also assumed to perform the inhibitory tasks of the attractor model [14]. ", "page_idx": 28}, {"type": "text", "text": "Biological plausibility in PAM mainly refers to excluding the obvious biologically implausible learning rules, such as backpropagation or copying of weights. This allows models like PC and HN to be referred to as biologically plausible despite clear contradictions with anatomy [54, 14]. Moreover, PAM also constrains its learning and representations to sparse and binary cell assemblies, which improves the plausibility of the model. Therefore, a biologically plausible model here refers to the ability to continually learn using only local learning rules (e.g., Hebbian, anti-Hebbian, STDP) and sparse binary representations. However, we do not intend to accurately map every detail in PAM to the anatomy of the neocortex. ", "page_idx": 28}, {"type": "text", "text": "F.2 Future Directions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In future work, we plan to implement a hierarchy of PAM blocks that supports compositional, partwhole predictions. Unlike simply stacking layers as in Transformer or CNN architectures, our aim is to establish a hierarchy where higher-level blocks make predictions based on broader, contextual patterns. For instance, in vision, higher levels could predict entire objects based on multiple visual cues, while in NLP, they could predict phrases or sentences rather than individual tokens. To achieve this, each PAM level would send a summary representation of the current sequence (its latent state) to the next level for recursive, higher-order processing. ", "page_idx": 28}, {"type": "text", "text": "Additionally, we plan to explore top-down feedback mechanisms between PAM blocks - inspired by the hierarchical feedback observed in the neocortex. In the visual system, for example, top-down signals travel from higher regions (e.g., V2) to lower ones (e.g., V1), reaching the apical dendrites of pyramidal neurons. These connections originate in layer VI(a) of the higher region and form en-passant synapses in layer VI(a) of the lower region in the hierarchy (e.g., V1). These connections reach Layer I and provide feedback on the apical dendrites of other layers [61]. In PAM, a similar feedback pathway could enable higher-level blocks to modulate lower-level processing based on partial observations, refining predictions by ruling out certain lower-level possibilities that conflict with higher-level context. For example, the attractor in the higher-level block can reach a decision on the object being seen (e.g., human) based on partial observation (V1: layer $\\mathrm{III}\\rightarrow\\mathrm{V}2$ : layer IV), and therefore, the feedback inhibitory connections on the apical dendrites can rule out some lower-level possibilities (e.g., paws). ", "page_idx": 28}, {"type": "text", "text": "G Sparse Distributed Representations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The neocortex stores and represents information using sparse activity patterns, as demonstrated by empirical evidence [4]. Inspired by HTM [25] and neuroscience-based theories of cortical function, we use Sparse Distributed Representations (SDRs) as the main representation format of PAM. An SDR is a sparse binary representation of a cell assembly where only a small fraction of the neurons in the SDR are active at any time. The location of these active neurons encodes the information that is represented by this SDR. In this section, we describe some useful properties of SDRs and discuss their robustness to noise as opposed to dense representations. ", "page_idx": 28}, {"type": "text", "text": "G.1 SDR Properties ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "SDRs are used to represent rich sensory information in the neocortex as a sparse activity pattern. Therefore, from a mathematical viewpoint, an SDR must have the ability to represent many patterns and easily distinguish between them. The capacity of an SDR can be calculated as the possible combinations of locations where neurons can be active. Consider an SDR with size $N$ and number of active neurons $W$ . The total capacity of this SDR is computed as shown in Equation 12. ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\binom{N}{W}}=\\frac{N!}{W!(N-W)!}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Based on the above Binomial coefficient equation, it may seem that sparsity is not optimal for capacity as the capacity will be the highest when $W$ is exactly half of $N$ . While capacity is important, we aim to represent multiple possibilities as a union of SDRs and, therefore, minimize the overlap between them. From an information-theoretic viewpoint, the goal is to minimize mutual information between SDRs to ensure that each SDR carries unique information and the union represents a more comprehensive and diverse set of features. We can minimize the expected IoU by using lower sparsities as shown in Theorem 2. In our experiments, we use $N=100$ and $W=5$ , which results in a capacity of $\\approx75\\times10^{6}$ and an expected IoU of $\\approx0.02$ . However, when scaled up to more typical values of SDR sizes and sparsities in the neocortex [25, 4] (i.e., $N=2048$ , $W=40)$ ), we get capacity of $\\approx2.37\\times10^{84}$ (more than the estimated number of atoms in the observable universe $\\approx10^{80}$ ) and expected IoU of $\\approx0.01$ . A sparsity of 0.5 maximizes the mutual information and results in an expected IoU of 0.33, which cannot be used to represent multiple possibilities as a union of SDRs in spite of the optimal capacity. In practice, the size $N$ of the SDR is increased to increase the capacity, and the sparsity $W/N\\bar{}$ is decreased to minimize the expected overlap. ", "page_idx": 29}, {"type": "text", "text": "G.2 The robustness of SDRs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Sparse representations naturally minimize the overlap between random SDRs; therefore, they are very tolerant to noise. To visualize this robustness property of SDRs, we design an experiment (Figure 16) where we train an SDR autoencoder with different sparsities and then decode SDRs at various levels of noise added. When the sparsity is increased to $\\bar{5}0\\%$ , there is a high chance of overlap between SDRs. Therefore, a small amount of noise can cause collisions between SDRs. However, a $5\\%$ sparsity can tolerate much more noise without overlapping with other SDRs. ", "page_idx": 29}, {"type": "image", "img_path": "lxhoVDf1Sw/tmp/443f6e6ac012f31c2c391b5a1611b0781647cfee88b75aaca0a86aee0f303677.jpg", "img_caption": ["Figure 16: Three examples of decoding an SDR with different noise levels. The results are shown for SDRs with different sparsities trained in an SDR autoencoder on the CLEVRER dataset. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": ". Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our claims are supported by experimental results and theoretical proofs where applicable. Derivations are provided in the supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, the limitations section has been moved from the supplemental material to the main paper for the camera ready version. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide proofs and derivations in the appendix, where applicable. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide general implementation details for all the methods used. Additionally, we describe the experimental setup for every plot/table in the main paper and in the appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our codebase (now released) includes a script for every single experiment to accurately reproduce the plots in the paper. We fix the seeds for exact reproducibility. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 32}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide implementation details for all methods and the datasets\u2019 details in the appendix. The code also contains a separate configuration file with all the needed parameters. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All experiments are repeated 10 times at different seeds. The means and standard deviations are provided in all plots and tables. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We specify the compute resources required for PAM (i.e., CPU) and plot a comparison of the time required by each method at different parameters in Figure $\\mathbf{\\nabla}3\\textbf{D}$ . Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper conforms with NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: PAM is a powerful sequence modeling algorithm. We do not see direct positive or negative societal impacts. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Authors of comparison methods and datasets are properly cited. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]