[{"type": "text", "text": "Rethinking Open-set Noise in Learning with Noisy Labels ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 To reduce reliance on labelled data, learning with noisy labels (LNL) has gained   \n2 increasing attention. However, prevailing works typically assume that such datasets   \n3 are primarily affected by closed-set noise (where the true/clean labels of noisy   \n4 samples come from another known category), and ignore therefore the ubiqui  \n5 tous presence of open-set noise (where the true/clean labels of noisy samples   \n6 may not belong to any known category). In this paper, we formally refine the   \n7 LNL problem setting considering the presence of open-set noise. We theoret  \n8 ically analyze and compare the effects of open-set noise and closed-set noise,   \n9 as well as the effects between different open-set noise modes. We also analyze   \n10 common open-set noise detection mechanisms based on prediction entropy values.   \n11 To empirically validate the theoretical results, we construct two open-set noisy   \n12 datasets - CIFAR100-O/ImageNet-O and introduce a novel open-set test set for   \n13 the widely used WebVision benchmark. Our work suggests that open-set noise   \n14 exhibits qualitatively and quantitatively distinct characteristics, and how to fairly   \n15 and comprehensively evaluate models in this condition requires more exploration. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 In recent years, the tremendous success of machine learning often relies on the assumption that data   \n18 labels are accurate and free from noise. However, in real-world scenarios, label noise caused by   \n19 factors such as annotation errors and label ambiguity is ubiquitous, posing a pervasive challenge to   \n20 the performance and generalization of models. To address this challenge, various methods have been   \n21 proposed to learn with noisy labels, including noise transition matrix [7, 23], label correction [17, 3],   \n22 robust loss functions [6, 29, 19], and recently dominant sample selection-based approaches [11, 2].   \n23 Most current efforts, however, primarily focus on closed-set noise, where the true labels of noisy   \n24 samples belong to another known class. This includes common noise models like symmetric noise   \n25 (assuming that the labels of samples are randomly flipped with a certain probability to any other   \n26 known classes) or asymmetric noise model (assuming that the probability of label confusion is   \n27 influenced by the classes, such as \u2019cat\u2019 being more likely to be confused with \u2019dog\u2019 than with   \n28 \u2019airplane\u2019). Recent advancements have also explored instance-dependent noise models [4, 26], where   \n29 label confusion depends directly on individual instances.   \n30 Unfortunately, unlike the in-depth exploration of closed-set noise, there is noticeably limited research   \n31 on open-set noise, where the true labels of noisy samples may not belong to any known category.   \n32 This gap becomes particularly crucial when considering one of the primary motivations for learning   \n33 with noisy labels: learning with datasets obtained through web crawling. Examining one of the most   \n34 commonly used benchmarks - the WebVision dataset [12], we validate the prevalence of open-set   \n35 noise (fig. 1). In fact, the \u2018open-world\u2019 assumption involving open-set samples has received more   \n36 attention in other weakly supervised learning problems, such as open-set recognition and outlier   \n37 detection, but lacks enough exploration in the context of LNL. To this end, we focus on a thorough   \n38 theoretical analysis of open-set noise in this paper. Specifically:   \n39 \u2022 Considering the presence of open-set noise, we introduce the concept of a complete noise   \n40 transition matrix and reformulate the LNL problem and label noise definition in this context.   \n41 \u2022 To enable offline analysis, we consider two pragmatic cases: fitted case, that the model   \n42 perfectly ftis the noisy distribution, and memorized case, that the model completely memorises   \n43 the noisy labels.   \n44 \u2022 We analyze and compare the open-set noise vs. closed-set noise on closed-set classification   \n45 accuracy and suggest that open-set noise has a less negative impact in both cases. We also   \n46 analyze and compare the \u2018hard\u2019 open-set noise vs. \u2018easy\u2019 open-set noise, but find that these   \n47 two different noise modes show opposite trends in two different cases.   \n48 \u2022 Since closed-set classification evaluation may be insufficient to fully reflect model perfor  \n49 mance, we consider introducing an additional open-set detection task and conduct preliminary   \n50 experiments.   \n51 \u2022 We derive and analyze the open-set noise detection mechanism based on the entropy values   \n52 of model predictions and suggest that it may be effective only for \u2018easy\u2019 open-set noise. We   \n53 also consider two representative LNL methods and combine them with such open-set noise   \n54 detection mechanism for further experiments.   \n55 \u2022 For controlled experiments, we construct two novel synthetic open-set noise datasets:   \n56 CIFAR100-O and ImageNet-O. Additionally, we introduce a new open-set test set to the   \n57 WebVision dataset for the open-set detection task. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/1ac8833d3185680917c1e119fd56c6a8f584404b808fa4a1079ec0016416e9c8.jpg", "img_caption": ["Figure 1: Example images of class \u201cTench\" from WebVision dataset. Clean samples are marked in extcolorgreenGreen, closed-set noise is marked in Blue and open-set noise is marked in Red. See appendix F for more details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 Methods for learning with noisy labels can be roughly categorized into two main directions. The first   \n60 direction typically focuses on estimating noise transition matrix [4, 26, 23, 7] or designing robust   \n61 loss functions [29, 19, 6], aiming to achieve theoretically risk-consistent or probabilistic-consistent   \n62 models. However, most of these works often assume an ideal scenario where the model can learn to   \n63 fit the sampled distribution well, overlooking the over-fitting issues arising from excessive model   \n64 capacity and insufficient data in practical situations. In this paper, we introduce the concept of   \n65 complete noise transition matrix considering the presence of open-set noise and conduct theoretical   \n66 analyses and experimental validations for both ideal case and over-fitting case, namely fitted case   \n67 and memorized case. The second type is often based on sample selection strategies, involving also   \n68 different regularization terms and off-the-shelf techniques such as semi-supervised learning and   \n69 model co-training, to achieve the state-of-the-art performance. Most sample selection methods are   \n70 based on the model\u2019s current predictions, such as the popular \u2018small loss\u2019 mechanism [2, 11, 8, 28,   \n71 10, 17, 13, 27, 24, 30], or model\u2019s feature space [21, 22, 15, 5].   \n72 Especially, the investigation on open-set noise is relatively scarce. Wang et al. [18] utilize Local   \n73 Outlier Factor algorithm to identify open-set noise in feature space, Wu et al. [22] propose to identify   \n74 open-set noise with subgraph connectivity, while both Sachdeva et al. [16] and Albert et al. [1] try to   \n75 identify open-set noise based on entropy-related dynamics. Instead, Feng et al. [5] do not identify   \n76 open-set noise explicitly while avoid relabelling and including open-set noise in the training. More   \n77 closely related to our work, Xia et al. [25] also investigates noise transition matrices involving open  \n78 set noise but considering all open-set noise belonging to a single meta-class. In this paper, we consider   \n79 that open-set noise may originate from different classes, and based on this premise, we analyze two   \n80 distinct open-set noise modes. Wei et al. [20] propose leveraging open-set noise to mitigate the   \n81 impact of closed-set noise, as it helps alleviating the model\u2019s over-ftiting tendency. Instead, we focus   \n82 on a thorough theoretical analysis of the effects with different noise modes, including open-set noise   \n83 versus closed-set noise, and different open-set noise versus each other. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "84 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 In section 3.1, we briefly introduce the traditional problem formulation of LNL. In section 3.2, we   \n86 reformulate the LNL problem considering open-set noise. In section 3.3, we formalize how label   \n87 noise influences model generalization, particularly, on the proposed error rate inflation metric. In   \n88 section 3.4, we analyze and compare the impact of open-set vs. closed-set noise, as well as \u2018easy   \n89 open-set noise vs. \u2018hard\u2019 open-set noise. In section 3.5, we scrutinize the open-set noise detection   \n90 mechanism based on model prediction entropy values. ", "page_idx": 2}, {"type": "text", "text": "91 3.1 Traditional formulation of LNL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "9923 aSnudp eirdveinsteidc aclllays sdiifsitcraitbioutne lde atrraniinnign tgy spiacmalpllye sa $\\{{\\pmb x}_{k},y_{k}\\}_{k=1}^{K}$ ef rsoamm pal jeo ai ncte rdtiasitrni bnuutmiobne $P(\\mathbf{x},\\mathbf{y};\\mathbf{\\dot{y}}\\in\\mathcal{y}^{i n})$ ,   \n94 i.e., the so-called train set. By default, here all the possible values for $y_{k}$ in the discrete label space   \n95 $\\mathcal{V}^{i n}:\\{1,2,...,A\\}$ (referred here as inlier classes), are known in advance. With a certain loss function,   \n96 given the train set $\\{\\pmb{x}_{k},y_{k}\\}_{k=1}^{K}$ we aim to train a model $f:x\\rightarrow y$ whose predictions can achieve the   \n97 minimum error rate under the whole clean distribution $P(\\mathbf{x},\\mathbf{y};\\mathbf{y}\\in\\mathcal{y}^{i n})$ .   \n98 Under LNL problem setting, we believe that the joint distribution $P(\\mathbf{x},\\mathbf{y};\\mathbf{y}\\in\\mathcal{y}^{i n})$ has been perturbed   \n99 to $P^{n}(\\mathbf{x},\\mathbf{y};\\mathbf{\\dot{y}}\\in\\mathcal{y}^{i n})$ ; especially, the conditional distribution $P^{n}(\\mathbf{y}|\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{i n})$ changes \u2014 normally   \n100 we assume the sampling prior is free of the label noise $(P(\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n})=P^{n}(\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n}))$ , leading   \n101 to the presence of noisy labels $y_{k}^{n}$ in the noisy train set $\\{{\\pmb x}_{k},y_{k}^{n}\\}_{k=1}^{K}$ that do not conform to the clean   \n102 conditional distribution $P(\\mathbf{y}|\\mathbf{x};\\overset{\\cdot}{\\mathbf{y}}\\in\\mathcal{Y}^{i n})$ ). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "103 3.2 Revisiting LNL considering open-set noise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 We here formally revisit the problem formulation of learning with noisy labels considering the   \n105 existence of open-set noise. Instead of assuming all the possible classes are known $(\\mathrm{y}\\in\\mathcal{V}^{i n\\bar{\\star}})$ , we   \n106 consider samples from some unknown outlier classes may also exist in the train set. Let us denote   \n107 these classes as outlier classes $\\mathcal{V}^{o u t}:\\{A+1,A+2,...,A+B\\}$ with $B$ as the number of possible   \n108 outlier classes. Then, we expand the support of joint distribution to contain both inlier and outlier   \n109 classes, denoted as $P(\\mathbf{x},\\mathbf{y};\\bar{\\mathbf{y}}\\in\\mathcal{Y}^{i n}\\cup\\mathcal{\\dot{Y}}^{o u t})$ and $P^{n}(\\mathbf{x},\\mathbf{y};\\mathbf{y}\\in\\mathcal{Y}^{i n}\\cup\\mathcal{Y}^{o u t})$ for the clean and noisy   \n110 ones, respectively. For brevity, we denote as $\\mathcal{V}^{a l l}\\triangleq\\mathcal{V}^{i n}\\cup\\mathcal{V}^{o u t}$ . Similarly as above, we still assume   \n111 the noisy labelling will not affect the sampling prior $(P(\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{a l l})\\stackrel{\\cdot}{=}P^{n}(\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{a l l}))$ . For   \n112 subsequent analysis, we first define below complete noise transition matrix:   \n113 Definition 3.1 (Complete noise transition matrix). For a specific sample $\\textbf{\\em x}$ , we define as $T$ (sample   \n114 index omitted here for simplicity) the complete noise transition matrix1: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nT=\\left[\\frac{T_{\\scriptscriptstyle\\mathrm{A}\\times\\mathtt{A}}^{i n}}{T_{\\scriptscriptstyle\\mathrm{B}\\times\\mathtt{A}}^{o u t}}\\left|\\begin{array}{l}{\\mathbf{0}_{\\scriptscriptstyle\\mathrm{A}\\times\\mathtt{B}}}\\\\ {\\mathbf{0}_{\\scriptscriptstyle\\mathrm{B}\\times\\mathtt{B}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "115 $T^{i n}$ corresponds to the confusion process between inlier classes $\\mathcal{V}^{i n}\\;:\\;\\{1,2,...,A\\}$ , and $T^{o u t}$   \n116 corresponds to the confusion process from outlier classes $\\mathcal{V}^{o u t}:\\{A+1,A+2,...,A+B\\}$ to inlier   \n117 classes $\\mathcal{V}^{i n}:\\{1,2,...,A\\}$ .   \n118 For brevity, we denote as $T_{i j}\\ \\triangleq\\ P(\\mathbf{y}^{n}\\ =\\ j|\\mathbf{y}\\ =\\ i,\\mathbf{x}\\ =\\ x;\\mathbf{y}^{n},\\mathbf{y}\\ \\in\\ \\mathcal{Y}^{a l l})$ . We have further   \n119 jA=+1B Tij = 1 for i \u2208{1, ..., A + B} - noise transition from each clean class sums to 1 over all   \n120 possible noisy classes. With such a complete noise transition matrix $T$ , we can connect the clean   \n121 conditional distribution $P(\\mathbf{y}|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{a l l})$ with the noisy conditional distribution $P^{n}(\\mathbf{y}|\\mathbf{x}=$   \n122 $\\mathbf{\\Delta}x;\\mathbf{y}\\in\\mathcal{V}^{a l l}\\,,$ ) as below: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nP^{n}(\\mathbf{y}=j|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=\\sum_{l=1}^{A+B}P(\\mathbf{y}=l|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{a l l})\\cdot T_{l j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "123 Label noise Recent works usually discriminate label noise into closed-set noise and open-set noise.   \n124 Before continuing with the further discussion, we feel it is necessary to elucidate these two concepts   \n125 here clearly to avoid any ambiguities, as we will try to comparably discriminate and analyze them   \n126 later. Specifically, most of recent works define open-set noise as \u2018a sample with its true label from   \n127 unknown classes but mislabelled with a known label\u2019. Formally, we have: ", "page_idx": 3}, {"type": "text", "text": "128 Definition 3.2 (Label noise). For sample $\\textbf{\\em x}$ with clean label $y$ and noisy label $y^{n}$ : ", "page_idx": 3}, {"type": "text", "text": "129 \u2022 When $y=y^{n}$ , $(\\pmb{x},y,y^{n})$ is a clean sample;   \n130 \u2022 When $y\\ne y^{n}$ and $y\\in\\mathcal{V}^{i n}$ , $(\\pmb{x},y,y^{n})$ is a closed-set noise;   \n131 \u2022 When $y\\ne y^{n}$ and $y\\in\\mathcal{V}^{o u t}$ , $({\\pmb x},{\\boldsymbol y},{\\boldsymbol y}^{n})$ is an open-set noise. ", "page_idx": 3}, {"type": "text", "text": "132 Specifically, we have $y\\sim P(\\mathbf{y}=y|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{y}^{a l l})$ while $y^{n}\\sim P^{n}(\\mathbf{y}=y^{n}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{a l l})$ . ", "page_idx": 3}, {"type": "text", "text": "133 However, we can only identify label noise type with $({\\pmb x},y,y^{n})\\,\\!-y,y^{n}$ yet to be sampled even with   \n134 known conditional probability. To enable sample-wise analysis on the impact of different label noise,   \n135 we further introduce below $(O_{x},C_{x})$ label noise:   \n136 Definition 3.3 $\\left(O_{x},C_{x}\\right)$ label noise). For sample $\\textbf{\\em x}$ with clean conditional probability $P(\\mathbf{y}|\\mathbf{x}=$   \n137 $\\mathbf{\\Delta}x;\\mathbf{y}\\in\\mathcal{V}^{a l l}\\,,$ ) and complete noise transition matrix $T$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\cal O}_{x}=\\sum_{i=A+1}^{A+B}\\sum_{j=1}^{A}T_{i j}{\\cal P}({\\bf y}=i|{\\bf x}=x;{\\bf y}\\in\\mathcal{y}^{a l l})=\\sum_{i=A+1}^{A+B}{\\cal P}({\\bf y}=i|{\\bf x}=x;{\\bf y}\\in\\mathcal{y}^{a l l})},}\\\\ {{\\displaystyle C_{x}=\\sum_{i=1}^{A}\\sum_{j=1,j\\neq i}^{A}T_{i j}{\\cal P}({\\bf y}=i|{\\bf x}=x;{\\bf y}\\in\\mathcal{y}^{a l l})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 Here, $O_{x}$ is the expected open-set noise ratio, $C_{x}$ is the expected closed-set noise ratio. We then   \n139 define sample $\\textbf{\\em x}$ as an $(O_{x},C_{x})$ label noise. Intuitively speaking, sample $\\textbf{\\em x}$ is expected to be an   \n140 open-set noise with probability as $O_{x}$ and to be a closed-set noise with probability $C_{x}$ .   \n141 With Definition 3.3, we formalize the concept of noise ratio for the whole distribution, as the   \n142 accumulated $(O_{x},C_{x})$ label noise at all sample points $\\pmb{x}\\in\\mathcal{X}$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nN=\\int_{x}(O_{x}+C_{x})\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{y}^{a l l})d x\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 3.3 Analyzing classification error rate inflation in LNL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "144 In this section, we try to analyze the impact of different label noise. Please note, while the reformulated   \n145 LNL setting encompasses outlier classes $y^{o u t}$ , in both the training and evaluation stage, they are   \n146 unknown (agnostic); the learned model $f$ is still tailored for the classification of inlier classes $\\upgamma^{i n}$ .   \n147 That is to say, the default classification evaluation protocol is still concerned with the classification   \n148 error rate over the inlier conditional probability, denoted as $P^{f}(\\mathbf{y}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})$ .   \n149 Error rate inflation With $P^{f}(\\mathbf{y}|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n})$ , in the evaluation phase, for specific sample $\\textbf{\\em x}$   \n150 we have its prediction as: $y^{f}=\\arg\\operatorname*{max}_{k}P^{f}(\\mathbf{y}=k|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{y}^{i n})\\in\\mathcal{y}^{i n}$ , and the corresponding   \n151 expected classification error rate as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{x}=\\sum_{\\mathbf{y}\\neq y_{f}}P(\\mathbf{x},\\mathbf{y};\\mathbf{y}\\in\\mathcal{Y}^{i n})=(1-P(\\mathbf{y}=y_{f})|\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n}))\\cdot P(\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "152 Specifically, we have the Bayes error rate corresponds to the Bayes optimal model $f^{*}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{x}^{*}=(1-\\operatorname*{max}_{k}P(\\mathbf{y}=k|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}))\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 To measure the negative impacts of noisy labels, we care about how much extra errors have been   \n154 introduced, measured by the error rate inflation of learned model $f$ compared to the Bayes optimal   \n155 model $f^{*}$ :   \n156 Definition 3.4 (Error rate inflation). With $E_{x}^{*}$ as the Bayes error rate, we define the error rate inflation   \n157 for sample $\\textbf{\\em x}$ as: $\\Delta E_{x}=E_{x}-E_{x}^{\\ast}$ .   \n158 Two pragmatic cases However, $P^{f}(\\mathbf{y}|\\mathbf{x}\\,=\\,\\mathbf{x};\\mathbf{y}\\,\\in\\,\\mathcal{Y}^{i n})$ , as the prediction of the final learned   \n159 model $f$ , is affected by many factors (model capacity/dataset size/training hyperparameters such   \n160 as training epochs, etc.), which is non-trivial to determine its specific value for an offline analysis2.   \n161 Thus, we consider two specific pragmatic cases:   \n162 \u2022 Fitted case: the model perfectly fits the noisy distribution: $P^{f}({\\mathfrak{y}}|{\\mathbf{x}}={\\boldsymbol{x}};{\\mathfrak{y}}\\in{\\mathcal{y}}^{i n})=P^{n}({\\mathfrak{y}}|{\\mathbf{x}}=$   \n163 $\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n\\cdot}$ );   \n164 \u2022 Memorized case: the model completely memorises the noisy labels: $P^{f}({\\mathfrak{y}}|{\\mathbf{x}}={\\boldsymbol{x}};{\\mathbf{y}}\\in{\\mathcal{y}}^{i n})=$   \n165 $P^{y^{n}}(\\mathbf{y}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})$ ; Here $P^{y^{n}}$ denotes the one-hot encoding of the noisy label $y^{n}$ .   \n166 Nonetheless, these two cases are very realistic and important; Empirically, it is highly possible that   \n167 the memorized case can correspond to scenarios such as scratch training based on a single-label   \n168 dataset with a normal deep neural network - as normally such model has enough capacity to memorize   \n169 all the labels, while the fitted case can correspond to scenarios such as fine-tuning a linear classifier   \n170 with a pre-trained model - as the pre-trained model already captures good sample representations and   \n171 the capacity of a linear classifier is limited. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "172 3.4 Error rate inflation analysis w.r.t different label noise ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "173 In this section, we focus on analyzing the error rate inflation of different label noise. Let us recall the   \n174 clean conditional distribution as $P(\\mathbf{\\bar{y}}|\\mathbf x;\\mathbf y\\in\\mathcal y^{a l l})$ . For ease of analysis, we contemplate a simple   \n175 scenario, wherein the entire clean conditional distribution remains unchanged, except only one of the   \n176 sample points, say $\\textbf{\\em x}$ , is afflicted by label noise: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP^{n}(\\mathbf{y}|\\mathbf{x}\\neq\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{a l l})=P(\\mathbf{y}|\\mathbf{x}\\neq\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{a l l}),\\ P^{n}(\\mathbf{y}|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{a l l})\\neq P(\\mathbf{y}|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{y}^{a l l}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 In this condition, we can simplify analyzing the impact of label noise on the whole distribution to   \n178 analyzing the error rate inflation of a single sample $\\textbf{\\em x}$ . Specifically, we consider two specific sample   \n179 points $x_{1}$ and $\\pmb{x}_{2}$ , corresponding to two in our later comparative analysis. Let us denote its clean   \n180 conditional probability as $P(\\mathbf{y}|\\breve{\\mathbf{x}}=\\mathbf{x}_{1};\\mathbf{y}\\in\\mathcal{y}^{a l l})=[p_{1}^{\\downarrow},...,p_{A}^{1},...,p_{A+B}^{\\bar{1}}]$ and $P(\\mathbf{y}|\\mathbf{x}=\\mathbf{\\boldsymbol{x}}_{2};\\mathbf{y}\\in$   \n181 $\\mathcal{V}^{a l l})=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}]$ , and noise transition matrix as $T^{1}$ and $T^{2}$ , respectively. We further   \n182 assume: ", "page_idx": 4}, {"type": "equation", "text": "$$\nO_{x_{1}}+C_{x_{1}}=O_{x_{2}}+C_{x_{2}}=\\delta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 We compare the error rate inflation $(\\Delta E_{x_{1}}\\,\\,\\nu s\\,\\,\\Delta E_{x_{2}})$ ) with different label noise given same/fixed   \n184 noise ratio for a strictly fair comparison. Note we assume that $x_{1}$ and $\\pmb{x}_{2}$ hold the same sampling   \n185 prior probability: $P(\\mathbf{\\dot{x}}=\\mathbf{x}_{1};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=P(\\mathbf{x}=\\mathbf{x}_{2};\\mathbf{y}\\in\\mathcal{Y}^{a l l}))$ ; so that, we assure that the whole   \n186 noise ratio $N$ is fixed, and more importantly, sample $x_{1}$ and $\\pmb{x}_{2}$ can be considered as probabilistic   \n187 exchangeable in the dataset collection process.   \n188 For better clarity, we depict the derivation relations for $\\Delta_{x}$ in fig. 2. Specifically, for our two   \n189 interested cases above, we have corresponding error rate inflation for sample $\\textbf{\\em x}$ (sample subscript   \n190 omitted for simplicity) as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "191 \u2022 Fitted case: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta E_{x}=\\operatorname*{max}[p_{1},...,p_{A}]-p_{\\mathrm{arg\\,max}[\\sum_{i=1}^{A+B}p_{i}T_{i1},...,\\sum_{i=1}^{A+B}p_{i}T_{i A}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 \u2022 Memorized case: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta E_{x}=\\operatorname*{max}[p_{1},...,p_{A}]-\\sum_{i=1}^{A}(p_{i}\\cdot\\sum_{j=1}^{A+B}p_{j}T_{j i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 We notice that $\\Delta_{x}$ in both cases are only affected by clean conditional probability $P(\\mathbf{y}|\\mathbf{x}=\\mathbf{x}_{1};\\mathbf{y}\\in$   \n194 $\\mathfrak{V}^{a l l}\\,,$ ) and complete noise transition matrix $T$ . ", "page_idx": 4}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/e36b7bfc60e5af17d48899fcc163c4f7a8f4b006d7ed714b758fad034d028570.jpg", "img_caption": ["Figure 2: All-in-one derivation flowchart. Full details in appendix C. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "195 3.4.1 How does open-set noise compare to closed-set noise? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "196 We first try to elucidate the difference between open-set noise and closed-set noise. Without loss of   \n197 generality, we consider: ", "page_idx": 5}, {"type": "equation", "text": "$$\nO_{x_{1}}>O_{x_{2}}\\;,\\;C_{x_{1}}<C_{x_{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "198 Intuitively speaking, we consider sample $x_{1}$ to be more prone to open-set noise compared to sample   \n199 $\\pmb{x}_{2}$ , thus corresponding to the \u2018more open-set noise\u2019 scenario. However, without extra regularizations,   \n200 there exist infinite $T^{1}$ and $T^{2}$ fulfliling eq. (7) and eq. (10) given specific $P(\\mathbf{y}|\\mathbf{x}=\\mathbf{x}_{1};\\mathbf{\\bar{y}}\\in\\mathcal{y}^{a l l})$ and   \n201 $P(\\mathbf{y}|\\mathbf{x}=\\mathbf{x}_{2};\\mathbf{y}\\in\\mathcal{y}^{a l l})$ (see toy example below), the analysis on $\\Delta E_{{\\pmb x}_{1}}$ vs $\\Delta E_{x_{2}}$ is thus infeasible. ", "page_idx": 5}, {"type": "text", "text": "Toy example about agnostic $T$ Assuming a ternary classification, with two known inlier classes $\"0\"$ and \u201c1\") and one unknown outlier class $^{\\bullet\\bullet}2\"$ . Say, we have sample $x_{1}$ with clean conditional probability as [0.1, 0.2, 0.7]. Assuming two different noise transition matrices for $T^{1}$ below: ", "page_idx": 5}, {"type": "equation", "text": "$$\n[0.55,0.45,0.0]=[0.1,0.2,0.7]\\left[\\frac{0.5}{0.75}\\quad0.25\\ \\middle|\\ 0\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n[0.45,0.55,0.0]=[0.1,0.2,0.7]\\left[\\frac{0}{0.5}\\quad0.5\\ |\\ 0\\ ]\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We have $O_{x_{1}}=0.7,C_{x_{1}}=0.2$ in both conditions but we arrive at different noisy conditional probability, similarly for sample $\\pmb{x}_{2}$ . ", "page_idx": 5}, {"type": "text", "text": "203 We thus consider a class concentration assumption \u2014 in most classification datasets, the majority of   \n204 samples belong to specific class exclusively with high probability. In this condition, we have proved: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Open-set noise vs closed-set noise). Let us consider sample $\\mathbf{\\mathcal{x}}_{1},\\,\\mathbf{\\Delta}x_{2}$ fulfilling eq. (7) and eq. (10) - compared to $\\pmb{x}_{2}$ , $x_{1}$ is considered as more prone to open-set noise. Let us denote a = arg $\\mathrm{max}_{i}\\:P(\\mathrm{y}\\,=\\,i|\\mathbf{x}\\,=\\,{\\pmb x}_{1};\\mathbf{y}\\,\\in\\,\\mathcal{Y}^{a l l})$ and b = arg maxi $P(\\mathrm{y}^{\\'}=\\,i|\\mathbf x\\,=\\,x_{2};\\mathbf y\\,\\in\\,\\mathcal y^{a l l})$ ), we assume (with a high probability): $p_{a}^{1}\\to1,\\{p_{i}^{1}\\to0\\}_{i\\neq a}$ and $p_{b}^{2}\\rightarrow{1}$ , $\\{p_{b}^{2}\\rightarrow0\\}_{i\\neq b}$ . Then, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}<\\Delta E_{{\\pmb x}_{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "205 in both Fitted case and Memorized case. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "206 Please refer to appendix D.1 for detailed proof. To summarize, we validate that in most conditions,   \n207 open-set noise is less harmful than closed-set noise in both fitted case and memorized case. ", "page_idx": 5}, {"type": "text", "text": "208 3.4.2 How does different open-set noise compare to each other? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "209 We further study how different open-set noise affect the model. Specifically, we consider: ", "page_idx": 5}, {"type": "equation", "text": "$$\nO_{\\mathbf{x}_{1}}=O_{\\mathbf{x}_{2}}\\;,\\;C_{\\mathbf{x}_{1}}=C_{\\mathbf{x}_{2}}=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 Intuitively speaking, we focus on the impacts of different open-set noise modes given the same/fixed   \n211 open-set noise ratio, while excluding the effect of closed-set noise. In this section, we assume   \n212 sample $x_{1}$ and sample $\\pmb{x}_{2}$ holds the same clean conditional probability: $[p_{1}^{1},...,p_{A}^{1},...,p_{A+B}^{1}]\\,=$   \n213 $[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}]$ , to only focus on the impact of different open-set noise modes with the same   \n214 original sample. It is straightforward that $O_{x_{1}}=O_{x_{2}}$ always holds since $\\begin{array}{r}{\\sum_{i=A+1}^{A+B}p_{i}^{1}=\\sum_{i=A+1}^{A+B}p_{i}^{2}}\\end{array}$   \n215 To ensure $C_{{\\bf x}_{1}}=C_{{\\bf x}_{2}}=0$ , we simply set $T_{i n}^{1}=T_{i n}^{2}={\\bf I}$ .   \n216 Thus, we have the flexibility to explore various forms of $T_{o u t}$ \u2014 corresponding to different open-set   \n217 noise modes. Specifically, we consider two distinct open-set noise modes: \u2018easy\u2019 open-set noise   \n218 when the transition from outlier classes to inlier classes involves completely random flipping, and   \n219 \u2018hard\u2019 open-set noise when there exists an exclusive transition between the outlier class and specific   \n220 inlier class. We denote as $T^{e a s y}$ for \u2018easy\u2019 open-set noise and $T^{h a r d}$ for \u2018hard\u2019 open-set noise, with   \n221 intuitive explanations below: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nT^{e a s y}=\\left[\\stackrel{\\textstyle1}{\\textstyle A}\\,\\,\\,\\,\\cdots\\,\\,\\,\\,\\frac{1}{A}\\right]_{\\mathrm{\\tiny~B\\timesA}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "222 and ", "page_idx": 6}, {"type": "equation", "text": "$$\nT^{h a r d}=\\left[\\stackrel{0}{\\dots}\\right.\\,\\dots\\quad\\cdots\\quad]_{\\phantom{h a r d}}^{\\phantom{h a r d}}=\\left[\\stackrel{0}{\\dots}\\right.\\,\\dots\\quad\\cdots\\quad\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "223 Especially, for $T^{e a s y}$ , we have $\\begin{array}{r}{T_{i j}=\\frac{1}{A}}\\end{array}$ everywhere; for $T^{h a r d}$ , we denote as $H_{i}:\\{\\mathrm{arg}_{j}(T_{j i}^{h a r d}=$   \n224 the set of corresponding outlier classes confused to inlier class $i\\in\\mathcal{V}^{i n}$ . Without   \n225 loss of generality, we consider $x_{1}$ with \u2018easy\u2019 open-set noise $T^{e a s y}$ and $\\pmb{x}_{2}$ with \u2018hard\u2019 open-set   \n226 noise $T^{\\check{h}a r d}$ . Please note, that we no longer require class concentration assumption here as the noise   \n227 transition matrix is already known. In this condition, we have proved: ", "page_idx": 6}, {"type": "text", "text": "228 Theorem 3.6 (\u2018Hard\u2019 open-set noise vs \u2018easy\u2019 open-set noise). Let us consider sample $\\mathbf{\\mathcal{x}}_{1},\\ \\mathbf{\\mathcal{x}}_{2}$ 223209 saentd  tdheen cotoer $T_{o u t}^{1}~=$ $T^{e a s y},T_{o u t}^{2}=T^{h a r d},T_{i n}^{1}=T_{i n}^{2}={\\bf I}$ $[p_{1}^{1},...,p_{A}^{1},...,p_{A+B}^{1}]=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}]=$ 231 $[p_{1},...,p_{A},...,p_{A+B}]$ . Then, we have: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Fitted case: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}\\leq\\Delta E_{{\\pmb x}_{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u2022 Memorized case: ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}-\\Delta E_{{\\pmb x}_{2}}=\\sum_{i=1}^{A}a_{i}b_{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "232 ", "page_idx": 6}, {"type": "text", "text": "Here, $\\begin{array}{r}{a_{i}=p_{i},b_{i}=\\sum_{j\\in H_{i}}p_{j}-\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "233 Please refer to appendix D.2 for detailed proof. Specifically, we further discuss about memo  \n234 rized case here. Since $\\begin{array}{r}{\\sum_{i=1}^{A}b_{i}=0,\\sum_{i=1}^{A}\\!\\!\\!\\!a_{i}=1}\\end{array}$ , we can easily infer $\\operatorname*{max}(\\Delta E_{{\\pmb x}_{1}}-\\Delta E_{{\\pmb x}_{2}})\\geq$   \n235 $0,\\operatorname*{min}(\\Delta E_{{\\pmb x}_{1}}-\\Delta E_{{\\pmb x}_{2}})\\leq0$ . With theorem D.3, we know when the ranking of $\\{p_{i}^{1}\\}_{i=1}^{A}$ is completely   \n236 in agreement with the ranking $\\{\\sum_{j\\in H_{i}}p_{j}^{1}\\}_{i=1}^{A}$ (constant term A1 iA=+AB+1 pi1 omitted here), we   \n237 reach its maximum value with $\\Delta E_{{\\pmb x}_{1}}-\\Delta E_{{\\pmb x}_{2}}\\geq0$ . Intuitively speaking, this implies a scenario that   \n238 the \u2018hard\u2019 open-set noise tends to confuse a sample into the inlier class it primarily belongs to (with   \n239 higher semantic similarity), as indicated by its higher probability (the higher the $p_{i}^{1}$ the higher the   \n240 j\u2208Hi pj1). For example, an outlier \u2018tiger\u2019 image is wrongly included as a \u2018cat\u2019 rather than a \u2018dog\u2019 in   \n241 a \u2018cat vs dog\u2019 binary classification dataset. As this is more consistent with the common intuition, we   \n242 default to such noise mode for \u2018hard\u2019 open-set noise \u2014 assuming the ranking of $\\{p_{i}^{1}\\}_{i=1}^{A}$ is of high   \n243 agreement with the ranking of $\\{\\sum_{j\\in H_{i}}p_{j}^{1}\\}_{i=1}^{A}$ .   \n244 To summarize, unlike the general comparison between open-set noise and closed-set noise, the \u2018hard\u2019   \n245 open-set noise and the \u2018easy\u2019 open-set noise exhibit an opposite trend in two different cases. In the   \n246 ftited case, \u2018easy\u2019 open-set noise appears to be less harmful, while in the memorized case, the impact   \n247 of \u2018hard\u2019 open-set noise is comparatively smaller. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "248 3.5 Rethinking open-set noise detection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "249 In this section, we try to investigate a commonly used open-set noise identification mechanism based   \n250 on entropy dynamics. Within the sample selection paradigm, several methods [1, 16] have proposed   \n251 to further identify open-set noise, based on the empirical phenomenon that samples with relatively   \n252 in-confident predictions are usually open-set samples, characterized by its high prediction entropy.   \n253 Specifically, we consider original sample $\\textbf{\\em x}$ without noise transition, $\\textbf{\\em x}$ with $T^{h a r d}$ and $\\textbf{\\em x}$ with $T^{e a s y}$   \n254 as a clean sample, a \u2018hard\u2019 open-set noise and an \u2018easy\u2019 open-set noise, respectively. For simplicity,   \n255 we omit the subscript.   \n256 Empirically, most sample selection method starts from the early training stages after certain epochs   \n257 of warm-up training, expecting the model to learn meaningful information before over-fitting. To   \n258 analyze the entropy dynamics, we thus consider the model predictions in the ftited case as a pragmatic   \n259 proxy. Let us denote as $\\mathcal{H}_{e a s y}$ , $\\mathcal{H}_{h a r d}$ and $\\mathcal{H}_{c l e a n}$ the prediction entropy corresponds to these three   \n260 conditions, we have3: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\mathcal{H}_{c l e a n}=\\mathcal{H}([\\frac{p_{1}}{\\sum_{i=1}^{A}p_{i}},...,\\frac{p_{A}}{\\sum_{i=1}^{A}p_{i}}])}\\\\ {\\quad\\quad=\\mathcal{H}([p_{1}+\\frac{p_{1}}{\\sum_{i=1}^{A}p_{i}}\\sum_{i=A+1}^{A+B}p_{i},...,p_{A}+\\frac{p_{A}}{\\sum_{i=1}^{A}p_{i}}\\sum_{i=A+1}^{A+B}p_{i}]),}\\\\ {\\mathcal{H}_{e a s y}=\\mathcal{H}([p_{1}+\\frac{1}{A}\\displaystyle\\sum_{i=A+1}^{A+B}p_{i},...,p_{A}+\\frac{1}{A}\\displaystyle\\sum_{i=A+1}^{A+B}p_{i}]),}\\\\ {\\mathcal{H}_{h a r d}=\\mathcal{H}([p_{1}+\\sum_{j\\in H_{1}}p_{j},...,p_{A}+\\displaystyle\\sum_{j\\in H_{A}}p_{j}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "261 We note $\\mathcal{H}_{e a s y}\\geq\\mathcal{H}_{c l e a n}{^{4}}$ . However, comparing $\\mathcal{H}_{h a r d}$ and $\\mathcal{H}_{c l e a n}$ is non-trivial without specific   \n262 values for each entry. Thus, we suggest open-set noise detection based on the prediction entropy may   \n263 only be effective for \u2018easy\u2019 open-set noise. ", "page_idx": 7}, {"type": "text", "text": "264 4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "265 In this section, we try to validate our theoretical findings. In section 4.1, we validate the theoretical   \n266 comparisons of different label noise. In section 4.2, we validate the entropy dynamics with different   \n267 label noise. Moreover, in appendix E.1, we revisit the performance of two existing LNL methods   \n268 involving open-set noise. To conduct more controllable, fair and accurate experiments, we propose   \n269 two synthetic open-set noisy datasets \u2014 CIFAR100-O and ImageNet-O, respectively based on   \n270 the CIFAR100 and ImageNet datasets. We also consider closed-set noise in some experiments,   \n271 particularly, the symmetric closed-set noise. Please refer to appendix A for more dataset and   \n272 implementation details and also details about open-set detection protocol. ", "page_idx": 7}, {"type": "text", "text": "273 4.1 Empirical validation on previous probabilistic findings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "274 In this section, we conduct experiments to validate the theorem 3.5 and theorem 3.6. Since most deep   \n275 models have sufficient capacity, we consider direct supervised learning from scratch on the noisy   \n276 dataset and consider the final model as the memorized case - as evidenced by nearly $100\\%$ train set   \n277 accuracy. Conversely, obtaining a model that perfectly ftis the data distribution is often challenging;   \n278 here, we consider training a single-layer linear classifier upon a frozen pretrained encoder. Due to the   \n279 limited capacity of the linear layer, we expect to roughly approach the fitted case.   \n280 We show classification accuracy on CIFAR100-O and ImageNet-O datasets under different noise   \n281 ratios, as shown in fig. 3(a/b). We find that: 1) in both cases, the presence of open-set noise has   \n282 a significantly smaller impact on classification accuracy compared to closed-set noise. 2) \u2018hard\u2019   \n283 open-set noise and \u2018easy\u2019 open-set noise show opposite trends in the two different scenarios. These   \n284 results align perfectly with our theoretical analysis.   \n285 In addition to closed-set classification accuracy, we also report the model\u2019s open-set detection   \n286 performance using the maximum prediction value as the indicator [9]) in fig. $3(\\mathrm{c/d})$ . We find that, in   \n287 both cases, the presence of open-set noise leads to a degraded open-set detection performance, while   \n288 conversely, the presence of closed-set noise can often even enhance open-set detection performance.   \n289 In light of this contrasting trend, we propose that the open-set detection task, in addition to the default   \n290 closed-set classification, may help to offer a more comprehensive evaluation of LNL methods. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/ddda2087e5e4e6472ab8b460325704d90d3f980aca6abb3f50a8aa04a86dcc31.jpg", "img_caption": ["Figure 3: Direct supervised training with different noise modes/ratios. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "291 4.2 Inspecting entropy-based open-set noise detection mechanism ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 In section 3.5, we briefly analyze the open-set detection mechanism based on the entropy values of   \n293 model predictions and find that it may be effective only for \u2018easy\u2019 open-set noise. Here, we again   \n294 utilize the CIFAR100-O and ImageNet-O datasets for validation experiments with different open-set   \n295 noise ratios and modes. Specifically, we adopt the common warm-up idea used in existing LNL   \n296 methods - training with the entire dataset for a certain number of epochs. We report the model\u2019s   \n297 predicted entropy values for each sample at the $\\{5t h,10t h,20t h,30t h\\}$ epoch in fig. 4.   \n298 We validate that the entropy dynamics is a more effective indicator for \u2018easy\u2019 open-set noise compared   \n299 to \u2018hard\u2019 open-set noise ((a) vs (b), (c) vs (d) in fig. 4). However, even for \u2018easy\u2019 open-set noise, we   \n300 also notice that the warm-up epoch matters a lot \u2014 too early $5t h$ epoch in fig. 4(c)) or too late $30t h$   \n301 epoch in fig. 4(c)) also make open-set noise difficult to distinguish. We also test with mixed noise   \n302 including both open-set noise and closed-set noise, please refer to appendix B for more discussions. ", "page_idx": 8}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/d8975fa2f50af3dfc1652e723b8fef4016df84aedaa29fc5e1669a22b658aadb.jpg", "img_caption": ["Figure 4: Entropy dynamics w.r.t different datasets/noise modes/noise ratios. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "303 5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "304 This paper focuses on exploring how open-set label noise affects the performance of models. While   \n305 the \u2018open world\u2019 setting involving open-set samples has been widely discussed in several other weakly   \n306 supervised learning settings, its application in the context of learning with noisy labels has been   \n307 understudied. In light of this, we reconsider the LNL problem, specifically focusing on the impact of   \n308 open-set noise compared to closed-set noise, and different types of open-set noise compared to each   \n309 other, on the evaluation performance. In light of the challenges existing testing frameworks face in   \n310 handling open-set noise, we explore the open-set detection task to address the deficiencies in model   \n311 evaluation for open-set noise and conducted preliminary experiments. Additionally, we look into   \n312 the common mechanism for detecting open-set noise based on the model\u2019s prediction entropy. Both   \n313 theoretical and empirical results highlight the urgent need for a deeper exploration of open-set noise   \n314 and its complex impact on model performance. ", "page_idx": 8}, {"type": "text", "text": "315 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "316 [1] Paul Albert, Diego Ortego, Eric Arazo, Noel E O\u2019Connor, and Kevin McGuinness. Addressing   \n317 out-of-distribution label noise in webly-labelled data. In Proceedings of the IEEE/CVF Winter   \n318 Conference on Applications of Computer Vision, pages 392\u2013401, 2022. 2, 7   \n319 [2] Eric Arazo, Diego Ortego, Paul Albert, Noel O\u2019Connor, and Kevin McGuinness. Unsupervised   \n320 label noise modeling and loss correction. In International Conference on Machine Learning,   \n321 pages 312\u2013321. PMLR, 2019. 1, 2   \n322 [3] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling:   \n323 Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference   \n324 on Artificial Intelligence, volume 35, pages 6912\u20136920, 2021. 1   \n325 [4] Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, and Pheng-Ann Heng. Beyond   \n326 class-conditional assumption: A primary attempt to combat instance-dependent label noise. In   \n327 Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11442\u201311450,   \n328 2021. 1, 2   \n329 [5] Chen Feng, Georgios Tzimiropoulos, and Ioannis Patras. Ssr: An efficient and robust framework   \n330 for learning with unknown label noise. In 33rd British Machine Vision Conference 2022, BMVC   \n331 2022, London, UK, November 21-24, 2022. BMVA Press, 2022. 2, 19, 20   \n332 [6] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep   \n333 neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31,   \n334 2017. 1, 2   \n335 [7] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta  \n336 tion layer. 2016. 1, 2   \n337 [8] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi   \n338 Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.   \n339 arXiv preprint arXiv:1804.06872, 2018. 2   \n340 [9] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution   \n341 examples in neural networks. In International Conference on Learning Representations, 2016.   \n342 8, 13   \n343 [10] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning   \n344 data-driven curriculum for very deep neural networks on corrupted labels. In International   \n345 Conference on Machine Learning, pages 2304\u20132313. PMLR, 2018. 2, 12   \n346 [11] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as   \n347 semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020. 1, 2, 12, 19, 20   \n348 [12] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database:   \n349 Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017. 1,   \n350 12   \n351 [13] Eran Malach and Shai Shalev-Shwartz. Decoupling\" when to update\" from\" how to update\".   \n352 arXiv preprint arXiv:1706.02613, 2017. 2   \n353 [14] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.   \n354 MIT press, 2018. 5   \n355 [15] Diego Ortego, Eric Arazo, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Multi  \n356 objective interpolation training for robustness to label noise. In Proceedings of the IEEE/CVF   \n357 Conference on Computer Vision and Pattern Recognition, pages 6606\u20136615, 2021. 2, 12   \n358 [16] Ragav Sachdeva, Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro.   \n359 Evidentialmix: Learning with combined open-set and closed-set noisy labels. In Proceedings of   \n360 the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3607\u20133615, 2021.   \n361 2, 7, 12   \n362 [17] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust   \n363 deep learning. In International Conference on Machine Learning, pages 5907\u20135915. PMLR,   \n364 2019. 1, 2   \n365 [18] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao   \n366 Xia. Iterative learning with open-set noisy labels. In Proceedings of the IEEE conference on   \n367 computer vision and pattern recognition, pages 8688\u20138696, 2018. 2   \n368 [19] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross   \n369 entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International   \n370 Conference on Computer Vision, pages 322\u2013330, 2019. 1, 2   \n371 [20] Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo An. Open-set label noise can improve robustness   \n372 against inherent label noise. Advances in Neural Information Processing Systems, 34:7978\u20137992,   \n373 2021. 3   \n374 [21] Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A   \n375 topological filter for learning with label noise. Advances in neural information processing   \n376 systems, 33:21382\u201321393, 2020. 2   \n377 [22] Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng Li. Ngc: A   \n378 unified framework for learning with open-world noisy data. arXiv preprint arXiv:2108.11035,   \n379 2021. 2, 12   \n380 [23] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi   \n381 Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in neural   \n382 information processing systems, 32, 2019. 1, 2   \n383 [24] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi   \n384 Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. arXiv   \n385 preprint arXiv:2106.00445, 2021. 2   \n386 [25] Xiaobo Xia, Bo Han, Nannan Wang, Jiankang Deng, Jiatong Li, Yinian Mao, and Tongliang   \n387 Liu. Extended T: Learning with mixed closed-set and open-set noisy labels. IEEE Transactions   \n388 on Pattern Analysis and Machine Intelligence, 45(3):3047\u20133058, 2022. 2   \n389 [26] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating   \n390 instance-dependent bayes-label transition matrix using a deep neural network. In International   \n391 Conference on Machine Learning, pages 25302\u201325312. PMLR, 2022. 1, 2   \n392 [27] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels.   \n393 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n394 pages 7017\u20137025, 2019. 2   \n395 [28] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does   \n396 disagreement help generalization against label corruption? In International Conference on   \n397 Machine Learning, pages 7164\u20137173. PMLR, 2019. 2   \n398 [29] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural   \n399 networks with noisy labels. arXiv preprint arXiv:1805.07836, 2018. 1, 2   \n400 [30] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label de  \n401 tection to noisy label self-correction. In International Conference on Learning Representations,   \n402 2020. 2 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "403 A Experiment details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "404 A.1 Dataset details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "405 Previous works involving open-set noise also try to build synthetic noisy datasets, typically treating   \n406 different datasets as open-set noise for each other to construct synthetic noisy dataset [16, 22]. In   \n407 this scenario, potential domain gaps could impact a focused analysis of open-set noise. In this   \n408 work, we propose selecting inlier/outlier classes from the same dataset to avoid this issue. Besides,   \n409 in previous works, the consideration of open-set noise patterns often focused on random flipping   \n410 from outlier classes to all possible inlier classes, which is indeed the \u2018easy\u2019 open-set noise adopted   \n411 here. However, both our theoretical analysis and experimental findings demonstrate that \u2018easy\u2019   \n412 open-set noise and \u2018hard\u2019 open-set noise exhibit distinct characteristics. Therefore, relying solely   \n413 on experiments with \u2018Easy\u2019 open-set noise is insufficient, emphasizing the necessity to explore and   \n414 understand the complexities associated with different types of open-set noise. We also evaluate with   \n415 closed-set noise in some experiments, by default, we consider the common symmetric closed-set   \n416 noise in this work.   \n417 CIFAR100-O For the original CIFAR100 dataset, in addition to the commonly-used 100 fine   \n418 classes, there exist 20 coarse classes each consisting of 5 fine classes. To build CIFAR100-O, we   \n419 select one fine class from each coarse class as an inlier class (20 classes in total) while considering   \n420 the remaining classes as outlier classes (80 classes in total). Then, we consider \u2018Hard\u2019 and \u2018Easy   \n421 open-set noise as below:   \n422 \u2022 \u2018Hard\u2019: Randomly selected samples from the same coarse category as the target category   \n423 were introduced as open-set noise.   \n424 \u2022 \u2018Easy\u2019: Regardless of the target category, samples from the remaining categories were   \n425 randomly introduced as open-set noise.   \n426 ImageNet-O For a more challenging benchmark, we consider ImageNet-1K datasets - consisting of   \n427 1,000 classes. Specifically, we randomly select 20 classes and artificially identify another 20 classes   \n428 similar to each of them:   \n429 inliers $=$ [\u2019tench\u2019, \u2019great white shark\u2019, \u2019cock\u2019, \u2019indigo bunting\u2019, \u2019European fire salamander\u2019, \u2019African   \n430 crocodile\u2019, \u2019barn spider\u2019, \u2019macaw\u2019, \u2019rock crab\u2019, \u2019golden retriever\u2019, \u2019wood rabbit\u2019, \u2019gorilla\u2019, \u2019abaya\u2019,   \n431 \u2019beer bottle\u2019, \u2019bookcase\u2019, \u2019cassette player\u2019, \u2019coffee mug\u2019, \u2019shopping basket\u2019, \u2019trifle\u2019, \u2019meat loaf\u2019]   \n432 outliers $=$ [\u2019goldfish\u2019, \u2019tiger shark\u2019, \u2019hen\u2019, \u2019robin\u2019, \u2019common newt\u2019, \u2019American alligator\u2019, \u2019garden   \n433 spider\u2019, \u2019sulphur-crested cockatoo\u2019, \u2019king crab\u2019, \u2019Labrador retriever\u2019, \u2019Angora\u2019, \u2019chimpanzee\u2019,   \n434 \u2019academic gown\u2019, \u2019beer glass\u2019, \u2019bookshop\u2019, \u2019CD player\u2019, \u2019coffeepot\u2019, \u2019shopping cart\u2019, \u2019ice cream\u2019,   \n435 \u2019pizza\u2019] ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "436 Then, we consider \u2018Hard\u2019 and \u2018Easy\u2019 open-set noise as below: ", "page_idx": 11}, {"type": "text", "text": "437 \u2022 \u2018Hard\u2019: Randomly select samples from the corresponding similar outlier class as the target   \n438 category were introduced as open-set noise.   \n439 \u2022 \u2018Easy\u2019: Samples from the remaining categories were randomly introduced as open-set noise.   \n440 For open-set detection, we directly use the corresponding test sets of these classes from the original   \n441 datasets.   \n442 WebVision WebVision [12] is an extensive dataset comprising 1,000 classes of images obtained   \n443 through web crawling, which thus contains a large amount of open-set noise. In line with previous   \n444 studies [10, 11, 15], we evaluate our methods using the first 50 classes from the Google Subset of   \n445 WebVision. To test the performance of open-set detection on the WebVision dataset, we collect a   \n446 separate test set consisting of open-set images, following the same collection process as the WebVision   \n447 dataset. Specifically, we utilize the Google search engine with the class names as keywords and   \n448 identify those open-set samples that haven\u2019t been included in the train set for this test set. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "449 A.2 Implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "450 Here, we provide detailed implementation specifications for the fitted case and memorized case in   \n451 section 4.1. We also briefly the applied open-set detection protocol.   \n452 Fitted case For the ftited case, we train a randomly initialized classifier - a single linear layer based   \n453 on the encoder of the ResNet18 model with pretrained weights. In the case of the CIFAR100-O   \n454 dataset, a weak augmentation strategy involving image padding and random cropping is applied   \n455 during training, with a batch size of 512. The weight decay (wd) is set to 0.0005, and the model   \n456 undergoes training for 100 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule   \n457 follows a cosine annealing strategy.   \n458 For the ImageNet-O dataset, no augmentation is applied during training. The batch size is maintained   \n459 at 512, with a weight decay (wd) of 0.01. The model is trained for 100 epochs, employing a learning   \n460 rate (lr) of 0.02. The learning rate schedule for this case also adheres to a cosine annealing strategy.   \n461 Memorized case In this case, we train a PreResNet18 model from scratch. For both datasets, a   \n462 weak augmentation strategy involving image padding and random cropping is applied during training,   \n463 with a batch size of 128. The weight decay (wd) is set to 0.0005, and the model undergoes training   \n464 for 200 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule also follows a cosine   \n465 annealing strategy.   \n466 Open-set detection protocol We use the maximum softmax probability in [9] for the open-set   \n467 detection task. Specifically, assume the trained model $f$ outputs a softmax vector $\\pmb{p}_{i}$ for each sample   \n468 $\\pmb{x}_{i}$ . We then choose a threshold value $t$ between 0 and 1. For evaluation, we consider binary labels   \n469 indicating whether a sample belongs to a known class (closed-set) or the open-set and convert the   \n470 open-set detection task into a binary classification problem. Samples with a maximum softmax value   \n471 $p_{i}^{m a x}$ below the threshold are considered potential open-set samples. This is because a low maximum   \n472 value indicates the model is less confident in any specific class for that sample. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "473 B Entropy dynamics for mixed label noise ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "474 In addition to the open-set noise only scenario, we also inspect the entropy dynamics with mixed   \n475 label noise in fig. 5. Here, we use the notation $\\mathbf{\\dot{\\rho}0.2a l l\\_0.5}$ easy\u2019 to represent a scenario where the   \n476 total noise ratio is 0.2, and within this, half of them are \u2019easy\u2019 open-set noise. In the presence of   \n477 mixed label noise, the existence of closed-set noise severely interferes with identifying open-set   \n478 noise. For example, in fig. 5(d), the entropy values of open-set noise even exceed those of clean   \n479 samples. Though not theoretically analyzed, this further suggests that entropy dynamics based on   \nmodel predictions, may be fragile, and we need to handle open-set noise more cautiously. ", "page_idx": 12}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/1c1c57e5790e339b9a0e2b8ca485e87d5e18a41ce32f205084f9e460774249f1.jpg", "img_caption": ["Figure 5: Entropy dynamics w.r.t mixed label noise. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "480 ", "page_idx": 12}, {"type": "text", "text": "481 C Error rate inflation in two different cases ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "482 In this section, we present the computation details of error rate inflation in two interested cases - ftited   \n483 case and memorized case. Specifically, we have: ", "page_idx": 12}, {"type": "text", "text": "484 \u2022 Fitted case: ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{x}=(1-P(\\mathbf{y}=\\operatorname{arg\\,max}_{k}P^{n}(\\mathbf{y}=k|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}))\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "485 \u2022 Memorized case: ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{x}=(1-P(\\mathbf{y}=\\arg\\operatorname*{max}_{k}P^{y^{n}}(\\mathbf{y}=k|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}))\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})}\\\\ &{\\quad=\\displaystyle\\sum_{y^{n}\\in\\mathcal{Y}^{i n}}(1-P(\\mathbf{y}=y^{n}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}))P^{n}(\\mathbf{y}=y^{n}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})}\\\\ &{\\quad=[1-\\displaystyle\\sum_{y^{n}\\in\\mathcal{Y}^{i n}}P(\\mathbf{y}=y^{n}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})P^{n}(\\mathbf{y}=y^{n}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})]\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "486 While $E_{x}^{*}$ denotes the Bayes optimal error rate: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{x}^{*}=(1-\\operatorname*{max}_{k}P(\\mathbf{y}=k|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}))\\cdot P(\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "487 We thus have $\\Delta E_{x}$ in both cases as: ", "page_idx": 13}, {"type": "text", "text": "488 \u2022 Fitted case: ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x}=[\\operatorname*{max}_{k}P(\\mathbf{y}=k|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n})-P(\\mathbf{y}=\\operatorname{arg\\,max}_{k}P^{n}(\\mathbf{y}=k|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n})|\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n})]}\\\\ &{\\qquad\\cdot P(\\mathbf{x}=\\mathbf{x};\\mathbf{y}\\in\\mathcal{Y}^{i n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "489 \u2022 Memorized case: ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\Delta E_{x}=[\\operatorname*{max}_{k}P({\\mathbf y}=k|{\\mathbf x}=x;{\\mathbf y}\\in\\mathcal{Y}^{i n})-\\displaystyle\\sum_{y^{n}\\in\\mathcal{Y}^{i n}}P({\\mathbf y}=y^{n}|{\\mathbf x}=x;{\\mathbf y}\\in\\mathcal{Y}^{i n})P^{n}({\\mathbf y}=y^{n}|{\\mathbf x}=x;{\\mathbf y}\\in\\mathcal{Y}^{i n})]}\\\\ {\\quad\\cdot\\ P({\\mathbf x}=x;{\\mathbf y}\\in\\mathcal{Y}^{i n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "490 Details on the derivation of error rate inflation (fig. 2) Then, we describe the essential concepts   \n491 depicted in fig. 2 in detail. For better clarity, we here restate the notations in section 3.4. We explicitly   \n492 consider two specific sample points $x_{1}$ and $\\pmb{x}_{2}$ being perturbed independently, corresponding to two   \n493 different label noise modes. Let us assume its clean conditional probability as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\mathbf{y}|\\mathbf{x}=\\pmb{x}_{1};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=[p_{1}^{1},...,p_{A}^{1},...,p_{A+B}^{1}],}\\\\ &{P(\\mathbf{y}|\\mathbf{x}=\\pmb{x}_{2};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "494 and denote its noise transition matrix as $T^{1}=\\{T_{i j}^{1}\\}_{i,j=1}^{A+B}$ and $T^{2}=\\{T_{i j}^{2}\\}_{i,j=1}^{A+B}$ , respectively. Here,   \n495 $\\{T_{i j}^{1}=0\\},\\{T_{i j}^{2}=0\\}$ for all $j>A$ . ", "page_idx": 13}, {"type": "text", "text": "496 With eq. (1), we compute the corresponding noisy conditional probability for both samples as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{n}(\\mathbf{y}|\\mathbf{x}=x_{1};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=[\\displaystyle\\sum_{i=1}^{A+B}p_{i}^{1}T_{i1}^{1},...,\\displaystyle\\sum_{i=1}^{A+B}p_{i}T_{i A}^{1},0,...,0],}\\\\ &{P^{n}(\\mathbf{y}|\\mathbf{x}=x_{2};\\mathbf{y}\\in\\mathcal{Y}^{a l l})=[\\displaystyle\\sum_{i=1}^{A+B}p_{i}^{2}T_{i1}^{2},...,\\displaystyle\\sum_{i=1}^{A+B}p_{i}^{2}T_{i A}^{2},0,...,0].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "497 Note that the error rate inflation is dependent on the clean conditional probability over inlier classes,   \n498 noisy conditional probability over inlier classes and sampling prior over inlier classes as shown in   \n499 eq. (18) and eq. (19). ", "page_idx": 13}, {"type": "text", "text": "500 Specifically, for sample $x_{1}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tau=k[\\mathbf{x}-\\mathbf{x}_{1};\\forall\\,\\mathbf{y}^{i}]\\,{=}\\,\\frac{P\\left(\\mathbf{y}\\right)\\,-k\\left(\\mathbf{x}\\right)\\,\\mathbf{x}\\,{=}\\,\\mathbf{x}\\,{\\mathrm{~\\frac{y}{2}}}\\,{\\mathrm{e}}^{\\mathrm{i}\\left(\\mathbf{x}\\right)}}{\\sum_{i\\in\\mathcal{I}^{n}}P\\left(\\mathbf{y}\\right)\\,{=}\\,{\\mathrm{~\\frac{1}{\\sqrt{2}}}}\\,\\mathbf{y}\\,{\\mathrm{~\\frac{1}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{1}{\\sqrt{2}}}}\\,}&{{=}\\,\\frac{p_{k}^{i}}{\\sum_{i=1}^{k}P_{i}^{i}}\\,,}\\\\ {\\tau=k[\\mathbf{x}-\\mathbf{x}_{1};\\forall\\,\\mathbf{y}^{i}\\,{\\mathrm{~\\frac{1}{\\sqrt{6}}}})\\,{=}\\,\\frac{P\\left(\\mathbf{y}\\right)\\,{=}\\,k\\left(\\mathbf{x}\\right)\\,\\mathbf{x}\\,{=}\\,\\mathbf{x}\\,{\\mathrm{~\\frac{1}{\\sqrt{2}}}}\\,{\\mathrm{~\\frac{y}{2}}}\\,{\\mathrm{~\\frac{1}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{2}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{3}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{3}{\\sqrt{6}}}}\\,}&{{\\frac{4\\mathbf{x}\\,{=}\\,\\mathbf{y}^{i}\\,{\\mathrm{T}_{k}^{i}}}{\\sum_{i\\in\\mathcal{I}^{n}}P_{i}^{i}}}}\\\\ {P(\\mathbf{x}=\\mathbf{x}_{1};\\forall\\,\\mathbf{y}^{i}\\,{\\mathrm{\\frac{1}{\\sqrt{6}}}})\\,{=}\\,\\frac{\\sum_{i\\in\\mathcal{I}^{n}}P\\left(\\mathbf{x}\\,{=}\\,\\mathbf{x}_{1},\\forall\\,\\mathbf{y}^{i}\\,{\\mathrm{\\forall}}\\,\\mathbf{y}^{i}\\,{\\mathrm{\\forall}}\\,{\\mathrm{\\frac{3}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{5}{\\sqrt{6}}}}\\,{\\mathrm{~\\frac{3}{\\sqrt{6}}}}\\right)}{\\sum_{i\\in\\mathcal{I}^{n}\\setminus}P\\left(\\mathbf{x}\\,{=}\\,\\mathbf{x}_{1},\\forall\\,{=}\\,\\mathbf{y}^{i} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "501 Simply changing the subscript leads us to the formulations for sample $\\pmb{x}_{2}$ . To summarize, wrapping   \n502 the above together, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle P(\\mathbf{y}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})=[\\frac{p_{1}}{\\sum_{i=1}^{A}p_{i}},...,\\frac{p_{A}}{\\sum_{i=1}^{A}p_{i}}],}\\\\ &{\\displaystyle P^{n}(\\mathbf{y}|\\mathbf{x}=x;\\mathbf{y}\\in\\mathcal{Y}^{i n})=[\\sum_{i=1}^{A+B}p_{i}T_{i1},...,\\sum_{i=1}^{A+B}p_{i}T_{i A}],}\\\\ &{\\displaystyle P(\\mathbf{x}=x_{1};\\mathbf{y}\\in\\mathcal{Y}^{i n})=\\sum_{i=1}^{A}p_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "503 We here omit the sample subscript and abbreviate the proportional symbol for simplicity. With   \n504 eq. (18), eq. (19) and eq. (23), we can then compute and compare $\\Delta E_{x}$ in both fitted case and   \n505 memorized case: ", "page_idx": 14}, {"type": "text", "text": "506 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\Big[\\Delta E_{x}=\\operatorname*{max}[p_{1},...,p_{A}]-p_{\\mathrm{arg\\,max}[\\sum_{i=1}^{A+B}p_{i}T_{i1},...,\\sum_{i=1}^{A+B}p_{i}T_{i A}]}\\quad(F i t t e d\\,c a s e)}{\\Bigg]\\Delta E_{x}=\\operatorname*{max}[p_{1},...,p_{A}]-\\sum_{i=1}^{A}(p_{i}\\cdot\\sum_{j=1}^{A+B}p_{j}T_{j i})\\quad(M e m o r i z e d\\,c a s e)}\\Bigg.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "507 D Full proof of theorem 3.5 and theorem 3.6 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "508 Error rate inflation comparison s.t. same noise ratio To ensure a fair comparison, in this work,   \n509 we focus on the impact of different label noise given the same noise ratio - modifying $O_{x}$ and $C_{x}$   \n510 while analyzing the trend of $\\Delta E_{x}$ . Specifically, for above mentioned $x_{1}$ and $\\pmb{x}_{2}$ , we further assume: ", "page_idx": 14}, {"type": "equation", "text": "$$\nO_{x_{1}}+C_{x_{1}}=O_{x_{2}}+C_{x_{2}}=\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 which leads us to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=A+1}^{A+B}p_{i}^{1}+\\sum_{i=1}^{A}\\sum_{j=1,j\\neq i}^{A}T_{i j}^{1}p_{i}^{1}=\\sum_{i=A+1}^{A+B}p_{i}^{2}+\\sum_{i=1}^{A}\\sum_{j=1,j\\neq i}^{A}T_{i j}^{2}p_{i}^{2}\\longrightarrow\\sum_{i=1}^{A}T_{i i}^{1}p_{i}^{1}=\\sum_{i=1}^{A}T_{i i}^{2}p_{i}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 Please note, here the clean conditional probability is considered as known and fixed, while eq. (27)   \n513 restricts the values of the noise transition matrix $T^{1}$ and $T^{2}$ , given specific clean conditional   \n514 probability. We then analyze and compare the error rate inflation in both conditions. ", "page_idx": 14}, {"type": "text", "text": "515 D.1 Proof of theorem 3.5 \u2014 Open-set noise vs Closed-set noise ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "516 In this section, we try to compare open-set noise and closed-set noise. Without loss of generality, we   \n517 consider: ", "page_idx": 15}, {"type": "equation", "text": "$$\nO_{x_{1}}>O_{x_{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "518 Intuitively speaking, sample $x_{1}$ is more affected by open-set noise compared to sample $\\pmb{x}_{2}$ , thus   \n519 corresponding to the interested \u2018open-set noise\u2019.   \n520 As clarified by the toy example in section 3.4.1, without extra regularizations, the noise transition   \n521 matrix is not identifiable. We thus consider a simple compromise situation - in most classification   \n522 problems, the majority of samples (with a high probability) belong to a specific class exclusively with   \n523 high probability. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Let us denote: ", "page_idx": 15}, {"type": "equation", "text": "$$\na=\\arg\\operatorname*{max}_{i}P(\\mathbf{y}=i|\\mathbf{x}=\\mathbf{x}_{1};\\mathbf{y}\\in\\mathcal{y}^{a l l})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\arg\\operatorname*{max}_{i}P(\\mathbf{y}=i|\\mathbf{x}=\\mathbf{x}_{2};\\mathbf{y}\\in\\mathcal{Y}^{a l l}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We assume : ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{a}^{1}\\to1,\\{p_{i}^{1}\\to0\\}_{i\\neq a},p_{b}^{2}\\to1,\\{p_{i}^{2}\\to0\\}_{i\\neq b},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nO_{x_{1}}=\\sum_{i=A+1}^{A+B}p_{i}^{1},\\;O_{x_{2}}=\\sum_{i=A+1}^{A+B}p_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "524 With eq. (28), we easily infer that: $a\\in\\mathcal{V}^{o u t}$ while $b\\in\\mathcal{V}^{i n}$ . Intuitively speaking, $x_{1}$ is an open-set   \n525 noise, with its clean conditional probability concentrated on one of the outlier classes, and vice versa   \n526 for $\\pmb{x}_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "With eq. (27), we further have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{A}T_{i i}^{1}p_{i}^{1}\\approx\\sum_{i=1}^{A}T_{i i}^{1}\\times0\\approx0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{A}T_{i i}^{2}p_{i}^{2}\\approx\\sum_{i=1,i\\neq b}^{A}T_{i i}^{2}\\times0+T_{b b}^{2}\\times1\\approx T_{b b}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "527 Thus we have: $T_{b b}^{2}\\approx0$ , which enables us to analyze and compare $\\Delta E_{{\\pmb x}_{1}}$ and $\\Delta E_{{\\bf x}_{2}}$ : ", "page_idx": 15}, {"type": "text", "text": "528 Fitted case In this case, according to eq. (24), we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x_{1}}=\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-p_{\\mathrm{arg\\,max}[\\sum_{i=1}^{A+B}p_{i}^{1}T_{i1}^{1},...,\\sum_{i=1}^{A+B}p_{i}^{1}T_{i A}^{1}]}}\\\\ &{\\qquad<\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-\\operatorname*{min}[p_{1}^{1},...,p_{A}^{1}]}\\\\ &{\\qquad\\xrightarrow{p_{a}^{1}\\to1,\\{p_{i}^{1}\\to0\\}_{i\\neq a},a\\in\\mathcal{Y}^{o u t}}}\\\\ &{\\qquad\\approx0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "529 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x_{2}}=\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-p_{\\mathrm{arg\\,max}[\\sum_{i=1}^{A+B}p_{i}^{2}T_{i1}^{2},...,\\sum_{i=1}^{A+B}p_{i}^{2}T_{i A}^{2}]}}\\\\ &{\\qquad\\xrightarrow{[\\sum_{i=1}^{A+B}p_{i}^{2}T_{i1}^{2},...,\\sum_{i=1}^{A+B}p_{i}^{2}T_{i A}^{2}]\\approx[T_{a1}^{2},T_{a2}^{2},...,\\stackrel{b}{\\overbrace{0\\mathrm{,~}},...,T_{a A}^{2}]}}}\\\\ &{\\qquad=p_{b}^{2}-p_{n}^{2}}\\\\ &{\\qquad\\xrightarrow{p_{b}^{2}\\rightarrow1,\\{p_{i}^{2}\\rightarrow0\\}_{i\\neq b},b\\in\\mathcal{Y}^{i n},n\\neq b}}\\\\ &{\\approx1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "530 Memorized case In this case, according to eq. (25), we similarly have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta E_{x_{1}}=\\mathrm{max}[p_{1}^{1},...,p_{A}^{1}]-\\sum_{i=1}^{A}(p_{i}^{1}\\cdot\\sum_{j=1}^{A+B}p_{j}^{1}T_{j i}^{1})\\approx0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta E_{x_{2}}=\\mathrm{max}[p_{1}^{2},...,p_{A}^{2}]-\\sum_{i=1}^{A}(p_{i}^{2}\\cdot\\sum_{j=1}^{A+B}p_{j}^{2}T_{j i}^{2})\\approx1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "531 We wrap up above for theorem D.2: ", "page_idx": 16}, {"type": "text", "text": "Theorem D.1 (Open-set noise $\\nu s$ Closed-set noise). Let us consider sample $x_{1}$ , $\\pmb{x}_{2}$ fulfliling eq. (26) and eq. (28) - compared to $\\pmb{x}_{2}$ , $x_{1}$ is considered as more prone to open-set noise. Let us denote $a\\,=\\,\\arg\\operatorname*{max}_{i}P(\\mathbf{y}\\,=\\,i|\\mathbf{x}\\,=\\,\\pmb{x}_{1};\\mathbf{y}\\,\\in\\,\\mathcal{Y}^{a l l})$ and $b\\,=\\,\\arg\\operatorname*{inax}_{i}P(\\mathbf{y}^{\\,}=\\,i|\\mathbf{x}\\,=\\,\\pmb{x}_{2};\\mathbf{y}\\,\\in\\,\\mathcal{Y}^{a l l})$ ), we assume (with a high probability): $\\dot{p}_{a}^{1}\\rightarrow1,\\{p_{i}^{1}\\rightarrow0\\}_{i\\neq a}$ and $p_{b}^{2}\\to\\bar{1}$ , $\\{p_{b}^{2}\\rightarrow0\\}_{i\\neq b}$ . Then, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}<\\Delta E_{{\\pmb x}_{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "532 in both fitted case and memorized case. ", "page_idx": 16}, {"type": "text", "text": "533 D.2 Derivation of theorem 3.5 \u2014 \u2018hard\u2019 open-set noise vs \u2018easy\u2019 open-set noise ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "534 In this part, we try to analyze and compare \u2018hard\u2019 open-set noise with \u2018easy\u2019 open-set noise. For   \n535 better clarification, we repeat here the essential statements: ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{o u t}^{1}=T^{e a s y}=\\left[\\!\\!\\begin{array}{c c c}{{\\frac{1}{A}}}&{{\\dots}}&{{\\frac{1}{A}}}\\\\ {{\\dots}}&{{\\dots}}&{{\\dots}}\\\\ {{\\frac{1}{A}}}&{{\\dots}}&{{\\frac{1}{A}}}\\end{array}\\!\\!\\right]_{\\mathtt{B}\\times\\mathtt{A}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "536 and ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{o u t}^{2}=T^{h a r d}=\\left[\\!\\!\\begin{array}{c c c}{{0}}&{{\\ldots}}&{{1}}\\\\ {{\\ldots}}&{{\\ldots}}&{{\\ldots}}\\\\ {{1}}&{{\\ldots}}&{{0}}\\end{array}\\!\\!\\right]_{\\scriptscriptstyle{\\mathrm{B}\\times\\mathrm{A}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "537 and ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{i n}^{1}=T_{i n}^{2}={\\bf I}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Especially, for $T^{e a s y}$ , we have $\\begin{array}{r}{T_{i j}=\\frac{1}{A}}\\end{array}$ everywhere; for $T^{h a r d}$ , we denote as $H_{i}:\\{\\mathrm{arg}_{j}(T_{j i}^{h a r d}=$ $1)\\}_{i=1}^{A}$ the set of corresponding outlier classes $j\\in\\mathcal{V}^{o u t}$ confused to inlier class $i\\in\\mathcal{V}^{i n}$ . We also have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n[p_{1}^{1},...,p_{A}^{1},...,p_{A+B}^{1}]=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "538 ", "page_idx": 16}, {"type": "text", "text": "539 Fitted case In this case, according to eq. (24), for sample $x_{1}$ with \u2018easy\u2019 open-set noise, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x_{1}}=\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-p_{\\arg\\operatorname*{max}[\\sum_{i=1}^{A+B}p_{i}^{1}T_{i1}^{1},...,\\sum_{i=1}^{A+B}p_{i}^{1}T_{i A}^{1}]}}\\\\ &{\\qquad=\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-p_{\\arg\\operatorname*{max}[p_{1}^{1}+\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}^{1},...,p_{A}^{1}+\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}^{1}]}}\\\\ &{\\qquad=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "540 and, for sample $\\pmb{x}_{2}$ with \u2018hard\u2019 open-set noise, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x_{2}}=\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-p_{\\mathrm{arg\\,max}}[\\sum_{i=1}^{A+B}p_{i}^{2}T_{i1}^{2},...,\\sum_{i=1}^{A+B}p_{i}^{2}T_{i A}^{2}]}\\\\ &{\\qquad=\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-p_{\\mathrm{arg\\,max}}[p_{1}^{2}+\\sum_{b\\in H_{1}}p_{b}^{2},...,p_{A}^{2}+\\sum_{b\\in H_{A}}p_{b}^{2}]}\\\\ &{\\qquad\\in[0,\\,\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-\\operatorname*{min}[p_{1}^{2},...,p_{A}^{2}]].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "541 Memorized case In this case, according to eq. (25), for sample $x_{1}$ with \u2018easy\u2019 open-set noise, we   \n542 have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta E_{x_{1}}=\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-\\displaystyle\\sum_{i=1}^{A}(p_{i}^{1}\\cdot\\displaystyle\\sum_{j=1}^{A+B}p_{j}^{1}T_{j i}^{1})}\\\\ &{\\qquad=\\operatorname*{max}[p_{1}^{1},...,p_{A}^{1}]-\\displaystyle\\sum_{i=1}^{A}p_{i}^{1}(p_{i}^{1}+\\displaystyle\\frac{1}{A}\\displaystyle\\sum_{i=A+1}^{A+B}p_{i}^{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "543 and, for sample $\\pmb{x}_{2}$ with \u2018hard\u2019 open-set noise, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\ }&{\\Delta E_{x_{2}}=\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-\\displaystyle\\sum_{i=1}^{A}(p_{i}^{2}\\cdot\\displaystyle\\sum_{j=1}^{A+B}p_{j}^{2}T_{j i}^{2})}&\\\\ {\\ }&{=\\operatorname*{max}[p_{1}^{2},...,p_{A}^{2}]-\\displaystyle\\sum_{i=1}^{A}p_{i}^{2}(p_{i}^{2}+\\displaystyle\\sum_{j\\in H_{i}}p_{j}^{2})}&\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We further have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta E_{x_{1}}-\\Delta E_{x_{2}}=\\sum_{i=1}^{A}p_{i}^{1}\\big(\\sum_{j\\in H_{i}}p_{j}^{1}-\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}^{1}\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\begin{array}{r}{a_{i}=p_{i}^{1},b_{i}=\\sum_{j\\in H_{i}}p_{j}^{1}-\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}^{1}}\\end{array}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}-\\Delta E_{{\\pmb x}_{2}}=\\sum_{i=1}^{A}a_{i}b_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "544 To summarize, we wrap up the above together: ", "page_idx": 17}, {"type": "text", "text": "545 Theorem D.2 (\u2018Hard\u2019 open-set noise vs \u2018easy\u2019 open-set noise). Let us consider sample $\\pmb{x}_{1}$ , $\\pmb{x}_{2}$   \n546 fulfliling eq. (26) and eq. (11). We set the corresponding noise transition matrix as in eq. (33), eq. (34)   \n547 and eq. (35). We further assume $[p_{1}^{1},...,p_{A}^{1},...;p_{A+B}^{1}]=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B}^{2}]$ . Then, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta E_{{\\pmb x}_{1}}\\leq\\Delta E_{{\\pmb x}_{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in fitted case, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta E_{x_{1}}-\\Delta E_{x_{2}}=\\sum_{i=1}^{A}a_{i}b_{i}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "548 in memorized case. Here, $\\begin{array}{r}{a_{i}=p_{i}^{1},b_{i}=\\sum_{j\\in H_{i}}p_{j}^{1}-\\frac{1}{A}\\sum_{i=A+1}^{A+B}p_{i}^{1}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem D.3 (Rearrangement Inequality). For the sequences $a_{1},a_{2},\\ldots,a_{n}$ and $b_{1},b_{2},\\ldots,b_{n}$ , where $a_{1}\\leq a_{2}\\leq\\ldots\\leq a_{n}$ and $b_{1}\\leq b_{2}\\leq\\ldots\\leq b_{n}$ , the rearrangement inequality is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\iota_{1}\\cdot b_{1}+a_{2}\\cdot b_{2}+.\\,.\\,.\\,+a_{n}\\cdot b_{n}\\geq a_{1}\\cdot b_{\\sigma(1)}+a_{2}\\cdot b_{\\sigma(2)}+.\\,.\\,.\\,+a_{n}\\cdot b_{\\sigma(n)}\\geq a_{1}\\cdot b_{n}+a_{2}\\cdot b_{n-1}+.\\,.\\,.\\,+a_{n}\\cdot b_{1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "549 Here, $\\sigma$ denotes a permutation of the indices $1,2,\\ldots,n$ . The leftmost expression corresponds to the   \n550 case where $\\sigma(i)=i$ (identity permutation), and the rightmost expression corresponds to the case   \n551 where $\\sigma(i)=n+1-i$ (reverse permutation). ", "page_idx": 17}, {"type": "text", "text": "552 E Revisiting LNL methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "553 E.1 Revisiting existing LNL methods with open-set noise ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "554 In this section, we further investigate the learning effectiveness of existing LNL methods on previously   \n555 discussed open-set label noise, especially the dominant ones based on sample selection - these methods   \n556 often integrate different regularization terms and off-the-shelf techniques, resulting in state-of-the-art   \n557 performance. In essence, such methods typically include a sample selection module along with a   \n558 robust training module. Here, we briefly denote the clean subset selected by the original method   \n559 as $X_{c l e a n}$ and denote the entire dataset as $X_{a l l}$ . Moreover, we consider integrating the previously   \n560 mentioned open-set detection mechanism into current LNL methods - we denote as $X_{i n}$ an inlier   \n561 subset based on entropy dynamics. Then, maintaining the robust training module unchanged, we   \n562 consider below three different variants (the involved LNL method abbreviated as $\\mathbf{X}$ , the inlier subset   \n563 detection method abbreviated as EntSel):   \n564 \u2022 X: Robust training using $X_{c l e a n}$ , i.e., the original method;   \n565 \u2022 EntSel: Robust training using $X_{i n}$ ;   \n566 \u2022 $\\mathrm{X+EntSel}$ : Robust training using $X_{i n}\\cap X_{c l e a n}$ .   \n567 Specifically, we test with two representative LNL methods with well-maintained open-source imple  \n568 mentations: SSR [5] and DivideMix [11]. Please refer to appendix E.2 for more details. In fig. 6, we   \n569 show results on CIFAR100-O and ImageNet-O. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/1411147107c241c7eb69cd71344c35071b5c099fee0f9c8c87deba706b291090.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Evaluation of directly supervised training with different noise modes/ratios. First row: Closed-set classification accuracy; Second row: Open-set detection ROC AUC. ", "page_idx": 18}, {"type": "text", "text": "570 First, focusing on the classification accuracy of the model, we observe that 1) using EntSel instead of   \n571 the original method leads to a reduction in classification accuracy in the mixed noise scenario (SSR   \n572 vs EntSel and DivideMix vs EntSel); in pure open-set noise only scenarios, there are no obvious   \n573 trends showing differences in different variant models. 2) the classification accuracy for mixed noise   \n574 is significantly lower than that of only open-set noise at the same noise ratio, which further confirms   \n575 that closed-set noise is more harmful than open-set noise. ", "page_idx": 18}, {"type": "text", "text": "576 Furthermore, we demonstrate the performance of this model in detecting open-set samples - the 577 introduction of EntSel significantly enhances the effectiveness of open-set detection, especially 578 when the open-set noise is set to \u2018easy\u2019 mode. This also further confirms our theoretical analysis in section 3.5 and experimental results in section 4.2. ", "page_idx": 18}, {"type": "table", "img_path": "ks0FrTSCnK/tmp/faff4e186c09cf3dfbad784fd980805b74590f8167700785b6bb583a91c7c20b.jpg", "table_caption": ["Table 1: Results on WebVision dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "580 We report results for the WebVision dataset in table 1, reaffirming that combining \u2018EntSel\u2019 with \u2018SSR\u2019   \n581 significantly enhances open-set detection performance. Notably, most open-set noise in WebVision   \n582 seems to arise from factors like text co-occurrence rather than semantic similarity, categorizing   \n583 it more as \u2018easy\u2019 open-set noise. This may explain why EntSel effectively improves open-set   \n584 detection in this context. However, when combining EntSel with DivideMix, both classification   \n585 accuracy and open-set detection decrease, indicating that the robustness of the EntSel method itself is   \n586 questionable. Additionally, simply merging SSR/DivideMix with EntSel using subset intersection (X   \n587 $^+$ EntSel) also leads to a decrease in both classification accuracy and open-set detection performance.   \n588 Finally, it\u2019s worth mentioning that, despite having lower classification accuracy than SSR, DivideMix   \n589 outperforms SSR in open-set detection ROC AUC scores. All above illustrates that simply evaluating   \n590 the classification accuracy may be one-sided. ", "page_idx": 19}, {"type": "text", "text": "591 E.2 Details of involved methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "592 DivideMix [11] Denoting as $\\mathcal{L}=\\{l_{i}\\}_{i=1}^{N}$ the losses of all samples, DivideMix proposes to model   \n593 it (after min-max normalization) with a Gaussian Mixture Model. The probabilities $\\{p_{i}\\}_{i=1}^{N}$ of each   \n594 sample belonging to the component with a smaller mean value are then extracted. Samples with   \n595 probability $p_{i}$ greater than the threshold $\\theta$ are then identified as a \u201cclean\" subset. Link to code:   \n596 https://github.com/LiJunnan1992/DivideMix.   \n597 SSR [5] In contrast to DivideMix, SSR extracts features for each sample and constructs a neigh  \n598 bourhood graph. By computing the nearest neighbour labels for each sample, a pseudo-label   \n599 distribution $\\pmb{p}$ is obtained through a KNN voting process. The consistency $c=p_{y}/p_{m a x}$ between   \n600 this voted distribution and the given noisy label $y$ (logit label) is then calculated. Samples with   \n601 consistency $c$ greater than the threshold $\\theta$ are identified as part of the \u201cclean\" subset. Link to code:   \n602 https://github.com/MrChenFeng/SSR_BMVC2022.   \n603 EntSel We also provide a concise overview of the steps involved in EntSel, following a methodology   \n604 similar to DivideMix. Denoting as $\\mathcal{E}=\\{e_{i}\\}_{i=1}^{N}$ the entropy of all samples\u2019 predictions, we similarly   \n605 model it (after min-max normalization) with a Gaussian Mixture Model. The probabilities $\\{p_{i}\\}_{i=1}^{N}$   \n606 of each sample belonging to the component with a smaller mean value are then extracted. Samples   \n607 with probability $p_{i}$ greater than the threshold $\\theta^{\\prime}$ are then identified as \u201cinlier\" subset.   \n608 Generally, we have a closed-set classifier $\\mathrm{g}$ and an encoder f, and we use it for training based on   \n609 the selected subset. Existing sample selection methods usually rely on an estimated prediction   \n610 and a threshold to help filter clean samples. Our proposed OpenAdaptor focuses on the difference   \n611 between open-set and closed-set samples. When integrating them, we propose two different strategies:   \n612 absorption and exclusion. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "613 E.3 Implementation details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "614 Experiment details For both SSR and DivideMix, we employ model and optimization configu  \n615 rations on the same dataset. Specifically, for CIFAR100-O and ImageNet-O, we utilize the Pres  \n616 ResNet18 model, trained for 300 epochs with a batch size of 128 and a learning rate of 0.02, and a   \n617 cosine annealing schedule was implemented. For the WebVision dataset, we utilize the ResNet18   \n618 model, training for 120 epochs with a reduced batch size of 32. The learning rate is set to 0.01 and   \n619 controlled by a cosine annealing scheduler too. Additionally, a warm-up training phase of 10 epochs   \n620 is implemented in the CIFAR100-O and ImageNet-O experiments, while a 5-epoch warm-up training   \n621 phase is utilized in the WebVision experiment.   \n622 Hyperparameters In all experiments, we set the sample selection threshold $\\theta^{\\prime}=0.5$ for EntSel.   \n623 For SSR, we employ a sample selection threshold $\\theta\\:=\\:1.0$ in all experiments. For DivideMix,   \n624 the sample selection threshold remains constant at $\\theta=0.5$ across all experiments. Both SSR and   \n625 DivideMix incorporate MixUp, and we adhere to the original paper\u2019s choices by setting the MixUp   \n626 coefficient to 4 for experiments on CIFAR100-O and ImageNet-O and to 0.5 for experiments on   \n627 WebVision. Please note, as exploring and comparing these methods are not our focus, we believe   \n628 there exist better hyperparameter settings.   \n629 Robustness of EntSel A smaller $\\theta^{\\prime}$ for EntSel leads to better performance on WebVision - especially   \n630 when EntSel is used with DivideMix. If we set $\\theta^{\\prime}=0.2$ , our classification accuracy increases from   \n631 $62.96\\%$ to $67.2\\%$ , ROC AUC increases from 0.8166 to 0.8599 (table 1). However, we are keen to use   \n632 fixed hyperparameters in all experiments as we emphasize that the hyperparameter robustness is also   \n633 critical for LNL methods. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "634 F More examples of open-set noise in WebVision dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "635 In this section, we present additional examples of open-set noise within the \u2018Tench\u2019 class of the   \n636 WebVision dataset. We trace the origin of web pages containing some open-set noise images.   \n637 Remarkably, we identify that the appearance of the term \u2018Tench\u2019 or related keywords is prevalent   \n638 on the web pages hosting these open-set noise images. We posit that this occurrence is attributed   \n639 to the data collection process on the web. Specifically, in the course of keyword searches and   \n640 crawling for images, instances were inadvertently included due to the presence of keywords in image   \n641 descriptions or accompanying text, such as people with \u2018tench\u2019 in the name, or related fishing tools.   \n642 As highlighted earlier, the prevalent belief in the current LNL community is that real-world noise   \n643 primarily arises from confusion induced by semantic similarity. Consequently, numerous recent   \n644 studies have concentrated on instance-dependent noise and related theoretical analysis. However, our   \n645 findings here indicate that in real-world scenarios, particularly in web-crawled datasets, noise may   \n646 be unrelated to semantics but instead caused by other latent high-dimensional information, such   \n647 as accompanying text here. Addressing such real-world noise requires increased attention and   \nfurther exploration. ", "page_idx": 20}, {"type": "image", "img_path": "ks0FrTSCnK/tmp/841f27a429b54fcd9864013921b29717ec4b28df45e2f3ab84d50445ce038567.jpg", "img_caption": ["Figure 7: Open-set noise examples in class \u2018Tench\u2019 of WebVision dataset with path: $/\\mathtt{g o o g l e/q0001/}$ . The source images are resized to fit the layout. Please note that the web links here are obtained in May 2024, and there is no guarantee that they will always be valid in the future. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "649 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "650 1. Claims   \n651 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n652 paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Justification: We clearly and briefly describe our method and our contributions in the abstract and introduction sections. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "666 2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In section 5 We specifically discuss the potential and limitations of current LNL method in learning with open-set noise. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "5 \u2022 The answer NA means that the paper does not include theoretical results.   \n6 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \nreferenced.   \n8 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n9 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n0 they appear in the supplemental material, the authors are encouraged to provide a short   \n1 proof sketch to provide intuition.   \n2 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n3 by formal proofs provided in appendix or supplemental material.   \n4 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "715 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "716 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n717 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n718 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Justification: All the dataset and implementation details are included in appendix A. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "22 \u2022 The answer NA means that the paper does not include experiments.   \n23 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n24 well by the reviewers: Making the paper reproducible is important, regardless of   \n25 whether the code and data are provided or not.   \n26 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n27 to make their results reproducible or verifiable.   \n28 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n29 For example, if the contribution is a novel architecture, describing the architecture fully   \n30 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n31 be necessary to either make it possible for others to replicate the model with the same   \n32 dataset, or provide access to the model. In general. releasing code and data is often   \n33 one good way to accomplish this, but reproducibility can also be provided via detailed   \n34 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n35 of a large language model), releasing of a model checkpoint, or other means that are   \n36 appropriate to the research performed.   \n37 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n38 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n39 nature of the contribution. For example   \n40 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n41 to reproduce that algorithm.   \n42 (b) If the contribution is primarily a new model architecture, the paper should describe   \n43 the architecture clearly and fully.   \n44 (c) If the contribution is a new model (e.g., a large language model), then there should   \n45 either be a way to access this model for reproducing the results or a way to reproduce   \n46 the model (e.g., with an open-source dataset or instructions for how to construct   \n47 the dataset).   \n48 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n49 authors are welcome to describe the particular way they provide for reproducibility.   \n50 In the case of closed-source models, it may be that access to the model is limited in   \n51 some way (e.g., to registered users), but it should be possible for other researchers   \n52 to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "753 5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "54 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n755 tions to faithfully reproduce the main experimental results, as described in supplemental   \n756 material?   \n57 Answer: [No]   \n758 Justification: The complete codes will be released upon acceptance.   \n759 Guidelines:   \n60 \u2022 The answer NA means that paper does not include experiments requiring code.   \n761 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n62 public/guides/CodeSubmissionPolicy) for more details.   \n63 \u2022 While we encourage the release of code and data, we understand that this might not be   \n64 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n65 including code, unless this is central to the contribution (e.g., for a new open-source   \n66 benchmark).   \n767 \u2022 The instructions should contain the exact command and environment needed to run to   \n68 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n69 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n770 \u2022 The authors should provide instructions on data access and preparation, including how   \n771 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n772 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n773 proposed method and baselines. If only a subset of experiments are reproducible, they   \n774 should state which ones are omitted from the script and why.   \n775 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n776 versions (if applicable).   \n777 \u2022 Providing as much information as possible in supplemental material (appended to the   \n778 paper) is recommended, but including URLs to data and code is permitted.   \n779 6. Experimental Setting/Details   \n780 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n781 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n82 results?   \n783 Answer: [Yes]   \n784 Justification: All the dataset and implementation details are included in appendix A.   \n785 Guidelines:   \n786 \u2022 The answer NA means that the paper does not include experiments.   \n787 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n788 that is necessary to appreciate the results and make sense of them.   \n789 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n90 material.   \n791 7. Experiment Statistical Significance   \n92 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n793 information about the statistical significance of the experiments?   \n794 Answer: [Yes]   \n795 Justification: We conduct multiple runs and report averaged results in most experiments.   \n96 Guidelines:   \n97 \u2022 The answer NA means that the paper does not include experiments.   \n98 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n99 dence intervals, or statistical significance tests, at least for the experiments that support   \n00 the main claims of the paper.   \n01 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n02 example, train/test split, initialization, random drawing of some parameter, or overall   \n03 run with given experimental conditions).   \n04 \u2022 The method for calculating the error bars should be explained (closed form formula,   \ncall to a library function, bootstrap, etc.) ", "page_idx": 23}, {"type": "text", "text": "\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "817 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "18 Question: For each experiment, does the paper provide sufficient information on the com  \n19 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n20 the experiments?   \n21 Answer: [Yes]   \n22 Justification: All experiments are conducted on a private server with 3 RX6000 GPUs.   \n23 Guidelines: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We confirm that the conducted reserach conform with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "844 10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "845 Question: Does the paper discuss both potential positive societal impacts and negative   \n846 societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 24}, {"type": "text", "text": "857 \u2022 The conference expects that many papers will be foundational research and not tied   \n858 to particular applications, let alone deployments. However, if there is a direct path to   \n859 any negative applications, the authors should point it out. For example, it is legitimate   \n860 to point out that an improvement in the quality of generative models could be used to   \n861 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n862 that a generic algorithm for optimizing neural networks could enable people to train   \n863 models that generate Deepfakes faster.   \n864 \u2022 The authors should consider possible harms that could arise when the technology is   \n865 being used as intended and functioning correctly, harms that could arise when the   \n866 technology is being used as intended but gives incorrect results, and harms following   \n867 from (intentional or unintentional) misuse of the technology.   \n868 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n869 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n870 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n871 feedback over time, improving the efficiency and accessibility of ML).   \n872 11. Safeguards   \n873 Question: Does the paper describe safeguards that have been put in place for responsible   \n874 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n875 image generators, or scraped datasets)?   \n876 Answer: [NA]   \n877 Justification: Not applicable.   \n878 Guidelines:   \n879 \u2022 The answer NA means that the paper poses no such risks.   \n880 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n881 necessary safeguards to allow for controlled use of the model, for example by requiring   \n882 that users adhere to usage guidelines or restrictions to access the model or implementing   \n883 safety filters.   \n884 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n885 should describe how they avoided releasing unsafe images.   \n886 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n887 not require this, but we encourage authors to take this into account and make a best   \n888 faith effort.   \n889 12. Licenses for existing assets   \n890 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n891 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n892 properly respected?   \n893 Answer: [Yes]   \n894 Justification: We include all essential information and references to the used datasets in this   \n895 work.   \n896 Guidelines:   \n897 \u2022 The answer NA means that the paper does not use existing assets.   \n898 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n899 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n900 URL.   \n901 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n902 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n903 service of that source should be provided.   \n904 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n905 package should be provided. For popular datasets, paperswithcode.com/datasets   \n906 has curated licenses for some datasets. Their licensing guide can help determine the   \n907 license of a dataset.   \n908 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n909 the derived asset (if it has changed) should be provided.   \n910 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n911 the asset\u2019s creators.   \n912 13. New Assets   \n913 Question: Are new assets introduced in the paper well documented and is the documentation   \n914 provided alongside the assets?   \n915 Answer: [NA]   \n916 Justification: Not applicable.   \n917 Guidelines:   \n918 \u2022 The answer NA means that the paper does not release new assets.   \n919 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n920 submissions via structured templates. This includes details about training, license,   \n921 limitations, etc.   \n922 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n923 asset is used.   \n924 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n925 create an anonymized URL or include an anonymized zip file.   \n926 14. Crowdsourcing and Research with Human Subjects   \n927 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n928 include the full text of instructions given to participants and screenshots, if applicable, as   \n929 well as details about compensation (if any)?   \n930 Answer: [NA]   \n931 Justification: Not applicable.   \n932 Guidelines:   \n933 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n934 human subjects.   \n935 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n936 tion of the paper involves human subjects, then as much detail as possible should be   \n937 included in the main paper.   \n938 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n939 or other labor should be paid at least the minimum wage in the country of the data   \n940 collector. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "42   \n43 Question: Does the paper describe potential risks incurred by study participants, whether   \n44 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n45 approvals (or an equivalent approval/review based on the requirements of your country or   \n46 institution) were obtained?   \n47 Answer: [NA]   \n48 Justification: Not applicable.   \n49 Guidelines:   \n50 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n51 human subjects.   \n52 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n53 may be required for any human subjects research. If you obtained IRB approval, you   \n54 should clearly state this in the paper.   \n55 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n56 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n57 guidelines for their institution.   \n58 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n59 applicable), such as the institution conducting the review. ", "page_idx": 26}]