{"references": [{"fullname_first_author": "Emmanuel Abbe", "paper_title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical analysis of the optimization landscape of neural networks, which is relevant to the current paper's investigation of gradient descent."}, {"fullname_first_author": "Jean Barbier", "paper_title": "Optimal errors and phase transitions in high-dimensional generalized linear models", "publication_date": "2019-00-00", "reason": "This paper establishes the Bayes-optimal error for generalized linear models, which is foundational to understanding the optimal performance in the current paper's setting."}, {"fullname_first_author": "Florent Benaych-Georges", "paper_title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices", "publication_date": "2011-00-00", "reason": "This paper's results on random matrix theory are crucial to analyzing the asymptotic behavior of neural networks in the high-dimensional limit considered in the current paper."}, {"fullname_first_author": "Jo\u00ebl Bun", "paper_title": "Rotational invariant estimator for general noisy matrices", "publication_date": "2016-00-00", "reason": "This paper introduces rotationally invariant estimators for matrix denoising, which are directly used in the GAMP-RIE algorithm proposed in the current paper."}, {"fullname_first_author": "David L. Donoho", "paper_title": "Message-passing algorithms for compressed sensing", "publication_date": "2009-00-00", "reason": "This paper introduces the generalized approximate message passing (GAMP) framework, which forms the basis for the GAMP-RIE algorithm in the current paper."}]}