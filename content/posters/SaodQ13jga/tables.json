[{"figure_path": "SaodQ13jga/tables/tables_6_1.jpg", "caption": "Table 1: GITA Outperforms LLM Baselines. As can be seen in Table 1, GITA consistently outperforms the LLM baselines under the same setting. This underscores its SOTA effectiveness in instruction-based graph reasoning tasks, showing robust capabilities across different parameter scales under both fine-tuning and zero-shot settings. Moreover, under the fine-tuning setting, incorporating the vision modality consistently benefits 7B models. But for the 13B models, the performance of some tasks may degrade. This could be attributed to the greater challenge of aligning representations of the visual and textual modalities in the larger 13B models compared to the 7B models, in the case of only fine-tuning LoRA adapters in the text decoder. We speculate that full training could potentially address this issue. However, we leave this as future work due to resource constraints.", "description": "This table compares the performance of the proposed GITA framework with several popular LLMs (LLaMA2, Vicuna, GPT-4 Turbo) on the GVLQA-BASE dataset.  It shows the accuracy of each model on seven different graph reasoning tasks, under both zero-shot and fine-tuning settings.  The results demonstrate GITA's superior performance, highlighting the benefits of integrating visual information into graph reasoning.", "section": "5.1 Evaluation on the GVLQA-BASE Dataset"}, {"figure_path": "SaodQ13jga/tables/tables_7_1.jpg", "caption": "Table 2: Accuracy (%) comparisons across GVLQA subsets using GITA-7B (VO). \u2191 denotes dramatic performance improvement.", "description": "This table presents the accuracy of the vision-only GITA-7B model across different subsets of the GVLQA dataset.  Each subset represents a different visual augmentation strategy applied to the graphs (layout, node shape, node outline, and edge thickness). The table shows a significant performance improvement when using layout augmentation, highlighting its importance in vision-based graph reasoning.  The average accuracy across all tasks is shown for each subset, allowing for comparison of the effectiveness of different augmentation strategies.", "section": "5.2 Evaluation for the Visual Graph Augmentations"}, {"figure_path": "SaodQ13jga/tables/tables_7_2.jpg", "caption": "Table 3: Accuracy (%) comparisons on real-world datasets under zero-shot and fine-tuning settings, where \u2021 indicates the usage of a checkpoint pretrained in the Cycle task of GVLQA-BASE.", "description": "This table compares the performance of various LLMs (LLaMA2-7B, Vicuna-7B, and GITA-7B) on five real-world datasets for node classification and link prediction tasks.  It shows the accuracy of each model under both zero-shot and fine-tuning settings. The  GITA-7B+ row indicates results when using a checkpoint pre-trained on the Cycle task from the GVLQA-BASE dataset. The table highlights the effectiveness of GITA, particularly when fine-tuned, and showcases improvement through visual graph augmentation.", "section": "5.3 Evaluation on Real-World Datasets"}, {"figure_path": "SaodQ13jga/tables/tables_8_1.jpg", "caption": "Table 4: Accuracy (%) comparisons among dedicated GNNs and GITAs on GVLQA-Base.", "description": "This table compares the performance of the proposed GITA framework with two dedicated Graph Neural Networks (GNNs), namely GCN and SAGE, on the GVLQA-Base dataset.  The comparison highlights the performance of GITA-7B and GITA-13B across seven graph reasoning tasks: Connectivity, Cycle, Topological Sort, Shortest Path, Maximum Flow, Bipartite Graph Matching, and Hamilton Path. The results showcase GITA's competitive performance against specialized GNNs, demonstrating its ability to tackle various graph reasoning tasks effectively.", "section": "5.4 Comparison of GITA with Dedicated Graph Baselines"}, {"figure_path": "SaodQ13jga/tables/tables_9_1.jpg", "caption": "Table 5: Accuracy (%) and Inference Time (in parentheses) for GNNs and GITA on ca-Hepth Dataset with different subgraph sampling hop number k \u2208 {1,2,3,4}.", "description": "This table presents the accuracy and inference time of GCN, SAGE, and GITA-7B models on the ca-HepTh dataset using different subgraph sampling hop numbers (k). It demonstrates the performance of each model with varying subgraph sizes, showing the trade-off between accuracy and computational cost.", "section": "5.2 Evaluation for Visual Graph Augmentations"}, {"figure_path": "SaodQ13jga/tables/tables_15_1.jpg", "caption": "Table 6: Statistics of the GVLQA dataset.", "description": "This table presents the statistical distribution of samples across different subsets of the GVLQA dataset.  Each subset is named, and the number of samples for each of seven graph reasoning tasks (Connectivity, Cycle, Topological Sort, Shortest Path, Maximum Flow, Bipartite Graph Matching, Hamilton Path) is provided.  The \"Total\" column shows the overall number of samples in each subset. The subsets represent different visual augmentation strategies applied to the base dataset.", "section": "C Datasets Statistics"}, {"figure_path": "SaodQ13jga/tables/tables_16_1.jpg", "caption": "Table 7: Average numbers of nodes and edges for each task in GVLQA.", "description": "This table presents the average number of nodes and edges for each of the seven graph reasoning tasks included in the GVLQA dataset.  The tasks are: Connectivity, Cycle, Topological Sort, Shortest Path, Maximum Flow, Bipartite Graph Matching, and Hamilton Path. The data provides insights into the scale and complexity of the graphs used in the dataset for each task.  This is useful context for understanding the difficulty level of the tasks presented to the models.", "section": "5.1 Evaluation on the GVLQA-BASE Dataset"}, {"figure_path": "SaodQ13jga/tables/tables_16_2.jpg", "caption": "Table 8: Statistics of real-world datasets", "description": "This table presents the statistics of five real-world datasets used in the paper's experiments, including the number of nodes and edges, the domain of each dataset, and the average degree of the nodes in the graph.  These datasets represent various graph types and are used to evaluate the generalizability of the proposed GITA framework.", "section": "5.3 Evaluation on Real-World Datasets"}, {"figure_path": "SaodQ13jga/tables/tables_17_1.jpg", "caption": "Table 1: GITA Outperforms LLM Baselines. As can be seen in Table 1, GITA consistently outperforms the LLM baselines under the same setting. This underscores its SOTA effectiveness in instruction-based graph reasoning tasks, showing robust capabilities across different parameter scales under both fine-tuning and zero-shot settings. Moreover, under the fine-tuning setting, incorporating the vision modality consistently benefits 7B models. But for the 13B models, the performance of some tasks may degrade. This could be attributed to the greater challenge of aligning representations of the visual and textual modalities in the larger 13B models compared to the 7B models, in the case of only fine-tuning LoRA adapters in the text decoder. We speculate that full training could potentially address this issue. However, we leave this as future work due to resource constraints.", "description": "This table presents the accuracy comparison results of different models (LLMs and GITA) across various graph reasoning tasks in the GVLQA-BASE dataset, under both zero-shot and fine-tuning settings. It highlights GITA's superior performance compared to baseline LLMs, showing consistent improvement in accuracy across various tasks and model sizes. The observation regarding the performance differences between 7B and 13B models under fine-tuning settings provides insightful discussion regarding the challenges of modality alignment in larger models. ", "section": "5.1 Evaluation on the GVLQA-BASE Dataset"}]