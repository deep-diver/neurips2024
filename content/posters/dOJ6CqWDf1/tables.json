[{"figure_path": "dOJ6CqWDf1/tables/tables_15_1.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "The table presents the performance of different models (Llama-2-7b-chat, Llama-2-70b-chat, Llama-3-8B-Instruct, Llama-3-70B-Instruct, gpt-3.5-turbo-instruct) on the instruction following task using Zephyr guidance.  It compares the performance of the base model (regular decoding), EFT (Emulated Fine-Tuning), BoN (Best-of-N Sampling), and weak-to-strong search using several metrics: Length-controlled win rate (LC WR), Win rate (WR), UltraRM score, and SRM score.  The results are categorized by vocabulary type (same and cross) and model type (black-box).", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_16_1.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction-following experiments using the Zephyr guidance.  It compares the performance of several methods: a baseline (Base), Emulated Fine-Tuning (EFT), Best-of-N Sampling (BON), and Weak-to-Strong Search (CBS). The table shows length-controlled win rates (LC WR) and raw win rates (WR) against gpt-4-turbo, as well as scores from two other reward models (UltraRM-13b and Starling-RM-34B). The results demonstrate the effectiveness of Weak-to-Strong Search in improving the performance of large language models.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_17_1.jpg", "caption": "Table 2: Vanilla beam search [20], without external guidance, shows limited improvements over regular decoding. 'w/ beam search (16)' denotes beam search with a beam width of 16. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction following experiments using different large language models. The baseline is regular decoding without any external guidance. Then, the performance of vanilla beam search (beam width =16) is tested and compared.  Finally, weak-to-strong search is applied to see if it improves the performance.  The results are measured by length-controlled win rates (LC WR) and raw win rates (WR) against GPT-4-turbo.  Additionally, two other reward models, UltraRM-13b and Starling-RM-34B, are used for evaluating the model responses.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_18_1.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction-following experiments using the Zephyr guidance.  It compares several methods, including weak-to-strong search, against baselines (regular decoding, best-of-N sampling, and emulated fine-tuning) on various instruction-tuned language models.  The metrics used are length-controlled win rate (LC WR), raw win rate (WR), and scores from two reward models (UltraRM-13b and Starling-RM-34B).  The table is categorized by whether the models use the same or a different vocabulary compared to the small guidance models and whether the large model is a black box model.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_19_1.jpg", "caption": "Table 4: Instruction following performance under the Tulu guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction-following experiments using the Tulu guidance.  It compares different methods' performance on various instruction-tuned large language models. The metrics used are length-controlled win rates (LC WR), raw win rates (WR), UltraRM scores, and Starling-RM scores.  The table highlights how weak-to-strong search enhances the performance of large models even when using weaker small models as guidance.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_21_1.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction-following experiments using the Zephyr guidance. It compares the performance of several methods, including weak-to-strong search, against baselines such as regular decoding, Best-of-N sampling and Emulated Fine-Tuning, in terms of length-controlled win rates (LC WR) and raw win rates (WR) against GPT-4-turbo.  Additionally, it includes scores from two reward models, UltraRM-13b and Starling-RM-34B.  The table is categorized by whether the models use the same vocabulary or different vocabularies as the small models used for guidance.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_21_2.jpg", "caption": "Table 3: The gold reward achieved for different large pre-trained models under the gpt2 guidance. We show the mean reward (\u00b1 standard deviations) across three random seeds. EFT (3*) denotes the best EFT results among \u03b2\u2208 {1/4,1/2, 1, 2, 4}; Weak-to-strong search (4, 4, 5) denotes CBS with W, K, L = 4, 4, 5; BoN (16) denotes BoN with N = 16.", "description": "This table presents the gold reward achieved by different large pre-trained language models when steered using small tuned and untuned GPT-2 models.  It compares the performance of weak-to-strong search against baselines like Best-of-N sampling and Emulated Fine-Tuning. The results are averaged over three random seeds and show the mean reward with standard deviation.", "section": "5.1 Controlled-Sentiment Generation & Summarization"}, {"figure_path": "dOJ6CqWDf1/tables/tables_21_3.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction following experiments using the Zephyr guidance. It compares the performance of several methods: Base (regular decoding), EFT (Emulated Fine-Tuning), BoN (Best-of-N Sampling), and Weak-to-strong search (the proposed method).  The performance is measured using Length-controlled Win Rate (LC WR), Win Rate (WR), UltraRM score, and Starling-RM score. The table is divided into sections based on the vocabulary of the models (same vocabulary, cross vocabulary, and black box) to show the method's adaptability. ", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_22_1.jpg", "caption": "Table 3: Instruction following performance under the Zephyr guidance. EFT (3*) denotes the best EFT results among \u03b2 \u2208 {0.25, 0.5, 1, 1.5}; Weak-to-strong search (2, 2, 30) denotes CBS with W = 2, K = 2, L = 30. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction-following experiments using the Zephyr guidance.  It compares several methods, including weak-to-strong search, against baselines (regular decoding, Best-of-N sampling, and Emulated Fine-Tuning). The evaluation metrics include length-controlled win rates (LC WR), raw win rates (WR), and scores from two reward models (UltraRM-13B and Starling-RM-34B). The table highlights the performance improvement achieved by weak-to-strong search across various instruction-tuned language models.", "section": "5.2 Instruction Following"}, {"figure_path": "dOJ6CqWDf1/tables/tables_23_1.jpg", "caption": "Table 2: Vanilla beam search [20], without external guidance, shows limited improvements over regular decoding. 'w/ beam search (16)' denotes beam search with a beam width of 16. LC WR and WR denote length-controlled and raw win rates against gpt-4-turbo; URM and SRM denote scores by UltraRM-13b [55] and Starling-RM-34B [56].", "description": "This table presents the results of instruction following experiments using different methods.  It compares the performance of vanilla beam search (without external guidance) against other methods, such as Weak-to-strong search, EFT, and BoN, in terms of length-controlled win rates (LC WR), raw win rates (WR), and scores from two reward models (UltraRM and Starling-RM). The results show that vanilla beam search offers only limited improvement over regular decoding.", "section": "5.2 Instruction Following"}]