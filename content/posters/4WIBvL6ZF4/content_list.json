[{"type": "text", "text": "Dynamic Subgroup Identification in Covariate-adjusted Response-adaptive Randomization Experiments ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanping Li ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jingshen Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Statistics and Data Science Nankai University yanpingli@mail.nankai.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Division of Biostatistics University of California, Berkeley jingshenwang@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Waverly Wei \u2217 Department of Data Sciences and Operations University of Southern California waverly@marshall.usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Identifying subgroups with differential responses to treatment is pivotal in randomized clinical trials, as tailoring treatments to specific subgroups can advance personalized medicine. Upon trial completion, identifying best-performing subgroups\u2013those with the most beneficial treatment effects\u2013is crucial for optimizing resource allocation or mitigating adverse treatment effects. However, traditional clinical trials are not customized for the goal of identifying best-performing subgroups because they typically pre-define subgroups at the beginning of the trial and adhere to a fixed subgroup treatment allocation rule, leading to inefficient use of experimental efforts. While some adaptive experimental strategies exist for the identification of the single best subgroup, they commonly do not enable the identification of the best set of subgroups. To address these challenges, we propose a dynamic subgroup identification covariate-adjusted response-adaptive randomization (CARA) design strategy with the following key features: (i) Our approach is an adaptive experimental strategy that allows the dynamic identification of the best subgroups and the revision of treatment allocation towards the goal of correctly identifying the best subgroups based on collected experimental data. (ii) Our design handles ties between subgroups effectively, merging those with similar treatment effects to maximize experimental efficiency. In the theoretical investigations, we demonstrate that our design has a higher probability of correctly identifying the best set of subgroups compared to conventional designs. Additionally, we prove the statistical validity of our estimator for the best subgroup treatment effect, demonstrating its asymptotic normality and semiparametric efficiency. Finally, we validate our design using synthetic data from a clinical trial on cirrhosis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most clinical trial designs adopt \u201cone-size-fits-all\" rules for treatment assignment and evaluation based on models that ignore patient heterogeneity. This approach is disconnected from medical practice in recent years, where physicians use each patient\u2019s diagnosis and prognostic variables to make personalized, precision medicine treatment decisions. As such, identifying patient subgroups with differential responses to a treatment plays a pivotal role in designing randomized clinical trials [28, 18, 2, 39]. Adaptive clinical trials\u2013that allow randomization probabilities to be adaptively optimized during the trial based on sequentially accrued data\u2013have received much attention due to their potential advantages in promoting precision health. Nevertheless, these trials often involve pre-specifying the patient subgroups to be analyzed [3, 38]. This approach not only leads to inefficient use of experimental efforts but may also reduce the statistical power to detect non-pre-specified subgroups that exhibit high effect sizes. Consequently, there is a pressing need for novel and statistical clinical trial designs for dynamic subgroup identification to address these issues. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel statistical design that dynamically performs subgroup identification in CARA experiments. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. From the design perspective, there are three highlights of our design: (i) Our design facilitates dynamic identification and sequential refinement of the best subgroups within the framework of covariate-adjusted response-adaptive (CARA) experiments (Section 4). This adaptive setting enhances the ability to identify and adjust to the best-performing subgroups over time. (ii) Unlike traditional designs that focus on identifying a single best-performing subgroup, our design is tailored to identify the best set of subgroups with competitive performance (Section 3). This broader objective makes our approach suitable for more general application settings. (iii) Our design demonstrates a higher probability of correctly identifying the best subgroups compared to conventional designs. This efficiency ensures that experimental efforts are utilized more effectively. ", "page_idx": 1}, {"type": "text", "text": "2. From a theoretical perspective, our proposed design strategy accommodates tied treatment effects among candidate subgroups, whereas many existing methods demand that these effects be distinctly separated. Additionally, our algorithm involves resampling in the presence of dependent data structures caused by adaptive treatment allocation, presenting technical challenges in proving statistical validity. We overcome these challenges and show that the best subgroup identified by our design asymptotically converges to the true set of best subgroups (Theorem 1). We also demonstrate that our proposed design strategy converges to the oracle design, which is the optimal design under the setting that the underlying datagenerating distribution is known (Theorem 2). Furthermore, we establish valid statistical inference for the treatment effect of the identified best subgroup and demonstrate that our constructed estimator is semiparametrically efficient (Theorem 3). ", "page_idx": 1}, {"type": "text", "text": "In comparison to the existing literature, our design strategy is closely related to CARA designs. Originating from response-adaptive randomization (RAR) designs [5, 33, 46]. Heuristically, CARA incorporates covariate information along with treatment assignment probabilities based on observed outcomes [14, 32, 30, 13]. Building upon RAR designs, CARA designs utilize both outcome and covariate information to optimize for design objectives. [15] introduces a family of CARA designs that balance efficiency and ethics objectives. Further generalizations to incorporate semiparametric estimates have been explored by [49]. Related developments in CARA designs include [22, 40, 48, 47]. However, conventional CARA designs often consider subgroups to be pre-specified, which is different from our goal of data-adaptively identifying the best subgroups. ", "page_idx": 1}, {"type": "text", "text": "Our proposed method also connects with the literature on subgroup identification. In post-hoc analyses using previously collected data, the first line of work uses data from prior randomized controlled trials. [20] employs clustering techniques based on randomized controlled trial data. A comprehensive review can be found in [23]. The second line of work uses data from prior observational studies. [43] identifies subgroups from existing observational data using machine learning. [36] introduces a causal inference tree approach for subgroup identification, which requires specifying the conditional distribution of the outcome given covariates. Similarly, [45] develops causal inference tree types of algorithm that include double robust estimators for constructing subgroupspecific splitting criteria. Other tree-based approaches for subgroup identification include [17, 24, 16]. Besides the tree-based approaches, [10] develops a subgroup identification method within the value function framework. While post-hoc subgroup analyses do not require new data collection, they often rely on untestable causal assumptions, limiting the credibility of causal conclusions. For instance, the unconfoundedness assumption necessary for causal inference in observational studies assumes random treatment assignment based on observed confounders, but unmeasured confounders can compromise these conclusions. Conversely, in randomized experiments, valid causal conclusions do not depend on such assumptions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the adaptive experiment literature, relatively few methods have been developed for identifying the best subgroups. While [11] is in the adaptive experiment setting and proposes a Bayesian adaptive design that sequentially revises treatment allocation to identify the effective subgroup-treatment pairs, their approach is carried out under the Bayesian framework, relying on the specification of prior distributions and does not provide theoretical justification for the identified subgroups. In contrast, our design is aligned with the frequentist framework and is model-free, avoiding any parametric modeling assumption on the joint distribution of potential outcomes and covariates, and providing theoretical investigations from three aspects. ", "page_idx": 2}, {"type": "text", "text": "As our method aims to identify the best subgroups, it shares some similarities with the multi-armed bandit (MAB) literature. Notable MAB algorithms, such as the Thompson sampling method [35], the $\\epsilon$ -greedy algorithm, and the upper confidence bound algorithm [37], focus on identifying the best arm. Similar to the contextual bandit literature [21, 6, 1], our design also incorporates covariate information. However, our design objective diverges significantly from those in traditional MAB approaches as we seek to identify the best subgroups rather than the best arm. Moreover, recognizing that randomized experiments can be time-consuming, costly, and may result in adverse outcomes for patients if treatments are ineffective, our approach seeks to efficiently allocate experimental resources within a constrained budget to identify the most beneficial subgroups. Specifically, we focus on scenarios in which the cost per experimental unit (e.g., per patient) is significant. For instance, in clinical settings, randomized experiments are often expensive due to the substantial costs of treatment medications. As a result, the primary resource constraint in our framework is the limited number of treatments that can be administered, underscoring the need for a resource-efficient experimental design. ", "page_idx": 2}, {"type": "text", "text": "2 Formulation of CARA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we shall introduce the formulation of our covariate-adjusted response-adaptive randomization (CARA) experiment framework. ", "page_idx": 2}, {"type": "text", "text": "We enroll participants sequentially across $T$ stages, where $T<\\infty$ . Denote the total number of enrolled participants as $\\textstyle N\\;=\\;\\sum_{t=1}^{T}n_{t}$ , where $n_{t}$ is the number of participants in Stage $t$ , for $t=1,\\dots,T$ . The cumulative s ample size up to Stage $t$ is denoted as $\\begin{array}{r}{N_{t}=\\sum_{s=1}^{t}n_{s}}\\end{array}$ . In Stage $t$ , we denote the treatment assignment status of participant $i$ as $D_{i t}\\in\\{0,1\\}$ , $i=1,\\dots,n_{t}$ , where $D_{i t}=1$ denotes the treatment arm, and $D_{i t}=0$ denotes the control arm. The observed outcome is denoted as $Y_{i t}\\in\\mathbb{R}$ . We follow the Neyman-Rubin causal model [27, 34] to define $Y_{i t}(d)$ as the potential outcome we would have observed if participant $i$ receives treatment $d$ at Stage $t$ , for $d\\in\\{0,1\\}$ . The observed outcome can then be represented as ", "page_idx": 2}, {"type": "equation", "text": "$$\nY_{i t}=D_{i t}Y_{i t}(1)+(1-D_{i t})Y_{i t}(0),\\quad i=1,\\dots,n_{t},\\quad t=1,\\dots,T.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We assume that the outcomes are observed without delay, and their underlying distributions do not shift over time [14]. ", "page_idx": 2}, {"type": "text", "text": "In CARA experiments, covariate information is also available to practitioners. We denote the covariate information for participant $i$ as $X_{i t}\\in\\mathbb{R}$ and assume the covariate space $\\mathcal{X}$ can be partitioned into $m$ regions, denoted as $\\{\\bar{S}_{j}\\}_{j=1}^{m}$ . In clinical settings, each partition of the sample space is commonly referred to as a subgroup [3, 19, 44]. We denote the number of subjects enrolled in subgroup $j$ at Stage $t$ as $\\begin{array}{r}{n_{t j}=\\overline{{\\sum_{i=1}^{n_{t}^{-}}\\mathbb{1}}}(X_{i t}\\!\\in\\!S_{j})}\\end{array}$ and the cumulative sample size for group $j$ up to Stage $t$ is $\\begin{array}{r}{N_{t j}=\\sum_{s=1}^{t}n_{s j}}\\end{array}$ . Denote the total number of subjects enrolled in subgroup $j$ as $N_{j}=N_{T j}$ . ", "page_idx": 2}, {"type": "text", "text": "As we are interested in assessing the effectiveness of the treatment in each subgroup, we define the subgroup average treatment effect as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tau_{j}=\\mathbb{E}[Y_{i t}(1)-Y_{i t}(0)|X_{i t}\\in S_{j}],\\quad j=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Due to the adaptive nature of CARA experiments, practitioners can sequentially revise treatment allocation based on outcome and covariate information accumulated during the experiment. Formally, we define the treatment assignment probability for participants in subgroup $j$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\ne_{t j}=\\mathbb{P}(D_{i t}=1|X_{i t}\\in S_{j},\\pmb{\\mathscr{H}}_{t-1}),\\quad t=1,\\dots,T,\\quad j=1,\\dots,m,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathscr{H}_{t-1}=\\{(Y_{i s},D_{i s},X_{i s})_{i=1}^{n_{s}}\\}_{s=1}^{t-1}$ denotes the historical information up to Stage $t-1$ . In CARA experiments, we aim to dynamically revise $e_{t j}$ to reach desired design goals, which shall be introduced in the following section. ", "page_idx": 3}, {"type": "text", "text": "3 Design objective for best subgroups identification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In real-world applications, suppose we start with $m$ subgroups, and practitioners may only aim to find subgroups with the largest treatment effects. It is possible that the best-performing subgroup is not unique. For example, the FORTE trial is a clinical trial aiming to investigate the treatment effect of carfilzomib-based induction\u2013intensification\u2013consolidation regimens on a patient\u2019s progressionfree survival rate [25]. Instead of reporting the single best subgroup with the largest survival rate improvement, the trial reports that both the risk myeloma patient subgroup and high-risk patient subgroup show similar survival rate improvement. ", "page_idx": 3}, {"type": "text", "text": "Without loss of generality, suppose the population subgroup treatment effects follow the order $\\tau_{1}\\geq\\tau_{2}\\geq\\ldots\\geq\\tau_{k}>\\ldots>\\tau_{m}$ . In this case, there are $k$ subgroups exhibiting the largest treatment effects. Thus, it is natural to identify all of the $k$ best subgroups instead of the single best subgroup. ", "page_idx": 3}, {"type": "text", "text": "To describe the set of best-performing subgroups, we introduce the concept of a tie set. We denote the tie set of $\\tau_{1}$ as $\\mathcal{T}_{1}=\\{k:|\\tau_{1}-\\tau_{k}|=o(N^{-1/2}),\\;k=1,\\ldots,m$ which contains the indices of the tied subgroups. This tie set is als\u221ao known as the \u201cnear tie set\" as it captures the subgroups of which the treatment effects lie in the $\\sqrt{N}$ -local neighborhood of $\\tau_{1}$ . We then denote the subgroups that belong to the best set as $S_{T_{1}}$ . Note that when $\\bar{T_{1}}=\\{1\\}$ , $S_{T_{1}}$ is equivalent to $S_{1}$ . The population treatment effect under the best subgroups is defined as $\\tau_{T_{1}}=\\mathbb{E}[\\bar{Y}_{i t}(1)-Y_{i t}(0)|X_{i t}\\in\\cup_{j\\in\\mathcal{T}_{1}}S_{j}]$ . We further assume that $\\tau_{T_{1}}>\\tau_{j}$ , for $j\\not\\in{\\cal T}_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "Our design objective is to correctly identify all the best subgroups. Mathematically, we aim to maximize the correct identification probability: ", "page_idx": 3}, {"type": "equation", "text": "$\\operatorname*{max}_{e}\\;\\mathbb{P}\\big(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\operatorname*{max}_{j\\notin\\mathcal{T}_{1}}\\widehat{\\tau}_{j}\\big)$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Leveraging the large deviation theory [8, 12], we can formulate our design objective as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\colon\\Big\\{\\underset{j\\notin\\mathcal{T}_{1}}{\\mathrm{min}}~G(S_{\\mathcal{T}_{1}},S_{j};e_{1},e_{j}):=\\frac{(\\tau_{j}-\\tau_{T_{1}})^{2}}{2\\left(\\mathbb{V}_{T_{1}}(e_{1})+\\mathbb{V}_{j}(e_{j})\\right)}\\Big\\},\\ \\gets\\ \\mathrm{Maximize~correct~selection~probability}}\\\\ &{\\delta\\leq e_{j}\\leq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{T}_{1}\\,=\\,\\left\\{1,\\ldots,k\\right\\}$ , $1\\,\\leq\\,k\\,<\\,m$ , $\\delta\\ \\in\\ (0,1/2)$ , and $\\mathbb{V}_{\\mathcal{T}_{1}}$ denotes the variance of the best subgroups. Detailed derivations of the equivalence between the correct identification probability and the optimization objective see Appendix (Section C). ", "page_idx": 3}, {"type": "text", "text": "However, in practice, solving this optimization problem is challenging. On the one hand, as we do not have knowledge regarding the membership of subgroups that have the largest treatment effects, we do not have any information regarding $S_{T_{1}}$ . On the other hand, because experimenters have no prior information about the joint distribution of the subgroup treatment effects, $\\tau_{j}$ \u2019s and $\\mathbb{V}_{j}$ \u2019s are also unknown. To address these two practical challenges, we propose a dynamic subgroup identification algorithm that can adaptively identify and merge the set of best subgroups. Additionally, the dynamic subgroup identification method operates seamlessly under a CARA experimental strategy, which allows experimenters to sequentially learn the unknown parameters and adjust the subgroup treatment allocation to attain our design objective. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed design: Dynamic subgroup identification with CARA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we shall illustrate our proposed dynamic subgroup identification strategy with CARA design in Algorithm 1: the design strategy in Section 4.1, encompassing two sub-algorithms (Algorithm 2 and 3), followed by the statistical inference procedure in Section 4.2. We defer several variations of our algorithm to Appendix (Section J). To clarify our design strategy, we also provide a notation table in the Appendix (Section A, Table 2). ", "page_idx": 3}, {"type": "text", "text": "4.1 Proposed design strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Stage 1 (line 1-4), we obtain initial estimates of the group-level treatment effect $\\widehat{\\tau}_{1j}$ and the associated variances $\\widehat{\\mathbb{V}}_{1j}$ . Then, in Stage $t=2,\\ldots,T-1$ (Algorithm 1 line 6\u201311), we perform three tasks: (1) adaptively update treatment allocation and treatment effect estimates, (2) dynamically identify best subgroups, and (3) select hyperparameters that help with dynamic subgroup identification. ", "page_idx": 4}, {"type": "text", "text": "For the first task, we obtain the optimal treatment allocation $\\widehat{e}_{t}^{*}=(\\widehat{e}_{t1}^{*},\\ \\cdot\\cdot\\cdot,\\ \\widehat{e}_{t m_{t}^{*}}^{*})$ by solving the following optimization problem based on sequentially collected data: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{e}\\,\\operatorname*{min}_{\\substack{2\\le j\\le m\\ell}}\\frac{(\\widehat{\\tau}_{t-1,(j)}-\\widehat{\\tau}_{t-1,(1)})^{2}}{2\\big(\\widehat{\\mathbb{V}}_{t-1,(1)}(e_{1})+\\widehat{\\mathbb{V}}_{t-1,(j)}(e_{j})\\big)},\\mathrm{s.t.}\\,\\sum_{l=1}^{m_{t}^{*}}\\widehat{p}_{t l}e_{l}\\le e_{l}\\le e_{l}\\le1-c_{2},\\,l=1,\\ldots,m_{t}^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c_{1}\\in(0,1)$ and $c_{2}\\in\\left(0,1/2\\right)$ , = s=1 i=ts1 1(Xis\u2208S(l)) is the estimated subgroup proportion. The total number of subgroups after merging the identified tie set in Stage $t-1$ is denoted as $m_{t}^{*}=m-|\\widehat{\\mathcal{T}}_{t-1,1}|+1$ and the subscript $(j)$ indexes the subgroup with the $j$ -th largest estimated treatment effect. The procedure of finding $\\widehat{\\mathcal{T}}_{t-1,1}$ shall be illustrated in Algorithm 2. In the set of constraints, the first one is the resource con straint, and the second one is the feasibility constraint. Because of the nonlinear objective function of the optimization problem above, we instead work with its equivalent epigraph representation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{s}_{t}^{*}=\\arg\\operatorname*{max}_{e}\\Big\\{z:\\sum_{l=1}^{m_{t}^{*}}\\widehat{p}_{l}e_{l}\\leq c_{1},\\;c_{2}\\leq e_{l}\\leq1-c_{2},\\operatorname*{min}_{2\\leq j\\leq m_{t}^{*}}\\frac{(\\widehat{\\tau}_{t-1,(j)}-\\widehat{\\tau}_{t-1,(1)})^{2}}{2\\left(\\widehat{\\nabla}_{t-1,(1)}(e_{1})+\\widehat{\\nabla}_{t-1,(j)}(e_{j})\\right)}-z\\geq0\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To calibrate for the complete randomized treatment allocation in Stage 1, we require an additional calibration step in Stage $t$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{e}_{t,(j)}=\\frac{\\left(\\widehat{e}_{t,(j)}^{*}N_{t,(j)}\\right)-N_{t-1,(j)}(1)}{n_{t,(j)}},\\;j=1,\\ldots,m_{t}^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r c l}{n_{t,(j)}\\!}&{=}&{\\!\\sum_{i=1}^{n_{t}}\\mathbb{1}_{(X_{i t}\\in S_{(j)})}}\\end{array}$ , $\\begin{array}{r}{N_{t-1,(j)}(1)\\ =\\ \\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{1}_{(X_{i s}\\in S_{(j)})}D_{i s}}\\end{array}$ , and $N_{t,(j)}\\;\\;=\\;\\;$ $\\textstyle\\sum_{s=1}^{t}n_{s,(j)}$ f. ecWtse  atsh einn o (c8a)t.e treatments with calibrated probability $\\widetilde{e}_{t,(j)}$ and update the subgroup $\\operatorname{Eq}$ ", "page_idx": 4}, {"type": "text", "text": "Dynamic identification of the best subgroups (Algorithm 2). The dynamic subgroup identification algorithm for identifying $\\widehat{\\mathcal{T}}_{t,1}$ at Stage $t$ involves a resampling step that generates bootstrap samples $\\widehat{\\tau}_{t}^{\\circ}$ from a Gaussian distri bution centering around $\\widehat{\\tau}_{t}$ at Stage 1 and a resampling step that generates bootstrap samples with accrued data $\\left\\{\\mathcal{H}_{s}\\right\\}_{s=1}^{t}$ at later stages. In line 9, we identify the best subgroups at Stage $t$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{\\mathcal{T}}_{t,1}=\\{k:w_{k,(1)}^{\\circ}=1,k=1,\\ldots,m_{t}^{*}\\},}\\\\ &{w_{k,(1)}^{\\circ}=\\mathbb{1}\\{-c_{\\mathrm{L}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta}\\leq(\\widehat{\\tau}_{t k}^{\\circ}-\\widehat{\\tau}_{t,(1)}^{\\circ})\\leq c_{\\mathrm{R}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the distance between the upper and lower bounds of the interval for $w_{k,(1)}^{\\circ}$ is of order $n^{\\delta}$ with $\\delta=0.25$ to guarantee the statistical validity of our proposed procedure and balance the trade-off between bias and variance. Note that the dynamic subgroup identification procedure relies on a pair of hyperparameters $(c_{\\mathrm{L}}^{t},c_{\\mathrm{R}}^{t})$ , which are selected data-adaptively. In what follows, we shall illustrate the algorithm for selecting these hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "Hyperparameter selection (Algorithm 3). In line 7 of Algorithm 3, we adopt a bootstrap method and propose several alternative bootstrap methods in Algorithm 5 (line 3) in Appendix (Section J). Algorithm 3 involves a resampling step that generates bootstrap samples $\\widehat{\\tau}_{t}^{*}$ from a Gaussian distribution centering around $\\tau_{t}^{*}$ at Stage 1. In line 2, we compute $\\tau_{t}^{*}=(\\tau_{t1}^{*},\\dots,\\tau_{t,m_{t}^{*}}^{*})^{\\prime}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{t j}^{*}=\\Delta\\cdot\\frac{\\sum_{j=1}^{m_{t}^{*}}\\widehat{\\tau}_{t j}}{m_{t}^{*}}+(1-\\Delta)\\cdot\\widehat{\\tau}_{t j},\\ j=1,\\dots,m_{t}^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta=\\operatorname*{min}\\{0.99,\\frac{\\sum_{j=1}^{m_{t}^{*}}\\widehat{\\mathbb{V}}_{t j}}{N_{t}\\sum_{j=1}^{m_{t}^{*}}\\left(\\widehat{\\tau}_{t j}-\\overline{{\\widehat{\\tau}}}_{t}\\right)^{2}}\\times N_{t}^{\\gamma}\\}}\\end{array}$ and $\\gamma\\in(0,0.2)$ . We impose a lower bound on $\\Delta$ to ensure that $\\Delta$ does not equal to 1. We choose $\\gamma=0.05$ in our simulation studies, and our procedure is shown to be not sensitive to the choice of $\\gamma<1$ . In line 7, we compute $\\widehat{\\tau}_{t}^{*}=(\\widehat{\\tau}_{t1}^{*},\\ldots,\\widehat{\\tau}_{t,m_{t}^{*}}^{*})^{\\prime}$ at Stage $t$ for $t>1$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{t j}^{*}=\\Delta\\cdot\\frac{\\sum_{j=1}^{m_{t}^{*}}\\widehat{\\tau}_{t j}^{\\circ}}{m_{t}^{*}}+(1-\\Delta)\\cdot\\widehat{\\tau}_{t j}^{\\circ},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widehat{\\tau}_{t j}^{\\circ}$ is computed with the bootstrap samples as in Eq (8). In line 10, we compute ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\tau}_{t,(1)}^{*}=\\sum_{k=1}^{m_{t}^{*}}w_{k,(1)}^{*}\\tau_{t k}^{*}\\Bigg/\\sum_{k=1}^{m_{t}^{*}}w_{k,(1)}^{*},\\quad\\mathcal{B}_{t b}(c_{\\mathrm{L}},c_{\\mathsf{R}})=\\mathbb{1}\\big(\\widetilde{\\tau}_{t,(1)}^{*,b}\\leq\\tau_{t,(1)}^{*}\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w_{k,(1)}^{*}=\\mathbb{1}\\{-c_{\\mathrm{L}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta}\\leq(\\widehat{\\tau}_{t k}^{*}-\\widehat{\\tau}_{t,(1)}^{*})\\leq c_{\\mathrm{R}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta}\\}$ V t\u03b4,(1)} and the subscript (1) indexes the subgroup with the largest estimated treatment effect. We also propose a double bootstrap-based alternative hyperparameter selection procedure in Algorithm 4 in Appendix (Section J). Finding the optimal hyperparameters involves minimizing a loss function $L_{t}(c_{\\mathrm{L}},c_{\\tt R})$ at each Stage $t$ , which is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\cal L}_{t}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)=\\frac{1}{2}\\left({\\cal L}_{t0}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)+{\\cal L}_{t1}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where for l = 0, 1, Ltl(cL, cR) = B1 bB=1(1(Btb(cL, cR) = l) \u2212  bB=1 1(BtBb(cL,cR)=l))2. Given a desirable pair of hyperparameters and value, the indicator function $l^{'}$ ) is binary, which roughly follows a Bernoulli distribution with probability $\\frac{\\sum_{b=1}^{B}1(\\mathcal{B}_{t b}(c_{\\mathrm{L}},c_{\\mathrm{R}}){=}l)}{B}$ . Intuitively, the loss function defined in Eq (7) measures the average of squared differences between $\\mathbb{1}(B_{t b}(c_{\\mathrm{L}},c_{\\mathbb{R}})=l)$ and the expected value of Bernoulli( $\\frac{\\sum_{b=1}^{B}\\mathbb{1}(\\overline{{\\mathscr{B}_{t b}}}(c_{\\mathtt{L},c_{\\mathtt{R}}})\\bar{=}l)}{B}\\Big)$ random variables. We would expect that the optimal pair of hyperparameters $(c_{\\mathrm{L}}^{t},c_{\\mathrm{R}}^{t})$ at Stage $t$ minimizes such a loss. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Dynamic subgroup identification CARA design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Stage 1 (Initialization): ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Enroll $n_{1}$ participants, and assign treatments in group $j$ with $\\begin{array}{r}{e_{1j}={\\frac{1}{2}}}\\end{array}$ ;   \n2: Compute $\\widehat{\\tau}_{1j}$ and $\\widehat{\\mathbb{V}}_{1j}$ ;   \n3: Choosing hyperparameters $\\left(c_{\\mathrm{L}}^{1},c_{\\mathsf{R}}^{1}\\right)$ using single bootstrap method (see Algorithm 3);   \n4: Identify tie set and merge tied subgroups with the best subgroup (see Algorithm 2).   \n$t$ ", "page_idx": 5}, {"type": "text", "text": "Stage (Adaptive treatment allocation revision): ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5: for $t\\rightarrow2$ to $T$ do ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "6: With $\\widehat{\\tau}_{t-1,(j)}$ and $\\widehat{\\mathbb{V}}_{t-1,(j)}$ estimated using $\\operatorname{Eq}\\,8$ , solve the optimization problem in Eq (1) to findet\u2217,(j ) ;   \n7: Enroll $n_{t}$ participants and assign treatment with calibrated probability $\\widetilde{e}_{t,(j)}^{*}$ as in Eq (2);   \n8: Update $\\widehat{\\tau}_{t,(j)}$ and $\\widehat{\\mathbb{V}}_{t,(j)}$ as in Eq (8);   \n9: Choosing hyperparameters $(c_{\\mathrm{L}}^{t},c_{\\mathrm{R}}^{t})$ using single bootstrap (see Algorithm 3);   \n10: Identify tie set and merge tied subgroups with the best subgroup (see Algorithm 2).   \n11: Calculate the merged subgroup ATE estimator $\\widehat{\\tau}_{t,\\widehat{\\tau}_{t1}}$ as in $\\operatorname{Eq}$ (9), and its variance estimator $\\widehat{\\mathbb{V}}_{t,\\widehat{\\tau}_{t1}}$ as in Eq (10). ", "page_idx": 5}, {"type": "text", "text": "12: end  for Stage $_T$ (Inference): ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "13: Identify the best tie set $\\widehat{\\mathcal{T}}_{1}$ , and construct two-sided confidence intervals for $\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}$ as in Eq (11). ", "page_idx": 5}, {"type": "text", "text": "4.2 Statistical inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our CARA design also enables making valid statistical inference on the estimated best subgroup treatment effect. We highlight two parts of the statistical inference procedure: (1) estimating unknown ", "page_idx": 5}, {"type": "text", "text": "parameters based on accrued experimental data while dynamically identifying best subgroups, and (2) constructing valid confidence intervals to confirm the estimated best subgroup treatment effect. ", "page_idx": 6}, {"type": "text", "text": "First, based on accumulated experimental data, the subgroup treatment effects and associated variances can be updated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\tau}_{t-1,(j)}=\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{1}_{(X_{i s}\\in S_{\\langle\\xi_{(j)}\\rangle})}D_{i s}Y_{i s}}{N_{t-1,(j)}(1)}-\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{1}_{(X_{i s}\\in S_{\\langle\\xi_{(j)}\\rangle})}(1-D_{i s})Y_{i s}}{N_{t-1,(j)}(0)},\\qquad(8)}\\\\ &{\\widehat{\\gamma}_{t-1,(j)}(e_{j})=\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{1}_{(X_{i s}\\in S_{\\langle\\xi_{(j)}\\rangle})}D_{i s}\\left(Y_{i s}-\\bar{Y}_{t-1,(j)}(1)\\right)^{2}}{N_{t-1,(j)}(1)}\\Bigl(\\frac{e_{j}\\cdot N_{t-1,(j)}}{N_{t-1}}\\Bigr)^{-1}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{1}_{(X_{i s}\\in S_{\\langle\\xi_{(j)}\\rangle})}(1-D_{i s})\\left(Y_{i s}-\\bar{Y}_{t-1,(j)}(0)\\right)^{2}}{N_{t-1,(j)}(0)}\\Bigl(\\frac{(1-e_{j})\\cdot N_{t-1,(j)}}{N_{t-1}}\\Bigr)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{t-1,(j)}(1)\\ =\\ \\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{(j)})}D_{i s},\\ N_{t-1,(j)}(0)\\ =\\ \\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{(j)})}(1-P_{i s}),}\\\\ &{N_{t-1,(j)}\\ =\\ \\ \\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{(j)})},\\ \\ \\bar{Y}_{t-1,(j)}(0)\\ \\ =\\ \\ \\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{(j)})}(1-P_{i s}),}\\\\ &{\\langle N_{t-1,(j)}(0),\\mathrm{and}\\,\\bar{Y}_{t-1,(j)}(1)=\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{(j)})}D_{i s}Y_{i s}/N_{t-1,(j)}(1).}\\end{array}\n$$$D_{i s})Y_{i s}/N_{t-1,(j)}(0)$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Additionally, denote the subgroup proportions as $p_{1},\\ldots,p_{m}$ . Following dynamic subgroup identification at each stage, we merge all subgroups in the $\\widehat{\\mathcal{T}}_{t1}$ and estimate the merged best subgroup treatment effect as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{t,\\widehat{\\tau}_{t1}}=\\sum_{j\\in\\widehat{\\mathcal{T}}_{t1}}p_{j}\\widehat{\\tau}_{t j}\\bigg/\\sum_{j\\in\\widehat{\\mathcal{T}}_{t1}}p_{j},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and estimate the variance of the merged best subgroup treatment effect estimator as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{V}}_{t,\\widehat{\\tau}_{t1}}=\\sum_{j\\in\\widehat{\\mathcal{T}}_{t1}}p_{j}^{2}\\widehat{\\mathbb{V}}_{t j}\\bigg/\\Big(\\sum_{j\\in\\widehat{\\mathcal{T}}_{t1}}p_{j}\\Big)^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Stage $T$ (line 13), we let $\\widehat{\\mathcal{T}}_{1}:=\\widehat{\\mathcal{T}}_{T1}$ , $\\widehat{\\tau}_{\\widehat{\\tau}_{1}}:=\\widehat{\\tau}_{T,\\widehat{T}_{1}}$ and $\\widehat{\\mathbb{V}}_{\\widehat{T}_{1}}:=\\widehat{\\mathbb{V}}_{T,\\widehat{T}_{1}}$ . Lastly, to confirm the estimated best subgroup treatment effect, we constru ct a two-sided level- $\\alpha$ confidence interval as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left[\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}\\pm\\Phi^{-1}(1-\\alpha/2)\\cdot\\sqrt{\\widehat{\\mathbb{V}}_{\\widehat{\\mathcal{T}}_{1}}/N}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Dynamic subgroup identification in Stage $t$ ", "page_idx": 6}, {"type": "text", "text": "Step 1 (Input):   \n1: Input $\\left\\{\\mathcal{H}_{s}\\right\\}_{s=1}^{t},\\widehat{\\tau}_{t j},\\widehat{\\mathbb{V}}_{t}$ $\\widehat{\\mathbb{V}}_{t j}$ , and $(c_{\\tt L}^{t},c_{\\tt R}^{t})$ computed from Algorithm 3. Step $b$ (Bootstr a p):   \n2: for $b\\gets1$ to $B$ do   \n3: if $t=1$ then   \n4: Generate $\\widehat{\\tau}_{1}^{\\circ}$ from ${\\mathcal{N}}\\left({\\widehat{\\tau}}_{1},{\\widehat{\\Omega}}_{n}/n_{1}\\right)$ , where $\\widehat{\\Omega}_{n}=\\mathtt{d i a g}\\left(\\widehat{\\mathbb{V}}_{11},\\ldots,\\widehat{\\mathbb{V}}_{1m}\\right)$ ;   \n5: else if $t>1$ then   \n6: Generate $n_{s}$ resamples randomly with replacement sequentially from each $\\mathcal{H}_{s}$ ;   \n7: Compute $\\widehat{\\tau}_{t j}^{\\circ}$ as in Eq (8) with the bootstrap samples;   \n8: end if   \n9: Identify the best subgroups $\\widehat{\\mathcal{T}}_{t1}$ with the bootstrap samples as in Eq (3).   \n10: end for Step $_B$ (Output):   \n11: Choose $\\widehat{\\mathcal{T}}_{t1}$ with the highest frequency of occurrence and merge subgroups that belong to ", "page_idx": 6}, {"type": "text", "text": "$\\widehat{\\mathcal{T}}_{t1}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical investigation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Assumption 1 (Regularity conditions). $(Y_{i t}(0),Y_{i t}(1),X_{i t})$ are independently identically distributed for $i=1,\\dots,n_{t}$ , $t=1,\\dots,T$ . In addition, $\\mathbb{E}[|Y_{i t}(d)|]^{4}<\\infty$ , $d\\in\\{0,1\\}$ . Lastly, there exists some $\\delta>0$ , such that $\\mathbb{V}[Y_{i t}(d)|X_{i t}\\in S_{j}]\\ge\\delta$ for $d\\in\\{0,1\\}.$ , $j=1,\\dots,m$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 2 (Positivity). The subgroup proportions $\\delta\\leq p_{1},\\ldots,p_{m}\\leq1-\\delta,\\,\\delta\\in(0,1/2).$ ", "page_idx": 7}, {"type": "text", "text": "Assumption 1 says that the potential outcomes have bounded moments and have variability in each subgroup. Assumption 2 says that the subgroup proportions are non-zero in the population. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (Dynamic best subgroup identification consistency). Under Assumptions $^{\\,l}$ and 2, for $j=1,\\dots,m$ and for $\\varepsilon>0$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}\\mathbb{P}\\left(|\\mathbb{1}(j\\in\\widehat{\\mathcal{T}}_{1})-\\mathbb{1}(j\\in\\mathcal{T}_{1})|>\\varepsilon\\right)=0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 1 suggests that our dynamic subgroup identification algorithm correctly identifies the best set of subgroups as the sample size tends to infinity. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Design strategy consistency). Under Assumptions $^{\\,l}$ and 2, for $\\delta\\,>\\,0,$ , as $n_{t}\\,\\rightarrow\\,\\infty$ , $t=1,\\dots,T,$ , for the actual treatment allocation, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(||\\widehat{\\boldsymbol{e}}_{t}-\\boldsymbol{e}^{*}||\\leq\\delta)\\rightarrow1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\boldsymbol{e}^{*}=(e_{1}^{*},\\,\\dots,\\,e_{m}^{*})$ is the optimal treatment allocation. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 says that the actual treatment allocation under our proposed design strategy converges to the optimal treatment allocation asymptotically. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Asymptotic normality). Under Assumptions $^{\\,I}$ and 2, as $N\\rightarrow\\infty$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sqrt{N}(\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}-\\tau_{\\mathcal{T}_{1}})\\rightarrow N\\big(0,\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1}^{*})\\big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{V}}_{\\widehat{T}_{1}}-\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1}^{*})=O_{p}(\\frac{1}{\\sqrt{N}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 3 says that the estimated treatment effect of the identified best subgroups converge to a Gaussian distribution asymptotically, and our variance estimator consistently estimates the asymptotic variance. Theorem 3 also verifies the validity of our constructed confidence interval. ", "page_idx": 7}, {"type": "text", "text": "6 Synthetic real data study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate the performance of our proposed design strategy for identifying the tie set of best-performing subgroups in a synthetic case study using clinical trial data. ", "page_idx": 7}, {"type": "text", "text": "We design our synthetic case study using the dataset from the Mayo Clinic\u2019s trial on primary biliary cirrhosis (PBC), containing clinical biomarkers, treatments, and patient outcomes. PBC is a progressive autoimmune liver disease marked by inflammation and damage to the intrahepatic bile ducts. The Mayo Clinic conducted an extensive trial from 1974 to 1984 to assess the effectiveness of D-penicillamine in treating PBC. This dataset includes 424 patients, encompassing both those who were actively enrolled in the trial and additional cases who consented to provide basic measurements [26]. ", "page_idx": 7}, {"type": "text", "text": "In this case study, we work with a subset $\\ n=312$ ) of patients who participate in the randomized controlled trial. These patients are randomly assigned to one of the two arms: the treatment arm $(n\\,=\\,158)$ , who receive D-penicillamine $D\\,=\\,1$ , and the control arm $n\\,=\\,154\\$ ), who receive placebos $D\\,=\\,0$ . The outcome of interest is the square root of the survival time, defined as the number of days from registration to the earlier death, transplantation, or the time of study analysis. This dataset includes 17 covariates, and we use median imputation to handle missingness in these covariates. We aim to investigate the effectiveness of D-penicillamine in improving liver function and symptoms in five subgroups defined by age (in days): (1) patients with age in [9, 598, 15, 695], (2) age in (15, 695, 17, 082], (3) age in (17, 082, 20, 440], (4) age in (20, 440, 21, 900], (5) age in (21, 900, 28, 650]. We generate synthetic experimental data based on the original dataset, which shall be illustrated in the next section. ", "page_idx": 7}, {"type": "text", "text": "We generate synthetic data that mimic the original PBC dataset. Denote the subgroup membership for each participant $i$ as $\\pmb{\\mathcal{S}}=\\big(\\mathbb{1}_{(X_{i}\\in S_{1})},\\cdot\\cdot\\cdot\\cdot,\\mathbb{1}_{(X_{i}\\in S_{5})}\\big)^{\\intercal}$ . We generate the potential outcome from $\\dot{Y}_{i}(d)|X_{i}\\,\\in\\,S_{j}\\,\\sim$ $\\begin{array}{r l r}{\\mathcal{N}(\\mu_{d j},\\sigma_{d j}^{2}),j}&{=}&{1,\\ldots,5}\\end{array}$ , where $\\mu_{1}\\quad=$ $(42.57,50.44,44.37,44.30,37.71)^{\\intercal}$ , $\\pmb{\\mu}_{0}$ = $(45.34,39.91,45.58,33.42,39.17)^{\\top}\\,,c$ $\\sigma_{1}$ $=$ $(10.85,12.29,12.64,14.28,14.64)^{\\top}\\,,\\sigma_{0}$ ${\\pmb\\sigma}_{0}$ $=$ (11.50, 15.18, 14.57, 13.09, 15.06)\u22ba. The subgroup proportions are $\\begin{array}{r l r l}{\\pmb{p}}&{{}}&{\\pmb{\\mathrm{~\\,~}}}&{{}}\\end{array}$ $(0.28,0.13,0.3\\bar{0},0.\\bar{1}1,\\bar{0}.19)^{\\intercal}$ . We denote the true subgroup treatment effects as $\\begin{array}{r l r}{\\tau}&{=}&{(-2.7\\bar{7},10.53,-1.21,10.89,-1.46)^{\\top}}\\end{array}$ . Therefore, Subgroup 2 and Subgroup 4 are the set of best subgroups with a merged average treatment effect of 10.70. The treatment assignment $D_{i}$ is decided based on different experiment strategies, which shall be discussed later in the section. To generate synthetic data, We mimic CARA experiments where participants are enrolled sequentially across $T$ experimental stages. Here, we set $T=15$ and $n_{t}\\,=\\,400$ , for $t=1,\\ldots,T$ . All experiments are conducted with an Intel Core i7-11800H CPU and $16\\:\\mathrm{GB}$ of RAM. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 3 Hyperparameter selection for dynamic subgroup identification ", "page_idx": 8}, {"type": "text", "text": "Step 1 (Input):   \n1: Input $\\left\\{\\mathcal{H}_{s}\\right\\}_{s=1}^{t},\\widehat{\\tau}_{t j},\\widehat{\\mathbb{V}}$ $\\widehat{\\mathbb{V}}_{t j}$ , and $(c_{\\mathrm{L}},c_{\\mathsf{R}})$ ;   \n2: SCtoemp u(teB $\\tau_{t j}^{*}$ satsr  ia np )E: q  (4). $b$   \n3: for $b\\gets1$ to $B$ do   \n4: if $t=1$ then   \n5: Generate $\\widehat{\\tau}_{1}^{*}$ from $\\mathcal{N}\\left(\\tau_{1}^{*},\\widehat{\\Omega}_{n}/n_{1}\\right)$ where $\\widehat{\\Omega}_{n}=\\mathtt{d i a g}\\left(\\widehat{\\mathbb{V}}_{11},\\ldots,\\widehat{\\mathbb{V}}_{1m}\\right)$   \n6: else if $t>1$ then   \n7: Generate $n_{s}$ resamples randomly with replacement sequentially from each $\\mathcal{H}_{s}$ (the same as Algorithm 2 line 6);   \n8: Compute $\\widehat{\\tau}_{t j}^{\\circ}$ as in Eq (8) with the bootstrap samples, and then $\\widehat{\\tau}_{t j}^{*}$ as in Eq (5);   \n9: end if   \n10: Compute \u03c4 t\u2217,(1) as in Eq (6), and $B_{t b}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)$ as in Eq (7).   \n11: end for Step $_B$ (Output):   \n12: Compute $L_{t}(c_{\\mathrm{L}},c_{\\mathrm{R}})$ as in Eq (7). Choose the pair $\\bar{(c}_{\\mathrm{L}}^{t},c_{\\mathrm{R}}^{t})$ that minimizes $L_{t}(c_{\\mathrm{L}},c_{\\mathrm{R}})$ . We compare our proposed design strategy with   \nthe complete randomization design and two   \nmulti-armed bandit (MAB) algorithms. (1) The complete randomization design refers to a design that fixes $\\begin{array}{r}{e_{t j}=\\frac{1}{2}}\\end{array}$ across all experimental stages, $t=1,\\dots,T,j=1,\\dots,m$ . (2) To customize the MAB algorithms to our setting, for each subgroup, we consider two candidate arms: treatment and control. We set the rewards as the negative asymptotic variance of the treatment effect estimator, i.e., $-\\mathbb{V}_{j}(e_{j})$ . We consider two MAB algorithms: (a) The $\\epsilon$ -greedy algorithm, which aims to balance the exploration and the exploitation efforts [37]. Here, we set $\\epsilon=0.1$ . (b) The upper confidence bound 1 algorithm balances exploration and exploitation using confidence intervals and chooses the arm that maximizes the upper confidence bound on the estimated reward [4]. When customizing MAB algorithms to our setting, we omit the step of identifying and merging tie sets using conventional methods, such as K-means or agglomerative clustering, due to several challenges: the requirement to predefine the number of clusters, the limited applicability of clustering for a small number of subgroups, and the inconsistency of these methods in effectively merging the best subgroups. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "4WIBvL6ZF4/tmp/7c6adcb789b488bc775f01968e8c65bade0404ef98f8d87ae9226de5fcc4ad59.jpg", "img_caption": ["Figure 1: Comparison of the correct selection probability among three conventional methods and our proposed design strategy.\u201cSingle and separate bootstrap\" refers to our proposed design. ", "Complete randomization design Epsilon greedy algorithm Methods Single and separate bootstrap Upper confidence bound 1 algorithm "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "4WIBvL6ZF4/tmp/6aa2e92fb8de468233e9c358569ebbba96525c6ccf6f0b8bbad090394620ca03.jpg", "table_caption": ["Table 1: Comparison among three conventional methods and our proposed design strategy based o\u221an estimated best tie set or subgroup treatment effect (Est), $95\\%$ confidence interval $95\\%$ CI), $\\sqrt{N}$ -scaled bias, and standard deviation (SD). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "To evaluate the performance of different design strategies, we assess the effectiveness of each adaptive experiment strategy from two aspects. First, we compare the correct selection probability of identifying the best subgroups. The correction selection probabil\u221aity can be written as $\\mathbb{P}(\\widehat{\\tau}_{T_{1}}\\geq$ $\\operatorname*{max}_{j\\notin{\\mathcal{T}}_{1}}\\widehat{\\tau}_{j})$ . Second, we compare the $95\\%$ confidence interval, $\\sqrt{N}$ -scaled bias, and standard deviation   of the estimated best subgroup treatment effect. In our resampling procedure, we set $B=2,000$ . The synthetic case study results are summarized in Figure 1 and Table 1. ", "page_idx": 9}, {"type": "text", "text": "First, from Figure 1, our proposed design strategy shows a higher correct selection probability than the complete randomization design and the MAB algorithms. Additionally, the correct selection probability under our proposed design strategy increases with the number of experimental stages. Specifically, our proposed design has a correct selection probability tending to 1 after 15 experimental stages. We also adopt the normalized mutual information as an additional metric to compare our proposed design and three competing methods in the Appendix (Section K.1), which further confirms that our proposed design strategy outperforms the conventional methods. ", "page_idx": 9}, {"type": "text", "text": "Second, from\u221a Table 1, we observe that our proposed design strategy has a smaller standard deviation and smaller $\\sqrt{N}$ -scaled bias, implying that our method is more efficient and less biased. In sum, our proposed adaptive design demonstrates efficient use of experimental data to correctly identify the best-performing subgroups of the three competing methods. We defer additional simulation studies to the Appendix (Section K.1) and an additional synthetic real data study to the Appendix (Section K.3). We also extend our proposed dynamic subgroup identification with CARA to the augmented inverse propensity score weighting (AIPW) estimator. Then we compare our proposed design with AIPW estimator with the three contextual MAB algorithms in the Appendix (Section K.2). ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a dynamic subgroup identification method within the CARA design framework that could significantly advance precision medicine. However, we acknowledge some limitations in our approach. We aim to explore and address these challenges in our future research. ", "page_idx": 9}, {"type": "text", "text": "First, our method assumes that outcomes in CARA experiments are observed immediately at the end of each stage without delay. This assumption, prevalent in adaptive experiments such as [15] and [49], simplifies the modeling process and facilitates quick adjustments based on the latest data. However, in some practical scenarios, outcomes may be observed with delays, complicating the process of adjusting treatment allocations, as highlighted by [31] and [29]. In future work, we plan to revise our design framework and update our estimators to account for the impact of delayed responses on both treatment effects and variance estimators. ", "page_idx": 9}, {"type": "text", "text": "Second, our work addresses scenarios where assigning a treatment is costly, and there is an overall constraint on how many treatments can be deployed. A key challenge arises from the potential misalignment between efficiently estimating the best causal effect and determining the optimal causal decision rule\u2013particularly when the best decision rule is to treat all patients when there is any positive effect. For instance, [9] focuses on ensuring that all individuals who would benefit are accurately assigned to the treatment arm, assuming negligible treatment costs. To maximize the welfare for participating subjects, in future work, we plan to incorporate an \u201cearly stopping\" step which would not only identify the most effective subgroups but also halt enrollment for subgroups exhibiting significantly adverse treatment effects. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127\u2013135. PMLR, 2013. [2] Derek C Angus and Chung-Chou H Chang. Heterogeneity of treatment effect: estimating how the effects of interventions vary across individuals. Jama, 326(22):2312\u20132313, 2021. [3] Susan F Assmann, Stuart J Pocock, Laura E Enos, and Linda E Kasten. Subgroup analysis and other (mis) uses of baseline data in clinical trials. The Lancet, 355(9209):1064\u20131069, 2000.   \n[4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235\u2013256, 2002.   \n[5] Uttam Bandyopadhyay and Atanu Biswas. Allocation by randomized play-the-winner rule in the presence of prognostic factors. Sankhy\u00afa: The Indian Journal of Statistics, Series B, pages 397\u2013412, 1999.   \n[6] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011.   \n[7] Rajeev H Dehejia and Sadek Wahba. Propensity score-matching methods for nonexperimental causal studies. Review of Economics and statistics, 84(1):151\u2013161, 2002.   \n[8] Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications, volume 38. Springer Science & Business Media, 2009.   \n[9] Carlos Fern\u00e1ndez-Lor\u00eda and Foster Provost. Causal decision making and causal effect estimation are not the same... and why it matters. INFORMS Journal on Data Science, 1(1):4\u201316, 2022.   \n[10] Haoda Fu, Jin Zhou, and Douglas E Faries. Estimating optimal treatment regimes via subgroup identification in randomized control trials and observational studies. Statistics in medicine, 35(19):3285\u20133302, 2016.   \n[11] Wentian Guo, Yuan Ji, and Daniel VT Catenacci. A subgroup cluster-based bayesian adaptive design for precision medicine. Biometrics, 73(2):367\u2013377, 2017.   \n[12] Frank Hollander. Large deviations, volume 14. American Mathematical Soc., 2000.   \n[13] F. Hu and L.-X. Zhang. Asymptotic normality of urn models for clinical trials with delayed response. Bernoulli, 10(3):447\u2013463, 2004.   \n[14] Feifang Hu and William F Rosenberger. The theory of response-adaptive randomization in clinical trials. John Wiley & Sons, 2006.   \n[15] Jianhua Hu, Hongjian Zhu, and Feifang Hu. A unified family of covariate-adjusted response-adaptive designs based on efficiency and ethics. Journal of the American Statistical Association, 110(509):357\u2013367, 2015.   \n[16] Joseph Kang, Xiaogang Su, Brian Hitsman, Kiang Liu, and Donald Lloyd-Jones. Tree-structured analysis of treatment effects with large observational data. Journal of Applied Statistics, 39(3):513\u2013529, 2012.   \n[17] Joseph Kang, Xiaogang Su, Lei Liu, and Martha L Daviglus. Causal inference of interaction effects with inverse propensity weighting, g-computation and tree-based standardization. Statistical Analysis and Data Mining: The ASA Data Science Journal, 7(5):323\u2013336, 2014.   \n[18] David M Kent, Ewout Steyerberg, and David Van Klaveren. Personalized evidence based medicine: predictive approaches to heterogeneous treatment effects. Bmj, 363, 2018.   \n[19] K Kubota, Y Ichinose, G Scagliotti, D Spigel, JH Kim, T Shinkai, K Takeda, S-W Kim, T-C Hsia, RK Li, et al. Phase iii study (monet1) of motesanib plus carboplatin/paclitaxel in patients with advanced nonsquamous nonsmall-cell lung cancer (nsclc): Asian subgroup analysis. Annals of oncology, 25(2): 529\u2013536, 2014.   \n[20] Beom S Lee et al. A clustering method to identify who beneftis most from the treatment group in clinical trials. Health Psychology and Behavioral Medicine: an Open Access Journal, 2(1):723\u2013734, 2014.   \n[21] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670, 2010.   \n[22] Yunzhi Lin, Ming Zhu, and Zheng Su. The pursuit of balance: an overview of covariate-adaptive randomization techniques in clinical trials. Contemporary clinical trials, 45:21\u201325, 2015.   \n[23] Ilya Lipkovich, Alex Dmitrienko, and Ralph B D\u2019Agostino Sr. Tutorial in biostatistics: data-driven subgroup identification and analysis in clinical trials. Statistics in medicine, 36(1):136\u2013196, 2017.   \n[24] Wei-Yin Loh, Xu He, and Michael Man. A regression tree approach to identifying subgroups with differential treatment effects. Statistics in medicine, 34(11):1818\u20131833, 2015.   \n[25] Roberto Mina, Pellegrino Musto, Delia Rota-Scalabrini, Laura Paris, Barbara Gamberi, Angelo Palmas, Sara Aquino, Paolo de Fabritiis, Nicola Giuliani, Luca De Rosa, et al. Carflizomib induction, consolidation, and maintenance with or without autologous stem-cell transplantation in patients with newly diagnosed multiple myeloma: pre-planned cytogenetic subgroup analysis of the randomised, phase 2 forte trial. The Lancet Oncology, 24(1):64\u201376, 2023.   \n[26] Paul A Murtaugh, Rolland E Dickson, Gooitzen M Van Dam, Michael Malinchoc, Patricia M Grambsch, Alice L Langworthy, and Chris H Gips. Primary biliary cirrhosis: prediction of short\u2013term survival based on repeated patient visits. Hepatology, 20(1):126\u2013134, 1994.   \n[27] Jerzy S Neyman. On the application of probability theory to agricultural experiments. essay on principles. section 9.(translated and edited by dm dabrowska and tp speed, statistical science (1990), 5, 465-480). Annals of Agricultural Sciences, 10:1\u201351, 1923.   \n[28] Mark J Pletcher and Charles E McCulloch. The challenges of generating evidence to support precision medicine. JAMA Internal Medicine, 177(4):561\u2013562, 2017.   \n[29] D. S. Robertson, K. M. Lee, B. C. L\u00f3pez-Kolkovska, and S. S. Villar. Response-adaptive randomization in clinical trials: from myths to practical considerations. Statistical Science: A Review Journal of the Institute of Mathematical Statistics, 38(2):185, 2023.   \n[30] David S Robertson, Kim May Lee, Boryana C L\u00f3pez-Kolkovska, and Sof\u00eda S Villar. Response-adaptive randomization in clinical trials: from myths to practical considerations. Statistical science: a review journal of the Institute of Mathematical Statistics, 38(2):185, 2023.   \n[31] W. F. Rosenberger, O. Sverdlov, and F. Hu. Adaptive randomization for clinical trials. Journal of Biopharmaceutical Statistics, 22(4):719\u2013736, 2012.   \n[32] William F Rosenberger. Randomized urn models and sequential design. Sequential Analysis, 21(1-2):1\u201328, 2002.   \n[33] William F Rosenberger, AN Vidyashankar, and Deepak K Agarwal. Covariate-adjusted response-adaptive designs for binary response. Journal of biopharmaceutical statistics, 11(4):227\u2013236, 2001.   \n[34] Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 66(5):688, 1974.   \n[35] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. Foundations and Trends\u00ae in Machine Learning, 11(1):1\u201396, 2018.   \n[36] Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A Levine, and Xin Yan. Facilitating score and causal inference trees for large observational studies. Journal of Machine Learning Research, 13:2955, 2012.   \n[37] Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. mit press. Cambridge, MA, 1998.   \n[38] Julien Tanniou, Ingeborg Van Der Tweel, Steven Teerenstra, and Kit CB Roes. Subgroup analyses in confirmatory clinical trials: time to be specific about their purposes. BMC medical research methodology, 16:1\u201315, 2016.   \n[39] U.S. Food and Drug Administration. Assessing and communicating heterogeneity of treatment effects in patient subpopulations: Challenges and opportunities. https://www.fda.gov/science-research/advancing-regulatory-science/ assessing-and-communicating-heterogeneity-treatment-effects-patient-subpopulations-challenges-and, 2024.   \n[40] Sof\u00eda S Villar and William F Rosenberger. Covariate-adjusted response-adaptive randomization for multi-arm clinical trials using a modified forward looking gittins index rule. Biometrics, 74(1):49\u201357, 2018.   \n[41] Waverly Wei, Xinwei Ma, and Jingshen Wang. Adaptive experiments toward learning treatment effect heterogeneity. arXiv preprint arXiv:2312.06883, 2023.   \n[42] Waverly Wei, Yuqing Zhou, Zeyu Zheng, and Jingshen Wang. Inference on the best policies with many covariates. Journal of Econometrics, page 105460, 2023.   \n[43] Jie Xu et al. Machine learning enabled subgroup analysis with real-world data to inform clinical trial eligibility criteria design. Scientific Reports, 13(1):613, 2023.   \n[44] Yanxun Xu, Lorenzo Trippa, Peter M\u00fcller, and Yuan Ji. Subgroup-based adaptive (suba) designs for multi-arm biomarker trials. Statistics in Biosciences, 8(1):159\u2013180, 2016.   \n[45] Jiabei Yang, Issa J Dahabreh, and Jon A Steingrimsson. Causal interaction trees: Finding subgroups with heterogeneous treatment effects in observational data. Biometrics, 78(2):624\u2013635, 2022.   \n[46] Li-Xin Zhang, Feifang Hu, Siu Hung Cheung, and Wai Sum Chan. Asymptotic properties of covariateadjusted response-adaptive designs. The Annals of Statistics, 35(3):1166\u20131182, 2007.   \n[47] Jinglong Zhao. Adaptive neyman allocation. arXiv preprint arXiv:2309.08808, 2023.   \n[48] Wanying Zhao, Wei Ma, Fan Wang, and Feifang Hu. Incorporating covariates information in adaptive clinical trials for precision medicine. Pharmaceutical Statistics, 21(1):176\u2013195, 2022.   \n[49] Hai Zhu and Hongjian Zhu. Covariate-adjusted response-adaptive designs based on semiparametric approaches. Biometrics, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Technical Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We present a notation table, as shown in Table 2, to illustrate the key symbols and their descriptions used in our covariate-adjusted response-adaptive randomization (CARA) experiment framework. ", "page_idx": 13}, {"type": "table", "img_path": "4WIBvL6ZF4/tmp/b44b644cd9690b1c0c1e3f4d73e81cb0b029cc285bb8a331bbd03dae280f8bcd.jpg", "table_caption": ["Table 2: Notation table for the proposed dynamic subgroup identification strategy with CARA design. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Assumptions, lemmas, and corollaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Before discussions, we define some additional notations first. For $t=1,\\dots,T,j=1,\\dots,m$ , and $c>0$ , denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j,c}(1)=\\mathbb{E}[Y_{i t}(1)^{c}|X_{i t}\\in\\mathcal{S}_{j}],\\quad\\mu_{j,c}(0)=\\mathbb{E}[Y_{i t}(0)^{c}|X_{i t}\\in\\mathcal{S}_{j}],}\\\\ &{\\widehat{\\mu}_{t j,c}(1)=\\frac{\\sum_{s=1}^{t}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{j})}D_{i s}Y_{i s}^{c}}{N_{t j}(1)},\\quad\\widehat{\\mu}_{t j,c}(0)=\\frac{\\sum_{s=1}^{t}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in\\mathcal{S}_{j})}(1-D_{i s})Y_{i s}^{c}}{N_{t j}(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assumption 3 (Subgroup treatment effects). For $\\delta\\,\\in\\,(0,0.5)$ , the asymptotic distance between treatment effects of Subgroup $j$ and Subgroup $^{\\,l}$ diverges as $N\\rightarrow\\infty$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nN^{\\delta}\\cdot\\operatorname*{min}_{j\\notin\\mathcal{T}_{1}}|\\tau_{1}-\\tau_{j}|\\to\\infty,\\;\\forall j=1,\\dotsc,m.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Assume Assumption $^{\\,l}$ and 2 holds. Let $p_{j}\\,=\\,\\mathbb{P}(X_{i t}\\,\\in\\,S_{j}|\\mathcal{H}_{t-1})$ be the subgroup proportions, and $\\widehat{e}_{t j}^{*}=\\mathbb{P}(D_{i t}=1|X_{i t}\\in S_{j},\\pmb{\\mathscr{H}}_{t-1})$ be the treatment probabilities. Assume there exists some $\\delta\\in(0,\\bar{1}/2)$ such that for all $j=1,2,\\dots,m$ and $t=1,2,\\dots T$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\delta\\leq\\widehat{e}_{t j}^{*}\\leq1-\\delta.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then for any $j=1,2,\\ldots,m,\\,d=0,1,$ , and any $t$ satisfying $n_{t}\\to\\infty$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{t j,c}(d)=\\mu_{j,c}(d)+O_{p}\\left(\\frac{1}{\\sqrt{N_{t}}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Corollary 1. Let $\\delta\\in(0,1/2)$ be some constant. Assume Assumptions $^{\\,l}$ and 2 hold, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\delta\\leq e\\leq1-\\delta}\\left|\\widehat{\\mathbb{V}}_{t j}(e)-\\mathbb{V}_{j}(e)\\right|=O_{p}\\left(\\frac{1}{\\sqrt{N_{t}}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Corollary 2. Assume Assumptions $^{\\,l}$ and 2 hold. Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{t j}-\\tau_{j}=O_{p}\\left(\\frac{1}{\\sqrt{N_{t}}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem 4. Under Assumptions $^{\\,I}$ and 2, as $N\\rightarrow\\infty$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{N}(\\widehat{\\tau}_{j}-\\tau_{j})\\to\\mathcal{N}\\big(0,\\mathbb{V}_{j}(e_{j}^{*})\\big),\\quad\\widehat{\\mathbb{V}}_{j}-\\mathbb{V}_{j}(e_{j}^{*})=O_{p}(\\frac{1}{\\sqrt{N}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 1, Corollaries 1 and 2, and Theorem 4 are similar to the theoretical results in [41]. We defer readers to [41] for more technical details. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. Under Assumptions 3, for $j\\,=\\,1,\\ldots,m,\\,t\\,=\\,1,\\ldots,T_{\\!}$ , any positive constant $C$ and $\\delta\\in(0,\\frac{1}{2})$ , the following statement holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\left|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right|\\geq N_{t}^{-\\delta}\\cdot C\\right)=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3. Let $\\delta\\in(0,1/2)$ be some constant. Assume Assumptions $^{\\,I}$ and 2 hold, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{t,\\widehat{\\tau}_{t1}}-\\tau_{T1}=O_{p}\\left(\\frac{1}{\\sqrt{N_{t}}}\\right),\\quad\\operatorname*{sup}_{\\delta\\leq e\\leq1-\\delta}\\left\\vert\\widehat{\\Psi}_{t,\\widehat{\\tau}_{t1}}(e)-\\Psi_{T1}(e)\\right\\vert=O_{p}\\left(\\frac{1}{\\sqrt{N_{t}}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Assume Assumptions $^{\\,l}$ and 2 hold. Let $\\mathcal{E}^{*}$ be the solution(s) to the problem as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{e}\\Big\\{\\operatorname*{min}_{k+1\\leq j\\leq m}_{k+1\\leq j\\leq m}\\,G(S_{\\mathcal{T}_{1}},S_{j};e_{1},e_{j})=\\frac{(\\tau_{j}-\\tau_{\\mathcal{T}_{1}})^{2}}{2\\big(\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1})+\\mathbb{V}_{j}(e_{j})\\big)}\\Big\\},}\\\\ &{s.t.\\,\\displaystyle\\sum_{j=k+1}^{m}p_{j}e_{j}\\leq c_{1},\\quad c_{2}<e_{j}<1-c_{2},\\;j=1,\\ldots,m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{1}\\in(0,1).$ , $c_{2}\\in(0,1/2)$ , $\\mathcal{T}_{1}=\\{1,\\ldots,k\\}$ , $1\\leq k<m,$ , and $\\mathbb{V}_{\\mathcal{T}_{1}}$ denotes the variance of the top subgroups. Let ${\\widehat{\\mathcal{E}}}^{*}$ be the optimized treatment allocations solved from the sample analog of this problem. Then for  any $\\delta>0$ , and any $t$ satisfying $N_{t-1}\\to\\infty$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\operatorname*{sup}_{\\hat{e}_{t}^{*}\\in\\hat{\\mathcal{E}}^{*}}\\operatorname*{inf}_{e^{*}\\in\\mathcal{E}^{*}}\\big\\lVert\\widehat{e}_{t}^{*}-e^{*}\\big\\rVert\\leq\\delta\\big)\\rightarrow1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Optimization problem objective function derivations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Because maximizing the correct selection probability is equivalent to minimizing the incorrect selection probability, we formulate the incorrect selection probability as $\\mathbb{P}\\left(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\leq\\operatorname*{max}_{j\\notin\\mathcal{T}_{1}}\\widehat{\\tau}_{j}\\right)$ and denote the cardinality of $\\mathcal{T}_{1}$ as $k$ , $1\\leq k\\leq m-1$ , where $m$ denotes the total number of su bgroups. The incorrect selection probability can be bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\notin{T_{1}}}\\mathbb{P}\\left(\\widehat{\\tau}_{{T_{1}}}\\leq\\widehat{\\tau}_{j}\\right)\\leq\\mathbb{P}\\left(\\widehat{\\tau}_{{T_{1}}}\\leq\\operatorname*{max}_{j\\notin{T_{1}}}\\widehat{\\tau}_{j}\\right)\\leq(m-k)\\cdot\\operatorname*{max}_{j\\notin{T_{1}}}\\mathbb{P}\\left(\\widehat{\\tau}_{{T_{1}}}\\leq\\widehat{\\tau}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the inequality follows from the union bound. For both sides, take the logarithm and divide by $N$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\log\\operatorname*{max}_{j\\notin\\mathcal{T}_{1}}\\left(1-\\mathbb{P}\\left(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\widehat{\\tau}_{j}\\right)\\right)\\leq\\frac{1}{N}\\log\\left(m-k\\right)+\\frac{1}{N}\\log\\left(\\operatorname*{max}_{j\\notin\\mathcal{T}_{1}}\\left(1-\\mathbb{P}\\left(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\widehat{\\tau}_{j}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Additionally, by the large deviation theory [8], there exists a rate function $G(S_{T_{1}},S_{j};e_{1},e_{j})$ , such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}\\frac{1}{N}\\log\\big(1-\\mathbb{P}\\,(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\widehat{\\tau}_{j})\\big)=-G(S_{\\mathcal{T}_{1}},S_{j};e_{1},e_{j}),\\;j\\notin\\mathcal{T}_{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\operatorname*{max}_{j\\notin\\mathcal{T}_{1}}\\Big(1-\\mathbb{P}\\big(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\widehat{\\tau}_{j}\\big)\\Big)}\\end{array}$ is equivalent to $\\begin{array}{r}{\\exp\\Big(-\\operatorname*{min}_{j\\not\\in\\mathcal{T}_{1}}G(\\mathcal{S}_{\\mathcal{T}_{1}},\\mathcal{S}_{j};e_{1},e_{j})\\Big)}\\end{array}$ . We obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\operatorname*{min}_{j\\not\\in T_{1}}G(S_{\\mathcal{T}_{1}},S_{j};e_{1},e_{j})\\leq\\frac{1}{N}\\log\\bigg(1-\\mathbb{P}\\left(\\widehat{\\tau}_{\\mathcal{T}_{1}}\\geq\\operatorname*{max}_{j\\not\\in T_{1}}\\widehat{\\tau}_{j}\\right)\\bigg)}}\\\\ &{}&{\\leq\\frac{\\log\\left(m-k\\right)}{N}-\\operatorname*{min}_{j\\not\\in T_{1}}G(S_{\\mathcal{T}_{1}},S_{j};e_{1},e_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As a result, based on the Gartner-Ellis Theorem [8, ch.2.3], when the sample size tends to infinity, we are able to obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{N\\rightarrow\\infty}{\\operatorname*{lim}}\\displaystyle\\frac{1}{N}\\log\\left(1-\\mathbb{P}\\big(\\widehat{\\tau}_{T_{1}}\\geq\\underset{j\\not\\in\\mathcal{T}_{1}}{\\operatorname*{max}}\\widehat{\\tau}_{j}\\big)\\right)=-\\underset{j\\not\\in\\mathcal{T}_{1}}{\\operatorname*{min}}G(S_{1},S_{j};e_{1},e_{j}),}\\\\ &{}&{G(S_{T_{1}},S_{j};e_{1},e_{j})=\\frac{(\\tau_{j}-\\tau_{T_{1}})^{2}}{2\\big(\\mathbb{V}_{T_{1}}(e_{1})+\\mathbb{V}_{j}(e_{j})\\big)}.\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right)=\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}-\\tau_{j}\\right)+\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t j}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As for the first term $\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}-\\tau_{j}\\right)$ , by Corollary 2, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}-\\tau_{j}\\right)=O_{p}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As for the second term $\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t j}\\right)$ , we consider two situations under $t=1$ and $t>1$ separately. When $t=1,\\widehat{\\tau}_{1j}^{\\circ}$ is generated from ${\\cal N}\\left(\\tau_{1j}^{\\circ},\\widehat{\\mathbb{V}}_{1j}/N_{1}\\right)$ , so we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{N_{1}}\\left(\\widehat{\\tau}_{1j}^{\\circ}-\\tau_{1j}^{\\circ}\\right)=O_{p}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $t>1$ , we compute $\\widehat{\\tau}_{t j}^{\\circ}$ the same way as $\\widehat{\\tau}_{t j}$ with the bootstrap samples instead, so $\\widehat{\\tau}_{t j}^{\\circ}$ and $\\widehat{\\tau}_{t j}$ are consistent and we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t j}\\right)=O_{p}\\left(1\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we obtain for $t\\geq1$ , $\\sqrt{N_{t}}\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right)=O_{p}\\left(1\\right)$ , i.e., for any given $\\varepsilon>0$ , there exists an $M$ , such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left({\\sqrt{N_{t}}}|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}|>M\\right)<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then for any $N_{t}$ such that $N_{t}>\\left(\\frac{M}{C}\\right)^{\\frac{1}{\\frac{1}{2}-\\delta}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}|>N_{t}^{-\\delta}\\cdot C\\right)<\\varepsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\left(|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}|\\geq N_{t}^{-\\delta}\\cdot C\\right)<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the above inequality holds for arbitrary $\\varepsilon>0$ . Hence, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\left(|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}|\\geq N_{t}^{-\\delta}\\cdot C\\right)=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof. ", "page_idx": 15}, {"type": "text", "text": "E Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Recall our definition of the tie set of best-performing subgroups: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{1}=\\{j:|\\tau_{1}-\\tau_{j}|=o(N^{-1/2}),\\;j=1,\\ldots,m\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This suggests that $\\forall j\\in\\mathcal{T}_{1}$ , there exists a sequence $\\delta_{n}\\to0$ as $N\\rightarrow0$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{j}=\\tau_{1}+N^{-\\frac{1}{2}}\\cdot\\delta_{n},\\;\\forall j\\in\\mathcal{T}_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We further define a set of subgroups with treatment effects smaller than those in the set $\\mathcal{T}_{1}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}^{\\mathrm{L}}=\\{j:\\tau_{j}<\\operatorname*{min}_{m\\in\\mathscr{T}_{1}}\\{\\tau_{m}\\},\\ j=1,\\ldots,m\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As for the estimated near tie set of $\\tau_{1}$ in Stage $t$ for $t=1,\\dots,T$ , we have for any $j\\in\\widehat{\\mathcal{T}}_{t1}$ , and a pair of hyperparameters $(c_{\\mathrm{L}}^{t},c_{\\mathrm{R}}^{t})$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-c_{\\mathrm{L}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta}\\leq\\big(\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t,(1)}^{\\circ}\\big)\\leq c_{\\mathsf{R}}^{t}\\cdot N_{t}^{-\\delta}\\cdot\\widehat{\\mathbb{V}}_{t,(1)}^{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\delta=0.25$ . Thus, there exists a positive constant $C$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t,(1)}^{\\circ}|}{N_{t}^{-\\delta}}<C,\\,\\forall j\\in\\widehat{\\boldsymbol{\\mathcal{T}}}_{t1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similar to the proof of Lemma 1 in [42], our proof is composed of the following three steps: ", "page_idx": 16}, {"type": "text", "text": "Step 1. We first show that the subgroup with the largest treatment effect in the resampled statistics falls into the set $\\mathcal{T}_{1}$ with high probability, that is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\check{1}_{t}\\in\\mathcal{T}_{1}\\right)=1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\check{\\boldsymbol{1}}_{t}=\\sum_{j=1}^{m_{t}^{*}}\\boldsymbol{j}\\cdot\\mathbb{1}(\\widehat{\\boldsymbol{\\tau}}_{t j}^{\\circ}=\\widehat{\\boldsymbol{\\tau}}_{t,(1)}^{\\circ})}\\end{array}$ for $t=1,\\dots,T$ . ", "page_idx": 16}, {"type": "text", "text": "Because $\\begin{array}{r}{\\widehat{\\tau}_{t,\\check{1}_{t}}^{\\circ}\\in[\\operatorname*{min}_{j\\in\\mathcal{T}_{1}}\\widehat{\\tau}_{t j},\\operatorname*{max}_{j\\in\\mathcal{T}_{1}}\\widehat{\\tau}_{t j}^{\\circ}]}\\end{array}$ by definition, coupled with the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\operatorname*{max}_{k\\in\\mathcal{T}_{1}^{\\perp}}\\widehat{\\tau}_{t k}^{\\circ}<\\operatorname*{min}_{j\\in\\mathcal{T}_{1}}\\widehat{\\tau}_{t j}^{\\circ}\\leq\\operatorname*{max}_{j\\in\\mathcal{T}_{1}}\\widehat{\\tau}_{t j}^{\\circ}\\right\\}\\subset\\left(\\check{1}_{t}\\in\\mathcal{T}_{1}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under Assumption 3, by Lemma 2, for any $k\\in\\mathcal{T}_{1}^{\\mathrm{L}}$ and $j\\in\\mathcal{T}_{1}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\widehat{\\tau}_{t k}^{\\circ}<\\widehat{\\tau}_{t j}^{\\circ}\\right)=1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "so we can obtain $\\begin{array}{r}{\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\operatorname*{max}_{k\\in{\\mathcal T}_{1}^{\\perp}}\\widehat{\\tau}_{t k}^{\\circ}<\\operatorname*{min}_{j\\in{\\mathcal T}_{1}}\\widehat{\\tau}_{t j}^{\\circ}\\leq\\operatorname*{max}_{j\\in{\\mathcal T}_{1}}\\widehat{\\tau}_{t j}^{\\circ}\\right)=1}\\end{array}$ . Thus we have shown that $\\begin{array}{r}{\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\check{1}_{t}\\in\\mathcal{T}_{1}\\right)=1}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Step 2. We then show, for $j\\not\\in{\\cal T}_{1}$ , and $t=1,\\dots,T$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\mathbb{1}(j\\in{\\widehat{\\mathcal{T}}}_{t1})>\\varepsilon,j\\notin{\\mathcal{T}}_{1}\\right)=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any $\\varepsilon>0$ and $j\\not\\in\\mathcal{T}_{1}$ , we have the following holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}\\left(\\left|1(j\\in\\widehat{\\mathcal{T}}_{t1})\\right|>\\varepsilon\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|1(j\\in\\widehat{\\mathcal{T}}_{t1})\\right|>\\varepsilon|j\\in\\widehat{\\mathcal{T}}_{t1}\\right)\\cdot\\mathbb{P}\\left(j\\in\\widehat{\\mathcal{T}}_{t1}\\right)}\\\\ &{\\quad+\\mathbb{P}\\left(\\left|1(j\\in\\widehat{\\mathcal{T}}_{t1})\\right|>\\varepsilon|j\\notin\\widehat{\\mathcal{T}}_{t1}\\right)\\cdot\\mathbb{P}\\left(j\\notin\\widehat{\\mathcal{T}}_{t1}\\right)}\\\\ &{\\quad\\leq\\mathbb{P}\\left(j\\in\\widehat{\\mathcal{T}}_{t1}\\right)}\\\\ &{\\stackrel{\\mathrm{Defp}}{=}\\mathbb{P}\\left(\\frac{|\\widehat{\\mathcal{T}}_{t j}^{\\alpha}-\\widehat{\\mathcal{T}}_{t,(1)}^{\\alpha}|}{N_{t}^{-\\delta}}<C\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{P}\\left(\\frac{\\left|\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t,\\frac{1}{1}}^{\\circ}\\right|}{N_{t}^{-\\delta}}<C\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|\\widehat{\\tau}_{t j}^{\\circ}-\\widehat{\\tau}_{t,\\frac{1}{1}}^{\\circ}\\right|<N_{t}^{-\\delta}\\cdot C\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|\\left(\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right)-\\left(\\widehat{\\tau}_{t,\\frac{1}{1}}^{\\circ}-\\tau_{\\frac{1}{1}}\\right)+\\left(\\tau_{j}-\\tau_{t,\\frac{1}{1}}\\right)\\right|<N_{t}^{-\\delta}\\cdot C\\right)}\\\\ &{\\leq\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{\\frac{1}{1}}\\right|-N_{t}^{\\delta}\\left|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right|-N_{t}^{\\delta}\\left|\\widehat{\\tau}_{t,\\frac{1}{1}}^{\\circ}-\\tau_{\\frac{1}{1}}\\right|<C\\right)}\\\\ &{=\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\widehat{\\tau}_{t j}^{\\circ}-\\tau_{j}\\right|+N_{t}^{\\delta}\\left|\\widehat{\\tau}_{t,\\frac{1}{1}}^{\\circ}-\\tau_{1}\\right|>N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{1}\\right|+C\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition, for $j\\not\\in\\mathcal{T}_{1}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{\\tilde{1}}\\right|<C\\right)\\leq\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{\\tilde{1}}\\right|<C,\\check{1}\\in\\mathcal{T}_{1}\\right)+\\mathbb{P}\\left(\\check{1}\\notin\\mathcal{T}_{1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{k\\in\\mathcal{T}_{1}}{\\operatorname*{max}}\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{k}\\right|<C,k\\in\\mathcal{T}_{1}\\right)+\\mathbb{P}\\left(\\check{1}\\notin\\mathcal{T}_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under Assumption 3, Lemma 2 and the conclusion in Eq (12) in Step 1 suggest that by letting $N_{t}\\to\\infty$ on both sides, we have the above probability converges to zero. Based on above derivation, we have shown that $\\begin{array}{r}{\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\Im\\bigl(j\\in\\widehat{\\mathcal{T}_{t1}}\\bigr)>\\varepsilon,j\\ \\overset{.}{\\notin}\\mathcal{T}_{1}\\right)=0.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Step 3. We are left to prove that for all $j\\in\\mathcal{T}_{1}$ and $t=1,\\dots,T$ , the following holds for all $\\varepsilon>0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\left|\\mathbb{1}(j\\in{\\widehat{\\mathcal{T}}}_{t1})-1\\right|>\\varepsilon\\right)=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following similar arguments, for a positive constant $C$ , we have for $k,j\\in\\mathcal{T}_{1}$ , the following statement holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}\\left(\\left|1(\\ell\\neq\\widehat{T}_{1})-1\\right|>\\varepsilon\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|1(\\ell\\neq\\widehat{T}_{1})-1\\right|>\\varepsilon\\right)}\\\\ &{\\quad+\\mathbb{P}\\left(\\left|1(\\ell)\\widehat{T}_{1}-1\\right|>\\varepsilon\\right)\\varepsilon\\left(\\widehat{T}_{1}\\right)\\cdot\\mathbb{P}\\left(\\varepsilon\\neq\\widehat{T}_{1}\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\left|1(\\ell)\\widehat{T}_{1}\\right|>\\varepsilon\\right)\\left|>\\varepsilon\\right|\\phi\\left(\\widehat{T}_{1}\\right)\\cdot\\mathbb{P}\\left(\\mu\\neq\\widehat{T}_{1}\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\left|\\phi\\widehat{T}_{1}\\right|\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\frac{\\widehat{T}_{1}\\cdot\\widehat{T}_{2}\\left(1\\right)}{N_{t}^{2}}\\sum_{\\ell=1}^{\\ell}\\right)}\\\\ &{=\\mathbb{P}\\left(\\frac{\\widehat{T}_{0}\\cdot\\widehat{T}_{1}\\right)\\cdot\\mathbb{P}\\left(\\varepsilon\\right)}{N_{t}^{2}}\\le\\mathcal{C}\\right)}\\\\ &{=\\mathbb{P}\\left(\\left|1(\\frac{\\eta}{\\eta}-\\widehat{T}_{1})-\\xi\\right|\\right)+(\\eta-\\eta_{1})\\left(1\\geq N_{t}^{-\\delta}\\cdot\\mathcal{C}\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\left|1_{\\mathcal{C}}^{-\\eta}-\\eta_{1}\\right|+\\left|1_{\\mathcal{C}}^{-\\eta}\\right|+\\left|2\\eta_{1}^{-\\delta}\\right|>\\varepsilon\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\left|1_{\\mathcal{C}}^{-\\eta}-\\eta_{1}\\right|+N_{t}^{-\\delta}\\left|1_{\\mathcal{C}}\\right|-\\eta_{1}\\right|\\left|\\widehat{T}_{1}\\right|-\\eta_{1}\\right|\\geq N_{t}^{-\\delta}\\cdot\\mathcal{C}\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(N_{t}^{-\\eta}\\right)\\left(\\left|1_{\\mathcal{C}}^{-\\eta}-\\eta_{1}\\right|+N_{t}^{-\\delta}\\left|1_{\\mathcal{C}}\\right|\\right)\\left|\\widehat{T}_{1}\\right|-\\eta_{1}\\left|1-\\eta_{1}\\right|\\geq N_{t}^{-\\delta}\\cdot\\mathcal{C}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition of the near-tie set, for $j\\in\\mathcal{T}_{1}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(N_{t}^{\\frac{1}{2}}\\left|\\tau_{j}-\\tau_{\\bar{1}}\\right|<C\\right)\\leq\\!\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{\\bar{1}}\\right|<C,\\bar{1}\\in\\mathcal{T}_{1}\\right)+\\mathbb{P}\\left(\\bar{1}\\notin\\mathcal{T}_{1}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\underset{k\\in\\mathcal{T}_{1}}{\\operatorname*{max}}\\mathbb{P}\\left(N_{t}^{\\delta}\\left|\\tau_{j}-\\tau_{k}\\right|<C,k\\in\\mathcal{T}_{1}\\right)+\\mathbb{P}\\left(\\bar{1}\\notin\\mathcal{T}_{1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again, under Assumption 3, Lemma 2 and the conclusion in Eq (13) we have derived in Step 1, by letting $N_{t}\\to\\infty$ on both side, we have the above probability converges to 1. According to above discussion, we have shown that $\\begin{array}{r}{\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(\\left|\\mathbb{1}(j\\stackrel{\\cdot}{\\in}\\widehat{\\mathcal{T}}_{t1})-\\mathbb{\\dot{1}}\\right|>\\varepsilon\\right)=0.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Combining the results obtained in the aforementioned three steps, we have for $j=1,\\dots,m_{t}^{*}$ and for $\\varepsilon>0$ , the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N_{t}\\to\\infty}\\mathbb{P}\\left(|\\mathbb{1}(j\\in\\widehat{\\mathcal{T}}_{t1})-\\mathbb{1}(j\\in\\mathcal{T}_{1})|>\\varepsilon\\right)=0,\\,\\forall t=1,\\dots,T.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then it is straightforward to get the final results by taking $t=T$ . ", "page_idx": 17}, {"type": "text", "text": "F Proof of Lemma 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\widehat{\\tau}_{t,\\widehat{\\tau}_{t1}}-\\tau_{T_{1}}=(\\widehat{\\tau}_{t,\\widehat{\\tau}_{t1}}-\\widehat{\\tau}_{t,{\\mathcal{T}}_{1}})+(\\widehat{\\tau}_{t,{\\mathcal{T}}_{1}}-\\tau_{T_{1}}),}\\\\ {\\widehat{\\mathbb{V}}_{t,\\widehat{\\tau}_{t1}}(e)-\\mathbb{V}_{T_{1}}(e)=(\\widehat{\\mathbb{V}}_{t,\\widehat{\\tau}_{t1}}(e)-\\widehat{\\mathbb{V}}_{t,{\\mathcal{T}}_{1}}(e))+(\\widehat{\\mathbb{V}}_{t,{\\mathcal{T}}_{1}}(e)-\\mathbb{V}_{T_{1}}(e)).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus this lemma follows directly from Corollary 1, Corollary 2 and Theorem 1. ", "page_idx": 18}, {"type": "text", "text": "G Proof of Lemma 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Denote the oracle objective function as ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(e)=\\operatorname*{min}_{|T_{1}|+1\\le j\\le m}\\,\\frac{(\\tau_{j}-\\tau_{T_{1}})^{2}}{\\mathbb{V}_{T_{1}}(e_{1})+\\mathbb{V}_{j}(e_{j})}=\\operatorname*{min}_{|T_{1}|+1\\le j\\le m}\\,\\frac{(\\tau_{(j-|T_{1}|+1)}-\\tau_{T_{1}})^{2}}{\\mathbb{V}_{T_{1}}(e_{1})+\\mathbb{V}_{(j-|T_{1}|+1)}(e_{j})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second equality follows from regarding the tie set $\\mathcal{T}_{1}$ as a whole after merging the data, and then $\\mathcal{T}_{1}$ ranks 1 and other subgroups rank from 2 to $m-|\\mathcal{T}_{1}|+1$ . Then, the original optimization problem can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{e}\\ f(e),\\qquad\\mathrm{s.t.}\\quad\\sum_{j=|T_{1}|+1}^{m}p_{j}e_{j}\\leq c_{1},\\quad c_{2}\\leq e_{j}\\leq1-c_{2},\\ j=|T_{1}|+1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For notational simplicity, let $\\mathcal{E}$ be the set of candidate allocation rules (i.e., all allocation rules satisfying the constraints). ", "page_idx": 18}, {"type": "text", "text": "Similarly, let $\\widehat{f}_{t}(e)$ be the estimated objective function, that is, with $m_{t}^{*}=m-|\\widehat{\\mathcal{T}}_{t-1,1}|+1,$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{f}_{t}(e)=\\underset{2\\leq j\\leq m_{t}^{*}}{\\operatorname*{min}}\\ \\frac{(\\widehat{\\tau}_{t-1,(j)}-\\widehat{\\tau}_{t-1,\\widehat{\\tau}_{t-1,1}})^{2}}{\\widehat{\\mathbb{V}}_{t-1,\\widehat{\\tau}_{t-1,1}}(e_{1})+\\widehat{\\mathbb{V}}_{t-1,(j)}(e_{j})}}\\\\ &{\\quad=\\underset{|\\widehat{\\mathcal{T}}_{t-1,1}|+1\\leq j\\leq m}{\\operatorname*{min}}\\ \\frac{(\\widehat{\\tau}_{t-1,(j-|\\widehat{\\mathcal{T}}_{t-1,1}|+1)}-\\widehat{\\tau}_{t-1,\\widehat{\\tau}_{t-1,1}})^{2}}{\\widehat{\\mathbb{V}}_{t-1,\\widehat{\\tau}_{t-1,1}}(e_{1})+\\widehat{\\mathbb{V}}_{t-1,(j-|\\widehat{\\mathcal{T}}_{t-1,1}|+1)}(e_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We solve the following optimization problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{e}\\ \\widehat f_{t}(e),\\quad\\quad\\mathrm{s.t.}\\quad\\sum_{j=|\\widehat{\\mathcal T}_{t-1,1}|+1}^{m}p_{j}e_{j}\\leq c_{1},\\quad c_{2}\\leq e_{j}\\leq1-c_{2},\\ j=|\\widehat{\\mathcal T}_{t-1,1}|+1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We denote the set of maximizers to the above optimization problem as ${\\widehat{\\mathcal{E}}}^{*}$ . Lastly, let the $\\delta$ - enlargement of $\\mathcal{E}^{*}$ be $\\mathcal{E}^{*}+B_{\\delta}$ , where $B_{\\delta}$ is the Euclidean ball centered at t he origin with radius $\\delta$ : $\\displaystyle\\mathcal{E}^{*}+B_{\\delta}=\\left\\{e+\\mathbf{u}:e\\in\\mathcal{E}^{*},||\\mathbf{u}||=\\delta\\right\\}$ . Finally, we notice that $\\begin{array}{r}{\\operatorname*{sup}_{e\\in\\mathcal{E}}|\\widehat{f}_{t}(e)-\\check{f}(e)|=o_{p}(1)}\\end{array}$ due to Lemma 3, and Theorem 1. By the same proof of Lemma C.2 in [41], we have $\\widehat{\\mathcal{E}}^{*}\\subseteq\\mathcal{E}^{*}+B_{\\delta}$ , which closes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "H Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Adopting the same proof of Theorem 1 in [41], we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{e}_{j}=\\!\\frac{\\sum_{t=1}^{T}\\sum_{i=1}^{n_{t}}\\mathbb{1}_{(X_{i t}\\in\\mathcal{S}_{j})}\\left(D_{i t}-\\widehat{e}_{t j}^{*}\\right)}{\\sum_{t=1}^{T}\\sum_{i=1}^{n_{t}}\\mathbb{1}_{(X_{i t}\\in\\mathcal{S}_{j})}}+\\frac{\\sum_{t=1}^{T}\\sum_{i=1}^{n_{t}}\\mathbb{1}_{(X_{i t}\\in\\mathcal{S}_{j})}\\widehat{e}_{t j}^{*}}{\\sum_{t=1}^{T}\\sum_{i=1}^{n_{t}}\\mathbb{1}_{(X_{i t}\\in\\mathcal{S}_{j})}}}\\\\ &{\\quad=\\!O_{p}\\left(\\frac{1}{\\sqrt{N}}\\right)+\\Big(1+o_{p}(1)\\Big)\\frac{\\sum_{t=1}^{T}n_{t}\\widehat{e}_{t j}^{*}}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From Lemma 4, one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{2\\leq t\\leq T}|\\widehat{e}_{t j}^{*}-e_{j}^{*}|=o_{p}(1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then with the calibration procedure presented in Section 4.1, by replacing $\\widehat{e}_{t j}^{\\ast}$ with $\\widetilde{e}_{t j}$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{t=1}^{T}n_{t}\\widetilde{e}_{t j}}{N}=\\displaystyle\\sum_{t=1}^{T}\\frac{n_{t}}{N}\\frac{\\left(\\widetilde{e}_{t j}^{*}(n_{t j}+N_{t-1,j})\\right)-N_{t-1,j}(1)}{n_{t j}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{T}\\frac{n_{t}}{N}\\widehat{e}_{t j}^{*}+\\sum_{t=1}^{T}\\frac{n_{t}}{N}\\frac{\\widetilde{e}_{t j}^{*}N_{t-1,j}-N_{t-1,j}(1)}{n_{t j}}}\\\\ &{\\qquad=e_{j}^{*}-\\displaystyle\\sum_{t=1}^{T}\\frac{1}{N p_{j}}\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}\\mathbb{I}_{(X_{i s}\\in{\\mathcal S}_{j})}\\big(D_{i s}-\\widehat{e}_{t j}^{*}\\big)+o_{p}(1)}\\\\ &{\\qquad=e_{j}^{*}+o_{p}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second term in the last equality has the order of $O_{p}(1/\\sqrt{N})$ with variance calculation. Thus, we complete the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "I Proof of Theorem 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}-\\tau_{\\mathcal{T}_{1}}=\\big(\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}-\\widehat{\\tau}_{\\mathcal{T}_{1}}\\big)+\\big(\\widehat{\\tau}_{\\mathcal{T}_{1}}-\\tau_{\\mathcal{T}_{1}}\\big),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widehat{\\tau}_{\\mathcal{T}_{1}}-\\tau_{\\mathcal{T}_{1}}=\\displaystyle\\sum_{j\\in\\mathcal{T}_{1}}p_{j}\\widehat{\\tau}_{j}\\Bigg/\\sum_{j\\in\\mathcal{T}_{1}}p_{j}-\\displaystyle\\sum_{j\\in\\mathcal{T}_{1}}p_{j}\\tau_{j}\\Bigg/\\sum_{j\\in\\mathcal{T}_{1}}p_{j}}&{}\\\\ {=\\displaystyle\\sum_{j\\in\\mathcal{T}_{1}}p_{j}\\left(\\widehat{\\tau}_{j}-\\tau_{j}\\right)\\Bigg/\\sum_{j\\in\\mathcal{T}_{1}}p_{j}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying the result presented in Theorem 4, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{N}(\\widehat{\\tau}_{j}-\\tau_{j})\\overset{\\mathcal{D}}{\\rightarrow}\\mathcal{N}\\Big(0,\\mathbb{V}_{j}(e_{j}^{*})\\Big),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{N}\\big(\\widehat{\\tau}_{\\mathcal{T}_{1}}-\\tau_{\\mathcal{T}_{1}}\\big)\\stackrel{\\mathcal{D}}{\\rightarrow}\\mathcal{N}\\Big(0,\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1}^{*})\\Big),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1}^{*})=\\sum_{j\\in\\mathcal{T}_{1}}p_{j}^{2}\\mathbb{V}_{j}\\left(e_{j}^{*}\\right)\\bigg/\\Big(\\sum_{j\\in\\mathcal{T}_{1}}p_{j}\\Big)^{2}}\\end{array}$ . By Theorem 1, we also have $\\widehat{\\tau}_{\\widehat{T}_{1}}-\\widehat{\\tau}_{\\mathcal{T}_{1}}\\rightarrow_{p}0$ . Thus by Slutsky\u2019s theorem, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{N}\\big(\\widehat{\\tau}_{\\widehat{\\mathcal{T}}_{1}}-\\tau_{\\mathcal{T}_{1}}\\big)\\stackrel{\\mathcal{D}}{\\rightarrow}\\mathcal{N}\\Big(0,\\mathbb{V}_{\\mathcal{T}_{1}}(e_{1}^{*})\\Big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consistency of the variance estimator follows from Theorem 1 and Lemma 1, which is similar to the proof of Lemma 3. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "J Additional algorithms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide another data-adaptive algorithm for choosing hyperparameters for Stage 1 and another bootstrap-based merging data bootstrap for later stages. ", "page_idx": 19}, {"type": "text", "text": "We present a double bootstrap method for choosing hyperparameters in Algorithm 4 as an alternative to Algorithm 3. ", "page_idx": 19}, {"type": "text", "text": "Step 1 (Input):   \n1: Input $\\mathcal{H}_{1},\\bar{\\widehat{\\tau}}_{1j},\\widehat{\\mathbb{V}}_{1j},$ , and $(c_{\\mathrm{L}},c_{\\mathsf{R}})$ .   \n2: Compute $\\tau_{1j}^{*}$ a s in Eq (4). Step $b$ (Bootstrap): 3: for $b\\gets1$ to $B$ do   \n4: Generate $\\widehat{\\tau}_{1}^{*}$ from $\\mathcal{N}\\left(\\tau_{1}^{*},\\widehat{\\Omega}_{n}/n_{1}\\right)$ , where $\\widehat{\\Omega}_{n}=\\mathtt{d i a g}\\left(\\widehat{\\mathbb{V}}_{11},\\ldots,\\widehat{\\mathbb{V}}_{1m}\\right)$ ;   \n5: for $r\\gets1$ to $R$ do   \n6: Generate $\\widehat{\\tau}_{1}^{**}$ from $\\mathcal{N}\\left(\\widehat{\\tau}_{1}^{*},\\widehat{\\Omega}_{n}/n_{1}\\right)$ ;   \n7: Compute \u03c4  1\u2217,\u2217(1) as in Eq (6);   \n8: end for   \n9: Record $\\mathcal{B}_{1,(b)}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)$ as in Eq (14).   \n10: end for Step $_B$ (Output):   \n11: Compute $L_{1}(c_{\\mathrm{L}},c_{\\mathrm{R}})$ as in Eq (14). Choose the pair $(c_{\\mathrm{L}}^{1},c_{\\mathrm{R}}^{1})$ that minimizes $L_{1}(c_{\\mathrm{L}},c_{\\mathrm{R}})$ . ", "page_idx": 20}, {"type": "text", "text": "Here, we define the loss function ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{B}_{1,(b)}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)=\\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{1}\\left(\\widetilde{\\tau}_{(1)}^{\\ast\\ast,r}\\leq\\tau_{(1)}^{\\ast,r}\\right),\\quad L_{1}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)=\\frac{1}{B}\\sum_{b=1}^{B}\\left(\\mathcal{B}_{1,(b)}(c_{\\mathrm{L}},c_{\\mathrm{R}})-\\frac{b}{B+1}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{B}_{1,(b)}\\left(c_{\\mathrm{L}},c_{\\mathrm{R}}\\right)$ is the $b$ -th smallest statistics in $\\mathcal{B}_{11}\\left(c_{\\mathrm{L}},c_{\\mathsf{R}}\\right),.\\,.\\,.\\,,\\mathcal{B}_{1B}\\left(c_{\\mathrm{L}},c_{\\mathsf{R}}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "We could know that $\\mathbb{P}(\\widetilde{\\tau}_{1,(1)}^{*}\\le\\tau_{1}|(Y_{i s},D_{i s},X_{i s})_{i=1}^{n_{1}})$ roughly follows $\\mathrm{Unif}(0,1)$ when the sample size $n_{1}$ is large. Given a desirable pair of hyperparameters $(c_{\\mathrm{L}},c_{\\mathsf{R}})$ , we would thus expect that $\\mathcal{B}_{1,(1)}(c_{\\mathrm{L}},c_{\\mathsf{R}}),\\dotsc,\\mathcal{B}_{1,(B)}(c_{\\mathrm{L}},c_{\\mathsf{R}})$ share a similar distribution with the ordered statistics of i.i.d. Unif(0, 1) random variables. The loss function defined in Eq (14) measures the average of squared differences between $\\mathcal{B}_{1,(b)}(c_{\\mathrm{L}},c_{\\mathrm{R}})$ and the expected value of the order statistics of the ${\\mathrm{Unif}}(0,1)$ random variables. Given the rational above, we would expect that the optimal pair of hyperparameters $(c_{\\mathrm{L}}^{1},c_{\\mathrm{R}}^{1})$ at Stage 1 minimizes such a loss. ", "page_idx": 20}, {"type": "text", "text": "We present na\u00efve bootstrap method for identifying and merging data for later stages in Algorithm 5 as an alternative to Algorithm 2. ", "page_idx": 20}, {"type": "text", "text": "Algorithm 5 Subgroup identification in Stage $t$ $\\left(t>1\\right)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Step 1 (Input):   \n1: Input $\\left\\{\\mathcal{H}_{s}\\right\\}_{s=1}^{t},\\widehat{\\tau}_{t j},\\widehat{\\mathbb{V}}$ $\\widehat{\\mathbb{V}}_{t j}$ , and $(c_{\\tt L}^{t},c_{\\tt R}^{t})$ computed from Algorithm 3 or Algorithm 4. Step $b$ (Bootstr a p):   \n2: for $b\\gets1$ to $B$ do   \n3: Generate totally $\\sum_{s=1}^{t}n_{s}$ resamples randomly with replacement from $\\left\\{\\mathcal{H}_{s}\\right\\}_{s=1}^{t}$ .   \n4: Compute $\\widehat{\\tau}_{t j}^{\\circ}$ as  in Eq (8) with the bootstrap samples;   \n5: Identify the best subgroups $\\widehat{\\mathcal{T}}_{t1}$ with the bootstrap samples as in Eq (3).   \n6: end for Step $_B$ (Output):   \n7: Choose $\\widehat{\\mathcal{T}}_{t1}$ with the highest frequency of occurrence and merge subgroups that belong to $\\widehat{\\mathcal{T}}_{t1}$ . ", "page_idx": 20}, {"type": "text", "text": "K Additional details on synthetic real data study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "K.1 Additional simulation results of our proposed algorithms ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we mainly focus on the comparison of four variations of our proposed design strategy adopted with single or double bootstrap for selecting hyperparameters and na\u00efve or separate bootstrap for data merging. We adopt the same setting mentioned in Section 6. ", "page_idx": 20}, {"type": "image", "img_path": "4WIBvL6ZF4/tmp/14807ae99d657dfd7ebe24d6568255849dc384581f63b78726d51e3055f034a8.jpg", "img_caption": ["Figure 2: Comparison of the correct selection probability among four variations of our proposed design strategies. ", "Methods Single and naive bootstrap Single and separate bootstrap "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "4WIBvL6ZF4/tmp/4d6ac50c2bd94d51b652c98806d78755742d76754e76f80de1f1b2fcd822040f.jpg", "table_caption": ["Table 3: Comparison among four variations of our proposed design \u221astrategy based on \u221aestimated best tie set treatment effect (Est), $95\\%$ confidence interval $95\\%$ CI), $\\sqrt{N}$ -scaled bias $(\\sqrt{N}\\mathrm{Bias})$ , and standard deviation (SD). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "First, from the comparison in Figure 2, as the adaptive design goes on, four variations of our proposed design strategy do not present a distinctly different performance, and they are all efficient with correct selection probability gradually rising over 0.9. Our design conducted with a single bootstrap for choosing hyperparameters and na\u00efve bootstrap for identifying and merging tie sets perform a little better than other variations after 15 stages. ", "page_idx": 21}, {"type": "text", "text": "Second, from Table 3, we obtain that our proposed strategy conducted with a single bootstrap for s\u221aelecting hyperparameters and a separate bootstrap for identifying and merging data has the smallest $\\sqrt{N}$ -scaled bias. Overall, these four variations demonstrate similar efficiency in identifying and merging the best subgroups with almost the same estimated treatment effect and standard deviation. ", "page_idx": 21}, {"type": "image", "img_path": "4WIBvL6ZF4/tmp/795bd972d2c52fbf2f9b5e05bf6bf456bdaf56c931c12d3d2eb92c5729ce3905.jpg", "img_caption": ["Figure 3: Comparison of the normalized mutual information among three conventional methods and four variations of our proposed design strategies. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "We add normalized mutual information as an additional metric to evaluate the similarity of estimated tie set and true tie set of best-performing subgroups. With the results presented in Figure 3 (a) and (b), the normalized mutual information for our proposed design strategy and the three conventional methods exhibits a similar trend in terms of correct selection probability. This observation further confirms that our proposed design strategy outperforms the conventional methods. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "K.2 Extension: AIPW estimator ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "When contextual information (or additional covariate information) is considered in our design, the subgroup treatment effects and associated variances in Eq.8 can be replaced by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\widehat{\\tau}_{t-1,(j)}=\\displaystyle\\frac{1}{N_{t-1,(j)}}\\sum_{s=1\\atop s=1}^{t-1}\\displaystyle\\sum_{i=1}^{n_{s}}{\\mathbf1}_{(X_{i}\\in\\mathcal{S}_{(j)})}\\left\\{\\frac{D_{i s}}{\\widehat{\\tau}_{t-1,(j)}}(Y_{i s}-\\widehat{\\mu}_{1}(Z_{i s}))+\\widehat{\\mu}_{1}(Z_{i s})\\right\\}}\\\\ {-\\displaystyle\\frac{1}{N_{t-1,(j)}}\\sum_{s=1\\atop i\\neq j_{s=1}}^{t-1}\\displaystyle\\sum_{\\substack{1\\atop(X_{i}\\in\\mathcal{S}_{(j)})}}^{1}{\\mathbf1}_{\\{X_{i}:=\\widehat{S}_{(j)}\\}}\\left\\{\\frac{1-D_{i s}}{1-\\widehat{\\mu}_{t-1,(j)}}(Y_{i s}-\\widehat{\\mu}_{0}(Z_{i s}))+\\widehat{\\mu}_{0}(Z_{i s})\\right\\}}\\\\ {\\widehat{\\Psi}_{t-1,(j)}(e_{j})=\\displaystyle\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}{\\mathbf1}_{(X_{i}\\in\\mathcal{S}_{(j)})}D_{i s}(Y_{i s}-\\widehat{\\mu}_{1}(Z_{i s}))^{2}}{N_{t-1,(j)}(1)}\\Big(\\frac{e_{j}\\cdot N_{t-1,(j)}}{N_{t-1}}\\Big)^{-1}}\\\\ {+\\displaystyle\\frac{\\sum_{s=1}^{t-1}\\sum_{i=1}^{n_{s}}{\\mathbf1}_{(X_{i}\\in\\mathcal{S}_{(j)})}(1-D_{i s})(Y_{i s}-\\widehat{\\mu}_{0}(Z_{i s}))^{2}}{N_{t-1,(j)}(0)}\\Big(\\frac{(1-e_{j})\\cdot N_{t-1,(j)}}{N_{t-1}}\\Big)^{-1}}\\\\ {+\\displaystyle\\frac{1}{N_{t-1,(j)}}\\sum_{s=1\\atop\\lfloor\\tau_{s=1}}^{t-1}\\displaystyle\\sum_{i=1}^{n_{s}}{\\mathbf1}_{\\{X_{i}\\in\\mathcal{S}_{(j)}\\}}\\left(\\widehat{\\mu}_{1}(Z_{i s})-\\widehat{\\mu}_{0}(Z_\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\widehat{\\mu}_{d}(Z_{i s})=\\widehat{\\mathbb{E}}[Y_{i s}|D_{i s}=d,Z_{i s}\\in S_{j}]$ , $d\\in\\{0,1\\}$ . We still generate synthetic data that mimic the original PBC dataset with the potential outcome from $Y_{i}(d)|X_{i}\\ \\stackrel{\\cdot}{\\in}S_{j}=\\bar{\\mu_{d j}}+0.5Z_{i}+\\varepsilon_{d j}$ , $Z_{i}\\sim$ $\\mathcal{N}(0,1)$ , $\\varepsilon_{d j}\\sim\\mathcal{N}(0,\\sigma_{d j}^{2}),j=1,\\ldots,5$ to investigate the performance of our proposed stratgy with AIPW estimator. ", "page_idx": 22}, {"type": "text", "text": "Through simulation studies in Figure 4, we found that our method (whether using IPW or AIPW) has the highest accuracy in identifying the best subgroups. The performance of the causal tree model is comparable to that of the contextual bandit algorithms where treatment effects are estimated with the AIPW estimator, thereby including contextual information. Regarding the causal tree model, we would like to note that the comparison may not be entirely fair for the causal tree model. This is because the causal tree method is a method used to identify subgroups after the data has already been collected, whereas our method focuses on designing data collection mechanisms to accurately identify the best subgroups. Therefore, it can be expected that our approach will have higher accuracy in identifying subgroups. ", "page_idx": 22}, {"type": "image", "img_path": "4WIBvL6ZF4/tmp/d1bfa0404374701badbea19b839136c62c0f7adf39f55c1bb6f8ad753fd0dcc6.jpg", "img_caption": ["Figure 4: Comparison of the correct selection probability and normalized mutual information among our proposed design with IPW estimator, three conventional algorithms and our proposed design with AIPW estimator and causal tree. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "K.3 Additional synthetic real data study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide an additional synthetic real study in another domain to showcase the performance of our proposed design. We consider a job dataset from the National Supported Work (NSW) program. This program, initiated in the 1970s, aims to provide disadvantaged workers with work experience. We utilize the dataset from a field experiment referenced in [7] $n=455)$ ), which includes 185 workers in the treatment group and 260 workers in the control group. The dataset features a treatment indicator variable, an outcome variable representing participant earnings post-treatment in 1978, and eight baseline variables. These baseline variables include age, years of education, an indicator for high school graduation, indicators for Black and Hispanic ethnicity, marital status, and pre-treatment earnings for the years 1974 and 1975. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We aim to investigate whether the job training program is indeed beneficial for certain groups of workers in for subgroups defined by: (1) education years $\\geq11$ and age $\\geq26$ , (2) education years $<11$ and $\\mathsf{a g e}\\geq26$ , (3) education years $\\ge11$ and age $<26$ , (4) education years $<11$ and age $<26$ . It is a suitable dataset because by conducting two-sample t-tests between every two subgroups the pairwise similarity of the distribution of subgroups, Subgroups (1), (2) and (3) are regarded as tie sets of the best subgroups under the significance level of $\\alpha=0.05$ . We provide the results in Figure 5, which can validate the performance of our proposed strategy. ", "page_idx": 23}, {"type": "image", "img_path": "4WIBvL6ZF4/tmp/2c9d4c3a3c891a9674f3d4a34fdca405207fdd9d2ba025741f7f1c3fe3056716.jpg", "img_caption": ["Figure 5: Comparison of the correct selection probability and normalized mutual information among three conventional methods and our proposed design strategy based on job dataset. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see Section 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Section 7. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please see Section 6 and Supplementary Materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Section 6 and Supplementary Materials. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Section 6 and Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Please see Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section 6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discussed the potential positive social impacts in Section 1. The negative social impacts are not applicable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have properly credited the original data source. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The manuscript does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]