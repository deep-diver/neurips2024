[{"type": "text", "text": "Uncovering the Redundancy in Graph Self-supervised Learning Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhibiao Wang1, Xiao Wang1\u2217, Haoyue Deng1, Nian Liu2, Shirui Pan3, Chunming Hu1 ", "page_idx": 0}, {"type": "text", "text": "1 Beihang University, China 2 Beijing University of Posts and Telecommunications, China 3 Griffith University, Australia {wzb2321, xiao_wang, haoyue_deng, hucm}@buaa.edu.cn, nianliu@bupt.edu.cn, s.pan@griffith.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph self-supervised learning, as a powerful pre-training paradigm for Graph Neural Networks (GNNs) without labels, has received considerable attention. We have witnessed the success of graph self-supervised learning on pre-training the parameters of GNNs, leading many not to doubt that whether the learned GNNs parameters are all useful. In this paper, by presenting the experimental evidence and analysis, we surprisingly discover that the graph self-supervised learning models are highly redundant at both of neuron and layer levels, e.g., even randomly removing $51.6\\%$ of parameters, the performance of graph self-supervised learning models still retains at least $96.2\\%$ . This discovery implies that the parameters of graph self-supervised models can be largely reduced, making simultaneously fine-tuning both graph self-supervised learning models and prediction layers more feasible. Therefore, we further design a novel graph pre-training and fine-tuning paradigm called SLImming DE-correlation Fine-tuning $\\mathrm{(SLIDE^{2})}$ ). The effectiveness of SLIDE is verified through extensive experiments on various benchmarks, and the performance can be even improved with fewer parameters of models in most cases. For example, in comparison with full fine-tuning GraphMAE on Amazon-Computers dataset, even randomly reducing $40\\%$ of parameters, we can still achieve the improvement of $0.24\\%$ and $0.27\\%$ for Micro-F1 and Macro-F1 scores respectively. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph self-supervised learning, aiming at learning the parameters of Graph Neural Networks (GNNs) without labels, has been a popular graph pre-training paradigm [1\u20133]. Usually, graph self-supervised learning is naturally divided into two learning methods, i.e., graph contrastive learning [4, 5] and graph generative self-supervised learning [6, 7]. After pre-training the parameters of self-supervised GNNs, graph self-supervised learning achieves state-of-the-art performance on a variety of tasks by fine-tuning an additional prediction layer [8, 7]. ", "page_idx": 0}, {"type": "text", "text": "Various researches have attempted to improve and understand graph self-supervised learning from different perspectives. For example, the graph augmentation techniques [9, 10], graph spectrum feature of graph self-supervised learning [11, 12], and others [7, 13]. Despite their remarkable achievements, little efforts have been made to understand the behavior of the learned model parameters by graph self-supervised learning. Here, we ask: Are the model parameters all always useful? Or what part of model parameters is useful? Whether there is the model redundancy in self-supervised GNNs? Understanding the model property in graph self-supervised learning, particularly the model redundancy, can provide valuable guidelines and insights for the development of advanced graph self-supervised learning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For this purpose, we conduct experiments and surprisingly find out that the graph self-supervised learning models are actually highly redundant. We start with the experiments (Section 2) to investigate the downstream performance of different graph self-supervised learning models with randomly removed parameters at both neuron and layer levels. Our findings indicate that graph self-supervised learning models with even half of the neurons randomly removed have virtually no impact on the performance of the node classification task. Moreover, by closely examining the learned representations, we find out that the representations in each layer with a substantial number of neurons removed are quite similar to the representations obtained by the full set of neurons. Meanwhile, there is also a strong similarity between the representations of each layer and its adjacent layer. All results demonstrate that graph self-supervised models exhibit high model redundancy at both neuron level and layer levels. ", "page_idx": 1}, {"type": "text", "text": "The above findings hold great potential to improve current graph self-supervised learning models. On the one hand, it may provide valuable guideline for the pruning or the sparsity of GNNs [14, 15]. On the other hand, the phenomenon of model redundancy provides a new opportunity for designing a full fine-tuning graph self-supervised learning model. Previously, considering that the number of parameters to be fine-tuned is excessive when we directly fine-tune both GNNs and prediction layers, we usually only have to fine-tune the attached prediction layer, i.e., linear probing [3, 5, 7]. Here, if the parameters can be greatly reduced, we can simultaneously fine-tune both graph selfsupervised learning models and prediction layers. Besides, the findings imply that de-correlating the learned representations is also necessary. Therefore, we propose a novel pre-training and finetuning paradigm called SLIDE by obtaining SLIm GNNs from the self-supervised GNNs and using the DE-correlation strategy to reduce the correlation between features during the fine-tuning phase. Extensive experiments on various benchmark datasets validate the effectiveness of SLIDE. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to uncover that graph self-supervised models exhibit high model redundancy at both neuron and layer levels. The model redundancy allows for the removal of a majority of model parameters with almost no impact on the performance of the downstream task. This discovery can significantly enhance model efficiency while maintaining acceptable task performance.   \n\u2022 This discovery provides two key guidelines for the subsequent graph pre-training and finetuning framework: one is that the parameters can be reduced, and the other is the representations should be de-correlated. These motivate us to propose a novel method, SLIDE, to achieve a pre-training and fine-tuning paradigm with fewer parameters and better performance on the downstream task.   \n\u2022 Comprehensive experiments demonstrate that our SLIDE outperforms baselines across multiple benchmark datasets. For example, using the Amazon-Computers dataset [16] with GRACE [1], compared to full fine-tuning, we achieve improvements of $0.37\\%$ and $0.16\\%$ for Macro-F1 and Micro-F1 scores respectively with a random reduction of $30\\%$ of parameters. ", "page_idx": 1}, {"type": "text", "text": "2 The Model Redundancy in Graph Self-supervised Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we investigate the model redundancy in two representative graph self-supervised learning models (GraphMAE [3] and GRACE [1]) on node classification tasks. Specifically, we pretrain these graph self-supervised models on Cora, Citeseer, Pubmed [17], Amazon-Photo, AmazonComputers [16], Ogbn-arxiv [18], and then we evaluate the performance again after removing neurons in various ways. If the performance gap between the original GNNs and the slim GNNs with fewer neurons is small, it indicates great model redundancy in self-supervised GNNs. In other words, even with reduced neurons, the performance remains largely unaffected, suggesting that many neurons in self-supervised GNNs are dispensable. Notably, model redundancy is not limited to node classification tasks. We conduct experiments in Appendix A demonstrating that model redundancy also persists in link prediction and graph classification tasks. ", "page_idx": 1}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/2e2c8c002cbaa659162391cbcd60c252d1d0de330933fa0dcc03d9d4a3a4f592.jpg", "img_caption": ["input neurons hidden neurons output neurons drop neurons "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Neuron dropout. To initialize a smaller variant of the self-supervised pre-trained GNNs, we select parameters from self-supervised GNNs in different ways. From left to right: randomly reduce the number of neurons in each layer proportionally, the original GNNs, retain only the first two layers while randomly reducing the number of neurons in the second layer proportionally. ", "page_idx": 2}, {"type": "text", "text": "Neuron removal We explore two different ways to reduce the number of neurons in selfsupervised GNNs, i.e., the original GNNs, which are illustrated in Figure 1. Specifically, we consider two approaches: (1) neuron level: randomly retaining the number of neurons by $50\\%$ and $25\\%$ in each layer named \"half\" and \"quarter\", and (2) layer level: retaining only the first two layers if the layer number is greater than two and randomly retaining the number of neurons by $100\\%$ , $50\\%$ and $25\\%$ in the second layer named $^{11}2.$ -original\", \"2-half\" and \"2-quarter\". In this way, we can obtain five types of slim GNNs: \"half GNNs\", \"quarter GNNs\", \"2-original GNNs\", \"2-half GNNs\" and \"2-quarter GNNs\". ", "page_idx": 2}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/e57f8a63910bf0634b70502051516b3815a59b75c163bda0e6d6b4ebf4fc7094.jpg", "table_caption": ["Table 1: The performance of different neuron removal methods on six datasets with GraphMAE. "], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/3a202dc4430f49917604c6efab9270cb69c09e72bf59e75d2b41cbddb73f1267.jpg", "table_caption": ["Table 2: The performance of different neuron removal methods on five datasets with GRACE. Ogbnarxiv is not included because it is \"out of memory\" when Ogbn-arxiv is pre-trained with GRACE. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Experimental results We first pre-train the original GNNs through GraphMAE and GRACE, then we can obtain five kinds of slim GNNs by using the neuron removal methods mentioned above. For both the slim GNNs and the original GNNs, we individually attach an additional prediction layer which is trained while keeping the original GNNs frozen on the node classification task to evaluate the Micro-F1 (F1-Mi) and Macro-F1 (F1-Ma) scores, as shown in Table 1 and Table 2, where \"-\" means that there are only two layers in the original GNNs (i.e.,\"2-Original\" and \"Original\" have the same performance) and \"Change-Param\" means the percentage change of the number of parameters compared to \"Original\". Surprisingly, we have following observations: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 In all cases, \"half GNNs\" retain at least $96.2\\%$ of the performance of the original GNNs while the numbers of parameters are reduced by at least $51.6\\%$ . In most of the cases, \"quarter GNNs\" still retain at least $90\\%$ of the performance of the original GNNs while the number of parameters are reduced by at least $76.2\\%$ . As can be seen, \"quarter GNNs\" of GRACE on Computers retains $95.5\\%$ of the Micro-F1 score and $94.7\\%$ of the Macro-F1 score, while the number of parameters is reduced by $82.5\\%$ . \u2022 The removal of layers after the second layer has a negligible impact on the performance. The performance of $^{1\\ast}2$ -original GNNs\" with GraphMAE on Ogbn-arxiv dataset demonstrates that even when layers after the second layer are removed, both the Micro-F1 score and the Macro-F1 score retain ${\\bf99.4\\%}$ of the performance, while the number of parameters is reduced by $47.0\\%$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The observations indicate that the graph self-supervised learning models are highly redundant, both at the neuron level and at the layer level. More details about the hyperparameters and the number of parameters can be found in Appendix B. ", "page_idx": 3}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/2062eac770e43c52250214ce6d86b140167fc297f279d21b65f0a9abcf798900.jpg", "img_caption": ["Figure 2: CKA scores between the representations of the slim GNNs and the same layer in the original GNNs with GraphMAE and GRACE on several datasets. \"all\" means we remove the neurons from all layers in the same proportion. \"l1\" means that we calculate CKA scores of the representations from the first layer, and \"l2\" means CKA scores from the second layer, and so on. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Redundancy analysis on neuron level Here, we further analyze the model redundancy by closely examining the learned representations in order to explore why the slim GNNs achieve similar performance to the original GNNs. Specifically, we get the slim GNNs by dropping the neurons of each layer in $25\\%$ , $50\\%$ and $75\\%$ to analyze the model redundancy at the neuron level. Given the learned representations of the $i$ -th layer of the slim GNNs and the original GNNs, Centered Kernel Alignment (CKA) is adopted to calculate their similarity [19]. A high CKA score implies a strong similarity. As shown in Figure 2, compared to the original GNNs, the CKA scores are over $85\\%$ when $50\\%$ neurons are dropped. Even if the neurons are removed by $75\\%$ , the CKA scores are still almost over $80\\%$ . This indicates that removing a significant number of neurons from each layer has a minimal impact on the significance of representations of each layer and task performance consequently. ", "page_idx": 3}, {"type": "text", "text": "Redundancy analysis on layer level ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As for the model redundancy at the layer level, given the learned representations of the $i$ -th layer and the $(i+1)$ -th layer of the original GNNs, we also adopt CKA to calculate their similarity, where the 0-th layer represents the original features of the nodes from the datasets. As shown in Figure 3, we report the CKA scores between the representations of each layer and its adjacent layer of the original GNNs for GraphMAE and GRACE. \"data-layer1\" means that we calculate CKA scores between the original features of the nodes from the datasets and the representations of ", "page_idx": 4}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/8eff33cc18d4add82824565a02e30d088eb054f12eeb0d3db187e851c9b3996d.jpg", "img_caption": ["Figure 3: CKA scores between the representations of each layer and its adjacent layer of the original GNNs for GraphMAE and GRACE on several datasets. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "the first layer, and so on. As can be seen, CKA scores between the original features and the representations of the first layer are relatively low, while CKA scores between the representations of the layers after the first layer and their next layer are much closer to 1 in most of the cases, indicating the model redundancy at the layer level. ", "page_idx": 4}, {"type": "text", "text": "3 Our Proposed Tuning Approach: Slimming De-correlation Fine-tuning ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/64e24c73133ecc072de1098dd8bf48819ac0196faf5de89dc87ec66951ac1a30.jpg", "img_caption": ["Figure 4: The overall framework of SLIDE. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "In general, for the self-supervised pre-trained GNNs, we attach an additional prediction layer which is fine-tuned while keeping the GNNs frozen to conduct the downstream tasks. Ideally, if we are able to tune the parameters of both the GNNs and the linear layer, it is possible to achieve the best performance. However, the number of parameters to be fine-tuned is excessive. Here, since we identify the model redundancy in the self-supervised pre-trained GNNs, this motivates us that we actually only need to tune the additional classifier and the slim GNNs, so as to obtain a better trade-off between the model performance and the number of fine-tunable parameters. Therefore, we propose a novel pre-training and fine-tuning paradigm called SLImming DE-correlation Fine-tuning (SLIDE), as shown in Figure 4. Specifically, firstly it reduces model redundancy in self-supervised pre-trained GNNs by randomly removing redundant neurons to obtain the slim GNNs. Then, we input the graph data into the slim GNNs to obtain the embeddings of the nodes, combined with an additional prediction layer, we can predict the label of each node. Meanwhile, we design another model de-correlation module based the squared Frobenius norm [20, 21] (an analogue corresponding to the Hilbert-Schmidt Independence Criterion, i.e., HSIC [22] in Euclidean space). The module learns the de-correlation weights for the classification loss, so as to reduce the redundancy among embeddings and make the embeddings more informative. ", "page_idx": 4}, {"type": "text", "text": "Specifically, let $G\\;=\\;(A,V,X)$ denote the input graph, where $V$ is the node set, $N\\,=\\,|V|$ is the number of nodes, $N_{t r}$ is the number of nodes in the training set and $d$ is the dimension of the node features, $A~\\in~\\{0,1\\}^{N\\times N}$ is the adjacency matrix, and $\\boldsymbol{X}\\ \\in\\ R^{N\\times d}$ is the input node feature matrix. For model slimming, we first pre-train the original GNNs through the existing pretraining frameworks, e.g., the graph contrastive learning [1, 4, 5] or graph generative self-supervised learning [3, 7, 2]. Then the slim GNNs $f_{S}$ can be obtained by randomly reducing both the neurons and the layers. Furthermore, given $G$ , we can get the node embeddings $H\\,\\in\\,R^{N\\times d_{h}}$ as $H\\,=$ $f_{S}(A,X)$ , where $d_{h}$ is the dimension of the node embeddings. ", "page_idx": 5}, {"type": "text", "text": "Motivated by Section 2 that the neurons, as well as the learned embeddings, in self-supervised GNNs are highly redundant, we aim to de-correlate the learned embeddings $H$ in the fine-tuning phase, making models with fewer parameters more informative. In particular, in inspiration of the de-correlation methods [21, 23], given the i-th dimension and j-th dimension of the node embeddings $\\mathbf{H}_{*,i}$ and $\\mathbf{H}_{*,j}$ , we obtain Random Fourier Features (RFF) [24, 25] as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u(\\mathbf{H}_{\\ast,i}):=(u_{1}(\\mathbf{H}_{\\ast,i}),u_{2}(\\mathbf{H}_{\\ast,i}),\\dots,u_{N_{R F F}}(\\mathbf{H}_{\\ast,i})),}\\\\ &{v(\\mathbf{H}_{\\ast,j}):=(v_{1}(\\mathbf{H}_{\\ast,j}),v_{2}(\\mathbf{H}_{\\ast,j}),\\dots,v_{N_{R F F}}(\\mathbf{H}_{\\ast,j})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $N_{R F F}$ is the number of functions in the random fourier space, $u_{q}$ and $v_{q}$ denote the functions from the space of Random Fourier Features. Then, we elaborate on reweighting of the weights, which encourages the independence of the node embeddings. Define the weights of the nodes in the training set as $\\bar{W}=\\{w_{n}\\}_{n=1}^{\\bar{N}_{t r}}$ , where $w_{n}$ is the learnable weight for the $n$ -th node of the training set in . Consequently, the reweighted partial cross-covariance matrix can be calculated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{C}_{\\mathbf{H}_{*},i,\\mathbf{H}_{*},j}^{\\mathbf{W}}=\\frac{1}{N_{t r}-1}\\!\\sum_{n=1}^{N_{t r}}\\left[\\left(w_{n}u(\\mathbf{H}_{n,i})-\\frac{1}{N_{t r}}\\sum_{m=1}^{N_{t r}}w_{m}u(\\mathbf{H}_{m,i})\\right)^{\\top}\\right.}\\\\ {\\left.\\cdot\\left(w_{n}v(\\mathbf{H}_{n,j})-\\frac{1}{N_{t r}}\\sum_{m=1}^{N_{t r}}w_{m}v(\\mathbf{H}_{m,j})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The learnable weights $W$ participate in the process of optimization to eliminate as much as possible the correlation between the dimensions of the node embeddings by minimizing the partial crosscovariance matrix in Eq. 2. Specifically, for the process of optimization, given the labels of the nodes $\\mathbf{Y}_{n}\\in R^{N_{t r}}$ , we iteratively optimize the weights of the nodes $W$ , the slim GNNs $f_{S}$ , and the additional prediction layer $R$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{S}^{*},R^{*}=\\operatorname*{argmin}_{f_{S},R}\\displaystyle\\sum_{n=1}^{N_{t r}}w_{n}\\ell\\left(R\\circ f_{S}\\left(X_{n}\\right),\\mathbf{Y}_{n}\\right),}\\\\ &{\\quad\\mathbf{W}^{*}=\\operatorname*{argmin}_{\\mathbf{W}}\\displaystyle\\sum_{1\\leq i<j\\leq d_{h}}\\|\\widehat{C}_{\\mathbf{H}_{*},i,\\mathbf{H}_{*,j}}^{\\mathbf{W}}\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell$ denotes the cross-entropy loss for the node classification task. The optimization of the weights $W$ encourages the slim GNNs $f_{S}$ to generate the node embeddings $H$ , and eliminates the correlations between embeddings. The optimization of the slim GNNs $f_{S}$ and the additional classifier $R$ will lead to good performance on the node classification task. ", "page_idx": 5}, {"type": "text", "text": "To put it in a nutshell, SLIDE is a general pre-training and fine-tuning framework in graph, which balances the number of parameters and the performance of self-supervised pre-trained GNNs. Therefore, SLIDE can be implemented using different ideas of reducing parameters and de-correlation methods. We use the methods mentioned above as examples and conduct some experiments to demonstrate the feasibility of SLIDE. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. For a comprehensive comparison, we use six real-world datasets to evaluate the performance of node classification (i.e., Cora, Citeseer, Pubmed, Amazon-Photo, Amazon-Computers and Ogbn-arxiv). More details about the datasets are in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "Baselines. Our proposed SLIDE is a general paradigm which removes neurons from the selfsupervised pre-trained GNNs randomly and introduces model de-correlation methods during the fine-tuning phase. We choose three representative graph pre-training frameworks for evaluation in our SLIDE: generative graph self-supervised learning (GraphMAE [3] and MaskGAE [7]), and graph contrastive learning (GRACE [1]). For each framework, we choose two classical fine-tuning methods as baselines: linear probing, and full fine-tuning. Notably, our proposed SLIDE is orthogonal to other fine-tuning methods. We provide additional experiments with SLIDE in Appendix C.2 as an example. ", "page_idx": 6}, {"type": "text", "text": "Experimental setup. For GraphMAE and MaskGAE, we use the implementations of their official codes [3, 7]. As for GRACE, we use the implementation of training an additional prediction layer for the node classification task, instead of using a LIBSVM classifier [26], in order to facilitate the comparison of the model\u2019s performance. For our proposed SLIDE, we use \"2-half GNNs\" as the slim GNNs for all datasets and pre-training frameworks. In all tables and datasets, we report averaged results along with the standard deviation computed over 5 different runs. All experiments are conducted on Linux servers equipped with NVIDIA RTX A5000 GPUs (22729 MB). We refer readers of interest to Appendix C.3 for more details on the experiments. ", "page_idx": 6}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/291994e222259a87a57c02e28bd7fc9e34be20cb402d01e5c119433cfba4644b.jpg", "table_caption": ["Table 3: Node classification accuracy $(\\%\\pm\\sigma)$ on six benchmark datasets with GraphMAE. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/f69662dd4f628db30dc7b433cce7529e9af746f0dd19e81c4c8587bf77b58248.jpg", "table_caption": ["Table 4: Node classification accuracy $(\\%\\pm\\sigma)$ on five benchmark datasets with GRACE. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/cab5a2c794c4b67247e8f626001194a0c0270e3bf66d4129501d9c0d096dc083.jpg", "table_caption": ["Table 5: Node classification accuracy $(\\%\\pm\\sigma)$ on six benchmark datasets with MaskGAE. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Effectiveness of SLIDE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate our proposed SLIDE more comprehensively, we use two common evaluation metrics, Macro-F1 and Micro-F1 scores, and show their difference between baselines and SLIDE. The results are shown in Table 3 - 5, where \"LP\" means \"linear probe\", \"FT\" means \"full fine-tune\", and \"OOM\" means \"out of memory\". We have the following observations: (1) In general, SLIDE improves the performance compared to \"LP\" because SLIDE is able to fine-tune both \"2-half GNNs\" and the additional prediction layer. For example, in comparison with \"LP\" on Computers with these three competitive pre-training frameworks, SLIDE achieves an average improvement of $0.75\\%$ and $0.69\\%$ for Micro-F1 and Macro-F1 scores, respectively. (2) Although SLIDE significantly reduces the number of parameters in self-supervised GNNs, SLIDE still achieves better performance than \"FT\". Especially on large-scale graphs like Ogbn-arxiv with GraphMAE, SLIDE is able to fine-tune both the pre-trained GNNs and the additional prediction layer. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Model Analysis ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/cdbd69dfb557c0a8344d8a29f60b8cce64803461f750f0147a23025e726a6362.jpg", "img_caption": ["Figure 5: Ablation studies of model de-correlation on six benchmark datasets and three pre-training frameworks. \"w/o de\" means that we fine-tune the slim GNNs without model de-correlation methods. \"Mi\" means Micro-F1 scores and \"Ma\" means Macro-F1 scores. The results of Ogbn-arxiv with GRACE are unseen because of \"out of memory\". "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Ablation study Here, we test the performance of the slim GNNs with and without model decorrelation on the node classification task. The results are shown in Figure 5, where \"w/o dec\" means that the slim GNNs are directly fine-tuned without model de-correlation. We find that the slim GNNs with de-correlation perform much better than the GNNs without de-correlation, proving that correlation is still present when self-supervised GNNs are directly fine-tuned. ", "page_idx": 7}, {"type": "text", "text": "Parameter analysis In order to quantify the number of parameters of the self-supervised GNN reduced by SLIDE, taking GraphMAE and GRACE as an example, we report the number of parameters of our proposed SLIDE and \"FT\" for fine-tuning. As can be seen in Figure 6, we observe that the parameters of \"2- half\" GNNs are significantly reduced. In particular, on Ogbn-arxiv with GraphMAE, the number of parameters for fine-tuning is reduced by $70.1\\%$ . More details about the number of parameters are provided in B.1. ", "page_idx": 7}, {"type": "image", "img_path": "7Ntft3U7jj/tmp/e93a5a9cb4de0216349ae475c86559e63b674a07ef1b2b358ffe78fb6bc5c399.jpg", "img_caption": ["Figure 6: The number of parameters on several datasets with GraphMAE and GRACE. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Graph self-supervised learning Self-supervised methods on graphs can be naturally divided into contrastive and generative domains according to objective designs and model architectures [9, 27\u2013 29]. Graph Contrastive Learning (GCL) has shown its outstanding ability in unsupervised setting, and many studies have been proposed [1, 4, 5, 8]. On the other hand, Generative self-supervised learning [6, 2] aims to recover missing parts of the input data. Among them, methods which have emerged in the last two years [3, 7] have significantly enhanced the performance of generative methods, resulting in competitive performance on downstream tasks and attracting much attention. Despite the remarkable achievements of these methods, the issue of model redundancy in these selfsupervised GNNs remains unexplored in the current research landscape. ", "page_idx": 8}, {"type": "text", "text": "Model redundancy In recent years, researchers have investigated redundancy in several pretrained model architectures for different domains. Among them, in [30], researchers dissect two pre-trained models, BERT [31] and XLNet [32], studying how much redundancy they exhibit by using feature selection to choose the subset of neurons. In [33], researchers dissected several pretrained visual models and randomly removed neurons of the penultimate layer in proportion, proving that redundancy exists in the penultimate layer. In [34], researchers find that many layers of LLMs exhibit high similarity. By removing some of the layers of large language models (LLMs), LLMs can still maintain good performance, proving that model redundancy exists in LLMs. Graph Neural Networks (GNNs) [35\u201337] have been widely applied in recent years and there are some studies focusing on graph sparsification and graph lottery ticket [14, 15]. Graph sparsification approximates a graph to a sparse graph by reducing the number of edges instead of parameters. And graph lottery ticket reduces parameters in networks systematically, not randomly. However, the study of model redundancy in self-supervised GNNs remains largely unexplored. ", "page_idx": 8}, {"type": "text", "text": "Pre-training and fine-tuning Traditional pre-training and fine-tuning paradigms mainly include \"linear probe\" and \"full fine-tune\". The former faces the challenge of insufficient performance, while the latter requires high computational cost and memory. In recent years, several Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to address these issues. Among them, Low Rank Adaptation (LoRA) [38] alters the fine-tuning phase by keeping the original model parameters frozen and introducing modifications to a separate, smaller set of parameters. These changes are then incorporated into the original parameters. On the other hand, Adapter Tuning [39] adds new modules, called adapters, between the layers of a pre-trained model. The parameters from the pretraining phase are frozen, and a smaller set of additional parameters is introduced for the new task. A common feature of these methods is the addition of a small number of additional parameters to the complete model for fine-tuning. The focus of this paper is orthogonal to these methods, as it aims to fine-tune the model under the condition of reduced parameters. In this paper, we provide a unique perspective on the pre-training and fine-tuning paradigm and contribute to the ongoing exploration of effective fine-tuning strategies. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Broader Impacts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we make an exploration of model redundancy in self-supervised pre-trained GNNs. We find out that model redundancy in self-supervised GNNs exists at both neuron level and layer level, which deepens our understanding of self-supervised GNNs. We then propose a novel pretraining and fine-tuning paradigm, SLIDE, which achieves better performance with fewer number of parameters for fine-tuning. Our experiments validate the effectiveness of SLIDE. ", "page_idx": 8}, {"type": "text", "text": "Limitations and broader impact Although we discover that the graph self-supervised learning models are highly redundant at neuron and layer levels and deepen our understanding of selfsupervised GNNs, a potential limitation is that some theoretical foundations are still lacking. Our findings hold great potential to improve current graph self-supervised learning models and may provide valuable guideline for the pruning or the sparsity of GNNs. In the future, we will further understand self-supervised GNNs from the perspective of model redundancy by theoretical analysis. Beyond that, we do not expect any immediate negative impact on society. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Natural Science Foundation of China (No. 62322203, 62172052). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph Representation Learning and Beyond, 2020. URL http://arxiv.org/abs/2006.04131.   \n[2] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.   \n[3] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 594\u2013604, 2022.   \n[4] Petar Velic\u02c7kovic\u00b4, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.   \n[5] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In International conference on machine learning, pages 4116\u20134126. PMLR, 2020.   \n[6] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pretraining of graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1857\u20131867, 2020.   \n[7] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, and Weiqiang Wang. What\u2019s behind the mask: Understanding masked graph modeling for graph autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1268\u20131279, 2023.   \n[8] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. Advances in Neural Information Processing Systems, 34:76\u201389, 2021.   \n[9] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812\u20135823, 2020.   \n[10] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pages 2069\u20132080, 2021.   \n[11] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. Spectral feature augmentation for graph contrastive learning and beyond. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 11289\u201311297, 2023.   \n[12] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning from the perspective of graph spectrum. Advances in Neural Information Processing Systems, 35:2972\u20132983, 2022.   \n[13] Ruijia Wang, Xiao Wang, Chuan Shi, and Le Song. Uncovering the structural fairness in graph contrastive learning. Advances in neural information processing systems, 35:32465\u201332473, 2022.   \n[14] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery ticket hypothesis for graph neural networks. In International conference on machine learning, pages 1695\u20131706. PMLR, 2021.   \n[15] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458\u201311468. PMLR, 2020.   \n[16] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \n[17] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[19] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019.   \n[20] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Machine Learning, pages 528\u2013539. PMLR, 2020.   \n[21] Xingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, and Zheyan Shen. Deep stable learning for out-of-distribution generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5372\u20135382, 2021.   \n[22] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch\u00f6lkopf. Measuring statistical dependence with hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63\u201377. Springer, 2005.   \n[23] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[24] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007.   \n[25] Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random fourier features. In International conference on machine learning, pages 3905\u20133914. PMLR, 2019.   \n[26] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1\u201327, 2011.   \n[27] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.   \n[28] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1150\u20131160, 2020.   \n[29] Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. Advances in neural information processing systems, 31, 2018.   \n[30] Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained transformer models. arXiv preprint arXiv:2004.04010, 2020.   \n[31] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[32] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.   \n[33] Vedant Nanda, Till Speicher, John Dickerson, Krishna Gummadi, Soheil Feizi, and Adrian Weller. Diffused redundancy in pre-trained representations. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024.   \n[35] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[36] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[37] Zhongying Zhao, Zhan Yang, Chao Li, Qingtian Zeng, Weili Guan, and MengChu Zhou. Dual feature interaction-based graph convolutional network. IEEE Transactions on Knowledge and Data Engineering, 35(9):9019\u20139030, 2022.   \n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n[40] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015.   \n[41] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A More Experiences about Model Redundancy ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/d789af7d0da1875181b281caf5c2b440eef44c2c6dbd746cf4f08cdcc28325a7.jpg", "table_caption": ["Table 6: The performance of different neuron removal methods on three datasets with MaskGAE on link prediction tasks. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/9cbbb059af761d8a3818c2a0ea6cc98bc480e300df713620eddbacdcff7d323e.jpg", "table_caption": ["Table 7: The performance of different neuron removal methods on four datasets with GraphMAE on graph classification tasks. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "To demonstrate that model redundancy exists across a broader spectrum of graph learning tasks, we conduct experiments on GraphMAE, effective for graph classification, and MaskGAE which excels in link prediction. For graph classification, we conduct experiments on 4 benchmarks: MUTAG, IMDB-B, IMDB-M, REDDIT-B [40]. The results in Table 6 - 7 indicate that model redundancy exists across a wide range of tasks. ", "page_idx": 12}, {"type": "text", "text": "B More Details about Model Redundancy ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 GNN Parameters and Linear Parameters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 8: More details about paramters with different neuron removal methods for GraphMAE, where the parameters in GNN is not fine-tunable while the parameters in Linear is fine-tunable. ", "page_idx": 12}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/19d691f7de8b147e7e304ae0002af27001008de53663d5979f9ac74dc8f55a5c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/da807e793bb15fa903034d0d4981c27fe03b64b01aec5013c7ca1e169355d158.jpg", "table_caption": ["Table 9: More details about paramters with different neuron removal methods for GRACE. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Tables 8 and Table 9 record the number of GNN parameters and Linear parameters for different ways of removing neurons. We find out that proper removal of neurons still maintains decent performance, as we show in Section 2, which means that there is a lot of model redundancy in the original GNNs. Note that we train the same number of epochs for the different ways of reducing neurons to save model training time, and the gap in model performance will be smaller, i.e. there will still be more model redundancy, if training time is more. ", "page_idx": 13}, {"type": "text", "text": "B.2 Hyper-parameters for GraphMAE and GRACE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For datasets where the original pre-training framework has been tested, we use the hyper-parameters from the official code, while for the other datasets, we obtain the hyper-parameters ourselves by testing on these datasets. ", "page_idx": 13}, {"type": "text", "text": "For GraphMAE, we obtain the hyper-parameters of pre-training on Amazon-Photo and AmazonComputers by ourselves. For both datasets, linear probes are trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.0005 while GNNs are pre-trained with a learning rate of 0.001, weight decay of 0, hidden number of 1024, head number of 4, layer number of 2, mask rate of 0.5, drop edge rate of 0.5 and epoch number of 1000. ", "page_idx": 13}, {"type": "text", "text": "For GRACE, we obtain the hyper-parameters of all linear probes on all datasets by ourselves because it trains a LIBSVM classifier in the official code while we obtain the the hyper-parameters of pretraining on Amazon-Photo and Amazon-Computers by ourselves. Here we list the hyper-parameters for pre-trained models and linear probes used in our experiments: ", "page_idx": 13}, {"type": "text", "text": "\u2022 For Cora, the linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9, epoch number of 1000 and weight decay of 0.   \n\u2022 For Citeseer, the linear probe is trained using Adam with a learning rate of 0.5, momentum of 0.9, epoch number of 1000 and weight decay of 0.   \n\u2022 For Pubmed, the linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9, epoch number of 500 and weight decay of 0.   \n\u2022 For Photo, the linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9, epoch number of 500 and weight decay of 0 whlie GNN is pre-trained with a learning rate of 0.001, weight decay of 0, hidden number of 512, layer number of 2, and epoch number of 200.   \n\u2022 For Computers, the linear probe is trained using Adam with a learning rate of 0.5, momentum of 0.9, epoch number of 500 and weight decay of 0 whlie GNN is pre-trained with a learning rate of 0.001, weight decay of 0, hidden number of 256, layer number of 2, and epoch number of 200. ", "page_idx": 13}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/89c254949ef86e652e4f39b928ebd8b178d5b62f1481906cc284c42d9862e3e7.jpg", "table_caption": ["Table 10: Dataset Statistics "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Datasets and Pre-training Frameworks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we give some details about datasets we choose to evaluate the performance of SLIDE. As we have mentioned in Section 4, we use several citation networks and two social networks and Ogbn-arxiv datasets. Among these, the edges in citation networks (i.e. Cora, Citeseer, and Pubmed) represent the citation relationship between two papers (undirected), the node features are the bag-ofwords vector of the papers, and the labels are the fields of the papers. The nodes in social networks (i.e. Amazon-Photo and Amazon-Computers) represent the products, the edges represent whether the two products are frequently purchased together, the features represent the product reviews encoded in bag-of-words, and the labels are the predefined product categories. Ogbn-arxiv captures citation relationships between computer science papers on arxiv. Nodes denote papers, edges denote citation relationships of papers, and each paper has a dimensional feature vector obtained by averaging the embeddings of words in the title and abstract. The embeddings are obtained using Word2Vec [41]. The test is to predict 40 domains over CS. ", "page_idx": 14}, {"type": "text", "text": "For the implementations of three pre-training frameworks, we use their original code. The sources are listed as follows: ", "page_idx": 14}, {"type": "text", "text": "1. GraphMAE: https://github.com/THUDM/GraphMAE   \n2. GRACE: https://github.com/CRIPAC-DIG/GRACE   \n3. MaskGAE: https://github.com/EdisonLeeeee/MaskGAE ", "page_idx": 14}, {"type": "text", "text": "C.2 Additional Experiments and Analysis of SLIDE with Fine-Tuning Methods ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "7Ntft3U7jj/tmp/72f29c12331152b48158764eeff6649781d1a03a79aac7bf1fb27a0097013401.jpg", "table_caption": ["Table 11: Orthogonality experiment of our proposed SLIDE and traditional fine-tuning methods, using LoRA as an example. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Here we examine the orthogonality of SLIDE in relation to traditional fine-tuning methods. We start with the classic LoRA method [38] applied to GraphMAE, where SLIDE randomly prunes a subset of neurons to create Slim GNNs. We then introduce an additional LoRA module designed for finetuning. The results are shown in Table 11. Notably, \"SLIDE-LoRA\" can only adjust the parameters of the LoRA modules, as the Slim GNNs remain fixed. Despite this limitation, \"SLIDE-LoRA\" enhances performance by reducing correlations among final representations, achieving slightly better results compared to using LoRA directly on Original GNNs. This supports the efficacy of our method in improving model capabilities. ", "page_idx": 14}, {"type": "text", "text": "C.3 Experimental Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we provide more experimental settings about the experience about SLIDE. We obtain the hyper-parameters ourselves by testing on these datasets with three frameworks except linear probing (we obtain the hyper-parameters from the official code). ", "page_idx": 15}, {"type": "text", "text": "Here we list the hyper-parameters for full fine-tuning: ", "page_idx": 15}, {"type": "text", "text": "\u2022 GraphMAE: ", "page_idx": 15}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9 and weight decay of 1e-4 whlie GNN is tuned with a learning rate of 1e-7, weight decay of 0.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 1e-1 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 1e-3.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 0.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.05 whlie GNN is tuned with a learning rate of 5e-6, weight decay of 0.   \n\u2013 Computers: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.05 whlie GNN is tuned with a learning rate of 5e-5, weight decay of 0.   \n\u2013 Ogbn-arxiv: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-4, weight decay of 1e-3. ", "page_idx": 15}, {"type": "text", "text": "\u2022 GRACE: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-7, weight decay of 0.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.01 whlie GNN is tuned with a learning rate of 1e-8, weight decay of 0.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 0.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-5, weight decay of 0.   \n\u2013 Computers: The linear probe is trained using Adam with a learning rate of 0.1, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-4, weight decay of 0. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MaskGAE: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 5e-3, momentum of 0.9 and weight decay of 1e-3 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 1e-3.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 5e-3 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 1e-4.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.015, momentum of 0.9 and weight decay of 5e-4 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 0.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.01 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 0.   \n\u2013 Computers: The linear probe is trained using Adam with a learning rate of 5e-3, momentum of 0.9 and weight decay of 5e-3 whlie GNN is tuned with a learning rate of 2e-4, weight decay of 0.   \n\u2013 Ogbn-arxiv: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 0. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 GraphMAE: ", "page_idx": 16}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9 and weight decay of 1e-4 whlie GNN is tuned with a learning rate of 1e-7, weight decay of 0.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 1e-1 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 1e-3.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 0.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.05 whlie GNN is tuned with a learning rate of 5e-6, weight decay of 0.   \n\u2013 Computers: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.05 whlie GNN is tuned with a learning rate of 5e-5, weight decay of 0.   \n\u2013 Ogbn-arxiv: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-4, weight decay of 1e-3. ", "page_idx": 16}, {"type": "text", "text": "\u2022 GRACE: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-7, weight decay of 0.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.01 whlie GNN is tuned with a learning rate of 1e-8, weight decay of 0.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 1e-6, weight decay of 0.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-5, weight decay of 0.   \n\u2013 Computers: The linear probe is trained using Adam with a learning rate of 0.1, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of 5e-4, weight decay of 0. ", "page_idx": 16}, {"type": "text", "text": "\u2022 MaskGAE: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2013 Cora: The linear probe is trained using Adam with a learning rate of 0.05, momentum of 0.9 and weight decay of 1e-3 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 5e-3.   \n\u2013 Citeseer: The linear probe is trained using Adam with a learning rate of 0.01, momentum of 0.9 and weight decay of 0.01 whlie GNN is tuned with a learning rate of 5e-4, weight decay of 1e-4.   \n\u2013 Pubmed: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 1e-3 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 1e-2.   \n\u2013 Photo: The linear probe is trained using Adam with a learning rate of 0.02, momentum of 0.9 and weight decay of 6e-3 whlie GNN is tuned with a learning rate of 1e-4, weight decay of 0. \u2013 Computers: The linear probe is trained using Adam with a learning rate of 5e-3, momentum of 0.9 and weight decay of 5e-3 whlie GNN is tuned with a learning rate of   \n1e-4, weight decay of 0. \u2013 Ogbn-arxiv: The linear probe is trained using Adam with a learning rate of 3e-3, momentum of 0.9 and weight decay of 0 whlie GNN is tuned with a learning rate of   \n2e-4, weight decay of 0. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "And the hyper-parameters of linear probing is the same as the config file of these pre-training frameworks. The hyper-parameters are different sometimes because their model structures are different, the parameters used to achieve optimal performance are sometimes different. And the hyperparameters of the slim GNNs without model de-correlation are the same with SLIDE. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In the abstract and introduction, we mention that the graph self-supervised learning models are highly redundant at neuron and layer levels and propose a new model to fine-tune both GNNs and predictive layers. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide some experience about CKA scores of presentations in Section 2, and to our best knowledge, many researches about model redundancy do not provide theory assumptions and proofs either. So I think it\u2019s difficult to provide the proofs. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix C.3. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 1 and Section 4. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 4 and Appendix C.3. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Table 3 - 5 in 4.1 and Figure 5 in Section 4.2. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Section 4. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: I have read the NeurIPS Code of Ethics https://neurips.cc/ public/EthicsGuidelines, and I think the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We test self-supervised GNNs on the node classification task. There is not a high risk for misuse in it. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We were unable to find the license for the assets we used. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}]