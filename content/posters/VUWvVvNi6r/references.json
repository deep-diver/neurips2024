{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of self-attention mechanisms and a crucial element of the current paper's focus."}, {"fullname_first_author": "Emmanuel J Cand\u00e8s", "paper_title": "Robust principal component analysis?", "publication_date": "2011-01-01", "reason": "This foundational paper on robust PCA is highly relevant due to the current paper's proposal of RPC-Attention, a robust attention mechanism based on the principles of robust PCA."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "BERT is a highly influential transformer-based model used as a comparative baseline in the current paper's experiments, highlighting the importance of self-attention."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduced the Vision Transformer (ViT), a crucial architecture used as a model in the experimental evaluation of the proposed approach."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Benchmarking neural network robustness to common corruptions and perturbations", "publication_date": "2019-01-01", "reason": "ImageNet-C, used in the current paper's experiments for robustness evaluation, is based on the corruption benchmark proposed in this paper."}]}