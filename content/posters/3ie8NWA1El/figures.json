[{"figure_path": "3ie8NWA1El/figures/figures_4_1.jpg", "caption": "Figure 1: The Overview of HyperPrism Framework. \u2460 Device i executes local update on its own dataset. \u2461 Device i adaptively generates degrees of  and  for the representation and decision parts by hypernetworks. \u2462Device i maps the model to the mirror space by raising the parameters with the degree of power  and  , respectively. \u2463 Device i communicates with its neighbors exchanging local models. \u2464 Device i aggregates received models in mirror space with WPM. \u2465 Device i inverses the model to primal space by the degree of power  and  , then finishes the round.", "description": "This figure illustrates the HyperPrism framework's workflow. Each device performs local updates on its dataset (1), generates adaptive power degrees for model layers using hypernetworks (2), maps the model to a mirror space (3), exchanges models with neighbors (4), aggregates models using WPM in mirror space (5), and finally inverse maps the model back to the primal space (6).", "section": "4 The Design of HyperPrism Framework"}, {"figure_path": "3ie8NWA1El/figures/figures_8_1.jpg", "caption": "Figure 2: The impact of different p and time cost.", "description": "This figure shows the impact of different degrees of p on the model's accuracy and the time cost.  Subfigure (a) shows accuracy curves for different p values, demonstrating faster convergence with higher p. Subfigure (b) shows maximum accuracy achieved at different p values, indicating an optimal p exists. Subfigure (c) compares the time cost of HyperPrism with other baselines; while HyperPrism has a higher cost per round, the faster convergence leads to lower overall time to achieve a specific accuracy.", "section": "6.2 Experimental Results"}, {"figure_path": "3ie8NWA1El/figures/figures_8_2.jpg", "caption": "Figure 3: The Impact of non-IID Degrees", "description": "This figure displays the performance of HyperPrism and other baseline methods (ADOM, SwarmSGD, Mudag, DPSGD) under different non-IID data distributions (Dirichlet distributions with \u03b1 = 0.1 and \u03b1 = 10) for two different models (Logistic Regression with MNIST and CNN with CIFAR-10).  The x-axis represents the training round, and the y-axis represents the accuracy. The shaded areas represent the standard deviation.  The results demonstrate HyperPrism's superior convergence speed and accuracy across various scenarios, especially in non-IID settings.", "section": "6.2 Experimental Results"}, {"figure_path": "3ie8NWA1El/figures/figures_15_1.jpg", "caption": "Figure 1: The Overview of HyperPrism Framework. \u2460 Device i executes local update on its own dataset. \u2461 Device i adaptively generates degrees of pe and po for the representation and decision parts by hypernetworks. \u2462Device i maps the model to the mirror space by raising the parameters with the degree of power pe and p\u03c6, respectively. \u2463 Device i communicates with its neighbors exchanging local models. \u2464 Device i aggregates received models in mirror space with WPM. \u2465 Device i inverses the model to primal space by the degree of power pe and pp, then finishes the round.", "description": "This figure illustrates the HyperPrism framework's workflow. It begins with local model updates on each device's dataset, followed by adaptive degree generation using hypernetworks.  The models are then mapped to a mirror space, aggregated using Weighted Power Mean (WPM), and finally inverse-mapped back to the primal space. This process is iterative, with communication between neighboring devices at each step.", "section": "4 The Design of HyperPrism Framework"}]