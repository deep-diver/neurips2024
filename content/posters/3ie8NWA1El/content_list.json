[{"type": "text", "text": "HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over non-IID Data and Time-varying Communication Links ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haizhou Du\u2217 Shanghai University of Electric Power Shanghai, China ", "page_idx": 0}, {"type": "text", "text": "Yijian Chen Shanghai University of Electric Power Shanghai, China ", "page_idx": 0}, {"type": "text", "text": "Ryan Yang\u2217\u2020   \nMassachusetts Institute   \nof Technology   \nMA, USA   \nYuchen Li   \nShanghai Jiao Tong   \nUniversity   \nShanghai, China   \nLinghe Kong   \nShanghai Jiao Tong   \nUniversity   \nShanghai, China ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While Distributed Machine Learning (DML) has been widely used to achieve decent performance, it is still challenging to take full advantage of data and devices distributed at multiple vantage points to adapt and learn; this is because the current linear aggregation paradigm cannot solve inter-model divergence caused by (1) heterogeneous learning data at different devices (i.e., non-IID data) and (2) in the case of time-varying communication links, the limited ability for devices to reconcile model divergence. In this paper, we present a non-linear class aggregation framework HyperPrism that leverages Kolmogorov Means to conduct distributed mirror descent with the averaging occuring within the mirror descent dual space; HyperPrism selects the degree for a Weighted Power Mean (WPM), a subset of the Kolmogorov Means, each round. Moreover, HyperPrism can adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings. We perform rigorous analysis and experimental evaluations to demonstrate the effectiveness of adaptive, mirror-mapping DML. In particular, we extend the generalizability of existing related works and position them as special cases within HyperPrism. For practitioners, the strength of HyperPrism is in making feasible the possibility of distributed asynchronous training with minimal communication. Our experimental results show HyperPrism can improve the convergence speed up to $98.63\\%$ and scale well to more devices compared with the state-of-the-art, all with little additional computation overhead compared to traditional linear aggregation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The proliferation of edge devices, such as mobile phones, wearable devices and unmanned aerial vehicles, has resulted in an exponential growth in diverse data types (e.g., images, sound and text). Addressing this surge necessitates advanced techniques capable of accurately, quickly, and practically processing vast amounts of data through efficient and scalable algorithms. Distributed Machine Learning (DML) has gained significant traction in recent years as a strategy to minimize data transfer and computational costs by bringing computation closer to the data sources. It has become a natural solution for scaling up machine learning while preserving data privacy in various domains (e.g., large language model training [39, 33], autonomous driving [60, 37], military applications [56], web search [29, 30, 28] and recommendation [32, 27, 31]). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While existing DML attempts have made significant progress, two technical barriers remain in realistic scenarios. First, data heterogeneity is one of the most critical concerns. Specifically, in a DML system where mobile devices serve as computing nodes, the data generated is typically non-independent and identically distributed (non-IID), which means the data distribution may be unbalanced across different classes or categories. Consequently, the model may exhibit bias toward the majority class or dominant data patterns, leading to suboptimal performance on the minority class or rare patterns. Second, traditional DML methods usually deliver poorer performance under the time-varying communication condition. Specifically, devices holding critical data may unexpectedly go offline or become out of range during the training, causing changes in the communication links. These interruptions can cause information loss in the aggregation, making local models no longer interchangeable and drifting away from the global model. We can summarize these powers of deviation and drift as \u201cdivergence forces\u201d, which drastically slows down the convergence and significantly impacts the efficiency and effectiveness of the training process in DML. ", "page_idx": 1}, {"type": "text", "text": "In order to tackle the above issues, we present a novel decentralized DML framework HyperPrism, which utilizes mirror descent [3] and employs adaptive mapping functions to project models into a mirror space, with both aggregation and gradient steps then carried out in the mapped space. Moreover, HyperPrism leverages the concept of \u201cweighted power means\u201d (WPM) as the aggregation function, raising the model parameters to the power of $p$ , and uses HyperNetworks [12] to adaptively adjust the power degree $p$ . In summary, the main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We study the problem of divergence forces in decentralized DML, where we particularly focus on two technical barriers due to data heterogeneity (i.e., non-IID data) and time-varying communication links. To the best of our knowledge, it is the first work to simultaneously address the challenges of non-IID data and time-varying communication links in realistic DML scenarios.   \n\u2022 We propose a non-linear class aggregation DML framework HyperPrism based on Kolmogorov Means, which can also be seen as mapping models to mirror space for aggregation, and we instantiate the means with an adaptive $p$ power function to enhance the convergence speed and scalability. HyperPrism achieves superior performance in\u221a its depen\u221adence on the number of devices $m$ and the power degree $p$ , upg\u221arading f\u221arom $m{\\sqrt{m}}$ to $\\bar{m}\\,\\%\\$ . This achievement also reduces the optimality gap [2] from $\\bar{\\sqrt{m}}$ to $\\sqrt[p]{m}$ .   \n\u2022 We conduct rigorous analysis and prove that the loss bound of HyperPrism is $O\\big(\\big(\\frac{m^{P+2}}{T_{.}}\\big)^{\\frac{1}{P+1}}\\big)$ . In cases where few communication epochs can occur (i.e., $T\\,\\leq\\,m)$ ), employing a larger value of $p$ yields improvements over traditional linear aggregation. Our theoretical results are consistent with state-of-the-art bounds in distributed gradient/mirror descent and single-device mirror descent.   \n\u2022 We carry out comprehensive experiments to assess the performance of HyperPrism framework. The experimental results demonstrate that HyperPrism achieves a remarkable acceleration in convergence speed with improvements of up to $98.63\\%$ . Moreover, HyperPrism also shows increased scalability in settings characterized by time-varying communication. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Decentralized DML with Data Heterogeneous. Numerous studies have addressed non-IID data using linear solutions, such as local fine-tuning of a global model [4, 9, 8, 10], personalization in Federated Learning (FL) as a meta-learning objective [15], knowledge distillation[61], and prototype aggregation [53]. Furthermore, Li et al. analytically demonstrate the limitations of FedAvg on non-IID data [26]. Li et al. propose a variant of FedAvg by incorporating a penalty term in the local objective function [25]. Liu et al. propose an algorithm that capture similarities between clients to compute personalized aggregation weights for personalized FL [36]. Recently, Aketi et al. [1] propose a tracking-based method to mitigate the impact of heterogeneous data distribution in DML without introducing communication overhead. However, these methods, less explored in decentralized DML, are confined by the boundaries of linear aggregation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "DML with Time-varying Communication setting. Many DML models and their variants have been proposed to process huge amounts of data locally over time-varying communication settings. Kovalev et al. propose the ADOM and $\\mathrm{ADOM+}$ method for decentralized optimization over time-varying networks with projected Nesterov gradient descent, respectively [18, 20]. Koloskova et al. introduce a framework covering local SGD updates and synchronous and pairwise gossip updates on adaptive network topology [17]. De Vos et al. [5] introduce Epidemic Learning in decentralized learning, leading to faster convergence and improved performance by randomly changing topologies. Nedic et al. [44, 42] tackle the DML with topology dynamicity from the consensus perspective. They propose a model aggregation method for agents in a time-varying network topology to collaboratively solve a convex objective function, with a convergence guarantee. Moreover, future generation DML systems [20, 19, 7] also consider these time-varying communication settings as an important research area, where the naturally time-varying connectivity among pairs of mobile devices or mobile devices and edge servers will be dictated by their physical proximity. ", "page_idx": 2}, {"type": "text", "text": "In this work, we focus on solving two barriers due to data heterogeneity (i.e., non-IID data) and time-varying communication links and conduct rigorous analysis to show that prior methods suffer from lower accuracy, higher loss, and slower convergence speed over time-varying networks than under a fixed topology. Additionally, their methods use linear averages for aggregation, which our framework encompasses as a particular case. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "bWye  ac doinrseicdteerd  dgercaepnht $m$ , dwevhiecrees vdeer ntiotmee -vveartriyciensg  acnod eids $G(t)=(V,\\varepsilon(A^{(t)}))$ $V$ $A^{(t)}=[a_{11}^{(t)},a_{12}^{(t)},...,a_{m m}^{(t)}]$ the weight matrix of the topology graph. Then the $\\varepsilon(A^{(t)})$ denotes the set of directed time-changing edges between vertexes. In each time interval, we assume that the communication links between devices are symmetrical and restricted, causing random disconnections and reconnections. Each device $i$ has its own private dataset, denoted by $D_{i}$ . Let $\\mathbf{w}=(w_{1},w_{2},\\dots,w_{m})$ denote the collection of all local models, where $w_{i}$ is the model held at device $i$ . Then, Device $i$ constructs its local loss function as $f_{i}(w)=\\mathbb{E}_{\\zeta_{i}\\sim D_{i}}[\\mathcal{F}(w_{i};\\zeta_{i},G(t))]$ . Then, devices aim to solve the following optimization problem collaboratively: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{d}}F(\\mathbf{w})=\\sum_{i=1}^{m}f_{i}(\\mathbf{w}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "without sharing local data (i.e., without revealing $f_{i.}$ ). ", "page_idx": 2}, {"type": "text", "text": "3.2 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In recent years, model merging has developed non-linear aggregation methods. One motivation for non-linear aggregation is the decreased variance in the original parameter space. HyperPrism is designed to combat stale gradients from diverged models with synchronization. ", "page_idx": 2}, {"type": "text", "text": "In existing work, synchronization is achieved with an aggregation mechanism, usually a mean or median, with clipping. Inspired by similarities between Mirror Descent [46] and Quasi-Arithmetic Means [16], also known as Kolmogorov Means, of the form n1 kn=1 f(xk) , HyperPrism maps models to the dual domain before averaging to better align with the geometry of the objective function. In implementation, we focus on the special case of $\\begin{array}{r}{\\phi(w)\\,=\\,\\frac{\\daleth}{p+1}\\|w\\|^{p+1}}\\end{array}$ , transforming models as $w\\rightarrow w^{p}$ , where HyperPrism therefore replaces linear means with the following weighted power mean: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{w}}_{i}^{(t)}=(\\sum_{j=1}^{m}a_{i j}^{(t)}{(w_{j}^{(t)})}^{p})^{\\frac{1}{p}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ai(jt) is the aggregation weights of device j in device i at t round. The WPM is a special case of the general strategy of averaging in a mirror descent dual space, but it is particularly useful due to its ease of computation and convexity guarantees. In general, the aggregation step of HyperPrism is the following Equation (3): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\boldsymbol{w}}}_{i}^{(t)}=[\\nabla\\phi]^{-1}(\\sum_{j=1}^{m}a_{i j}^{(t)}\\nabla\\phi(\\boldsymbol{w}_{j}^{(t)})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "But we choose to focus on $\\begin{array}{r}{\\phi(w)=\\frac{1}{p+1}\\|w\\|^{p+1}}\\end{array}$ which gives $\\nabla\\phi(w)=w^{p}$ (termwise exponentiation). This allows for simple run-time computation. For more intuition, in the $p=\\infty$ limit the distributed averaging consensus problem [47] turns into an easier \u201cdistributed maximum problem,\" which is what allows for HyperPrism\u2019s tighter synchronization. ", "page_idx": 3}, {"type": "text", "text": "3.3 Distributed Mirror Descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The phrase \u201cdistributed mirror descent\u201d can have multiple interpretations. In the existing literature, it typically refers to the process of taking local mirror descent steps and then linearly aggregating [23, 48, 6, 59] In the case of HyperPrism, mirror descent is transformed into the distributed algorithm in a different manner. It still takes local mirror descent steps but also uses the aggregation function in Equation (3). This approach proves to be more effective because, under the optimal model $\\mathbf{w}^{*}$ , distributed mirror descent with Equation (3) remains stable, while the linear aggregation functions do not. In other papers\u2019 analysis (e.g., [11], [58]), this inaccuracy is not obvious since the dominant terms are often related to communication costs, but in the perfect communication case, using a generalized mean is necessary for exact theoretical convergence. ", "page_idx": 3}, {"type": "text", "text": "3.4 HyperNetworks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Hypernetworks (HNs) are deep neural networks used to generate weights of other target networks [12]. HNs can learn the mapping relationship between the embedding vector and the target network, and adaptively generate the target network based on the input. Shamsian et al. apply HNs in federated learning to generate personalized model parameters [50]. Ma et al. present pFedLA [38] using HNs to generate aggregation weights of each model layer in personalized federated learning. Previous works [53, 55] found the different layer parameters of the local model have different impacts on model aggregation. For example, the locally learned feature representations are prone to over-ftiting and thus cannot generalize well when each device only has insufficient data. In HyperPrism, the mapping function chosen is a form of the Bregman divergence, specifically $\\nabla\\phi(w)=w^{p}$ , where $p$ represents the degree of WPM. Our experiments indicate that different functional layers of the model respond differently to the degree of $p$ . Increasing $p$ has a more significant impact on accelerating convergence in linear or fully-connected layers. This observation inspires us to select the appropriate $p$ for different functional layers adaptively. Through the chain rule [38], we demonstrate that HN can effectively correlate the mirror mapping with the objective function, enabling it to discover the optimal $p$ for each device\u2019s model parameters in each round. ", "page_idx": 3}, {"type": "text", "text": "4 The Design of HyperPrism Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "With the above preliminaries and motivation presented, we now give the design of HyperPrism framework. The framework assumes that each device $i$ maintains its local model denoted by $w_{i}$ ; for simplicity of specification, we use a round-based specification, in which the model holds by device $i$ after $t$ communication rounds is denoted as $w_{i}^{(t)}$ . ", "page_idx": 3}, {"type": "text", "text": "The framework first chooses the aggregation weights $a$ . Each $a_{i j}^{(t)}$ , defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\boldsymbol{a}_{i j}^{(t)}=\\operatorname*{min}\\{\\boldsymbol{e}_{i j}^{(t)},\\boldsymbol{e}_{j i}^{(t)}\\},}&\\\\ &{\\boldsymbol{a}_{i i}^{(t)}=1-\\displaystyle\\sum_{j\\neq i}\\boldsymbol{a}_{i j}^{(t)},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "3ie8NWA1El/tmp/324a94391fa04a445dd0a080f7825308ddfbd6137a232cdf4f9248f789a1420e.jpg", "img_caption": ["Figure 1: The Overview of HyperPrism Framework. $\\circled{1}$ Device $i$ executes local update on its own dataset. $\\circledast$ Device $i$ adaptively generates degrees of $p_{\\theta}$ and $p_{\\varphi}$ for the representation and decision parts by hypernetworks. $\\circled{3}$ Device $i$ maps the model to the mirror space by raising the parameters with the degree of power $p_{\\theta}$ and $p_{\\varphi}$ , respectively. $\\circledast$ Device $i$ communicates with its neighbors exchanging local models. $\\textcircled{5}$ Device $i$ aggregates received models in mirror space with WPM. $\\circledcirc$ Device $i$ inverses the model to primal space by the degree of power $p_{\\theta}$ and $p_{\\varphi}$ , then finishes the round. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where eij = $\\begin{array}{r}{e_{i j}^{(t)}=\\frac{1}{N_{i}^{(t)}+1}}\\end{array}$ when $i,j$ are connected, otherwise $e_{i j}^{(t)}=0$ . $N_{i}^{(t)}$ is the number of neighboring devices $i$ at $t$ -th round. $a_{i j}^{(t)}$ generated in this way satisfy Assumption 5.1. Note that the connectivity ei(jt) is determined by underlying communication systems, which consider both feasibility and security requirements (e.g., low probability of detection). HyperPrism applies to generic DML, the parameter server and all-reduce frameworks can be seen as special cases (where all ai(jt) values equal $\\textstyle{\\frac{1}{m}}$ ). ", "page_idx": 4}, {"type": "text", "text": "In summary, the core of HyperPrism is that at every round $t$ , device $i$ computes its local model for the next round $t+1$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i}^{(t+1)}=[\\nabla\\phi]^{-1}(\\sum_{j=1}^{m}a_{i j}^{(t)}\\nabla\\phi(w_{j}^{(t)})-\\eta\\nabla f_{i}(w_{i}^{(t)})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla\\phi$ is the mapping function with a selected degree of $p$ for each round. ", "page_idx": 4}, {"type": "text", "text": "4.2 Adaptive Degree of Power $p$ Mapping ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To determine the $p$ of various layers at each round, HyperPrism establishes a relationship between the optimization problem and the degree of $p$ by utilizing hypernetworks, which adaptively choose the optimal $p$ for different layers to achieve the best performance. ", "page_idx": 4}, {"type": "text", "text": "Without loss of generality, HyperPrism also decouples the local model into the representation and decision parts. The representation part $\\theta$ includes components like convolutional and embedding layers. The decision part $\\varphi$ includes components like fully-connected layers. Thus, for device $i$ , we have $w_{i}\\,=\\,\\{w_{\\theta,i}^{\\bar{}},w_{\\varphi,i}^{\\bar{}}\\}$ . Each device $i$ holds a local hypernetwork $H N_{i}$ and a randomly generated embedding vector $v_{i}$ . Every hypernetwork consists of several fully-connected layers and employs the softmax for the output layer. $H N_{i}$ takes $v_{i}$ and gradient of the local model as input, then output $p_{i}=\\{p_{\\theta,i},p_{\\varphi,i}\\}$ for $w_{\\theta,i}$ and $w_{\\varphi,i}$ parts, respectively. Then the mapping function evolves to $\\nabla\\phi(w_{i})=\\big\\{(w_{\\theta,i})^{p_{\\theta,i}},(w_{\\varphi,i})^{p_{\\varphi,i}}\\big\\}$ . The hypernetwork on device $i$ can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i}=H N_{i}(v_{i};\\psi_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\psi_{i}$ denotes the parameters of $H N_{i}$ . Hence, a new objective function can be derived from the original problem as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}F(\\mathbf{w})=\\sum_{i=1}^{m}f_{i}{\\big(}(w_{i})^{H N_{i}(v_{i};\\psi_{i})}{\\big)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "HyperPrism can transform the optimization problem for model parameters $w_{i}$ into the HN\u2019s $v_{i}$ and $\\psi_{i}$ . HNs adaptively output $p_{\\theta,i}$ and $p_{\\varphi,i}$ based on the input, and simultaneously update both $v$ and $\\psi$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 HyperPrism Framework. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: datasets $\\{D_{1},D_{2},\\ldots,D_{m}\\}$ , learning rate $\\eta$ , the number of Rounds $T$ .   \n2: Output: the final model of all devices after $T$ rounds. $w^{(T)}=w_{1}^{(T)},\\dots,w_{i}^{(T)},\\dots,w_{m}^{(T)}\\}$   \n3: Initialize all devices\u2019 models $w^{(0)}$ , hypernetworks $\\psi^{(0)}$ , and embedding vectors $\\ensuremath{\\boldsymbol{v}}^{(0)}$ .   \n4: for $t=0$ to $T$ do   \n5: for each device $i$ in parallel do   \n6: Local Update: $\\boldsymbol{w}_{i}^{(\\bar{t})}\\leftarrow\\boldsymbol{w}_{i}^{(t)}-\\eta\\nabla f_{i}^{(t)}$   \n7: Compute $p_{\\theta,i}^{(t)}$ and $p_{\\varphi,i}^{(t)}$ by $H N_{i}(v_{i}^{(t)};\\psi_{i}^{(t)})$   \n8: Compute $\\nabla\\phi(w_{i}^{(t)})\\gets\\{(w_{\\theta,i}^{(t)})^{p_{\\theta,i}^{(t)}},(w_{\\varphi,i}^{(t)})^{p_{\\varphi,i}^{(t)}}\\}.$ .   \n9: Send $\\nabla\\phi(w_{i}^{(t)})$ and $N_{i}^{(t)}$ to $i$ \u2019s neighbors.   \n10: Receive $\\nabla\\phi(w_{j}^{(t)})$ and N (t) from neighbors.   \n11: Compute weights $a_{i j}^{(t)}$ as Equation (4), (5).   \n12: Aggregate received models using $a_{i j}^{(t)}$ .   \n13: Update $w_{i}^{(t+1)}$ according to Equation (6).   \n14: Update vi(t $v_{i}^{(t+1)}$ \u03c8i(t+1)as Equation (11), (12).   \n15: end for   \n16: end for   \n17: return $w_{i}^{(T)}$ . ", "page_idx": 5}, {"type": "text", "text": "by gradient descent at each round. Specifically, the gradient of $v_{i}$ and $\\psi_{i}$ can be computed based on the chain rule [38] as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{v_{i}}f_{i}=(\\nabla_{v_{i}}\\overline{{w}}_{i})^{T}\\nabla_{\\overline{{w}}_{i}}f_{i},}\\\\ {\\nabla_{\\psi_{i}}f_{i}=(\\nabla_{\\psi_{i}}\\overline{{w}}_{i})^{T}\\nabla_{\\overline{{w}}_{i}}f_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, $v_{i}$ and $\\psi_{i}$ can be represented as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{i}^{(t+1)}=v_{i}^{(t)}-\\eta\\nabla_{v_{i}}^{(t)}f_{i}^{(t)},}\\\\ {\\psi_{i}^{(t+1)}=\\psi_{i}^{(t)}-\\eta\\nabla_{\\psi_{i}}^{(t)}f_{i}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 demonstrates the full procedure of HyperPrism. In our real deployment, we adopt the asynchronous protocol. ", "page_idx": 5}, {"type": "text", "text": "5 Analytical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We rigorously analyze the properties of HyperPrism. We fully introduce all assumptions, analyze the convergence behavior of HyperPrism, and finally compare HyperPrism to previous works. ", "page_idx": 5}, {"type": "text", "text": "5.1 Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Assumption 5.1 (Connectivity). The network graph $G(t)=(V,\\varepsilon(A^{(t)}))$ and the connectivity weight matrix $A^{(t)}$ satisfy the following: ", "page_idx": 5}, {"type": "text", "text": "\u2022 A(t) is doubly stochastic for all t \u22651; that is jm=1 ai(jt) = 1 and i=1 aij $\\textstyle\\sum_{i=1}^{m}a_{i j}^{(t)}=1$ . \u2022 There exists a scale $\\zeta>0$ , such that $a_{i j}^{(t)}\\geq\\zeta$ for all $i$ and $t\\geq1$ , if $\\{i,j\\}\\in E_{t}$ . \u2022 There exists an integer $B\\geq1$ such that the graph $(V,E_{k B+1}\\cup\\cdot\\cdot\\cdot\\cup E_{(k+1)B})$ is strongly connected for all $k\\geq0$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 5.2 (Bregman Divergence). The Bregman Divergence of a function $\\phi$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{\\phi}(x,y)=\\phi(x)-\\phi(y)-\\langle\\nabla\\phi(y),x-y\\rangle.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that for $\\phi(x)=\\|x\\|^{2},D_{\\phi}(x,y)=\\|x-y\\|^{2}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 5.3 (Uniform Convexity). Consider a differentiable convex function $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , an exponent $r\\geq2$ , and a constant $\\sigma>0$ . Then, $\\phi$ is $(\\sigma,r)$ -uniformly convex with respect to a $\\lVert\\cdot\\rVert$ norm if for any $x,y\\in\\mathbb{R}^{d}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi(x)\\geq\\phi(y)+\\langle\\nabla\\phi(y),x-y\\rangle+\\frac{\\sigma}{r}\\|x-y\\|^{r}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that for $r=2$ , this is known as strong convexity. This assumption also implies that $D_{\\phi}(x,y)\\ge$ ${\\frac{\\sigma}{r}}\\|x-y\\|^{r}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.4 (Smooth Gradient). Assume that the functions $f_{i}$ are convex with its gradients $\\nabla f_{i}(\\cdot)$ satisfying $L$ -Lipschitz continuity [59], namely: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\|\\leq L\\|x-y\\|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all $x,y$ pairs. ", "page_idx": 6}, {"type": "text", "text": "This final assumption is used in nearly every mathematical analysis of DML, including [54, 3, 52, 59, 43, 51, 49]. In our analysis, it is used to bound the distance between local models. ", "page_idx": 6}, {"type": "text", "text": "5.2 Weighted Power Mean ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we focused on the weighted power mean, generated by using $\\begin{array}{r}{\\phi(x)=\\frac{1}{p+1}\\|x\\|^{p+1}}\\end{array}$ which gives $\\nabla\\phi(\\boldsymbol{x})=\\boldsymbol{x}^{p}$ . Such $\\phi$ is uniformly convex as seen in Proposition 5.5, and the rest of the analysis will be generalized to all uniformly convex $\\phi$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.5 (Uniform Convexity of Power Functions). For $p~\\geq~1$ , the function $\\varphi_{p}(x)\\;=\\;$ $\\textstyle{\\frac{1}{p+1}}\\|x\\|^{p+1}$ is uniformly convex with degree $p+1$ . This is because $\\|\\nabla\\varphi_{p}(x)-\\nabla\\varphi_{p}(y)\\|=\\|x^{p}-$ $\\begin{array}{r}{\\bar{y}^{p}\\|\\geq\\frac{1}{2^{p-1}}\\|x-y\\|}\\end{array}$ , and as a corollary we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{\\varphi_{p}}(x,y)\\geq{\\frac{1}{2^{p-1}}}\\cdot{\\frac{1}{p+1}}\\|x-y\\|^{p+1}={\\frac{1}{2^{p-1}}}\\varphi_{p}(x-y).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given the function $\\phi$ , HyperPrism instructs local models to take a WPM of received models. Using the Connectivity Assumption (Assumption 5.1), we can bound the distance between local models. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.6 (Consensus). Under Assumption 5.1, for each device $i,$ , after $t$ rounds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\phi(w_{i}^{(t)})-\\nabla\\phi(\\overline{{w^{(t)}}})\\|}\\\\ &{\\leq\\vartheta(\\kappa^{t-1}\\displaystyle\\sum_{j=1}^{m}\\|\\nabla\\phi(w_{j}^{(0)})\\|+\\displaystyle\\frac{m\\eta G_{l}}{1-\\kappa}+2\\eta G_{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{=\\left(1-\\frac{\\zeta}{4m^{2}}\\right)^{-2},\\,\\kappa=\\left(1-\\frac{\\zeta}{4m^{2}}\\right)^{\\frac{1}{B}},G_{l}=2L\\left(\\operatorname*{max}f(w_{i}^{(t)})-f^{*}\\right)}\\end{array}$ and $\\zeta$ is a constant related to the graph connectivity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.7 (Convergence Behavior). Consider a $\\left({\\frac{1}{2^{p-1}}},p\\!+\\!1\\right)$ uniformly convex $\\phi$ and the sequence $w_{i}^{(t)}$ under Algorithm $^{\\,I}$ with constant step size $\\eta$ . Then, under Assumptions 5.1, and 5.4, if $x^{*}$ is the value that minimizes $\\begin{array}{r}{F({\\pmb w})=\\sum_{i=1}^{m}f_{i}(\\dot{\\boldsymbol w})}\\end{array}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{t}[F(\\overline{{w}}^{(t)})-F(x^{*})]}\\\\ &{\\displaystyle\\leq\\frac{4m G_{l}}{T}\\sum_{t=0}^{T-1}\\sqrt{\\frac{\\vartheta}{2}(\\kappa^{t-1}\\displaystyle\\sum_{j=1}^{m}\\lVert\\nabla\\phi(w_{j}^{(t)})\\rVert+\\frac{m\\eta G_{l}}{1-\\kappa}+2\\eta G_{l})}}\\\\ &{\\displaystyle+\\,m\\cdot\\frac{p}{p+1}\\sqrt{2^{r}\\cdot\\eta\\cdot G_{l}^{p+1}}+\\frac{m\\cdot D_{\\phi}(x^{*},\\overline{{w}}^{(0)})}{\\eta T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In this Theorem 5.7, the first term uses Lemma 5.6, and is caused by the differences between local models held at different devices. This is the effect of the diverging forces mentioned in earlier sections. Note that $\\vartheta$ and $\\kappa$ are constants related to the graph connectivity, reflecting the influence of time-varying communication links. The second term represents the error caused by a non-zero learning rate in the Distributed Mirror Descent process, and the third term represents the lingering effects of the initialization. For more proof details, please refer to the Appendix. ", "page_idx": 6}, {"type": "text", "text": "The first term becomes $O(m\\cdot\\sqrt[P]{m}\\eta)$ because since $\\kappa<1$ , the $\\kappa^{t}$ term goes to 0 as $t$ gets large. The second term is ${\\cal{O}}(m\\,\\sqrt[n]{\\eta})$ is smaller than the first term, so we drop it. The final term is clearly $\\textstyle{\\bar{O}}({\\frac{m}{\\eta T}})$ . ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.9. For $\\eta=O(\\sqrt[p+]{\\frac{T^{p}}{m}})$ , the upper bound becomes $\\begin{array}{r}{O(m\\sqrt[p]{\\eta m}+\\frac{m}{\\eta T})=O(\\sqrt[p+\\!\\!\\sqrt{\\frac{m^{p+2}}{T}}).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "5.3 Comparison to Previous Work ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "3ie8NWA1El/tmp/5ddda85cf22e2450271bb6c2083a71672bdeb1a6af16d105922b7512dc5c309b.jpg", "table_caption": ["Table 1: Convergence Rates in terms of $\\eta,p,m$ , and $T$ "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1 summarizes the big $O$ notation convergence rates of $f(\\overline{{w}}^{(t)})-f^{*}$ . Previous works also assume bounded gradients. Our analysis recovers the same bounds as the state-of-the-art in DML and single-device mirror descent with generic uniform convexity assumptions. Nedic [44] is standard gradient descent, and thus has $\\begin{array}{r}{\\phi(x)=\\frac{1}{2}\\|x\\|^{2}}\\end{array}$ , which is $(1,2)$ -uniformly convex, corresponding to $p=1$ . Yuan [59] considers distributed mirror descent under strong convexity $(r=2)$ ) equivalent to $p=1$ , and based on their analysis, the bound should be $O(\\eta m^{2}+\\overline{{{\\frac{m}{\\eta T}}}})$ . ", "page_idx": 7}, {"type": "text", "text": "6 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The experimental platform consists of 8 Nvidia Tesla T4 GPUs, 4 Intel XEON CPUs, and 256GB of memory. All the models and training scripts are implemented in RAY [40] and PyTorch [24]. ", "page_idx": 7}, {"type": "text", "text": "HyperNetworks Setup. We construct a hypernetwork model comprising three fully-connected layers and two additional output layers activated using softmax. The outputs of the fully-connected layers are fed into each of the two output layers to generate the degree of $P$ for various parts. ", "page_idx": 7}, {"type": "text", "text": "Time-varying Communication Links Setup. We employ the NS3 platform [13] to simulate realistic time-varying communication environments consisting of multiple distributed devices. Each device is configured with the WiFi 802.11a protocol and communication among themselves in Ad-Hoc mode. To quantify the degree of connectivity of the communication links, we define the topology density as the ratio of available tunnels to the total tunnels. ", "page_idx": 7}, {"type": "text", "text": "Models and Dataset. We use MNIST [22] and CIFAR-10 [21] datasets distributed among devices in non-IID settings. We construct two models based on layer functionalities. The Logistic Regression (LR) [14] model consists solely of linear layers used for the MNIST dataset. On the other hand, the CNN model consists of both convolutional and fully-connected layers used for CIFAR-10. ", "page_idx": 7}, {"type": "text", "text": "Non-IID Data Partitioning. To distribute datasets in a non-IID fashion, we employ Dirichlet distribution [35] to allocate all samples among devices. The Dirichlet Degree $\\alpha$ is used to control the non-IID degree. The $\\alpha=0.1$ represents the extreme scenario where each device possesses samples from only one class, while $\\alpha=10$ equals the IID scenario. These distributions reflect a challenging and realistic training environment. ", "page_idx": 7}, {"type": "text", "text": "Metrics. We consider two metrics to measure the performance of HyperPrism. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Average Accuracy. We evaluate the performance of the local model per device using a global test set that contains samples with all categories. The average Top-1 accuracy of all devices is calculated in each round to measure overall performance and convergence rate. \u2022 Convergence Speed. We track the loss of each round and the number of rounds to investigate the rounds needed to reach the convergence point for specific accuracy and loss. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We conduct a comparative analysis of HyperPrism with state-of-the-art methods for DML in non-IID data and time-varying communication links, including $p=1$ methods, which is one of the most influential and widely applied works in DML studies. SwarmSGD [41], Mudag [57] and ADOM [20]. To ensure a fair comparison, we made minor adjustments to each baseline method. ", "page_idx": 8}, {"type": "text", "text": "\u2022 $p{=}I$ . We define it as a class of methods using a linear aggregation function, including DPSGD [34], and its variants [43, 45]. These methods can be seen as special cases of HyperPrism without the mirror mapping process. \u2022 SwarmSGD. We set the number of local SGD updates equal to 1, where the selected pair of devices performs only a single local SGD update before aggregation. It can also be viewed as a special case of HyperPrism where the topology density is very low. \u2022 ADOM and Mudag. We set the condition number $k$ to 10, and the number of features $d$ equals the number of classes. The gossip matrix $W(t)$ at round $t$ is chosen to be the Laplacian of time-varying communication links divided by its largest eigenvalue. ", "page_idx": 8}, {"type": "text", "text": "Hyperparameters. For all experiments, the learning rate and batch size are both fixed at 0.01 and 128. We generate time-varying communication graphs with different sizes and densities and evaluate 100 rounds total. The graph changes every round. ", "page_idx": 8}, {"type": "text", "text": "6.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "3ie8NWA1El/tmp/8968ecdcb4fbe234de55c05f795b31e77635250290b2e27ccc87ba5176de8cc0.jpg", "img_caption": ["Figure 2: The impact of different $p$ and time cost. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of different degrees of $p$ . We present the impact of varying degrees of $p$ in HyperPrism on model performance in Figure 2. In Figure 2(a), it is evident that the model converges more swiftly and attains greater accuracy as $p$ grows larger. Figure 2 (b) demonstrates that accuracy exhibits a significant fluctuation with different $p$ . This underscores the substantial impact of the $p$ value selection on the performance of HyperPrism. These findings highlight the importance of choosing an appropriate value for $p$ in HyperPrism to achieve optimal performance. ", "page_idx": 8}, {"type": "text", "text": "Comparison of time cost. In HyperPrism, each hypernetwork contains only 3 linear layers with 64 nodes per layer to ensure a minimal extra computational resource cost. We record the time cost in Figure 2(c). Although HyperPrism does result in a higher time cost per iteration, it notably decreases the total number of rounds required for convergence, thereby reducing the overall time needed to achieve a specific accuracy. ", "page_idx": 8}, {"type": "text", "text": "Performance of HyperPrism. To showcase the practicality of HyperPrism, we consider the basic configuration with $D i r i c h l e t=0.1,$ , density $=0.5$ , and $m=50$ devices. The accuracy results for all benchmarks and HyperPrism are presented in Figure 3. The convergence speed is summarized in Table 2, 3, 4. Notably, HyperPrism outperforms all benchmarks across all models. It demonstrated a remarkable superior performance over state-of-the-art baselines with convergence accuracy and convergence speed improvements of up to $4.87\\%$ and $98.63\\%$ , respectively. ", "page_idx": 8}, {"type": "image", "img_path": "3ie8NWA1El/tmp/19545d2514c6ea38f415e36a20f0d99f917b60cdfbbc17224040007a8f0e7407.jpg", "img_caption": ["(a) LR, Dirichlet=0.1 (c) LR, Dirichlet=10 (a) CNN, Dirichlet=0.1 (c) CNN, Dirichle ${\\it=}10\\$ Figure 3: The Impact of non-IID Degrees "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "3ie8NWA1El/tmp/902488360b928ff69fb234dec4e0dd0b00ebcc173f8b206cfc465dc38cbb3c6c.jpg", "table_caption": ["Table 2: Comparison on Different non-IID Degree "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "3ie8NWA1El/tmp/abccf1472a9ebf0549973a6319c9feeb33bcd1c6923d92e2c9956388d9437a00.jpg", "table_caption": ["Table 3: Comparison on Different Connection Densities "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "3ie8NWA1El/tmp/73dc187f9923b526f98ffe0d0762db79205f14cae42bfcc6ffa78ba1e3101065.jpg", "table_caption": ["Table 4: Comparison on Different Device Numbers "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Impact of non-IID. To investigate the impact of the non-IID Dirichlet degree on HyperPrism, we experiment with various $\\alpha=0.1,1,10$ . The corresponding results are presented in Figure 3, and Table 2. All methods exhibit poorer performance as the non-IID degree becomes more extreme, which aligns with common intuition. However, HyperPrism demonstrates enhanced stability and faster convergence speed, especially at highly non-IID degrees. ", "page_idx": 9}, {"type": "text", "text": "Scalability. We further evaluate the performance of HyperPrism with varying scales $m\\in$ $\\{20,50,100\\}$ . The results are summarized in Table 4. It can be noticed that most of the baseline methods exhibit deteriorating performance as the number of devices increases. The ADOM even barely converges under 100 devices. In contrast, HyperPrism is minimally affected by the scale and maintains superior acceleration and model performance. ", "page_idx": 9}, {"type": "text", "text": "Communication Graph Densities. To further analyze the impact of connected densities on model performance, we present the performance and convergence speed of HyperPrism with various density $\\in\\{0.2,0.5,0.\\bar{8}\\}$ in Table 3. In the extreme non-IID case, the performance of baselines deteriorates as the communication becomes denser. Particularly, ADOM exhibits significant fluctuations at a density of 0.8. However, HyperPrism maintains better performance across different densities. This can be attributed to the fact that as the communication becomes denser, the information exchanged between devices becomes more complex. Given HyperPrism\u2019s resilience to non-IID scenarios, it maintains good performance in such cases. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we studied the important problem of divergence forces in decentralized DML, due to data heterogeneity (i.e., non-IID data) and time-varying communication links. We propose a non-linear class aggregation DML framework with adaptive Kolmogorov Means for aggregation to enhance the convergence speed and scalability. HyperPrism\u221a achieves \u221asuperior performance in its dependence on the number of devices $m$ , improving from $m{\\sqrt{m}}$ to $m\\,\\Bar{\\sqrt{m}}$ , and achieving optimality in the limit $p\\rightarrow\\infty$ . We also conduct rigorous analysis and demonstrate that the loss bound of HyperPrism is ${\\cal O}\\big(\\big(\\frac{m^{P+2}}{T}\\big)^{\\frac{1}{P+1}}\\big)$ . In cases with few communication epochs (i.e., $T\\leq m$ ), employing a larger value of $p$ yields improvements over traditional linear aggregation. Our theoretical results are consistent with state-of-the-art bounds in distributed gradient/mirror descent and single-device mirror descent, all under a general uniform convexity assumption. To verify the effectiveness of HyperPrism, we carry out extensive experiments that demonstrate that HyperPrism achieves a remarkable acceleration in convergence speed with improvements of up to $98.63\\%$ . Moreover, HyperPrism shows increased scalability. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to express our gratitude to Chengdong Ni for providing technical support. This work was supported in part by NSFC grant 62141220 and Yunnan Key Research Program grant 202402AD080004. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sai Aparna Aketi, Abolfazl Hashemi, and Kaushik Roy. Global update tracking: A decentralized learning algorithm for heterogeneous data. Advances in Neural Information Processing Systems, 36, 2024.   \n[2] Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization, 2022.   \n[3] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \n[4] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Fedavg with fine tuning: Local updates lead to representation learning. Advances in Neural Information Processing Systems, 35:10572\u201310586, 2022. [5] Martijn De Vos, Sadegh Farhadkhani, Rachid Guerraoui, Anne-Marie Kermarrec, Rafael Pires, and Rishi Sharma. Epidemic learning: Boosting decentralized learning with randomized communication. Advances in Neural Information Processing Systems, 36, 2024.   \n[6] Thinh T. Doan, Subhonmesh Bose, D. Hoa Nguyen, and Carolyn L. Beck. Convergence of the iterates in mirror descent methods, 2018.   \n[7] Haizhou Du, Yijian Chen, Xiaojie Feng, Qiao Xiang, and Haoyu Liu. An efficient federated learning framework for multi-channeled mobile edge network with layered gradient compression. Computer Networks, 221:109517, 2023.   \n[8] Haizhou Du, Chaoqian Cheng, and Chengdong Ni. A unified momentum-based paradigm of decentralized SGD for non-convex models and heterogeneous data. Artificial Intelligence, 332:104130, 2024.   \n[9] Haizhou Du, Chengdong Ni, Chaoqian Cheng, Qiao Xiang, Xi Chen, and Xue Liu. FedSwarm: An adaptive federated learning framework for scalable aiot. IEEE Internet of Things Journal, 2024.   \n[10] Haizhou Du and Zhiyuan Yang. FedPrime: An adaptive critical learning periods control framework for efficient federated learning in heterogeneity scenarios. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 125\u2013141. Springer, 2024.   \n[11] Nima Eshraghi and Ben Liang. Distributed online optimization over a heterogeneous network with any-batch mirror descent. In International Conference on Machine Learning, pages 2933\u20132942. PMLR, 2020.   \n[12] Tomer Galanti and Lior Wolf. On the modularity of hypernetworks. Advances in Neural Information Processing Systems, 33:10409\u201310419, 2020.   \n[13] Thomas R Henderson, Mathieu Lacage, George F Riley, Craig Dowell, and Joseph Kopena. Network simulations with the NS-3 simulator. SIGCOMM demonstration, 14(14):527, 2008.   \n[14] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. Applied logistic regression, volume 398. John Wiley & Sons, 2013.   \n[15] Yihan Jiang, Jakub Konec\u02c7ny\\`, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.   \n[16] Anna Koles\u00e1rov\u00e1. Limit properties of quasi-arithmetic means. Fuzzy Sets and Systems, 124(1):65\u201371, 2001.   \n[17] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized SGD with changing topology and local updates. In ICML, pages 5381\u20135393, 2020.   \n[18] Dmitry Kovalev, Elnur Gasanov, Alexander Gasnikov, and Peter Richtarik. Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks. NIPS, 34, 2021.   \n[19] Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly convergent algorithm for decentralized optimization: Sending less bits for free! In ICAIS, pages 4087\u20134095. PMLR, 2021.   \n[20] Dmitry Kovalev, Egor Shulgin, Peter Richtarik, Alexander V Rogozin, and Alexander Gasnikov. ADOM: Accelerated decentralized optimization method for time-varying networks. In ICML, pages 5784\u20135793, 2021.   \n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Master\u2019s thesis, University of Toronto, 2009.   \n[22] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. IEEE, 86(11):2278\u20132324, 1998.   \n[23] Jueyou Li, Guo Chen, Zhaoyang Dong, and Zhiyou Wu. Distributed mirror descent method for multi-agent optimization with delay. Neurocomputing, 177:643\u2013650, 2016.   \n[24] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704, 2020.   \n[25] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[26] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on non-IID data. arXiv preprint arXiv:1907.02189, 2019.   \n[27] Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, and Dawei Yin. Gs2p: a generative pre-trained learning to rank model with over-parameterization for web-scale search. Machine Learning, pages 1\u201319, 2024.   \n[28] Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, and Dawei Yin. Mpgraf: a modular and pre-trained graphformer for learning to rank at web-scale. In 2023 IEEE International Conference on Data Mining (ICDM), pages 339\u2013348. IEEE, 2023.   \n[29] Yuchen Li, Haoyi Xiong, Linghe Kong, Qingzhong Wang, Shuaiqiang Wang, Guihai Chen, and Dawei Yin. S2phere: Semi-supervised pre-training for web search over heterogeneous learning to rank data. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4437\u20134448, 2023.   \n[30] Yuchen Li, Haoyi Xiong, Linghe Kong, Shuaiqiang Wang, Zeyi Sun, Hongyang Chen, Guihai Chen, and Dawei Yin. Ltrgcn: Large-scale graph convolutional networks-based learning to rank for web search. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 635\u2013651. Springer, 2023.   \n[31] Yuchen Li, Haoyi Xiong, Linghe Kong, Rui Zhang, Dejing Dou, and Guihai Chen. Meta hierarchical reinforced learning to rank for recommendation: a comprehensive study in moocs. In Joint European conference on machine learning and knowledge discovery in databases, pages 302\u2013317. Springer, 2022.   \n[32] Yuchen Li, Haoyi Xiong, Linghe Kong, Rui Zhang, Fanqin Xu, Guihai Chen, and Minglu Li. Mhrr: Moocs recommender service with meta hierarchical reinforced ranking. IEEE Transactions on Services Computing, 2023.   \n[33] Yuchen Li, Haoyi Xiong, Qingzhong Wang, Linghe Kong, Hao Liu, Haifang Li, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dejing Dou, et al. Coltr: Semi-supervised learning to rank with co-training and over-parameterization for web search. IEEE Transactions on Knowledge and Data Engineering, 35(12):12542\u201312555, 2023.   \n[34] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In NIPS, page 5336\u20135346. Curran Associates Inc., 2017.   \n[35] Tao Lin, Sai Praneeth Karimireddy, Sebastian Stich, and Martin Jaggi. Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. In ICML, pages 6654\u20136665, 2021.   \n[36] Jiahao Liu, Jiang Wu, Jinyu Chen, Miao Hu, Yipeng Zhou, and Di Wu. FedDWA: Personalized federated learning with dynamic weight adjustment. In International Joint Conference on Artificial Intelligence, 2023.   \n[37] Zhonghao Lyu, Yuchen Li, Guangxu Zhu, Jie Xu, H Vincent Poor, and Shuguang Cui. Rethinking resource management in edge learning: A joint pre-training and fine-tuning design paradigm. arXiv preprint arXiv:2404.00836, 2024.   \n[38] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Layer-wised model aggregation for personalized federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10092\u201310101, 2022.   \n[39] Marie Maros and Gesualdo Scutari. Decentralized matrix sensing: Statistical guarantees and fast convergence. Advances in Neural Information Processing Systems, 36, 2024.   \n[40] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for emerging AI applications. USENIX Association, 2018.   \n[41] G. Nadiradze, A. Sabour, D. Alistarh, A. Sharma, I. Markov, and V. Aksenov. SwarmSGD: Scalable decentralized SGD with local updates. 2019.   \n[42] Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed optimization. IEEE Signal Processing Magazine, 37(3):92\u2013101, 2020.   \n[43] Angelia Nedic, Alex Olshevsky, Asuman Ozdaglar, and John N Tsitsiklis. Distributed subgradient methods and quantization effects. In ICDC\u20192008, pages 4177\u20134184. IEEE, 2008.   \n[44] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. TACON, 54(1):48\u201361, 2009.   \n[45] Giovanni Neglia, Chuan Xu, Don Towsley, and Gianmarco Calbi. Decentralized gradient methods: does topology matter? In ICAIS, pages 2348\u20132358. PMLR, 2020.   \n[46] Arkadij Semenovic\u02c7 Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. 1983.   \n[47] Alex Olshevsky and John N. Tsitsiklis. Convergence speed in distributed consensus and control, 2009.   \n[48] Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments using mirror descent, 2016.   \n[49] Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments using mirror descent. TACON, 63(3):714\u2013725, 2018.   \n[50] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In International Conference on Machine Learning, pages 9489\u20139502. PMLR, 2021.   \n[51] Pranay Sharma, Prashant Khanduri, Lixin Shen, Donald J. Bucci, and Pramod K. Varshney. On distributed online convex optimization with sublinear dynamic regret and fit, 2020.   \n[52] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent, 2011.   \n[53] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8432\u20138440, 2022.   \n[54] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan S Rellermeyer. A survey on distributed machine learning. CSUR, 53(2):1\u201333, 2020.   \n[55] Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized federated learning with feature alignment and classifier collaboration. arXiv preprint arXiv:2306.11867, 2023.   \n[56] Ryan Yang, Haizhou Du, Andre Wibisono, and Patrick Baker. Aggregation in the mirror space (AIMS): Fast, accurate distributed machine learning in military settings. MILCOM 2022 - 2022 IEEE Military Communications Conference (MILCOM), pages 470\u2013477, 2022.   \n[57] Haishan Ye, Luo Luo, Ziang Zhou, and Tong Zhang. Multi-consensus decentralized accelerated gradient descent. arXiv preprint arXiv:2005.00797, 2020.   \n[58] Zhan Yu, Daniel WC Ho, and Deming Yuan. Distributed randomized gradient-free mirror descent algorithm for constrained optimization. IEEE Transactions on Automatic Control, 67(2):957\u2013964, 2021.   \n[59] Deming Yuan, Yiguang Hong, Daniel WC Ho, and Shengyuan Xu. Distributed mirror descent for online composite optimization. TACON, 66(2):714\u2013729, 2020.   \n[60] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Botian Shi, Tao Chen, Yikang Li, and Yu Qiao. Ad-pt: Autonomous driving pre-training with large-scale point cloud dataset. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledge distillation for non-IID federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10174\u201310183, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "8 Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "8.1 Main Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the main notations in Table 5. ", "page_idx": 14}, {"type": "table", "img_path": "3ie8NWA1El/tmp/e1a87026dbeda549a6b38bda30075f20a7616843f36c3e5ec9d7af4a4ddab9f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "8.2 Impact of WPM to parameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We illustrate how HyperPrism leverages weighted power mean (WPM) to facilitate more efficient aggregation in Figure 4. By examining the distribution of model parameters, we observe that the traditional linear averaging model (w/o WPM) has approximately $11.65\\%$ of parameters lying within the range of [-0.01, 0.01] after 80 rounds. In contrast, when utilizing WPM with degrees of $p=9$ and $p=15$ , only $1.67\\%$ and $1.21\\%$ of parameters, respectively, fall within the same range. This indicates that WPM enables the model to effectively preserve a broader range of features. Consequently, HyperPrism can extract more information from the model parameters during aggregation, leading to enhanced performance. These results underscore the effectiveness of WPM in enabling HyperPrism to capture a wider range of features and facilitate a more informative aggregation. ", "page_idx": 14}, {"type": "image", "img_path": "3ie8NWA1El/tmp/e43c5a762b2293b5b3fcdaeb4171ae9309683605dc9d1bac339e86a958161239.jpg", "img_caption": ["Figure 4: WPM\u2019s impact on parameter distribution. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "8.3 Proof under Uniform Convexity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To aid analysis, introduce the sequence $\\mathbf{y}_{i}^{(t)}$ . Note that the $w_{i}^{(t)}$ t)and yi(t) sequences satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla\\phi(\\mathbf{y}_{i}^{(t)})=\\displaystyle\\sum_{j=1}^{m}a_{i,j}^{(t)}\\nabla\\phi(w_{j}^{(t)}),}\\\\ &{\\quad\\nabla\\phi(w_{i}^{(t+1)})=\\nabla\\phi(\\mathbf{y}_{i}^{(t)})-\\eta\\nabla f_{i}(w_{i}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, we can define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{w}}^{(t)}=[\\nabla\\phi]^{-1}(\\frac{1}{m}\\sum_{i=1}^{m}\\nabla\\phi(w_{i}^{(t)}))=[\\nabla\\phi]^{-1}(\\frac{1}{m}\\sum_{i=1}^{m}\\nabla\\phi(\\mathbf{y}_{i}^{(t)})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "8.4 Proof of Lemma 5.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Define a transition matrix $\\Phi(t,s)=A(t)A(t-1)\\cdot\\cdot\\cdot A(s+1)A(s)$ . Then, under Assumption 5.1, Corollary 1 in [43] states that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|[\\Phi(t,\\tau)]_{i j}-\\frac{1}{m}|\\leq\\vartheta\\kappa^{t-\\tau},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\vartheta$ and $\\kappa$ are defined in the lemma statement. ", "page_idx": 15}, {"type": "text", "text": "We are able to write out a general formula for $\\nabla\\phi(w_{i}^{(t+1)})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla\\phi({w_{i}^{(t+1)}})=\\sum_{j=1}^{m}[\\Phi(t,k)]_{i j}\\nabla\\phi({w_{j}^{(k)}})}}\\\\ {{\\displaystyle-\\,\\eta\\sum_{\\tau=k+1}^{t}\\sum_{j=1}^{m}[\\Phi(t,k)]_{i j}\\cdot\\nabla f_{j}({w_{j}^{(\\tau-1)}})-\\eta\\nabla f_{i}({w_{i}^{(t)}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as well as $\\nabla\\phi(\\overline{{w}}^{(t)})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\phi(\\overline{{w}}^{(t)})=\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\nabla\\phi(w_{i}^{(t)})=\\displaystyle\\sum_{j=1}^{m}\\frac{1}{m}\\nabla\\phi(w_{j}^{(k)})}\\\\ &{-\\displaystyle\\eta\\sum_{\\tau=k+1}^{t}\\sum_{j=1}^{m}\\frac{1}{m}\\cdot\\nabla f_{j}(w_{j}^{(\\tau-1)})-\\frac{\\eta}{m}\\sum_{i=1}^{m}\\nabla f_{i}(w_{i}^{(t)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is known that for a $L$ -Lipschitz continuous function $f$ , if $x^{*}$ is the optimum of $f$ , then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)-\\nabla f(x^{*})\\|\\leq2L(f(x)-f^{*})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, $\\nabla\\phi(w_{i}^{(t)})-\\nabla\\phi(\\overline{{w}}^{(t)})$ can be bounded by applying the Triangle Inequality and Equation (22): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\nabla\\phi(w_{i}^{(t+1)})-\\nabla\\phi(\\overline{{w}}^{(t+1)})\\|\\leq\\displaystyle\\sum_{j=1}^{m}\\vartheta\\kappa^{(t-k)}\\|\\nabla\\phi(w_{j}^{(k)})\\|}\\\\ &{\\displaystyle+\\sum_{\\tau=k+1}^{t}\\sum_{j=1}^{m}\\vartheta\\kappa^{(t-\\tau)}\\|\\eta\\nabla f_{j}(w_{j}^{(\\tau-1)})\\|+2\\eta G_{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, plugging in $k=0$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\vartheta(\\kappa^{(t)}\\sum_{j=1}^{m}\\|\\nabla\\phi(w_{j}^{(0)})\\|+m\\eta G_{l}\\sum_{\\tau=1}^{t}\\kappa^{(t-\\tau)}+2\\eta G_{l})}\\\\ {\\displaystyle\\leq\\vartheta(\\kappa^{(t)}\\sum_{j=1}^{m}\\|\\nabla\\phi(w_{j}^{(0)})\\|+m\\eta G_{l}\\cdot\\frac{1}{1-\\kappa}+2\\eta G_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, shifting $t$ down by 1 gives the desired bound. ", "page_idx": 16}, {"type": "text", "text": "8.5 Proof of Theorem 5.7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Firstly, note that under Algorithm 1 and Assumption 5.4, Weighted AM-GM gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(r-1)\\cdot\\sqrt[\\substack{\\frac{1}{\\sigma r^{r-1}m}\\cdot(m\\eta G_{l})r}+m\\cdot\\frac{\\sigma}{r}\\|\\overline{{y}}^{(t)}-\\overline{{y}}^{(t+1)}\\|^{r}}\\\\ &{\\geq r\\cdot\\left((\\sqrt[\\,r-1]{\\frac{1}{\\sigma r^{r-1}m}\\cdot(m\\eta G_{l})^{r}})^{r-1}\\cdot(m\\cdot\\frac{\\sigma}{r}\\|\\overline{{y}}^{(t)}-\\overline{{y}}^{(t+1)}\\|^{r})^{1}\\right)^{\\frac{1}{r}}}\\\\ &{=r\\cdot\\left(\\frac{1}{r^{r}}\\cdot(m\\eta G_{l})^{r}\\cdot\\|\\overline{{y}}^{(t)}-\\overline{{y}}^{(t+1)}\\|^{r}\\right)^{\\frac{1}{r}}}\\\\ &{=(m\\eta G_{l})\\cdot\\|\\overline{{y}}^{(t)}-\\overline{{y}}^{(t+1)}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We may follow the main line of reasoning that proves Theorem 5.7. ", "page_idx": 16}, {"type": "text", "text": "8.6 Main Line of Reasoning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We prove bounds for generic $x$ . Note that $\\begin{array}{r}{\\overline{{\\boldsymbol{w}}}^{(t)}=h^{-1}(\\frac{1}{m}\\sum_{i=1}^{m}h(\\boldsymbol{w}_{i}^{(t)}))}\\end{array}$ . Then, we get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\eta\\sum_{i=1}^{m}[f_{i}(w_{i}^{(t)})-f_{i}(x)]\\leq\\sum_{i=1}^{m}\\langle\\eta\\nabla f_{i}(w_{i}^{(t)}),w_{i}^{(t)}-x\\rangle}\\\\ &{\\displaystyle=\\sum_{i=1}^{m}\\langle\\eta\\nabla f_{i}(w_{i}^{(t)}),w_{i}^{(t)}-\\overline{{w}}^{(t)}\\rangle}\\\\ &{\\displaystyle+\\sum_{i=1}^{m}\\langle\\eta\\nabla f_{i}(w_{i}^{(t)}),\\overline{{w}}^{(t)}-\\overline{{w}}^{(t+1)}\\rangle+\\sum_{i=1}^{m}\\langle\\eta\\nabla f_{i}(w_{i}^{(t)}),\\overline{{w}}^{(t+1)}-x\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Cauchy\u2019s inequality can bound the first term. The third term can be manipulated using Equation (20). Then, combined with $f_{i}(\\overline{{w}}^{(t)})\\leq f_{i}(w_{i}^{(t)})+G_{l}\\|\\overline{{w}}^{(t)}-w_{i}^{(t)}\\|$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\eta\\cdot(F(\\overline{{w}}^{(t)})-F(x))\\leq(2\\cdot\\displaystyle\\sum_{i=1}^{m}\\eta G_{l}\\cdot\\|w_{i}^{(t)}-\\overline{{w}}^{(t)}\\|)}\\\\ &{\\displaystyle+\\,m\\eta G_{l}\\cdot\\|\\overline{{w}}^{(t)}-\\overline{{w}}^{(t+1)}\\|}\\\\ &{\\displaystyle+\\sum_{i=1}^{m}\\langle\\nabla\\phi(y_{i}^{(t)})-\\nabla\\phi(w_{i}^{(t+1)}),\\overline{{w}}^{(t+1)}-x\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The factor of 2 comes from the 2nd term of Equation (27) added to the error from $f_{i}(\\overline{{w}}^{(t)})\\leq$ $f_{i}(w_{i}^{(t)})+G_{l}\\|\\overline{{w}}^{(t)}-w_{i}^{(t)}\\|$ . Note that the last term is equal to $m\\cdot\\langle\\overline{{w}}^{(t)}-\\overline{{w}}^{(t+1)},\\overline{{w}}^{(t+1)}-x\\rangle$ . This is also equal to $m(D_{\\phi}(x,\\overline{{{w}}}^{(t)})-D_{\\phi}(x,\\overline{{{w}}}^{(t+1)})-D_{\\phi}(\\overline{{{w}}}^{(t+1)},\\overline{{{w}}}^{(t)}))$ by the Triangle Inequality for Bregman Divergences. We also substitute Claim 1 to replace the second term, so the value is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq(2\\cdot\\sum_{i=1}^{m}\\eta G_{l}\\cdot\\|w_{i}^{(t)}-\\overline{{w}}^{(t)}\\|)}\\\\ {\\displaystyle+\\,\\frac{r-1}{r}\\,\\,\\sqrt[\\eta]{\\frac{1}{\\sigma m}\\cdot(m\\eta G_{l})^{r}}+m\\sigma\\|\\overline{{w}}^{(t)}-\\overline{{w}}^{(t+1)}\\|^{r}}\\\\ {\\displaystyle+\\,m(D_{\\phi}(x,\\overline{{w}}^{(t)})-D_{\\phi}(x,\\overline{{w}}^{(t+1)})-D_{\\phi}(\\overline{{w}}^{(t+1)},\\overline{{w}}^{(t)})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "But, by uniform convexity, $D_{\\phi}(\\overline{{w}}^{(t+1)},\\overline{{w}}^{(t)})\\geq\\sigma\\|\\overline{{w}}^{(t+1)}-\\overline{{w}}^{(t)}\\|^{r}$ , and thus this is also ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq(2\\cdot\\displaystyle\\sum_{i=1}^{m}\\eta G_{l}\\cdot\\|w_{i}^{(t)}-\\overline{{w}}^{(t)}\\|)+\\displaystyle\\frac{r-1}{r}\\cdot\\sqrt[\\eta]{\\frac{1}{\\sigma m}\\cdot(m\\eta G_{l})^{r}}}\\\\ &{+\\,m(D_{\\phi}(x,\\overline{{w}}^{(t)})-D_{\\phi}(x,\\overline{{w}}^{(t+1)})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, taking the sum over $T$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t=0}^{T-1}[F(\\overline{{w}}^{(t)})-F(x)]\\leq\\displaystyle\\sum_{t=0}^{T-1}2\\cdot\\Big(\\displaystyle\\sum_{i=1}^{m}\\eta G_{l}\\cdot\\|w_{i}^{(t)}-\\overline{{w}}^{(t)}\\|)}\\\\ &{+T\\cdot\\displaystyle\\frac{r-1}{r}\\cdot\\sqrt{\\frac{1}{\\sigma\\sigma}\\cdot(m\\sigma G_{l})r}}\\\\ &{+m(D_{\\phi}(x,\\overline{{w}}^{(0)})-D_{\\phi}(x,\\overline{{w}}^{(t)}))}\\\\ &{\\leq2m\\eta G_{l}\\cdot\\displaystyle\\sum_{t=0}^{T-1}\\sqrt{\\frac{1}{\\sigma}\\|\\nabla\\phi(w_{i}^{(t)})-\\nabla\\phi(\\overline{{w}}^{(t)})\\|}}\\\\ &{+T m\\cdot\\displaystyle\\frac{r-1}{r}\\sqrt{\\frac{1}{\\sigma}\\cdot(\\eta G_{l})r}+m\\cdot D_{\\phi}(x,\\overline{{w}}^{(0)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Dividing through by $\\eta T$ , substituting $\\begin{array}{r}{\\sigma=\\frac{1}{2^{p-1}}}\\end{array}$ and $r=p+1$ , and substituting $x=x^{*}$ and Lemma 5.6 gives the desired result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "8.7 Weighted Power Mean Skew Correction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 8.1. Consider positive $x_{i}$ and $\\alpha_{i}$ close to $\\textstyle{\\frac{1}{m}}$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\right)^{\\frac1p}\\approx\\left(\\sum_{i=1}^{m}{\\frac{1}{m}}x_{i}\\right)^{\\frac1p}+O\\left({\\frac{\\operatorname*{max}\\left|\\alpha_{i}-{\\frac{1}{m}}\\right|}{p}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This allows us to see the weighted power mean as a way to decrease skew; this theorem is relevant in the setting of Lemma 5.6. ", "page_idx": 17}, {"type": "text", "text": "Write $\\begin{array}{r}{\\alpha_{i}=\\frac{1}{m}+\\epsilon\\cdot e_{i}}\\end{array}$ where all $\\begin{array}{r}{e_{i}\\leq\\frac{1}{m}}\\end{array}$ . Hold $e_{i}$ constant. Then, for $\\alpha_{i}\\approx m$ , it is true that $\\epsilon\\approx0$ The derivative of the average with respect to is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\sum e_{i}x_{i}^{p}}{p\\cdot\\left(\\sum\\frac{1}{m}x_{i}^{p}+\\epsilon\\cdot(\\sum e_{i}x_{i}^{p})\\right)^{\\frac{p-1}{p}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the derivative at $\\epsilon=0$ i sp\u00b7(  1e ixxp ip)pp\u22121 . Thus, we have the following first-order approximation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\right)^{\\frac{1}{p}}\\approx\\left(\\sum_{i=1}^{m}\\frac{1}{m}x_{i}\\right)^{\\frac{1}{p}}+\\epsilon\\cdot\\frac{\\sum e_{i}x_{i}^{p}}{p\\cdot\\left(\\sum\\frac{1}{m}x_{i}^{p}\\right)^{\\frac{p-1}{p}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "But we have $\\textstyle\\sum e_{i}x_{i}^{p}\\leq\\sum{\\frac{1}{m}}x_{i}^{p}$ , so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\right)^{\\frac{1}{p}}\\approx\\left(\\sum_{i=1}^{m}\\frac{1}{m}x_{i}\\right)^{\\frac{1}{p}}+\\epsilon\\cdot\\frac{1}{p}\\left(\\sum\\frac{1}{m}x_{i}^{p}\\right)^{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The ${\\frac{1}{p}}\\,\\left(\\sum{\\frac{1}{m}}x_{i}^{p}\\right)^{\\frac{1}{p}}$ term is bounded, so the error is proportional to $O(\\frac{\\epsilon}{p})$ . ", "page_idx": 18}, {"type": "text", "text": "8.8 Linear Bound on Power Mean ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As a starter, we first prove a useful lemma on two real numbers with different signs. Note that all variables used in this subsection are generic ones not tied to the HyperPrism-based mechanism. ", "page_idx": 18}, {"type": "text", "text": "Lemma 8.2 (Two Numbers with Different Signs). Given any two real numbers $x>0,y<0$ , an odd integer $p\\geq1$ , and $0\\leq\\alpha\\leq1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\alpha x^{p}+(1-\\alpha)y^{p}\\right)^{1/p}\\geq\\frac{\\alpha}{2p}x+(1-\\frac{\\alpha}{2p})y.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Denote C = 21p and rewrite the right-hand side (RHS) of Equation (34) as $C(\\alpha x+(1-$ $\\alpha)y)+(1-C)y$ . We first note that we can scale both $x,y$ by $\\textstyle{\\frac{1}{|y|}}$ such that $y=-1$ . As such, we can simplify our proof to only focus on the case of $y=-1$ . We will use shorthand $\\Delta$ , and rewrite as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{(C\\cdot(\\alpha x+(1-\\alpha)y)+(1-C)\\cdot y)^{p}}\\\\ {=(C\\alpha(x+1)-1)^{p}=-\\left(1-C\\alpha(x+1)\\right)^{p}\\Delta^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then consider two cases. ", "page_idx": 18}, {"type": "text", "text": "Case 1: $x\\leq2p-1$ . Since $-C\\alpha(x+1)\\geq-C\\alpha(2p)=-\\alpha\\geq-1$ , by Bernoulli\u2019s inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta^{p}\\leq-\\left(1+p(-C\\alpha(x+1))\\right)=-1+p C\\alpha(x+1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\textstyle{\\frac{x+1}{2}}\\leq x^{p}+1$ , which can be verified by casework on $x\\geq1$ or $x\\leq1$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta^{p}\\leq-1+\\alpha(x^{p}+1)=\\alpha x^{p}+(1-\\alpha)\\cdot(-1)^{p}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then finish this case by taking the power of $1/p$ on both sides. ", "page_idx": 18}, {"type": "text", "text": "Case 2: $x\\,\\geq\\,2p\\mathrm{~-~}1$ . We will denote the expression on the left-hand side and right-hand side of Equation (34) with LHS and RHS, respectively. First note that both sides of the inequality in Equation (34) are $-1$ when $\\alpha=0$ and $y=-1$ . Next, we will show that the derivative with respect to $\\alpha$ is always larger for the LHS of Equation (34) when $x\\geq2p-1$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{d}{d\\alpha}}L H S=x^{p}+1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{d}{d\\alpha}R H S=p C(x+1)\\cdot(C\\alpha(x+1)-1)^{p-1}}}\\\\ {{\\displaystyle=\\frac12(x+1)\\cdot(C\\alpha(x+1)-1)^{p-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The LHS\u2019s derivative w.r.t $\\alpha$ is clearly constant. Also, the ${\\frac{d}{d\\alpha}}R H S$ clearly has local maxima at $\\alpha=0,1$ . At , we clearly have that since $x\\geq2p-1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{d}{d\\alpha}L H S|_{\\alpha=0}=x^{p}+1}\\\\ {\\geq\\displaystyle\\frac{1}{2}(x+1)\\cdot(0-1)^{p-1}=\\frac{d}{d\\alpha}R H S|_{\\alpha=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $\\alpha=1$ , since $\\begin{array}{r}{x\\geq\\frac{1}{2p}(x+1)-1\\geq0}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{d}{d\\alpha}L H S|_{\\alpha=1}=x^{p}+1\\ge x\\cdot(x)^{p-1}}}\\\\ &{\\qquad\\qquad\\ge\\left(\\frac{1}{2}(x+1)\\right)\\left(\\frac{1}{2p}(x+1)-1\\right)^{p-1}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{d}{d\\alpha}R H S|_{\\alpha=1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, for all $0\\leq\\alpha\\leq1$ , $\\begin{array}{r}{{\\frac{d}{d\\alpha}}L H S\\,\\geq\\,{\\frac{d}{d\\alpha}}R H S}\\end{array}$ , and they are equal at $\\alpha\\,=\\,0$ , so we always have $L H S\\ge R H S$ for all $0\\leq\\bar{\\alpha}\\leq1$ when $\\hat{x}\\geq2p-1$ , and we are done. ", "page_idx": 19}, {"type": "text", "text": "Next, consider a list of real numbers $x_{i}$ , $,i=1,\\dots,m$ . We let $M=\\operatorname*{min}x_{i}$ and $U=\\operatorname*{max}x_{i}$ . We then prove the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 8.3. Assume a list of non-negative real numbers $x_{i}$ $,\\,i=1,\\ldots,m$ and $p\\geq1$ . Given $\\alpha_{i}$ , $i=1,\\hdots,m$ such that $\\textstyle\\sum_{k=1}^{m}\\alpha_{k}=1$ and $\\alpha_{i}\\geq0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\leq\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}^{p}\\right)^{1/p}\\leq\\frac{1}{p}\\sum_{i=1}^{m}\\alpha_{i}x_{i}+\\frac{p-1}{p}U.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The lower bound in Equation (42) is a direct result from the power mean inequality. As for the upper bound, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}^{p}\\right)^{1/p}\\le\\left(\\sum_{i=1}^{m}\\alpha_{i}U^{p-1}x_{i}\\right)^{1/p}}}\\\\ &{}&{\\qquad=\\left(U^{p-1}\\sum_{i=1}^{m}\\alpha_{i}x_{i}\\right)^{1/p}\\le\\frac{1}{p}\\sum_{i=1}^{m}\\alpha_{i}x_{i}+\\frac{p-1}{p}U,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last step uses the generalized AM-GM inequality. ", "page_idx": 19}, {"type": "text", "text": "We now tie these two Lemmas together to prove Theorem 8.4, which to the best of our knowledge is a novel inequality on weighted power mean. ", "page_idx": 19}, {"type": "text", "text": "Theorem 8.4 (Linear Bound on Weighted Power Mean). Assume a list of real numbers $x_{i},\\;i=$ $1,\\ldots,m$ and $\\alpha_{i}$ , $i=1,\\hdots,m$ such that $\\textstyle\\sum_{k=1}^{m}\\alpha_{k}=1$ and $\\alpha_{i}\\geq0$ . For any odd integer $p\\geq1$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}^{p}\\right)^{\\frac{1}{p}}\\geq\\frac{1}{2p}\\sum_{i=1}^{m}\\alpha_{i}x_{i}+(1-\\frac{1}{2p})M.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. First, note that if all $x_{i}\\mathbf{s}$ are positive, Equation (44) holds from the lower bound of Lemma 8.2. Second, if all $x_{i}\\mathbf{s}$ are $\\leq0$ , Equation (44) holds from the upper bound of Lemma 8.3 after filpping all the signs. ", "page_idx": 19}, {"type": "text", "text": "For the case where $x_{i}\\mathbf{s}$ have mixed signs, let $\\alpha_{p o s},\\alpha_{n e g}$ be the sum of the corresponding $\\alpha\\mathbf{S}$ for the negative and positive elements in $\\mathbf{x}$ , respectively. We have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}x_{i}^{p})^{\\frac{1}{p}}=(\\displaystyle\\sum_{p\\neq s}\\alpha_{i}x_{i}^{p}+\\displaystyle\\sum_{n\\neq s}\\alpha_{i}x_{i}^{p})^{\\frac{1}{p}}}\\\\ &{\\geq(\\alpha_{p o s}(\\displaystyle\\sum_{p\\neq s}\\alpha_{p o s}^{()}x_{i})^{p}+\\alpha_{n e g}(\\displaystyle\\frac{1}{p}\\displaystyle\\sum_{n\\neq s}\\alpha_{n e g}x_{i}+\\frac{p-1}{p}M)^{p})^{\\frac{1}{p}}}\\\\ &{\\geq\\displaystyle\\frac{\\alpha_{p o s}(\\displaystyle\\sum_{p\\neq s}\\alpha_{p o s}^{()}x_{i})}{2p}+(1-\\frac{\\alpha_{p o s}}{2p})(\\frac{1}{p}\\displaystyle\\sum_{n\\neq s}\\alpha_{n e g}x_{i}+\\frac{p-1}{p}M)}\\\\ &{=\\displaystyle\\sum_{p\\neq s}(\\frac{1}{2p})\\alpha_{i}x_{i}+\\displaystyle\\sum_{n\\neq s}(\\frac{1}{2p^{2}}+\\frac{2p-1}{2p^{2}\\alpha_{n e g}})\\alpha_{i}x_{i}}\\\\ &{\\quad+\\displaystyle(\\frac{p-1}{2p^{2}}\\alpha_{n e g}+\\frac{(2p-1)(p-1)}{2p^{2}})M,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality comes from the $n$ -variable same sign case in Lemma 8.3, and the second inequality comes from the 2-variable case in Lemma 8.2. Here, each positive $x_{i}$ is weighted with at leas t 21p\u03b1i and each negative xi is weighted with at least $\\begin{array}{r}{\\left(\\frac{1}{2p^{2}}+\\frac{2p-1}{2p^{2}}\\right)\\alpha_{i}}\\end{array}$ , which equals to $\\textstyle{\\frac{1}{p}}\\alpha_{i}\\geq{\\frac{1}{2p}}\\alpha_{i}$ , since $\\alpha_{n e g}\\le1$ . ", "page_idx": 20}, {"type": "text", "text": "Since $\\alpha_{i}\\geq M$ ( $M<0$ when $x_{i}\\mathbf{s}$ have different signs), we can turn the extra $\\alpha_{i}$ terms into $M$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\alpha_{i}x_{i}^{p}\\right)^{\\frac{1}{p}}\\geq\\frac{1}{2p}\\sum_{i=1}^{n}\\alpha_{i}x_{i}+\\frac{2p-1}{2p}M,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes our proof. ", "page_idx": 20}, {"type": "text", "text": "In addition to a new lower bound of HyperPrism, we can also prove an upper bound of the HyperPrism at $\\begin{array}{r}{\\frac{1}{2p}\\sum_{i=1}^{m}\\alpha_{i}x_{i}+\\frac{2p-1}{2p}U}\\end{array}$ following similar steps. Note that these bounds are generic and can be applied outside of DML. These bounds are necessary for analyzing the beneftis of using a HyperPrism aggregation function in DML because they guarantee that the HyperPrism takes at least some minimum consideration of each $x_{i}$ term. This consideration is crucial because each device\u2019s local objective function is unique, and they must all be considered to optimize the overall objective function. ", "page_idx": 20}, {"type": "text", "text": "9 Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract provides a concise summary of the paper\u2019s key findings and contributions, allowing viewers to quickly grasp the main points. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discussed the additional computational overhead that Hypernetworks may introduce, and we provided some time consumption comparisons. However, further research is needed to investigate whether the computational resource consumption will limit the performance. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide comprehensive definitions of all theories and assumptions and presented complete proofs in the appendix. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide detailed explanations of all experimental settings in the experimental section and uploaded the source code and data for reference. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We utilized an open-source dataset and provided the complete source code for reference. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide detailed description of the experimental setup in the evaluation section. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide comprehensive experimental result and detailed explanations in the evaluation section. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide sufficient experimental information to reproduce the experiments, some detailed information are present in appendix due to space limitation. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, the research in this paper satisfies all of the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research in this paper focuses on foundational research and does not have potential negative societal impacts. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: There is no such risk with this paper. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All existing assets used in the paper are properly credited, and the licenses and terms of use are explicitly mentioned and properly respected. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The new assets introduced in the paper are well documented and provided alongside the assets. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}]