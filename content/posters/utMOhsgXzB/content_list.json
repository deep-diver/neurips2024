[{"type": "text", "text": "BendVLM: Test-Time Debiasing of Vision-Language Embeddings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Walter Gerych1 Haoran Zhang1 Kimia Hamidieh1 Eileen Pan1 ", "page_idx": 0}, {"type": "text", "text": "Maanas Sharma1 Thomas Hartvigsen2 Marzyeh Ghassemi1 ", "page_idx": 0}, {"type": "text", "text": "1MIT, 2University of Virginia {wgerych, haoranz, hamidieh, eileenp, maanas, mghassem}@mit.edu, hartvigsen@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-language model (VLM) embeddings have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. VLMs are being quickly adopted for a variety of tasks ranging from few-shot classification to text-guided image generation, making debiasing VLM embeddings crucial. Debiasing approaches that fine-tune the VLM often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a \u201cone-size-fits-all\" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose BEND-VLM, a nonlinear, fine-tuning-free approach for VLM embedding debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online, open-set tasks such as retrieval and text guided image generation.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Background. Pretrained foundation Vision-language models (VLMs) such as CLIP [33], BLIP [22], and LLaVA [25] have seen wide adoption for tasks like image retrieval [21], zero and few-shot classification [33, 4], text-guided image generation [32], and facial recognition [58]. But VL models also encode societal biases [5, 27, 43, 49, 53]. As more and more systems rely on CLIP, the encoded representational harm [12, 3, 15, 52] can lead to allocative harm [34, 46, 14, 51, 16, 29], such as Black individuals being three times more likely to be misclassified into a nonhuman category by computer vision systems [1]. ", "page_idx": 0}, {"type": "text", "text": "State of the art. Debiasing VLMs is an active area of research [6, 10, 20, 19, 50, 28]. One common approach is finetuning the embedding models to remove spurious correlations [59, 2, 42]. However, finetuning often decreases accuracy and generalizability of foundation models [31]\u2014a significant drawback as these models are commonly used for zero-shot tasks. Most existing finetuning-free methods learn debiasing transformations of the initial text embeddings, but typically use one-size-ftisall linear debiasing functions that apply the same fixed transformation to every input [6, 10, 50]. ", "page_idx": 0}, {"type": "text", "text": "While recent work has explored nonlinear VLMs [11], their method assumes access to the set of classes at test-time, requiring the debiasing training pipeline to be rerun if a query for a new class is made. This is a major limitation in practice because many tasks VLMs are used for are often naturally open-set, where the classes to be evaluated for at test-time are unknown prior to inference. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Problem Definition. We study online, open-set debiasing for VLM embeddings. In this setup, we only have access to a VLM, along with a single-modal image dataset. This image dataset is only for the purpose of \"training\", and is not the dataset that the downstream task will work on. We assume that this dataset, which we call the reference dataset, has labels for the protected attribute(s) of interest. During test-time, we receive online input queries one at a time. These queries are also open-set, meaning that the classes or concepts they refer to are not known to us beforehand. For instance, the query may be \"a photo of a nurse\", but we do not have knowledge that nurse is a potential class of interest before receiving the query. Our goal is to debias the query embedding from the VLM in such as way that it does not more strongly associate the query embedding with any protected attribute value over another. For instance, the embedding for $^{11}\\mathtt{a}$ photo of a nurse\" should not be more associated with images of women than with men. ", "page_idx": 1}, {"type": "text", "text": "Challenges. Online, open-set VLM debiasing is a challenging task. First, we must overcome catastrophic forgetting\u2014a solution that debiases the embeddings, but degrades performance. Second, the interaction between protected attributes and query classes may be nonlinear and instancedependent. For example, the transformation required to remove the gender bias from the embedding of \"nurse\" is likely not the same as the one to untangle gender bias associated with the embedding of \"handyman\". Third, queries from open-set classes means that our approach must be flexible enough to remove the association of protected attributes from classes unknown prior to inference time. Lastly, online settings demand computational efficiency and thus rule out reftiting the debiasing component for each now class or query. ", "page_idx": 1}, {"type": "text", "text": "Proposed approach. We propose Bias Elimination with Nonlinear Debiasing of Vision Language Models (BEND-VLM), a test-time VLM debiasing method that leaves the VLM\u2019s weights unchanged, being efficient enough for online streaming queries. By using the easy-to-get pre-debiasing reference dataset with protected attributes, BEND-VLM allows for unsupervised test-time debiasing. On a high level, BEND-VLM consists of two main parts: ", "page_idx": 1}, {"type": "text", "text": "First, given an online query, we generate augmented queries that introduce protected attribute information. For example, given $^{11}\\mathtt{a}$ photo of a nurse\" we generate \"a photo of a {ATTRIBUTE} nurse\", fliling in $\\{{\\tt A T T R I B U T E}\\}$ with male / female / nonbinary for gender debiasing, for instance. We get these augmented queries from a small language model, and use them to find the directions in the embedding space for that specific query that are most associated with the protected attribute. Given these directions, we project the embedding such that it is orthogonal to the protected attribute dimension, resulting in the first-stage debiased representation. ", "page_idx": 1}, {"type": "text", "text": "For the second step, we make use of the reference image dataset. We find the images in this dataset that are most associated with the query, and then subset them by protected attribute value. We find an updated, debiased query representation by solving a constrained optimization equation with the goal of finding an embedding with minimal distance to the first-stage debiased representation while being equally similar to the example images for each attribute value. For instance, we find an embedding that is equally similar to the nearest images for each gender. The resulting embedding will have little to no excess association with any of the debiased protected attribute values over any other. The output can then be passed to the downstream task. ", "page_idx": 1}, {"type": "text", "text": "Contributions. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce BEND-VLM, a novel test-time VLM debiasing approach that does not require finetuning. \u2022 We propose a technique for finding local attribute subspaces specific to each query on-the-fly. \u2022 We introduce a novel method for equalization by using a reference image dataset. \u2022 We experimentally evaluate for classification, retrieval, and image captioning settings, showing BEND-VLM consistently outperforms the compared approaches. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $(\\mathbf{m},\\mathbf{t},\\mathbf{c},\\mathbf{a})$ be an (image, text, class, attribute) tuple distributed according to $P_{M}\\times P_{T}\\times P_{C}\\times P_{A}$ , a joint distribution over images, texts, classes, and attributes. Using the running example of nurses, a realization of $\\mathbf{\\nabla}m$ could be an image of a nurse, $\\pmb{t}$ the text $^{11}\\mathtt{a}$ photo of a nurse\", $^c$ the class nurse, and $\\textbf{\\em a}$ a protected attribute such as gender. Importantly, we do not assume that $\\mathbb{C}$ , the support of $P_{C}$ , is known. This means we do not know what classes the user will query for during inference, and do not have access to a training set with these class labels. ", "page_idx": 1}, {"type": "image", "img_path": "utMOhsgXzB/tmp/ccf546fbe272773a54bfe0bfbde91d53a89e864208fa350ed78b4f930cfa4e5b.jpg", "img_caption": ["Figure 1: Overview of our two-step BEND-VLM method. In this example, the initial query embedding of doctor is more strongly associated with males, and the CCF distance is 0.10. After performing debiasing step 1, Orthogonalizing the Embedding, the embedding is modified to remove bias along the gender direction defined by \"male doctor\" and \"female doctor\". This still results in a CCF distance of 0.05. We then perform the second debiasing step, where the query embedding is again modified to be equidistant to the relevant male and female images. The final representation achieves the optimal distance of 0.00. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Let $f_{\\theta}^{T}:\\mathbb{T}\\rightarrow\\mathbb{R}^{d}$ represent the text embedding model (e.g., CLIP\u2019s image encoder) and $f_{\\theta}^{M}:\\mathbb{M}\\!\\to$ $\\mathbb{R}^{d}$ represent the image encoder, where $\\mathbb{T}$ and $\\mathbb{M}$ are the text and image domain, respectively. We will use $f_{\\theta}=\\{f_{\\theta}^{T},f_{\\theta}^{\\bar{M}}\\}$ when referring to the VL model in general, rather than its modality-specific encoders. $f_{\\theta}$ is used to obtain $d\\big(f_{\\theta}^{M}(\\pmb{m}),f_{\\theta}^{T}(\\pmb{t})\\big)$ , where $d(\\cdot,\\cdot)$ is a distance metric such as cosine distance. In practice, these (image, text) distance scores are used for zero-shot classification or image retrieval. ", "page_idx": 2}, {"type": "text", "text": "Let $\\pmb{t}_{c}\\in\\mathbb{T}$ be a textual instance relating to class $^c$ . For instance, class $^c$ could be nurse and $\\pmb{t}_{c}$ \"a picture of a nurse\". Then, our goal is to obtain a text embedding $z_{c}^{\\ast}\\in\\mathbb R^{d}$ that is Class Conditionally Fair. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Class Conditionally Fair (CCF)). A text embedding $z_{c}^{*}$ is Class Conditionally Fair for embedding model $f_{\\theta}$ , class $^c$ , and metric $d$ if for all $\\mathbf{a}_{i}$ , $a_{j}\\in\\mathcal{A}$ the following holds: $\\mathbb{E}_{\\mathbf{m}|a_{i},c}\\big[d\\big(f_{\\theta}^{M}(\\mathbf{m}^{\\prime}),z_{c}^{*}\\big)\\big]=\\mathbb{E}_{\\mathbf{m}^{\\prime}|a_{j},c}\\big[d\\big(f_{\\theta}^{M}(\\mathbf{m}^{\\prime}),z_{c}^{*}\\big)\\big].$ ", "page_idx": 2}, {"type": "text", "text": "Intuitively, a text embedding is CCF for class $^c$ if the expected similarity between the text representation and relevant image embeddings \u2014 image embeddings that are also associated with class $^c$ \u2014 is independent of the protected attribute value $\\textbf{\\em a}$ . For instance, an embedding of the query $\"a$ picture of a nurse\" is CCF if its expected similarity score for pictures of female nurses is equal to the expected similarity score for male nurses. ", "page_idx": 2}, {"type": "text", "text": "We also define Class Conditionally Fair Distance as a measure from how far off an embedding is from being CCF: ", "page_idx": 2}, {"type": "table", "img_path": "utMOhsgXzB/tmp/77eed953501941af9347a4b55480aa03137b6b48e3bc8534286337b36a122399.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "The CCF distance of $z_{c}$ is 0 if and only if $z_{c}$ is CCF. In practice, we can\u2019t exactly compute the expectations in the CCF distance definition. Instead, these expectations can be replaced with the average distances from relevant samples in the evaluation dataset. ", "page_idx": 3}, {"type": "text", "text": "Reference and Target Datasets. In practice, we assume that we have a dataset $\\begin{array}{r l}{D_{\\mathrm{ref}}}&{{}=}\\end{array}$ $\\{(m_{i},\\mathbf{a}_{i})\\}_{i=1}^{N}$ consisting of $N$ images with labeled attributes. For instance, $D_{\\mathrm{ref}}$ could be a dataset of pictures of people with corresponding gender, race, or age labels2. We focus on both the image retrieval and zero-shot classification setting. This reference dataset will be used to obtain debiased text embedding, as we describe in detail in the following section. We refer to the downstream dataset to be used in retrieval or zero-shot applications as the target dataset Dtarget = {mj }jN=ta1rget. Dtarget is not available prior to inference. ", "page_idx": 3}, {"type": "text", "text": "For retrieval, we assume that $D_{\\mathrm{target}}$ is an unlabeled dataset of images, such that we want to retrieve images from this dataset that relate to streaming, open-set queries. For instance, the queries can be free-form text searches coming from a search engine user. In this open-set scenario the set of classes $\\mathbb{C}$ is unknown \u2014 we do not know what classes users will search for a priori. ", "page_idx": 3}, {"type": "text", "text": "For zero-shot classification, we likewise focus on the streaming, open-set scenario. Images from $D_{\\mathrm{target}}$ will be compared against a set of texts $\\{t_{c0},t_{c1},\\cdot\\cdot\\cdot\\,,t_{c K}\\}$ for the purpose of classification, where this set of texts relates to classes $z_{1},c_{2},\\dots,c_{K}\\in\\mathbb{C}$ , where $\\mathbb{C}$ is unknown to us and potentially variable. For instance, a user may first wish to obtain zero-shot predictions of hair color of the portraits in $D_{\\mathrm{target}}$ , and later wish to obtain predictions of whether the individuals have eyeglasses. ", "page_idx": 3}, {"type": "text", "text": "In both settings, we make the simplifying assumption that each user query $\\pmb{t}_{c}$ does not explicitly reference the protected attribute of interest. For instance, the query is $^{11}\\mathtt{a}$ picture of a nurse\", not $^{11}\\mathtt{a}$ picture of a male nurse\" \u2014 and thus it is desirable for the query embedding to not be more associated with a particular gender. In the case where the query does contain explicit reference to $a-\\\"{\\mathtt{a}}$ picture of a male nurse\" \u2014 it is straightforward to abstain from debiasing by using a language model to filter out these queries, or by checking for explicit attribute terms 3. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "On a high level, our BEND-VLM approach consists of a two-phase debiasing pipeline. We perform an initial debiasing pass by first employing the classic approach of orthogonalizing $f_{\\theta}(t)$ to the attribute subspace $\\pmb{v}$ [24, 9]. However, unlike most prior works, we do not assume that the attribute subspace is globally constant for all queries; it may be the case that the direction in the embedding space corresponding to gender that differentiates $^{11}\\mathtt{a}$ picture of a male nurse\" from $\"a$ picture of a female nurse\" may not be equivalent to the gender direction between \"a picture of a baby boy\" and \"a picture of a baby girl\". We find these local attribute subspaces using our ATTRIBUTEAUGMENT module to obtain attribute augmented versions of $\\pmb{t}$ . After this first phase, we are left with the partially-debiased embedding $z_{c}^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "Our second and final debiasing pass consists of equalizing the distances between the embedding and relevant images from the reference dataset $D_{r e f}$ belonging to each attribute class. We obtain the final debiased embedding $\\boldsymbol{z}_{c}^{*}$ through an analytical solution to a constrained optimization equation. ", "page_idx": 3}, {"type": "text", "text": "3.1 Step 1: Making The Embedding Orthogonal To Local Attribute Subspace ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Orthogonalizing text embeddings with respect to an attribute subspace, such as setting embedding dimensions corresponding to gender or race equal to zero, is a classic approach used for standard text embeddings [24, 9] and has recently shown promise in debiasing VL models [10]. Whereas existing approaches typically find a single attribute subspace for instances, we find local attribute subspaces in addition to the global subspace. ", "page_idx": 3}, {"type": "text", "text": "Let $\\pmb{t}_{c}$ be the initial text query coming in to the system. We then obtain $\\pmb{t}_{c,a_{i}}$ for all $\\mathbf{\\Delta}_{a_{i}}\\in\\mathcal{A}$ . For instance, if $\\textbf{\\em a}$ refers to gender and $\\pmb{t}_{c}\\,=\\,\"\\,\\mathbf{a}$ picture of a nurse\", then we would obtain $^{11}\\mathtt{a}$ picture of a male nurse\" and $^{11}\\mathtt{a}$ picture of a female nurse\" for $\\pmb{t}_{c,a_{m a l e}}$ and $\\pmb{t}_{c,a_{f e m a l e}}$ , respectively. We draw each $\\pmb{t}_{c,a_{i}}$ from our ATTRIBUTEAUGMENT module: $\\{t_{c,a_{i}}\\}_{i\\in\\mathbb{A}}=$ ATTRIBUTEAUGMENT $(t_{c,a_{i}};\\mathbb{A})$ . In practice, we use an LLM to instantiate ATTRIBUTEAUGMENT. In a lower resource setting, ATTRIBUTEAUGMENT could feasibly be implemented through simpler text processing techniques to identify the subject of the query and insert corresponding attribute strings before the subject; e.g. inserting $\"\\mathtt{m a l e}\"$ and \"female\" before the subject for gender debiasing. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Let $A$ be a matrix whose columns are $f_{\\theta}^{T}(t_{c,a_{i}})-f_{\\theta}^{T}(t_{c})$ for $i=1\\to|\\mathbb{A}|$ . To combat potential noise from estimating the local attribute subspace, we additonally include generic attribute text embeddings into the columns of $A$ as well. For instance, for gender debiasing we include the embeddings of $^{11}\\mathtt{a}$ picture of a man\" and $^{11}\\mathtt{a}$ picture of a woman\". We then obtain the initial debiased embedding $z_{c}^{\\prime}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{c}^{\\prime}=V f_{\\theta}^{T}(t_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $V=I-A(A^{\\top}A)^{-1}A^{\\top}$ is the orthogonal projection matrix of $A$ [10]. ", "page_idx": 4}, {"type": "text", "text": "Importantly, despite $z_{c}^{\\prime}$ being orthogonal to the local attribute subspace it is not necessarily equally similar to the image embeddings of relevant instances when conditioned on the \"debiased\" attribute. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Orthogonalization does not yield Class Conditional Fairness.). The following does not hold in general: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{m}|a_{i},c}\\big[d(f_{\\theta}^{M}(\\mathbf{m}^{\\prime}),z_{c}^{\\prime})\\big]=\\mathbb{E}_{\\mathbf{m}^{\\prime}|a_{j},c}\\big[d(f_{\\theta}^{M}(\\mathbf{m}^{\\prime}),z_{c}^{\\prime})\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We show an example of this in Figure 1, where we see that step 1 does not result in significantly improved CCF distances. To mitigate this, we propose a second debiasing step. ", "page_idx": 4}, {"type": "text", "text": "3.2 Step 2: Using Reference Images To Equalizing the Text Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this second stage, we equalize the distances between the images in $D_{\\mathrm{ref}}$ and the debiased embedding $z_{c}^{\\prime}$ , with the goal of making relevant images from each attribute group equally similar to the text embedding. Let $D_{\\mathrm{ref}}(\\pmb{a}_{i},\\pmb{c})$ be images in the reference dataset that are associated with attribute class $\\pmb{a}_{i}$ and class $^c$ . We want to find the embedding $\\boldsymbol{z}_{c}^{*}$ that satisfies the following set of conditions $\\mathcal{C}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\left\\{\\frac{\\sum_{m_{j}\\in D_{\\mathrm{ref}}(a_{i},c)}d(f_{\\theta}^{M}(\\boldsymbol{m}_{j},z_{c}^{*}))}{|D_{\\mathrm{ref}}(a_{i},c)|}=\\frac{\\sum_{m_{k}\\in D_{\\mathrm{ref}}(a_{1},c)}d(f_{\\theta}^{M}(\\boldsymbol{m}_{k},z_{c}^{*}))}{|D_{\\mathrm{ref}}(a_{1},c)|}\\right\\}_{\\substack{i=1\\to|\\boldsymbol{\\Lambda}|}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These constraints say that the average distance between relevant image embeddings should be equal for all attribute value splits. For example, the distance between the embedding of $^{11}\\mathtt{a}$ picture of a nurse\" and relevant male images should match the distance between the embedding and relevant female images. ", "page_idx": 4}, {"type": "text", "text": "Note that since we do not assume access to context labels for $D_{\\mathrm{ref}}$ , it is not immediately obvious on how to obtain each $D_{\\mathrm{ref}}(a_{i},c)$ . Instead, $D_{\\mathrm{ref}}(\\pmb{a}_{i},\\pmb{c})$ is by selecting $n$ images with attribute value $\\mathbf{a}_{i}$ that are most similar to the query embedding $z_{c}^{\\prime}$ . The value of $n$ could be found using change-point detection, such that $n$ is the value where the elbow in the plot of similarity over indexes sorted by similarity score [38]. A less sophisticated approach \u2014 but one we find works well in practice \u2014 is to simple chose $n$ as a hyperparameter, and use the same value for each attribute and query. ", "page_idx": 4}, {"type": "text", "text": "Finding any embedding that satisfies $\\mathcal{C}$ is not enough, since we want to ensure that the debiased embedding does not lose information unrelated to the protected attribute $\\textbf{\\em a}$ . This means we want to find a debiased embedding with minimal distance to the previous embedding. We want to find a $\\boldsymbol{z}_{c}^{*}$ that minimizes distance to the first-pass debiased $z_{c}^{\\prime}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{initial}}=d\\!\\left(z_{c}^{*},z_{c}^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We thus find $\\boldsymbol{z}_{c}^{*}$ by solving the following constrained optimization equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{c}^{*}=\\underset{z_{c}^{*}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mathrm{initial}},\\mathrm{~under~the~set~of~constraints~}\\mathcal{C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation 1 has a simple analytical solution for the binary attribute case, when $d(\\cdot,\\cdot)$ is cosine distance and each embedding has unit norm length. ", "page_idx": 4}, {"type": "image", "img_path": "utMOhsgXzB/tmp/34b7146171c7a8c65ac0d1113054ccdde7e94719773f48a745e525c938c959d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As the requirement that the embeddings have unit norm length simplifies the analytical solution, we add in this norm constraint $\\{||z_{c}^{*}||_{2}=\\overline{{1}}\\}$ to the set $\\mathcal{C}$ . In the case where the protected attribute is not binary, $\\boldsymbol{z}_{c}^{*}$ can be found using a constrained optimization solver [48]. ", "page_idx": 5}, {"type": "text", "text": "After obtaining the result of this final debiasing step, our modified embedding can then be passed along to a downstream task such as retrieval or zero-shot classification on a target dataset $D_{\\mathrm{target}}$ , or used to condition another model such as a text to image generator. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We compare our BEND-VLM to existing debiasing approaches on the FAIRFACE [18], CELEBA [26], and UTKFACE [57] datasets. Each dataset contains pictures of people. CELEBA has gender annotations, while FAIRFACE and UTKFACE have both gender and race labels. ", "page_idx": 5}, {"type": "text", "text": "Models. We evaluate the ability of the debiasing approaches to improve the performance of the CLIP-ViT-Base-Patch16 (CLIP-ViT-B-P16) and CLIP-ViT-Large-Patch14 (CLIP-ViT-L-P14) VLMs. For image captioning, we use ClipCap [30] pretrained on Conceptual Captions [41], which uses a ViT-B/32 architecture. We use Mistral-7B-Instruct-v0.2 [17] for our ATTRIBUTEAUGMENT module. ", "page_idx": 5}, {"type": "text", "text": "Compared Methods. We compare BEND-VLM against the following debiasing methods: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Baseline CLIP [33] is simply the original CLIP model (e.g. ViT-B-P16 or ViT-L-P14) without any debiasing steps. This acts as our baseline. \u2022 Orthogonal Projection (Orth-Proj.) [10] debiases the query embedding by making the embedding orthogonal to the global spurious attribute subspace (e.g. making the embedding orthogonal to the directions in the embedding space most correlated with gender). \u2022 Orthogonal Calibration (Orth-Cal.) [10] likewise makes the embedding orthogonal to the global spurious attribute subspace, but introduces an additional regularization term to encourage attribute-augmented versions of the query to be close together after projection. \u2022 DebiasCLIP [6] finetunes a CLIP model to remove spurious attribute bias. The authors have released the weights for DebiasCLIP trained to do gender debiasing on CLIP-ViT-B-P16, but have not made their training code available. This means we compare against this method only when evaluating on experiments that use CLIP-ViT-B-P16. Note that while the released DebiasCLIP model was trained for gender debiasing, we also include it in evaluations for race debiasing but do not expect it to be competitive in these settings. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. We do a 50/50 split of each dataset for the reference and target datasets. We additionally create 5 folds for the target dataset so that we can compute confidence intervals for all methods. We chose $n=100$ when selecting the $n$ most relevant images for computing each $D_{\\mathrm{ref}}(\\pmb{a}_{i},\\pmb{c})$ (see Section 3.2). We use the default value of $\\lambda=1000$ for Orth-Cal. and Orth-Proj.\u2019s main hyperparameter. During retrieval, we always sample 500 images from the target dataset. Our reference and target datasets are drawn from the pre-established training split of each dataset. ", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics. We measure $K L[\\hat{P}_{a}||P_{a}]$ , the KL divergence between the attribute prior $P_{a}$ (e.g. the true distribution of genders in the target dataset) and $\\hat{P}_{a}$ , the empirical distribution of attribute labels for the set of images retrieved from the target dataset for a given query. Intuitively, if the query does not rely on the spurious attribute when computing similarity, then the instances retrieved (e.g. the most similar instances) should result in an empirical attribute distribution that matches the overall distribution of the spurious attribute. For instance, if a dataset contains $40\\%$ males and $60\\%$ females, then if we sample independently of gender we should retrieve roughly $40\\%$ males and $60\\%$ females. We also report the MaxSkew between the attribute prior and empirical retrieved distribution, $M a x S k e w=m a x_{{\\pmb a}_{i}}l o g(\\hat{P}_{\\pmb{a}}({\\pmb a}_{i})/P_{\\pmb{a}}({\\pmb a}_{i}))$ . ", "page_idx": 5}, {"type": "image", "img_path": "utMOhsgXzB/tmp/a6537bc0b449a353c76d94872971adb2dd75863299a6e8bf701ac715d43abb47.jpg", "img_caption": ["Figure 2: Our approach increases accuracy while decreasing bias. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For zero-shot classification, we compute the AUC ROC for each group using the similarity between the query and images from each group in the retrieval set as the score. We then report Worst Group A $\\begin{array}{r}{U\\bar{C}\\,R O C:\\,m i n_{a_{i}}\\overset{\\cdot}{A}U C\\_R O C\\left([1-\\overset{\\cdot}{d}(m_{j,a_{i}},z)]_{j=1}^{n_{a_{i}}},[c_{j}]_{j=1}^{n_{a_{i}}}\\right)}\\end{array}$ , where $d(\\cdot,\\cdot)$ is cosine distance and $1-d(\\cdot,\\cdot)$ is cosine similarity. Worst Group AUC ROC tells us how useful the similarity score to the text embedding is for zero-shot classification for members of the most disadvantaged group. ", "page_idx": 6}, {"type": "text", "text": "Queries sets. Since CELEBA has class labels for hair color, we use a set of queries relating to this which we refer to as HAIRCOLOR so that we can measure zero-shot classification performance via Worst Group AUC. HAIRCOLOR is the set $\\{\"\\mathtt{A}$ photo of a celebrity with $\\{\\mathrm{C0LOR}\\}~\\mathrm{\\hair\"\"}\\}$ , for COLOR $\\in$ {blond, black, brown, gray}. We also use the query set STEREOTYPES, a set of negative words such as \"delinquent\" and \"terrorist\" taken from the SO-B-IT VLM auditing taxonomy [15], which is known to contain race and gender bias. Each of our queries is given in the appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Optimizing Accuracy And Minimzing Fairness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We study the effect debiasing has on accuracy through Worst Group AUC ROC as well as the KL divergence and MaxSkew bias metrics. We use CELEBA since it has class labels for HAIRCOLOR. ", "page_idx": 6}, {"type": "text", "text": "Figure 2 shows Worst Group AUC vs MaxSkew. The ideal method would be in the top left of the plot, indicating high accuracy and low bias. Our BEND-VLM method is close to this ideal region. We increase Worst Group AUC over the baseline, roughly matching the AUC performance of Orth-Proj. and Orth-Cal. while having significantly less bias than them. DebiasCLIP has a better MaxSkew than Orth-Proj. and Orth-Cal \u2014 but still worse than BEND-VLM \u2014 while decreasing AUC compared to the baseline. We include additional results for this experiment in Section A.1 in the appendix; see Table 6 for results for this same setting, along with the KL divergence metric. We clearly see that BEND-VLM consistently has significantly better bias scores than all compared method, while having negligibly worse AUC than the next method and significantly better AUC than the baseline. ", "page_idx": 6}, {"type": "text", "text": "4.2 Mitigating STEREOTYPE Bias ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our method on removing the association the Stereotype words have to race and gender. The results for UTKFACE, FAIRFACE, CELEBA are shown in Tables 1, 2, and 3 respectively. We again see that BEND-VLM consistently has less bias than the compared methods in all the ", "page_idx": 6}, {"type": "text", "text": "scenarios we evaluated. Notably, the other debiasing techniques generally improve over the baseline but sometimes have worse MaxSkew or KL Divergence \u2014 which is never observed for our approach. ", "page_idx": 7}, {"type": "table", "img_path": "utMOhsgXzB/tmp/094c1b406403d829cefb75323f89ab8bec16ba8df418aec44999fb50d9136b40.jpg", "table_caption": ["Table 1: Debiasing the UTKFACE dataset with respect to gender and race for STEREOTYPE queries. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "utMOhsgXzB/tmp/52f7d6828dcb1349e5c2105bf5dc5aa6a1baddd006b0a98fd6fc38926c7cf0a3.jpg", "table_caption": ["Table 2: Debiasing the FAIRFACE dataset with respect to gender and race for STEREOTYPE queries. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "utMOhsgXzB/tmp/f651e07f73572f6b361c5d002f67024b41edf4a7c790062ea8e291b4ac85c473.jpg", "table_caption": ["Table 3: Debiasing the CELEBA dataset with respect to gender for STEREOTYPE queries. We do not evaluate race on CELEBA as this dataset lacks race annotations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Intersecrtional Debiasing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have conducted a new experiment where we debias FairFace with respect to gender for HairColor queries, but evaluate on race. We do not expect to see improvements with respect to racial bias after gender debiasing for any method. Table 4 that racial bias goes up for all debiasing methods after gender debiasing. This reflects a known, frustrating \u201cWhac-A-Mole\u201d issue where debiasing for one attribute often increases the bias of another attribute [23]. Interestingly, we do not see racial bias increase when performing only Step 2 of the Bend-VLM debiasing, indicating that this short cut issue is most strongly affected by the orthogonalization operation performed in Step 1. The other debiasing methods also perform a similar orthogonalization step and likewise experience this shortcut problem. ", "page_idx": 7}, {"type": "table", "img_path": "utMOhsgXzB/tmp/5240bc4b2a52e01a19134545c9eaa9e991727e20a253fe483affd69281bc1133.jpg", "table_caption": ["Table 4: Debiasing FAIRFACE with respect to HAIRCOLOR queries with respect to gender, but evaluated on race. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Debiasing Image Captioning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this experiment, we evaluate the effect of BEND-VLM on debiasing automatic image captioning. We study ClipCap [30] (ViT-B/32 vision encoder, pretrained on Conceptual Captions [41]), as it is one of the few captioning methods which takes in only the final layer embedding vector, as opposed to BLIP [22] or LLaVA [25], which take in the sequence of embeddings from the ViT. ", "page_idx": 8}, {"type": "text", "text": "We hand picked 20 images that we observed to have significantly negative or harmful captions generated from the Baseline CLIP embeddings. After debiasing with BEND-VLM, we performed a manual inspection and determined that 6 out of the 20 had less harmful captions after debiasing, 3 had increased harm, and 11 were equal to the original captions. ", "page_idx": 8}, {"type": "text", "text": "Next, we randomly sample 1600 images from FAIRFACE\u2019s validation set that result in captions containg any of the following negative words: [ \"abandoned\", \"murder\", \"homeless\", \"accuse\", \"kill\", \"anime\", \"arrest\", \"surprised\", \"blood\", \"shot\", \"pregnant\", \"intoxicat\", \"charged\", \"bad day\", \"permanently surprised\", \"bandage\", \"hit\", \"wilful\", \"no idea\", \"prison\", \"abuse\", \"attack\" ]. We then perform automated sentiment analysis using CLIP. Table 5 shows that BEND-VLM decreases the average negative sentiment per race, and makes this average more equal between the races. ", "page_idx": 8}, {"type": "table", "img_path": "utMOhsgXzB/tmp/f5a3966f5f411c0899369a15a3e130a418941318317e7096e06ec62713613f1a.jpg", "table_caption": ["Table 5: Average negative sentiment scores for the generated FAIRFACE captions. Lower is better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Limitations and Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "BEND-VLM requires a reference dataset with protected attribute annotations, which is not feasible for every scenario. In our current implementation, our ATTRIBUTESWAP module requires the use of a relatively small 7B LLM. This could still incur too much computational overhead for very resource-constrained settings. Additionally, our evaluation datasets are not perfect. They contain only binary gender labels, but there is a large population of people who don\u2019t identify that way. Moreover, the race and gender labels are not from self-identification, meaning they are only a noisy signal for identity. We believe that our method overall takes a step towards understanding and mitigating biases, and can still be directly extended to support a more nuanced solution to the extreme challenges of mitigating social biases. ", "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Biases in Vision-Language Models. Vision-Language models have become increasingly widespread in recent years [33, 35, 37, 36]. However, these models are known to suffer from spurious correlations [55] and can be biased towards certain races and genders [8]. Studies have shown that biases in these models can stem from the datasets they are trained on. For example, Agarwal et al. [1] found that the CLIP model associates \"white\" text labels less accurately with white individuals than with individuals from other racial groups, and images of people labeled as Black are more likely to be mislabeled as animals. Additionally, Dehouche [12] identified gender bias in CLIP when prompted with gender-neutral text, and Wolfe et al. [53] noted that multiracial individuals are more likely to be assigned minority racial labels. The biases embedded in these models reflect the biases present in the training data, which often include offensive and stereotypical content [7, 8, 47, 39]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Debiasing Vision-Language Models. Recent advancements in debiasing vision, language, and vision-language models have led to various methods for mitigating biases, ranging from data augmentation and balancing [7] to model-level adjustments such as adversarial training [45]. For instance, Wang et al. [50] proposed removing dimensions in the CLIP embedding correlated with gender attributes, while Berg et al. [6] used prompt learning via an adversarial approach to debias CLIP models. Other techniques include learning additive residual image representations [40] and improving robustness to spurious correlations in CLIP via employing contrastive learning [56] and spuriousaware fine-tuning [55]. Friedrich et al. [13] developed a look-up table for fair text-to-image diffusion models. Similarly, Kong et al. [20] addressed test-time bias in image retrieval by downsampling the majority class in query results, and the Adept framework [54] use debiasing prompts for text embeddings. Chuang et al. [10] reduced bias without extensive fine-tuning by orthogonalizing embedding dimensions associated with protected attributes. Kim et al. [19] emphasized the importance of addressing gender and racial biases in vision-language models. Despite these efforts, achieving effective debiasing without extensive retraining remains challenging. In contrast, our approach, which is fully zero-shot and does not depend on any downstream dataset or model training, aims to provide a more scalable solution to debiasing vision-language models, especially in open-set scenarios where only a piece of text is provided, rather than multiple classes. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work proposes a test-time VLM debiasing method that does not require finetuning, and is able to perform query-specific nonlinear debiasing rather than a one-size-fits-all approach. Our experiments on removing race and gender bias in retrieval, classification, and image captioning indicate that our method consistently decreases bias while improving worst group performance. We found that our method consistently matches the accuracy of the best performing compared method, while significantly decreasing bias beyond all compared methods. We hope that our method inspires more work on efficient, nonlinear debiasing techniques for VLMs. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by a National Science Foundation (NSF) 22-586 Faculty Early Career Development Award (#2339381), a Gordon & Betty Moore Foundation award & a Google Research Scholar award. Thomas Hartvigsen\u2019s contribution was funded in part by the National Security Data & Policy Institute, Contracting Activity #2024-24070100001. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles Brundage. Evaluating clip: towards characterization of broader capabilities and downstream implications. arXiv preprint arXiv:2108.02818, 2021.   \n[2] Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander D\u2019Amour, and Xiaohua Zhai. Clip the bias: How useful is balancing data in multimodal learning? arXiv preprint arXiv:2403.04547, 2024.   \n[3] Junaid Ali, Matth\u00e4us Kleindessner, Florian Wenzel, Kailash Budhathoki, Volkan Cevher, and Chris Russell. Evaluating the fairness of discriminative foundation models in computer vision. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pages 809\u2013833, 2023.   \n[4] Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, and Furong Huang. More context, less distraction: Visual classification by inferring and conditioning on contextual attributes. arXiv preprint arXiv:2308.01313, 2023.   \n[5] P\u0131nar Barlas, Kyriakos Kyriakou, Styliani Kleanthous, and Jahna Otterbacher. Person, human, neither: the dehumanization potential of automated image tagging. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 357\u2013367, 2021.   \n[6] Hugo Berg, Siobhan Hall, Yash Bhalgat, Hannah Kirk, Aleksandar Shtedritski, and Max Bain. A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 806\u2013822, Online only, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.aacl-main.61.   \n[7] Shruti Bhargava and David Forsyth. Exposing and correcting the gender bias in image captioning datasets and models. arXiv preprint arXiv:1912.00578, 2019.   \n[8] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.   \n[9] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29, 2016.   \n[10] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debiasing vision-language models via biased prompts. arXiv preprint arXiv:2302.00070, 2023.   \n[11] Sepehr Dehdashtian, Lan Wang, and Vishnu Boddeti. Fairvlm: Mitigating bias in pre-trained vision-language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[12] Nassim Dehouche. Implicit stereotypes in pre-trained classifiers. IEEE Access, 9:167936\u2013 167947, 2021.   \n[13] Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Kristian Kersting. Fair diffusion: Instructing text-to-image generation models on fairness. arXiv preprint arXiv:2302.10893, 2023.   \n[14] Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan Misra, and Candace Ross. Vision-language models performing zero-shot tasks exhibit disparities between gender groups. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2778\u20132785, 2023.   \n[15] Kimia Hamidieh, Haoran Zhang, Walter Gerych, Thomas Hartvigsen, and Marzyeh Ghassemi. Identifying implicit social biases in vision-language models. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, volume 7, pages 547\u2013561, 2024.   \n[16] Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 743\u2013756, 2022.   \n[17] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[18] Kimmo K\u00e4rkk\u00e4inen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age. arXiv preprint arXiv:1908.04913, 2019.   \n[19] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin. Discovering and mitigating visual biases through keyword explanation, 2024.   \n[20] Fanjie Kong, Shuai Yuan, Weituo Hao, and Ricardo Henao. Mitigating test-time bias for fair image retrieval. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Naresh Kumar Lahajal et al. Enhancing image retrieval: A comprehensive study on photo search using the clip mode. arXiv preprint arXiv:2401.13613, 2024.   \n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[23] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20071\u201320082, 2023.   \n[24] Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, and Louis-Philippe Morency. Towards debiasing sentence representations. arXiv preprint arXiv:2007.08100, 2020.   \n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[27] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Evaluating societal representations in diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024.   \n[29] Subha Maity, Mayank Agarwal, Mikhail Yurochkin, and Yuekai Sun. An investigation of representation and allocation harms in contrastive learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[30] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.   \n[31] Jishnu Mukhoti, Yarin Gal, Philip HS Torr, and Puneet K Dokania. Fine-tuning can cripple your foundation model; preserving features may be the solution. arXiv preprint arXiv:2308.13320, 2023.   \n[32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[34] Inioluwa Deborah Raji, I Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. The fallacy of ai functionality. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 959\u2013972, 2022.   \n[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[38] Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. Finding a\" kneedle\" in a haystack: Detecting knee points in system behavior. In 2011 31st international conference on distributed computing systems workshops, pages 166\u2013171. IEEE, 2011.   \n[39] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[40] Ashish Seth, Mayur Hemani, and Chirag Agarwal. Dear: Debiasing vision-language models with additive residuals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6820\u20136829, 2023.   \n[41] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.   \n[42] Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, and Mohan Kankanhalli. Finetuning text-to-image diffusion models for fairness. arXiv preprint arXiv:2311.07604, 2023.   \n[43] Andrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2383\u20132389, 2021.   \n[44] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15638\u201315650, 2022.   \n[45] Tejas Srinivasan and Yonatan Bisk. Worst of both worlds: Biases compound in pre-trained vision-and-language models. arXiv preprint arXiv:2104.08666, 2021.   \n[46] Harini Suresh and John Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pages 1\u20139, 2021.   \n[47] Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Na Zou, and Xia Hu. Mitigating gender bias in captioning systems. In Proceedings of the Web Conference 2021, page 633\u2013645, 2021.   \n[48] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2.   \n[49] Angelina Wang, Solon Barocas, Kristen Laird, and Hanna Wallach. Measuring representational harms in image captioning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 324\u2013335, 2022.   \n[50] Jialu Wang, Yang Liu, and Xin Eric Wang. Are gender-neutral queries really gender-neutral? mitigating gender bias in image search. arXiv preprint arXiv:2109.05433, 2021.   \n[51] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.   \n[52] Robert Wolfe and Aylin Caliskan. Markedness in visual semantic ai. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1269\u20131279, 2022.   \n[53] Robert Wolfe, Mahzarin R Banaji, and Aylin Caliskan. Evidence for hypodescent in visual semantic ai. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1293\u20131304, 2022.   \n[54] Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng Ji. Adept: A debiasing prompt framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10780\u201310788, 2023.   \n[55] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman. Mitigating spurious correlations in multi-modal models during fine-tuning. In International Conference on Machine Learning, pages 39365\u201339379. PMLR, 2023.   \n[56] Michael Zhang and Christopher R\u00e9. Contrastive adapters for foundation model group robustness. Advances in Neural Information Processing Systems, 35:21682\u201321697, 2022.   \n[57] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5810\u20135818, 2017.   \n[58] Zengqun Zhao and Ioannis Patras. Prompting visual-language models for dynamic facial expression recognition. arXiv preprint arXiv:2308.13382, 2023.   \n[59] Beier Zhu, Yulei Niu, Saeil Lee, Minhoe Hur, and Hanwang Zhang. Debiased fine-tuning for vision-language models by prompt regularization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 3834\u20133842, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Expanded CelebA HAIRCOLOR Results ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "utMOhsgXzB/tmp/00beb56cc186586fed2cca76e449aadd2fd4245da39bd0ca16c394398b55517a.jpg", "table_caption": ["Table 6: Debiasing the CELEBA dataset with respect to gender for the HAIRCOLOR queries. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 6 shows the results for debiasing Gender for the CELEBA dataset. We clearly see that BENDVLM consistently has significantly better bias scores than all compared method, while having negligibly worse AUC than the next method and significantly better AUC than the baseline. ", "page_idx": 14}, {"type": "text", "text": "A.2 Ablation Study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We verify that both Step 1 and Step 2 contribute to the success of BEND-VLM through an ablation study. Table 7 shows that while most of the Worst-Group Accuracy performance comes from Step 1, utilizing only step 1 results in a much more biased retrieval metric by having a much higher KL divergence from a fair distribution. Utilizing step 2 alone results in a fair retrieval roughly equivalent to the full BEND-VLM approach, but does not have as good of a Worst Group Accuracy. We achieve the best results by combining Step 1 and Step 2 to make the full BEND-VLM approach. Results shown on CELEBA for HAIRCOLOR queries. ", "page_idx": 14}, {"type": "table", "img_path": "utMOhsgXzB/tmp/47f2d6812d69a7b637d11c2f60f1dfc81868743facde41f9bac4e6f0379b98ef.jpg", "table_caption": ["Table 7: Ablation study. Debiasing the CELEBA dataset with respect to gender for the HAIRCOLOR queries. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Evaluation Using An OOD Reference Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this experiement, FAIRFACE is used as the reference dataset while CELEBA is the target dataset. While BEND-VLM with this out of distribution (OOD) reference dataset does not perform as well as BEND-VLM with an in-distribution reference dataset, it still outperforms the other compared approaches. See Table 8. Results shown for HairColor queries. ", "page_idx": 14}, {"type": "table", "img_path": "utMOhsgXzB/tmp/3c858686969f12160e150aae8e25661f31dda978f4a2d7e55ede3f967ef7874b.jpg", "table_caption": ["Table 8: OOD reference data experiment. Reference data from FAIRFACE while the target data is CELEBA. Debiasing the CELEBA dataset with respect to gender for the HAIRCOLOR queries. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Applying to non-CLIP VLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our method requires a VLM that can construct a vector representation of text and images in a joint space, but this does not need to be a CLIP model. To show this generalizability, we evaluate our method on FLAVA [44]. Table 9 shows that Bend-VLM still outperforms the compared methods when FALVA is the VLM. Results shown for the CelebA dataset. Note that there are no \u201cground truth\u201d labels for the stereotype queries, so it isn\u2019t possible to compute AUC for them. ", "page_idx": 15}, {"type": "table", "img_path": "utMOhsgXzB/tmp/feba0038f67b5dc0d17b8ff7a101677210337c65146f8ddcfc99820cbda91426.jpg", "table_caption": ["Table 9: Debiasing the CELEBA dataset with FLAVA. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Lemma 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 1. We will prove by counter example. Without lack of generalizability, consider the case where the embedding space is 2 dimensional and there are two instances in the reference dataset, $\\pmb{m}_{1}$ and $\\pmb{m}_{2}$ , where the first is associated with the spurious attribute value $\\pmb{a}_{1}$ and one associated with $\\mathbf{\\deltaa}_{2}$ . Define a basis where $[0,1]$ corresponds to the spurious attribute subspace and $[1,0]$ is the space orthogonal to it. Let $[1,0]$ be the directtion of $\\pmb{a}_{1}$ and $[-1,0]$ be the direction of $\\mathbf{\\deltaa}_{2}$ . After orthogonalizing, the query embedding $z^{\\prime}$ lies on $[0,1]$ , and has equal cosine similarity to $[1,0]$ and $[-1,\\bar{0}]$ . Since $m_{1}$ is associated with $\\pmb{a}_{1}$ , it will have a higher cosine similarity with $[1,0]$ than $[-1,0]$ . The opposite is true for $m_{2}$ . However, this does not mean that the $d(\\pmb{m}_{1},[1,0])\\stackrel{.}{=}d\\bar{(\\pmb{m}_{2},[\\stackrel{.}{-1},0])}$ . This implies that $d(\\pmb{m}_{1},\\pmb{z}_{c}^{\\prime})=d(\\pmb{m}_{2},\\pmb{z}_{c}^{\\prime})$ does not always hold. ", "page_idx": 15}, {"type": "text", "text": "A.7 Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 2. . We can obtain this solution using Lagrange multipliers. In the binary case, we will have two constraints: $\\begin{array}{r l}{c o n s t r a i n t_{1}}&{:\\;\\;\\frac{\\daleth}{|D_{r e f}(a_{2},c)|}\\sum_{m_{j}\\in D_{r e f}(a_{2},c)}^{\\sim}d(f_{\\theta}^{M}(m_{j},z_{c}^{*}))\\;=\\;}\\end{array}$ |Dref (1a1,c)| mk\u2208Dref (a1,c) d(f \u03b8M (mk, zc\u2217)), (which states that the average distances to both attribute values should equal), and $z^{*}\\cdot z^{*}=1$ (which states that the solution should have a length of 1). We want to minimize $\\bar{-}d(z^{*},z)=-z^{*}\\cdot z/||z^{*}||\\cdot||z||=-z^{*}\\cdot z$ (as each vector has a norm of 1). ", "page_idx": 15}, {"type": "text", "text": "For ease of notation, let us refer to |Dref (1a2,c)| as n1x , |Dref (1a2,c)| as n12 , the jth instance of $D_{\\mathrm{ref}}(a_{1},c)$ as $\\pmb{x}_{j}$ and the $i$ th instance of $D_{\\mathrm{ref}}(a_{2},c)$ as $\\pmb{y}_{i}$ .   \nWe can write then Lagrange multiplier equation as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(z_{c}^{*},\\lambda,\\pi)=-z_{c}^{*}\\cdot z_{c}+\\lambda\\big(\\frac{1}{n_{y}}\\sum_{i=1}^{n_{y}}y_{i}\\cdot z_{c}^{*}-\\frac{1}{n_{x}}\\sum_{j=1}^{n_{x}}x_{j}\\cdot z_{c}^{*}\\big)+\\pi\\big(z_{c}^{*}\\cdot z_{c}^{*}-1\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the gradient with respect to $\\boldsymbol{z}_{c}^{*}$ and setting it to 0, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=-z_{c}+\\lambda\\big(\\frac{1}{n_{y}}\\sum_{i=1}^{n_{y}}{y}_{i}-\\frac{1}{n_{x}}\\sum_{j=1}^{n_{x}}{x}_{j}\\big)+2\\pi z_{c}^{*}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{\\pmb{y}}=\\frac{1}{n_{y}}\\sum_{i=1}^{n_{y}}{\\pmb{y}}_{i}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\pmb x}=\\frac{1}{n_{x}}\\sum_{j=1}^{n_{x}}{\\pmb x}_{j}}\\end{array}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=-z_{c}+\\lambda\\big(\\displaystyle\\frac{1}{n_{y}}\\displaystyle\\sum_{i=1}^{n_{y}}y_{i}-\\displaystyle\\frac{1}{n_{x}}\\displaystyle\\sum_{j=1}^{n_{x}}x_{j}\\big)+2\\pi z_{c}^{*}}\\\\ &{\\quad=-z_{c}+\\lambda\\big(\\bar{y}-\\bar{x}\\big)+2\\pi z_{c}^{*}}\\\\ &{\\quad=-z_{c}+\\lambda\\bar{y}-\\lambda\\bar{x}+2\\pi z_{c}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solving for $z_{c}^{*}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nz_{c}^{*}=\\frac{z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}}{2\\pi}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging this into our norm constraint: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{0=z_{c}^{*}\\cdot z_{c}^{*}-1}\\\\ &{\\;\\;={\\frac{z_{c}-{\\lambda}{\\bar{y}}+{\\lambda}{\\bar{x}}}{2\\pi}}\\cdot{\\frac{z_{c}-{\\lambda}{\\bar{y}}+{\\lambda}{\\bar{x}}}{2\\pi}}-1}\\\\ &{\\;\\;={\\frac{\\left(z_{c}-{\\lambda}{\\bar{y}}+{\\lambda}{\\bar{x}}\\right)\\cdot\\left(z_{c}-{\\lambda}{\\bar{y}}+{\\lambda}{\\bar{x}}\\right)}{4\\pi^{2}}}-1}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solving for $\\pi$ ; ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi=\\frac{\\sqrt{\\left(z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}\\right)\\cdot\\left(z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}\\right)}}{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now plugging our equation for $\\boldsymbol{z}_{c}^{*}$ into constraint1: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{0=\\frac{1}{n_{y}}\\sum_{i=1}^{n_{y}}y_{i}\\cdot z_{c}-\\frac{1}{n_{x}}\\sum_{j=1}^{n_{x}}x_{j}\\cdot z_{c}}}\\\\ &{=(\\frac{1}{n_{y}}\\sum_{i=1}^{n_{y}}y_{i})\\cdot z_{c}-(\\frac{1}{n_{x}}\\sum_{j=1}^{n_{x}}x_{j})\\cdot z_{c}}\\\\ &{=\\bar{y}\\cdot z_{c}^{*}-\\bar{x}\\cdot z_{c}^{*}}\\\\ &{=\\bar{y}\\cdot(\\frac{z_{c}-\\lambda\\bar{y}}{2\\pi}+\\lambda\\bar{x})-\\bar{x}\\cdot(\\frac{z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}}{2\\pi})}\\\\ &{=\\frac{\\bar{y}\\cdot\\big(z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}\\big)-\\bar{x}\\cdot\\bar{y}\\cdot\\big(z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}\\big)}{2\\pi}}\\\\ &{=\\bar{y}\\cdot(z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x})-\\bar{x}\\cdot\\bar{y}+\\lambda\\bar{x})}\\\\ &{=\\bar{y}\\cdot z_{c}-\\lambda\\bar{y}\\cdot\\bar{y}+\\lambda\\bar{y}\\cdot\\bar{x}-\\bar{x}\\cdot z_{c}+\\lambda\\bar{x}\\cdot\\bar{y}-\\lambda\\bar{x}\\cdot\\bar{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Solving for $\\lambda$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda=\\frac{\\bar{x}\\cdot z_{c}-\\bar{y}\\cdot z_{c}}{2\\bar{x}\\cdot\\bar{x}-\\bar{y}\\cdot\\bar{y}-\\bar{x}\\cdot\\bar{x}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\bar{\\pmb{x}}$ is equivalent to $\\mu(\\pmb{a}_{1},\\boldsymbol{c})$ and $\\bar{\\pmb{y}}$ is equivalent to $\\mu(\\pmb{a}_{2},c)$ . The last thing to note is that when plugging in $\\pi$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{c}^{*}=\\frac{z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}}{2\\pi}}\\\\ &{\\quad=\\frac{z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}}{||z_{c}-\\lambda\\bar{y}+\\lambda\\bar{x}||_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have now obtained the solution given in Lemma 2. ", "page_idx": 17}, {"type": "text", "text": "A.8 Manually Evaluated Generation Captions ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "utMOhsgXzB/tmp/920a9aec642c8d5431067a67389a273b21b7be9da4a13907f6250553933b0f36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "utMOhsgXzB/tmp/a3db53427050a7cde0b8c14cb20c9bf787d4f900719915b3c93b99515724195a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.9 Prompt Templates ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.10 Minstrel 7B Prompt for ATTRIBUTEAUGMENT ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "utMOhsgXzB/tmp/54b34e538053f740e6e6bead57c002eb779916f99a19c3643727abbf36bee1de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.10.1 Attribute subspace prompts ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "utMOhsgXzB/tmp/cfb33603071c13189195ba45b794bdc8a83b3e9683ead3665c904516413b4300.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We replace {ATTRIBUTE} with male or female for gender debiasing, and each of the races in UTKFACE or FAIRFACE for race debiasing. ", "page_idx": 20}, {"type": "text", "text": "A.11 Prompts for HAIRCOLOR ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "utMOhsgXzB/tmp/d6d53ea2e1a707b0102a8e1d21a3fd48bb52edc9299b8c5c4a143ffabdb97f2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.12 Prompts for STEREOTYPE Queries ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "utMOhsgXzB/tmp/c407bfa7ebbfd0388ee78c14880f8913c2bab8c3911ebec4ed0ea282bf93573f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Reproducibility Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We describe our problem setting and scope of our proposed approach in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include the assumptions in the surrounding text and the proofs in the appendix. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We described our setup in Section 4 and include our experimental code publicly. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, our code is available at: https://github.com/waltergerych/bend_ vlm ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include key details in Section 4. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include $95\\%$ confidence intervals from 5 random seeds. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We used a high-performance computing cluster for our experiments. All experiments can be reproduced in under a day with a single NVIDIA A100 (80GB VRAM) and 100GB of RAM. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Justification: We have read and understood to the NeurIPS code of ethics. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: This work does not result in new datasets or new pretrained models. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We cite the datasets and compared methods. Licence information is included by the authors of these resources in their respective documents. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not use crowdsouring or human subjects. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not have human subjects. ", "page_idx": 22}]