{"references": [{"fullname_first_author": "He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-00-00", "reason": "This paper introduces the ResNet architecture, a fundamental building block in many computer vision models, including this paper's approach."}, {"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP's introduction of contrastive learning using image-text pairs is foundational to this paper's approach, which also leverages image-text pairs."}, {"fullname_first_author": "Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-00-00", "reason": "SAM provides a foundation model for segmentation that is incorporated in this paper's approach."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper's demonstration of large language model capabilities for few-shot learning provides a foundation for this paper's use of LLMs."}, {"fullname_first_author": "Liu", "paper_title": "Referring image segmentation using text supervision", "publication_date": "2023-00-00", "reason": "This paper, closely related to the target paper, directly addresses weakly-supervised referring image segmentation, making it highly relevant."}]}