[{"type": "text", "text": "Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mitchell Keren Taraday\u2217 ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science Technion Haifa, Israel   \nbutovsky.mitchell@gmail.com   \nAlmog David\u2217   \nDepartment of Computer Science   \nTechnion   \nHaifa, Israel   \nalmogdavid@gmail.com ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Chaim Baskin School of Electrical and Computer Engineering Ben-Gurion University of the Negev Be\u2019er Sheva, Israel chaimbaskin@bgu.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Message Passing Graph Neural Networks (MPGNNs) have emerged as the preferred method for modeling complex interactions across diverse graph entities. While the theory of such models is well understood, their aggregation module has not received sufficient attention. Sum-based aggregators have solid theoretical foundations regarding their separation capabilities. However, practitioners often prefer using more complex aggregations and mixtures of diverse aggregations. In this work, we unveil a possible explanation for this gap. We claim that sum-based aggregators fail to \"mix\" features belonging to distinct neighbors, preventing them from succeeding at downstream tasks. To this end, we introduce Sequential Signal Mixing Aggregation (SSMA), a novel plug-and-play aggregation for MPGNNs. SSMA treats the neighbor features as 2D discrete signals and sequentially convolves them, inherently enhancing the ability to mix features attributed to distinct neighbors. By performing extensive experiments, we show that when combining SSMA with well-established MPGNN architectures, we achieve substantial performance gains across various benchmarks, achieving new state-of-the-art results in many settings. We published our code at https://almogdavid.github.io/SSMA/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Message-passing Graph Neural Networks (MPGNNs) have established themselves as the major workhorses for graph representation learning over the past decade [24]. These models have been proven to be effective in graph-structured problems in a variety of domains, ranging from social networks [18] to natural sciences [14, 23, 4] and having some non-trivial applications in computer vision and natural language processing [26, 33, 45, 28]. ", "page_idx": 0}, {"type": "text", "text": "Such renowned models of this nature owe their success to their high efficiency, along with good generalization capabilities and simplicity. A typical MPGNN takes graph-structured data containing node and edge features as input. It then iteratively updates node representations by combining their egocentric view with a symmetrized aggregation of their proximate neighbor features. ", "page_idx": 0}, {"type": "text", "text": "The key insight regarding the expressive power of such models is their equivalence to the Weisfeller-Lehman (WL) graph isomorphism test [47]. Consequently, past research directions were majorly directed toward developing models that surpass the vanilla WL test by tackling the graph learnability problem from various perspectives, including stronger notions of the WL test [31, 35], spectral graph methods [46, 13, 44] and graph transformers [49, 36]. ", "page_idx": 1}, {"type": "text", "text": "However, one subtle but often overlooked detail in such expressivity analyses is the existence of a Hash function, which compresses the neighbor features into a fixed-sized representation. Such Hash function need not only be injective but also differentiable and efficient in terms of memory ", "page_idx": 1}, {"type": "image", "img_path": "aRokfUfIQs/tmp/3d55b912fe735c3484e07fa77c38199072f8036ced8736adadedf36c703c5b56.jpg", "img_caption": ["Figure 1: An efficient and provable generalization of the DeepSets polynomial to vector features. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "and computation. The seminal DeepSets paper [50] showcased such a sum-based construction for the Hash function. While this construction was very simple and computationally efficient, the theoretical representation size required in this construction is exponential in the node feature dimension. Although the bound on this representation size was improved in later works [17, 1], sum-based aggregations seem to lag behind the aggregators used in practice [11]. ", "page_idx": 1}, {"type": "text", "text": "In this work, we suggest that a possible explanation for this gap is the inability of sum-based aggregators to \"mix\" features belonging to distinct neighbors. We formalize the \"neighbor-mixing\" property and show that sum-based aggregators have limited neighbor-mixing capability. This observation is later verified by conducting an experiment showing that sum-based aggregators struggle with approximating even a very simple function requiring neighbor-mixing. ", "page_idx": 1}, {"type": "text", "text": "With this motivation in mind, we propose a new aggregation module that treats the neighbor features as two-dimensional discrete signals and sequentially convolves them - hence coined as Sequential Signal Mixing Aggregation (SSMA). SSMA has a provably polynomial representation size $m={\\mathcal{O}}({\\bar{n}}^{2}d)$ (where $n$ is the number of neighbors and $d$ is feature dimensionality). The theoretical construction underlying SSMA provides a positive answer to a lasting mystery regarding DeepSets [50] - \u201cCan the DeepSets polynomial be efficiently generalized to handle vector features?\u201das depicted in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "As later investigated, the convolutional component in SSMA allows it to directly mix features attributed to distinct neighbors, inducing a higher-order notion of neighbor mixing. We then discuss some practical aspects of SSMA. Particularly, we discuss how to implement it in a computationally efficient manner, how to scale it to larger graphs and how to make it easier to optimize. ", "page_idx": 1}, {"type": "text", "text": "Finally, we demonstrate that when integrated into a wide range of well established MPGNN architectures, SSMA greatly enhances their performance. We observe significant gains across all benchmarks tested, including the TU datasets [32], open graph benchmark (OGB) [21] datasets, long-range graph benchmarks (LRGB) [16] datasets and the ZINC [19] molecular property prediction dataset achieving state-of-the-art results in many settings. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Our contributions may be summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We define the notion of \"neighbor-mixing\" and show that sum-based aggregators have limited neighbor-mixing power. We verify this idea by conducting an experiment on a simple and natural synthetic task.   \n2. We propose Sequential Signal Mixing Aggregation (SSMA) - an aggregation module of dimension $m={\\bar{O}}(n^{2}d)$ which treats the neighbor features as discrete signals and sequentially convolves them. The theoretical construction underlying SSMA builds upon the DeepSets polynomial, efficiently extending it to multidimensional features.   \n3. We introduce a few practices for stabilizing the optimization process of SSMA and show how to scale it to larger graphs.   \n4. Finally, we conduct extensive experiments showing that enriching prominent MPGNN architectures with SSMA yields large improvements on a variety of benchmarks, achieving state-of-the-art results. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{X}$ be some domain. We are interested in representing multisets (sets in which repeated elements are allowed) over that domain. We denote multisets by $\\{\\{x_{1},...,x_{n}\\}\\}$ where each $x_{i}\\,\\in\\,{\\mathcal{X}}$ , and denote by ${\\mathcal{M}}_{n}:=({\\mathcal{X}})^{n}$ the $n$ -tuple space over $\\mathcal{X}$ . We seek a (possibly learnable) permutation invariant mapping $f:\\mathcal{M}_{n}\\rightarrow\\mathbb{R}^{m}$ separating distinct multisets 2. When combined with a learnable compression network $g_{\\theta}:\\mathbb{R}^{m}\\rightarrow\\mathcal{X}$ , their composition $\\gamma=g_{\\theta}\\circ f$ can be utilized as an aggregation module for MPGNNs over the domain $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Particularly, we are interested in continuous features, namely the domains $\\mathcal{X}=\\mathbb{R}$ and $\\mathcal{X}=\\mathbb{R}^{d}$ . We consider the symmetry group ${\\mathcal{G}}=S_{n}$ acting on $\\mathcal{M}_{n}=\\mathbb{R}^{n}$ by $[\\sigma.{\\pmb x}]_{i}={\\pmb x}_{\\sigma^{-1}(i)}$ and on $\\mathcal{M}_{n}=\\mathbb{R}^{n\\times d}$ by $[\\sigma.\\mathsf{X}]_{i j}\\;=\\;\\mathsf{X}_{\\sigma^{-1}(i)j}$ correspondingly. It is widely agreed that finding a good representation $f:\\mathcal{M}_{n}\\overset{^{\\cdot}}{\\rightarrow}\\mathbb{R}^{m}$ for these domains is crucial for building better aggregation modules $\\gamma$ and has a direct influence on the performance of the model on a variety of downstream tasks [47, 12, 27, 40]. ", "page_idx": 2}, {"type": "text", "text": "DeepSets [50] was the pioneering work introducing a sum-based aggregator with a provably finite representation size $m$ : $\\begin{array}{r}{\\dot{\\gamma}(\\{\\{x_{1},...,\\bar{x}_{n}\\}\\})=\\rho(\\sum_{k=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\phi(x_{k}))}\\end{array}$ where $\\phi:\\mathbb{R}^{\\breve{d}}\\to\\mathbb{R}^{m}$ and $\\rho:\\mathbb{R}^{m^{\\bullet}}\\!\\to\\mathbb{R}^{d}$ . Their construction consisted of \"hand-crafted\" moment-based features. Despite being efficient for scalar-based features, the representation size grew exponentially with the node feature dimensionality, $m\\in{\\mathcal{O}}({\\binom{n+d}{d}})$ . This upper bound was later improved to $O(n^{2}d)$ and eventually to a tight $\\Theta(n d)$ [17]. While moment-based features served as a powerful tool for achieving theoretical separation, learnable neural features are favored over such hand-crafted features in practice. As was unveiled, neural features can achieve theoretical separation as well, as long as non-polynomial analytic activations are used [1]. ", "page_idx": 2}, {"type": "text", "text": "Despite their clear theoretical advantages, sum-based aggregators seem to have limited performance in practice [12, 27]. Consequently, many works focused on different species of permutation invariant aggregators. For instance, attention-based aggregators have been proposed to capture the most important signals incoming from the neighborhood [2, 7]. Others suggested using a mixture of symmetric aggregators such as min, max, mean, sum, std as each of these aggregators helps separate different kinds of multisets [47, 41, 12]. Other works focused on aggregations preserving intrinsic properties of the neighborhood data such as variance and fisher-information [38, 30]. ", "page_idx": 2}, {"type": "text", "text": "Another intriguing type of work deals with the relaxation of the neighbor ordering invariance constraint. Particularly, regularizing recurrent neural network-based aggregations to maintain permutation invariance - either by choosing a random neighbor permutation [20] or by explicit regularization terms [10, 34] has raised some interest. ", "page_idx": 2}, {"type": "text", "text": "3 On the limited neighbor-mixing of sum-based aggregators ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Despite their provable separation power, sum-based aggregators seem to lag behind other aggregators used in practice [11]. We claim that a possible explanation for this phenomenon lies in their inability to \"mix\" the neighbor\u2019s features, in that the mutual effect of perturbing the features of two distinct neighbors on each aggregation output is very small. In practice, many downstream tasks require high \"mixing\" values as the aggregator should mix information from different distinct neighbors to produce a useful representation for tackiling the downstream task. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1. Let $\\gamma\\,:\\,\\mathbb{R}^{n\\times d}\\,\\rightarrow\\,\\mathbb{R}^{d}$ be some aggregation function that is continuously twice differentiable. We define the neighbor mixing of the $\\ell$ -th aggregation output with respect to the neighbor pair $(i,j)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{m i x}_{i,j}^{(\\ell)}:=\\left\\|\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}\\gamma^{(\\ell)}(x_{1},...,x_{n})\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "At an intuitive level, sum-based aggregators have small $\\operatorname*{mix}_{i,j}^{(\\ell)}$ values as the result of the local pooling operation is summed across the neighbors. Namely, without explicitly \"mixing\" features from distinct neighbors before the summation. Indeed, given $\\begin{array}{r}{\\dot{\\gamma}(\\{\\{x_{1},...,x_{n}\\}\\})=\\sum_{k=1}^{n}\\bar{\\phi}(x_{k})}\\end{array}$ we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}}\\sum_{k=1}^{n}\\phi^{(\\ell)}(x_{k})=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Formally, to account for mixing that may occur in any subsequent (global) transformation we have the following proposition: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. Let $\\begin{array}{r}{\\gamma(\\left\\{\\left\\{x_{1},...,x_{n}\\right\\}\\right\\})=\\rho\\left(\\sum_{k=1}^{n}\\phi(x_{k})\\right)}\\end{array}$ where $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ is a local operator and $\\rho:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{d}$ is a pooling operator that is continuously twice differentiable. Then, we have $\\forall i\\ne j$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{mix}_{i,j}^{(\\ell)}\\leq\\|J_{\\phi}(x_{i})\\|_{2}\\cdot\\left\\|H_{\\rho^{(\\ell)}}(\\sum_{k=1}^{n}\\phi(x_{k}))\\right\\|_{2}\\cdot\\|J_{\\phi}(x_{j})\\|_{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $J_{\\phi}(.)$ is the Jacobian matrix of $\\phi$ and $H_{\\rho^{(\\ell)}}(.)$ is the Hessian matrix corresponding to $\\ell$ -th output of $\\rho$ . Particularly, for typical choices of $\\phi$ and $\\rho$ it follows: $\\mathsf{m i x}_{i,j}^{(\\ell)}\\in\\mathcal O(\\left||\\theta|\\right|_{2}^{2})$ where $\\theta$ is the concatenation of the parameters in $\\phi$ and $\\rho$ . ", "page_idx": 3}, {"type": "text", "text": "The proof of Proposition 3.2 is given in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Motivated by the above observation, we propose a new species of aggregation module which is convolution-based rather than sum-based. ", "page_idx": 3}, {"type": "text", "text": "4 SSMA- Sequential Signal Mixing Aggregation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Warm-up: DeepSets polynomial from a convolutional point of view ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\overline{{\\pmb{x}}}=\\{\\{\\pmb{x}_{1},...,\\pmb{x}_{n}\\}\\}$ be a scalar multiset. We define its DeepSets polynomial by considering a polynomial of variable $t$ having the multiset elements as its roots: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\overline{{\\mathbf{x}}}}(t):=\\prod_{i=1}^{n}(t-\\pmb{x}_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Its coefficients, we denote by $e_{k}({\\pmb x})$ , are permutation invariant functions. Moreover, the $(e_{k}(\\mathbf{\\boldsymbol{x}}))_{k=0}^{m}\\mathbf{\\boldsymbol{s}}$ form an ensemble of invariant separators 3. ", "page_idx": 3}, {"type": "text", "text": "Instead of describing a polynomial by its coefficients, one can represent a polynomial by evaluating it on some fixed set of points. Given a set of $n+1$ fixed points, the polynomial may be represented by evaluating its value on these points. One can switch from this representation back to the coefficients by solving a system of linear equations, which always has a unique solution. Now, by allowing the evaluation points to be complex, we can choose them as the roots of unity. By doing so, we get the discrete Fourier transform (DFT) of the polynomial coefficients: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\zeta_{j}(x)=\\sum_{k=0}^{n}e_{k}({\\pmb x})\\cdot e^{-\\frac{2\\pi i j}{n+1}k}\\quad(j=0,...,n)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Next, we denote the factors in $\\begin{array}{r}{p_{\\overline{{\\mathbf{x}}}}(t)=\\prod_{i=1}^{n}p_{i}(t)}\\end{array}$ where $p_{i}(t):=t-{\\pmb x}_{i}$ . The (padded) coefficients of each $p_{i}(t)$ are then given by the affi ne transformation: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(\\pmb{x}_{i})=[-\\pmb{x}_{i},1,0,...,0]\\in\\mathbb{R}^{n+1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The nice thing about representing a polynomial by evaluating its values at a list of fixed points is that polynomial multiplication becomes point-wise. It can be deduced that the coefficients of $p_{\\overline{{\\mathbf{x}}}}(t)$ can be computed by transforming the coefficients $h(x_{i})$ of each $p_{i}(t)$ to the Fourier domain, performing ", "page_idx": 3}, {"type": "text", "text": "elementwise multiplication and then transforming back to the coefficients domain. According to the circular convolution theorem, this exactly amounts to sequentially convolving the coefficients $h(x_{i})$ . We combine the above ideas into the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Scalar multisets $\\overline{{\\pmb{x}}}=\\{\\{\\pmb{x}_{1},...,\\pmb{x}_{n}\\}\\}$ can be represented by an invariant and separating map $f_{c o n v}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{c o n v}(\\pmb{x})=\\bigoplus_{i=1}^{n}\\pmb{h}(\\pmb{x}_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $h:\\mathbb{R}\\rightarrow\\mathbb{R}^{m}$ is an affine map, $\\circledast$ is the circular convolution operator, and the number of separators is $m=n+1\\in{\\mathcal{O}}(n)$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 simply states that sequential convolution can be utilized to compute the renowned DeepSets polynomial coefficients. While not particularly surprising, Theorem 4.1 shows that the coefficients of the DeepSets polynomial can be efficiently computed and directly utilized as a multiset representation. Moreover, it paves the way for our construction, as seen in the next section. ", "page_idx": 4}, {"type": "text", "text": "4.2 Efficient generalization to multidimensional features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "\u201cHow does the DeepSets polynomial can be efficiently extended to handle vector features?\u201d ", "page_idx": 4}, {"type": "text", "text": "The key idea underlying our answer to this question is to encode each feature vector as another polynomial, and then to reduce the problem to the scalar case. ", "page_idx": 4}, {"type": "text", "text": "Generalized DeepSets Polynomial ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We encode each element $\\mathbf{x}_{i}$ belonging to the multiset $\\overline{{\\mathsf{X}}}=\\{\\{{\\mathsf{X}}_{1},...,{\\mathsf{X}}_{n}\\}\\}$ as a polynomial of another variable $z$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{E n c}(\\mathsf{X}_{i})=\\sum_{j=1}^{d}\\mathsf{X}_{i j}\\cdot z^{j-1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we can perform a reduction to the scalar case by replacing each $\\mathbf{x}_{i}$ with $\\mathsf{E n c}(\\pmb{\\{x}_{i}\\})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i}(t,z):=t-\\mathsf{E n c}(\\mathsf{X}_{i})=t-\\sum_{j=1}^{d}\\mathsf{X}_{i j}\\cdot z^{j-1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "And define the generalized DeepSets polynomial: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\overline{{\\mathbf{X}}}}(t,z):=\\prod_{i=1}^{n}p_{i}(t,z)=\\sum_{k,l}e_{k\\ell}(\\mathbf{X})\\cdot t^{k}z^{\\ell}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $e_{k\\ell}({\\pmb X})$ is the coefficient of $t^{k}z^{\\ell}$ in $p_{\\overline{{\\mathbf{x}}}}(t,z)$ . Note $0\\leq k\\leq n$ while $0\\leq\\ell\\leq n(d-1)$ . ", "page_idx": 4}, {"type": "text", "text": "Opposed to the scalar case, it is not evident why the obtained coefficients $(e_{k\\ell}(\\mathbf{x}))_{k,\\ell}$ in the above construction form an ensemble of separators. We prove injectivity by utilizing ideas from ring theory, particularly the notions of unique factorization domains (UFDs) and Gauss\u2019s lemma in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "We can now repeat the steps in Section 4.1 to achieve the actual representation. We compute the coefficient matrix of each $p_{i}(t,z)$ and sequentially perform two-dimensional circular convolution. ", "page_idx": 4}, {"type": "text", "text": "This leads us to an analogous theorem for the $d$ -dimensional case: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. Vector multisets $\\overline{{\\mathsf{X}}}=\\{\\{{\\mathsf{X}}_{1},...,{\\mathsf{X}}_{n}\\}\\}$ can be represented by an invariant and separating map $f_{c o n v}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{c o n v}(\\mathbf{X})=\\underset{i=1}{\\overset{n}{\\oplus}}\\oplus(\\mathbf{X}_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m_{1}\\times m_{2}}$ is an affine map, $\\circledast$ is the $2D$ circular convolution operator and the number of separators is $n=m_{1}\\times m_{2}=(n+1)(n(d-1)+1)\\in\\mathcal{O}(n^{2}d$ ). ", "page_idx": 4}, {"type": "text", "text": "The full proof of Theorem 4.2 is given in Appendix A.3. ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r}{{\\underline{{u}}}\\quad{\\underline{{v}}}\\quad}&{}&{h}\\\\ {{\\underline{{\\overline{{\\eta}}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad=\\quad{\\left(\\begin{array}{l}{\\boxed{0\\lceil0\\rceil+\\lceil1\\rceil2\\rfloor+\\lfloor2\\rceil\\rceil}}\\\\ {\\boxed{0\\rceil+\\lfloor{\\frac{\\cdots-\\ddots}{\\dots}}\\rfloor}}\\\\ {\\boxed{{\\frac{\\lceil0\\rceil+\\rceil}{\\dots}}+\\lfloor{\\frac{\\rceil+\\lfloor1\\rceil0\\rfloor+\\lfloor2\\rfloor\\rfloor}{\\dots}}}}\\\\ {\\boxed{0\\lfloor2\\rfloor+\\lceil1\\rceil+\\lfloor2\\rfloor0\\rceil}}\\\\ {\\boxed{0\\rceil+\\rceil+\\lfloor1\\rfloor+\\lfloor2\\rfloor0\\rceil}}\\\\ {\\quad}&{}&{\\qquad{\\frac{\\partial^{2}h^{\\ell}}{\\partial u^{\\ell}\\partial v^{\\ell}}}=I}\\end{array}\\right)}}&{}&{{\\underline{{\\overline{{\\eta}}}}}\\right|{\\underline{{\\check{\\eta}}}}\\right|{\\underline{{\\eta}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad{\\underline{{\\omega}}}\\quad{\\underline{{w}}}\\quad{\\underline{{w}}}\\quad}\\\\ {{\\underline{{\\overline{{\\eta}}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad+\\quad\\sum_{i=1\\dots\\infty}^{\\infty}.}&{}\\\\ {{\\underline{{\\hat{\\eta}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad=\\quad}&{}\\\\ {{\\underline{{\\partial^{\\ell}\\partial v^{\\ell}\\partial v^{\\ell}}}}=I}&{}&{{\\underline{{\\hat{\\eta}}}}\\quad{\\underline{{\\check{\\eta}}}}\\quad\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Figure 2: Visualization of the higher order notion of neighbor mixing. We visualize the convolution result $h$ for 3-dimensional features, considering 2 neighbors $u,v$ (left) and 3 neighbors $u,v,w$ (right). We demonstrate for each $n$ -tuple matching a feature per node, the corresponding $n$ -th order derivative of exactly one entry of $h$ is 1. ", "page_idx": 5}, {"type": "text", "text": "4.3 How does circular convolution impact neighbor Mixing? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\pmb{u}_{1},...,\\pmb{u}_{n}\\in\\mathbb{R}^{m}$ be discrete signals representing the locally-transformed neighbors before being aggregated. For the sake of simplicity, we slightly override the notation in this section, and refer to the $j$ -th element of the $i$ -th signal as $\\pmb{u}_{i}^{j}$ with $j$ starting from 0. ", "page_idx": 5}, {"type": "text", "text": "The core factor causing the neighbor mixing bottleneck of sum-based aggregators $\\textstyle h=\\sum_{i=1}^{n}u_{i}$ lies within the fact that no mixing is done in the representation, but only in the MLP compressor that comes afterward: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial{\\pmb u}_{i}^{k}\\partial{\\pmb u}_{j}^{\\ell}}{\\pmb h}=0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "On the contrary, each element of sequential circular convolution $h=\\pmb{u}_{1}\\circledast\\ldots\\circledast\\pmb{u}_{n}$ is composed of sums of terms of the form ${\\pmb u}_{1}^{j_{1}}{\\pmb u}_{2}^{j_{2}}\\cdot\\dots\\cdot{\\pmb u}_{n}^{j_{n}}$ . Particularly: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{h}^{k}=\\sum_{j_{1}+...+j_{n}\\equiv k}\\pmb{u}_{1}^{j_{1}}\\pmb{u}_{2}^{j_{2}}\\cdot...\\cdot\\pmb{u}_{n}^{j_{n}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This implies that the convolutional representation achieves, in fact, a generalized, higher-order notion of the mix values: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall0\\leq j_{1},...,j_{n}\\leq m-1\\ \\exists k:\\quad{\\frac{\\partial^{n}}{\\partial{\\pmb u}_{1}^{j_{1}}\\partial{\\pmb u}_{2}^{j_{2}}...\\partial{\\pmb u}_{n}^{j_{n}}}}{\\pmb h}^{k}=1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This notion of higher-order neighbor mixing is visualized in Figure 2. We refer the reader to Appendix A.4 for further theoretical discussions on the stability of permutation-invariant representations. ", "page_idx": 5}, {"type": "text", "text": "4.4 Practical considerations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Combining Theorem 4.2 with an MLP compressor yields the \"vanilla\" version of SSMA: it first applies the local affine map, then computes 2D circular convolution across the neighbor axis and finally compresses the result back using MLP as a universal compressor. The circular convolution is implemented by applying FFT, performing product aggregation along the neighbor axis and then transforming the result back using IFFT. As \"scatter_mul\" is not implemented for complex numbers in standard libraries, we convert complex values to their polar representation in which multiplication is equivalent to multiplying the magnitudes and summing up the arguments. The \"vanilla\" version of SSMA is presented in Figure 3. ", "page_idx": 5}, {"type": "text", "text": "We now suggest a few practical adjustments to the \"vanilla\" version of SSMA: ", "page_idx": 5}, {"type": "text", "text": "Normalizing the circular convolution. As SSMA performs a product over the neighbors\u2019 axis, the optimization process of the vanilla SSMA might get unstable. To address this instability, we normalize the element-wise magnitudes of the product by taking their geometric means. ", "page_idx": 5}, {"type": "image", "img_path": "aRokfUfIQs/tmp/3ec337393c38b3840e7bca45ca916db6a41edf67a4139472f44e8b9f65c407d9.jpg", "img_caption": ["Figure 3: Visualization of the Sequential Signal Mixing Aggregation. Left: demonstration of the aggregation stage in an off-the-shelf MPGNN layer. The goal is to create a compressed view of $t$ \u2019s incoming neighbors. Right: our proposed aggregation. We convert the neighbor features into two-dimensional discrete signals. We then apply 2D circular convolution by applying 2D FFT, performing pointwise multiplication and transforming back using IFFT. Finally, we compress the result back into a $d\\!.$ -dimensional vector using a multi-layer perceptron as a universal compressor. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Low-rank compressor. Since the number of parameters in the MLP compressor rapidly increases with the representation dimension $m$ , we opted for a single linear layer as our compressor. To accommodate a higher number of neighbor slots and allow for a larger hidden dimension, we reduced the number of parameters in the linear layer by splitting it into two consecutive linear layers that squeezes the representation to low dimension and than expands it back. This effectively performs a low-rank factorization of the weight matrix of the original single linear layer. ", "page_idx": 6}, {"type": "text", "text": "Neighbor selection methods. The representation size of the vanilla SSMA $m=\\mathcal{O}(n^{2}d)$ may become prohibitively high in dense neighborhoods (e.g in transductive settings). To address this issue, we employ two neighbor selection techniques that reduce the original neighborhood to a new set of $\\kappa$ neighbors. The first technique simply draws at most $\\kappa$ random neighbors without replacement. The second technique draws inspiration from Graph Attention Networks (GAT) and attention slots [29, 42] and map the neighbors into $\\kappa$ attention slots. The attention coefficient for the edge $e:j\\rightarrow i$ for the $k$ -th slot is expressed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{k}(h_{i},h_{j})=\\mathrm{LeakyReLU}(a_{k}^{T}h_{i}+b_{k}^{T}h_{j})}\\\\ &{\\alpha_{i j}^{(k)}=\\mathrm{softmax}_{j}e_{k}(h_{i},h_{j})=\\frac{\\exp(e_{k}(h_{i},h_{j}))}{\\sum_{j^{\\prime}\\in\\mathcal{N}_{i n}(i)}\\exp(e_{k}(h_{i},h_{j^{\\prime}}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $\\mathbf{a}_{k},\\mathbf{b}_{k}\\in\\mathbb{R}^{d}$ are per-slot learnable weight vectors. Thereafter, the $k$ -th slot for the $i$ -th node is produced by considering the weighted average of the incoming neighbors: ", "page_idx": 6}, {"type": "equation", "text": "$$\ns_{i}^{(k)}=\\sum_{j\\in\\mathcal{N}_{i n}(i)}\\alpha_{i j}^{(k)}h_{j}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Synthetic task ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To empirically demonstrate the success of SSMA in managing tasks characterized by high neighbor mixing (opposed to sum-based aggregators), we introduce a synthetic regression task we name SUMOFGRAM. In this task, we sample random neighbor features and then generate the labels by considering the sum of the Gramian matrix corresponding to the neighbor features. ", "page_idx": 6}, {"type": "text", "text": "In some sense, the SUMOFGRAM task is the \"simplest\" task that involves neighbor mixing: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall i\\neq j:\\mathsf{m i x}_{i,j}=\\left\\|\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}\\mathsf{S U M O F G R A M}(x_{1},...,x_{n})\\right\\|_{2}=\\left\\|\\mathbb{I}_{d\\times d}\\right\\|_{2}=1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "aRokfUfIQs/tmp/6142967d0d3db26afa5892dbafde941046ff92982839b21eddd56fe6320a13ea.jpg", "img_caption": ["Number of trainable parameters "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: SUMOFGRAM train and test regression $L_{1}$ errors for different activation functions. The sum aggregator (not dashed) performs poorly and fails to scale with the capacity of the aggregation module, even when used in conjunction with analytic activations. On the contrary, SSMA (dashed) consistently achieves low regression errors and scales well with the number of learnable parameters. ", "page_idx": 7}, {"type": "text", "text": "We train both the sum aggregator and our proposed Sequential Signal Mixing Aggregation until convergence with varying representation sizes $m$ on the SUMOFGRAM task. ", "page_idx": 7}, {"type": "text", "text": "As may be observed in Figure 4, sum aggregators fail at this task, even when used in conjunction with analytic activations, which as claimed previously [1], is sufficient to achieve separation. This shows that sole separation is insufficient for performing arbitrary downstream tasks. On the contrary, SSMA has low regression errors, consistently along different activation functions. ", "page_idx": 7}, {"type": "text", "text": "5.2 Benchmarking SSMA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental Setup. We test the effectiveness of SSMA by incorporating it into popular MPGNN architectures. We evaluate both original and augmented architectures across a wide range of benchmarks. These benchmarks cover learning in both the transductive and inductive settings, node and graph-level prediction tasks, regression and classification problems, feature-oriented as well as purely topological data and tasks that involve challenging neighborhood configurations including dense neighborhoods and distant neighbor dependencies. As SSMA introduces learnable parameters, we ensure a fair comparison by maintaining an equal total parameter count to that of the original architectures in each experimental setting, adjusting the architecture\u2019s hidden dimension to adhere to the budget constraints. For a detailed discussion on the parameter budget in each experiment, please refer to Appendix C.4. Given the budget for each experiment, we conduct a hyperparameter search (HPS) on SSMA parameters to find the best configuration. We further perform ablation studies to closely examine the effect of each hyperparameter, as detailed in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Results. We observe substantial performance gains across all tested combinations of benchmarks and MPGNN architectures. Notably, the most significant relative improvements were observed on the IMDB-B benchmark, which lacks node and edge features. This phenomenon is likely attributed to SSMA\u2019s neighbor mixing capabilities, enabling it to learn joint topological relationships among neighbors. The Improvements observed on the LRGB [16] datasets indicate that SSMA better extracts relevant neighborhood information to be propagated to distant parts of the graph. Additionally, the experiments on the OGBN networks (OGBN-Arxiv and OGBN-Products) [21] confirm that SSMA is robust to dense neighborhoods and highlight the efficiency of its attentional neighbor selection mechanism. Another noteworthy observation is that SSMA utilizes the hidden dimension more effectively. Since we use the same parameter budget, experiments with SSMA employ a lower hidden dimensionality than those using \u2019sum\u2019 aggregation. This is because SSMA allocates learnable parameters, whereas \u2019sum\u2019 aggregation does not. Despite a smaller hidden dimension, SSMA outperforms \u2019sum\u2019 aggregation, indicating its efficiency in retaining relevant information for downstream tasks. Benchmarks for more common aggregation functions is in Appendix F ", "page_idx": 7}, {"type": "table", "img_path": "aRokfUfIQs/tmp/89f067fef97eee718e2a2f1fecc28cea4675ddd0d9c82cb6f8a2db4fd48a25ec.jpg", "table_caption": ["Table 1: Results for TU datasets [32] & ZINC [19] using sum aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "aRokfUfIQs/tmp/4e52d504160c9f6cca81820fe67f95749a5ee0ad92e0df6686e54ff6767fd8f8.jpg", "table_caption": ["Table 2: Test performance on the OGB [21] & LRGB [16] benchmarks using sum aggregation as a baseline. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion and discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we re-examined the field of aggregation functions in MPGNNs and introduced the Sequential Signal Mixing Aggregation, a new plug-and-play aggregation method with a solid theoretical foundation. We demonstrated its effectiveness across various datasets and message-passing architectures with different parameter budgets. Each component of our method was systematically examined and its contribution to performance is verified. We hope the observed performance gains will motivate further research into harnessing our aggregation for specific applications and developing more advanced aggregation functions for MPGNNs. Future research could address some limitations of our method, such as the representation size scaling quadratically with the number of neighbors and the need for explicit normalization due to the instability of the convolution operation. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] T. Amir, S. J. Gortler, I. Avni, R. Ravina, and N. Dym. Neural injective functions for multisets, measures and graphs via a finite witness theorem. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[2] J. Baek, M. Kang, and S. J. Hwang. Accurate learning of graph representations with graph multiset pooling. In International Conference on Learning Representations, 2021.   \n[3] B. Bevilacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein, and H. Maron. Equivariant subgraph aggregation networks. In International Conference on Learning Representations, 2022.   \n[4] J. Bradshaw, M. J. Kusner, B. Paige, M. H. S. Segler, and J. M. Hern\u00e1ndez-Lobato. A generative model for electron paths. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[5] X. Bresson and T. Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.   \n[6] S. Brody, U. Alon, and E. Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022.   \n[7] D. Buterez, J. P. Janet, S. J. Kiddle, D. Oglic, and P. Li\u00f2. Graph neural networks with adaptive readouts. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[8] J. Cahill, J. W. Iverson, and D. G. Mixon. Towards a bilipschitz invariant theory, 2024.   \n[9] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 257\u2013266, 2019.   \n[10] E. Cohen-Karlik, A. B. David, and A. Globerson. Regularizing towards permutation invariance in recurrent models. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.   \n[11] G. Corso, L. Cavalleri, D. Beaini, P. Li\u00f2, and P. Velic\u02c7kovic\u00b4. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260\u201313271, 2020.   \n[12] G. Corso, L. Cavalleri, D. Beaini, P. Li\u00f2, and P. Velic\u02c7kovic\u00b4. Principal neighbourhood aggregation for graph nets. In Advances in Neural Information Processing Systems, 2020.   \n[13] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral flitering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[14] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G\u00f3mez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. CoRR, abs/1509.09292, 2015.   \n[15] V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1\u201348, 2023.   \n[16] V. P. Dwivedi, L. Ramp\u00e1\u0161ek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326\u201322340, 2022.   \n[17] N. Dym and S. J. Gortler. Low-dimensional invariant embeddings for universal geometric learning. Foundations of Computational Mathematics, pages 1\u201341, 2024.   \n[18] W. Fan, Y. Ma, Q. Li, J. Wang, G. Cai, J. Tang, and D. Yin. A graph neural network framework for social recommendations. IEEE Trans. Knowl. Data Eng., 34(5):2033\u20132047, 2022.   \n[19] R. G\u00f3mez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern\u00e1ndez-Lobato, B. S\u00e1nchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[20] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 1025\u20131035, Red Hook, NY, USA, 2017. Curran Associates Inc.   \n[21] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[22] J. Irwin, T. Sterling, M. Mysinger, E. Bolstad, and R. Coleman. Zinc: A free tool to discover chemistry for biology. Journal of chemical information and modeling, 52, 05 2012.   \n[23] W. Jin, K. Yang, R. Barzilay, and T. S. Jaakkola. Learning multimodal graph-to-graph translation for molecule optimization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[24] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.   \n[25] R. Kortvelesy, S. Morad, and A. Prorok. Generalised f-mean aggregation for graph neural networks. Advances in Neural Information Processing Systems, 36:34439\u201334450, 2023.   \n[26] E. Kosman and D. D. Castro. Graphvid: It only takes a few nodes to understand a video. In S. Avidan, G. J. Brostow, M. Ciss\u00e9, G. M. Farinella, and T. Hassner, editors, Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, volume 13695 of Lecture Notes in Computer Science, pages 195\u2013212. Springer, 2022.   \n[27] G. Li, C. Xiong, A. Thabet, and B. Ghanem. Deepergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020.   \n[28] X. Liu, X. You, X. Zhang, J. Wu, and P. Lv. Tensor graph convolutional networks for text classification. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8409\u20138416. AAAI Press, 2020.   \n[29] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525\u201311538, 2020.   \n[30] T. L. Makinen, J. Alsing, and B. D. Wandelt. Fishnets: Information-optimal, scalable aggregation for sets and graphs, 2023.   \n[31] H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. Provably powerful graph networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 2153\u20132164, 2019.   \n[32] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020.   \n[33] M. Narasimhan, S. Lazebnik, and A. G. Schwing. Out of the box: Reasoning with graph convolution nets for factual visual question answering. CoRR, abs/1811.00538, 2018.   \n[34] E. Ong and P. Velic\u02c7kovic\u00b4. Learnable commutative monoids for graph neural networks. In The First Learning on Graphs Conference, 2022.   \n[35] O. Puny, D. Lim, B. T. Kiani, H. Maron, and Y. Lipman. Equivariant polynomials for graph neural networks. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 28191\u201328222. PMLR, 2023.   \n[36] L. Ramp\u00e1sek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful, scalable graph transformer. CoRR, abs/2205.12454, 2022.   \n[37] L. Ramp\u00e1\u0161ek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. Advances in Neural Information Processing Systems, 35, 2022.   \n[38] L. Schneckenreiter, R. Freinschlag, F. Sestak, J. Brandstetter, G. Klambauer, and A. Mayr. GNN-VPA: A variance-preserving aggregation strategy for graph neural networks. In The Second Tiny Papers Track at ICLR 2024, 2024.   \n[39] L. Schneckenreiter, R. Freinschlag, F. Sestak, J. Brandstetter, G. Klambauer, and A. Mayr. Gnn-vpa: A variance-preserving aggregation strategy for graph neural networks. arXiv preprint arXiv:2403.04747, 2024.   \n[40] S. A. Tailor, F. Opolka, P. Lio, and N. D. Lane. Adaptive filters for low-latency and memoryefficient graph neural networks. In International Conference on Learning Representations, 2022.   \n[41] S. A. Tailor, F. Opolka, P. Lio, and N. D. Lane. Do we need anistropic graph neural networks? In International Conference on Learning Representations, 2022.   \n[42] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[43] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. accepted as poster.   \n[44] X. Wang and M. Zhang. How powerful are spectral graph neural networks. ICML, 2022.   \n[45] Z. Wang, Q. Lv, X. Lan, and Y. Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013357, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.   \n[46] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, pages 6861\u20136871. PMLR, 2019.   \n[47] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[48] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.   \n[49] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph transformer networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11960\u201311970, 2019.   \n[50] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs and extended theory discussion ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. Let us consider a general sum-based aggregator: $\\begin{array}{r}{F(x_{1},...,x_{n})=\\rho(\\sum_{k=1}^{n}\\phi(x_{k}))}\\end{array}$ . Then, we have that for $i\\neq j$ and for the $\\ell_{}$ -th output: ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r l}&{\\left|\\overline{{\\partial_{x}\\partial_{y}^{(2)}}}=\\right.}\\\\ &{\\left|\\overline{{\\partial_{x}\\partial_{x}}}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{x}^{(2)}}}\\overline{{\\partial_{\\mathbf{i}}(x_{1})}}\\right|\\right|_{2}=}\\\\ &{\\left|\\overline{{\\partial_{x}}}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{x}^{(2)}}}\\overline{{\\partial_{x}^{(3)}}}\\overline{{\\partial_{y}}}^{3}\\overline{{\\partial_{x}(x_{1})}}\\right|\\right|_{2}=}\\\\ &{\\left|\\overline{{\\partial_{x}}}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{y}^{(2)}}}\\overline{{\\Delta_{\\mathbf{i}}(x_{1})}}\\right|\\cdot\\overline{{\\partial_{y}(x_{1})}}\\right|\\Bigg|_{2}=}\\\\ &{\\left|\\overline{{\\partial_{x}}}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{y}^{(3)}}}\\overline{{\\partial_{y}(x_{1})}}\\right|\\cdot\\overline{{\\partial_{y}(x_{1})}}\\Bigg|\\Bigg|_{2}=}\\\\ &{\\left|\\overline{{\\partial_{y}}}\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{y}(y_{2})}}\\overline{{\\partial_{\\mathbf{i}}(x_{1})}}\\right|\\cdot\\overline{{\\partial_{y}(x_{1})}}\\Bigg|\\Bigg|_{2}=}\\\\ &{\\left|\\overline{{\\partial_{\\xi}(x_{1})}}^{2}\\cdot\\overline{{\\partial_{y}}}^{2}\\overline{{\\partial_{x}(y_{2})}}\\left(\\sum_{k=1}^{D}\\overline{{\\partial_{z}(x_{1})}}\\right)\\cdot J_{k}(x_{1})\\Bigg|\\leq}\\\\ &{\\left|\\overline{{J_{y}(x_{1})}}\\right|_{2}\\cdot\\overline{{\\partial_{y}(y_{2})}}\\left(\\sum_{k=1}^{D}\\overline{{\\partial_{z}(x_{1})}}\\right)\\Bigg|\\mathrm{J_{y}(y_{2})}\\Bigg|,}\\end{array}$ 2 ", "page_idx": 12}, {"type": "text", "text": "Typically, the local operator $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ is of the form $\\phi(x)=\\sigma(A x+b)$ where $\\sigma$ is an activation function applied element-wise and $A\\in\\mathbb{R}^{m\\times d},b\\in\\mathbb{R}^{m}$ are learnable parameters. ", "page_idx": 12}, {"type": "text", "text": "Usually $|\\sigma^{\\prime}(z)|$ is bounded by some small constant $c_{1}$ . ", "page_idx": 12}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|J_{\\phi}(x)\\right\\|_{2}=\\left\\|{\\tt d i a g}(\\sigma^{\\prime}(A x+b))\\cdot A\\right\\|\\leq c_{1}\\cdot\\left\\|A\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover, the global pooling operator $\\rho:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{d}$ is an MLP of the form: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\rho(z)=W_{2}\\cdot\\sigma(W_{1}z+\\beta_{1})+\\beta_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $W_{1}\\in\\mathbb{R}^{m\\times m},\\beta_{1}\\in\\mathbb{R}^{m},W_{2}\\in\\mathbb{R}^{d\\times m},\\beta_{2}\\in\\mathbb{R}^{d}.$ ", "page_idx": 12}, {"type": "text", "text": "Therefore: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{d}{d z}\\rho^{(\\ell)}(z)=W_{2}[\\ell,:]^{T}\\cdot\\mathsf{d i a g}(\\sigma^{\\prime}(W_{1}z+\\beta_{1}))\\cdot W_{1}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Let us denote: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u^{(\\ell)}(z):=W_{2}[\\ell,:]^{T}\\cdot\\mathsf{d i a g}(\\sigma^{\\prime}(W_{1}z+\\beta_{1}))=(W_{2}[\\ell,k]\\cdot\\sigma^{\\prime}([W_{1}z+\\beta_{1}]_{k}))_{k=1}^{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{d^{2}}{d z^{2}}\\rho^{(\\ell)}(z)=\\frac{d}{d z}\\boldsymbol{u}^{(\\ell)}(z)\\cdot\\boldsymbol{W}_{1}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We compute $\\textstyle{\\frac{d}{d z}}u^{(\\ell)}(z)$ for each output dimension $1\\leq k\\leq m$ separately: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d z}\\left\\lbrace u^{(\\ell)}(z)[k]\\right\\rbrace=\\frac{d}{d z}\\left\\lbrace W_{2}[\\ell,k]\\cdot\\sigma^{\\prime}(W_{1}[k,:]z+\\beta_{1}[k])\\right\\rbrace=}\\\\ {\\displaystyle W_{2}[\\ell,k]\\cdot\\sigma^{\\prime\\prime}(W_{1}[k,:]z+\\beta_{1}[k])\\cdot W_{1}[k,:]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Summarizing this for all dimensions $k$ yields: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d z}u^{(\\ell)}(z)=\\mathsf{d i a g}_{k}(W_{2}[\\ell,k])\\cdot\\mathsf{d i a g}\\big(\\sigma^{\\prime\\prime}(W_{1}z+\\beta_{1})\\big)\\cdot W_{1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "So under the assumption $|\\sigma^{\\prime\\prime}(z)|\\leq c_{2}$ we get the following bound on the Hessian norm: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|H_{\\rho^{(\\ell)}}(z)\\right\\|_{2}\\leq c_{2}\\cdot\\|W_{2}[\\ell,:]\\|_{2}\\cdot\\|W_{1}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And the final bound on $\\operatorname{mix}_{i,j}^{(\\ell)}$ is given by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{m i x}_{i,j}^{(\\ell)}\\leq c_{2}\\cdot c_{1}^{2}\\cdot\\|W_{2}[\\ell,:]\\|_{2}\\cdot\\|W_{1}\\|_{2}^{2}\\cdot\\|A\\|_{2}^{2}\\in\\mathcal{O}(\\|\\theta\\|_{2}^{2})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Ring theory and the factorization lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A ring is an algebraic structure that generalizes the notion of a field. In particular, univariate and multivariate polynomials obey this structure. Formally, a ring $R$ is a set associated with two binary operations $^+$ (addition) and $\\cdot$ (multiplication) satisfying the ring axioms: ", "page_idx": 14}, {"type": "text", "text": "1. $R$ is an abelian group under the addition operation.   \n2. $R$ is a monoid under the multiplication operation: (a) associativity: $(a\\cdot b)\\cdot c=a\\cdot(b\\cdot c)$ for all $a,b,c\\in R$ . (b) existence of identity: there is an element $1\\in R$ such that: $1\\cdot a=a\\cdot1=a$ .   \n3. Distributivety: $a\\cdot(b+c)=a\\cdot b+a\\cdot c$ and $(b+c)\\cdot a=b\\cdot a+c\\cdot a.$ ", "page_idx": 14}, {"type": "text", "text": "A ring is said to be commutative if its elements commute under multiplication: $a\\cdot b=b\\cdot a$ for all $a,b\\in R$ . ", "page_idx": 14}, {"type": "text", "text": "Given a commutative ring $R$ , we can define its corresponding univariate polynomial ring denoted as $R[X]$ by considering a set of formal expressions $\\sum_{i=0}^{2}\\alpha_{i}\\bar{X}^{i}$ where $n$ is a non-negative integer and $\\alpha_{i}\\in R$ . We consider $X$ a formal variable and define addition and multiplication according to the ordinary rules for manipulating algebraic expressions. Each polynomial $p\\in R[X]$ has a degree defined as $\\operatorname*{max}_{i}\\alpha_{i}\\neq0$ . For each $r\\in R$ , we can define an evaluation map $T_{r}$ which takes some polynomial as an input and returns an element in the underlying ring by substituting $X=r$ , namely: $\\begin{array}{r}{\\dot{T}_{r}(\\sum_{i=0}^{n}\\alpha_{i}X^{i})=\\dot{\\sum}_{i=0}^{n}\\,\\alpha_{i}r^{i}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Multivariate polynomials can be defined similarly by considering multiple formal variables. Alternatively, note that since the polynomial ring of some commutative ring $R$ is also a commutative ring by itself, we can equivalently define multivariate polynomials by considering the ring of polynomials above $R[X]$ , namely: $(R[X])[Y]\\cong R[X,Y]$ . ", "page_idx": 14}, {"type": "text", "text": "An integral domain is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. In particular, any field is an integral domain, and any polynomial ring is an integral domain, given that its underlying ring is itself an integral ring. Am immediate conclusion is that $\\bar{\\mathbb{R}}[x]$ and $\\mathbb{R}[x_{1},...,x_{n}]$ are integral domains. ", "page_idx": 14}, {"type": "text", "text": "An irreducible element of an integral domain is a non-zero element that is not invertible and is not the product of two non-invertible elements. For instance, for every commutative ring $R$ , every polynomial of the form $x-r$ where $r\\in R$ is an irreducible element of $R[x]$ . ", "page_idx": 14}, {"type": "text", "text": "A unique factorization domain (UFD) is an integral domain $R$ in which every non-zero element $r$ of $R$ can be written as a product (an empty product if $x$ is invertible) of irreducible elements $p_{i}$ of $R$ and an invertible element $u$ : $\\begin{array}{r}{r=u\\cdot\\bar{\\Pi}_{i=1}^{\\bar{n}}\\,p_{i}}\\end{array}$ . This representation ought to be unique up to multiplication with invertible elements. The key fact underlying our construction is the fact that a polynomial ring of a UFD is by itself a UFD (known as Gauss\u2019s lemma). In conjunction with the fact that any field $\\mathbb{F}$ is a UFD, we get that $\\mathbb{R}[x_{1},...,x_{n}]$ is a UFD for any amount of variables. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Let $p(t,z)\\in\\mathbb{R}[t,z]$ be some polynomial that can be factorized as: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(t,z)=\\prod_{i=1}^{n}(t-u_{i}(z))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $u_{i}(z)$ is some polynomial of $z$ . ", "page_idx": 14}, {"type": "text", "text": "Then, such factorization is unique to the order of the terms $(t-u_{i}(z))$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Gauss\u2019s lemma implies that $\\mathbb{R}[t,z]\\cong(\\mathbb{R}[z])[t]$ is a unique factorization domain. Since every polynomial of the form $t-r(z)$ is an irreducible element in $(\\mathbb{R}[z])[t]$ , it follows that a factorization of the form $\\begin{array}{r}{p(t,z)=\\prod_{i}(t-\\dot{q_{i}}(z))}\\end{array}$ is unique up to permutation of the terms. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Corollary A.2. The coefficients of the polynomial $p_{\\mathbf{\\overline{{x}}}}(t,z)$ defined in Section 4.2 form an ensemble of separating invariants. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Let $p_{\\overline{{\\mathbf{x}}}}(t,z)$ be the polynomial in the construction. Corollary A.2 implies that its coefficients form an ensemble of separating invariants. Consequently, we repeat the steps in Section 4.1 to get an analogous result to Theorem 4.1, arriving at the desired form. ", "page_idx": 15}, {"type": "text", "text": "We represent $p_{\\mathbf{\\overline{{x}}}}(t,z)$ by evaluating its value on a grid of points $(u_{s},v_{b})$ where $0\\leq s\\leq n,0\\leq b\\leq\\tau$ . Specifically, we can choose $u_{s}:=e^{-\\frac{2\\pi i}{n+1}s}$ and $v_{b}:=e^{-\\frac{2\\pi i}{\\tau+1}b}$ and to get the two-dimensional DFT of the polynomial coefficients: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\overline{{\\mathbf{X}}}}(u_{s},v_{b})=\\sum_{k=0}^{n}\\sum_{\\ell=0}^{\\tau}e_{k\\ell}(\\mathbf{X})\\cdot(u_{s})^{k}(v_{b})^{\\ell}=\\sum_{k,\\ell}e_{k\\ell}(\\mathbf{X})\\cdot e^{-\\frac{2\\pi i s}{n+1}k}\\cdot e^{-\\frac{2\\pi i b}{\\tau+1}\\ell}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Again, by setting $\\Phi(\\mathbf{X}_{i})$ to be the coefficients matrix of $p_{i}(t,z)=t-q\\mathsf{x}_{i}(z)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi({\\mathsf X}_{i})=\\left[\\begin{array}{c c c c c c}{-{\\mathsf X}_{i1}}&{-{\\mathsf X}_{i2}}&{\\cdot\\cdot\\cdot}&{-{\\mathsf X}_{i d}}&{\\cdot\\cdot\\cdot}&{0}\\\\ {1}&{0}&{0}&{0}&{\\cdot\\cdot\\cdot}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{0}\\\\ {0}&{0}&{0}&{0}&{\\cdot\\cdot\\cdot}&{0}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we get that: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{k\\ell}(\\mathbf{X})={\\mathcal{F}}_{2D}^{-1}\\left\\{\\bigoplus_{i=1}^{n}{\\mathcal{F}}_{2D}\\left\\{\\Phi(\\mathbf{X}_{i})\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where the multiplication is elementwise. According to the 2D circular convolution theorem, this exactly amounts to convolving the vector coefficients of $p_{i}(t,z)\\mathbf{s}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{e}_{k\\ell}(\\mathbf{X})=\\bigoplus_{i=1}^{n}\\Phi(\\mathbf{X}_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.4 On the stability of permutation-invariant representations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Ideally, we would like our representation to be numerically stable. That is, to require that if two distinct multisets are close to each other, then their representations should also be close to each other, and vice versa. This was formalized previously [1] using the notions of bi-Lipschitzness and Wasserstein distance as we now recapitulate. ", "page_idx": 16}, {"type": "text", "text": "Let $\\overline{{\\mathbf{x}}},\\overline{{\\mathbf{Y}}}$ be two multisets of $d$ dimensional vectors with $|{\\overline{{\\mathbf{X}}}}|=|{\\overline{{\\mathbf{Y}}}}|=n$ ", "page_idx": 16}, {"type": "text", "text": "Wasserstein distance. we measure the distance between equally sized multisets $\\subseteq\\mathbb{R}^{d}$ using the notion of Wasserstein distance: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{W}_{p}(\\overline{{\\mathbf{X}}},\\overline{{\\mathbf{Y}}}):=\\operatorname*{min}_{\\pi\\in S_{n}}\\big(\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|\\mathbf{X}_{i}-\\mathbf{Y}_{\\pi(i)}\\big\\|^{p}\\big)^{1/p}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $\\lVert.\\rVert$ is the $L_{1}$ norm over $\\mathbb{R}^{d}$ . ", "page_idx": 16}, {"type": "text", "text": "We are interested in the bi-Lipschitzness property - whether there exist constants $c,C>0$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nc\\cdot\\mathcal{W}_{p}(\\overline{{\\mathsf{X}}},\\overline{{\\mathsf{Y}}})\\leq\\left\\|\\hat{f}(\\pmb{\\mathsf{X}})-\\hat{f}(\\pmb{\\mathsf{Y}})\\right\\|\\leq C\\cdot\\mathcal{W}_{p}(\\overline{{\\mathsf{X}}},\\overline{{\\mathsf{Y}}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Unfortunately, it turns out that this notion of stability is unattainable by any differentiable permutationinvariant representation. We prove this by generalizing such results for sum-based aggregators [1]. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.3. Let $\\hat{f}$ be some differentiable multiset representation. Then, there exist $n,d$ such that for every $\\epsilon>0$ there exist $\\overline{{\\mathbf{X}}}_{\\epsilon},\\widetilde{\\mathbf{Y}}_{\\epsilon}\\subseteq\\mathbb{R}^{d}$ two multisets of size n such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{f}(\\pmb{\\mathscr{X}}_{\\epsilon})-\\hat{f}(\\pmb{\\mathscr{Y}}_{\\epsilon})\\right\\|\\leq\\epsilon\\cdot\\mathscr{W}_{p}(\\overline{{\\pmb{\\mathscr{X}}_{\\epsilon}}},\\overline{{\\pmb{\\mathscr{Y}}_{\\epsilon}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "An independent proof for general invariant embeddings is given in [8]. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\hat{f}$ be some permutation-invariant representation. We use the same construction as in [1] and generalize the proof to arbitrary symmetric and differential representations. ", "page_idx": 16}, {"type": "text", "text": "We choose $n=2$ and some arbitrary $d\\in\\mathbb{N}$ . ", "page_idx": 16}, {"type": "text", "text": "Let $\\mathbf{x_{0}},\\mathbf{d}\\in\\mathbb{R}^{d}$ where $\\mathbf{d}$ has a unit norm, and consider $S_{h}=\\{\\{{\\bf x_{0}}+h{\\bf d},{\\bf x_{0}}-h{\\bf d}\\}\\}$ ", "page_idx": 16}, {"type": "text", "text": "We note that the Wasserstein distance $\\mathscr{W}_{p}(S_{h},S_{0})$ is $h$ . Thus, it is sufficient to show that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}{\\frac{\\left\\|{\\hat{f}}(S_{h})-{\\hat{f}}(S_{0})\\right\\|}{h}}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\hat{f}_{k}:\\mathbb{R}^{2d}\\rightarrow\\mathbb{R}$ is invariant we have that for every $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{d}\\colon\\hat{f}_{k}(\\mathbf{u},\\mathbf{v})=\\hat{f}_{k}(\\mathbf{v},\\mathbf{u})$ . ", "page_idx": 16}, {"type": "text", "text": "The differentiability of $\\hat{f}_{k}$ at $(\\mathbf{u},\\mathbf{v})$ implies that for every $1\\leq i\\leq d$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{i}\\hat{f}_{k}(\\mathbf{u},\\mathbf{v})=\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{\\hat{f}_{k}(u_{1},...,u_{i}+h,...,u_{d},\\mathbf{v})-\\hat{f}_{k}(\\mathbf{u},\\mathbf{v})}{h}=}\\\\ {\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{\\hat{f}_{k}(\\mathbf{v},u_{1},...,u_{i}+h,...,u_{d})-\\hat{f}_{k}(\\mathbf{v},\\mathbf{u})}{h}=\\partial_{d+i}\\hat{f}_{k}(\\mathbf{v},\\mathbf{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Particularly, this implies that $\\partial_{i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})=\\partial_{d+i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})$ . ", "page_idx": 16}, {"type": "text", "text": "Using the differentiability of $\\hat{f}_{k}$ at $(\\mathbf{x_{0}},\\mathbf{x_{0}})$ we can write: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{f}_{k}(\\mathbf{x_{0}}+\\delta_{\\mathbf{1}},\\mathbf{x_{0}}+\\delta_{2})=}\\\\ {\\displaystyle\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})+\\sum_{i=1}^{d}\\partial_{i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})\\cdot\\delta_{1i}+\\sum_{i=1}^{d}\\partial_{d+i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})\\cdot\\delta_{2i}+o_{k}(\\|(\\delta_{\\mathbf{1}},\\delta_{\\mathbf{2}})\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging in $\\delta_{\\bf1}=h{\\bf d}$ and $\\delta_{2}=-h\\mathbf{d}$ we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{f}_{k}(\\mathbf{x_{0}}+h\\mathbf{d},\\mathbf{x_{0}}-h\\mathbf{d})-\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})=}\\\\ {\\displaystyle h\\sum_{i=1}^{d}\\partial_{i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})\\cdot d_{i}-h\\sum_{i=1}^{d}\\partial_{d+i}\\hat{f}_{k}(\\mathbf{x_{0}},\\mathbf{x_{0}})\\cdot d_{i}+o_{k}(h)=o_{k}(h)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "All in all, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{\\left\\|\\hat{f}(S_{h})-\\hat{f}(S_{0})\\right\\|}{h}=\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{\\sum_{k=1}^{m}|\\hat{f}_{k}(\\mathbf x_{0}+h\\mathbf d,\\mathbf x_{0}-h\\mathbf d)-\\hat{f}_{k}(\\mathbf x_{0},\\mathbf x_{0})|}{h}=}\\\\ {\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{\\sum_{k=1}^{m}|o_{k}(h)|}{h}=\\sum_{k=1}^{m}\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{|o_{k}(h)|}{h}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Meaning that for every $\\epsilon>0$ there exists sufficiently small $h$ such that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{f}(S_{h})-\\hat{f}(S_{0})\\right\\|\\le\\epsilon\\cdot h=\\epsilon\\cdot\\mathcal{W}_{p}(S_{h},S_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Complexity analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Theoretical analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide a theoretical complexity analysis of SSMA for both its \"vanilla\" and refined versions. We consider the total cost of the aggregation stage in a single MPGNN layer within a graph $G=(V,E)$ . We let $d$ be the hidden dimension and $m=m_{1}\\cdot m_{2}$ be the total representation size. We let $\\kappa$ be the number of slots in the refined version of SSMA. We first analyze the vanilla version of SSMA step-by-step: ", "page_idx": 18}, {"type": "text", "text": "1. Local affine layer: locally transforms each node by an affine transformation $O(|V|\\cdot m\\cdot d)$ .   \n2. Local FFT: the FFT is computed per node yielding a cost of ${\\mathcal{O}}(|V|\\cdot m\\log(m))$ .   \n3. Product aggregation: The complex variant of scatter_mul aggregation - $\\mathcal{O}(|E|\\cdot m)$ .   \n4. Local IFFT: Same as stage 2.   \n5. MLP compressor: We used linear layer as a compressor to the original dimension -   \n$O(|V|\\cdot m\\cdot d)$ . ", "page_idx": 18}, {"type": "text", "text": "All in all we get: $\\mathcal{O}(m(d+\\log(m))\\vert V\\vert+m\\vert E\\vert)$ compared to the standard $\\mathcal{O}(m d|V|+m|E|)$ which is a negligible slowdown. ", "page_idx": 18}, {"type": "text", "text": "In the refined version of SSMA, we consider the modifications we applied: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Neighbor selection: If the random neighbor selection is used, then the cost is simply $\\mathcal{O}(m\\cdot\\kappa\\cdot|V|+|E|)$ for this stage. Alternatively, if the soft-attentional neighbor selection is picked, the attention weights in Equation (16) may be computed per edge and attention slot in a total running time of $\\mathcal{O}(m\\cdot\\bar{\\kappa}\\cdot|V|+\\kappa\\cdot|E|)$ and the contents of the slots may be implemented using sum aggregation in $O(m\\cdot\\kappa\\cdot|E|)$ . We get a total of $\\mathcal{O}(m\\cdot\\kappa\\cdot|V|+\\dot{|E|})$ for the random neighbor selection or $\\mathcal{O}(m\\cdot\\kappa\\cdot(|V|+|E|))$ for the soft-attentional neighbor selection for this stage.   \n\u2022 Other modifications: the normalization method does not affect the complexity of the aggregation. While the complexity of the MLP compressor is slightly reduced, the asymptotic complexity remains the same due to the local affine layer bottleneck.   \n\u2022 Application of SSMA on the new neighborhood: We consider the complexity obtained in the analysis of the vanilla SSMA and replace the number of edges, $|E|$ , with $\\kappa\\cdot|V|$ . The complexity of this stage then becomes: $\\mathcal{O}(m(d+\\log(m))|\\bar{V}|+\\dot{m}\\cdot\\kappa\\cdot|V|)\\stackrel{*}{=}$ $\\mathcal{O}(m(d+\\log(\\dot{m})+\\kappa)\\vert V\\vert)$ . ", "page_idx": 18}, {"type": "text", "text": "All in all, we get $\\mathcal{O}(m(d+\\log(m)+\\kappa)\\vert V\\vert+\\vert E\\vert)$ for the random neighbor selection refined SSMA or $\\mathcal{O}(m(d+\\log(m)+\\kappa)\\vert V\\vert+\\dot{m}\\kappa\\vert E\\vert)$ for the soft neighbor selection refined version of SSMA. ", "page_idx": 18}, {"type": "text", "text": "B.2 Runtime measurement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further show that SSMA is scalable we computed the runtime of it compared to other common networks, we measured the runtime of the layer with our aggregation with and without attention, for each configuration we show the downstream task results. As can be seen in Table 3 SSMA runtime is comparable to other methods while achieving higher downstream task performance. ", "page_idx": 18}, {"type": "table", "img_path": "aRokfUfIQs/tmp/45223e0276d6ffc0706a16bc09883f4b5c519b6ddc61c5de5415b4e003663352.jpg", "table_caption": ["Table 3: Comparison of the training and inference times in (ms) of MPGNNs+SSMA against PNA and GraphGPS. rSSMA indicates random neighbor selection while aSSMA indicates attentional neighbor selection. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C Extended experimental setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "TU Datasets [32]. We conducted experiments on five widely used datasets, including four bioinformatics datasets (ENZYMES, PTC-MR, MUTAG, and PROTEINS) and one movie collaboration dataset that relies solely on topological data (IMDB-Binary). While these datasets are all relatively small, as detailed in Table 4, they span a diverse range of domains. ", "page_idx": 19}, {"type": "text", "text": "The ENZYMES dataset consists of 600 enzyme graphs, where the task is to classify each enzyme into one of six EC top-level classes. Nodes represent secondary structure elements (SSEs) and are annotated by type. An edge connects two nodes if they are neighbors along the amino acid sequence or among the three nearest spatial neighbors, with edge features indicating their type (structural or sequential). ", "page_idx": 19}, {"type": "text", "text": "The PTC-MR dataset contains 344 compounds labeled according to their carcinogenicity in male rats. The MUTAG dataset consists of 188 chemical compounds, divided into two classes based on their mutagenic effect on bacteria. The PROTEINS dataset includes 1,113 protein molecules, with the task being a multiclass protein function prediction. For all of the above datasets, nodes represent atoms, while edges mean that two atoms are connected. ", "page_idx": 19}, {"type": "text", "text": "As these datasets lack official train/validation/test splits, we employ random 10-fold cross-validation for evaluation. The prediction task for all these datasets is multiclass classification, and we use accuracy as the performance metric. ", "page_idx": 19}, {"type": "text", "text": "The IMDB-Binary dataset is a movie collaboration dataset comprising 1,000 graphs. In each graph, nodes represent actors/actresses, with edges indicating co-appearance in movies. Collaboration graphs for the Action and Romance genres were generated as ego networks for each actor/actress, and each ego network was labeled with the corresponding genre. The task is to identify the genre of an ego-network graph. ", "page_idx": 19}, {"type": "text", "text": "ZINC dataset [19, 22] . We benchmark on the ZINC dataset. Specifically, the subset of 12K graphs as defined in [15], from the full 250K ZINC dataset. The prediction task is to regress a molecular property known as constrained solubility, which is calculated as \"logP - SA - cycle\" (octanol-water partition coefficients, logP, penalized by the synthetic accessibility score, SA, and the number of long cycles, cycle). In this dataset, node features represent the types of heavy atoms, and edge features represent the bonds between them. The ZINC dataset is widely used for research in molecular graph generation. We used the dataset versions available via PYG without any further preprocessing. ", "page_idx": 19}, {"type": "text", "text": "OGB - Graph property prediction [21]. We conduct experiments on the commonly used ogbgmolhiv and ogbg-molpcba molecule datasets to assess SSMA on graph-level prediction tasks. In these datasets, each graph represents a molecule, with nodes corresponding to atoms and edges to chemical bonds. The node features are 9-dimensional, encompassing atomic number, chirality, and other atomic attributes such as formal charge and ring membership. We refer the reader to code for further details. The prediction task involves accurately predicting molecular properties, represented as binary labels, such as whether a molecule inhibits HIV virus replication. The ogbg-molpcba dataset includes multiple tasks, some of which may contain \u2019nan\u2019 values indicating that the corresponding label is not assigned to the molecule. These datasets differ in size: ogbg-molhiv is smaller, while ogbg-molpcba is medium-sized. The evaluation metrics used are ROC-AUC for ogbg-molhiv and Average Precision (AP) for ogbg-molpcba. We used the official train/validation/test splits provided by the OGB team. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "OGB - Node property prediction [21]. We benchmark on two dense node-level citation graph datasets from OGB-N: ogbn-products and ogbn-arxiv. The login-products dataset is an undirected and unweighted graph representing an Amazon product co-purchasing network. Nodes represent products sold on Amazon, and edges indicate co-purchased products. Node features are generated by extracting bag-of-words features from product descriptions, followed by Principal Component Analysis for dimensionality reduction. The prediction task is multiclass classification to predict a product\u2019s category. ", "page_idx": 20}, {"type": "text", "text": "The ogbn-arxiv dataset is a smaller directed graph representing the citation network among Computer Science papers indexed by the Microsoft Academic Graph (MAG). Each node represents an arXiv paper, and the directed edges indicate citations between papers. Node embeddings are created by averaging the embeddings of words in the paper titles and abstracts, computed using the skip-gram model over the MAG corpus. The task is to predict the 40 subject areas of the arXiv CS papers. ", "page_idx": 20}, {"type": "text", "text": "We used the graph obtained from the OGB python package without any preprocessing. ", "page_idx": 20}, {"type": "text", "text": "Long Range Graph Benchmark (LRGB) [16]. We benchmark on two graph-level datasets from the Long-Range Graph Benchmark (LRGB): peptides-func and peptides-struct. Each graph in these datasets represents a peptide, a short chain of amino acids shorter than proteins and abundant in nature. Each amino acid is composed of many heavy atoms, making the molecular graph of a peptide much larger than that of a small drug-like molecule. Peptide graphs have a low average node degree. Still, they have significantly larger diameters compared to other drug-like molecules, making them ideal for studying long-range dependencies in Graph Neural Networks (GNNs). Both datasets use the same set of graphs but differ in their prediction tasks. Peptides-func is a multi-label graph classification dataset based on peptide function, while Peptides-struct is a multi-label graph regression dataset based on the 3D structure of peptides. More details can be found in the LRGB GitHub repository. We used the versions of the datasets available via PYG without any further preprocessing. ", "page_idx": 20}, {"type": "text", "text": "Dataset statistics. The statistics of the datasets we used in our experiments are shown in Table 4. ", "page_idx": 20}, {"type": "table", "img_path": "aRokfUfIQs/tmp/bb11dcab1ecc779bb05f62e23e1a04d65d0ead1114d5ef4d501d4687c7f28ed8.jpg", "table_caption": ["Table 4: The statistics of the datasets used in our experiments "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Preprocessing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We did not modify the features of the graph topology in any of the experiments except in the following cases: ", "page_idx": 21}, {"type": "text", "text": "\u2022 For purely topological datasets lacking node features, we assigned the zero vector as the initial node feature. \u2022 For the large graphs \"OGBN-arxiv\" and \"OGBN-Products,\" we undirected the graphs and then clustered them following [9]. We did it so we will be able to fit them into memory. \u2022 In the ESAN experiments [3], we followed the preprocessing steps outlined by the authors, as these were integral parts of their suggested method. Specifically, we used the configuration that yielded the best empirical results reported in the paper, \"DSS-GNN (GIN) $(\\mathrm{EGO+})$ .\" ", "page_idx": 21}, {"type": "text", "text": "C.3 Tailoring GNN architectures to different benchmarks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In all our experiments, we used consistent architectures with minor variations tailored to each dataset. The primary difference is in the initial layer, which adapts to the specific characteristics of the dataset\u2019s node and edge features. This initial layer can be either an embedding layer or a linear layer designed to project the input into the network\u2019s hidden space. Following the initial layer, the architecture consists of stacked message-passing layers with residual connections and batch normalization layers. For graph-prediction benchmarks, a readout function is applied. Finally, a two-layer multilayer perceptron (MLP) with ReLU activation produces the output. ", "page_idx": 21}, {"type": "text", "text": "C.3.1 GraphGPS configuration ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Due to the flexibility of GraphGPS and its numerous configuration options, we selected and focused on a specific setup. We utilized the Residual Gated Graph ConvNet from [5] as our convolution operator.For the attention module we used, dropout rate of 0.5 and 4 attention heads. For the experiments that uses Positional Encoding we used random walk with length 20. This configuration was taken from graph-gps repository. The full initialization details are available in our code. ", "page_idx": 21}, {"type": "text", "text": "C.4 Parameter budget ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We made our best effort to find the most common parameter budget for each benchmark. Specifically: ", "page_idx": 21}, {"type": "text", "text": "\u2022 For the ZINC [19] dataset, we employed a $100\\mathrm{k}$ parameter budget in order to obtain a fair comparison to previous works [12, 38, 3, 15].   \n\u2022 For the TUDatasets (MUTAG, ENZYMES, PROTEINS, IMDB-BINARY), where there is no consensus on the parameter budget, we used a 500,000 parameter budget. An exception was made for ESAN, for which we used a 100,000-parameter budget to allow fair comparison.   \n\u2022 For the OGB-N datasets, we used a 500,000 parameter budget. This is reasonable when considering the parameter counts of models on the ogbn-arxiv leaderboard and ogbn-products leaderboard.   \n\u2022 For OGB-G datasets, we used 500,000 parameters for ogbg-molhiv and 2,000,000 for ogbg-molpcba since its prediction task is more complex and contains 128 label prediction tasks. These are reasonable numbers as can be seen from ogbg-molhiv leaderboard & ogbg-molpcba leaderboard   \n\u2022 For the LRGB datasets, we also used a 2,000,000 parameter budget as those datasets require deeper GNNs because they need to embed long-range dependencies in the graph. ", "page_idx": 21}, {"type": "text", "text": "A summary of the parameter budget can be seen in Table 5 ", "page_idx": 21}, {"type": "text", "text": "Table 5: Parameter budget used for each dataset. \\*for the ESAN, we used a 300,000 parameter budget.   \n\\*\\*for the ESAN we used a 100,000 parameter budget. ", "page_idx": 22}, {"type": "table", "img_path": "aRokfUfIQs/tmp/b2cf3e9bc113ff751749ca556e58c0ffef13b65a806cb30cc95d8db6118523f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.5 Implementation details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We conduct our experiments using PyTorch Geometric as the underlying framework, running them on NVIDIA RTX A5000 GPUs. Detailed information on the selected hyperparameters for each dataset and layer configuration, along with instructions for reproducibility, can be found in our GitHub repository (https://almogdavid.github.io/SSMA/ ). For ESAN [3] and VPA [38], we integrated our SSMA directly into the official implementation shared by the authors (ESAN, VPA). We performed hyperparameter search only within the space the authors used, without altering the training or evaluation protocols. ", "page_idx": 22}, {"type": "text", "text": "C.6 Hyper parameter search ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use the Weights & Biases platform to perform hyperparameter searches (HPS), aiming to identify the optimal configuration for each dataset. For each configuration, we determine the maximum hidden dimension that fits within the predetermined parameter budget. ", "page_idx": 22}, {"type": "text", "text": "\u2022 MLP compression strength: We search for the optimal compression rate of the low-rank MLP compressor rate. We define the compression rate $\\gamma$ as the ratio between the bottleneck rank and the inner aggregation representation dimension $m$ . We perform a simple range search over [0.1, 0.25, 0.5, 0.75, 1.0].   \n\u2022 Neighbor selection method: We search for the best neighbor selection method. We perform a simple range search over [random, attention_slots].   \n\u2022 Effective neighborhood size: we search for the optimal neighborhood size $\\kappa$ . We perform a simple range search over [2,3,...,CLIP(max_neighbors,7)]. ", "page_idx": 22}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Comparison to variance preserving aggregation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We further evaluate our approach against a recently proposed method [39], we substitute the suggested aggregation technique with our own while retaining the original architecture and training protocol outlined in the work. We refer the reader to the original paper for an overview of architecture and training procedures. As illustrated in Table 6, our method demonstrates notable superiority over existing method without additional adjustments. Furthermore, optimizing hyperparameters and architecture selection has the potential for further enhancement. ", "page_idx": 23}, {"type": "text", "text": "Table 6: Test accuracy (higher is better). Shown is the mean $\\pm$ STD of 10-fold cross-validation runs, VPA results are taken directly from [39], SSMA results are generated by us using the code provided in [39] without any architecture or training protocol modifications ", "page_idx": 23}, {"type": "table", "img_path": "aRokfUfIQs/tmp/350f867cb4c7072408c176ef005209f1683311fadde59d2ac669b691423219c6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 Comparison to Generalised f-Mean Aggregation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conducted a further evaluation of our approach against a recently proposed aggregation function that parameterizes a function space encompassing all standard aggregators [25]. This aggregation was incorporated into our framework, and the experiments were performed using the identical setup described in Appendix C. For the new aggregation method, we employed the configuration specified by the authors in their experiments, as detailed in their repository). As shown in Appendix D.2, SSMA outperforms the proposed method. This demonstrates that even a method capable of learning a variety of aggregation functions experiences a relative decline in performance if it cannot effectively mix node features like SSMA. This underscores the critical importance of feature mixing in SSMA. ", "page_idx": 23}, {"type": "text", "text": "Table 7: Results for TU datasets [32] & ZINC [19] using the aggregation from [25] aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while  indicates the reported results from the relevant paper. ", "page_idx": 23}, {"type": "table", "img_path": "aRokfUfIQs/tmp/e5893d6c2dfca0f5452fc2882adb987ab9bb7b1f2e75e77f828613831301fe4e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.3 Comparison to GraphGPS with Positionl-Encoding ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Given the significant improvement in GraphGPS performance with positional encoding, we conducted additional experiments involving GraphGPS with positional encoding. The experiment details are provided in Appendix C. As shown in Appendix D.3, SSMA enhances GraphGPS performance even with positional encoding. ", "page_idx": 23}, {"type": "table", "img_path": "aRokfUfIQs/tmp/5c4f1125c32ed095fbea95eb6be6ca33ec952c2c97dc8d1d5488790e90eb36ee.jpg", "table_caption": ["Table 8: Results for GraphGPS with positional encoding, the aggregation used for the baselines is Add. See Appendix C for more information. ", "Accuracy vs. number of neighbors "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E Ablation studies ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Neighbor selection method ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this experiment, we compare the strategy of selecting random neighbors for each node with our proposed soft-neighbor selection mechanism. There are two main reasons for this comparison. i) To demonstrate that SSMA can establish strong aggregation capabilities independently of the aggregation occurring in the attention slots. ii) To provide empirical justification for the proposed soft-neighbor selection mechanism. ", "page_idx": 24}, {"type": "text", "text": "To achieve this, we conducted ablation experiments on two different datasets: ", "page_idx": 24}, {"type": "text", "text": "1. \"OGBN-Arxiv,\" a citation network where most nodes have a very low in-degree, while a few nodes have an extremely high in-degree. 2. \"Proteins,\" a dataset with an in-degree distribution highly concentrated around the mean. ", "page_idx": 24}, {"type": "text", "text": "For each one of these datasets, we compared the test accuracy using both neighbor selection methods for a varying number of neighbors and types of MPGNN layers. The results are presented in Figure 5. ", "page_idx": 24}, {"type": "image", "img_path": "aRokfUfIQs/tmp/9a64c0feb5868652ef4ef9e9d5102e4a528f86ab9cc34d9b3388df0603b94788.jpg", "img_caption": ["Figure 5: Comparison of the neighbor selection methods across different neighbor counts and MPGNN layer types on the \"OGBN-Arxiv\" and \"Proteins\" datasets. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The experiment results confirm that SSMA achieves strong performance even with the random neighbor selection and that the soft-neighbor selection works better in most cases. Interestingly, we observe that an increase in the number of neighbors does not necessarily correlate with improved performance, which was apparent most prominently in \"OGBN-Arxiv.\" ", "page_idx": 24}, {"type": "text", "text": "Furthermore, we find that the GAT layer does not benefti from the proposed slot attention mechanism. This lack of improvement may be attributed to the intrinsic attention mechanism within the GAT architecture, rendering the additional attention mechanism redundant. ", "page_idx": 24}, {"type": "text", "text": "E.2 Performance of SSMA under different budget constraints ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this experiment, we explore the performance of SSMA compared to sum-based aggregators across various hidden dimension sizes used by the MPGNN architectures. This investigation is motivated by two key objectives: i) To assess the relative gain of SSMA over sum-based aggregators in both the low-budget and high-budget regimes. ii) To analyze how the scaling behavior of sum-based aggregators compares to that of SSMA. ", "page_idx": 25}, {"type": "text", "text": "We conducted ablation studies on the \"IMDB-B\" and \"MUTAG\" datasets to achieve these goals. For each dataset, we measured the test accuracy using both sum-based aggregators and SSMA, varying the hidden dimensions and types of MPGNN layers. The results of these experiments are illustrated in Figure 6. ", "page_idx": 25}, {"type": "image", "img_path": "aRokfUfIQs/tmp/0743a74fd38d415d6d5e104e6679cb282a1f22f4baeadbd856927adad047a8d2.jpg", "img_caption": ["Figure 6: SSMA achieves peak performance with significantly lower hidden dimensions. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "The experimental results clearly indicate that SSMA outperforms sum-based aggregators across all parameter regimes, showcasing its effectiveness in propagating relevant information for downstream tasks. Additionally, SSMA does not always benefit from higher dimensionality, reaching saturation much earlier than its counterpart aggregators. This further demonstrates the efficiency of SSMA. ", "page_idx": 25}, {"type": "text", "text": "E.3 On the effectiveness of low-rank compressors ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This experiment investigates the impact of low-rank MLP compression on the test performance of diverse architectures. As detailed in Section 4.4, low-rank compression significantly reduces learnable parameters while maintaining good expressive power. This allows for an intriguing tradeoff: fewer parameters for a larger number of hidden units or slots. We evaluate this trade-off on the \"OGBN-Arxiv\" and \"ZINC\" datasets to understand its effect on performance. ", "page_idx": 26}, {"type": "text", "text": "The experiment results are presented in Figure 7. ", "page_idx": 26}, {"type": "image", "img_path": "aRokfUfIQs/tmp/945c5ba40703e57c032da7df55508aacac75a3c40fa1740811100c923d232cb3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 7: The performance under different compression strengths. SSMA can handle strong compression rates without losing much of its performance. ", "page_idx": 26}, {"type": "text", "text": "As demonstrated above, we can observe that SSMA exhibits resilience to strong compression, maintaining high performance. Notably, the best results are achieved at compression strengths significantly lower than one, demonstrating the expressive power of the MLP compressor even under significant compression and the efficiency of SSMA in propagating information. ", "page_idx": 26}, {"type": "text", "text": "To validate the selection of our affine transformation, we conducted an ablation study where we initialized the affine transformation with our proposed configuration and allowed the model to optimize it further. We used the ZINC dataset for this study and performed a brief hyperparameter search to identify the optimal configuration. The best results are presented in Table 9. The experiments show a slight improvement when the affine transformation is learned, but the gain is minimal, supporting the validity of our proposed affine transformation. Additionally, we examined the differences between the learned transformation and our proposed one. The learned transformation was similar to our proposal, with an average absolute difference of $0.023\\pm0.019$ , while the average norm of the affine weights is $0.026\\pm0.057$ . ", "page_idx": 27}, {"type": "table", "img_path": "aRokfUfIQs/tmp/91a9b4ebd9a05cfb69304feb7829073c7fb219d46b11fe02d0917d211a8e6ddb.jpg", "table_caption": ["Table 9: Learning the affine transformation on the ZINC dataset "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Common aggregation functions benchmarks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In order to further show the effectiveness of SSMA we perform more benchmarks on other common aggregation functions. The results can be seen in the tables below. ", "page_idx": 28}, {"type": "text", "text": "Table 10: Results for TU datasets [32] & ZINC [19] using LSTM module as an aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while  indicates the reported results from the relevant paper. ", "page_idx": 28}, {"type": "table", "img_path": "aRokfUfIQs/tmp/8a00518311264d10e92858d246eb52488d5fc2998eb6a203df5b1b8378d76170.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 11: Results for TU datasets [32] & ZINC [19] using max pooling aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. ", "page_idx": 28}, {"type": "table", "img_path": "aRokfUfIQs/tmp/85b7b7830c1aaaf24229182a05e73826fde0cdd229b3e20b364f7c95cc0804da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 12: Results for TU datasets [32] & ZINC [19] using mean aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. ", "page_idx": 29}, {"type": "table", "img_path": "aRokfUfIQs/tmp/748963767dfdaaf79e394d2dee558fdd653a5fac868111c65406d37984479a74.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 13: Results for TU datasets [32] & ZINC [19] using min pooling aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. ", "page_idx": 29}, {"type": "table", "img_path": "aRokfUfIQs/tmp/4cde85525556bbc7622cb17dc6e3c033563b6defd2398b2c8e6999ce37fd6a71.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 14: Results for TU datasets [32] & ZINC [19] using multiplication aggregation as a baseline. We report the TU datasets\u2019 accuracy mean and STD of a 10-fold cross-validation run. For the ZINC dataset, we report mean MAE and STD on the test set according to 5 distinct runs. \u2020 indicates reproduced results while \u2217indicates the reported results from the relevant paper. ", "page_idx": 29}, {"type": "table", "img_path": "aRokfUfIQs/tmp/f935b35b53b2266c3cd18f0034ef725be76a43894722ac7ecd9972aff0576051.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In this paper, we introduce a novel aggregation method that enhances expressive power capabilities for various message-passing graph neural networks. We provide theoretical justification and conduct extensive experiments on multiple datasets and architectures. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provided information on method limitations in section 4.4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) the full set of assumptions and a complete? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In sections 3 and 4, where we discuss the proposed method\u2019s theoretical proposition, we provide a full set of assumptions and a complete proof. The appendix references part of the proofs. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide a fully detailed experiment setup in Section 5 and additional, comprehensive information on evaluations and datasets in the appendix. We also provided the code for our experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the code of our experiments in the Abstract. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All implementation details for experiments have been provided in Section 5 and Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We reported the standard deviation error of the mean and all the results in section 5 and the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Compute resources used are presented in Appendix C.5. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and ensured our compliance with its guidelines ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: As the proposed methods improve the expressive power of existing techniques that operate on datasets with no direct social impact, we believe this work also does not have a direct social impact. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the datasets used in this work are known and widely used. We are not aware of any risks involved in these datasets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In our code repository we share all the packages we used for our code, and we also specify the version of each package The repository is here:https://almogdavid. github.io/SSMA/ ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We document our experiments thoroughly, anonymize the data, and provide the code. The code link is available in the Abstract. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]