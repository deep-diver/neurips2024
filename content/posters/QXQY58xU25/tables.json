[{"figure_path": "QXQY58xU25/tables/tables_3_1.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy results of different methods on various benchmark tasks. The methods are categorized into three groups: neurosymbolic, black-box gradient estimation, and REINFORCE-based.  The table shows the accuracy achieved by each method on several tasks, including sum, HWF, leaf classification, scene recognition, and sudoku.  \"TO\" indicates that the method timed out, and \"N/A\" means the task was not applicable to the given method.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_4_1.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy of different methods on several benchmark tasks.  The methods are categorized into three groups: neurosymbolic methods, black-box gradient estimation methods, and REINFORCE-based methods. The accuracy is presented as a percentage for each method and task.  \"TO\" indicates that the method timed out on the task, while \"N/A\" means the task could not be implemented using that framework.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_6_1.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy results of different methods on several benchmark tasks. The methods are categorized into three groups: neurosymbolic, black-box gradient estimation, and REINFORCE-based.  The table shows the performance of each method on various tasks, including sum2, sum3, sum4, HWF, DT leaf, GPT leaf, scene, and sudoku.  \"TO\" indicates that the method timed out on the specific task, and \"N/A\" means that the task was not applicable for that method.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_7_1.jpg", "caption": "Table 4: Performance comparisons for sum8, sum12, and sum16 with different sample counts k.", "description": "This table presents the accuracy results for three different tasks (sum8, sum12, and sum16) using four different methods (REINFORCE, IndeCateR, IndeCateR+, and ISED) with varying sample counts (k = 80, 800, 120, 1200, 160, 1600).  The tasks involve adding different numbers of MNIST digits. The table demonstrates the sample efficiency of each method by showing how accuracy changes as the number of samples increases.", "section": "4.4 RQ2: Sample Efficiency"}, {"figure_path": "QXQY58xU25/tables/tables_14_1.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy results of different methods on several benchmark tasks.  The benchmarks are categorized into three types: neurosymbolic, black-box gradient estimation, and REINFORCE-based.  Each row represents a different learning method. Each column represents the accuracy of the method on a specific benchmark.  \"TO\" indicates that the method timed out on that benchmark, and \"N/A\" indicates that the benchmark was not applicable to that method. The table allows for comparison of various approaches to neural program learning.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_14_2.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy of different methods on various benchmark tasks.  The methods are categorized into three groups: neurosymbolic, black-box gradient estimation, and REINFORCE-based.  The table shows the accuracy achieved by each method on several tasks, including those involving sum, HWF, leaf classification, scene recognition, and sudoku.  \"TO\" indicates that the method timed out for the given task, and \"N/A\" means the task could not be implemented using that method.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_15_1.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy results (%) of different methods on various benchmark tasks.  The methods are categorized into three groups: neurosymbolic methods (DPL, Scallop, A-NeSI), black-box gradient estimation methods (REINFORCE), and REINFORCE-based methods (IndeCateR, NASR, ISED). The benchmarks include tasks from various domains such as sum, HWF, leaf classification, and scene recognition.  \"TO\" indicates that the method timed out on the task, and \"N/A\" indicates that the task could not be implemented with the particular method. ISED represents the proposed method of the paper.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_15_2.jpg", "caption": "Table 3: Performance on selected benchmarks. \"TO\" means time-out, and \"N/A\" means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.", "description": "This table presents the accuracy of different methods on several benchmark tasks.  The methods are categorized into three groups: neurosymbolic, black-box gradient estimation, and REINFORCE-based.  The table shows the accuracy for each method on different tasks, with \"TO\" indicating a timeout and \"N/A\" indicating that the task could not be performed by the given method. This allows for a comparison of the performance of different approaches across various types of tasks.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_15_3.jpg", "caption": "Table 9: Performance comparison for mult2, mod2, less-than, and add-mod-3.", "description": "This table presents a comparison of the accuracy achieved by various methods (DPL, Scallop, A-NeSI, REINFORCE, IndeCateR, NASR, and ISED) on four different MNIST-R tasks: mult2, mod2, less-than, and add-mod-3.  Each task involves performing a specific arithmetic or comparison operation on pairs of handwritten digits from the MNIST dataset. The table shows the accuracy of each method, along with the standard deviation, illustrating the performance variations. This allows for a comparison of the relative performance of different approaches on these tasks.", "section": "4.1 Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic"}, {"figure_path": "QXQY58xU25/tables/tables_15_4.jpg", "caption": "Table 10: Performance comparison for add-sub, equal, not-3-or-4, and count-3-4.", "description": "This table presents the accuracy results for four different MNIST-R tasks (add-sub, equal, not-3-or-4, and count-3-4). It compares the performance of several methods: DPL, Scallop, A-NeSI, REINFORCE, IndeCateR, NASR, and ISED (the proposed method).  The accuracy is given as a percentage, with standard deviations.", "section": "4.3 RQ1: Performance and Accuracy"}, {"figure_path": "QXQY58xU25/tables/tables_16_1.jpg", "caption": "Table 4: Performance comparisons for sum8, sum12, and sum16 with different sample counts k.", "description": "This table presents a comparison of the accuracy achieved by different methods (REINFORCE, IndeCateR, IndeCateR+, and ISED) on three tasks (sum8, sum12, and sum16) with varying sample counts (k = 80, 800, 120, 1200, 160, 1600).  The tasks involve adding a certain number of MNIST digits, testing the algorithms' sample efficiency and performance under different input sizes and complexities. The results highlight how the accuracy of each method changes with the increasing sample count and input size.", "section": "4.4 RQ2: Sample Efficiency"}, {"figure_path": "QXQY58xU25/tables/tables_16_2.jpg", "caption": "Table 4: Performance comparisons for sum8, sum12, and sum16 with different sample counts k.", "description": "This table presents the accuracy achieved by four different methods (REINFORCE, IndeCateR, IndeCateR+, and ISED) on three different tasks (sum8, sum12, and sum16) with two different sample counts (k=80/800, k=120/1200, k=160/1600).  The tasks involve adding up to 8, 12, and 16 MNIST digits respectively.  The table shows how the accuracy of each method changes with varying sample counts, demonstrating the sample efficiency of each approach. The results highlight that ISED generally outperforms the other methods, especially with lower sample counts. ", "section": "4.4 RQ2: Sample Efficiency"}, {"figure_path": "QXQY58xU25/tables/tables_16_3.jpg", "caption": "Table 4: Performance comparisons for sum8, sum12, and sum16 with different sample counts k.", "description": "This table presents the accuracy results for three different tasks (sum8, sum12, sum16) under varying sample counts (k=80, 800, 120, 1200, 160, 1600).  The tasks involve adding MNIST digits.  The table compares the performance of four different methods: REINFORCE, IndeCateR, IndeCateR+, and ISED.  It shows how the accuracy of each method changes as the sample count increases for each task, demonstrating the sample efficiency of the algorithms.", "section": "4.4 RQ2: Sample Efficiency"}]