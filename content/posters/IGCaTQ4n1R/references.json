{"references": [{"fullname_first_author": "C. R. Qi", "paper_title": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "publication_date": "2017-00-00", "reason": "This paper is foundational to the field of deep learning on point clouds, introducing the PointNet architecture which many subsequent methods build upon."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a powerful vision-language model that is central to OpenDlign's multimodal alignment strategy."}, {"fullname_first_author": "X. Zhu", "paper_title": "Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning", "publication_date": "2022-00-00", "reason": "This paper is highly relevant as it directly addresses open-world 3D representation learning using vision-language models, a key problem tackled by OpenDlign."}, {"fullname_first_author": "R. Zhang", "paper_title": "Pointclip: Point cloud understanding by clip", "publication_date": "2021-00-00", "reason": "As one of the earliest works to leverage CLIP for 3D understanding, PointCLIP is a crucial predecessor to OpenDlign, demonstrating the potential of using vision-language models for 3D tasks."}, {"fullname_first_author": "T. Huang", "paper_title": "Clip2point: Transfer clip to point cloud classification with image-depth pre-training", "publication_date": "2022-00-00", "reason": "CLIP2Point is a direct competitor to OpenDlign, also focusing on using depth information for multimodal alignment; comparing against it is crucial for demonstrating OpenDlign's advancements."}]}