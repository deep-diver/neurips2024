[{"type": "text", "text": "OpenDlign: Open-World Point Cloud Understanding with Depth-Aligned Images ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ye Mao Junpeng Jing\u2217 Krystian Mikolajczyk Imperial College London https://yebulabula.github.io/OpenDlign/ {ye.mao21, j.jing23, k.mikolajczyk}@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D point clouds with image-text information have shown superior 3D zero-shot performance. However, CAD-rendered images for this alignment often lack realism and texture variation, compromising alignment robustness. Moreover, the volume discrepancy between 3D and 2D pretraining datasets highlights the need for effective strategies to transfer the representational abilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment. These images exhibit greater texture diversity than CAD renderings due to the stochastic nature of the diffusion model. By refining the depth map projection pipeline and designing depth-specific prompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D representation learning with streamlined fine-tuning. Our experiments show that OpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks, despite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In zero-shot classification, OpenDlign surpasses previous models by $8.0\\%$ on ModelNet40 and $16.4\\%$ on OmniObject3D. Additionally, using depth-aligned images for multimodal alignment consistently enhances the performance of other state-of-the-art models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D understanding, including tasks like 3D object classification [1, 2], 3D scene segmentation [3], and 3D reconstruction [4, 5], is becoming increasingly pivotal in real-world applications like augmented/virtual reality [6, 7], autonomous vehicles [8]. Traditional 3D models [1, 2, 9, 10, 11] are closed-world, recognizing only pre-defined categories and struggling with \u2018unseen\u2019 ones. Recent studies aim to leverage Vision-Language Models (e.g., CLIP [12]) to develop open-world 3D models that generalize beyond \u2018seen\u2019 3D data, enabling zero-shot handling of various 3D tasks. ", "page_idx": 0}, {"type": "text", "text": "Existing open-world 3D learning methods can be classified into depth-based and point-based approaches. Depth-based methods [13, 14, 15] convert point clouds into multi-view depth maps and use the pre-trained CLIP image encoder for 3D representations. A significant challenge is the domain gap since CLIP is pre-trained on RGB images, not depth maps. To mitigate this gap, methods like [15] introduce an additional depth encoder to align depth features with the image and text features from the pre-trained CLIP encoders, as shown in Fig. 1(a). The images used for feature alignment are produced by rendering synthetic CAD models from various camera viewpoints. Point-based methods [16, 17, 18, 19, 20] directly extract 3D representations from point clouds, thereby bypassing the need for depth map projection. However, they also require an extra point encoder for feature alignment to address format disparities between images and point clouds, as shown in Fig. 1(b). Thus, employing an extra 3D encoder for multimodal feature alignment of 3D data, CAD-rendered images, and text modalities is a standard practice in modern open-world 3D learning methods. ", "page_idx": 0}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/d759aad15ecfff6ae8130b74751e202c7f25686690c224d2cfab4b877e961569.jpg", "img_caption": ["Figure 1: Top: Comparison of OpenDlign with traditional open-world 3D learning models. Depthbased (a) and point-based (b) methods employ additional depth or point encoders for pre-training to align with CAD-rendered images. Conversely, OpenDlign (c) fine-tunes only the image encoder, aligning with vividly colored and textured depth-aligned images for enhanced 3D representation. Both rendered and depth-aligned images are utilized solely during training. Bottom: Visual comparison between multi-view CAD-rendered and corresponding depth-aligned images in OpenDlign. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite widespread usage, we argue that CAD-rendered images fall short of providing the visual diversity and realism necessary for robust multimodal alignment. This limitation arises because CAD models in current open-source datasets [21, 22] often feature simplistic or entirely absent textures. These models also struggle to realistically simulate environmental effects like lighting, shadows, and reflections. Moreover, most CAD models maintain visual coherence across viewpoints, leading to overly consistent textures and colors from all angles. To achieve generalizable 3D representations, each image view for alignment is expected to display significant visual variations (See Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "Additionally, pretraining an extra 3D encoder for alignment may not fully leverage the rich knowledge in CLIP for 3D understanding due to the significant volume discrepancy between 2D and 3D pretraining datasets. Mainstream 3D datasets like ShapeNet [21] and Objaverse [23] contain fewer than 1 million synthetic 3D objects, significantly less than the vast image datasets such as DFN5B [24] and LAION-5B [25], which include around 5 billion images used to train cutting-edge CLIP models. While direct fine-tuning of CLIP\u2019s encoders facilitates more straightforward knowledge transfer, it restricts inputs to depth maps. Yet, developing 3D representations from depth maps is currently less effective than from point clouds for two primary reasons: (1) The current widely used CLIP text prompt templates are tailored for matching with RGB images, not depth maps, and (2) the lack of a robust projection method for creating dense depth maps with smooth contours from point clouds. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present OpenDlign, the first 3D open-world framework that develops 3D representation by aligning with multi-view diffusion model-generated images, termed depth-aligned images. These images benefit from the stochastic nature of the diffusion model, offering greater texture diversity compared to CAD renderings while maintaining the original 3D data\u2019s geometric and semantic integrity (See Fig. 1). Remarkably, OpenDlign demonstrates competitive open-world capability by fine-tuning only 6 million parameters of the CLIP image encoder on ShapeNet [21], unleashing CLIP\u2019s vast potential in 3D learning (See Fig. 1(c)). The success of this fine-tuning stems from refining the depth map projection pipeline, developing depth-specific text prompts, and introducing a logit aggregation strategy to merge multi-view prediction results. Experimental results reveal that OpenDlign significantly outperforms previous state-of-the-art (SOTA) models on a variety of 3D tasks, including zero-shot/few-shot classification, object detection, and cross-modal retrieval. In zero-shot classification, OpenDlign achieves accuracy improvements of $8.0\\%$ on ModelNet40 and $16.4\\%$ on OmniObject3D, the largest real-world 3D shape dataset. Additionally, depth-aligned images markedly enhance the performance of SOTA models, consistently improving results across diverse benchmarks and highlighting their transformative impact on open-world 3D learning pipelines. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The main contributions of this paper are outlined as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce depth-aligned images as a robust alternative to CAD-rendered images for open-world 3D learning. These images, generated from point cloud-projected depth maps using a diffusion model, offer rich and realistic texture diversity across viewpoints. \u2022 We propose a multimodal alignment framework that robustly aligns depth maps, depthaligned images, and text through streamlined fine-tuning of the CLIP image encoder. \u2022 We develop a contour-aware projection pipeline to produce dense and contour-preserving multi-view depth maps from point clouds. \u2022 We present depth-specific text prompts and a logit aggregation strategy to boost OpenDlign\u2019s zero-shot capabilities and mitigate catastrophic forgetting during alignment fine-tuning. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Open-World 3D Representation Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-Language Models (VLMs) such as CLIP [12] have revolutionized 2D representation learning in open-world settings through contrastive learning with large-scale image-text pairs [26, 27, 28, 29]. Building on this, recent studies have adapted CLIP for 3D representation learning, achieving impressive performance in diverse 3D zero-shot tasks [18, 19]. ", "page_idx": 2}, {"type": "text", "text": "PointCLIP [14], as a pioneering study, utilizes the CLIP image encoder for extracting 3D representations from depth maps of point clouds, achieving zero-shot recognition by aligning with text embeddings of semantic categories. To address CLIP\u2019s training bias towards RGB images, Zhu et al. [13] introduced GPT-generated 3D-specific prompts and a denser depth map projection, while CLIP2Point [15] pre-trains a depth encoder for closer alignment with CLIP\u2019s encoders. These methods derive representations from depth maps with noisy contours, causing a loss of key shape features needed for precise recognition. Moreover, their reliance on either natural image text prompts or depth-specific prompts generated by GPT-3 [30] for certain categories highlights a lack of versatility in handling diverse 3D contexts. Alternative methods [17, 18, 19, 20] avoid depth map projection by directly aligning point clouds, images, and text using specialized 3D encoders. By scaling up the dataset and encoder sizes, these methods show promise in diverse 3D tasks. However, these methods are limited by their reliance on CAD-rendered images, which have limited texture diversity across views, leading to less generalizable representations. Additionally, the smaller volume of 3D datasets compared to CLIP\u2019s training data hinders effective knowledge transfer to point cloud encoders. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we substitute rendered images with depth-aligned images generated from a diffusion model to enhance texture diversity. We also fine-tune the CLIP image encoder for 3D representation learning instead of training a new 3D encoder from scratch, reducing the reliance on large 3D datasets. ", "page_idx": 2}, {"type": "text", "text": "2.2 Continual Learning in CLIP Fine-Tuning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Continual Learning (CL) in CLIP aims to mitigate catastrophic forgetting [31], ensuring retention of zero-shot capabilities across varied data distributions while fine-tuning to new tasks. CL methods fall into three categories: adaptive-plasticity methods [32, 33], replay methods [34, 35], and architecturebased methods [36, 37]. Adaptive-plasticity methods limit the plasticity of the essential model parameters for past tasks during fine-tuning. For instance, the IMM-Mean [38] method achieves CL by simply averaging parameters of pre-trained and fine-tuned models for inference, although its efficacy might be limited for complex tasks [39]. Replay methods leverage stored exemplars to enable CLIP to recall previously learned knowledge, while they encounter scalability challenges. Without relying on exemplars, architecture-based CL methods dynamically adjust the model\u2019s architecture to accommodate new information without losing existing knowledge [39]. In this study, we align the depth map with the RGB image by freezing the pre-trained CLIP encoder weights and incorporating a trainable transformer-based branch for encoding depth maps, adhering to architecturebased principles. Inspired by IMM-Mean [38], we use pre-trained and fine-tuned model weights to compute classification logits from multi-view depth maps for inference. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The OpenDlign framework, depicted in Fig. 2, starts with a contour-aware projection method that transforms point clouds into multi-view depth maps with preserved contours. These maps guide a diffusion model to produce depth-aligned images with varied colors and textures, maintaining consistent geometry with the depth maps. OpenDlign then aligns features from depth maps and depthaligned images by fine-tuning a transformer block connected to the pre-trained image encoder. The goal is to align feature embeddings from depth maps and corresponding depth-aligned images using contrastive learning. At inference, 3D representations are composed of embeddings from multi-view depth maps, derived using both fine-tuned and pre-trained encoder states. These embeddings are matched with depth-specific text embeddings, which capture the semantic and visual properties of the depth maps, to generate multi-view logits. These logits are then aggregated to facilitate label prediction in a zero-shot setting. In few-shot scenarios, these embeddings can be further refined with a logistic regressor. Detailed training and model implementation are provided in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Contour-Aware Depth Map Projection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The contour-aware projection method transforms the input point cloud into multi-view depth maps with clear contours. Inspired by the pipeline in [13], this method involves four main steps: Quantize, Densify, Smooth, and Squeeze. ", "page_idx": 3}, {"type": "text", "text": "In the Quantize step, for the $i^{\\mathrm{th}}$ e{}}xh view of point cloud $P_{i}$ i, the 3D coordinates $(x,y,z)\\;\\in\\;P_{i}$ ii are normalized to $[0,1]$ and mapped onto a discrete grid $G\\in\\mathbb{R}^{H\\times W\\times B}$ iittt,hRBHW where $H$ and $W$ correspond to the dimensions required by the CLIP image encoder, and $B$ is a pre-defined depth dimension. Next, the Densify step enhances $G$ by updating each voxel to the maximum value within its $7\\times7\\times7$ neighborhood, yielding a denser map $G^{\\prime}$ . Subsequently, the Smooth step applies bilateral flitering to each voxel $v_{i}$ i in $G^{\\prime}$ , adjusting its intensity $I_{v_{i}}$ i to $I_{v_{i}}^{\\prime}$ {v_}i using: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{v_{i}}^{\\prime}=\\frac{1}{W_{v}}\\sum_{v_{j}\\in S}G_{\\sigma_{1}}(\\|v_{i}-v_{j}\\|)G_{\\sigma_{2}}(|I_{v_{i}}-I_{v_{j}}|)I_{v_{j}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{W_{v}=\\sum_{v_{j}\\in S}G_{\\sigma_{1}}(\\|v_{i}-v_{j}\\|)G_{\\sigma_{2}}(|I_{v_{i}}-I_{v_{j}}|)}\\end{array}$ ()()1g2is the normalization factor that ensures voxel weights sum to 1.0. The Gaussian functions $G_{\\sigma_{1}}$ and $G_{\\sigma_{2}}$ adjust the influence of each neighboring voxel $v_{j}$ within the $5\\times5\\times5$ kernel from set $S$ around $v_{i}$ , based on spatial and intensity differences, enhancing contour sharpness and reducing jagged edges in $G^{\\prime}$ . Finally, the Squeeze step applies the minimal pooling on the depth channel of the smoothed $G^{\\prime}$ , then triples the output to mimic RGB intensity, producing the final depth map $D\\in\\mathbb{R}^{H\\times W\\times3}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Depth-Aligned Image Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A total of 524,700 depth-aligned images are generated from ShapeNet [21], a leading public 3D CAD dataset containing approximately 52,470 models, each annotated with semantic metadata. To generate these images, a point cloud of 10,000 points is sampled from each CAD model, aligning with prior experimental protocols [18, 17]. For each point cloud, 10 views of depth maps are projected using the proposed contour-aware projection method. Subsequently, the ControlNet v1.1 [40] depth model produces depth-aligned images for each contour-aware depth map view, using the CAD model\u2019s metadata as text input and the inverse of the depth map $\\textstyle{\\frac{1}{D}}$ as image generation control. This approach ensures that the generated images remain consistent with the depth maps both geometrically and semantically, while also adding texture diversity across different views. ControlNet utilizes $\\textstyle{\\frac{1}{D}}$ instead of $D$ for controlling image outputs because it is predominantly pre-trained on depth images where brighter regions indicate closer proximity. Please refer to Appendix A.2 for the positive and negative prompts used in ControlNet to achieve high-fidelity depth-aligned image generation. ", "page_idx": 3}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/33395bf399466f48c4fa0b6d633be2ceae8e2494fe0432bd665d74aeb68b1226.jpg", "img_caption": ["(a) 3D Representation Learning via Generated Depth-Aligned Images ", "Figure 2: Overview of OpenDlign. In (a), OpenDlign converts point clouds into multi-view depth maps using a contour-aware projection, which then helps generate depth-aligned RGB images with diverse textures, geometrically and semantically aligned with the maps. A transformer block, residually connected to the CLIP image encoder, is fine-tuned to align depth maps with depthaligned images for robust 3D representation. For zero-shot classification (b), OpenDlign aggregates multi-view logits from both pre-trained and fine-tuned encoders for label prediction. For few-shot classification (c), it employs a logistic regressor trained on multi-view features from the encoders. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Multimodal Representation Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "OpenDlign aligns depth maps and depth-aligned images by fine-tuning a transformer block residually linked to the last block of the CLIP image encoder, using contrastive learning. With CLIP pre-trained to align images and text, OpenDlign implicitly aligns depth maps with the shared image-text space. ", "page_idx": 4}, {"type": "text", "text": "Multimodal Feature Extraction. Given a 3D point cloud input, let ${D}=\\{{D}_{i}\\}_{i=1}^{N}$ \\}_{i}^N 1=represent the set of its $N$ Nprojected depth map views, and $R=\\{R_{i}\\}_{i=1}^{N}$ \\}_{i}^N 1=the corresponding set of depth-aligned images. Each image $R_{i}$ iis encoded through $L$ Llayers of a pre-trained CLIP image encoder, $\\{\\mathrm{T}_{l}(\\cdot)\\}_{l=1}^{L}$ (),1= to obtain feature representations $I_{i}^{R}=\\mathrm{T}_{1\\ldots L}(R_{i})$ t {T}_{\\dts L}R_i .l()e1ox Each depth map $D_{i}$ _ iis processed up to layer $\\mathrm{T}_{L-1}$ ,1 yielding preliminary features $\\mathrm{T}_{1\\dots L-1}(D_{i})$ {\\dts L-}D_i .l()1o1 Subsequently, these depth features are passed through the frozen layer ${\\mathrm{T}}_{L}$ ttTLand its trainable counterpart $\\mathrm{T}_{L}^{t}$ tt,TL where only the attention layers for spatial interaction in $\\mathrm{T}_{L}^{t}$ ttTLare trainable, as inspired by [41]. This process produces the feature for the $i_{t h}$ depth map view ", "page_idx": 4}, {"type": "text", "text": "$I_{i}^{D}=\\mathrm{T}_{1\\dots L}(D_{i})+\\mathrm{T}_{L}^{t}(\\mathrm{T}_{1\\dots L-1}(D_{i}))$ . The final feature vectors for multi-view depth maps $D$ and depth-aligned images $R$ are $\\begin{array}{r}{\\mathbf{h}^{D}=\\frac{1}{N}\\sum_{i=1}^{N}\\|I_{i}^{D}\\|}\\end{array}$ = \\rac {1}{N} \\sum ^N_{i=1}\\|I^D_i\\| fand $\\begin{array}{r}{\\mathbf{h}^{R}=\\frac{1}{N}\\sum_{i=1}^{N}\\|I_{i}^{R}\\|}\\end{array}$ ,f respectively. ", "page_idx": 5}, {"type": "text", "text": "Loss Functions. The alignment of $\\mathbf{h}^{D}$ and $\\mathbf{h}^{R}$ is achieved by minimizing a composite loss function, comprising the contrastive loss $\\mathcal{L}_{\\mathrm{cont}}$ and the feature distance loss $\\mathcal{L}_{\\mathrm{dist}}$ , defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{total}}=\\underbrace{\\sum_{i,j}-\\frac{1}{2}\\log\\frac{\\exp\\left(\\mathbf{h}_{i}^{D}\\mathbf{h}_{j}^{R}/\\tau\\right)}{\\sum_{k}\\exp\\left(\\mathbf{h}_{i}^{D}\\mathbf{h}_{k}^{R}/\\tau\\right)}-\\frac{1}{2}\\log\\frac{\\exp\\left(\\mathbf{h}_{i}^{D}\\mathbf{h}_{j}^{R}/\\tau\\right)}{\\sum_{k}\\exp\\left(\\mathbf{h}_{k}^{D}\\mathbf{h}_{j}^{R}/\\tau\\right)}}_{\\mathcal{L}_{\\mathrm{coul}}}+\\underbrace{\\sum_{(i,j)}\\|\\mathbf{h}_{i}^{D}-\\mathbf{h}_{j}^{R}\\|_{2}}_{\\mathcal{L}_{\\mathrm{dai}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In each training batch, $(\\mathbf{h}_{i}^{D},\\mathbf{h}_{j}^{R})$ {h}^D_i, \\mathbf {h}^R_j )is a positive pair, while $(\\mathbf{h}_{i}^{D},\\mathbf{h}_{k}^{R})$ )and $(\\mathbf{h}_{k}^{D},\\mathbf{h}_{j}^{R})$ )are negative pairs where $k\\neq i,j$ q ,  i.j $\\tau$ is a learnable temperature parameter. The contrastive loss enables learning robust representations by maximizing similarity in positive pairs and minimizing it in negative pairs [42, 43]. ", "page_idx": 5}, {"type": "text", "text": "3.4 3D Zero-Shot Transfer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The alignment between depth maps and depth-aligned images facilitates 3D zero-shot classification by aggregating multi-view classification logits. Each logit represents the similarity between single-view depth features and text features of category candidates, as illustrated in Fig. 2(b). ", "page_idx": 5}, {"type": "text", "text": "Depth-Specific Text Generation. Depth-specific prompt templates are developed by augmenting a base set of 80 text prompts, initially designed for ImageNet classification2, with depth-related keywords such as \"depth map\", \"raytraced image\", and \"silhouette of [CLASS]\". These keywords direct OpenDlign\u2019s attention to geometric details and contours rather than color or texture. The CLIP Interrogator [44] is a prompt engineering tool that combines CLIP and BLIP [45] to select optimal text prompts for specific images. To identify these keywords, this tool identifies the top 10 prompts that match depth maps from the ShapeNet dataset [21], chosen as targeted keywords. For zero-shot inference, we employ our depth-specific templates to generate 80 text descriptions for each label $l$ . These descriptions $\\{{t}_{i}\\}_{i=1}^{80}$ are encoded by a texture encoder $F(\\cdot)$ , normalized, and then merged into a unified text feature $F_{l}$ via average pooling, calculated as $\\textstyle{\\frac{1}{80}}\\sum_{i=1}^{80}\\|F(t_{i})\\|$ . ", "page_idx": 5}, {"type": "text", "text": "Multi-View Logits Aggregation. To calculate classification logits, we first gather visual features from multi-view depth maps $\\{V_{i}\\}_{i=1}^{N}$ , aiming to align with depth-specific text features of $M$ candidate labels $\\mathbf{F}=\\{F_{i}\\}_{i=1}^{M}$ . The feature extraction utilizes a dual-encoder strategy: the first half of the views $\\{V_{i}\\}_{i=1}^{N/2}$ 12=utilize a pre-trained CLIP image encoder, while the second half of the views $\\{V_{i}\\}_{i=N/2+1}^{N}$ employs a fine-tuned encoder. The strategy ensures that OpenDlign maintains its capability to recognize previously identifiable depth maps after learning multimodal alignment via fine-tuning. As shown in Fig. 2(b), the logit for a single depth map view is the product of $V_{i}$ and $\\mathbf{F}$ , with the overall classification logit being the sum of logits across all views, calculated as $\\textstyle\\sum_{i=1}^{N}V_{i}\\mathbf{F}^{T}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Zero-Shot 3D Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first evaluated OpenDlign under the zero-shot shape classification task on four benchmark datasets: ModelNet40 [46], ScanObjectNN [47], OmniObject3D [48], and Objaverse-LVIS [23]. ModelNet40 offers synthetic 3D CAD models in 40 categories. ScanObjectNN provides real-scanned objects in 15 categories from OBJ_ONLY version. OmniObject3D, the largest, includes 5,911 realscanned objects in 216 categories, well-suited for fine-grained, real-world classification evaluation. Objaverse-LVIS contains 1,156 categories for evaluating long-tail classification. Point cloud sizes are 10,000 points for ModelNet40 and Objaverse-LVIS, 2,048 for ScanObjectNN, and 4,096 for OmniObject3D. OpenDlign was compared against existing methods, including three depth-based methods: PointCLIP [14], PointCLIP V2 [13], and CLIP2Point [15], and three point-based methods: ULIP [17], OpenShape [18], and TAMM [20]. To investigate if depth-aligned images consistently enhance the representational abilities of other 3D open-world methods, we retrained all OpenShape and TAMM variants using their original CAD-rendered images and some depth-aligned images. Note that the depth-aligned images for all model variants were exclusively generated from ShapeNet [21], while the pretraining CAD-rendered images could come from ShapeNet or a large-scale ensemble dataset [18] that includes Objaverse [23], ShapeNet [21], 3D-Future [49], and ABO [22]. Furthermore, we evaluated OpenDlign\u2019s scalability by training it with various CLIP variants. ", "page_idx": 5}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/c4c31d2bd6396fe700d3ad870c0635e1428ae6a59b2dd4f0c8834c1a53b501ed.jpg", "table_caption": ["Table 1: Zero-shot classification results on ModelNet40 [46], ScanObjectNN [47] and OmniObject3D [48]. The best-performing results are presented in bold, while the second-best results are underlined. Our models are highlighted in blue . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 6 shows OpenDlign substantially outperforms existing methods trained on ShapeNet across all benchmarks, exceeding the previous best, TAMM-SparseConv, by margins of $8.0\\%$ on ModelNet40, $1.6\\%$ on ScanObjectNN, and $16.4\\%$ on OmniObject3D in Top1 accuracy. Appendix A.5 Table 7 demonstrates OpenDlign\u2019s excellence in handling long-tail categories by outperforming previous methods by $21.3\\%$ on the Objaverse-LVIS dataset. OpenDlign also outperforms the top depthbased method, PointCLIP V2, by $19\\%$ on ModelNet40 and $27.4\\%$ on OmniObject3D. Notably, OpenDlign outshines all methods pre-trained on the ensemble dataset in the ScanObject3D benchmark. Additionally, its performance increases linearly with the complexity of CLIP variants, outperforming most baselines on ModelNet40 and OmniObject3D, even using the lighter ViT-B-16 CLIP model. Remarkably, using depth-aligned images $\\left(+\\mathrm{dlign}\\right)$ consistently boosts the performance of OpenShape and TAMM variants pre-trained on the ShapeNet dataset across all benchmarks. Despite depthaligned images being available only for ShapeNet, which comprises just $10\\%$ of the ensemble dataset, TAMM-PointBERT $\\left(+\\mathrm{dlign}\\right)$ shows a $4.8\\%$ increase in Top1 accuracy on the ScanObjectNN dataset, and OpenShape-PointBERT ( $^{\\,\\prime}+$ dlign) records a $1.6\\%$ increase on the OmniObject3D. Note that for parts of the ensemble dataset without depth-aligned images, we used CAD-rendered images instead. ", "page_idx": 6}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/3c3407c6dce795e19c2aad4c9899f8bf046e8e4b6636ff77db4326897d9ff07e.jpg", "table_caption": ["Table 2: Few-shot classification results on ModelNet40 [46], ScanObjectNN [47] and OmniObject3D [48]. Our results are averaged over 10 random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Few-Shot 3D Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We then assessed OpenDlign\u2019s few-shot classification capability by training a logistic regressor with linear probing on features from $N$ -shot, 10-view depth maps. Similar to the zero-shot scenario, we extracted multi-view features using both fine-tuned and pre-trained OpenDlign encoders. At inference, the regressor aggregates logits from 10 views to predict the final label. We compared OpenDlign with ULIP [17], OpenShape [18], and TAMM [20], which extract features for training regressor from their point encoders pre-trained on ShapeNet. Table 2 shows OpenDlign outperforms all baselines across varied few-shot scenarios with 1 to 16 training samples per class. OpenDlign significantly outperforms the leading baseline on the OmniObject3D dataset, exceeding it by $8.8\\%$ and $11.8\\%$ in the 4-shot and 8-shot classification, respectively. See Appendix A.5 for more results. ", "page_idx": 6}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/de766303f23916708ba77c77f0a4edbdd619bb8f523ab871d7a3fd495240d84b.jpg", "table_caption": ["Table 3: Zero-shot 3D object detection results on ScanNet V2 [51]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Zero-Shot 3D Object Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "OpenDlign\u2019s capability in zero-shot 3D object detection was evaluated on the ScanNet V2 dataset [51], which contains richly annotated 3D indoor scenes in 18 object categories. Following the PointCLIP V2 methodology [13], we started with the pre-trained 3DETR-m [11] model to identify 3D regions of interest, delineate 3D bounding boxes, and extract points within each box. Finally, we applied OpenDlign to these points to generate our predictions. Table 3 illustrates OpenDlign\u2019s zero-shot detection prowess using mean Average Precision (mAP) at IoU thresholds of 0.25 and 0.5, achieving scores of $50.72\\%$ and $37.97\\%$ , respectively. It significantly outperforms PointCLIP V2 by more than $31.75\\%$ and $26.44\\%$ . Remarkably, OpenDlign can detect the \u2018Sofa\u2019 shape with an $\\mathrm{AP_{50}}$ of $54.96\\%$ , whereas PointCLIP and V2 score below $10\\%$ , demonstrating OpenDlign\u2019s superior ability to extract robust 3D representations from sparse and noisy point clouds in real-world indoor scenes. ", "page_idx": 7}, {"type": "text", "text": "4.4 Cross-Modal Retrieval ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "3D shapes were retrieved by computing the cosine similarity between the embeddings of a query and those generated by OpenDlign, followed by a $\\mathbf{k}$ -nearest neighbors (kNN) analysis to find the most similar shapes. Fig. 3 showcases OpenDlign\u2019s ability to match 3D shapes to image and text queries. Column (a) shows its precision in distinguishing sub-categories like grand versus upright pianos from image queries. Column (b) demonstrates successful shape retrieval using distinctive text descriptions like \"Batmobile armored\". Notably, averaging image and text query embeddings allows OpenDlign to find shapes that combine elements of both queries. For instance, merging a running horse image with the text \"man\" retrieves both a centaur and a running man, as shown in Fig. 3(c). Similarly, combining a house image with \"tree\" retrieves a treehouse. See Appendix A.6 for more results. ", "page_idx": 7}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/83f52c11ca7218ddc462b1272685eda7a69787aa505c1f20be4abf6e0bf9fa62.jpg", "img_caption": ["Figure 3: 3D shape retrieval results. (a) Two most similar shapes for each image query. (b) Most similar shapes for each text query. (c) Two most similar shapes for combined image and text queries. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/7b4dfefe3edeb9bc60f2e1a1ee37e52b939fd5f77ab34dfbd89821c3f81ce8c6.jpg", "table_caption": ["Table 4: Ablation study for OpenDlign on ModelNet40 [46] and ScanObjectNN [47]. Accuracy improvements over the baseline (first-row) are highlighted in green. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation studies were conducted on zero-shot classification benchmarks to assess the contribution of each component in OpenDlign. Consistently, all OpenDlign variants used in these studies employed OpenCLIP-ViT-H-14 as their backbone. ShapeNet was the default training dataset for all models. ", "page_idx": 8}, {"type": "text", "text": "Contour-Aware Projection. Replacing PointCLIP V2\u2019s projection pipeline [13] with our contouraware version enables a pre-trained CLIP to achieve $68.8\\%$ zero-shot accuracy on ModelNet40, outperforming several baselines that require extra training (See Table 4). This indicates that CLIP can understand RGB images and depth maps when shape features are highlighted. ", "page_idx": 8}, {"type": "text", "text": "Effect of Alignment with Depth-Aligned Images. Table 4 shows that aligning depth maps with depth-aligned images (i.e., depth-dlign) significantly boosts performance, improving Top1 accuracy by around $10\\%$ on ScanObjectNN, with or without depth-specific prompts. This indicates that depth-d alignment effectively transfers CLIP\u2019s rich knowledge to interpret depth maps. ", "page_idx": 8}, {"type": "text", "text": "Further analysis compared depth-dlign alignment against three alternatives: depth-rendCAD (aligning depth maps with CAD-rendered RGB images), dlign-text & depth (aligning depth-aligned images with text before depth-dlign alignment), and depth-text & dlign (simultaneous alignment of depth maps with text and depth-aligned images). Table 5 shows depth-dlign outperforming depth-rendCAD by $6.8\\%$ on the ScanObjectNN dataset, confirming concerns that alignment with rendered images may lead to overfitting on specific 3D shapes. Moreover, dlign-text & depth performs worst, suggesting that pre-aligning depth-aligned images with text compromises CLIP\u2019s ability to generate robust image representations, thus affecting subsequent depth-dlign alignment efficacy. The superior performance of depth-dlign on ModelNet40 and OmniObject3D compared to depth-text & dlign shows that aligning depth maps with depth-aligned images indirectly aligns with text, making additional text alignment unnecessary and potentially limiting OpenDlign\u2019s generalization. ", "page_idx": 8}, {"type": "text", "text": "Depth-Specific Texts. Table 4 shows that depth-specific prompts enhance OpenDlign\u2019s performance, regardless of using multimodal alignment or logit aggregation. This indicates that some recognition inaccuracies arise from processing input data as typical RGB images instead of depth maps. ", "page_idx": 8}, {"type": "text", "text": "Logits Aggregation. Results in Table 4 show that multi-view logit aggregation improves zeroshot classification on all datasets by combining logits from pre-trained and fine-tuned encoders. This approach effectively mitigates the catastrophic forgetting problem in OpenDlign\u2019s multimodal alignment, enabling it to recognize 3D objects identifiable by both pre-trained CLIP and OpenDlign. ", "page_idx": 8}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/0f5e9f97ecadade87bfddbcfe451b057f342796b5f52fa8577cc2320be20f75d.jpg", "table_caption": ["Table 5: Ablation study on various alignment strategies. Aligning with text modality was achieved by fine-tuning the image encoder. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/03d7c4dc6c294061f12a768ee7543f2a9c98bbce9f3adfb93990c5da64b86cfb.jpg", "img_caption": ["Figure 4: Effect of the number of views on OpenDlign\u2019s zero-shot performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Varying Number of Views. OpenDlign, like other depth-based methods, requires extracting multiple embeddings from multi-view depth maps for zero-shot inference. Fig. 7 shows that OpenDlign\u2019s zero-shot accuracy on ModelNet40 and OmniObject3D improves with more depth map views. Notably, OpenDlign achieves top performance, comparable to TAMM-PointBERT, with just two views, balancing latency and effective zero-shot classification. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we introduce OpenDlign, an open-world framework that learns robust 3D representations from multi-view depth maps by efficiently fine-tuning with depth-aligned images, which are more visually diverse than CAD-rendered images. The effectiveness of OpenDlign is validated on various 3D zero-shot and few-shot tasks. We also show that depth-aligned images consistently enhance the performance of existing 3D open-world methods. Future work will explore the application of depth-aligned images in designing open-world models for 3D scenes (See Appendix A.1). ", "page_idx": 9}, {"type": "text", "text": "Limitations. Due to limited computational resources, we cannot generate depth-aligned images from the largest 3D dataset [23], containing around 1 million 3D objects. Retraining 3D open-world models with billions of parameters using these images is also too expensive. Moreover, a data flitering strategy is needed to remove low-quality depth-aligned images (See details in Appendix A.6). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We extend our gratitude to Dylan Auty, Maojun Zhang, Ranran Huang, Jiangnan Ye, Ziyang Chen, and Chengzu Li for their valuable discussions and insightful feedback on the early drafts of this work. This research was supported by the Imperial College President\u2019s PhD Scholarships 2023. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652\u2013660, 2017.   \n[2] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet $^{++}$ : Deep hierarchical feature learning on point sets in a metric space,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[3] L. Tchapmi, C. Choy, I. Armeni, J. Gwak, and S. Savarese, \u201cSegcloud: Semantic segmentation of 3d point clouds,\u201d in 2017 international conference on 3D vision (3DV), pp. 537\u2013547, IEEE, 2017.   \n[4] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, \u201cNerf: Representing scenes as neural radiance fields for view synthesis,\u201d Communications of the ACM, vol. 65, no. 1, pp. 99\u2013106, 2021.   \n[5] J. Jing, Y. Mao, and K. Mikolajczyk, \u201cMatch-stereo-videos: Bidirectional alignment for consistent dynamic stereo matching,\u201d arXiv preprint arXiv:2403.10755, 2024.   \n[6] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese, \u201c3d semantic parsing of large-scale indoor spaces,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1534\u20131543, 2016.   \n[7] T. Vu, K. Kim, T. M. Luu, T. Nguyen, and C. D. Yoo, \u201cSoftgroup for 3d instance segmentation on point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2708\u20132717, 2022.   \n[8] Y. Zeng, Y. Hu, S. Liu, J. Ye, Y. Han, X. Li, and N. Sun, \u201cRt3d: Real-time 3-d vehicle detection in lidar point cloud for autonomous driving,\u201d IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3434\u20133440, 2018.   \n[9] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 16259\u201316268, 2021.   \n[10] B. Li, T. Zhang, and T. Xia, \u201cVehicle detection from 3d lidar using fully convolutional network,\u201d arXiv preprint arXiv:1608.07916, 2016.   \n[11] I. Misra, R. Girdhar, and A. Joulin, \u201cAn end-to-end transformer model for 3d object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2906\u20132917, 2021.   \n[12] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International conference on machine learning, pp. 8748\u20138763, PMLR, 2021.   \n[13] X. Zhu, R. Zhang, B. He, Z. Guo, Z. Zeng, Z. Qin, S. Zhang, and P. Gao, \u201cPointclip v2: Prompting clip and gpt for powerful 3d open-world learning,\u201d 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2639\u20132650, 2022.   \n[14] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. J. Qiao, P. Gao, and H. Li, \u201cPointclip: Point cloud understanding by clip,\u201d 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8542\u20138552, 2021.   \n[15] T. Huang, B. Dong, Y. Yang, X. Huang, R. W. H. Lau, W. Ouyang, and W. Zuo, \u201cClip2point: Transfer clip to point cloud classification with image-depth pre-training,\u201d 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 22100\u201322110, 2022.   \n[16] D. Hegde, J. M. J. Valanarasu, and V. M. Patel, \u201cClip goes 3d: Leveraging prompt tuning for language grounded 3d recognition,\u201d 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pp. 2020\u20132030, 2023.   \n[17] L. Xue, M. Gao, C. Xing, R. Mart\u00edn-Mart\u00edn, J. Wu, C. Xiong, R. Xu, J. C. Niebles, and S. Savarese, \u201cUlip: Learning a unified representation of language, images, and point clouds for 3d understanding,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1179\u20131189, 2023.   \n[18] M. Liu, R. Shi, K. Kuang, Y. Zhu, X. Li, S. Han, H. Cai, F. M. Porikli, and H. Su, \u201cOpenshape: Scaling up 3d shape representation towards open-world understanding,\u201d ArXiv, vol. abs/2305.10764, 2023.   \n[19] J. Zhou, J. Wang, B. Ma, Y.-S. Liu, T. Huang, and X. Wang, \u201cUni3d: Exploring unified 3d representation at scale,\u201d ArXiv, vol. abs/2310.06773, 2023.   \n[20] Z. Zhang, S. Cao, and Y.-X. Wang, \u201cTamm: Triadapter multi-modal learning for 3d shape understanding,\u201d arXiv preprint arXiv:2402.18490, 2024.   \n[21] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q.-X. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, \u201cShapenet: An information-rich 3d model repository,\u201d ArXiv, vol. abs/1512.03012, 2015.   \n[22] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente, T. Dideriksen, H. Arora, et al., \u201cAbo: Dataset and benchmarks for real-world 3d object understanding,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 21126\u201321136, 2022.   \n[23] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, \u201cObjaverse: A universe of annotated 3d objects,\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13142\u201313153, 2022.   \n[24] A. Fang, A. M. Jose, A. Jain, L. Schmidt, A. Toshev, and V. Shankar, \u201cData flitering networks,\u201d ArXiv, vol. abs/2309.17425, 2023.   \n[25] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al., \u201cLaion-5b: An open large-scale dataset for training next generation image-text models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 25278\u201325294, 2022.   \n[26] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, \u201cOpen-vocabulary object detection via vision and language knowledge distillation,\u201d in International Conference on Learning Representations, 2021.   \n[27] X. Zhou, R. Girdhar, A. Joulin, P. Krahenbuhl, and I. Misra, \u201cDetecting twenty-thousand classes using image-level supervision,\u201d ArXiv, vol. abs/2201.02605, 2022.   \n[28] X. Dong, Y. Zheng, J. Bao, T. Zhang, D. Chen, H. Yang, M. Zeng, W. Zhang, L. Yuan, D. Chen, F. Wen, and N. Yu, \u201cMaskclip: Masked self-distillation advances contrastive language-image pretraining,\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10995\u201311005, 2022.   \n[29] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu, \u201cOpenvocabulary semantic segmentation with mask-adapted clip,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7061\u20137070, 2023.   \n[30] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, \u201cLanguage models are few-shot learners,\u201d ArXiv, vol. abs/2005.14165, 2020.   \n[31] M. McCloskey and N. J. Cohen, \u201cCatastrophic interference in connectionist networks: The sequential learning problem,\u201d Psychology of Learning and Motivation, vol. 24, pp. 109\u2013165, 1989.   \n[32] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou, \u201cOvercoming catastrophic forgetting with hard attention to the task,\u201d in International Conference on Machine Learning, 2018.   \n[33] J. Schwarz, W. M. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell, \u201cProgress & compress: A scalable framework for continual learning,\u201d ArXiv, vol. abs/1805.06370, 2018.   \n[34] D. Isele and A. Cosgun, \u201cSelective experience replay for lifelong learning,\u201d ArXiv, vol. abs/1802.10269, 2018.   \n[35] S.-A. Rebuff,i A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201cicarl: Incremental classifier and representation learning,\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5533\u20135542, 2016.   \n[36] S. Yan, J. Xie, and X. He, \u201cDer: Dynamically expandable representation for class incremental learning,\u201d 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3013\u20133022, 2021.   \n[37] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, \u201cLifelong learning with dynamically expandable networks,\u201d ArXiv, vol. abs/1708.01547, 2017.   \n[38] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, \u201cOvercoming catastrophic forgetting by incremental moment matching,\u201d ArXiv, vol. abs/1703.08475, 2017.   \n[39] Y. Ding, L. Liu, C. Tian, J. Yang, and H. Ding, \u201cDon\u2019t stop learning: Towards continual learning for the clip model,\u201d ArXiv, vol. abs/2207.09248, 2022.   \n[40] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding conditional control to text-to-image diffusion models,\u201d 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3813\u20133824, 2023.   \n[41] S. Cho, H. Shin, S. Hong, S. An, S. Lee, A. Arnab, P. H. Seo, and S. Kim, \u201cCat-seg: Cost aggregation for open-vocabulary semantic segmentation,\u201d arXiv preprint arXiv:2303.11797, 2023.   \n[42] S. Arora, H. Khandeparkar, M. Khodak, O. Plevrakis, and N. Saunshi, \u201cA theoretical analysis of contrastive unsupervised representation learning,\u201d arXiv preprint arXiv:1902.09229, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[43] W. Huang, M. Yi, X. Zhao, and Z. Jiang, \u201cTowards the generalization of contrastive selfsupervised learning,\u201d arXiv preprint arXiv:2111.00743, 2021. ", "page_idx": 12}, {"type": "text", "text": "[44] pharmapsychotic, \u201cClip interrogator.\u201d https://github.com/pharmapsychotic/ clip-interrogator, 2022.   \n[45] J. Li, D. Li, C. Xiong, and S. Hoi, \u201cBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,\u201d in International conference on machine learning, pp. 12888\u201312900, PMLR, 2022.   \n[46] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, \u201c3d shapenets: A deep representation for volumetric shapes,\u201d 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1912\u20131920, 2014.   \n[47] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, \u201cRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 1588\u20131597, 2019.   \n[48] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu, \u201cOmniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation,\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 803\u2013814, 2023.   \n[49] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao, \u201c3d-future: 3d furniture shape with texture,\u201d International Journal of Computer Vision, vol. 129, pp. 3313\u20133337, 2021.   \n[50] N. Mu, A. Kirillov, D. Wagner, and S. Xie, \u201cSlip: Self-supervision meets language-image pre-training,\u201d in European Conference on Computer Vision, pp. 529\u2013544, Springer, 2022.   \n[51] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2432\u20132443, 2017.   \n[52] Y. Lu, C. Xu, X. Wei, X. Xie, M. Tomizuka, K. Keutzer, and S. Zhang, \u201cOpen-vocabulary pointcloud object detection without 3d annotation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1190\u20131199, 2023.   \n[53] Y. Cao, Z. Yihan, H. Xu, and D. Xu, \u201cCoda: Collaborative novel box discovery and cross-modal alignment for open-vocabulary 3d object detection,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024. ", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Broader Impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The broader impact of OpenDlign, a novel 3D open-world framework, includes both potential beneftis and negative impacts associated with its deployment and use. Some considerations are unique to OpenDlign due to its requirement to generate multi-view depth-aligned images, while others are similar to those of existing 3D open-world methods like OpenShape [18], and TAMM [20]. ", "page_idx": 12}, {"type": "text", "text": "Positive Impacts. While OpenDlign\u2019s primary focus is on 3D shape understanding, it shows significant potential for 3D scene understanding. Existing 3D open-world methods are limited by relying on CAD-rendered images for alignment, as CAD models are typically available only for individual objects, not entire scenes. In contrast, OpenDlign generates depth-aligned images using depth maps instead of CAD models. These depth maps can be produced from point clouds obtained either by sampling from CAD models or using real-scanned point clouds collected via LiDAR. Using real-scanned point clouds allows OpenDlign to have richly textured scene images for multimodal alignment, facilitating the learning of robust 3D scene representations. ", "page_idx": 12}, {"type": "text", "text": "Negative Impacts. Biases present in CLIP can be transferred to OpenDlign, potentially resulting in biased outcomes or unfair 3D representations of diverse content. Additionally, the generated depth-aligned images may exhibit biased and stereotypical traits due to inherent biases in the training data of ControlNet. OpenDlign requires generating a large number of multi-view depth-aligned images for multimodal alignment using a diffusion model, raising concerns about energy consumption. Furthermore, if a larger CLIP backbone is released in the future, the increasing fine-tuning costs of OpenDlign could exacerbate these energy concerns. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "In summary, OpenDlign shares common concerns with existing 3D open-world frameworks. However, its positive impacts are unique and hold significant potential to transform the design of future 3D open-world understanding pipelines. ", "page_idx": 13}, {"type": "text", "text": "A.2 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Training Details. OpenDlign was implemented in PyTorch, utilizing the image-text encoders from OpenCLIP-ViT-H-14, pre-trained on the DFN5B dataset [24]. OpenDlign uses contour-aware projections to transform point clouds into depth maps with dimensions of $224\\times224\\times3$ , creating 10 views along the $\\mathbf{X}$ -axis, ranging from 30 to 330 degrees at intervals of 30 degrees between each pair of views. The multimodal alignment was achieved by fine-tuning 10 epochs on an A100-80 GB GPU, employing the AdamW optimizer and the OneCycle scheduler with a peak learning rate of $3\\times10^{-4}$ and a batch size of 128. Since we precache the text and image CLIP embeddings of all shapes, training is significantly accelerated, achieving convergence in about 10 hours. ", "page_idx": 13}, {"type": "text", "text": "Depth-Aligned Image Generation Details. We employed the ControlNet v1.1 model [40], pretrained on depth maps, with settings of 20 DDIM steps, a 9.0 guidance scale, and a 1.0 control strength for generating depth-aligned images. The prompts for generating high-quality, noise-free depth-aligned images are detailed in Table 6. We discovered that ControlNet [40] is more sensitive to jagged edges in the conditional depth map than Vision-Language Models. Our contour-aware projection reduces these jagged edges using bilateral filtering in each depth channel. However, the edges may reappear during the channel squeezing step. To combat this, we applied a $7\\times7$ kernel median fliter to each projected depth map to further diminish the edges. The output images, initially sized at $256\\times256\\times3$ , were then downsampled to match the depth maps\u2019 dimensions. The entire generation process for ShapeNet [21] spanned 16 days on 8 RTX 6000 GPUs. ", "page_idx": 13}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/132aecad33309577f7189a1011f08b2171255f977b96d26ea4198bf76be9dda7.jpg", "table_caption": ["Table 6: Main, positive, and negative prompts guide depth-aligned image generation. Metadata textually describes the semantic information of the 3D data. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Discussion ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "How to ensure multi-view geometry consistency in depth-aligned images? There is no geometric inconsistency because depth-aligned images are generated view by view, each paired with its respective depth map. ControlNet\u2019s [40] conditional control ensures the generated images maintain the same shape and pose as the input depth map, while a text prompt preserves object identity. Since these depth maps originate from the same 3D point cloud, the resulting images remain geometrically consistent across views. Fig. 5 visually demonstrates the generated images are geometry-consistent. ", "page_idx": 13}, {"type": "text", "text": "Why does increasing texture diversity in multi-view images positively impact open-world 3D learning? First, more effective knowledge transfer is achieved by leveraging inconsistent textures in multi-view generated images, allowing depth maps to align with diverse sets of RGB images, which enhances the transfer of rich 2D knowledge embedded in CLIP to 3D representation learning. Second, generating geometry-consistent but texture-inconsistent multi-view images benefits 3D representation learning by focusing on invariant features like object shape, size, and contour, which remain robust to texture variations. Third, similar to previous methods [15, 17, 18], OpenDlign pairs a single-view depth map with a single-view RGB image for contrastive learning, treating each view independently and eliminating the need for texture consistency across views. Lastly, texture features are generally unreliable in both 3D and 2D data, as shown by the common use of color jittering for data augmentation in 2D contrastive learning. Increasing texture diversity is a standard approach to improve the robustness of contrastive learning models. ", "page_idx": 13}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/865ee7bc569ee2499fef18563c900432b286c2686f0972e96aae8380adf133b1.jpg", "img_caption": ["Figure 5: Examples of multi-view depth maps and their corresponding depth-aligned images. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.4 New Asset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our training and evaluation code, along with the depth-specific text prompts, are included in the supplementary material. The generated depth-aligned images and the processed benchmark dataset will be made publicly available or provided to reviewers upon request for this submission. ", "page_idx": 14}, {"type": "text", "text": "A.5 Additional Quantitative Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "More Results for Zero-Shot Classification. Table 7 presents the zero-shot classification results of OpenDlign on the long-tail Objaverse-LVIS [23] dataset. Remarkably, even the lightest OpenDlign variant (OpenDlign-B32) outperforms the previous state-of-the-art method by $8.3\\%$ in Top1 accuracy. ", "page_idx": 14}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/30446e953a0b3aa529c315e98910396b15f444762e7a25fd049632c7d745bf0c.jpg", "table_caption": ["Table 7: Zero-shot classification results on Objaverse-LVIS [23]. The best-performing results are presented in bold, while the second-best results are underlined. Our models are highlighted in blue . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "More Results for Few-Shot Classification. Table 8 presents the few-shot classification results of OpenDlign on the long-tail Objaverse-LVIS [23] dataset. OpenDlign consistently outperforms all baselines from 1-shot to 16-shot scenarios. Surprisingly, although OpenShape demonstrates higher zero-shot classification performance than ULIP, their few-shot results are poorer, indicating potential overfitting during multimodal alignment. ", "page_idx": 14}, {"type": "text", "text": "More Results for Zero-Shot 3D Object Detection. We followed the setting in [52] to compare our method with two 3D open-world approaches, focusing on object detection tasks OV-3DET [52] and CoDA [53] on the Scannet [51] dataset, as shown in Table 9. The results demonstrate that OpenDlign performs comparably to the state-of-the-art CoDA method, which is promising given that OpenDlign is not specifically designed for open-world 3D detection. ", "page_idx": 14}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/25aef3dfb03122e6e2c2c5b3ec27f68c4bc52ef413830ab3ea04b2e46b63b663.jpg", "table_caption": ["Table 8: Few-shot classification results on Objaverse-LVIS [23] "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/c2053d04761f77aceb5f6341702d6ab80e0364db2e74e15d3d09195b578eea88.jpg", "table_caption": ["Table 9: Comparison of 3D open-vocabulary object detection methods in the same setting as OV3DET on ScanNet. \u2019Mean\u2019 represents the average precision across all 20 categories. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Ablation Study on Fine-Tuning Strategies. Table 10 demonstrates that fine-tuning only attention layers of 1 transformer block is adequate for OpenDlign to learn effective 3D representation. Moreover, placing the trainable block for fine-tuning in an independent branch (residual tuning) is crucial, as it preserves the strong capability of pre-trained CLIP in extracting robust image representations from depth-aligned images. ", "page_idx": 15}, {"type": "text", "text": "Table 10: Ablation study on various fine-tuning strategies. \u2018Residual Tuning\u2019 decides whether to tune the transformer blocks connected residually to the CLIP encoder. \u2018Trainable Copy\u2019 initializes the block randomly or with CLIP parameters. \u2018Block Count\u2019 is the number of trainable blocks. ", "page_idx": 15}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/31a8aa748e80ce17ea857653452bb72d4c77e394d432451e5fa912296add2d23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/b3ac020610a9431429dc401b4fabeed2ba35e865e7154688e509be2f1dbfdb4b.jpg", "img_caption": ["Figure 6: Effect of view count on OpenDlign\u2019s zero-shot performance on ScanObjectNN [47] "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "More Results for Varying Number of Views. Fig. 6 demonstrates that OpenDlign\u2019s zero-shot classification performance on the ScanObjectNN dataset generally improves as the number of views increases. Notably, OpenDlign surpasses the leading baseline model, TAMM-PointBERT [20], with approximately two views of depth maps. Table 11 presents detailed results of OpenDlign\u2019s performance on 16-shot classification with varying view counts across three benchmark datasets. These results further confirm that OpenDlign\u2019s few-shot performance, like its zero-shot performance, consistently improves with an increasing number of views. ", "page_idx": 15}, {"type": "table", "img_path": "IGCaTQ4n1R/tmp/6c6195d67aa21171a045c8ee3f46f9ac97cc974172d0094e04ded9281b4d0f88.jpg", "table_caption": ["Table 11: Effect of the number of viewpoints for 16-shot classification. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.6 Additional Qualitative Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Effect of Contour-Aware Projection. Fig. 7 visually compares depth maps generated using PointCLIP V2\u2019s projection [13] technique and our contour-aware projection method. In the PointCLIP V2 projection, the projected maps are very blurry, losing some contour and shape details, as evidenced by the bed frame gaps being inaccurately fliled. Conversely, our contour-aware projection preserves more of the original objects\u2019 contours and structures, accurately showing the bed frame gaps. ", "page_idx": 16}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/32692ca70096599312d63a606d929c0d4042c7f49b5933e1f1d8af58593061d4.jpg", "img_caption": ["Figure 7: Comparison between depth maps from PointCLIP V2 [13] and contour-aware projection. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "More Visualizations of Cross-Modal Retrieval. More examples of 3D shape retrieval that showcase the cross-modal retrieval capabilities of OpenDlign are illustrated in Fig. 8. The results demonstrate that the 3D shapes retrieved are in semantic alignment with the image and text queries. ", "page_idx": 16}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/7c6caf51f91f099550828a0e3e18c0cae53497e5169b08249075fa7334831dd0.jpg", "img_caption": ["Figure 8: Additional 3D shape retrieval results. (a) Most similar 3D shapes for each image query. (b) Two most similar 3D shapes for each text query. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "IGCaTQ4n1R/tmp/76ed5f9ce6faf1163611e74ad5318c17be280ac36577c53cbf2ad01caf5b73a3.jpg", "img_caption": ["Figure 9: Examples of low-quality generated depth-aligned images. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Examples of Low-Quality Depth-Aligned Images. Fig. 9 showcases bad examples of depth-aligned images. For instance, the diffusion model may interpret \u2018monitor\u2019 as a person observing something, or it might create unrealistic images such as a washing machine made of stone. Additionally, it might depict a tent floating in the sky, which deviates from real-world expectations. These examples underscore the need to eliminate inferior depth-aligned images for more generalized alignment. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We clearly highlight our main contributions and the scope of this paper in both the abstract and the concluding paragraphs of the introduction. Notably, we outline the paper\u2019s contributions using a bullet-point list for clarity. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We have included a dedicated \"Limitations\" section in our paper. For detailed information on the limitations of our work, please refer to Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: See the methodology section 3. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All essential components of the OpenDlign architecture are described in the Methodology section 3. The experimental setup for downstream tasks, including zero-shot classification, object detection, and 3D shape retrieval, is detailed in the Experiment section 4. Additionally, Appendix A.2 provides the implementation, training, and image generation details, ensuring the reproducibility of our results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The supplementary material includes the complete code for image generation, model training, and evaluation. All evaluation datasets and depth-aligned image datasets are open-sourced. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The testing details for various downstream tasks, including zero-shot classification, few-shot classification, object detection, and cross-modal retrieval, are illustrated in Experiment section 4. The paper specifies all the training details, including hyperparameter selection, training resources, and optimizer, in Appendix A.2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Error bars are not reported due to the computational expense. However, for the few-shot classification analysis, our results are averaged over 10 random seeds. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Appendix A.2, we specified that OpenDlign model training was conducted using a single A100-80 GB GPU, which took 10 hours. Depth-aligned image generation was performed using 8 RTX 6000 GPUs around 16 days. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics, ensuring data integrity, transparency, and reproducibility. Both the manuscript and supplementary material follow the code of ethics and remain anonymous until camera-ready. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The broader impacts, both positive and negative, are discussed in Appendix A.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of diffusion models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not explicitly pose any risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All assets used in this paper, including code, pretraining and evaluation datasets, and pre-trained models, are properly credited to their original creators or owners. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The code for this paper is well-structured with clear usage guidance. All assets have been anonymized. The dataset and checkpoint models will be released upon the paper\u2019s acceptance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]