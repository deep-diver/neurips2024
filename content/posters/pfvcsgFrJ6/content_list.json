[{"type": "text", "text": "On Causal Discovery in the Presence of Deterministic Relations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Loka ${\\bf L i}^{1*}$ , Haoyue $\\bf{D a i^{2*}}$ , Hanin Al Ghothani1, Biwei Huang3, Jiji Zhang4, Shahar Harel5, Isaac Bentwich5, Guangyi Chen1,2, Kun Zhang1,2 ", "page_idx": 0}, {"type": "text", "text": "1 Mohamed bin Zayed University of Artificial Intelligence 2 Carnegie Mellon University, 3 University of California San Diego 4 The Chinese University of Hong Kong, 5 Quris AI {longkang.li, kun.zhang}@mbzuai.ac.ae ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many causal discovery methods typically rely on the assumption of independent noise, yet real-life situations often involve deterministic relationships. In these cases, observed variables are represented as deterministic functions of their parental variables without noise. When determinism is present, constraint-based methods encounter challenges due to the violation of the faithfulness assumption. In this paper, we find, supported by both theoretical analysis and empirical evidence, that score-based methods with exact search can naturally address the issues of deterministic relations under rather mild assumptions. Nonetheless, exact scorebased methods can be computationally expensive. To enhance the efficiency and scalability, we develop a novel framework for causal discovery that can detect and handle deterministic relations, called Determinism-aware Greedy Equivalent Search (DGES). DGES comprises three phases: (1) identify minimal deterministic clusters (i.e., a minimal set of variables with deterministic relationships), (2) run modified Greedy Equivalent Search (GES) to obtain an initial graph, and (3) perform exact search exclusively on the deterministic cluster and its neighbors. The proposed DGES accommodates both linear and nonlinear causal relationships, as well as both continuous and discrete data types. Furthermore, we investigate the identifiability conditions of DGES. We conducted extensive experiments on both simulated and real-world datasets to show the efficacy of our proposed method. The code is available at https://github.com/lokali/DGES.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal discovery from observational data has attracted considerable attention in recent decades and has been widely applied in various fields such as machine learning [1], healthcare [2], manufacturing [3] and neuroscience [4]. Most causal discovery methods operate under the assumption of independent noises in the probabilistic system. However, real-world scenarios frequently encounter deterministic relationships. For example, the body mass index (BMI) is defined as the weight divided by the square of the body height, composing a deterministic relation among weight, height, and BMI. ", "page_idx": 0}, {"type": "text", "text": "Constraint-based and score-based methods are two primary categories in causal discovery. Constraintbased methods, such as PC [5] and FCI [6], leverage conditional independence tests (CIT) to estimate the graph skeleton and then determine the orientation. Under the Markov and faithfulness assumptions [7], these methods are guaranteed to asymptotically output the true Markov equivalence class (MEC). However, the faithfulness assumption is sensitive to many factors, such as the statistical errors with finite samples. Moreover, in the presence of deterministic relations, the faithfulness assumption is always violated. Take the chain structure $X\\rightarrow Y\\rightarrow Z$ for example where $Y=f(X)$ . In this case, faithfulness is violated due to the conditional independence $Z\\ {\\overset{\\cdot}{\\perp}}\\ Y|X$ , i.e., when $X$ is given, $Y$ degenerates to a constant that is independent to any variables. Several variants of constraint-based methods [8, 9] have been proposed to accommodate certain types of unfaithfulness. However, they generally provide practical flexibility but do not guarantee the identification to the true MEC. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For score-based methods, the approach can vary based on the search strategy, which may involve greedy search, exact search, or continuous optimization. One typical score-based method with greedy search is Greedy Equivalent Search (GES) [10], which searches in the space of MECs greedily by maximizing a well-defined score, such as Bayesian information criterion (BIC) score [11]. Specifically, GES starts with an empty graph and consists of two phases. In the forward phase, it incrementally adds one edge at a time if it yields the maximum score improvement, continuing until no further edge can be added to enhance the score. In the backward phase, it checks all edges to eliminate some if removal further improves the score. Similar to the aforementioned constraint-based methods, GES converges to the true MEC in the large sample limit. ", "page_idx": 1}, {"type": "text", "text": "Some exact score-based methods aim at weakening the faithfulness assumption required for asymptotic correctness of the search results, such as dynamic programming (DP) [12, 13], $\\mathrm{A^{*}}$ [14, 15], and integer programming [16, 17]. The DAGs estimated by these methods can be converted to their MECs for causal interpretation [18]. Lu et al. [19] demonstrated that these exact methods may produce correct results in cases where methods relying on faithfulness fail. Furthermore, $\\mathrm{Ng}$ et al. [20] proved that exact score-based search with BIC can asymptotically outputs the true MEC when the sparsest Markov representation (SMR) assumption [21] is satisfied. Note that the SMR assumption is strictly weaker than the faithfulness assumption. ", "page_idx": 1}, {"type": "text", "text": "Deterministic relations have been considered in a few works of causal discovery. D-separation condition [7] is proposed for graphically determining conditional independence. Glymour [22] proposed a heuristic procedure to learn the causal graph in a deterministic system, called DPC, where only a subset of variables will be conditioned in testing conditional independence. Daniusis et al. [23] and Janzing et al. [24] considered a deterministic system with only two variables, and presented the idea of independent changes to infer the causal direction. Luo [25] and [26] incorporated the classical PC algorithm and utilized additional independence tests to handle determinism. Mabrouk et al. [27] combined a constraint-based approach with a greedy search that included specific rules to deterministic nodes and significantly reduce the incorrect learning. However, there is no identifiability guarantee in those related works. Moreover, Zeng et al. [28] assumes nonlinear additive noise model under high-dimensional deterministic data while Yang et al. [29] assumes linear non-Gaussian model. Different from them, this paper aims to provide a principled framework to handle deterministic relations for arbitrary functional models. More related works are given in Appendix A2. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Firstly, we find that exact score-based methods can naturally be used to address the issues of deterministic relations when mild assumptions are fulfilled. Secondly, due to the large search space of the possible DAGs, the exact score-based methods are feasible only for small graphs and can be inefficient for large graphs. To enhance the efficiency and scalability, we propose a novel framework called Determinism-aware Greedy Equivalent Search (DGES), aimed at enhancing the efficiency and scalability to handle deterministic relations. Importantly, DGES is a general threephase method, with no restricted assumption on the underlying functional causal models, i.e., it can accommodate both linear and nonlinear relationships, Gaussian and non-Gaussian data distributions, as well as continuous and discrete data types. Thirdly, we provide the identifiability conditions of DGES under general functional models. Last but not least, we conducted extensive experiments on both simulated and real-world datasets to validate our theoretical findings and show the efficacy of our proposed method. ", "page_idx": 1}, {"type": "text", "text": "Paper organization. In Section 2, we review the common assumptions, provide a motivating example why PC fails in dealing with deterministic relations, then present our intuitive solution using exact score-based method. In Section 3, we present our proposed DGES with three phases in details. Furthermore, we provide the identifiability conditions for DGES presented in a general form in Section 4. The empirical studies in Section 5 validate our theoretical results and show the efficacy of our method. Finally, we conclude our work with further discussions in Section 6. ", "page_idx": 1}, {"type": "text", "text": "2 Causal Discovery with Deterministic Relations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first review the preliminaries of causal discovery, especially with deterministic relations, and then we provide some common assumptions that are related to our further analysis, as presented in section 2.1. Furthermore, we display two scenarios with deterministic relations where faithfulness can be violated in section 2.2, explaining why using constraint-based methods such as the PC algorithm can be problematic in addressing deterministic issues. Lastly, we provide an intuitive solution to handle the deterministic issues by exact score-based methods, as shown in section 2.3. ", "page_idx": 2}, {"type": "text", "text": "2.1 Causal Discovery and Common Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{G}=(V,E)$ be a DAG with the vertex set $V$ and edge set $\\boldsymbol{E}$ . Consider $d$ observable variables denoted by $\\boldsymbol{V}=(V_{1},V_{2},...,V_{d})$ , and denote $\\mathbb{P}$ as its probability distribution. From a statistical view, $X\\perp Y|Z$ denotes that $X$ and $Y$ are conditionally independent given $Z$ . Moreover, from a graph view, $X\\perp_{\\ensuremath{\\mathbb{L}}_{d}}Y|Z$ denotes that $X$ and $Y$ are ${\\mathrm d}$ -separated by $Z$ . Given $n$ data samples, the task of causal discovery aims at recovering the causal graph $\\mathcal{G}$ from the data matrix $V\\in\\mathbb{R}^{n\\times d}$ . Usually, each variable $V_{i}\\in V$ with random noises can be represented by the following structural causal model (SCM): $V_{i}=f_{i}(\\mathrm{PA}_{i},\\epsilon_{i})$ , where $\\mathrm{PA}_{i}$ is the set of all direct causes of $V_{i}$ , and $\\epsilon_{i}$ is the random noise with non-zero variance related to $V_{i}$ , and we assume that $\\epsilon_{i}$ \u2019s are mutually independent. For variables with deterministic relations, the SCM becomes: $V_{i}=f_{i}(\\mathrm{PA}_{i})$ , where there is no extra noise. The relation can also be denoted as $\\mathrm{PA}_{i}\\mapsto V_{i}$ , where $\\mapsto$ is the deterministic function mapping, showing $\\mathrm{PA}_{i}$ determines $V_{i}$ . Throughout this paper, we assume causal sufficiency, i.e., no latent confounder. ", "page_idx": 2}, {"type": "text", "text": "Terminologies. Consider Figure 1(a) as an example, where $V_{3}$ has deterministic relation with $V_{1}$ and $V_{2}$ , i.e., $V_{3}=V_{1}+V_{2}$ , and $V_{4}$ is a non-deterministic variable. Here we call the set of deterministic variables as a deterministic cluster $(D C)$ , e.g., $\\left\\{V_{1},V_{2},V_{3}\\right\\}$ . Accordingly, all the non-deterministic variables make up a non-deterministic cluster $(N D C)$ , e.g., $\\{V_{4}\\}$ . Meanwhile, the edges connecting between DC and NDC compose a bridge set $(B S)$ , e.g., $\\{V_{2}\\bar{\\rightarrow}\\bar{V_{4}},V_{3}{\\rightarrow}V_{4}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Markov) Given a DAG $\\mathcal{G}$ and the distribution $\\mathbb{P}$ over the variable set $V$ , each variable is probabilistically independent of its non-descendants given its parents in $\\mathcal{G}$ . ", "page_idx": 2}, {"type": "text", "text": "There are many DAGs that induce the same conditional independence relations with the distribution $\\mathbb{P}$ , and it is said to be Markov equivalent. The Markov equivalent class (MEC) contains all the DAGs which entail the same conditional independence relations as $\\mathcal{G}$ does. ", "page_idx": 2}, {"type": "text", "text": "Another widely used assumption is faithfulness [7]. It states that any conditional independence that holds in the probability distribution must correspond to a dseparation in the causal graph. When the Markov and faithfulness assumptions hold true, constraint-based methods, such as PC, have been proven to output the correct MEC asymptotically. However, in the finite sample regime, the faithfulness assumption is sensitive to statistical testing errors when inferring the CI relations, and the violations might occur often. When there are deterministic relations, faithfulness also fails. Glymour [22] proposes the non-deterministic faithfulness regarding only non-deterministic variables. Moreover, relaxations of faithfulness have been proposed, such as adjacency-faithfulness [8] and triangle-faithfulness [9]. Another strictly weaker assumption is called Sparsest Markov Representation ", "page_idx": 2}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/c2d4a03f60b4234713b67314076df358b85f116eb79d62a398c07cae92913ef6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Two examples of causal graphs where faithfulness is violated. The gray nodes are deterministic variables. (a) $\\{\\bar{V}_{1},\\dot{V}_{2}\\}\\mapsto$ $V_{3}$ . Violation reason is $V_{4}$ \u22a5\u22a5 $V_{3}|\\{V_{1},V_{2}\\}$ but $V_{4}\\not\\perp\\!\\!\\!\\!/_{d}V_{3}|\\{V_{1},V_{2}\\}$ . (b) $V_{1}\\mapsto V_{2}$ . Violation reason is $V_{3}\\bot V_{4}|V_{1}$ but $V_{3}$ \u0338\u22a5\u22a5 $._{d}\\mathit{V}_{4}|V_{1}$ . ", "page_idx": 2}, {"type": "text", "text": "(SMR) [21], which is also known as the unique-frugality assumption [30, 31]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (Sparsest Markov Representation (SMR) [21]) Given a DAG $\\mathcal{G}$ and the distribution $\\mathbb{P}$ over the variable set $V$ , the MEC of $\\mathcal{G}$ is the unique sparsest MEC which satisfies the Markov assumption. ", "page_idx": 2}, {"type": "text", "text": "The idea behind SMR is to find the sparsest graphical representation that captures the essential conditional independence relationships in the data. The term \u201csparsest\" refers to the minimal number of edges in the graphical model. Under the SMR assumption, the exact score-based methods, such as $\\mathbf{A}^{*}$ [15] and DP [13], can produce asymptotically correct results for learning the true MEC. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Faithfulness Violation by Deterministic Relations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Glymour [22] pointed out two and only two scenarios in the presence of deterministic relations where faithfulness can be violated. We summarize the two conditions and present the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Non-deterministic Faithfulness [22]) Define a DAG $\\mathcal{G}$ and the distribution $\\mathbb{P}$ over the variable set $V$ . $\\forall X,Y$ and $S$ in $V$ , if $X$ \u22a5\u22a5 $Y|S$ in $\\mathbb{P}$ and none of the following conditions holds: ", "page_idx": 3}, {"type": "text", "text": "i. $S\\mapsto X$ or $S\\mapsto Y$ , $i i.\\ \\exists S^{\\prime}\\,s.t.\\ X\\ \\bot_{d}\\ Y|S^{\\prime}\\,a n d\\,S\\vdash$ $S\\mapsto S^{\\prime}$ , then $X\\perp_{d}Y|S\\,i n\\,{\\mathcal{G}}$ . ", "page_idx": 3}, {"type": "text", "text": "Remarks: It assumes there is no other coincidental independence besides the two conditions. In other words, the two conditions are the only two cases leading to faithfulness violation due to deterministic relations. In fact, this assumption is equivalent to the completeness of D-separation criteria in Spirtes et al. [7]. We will use two graph examples, as shown in Figure 1, to explain the above two conditions. ", "page_idx": 3}, {"type": "text", "text": "Firstly, given condition $(i)$ and Figure 1(a), we can assign $S=\\{V_{1},V_{2}\\}$ and $X=V_{3}$ , where $S\\mapsto X$ . Given $\\bar{\\{V_{1},V_{2}\\}}$ , $V_{3}$ will always be conditionally independent from $V_{4}$ , because $V_{3}$ can be determined by $\\{V_{1},V_{2}\\}$ with no extra noise term, the estimated residue for regressing $V_{3}$ on $\\{V_{1},V_{2}\\}$ will be close to 0. Therefore, $V_{4}\\perp V_{3}|\\{V_{1},V_{2}\\}$ holds true from a statistical view. However, $V_{4}$ \u0338\u22a5\u22a5 $\\,\\,\\hat{\\b{\\mathscr{A}}}\\,V_{3}|\\{V_{1},V_{2}\\}$ from a graph view. Therefore, in this case, faithfulness is violated. ", "page_idx": 3}, {"type": "text", "text": "The key rule of constraint-based method (e.g., PC algorithm) is that if we find at least one conditional set or an empty set so that two variables are conditionally independent, then the edge between these two variables in the graph will be removed. Therefore, we can conclude that using constraint-based methods which rely on faithfulness to deal with deterministic relations can be problematic. ", "page_idx": 3}, {"type": "text", "text": "Secondly, given condition $(i i)$ and Figure 1(b), we can assign $S\\,=\\,V_{1}$ , $S^{\\prime}\\,=\\,V_{2}$ , $X\\,=\\,V_{3}$ and $Y=V_{4}$ , where $S\\mapsto S^{\\prime}$ . From the graph, we can see that $V_{3}\\perp_{d}V_{4}|V_{2}$ and $V_{3}$ \u0338\u22a5\u22a5 $d\\mathrm{~}V_{4}|V_{1}$ . However, from a statistical view $V_{3}\\perp V_{4}|V_{2}$ , since $V_{2}=V_{1}$ , we also have $V_{3}\\perp V_{4}|V_{1}$ . Here, conditional independence does not imply d-separation. Therefore, faithfulness is also violated. ", "page_idx": 3}, {"type": "text", "text": "2.3 Intuitive Solution: Exact Search ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Beneftiing from the recent theoretical progress on exact score-based methods, which do not explicitly rely on faithfulness assumption, it enables us to deal with deterministic relations from an intuitive view. Here, we are inspired by the lemma as follows. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Linear Identifiability of Exact Search [20]) Exact score-based search with BIC score asymptotically outputs a DAG that belongs to the MEC of the true DAG G if and only if the DAG $\\mathcal{G}$ and distribution $\\mathbb{P}$ satisfy the SMR assumption. ", "page_idx": 3}, {"type": "text", "text": "According to Lemma 1, in the linear case, as long as the SMR assumption is satisfied, the exact score-based method with BIC score [11] can asymptotically obtain the true MEC. Then, we can extend the theoretical result from linear to nonlinear scenarios. The exact score-based method with generalized score [32] can also asymptotically output the true MEC. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (General Identifiability of Exact Search) Exact score-based search with generalized score asymptotically outputs a DAG that belongs to the MEC of the true DAG G if and only if the DAG $\\mathcal{G}$ and distribution $\\mathbb{P}$ satisfy the SMR assumption and some mild conditions are satisfied. ", "page_idx": 3}, {"type": "text", "text": "Remarks: The complete proof is given in Appendix A4.1. Based on the theoretical findings in Lemma 1 and Theorem 2, the exact score-based methods, which do not specifically require faithfulness but SMR, pave a promising way to deal with the deterministic relations for causal discovery. However, one critical disadvantage of the exact methods is their low computational efficiency and poor scalability. To that end, we propose a novel framework, called DGES, which is demonstrated in section 3. The identifiability conditions for DGES are provided in section 4. ", "page_idx": 3}, {"type": "table", "img_path": "pfvcsgFrJ6/tmp/9c9980f700e5f409930f589661ee1dba2706ab7b8ee9e3f74e3da2101c1809d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 Determinism-aware Greedy Equivalent Search (DGES) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we will introduce our proposed DGES in detail. Throughout this paper, we consider the general case without assuming any functional causal models. In general, DGES contains three phases: Firstly, we need to detect all the minimal deterministic clusters. If one variable can be deterministically represented by some other variables, we may conclude that it is a deterministic variable. Secondly, based on the DC information, we run modified GES to get the initial causal graph. Thirdly we perform the exact search exclusively on the DC and their neighbors, as post-processing. The general framework is given in Algorithm 1. The contents are organized as follows. The details of deterministic cluster detection in Phase 1 are discussed in section 3.1. More information about our modified GES in Phase 2 is introduced in section 3.2. Finally, we discuss exact search in section 3.3. ", "page_idx": 4}, {"type": "text", "text": "3.1 Minimal Deterministic Clusters Detection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A minimal deterministic cluster (MinDC) refers to a minimal set of variables involved in a deterministic relation. A DC can be seen as a union of all MinDCs in the graph. For example, $V_{1}\\mapsto V_{2}$ and $V_{1}\\mapsto V_{3}$ , then $\\{V_{1},V_{2}\\}$ and $\\{V_{1},V_{3}\\}$ are two MinDCs, while $\\{\\bar{V_{1}},\\bar{V_{2}},V_{3}\\}$ composes a DC. ", "page_idx": 4}, {"type": "text", "text": "First of all, we need to obtain the DC, which contains all the deterministic variables. For each variable $V_{i}$ , $\\textrm{\\,i\\in}\\{1,...,d\\}$ , if this variable can be deterministically represented by all the other variables, i.e., $\\{V\\backslash V_{i}\\}\\mapsto V_{i}$ , then this variable must be in DC. After traversing all $d$ variables, we obtain the DC. ", "page_idx": 4}, {"type": "text", "text": "However, within the DC, there may be multiple deterministic relations, even some overlapping deterministic variables. Therefore, out of the DC, we need to get a set of MinDCs. For each variable $V_{i}$ , we try to detect whether there exists a minimal set $S$ such that $S\\mapsto V_{i}$ , where $V_{i}\\in\\mathrm{DC}$ , $S\\subset\\mathrm{DC}$ and $V_{i}\\notin S$ . Here, we need to traverse all the possible combination sets of DC, and see whether one deterministic variable can be minimally represented by some other variables. If so, then those variables compose a MinDC. In the end, we can obtain a list of MinDCs. More details about DC detection, MinDC detection, and how to check $S\\mapsto V_{i}$ , are given in Appendix A3.1. ", "page_idx": 4}, {"type": "text", "text": "3.2 Modified Greedy Equivalent Search ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The modified GES is based on the standard GES [10]. We add some extra constraints during the forward and backward steps and adjust the score functions due to the deterministic relations. When using score functions for causal discovery, we aim for the underlying causal graph or its equivalent class to give the optimal score. Specifically, we desire that the score of a DAG model (1) increases as the result of adding any edge that eliminates an independence constraint that does not hold in the generative distribution, and (2) decreases as a result of adding any edge that does not eliminate such a constraint. More formally, we have the following definition of score local consistency. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Score Local Consistency [10]) Let $\\mathcal{G}$ be any $D A G$ , and let $\\mathcal{G}^{\\prime}$ be the DAG that results from adding the edge $V_{i}\\to V_{j}$ on $\\mathcal{G}$ . Let $D$ be the dataset from the distribution $\\mathbb{P}.$ . A score function $\\mathbb{S}(\\mathcal{G};D)$ is locally consistent if the following two properties hold as the sample size $n\\to\\infty$ : ", "page_idx": 4}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/4b8105306b63bb70274060af3e77ecae52621e9bcca187e37a3cf6cbd9f9b1b9.jpg", "img_caption": ["Figure 2: An example graph where $V_{1}~=~f(V_{2},V_{3},V_{4})$ . (a) the true graph where $\\mathrm{DC}\\ =$ $\\{\\bar{V}_{1},V_{2},V_{3},V_{4}\\}$ , $\\mathrm{NDC}\\,=\\,\\{\\bar{V}_{5},V_{6},V_{7}\\}$ , and $\\mathrm{BS}\\,=\\,\\{V_{2}{\\rightarrow}\\dot{V}_{5},V_{3}{\\rightarrow}V_{6},V_{4}{\\rightarrow}V_{6}\\}$ . (b) one possible DAG from the estimated CPDAG by GES, where ${\\mathrm{BS}}^{\\prime}=\\{V_{2}{\\rightarrow}V_{5},V_{1}{\\rightarrow}V_{6},V_{2}{\\rightarrow}V_{6},V_{4}{\\rightarrow}V_{6}\\}$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Modification 1: Edge Adding and Deleting. During the forward phase, at each step with a DAG $\\mathcal{G}$ in the equivalence class, an edge $V_{i}\\to V_{j}$ is added when 1) $V_{i}\\not\\perp V_{j}|\\operatorname{PA}_{j}^{\\mathcal{G}}$ , and 2) $\\mathrm{\\bar{P}A}_{j}^{\\mathcal{G}}$ does not determine any of $V_{i},V_{j}$ , until no edge can be added. However, when $\\mathrm{PA}_{j}^{\\mathcal{G}}$ determines $V_{i}$ or $V_{j}$ , we always have $V_{i}\\perp V_{j}|\\operatorname{PA}_{j}^{\\mathcal{G}}$ . In this case, we always ignore such independence, directly regard it as dependent, and add such an edge to the graph. The motivation behind the modification is to ensure that no false independence due to deterministic relations is introduced, and in the end, the output graph is guaranteed to be Markovian. ", "page_idx": 5}, {"type": "text", "text": "During the backward phase, at each step with a DAG $\\mathcal{G}$ in the equivalence class, an edge $V_{i}\\to V_{j}$ is removed when both 1) $V_{i}\\perp V_{j}|\\operatorname{PA}_{j}^{\\mathcal{G}}$ , and 2) $\\mathrm{PA}_{j}^{\\mathcal{G}}$ does not determine any of $V_{i},V_{j}$ , until no edge can be removed. Similar to the modification in the forward phase, when $\\mathrm{PA}_{j}^{\\mathcal{G}}$ determines $V_{i}$ or $V_{j}$ , we still trust the dependency and keep the edge $V_{i}\\to V_{j}$ . Although the resulting equivalence class will be Markovian to the ground truth, redundant edges will exist. ", "page_idx": 5}, {"type": "text", "text": "Fortunately, we have Phase 3 exact search as post-processing, which will be introduced next in Section 3.3. Under the SMR assumption, we can obtain a more sparse graph. In the end, the exact search will remove all those redundant edges. A motivating example showing the advantages of our modified forward and backward phases is provided in Appendix A3.2 and Figure A2. ", "page_idx": 5}, {"type": "text", "text": "Modification 2: Score Function. During the phase 1 with greedy search and phase 3 with exact search, a proper score function is inevitably needed. For any scoring criterion $S(\\mathcal{G},\\mathcal{D})$ , we say that a score is decomposable if it can be written as a sum of local scores, where each local score is a function of only one variable and its parents. Following the property, the score of a DAG $\\mathcal{G}$ can be represented as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{S}(\\mathcal{G};\\mathcal{D})=\\sum_{i=1}^{d}S(V_{i},\\mathrm{PA}_{i}^{\\mathcal{G}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under the linear Gaussian model, the BIC score [11] is preferred, which is given as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{S_{B I C}(V_{i},\\mathrm{PA}_{i}^{\\mathcal{G}})=-\\log L+\\lambda^{\\prime}k\\log n,}}\\\\ {{a n d~\\log L\\propto-\\displaystyle\\frac{n}{2}(1+\\log|\\Sigma|),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L$ is the maximized value of the likelihood function of the model based on the observed data $\\mathcal{D}$ related to $V_{i}$ and $\\mathrm{PA}_{i}$ , $k$ denotes the number of edges between $V_{i}$ and $\\mathrm{PA}_{i}$ in $\\textstyle{\\mathcal{G}},n$ is the number of data samples in $\\mathcal{D}$ , $\\lambda^{\\prime}$ is the penalty parameter, $\\Sigma$ is the variance of the noise term. ", "page_idx": 5}, {"type": "text", "text": "However, in the deterministic scenarios, the estimated noise variance $\\hat{\\Sigma}$ will asymptotically get closer to 0, which leads to numerical error because of the term $\\log\\left|{\\hat{\\Sigma}}\\right|$ . To deal with such an issue, we provide the adjusted BIC score, formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle S^{\\prime}{}_{B I C}(V_{i},\\mathrm{PA}_{i}^{\\mathcal{G}})=-\\log L^{\\prime}+\\lambda^{\\prime}k\\log n,}}\\\\ {{\\displaystyle a n d~\\log L^{\\prime}\\propto-\\frac{n}{2}(1+\\log|\\Sigma+\\xi|),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\xi$ is a small constant, and $\\xi>0$ . ", "page_idx": 5}, {"type": "text", "text": "Under the general nonlinear model, the generalized score (GS) [32] which is in a non-parametric form is favored. There are two types of likelihoods as introduced in the paper, for computational efficiency, we choose the generalized score with cross-validated (CV) likelihood. ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{G S}(V_{i},\\mathrm{PA}_{i}^{\\mathcal{G}})=\\frac{1}{Q}\\sum_{q=1}^{Q}\\ell(F_{i}^{(q)}|D_{0,i}^{(q)}),\\quad a n d\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\hat{F}_{i}^{(q)}|D_{0,i}^{(q)})=-\\frac{n_{0}^{2}}{2}\\log(2\\pi)-\\frac{n_{0}}{2}\\log\\left|n_{1}\\lambda^{2}\\tilde{K}_{V_{i}}^{1(q)}(\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{1(q)}+n_{1}\\lambda I)^{-2}\\tilde{K}_{V_{i}}^{0(q)}\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad-\\frac{1}{2}\\mathrm{trace}\\{\\frac{1}{\\lambda}\\tilde{K}_{V_{i}}^{0(q)}\\tilde{K}_{V_{i}}^{0(q)}+\\frac{1}{\\lambda}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{0,1(q)}A_{i}^{\\top}A_{i}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{1,0(q)}-n_{1}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{0,1(q)}A_{i}^{\\top}B_{i}A_{i}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{1,0(q)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+2n_{1}\\tilde{K}_{V_{i}}^{0(q)}B_{i}A_{i}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{1,0(q)}-\\frac{2}{\\lambda}\\tilde{K}_{V_{i}}^{0(q)}A_{i}\\tilde{K}_{P\\bar{A}_{i}^{\\sigma}}^{1,0(q)}-n_{1}\\tilde{K}_{V_{i}}^{0(q)}B_{i}\\tilde{K}_{V_{i}^{\\sigma}}^{0(q)}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $A_{i}\\,=\\,\\tilde{K}_{V_{i}}^{1(q)}(\\tilde{K}_{P A_{i}^{g}}^{1(q)}\\,+\\,n_{1}\\lambda I)^{-1}$ , $B_{i}\\,=\\,A_{i}\\big(I+n_{1}\\lambda A_{i}^{\\mathrm{T}}A_{i}\\big)^{-1}A_{i}^{\\mathrm{T}}$ , $\\lambda$ is the regularization parameter, $n_{1}$ is the sample size of each training set, $n_{0}$ is the sample size of each test set, $n=n_{1}\\!+\\!n_{0}$ , $D_{1,i}^{(q)}$ )and D(q) are the corresponding data of variable $V_{i}$ and its parents, K\u02dcV1(q)denotes the centralized kernel matrix of the q-th training set of Vi, K\u02dcV0i(q) denotes that of the $q_{\\mathrm{~\\,~}}$ -th test set of $V_{i}$ , and similar notations are used for other kernel matrices. ", "page_idx": 6}, {"type": "text", "text": "3.3 Exact Search as Post-processing ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As demonstrated by Lu et al. [19], GES may get sub-optimal results when the faithfulness assumption is violated, e.g., when there are deterministic relations. An example is given in Figure 2. In this example, the DC is $\\{V_{1},V_{2},V_{3},V_{4}\\}$ . The true incoming edges to $V_{6}$ should be $\\{V_{3},V_{4}\\}$ , however, the estimated graph by GES may have $\\left\\{V_{1},V_{2},V_{4}\\right\\}$ pointing to $V_{6}$ . We need to partially conduct an exact search based on the GES result to identify BS, under the SMR assumption. Therefore, in Phase 3, we perform the exact search exclusively on the DC and their neighbors. Beneftiing from the recent theoretical progress on exact score-based methods, which do not explicitly rely on faithfulness assumption, it enables us to deal with deterministic relations from an intuitive view. ", "page_idx": 6}, {"type": "text", "text": "4 Identifiability Conditions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide the identifiability conditions of DGES. The conditions are presented in a general form, applicable to both linear and nonlinear causal models. As mentioned above, in a general deterministic system, the whole causal graph mainly can be divided into three parts: DC, NDC, and BS. In this paper, we focus on the identifiability for the BS and NDC parts. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Partial Identifiability) Denote a causal graph $\\mathcal{G}$ with deterministic relations. Let $V_{i}$ be any non-deterministic variable in $\\mathcal{G}$ , and $\\mathrm{PA}_{i}$ be the set of direct causes or undirected neighbors of $V_{i}$ in one MinDC. Suppose the following conditions hold ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{i i.\\ \\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\mid-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\big|\\cdot\\big|$ denotes the cardinality of a set. Then, when the sample size $n\\to\\infty$ , we can identify the $B S$ and NDC parts of the causal graph $\\mathcal{G}$ to their true Markov equivalent class. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate our theoretical findings and show the efficacy of our method, we conducted extensive experiments on simulated and real-world datasets. Specifically, for simulated datasets, we evaluate both linear and general nonlinear functional models. ", "page_idx": 6}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/86ab56b31c19c39af5d5e280ff35c06f9b55a58150d67f3d00c67edbb71b7119.jpg", "img_caption": ["Figure 3: Results on the simulated datasets with one MinDC. We evaluate different functional causal models on varying number of variables and samples, respectively. For each setting, we consider SHD $\\left(\\downarrow\\right)$ , $F_{1}$ score $(\\uparrow)$ , precision $(\\uparrow)$ , recall (\u2191) and runtime $\\left(\\downarrow\\right)$ as evaluation criteria. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Simulated Datasets. The true DAGs are simulated using the Erd\u00f6s\u2013R\u00e9nyi model [33] with the number of edges equal to the number of variables. We evaluate linear Gaussian model and general nonlinear model with mixed functions, each with varying number of variables and samples. Moreover, we also evaluate general nonlinear model generated by MLP on varying number of variables. For each setting, we randomly choose one MinDC or two MinDCs where each MinDC has at least three variables. For the exact method in Phase 3, we choose $\\mathbf{A}^{*}$ [15] without the heuristic tricks. We compare our DGES with other baselines, including DPC [22], GES [10], and $\\mathbf{A}^{*}$ [15]. We compare the MEC of the output by all methods. Note that we only evaluate the BS part which we aim to identify. We consider the structural Hamming distance (SHD), the $F_{1}$ score, the precision, the recall, and the computational time as evaluation criteria. For each setting, we run 10 different random seeds and report the mean and standard deviation. More implementation details are in Appendix A5.1. ", "page_idx": 7}, {"type": "text", "text": "The simulated results about graphs with only one DC has been shown in Figure 3, and the results with two DCs (which may have overlapping variables) are given in Figure A4 of Appendix. Clearly, when there are more deterministic variables in the system, the runtime of our DGES will obviously increase. The reason is because there are more deterministic variables to be detected and fed into Phase 3 for exact search. According to the results, the general performance of DGES is competitive compared to other baselines. We observe that the exact method $\\mathbf{A}^{*}$ and our proposed DGES generally outperform the other baselines such as GES and DPC across different criteria and settings. Meanwhile, scorebased method GES presents better performance than constraint-based method DPC in a deterministic system. As the number of variable increases, the runtime of $\\mathrm{A^{*}}$ will increase rapidly. Compared to $\\mathrm{A^{*}}$ , the increasing of runtime for DGES is much more steady, both in linear and nonlinear models. More results about two MinDCs, non-deterministic scenarios, and relaxed exact search such as GRaSP [31], are provided in Appendix A5. ", "page_idx": 7}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/3232785fef8b549de015c752f9b2a962378a73a6a277e545f4c7e0ce3646563a.jpg", "img_caption": ["Figure 4: Results on the real-world dataset with deterministic relations by DGES with Generalized score. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Real-world Datasets. We also evaluate our method and the baselines on two real-world datasets. One is the pharmacokinetics dataset [34], which is an open database for pharmacokinetics information from clinical trials. It provides curated information mainly in two categories: the characteristics of the studied individuals (e.g., age, height) and the measurement records (e.g., the clearance, $T_{m a x}$ , $C_{m a x}$ when one certain individual takes one certain drug), and we name the two categories of variables as class \u201cI\" (individual) and \u201cM\" (measurement), respectively. Out of more than 200 variables and more than 200000 data samples containing missing values, we cleaned the data and finally obtained 32 important variables with 4194 data samples which may contain deterministic relations. The 32 variables contains 18 and 14 variables from the class \u201cI\" and $\\mathbf{\\omega}^{\\bullet}\\mathbf{M}^{\\prime\\prime}$ , respectively. We prepend the class label to each variable name as a prefix. We use linear BIC score and nonlinear generalized score to conduct the search. Figure 4 gives the DGES result with generalized score, where we can successfully detect at least three MinDCs: {height, weight, BMI}, $\\{k_{e l}$ , $V_{d}$ , Clearance}, $\\{k_{e l},T_{h a l f}\\}$ . Compared with the linear DGES result with BIC score, we can see more reasonable edges existing in the nonlinear DGES result with the generalized score, for example, {age \u2212medication, healthy $\\rightarrow$ disease, healthy \u2212BMI}. More results and analysis are provided in Appendix A6. ", "page_idx": 8}, {"type": "text", "text": "The other one is the US census Public Use Microdata Sample (PUMS). We follow the data preprocessing procedure outlined in [35], which is a modern version of the UCI Adult data set [36]. Datasets based on census data are widely considered in the algorithmic fairness literature [37\u201341]. Here we choose 5 important variables, i.e., Age, Occupation, Sex, Annual income (AI), and Adjusted annual income (AAI), in total there are 3000 samples. Because of the potentially different timeframe of the survey cycle, AAI $(=\\mathrm{{AI}^{\\ast}}$ Adjusted factor) are the adjusted dollar amounts that they have earned entirely during the calendar year. Within one calendar year, this adjusted factor is a constant. Here, we choose the data in 2021. Therefore, AAI and AI have a deterministic relation. The result of DGES is: $\\{\\mathrm{Sex}\\to\\mathrm{Occupation}\\leftarrow\\mathrm{AI}$ , $\\mathrm{AI}\\gets\\mathrm{AAI}$ , $\\mathrm{Sex}\\to\\mathrm{AAI}\\leftarrow\\mathrm{Age}\\}$ }, 5 edges. The result of GES is: {Sex $\\leftarrow$ Occupation \u2013 AAI, AI \u2013 AAI \u2013 Age, $\\mathrm{AI}\\to\\mathrm{Sex}\\leftarrow\\mathrm{AAI}\\}$ , 6 edges. The result of PC is: $\\mathrm{[}S e x\\rightarrow$ Occupation $\\leftarrow\\mathrm{Age}$ , AI \u2013 AAI}, 3 edges. Compared with GES, the result of DGES is more sparse. Particularly, we can detect that AI and AAI have a deterministic relation, and GES gives redundant edges by $\\{{\\mathrm{AI}}\\to{\\mathrm{Sex}}\\leftarrow{\\mathrm{AAI}}\\}$ while our DGES only keeps one edge $\\mathrm{\\Delta[Sex\\rightarrowAAI]}$ }. Moreover, the result of PC is totally different from the other two. Clearly, in our DGES result, AI and AAI are still connected, and we can still see the BS, i.e., $\\{\\mathbf{AI}\\rightarrow$ Occupation, AAI \u2013 Sex, AAI\u2013Age}. However, as a result of PC with FisherZ test, the BS becomes empty, which is exactly due to the violation of faithfulness. ", "page_idx": 8}, {"type": "text", "text": "6 Discussions and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. While presenting a versatile framework, our paper does have certain limitations. Firstly, in some cases, e.g., with overlapping deterministic variables, our method cannot identify the skeleton and directions in the DC part so far. We display two graphs that we can identify up to MEC and the other two that we cannot identify in Figure A1. More discussion is in Appendix A1. Secondly, inherited from the disadvantages of exact methods, our method can be somewhat computationally expensive in Phase 3 when there are a large number of MinDCs. Fortunately, each MinDC is usually not too large, and we may execute the exact search for different MinDCs simultaneously. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. The overarching aim of our proposed method is to learn the causal structures from any general functional causal models in the presence of deterministic relations. This is a fundamental and critical task with wide-ranging applications in practical life, and we firmly believe that our method will serve beneficial purposes without engendering negative societal impacts. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. This paper dives into the challenges of causal discovery in the presence of deterministic relations. Notably, we make a compelling discovery that exact score-based methods can elegantly address the deterministic issues, provided the SMR assumption is met. In an effort to bolster efficiency and scalability in a deterministic system, we propose the novel and versatile framework called DGES, encompassing both linear and nonlinear models, as well as both continuous and discrete data types. Furthermore, we establish the partial identifiability conditions for DGES. Hopefully, our method can help to construct a holistic view to see the deterministic relations. The extensive experiments on simulated and real-world datasets, validate our theoretical findings and the efficacy of our method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ana Rita Nogueira, Jo\u00e3o Gama, and Carlos Abreu Ferreira. Causal discovery in machine learning: Theories and applications. Journal of Dynamics & Games, 8(3):203, 2021.   \n[2] Xinpeng Shen, Sisi Ma, Prashanthi Vemuri, and Gyorgy Simon. Challenges and opportunities with causal discovery algorithms: application to alzheimer\u2019s pathophysiology. Scientific reports, 10(1):1\u201312, 2020.   \n[3] Matej Vukovic\u00b4 and Stefan Thalmann. Causal discovery in manufacturing: A structured literature review. Journal of Manufacturing and Materials Processing, 6(1):10, 2022.   \n[4] Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, and Cheng Zhang. Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation. Advances in Neural Information Processing Systems, 32, 2019.   \n[5] Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social science computer review, 9(1):62\u201372, 1991.   \n[6] Peter L Spirtes, Christopher Meek, and Thomas S Richardson. Causal inference in the presence of latent variables and selection bias. Conference on Uncertainty in Artificial Intelligence, 1995.   \n[7] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.   \n[8] Joseph Ramsey, Peter Spirtes, and Jiji Zhang. Adjacency-faithfulness and conservative causal inference. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pages 401\u2013408, 2006.   \n[9] Peter Spirtes and Jiji Zhang. A uniformly consistent estimator of causal effects under the k-triangle-faithfulness assumption. Statistical Science, pages 662\u2013678, 2014.   \n[10] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507\u2013554, 2002.   \n[11] Gideon Schwarz. Estimating the dimension of a model. The annals of statistics, pages 461\u2013464, 1978.   \n[12] Mikko Koivisto and Kismat Sood. Exact bayesian structure discovery in bayesian networks. The Journal of Machine Learning Research, 5:549\u2013573, 2004.   \n[13] Ajit P Singh and Andrew W Moore. Finding optimal Bayesian networks by dynamic programming. Carnegie Mellon University. Center for Automated Learning and Discovery, 2005.   \n[14] Changhe Yuan, Brandon Malone, and Xiaojian Wu. Learning optimal bayesian networks using a\\* search. In Twenty-second international joint conference on artificial intelligence, 2011.   \n[15] Changhe Yuan and Brandon Malone. Learning optimal bayesian networks: A shortest path perspective. Journal of Artificial Intelligence Research, 48:23\u201365, 2013.   \n[16] James Cussens. Bayesian network learning with cutting planes. In Proceedings of the TwentySeventh Conference on Uncertainty in Artificial Intelligence, pages 153\u2013160, 2011.   \n[17] Mark Bartlett and James Cussens. Integer linear programming for the bayesian network structure learning problem. Artificial Intelligence, 244:258\u2013271, 2017.   \n[18] Peter Spirtes and Kun Zhang. Search for causal models. Handbook of graphical models, pages 457\u2013488, 2018.   \n[19] Ni Y Lu, Kun Zhang, and Changhe Yuan. Improving causal discovery by optimal bayesian network learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8741\u20138748, 2021.   \n[20] Ignavier Ng, Yujia Zheng, Jiji Zhang, and Kun Zhang. Reliable causal discovery with improved exact search and weaker assumptions. Advances in Neural Information Processing Systems, 34: 20308\u201320320, 2021.   \n[21] Garvesh Raskutti and Caroline Uhler. Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1):e183, 2018.   \n[22] Clark Glymour. Learning the structure of deterministic systems. Causal learning. Psychology, philosophy, and computation, pages 231\u2013240, 2007.   \n[23] P Daniusis, D Janzing, J Mooij, J Zscheischler, B Steudel, K Zhang, and B Sch\u00f6lkopf. Inferring deterministic causal relations. In 26th Conference on Uncertainty in Artificial Intelligence (UAI 2010), pages 143\u2013150. AUAI Press, 2010.   \n[24] Dominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniu\u0161is, Bastian Steudel, and Bernhard Sch\u00f6lkopf. Information-geometric approach to inferring causal directions. Artificial Intelligence, 182:1\u201331, 2012.   \n[25] Wei Luo. Learning bayesian networks in semi-deterministic systems. In Advances in Artificial Intelligence: 19th Conference of the Canadian Society for Computational Studies of Intelligence, Canadian AI 2006, Qu\u00e9bec City, Qu\u00e9bec, Canada, June 7-9, 2006. Proceedings 19, pages 230\u2013241. Springer, 2006.   \n[26] Jan Lemeire, Stijn Meganck, Francesco Cartella, and Tingting Liu. Conservative independencebased causal structure learning in absence of adjacency faithfulness. International Journal of Approximate Reasoning, 53(9):1305\u20131325, 2012.   \n[27] Ahmed Mabrouk, Christophe Gonzales, Karine Jabet-Chevalier, and Eric Chojnacki. An efficient bayesian network structure learning algorithm in the presence of deterministic relations. In ECAI 2014, pages 567\u2013572. IOS Press, 2014.   \n[28] Yan Zeng, Zhifeng Hao, Ruichu Cai, Feng Xie, Libo Huang, and Shohei Shimizu. Nonlinear causal discovery for high-dimensional deterministic data. IEEE Transactions on Neural Networks and Learning Systems, 2021.   \n[29] Yuqin Yang, Mohamed S Nafea, AmirEmad Ghassami, and Negar Kiyavash. Causal discovery in linear structural causal models with deterministic relations. In Conference on Causal Learning and Reasoning, pages 944\u2013993. PMLR, 2022.   \n[30] Malcolm Forster, Garvesh Raskutti, Reuben Stern, and Naftali Weinberger. The frugal inference of causal relations. The British Journal for the Philosophy of Science, 2018.   \n[31] Wai-Yin Lam, Bryan Andrews, and Joseph Ramsey. Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence, pages 1052\u20131062. PMLR, 2022.   \n[32] Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Sch\u00f6lkopf, and Clark Glymour. Generalized score functions for causal discovery. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1551\u20131560, 2018.   \n[33] Paul Erd\u02ddos, Alfr\u00e9d R\u00e9nyi, et al. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17\u201360, 1960.   \n[34] Jan Grzegorzewski, Janosch Brandhorst, Kathleen Green, Dimitra Eleftheriadou, Yannick Duport, Florian Barthorscht, Adrian K\u00f6ller, Danny Yu Jia Ke, Sara De Angelis, and Matthias K\u00f6nig. Pk-db: pharmacokinetics database for individualized and stratified computational modeling. Nucleic acids research, 49(D1):D1358\u2013D1364, 2021.   \n[35] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems, 34:6478\u20136490, 2021.   \n[36] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 10:C5XW20, 1996.   \n[37] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[38] Zeyu Tang, Jialu Wang, Yang Liu, Peter Spirtes, and Kun Zhang. Procedural fairness through decoupling objectionable data generating components. arXiv preprint arXiv:2311.14688, 2023.   \n[39] Zeyu Tang, Jiji Zhang, and Kun Zhang. What-is and how-to for fairness in machine learning: A survey, reflection, and perspective. ACM Computing Surveys, 55(13s):1\u201337, 2023.   \n[40] Zeyu Tang, Yatong Chen, Yang Liu, and Kun Zhang. Tier balancing: Towards dynamic fairness over underlying causal factors. arXiv preprint arXiv:2301.08987, 2023.   \n[41] Zeyu Tang and Kun Zhang. Attainability and optimality: The equalized odds fairness revisited. In Conference on Causal Learning and Reasoning, pages 754\u2013786. PMLR, 2022.   \n[42] Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological advances. In Applied informatics, volume 3, pages 1\u201328. SpringerOpen, 2016.   \n[43] Peter Spirtes. An anytime algorithm for causal inference. In International Workshop on Artificial Intelligence and Statistics, pages 278\u2013285. PMLR, 2001.   \n[44] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Kernel-based conditional independence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.   \n[45] Eric V Strobl, Kun Zhang, and Shyam Visweswaran. Approximate kernel-based conditional independence tests for fast non-parametric causal discovery. Journal of Causal Inference, 7(1), 2019.   \n[46] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. Advances in Neural Information Processing Systems, 31, 2018.   \n[47] Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. Advances in Neural Information Processing Systems, 33:17943\u201317954, 2020.   \n[48] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. In International Conference on Machine Learning, pages 7154\u20137163. PMLR, 2019.   \n[49] Yue Yu, Tian Gao, Naiyu Yin, and Qiang Ji. Dags with no curl: An efficient dag structure learning approach. In International Conference on Machine Learning, pages 12156\u201312166. PMLR, 2021.   \n[50] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity constraints. International Conference on Learning Representations, 2022.   \n[51] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard Sch\u00f6lkopf. Amortized inference for causal structure learning. Advances in Neural Information Processing Systems, 35: 13104\u201313118, 2022.   \n[52] Dennis Wei, Tian Gao, and Yue Yu. DAGs with no fears: A closer look at continuous optimization for learning Bayesian networks. In Advances in Neural Information Processing Systems, 2020.   \n[53] Alexander Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated DAG! causal discovery benchmarks may be easy to game. In Advances in Neural Information Processing Systems, 2021.   \n[54] Ignavier Ng, Biwei Huang, and Kun Zhang. Structure learning with continuous optimization: A sober look and beyond. arXiv preprint arXiv:2304.02146, 2023.   \n[55] Shohei Shimizu, Patrik O Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.   \n[56] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. Nonlinear causal discovery with additive noise models. Advances in neural information processing systems, 21, 2008.   \n[57] Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. arXiv preprint arXiv:1205.2599, 2012.   \n[58] Alexander Marx, Arthur Gretton, and Joris M Mooij. A weaker faithfulness assumption based on triple interactions. In Uncertainty in Artificial Intelligence, pages 451\u2013460. PMLR, 2021.   \n[59] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331\u2013368, 2007.   \n[60] Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python. arXiv preprint arXiv:2307.16405, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "\u201cOn Causal Discovery in the Presence of Deterministic Relations\u201d ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents: ", "page_idx": 13}, {"type": "text", "text": "A1 More Discussions 14 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A2 Related Works 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A3 Method Details 16 ", "page_idx": 13}, {"type": "text", "text": "A3.1 Phase 1: Minimal Deterministic Clusters Detection 16   \nA3.2 Phase 2: Modified Greedy Equivalent Search 18   \nA4.1 Proof of Theorem 2 18   \nA4.2 Proof of Theorem 3 20 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A5 More Details about the Simulated Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A5.1 Implementation Details . . 22   \nA5.2 Evaluation on Two MinDCs 23   \nA5.3 Evaluation on Non-deterministic Scenario 24   \nA5.4 Evaluation on Relaxed Exact Search . . 25 ", "page_idx": 13}, {"type": "text", "text": "A6 More Details about the Real-world Experiments 25 ", "page_idx": 13}, {"type": "text", "text": "A6.1 Results of GES with BIC Score . 25   \nA6.2 Results of DGES with BIC Score . 25   \nA6.3 Results of GES with Generalized Score 26   \nA6.4 Analysis of DGES with Generalized Score . . 26 ", "page_idx": 13}, {"type": "text", "text": "A1 More Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Q1: Why current method cannot identify the skeleton and directions in the DC part? ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A1: To achieve that goal, we usually need strong assumptions on the underlying functional causal model, i.e., Yang et al. [29] assumed linear non-Gaussian model. However, those assumptions are not in alignment with our goal of a general method, i.e., with no restricted assumption on the underlying functional causal model. That is why currently our method cannot identify the skeleton and directions in the DC part. However, fortunately, we can exactly find out which set of variables are in the DC/MinDCs using some DC detection strategies, as shown in Section 3.1. ", "page_idx": 13}, {"type": "text", "text": "Figure A1 gives three example graphs where two of them can be identified up to the true MEC while the other one cannot. In graph (a), $V_{1}\\mapsto V_{2}$ , after DGES, we can capture the dependence between $V_{1}$ and $V_{2}$ , therefore, we can identify in this case. Similarly, in the graph (b), $\\{V_{1},V_{3}\\}\\mapsto V_{2}$ , since $V_{1}$ and $V_{3}$ are independent, GES can still capture this v-structure. Therefore, we can identify in this case. However, things are different in the two examples at right. In graph (c), $V_{1}\\mapsto V_{2}$ , $V_{2}\\mapsto V_{3}$ , after running GES, we may get a fully-connected graph. Obviously, this fully-connected graph has different skeleton and directions than the true one. ", "page_idx": 13}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/ffc8aa17bb5b07db35df60ad5578fa4d2a4a01c1b8f3956bb86723ab087dbb19.jpg", "img_caption": ["Figure A1: Some graphs where DGES can (Left) or cannot (Right) identify the MEC: (a) $V_{1}\\mapsto V_{2}$ , (b) $\\{V_{1},V_{3}\\}\\mapsto V_{2}$ , (c) $V_{1}\\mapsto V_{2}$ , $V_{2}\\mapsto V_{3}$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Q2: Why GES can be problematic in the cases where DC variables cause NDC variables? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A2: Take Figure 2 as an example, where the true edges related to $V_{6}$ include $V_{3}\\to V_{6}$ and $V_{4}\\to V_{6}$ . However, during the forward phase of GES, it is very likely that the edge $V_{1}\\to V_{6}$ can be added in the beginning. Then the edges $V_{2}\\to V_{6}$ and $V_{4}\\to V_{6}$ are added subsequently. During the backward phase of GES, the edge $V_{1}\\to V_{6}$ will not be deleted, because $V_{1}$ also contains information from $V_{3}$ , in other words, $V_{6}$ is represented by $V_{1},V_{2}$ and $V_{4}$ by GES, which contains more edges than the ground-true. Therefore, in this case GES can be problematic, and we need exact search under the SMR assumption for post-processing, in order to correctly identify the BS part. ", "page_idx": 14}, {"type": "text", "text": "Q3: How GES performs in the cases where NDC variables cause DC variables? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A3: Let\u2019s consider this example. Three variables compose a DC $(V_{1},V_{2},V_{3}$ , and $V_{1}=V_{2}+V_{3})$ , and denote another variable from NDC as $V_{4}$ . In this case, there must not be an edge from $V_{4}$ to $V_{1}$ , because $V_{4}$ will be in DC rather than NDC, if so. Then, if $V_{4}$ causes $V_{2}$ , there will definitely be an edge between them by GES because $V_{4}$ is clearly dependent on $V_{2}$ , and theoretically, GES can capture this dependence based on the local consistency. ", "page_idx": 14}, {"type": "text", "text": "Q4: What the characterization of Markov equivalence class is in the context with deterministic relations? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A4: Regarding only the variables involved in BS and NDC (that is how Theorem 4 claimed), the characterization of Markov equivalence class (MEC) with deterministic relations is still the same as the context of not having deterministic relations. ", "page_idx": 14}, {"type": "text", "text": "However, if we consider the whole graph, i.e., all of the variables in DCs are also involved, the characterization of the Markov equivalence class should be different. As shown in the condition $(i)$ of Assumption 3 and Figure 1, there will be \u201cconstant independence\u201d caused by the deterministic relations. Therefore, we need to remove those \u201cconstant independence\u201d for the new characterization of MEC. ", "page_idx": 14}, {"type": "text", "text": "A2 Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this part, we will introduce more related works in causal discovery [42]. As we mentioned in the main paper, constraint-based and score-based methods are two primary categories in causal discovery. Constraint-based methods utilize the conditional independence test (CIT) to learn a skeleton of the directed acyclic graph (DAG), and then orient the edges upon the skeleton. Such methods contain Peter-Clark (PC) algorithm [42] and Fast Causal Inference (FCI) algorithm [43]. Some typical CIT methods include kernel-based independent conditional test [44] and approximate kernel-based conditional independent test [45]. ", "page_idx": 14}, {"type": "text", "text": "Algorithm A1 Detecting Deterministic Cluster (DC) ", "page_idx": 15}, {"type": "text", "text": "Input: variable set $V\\in\\mathbb{R}^{d}$   \nOutput: DC   \n1: $\\bar{\\mathbf{D}}\\mathbf{C}\\gets\\emptyset$   \n2: for $i=1$ to $d$ do   \n3: if $\\{V\\backslash V_{i}\\}\\mapsto V_{i}$ then   \n4: $\\mathrm{DC}\\leftarrow\\mathrm{DC}\\cup\\{V_{i}\\}$   \n5: end if   \n6: end for ", "page_idx": 15}, {"type": "text", "text": "Score-based methods normally use a score function and rely on a particular search strategy to look for the intended graph. The search strategy usually involve greedy search, exact search, or continuous optimization. The first continuous-optimization based method is NOTEARS [46], which casts the Bayesian network structure learning task into a continuous constrained optimization problem with the least squares objective, using an algebraic characterization of directed acyclic graph (DAG). Subsequent work GOLEM [47] adopts a continuous unconstrained optimization formulation with a likelihood-based objective. NOTEARS is designed under the assumption of the linear relations between variables, therefore, another subsequent works have extended NOTEARS to handle nonlinear cases via deep neural networks, such as DAG-GNN [48] and DAG-NoCurl [49]. Moreover, ENCO [50] presents an efficient DAG discovery method for directed acyclic causal graphs utilizing both observational and interventional data. AVCI [51] infers causal structure by performing amortized variational inference over an arbitrary data-generating distribution. These methods might suffer from various optimization issues, including convergence [52], sensitivity to data standardization [53], and nonconvexity [54]. Since they are only guaranteed to find a local optimum, therefore the quality of the solution can not be guaranteed, even in the asymptotic cases. ", "page_idx": 15}, {"type": "text", "text": "Besides the constrain-based and score-based methods, another major category of causal discovery methods is function causal model based methods. Those methods rely on the causal asymmetry property, such as the linear non-Gaussian model (LiNGAM) [55], the additive noise model [56], and the post-nonlinear causal model [57]. Apart from those methods, there are also some hybrid methods, such as neural conditional dependence (NCD) method, which reframes the GES algorithm to be more flexible than the standard score-based version and readily lends itself to the nonparametric setting with a general measure of conditional dependence. ", "page_idx": 15}, {"type": "text", "text": "deterministic relations and faithfulness violation. It is interesting to discuss the relationships between deterministic relations and faithfulness violation. These faithfulness relaxation methods such as [58] work on general faithfulness violation and propose some weaker faithfulness assumptions. They usually focus on certain types of structure, such as canceling path, XOR-type, triangle faithfulness, etc. However, to the best of our knowledge, deterministic relations will break all those relaxed faithfulness assumptions, as the distribution is even not a graphoid. Therefore, we need to develop specific algorithms to handle determinism. ", "page_idx": 15}, {"type": "text", "text": "A3 Method Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A3.1 Phase 1: Minimal Deterministic Clusters Detection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "DC Detection. In order to detect the DC, which contains all the deterministic variables, we need to traverse all $d$ variables. If $\\{V\\backslash V_{i}\\}\\mapsto V_{i}$ holds true, then this variable $V_{i}$ must be in DC. The general pseudocode is stated in Algorithm A1. ", "page_idx": 15}, {"type": "text", "text": "MinDCs Detection. We aim to get a set of MinDCs from the DC obtained above. Given the DC, for each variable $V_{i}$ , we try to detect whether there exists a minimal set $S$ such that $S\\mapsto V_{i}$ , $S\\subset\\mathrm{DC}$ . As shown in the Algorithm A2, we need to traverse all the possible sets for $S$ with increasing cardinality $k$ $\\iota,\\,|S|=k\\,(|\\cdot\\,|$ means the cardinality of a set). If we find that $S\\mapsto V_{i}$ and meanwhile $\\{S\\cup V_{i}\\}$ is not a superset of any MinDC in current MinDCs, then we can conclude that $S\\cup V_{i}$ composes one MinDC. Otherwise, if we find that $\\{S\\cup V_{i}\\}$ is a superset of one MinDC $M$ in current MinDCs, we may conclude that $\\{S\\cup V_{i}\\}$ is not a minimal DC because $|S\\cup V_{i}|>|M|$ . ", "page_idx": 15}, {"type": "text", "text": "Algorithm A2 Detecting Minimal Deterministic Clusters (MinDCs) ", "page_idx": 16}, {"type": "table", "img_path": "pfvcsgFrJ6/tmp/35db9fc34aa149d4dc460ec33d561ef49e76a24d228bde731dbe9809a02eea7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "How to Evaluate $S\\mapsto X?$ Here we use regression and evaluate the variance term of residue to decide whether there is a deterministic relation or not. Please note that DGES does not assume any functional causal model. Therefore, we also evaluate it in a general form. Specifically, we provide two versions: one assumes a linear model and is based on linear regression as shown in Lemma 4, and another is based on a general non-linear model as exhibited in Lemma 5. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Representation in Linear Model) Let $X$ be a random variable and $S$ be a set of random variables, where $X\\not\\in S$ . Define $X$ and $S$ are with domain $\\mathcal{X}$ and $\\mathbb{S},$ , respectively. Consider a linear regression framework: ${\\mathcal{X}}(X)=a*S+u,$ , where a and u represent the regression coefficient and residue, respectively. ", "page_idx": 16}, {"type": "text", "text": "$X$ can be represented by $S$ if and only if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(u)=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathrm{Var}(u)$ is the variance of the residue $u$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (Representation in General Nonlinear Model) Let $X$ be a random variable and $S$ be a set of random variables, where $X\\not\\in S$ . Define $X$ and $S$ are with domain $\\mathcal{X}$ and $\\mathbb{S}$ , respectively. Define a RKHS $\\mathcal{H}_{\\mathcal{X}}$ on $\\mathcal{X}$ with continuous feature mapping $\\phi_{\\mathcal{X}}:\\mathcal{X}\\to\\mathcal{H}_{\\mathcal{X}}$ . Consider a regression framework in the RKHS: $\\phi_{\\mathcal{X}}(X)=F_{\\mathbb{S}}(S)+u_{\\varepsilon}$ , where $F_{\\mathbb{S}}:S\\rightarrow\\mathcal{H}_{\\mathcal{X}}$ and u represents the regression residue. ", "page_idx": 16}, {"type": "text", "text": "$X$ can be represented by $S$ if and only if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Sigma_{u}\\|_{H S}^{2}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Sigma_{u}$ is the variance matrix of the residue, $\\Sigma_{u}\\,=\\,R_{u}^{T}R_{u}$ , ${\\cal R}_{u}=\\varepsilon({\\cal K}_{S}+\\varepsilon I)^{-1}\\phi(X)$ , $\\varepsilon$ is $a$ small positive regularization parameter for kernel ridge regression, and $\\kappa_{S}$ is the centralized kernel matrix of $S$ . ", "page_idx": 16}, {"type": "text", "text": "Discussion: Why consider kernel regression in Lemma 5? ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Because we are considering the general functional causal form. Particularly, this Lemma can be used for both linear and nonlinear functional relationships, Gaussian and non-Gaussian data distributions, which is in alignment with the general goal of our proposed method. For more details, inspired by [44], the functions $\\phi_{\\mathcal{X}}$ and $F(\\cdot)$ that we use are all in the infinite Hilbert spaces, and we evaluate the representation with the Hilbert-Schmidt norm of the variance operator $\\Sigma_{u}$ in infinite dimension. In this case, we can exhibit a general functional causal form. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Assume there is a MEC $\\mathcal{M}$ , which contains both directed edges and undirected edges. Let $X$ be a random variable in $\\mathcal{M}$ and $S$ be the set of all non-descendant neighbors, including direct causes and undirected neighbors of $X$ . Suppose the random variables $X$ and $S$ are over measurable spaces $\\mathcal{X}$ and $\\boldsymbol{S}$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "Without assuming a particular functional causal form, we usually exploit a regression framework in the RKHS, to encode general dependence relations between two random variables. Define a RKHS $\\mathcal{H}_{\\mathcal{X}}$ on $\\mathcal{X}$ with continuous feature mapping $\\phi_{\\mathcal{X}}:\\mathcal{X}\\to\\mathcal{H}_{\\mathcal{X}}$ . Here, we consider ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{\\mathcal{X}}(X)=F(S)+u,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $F:S\\to{\\mathcal{H}}_{X}$ and $u$ represents the regression residue or noise. When applying the kernel ridge regression, we can obtain the estimated residue ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{u}=\\varepsilon(K_{Z}+\\varepsilon I)^{-1}\\phi(X),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\varepsilon$ is a small positive regularization parameter for kernel ridge regression, and $K_{Z}$ is the centralized kernel matrix of $Z$ . To evaluate whether such a residue exists, one may consider the Hilbert-Schmidt norm of the variance matrix ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\Sigma}_{u}\\|_{H S}^{2}=\\|\\boldsymbol{\\hat{u}}^{T}\\boldsymbol{\\hat{u}}\\|_{H S}^{2}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If the above equation holds true, then we may conclude that there is no noise term in the relationship between $X$ and $S$ , in other words, $X$ can be represented by $S$ (without extra noise term). ", "page_idx": 17}, {"type": "text", "text": "Vice versa. ", "page_idx": 17}, {"type": "text", "text": "A3.2 Phase 2: Modified Greedy Equivalent Search ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure A2 presents an example comparing our modified GES with traditional GES. From this example, we can see that: in the backward phase, if we use the \u201cconstant independence\u201d information $C\\perp A|D$ , then the result graph will become totally wrong where A and B will be connected. In our modifies GES, we indifferently ignore such \u201cconstant independence\u201d information. In the end, some other information will be considered as a priority. For example, as shown on the right side, $B\\perp\\!\\!\\!\\perp A$ and the edge between A and B will be removed first. In the end, we can obtain a more correct graph than the one on the left side. ", "page_idx": 17}, {"type": "text", "text": "However, the result of the modified GES is still not perfect; we can see there are redundant edges existing, such as $A\\rightarrow D$ . Therefore, we need the Phase 3 exact search for post-processing. Under the SMR assumption, we can obtain a more sparse graph, where either edge $A\\rightarrow C$ or edge $A\\rightarrow D$ will be deleted. ", "page_idx": 17}, {"type": "text", "text": "A4 Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the proofs of Theorem 2 and Theorem 3 in the main paper. ", "page_idx": 17}, {"type": "text", "text": "A4.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof: As suggested by the generalized score [32], with proper score functions and search procedures, asymptotically, the resulting Markov equivalence class has the same independence constraints as the data generative distribution. ", "page_idx": 17}, {"type": "text", "text": "(i) First of all, we would like to discuss the local consistency of the generalized score. ", "page_idx": 17}, {"type": "text", "text": "For the regression problem, one can define the effective dimension of the kernel space and the complexity of the regression function according to [59]. Then under mild conditions, the CVlikelihood score is locally consistent. ", "page_idx": 17}, {"type": "text", "text": "Lemma 6 Suppose that the sample size of each test set $n_{0}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\nn_{0}\\rightarrow\\infty,\\frac{n_{0}}{n}\\rightarrow0\\;a s\\;n\\rightarrow\\infty,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and suppose that the regularization parameter $\\lambda$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda={\\cal O}(n^{-\\frac{b}{b c+1}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Ground-truth Graph: ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/0682d5e55904d2e440393dc798f3f6e21e5b4daa045d24ba9f1412437d2cccc8.jpg", "img_caption": ["{C,D} is a deterministic cluster. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "pfvcsgFrJ6/tmp/494cb827f27bc186a4badd5a4a479e361343ee1cfed9e4330cf55ab9d4099561.jpg", "table_caption": ["Modified GES Forward Phase:( $\\longrightarrow$ : added edge) "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/e4e7e52c7bd7755723d5a1fc6e9810f6b0a221b3f5ebe3455ceae1b4ff24f590.jpg", "img_caption": ["Figure A2: An Example: Original GES vs. Modified GES. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "where n is the total sample size, b is a parameter of the effective dimension of the kernel space with $b>1$ , and c indicates the complexity of the regression function with $1<c\\leq2$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 7 Assume that all conditions given in Lemma 6 hold. With the CV likelihood under the regression framework in RKHS as a score function and with the GES search procedure, it guarantees to find the Markov equivalence class which is consistent to the data generative distribution asymptotically. ", "page_idx": 18}, {"type": "text", "text": "Lemma 7 ensures that, with proper score functions and search procedures, asymptotically, the resulting Markov equivalence class has the same independence constraints as the data generative distribution. For the complete proofs, please refer to the Appendix A5 of paper [32]. ", "page_idx": 18}, {"type": "text", "text": "(ii) Then, We will provide the proof by contra-positive in both directions based on the consistency of the generalized score as shown above. ", "page_idx": 18}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/bcb67f3cf54ba9cb625753c3133afead78091e567f832fd6f074187c08492a9e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure A3: An example graph with determinisic relation where $V_{3}~=~f(V_{1},V_{2})$ . (a) A nondeterministic variable $V_{4}$ connects to $\\left\\{V_{1},V_{2},V_{3}\\right\\}$ . (b) A non-deterministic variable $V_{4}$ connects to $\\{V_{2},V_{3}\\}$ . (c) A non-deterministic variable $V_{4}$ connects to $\\{V_{3}\\}$ . Here among the three graphs, only the graph (c) can be partially identified. ", "page_idx": 19}, {"type": "text", "text": "1) \u201cIf\" direction: Suppose that an exact score-based search asymptotically outputs a DAG $\\mathcal{H}$ (having the highest generalized score) that does not belong to the MEC of the true DAG $\\mathcal{G}$ . Since the generalized score is known to be consistent, $(\\mathcal{H},\\mathbb{P})$ must satisfy the Markov assumption because otherwise, its generalized score is lower than that of the true DAG $\\mathcal{G}$ and exact search would not have output $\\mathcal{H}$ . By assumption, the generalized score of $\\mathcal{H}$ is higher than that of $\\mathcal{G}$ , which, by the consistency of generalized, implies that $|\\mathcal{H}|\\leq|\\mathcal{G}|$ , and therefore, $(\\mathcal{G},\\mathbb{P})$ does not satisfy the SMR assumption. ", "page_idx": 19}, {"type": "text", "text": "2) \u201cOnly if\" direction: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Suppose that $(\\mathcal{G},\\mathbb{P})$ does not satisfy the SMR assumption. Then there exists a DAG $\\mathcal{H}$ not in the MEC of $\\mathcal{G}$ such that $|\\mathcal{H}|\\leq|\\bar{\\mathcal{G}}|$ , and $(\\mathcal{H},\\mathbb{P})$ satisfies the Markov assumption. Without loss of generality, we choose $\\mathcal{H}$ with the least number of edges. We first consider the case in which $|\\mathcal{H}|<|\\mathcal{G}|$ . Since both $\\mathcal{H}$ and $\\mathcal{G}$ satisfy the Markov assumption, by the consistency of generalized, the generalized score of $\\mathcal{H}$ is higher than that of $\\mathcal{G}$ , which implies that exact score-based search will not output any DAG from the MEC of $\\mathcal{G}$ . For the case with $|\\mathcal{H}|=|\\mathcal{G}|$ , since they are both Markov with distribution $\\mathbb{P}$ , they have the same generalized score. Therefore, an exact search will output a DAG that belongs to the MEC of either $\\mathcal{H}$ or $\\mathcal{G}$ and is not guaranteed to output a DAG from the MEC of the true DAG $\\mathcal{G}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof ends. ", "page_idx": 19}, {"type": "text", "text": "A4.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "First, we will explain why we need the three assumptions listed. Secondly, we will explain why we need to have constraint on $\\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\,\\mid-1$ . Thirdly, we will give the complete proof based on the conditions. ", "page_idx": 19}, {"type": "text", "text": "(i) Why do we need the listed three assumptions? ", "page_idx": 19}, {"type": "text", "text": "As mentioned in our main paper, there are three phases of our proposed DGES. During the second phase, we need to run GES. To ensure the accuracy of output (particularly on the NDC part), we need the assumptions of Markov and non-deterministic faithfulness (See Assumptions 1 and 3). Then in the third phase, we need to perform the exact search exclusively on the EDC, where the Sparsest Markov Representation (SMR) assumption (See Assumption 2) will be needed. ", "page_idx": 19}, {"type": "text", "text": "(ii) Why do we assume $\\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\mid-1?$ ", "page_idx": 19}, {"type": "text", "text": "As for why we need to condition on $\\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\mid-1$ , we can start with explaining why $|\\,\\mathrm{PA}_{i}\\,|=|\\,\\mathrm{\\dot{M}i n D C}\\,|$ and $\\mid\\mathrm{PA}_{i}\\mid=\\mid\\mathrm{MinDC}\\mid-1$ will fail the provided identifiability. ", "page_idx": 19}, {"type": "text", "text": "Let\u2019s take an example with four variables, where three of them are deterministically related, as shown in Figure A3. Here among the three graphs, only the graph (c) can be partially identified, and the graph (a) and (b) cannot achieve partial identifiability. ", "page_idx": 19}, {"type": "text", "text": "We further assume a linear functional causal model, then we can formulate the deterministic relationship as ", "page_idx": 20}, {"type": "equation", "text": "$$\na V_{1}+b V_{2}+c V_{3}=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $a,b,c$ are any linear coefficients. Based on the above formulation, the causal equation of variable $V_{4}$ in Figure A3(a) can be represented as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{4}=d V_{1}+e V_{2}+f V_{3}+\\epsilon}}\\\\ {{\\displaystyle{\\quad=d V_{1}+e V_{2}+f\\frac{1}{c}(a V_{1}+b V_{2})+\\epsilon}}}\\\\ {{\\displaystyle{\\quad=d V_{1}+e\\frac{1}{b}(a V_{1}+c V_{3})+f V_{3}+\\epsilon}}}\\\\ {{\\displaystyle{\\quad=d\\frac{1}{a}(b V_{2}+c V_{3})+e V_{2}+f V_{3}+\\epsilon,}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\epsilon$ is the random noise injected into $V_{4}$ . Clearly, the above four equations are all valid, in other words, $V_{4}$ can be possibly represented by different sets of variables, meaning that this case is not guaranteed to be identified. ", "page_idx": 20}, {"type": "text", "text": "Regarding the variable $V_{4}$ in Figure A3(b), the causal equation can be represented as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{4}=e V_{2}+f V_{3}+\\epsilon}}\\\\ {{\\displaystyle~~~=e V_{2}+f\\frac{1}{c}(a V_{1}+b V_{2})+\\epsilon}}\\\\ {{\\displaystyle~~~=e\\frac{1}{b}(a V_{1}+c V_{3})+f V_{3}+\\epsilon.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, the above three equations are all valid, in other words, $V_{4}$ can be possibly represented by different sets of variables, meaning that this case is also not guaranteed to be identified. ", "page_idx": 20}, {"type": "text", "text": "However, in Figure A3(c), things are different. The causal equation of variable $V_{4}$ can be represented as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{4}=f V_{3}+\\epsilon}}\\\\ {{\\ =f\\frac{1}{c}(a V_{1}+b V_{2})+\\epsilon.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When the SMR assumption is satisfied, we can identify the only one case, which is $V_{3}\\to V_{4}$ . ", "page_idx": 20}, {"type": "text", "text": "Now, we extend the three-variable case to the general linear case where there is a MinDC with the cardinality $\\left|\\,\\mathrm{MinDC}\\,\\right|$ . And we can easily conclude the true conditions to be: $\\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\,\\mid-1$ ", "page_idx": 20}, {"type": "text", "text": "Furthermore, we extend the linear to the nonlinear case, where we can also conclude that the listed conditions ensure partial identifiability. ", "page_idx": 20}, {"type": "text", "text": "(iii) The complete proof: ", "page_idx": 20}, {"type": "text", "text": "Part I: ", "page_idx": 20}, {"type": "text", "text": "Beneftiing from the local consistency of BIC score (See Lemma 7 of paper [10]) and generalized score (See Proposition 1 of paper [32]), the NDC part is guaranteed to find the true Markov equivalence class which is consistent to the data generative distribution asymptotically. ", "page_idx": 20}, {"type": "text", "text": "The proof contains two parts: the forward phase and the backward phase of GES. In the forward phase, the resulting equivalence class $\\mathcal{E}_{f}$ contains underlying distribution $\\mathbb{P}$ ; i.e., all independence constraints holding in $\\mathcal{E}_{f}$ hold in $\\mathbb{P}$ . It has been proved by making use of local consistency of score functions in [10]. Here we focus on showing that the backward phase is guaranteed to find a perfect map of $\\mathbb{P}$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $\\mathcal{E}_{b}$ denote the equivalence class resulting from the backward phase of GES, and let $\\mathcal{E}^{*}$ be the perfect map of $\\mathbb{P}$ ; i.e., all independence constraints in $\\mathcal{E}^{*}$ are in $\\mathbb{P}$ , and vice versa. Here we aim to show that as the sample size $n\\to\\infty$ , $\\mathcal{E}_{b}=\\mathcal{E}^{*}$ . ", "page_idx": 21}, {"type": "text", "text": "1) First, we show that the equivalence class $\\mathcal{E}$ results from each step in the backward phase contains $\\mathbb{P}$ . Consider a move from $\\mathcal{E}$ to $\\mathcal{E}^{-}(\\mathcal{E})$ by applying Delete $(X_{i},X_{j},\\mathbf{H})$ (see the definition in [10]), where $\\mathcal{E}$ contains $\\mathbb{P}$ and $\\mathcal{E}^{-}(\\mathcal{E})$ does not contain $\\mathbb{P}$ . Let $\\mathcal G\\in\\mathcal E$ and $\\mathcal{G}^{\\prime}\\in\\mathcal{E}^{-}(\\mathcal{E})$ with the difference in $X_{i}\\rightarrow X_{j}$ . From the fact that the score functions are locally consistent, the local score change $\\Delta S<0$ , so $S(\\mathcal{G};D)>S(\\mathcal{G}^{\\prime};D)$ . The attempted move from $\\mathcal{E}$ to $\\mathcal{E}^{-}(\\mathcal{E})$ will be rejected. ", "page_idx": 21}, {"type": "text", "text": "2) Second, we show that the backward phase will not terminate with some suboptimal equivalence class $\\mathcal{E}$ ; that is, there are no independence constraints which containing in $\\mathbb{P}$ are not in $\\mathcal{E}$ . Suppose that the backward phase terminates with some suboptimal equivalence class $\\mathcal{E}$ , and there is one more edge $X_{i}\\to X_{j}$ or $X_{i}-X_{j}$ in $\\mathcal{E}$ than in $\\mathcal{E}^{*}$ . According to local consistency, and the calculation of local score change with Delete operator, $\\Delta S$ from $\\mathcal{E}$ to $\\mathcal{E}^{*}$ is positive; that is, the score of $\\mathcal{E}^{*}$ is larger than that of $\\mathcal{E}$ . Hence it will move to $\\mathcal{E}^{*}$ . It contradicts with the assumption that the backward phase terminates with some suboptimal equivalence class. Therefore, the resulting equivalence class in the backward phase is a perfect map of $\\mathbb{P}$ . ", "page_idx": 21}, {"type": "text", "text": "Part II: ", "page_idx": 21}, {"type": "text", "text": "However, the BS part will not be guaranteed to find the true Markov equivalence class so far by GES. Due to the deterministic relations, more dependent edges will be added during the forward phase, e.g., $\\{V_{1}\\to V_{6}\\}$ in Figure 2(b) and $\\{A\\to{\\bar{D}}\\}$ in Figure A2. However, during the backward phase, all \u201cconstant independencies\u201d (i.e., $V_{i}\\perp\\!\\!\\!\\!\\perp V_{j}|S$ with $S\\mapsto V_{i}$ or $S\\mapsto V_{j}$ ) are ignored indifferently, e.g., $V_{1}$ \u22a5\u22a5 $V_{6}|V_{2},V_{3},V_{4}$ in Figure 2(b) and $\\textit{C}\\bot A|D$ in Figure A2. In the end, the edges $V_{1}\\to V_{6}$ and $C\\rightarrow A$ will be kept. ", "page_idx": 21}, {"type": "text", "text": "Lemma 8 (Sparsity [10]) Let $\\mathcal{G}$ and $\\mathcal{H}$ be any two DAGs that contain the generative distribution and for which $\\mathcal{G}$ has fewer parameters than $\\mathcal{H}$ , and let $S$ be any consistent $(D A G)$ scoring criterion. If all DAGs in an equivalence class have the same number of parameters, then for every $\\mathcal{G}^{\\prime}\\approx\\mathcal{G}$ and for every $\\mathcal{H}^{\\prime}\\approx\\mathcal{H}$ , $\\mathbb{S}(\\mathcal{G}^{\\prime};D)>\\mathbb{S}(\\mathcal{H}^{\\prime};D)$ . ", "page_idx": 21}, {"type": "text", "text": "Fortunately, we have Phase 3 exact search as post-processing. Under the SMR assumption, we perform the exact search exclusively on the DC and their neighbors. Benefiting from the Lemma 8, a more sparse graph with smaller BS will be selected out of all possible sets. For example, $\\{V_{3}{\\rightarrow}V_{6},\\bar{V_{4}}{\\rightarrow}V_{6}\\}$ will be favoured over $\\{V_{1}{\\rightarrow}V_{6},V_{2}{\\rightarrow}V_{6},V_{4}{\\rightarrow}V_{6}\\}$ in Figure 2, and $\\{A\\to{\\bar{C}}\\}$ will be favoured over $\\{A\\to C,A\\to D\\}$ in Figure A2. ", "page_idx": 21}, {"type": "text", "text": "Given the condition $\\mid\\mathrm{PA}_{i}\\mid<\\mid\\mathrm{MinDC}\\,\\mid-1$ , the sparsest graph is unique. Therefore, we can identify the BS in such a scenario, e.g., Figure 2. However, when the condition is violated, e.g., Figure A2, we can not uniquely obtain the BS, because both $\\{A\\to C\\}$ and $\\{A\\to D\\}$ can be acceptable BS after executing Phase 3 exact search. ", "page_idx": 21}, {"type": "text", "text": "In summary, when the two conditions are satisfied by our DGES, the BS and NDC parts of the causal graph $\\mathcal{G}$ are guaranteed to find their true Markov equivalent class, which is consistent with the data generative distribution asymptotically. ", "page_idx": 21}, {"type": "text", "text": "Proof ends. ", "page_idx": 21}, {"type": "text", "text": "A5 More Details about the Simulated Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A5.1 Implementation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide the implementation details of our method and other baseline methods for synthetic datasets. ", "page_idx": 21}, {"type": "text", "text": "Datasets. The true DAGs are simulated using the Erd\u00f6s\u2013R\u00e9nyi model [33] with the number of edges equal to the number of variables. The data is generated according to the functional causal model $\\begin{array}{r}{\\bar{V_{i}}\\mathrm{=}\\sum_{V_{j}\\in\\mathrm{PA}_{i}}b_{i j}\\,f_{i}(V_{j})\\mathrm{+}\\epsilon_{i}}\\end{array}$ , where $V_{j}\\{\\mathrm{{ePA}}_{i}$ is the $j$ -th direct cause of $V_{i}$ , $\\epsilon_{i}$ is the random noise related to variable $V_{i}$ , and $f_{i}$ is causal function. For deterministic variables, the noise term is removed, then the model becomes $\\scriptstyle V_{i}=\\sum_{V_{j}\\in\\mathrm{PA}_{i}}b_{i j}f_{i}(V_{j})$ . We evaluate both linear and nonlinear models. For the linear Gaussian model, we let $f_{i}(V_{j})=V_{j}$ and $\\epsilon_{i}$ follow Gaussian distribution whose mean is zero and variance is uniformly sampled from $\\bar{\\mathcal{U}}(1,2)$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "For the general nonlinear model, we try two different types. One is by mixed functions, where $f_{i}$ is randomly chosen from linear, square, sinc, and tanh functions, and $\\epsilon_{i}$ is sampled from uniform distribution $\\mathcal{U}(-0.5,0.5)$ or Gaussian distribution $\\mathcal{N}(0,1)$ . The other is generated by MLP, where we consider two hidden layers and each hidden layer has 100 hidden dimensions. We use Sigmiod as the activation function. All the weights are randomly generated from the uniform distribution $\\mathcal{U}(0.5,2)$ . For each setting, we also run 10 different random seeds and report the mean and standard deviation. ", "page_idx": 22}, {"type": "text", "text": "Hyperparameters. During the first phase, when we aim to detect the DC and MinDCs and check whether a variable can be deterministically represented by some others, we set that if the term $\\|\\Sigma_{u}\\|_{H S}^{2}<1e{-3}$ , although theoretically the value should exactly be zero. Meanwhile, the regularization parameter for the kernel ridge regression is set to $1e\\!-\\!10$ . The second phase of our method is to run modified GES, and the setting is by default. The penalty parameter for controlling the sparsity is set to 1. The exact search in the third phase we incorporate is the $\\mathbf{A}^{*}$ . We run our method and the other baseline methods in Ubuntu 20.04 LTS 64-bit System with Intel(R) Xeon(R) Silver $4214\\;2.20\\mathrm{GHz}\\times64$ CPU. s ", "page_idx": 22}, {"type": "text", "text": "Baselines and Evaluations. We compare our DGES with other baselines, including DPC [22], GES [10], and $\\mathbf{A}^{*}$ [15]. We compare the MEC of the output by all methods. For each method, we consider the structural Hamming distance (SHD), the $F_{1}$ score, the precision, the recall, and the computational time as evaluation criteria. Note that we only evaluate the BS part which we can identify in the graph under mild assumptions. We conduct the experiments on varying number of variables, varying number of samples, and some other hyperparameter studies. For linear model, we evaluate variable $d\\in\\{8,10,12,14,16\\}$ while fixing sample size $n\\,=\\,500$ , and evaluate sample $n\\,\\in\\,\\{100,250,500,1000,2000\\}$ while fixing variable $d\\,=\\,8$ . For nonlinear model, we evaluate variable $d\\in\\{6,7,8,9,10\\}$ while fixing sample size $n=100$ , and evaluate sample $n\\in\\{50,100,150,200,250\\}$ while fixing variable $d=6$ . We run 10 instances with different random seeds and report the means and standard deviations. ", "page_idx": 22}, {"type": "text", "text": "Furthermore, we provide more implementation details for the baseline methods. ", "page_idx": 22}, {"type": "text", "text": "\u2022 DPC [22]: The method is an extension for traditional PC algorithm [7], the key idea is that: every time when we do the conditional independence test, we aim to remove the potential deterministic variables from the conditioning set so that the faithfulness will not be violated. Here we follow the paper, and use the covariance to measure the closeness of two variables. If the covariance between two variables are greater than 0.9, we then remove the variable from the conditioning set in conditional independence test. Meanwhile, for linear Gaussian model, we choose FisherZ test, while for nonlinear model we choose kernel-based test [44], and the significance level is set to $\\alpha=0.05$ by default. We implement this method based on the Causal-learn package https://github.com/py-why/causal-learn [60]. \u2022 GES [10]: This method is a classical score-based method with greedy search. Our implementation is based on the code from https://github.com/juangamella/ges. For linear Gaussian model, we use BIC score. And for general nonlinear model, we use generalized score with cross-validation likelihood [32]. The penalty parameter for controlling the sparsity is set to 1. \u2022 $\\mathrm{A^{*}}$ [15]: $\\mathbf{A}^{*}$ is one of the classical exact score-based methods. Actually, there are some heuristic algorithms proposed to accelerate the search procedure. Considering in our scenarios, we do not utilize any heuristic tricks for the experiments in order to ensure the accuracy of solutions. Our experiments are based on the implementations on the Causal-learn package https://github.com/py-why/causal-learn [60]. ", "page_idx": 22}, {"type": "text", "text": "A5.2 Evaluation on Two MinDCs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 3 in the main paper presents the simulated results focused on graphs containing just a single deterministic constraint (DC). In contrast, Figure A4 in the Appendix offers insights into scenarios involving two DCs, even allowing for the possibility of overlapping variables. An evident trend emerges: as the system incorporates more deterministic variables, the runtime of our proposed DGES inevitably escalates. This phenomenon can be attributed to the increased number of deterministic variables demanding detection and inclusion in Phase 3, where an exact search is performed. ", "page_idx": 22}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/c56f5897795fb635b70994378f395bd4aec26fef058d85b747f1af6900398bd0.jpg", "img_caption": ["Figure A4: Results on the simulated datasets with two MinDCs. We evaluate different functional causal models on varying number of variables and samples, respectively. For each setting, we consider SHD (\u2193), $F_{1}$ score $(\\uparrow)$ , precision $(\\uparrow)$ , recall (\u2191) and runtime (\u2193) as evaluation criteria. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "It is worth noting that as the number of variables in the system increases, the runtime of $\\mathbf{A}^{*}$ experiences a rapid surge. In stark contrast, DGES exhibits a more stable increase in runtime, demonstrating its efficiency and suitability for both linear and nonlinear models. ", "page_idx": 23}, {"type": "text", "text": "The outcomes gleaned from these experiments collectively indicate that DGES exhibits competitive performance compared to established baselines. Notably, the exact method $\\mathbf{A}^{*}$ and our proposed DGES consistently outperform other baseline methods like Greedy Equivalence Search (GES) and PC, across a spectrum of evaluation criteria and diverse settings. It is intriguing to note that in deterministic systems, the score-based method GES consistently outperforms the constraint-based method DPC. This observation suggests that score-based approaches maintain a comprehensive perspective on causal discovery, which appears to be less susceptible to the challenges posed by deterministic relationships, unlike constraint-based methods. ", "page_idx": 23}, {"type": "text", "text": "A5.3 Evaluation on Non-deterministic Scenario ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We also conducted the experiments in a standard setting, where there is no deterministic relation at all. We consider the linear Gaussian model with a varying number of variables. We evaluate the SHD, the $F_{1}$ score, the precision, the recall, and the runtime. We run 10 instances with different random seeds and report the means and standard deviations. ", "page_idx": 23}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/fb12cecf92a40d9c07e14d993df9e493dc2fc4ffe481bf7dee09ca282760d7dd.jpg", "img_caption": ["Figure A5: Results of non-deterministic scenarios on linear Gaussian model. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/a22a8c49393fb70b2b4f26d35a393367ed9fdad267e7bc3bbfdf8723546b7fc8.jpg", "img_caption": ["Figure A6: Results of GRaSP [31] on linear Gaussian model. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The results have been shown in Figure A5. According to the results, we can see that GES and our proposed DGES method present the same performance regarding the SHD, the $F_{1}$ score, the precision, and the recall. However, the runtime of DGES is a bit more than GES, because DGES runs 2 phases. It is understandable that when there is no deterministic relation, DGES will be reduced to GES. In Phase 1, DGES will not find any deterministic clusters, then it will terminate and output the result of GES in Phase 2. ", "page_idx": 24}, {"type": "text", "text": "A5.4 Evaluation on Relaxed Exact Search ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "GRaSP [31] is a greedy relaxation of the sparsest permutation algorithm. We follow the same setting as mentioned in Section 5. Here we consider the linear Gaussian model with a varying number of variables, and within the generated dataset there is one MinDC. We evaluate the SHD, the $F_{1}$ score, the precision, the recall, and the runtime. In this case, we evaluate based on only the BS part. ", "page_idx": 24}, {"type": "text", "text": "The results have been shown in Figure A6. According to the results, we can see that: in general, $\\mathbf{A}^{*}$ and our proposed DGES still outperform other baselines. GRaSP performs slightly better than GES regarding the SHD, the $F_{1}$ score, the precision, and the recall. However, according to our data record, the runtime of GRaSP is a bit more than GES. ", "page_idx": 24}, {"type": "text", "text": "A6 More Details about the Real-world Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Due to the comparative poor performance of DPC and the expensive computation of exact search such as $\\mathbf{A}^{*}$ , here for the large real dataset, we mainly compare our DGES with GES, in both linear (using BIC score) and nonlinear (generalized score) settings. ", "page_idx": 24}, {"type": "text", "text": "A6.1 Results of GES with BIC Score ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this case, we run GES with BIC score, assuming the model is following linear Gaussian. The causal graph result is given in Figure A7. And in this graph, we can clearly see some deterministic variables are reasonably connected, such as BMI $\\rightarrow$ weight $\\rightarrow$ height. ", "page_idx": 24}, {"type": "text", "text": "A6.2 Results of DGES with BIC Score ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We run our proposed DGES with BIC score. The first phase is to identify all the MinDCs. Here, we can detect some MinDCs, such as: {BMI, weight, height.}, $\\{k_{e l},V_{d}$ , Clearance}. The final result is given in Figure A8. ", "page_idx": 24}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/036bae5820d07475fc943e2da94967b6964c02582173c32e52df554425d8d4dd.jpg", "img_caption": ["Figure A7: Results of real-world dataset with deterministic relations by GES with BIC Score. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/290f07f68e538ce14cff3b1803bfeec70e2c0aaf7b37fb8b17a887467475c3dc.jpg", "img_caption": ["Figure A8: Results of real-world dataset with deterministic relations by DGES with BIC Score. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "A6.3 Results of GES with Generalized Score ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "BIC score assumes a linear Gaussian model; here, using a generalized score can present general functional models. The GES result with generalized score is given in Figure A9. In this graph, we can still see some deterministic variables are reasonably connected, such as BMI $\\rightarrow$ weight $\\rightarrow$ height. ", "page_idx": 25}, {"type": "text", "text": "A6.4 Analysis of DGES with Generalized Score ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This graph is presented in Figure 4 in the main paper. In phase 1, we can detect the following MinDCs: {BMI, weight, height}, $\\{k_{e l},T_{h a l f}\\}$ , $\\{k_{e l},V_{d}$ , Clearance}, which are all correct. ", "page_idx": 25}, {"type": "text", "text": "Specifically, the ground-truth functions are ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{{BMI}=\\frac{w e i g h t}{h e i g h t^{2}},}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "pfvcsgFrJ6/tmp/cc446d8e4789d9a07fc1c46294692dab5acb26e8235e7da53476bc6d5f4f01c3.jpg", "img_caption": ["Figure A9: Results of real-world dataset with deterministic relations by GES with Generalized Score. "], "img_footnote": [], "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{k_{e l}=\\displaystyle\\frac{l n2}{T_{h a l f}},}\\\\ {k_{e l}=\\displaystyle\\frac{\\mathrm{Clearance}}{V_{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Take this MinDC {height, weight, BMI} as an example for further analysis. As we all know, BMI is defined as the body weight divided by the square of the body height, which composes a deterministic relation among the three variables. By applying the GES method, the three variables are connected like a cluster comprising a MinDC. At the same time, many other non-deterministic variables still connect with at least one of the three deterministic variables, as usual. Imagine if we use a constraintbased method to deal with it, there should be no edge connecting from the three variables to any others. Furthermore, according to our common knowledge, the true arrows should be {weigh $\\scriptstyle\\;\\to\\mathrm{BMI}$ , height $\\rightarrow\\mathrm{BMI}$ , height $\\rightarrow$ weight}, but now our graph just presents {weigh $\\rightarrow\\mathrm{BMI}$ , weight $\\rightarrow$ height}. As discussed in Appendix A1 (Q1), because currently our method cannot identify the skeleton and directions in a MinDC without further assumptions, and in this case, there are common causes behind BMI and height, which are Healthy and Medication. ", "page_idx": 26}, {"type": "text", "text": "Compared with the GES result with the generalized score, we can see some edges are corrected in the DGES result. For example, the MinDC $k_{e l}$ , $V_{d}$ , Clearance} is clustered together. ", "page_idx": 26}, {"type": "text", "text": "Compared with the DGES result with BIC score, we can see more reasonable edges existing in the nonlinear DGES result with the generalized score, for example, {age \u2212medication, healthy $\\rightarrow$ disease, healthy \u2212BMI}. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is about causal discovery with deterministic relations. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitation is in Section 6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide assumption 1,3,4 for the proof of Theorem 4. The complete proof is given in Appendix [? ]. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Implementation details are presented in Appendix A5.1 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our source code has been put in the supplementary files. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The general details of experiment setting are presented in Section 5. The rest of the details are included in the Appendix A5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The standard deviation has been reported in Figure 3 and Figure A4. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The computation details are given in Appendix A5.1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research follows the ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The discussion about broader impacts is given in Section 6. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The generated linear or nonlinear models are discussed in Appendix A5.1. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The data generation details for simulated datasets are given in Appendix A5.1. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]