[{"figure_path": "jRtxzzk0a6/tables/tables_4_1.jpg", "caption": "Table 1: Model configurations and perplexity on OpenWebText for Kraken models compared to similarly sized GPT-2 models. Lower perplexity is better.", "description": "This table compares the performance of Kraken and GPT-2 models on the OpenWebText dataset.  It shows different model configurations (number of layers, embedding dimension, attention heads) and the resulting number of parameters and validation perplexity.  Lower perplexity indicates better language modeling performance. The table allows comparison of models with similar numbers of parameters but different architectures (Kraken vs. GPT-2) to assess the impact of the Kraken design on language modeling quality.", "section": "4.1 Model configurations and perplexity"}, {"figure_path": "jRtxzzk0a6/tables/tables_5_1.jpg", "caption": "Table 1: Model configurations and perplexity on OpenWebText for Kraken models compared to similarly sized GPT-2 models. Lower perplexity is better.", "description": "This table presents the configurations and validation perplexity scores for various Kraken and GPT-2 language models trained on the OpenWebText dataset.  It shows that Kraken models, with varying degrees of intra-layer parallelism (2-way, 4-way, 6-way), achieve comparable perplexity to similarly sized GPT-2 models, indicating that the Kraken architecture doesn't compromise model quality while offering potential performance advantages.", "section": "4.1 Model configurations and perplexity"}, {"figure_path": "jRtxzzk0a6/tables/tables_5_2.jpg", "caption": "Table 2: Zero-Shot performance on SuperGLUE. ReCoRD uses the F1 score as the evaluation metric. All other benchmarks use accuracy.", "description": "This table presents the zero-shot performance of various language models on the SuperGLUE benchmark.  The models include GPT-2 and Kraken models of different sizes and degrees of parallelism.  The benchmark consists of multiple tasks, each evaluating a different aspect of language understanding, and the results are reported using accuracy (except for ReCoRD which uses F1-score). The table shows that Kraken models generally achieve comparable performance to GPT-2 models across various tasks, even with different model sizes and degrees of parallelism.", "section": "4.2 Performance On SuperGLUE"}, {"figure_path": "jRtxzzk0a6/tables/tables_6_1.jpg", "caption": "Table 2: Zero-Shot performance on SuperGLUE. ReCoRD uses the F1 score as the evaluation metric. All other benchmarks use accuracy.", "description": "This table presents the zero-shot performance of different language models on the SuperGLUE benchmark.  The models compared include various sizes of GPT-2 and Kraken models with different degrees of parallelism.  The SuperGLUE benchmark consists of multiple tasks, each assessed using either accuracy or F1 score (for ReCoRD), providing a comprehensive evaluation of language understanding capabilities.  The results demonstrate the performance of Kraken models relative to standard GPT-2 models on diverse language tasks without any fine-tuning.", "section": "4.2 Performance On SuperGLUE"}, {"figure_path": "jRtxzzk0a6/tables/tables_6_2.jpg", "caption": "Table 4: Configurations for the different model engines used to compare TTFT. For models of similar sizes, hyperparameters are shared for the GPT-like and Parallel Attention + FeedForward variants. 4-way denotes Kraken configurations used when evaluating tensor parallelism across 4 devices and likewise for 8-way.", "description": "This table shows the configurations used for different model engines in the evaluation of Time To First Token (TTFT). It compares standard, GPT-like, and Parallel Attention + FeedForward models with Kraken models, demonstrating the various hyperparameters (number of layers, embedding dimension, parameters per layer, number of attention heads) used for both 4-way and 8-way tensor parallelism.", "section": "4 Evaluation"}, {"figure_path": "jRtxzzk0a6/tables/tables_15_1.jpg", "caption": "Table 5: Pretraining compute and setup for each Kraken configuration", "description": "This table presents the hyperparameters used during the pre-training phase for various Kraken model configurations.  It details the number of layers, embedding dimension, number of attention heads, total number of parameters, the approximate number of A100 GPU hours required for training, and the initial learning rate used for each configuration.  The configurations are differentiated by the degree of parallelism (2-way, 4-way, 6-way) employed.  This information allows readers to understand the computational resources and training settings associated with each Kraken model.", "section": "4.1 Model configurations and perplexity"}, {"figure_path": "jRtxzzk0a6/tables/tables_16_1.jpg", "caption": "Table 6: Inference latency in milliseconds for 4-way parallelism", "description": "This table presents the inference latency in milliseconds for various model sizes (1.3B, 6.7B, 13B, 65B, and 175B parameters) and context lengths (128 and 2048 tokens) using 4-way parallelism.  It compares the latency of three different model architectures: Standard Transformer, Parallel Attention + FeedForward, and Kraken.  The Kraken architecture aims to reduce latency by overlapping collective operations with compute. The table shows the inference time for each model and architecture, allowing for a direct comparison of their efficiency.", "section": "4.4 Speedup in Time To First Token"}, {"figure_path": "jRtxzzk0a6/tables/tables_16_2.jpg", "caption": "Table 7: Inference latency in milliseconds for 8-way parallelism", "description": "This table presents the inference latency results for 8-way parallelism across different model sizes (1.3B, 6.7B, 13B, 65B, and 175B parameters) and context lengths (128 and 2048 tokens).  The latency is measured for three different model architectures: Standard Transformers, Parallel Attention + FeedForward Transformers, and Kraken Transformers. Kraken consistently shows lower latency than the other two architectures.", "section": "4.4 Speedup in Time To First Token"}]