{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All you Need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the models studied in this paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language Models are Few-Shot Learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the capabilities of large language models, motivating the focus on efficient inference for such models."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduced the GPT models, which are used as a benchmark for comparison in this paper."}, {"fullname_first_author": "Mohammad Shoeybi", "paper_title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "publication_date": "2019-09-08", "reason": "This paper introduced a tensor parallelism scheme that is extended in this work, showing that it is relevant to the work presented in this paper."}, {"fullname_first_author": "Reiner Pope", "paper_title": "Efficiently Scaling Transformer Inference", "publication_date": "2023-01-01", "reason": "This paper addresses the efficiency of Transformer inference, making it highly relevant to the topic of this paper."}]}