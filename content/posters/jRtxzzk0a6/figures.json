[{"figure_path": "jRtxzzk0a6/figures/figures_1_1.jpg", "caption": "Figure 1: One layer of a standard Transformer consisting of Multi-Head Attention (also shown) followed by a FeedForward Network. Residual connections have been omitted.", "description": "This figure shows a single layer of a standard Transformer architecture.  It details the sequence of operations: Layer Normalization, Multi-Head Attention (with a detailed breakdown of its GEMM operations), AllReduce, another Layer Normalization, Feed Forward Network, and a final AllReduce.  The diagram highlights the key computational blocks and the data flow between them, illustrating the structure of a single Transformer layer.  It omits residual connections for clarity.", "section": "2.1 Decoder-Only Transformer models"}, {"figure_path": "jRtxzzk0a6/figures/figures_1_2.jpg", "caption": "Figure 2: Increasing the degree of tensor parallelism decreases the Time To First Token. Even when weights and KV cache fit on device memory, parallelism can be worthwhile. These results are for a 6.7B parameter GPT-3 like model and were collected using TensorRT-LLM engines on our evaluation platform: an HGX A100 40GB system.", "description": "This figure shows the impact of increasing tensor parallelism on the Time To First Token (TTFT) for a 6.7B parameter GPT-3-like language model. The results demonstrate that even when model weights and the key-value cache fit within the memory of a single GPU, increasing the degree of parallelism (using multiple GPUs) leads to a reduction in TTFT.  This is likely due to the improved utilization of compute resources across multiple GPUs, which helps to reduce the latency associated with collective operations such as AllReduce. The experiment was conducted using TensorRT-LLM engines on an HGX A100 40GB system.", "section": "Background"}, {"figure_path": "jRtxzzk0a6/figures/figures_3_1.jpg", "caption": "Figure 3: Parallelizing two standard Transformer layers compared to executing two layers of a Kraken Transformer with 2-way parallelism. Kraken Transformers have fewer AllReduce operations and these can be run concurrently with the Multi-Head Attention in the next layer. Step lengths are illustrative and not indicative of how much wall-clock time a particular operation might require.", "description": "This figure compares the execution of two standard Transformer layers with the execution of two Kraken Transformer layers, both with 2-way parallelism. The figure highlights that Kraken reduces the number of AllReduce operations and enables concurrent execution of these operations with Multi-Head Attention in the subsequent layer, thereby potentially reducing latency.", "section": "3 Kraken: Model architecture"}, {"figure_path": "jRtxzzk0a6/figures/figures_7_1.jpg", "caption": "Figure 4: Speedup in Time To First Token over standard Transformers on a system that uses NVSwitch and with 4-way parallelism. x-axis labels denote the size of the model followed by the context length. Bar labels are in percentage improvement.", "description": "This figure displays the speedup achieved by Kraken and Parallel Attention + FFN models compared to standard Transformers in terms of Time To First Token (TTFT). The improvements are shown as percentages for various model sizes (1.3B, 6.7B, 13B, 65B, and 175B parameters) and context lengths (128 and 2048 tokens). The system used has NVSwitch for inter-device communication.  The results demonstrate that Kraken significantly outperforms standard Transformers and shows competitive performance compared to the Parallel Attention + FFN architecture in reducing TTFT.", "section": "4.4 Speedup in Time To First Token"}, {"figure_path": "jRtxzzk0a6/figures/figures_7_2.jpg", "caption": "Figure 5: Speedup in Time To First Token relative to standard Transformers on a system that uses NVSwitch and with 8-way parallelism. x-axis labels denote the size of the model followed by the context length. Bar labels are in percentage improvement.", "description": "This figure compares the speedup in Time To First Token (TTFT) achieved by Kraken models against standard Transformers and models with Parallel Attention + FFN, using 8-way tensor parallelism on a system with NVSwitch.  The x-axis shows different model sizes (1.3B, 6.7B, 13B, 65B, 175B parameters) and context lengths (128, 2048 tokens). The bars represent the percentage speedup of Kraken and Parallel Attention + FFN compared to the standard Transformer for each model and context size combination.  A geomean of the speedups is also shown.", "section": "4.4 Speedup in Time To First Token"}, {"figure_path": "jRtxzzk0a6/figures/figures_8_1.jpg", "caption": "Figure 6: Runtime characterization: 4-way parallelism. For each cluster on the x-axis, labels denote the size of model followed by the context length.", "description": "This figure shows a detailed breakdown of the runtime for each operation in the forward pass of Kraken and other Transformer models with 4-way parallelism. It compares the time spent on different operations such as LayerNorm, GEMM (General Matrix Multiply), Attention, and AllReduce, for various model sizes and context lengths. The \"Overlapped GEMM\" represents the time spent on GEMM operations that are overlapped with communication, showcasing Kraken's efficiency in overlapping computation and communication.", "section": "4.5 Runtime characterization"}, {"figure_path": "jRtxzzk0a6/figures/figures_8_2.jpg", "caption": "Figure 7: Runtime characterization: 8-way parallelism. For each cluster on the x-axis, labels denote the size of model followed by the context length.", "description": "This figure shows the breakdown of the forward pass runtime for different model sizes (1.3B, 6.7B, 13B, 65B, 175B parameters) and context lengths (128, 2048 tokens) with 8-way parallelism.  It compares the runtime of standard Transformers, Transformers with parallel Attention and Feed-Forward Networks, and Kraken Transformers. The runtime is broken down into components: Layer Normalization (Norm), General Matrix Multiplication (GEMM), Overlapped GEMM (where computation and AllReduce are overlapped), Attention, Linear Mapping (LM Head), and AllReduce. The figure helps visualize how Kraken reduces the time spent in AllReduce operations by overlapping them with computation, leading to faster inference.", "section": "4.5 Runtime characterization"}]