[{"figure_path": "x7AD0343Jz/figures/figures_1_1.jpg", "caption": "Figure 1: Translation of a compositional algorithmic task A, PEN (see Section 3 for details), into its corresponding compositional graph GA(x), for the input x = \"ab xy ab4fq wv7ql\". The operations (edges) are color-matched with the respective operations in the pseudo-code of the algorithmic task A(x).", "description": "This figure shows a visualization of how a compositional algorithmic task, specifically the PEN task, can be represented as a computational graph. The PEN task involves processing a sequence of words by jumping between different words according to a matching criterion and outputting their neighbors. The graph illustrates the task's structure, where nodes represent intermediate variables and edges represent primitive operations. The color coding helps match the operations in the graph to their corresponding steps in the PEN task's pseudo-code.  It highlights the relationships between primitive operations (left, match, right) and the overall compositional structure of the task.", "section": "2 Preliminaries"}, {"figure_path": "x7AD0343Jz/figures/figures_3_1.jpg", "caption": "Figure 2: Introduced compositional algorithmic tasks. Left: The Pointer Execution (PE)'s neighbor (PEN), together with the Pointer Execution (PE) and Pointer Execution Verbose (PEV) sub-tasks. Starting left, the output is obtained by matching words and predicting the current word (in PE) or its neighbor (in PEV and PEN). Our matching criterion is that the two end characters of the current word are equal to the first two characters of the matched word. By ensuring that there are no ambiguities in the input string, an attention mechanism can find the match by retrieving the last two characters of the word and matching it with the (unique) word that starts with them. Right: The Pointer Execution Reverse Multicount (PERM), together with the Pointer Execution (PE) and Pointer Execution Reverse (PER) sub-tasks. PERM first outputs the last word in the matching sequence and then goes backward. The number in the answer for each word is the count of matches times the count of left-matches (i.e., arrow to the left in the forward matching sequence).", "description": "This figure illustrates two new algorithmic tasks introduced in the paper: Pointer Execution's Neighbor (PEN) and Pointer Execution Reverse Multicount (PERM).  The left side shows PEN and its related sub-tasks (PE, PEV, and RCpy), detailing how the algorithm progresses through a sequence of words based on matching criteria and retrieving neighbor words. The right side illustrates PERM and its sub-tasks (PE, PER, and PEM), highlighting how the algorithm matches a sequence, reverses it, and calculates a multicount value for each element based on match and left-match counts.  The visual representation aids in understanding the compositional nature of the tasks and the independent observability of their primitive operations.", "section": "Probing compositionality with algorithmic tasks"}, {"figure_path": "x7AD0343Jz/figures/figures_5_1.jpg", "caption": "Figure 3: Accuracy of LLaMA models on PEN, PERM, HSS, and MUL, and their respective sub-tasks. While LLaMa achieves perfect accuracy on all the individual sub-tasks, it needs much larger amounts of training data to learn their composition. This observation makes hypothesis H2 (learning the composition requires less samples than the hardest sub-task, green) and H3 (learning the composition requires less samples than the sum of the sub-tasks, yellow) impossible to achieve on every task, where LLaMa seems to always fall into H4 (learning the composition requires more samples than the sum of the sub-tasks, red). Moreover, training together with the sub-tasks does not seem to perform sensibly better than training only on the main task (i.e., w/o sub-tasks).", "description": "This figure shows the accuracy of LLaMA models on four compositional algorithmic tasks (PEN, PERM, HSS, and MUL) and their respective sub-tasks, plotted against the number of training samples.  It demonstrates the sample inefficiency of LLMs in compositional learning, supporting the hypothesis that learning the composition of sub-tasks requires significantly more data than learning the individual sub-tasks.  The colored regions represent different hypotheses about sample efficiency, with H4 (requiring more samples than the sum of samples needed for sub-tasks) being the most consistently observed result.", "section": "4 Sample efficiency on compositional learning"}, {"figure_path": "x7AD0343Jz/figures/figures_17_1.jpg", "caption": "Figure A.4: Intuition for our statement: Based on widely accepted assumptions, there are problems requiring much more learning computation time than usage computation time (here: per sample available). Gradient Descent on Feedforward Networks under the assumption of constant-step-memorization however, will only be able to learn problems as hard to learn as they are during inference. Therefore, the model size needs to be much larger than necessary, or the training samples need to be much larger than necessary, or gradient descent will have trouble learning.", "description": "This figure illustrates the learning-inference gap discussed in the paper.  It visually represents the difference between the complexity of learning a problem (high learning effort) compared to the complexity of performing the learned task once it's acquired (low inference effort). The figure highlights that gradient descent on feedforward networks struggles to learn problems that are significantly more complex to learn than to perform (infer).  The model's capacity to memorize the training data limits its ability to generalize.", "section": "A Feedforward models on gradient descent can only learn the obvious"}, {"figure_path": "x7AD0343Jz/figures/figures_28_1.jpg", "caption": "Figure 3: Accuracy of LLaMA models on PEN, PERM, HSS, and MUL, and their respective sub-tasks. While LLaMa achieves perfect accuracy on all the individual sub-tasks, it needs much larger amounts of training data to learn their composition. This observation makes hypothesis H2 (learning the composition requires less samples than the hardest sub-task, green) and H3 (learning the composition requires less samples than the sum of the sub-tasks, yellow) impossible to achieve on every task, where LLaMa seems to always fall into H4 (learning the composition requires more samples than the sum of the sub-tasks, red). Moreover, training together with the sub-tasks does not seem to perform sensibly better than training only on the main task (i.e., w/o sub-tasks).", "description": "The figure shows the accuracy of LLaMA models (150M parameters) on four different compositional tasks (PEN, PERM, HSS, MUL) and their corresponding sub-tasks, as a function of training samples. Each task is broken down into multiple sub-tasks, with independently observable primitives, which allows to evaluate the model's capacity to reuse and compose the sub-tasks' knowledge.  The results indicate that achieving high accuracy on the complete tasks necessitates significantly more training samples than needed to learn the individual sub-tasks, supporting the hypothesis that these models fail to learn compositionally.", "section": "4 Sample efficiency on compositional learning"}, {"figure_path": "x7AD0343Jz/figures/figures_30_1.jpg", "caption": "Figure 3: Accuracy of LLaMA models on PEN, PERM, HSS, and MUL, and their respective sub-tasks. While LLaMa achieves perfect accuracy on all the individual sub-tasks, it needs much larger amounts of training data to learn their composition. This observation makes hypothesis H2 (learning the composition requires less samples than the hardest sub-task, green) and H3 (learning the composition requires less samples than the sum of the sub-tasks, yellow) impossible to achieve on every task, where LLaMa seems to always fall into H4 (learning the composition requires more samples than the sum of the sub-tasks, red). Moreover, training together with the sub-tasks does not seem to perform sensibly better than training only on the main task (i.e., w/o sub-tasks).", "description": "This figure presents the results of training LLAMA models on four different tasks (PEN, PERM, HSS, and MUL) and their corresponding subtasks. The x-axis represents the number of training samples, while the y-axis shows the accuracy achieved on each task.  The results demonstrate that LLAMA struggles with compositional learning, requiring significantly more data to learn the compositional tasks than to learn the individual subtasks. The figure highlights the significant sample inefficiency of LLAMA in compositional learning and supports hypothesis H4, indicating that learning a compositional task requires more samples than the sum of the samples needed to learn all the individual subtasks.", "section": "Sample efficiency on compositional learning"}, {"figure_path": "x7AD0343Jz/figures/figures_33_1.jpg", "caption": "Figure 2: Introduced compositional algorithmic tasks. Left: The Pointer Execution (PE)'s neighbor (PEN), together with the Pointer Execution (PE) and Pointer Execution Verbose (PEV) sub-tasks. Starting left, the output is obtained by matching words and predicting the current word (in PE) or its neighbor (in PEV and PEN). Our matching criterion is that the two end characters of the current word are equal to the first two characters of the matched word. By ensuring that there are no ambiguities in the input string, an attention mechanism can find the match by retrieving the last two characters of the word and matching it with the (unique) word that starts with them. Right: The Pointer Execution Reverse Multicount (PERM), together with the Pointer Execution (PE) and Pointer Execution Reverse (PER) sub-tasks. PERM first outputs the last word in the matching sequence and then goes backward. The number in the answer for each word is the count of matches times the count of left-matches (i.e., arrow to the left in the forward matching sequence).", "description": "This figure illustrates two novel compositional algorithmic tasks introduced in the paper: Pointer Execution's neighbor (PEN) and Pointer Execution Reverse Multicount (PERM).  The left side shows PEN, decomposing it into sub-tasks (PE, PEV, Copy, Reverse Copy) to highlight the compositional nature of the task.  The right side shows PERM with its sub-tasks (PE, PER, PEM), emphasizing how the task requires not only matching but also reverse ordering and counting operations.  The figure uses color-coding and diagrams to visually represent the tasks and their respective sub-tasks.", "section": "Probing compositionality with algorithmic tasks"}, {"figure_path": "x7AD0343Jz/figures/figures_34_1.jpg", "caption": "Figure 2: Introduced compositional algorithmic tasks. Left: The Pointer Execution (PE)'s neighbor (PEN), together with the Pointer Execution (PE) and Pointer Execution Verbose (PEV) sub-tasks. Starting left, the output is obtained by matching words and predicting the current word (in PE) or its neighbor (in PEV and PEN). Our matching criterion is that the two end characters of the current word are equal to the first two characters of the matched word. By ensuring that there are no ambiguities in the input string, an attention mechanism can find the match by retrieving the last two characters of the word and matching it with the (unique) word that starts with them. Right: The Pointer Execution Reverse Multicount (PERM), together with the Pointer Execution (PE) and Pointer Execution Reverse (PER) sub-tasks. PERM first outputs the last word in the matching sequence and then goes backward. The number in the answer for each word is the count of matches times the count of left-matches (i.e., arrow to the left in the forward matching sequence).", "description": "This figure illustrates two newly introduced algorithmic tasks, PEN and PERM, designed to evaluate the compositionality of LLMs.  The left panel depicts PEN, which involves matching words based on a substring criterion and outputting their neighbors, broken down into subtasks to highlight the composition. The right panel shows PERM, requiring matching, reversing, and multicounting steps within a sequence. Both tasks are represented visually as computational graphs, emphasizing the compositional nature of the problems.", "section": "Probing compositionality with algorithmic tasks"}]