[{"figure_path": "PquRXu9pQ6/tables/tables_5_1.jpg", "caption": "Table 1: Results of audio-image-text experiments. The best results are bolded.", "description": "This table presents the results of audio-image-text experiments comparing different methods.  The performance is measured by Recall@1 and Recall@5 for audio-image, audio-text, and image-text retrieval tasks on three datasets: FlickrNet, AVE, and COCO.  The best results for each metric and dataset are shown in bold. The methods compared include CLAP, CLIP, AudioCLIP, WAV2CLIP, ImageBind, C-MCR, and Ex-MCR (with two versions, base and huge, and a combination with ImageBind).", "section": "4.2 Audio-Image-Text Results"}, {"figure_path": "PquRXu9pQ6/tables/tables_5_2.jpg", "caption": "Table 2: Results of 3D-image-text experiments.", "description": "This table presents the results of experiments evaluating the performance of different methods on 3D-image-text tasks.  It compares several methods (CLIP, ULIP, ULIP v2, C-MCRCLIP-ULIP, and Ex-MCR-base) across three metrics: 3D-Text (ModelNet40 dataset, using Acc@1, Acc@3, and Acc@5), 3D-Image (Objaverse-LVIS dataset, using R@1 and R@5), and Image-Text (COCO dataset, using R@1 and R@5). The metrics assess the accuracy and retrieval performance of each method in various aspects of 3D and image-text understanding. Ex-MCR-base is shown to be competitive with or exceeding the state-of-the-art methods, especially for image-text tasks.", "section": "4.3 3D-Image-Text Results"}, {"figure_path": "PquRXu9pQ6/tables/tables_7_1.jpg", "caption": "Table 5: Structure of f<sub>i</sub>(\u00b7). \"Linear\" means single linear layer, and \"n MLP\" indicates n-layer MLP.", "description": "This table presents ablation study results on the structure of the projector f<sub>i</sub>(\u00b7), which is responsible for aligning modalities within the leaf space.  It compares the performance of using a single linear layer versus multiple-layer perceptrons (MLPs) with varying numbers of layers (1, 2). The results are presented in terms of mean Average Precision (mAP) and Recall@5 (R@5) metrics across four datasets: FlickrNet, AVE, VGGSS, and AudioCaps.", "section": "3.2 Enhancing Alignment Learning Pipeline"}, {"figure_path": "PquRXu9pQ6/tables/tables_7_2.jpg", "caption": "Table 6: Structure of fm(\u00b7)", "description": "This table presents the ablation study results on the structure of the fm(\u00b7) projector within the Ex-MCR model.  The fm(\u00b7) projector is responsible for inter-space alignment, mapping the leaf space embeddings to the base space. The table shows the performance (mAP and R@5) on four different datasets (FlickrNet, AVE, VGGSS, and AudioCaps) using various structures for fm(\u00b7), ranging from a simple linear layer to a multi-layer perceptron (MLP) with up to 5 layers.  The results indicate the optimal number of layers in the MLP for best performance across the different datasets.", "section": "3.2 Enhancing Alignment Learning Pipeline"}, {"figure_path": "PquRXu9pQ6/tables/tables_14_1.jpg", "caption": "Table 7: Model configurations of projectors.", "description": "This table shows the detailed architecture of the two projectors used in the Ex-MCR model.  The table specifies the layers (Linear, BatchNorm1D, Relu) and their input and output dimensions (Cin, Cout) for each projector,  f1 and fm.  Projector f1 focuses on intra-space alignment, while fm focuses on inter-space alignment.", "section": "B Architecture of Projectors"}, {"figure_path": "PquRXu9pQ6/tables/tables_15_1.jpg", "caption": "Table 8: Detailed results of experiments on data modality-centric.", "description": "This table presents the mean average precision (mAP) and Recall@5 (R@5) metrics for audio-image-text retrieval experiments.  Different combinations of modality-centric data (audio only, image only, text only, and various combinations thereof) were used to train the model. The results show the impact of using various modality-centric data on the performance of the audio-image-text retrieval task across different datasets (FlickrNet, AVE, VGGSS, and AudioCaps).  The table allows for analysis of which types of data are most effective and whether combinations of data improve performance.", "section": "4.2 Audio-Image-Text Results"}, {"figure_path": "PquRXu9pQ6/tables/tables_15_2.jpg", "caption": "Table 9: Detailed results of experiments on alignment objective.", "description": "This table presents the mean average precision (mAP) and Recall@5 (R@5) for different alignment objectives used in the Ex-MCR model. The objectives include aligning audio and text (A-T), text and text (T-T), audio and image (A-V), text and image (T-V), and a dense alignment approach that considers multiple modalities simultaneously (Dense). The results are evaluated on four datasets: FlickrNet, AVE, VGGSS, and AudioCaps, showcasing the effectiveness of each alignment strategy on different audio-visual retrieval tasks.", "section": "3.2 Enhancing Alignment Learning Pipeline"}, {"figure_path": "PquRXu9pQ6/tables/tables_15_3.jpg", "caption": "Table 10: Detailed results of experiments on the structure of f1(\u00b7).", "description": "This table presents a detailed breakdown of the performance of different projector structures (f1(\u00b7)) used in the Ex-MCR model.  It compares the performance using a linear layer, a single Multilayer Perceptron (MLP) layer, and a 2-layer MLP. The metrics used for evaluation are mean Average Precision (mAP) and Recall@5 (R@5) across four different datasets: FlickrNet, AVE, VGGSS, and AudioCaps. This helps determine the optimal complexity for the f1(\u00b7) module in achieving a balance between performance and efficiency.", "section": "3.2 Enhancing Alignment Learning Pipeline"}, {"figure_path": "PquRXu9pQ6/tables/tables_15_4.jpg", "caption": "Table 11: Detailed results of experiments on the structure of fm(\u00b7).", "description": "This table presents the ablation study results focusing on the structure of the inter-space alignment projector, fm(\u00b7), in the Ex-MCR model.  It shows the performance (mAP and R@5) on four different datasets (FlickrNet, AVE, VGGSS, AudioCaps) for different numbers of MLP layers in the fm(\u00b7) module.  The results indicate how the number of layers impacts the model's performance in audio-image-text retrieval.", "section": "4.2 Audio-Image-Text Results"}, {"figure_path": "PquRXu9pQ6/tables/tables_16_1.jpg", "caption": "Table 12: Detailed results of experiments on the hyperparameter of \u03c42.", "description": "This table presents the results of ablation experiments conducted to determine the optimal value for the hyperparameter \u03c42.  The results are presented in terms of Recall@5 (R@5) for various datasets including FlickrNet, AVE, VGGSS, and AudioCaps. Each row represents a different value of \u03c42, showing the impact of this parameter on the model's performance across different datasets.", "section": "4.5 Ablation Studies"}, {"figure_path": "PquRXu9pQ6/tables/tables_16_2.jpg", "caption": "Table 13: Detailed results of experiments on the hyperparameter of \u03bb.", "description": "This table presents the ablation study results on the hyperparameter \u03bb, which balances the intra-space and inter-space alignment losses in the Ex-MCR model.  It shows the impact of different \u03bb values on the retrieval performance (measured by R@5) across four datasets: FlickrNet, AVE, VGGSS, and AudioCaps. The results help to determine an optimal value for \u03bb that yields the best performance.", "section": "4.5 Ablation Studies"}]