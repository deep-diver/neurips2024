[{"type": "text", "text": "Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jerry Yao-Chieh Hu\u2217\u2020\u2021 Dennis Wu\u2217\u2021 Han Liu\u2020\u2021\u00a7 ", "page_idx": 0}, {"type": "text", "text": "\u2020Center for Foundation Models and Generative AI, \u2021Department of Computer Science, \u00a7Department of Statistics and Data Science, Northwestern University, Evanston, IL 60208, USA {jhu,hibb}@u.northwestern.edu, hanliu@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the optimal memorization capacity of modern Hopfield models and Kernelized Hopfield Models (KHMs), a transformer-compatible class of Dense Associative Memories. We present a tight analysis by establishing a connection between the memory configuration of KHMs and spherical codes from information theory. Specifically, we treat the stored memory set as a specialized spherical code. This enables us to cast the memorization problem in KHMs into a point arrangement problem on a hypersphere. We show that the optimal capacity of KHMs occurs when the feature space allows memories to form an optimal spherical code. This unique perspective leads to: (i) An analysis of how KHMs achieve optimal memory capacity, and identify corresponding necessary conditions. Importantly, we establish an upper capacity bound that matches the well-known exponential lower bound in the literature. This provides the first tight and optimal asymptotic memory capacity for modern Hopfield models. (ii) A sub-linear time algorithm $\\scriptstyle\\mathrm{U}-\\mathrm{Hop}+$ to reach KHMs\u2019 optimal capacity. (iii) An analysis of the scaling behavior of the required feature dimension relative to the number of stored memories. These efforts improve both the retrieval capability of KHMs and the representation learning of corresponding transformers. Experimentally, we provide thorough numerical results to back up theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the optimal memorization capacity of Kernelized modern Hopfield Models (KHMs) [Wu et al., 2024a], propose a sublinear-time algorithm to achieve it, and analyze parameter selection for these models. KHMs belong to a class of transformer-compatible Dense Associative Memory [Krotov and Hopfield, 2021, 2016] known as Modern Hopfield Models (MHMs) [Wu et al., 2024a,b, Hu et al., 2024a, 2023, Ramsauer et al., 2020]. The defining characteristics of these models include their super-linear memory capacity and strong connection to transformer attention mechanisms [Vaswani et al., 2017]. The former makes them interesting models for associative memory, and the latter makes them versatile transformer-compatible backbones with diverse empirical successes [Burns, 2024, Burns and Fukai, 2023, Hu et al., 2024a,c, Xu et al., 2024, Wu et al., 2024a,b, Hoover et al., 2023a, Seidl et al., 2022, F\u00fcrst et al., 2022]. However, one major limitation of MHMs is their reliance on the quality of memory distribution for effective pattern storage and retrieval [Wu et al., 2024a, Sec. 1]. ", "page_idx": 0}, {"type": "text", "text": "Studying this limitation in these models is fundamental and of practical importance. One one hand, it prevents MHMs from functioning as full-fledged content-addressable memory models. On the other hand, it implies that the representation learning ability of current transformer attention [Vaswani et al., 2017] is suboptimal [Wu et al., 2024a, Thm. 3.1]. Addressing this issue beneftis both computational neuroscience and large foundation model research [Bietti et al., 2024, Krotov, 2023, Kozachkov et al., 2023, Cabannes et al., 2023, Hoover et al., 2023b]. Kernelized modern Hopfield Models (KHMs) [Wu et al., 2024a] alleviate this issue by storing memories in the kernelized feature space. A key advantage of KHMs is their ability to reposition memories in the feature space, resulting in larger storage capacity. However, despite strong empirical performance, their capacity still lacks an optimal guarantee [Wu et al., 2024a, Sec. 5]. In this work, we close this gap by establishing the optimality of KHMs\u2019 memory capacity and presenting a sublinear-time algorithm to achieve it. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Let $\\Xi\\,:=\\,[\\xi_{1},\\cdot\\cdot\\cdot\\,,\\xi_{M}]\\;\\in\\;\\mathbb{R}^{d\\times M}$ be a set of memory patterns where each column (indexed by $\\mu\\in[M])$ represents a memory $\\xi_{\\mu}\\in\\mathbb{R}^{d}$ , and let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be the input query. The Hopfield models [Hopfield, 1982] are energy-based associative memory models, which store memories on the local minima of their energy functions. They retrieve a pattern by iteratively updating the query $x^{t}\\mapsto x^{t+1}$ with its update rule $\\dot{\\mathcal{T}}(\\boldsymbol{x}^{t})$ , for some $t\\in\\{0,1,\\bar{\\ldots}\\}$ . This update rule converge to a fixed point $x^{\\star}$ , defined by $x^{\\star}=\\mathcal{T}(x^{\\star})$ . $x^{\\star}$ is the retrieved pattern2 based on initial query $x^{0}$ . ", "page_idx": 1}, {"type": "text", "text": "Explicitly, iteratively updating $x$ with $\\tau$ is defined as a process of minimization to an energy function $E(x)$ . For example, the Modern Hopfield Model [Ramsauer et al., 2020] has the energy function: ", "page_idx": 1}, {"type": "equation", "text": "$$\nE(\\boldsymbol{x})=\\frac{1}{2}\\left\\langle\\boldsymbol{x},\\boldsymbol{x}\\right\\rangle+\\mathrm{lse}\\left(\\beta,\\Xi^{\\mathsf{T}}\\boldsymbol{x}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{lse}(\\beta,z):=\\log\\Bigl(\\sum_{\\mu=1}^{M}\\exp\\{\\beta z_{\\mu}\\}\\Bigr)}\\end{array}$ , with some $\\beta>0$ . With the Concave-Convex Procedure (CCCP) [Yuille and Rangarajan, 2001], (1.1) is monotonically decreased by an iterative update rule ", "page_idx": 1}, {"type": "equation", "text": "$$\nx^{t+1}\\leftarrow T(x^{t})=\\Xi\\cdot\\mathrm{Softmax}(\\beta\\Xi^{\\mathsf{T}}x^{t}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This design boosts MHMs to store exponentially (in pattern dimension $d$ ) many memories compared to the linear capacity of the classic Hopfield model [Hopfield, 1982]. It also provides a model-based interpretation of the transformer attention mechanism [Wu et al., 2024a,b, Hu et al., 2024a,b, 2023, Ramsauer et al., 2020]. However, their retrieval accuracy and memory capacity hinge on the quality of the stored memory set $[\\mathrm{Wu}$ et al., 2024a, Sec. 1], and hence are suboptimal in most scenarios. ", "page_idx": 1}, {"type": "text", "text": "To be concrete, for retrieving the $\\mu$ -th memory $(\\mu\\in[M])$ , the retrieval error of MHM is exponentially suppressed by the pattern separation: $\\Delta_{\\mu}\\;:=\\;\\langle\\xi_{\\mu}^{\\cdot},\\xi_{\\mu}^{\\cdot}\\rangle\\,-\\,\\mathrm{max}_{\\nu,\\nu\\neq\\mu}\\,\\langle\\xi_{\\nu},\\xi_{\\mu}\\rangle$ ([Wu et al., 2024a, Eqn. 1.3] or [Hu et al., 2023, Eqn. 2.7]). This $\\Delta_{\\mu}$ -dependence in MHM retrieval accuracy also manifests the $\\Delta_{\\mu}$ -dependence in memory capacity. To see this, recall that the standard memory capacity is a high-probability bound based on thresholding the separation $\\Delta_{\\mu}$ for each pattern $\\mu\\in[M]$ to determine storage and retrieval (Section 2.1). Explicitly, storing a pattern requires its separation to exceed a threshold that decreases with the minimal separation: $\\Delta_{\\mathrm{min}}:=\\mathrm{min}_{\\mu\\in[M]}\\,\\Delta_{\\mu}$ . Namely, a larger $\\Delta_{\\mathrm{min}}$ leads to larger capacity (Appendix B.1). Thus, the capacity depends on $\\Delta_{\\mathrm{min}}$ Yet, $\\Delta_{\\mathrm{min}}$ depends on the stored memories $\\Xi$ . This $\\Xi_{}$ -dependence makes the capacity suboptimal. ", "page_idx": 1}, {"type": "text", "text": "Wu et al. [2024a] relax such limitation by introducing a kernel as a learnable similarity measure, using stored memory patterns as training data to enhance memory capacity. Specifically, they propose the Kernelized Modern Hopfield Model (KHM) defined by following update rule and energy function: ", "page_idx": 1}, {"type": "equation", "text": "$$\nx^{t+1}\\gets\\mathcal{T}_{\\Phi}(x^{t}):=\\Xi\\cdot\\mathrm{Softmax}\\left(\\beta K(\\Xi,x)\\right),\\quad E_{K}(x)=\\frac{1}{2}K(x,x)+\\mathrm{lse}\\left(\\beta,K(\\Xi,x)\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the kernel $\\mathcal{K}(\\cdot,\\cdot)\\,:=\\,\\langle\\Phi(\\cdot),\\Phi(\\cdot)\\rangle\\,:\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is associated with a learnable feature map $\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{D_{\\Phi}}$ . Here, $\\boldsymbol{\\kappa}(\\cdot,\\cdot)$ acts column-wise on matrix: $K(\\Xi,x)\\,=\\,[\\{K(\\xi_{\\mu},x)\\}_{\\mu=1}^{M}]\\,=$ $[\\{\\langle\\Phi(\\xi_{\\mu}),\\Phi(x)\\rangle\\}_{\\mu=1}^{M}]\\in\\mathbb{R}^{M}$ . Importantly, KHMs shift the dependency on $\\Delta_{\\mathrm{min}}$ to $\\Delta_{\\mathrm{min}}^{\\Phi}$ , with ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{min}}^{\\Phi}:=\\operatorname*{min}_{\\mu\\in[M]}\\Delta_{\\mu}^{\\Phi},\\quad\\mathrm{where}\\quad\\Delta_{\\mu}^{\\Phi}:=K\\left(\\xi_{\\mu},\\xi_{\\mu}\\right)-\\operatorname*{max}_{\\nu,\\nu\\neq\\mu}K\\left(\\xi_{\\nu},\\xi_{\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Notably, $\\Delta_{\\mathrm{min}}^{\\Phi}$ is learnable and parameterized via $\\Phi$ . Wu et al. [2024a] point out that with $\\Phi(x)=$ $W x$ , where $W\\in\\mathbb{R}^{d\\times D_{\\Phi}}$ , finding a suitable $\\Phi$ that maximizes $\\Delta_{\\mathrm{min}}^{\\Phi}$ benefits memory storage. This construction of $\\Phi$ preserves key MHM properties, such as accurate [Wu et al., 2024a, Lemma 2.1] and consistent $\\mathrm{[Wu}$ et al., 2024a, Thm 2.1] retrieval. However, direct maximization of $\\Delta_{\\mathrm{min}}^{\\Phi}$ is challenging due to its max-min nature. To circumvent, Wu et al. [2024a] propose a surrogate loss to maximize $\\mathbf{\\bar{\\Delta}}\\Delta_{\\mu}^{\\Phi}$ on average [Wu et al., 2024a, Def. 2.2]. As a result, their approach achieves strong empirical results in memory retrieval for MHMs and supervised learning for transformer models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Nevertheless, maximizing $\\Delta_{\\mu}^{\\Phi}$ on average, rather than $\\Delta_{\\mathrm{min}}^{\\Phi}$ , raises questions about how their lsae ucarldreosa gtr aoat eon palltoiystmsi acbla el mnceehfmaitrsoa rcimtzeeartmiizooanrt iyco asntp,o aarcanitgdy e..n o Mthoreeoorevteirc, atl haen iamlypsaisc tc oofn \u2206\u03a6min  lacks wonh emtheemr omray xciampiazciintgy $\\Delta_{\\mathrm{min}}^{\\Phi}$ ", "page_idx": 2}, {"type": "text", "text": "In this paper, we address these questions from the perspective of (optimal) spherical codes from information thoery [Delsarte et al., 1991]. A spherical code is a set of points (vectors) distributed on the surface of a hypersphere, and an optimal spherical code is when the minimum angular distance between any two points is maximized. In other words, optimal spherical codes aim to spread the points as evenly as possible over the surface of the sphere. This aligns with the intuition behind KHM \u2014 increasing average separation between stored memories improves memory capacity. Therefore, we treat the stored memory pattern set as a spherical code (Definition 2.3), and require this spherical code to satisfy the well-separation condition [Hu et al., 2023, Thm 3.1]. We term this spherical code as memory code. Surprisingly, this unique connection enables a tight analysis on KHMs\u2019 capacity. ", "page_idx": 2}, {"type": "text", "text": "Contributions. Through the memory code perspective, this work makes three main contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Provably Optimal Capacity. We study the optimal memory capacity of KHMs and identify the conditions necessary to achieve it. Specifically, we derive a provably tight and optimal capacity by matching the well-known exponential lower bound for the memory capacity of MHMs [Wu et al., 2024a,b, Hu et al., 2023, Ramsauer et al., 2020] with an upper bound in the low-temperature region. Notably, we establish this tight bound by showing that KHMs store the most memories when the memory set forms an optimal spherical code (Lemma 2.2). This result suggests a tight exponential scaling of memory capacity with the pattern dimension $D_{\\Phi}$ (Proposition 2.1). ", "page_idx": 2}, {"type": "text", "text": "\u2022 Fast Algorithm. We introduce an algorithm, $\\mathrm{U-Hop+}$ , that achieves the optimal capacity of KHM in sublinear time. Theoretically, we show that, as temperature approaches zero, $\\mathrm{U-Hop+}$ finds the optimal feature map for maximal KHM capacity (Theorem 3.1). This result bridges our theoretical findings with practical applications and explains the empirical successes of [Wu et al., 2024a]. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Numerical Validation. Our experiments validate our theoretical analysis. We observe that (i) U-Hop+ creates distinct low-energy regions for each memory pattern, addressing the memory confusion problem in MHMs [Wu et al., 2024a, Krotov and Hopfield, 2016]; (ii) $\\mathrm{U-Hop+}$ significantly reduces metastable states on both MNIST and synthetic datasets, indicating larger memory capacity; (iii) with $\\mathrm{U-Hop+}$ , the KHMs update rule converges to fixed points faster. ", "page_idx": 2}, {"type": "text", "text": "Organization. Section 1 presents a brief review of MHMs and KHMs. Appendix A includes related work discussions. Section 2 presents our main results. Specifically, Section 2.1 presents a memory capacity lower bound for KHMs, Section 2.2 presents the optimal capacity bound based on the notation of memory code. Section 3.1 presents a sublinear time algorithm to search for the optimal $\\Phi$ . Section 3.2 discusses the relationship between $\\Phi$ and $M$ . Section 4 includes numerical experiments. ", "page_idx": 2}, {"type": "text", "text": "Notations. Lower case letters denote (column) vectors and upper case letters denote matrices. We write $\\langle a,b\\rangle:=a^{\\mathsf{T}}b$ as the inner product for vectors $a,b\\in\\mathbb{R}^{\\dot{d}}$ . The index set $\\{1,...,I\\}$ is denoted by $[I]$ , where $I\\in\\mathbb{N}^{+}$ . The spectral norm is denoted by $\\lVert\\cdot\\rVert_{2}$ which is equivalent to the $\\ell_{2}$ -norm when applied to a vector. We denote the memory patterns (keys) by $\\xi\\in\\mathbb{R}^{d}$ and the query pattern by $x\\in\\mathbb{R}^{d}$ , and $\\Xi:=[\\xi_{1},...,\\xi_{M}]\\,\\in\\,\\mathbb{R}^{d\\times M}$ as shorthand for stored memory patterns $\\{\\dot{\\xi}_{\\mu}\\}_{\\mu\\in[M]}$ Throughout this work, we use $\\Xi$ interchangeably to refer to either a $d\\times M$ matrix or a set of $\\bar{M}$ $d$ -dimensional memory pattern vectors. ", "page_idx": 2}, {"type": "text", "text": "2 Main Theory ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide a theoretical analysis on the optimal memory capacity of KHMs. First, we begin by comparing the memory capacity between MHM and KHM using the standard high-probability lower bound [Hu et al., 2023, Ramsauer et al., 2020]. Then, we present a spherical code perspective as a framework for depicting the optimal memory capacity of both MHMs and KHMs. In our analysis, we make the following pattern normalization assumption on memory patterns:3 ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. We assume memory patterns $\\|\\xi_{\\mu}\\|=1$ in the rest of our paper. ", "page_idx": 3}, {"type": "text", "text": "2.1 High-Probability Capacity Lower Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start by showing the memory capacity of KHM using the standard capacity lower bound introduced by Ramsauer et al. [2020]. This provides a direct comparison between KHMs and previous works. The definition of the generalized fixed point [Sriperumbudur and Lanckriet, 2009] is ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Generalized Fixed Point [Sriperumbudur and Lanckriet, 2009]). We say a set $S\\subseteq\\mathbb{R}^{d}$ is a generalized fixed point w.r.t. $\\tau$ if $\\tau(y)\\in S$ for every $y\\in S$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. In contrast to Definition 2.1, a fixed point of $\\tau$ is a point $y$ satisfying $\\tau(y)=y$ . ", "page_idx": 3}, {"type": "text", "text": "Let $S_{\\mu}^{\\Phi}$ be a ball with radius $R_{\\Phi}{}^{4}$ centered at every memory pattern in the feature space $\\Phi(\\xi_{\\mu})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{\\mu}^{\\Phi}=\\{y\\mid\\|\\Phi(\\xi_{\\mu})-y\\|\\leq R_{\\Phi}\\},\\quad\\mathrm{where}\\quad R_{\\Phi}:=\\frac{1}{2\\operatorname*{min}_{\\mu,\\nu\\in[M]}}\\|\\Phi(\\xi_{\\mu})-\\Phi(\\xi_{\\nu})\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Following [Wu et al., 2024a], we define the memory storage and retrieval as: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Pattern Storage and Retrieval). We say a memory pattern $\\xi_{\\mu}$ is stored if $S_{\\mu}^{\\Phi}$ is a generalized fixed point of $\\tau$ , and there exists a fixed point $x_{\\mu}^{\\star}\\in S_{\\mu}^{\\Phi}$ . A memory pattern $\\xi_{\\mu}$ gets $\\epsilon$ -retrieved by $\\tau$ with an input query $x$ if $\\|\\mathcal{T}(x)-\\xi_{\\mu}\\|\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "This definition is compatible with both KHMs and MHMs (with identity feature map). Under Definition 2.2, KHM\u2019s memory capacity is lower bounded by the following lemma. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1 (Memory Capacity of KHM). Let $1-p$ be the probability of successfully storing and retrieving a pattern. Assuming the patterns are normalized, the number of patterns $M_{\\Phi}$ that can be stored and retrieved by the KHM, following the update rule (1.3), is lower-bounded by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{\\Phi}\\geq\\sqrt{p}C^{(D_{\\Phi}-1)/4},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C$ is the solution to $C=b\\big/(W_{0}(\\exp\\{a+\\ln b\\}))$ , with $W_{0}(\\cdot)$ being the principal branch of Lambert $W$ function, $a\\,:=\\,\\bigl(4\\big/(D_{\\Phi}\\!-\\!1)\\big)\\,\\bigl(\\ln\\bigl((2\\sqrt{p}\\!-\\!2)\\big/R_{\\Phi}\\bigr)+1\\bigr)$ and $b:=\\,^{4\\beta}\\!/(5(D_{\\Phi}\\!-\\!1))$ . For comparison, $M_{\\Phi}$ reduces to MHM\u2019s capacity lower bound by setting $\\Phi=I_{d}$ , with $D_{\\Phi}=d$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. Our proof follows [Hu et al., 2023, Wu et al., 2024b]. See Appendix C.1 for a proof. ", "page_idx": 3}, {"type": "text", "text": "With a fixed $D_{\\Phi}$ , the highest lower bound of Lemma 2.1 corresponds to specific a $\\Phi$ that maximizes $R_{\\Phi}$ . This provides an intuitive insight on the design of separation loss [Wu et al., 2024a, Definition 2.2] for kernel learning in [Wu et al., 2024a, Algorithm 1]. With an additional feature space, KHM has an exponential memory capacity in $D_{\\Phi}$ that does not depend on $d$ . When $D_{\\Phi}=d$ , KHMs obtain a tighter lower bound than MHMs if $R_{\\Phi}>R$ . This bound connects the storage capacities of KHMs and MHMs, showing that their capacities scale exponentially with respect to $D_{\\Phi}$ and $d$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Memory Code: Memories as Spherical Code ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There are two aspects the lower bound in Lemma 2.1 does not address: the maximal capacity of KHMs and the flexibility of choosing different $\\Phi$ in KHMs. Therefore, we present a new framework using spherical codes to take the above perspectives into consideration for further analysis. We begin by introducing the concepts of spherical code and optimal spherical code. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 (Spherical Code). A $d_{\\cdot}$ -dimensional spherical code on the unit sphere $\\mathbb{S}^{d-1}$ is a finite set $\\mathcal{C}_{N}=\\{c_{1},...,c_{N}\\}$ of $\\mathbb{S}^{d-1}$ with $N$ points, where $c_{i}\\in\\mathbb{R}^{d}$ for $i\\in[N]$ and $|{\\mathcal{C}}_{N}|=N$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 (Minimal Separation). The minimal separation $\\rho(\\mathcal{C}_{N})$ of a spherical code $\\mathcal{C}_{N}$ is the maximal inner product between two distinct points in $\\mathcal{C}_{N}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho(\\mathcal{C}_{N})=\\operatorname*{max}_{c_{i},c_{j}\\in\\mathcal{C}_{N}}\\left\\langle c_{i},c_{j}\\right\\rangle,\\quad\\mathrm{for\\,every}\\ i\\neq j.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 2.5 (Optimal Spherical Code). Let $\\mathcal{C}_{N}\\,=\\,\\{c_{1},\\dots,c_{N}\\}\\,\\subseteq\\,\\mathbb{S}^{d-1}$ be a $d$ -dimensional spherical code with $N$ points. An optimal spherical code $\\mathcal{C}_{N}^{\\star}$ minimizes the maximal pairwise inner product, which corresponds to maximizing the minimal separation between points in the code. Formally, the optimal spherical code $\\mathcal{C}_{N}^{\\star}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}_{N}^{\\star}=\\underset{\\mathcal{C}_{N}\\subset\\mathbb{S}^{d-1}}{\\operatorname{argmin}}\\,\\underset{i\\neq j}{\\operatorname*{max}}\\left\\langle c_{i},c_{j}\\right\\rangle,\\quad\\mathrm{for}\\;i,j\\in[N].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The minimal separation of the optimal spherical code is denoted as $\\rho^{\\star}$ .a ", "page_idx": 4}, {"type": "text", "text": "aThe optimal arrangement of most spherical codes is unknown, except for specific pairs of $(d,N)$ . A list of known optimal arrangements and minimal separations can be found at http://neilsloane.com/ packings/ and in [Conway and Sloane, 2013]. ", "page_idx": 4}, {"type": "text", "text": "Next, we recall the function class $\\mathcal{H}$ of the linear feature map introduced by Wu et al. [2024a]: ", "page_idx": 4}, {"type": "text", "text": "Definition 2.6. The function class $\\mathcal{H}$ consists of linear maps that satisfy the following properties: 1. For all $\\Phi\\in{\\mathcal{H}}$ , $\\Phi:\\mathbb{S}^{d-1}\\rightarrow\\mathbb{S}^{D_{\\Phi}-1}$ is a linear map defined by a matrix $W\\in\\mathbb{R}^{d\\times D_{\\Phi}}$ . 2. The matrix $W$ has full column rank. 3. When applying $\\Phi$ to different inputs: \u2022 For a vector $\\xi\\in\\mathbb{R}^{d}$ , $\\Phi(\\xi)=W^{\\mathsf{T}}\\xi\\in\\mathbb{R}^{D_{\\Phi}}$ . \u2022 For a matrix $\\Xi\\in\\mathbb{R}^{d\\times M}$ , $\\Phi(\\Xi)=(\\Phi(\\xi_{1}),\\dots,\\Phi(\\xi_{M}))\\in\\mathbb{R}^{D_{\\Phi}\\times M}.$ \u2022 For a set of vectors $\\mathcal{V}=\\{v_{1},\\ldots,v_{N}\\}$ , $\\Phi(\\mathcal{V})=\\left\\{\\Phi(v_{1}),\\ldots,\\Phi(v_{N})\\right\\}$ with $|\\Phi(\\mathcal{V})|=N$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.6 ensures KHMs with feature map $\\Phi(\\cdot)\\in\\mathcal{H}$ satisfying the defining characteristics of MHMs: accurate [Wu et al., 2024a, Lemma 2.1] and consistent [Wu et al., 2024a, Thm 2.1] retrieval according to Definition 2.2.5 Now, we combine the concept of spherical code and memory storage. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.7 (Kernelized Well-Separation Condition [Wu et al., 2024a,b, Hu et al., 2023, Ramsauer et al., 2020]). Given a set of kernelized memory patterns $\\Phi(\\Xi)\\;=\\;\\{\\Phi(\\xi_{\\mu})\\}_{\\mu=1}^{M}\\;\\subseteq\\;\\mathbb{S}^{D_{\\Phi}-1}$ , the kernelized memory pattern $\\Phi(\\xi_{\\mu})$ satisfies the well-separation condition if the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\mu}^{\\Phi}\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the inverse temperature $\\beta$ is given by (1.3) and $R_{\\Phi}$ is defined by (2.1). ", "page_idx": 4}, {"type": "text", "text": "The inequality (2.2) is a necessary condition for the $\\mu$ -th memory to have a well-defined attractor basin. Hence, the more memories satisfying (2.2) the greater the memory capacity of the model. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.8 (Memory Code). Let $M\\in\\ensuremath{\\mathbb{N}}_{+}$ , $\\beta\\,>\\,0$ , $D_{\\Phi}\\,>\\,1$ and $\\Phi\\in\\mathcal H$ . For any finite set $\\Phi(\\Xi)=\\{\\Phi(\\xi_{\\mu})\\}_{\\mu=1}^{M}\\subseteq\\^{\\cdot}\\mathbb{S}^{D_{\\Phi}-1}$ , we say the set $\\Phi(\\Xi)$ is a memory code if all points in $\\Phi(\\overbar{\\Xi})$ satisfies (2.2). Further, we denote $\\Lambda_{D_{\\Phi}}$ as the set of all memory codes in $\\mathbb{S}^{D_{\\Phi}-1}$ , including all possible $\\Xi,\\Phi$ . ", "page_idx": 4}, {"type": "text", "text": "Notably, $\\Lambda$ includes all the possible pattern sets $\\{\\Phi(\\Xi)\\}$ that are able to be stored and retrieved by kernelized Hopfield models and modern Hopfield models. Naturally, the optimal memory capacity is the size of the largest memory code in $\\mathbb{S}^{D_{\\Phi}\\bar{-}1}$ . This leads to our next definition: ", "page_idx": 4}, {"type": "text", "text": "Definition 2.9 (Optimal Memory Capacity). For $D_{\\Phi}>1$ and $\\Phi\\in{\\mathcal{H}}$ , the optimal capacity $M^{\\star}$ is the cardinality of the largest memory code in $\\Lambda_{D_{\\Phi}}$ , i.e., $M^{\\star}:=\\operatorname*{max}_{\\Phi(\\Xi)\\in\\Lambda}|\\Phi(\\Xi)|$ for all possible $\\Xi,\\Phi$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.9 specifies the largest possible memory code in $\\Lambda_{D_{\\Phi}}$ for a given $D_{\\Phi}$ . Let $\\widetilde{\\Xi}$ denote the memory set associated with $M^{\\star}$ , such that $\\|\\widetilde{\\Xi}\\|=M^{\\star}$ . To store all patterns in\u039e, we need to find a suitable feature map $\\widetilde{\\Phi}$ such that $\\widetilde\\Phi(\\widetilde\\Xi)$ is a valid memory code. ", "page_idx": 4}, {"type": "text", "text": "Following this definition, we present the next lemma and proposition on optimal memory capacity. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2 (Capacity of Optimal Spherical Code). Given a fixed $D_{\\Phi}>1$ , and its corresponding $M^{\\star}$ , if an optimal code $\\mathcal{C}_{\\mathrm{opt}}$ is in $\\mathbb{S}^{D_{\\Phi}^{\\star}-1}$ and has size $M^{\\star}$ , then $\\mathcal{C}_{\\mathrm{opt}}\\in\\Lambda_{D_{\\Phi}}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1 (Optimal Memory Capacity). Following Lemma 2.2, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nM^{\\star}\\asymp c^{D_{\\Phi}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some $c>1$ . Here $\\asymp$ indicates matching upper and lower bounds up to constant factors. ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. We proof Lemma 2.2 by showing that the model capacity is a increasing function w.r.t. the minimal separation value. For Proposition 2.1, we utilize the upper bound in [Kabatiansky and Levenshtein, 1978] and lower bound in [Wyner, 1965, Shannon, 1959, Chabauty, 1953] to bound the quantity. Please see Appendix C.2 for a detailed proof. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1 indicates that the optimal capacity of MHMs and KHMs scales exponentially with $D_{\\Phi}$ . This capacity bound is provably tight and optimal for large feature dimension $D_{\\Phi}$ . It echos the exponential capacity lower bound in Lemma 2.1 and in prior works [Wu et al., 2024a,b, Hu et al., 2024a,b,c, 2023, Ramsauer et al., 2020]. Moreover, Lemma 2.2 shows that achieving the maximal capacity in any $D_{\\Phi}$ is equivalent to achieving optimal codes. Thus, for a given memory set $\\Xi$ of size $M$ , the memory storage problem with KHMs divides into two sub-problems: ", "page_idx": 5}, {"type": "text", "text": "$(\\mathbf{P1})$ Finding a sufficiently large $D_{\\Phi}$ (in Section 3.2), and ", "page_idx": 5}, {"type": "text", "text": "$(\\mathbf{P}2)$ Finding a $\\Phi$ such that $\\Phi(\\Xi)$ is an optimal spherical code (in Section 3.1).   \nNext, we examine these two sub-problems and present a sub-linear time algorithm to solve them. ", "page_idx": 5}, {"type": "text", "text": "3 Sub-Linear Time Algorithm for Optimal Memory Capacity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present an sub-linear time algorithm that achieves optimal capacity. Then, we analyze the scaling behavior of $D_{\\Phi}$ for KHMs to store any desired amount of memories. ", "page_idx": 5}, {"type": "text", "text": "3.1 Learning to Achieve Optimal Memory Code ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here we present an asymptotic result showing that an algorithm exists to find the optimal $\\Phi$ for maximizing memory storage in dimension $D_{\\Phi}$ . Building on the results from the previous sections, we consider the following problem: ", "page_idx": 5}, {"type": "text", "text": "Problem 1 (HardMax Problem). Given a memory set $\\Xi=\\{\\xi_{1},\\ldots,\\xi_{M}\\}$ , and assuming that $D_{\\Phi}$ is sufficiently large to satisfy (2.2), we define the HardMax problem as finding a $\\Phi$ such that $\\Phi(\\Xi)$ forms an optimal spherical code: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Phi\\in\\mathcal{H}}\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi),\\quad\\mathrm{where}\\quad\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi):=\\operatorname*{max}_{\\nu,\\mu\\in[M],\\nu\\not=\\mu}\\langle\\Phi(\\xi_{\\mu}),\\Phi(\\xi_{\\nu})\\rangle\\geq\\rho^{\\star}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This problem setup involves finding a $\\Phi$ such that $\\Phi(\\Xi)$ forms an optimal spherical code. Ideally, a more expressive function class $\\mathcal{H}$ would simplify finding such a $\\Phi$ ; exploring explicit forms of more powerful mappings is left for future work. Note that (3.1) represents a min-max optimization problem. Achieving the global optimum is notoriously challenging [Hsieh et al., 2021, Daskalakis et al., 2021, Shen et al., 2020]. Thus, we introduce a surrogate objective to solve (3.1): ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Average Separation Loss). For $\\tau>0$ , given a set of memory patterns $\\Xi$ and a feature map $\\Phi$ , we define the average separation loss as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\Xi,\\tau,\\Phi):=\\frac{1}{M}\\sum_{\\mu=1}^{M}\\ell_{\\mu}(\\Xi,\\Phi,\\tau),\\mathrm{~where~}\\ell_{\\mu}(\\Xi,\\Phi,\\tau):=\\log\\left[\\sum_{\\nu=1}^{M}\\exp\\left(\\frac{\\left<\\Phi(\\xi_{\\mu}),\\Phi(\\xi_{\\nu})\\right>}{\\tau}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The primary difference between (3.1) and (3.2) is that (3.2) calculates average separation, whereas (3.1) focuses on the maximum separation between a single pair. This surrogate loss alleviates the challenging optimization, as (3.2) is convex. Therefore, with vanishing temperature $\\tau$ , the next theorem shows that (3.2) converges to the HardMax problem asymptotically. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. For any possible integer $M$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau\\to0}\\operatorname*{sup}_{\\mathbf{\\Phi}}\\left(\\operatorname{argmin}_{\\Phi\\in\\mathcal{H}}\\mathcal{L}(\\Xi,\\Phi,\\tau)\\right)\\subseteq\\operatornamewithlimits{a r g m i n}_{\\Phi\\in\\mathcal{H}}\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Input: Iterations $N$ , feature map $\\Phi(x):=W x$ , memory set \u039e, learning rate $\\gamma\\le1/G$ where $G$ is the Lipschitz constant of $\\mathcal{L}$ ", "page_idx": 6}, {"type": "text", "text": "1: $W_{0}\\leftarrow W$   \n2: for $t=0,...N-1$ do   \n3: $W_{t+1}\\gets\\tt P G D\\left(W_{t},\\gamma,\\Xi\\right)$   \n4: end for   \n5: return $W_{N}$ ", "page_idx": 6}, {"type": "text", "text": "Proof. we first introduce a helper function $\\scriptstyle{\\mathcal{L}}_{0}$ in (C.10). We show that as $\\tau\\rightarrow0$ , $\\scriptstyle{\\mathcal{L}}_{0}$ converges uniformly to $\\mathcal{L}_{\\mathrm{HardMax}}$ . Then, we prove that optimizing $\\scriptstyle{\\mathcal{L}}_{0}$ and $\\mathcal{L}$ yields the same optimal solution. Please see Appendix C.3 for a detailed proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 indicates that, with vanishing temperature, the minimiozation of (3.2) converges to the HardMax problem, i.e., their share the same optimal solution. This provide a theoretical justification for the empirical success of [Wu et al., 2024a]. In particular, the surrogate objective \u2013 the maximizing average separation between memories \u2014 leads to provably optimal memory capacity in low-temperature region (i.e., $\\tau\\rightarrow0$ ). Lastly, we remark that that this analysis provides theoretical insights rather than practical guidance. To achieve high retrieval accuracy, the setting $\\left[\\tau=1\\right]$ ) in $[\\mathrm{Wu}$ et al., 2024a] is sufficient for a wide range of applications. ", "page_idx": 6}, {"type": "text", "text": "U-Hop+: Sub-Linear Time Algorithm for Achieving Optimal Memory Capacity. Next, we present Algorithm 1 for finding a $\\Phi$ such that $\\Phi(\\Xi)$ forms an optimal spherical code. To meet the conditions in Definition 2.6, we use projected gradient descent to convert this constrained optimization problem into an unconstrained one. Several methods satisfy the requirements in Definition 2.6; we discuss them in Appendix B.2. We denote the learning rate as $\\gamma$ , the input matrix of the loss function as $X$ , and the weight matrix as $W$ . We define a single Projected Gradient Descent (PGD) step as ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{t+1}=\\operatorname{PGD}(W_{t},\\gamma,X),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We defer the detailed formulation to Appendix B.2. Since the separation loss is convex and smooth, using projected gradient descent with a learning rate $\\gamma\\leq1/G$ , yields a sub-linear convergence rate of $\\mathcal{O}(1/N)$ [Iusem, 2003]. This provides an asymptotic solution to the first sub-problem ((P1)). Next, we examine the relationship between feature dimension $D_{\\Phi}$ and the number of memories $M$ . ", "page_idx": 6}, {"type": "text", "text": "3.2 Impact of $D_{\\Phi}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This subsection analyzes the minimum $D_{\\Phi}$ to store a given set of $M$ memories. Based on the wellseparation condition and the derivation in Appendix C.2, the required $\\Delta_{\\mathrm{min}}^{\\Phi}$ to store $M$ memories scales as $\\mathcal{O}(\\ln(M))$ . With this insight, the following proposition shows the scaling behavior of required $D_{\\Phi}$ with respect to $M$ and $\\bar{\\Delta}_{\\mathrm{min}}^{\\Phi}$ min ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1. Let $M^{\\star}$ be the optimal memory capacity in $\\mathbb{S}^{D_{\\Phi}}$ and $\\Phi\\in{\\mathcal{H}}$ . For any optimal code $C^{\\star}$ in $\\mathbb{S}^{D_{\\Phi}-1}$ of size $M^{\\star}$ , the minimal separation $\\rho(C^{\\star})$ is bounded by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(\\frac{\\sqrt{\\pi}}{M^{\\star}}\\cdot\\frac{\\Gamma\\left(\\frac{D_{\\Phi}+1}{2}\\right)}{\\Gamma\\left(\\frac{D_{\\Phi}}{2}+1\\right)}\\right)^{\\frac{2}{D_{\\Phi}-1}}\\leq\\operatorname*{max}_{\\Phi\\in\\mathcal{H}}\\Delta_{\\operatorname*{min}}^{\\Phi}\\leq2\\left(\\frac{2\\sqrt{\\pi}}{M^{\\star}}\\cdot\\frac{\\Gamma\\left(\\frac{D_{\\Phi}+1}{2}\\right)}{\\Gamma\\left(\\frac{D_{\\Phi}}{2}\\right)}\\right)^{\\frac{1}{D_{\\Phi}-1}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the gamma function. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. By gamma function asymptotics, Proposition 3.1 is consistent with Proposition 2.1. ", "page_idx": 6}, {"type": "text", "text": "Proof. Please see Appendix C.4 for a detailed proof. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1 establishes the separation value for memory codes that achieve optimal capacity in $D_{\\Phi}$ -dimensional space. Using this bound, for a given separation value $\\Delta_{\\mathrm{min}}^{\\Phi}$ , the minimum $D_{\\Phi}$ required to store $M$ points scales as $\\log\\left(M^{2}/\\Delta_{\\operatorname*{min}}^{\\Phi}\\right)$ . We conduct an experiment to demonstrate the bound\u2019s tightness and provide an example with $D_{\\Phi}=3$ in Figure 3. ", "page_idx": 6}, {"type": "table", "img_path": "4UReW4Ez6s/tmp/1eb8bfb11e9f65c0490dc3da5f514879d9b2e6f1197aaf603ad96448e624a130.jpg", "table_caption": ["Table 1: Distribution of Metastable State $(\\mathbf{in}\\,\\%)$ ). For MNIST, we use the training set as memories and test set as queries. For synthetic data, we randomly generate the memories and queries. $\\left\\|p\\right\\|_{0}$ denotes the size of metastable state , which is the amount of non-zero entries of the probability distribution. For Softmax, we use a threshold of 0.01. For hyperparameter settings, see Table 3. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Experimental Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 U-Hop+ Reduces Metastable States ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the distribution of metastable state size under standard MHM and KHM update rules. The results are in Table 1. In general, with more metastable state having the size of 1, meaning the Hopfield model stores more memories as the query converges to a single memory. For metastable state size larger than 1, it represents that the retrieved pattern converges near the mean of a subset of memories, violating the requirement of $S_{\\mu}^{\\Phi}\\cap S_{\\mu}^{\\Phi}=\\emptyset$ . ", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare different variants of MHMs and KHMs. Santos et al. [2024], Wu et al. [2024b] provide comprehensive analyses of modern Hopfield models with various normalization functions. Here, we consider softmax, 1.5-entmax, and sparsemax for normalization. We equip these three baselines with U-Hop to compare against standard MHMs. ", "page_idx": 7}, {"type": "text", "text": "Settings and Metrics. Let $p=\\mathrm{Softmax}(\\beta x^{\\mathsf{T}}\\Xi)$ . We determine whether the update rule converges to either a single memory or a mixture of memories by observing the probability distribution $p$ . The quantity $\\|p\\|_{0}$ represents the size of the metastable state, which is the number of non-zero entries in the probability distribution. In the case of 1.5-entmax and sparsemax, we calculate $\\left\\|p\\right\\|_{0}$ directly. For softmax, since it only generates non-zero entries, we use a threshold of 0.01 and consider the entries under the threshold as 0. We conduct experiments using both synthetic and MNIST datasets. For MNIST, we use the training set as memories and the test set as queries. For synthetic datasets, we randomly generate memories and queries with Gaussian initialization. To ensure the convergence to the fixed point, we perform multiple updates on the query. For more details, refer to Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Results. On both synthetic and MNIST datasets, it is evident that under separation maximization, the size of the metastable state dramatically decreases within just 20 iterations of Algorithm 1. This result demonstrates that, with Algorithm 1, KHMs are capable of storing patterns that MHMs cannot store. The significant percentage of size 1 metastable states in KHMs indicates that they circumvent the memory confusion problem in dense associative memory models [Krotov and Hopfield, 2016]. For the MNIST dataset, we see MHMs show close performance with KHMs under 1.5-entmax and sparsemax, showing that the methods in [Santos et al., 2024, Wu et al., 2024b, Hu et al., 2023] also circumvent the memory confusion problem. Notably, KHMs require only one-fourth of the dimensions to store memories while perfectly storing 60,000 MNIST patterns. These results suggest that KHMs with Algorithm 1 efficiently utilize feature dimensions for memory storage. ", "page_idx": 7}, {"type": "text", "text": "4.2 Energy Landscape under U-Hop $^+$ Stores More Memories ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Settings and Metrics. We visualize the energy landscape of KHMs at different stages of Algorithm 1 using contour plots. The results are presented in Figure 1. We consider two settings: 2 and 4 memories stored in a 2-dimensional space. Ideally, the energy landscape should position memories in multiple separated low-energy regions (valley), with each region isolated from others by high-energy regions. If multiple memories share the same valley, it leads to memory confusion and the presence of metastable states during energy minimization. For experiment details, refer to Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Results. The first row in Figure 1 shows the raw energy landscape without KHM and Algorithm 1, corresponding to the modern Hopfield energy landscape. On the right side of Figure 1, we observe that MHMs are only able to store 2 out of 4 points, but KHMs are able to further separate one point ", "page_idx": 7}, {"type": "image", "img_path": "4UReW4Ez6s/tmp/cb47763065d22ab8cd5c554b32ac82c7eee0a1c1d9cb5ad6b878121a58100cbf.jpg", "img_caption": ["Figure 1: Energy Landscape under Different Iterations of Algorithm 1. Left: $M=2$ , Right: $M=4$ . Lighter color represents higher energy. The first row represents the raw energy landscape without applying U-Hop+. The second to last row represents the energy landscape when $N=(1,2,5)$ . The visualization shows that Algorithm 1 not only separates the local minima better, but also pushes memories closer to the fixed point. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Test AUC of Multiple Instance Learning Datasets. We compare the HopfieldPooling-based model with and without U-Hop+. We use the dense [Ramsauer et al., 2020] and sparse $\\mathrm{[Hu}$ et al., 2023] modern Hopfield models as baselines. We use $K$ -fold cross validation on all 4 datasets, with $K=10$ . The reported AUC is the average AUC score across 10 folds. For the baselines, we use the results reported in [Hu et al., 2023]. For our method, we directly use the default hyperparameter without grid search instead of using hyperparameter optimization (HPO) in [Hu et al., 2023, Ramsauer et al., 2020]. We exclude the variance as they are all smaller than 0.07. The result shows that even without HPO, $\\scriptstyle\\mathrm{U}-\\mathrm{Hop}+$ is still able to obtain a performance gain. ", "page_idx": 8}, {"type": "table", "img_path": "4UReW4Ez6s/tmp/3a2aa3aafb684152c92d87ea90b5dac6d5179fc9789d5060ee36781542954e2d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "from the others, resulting in memorizing one extra pattern. Lemma 2.2 and Theorem 3.1 indicate that $\\mathrm{U-Hop+}$ pushes memories away from each other, providing more isolated $S_{\\mu}^{\\Phi}$ , for $\\mu\\in[M]$ . We observe this phenomenon across all settings, especially under the 2-point configuration with Softmax and 1.5-entmax, where the low-energy region is split into two distinct valleys as $N$ increases. This process shows how $\\mathrm{U-Hop^{+}}$ is able to store memories that MHMs cannot. With energy minimization, the query converges to either one of the minima instead of the mixture thereof (also showed in Figure 2). Additionally, we also notice that the contour lines exhibit steep slopes between different local minima in the 2-point setting under 1.5-entmax and sparsemax. This implies that $\\mathrm{U-Hop+}$ pushes local minima further away from each other and deepens each one of them. Such sharp changes in energies lead to faster convergence to fixed points due to larger gradients. ", "page_idx": 8}, {"type": "text", "text": "Basins of Attraction. Figure 2 shows the basins of attraction of queries w.r.t. MHM and KHM under the scenario of storing 5 patterns. We randomly initialize 5 patterns with normal distribution. We run the update rule for 5 iterations and see whether each query converges to a single memory (colored) or to a metastable state (white). Following the above setting, we track the attraction basins throughout each iteration of Algorithm 1. We defer more details to Appendix D.3. Specifically, most MHM variants are not capable of converging to fixed points in 5 updates. While $\\mathrm{U-Hop+}$ dramatically improves such aspect, where most queries are able to converge either one of the memories. Moreover, the increased $R_{\\Phi}$ also leads to a larger $S_{\\mu}^{\\Phi}$ , making more queries to converge to a single memory. Additionally, there is a performance gap between Softmax $\\left.\\alpha\\right.=1$ ) and other sparse variants, which matches the findings in [Santos et al., 2024, Hu et al., 2023]. ", "page_idx": 8}, {"type": "text", "text": "4.3 Multiple Instance Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct multiple instance learning (MIL) on 4 real-world datasets using Hopfied-based models with and without our U-Hop+ algorithm. We follow the setup in [Santos et al., 2024, Wu et al., 2024a, Hu et al., 2023] by using a model with 1 embedding layer, 1 HopfieldPooling layer and a linear readout layer. We first utilize Algorithm 1 to \u201cpretrain\u201d the embedding and HopfieldPooling layer, and then fine-tune the whole model on the MIL task. The results are in Table 2. We observe that both dense and sparse Hopfield-based models obtain performance boost when equipped with $\\mathrm{U-Hop+}$ , indicating our method is also effective in practical scenarios. Further, as demonstrated in Figure 5, the separation loss converges fast, indicating U-Hop $^{+}$ is a lightweight method for performance boost. ", "page_idx": 8}, {"type": "image", "img_path": "4UReW4Ez6s/tmp/2640d96c29e264c35980dd25a24405061e00643aba44d9fa8b8976941b950892.jpg", "img_caption": ["Figure 2: Basins of Attraction Comparison of Algorithm 1. The first row represents the raw Basins of Attraction without applying $\\scriptstyle\\mathrm{U}-\\mathrm{Hop}+$ or KHM. The second to last row shows the basins when $N=(1,2,5)$ . Square points are memories. White area is where queries are not able to converge to a single memory. Colored area is where queries converges to the corresponding memory. The result indicates that $\\scriptstyle\\mathrm{U}-\\mathrm{Hop}+$ is capable of converging to fixed point fast and reduce metastable states. 1 and 2-entmax corresponds to Softmax [Ramsauer et al., 2020] and Sparsemax [Hu et al., 2023]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work complements U-Hop [Wu et al., 2024a] by establishing the optimal capacity of kernelized modern Hopfield models (KHMs) and providing the first tight, optimal memory capacity bound for transformer-compatible dense associative memories. We start by connecting stored memories in KHMs to spherical codes from information theory. We then prove that maximizing memory storage in KHMs requires arranging memories as an optimal spherical code in feature space. This allows us to matches the well-known exponential lower bound [Wu et al., 2024a,b, Hu et al., 2024a,b,c, 2023, Ramsauer et al., 2020] with an upper bound. This achievement is notable, as deriving such a tight bound is challenging due to the max-min structure of maximal separation among stored memories [Wu et al., 2024a, Section 5]. Moreover, we introduce a sub-linear time algorithm to achieve this optimal capacity, $\\mathrm{U-Hop+}$ (Algorithm 1). U-Hop $^+$ performs this rearrangement with a convergence rate of $\\bar{\\mathcal{O}}(\\bar{17}N)$ . Additionally, we analyze the minimum dimension $D_{\\Phi}$ required to store $M$ memories. Numerically, we validate the effectiveness of KHMs and demonstrate how Algorithm 1 enhances memory storage in both KHM retrieval tasks and transformer representation learning tasks. ", "page_idx": 9}, {"type": "text", "text": "Can U-Hop $^+$ Preserve Semantic Meanings? In representation learning, it is crucial to preserve relationships in the feature space after encoding data [Wang et al., 2023, Neelakantan et al., 2022]. The primary strategy is to ensure the embeddings of similar instances share similar directions in Euclidean space. At first glance, the approach of pushing all memories away from each other in Equation (3.2) may seem counterintuitive. However, as detailed in Appendix D.5, we find that the learned feature map still encodes similar instances closely together (Figure 4), even without semantic information involved. This result indicates that $\\mathrm{U-Hop+}$ stores memories in a semantically coherent manner. A discussion of the separation capability of $\\Phi$ can be found in Figure 4. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation of our work only considers linear affine functions as the feature map $\\Phi\\in\\mathcal H$ . Additionally, standard spherical code analysis focuses only on normalized points on a hypersphere, ignoring memories with varying magnitudes. We leave them for future research. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We expect no negative social impacts as this work mostly present theoretical results and numerical simulations. As discussed in our introduction, this paper develops a theoretical framework to study Kernelized Hopfield models, potentially benefit the area of computational associative (Hopfield) memory models, transformer networks and large foundation models. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "JH thanks Thomas Burns, Dmitry Krotov, Dino Feng, and Andrew Chen for enlightening discussions; Robin Luo, Jiahao Yu, Weimin Wu, and Teng-Yun Hsiao for collaboration on related topics; the Red Maple Family for their support; and Jiayi Wang for facilitating experimental deployments. The authors also thank the anonymous reviewers and program chairs for their constructive comments. ", "page_idx": 10}, {"type": "text", "text": "JH is partially supported by the Walter P. Murphy Fellowship. DW is supported by NIH R01LM1372201. HL is partially supported by NIH R01LM1372201, AbbVie and Dolby. This research was supported in part through the computational resources and staff contributions provided for the Quest high performance computing facility at Northwestern University which is jointly supported by the Office of the Provost, the Office for Research, and Northwestern University Information Technology. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nAndreas Auer, Martin Gauch, Daniel Klotz, and Sepp Hochreiter. Conformal prediction for time series with modern hopfield networks. Advances in Neural Information Processing Systems, 36: 56027\u201356074, 2023.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   \nAlberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36, 2024.   \nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \nAndrea Braides. A handbook of $\\gamma$ -convergence. In Handbook of Differential Equations: stationary partial differential equations, volume 3, pages 101\u2013213. Elsevier, 2006.   \nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \nThomas F Burns. Semantically-correlated memories in a dense associative model. arXiv preprint arXiv:2404.07123, 2024.   \nThomas F Burns and Tomoki Fukai. Simplicial hopfield networks. arXiv preprint arXiv:2305.05179, 2023.   \nVivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. arXiv preprint arXiv:2310.02984, 2023.   \nClaude Chabauty. Resultats sur lempilement de calottes egales sur une perisphere de rn et correction a un travail anterieur. COMPTES RENDUS HEBDOMADAIRES DES SEANCES DE L ACADEMIE DES SCIENCES, 236(15):1462\u20131464, 1953.   \nJohn Horton Conway and Neil James Alexander Sloane. Sphere packings, lattices and groups, volume 290. Springer Science & Business Media, 2013.   \nConstantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466\u20131478, 2021.   \nPhilippe Delsarte, Jean-Marie Goethals, and Johan Jacob Seidel. Spherical codes and designs. In Geometry and Combinatorics, pages 68\u201393. Elsevier, 1991.   \nMete Demircigil, Judith Heusel, Matthias L\u00f6we, Sven Upgang, and Franck Vermet. On a model of associative memory with huge storage capacity. Journal of Statistical Physics, 168:288\u2013299, 2017.   \nJacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \nSjoerd Dirksen, Martin Genzel, Laurent Jacques, and Alexander Stollenwerk. The separation capacity of random neural networks. Journal of Machine Learning Research, 23(309):1\u201347, 2022.   \nIrene Gil Fern\u00e1ndez, Jaehoon Kim, Hong Liu, and Oleg Pikhurko. New lower bounds on kissing numbers and spherical codes in high dimensions. arXiv preprint arXiv:2111.01255, 2021.   \nAndreas F\u00fcrst, Elisabeth Rumetshofer, Johannes Lehner, Viet T Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Angela Bitto, et al. Cloob: Modern hopfield networks with infoloob outperform clip. Advances in neural information processing systems (NeurIPS), 35:20450\u201320468, 2022.   \nPromit Ghosal, Srinath Mahankali, and Yihang Sun. Randomly initialized one-layer neural networks make data linearly separable. arXiv preprint arXiv:2205.11716, 2022.   \nFlorian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised contrastive learning. In International Conference on Machine Learning, pages 3821\u20133830. PMLR, 2021.   \nZexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogerio Feris. Camelot: Towards large language models with training-free consolidated associative memory. arXiv preprint arXiv:2402.13449, 2024.   \nClaus Hofmann, Simon Schmid, Bernhard Lehner, Daniel Klotz, and Sepp Hochreiter. Energy-based hopfield boosting for out-of-distribution detection. arXiv preprint arXiv:2405.08766, 2024.   \nBenjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau, Mohammed J Zaki, and Dmitry Krotov. Energy transformer. arXiv preprint arXiv:2302.07253, 2023a.   \nBenjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, and Duen Horng Chau. Memory in plain sight: A survey of the uncanny resemblances between diffusion models and associative memories. arXiv preprint arXiv:2309.16750, 2023b.   \nJohn J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982.   \nJohn J Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the national academy of sciences, 81(10):3088\u20133092, 1984.   \nYa-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In International Conference on Machine Learning, pages 4337\u20134348. PMLR, 2021.   \nWei-Yen Hsu. Application of competitive hopfield neural network to brain-computer interface systems. International journal of neural systems, 22(01):51\u201362, 2012.   \nJerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023.   \nJerry Yao-Chieh Hu, Pei-Hsuan Chang, Haozheng Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, and Han Liu. Outlier-efficient hopfield layers for large transformer-based models. In Forty-first International Conference on Machine Learning (ICML), 2024a.   \nJerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, and Han Liu. Nonparametric modern hopfield models. arXiv preprint arXiv:2404.03900, 2024b.   \nJerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In Forty-first International Conference on Machine Learning (ICML), 2024c.   \nJerry Yao-Chieh Hu, Weimin Wu, Zhuoru Li, Sophia Pi, , Zhao Song, and Han Liu. On statistical rates and provably efficient criteria of latent diffusion transformers (dits). In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS), 2024d.   \nGeorgios Iatropoulos, Johanni Brea, and Wulfram Gerstner. Kernel memory networks: A unifying framework for memory modeling. Advances in Neural Information Processing Systems, 35: 35326\u201335338, 2022.   \nAlfredo N Iusem. On the convergence properties of the projected gradient method for convex optimization. Computational & Applied Mathematics, 22:37\u201352, 2003.   \nMatthew Jenssen, Felix Joos, and Will Perkins. On kissing numbers and spherical codes in high dimensions. Advances in Mathematics, 335:307\u2013321, 2018.   \nYanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, 37 (15):2112\u20132120, 2021.   \nJiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, and Zhihui Zhu. Generalized neural collapse for a large number of classes. arXiv preprint arXiv:2310.05351, 2023.   \nGrigorii Anatol\u2019evich Kabatiansky and Vladimir Iosifovich Levenshtein. On bounds for packings on a sphere and in space. Problemy peredachi informatsii, 14(1):3\u201325, 1978.   \nPentti Kanerva. Sparse distributed memory. MIT press, 1988.   \nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199\u201322213, 2022.   \nLeo Kozachkov, Ksenia V Kastanenka, and Dmitry Krotov. Building transformers from neurons and astrocytes. Proceedings of the National Academy of Sciences, 120(34):e2219150120, 2023.   \nDmitry Krotov. A new frontier for hopfield networks. Nature Reviews Physics, 5(7):366\u2013367, 2023.   \nDmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition. Advances in neural information processing systems, 29, 2016.   \nDmitry Krotov and John J. Hopfield. Large associative memory problem in neurobiology and machine learning. In International Conference on Learning Representations (ICLR), 2021.   \nMikhail A Lebedev and Miguel AL Nicolelis. Brain\u2013machine interfaces: past, present and future. TRENDS in Neurosciences, 29(9):536\u2013546, 2006.   \nAnatole L\u00e9cuyer, Fabien Lotte, Richard B Reilly, Robert Leeb, Michitaka Hirose, and Mel Slater. Brain-computer interfaces, virtual reality, and videogames. Computer, 41(10):66\u201372, 2008.   \nDebra A Lelewer and Daniel S Hirschberg. Data compression. ACM Computing Surveys (CSUR), 19 (3):261\u2013296, 1987.   \nXilin Liu, Milin Zhang, Basheer Subei, Andrew G Richardson, Timothy H Lucas, and Jan Van der Spiegel. The pennbmbi: Design of a general purpose wireless brain-machine-brain interface system. IEEE transactions on biomedical circuits and systems, 9(2):248\u2013258, 2015.   \nCarlo Lucibello and Marc M\u00e9zard. Exponential capacity of dense associative memories. Physical Review Letters, 132(7):077301, 2024.   \nAndre Martins, Vlad Niculae, and Daniel C McNamee. Sparse modern hopfield networks. In Associative Memory & Hopfield Networks in 2023, 2023.   \nMichael H Moore. Vector packing in finite dimensional vector spaces. Linear Algebra and its Applications, 8(3):213\u2013224, 1974.   \nMegan Morrison, Pedro D Maia, J Nathan Kutz, et al. Preventing neurodegenerative memory loss in hopfield neuronal networks using cerebral organoids or external microelectronics. Computational and Mathematical Methods in Medicine, 2017, 2017.   \nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.   \nOpenAI. Sora: A video generative model based on transformer diffusion. OpenAI Research, 2024. Accessed: 08/16/2024.   \nToshihiro Ota, Ikuro Sato, Rei Kawakami, Masayuki Tanaka, and Nakamasa Inoue. Learning with partial forgetting in modern hopfield networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 6661\u20136673. PMLR, 2023.   \nVardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40): 24652\u201324663, 2020.   \nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4195\u20134205, 2023.   \nBen Peters, Vlad Niculae, and Andr\u00e9 FT Martins. Sparse sequence-to-sequence models. arXiv preprint arXiv:1905.05702, 2019.   \nWilliam Wesley Peterson and Edward J Weldon. Error-correcting codes. MIT press, 1972.   \nAdam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024.   \nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nParameswaran Raman and Jiasen Yang. Optimization on the surface of the (hyper)-sphere. arXiv preprint arXiv:1909.06463, 2019.   \nAnder Ramos-Murguialday, Doris Broetz, Massimiliano Rea, Leonhard L\u00e4er, \u00d6zge Yilmaz, Fabricio L Brasil, Giulia Liberati, Marco R Curado, Eliana Garcia-Cossio, Alexandros Vyziotis, et al. Brain\u2013 machine interface in chronic stroke rehabilitation: a controlled study. Annals of neurology, 74(1): 100\u2013108, 2013.   \nHubert Ramsauer, Bernhard Sch\u00e4f,l Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic\u00b4, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.   \nR Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009.   \nBishwajit Saha, Dmitry Krotov, Mohammed J Zaki, and Parikshit Ram. End-to-end differentiable clustering with associative memories. In International Conference on Machine Learning, pages 29649\u201329670. PMLR, 2023.   \nSaul Santos, Vlad Niculae, Daniel McNamee, and Andre FT Martins. Sparse and structured hopfield networks. arXiv preprint arXiv:2402.13725, 2024.   \nPhilipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas Verhoeven, Jorg K Wegner, Marwin Segler, Sepp Hochreiter, and Gunter Klambauer. Improving few-and zero-shot reaction template prediction using modern hopfield networks. Journal of chemical information and modeling, 62(9):2111\u20132120, 2022.   \nMaryam M Shanechi. Brain\u2013machine interfaces from motor to mood. Nature neuroscience, 22(10): 1554\u20131564, 2019.   \nClaude E Shannon. Probability of error for optimal codes in a gaussian channel. Bell System Technical Journal, 38(3):611\u2013656, 1959.   \nJiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin, and Zhangyang Wang. Learning a minimax optimizer: A pilot study. In International Conference on Learning Representations, 2020.   \nBharath K Sriperumbudur and Gert RG Lanckriet. On the convergence of the concave-convex procedure. In Advances in neural information processing systems, volume 9, pages 1759\u20131767, 2009.   \nThomas Strohmer and Robert W Heath Jr. Grassmannian frames with applications to coding and communication. Applied and computational harmonic analysis, 14(3):257\u2013275, 2003.   \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nNilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I Jordan. Averaging stochastic gradient descent on riemannian manifolds. In Conference On Learning Theory, pages 650\u2013687. PMLR, 2018.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJeffrey Wang. Finding and investigating exact spherical codes. Experimental Mathematics, 18(2): 249\u2013256, 2009.   \nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.   \nMichael Widrich, Bernhard Sch\u00e4f,l Milena Pavlovi\u00b4c, Hubert Ramsauer, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter, Geir Kjetil Sandve, Victor Greiff, Sepp Hochreiter, et al. Modern hopfield networks and attention for immune repertoire classification. Advances in Neural Information Processing Systems, 33:18832\u201318845, 2020.   \nDavid J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic associative memory. Nature, 222(5197):960\u2013962, 1969.   \nDennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrieval with larger capacity for modern hopfield models. In Forty-first International Conference on Machine Learning (ICML), 2024a.   \nDennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. Stanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations (ICLR), 2024b.   \nAaron D Wyner. Capabilities of bounded discrepancy decoding. Bell System Technical Journal, 44 (6):1061\u20131122, 1965.   \nChenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, Hsi-Sheng Goan, and Han Liu. Bishop: Bi-directional cellular learning for tabular data with generalized sparse modern hopfield model. arXiv preprint arXiv:2404.03830, 2024.   \nAlan L Yuille and Anand Rangarajan. The concave-convex procedure (cccp). Advances in neural information processing systems, 14, 2001.   \nZhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006, 2023.   \nZhihan Zhou, Winmin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong Wang, and Han Liu. Dnabert-s: Learning species-aware dna embedding with genome foundation models. arXiv preprint arXiv:2402.08777, 2024.   \nZhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820\u201329834, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Related Works 18 ", "page_idx": 16}, {"type": "text", "text": "B More Discussions 19 ", "page_idx": 16}, {"type": "text", "text": "B.1 MHM Capacity 19   \nB.2 Learning on Stiefel Manifolds 19   \nB.3 KHMs as Brain-Machine Interface 20 ", "page_idx": 16}, {"type": "text", "text": "C Proofs of Main Text ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Lemma 2.1 . 2121242628 C.2 Proofs of Lemma 2.2 and Proposition 2.1   \nC.3 Proof of Theorem 3.1   \nC.4 Proof of Proposition 3.1 . ", "page_idx": 16}, {"type": "text", "text": "D Experimental Details 29 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Metastable States 292930303031 D.2 Energy Landscape . .   \nD.3 Basins of Attraction . .   \nD.4 Simulation of Proposition 3.1   \nD.5 Assignment Problems . .   \nD.6 Additional Experiments . . ", "page_idx": 16}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Hopfield and Dense Associative Memory Models. Associative memory models [Kanerva, 1988, Willshaw et al., 1969] are extensively studied in neuroscience and machine learning due to their biologically plausible designs. These models aim to store a set of memories and accurately retrieve each one given an input query. Hopfield models [Hopfield, 1982] are a class of energy-based associative memory models, beginning with classical versions [Hopfield, 1984, 1982] that handle binary patterns and have a (sub-)linear memory capacity of $O(d)$ for a pattern dimension $d$ . Dense associative memory models [Krotov and Hopfield, 2021, 2016, Ramsauer et al., 2020, Demircigil et al., 2017] are later proposed with superlinear memory capacity enabled by sharper energy functions (e.g., polynomial and exponential). Notably, the latest advancement in these dense models is their exponential-in- $d$ capacity [Lucibello and M\u00e9zard, 2024, Santos et al., 2024, Wu et al., 2024b, Hu et al., 2024a,c,b, 2023, Ramsauer et al., 2020]. However, the current literature reports are mostly6 lower bound results for this exponential memory capacity. This work matches these lower bounds with an upper bound, making the capacity both tight and provably optimal. ", "page_idx": 17}, {"type": "text", "text": "Transformer-Compatible Dense Associative Memories: Modern Hopfield Models. Recently, a special class of dense associative memory models, modern Hopfield models (MHMs), has gained increasing interest in deep learning due to their connection to the attention mechanism in transformers [Wu et al., 2024a,b, Hu et al., 2024a,c,b, 2023, Ramsauer et al., 2020]. Therefore, we also refer to them as transformer-compatible dense associative memories. Notably, the defining characteristic of MHMs is that their single-step update is equivalent to the attention mechanism [Vaswani et al., 2017]. This striking feature makes them interesting given the prevalence and dominance of transformer architectures in the era of large foundation models [Polyak et al., 2024, OpenAI, 2024, Hu et al., 2024d, Peebles and Xie, 2023, Bubeck et al., 2023, Bai et al., 2023, Achiam et al., 2023, Zhou et al., 2024, 2023, Ji et al., 2021, Touvron et al., 2023, Kojima et al., 2022, Bommasani et al., 2021, Radford et al., 2019, Devlin, 2018]. ", "page_idx": 17}, {"type": "text", "text": "As a result, such a connection facilitates the integration of associative memory models into modern deep learning and large foundation models [Burns, 2024, Xu et al., 2024, Hu et al., 2024a,b, Hofmann et al., 2024, Wu et al., 2024b, Auer et al., 2023, Burns and Fukai, 2023, F\u00fcrst et al., 2022, Krotov and Hopfield, 2016]. Moreover, recent studies introduce several modern Hopfield model variants. Hu et al. [2023] propose the sparse modern Hopfield model, a sparse counterpart to MHM with larger capacity and lower retrieval error. Wu et al. [2024b], Santos et al. [2024], Martins et al. [2023] introduce generalized sparse Hopfield models that unify all MHMs with different degrees of sparsity. Hu et al. [2024b] present a nonparametric construction for deep learning compatible Hopfield layers and several efficient modern Hopfield variants. Hu et al. [2024a] introduce OutEffHop, an outlierremoving, deep learning compatible Hopfield layer for robust large pretrained model quantization. Wu et al. [2024a] propose U-Hop facilitating memory retrieval in a learnable feature space (see introduce for a review). Empirically, U-Hop improves the memory confusion problem [Krotov and Hopfield, 2021] by a significant margin. ", "page_idx": 17}, {"type": "text", "text": "Applications of Modern Hopfield Models in Modern Machine Learning. Recently, modern Hopfield models also achieve empirical success across various deep learning tasks [Krotov, 2023]. Starting with [Krotov and Hopfield, 2016], the polynomial Hopfield model is proposed for image classification tasks (e.g., MNIST). Later, Ramsauer et al. [2020] introduce modern Hopfield layers compatible with deep learning architectures. Since then, various modern Hopfield layers [Hu et al., 2024a, Wu et al., 2024b, Hu et al., 2023] have been applied in many deep learning tasks, such as language modeling [He et al., 2024], multiple instance learning [Ramsauer et al., 2020], immune repertoire classification [Widrich et al., 2020], multivariate time series prediction [Wu et al., 2024b], image generation [Hoover et al., 2023b], tabular learning [Xu et al., 2024], unsupervised clustering [Saha et al., 2023], and image captioning [F\u00fcrst et al., 2022]. ", "page_idx": 17}, {"type": "text", "text": "Additionally, modern Hopfield layers introduce new operations in large foundation models to improve performance. For example, Wu et al. [2024b] leverage the retrieval dynamics of modern Hopfield models to enable an external memory plugin in time series prediction, and $\\mathrm{Xu}$ et al. [2024] apply a similar approach to tabular data. In image captioning, F\u00fcrst et al. [2022] address issues with covariance structure using modern Hopfield models. Hu et al. [2024a] propose an outlier-free Hopfield layer as a quantization-strong and resource-efficient transformer backbone for large language models and large foundation models. Ota et al. [2023] embed a partial forgetting functionality in modern Hopfield models to enhance model performance. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Kernelized Hopfield Models. Wu et al. [2024a] propose kernelized Hopfield models (KHMs)7, which have the capability to store memories in a learnable feature space. KHMs offer the flexibility to relocate memories while maintaining several defining properties of modern Hopfield models [Ramsauer et al., 2020, Theorems 1 to 4]. By maximizing the average separation between memories, KHMs empirically achieve lower retrieval errors [Wu et al., 2024a, Section 4]. However, the theoretical understanding of KHMs is larking due to their new flexibility in rearranging memories. This work aims to fill this gap by analyzing the capacity limits and theoretical justifications for KHMs. ", "page_idx": 18}, {"type": "text", "text": "Spherical Code. Spherical codes are mathematical constructions describing the arrangement of $M$ points on the surface of a $d$ -dimensional hyper-sphere [Delsarte et al., 1991]. The main problem around spherical codes is to arrange points in a way such that the minimum distance between any two points are maximized. This arrangement is called the optimal spherical code. It is crucial for minimizing errors and maximizing efficiency in signal transmission and data storage. In general, the value of max minimal separation and arrangement of points is unsolved except for certain pairs of $(d,M)$ [Wang, 2009]. Various of fields such as communications [Strohmer and Heath Jr, 2003] and data compression [Lelewer and Hirschberg, 1987]. Moreover, spherical codes are related to the area of error-correcting codes [Peterson and Weldon, 1972], which are used to detect and correct errors in data transmission and storage. ", "page_idx": 18}, {"type": "text", "text": "B More Discussions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 MHM Capacity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We review the modern Hopfield capacity lower bound in [Ramsauer et al., 2020]. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.1 (Memory Capacity of MHM). Let $1-p$ be the probability of successfully storing and retrieving a pattern. Assuming patterns are normalized, the amount of patterns randomly sampled from a $d$ -dimensional unit-sphere that the MHM with update rule in (1.2), can store and retrieve is lower-bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\nM\\geq\\sqrt{p}C^{(d-1)/4},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C$ is the solution to $C=b/W_{0}(\\exp\\{a+\\ln b\\})$ , with $W_{0}(\\cdot)$ being the principal branch of Lambert $W$ function, $a:=(4/d-1)$ $\\left(\\ln{2\\sqrt{p}}\\!-\\!2\\!\\middle/_{R}+1\\right)$ and $b:=4\\beta\\big/5(d\\!-\\!1)$ . ", "page_idx": 18}, {"type": "text", "text": "Observe $W_{0}$ is an increasing function in $\\exp\\{a+\\ln b\\}$ , indicating that $C$ is increasing in $R$ ( $C$ is increasi\u221ang in $a;a$ is decreasing with fixed $R$ ). Finally, we observe that under Assumption 1, we have $\\begin{array}{r}{R=\\frac{1}{2}\\sqrt{2\\Delta_{\\operatorname*{min}}}}\\end{array}$ , implying that the memory capacity of MHM is constrained by large $\\Delta_{\\mathrm{min}}$ . ", "page_idx": 18}, {"type": "text", "text": "B.2 Learning on Stiefel Manifolds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We review ways to do optimization on the surface of an unit-hypersphere (Stiefel manifold). ", "page_idx": 18}, {"type": "text", "text": "Projected Gradient Descent. Given any loss function $\\mathcal{L}(\\cdot)$ , an input matrix $X$ , and learning rate $\\gamma$ , a single gradient descent step at $t$ -th time step is: ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{t+1/2}=W_{t}-\\gamma_{t}\\nabla_{X}{\\mathcal{L}}(X).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Projected gradient descent [Raman and Yang, 2019] then projects $W_{t+\\frac{1}{2}}$ onto the feasible set, in this case, the surface of a unit hypersphere ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{t+1}=\\frac{W_{t+1/2}}{\\lVert W_{t+1/2}\\rVert}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "7This is different from [Iatropoulos et al., 2022] while they share similar names. ", "page_idx": 18}, {"type": "text", "text": "Combining (B.1) and (B.2), we obtain the projected gradient descent step as ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{t+1}=\\operatorname{PGD}(W_{t},\\gamma,X).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Riemannian Optimization. [Tripuraneni et al., 2018] construct and analyze an approach on optimization on Riemannian manifolds from a geometric perspective. Their adapt the Polyak-Ruppert [Polyak and Juditsky, 1992] iterate averaging technique to the Riemannian setting. In general, with carefully selected step-size, their method achieves $\\mathcal{O}(1/N)$ convergence rate which is the same in the euclidean setting. Overall, there are various of methods for optimization on Riemannian manifolds with comparable convergence rate to Euclidean space. Thus, giving the advancement of Riemannian optimization, our assumption in Definition 2.6 is reasonably mild. ", "page_idx": 19}, {"type": "text", "text": "L2 Regularization. A simple alternative to satisfy the norm constraint is through L2 regularization. From the aspect of neural collapse [Papyan et al., 2020], learning under (3.2) is similar to learning under cross-entropy loss or supervised contrastive learning without positive samples [Graf et al., 2021]. Further, from the analysis on unconstrainted feature models [Zhu et al., 2021], we can see that with carefully chosen coefficient on the regularization term on $W$ , the optimal solution of (3.2) or cross-entropy loss ended up outputting normalized features. ", "page_idx": 19}, {"type": "text", "text": "Based on the previous research, we are able to see that the constraint in Definition 2.6 is not difficult to satisfy. ", "page_idx": 19}, {"type": "text", "text": "B.3 KHMs as Brain-Machine Interface ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The ultimate goal in the field of Brain-Machine Interface (BMI) [Lebedev and Nicolelis, 2006] is to design communication between human brains and external devices. It has potential in various real-world applications such as medical treatment [Ramos-Murguialday et al., 2013, Shanechi, 2019], virtual reality [L\u00e9cuyer et al., 2008], etc. While Hopfield models serve as computational models for simulating human brains and their memory recall system [Liu et al., 2015, Hsu, 2012], KHMs correspond to external devices (storage space) to assist/enhance the process of memory recall. ", "page_idx": 19}, {"type": "text", "text": "In this paper, we study the scenario where we try to optimize the usage of external neurons for memory storage by increasing the separation between memories in such external space ( $\\Phi$ -space). We show that to prevent memory confusion, minimizing separation loss is an effective way to utilize the external space efficiently. The closest work we can find is [Morrison et al., 2017] and [Kozachkov et al., 2023]. In particular, they use Hopfield (and modern Hopfield or dense associative memory) models as the computational model for human brain. Specifically, Morrison et al. [2017] study the case of memory loss caused by disease or injuries. Their proposed framework is able to consider the level of memory damage and then estimate the required dimension to fully or partially recover memories. Kozachkov et al. [2023] explore the potential of building biological computers through links between transformers and modern Hopfield models. They show that neuron\u2013astrocyte networks can perform the core computations of a transformer. In this context, our work provide an improved connection between brain and transformer models, offering a more learnable and powerful approach to brain-machine interfaces (BMI). ", "page_idx": 19}, {"type": "text", "text": "C Proofs of Main Text ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first introduce a helper lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1 ([Ramsauer et al., 2020, Hu et al., 2023]). Given real numbers, $a,b\\in\\mathbb{R}$ . If the equation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a c+c\\ln c-b=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds, then the solution is ", "page_idx": 20}, {"type": "equation", "text": "$$\nc=\\frac{b}{W_{0}(\\exp\\{a+\\ln b\\})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By [Hu et al., 2023, Corollary 3.1.1], we state the well-separation condition of dense modern Hopfield model [Ramsauer et al., 2020]. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2 (Well Separation Condition of Dense Modern Hopfield Model [Ramsauer et al., 2020]). Following Definition 2.2, suppose the memory patterns $\\{\\xi_{\\mu}\\}_{\\mu\\in[M]}$ are located within the sphere $S_{\\mu}:=\\{x|\\|x-\\xi_{\\mu}\\|\\leq R\\}$ . Then, assuming normalized memory patterns, the retrieval dynamics $\\mathcal{T}_{\\mathrm{MHM}}$ maps the sphere $S_{\\mu}$ onto itself under the following conditions: ", "page_idx": 20}, {"type": "text", "text": "1. The initial query $x$ is located within the sphere $S_{\\mu}$ , i.e. $x\\in S_{\\mu}$ .   \n2. The well-separation condition is satisfied, which is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta_{\\mu}\\geq\\frac{1}{\\beta}\\ln\\frac{2(M-1)}{R}+2R.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This specifies the necessary condition for a pattern $\\xi_{\\mu}$ to be stored in $E$ and be able to retrieved by $\\tau$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 2.1. Let $\\Delta_{\\mathrm{min}}^{\\Phi}=\\mathrm{Min}_{\\mu\\in[M]}\\,\\Delta_{\\mu}^{\\Phi}$ , and $\\theta_{\\mu\\nu,\\Phi}$ be the angle between two patterns $\\Phi(\\xi_{\\nu})$ and $\\Phi(\\xi_{\\mu})$ . Note that $\\theta_{\\Phi,\\mu\\nu}\\in[0,\\pi]$ . ", "page_idx": 20}, {"type": "text", "text": "By Definition 2.7, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{min}}^{\\Phi}\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi}}\\right)+2R_{\\Phi},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta_{\\operatorname*{min}}^{\\Phi}=\\operatorname*{Min}_{1\\leq\\mu\\leq\\nu\\leq M}\\left(1-\\cos(\\theta_{\\Phi,\\mu\\nu})\\right)=\\left[1-\\cos(\\theta_{\\operatorname*{min}})\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\theta_{\\operatorname*{min}}:=\\mathrm{Min}_{1\\leq\\mu\\leq\\nu\\leq M}\\left(1-\\cos(\\theta_{\\Phi,\\mu\\nu})\\right)\\in[0,\\pi].}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Then, it holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n[1-\\cos(\\theta_{\\mathrm{min}})]\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi}}\\right)+2R_{\\Phi}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, Let $1-p$ be the success storage and retrieval probability under Definition 2.7. We have ", "page_idx": 20}, {"type": "equation", "text": "$$\nP\\left(\\Delta_{\\mu}^{\\Phi}\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi}}\\right)+2R_{\\Phi}\\right)=1-p.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By (C.1), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nP\\left(1-\\cos(\\theta_{\\mathrm{min}})\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi,\\mu}}\\right)+2R_{\\Phi}\\right)=1-p.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We observe that $\\cos(\\theta_{\\mathrm{min}})$ connects to the maximal separation loss via ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\cos(\\theta_{\\mathrm{min}})=\\frac{\\mathcal{L}_{\\Phi}(\\Xi)+2t}{2t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Following the proof of [Hu et al., 2023, Lemma 3.1] and by Lemma C.1, it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\nM=\\sqrt{p}C^{d-1/4},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with some real value $C\\in\\mathbb{R}$ . Here $C$ is solution to the upper branch of the Lambert $W$ function deduced from (C.2), ", "page_idx": 21}, {"type": "equation", "text": "$$\nC=\\frac{b}{W_{0}(\\exp\\{a+\\ln b\\})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\na:=\\frac{4}{d-1}\\left\\{\\ln\\left[\\frac{2(\\sqrt{p}-1)}{R_{\\Phi}}\\right]+1\\right\\},\\quad\\mathrm{and}\\quad b:=\\frac{4\\beta}{5(d-1)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we arrive a lower bound on the exponential storage capacity $\\mathbf{M}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nM\\geq\\sqrt{p}C^{\\frac{d-1}{4}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To compare the the results from [Ramsauer et al., 2020, Theorem 3] (with the assumption of pattern normalization), we denote the results from [Ramsauer et al., 2020] with $\\widetilde{.}$ notation, i.e. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{a}:=\\frac{4}{d-1}\\left\\{\\ln\\left[\\frac{2(\\sqrt{p}-1)}{R}\\right]+1\\right\\},\\quad\\mathrm{and}\\quad\\widetilde{b}=b.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And we also have $\\widetilde{\\theta}_{\\operatorname*{min}}\\,:=\\,\\mathrm{Min}_{1\\leq\\mu\\leq\\nu\\leq M}\\left(1-\\cos(\\theta_{\\mu\\nu})\\right)\\,\\in\\,[0,\\pi]$ be the angle between two raw memory patterns $\\xi_{\\nu},\\xi_{\\mu}$ . ", "page_idx": 21}, {"type": "text", "text": "We denote the optimal separation loss be $\\mathcal{L}^{\\star}(\\Xi)$ , and the loss value at $t$ -th step be $\\mathcal{L}^{t}(\\Xi)$ . ", "page_idx": 21}, {"type": "text", "text": "We denote $R_{\\Phi}^{\\star}$ be the corresponding $R_{\\Phi}$ when $\\mathcal{L}_{\\Phi}(\\Xi)$ is at its global minimum. ", "page_idx": 21}, {"type": "text", "text": "By (3.2), the convexity of $\\mathcal{L}_{\\Phi}$ , the optimality of $\\mathcal{L}_{\\Phi}$ gives ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{\\Phi}^{\\star}=\\frac{1}{2}\\sqrt{\\frac{\\mathcal{L}_{\\Phi}^{\\star}(\\Xi)}{-t}}\\ge R_{\\Phi}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we prove that to achieve $R_{\\Phi}\\geq R$ , we need \u22124tR21\u2212L\u22c6(\u039e) sub linear time (iterations.). ", "page_idx": 21}, {"type": "text", "text": "Recall that $\\begin{array}{r}{R:=\\frac{1}{2}\\operatorname*{Min}_{\\nu,\\nu\\neq\\mu}\\|\\xi_{\\nu}-\\xi_{\\mu}\\|}\\end{array}$ . By $R_{\\Phi}=\\sqrt{\\mathcal{L}_{\\Phi}(\\Xi)\\middle/\\not-t}/2$ , for $R_{\\Phi}\\geq R$ , we need ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sqrt{\\frac{\\mathcal{L}_{\\Phi}(\\Xi)}{-t}}\\geq\\frac{1}{2}\\operatorname*{Min}_{\\nu,\\nu\\neq\\mu}\\|\\xi_{\\nu}-\\xi_{\\mu}\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies, by $t>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\Phi}(\\Xi)\\leq-t\\cdot(2R)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Subtracting $-\\mathcal{L}_{\\Phi}^{\\star}(\\Xi)$ on both sides, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\Phi}(\\Xi)-\\mathcal{L}_{\\Phi}^{\\star}(\\Xi)\\leq-t\\cdot(2R)^{2}-\\mathcal{L}_{\\Phi}^{\\star}(\\Xi):=\\epsilon\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Which implies the iteration number needed to achieve improved memory capacity bound is: ", "page_idx": 22}, {"type": "equation", "text": "$$\nN=\\mathcal{O}\\left(\\frac{1}{-4t R^{2}-\\mathcal{L}_{\\Phi}^{\\star}(\\Xi)}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives us a sub-linear time complexity. ", "page_idx": 22}, {"type": "text", "text": "Let ", "page_idx": 22}, {"type": "equation", "text": "$$\na:=\\frac{4}{d-1}\\left\\{\\ln\\left[\\frac{2(\\sqrt{p}-1)}{R_{\\Phi}}\\right]+1\\right\\},\\quad\\mathrm{and}\\quad b:=\\frac{4\\beta}{5(d-1)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As long as Algorithm 1 runs for $\\mathcal{O}\\left({1}/{-4t R^{2}{-}\\mathcal{L}_{\\Phi}^{\\star}(\\Xi)}\\right)$ iterations, its output $\\Phi$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{a}\\leq a,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{C}=W_{0}\\left(\\exp\\Bigl\\{\\widetilde{a}+\\ln\\widetilde{b}\\Bigr\\}\\right)\\le W_{0}\\left(\\exp\\{a+\\ln b\\}\\right)=C.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have the memory capacity comparison as ", "page_idx": 22}, {"type": "equation", "text": "$$\nM=\\sqrt{p}C^{\\frac{d-1}{4}}\\geq\\sqrt{p}\\widetilde{C}^{\\frac{d-1}{4}}=\\widetilde{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the upper branch of Lambert W function is monotonically increasing on its domain, $R_{\\Phi}>R$ implies ", "page_idx": 22}, {"type": "equation", "text": "$$\nM=\\sqrt{p}C^{\\frac{d-1}{4}}\\geq\\sqrt{p}\\widetilde{C}^{\\frac{d-1}{4}}=\\widetilde{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence we finish the proof. ", "page_idx": 22}, {"type": "text", "text": "C.2 Proofs of Lemma 2.2 and Proposition 2.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma C.3 (Capacity of Optimal Spherical Code, Lemma 2.2 Restated). Given a fixed $D_{\\Phi}>1$ , and its corresponding $M^{\\star}$ , if an optimal code $\\mathcal{C}_{\\mathrm{opt}}$ is in $\\mathbb{S}^{D_{\\Phi}-1}$ and has size $M^{\\star}$ , then $\\mathcal{C}_{\\mathrm{opt}}\\in\\Lambda_{D_{\\Phi}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proposition C.1 (Optimal Memory Capacity, Proposition 2.1 Restated). Following Lemma 2.2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nM^{\\star}\\asymp c^{D_{\\Phi}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some $c>1$ . Here $\\asymp$ indicates matching upper and lower bounds up to constant factors. ", "page_idx": 23}, {"type": "text", "text": "Proof. By Assumption 1, all memories are normalized. Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{\\Phi}=\\cfrac{1}{2}\\sqrt{2-2\\underset{\\mu,\\nu\\in[M]}{\\operatorname*{max}}}\\left\\langle\\Phi(\\xi_{\\mu}),\\Phi(\\xi_{\\nu})\\right\\rangle}\\\\ &{\\underset{=\\sqrt{\\cfrac{1}{2}\\Delta_{\\operatorname*{min}}^{\\Phi}}}{\\mu\\neq\\nu}}\\\\ &{\\underset{=\\sqrt{\\cfrac{1}{2}\\Delta_{\\operatorname*{min}}^{\\Phi}}}{=\\sqrt{\\cfrac{1}{2}\\Delta_{\\operatorname*{min}}^{\\Phi}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall the storage condition ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{\\mu}^{\\Phi}\\geq\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{R_{\\Phi}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here we consider the minimal $\\Delta_{\\mu}^{\\Phi}$ among all possible $\\mu\\in[M]$ . We plug (C.9) into the well-separation condition and change \u2206\u00b5\u03a6 to \u2206\u03a6min. We arrive ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{min}}^{\\Phi}\\ge\\frac{1}{\\beta}\\ln\\left(\\frac{2(M-1)}{\\sqrt{\\Delta_{\\mathrm{min}}^{\\Phi}/2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By rearranging terms, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{min}}^{\\Phi}+\\frac{1}{2\\beta}\\ln\\left(\\frac{1}{2}\\Delta_{\\mathrm{min}}^{\\Phi}\\right)\\geq\\frac{1}{\\beta}\\ln\\left(2(M-1)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The derivative w.r.t. $\\Delta_{\\mathrm{min}}^{\\Phi}$ on the LHS is ", "page_idx": 23}, {"type": "equation", "text": "$$\n1+\\frac{1}{2\\beta\\Delta_{\\mathrm{min}}^{\\Phi}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "indicating that LHS is increasing in $\\Delta_{\\mathrm{min}}^{\\Phi}$ for all $\\Delta_{\\mathrm{min}}^{\\Phi}>0$ . The derivative w.r.t. $M$ on the RHS is ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{1}{\\beta(M-1)}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "indicating that the RHS is increasing in $M$ for all $M>1$ . Since we are handling $\\Delta_{\\mathrm{min}}^{\\Phi}$ , this property holds for all $\\Delta_{\\mu}^{\\Phi}$ . ", "page_idx": 23}, {"type": "text", "text": "Let $\\delta$ be the minimum value for $\\Delta_{\\mathrm{min}}^{\\Phi}$ that satisfies the storage condition such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta+\\frac{1}{2\\beta}\\ln\\left(\\frac{1}{2}\\delta\\right)\\geq\\frac{1}{\\beta}\\ln\\left(2(M-1)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With the definition of optimal spherical code, we have $\\delta\\le1-\\rho^{\\star}$ . Thus an optimal spherical code must satisfy this inequality. ", "page_idx": 23}, {"type": "text", "text": "Now we further analyze the quantity $M^{\\star}$ . Let $\\theta=\\operatorname{arccos}{(\\rho(C_{\\mathrm{opt}}))}$ , with $\\theta\\in(0,\\pi/2)$ , we apply the upper bound in [Kabatiansky and Levenshtein, 1978], we get ", "page_idx": 24}, {"type": "equation", "text": "$$\ne^{\\varphi(\\theta)D_{\\Phi}(1+o(1))}\\geq M^{\\star}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\Phi\\in{\\mathcal{H}}$ . With the above result, we get $M^{\\star}=o(c^{D_{\\Phi}})$ for some $c>1$ . ", "page_idx": 24}, {"type": "text", "text": "For the lower bound of $M^{\\star}$ , we use the classic sphere code bound in [Chabauty, 1953, Shannon, 1959, Wyner, 1965], and get ", "page_idx": 24}, {"type": "equation", "text": "$$\nM^{\\star}\\geq\\left[\\frac{1}{\\sqrt{\\pi}}\\frac{\\Gamma(^{D_{\\Phi}}\\!/2)}{\\Gamma(^{D_{\\Phi}-1}\\!/2)}\\int_{0}^{\\theta}\\sin^{D_{\\Phi}-2}(x)d x\\right]^{-1}=(1+o(1))\\sqrt{2\\pi D_{\\Phi}}\\cdot\\frac{\\cos(\\theta)}{\\sin^{D_{\\Phi}-1}(\\theta)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where x \u2208SD\u03a6\u22121. ", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\ne^{\\varphi(\\theta)D_{\\Phi}(1+o(1))}\\;\\geq M^{\\star}\\geq(1+o(1))\\sqrt{2\\pi D_{\\Phi}}\\cdot\\frac{\\cos(\\theta)}{\\sin^{D_{\\Phi}-1}(\\theta)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\varphi(\\theta)>-\\log\\sin(\\theta)$ , and $o(\\cdot)$ is \u201c strictly slower than\u201d notation as $D_{\\Phi}\\to\\infty$ . ", "page_idx": 24}, {"type": "text", "text": "This completes the proof. Tighter bounds can be found in [Jenssen et al., 2018, Fern\u00e1ndez et al., 2021].   \nWe selected bounds that most clearly show the exponential scaling behavior for better intuition. ", "page_idx": 24}, {"type": "text", "text": "C.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first restate Theorem 3.1: ", "page_idx": 25}, {"type": "text", "text": "Theorem C.1 (Theorem 3.1 Restated). For any possible integer $M$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau\\to0}\\operatorname*{sup}_{\\theta\\in\\mathcal{H}}\\left(\\operatorname{argmin}_{\\Phi\\in\\mathcal{H}}\\frac{1}{M}\\sum_{\\mu=1}^{M}\\mathcal{L}_{\\Phi}(\\xi_{\\mu},\\tau)\\right)\\subseteq\\operatornamewithlimits{a r g m i n}_{\\Phi\\in\\mathcal{H}}\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where all $\\mathcal{H}$ is the hypothesis space of $\\Phi$ . ", "page_idx": 25}, {"type": "text", "text": "Then we introduce a helper lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.4. Let ${\\mathcal{L}}_{0}(\\Phi,\\tau)$ be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}\\left(\\Phi,\\tau\\right):=\\tau\\cdot\\log\\sum_{\\mu=1}^{M}\\mathcal{L}_{\\Phi}(\\xi_{\\mu},\\tau).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "${\\mathcal{L}}_{0}(\\Phi,\\tau)$ converges uniformly to $\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)$ as $\\tau\\rightarrow0$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 3.1. We first organize terms in (3.2). We obtain: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\Phi}(\\xi_{\\mu},\\tau)=-\\left[\\log\\left(\\exp\\Bigl\\{\\displaystyle\\frac{\\langle\\Phi(\\xi_{\\mu}),\\Phi(\\xi_{\\mu})\\rangle}{\\tau}\\Bigr\\}\\right)-\\log\\left(\\displaystyle\\sum_{\\nu=1}^{M}\\exp\\Bigl\\{\\displaystyle\\frac{\\langle\\Phi(\\xi_{\\mu}),\\Phi(\\xi_{\\nu})\\rangle}{\\tau}\\Bigr\\}\\right)\\right]}\\\\ &{\\qquad\\qquad=-\\left[\\displaystyle\\frac{1}{\\tau}-\\log\\left(\\displaystyle\\sum_{\\nu=1}^{M}\\exp\\Bigl\\{\\displaystyle\\frac{\\langle\\xi_{\\mu},\\xi_{\\nu}\\rangle}{\\tau}\\Bigr\\}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We define a helper function ${\\mathcal{L}}_{0}$ , denoted as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}\\left(\\Phi,\\tau\\right):=\\tau\\cdot\\log\\sum_{\\mu=1}^{M}\\ell_{\\mu}(\\Xi,\\Phi,\\tau).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{0}\\left(\\Phi,\\tau\\right):=\\tau\\cdot\\log\\displaystyle\\sum_{\\mu=1}^{M}\\mathcal{L}_{\\Phi}(\\xi_{\\mu},\\tau)}\\\\ &{\\qquad\\qquad=\\tau\\log\\displaystyle\\sum_{\\mu=1}^{M}\\log\\left(1+\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\Biggl\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\Biggr\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Due to the fact that $x/(1{+}x)\\leq\\log(1+x)\\leq x$ for all $x>-1$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\frac{\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\Big\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\Big\\}}{1+\\sum_{\\nu^{\\prime}\\in[M]\\backslash\\mu}^{M}\\exp\\Big\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\Big\\}}}\\\\ &{\\leq\\log\\Bigg(1+\\underset{\\nu\\in[M]\\backslash\\mu}{\\overset{M}{\\sum}}\\,\\exp\\Big\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\Big\\}\\Bigg)}\\\\ &{\\leq\\underset{\\nu\\in[M]\\backslash\\mu}{\\overset{M}{\\sum}}\\,\\exp\\Bigg\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Given the fact that $\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle\\,-\\,1\\,\\leq\\,0$ for all possible $\\nu,\\mu$ . With the monotonicity of the exponential function, we obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\biggl\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\biggr\\}\\leq M-1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining this with LHS of (C.11), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\displaystyle\\frac{\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}}{M}}\\\\ &{\\leq\\log\\left(1+\\displaystyle\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Summing over all possible $\\mu\\in[M]$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{\\mu=1}^{M}\\frac{\\sum_{\\nu\\in[M]\\setminus\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}}{M}}\\\\ &{\\leq\\displaystyle\\sum_{\\mu=1}^{M}\\log\\left(1+\\sum_{\\nu\\in[M]\\setminus\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{\\mu=1}^{M}\\sum_{\\nu\\in[M]\\setminus\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using the property of max function, we further get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mu,\\nu\\in[M],\\mu\\neq\\nu}\\frac{\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}}{M}}\\\\ &{\\leq\\displaystyle\\sum_{\\mu=1}^{M}\\frac{\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{M}\\right\\}}{M},}\\\\ &{\\leq\\displaystyle\\sum_{\\mu=1}^{M}\\log\\left(1+\\displaystyle\\sum_{\\nu\\in[M]\\backslash\\mu}^{M}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}\\right)}\\\\ &{\\leq M\\cdot(M-1)\\cdot\\displaystyle\\operatorname*{max}_{\\mu,\\nu\\in[M],\\mu\\neq\\nu}\\exp\\left\\{\\frac{\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1}{\\tau}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now by taking logarithmic on both sides and multiplying all three terms by $\\tau$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{*,\\nu\\in[M],\\mu\\neq\\nu}}\\left(\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1\\right)-\\tau\\log M\\leq\\mathcal{L}_{0}(\\Phi,\\tau)\\leq\\tau\\log\\left(M\\cdot(M-1)\\right)+\\operatorname*{max}_{\\substack{\\mu,\\nu\\in[M],\\mu\\neq\\nu}}\\left(\\langle\\Phi(\\xi_{\\nu}),\\Phi(\\xi_{\\mu})\\rangle-1\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By $\\begin{array}{r}{\\operatorname*{max}_{\\mu,\\nu\\in[M],\\mu\\neq\\nu}\\left(\\alpha_{\\mu,\\nu}\\right)=\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)-1}\\end{array}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)-\\tau\\log M-1\\leq\\mathcal{L}_{0}(\\Phi,\\tau)\\leq\\tau\\log M\\cdot(M-1)+\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)-1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, for any $\\epsilon>0$ , by taking max (log M,log(M\u00b7(M\u22121))), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{L}_{0}(\\Phi,\\tau)-\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)|\\leq\\tau\\operatorname*{max}\\{\\log M,\\log(M\\cdot(M-1))\\}\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $\\tau<\\tau_{0}$ . That is, ${\\mathcal{L}}_{0}(\\Phi,\\tau)$ converges uniformly to $\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)$ , leading to Lemma C.4. ", "page_idx": 27}, {"type": "text", "text": "Now we know $\\mathcal{L}_{0}(\\Phi,\\tau)$ converges uniformly to $\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)$ as $\\epsilon\\rightarrow0$ , by [Rockafellar and Wets, 2009, Proposition 7.15], we have $\\mathcal{L}_{0}(\\Phi,\\tau)$ $\\Gamma$ -converges to $\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi)$ as well. By [Braides, 2006, Theorem 2.10], we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau\\to0}\\operatorname*{inf}_{\\Phi\\in\\mathcal{H}}\\mathcal{L}_{0}(\\Phi,\\tau)\\subseteq\\underset{\\Phi\\in\\mathcal{H}}{\\mathrm{argmin}}\\,\\mathcal{L}_{\\mathrm{HardMax}}(\\Phi).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This completes the proof8. ", "page_idx": 27}, {"type": "text", "text": "C.4 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. We first define the one-vs-one distance. ", "page_idx": 27}, {"type": "text", "text": "Definition C.1 (one-vs-one distance). We define the one-vs-one distance of a set of points $\\mathcal{V}=$ $\\{v_{\\mu}\\}_{\\mu=1}^{M}\\subseteq\\mathbb{S}^{d-1}$ , with $|\\gamma|=M$ , as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{one-vs-one}}\\left(\\mathcal{V}\\right):=\\operatorname*{min}_{\\mu\\in\\left[M\\right]\\nu\\neq\\mu}\\!\\left\\|v_{\\nu}-v_{\\mu}\\right\\|\\!.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The one-vs-one distance is lower bounded as following ", "page_idx": 27}, {"type": "text", "text": "Lemma C.5. [Jiang et al., 2023, Lemma C.13] ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\frac{\\sqrt{\\pi}}{M}\\frac{\\Gamma\\left(\\frac{d+1}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}+1\\right)}\\right]^{\\frac{1}{d-1}}\\leq\\operatorname*{max}_{\\gamma\\subseteq\\mathbb{S}^{d-1}}\\rho_{\\mathrm{one-vs-one}}\\left(\\mathcal{V}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that by the definition of the one-vs-one distance, we have the equivalent expression such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\rho_{\\mathrm{one-vs-one}}^{2}\\left(\\mathcal{V}\\right)}{2}\\equiv\\operatorname*{min}_{\\nu,\\mu\\in\\left[M\\right]}\\left\\langle v_{\\mu},v_{\\mu}\\right\\rangle-\\left\\langle v_{\\mu},v_{\\nu}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the above property, Lemma C.5 and a known upper bound in [Moore, 1974, Theorem 1], we obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left[\\frac{\\sqrt{\\pi}}{M}\\frac{\\Gamma\\left(\\frac{D_{\\Phi}+1}{2}\\right)}{\\Gamma\\left(\\frac{D_{\\Phi}}{2}+1\\right)}\\right]^{\\frac{2}{D_{\\Phi}-1}}\\leq\\Delta_{\\operatorname*{min}}^{\\Phi}\\leq2\\left[\\frac{2\\sqrt{\\pi}}{M}\\frac{\\Gamma\\left(\\frac{D_{\\Phi}+1}{2}\\right)}{\\Gamma\\left(\\frac{D_{\\Phi}}{2}\\right)}\\right]^{\\frac{1}{D_{\\Phi}-1}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The upper bound in [Moore, 1974] is derived by the normalized surface area of a spherical cap of angular radius $\\theta$ . ", "page_idx": 27}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Computational Environments. All experiments are conducted on the platform with NVIDIA GEFORCE RTX 2080 Ti and INTEL XEON SILVER 4214 $@$ 2.20GHz. We use PyTorch 1.8.0 for all experiments. The experiments are relatively lightweight which can also be ran on CPU-only environments. ", "page_idx": 28}, {"type": "text", "text": "D.1 Metastable States ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Hyperparameters. The hyperparameters we used for the metastable state experiment is listed in Table 3. ", "page_idx": 28}, {"type": "table", "img_path": "4UReW4Ez6s/tmp/66a8456bf4e71d23bbdd4277dcaac4a5d979a26e938720fd67bce37f171440ff.jpg", "table_caption": ["Table 3: Hyperparameter used Metastable State Experiment. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Implementation Details. The batch size in Table 3 denotes the batch size we use to train the feature map $\\Phi$ . For the synthetic dataset, we directly train $\\Phi$ on the whole memory set. For the softmax threshold, we follow the settings used in [Santos et al., 2024]. ", "page_idx": 28}, {"type": "text", "text": "D.2 Energy Landscape ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Hyperparameters. The hyperparameters we used for the basins of attraction experiment is listed in Table 5. ", "page_idx": 28}, {"type": "table", "img_path": "4UReW4Ez6s/tmp/c49333feb98afe1bea3a3879a734c9977c1e39cdbff4d5278ae2de9d0d17a764.jpg", "table_caption": ["Table 4: Hyperparameter used in the energy landscape experiment. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Implementation Details. We first prepare a set of randomly generated patterns as memories. Next we record its energy landscape with respect to different query (the coordinate in the figure). Next we train $\\Phi$ for 5 iterations and record its resulting energy landscape with $N=1,2,5$ . We use the entmax and sparsemax package used in [Peters et al., 2019]. ", "page_idx": 28}, {"type": "text", "text": "D.3 Basins of Attraction ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Hyperparameters. The hyperparameters we used for the basins of attraction experiment is listed in Table 5. ", "page_idx": 29}, {"type": "table", "img_path": "4UReW4Ez6s/tmp/e4f73be81328bc8cb055c7ac6476600768ae75d13d6fbf14f7088cb6842aa676.jpg", "table_caption": ["Table 5: Hyperparameter used in the basins of attraction experiment. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Implementation Details. We specifically set the update rule iteration to 5 as we see the sharp energy gradient in Figure 1. Demonstrating that the standard MHM and its variants are not able to converge to fixed points fast. ", "page_idx": 29}, {"type": "text", "text": "D.4 Simulation of Proposition 3.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We provide a numerical simulation of our bound with $D_{\\Phi}\\,=\\,3$ . We take the known solution of minimal separation published in http://neilsloane.com/packings/ a ground truth. ", "page_idx": 29}, {"type": "image", "img_path": "4UReW4Ez6s/tmp/c5f1bcfe7fd5bb8a532d70dd238907280b5eff0719e860d9fb244c97342cf7a9.jpg", "img_caption": ["Figure 3: Separation Bound Numerical Simulation We visualize the bound presented in Proposition 3.1 in 3-D dimension. The bound goes tighter as the number of points increases. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.5 Assignment Problems ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here we conduct the point assignment problem in 2D space. In 2D space, the optimal arrangement of six points is well-established: they equally divide the unit circle, with each point neighboring two others. ", "page_idx": 29}, {"type": "text", "text": "We consider the case where we try to learn a feature map $\\Phi$ under $\\mathrm{U-Hop+}$ with 6 different images sampled from CIFAR10 We sample each image from cat, dog, car, truck, deer and horse class. Intuitively, cat has closer semantic relationship with dog, car is more similar to truck and deer has closer semantic relationship with horse [Jiang et al., 2023, Neelakantan et al., 2022]. We show that our learned feature map consistently puts similar pairs closer to each other in 7 out of 10 trials. This implies that while our method does not force or even considers the underlying semantic meanings behind each memories, our feature map is still able to present such relationship. The result is in Figure 4. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Discussion. The separation loss encourages the feature map to make the whole dataset the most linearly separable to each instance. A similar analysis can be found in [Dirksen et al., 2022, Ghosal et al., 2022], where they found out that if data has subtle clustered structure, a random neural network is able to make it linearly separable with high probability. ", "page_idx": 30}, {"type": "image", "img_path": "4UReW4Ez6s/tmp/cc4fc582562ea2215d318eba9671ddc10e54854d82ac7ee56f0f33fde80990bf.jpg", "img_caption": ["Figure 4: Assignment Problem in 2D We observe that the learned feature map consistently put similar pairs closer to each other, leading to preserving some level of semantic information. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "D.6 Additional Experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Here we observe the loss curve of $\\mathcal{L}$ w.r.t. different memory set size. We aim to verify whether $\\mathcal{L}$ is able to converge well through proposed algorithm. ", "page_idx": 30}, {"type": "image", "img_path": "4UReW4Ez6s/tmp/d7240cfa890061b07638257c8ec365dccbef7d7c3e647f4a6df20ebd0a4fdf14.jpg", "img_caption": ["Figure 5: Loss Curve of $\\mathcal{L}$ w.r.t. different memory set size. We run separation maximization for 100 epochs on MNIST under 2 settings, $M=100/200$ . We set $\\tau=0.1$ , learning rate 1e-3, $D_{\\Phi}=100$ . The result shows $\\mathcal{L}$ converges fast, which echoes our sub-linear time complexity. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In our abstract, we claim we propose a new framework to describe the optimal memory capacity of kernelized Hopfield models (KHMs) and modern Hopfield models. We consdier stored memory sets as a special type of spherical code that all points in the set satisfies the well-separation condition. Next, we show there is a sublinear time algorithm to find an optimal feature map for KHMs to achieve maximal memory capacity. The main claims are detailed described in the following sections ", "page_idx": 31}, {"type": "text", "text": "\u2022 Optimal memory capacity for KHMs and MHMs: Section 2, Lemma 2.2.   \n\u2022 Memory code: Section 2.2, Definition 2.8.   \nOptimal capacity algorithm: Section 3.1, Algorithm 1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discussed our limitations in the last section, the limitations paragraph. The time complexity of Algorithm 1 was discussed in Section 3.1. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: We have several theoretical results: ", "page_idx": 32}, {"type": "text", "text": "(a) Lemma 2.1: Proof in Appendix C.1.   \n(b) Lemma 2.2: Proof in Appendix C.2.   \n(c) Theorem 3.1: Proof in Appendix C.3.   \n(d) Proposition 3.1: Proof in Appendix C.4. ", "page_idx": 32}, {"type": "text", "text": "The main assumption we made is normalized memory patterns, which is described in Lemma 2.1 and Lemma 2.2. Similar assumption was also made in [Santos et al., 2024, Wu et al., 2024a]. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our experimental results are in Section 4. We describe our experimental details in both Section 4 and Appendix D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 32}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the code in our supplementary materials. As for data, we mainly use synthetic data and MNIST in all experiments. We also describe the data generation and download in our code and experimental details. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The details are fully described in Appendix D. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The results are ran over at least 5 runs with different random seeds. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We describe our computational environment in the first paragraph of Appendix D. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have read and agreed to every aspect of the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The broader impacts can be found in the section right after conclusions. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The experiments conducted are mostly numerical simulations which does not serve practical usage. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: For our experiments, we only use datasets provided or generated by PyTorch. PyTorch\u2019s licenses can be found in https://github.com/pytorch/pytorch/blob/ main/LICENSE. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not provide new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcing our human subjects experiments. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcing our human subjects experiments. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]