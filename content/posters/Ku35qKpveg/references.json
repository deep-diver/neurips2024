{"references": [{"fullname_first_author": "Yann LeCun", "paper_title": "Deep learning", "publication_date": "2015-05-27", "reason": "This foundational paper provides a comprehensive overview of deep learning, a core concept underlying the current work on artificial neural networks."}, {"fullname_first_author": "Alex Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2017-06-01", "reason": "This seminal paper introduced AlexNet, a groundbreaking deep convolutional neural network that significantly advanced the field of computer vision and inspired many subsequent architectures."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced Vision Transformers (ViTs), a novel architecture that leverages the transformer mechanism for image recognition, substantially impacting the field and directly relevant to the current work's use of ViTs."}, {"fullname_first_author": "Xavier Glorot", "paper_title": "Deep sparse rectifier neural networks", "publication_date": "2011-04-01", "reason": "This paper introduced the concept of deep sparse rectifier neural networks, highlighting the benefits of sparsity in neural networks, a key motivation behind the current research on efficient channel denoising."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-06-01", "reason": "This highly influential paper introduced ResNet, a deep residual network that addressed the vanishing gradient problem in deep learning, enabling the training of significantly deeper and more accurate networks, which are relevant to the current work's experiments with various CNN architectures."}]}