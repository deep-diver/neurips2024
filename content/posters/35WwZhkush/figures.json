[{"figure_path": "35WwZhkush/figures/figures_1_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that feed-forward methods (like Depth Anything) excel at capturing overall 3D shape, but lack detail. Diffusion-based methods (like Marigold) produce detailed results but struggle with accurate global shape. BetterDepth, the authors' proposed method, aims to combine the strengths of both approaches, achieving both accurate global shape and fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_4_1.jpg", "caption": "Figure 2: BetterDepth training pipeline. Given training images x and labels d, we first estimate coarse depth maps d with the pre-trained MFFD and apply global pre-alignment to d using d as reference. Afterwards, the frozen latent encoder is employed to convert the image x, the depth labels d, and the aligned depth conditioning d' to the latent space. To construct the masked training objective, d' and d are split into non-overlapping patches {d} and {dn}, and dissimilar patches are filter out by thresholding, producing the patch-level similarity mask. Finally, the mask is downscaled to the latent space resolution for diffusion training.", "description": "This figure illustrates the BetterDepth training pipeline. It starts by using a pre-trained feed-forward model (MFFD) to generate an initial depth map (d).  This depth map is then globally aligned to the ground truth depth labels (d) to create an aligned conditioning depth map (d'). Both d and d' are then encoded into latent space using a latent encoder. Next, d and d' are split into patches.  A similarity measure is used to identify dissimilar patches, which are masked to prevent overfitting.  This masked training objective is used in combination with the latent space representations of the image (x) and the noisy depth (zd) to train a diffusion model (MDM) to refine the details of the initial depth map.", "section": "3.3 Training Strategies"}, {"figure_path": "35WwZhkush/figures/figures_5_1.jpg", "caption": "Figure 2: BetterDepth training pipeline. Given training images x and labels d, we first estimate coarse depth maps d with the pre-trained MFFD and apply global pre-alignment to d using d as reference. Afterwards, the frozen latent encoder is employed to convert the image x, the depth labels d, and the aligned depth conditioning d' to the latent space. To construct the masked training objective, d' and d are split into non-overlapping patches {d} and {d}, and dissimilar patches are filter out by thresholding, producing the patch-level similarity mask. Finally, the mask is downscaled to the latent space resolution for diffusion training.", "description": "The figure illustrates the training pipeline of BetterDepth, a two-stage framework that refines depth maps from a pre-trained model (MFFD).  The first stage uses global pre-alignment to align the initial coarse depth map with the ground truth. Then, both the coarse map and ground truth are divided into patches. Dissimilar patches are masked out, and the remaining patches are used to train a conditional diffusion model (MDM) to refine details in the latent space. This approach helps maintain the global structure while focusing on local detail improvements.", "section": "3.3 Training Strategies"}, {"figure_path": "35WwZhkush/figures/figures_6_1.jpg", "caption": "Figure 2: BetterDepth training pipeline. Given training images x and labels d, we first estimate coarse depth maps d with the pre-trained MFFD and apply global pre-alignment to d using d as reference. Afterwards, the frozen latent encoder is employed to convert the image x, the depth labels d, and the aligned depth conditioning d' to the latent space. To construct the masked training objective, d' and d are split into non-overlapping patches {d} and {d}, and dissimilar patches are filter out by thresholding, producing the patch-level similarity mask. Finally, the mask is downscaled to the latent space resolution for diffusion training.", "description": "The figure illustrates the training pipeline of the BetterDepth model. It starts with a pre-trained feed-forward model (MFFD) that provides a coarse depth map (d) from an input image (x). This coarse map is then pre-aligned with the ground truth depth labels (d) to improve accuracy, generating an aligned depth map (d'). Both the input image and aligned depth map are encoded into a latent space using a latent encoder. To refine the depth details and prevent overfitting, the aligned depth map and the ground truth are divided into patches, and patches with low similarity are masked out. The masked patches are then used to train the diffusion model (MDM). This training scheme ensures that the refined depth remains faithful to the original prediction while adding fine-grained details.", "section": "3.3 Training Strategies"}, {"figure_path": "35WwZhkush/figures/figures_8_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure shows a comparison of monocular depth estimation results from different methods.  The input image is a photo of a cat.  The first row shows depth maps generated by MiDaS, DPT, Depth Anything, Marigold, and BetterDepth.  The second row shows 3D reconstructions of the scene using the corresponding depth maps, color-coded by surface normals.  The figure highlights the trade-off between global shape accuracy and fine detail preservation in existing methods. BetterDepth aims to achieve both. Feed-forward methods (like Depth Anything) are good at global structure but lack detail, while diffusion-based methods (like Marigold) capture fine details but may struggle with global geometry.  BetterDepth is presented as a solution that successfully integrates the strengths of both approaches.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_8_2.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that while feed-forward methods (like Depth Anything) are good at capturing the overall shape, they lack detail. Diffusion-based methods (like Marigold) excel at detail but struggle with accurate global shape, especially in complex scenes. The authors' proposed method, BetterDepth, aims to combine the strengths of both approaches, resulting in accurate depth maps with fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_9_1.jpg", "caption": "Figure 7: Training and inference efficiency compared with Marigold [17] on the KITTI dataset.", "description": "This figure compares BetterDepth and Marigold across three aspects: training iterations, ensemble size, and denoising steps.  The (a) Convergence comparisons subplot shows that BetterDepth converges faster than Marigold, achieving comparable performance with fewer iterations.  The (b) Impact of ensembling size subplot shows BetterDepth maintains higher performance with a smaller ensemble size, indicating better stability.  The (c) Impact of denoising step subplot demonstrates that BetterDepth requires fewer denoising steps than Marigold to achieve similar performance.", "section": "4.4 Method Analysis"}, {"figure_path": "35WwZhkush/figures/figures_14_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that feed-forward methods (like Depth Anything) excel at capturing the overall 3D shape, but lack fine details. Diffusion-based methods (like Marigold) are better at capturing details, but struggle with accurate global shape representation. The authors' proposed method, BetterDepth, aims to combine the strengths of both approaches, achieving both accurate global shape and fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_15_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure shows a comparison of monocular depth estimation results from several different methods.  The input image is a photograph of a cat.  The results show that feed-forward methods (like Depth Anything) produce good overall depth but lack fine details. Diffusion-based methods (like Marigold) produce fine details but struggle with the overall depth layout.  BetterDepth aims to combine the strengths of both approaches, providing both accurate depth and fine detail.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_16_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of several monocular depth estimation methods.  It shows input images and their corresponding depth maps and 3D reconstructions.  The figure highlights the strengths and weaknesses of different approaches: feed-forward methods (like Depth Anything) excel at capturing the overall 3D shape but lack fine details, while diffusion-based methods (like Marigold) are better at detail extraction but struggle with accurate global shape representation, particularly in complex scenes.  BetterDepth, the proposed method, aims to combine the advantages of both approaches, achieving both accurate global shape and fine details in zero-shot settings.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_16_2.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that feed-forward methods (like Depth Anything) excel at capturing the overall 3D shape but lack fine details. Conversely, diffusion-based methods (like Marigold) are good at detail extraction but struggle with the global shape, particularly in complex scenes. BetterDepth, the authors' proposed method, aims to combine the strengths of both approaches, achieving both accurate geometry and fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_17_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the depth estimation and 3D reconstruction results of several methods, including input image, MiDaS, DPT, Depth Anything, Marigold and BetterDepth.  It highlights the strengths and weaknesses of different approaches. Feed-forward methods (like Depth Anything) excel at capturing the overall shape but lack fine details; diffusion-based methods (like Marigold) are better at capturing details but struggle with the global shape. BetterDepth aims to combine the advantages of both.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_18_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the depth estimation and 3D reconstruction results of several monocular depth estimation methods, including Depth Anything, Marigold, and the proposed BetterDepth.  It showcases that feed-forward methods excel at producing accurate global 3D shapes, while diffusion-based methods are better at capturing fine details.  BetterDepth aims to combine the strengths of both approaches, resulting in depth maps that are both geometrically accurate and rich in detail.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_19_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that feed-forward methods like Depth Anything are good at capturing overall shape but lack detail. Diffusion-based methods such as Marigold excel at detail but struggle with overall shape.  The authors' proposed method, BetterDepth, aims to combine the best of both approaches, resulting in accurate depth maps with fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_20_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of several monocular depth estimation methods. The input image is processed by different methods: MiDaS, DPT, Depth Anything, Marigold, and BetterDepth.  The results show that feed-forward methods (like Depth Anything) produce good overall shape but lack detail, while diffusion-based methods (like Marigold) capture fine details but struggle with accurate global shape. BetterDepth aims to combine the strengths of both approaches, providing both accurate overall shape and fine details. The 3D reconstructions with color-coded normals further illustrate the differences in depth map quality.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_21_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of several monocular depth estimation methods.  The input image is shown, followed by depth maps and 3D renderings generated by MiDaS, DPT, Depth Anything, Marigold, and the proposed BetterDepth method.  The figure highlights that feed-forward methods (e.g., Depth Anything) produce good overall depth but lack fine details, while diffusion-based methods (e.g., Marigold) excel at detail but sometimes struggle with global shape accuracy.  BetterDepth aims to improve upon both approaches, achieving both robust global depth and fine detail. The color-coded normals in the 3D reconstructions provide additional visual information about surface orientation.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_22_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows that while feed-forward methods (like Depth Anything) excel at predicting the overall 3D shape, they lack fine details. Conversely, diffusion-based methods (like Marigold) are better at capturing details but struggle with global shape accuracy. BetterDepth, the proposed method, aims to combine the strengths of both approaches by refining the output of a pre-trained feed-forward model using a diffusion model to improve both global shape and fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_23_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure showcases the performance comparison of different monocular depth estimation methods.  It compares the depth maps and 3D reconstructions generated by several state-of-the-art techniques including feed-forward methods (like Depth Anything) and diffusion-based methods (like Marigold), highlighting their respective strengths and weaknesses.  Depth Anything excels at providing robust global shapes but lacks detail.  Marigold produces highly detailed depth maps but struggles with global shape. The authors' method, BetterDepth, is shown to combine the strengths of both, achieving both accurate global geometry and fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_24_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods, including feed-forward methods (Depth Anything) and diffusion-based methods (Marigold).  It shows that feed-forward methods are good at capturing the overall shape but lack detail, while diffusion-based methods excel at detail but struggle with global shape. The authors' method, BetterDepth, aims to combine the strengths of both approaches, resulting in accurate depth estimation with fine details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_25_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of several monocular depth estimation methods, including feed-forward methods (Depth Anything) and diffusion-based methods (Marigold). It highlights the strengths and weaknesses of each approach, demonstrating that feed-forward methods excel at capturing global 3D shapes but lack fine details, while diffusion-based methods produce more detailed depth maps but struggle with the overall shape in more complex scenes. The authors propose their method, BetterDepth, as a solution that combines the strengths of both approaches and delivers more accurate and detailed zero-shot depth estimation.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_26_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure shows a comparison of different monocular depth estimation methods applied to the same input image.  The methods include MiDaS, DPT, Depth Anything, Marigold, and the authors' proposed method, BetterDepth.  Each method's output is visualized as a depth map and a 3D reconstruction with color-coded normals.  The figure highlights the strengths and weaknesses of each approach: feed-forward methods (like Depth Anything) produce good overall shape but lack detail, diffusion-based methods (like Marigold) capture fine details but struggle with global shape, while BetterDepth aims to combine the best of both.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_27_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure compares the performance of different monocular depth estimation methods.  It shows input images and their corresponding depth maps and 3D reconstructions generated by several methods, including MiDaS, DPT, Depth Anything, Marigold, and the authors' proposed method, BetterDepth. The comparison highlights BetterDepth's ability to achieve both accurate global geometry and fine-grained details, outperforming other methods that either excel at one aspect but struggle with the other.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_28_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure shows a comparison of monocular depth estimation results from different methods: MiDaS, DPT, Depth Anything, Marigold, and the proposed BetterDepth.  It highlights the trade-off between capturing accurate global shape and fine details. Feed-forward methods (MiDaS, DPT, Depth Anything) excel at global shape but lack fine detail, while diffusion-based methods (Marigold) are better at details but struggle with global accuracy. BetterDepth aims to combine the best of both worlds, showing accurate global shape with sharp details.", "section": "1 Introduction"}, {"figure_path": "35WwZhkush/figures/figures_29_1.jpg", "caption": "Figure 1: Monocular depth estimation (depth map and 3D reconstruction with color-coded normals). Feed-forward methods, like Depth Anything [49], produce robust global 3D shape but suffer from over-smoothed details. Diffusion-based methods, like Marigold [17], extract fine details but fall short in zero-shot global shape recovery. Our proposed BetterDepth offers the best of both worlds and achieves robust zero-shot depth estimation with fine details.", "description": "This figure shows a comparison of monocular depth estimation results from different methods. The input is a single image of a cat.  The first method, MiDaS, provides a coarse depth map.  DPT improves on this, while Depth Anything produces a more accurate global shape but lacks detail. Marigold, a diffusion-based method, produces a very detailed depth map but the global shape is less accurate. BetterDepth, the proposed method, aims to combine the strengths of these approaches, offering both accurate global shape and fine details.", "section": "1 Introduction"}]