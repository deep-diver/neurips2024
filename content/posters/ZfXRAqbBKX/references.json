{"references": [{"fullname_first_author": "Stephanie C. Y. Chan", "paper_title": "Data distributional properties drive emergent in-context learning in transformers", "publication_date": "2022", "reason": "This paper is foundational to the understanding of in-context learning in transformers, a key concept related to the paper's approach to addressing knowledge conflicts in LLMs."}, {"fullname_first_author": "Hung-Ting Chen", "paper_title": "Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence", "publication_date": "2022-12-07", "reason": "This work directly addresses knowledge conflicts in LLMs, providing a strong basis for the current research's investigation into mitigating these conflicts."}, {"fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "publication_date": "2022-05-22", "reason": "The concept of 'knowledge neurons' introduced in this paper is central to the proposed IRCAN framework, which leverages these neurons to address knowledge conflict issues."}, {"fullname_first_author": "Weijia Shi", "paper_title": "Trusting your evidence: Hallucinate less with context-aware decoding", "publication_date": "2023-05-14", "reason": "This paper introduces context-aware decoding (CAD), a method that is compared with the proposed approach, highlighting the novelty and significance of the current work."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper presents LLaMA-2, a prominent LLM used extensively in the experiments of the current paper, showcasing the relevance and impact of the research on widely adopted models."}]}