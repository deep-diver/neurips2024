[{"figure_path": "ZfXRAqbBKX/tables/tables_5_1.jpg", "caption": "Table 1: Results (in %) of the completion task on the MemoTrap dataset. The best results are highlighted in bold, and the second-best results are underlined.", "description": "This table presents the accuracy and stubbornness rate (SR) for different LLMs on the MemoTrap completion task.  The accuracy reflects how well the models generated contextually appropriate endings for proverbs, while the SR measures the tendency of models to stick to memorized, common proverb endings rather than adapting to the given context. The results show that IRCAN significantly improves both metrics across different LLMs.", "section": "4 Experiments"}, {"figure_path": "ZfXRAqbBKX/tables/tables_5_2.jpg", "caption": "Table 2: Results (in %) of the multiple-choice task on the COSE_KRE and ECARE_KRE datasets.", "description": "This table presents the results of the multiple-choice task using the COSE_KRE and ECARE_KRE datasets.  It compares the accuracy (ACC) and stubbornness rate (SR) of several different LLMs (large language models), including baselines (Original, prompt engineering methods, ITI), CAD, IRCAN, and IRCAN combined with CAD.  The results show the performance improvement of the proposed method (IRCAN) in handling knowledge conflicts in the context of multiple-choice questions.", "section": "4.4 Main Results"}, {"figure_path": "ZfXRAqbBKX/tables/tables_9_1.jpg", "caption": "Table 3: Results of general abilities of LLMs on widely-used benchmarks.", "description": "This table presents the results of evaluating various LLMs on six widely-used benchmarks: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.  The table compares the performance of the original LLMs against those enhanced by IRCAN.  It shows the accuracy scores for each model on each benchmark, giving an overall sense of the models' general abilities and how IRCAN impacts them.", "section": "4 Experiments"}, {"figure_path": "ZfXRAqbBKX/tables/tables_16_1.jpg", "caption": "Table 1: Results (in %) of the completion task on the MemoTrap dataset. The best results are highlighted in bold, and the second-best results are underlined.", "description": "This table presents the results of the completion task using the MemoTrap dataset.  It shows the accuracy and stubbornness rate for several LLMs (Gemma-2B, LLaMA-2-7B, Amber (7B), LLaMA-3-8B, LLaMA-2-13B) under different conditions: Original, ITI (Probe Weight Direction), ITI (Mass Mean Shift), CAD, IRCAN, and IRCAN-CAD.  The best performing method for each LLM is highlighted in bold, and the second best is underlined.  Accuracy measures the percentage of correctly generated words, while the stubbornness rate shows how often the model uses memorized knowledge instead of contextual knowledge.", "section": "4 Experiments"}, {"figure_path": "ZfXRAqbBKX/tables/tables_17_1.jpg", "caption": "Table 1: Results (in %) of the completion task on the MemoTrap dataset. The best results are highlighted in bold, and the second-best results are underlined.", "description": "This table presents the results of the completion task experiments conducted on the MemoTrap dataset.  The task involved generating a continuation to a well-known proverb in a way that deviated from the standard ending.  The table shows the accuracy and stubbornness rate (SR) for several large language models (LLMs) using various methods.  Accuracy represents the percentage of correctly generated words, while SR indicates the tendency of the LLM to stick to its pre-trained knowledge instead of incorporating contextual information provided in the prompt. The best performing model for each LLM is highlighted in bold, with the second-best underlined.", "section": "4 Experiments"}, {"figure_path": "ZfXRAqbBKX/tables/tables_20_1.jpg", "caption": "Table 1: Results (in %) of the completion task on the MemoTrap dataset. The best results are highlighted in bold, and the second-best results are underlined.", "description": "This table presents the results of the completion task experiments performed on the MemoTrap dataset.  The accuracy and stubbornness rate are measured for various LLMs (Gemma-2B, LLaMA-2-7B, Amber (7B), LLaMA-3-8B, LLaMA-2-13B) using different methods: Original, ITI (Probe Weight Direction and Mass Mean Shift), CAD, IRCAN and IRCAN-CAD. The best performing method for each LLM is highlighted in bold, while the second best is underlined.", "section": "4. Experiments"}]