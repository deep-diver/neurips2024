[{"figure_path": "3zYmlmkIuK/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of our result against baseline methods for multi-agent contextual bandits and MDPs. Note that the first four rows are for contextual bandits, and the last three are for reinforcement learning. Only our algorithms are in the general function approximation setting. We abbreviate dime = dime(F) and N = N(F), and hide logarithmic factors. For algorithms with synchronized communication, each communication round actually corresponds to M rounds in asynchronous settings, which explains the extra M terms.", "description": "This table compares the performance of the proposed Async-NLin-UCB and Async-NLSVI-UCB algorithms against existing state-of-the-art algorithms for multi-agent contextual bandits and MDPs.  It highlights the regret (measure of performance) and communication complexity for each algorithm, distinguishing between fully synchronous and asynchronous communication protocols.  The table shows that the proposed algorithms achieve comparable or better performance with significantly lower communication overhead, especially in the general function approximation setting.", "section": "Related Work"}]