[{"figure_path": "UoxuaOGV6B/tables/tables_4_1.jpg", "caption": "Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. Spectral is abbreviation for Spectral Adapter4. See Section 4.1 for experimental details.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark when fine-tuning the DeBERTaV3-base model.  The methods compared include LoRA, DoRA, OFT, AdaLoRA, and the proposed Spectral Adapter.  The table shows the accuracy achieved by each method on various GLUE tasks (MNLI, SST-2, MRPC, etc.) along with the number of trainable parameters used. Spectral Adapter demonstrates superior performance with fewer trainable parameters.", "section": "Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/tables/tables_4_2.jpg", "caption": "Table 2: Accuracy comparison of fine-tuning Mistral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experimental details.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on the GSM8K benchmark when fine-tuning the Mistral 7B language model.  The methods compared include LoRA, DoRA, and the proposed Spectral Adapter.  The table shows the accuracy achieved by each method, along with the percentage of trainable parameters used relative to the full model.  The results indicate that the Spectral Adapter achieves the highest accuracy with a comparable number of parameters to LoRA.", "section": "4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/tables/tables_7_1.jpg", "caption": "Table 3: Baseline methods comparison for parameter efficiency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details.", "description": "This table compares different parameter-efficient fine-tuning (PEFT) methods in terms of their parameter efficiency and granularity.  It shows the number of trainable parameters required by each method (LoRA, SVDiff, LiDB, OFT, VeRA, and Spectral Adapter<sup>R</sup>), the granularity of the parameter budget (how finely the number of parameters can be adjusted), and whether auxiliary parameters are required. Spectral Adapter<sup>R</sup> offers a good balance between parameter efficiency and granularity.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral Adapter<sup>R</sup>"}, {"figure_path": "UoxuaOGV6B/tables/tables_16_1.jpg", "caption": "Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. Spectral is abbreviation for Spectral Adapter4. See Section 4.1 for experimental details.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark when fine-tuning the DeBERTaV3-base model.  The methods compared include LoRA, DORA, OFT, AdaLoRA, and the proposed Spectral Adapter. The table shows the accuracy achieved by each method on various GLUE tasks (MNLI, SST-2, MRPC, COLA, QNLI, QQP, RTE, and STS-B) along with the number of trainable parameters used.  The results demonstrate the superior performance and parameter efficiency of the proposed Spectral Adapter4.", "section": "4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/tables/tables_17_1.jpg", "caption": "Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1", "description": "This table displays the hyperparameter settings used for fine-tuning the Mistral 7B language model in Section 4.1 of the paper.  It includes the learning rate, LoRA alpha, batch size, number of epochs, LoRA dropout rate, and weight decay for LoRA, DoRA, and the proposed Spectral Adapter method. Note the absence of a LoRA alpha value for the Spectral Adapter, indicating a difference in hyperparameter optimization for this method compared to the others.", "section": "4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/tables/tables_17_2.jpg", "caption": "Table 2: Accuracy comparison of fine-tuning Mistral 7B model [23] with different PEFT methods on GSM8K benchmark. See Section 4.1 for experimental details.", "description": "This table presents the results of fine-tuning the Mistral 7B language model on the GSM8K benchmark using different parameter-efficient fine-tuning (PEFT) methods.  The methods compared are LoRA, DoRA, and the proposed Spectral Adapter. The table shows the accuracy achieved by each method, along with the number of trainable parameters used as a percentage of the total model parameters.  The results demonstrate the superior performance of the Spectral Adapter in achieving higher accuracy with a comparable number of trainable parameters.", "section": "Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter4"}, {"figure_path": "UoxuaOGV6B/tables/tables_20_1.jpg", "caption": "Table 3: Baseline methods comparison for parameter efficiency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details.", "description": "This table compares different parameter-efficient fine-tuning (PEFT) methods in terms of their parameter efficiency.  It shows the number of trainable parameters required for each method, categorized by the granularity of parameter tuning (how many parameters are adjusted). The table highlights the parameter efficiency improvement of Spectral AdapterR compared to other baselines, including LoRA, SVDiff, LiDB, OFT, and VeRA.  It also includes an \"Auxiliary Param\" column to indicate if any auxiliary parameters are used.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR"}, {"figure_path": "UoxuaOGV6B/tables/tables_21_1.jpg", "caption": "Table 8: Hyperparameters for Spectral Adapter<sup>R</sup> for diffusion model fine-tuning task in Section 4.3", "description": "This table presents the hyperparameter settings used for the Spectral Adapter<sup>R</sup> model in the diffusion model fine-tuning experiments described in Section 4.3 of the paper.  It details the learning rates for both the 'text' and 'unet' components of the model for different rank values (r) across three different concepts (vase, chair, table).  The variations in learning rates across the concepts and ranks highlight the model's adaptability and the influence of hyperparameter tuning on performance.", "section": "4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral Adapter<sup>R</sup>"}]