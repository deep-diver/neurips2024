{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021", "reason": "This paper introduces the LoRA method, a foundational low-rank adaptation technique that the current paper builds upon and improves."}, {"fullname_first_author": "Aidan Clark", "paper_title": "Kronecker Adapters: Low-Rank, Efficient Fine-Tuning for Large Language Models", "publication_date": "2022", "reason": "This paper proposes Kronecker adapters, a related low-rank adaptation technique which provides further context for low-rank methods in efficient fine-tuning."}, {"fullname_first_author": "S. Hayou", "paper_title": "LoRA+: Efficient Low Rank Adaptation of Large Models", "publication_date": "2024", "reason": "This paper provides a significant improvement to the original LoRA algorithm and further explores its parameter efficiency, directly relevant to the current paper's goal."}, {"fullname_first_author": "Tom Dettmers", "paper_title": "QLoRA: Efficient Fine-tuning of Quantized LLMs", "publication_date": "2023", "reason": "This work explores further efficiency by combining LoRA with quantization, a relevant optimization strategy considered in the current paper."}, {"fullname_first_author": "Si-Yuan Liu", "paper_title": "DoRA: Weight-decomposed low-rank adaptation", "publication_date": "2024", "reason": "This paper proposes DoRA, a weight decomposition technique to address limitations of LoRA, which is closely related to and compared with the approach of the current paper."}]}