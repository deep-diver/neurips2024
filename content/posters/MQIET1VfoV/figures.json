[{"figure_path": "MQIET1VfoV/figures/figures_0_1.jpg", "caption": "Figure 1: An example of how using an equivariant function approximator shrinks the total search space.", "description": "This figure illustrates how using an equivariant function approximator, which is a function that maintains its properties under certain transformations (like rotations), reduces the search space for learning a target function.  The figure shows three overlapping circles representing different sets of functions. The smallest, purple circle represents all learnable equivariant functions. This is a subset of all learnable functions (the larger, light-blue circle). The largest, light-orange circle contains all learnable functions, constrained by the data available during training. The arrow highlights that using equivariance restricts the search to a much smaller set of functions, making the learning process more efficient and likely to find a suitable solution.", "section": "1 Introduction"}, {"figure_path": "MQIET1VfoV/figures/figures_1_1.jpg", "caption": "Figure 2: An example of rotational equivariance/symmetry in MPE simple spread environment. Note as the agent (in red) positions are rotated, the optimal actions (arrows) are also rotated.", "description": "This figure shows an example of rotational equivariance in the Multi-agent Particle Environment (MPE) simple spread environment.  The left panel depicts three agents (red circles numbered 1, 2, and 3) and their optimal actions (red arrows) in a particular configuration. The right panel shows the same agents and their optimal actions after a 90-degree rotation.  The key observation is that when the agent positions are rotated, the corresponding optimal actions also rotate accordingly, demonstrating the rotational symmetry inherent in the environment and the importance of equivariance for efficient learning. This symmetry allows an agent to generalize its learned policy from one configuration to another simply by applying the rotation, rather than learning each separately.", "section": "Abstract"}, {"figure_path": "MQIET1VfoV/figures/figures_4_1.jpg", "caption": "Figure 3: An example of biased learning in MPE simple spread environment. Left: We observed the behavior of the EGNN agents in this early training phase. Each agent moved away from the origin due to the EGNN bias. Right: Note the very low reward in early training steps due to the biased policies moving away from the goals.", "description": "This figure shows an example of how EGNNs suffer from early exploration bias in the simple spread environment of MPE.  The left panel illustrates the agents' biased movement away from the origin (goal) during the initial training phase, a consequence of the inherent bias in the EGNN structure. The right panel depicts the resulting poor reward performance during this early training period, highlighting the negative impact of the exploration bias on overall learning.", "section": "4.1 Biased Exploration in EGNN Policies"}, {"figure_path": "MQIET1VfoV/figures/figures_6_1.jpg", "caption": "Figure 4: An example of using an Equivariant Graph Neural Network in MARL. Note that the state must be structured as a graph with each node having an equivariant  and invariant component. As discussed in 4.3, the output of the policy uses  for equivariant (typically spatial) actions, and the  for invariant components of the actions", "description": "This figure shows an example of how an Equivariant Graph Neural Network (EGNN) can be used in multi-agent reinforcement learning (MARL).  The input state is structured as a graph where each node represents an agent or entity. Each node has two types of embeddings: an invariant embedding (h) and an equivariant embedding (u). The invariant embeddings are fed into a value head to output a value estimate for the state, while the equivariant embeddings are fed into a policy head to output actions. The figure highlights the way equivariance and invariance are used to generate actions, showing how the output from the EGNN is separated to deal with different properties of the action space. ", "section": "4.3 Adapting Architectures for Complex Action Spaces"}, {"figure_path": "MQIET1VfoV/figures/figures_7_1.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the performance of Proximal Policy Optimization (PPO) using different neural network architectures (MLP, GNN, EGNN, E2GN2, and E3AC) on two Multi-agent Particle Environments (MPE): simple spread and predator-prey (tag).  The top row shows reward as a function of environment steps, while the bottom row shows reward as a function of wall-clock time.  Standard errors across 10 seeds are shown.", "section": "5.1 Training Environments"}, {"figure_path": "MQIET1VfoV/figures/figures_8_1.jpg", "caption": "Figure 6: Comparing performance of PPO on SMACv2 with various Neural Networks representing the policy and value function. Each chart represents a different race from the SMACv2 environment. We show the standard errors computed across 10 seeds.", "description": "This figure compares the performance of different neural network architectures (E2GN2, EGNN, GNN, E3AC, and MLP) in the context of Proximal Policy Optimization (PPO) on the Starcraft Multi-Agent Challenge (SMACv2) environment.  The performance is measured by win rate and is shown separately for three different races (Terran, Protoss, and Zerg). The standard errors computed across 10 different seeds are displayed to show the variability in performance. This illustrates how the E2GN2 model outperforms other architectures in sample efficiency and achieving a higher win rate in the SMACv2 environment across different races.", "section": "5.2 Training Results"}, {"figure_path": "MQIET1VfoV/figures/figures_8_2.jpg", "caption": "Figure 7: SMACv2 initialization schemes used for testing generalization", "description": "This figure shows three different initializations of agents in the StarCraft Multi-Agent Challenge (SMACv2) environment used to test the generalization capabilities of the proposed E2GN2 model.  The \"Surrounded\" configuration shows a symmetric arrangement of agents around the center. The \"Surrounded Left\" and \"Surrounded Right\" configurations are variations where agents are placed only on the left or right sides respectively, while maintaining the same overall count and positions relative to the center.  These configurations allow the researchers to evaluate whether the learned agent policies generalize to scenarios not seen during training.", "section": "5.3 Generalization"}, {"figure_path": "MQIET1VfoV/figures/figures_15_1.jpg", "caption": "Figure 6: Comparing performance of PPO on SMACv2 with various Neural Networks representing the policy and value function. Each chart represents a different race from the SMACv2 environment. We show the standard errors computed across 10 seeds.", "description": "This figure compares the performance of Proximal Policy Optimization (PPO) using different neural network architectures (E2GN2, EGNN, GNN, E3AC, and MLP) on the StarCraft Multi-Agent Challenge (SMACv2) benchmark.  The results are shown for three different races (Terran, Protoss, and Zerg) in SMACv2, illustrating the win rate against the number of environment steps.  Error bars represent standard errors calculated across 10 independent training runs. The figure helps visualize the sample efficiency and performance differences between the various neural networks in multi-agent reinforcement learning tasks.", "section": "5.2 Training Results"}, {"figure_path": "MQIET1VfoV/figures/figures_16_1.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the learning performance of Proximal Policy Optimization (PPO) using different neural network architectures (MLP, GNN, EGNN, E2GN2, E3AC) on two Multi-Agent Particle Environment (MPE) tasks: cooperative navigation (spread) and predator-prey (tag).  The top panel shows the reward achieved as a function of environment steps, while the bottom panel shows the reward as a function of wall-clock time. Standard errors across 10 random seeds are included for each network in both panels.  The results show that E2GN2 consistently outperforms other networks in both reward and training time.", "section": "5 Experiments"}, {"figure_path": "MQIET1VfoV/figures/figures_16_2.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the performance of Proximal Policy Optimization (PPO) using different neural network architectures (MLP, GNN, EGNN, E2GN2, and E3AC) on two Multi-Agent Particle Environment (MPE) tasks: simple spread and tag.  The top panel shows the reward achieved as a function of the number of environment steps, while the bottom panel shows the reward as a function of wall-clock training time.  Standard errors across 10 different training runs are included for each method.  The results illustrate the differences in sample efficiency and training speed among the different neural networks.", "section": "5.1 Training Environments"}, {"figure_path": "MQIET1VfoV/figures/figures_16_3.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the learning performance of Proximal Policy Optimization (PPO) using different neural network architectures on two Multi-agent Particle Environment (MPE) tasks: spread and tag.  The top panel shows the reward accumulated over environment steps, illustrating the sample efficiency of each approach. The bottom panel displays the reward achieved as a function of wall clock time, highlighting the training speed.  The neural networks compared include Multilayer Perceptrons (MLPs), Graph Neural Networks (GNNs), Equivariant Graph Neural Networks (EGNNs), and the proposed Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). Error bars indicate standard errors calculated across 10 independent runs.", "section": "5.1 Training Environments"}, {"figure_path": "MQIET1VfoV/figures/figures_17_1.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the performance of Proximal Policy Optimization (PPO) with different neural network architectures (MLP, GNN, EGNN, E2GN2, and E3AC) on two Multi-Agent Particle Environment (MPE) tasks: simple spread and tag. The top panel shows the reward achieved as a function of the number of environment steps, while the bottom panel displays the reward as a function of wall-clock training time.  Standard errors across 10 different random seeds are shown for each method to demonstrate statistical significance.", "section": "5.1 Training Environments"}, {"figure_path": "MQIET1VfoV/figures/figures_17_2.jpg", "caption": "Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time", "description": "This figure compares the performance of Proximal Policy Optimization (PPO) using different neural network architectures (MLP, GNN, EGNN, E2GN2, and E3AC) on two Multi-agent Particle Environment (MPE) tasks: simple spread and tag.  The top panel shows the cumulative reward over environment steps, illustrating the learning speed and final performance of each method.  The bottom panel shows the cumulative reward against wall-clock training time, highlighting the training efficiency.  Error bars represent standard errors across 10 independent training runs. The figure demonstrates the improved sample efficiency and final performance of E2GN2, especially in comparison to traditional networks (MLP and GNN) and other equivariant networks (EGNN and E3AC).", "section": "5 Experiments"}]