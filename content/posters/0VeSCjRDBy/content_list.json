[{"type": "text", "text": "Adversarial Moment-Matching Distillation of Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chen Jia SI-TECH Information Technology jiachenwestlake@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions. Instead of optimizing these mandatory behavior cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher\u2019s behavior from both on- and off-policy perspectives. To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the momentmatching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) like GPT-4 [1] and LLaMA [35] have revolutionized natural language processing, significantly enhancing the quality of text generation across various tasks. This success is largely due to the extensive scale of training data and the substantial increase in model parameters [19]. However, the high computational and memory requirements of these models present significant challenges for practical deployment. To address these issues, knowledge distillation (KD) [16] has emerged as a key technique. KD involves transferring knowledge from a large, complex teacher model to a smaller, more efficient student model, thereby maintaining high performance while reducing resource demands. Most distillation methods for auto-regressive text generation models, including LLMs, employ metrics of probability distribution distance, such as Kullback-Leibler (KL) divergence [20] and reverse KL divergence [14], aiming to align the token-level probability distributions between the teacher and student models. ", "page_idx": 0}, {"type": "text", "text": "The distribution matching-based distillation methods can be viewed as behavior cloning on a decisionmaking problem from the perspective of imitation learning [24, 14, 2]. Based on this concept, early works based on the teacher-generated outputs [20] or a supervised dataset [30] can be viewed as an offpolicy approach. Recent works further incorporate an on-policy approach, training the student on its self-generated outputs [24], using KL-based divergence [14, 2, 21] and total variation (TV) distance [38]. Accordingly, such distribution matching-based methods face the sub-optimality problem. The objective functions aimed at aligning the probability distributions between the teacher and student models can be straightforward but cannot fully capture the goal of distilling language knowledge. First, intuitively, the correct output for an input can vary, and thus behavior cloning cannot capture the full knowledge of a teacher. Besides, there is no standardized definition for the quality of a generated output given an input, which makes it difficult to define the objective of knowledge distillation. This ", "page_idx": 0}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/2bff0de4fc4b34279c9c45625c83dcb644a8e2cd184ee9b5c925d51be6cd5c8e.jpg", "img_caption": ["(c) On-policy $\\boldsymbol{Q}$ -value moment-match. distillation (ours). "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/2345f521e5c87854b187231d4222f6e611b828ca4a8d0a3a7507ef128270cd25.jpg", "img_caption": ["(d) Off-policy $Q_{\\mathrm{~\\,~}}$ -value moment-match. distillation (ours). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. $\\pi_{\\theta}$ and $\\pi_{*}$ denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teachergenerated outputs) perspectives, our approach optimizes moment-matching of action-value functions ( $Q$ -functions) instead of minimizing the distribution distance measured by $\\mathcal{M}=\\mathbb{K}\\boldsymbol{\\mathrm{L}}$ , RKL, TV, etc. ", "page_idx": 1}, {"type": "text", "text": "imposes a significant limitation on the generalization performance of the student model through distillation. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned issues, we employ a reinforcement learning (RL) formulation for the auto-regressive text generation problem and utilize the definition of imitation gap to describe the high-level goal of knowledge distillation. Additionally, we address the imitation gap for KD by matching moments of the action-value function, which reflects the quality of token-level predictions for the entire output. In addressing the action-value function, we adopt the approach of Swamy et al. [33], considering a two-player minimax game between the language policy and the action-value functions, aiming to minimize an upper bound of the moment-matching objective. For this purpose, we introduce an adversarial training algorithm based on the policy gradient to jointly optimize the on-/off-policy objectives. Figure 1 illustrates the overall approach. ", "page_idx": 1}, {"type": "text", "text": "Theoretically, we compare the moment-matching objective with other distribution-matching measurements such as step-wise TV distance and analyze the convergence rate of our algorithm to an $\\epsilon_{}$ -accurate stationary point for optimization. Empirically, we evaluate our approach on both the instruction-following dataset and three task-specific datasets for text summarization, machine translation, and commonsense reasoning. Results demonstrate that the proposed adversarial momentmatching approach effectively optimizes the moment-matching distance of the imitation gap and outperforms state-of-the-art KD methods and a range of distribution-matching-based methods. The code and implementation are released at https://github.com/jiachenwestlake/MMKD. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Distillation of large language models. There has been an increasing interest in knowledge distillation (KD) of auto-regressive LMs, especially concerning large language models (LLMs) [40, 41]. This process effectively transfers elicited knowledge from teacher LLMs to smaller student models, aiming to compress the large size of neural network parameters and make LLMs more efficient. Sequencelevel KD (SeqKD) [20] is a variation of supervised fine-tuning (SFT) in KD. It can be viewed as the simplest method for distillation of black-box LLMs by fine-tuning the student model with teachergenerated outputs. This method has been extensively used for LLMs and has achieved success [34, 6]. In contrast, distillation of white-box LLMs can make full use of internal information of the teacher model, such as logits [30, 38] and hidden states [23], for distribution alignment, making it more effective and efficient for KD. However, unlike previous work that explicitly clones the distribution of teacher LLMs into student models, this work learns an auxiliary $Q$ -value function to guide KD. ", "page_idx": 1}, {"type": "text", "text": "Distillation via distribution matching. Most promising results in the distillation of white-box LLMs are achieved by minimizing divergence between the probability distributions of the teacher model and student models. Kullback-Leibler (KL) divergence, reverse Kullback-Leibler (RKL) divergence, and Jensen\u2013Shannon (JS) divergence are three widely used KD objectives for auto-regressive LMs [38, 14, 2, 21, 40]. Wen et al. [38] have shown the equivalent formulations of sequence-level KL, RKL, JS divergences, and the step-wise terms. Additionally, they also present the strong performance of step-wise total variation (TV) distance for KD, which can upper bound the sequence-level term. As a result, most recent works focus on on-policy approaches for KD [2] and combine the realtime-generated outputs by students (on-policy) with the real-time-generated outputs by teachers (or from supervised datasets) (off-policy). Following this line, Gu et al. [14] further propose a policy gradient-based method to address the high variance issues of RKL-based methods while Ko et al. [21] propose a more efficient and effective method using a skew KL divergence loss and an adaptive off-policy approach. We also focus on a combination of on-policy and off-policy objectives for KD, but we introduce a more sophisticated moment-matching approach instead of directly using the well-studied distribution-matching metrics such as KL, RKL, JS divergences, and TV distance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Distillation via reinforcement learning. In a common formulation of RL in text generation [43, 26, 15], an auto-regressive model can be viewed as a language policy, making decisions on the next token (action) based on the currently generated sequence (state). From this perspective, KD corresponds to behavior cloning in imitation learning [20, 7, 14, 2]. For imitation learning in text generation, early works such as SeqGAN [43] and TextGAIL [39] utilize a generative adversarial framework to balance between the reward model, optimized by discriminating generated/real-word text, and the language policy, optimized by policy gradient-based methods using the reward model. Existing work on KD via imitation learning refers to ImitKD [24], which optimizes the student policy by learning from demonstrations of the teacher model. RL-based distillation can also be especially relevant for leveraging the feedback from the teacher to train student models [4, 9], in which teacher models are used to generate the feedback data for training a reward model. We build our method upon an RL-based imitation learning framework. However, unlike previous work [20, 14, 2], we propose an adversarial moment-matching approach to enhance behavior cloning. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Notations and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we consider the text generation task as a decision-making process and give a corresponding reinforcement learning (RL) formulation. ", "page_idx": 2}, {"type": "text", "text": "Text generation. Given an input $\\textbf{\\em x}$ , the auto-regressive generation task in our work aims to generate a sequence of tokens as the output $(y_{1},\\dots,y_{T})$ , where $y_{t}$ comes from a vocabulary $\\mathcal{V}$ . For simplicity, we define $\\pmb{y}=(y_{0},y_{1},\\dots,y_{T})$ as the full input-output sequence, where $y_{0}=x$ denotes the input. The generator is modeled by a conditional probability distribution $p_{\\theta}(\\pmb{y}|\\pmb{x})=\\Pi_{t=0}^{T-1}p_{\\theta}(y_{t+1}|\\pmb{y}_{\\le t})$ where $\\pmb{y}_{\\leq t}$ denotes the prefix $\\left(y_{0},y_{1},\\ldots,y_{t}\\right)$ , $t\\in\\{0,1,\\ldots,T-1\\}$ . ", "page_idx": 2}, {"type": "text", "text": "RL formulation. We formulate text generation as a sequential decision-making process. At each time step $t\\in\\{0,\\ldots,T-1\\}$ , the policy $\\pi_{\\theta}$ takes an action $(t)$ : $y_{t+1}\\in\\mathcal{V}$ based on the current state $(t)$ : $\\pmb{y}_{\\leq t}\\in\\mathcal{Y}$ , transits to the next state $(t+1)$ : $\\pmb{y}_{\\leq t+1}\\in\\mathcal{Y}$ and receives a reward $(t)$ : $r(\\pmb{y}_{\\leq t},y_{t+1})$ by a reward function $r:\\mathcal{Y}\\times\\mathcal{V}\\to\\mathbb{R}$ . The policy corresponds to the generation model $\\pi_{\\theta}(\\overline{{y_{t+1}}}|y_{\\leq t})=$ $p_{\\theta}(y_{t+1}|\\pmb{y}_{\\le t})$ . We focus on a (conditional) trajectory $\\{y_{1},y_{\\leq1},y_{2},\\ldots,y_{\\leq T-1},y_{T}\\}=:\\tau\\sim\\bar{\\pi}_{\\theta}|x$ which refers to a sequence of state-action pairs generated by given an initial state $y_{0}\\,=\\,{\\pmb x}\\,\\sim\\,p_{\\pmb x}$ and then repeatedly sampling an action $y_{t+1}\\;\\sim\\;\\pi_{\\theta}(\\cdot|\\pmb{y}_{\\le t})$ and obtain the next state $y_{\\le t+1}\\;\\sim$ $T(\\cdot\\vert y_{\\leq t},y_{t+1})^{1}$ for $T$ time steps. In such case, the probability of a (conditional) trajectory is formally represented as $p(\\tau|x,\\pi_{\\theta})=\\Pi_{t=0}^{T-1}T(\\pmb{y}_{\\le t+1}|\\pmb{y}_{\\le t},y_{t+1})\\pi_{\\theta}(y_{t+1}|\\pmb{y}_{\\le t})$ . We also define our value function and $Q$ -value function as $\\begin{array}{r}{V^{\\pi_{\\theta}}(\\pmb{y}_{\\leq t})=\\mathbb{E}_{\\tau_{(t)}\\sim\\pi_{\\theta}|\\pmb{y}_{\\leq t}}\\left[\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}r(\\pmb{y}_{\\leq t^{\\prime}},y_{t^{\\prime}+1})\\right]}\\end{array}$ and $\\begin{array}{r}{Q^{\\pi_{\\theta}}(y_{\\leq t},y_{t+1})\\,=\\,\\mathbb{E}_{\\tau_{(t)}\\sim\\pi_{\\theta}|y_{\\leq t},y_{t+1}}\\left[\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}r(y_{\\leq t^{\\prime}},y_{t^{\\prime}+1})\\right]^{-1}}\\end{array}$ , where $\\gamma\\,\\in\\,(0,1)$ denotes the discounting factor. We define the RL objective in our generation task to maximize the performance $\\begin{array}{r}{J(\\pi_{\\theta})=\\overset{\\cdot}{\\mathbb{E}_{x\\sim p_{x}}}\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}|x}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}r(y_{\\leq t}^{-},y_{t+1})\\right]}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Knowledge Distillation as Moment-Matching Imitation Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the RL formulation of auto-regressive generation, we can view the goal of knowledge distillation at a high-level as to bridge the performance gap between the teacher policy and the student policy. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Imitation gap). We define the imitation gap between the teacher policy and student policy as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{\\theta})=\\underset{\\tau\\sim\\pi_{*}\\pi_{*}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}r(y_{\\le t},y_{t+1})\\right]-\\underset{\\tau\\sim\\pi_{\\theta}\\pi_{*}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}r(y_{\\le t},y_{t+1})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From the perspective of imitation learning [33, 32], the objective of distillation from the teacher policy $\\pi_{*}$ to the student policy $\\pi_{\\theta}$ can be represented as to minimize the imitation gap of Eq. (1) w.r.t. the parameters of student policy $\\theta$ . A direct idea from Eq. (1) is to use moment matching over the reward to optimize the imitation gap [33]. However, we actually care about the long-term reward, at each time step, we should consider the accumulated reward in the future output rather than the immediate reward to the ftiness of previous tokens (prefix). To this end, we can alternatively use the $Q$ -value function (def. in $\\S3.1\\$ ) for each timestep to represent the overall reward from the current timestep to the last timestep. Similar to [33], we can apply the Performance Difference Lemma (PDL) [18, 3, 33] to expand the imitation gap in Eq. (1) into either off-policy or on-policy expressions. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Off-policy bound of imitation gap [33]). Let $\\mathcal{F}_{Q}$ denote the set of $Q$ -value functions induced by sampling actions from $\\pi_{\\theta}$ , then we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{\\theta})\\leq\\operatorname*{sup}_{f\\in\\mathcal{F}_{Q}}\\underbrace{\\mathbb{E}_{\\tau\\sim\\pi_{*}}}_{\\tau\\sim\\pi_{*}\\mid s}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\left(f(y_{\\leq t},y_{t+1})-\\underset{y\\sim\\pi_{\\theta}(\\cdot\\mid y_{\\leq t})}{\\mathbb{E}}\\left[f(y_{\\leq t},y)\\right]\\right)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following sections, we will use $\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)$ to represent the off-policy moment-matching objective of mitation learning for $K D$ . ", "page_idx": 3}, {"type": "text", "text": "The off-policy moment-matching objective in Proposition 1 only requires a collected dataset of teacher-generated trajectories to be evaluated and minimized. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (On-policy bound of imitation gap [33]). Let $\\mathcal{F}_{Q_{*}}$ denote the set of $Q$ -value functions induced by sampling actions from $\\pi_{*}$ , then we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi_{*})-J(\\pi_{\\theta})\\leq\\operatorname*{sup}_{f\\in\\mathcal{F}_{Q_{*}}}\\,\\underset{\\tau\\sim\\tau_{\\theta}\\mid x}{\\mathbb{E}}\\,\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\left(\\underset{y\\sim\\pi_{*}(\\cdot\\mid y_{\\leq t})}{\\mathbb{E}}\\left[f(y_{\\leq t},y)\\right]-f(y_{\\leq t},y_{t+1})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following sections, we will use $\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f)$ to represent the on-policy moment-matching objective of an imitation learning for $K D$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. See Appendix A.1 and Appendix A.2 for the complete derivations of Proposition 1 and Proposition 2, respectively. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "It is notable from Proposition 2 that the on-policy moment-matching objective requires interactions with the teacher to tell us what action they would take in any state visited by the student as well as on-policy samples from the student\u2019s current policy $\\tau\\sim\\pi_{\\theta}|x$ . ", "page_idx": 3}, {"type": "text", "text": "In the remaining content of this section, we will explore the relationship between the momentmatching objectives and the existing distribution-matching objectives [38]. At the beginning, we draw a general formulation of the state-of-the-art methods for distillation of LLMs [38, 14, 2, 21] that rely on distribution-matching between the student\u2019s and teacher\u2019s predictions, through minimizing the step-wise probability distribution distance between the teacher policy and student policy. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Generalized step-wise distribution distance). The off-policy and on-policy versions are defined as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathcal{M}}^{\\mathrm{off}}\\big(\\pi_{\\theta},\\pi_{*}\\big):=\\underset{\\underset{\\tau\\sim\\pi_{*}}{\\mathbb{R}}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\mathcal{M}\\big(\\pi_{*}(\\cdot|\\boldsymbol{y}_{\\le t}),\\pi_{\\theta}(\\cdot|\\boldsymbol{y}_{\\le t})\\big)\\right];}\\\\ &{d_{\\mathcal{M}}^{\\mathrm{on}}\\big(\\pi_{\\theta},\\pi_{*}\\big):=\\underset{\\underset{\\tau\\sim\\pi_{\\theta}}{\\mathbb{R}}}{\\mathbb{E}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\mathcal{M}\\big(\\pi_{*}(\\cdot|\\boldsymbol{y}_{\\le t}),\\pi_{\\theta}(\\cdot|\\boldsymbol{y}_{\\le t})\\big)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{M}(\\cdot,\\cdot)$ denotes a distribution distance, consisting of total variation (TV) distance [38] and Kullback-Leibler $(K L)$ -based divergence [14, 2]. Detailed definitions for these distances refer to Appendix A.3. For simplicity, we directly replace M with TV, KL, RKL, etc in the following sections. ", "page_idx": 4}, {"type": "text", "text": "It is notable from Wen et al. [38] that the sequence-level KL, RKL and JS divergences can be equivalently represented as the step-wise terms, and the sequence-level TV distance can be upper bounded by the step-wise terms, which can be actually implemented by algorithms. To make a connection with the step-wise distribution distance (Definition 2), we use the following definition. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Distribution-matching formulation of moment-matching objectives). Based on Definition 2, we can re-formulate the off-policy and on-policy moment-matching (MM) objectives (Proposition 1 and Proposition 2, respectively) via step-wise distribution-matching, which can be defined as $d_{\\mathrm{MM}}^{\\mathrm{off}}(\\pi_{\\theta},\\pi_{*})$ and $d_{\\mathrm{MM}}^{\\mathrm{on}}(\\pi_{\\theta},\\pi_{*})$ respectively, where the distance metric $\\mathrm{MM}(\\cdot,\\cdot)$ can be defined as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{MM}^{\\mathrm{off(on)}}\\{\\pi_{*}(\\cdot|y_{\\le t}),\\pi_{\\theta}(\\cdot|y_{\\le t}))=\\underset{y\\sim\\pi_{*}(\\cdot|y_{\\le t})}{\\mathbb{E}}\\Big[f_{*}^{\\mathrm{off(on)}}\\{y_{\\le t},y\\}\\Big]-\\underset{y\\sim\\pi_{\\theta}(\\cdot|y_{\\le t})}{\\mathbb{E}}\\Big[f_{*}^{\\mathrm{off(on)}}\\{y_{\\le t},y\\}\\Big]\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Off-policy: $f_{*}^{\\mathrm{off}}=\\arg\\operatorname*{max}_{f\\in\\mathcal{F}_{Q}}\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)$ ; On-policy: $f_{*}^{\\mathrm{on}}=\\underset{f\\in\\mathcal{F}_{Q_{*}}}{\\arg\\operatorname*{max}}\\,\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f)$ , where $\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)$ and $\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f)$ denote the off-policy and on-policy moment-matching objectives, which are defined in Proposition $^{\\,I}$ and Proposition 2, respectively. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Under Definition 3, we observe that the main difference between the moment-matching objectives and other step-wise distribution distance, e.g., TV distance and KL-based divergences in formulation comes from the optimal Q-value function f \u2217off(on), a iming to maximize the discrepancy of its expectations based on $\\pi_{*}(\\cdot|y_{\\leq t})$ v.s. $\\pi_{\\theta}(\\cdot|y_{\\leq t})$ for each step $t\\in\\{0,1,\\ldots,T-1\\}$ . To look deeper, we draw a connection between the moment-matching objectives and step-wise TV distance using the following corollary. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Relationship between moment-matching objective and TV distance). Under a constrain of uniform boundness on the class of $Q$ -value functions for off-/on-policy learning: $\\mathcal{F}_{Q}=$ $\\mathcal{F}_{Q_{*}}=\\{f:\\|f\\|_{\\infty}\\leq1\\}$ , the moment-matching objectives in Proposition $^{\\,l}$ and Proposition 2 can be upper-bounded by the step-wise TV distance, Formally, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi_{*})-J(\\pi_{\\theta})\\leq\\underset{f:\\|f\\|_{\\infty}\\leq1}{\\operatorname*{sup}}\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)\\leq d_{\\mathrm{TV}}^{\\mathrm{off}}(\\pi_{\\theta},\\pi_{*});}\\\\ &{J(\\pi_{*})-J(\\pi_{\\theta})\\leq\\underset{f:\\|f\\|_{\\infty}\\leq1}{\\operatorname*{sup}}\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f)\\leq d_{\\mathrm{TV}}^{\\mathrm{on}}(\\pi_{\\theta},\\pi_{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for the off-policy and on-policy perspectives, respectively. ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appendix A.4 for the complete derivation. ", "page_idx": 4}, {"type": "text", "text": "We can observe from Theorem 1 that minimizing the step-wise TV distance can achieve suboptimal results compared to optimizing the moment-matching objectives $\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)$ , $\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f)$ for off-policy and on-policy imitation learning, which are defined in Proposition 1 and Proposition 2, respectively. Thus, optimizing the moment-matching objectives can potentially achieve better optimization results for imitation learning. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Adversarial training procedure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Dataset $\\mathcal{D}_{x y}$ with inputs and ground-truth outputs   \nTeacher policy $\\pi_{*}$ ; Student policy $\\pi_{\\theta}$ with initial parameters $\\theta$ pretrained on $\\mathcal{D}_{x y}$ ; Off-policy $Q$ -value function $f_{\\phi_{1}}$ and on-policy $Q$ -value function $f_{\\phi_{2}}$ with initial parameters $\\phi_{1}$ and $\\phi_{2}$ , respectively; Step sizes $K$ (outer), $N$ (inner); Learning rate $\\eta$ ; Controlling factor $\\alpha$ ; Off-/on-policy combination factor $\\beta$ Output: The optimized student policy $\\pi_{\\theta_{\\ast}}$ ", "page_idx": 5}, {"type": "text", "text": "for $k=0,1,2,\\ldots,K-1$ do for $n=0,1,2,\\dots,N-1$ do Sample an input $\\pmb{x}\\sim\\mathcal{D}_{\\pmb{x}}$ and generate an trajectory $\\tau^{\\mathrm{off}}\\sim\\pi_{*}|x$ $\\phi_{1}\\leftarrow\\phi_{1}+\\alpha\\beta\\eta\\nabla_{\\phi_{1}}\\hat{\\mathcal{L}}^{\\mathrm{off}}(\\tau^{\\mathrm{off}},\\theta_{k},f_{\\phi_{1}})$ maximize $\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta_{k}},f_{\\phi_{1}})$ in Eq. (9) Sample an input $\\pmb{x}\\sim\\mathcal{D}_{\\pmb{x}}$ and generate an trajectory $\\tau^{\\mathrm{on}}\\sim\\pi_{\\theta}|\\pmb{x}$ $\\phi_{2}\\gets\\phi_{2}+\\alpha(1-\\beta)\\eta\\nabla_{\\phi_{2}}\\hat{\\mathcal{L}}^{\\mathrm{on}}(\\tau^{\\mathrm{on}},\\theta_{k},f_{\\phi_{2}})$ $\\vartriangleright$ maximize $\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta_{k}},f_{\\phi_{2}})$ in Eq. (9) end Sample an input $\\pmb{x}_{k}\\sim\\mathcal{D}_{\\pmb{x}}$ and generate trajectories $\\tau_{k}^{\\mathrm{off}}\\sim\\pi_{*}|\\pmb{x}_{k}$ and $\\tau_{k}^{\\mathrm{on}}\\sim\\pi_{\\theta}|\\pmb{x}_{k}$ \uff080\uff09 ", "page_idx": 5}, {"type": "text", "text": "3.3 Adversarial Training Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Optimization objective. As shown in previous work [14, 2, 21] incorporating both the off-policy and on-policy distillation beneftis effectiveness and efficiency. We thus consider a training objective to jointly minimize the off-policy moment-matching objective in Proposition 1 and the on-policy moment-matching objective in Proposition 2. Both the off-/on-policy objectives can be optimized by viewing the learning procedure as solving a game. More specifically, we consider a two-player minimax game between the student policy and the $Q$ -value functions. To this end, we initialize two small networks of a single-layer MLP to estimate the off-/on-policy $Q$ -value functions, respectively. For example in a causal/seq-to-seq LM, the $Q$ -value estimate module can be represented as $f_{\\phi_{1(2)}}(\\pmb{y}_{\\leq t},y)=(\\mathbf{h}_{t}^{\\pi_{\\theta}}+\\mathbf{v}_{y}^{\\mathrm{off(on)}})^{\\top}\\mathbf{w}_{y}^{\\mathrm{off(on)}}$ wyoff(on)for any action token y \u2208V. This estimates the Q-value function by taking the current $t\\in\\{0,1,\\ldots,T-1\\}$ hidden step of a policy network $\\mathbf{h}_{t}^{\\pi_{\\theta}}\\in\\mathbb{R}^{H}$ (for next token prediction) to combine with the feature vector of the token $\\mathbf{v}_{y}^{\\mathrm{off(on)}}\\in\\mathbb{R}^{H}$ with a linear transformation by $\\mathbf{w}_{y}^{\\mathrm{off(on)}}\\in\\mathbb{R}^{H}$ for off(on)-policy learning. Here, $H$ represents the hidden size and the additional parameter cost is $\\mathcal{O}(H|\\mathcal{V}|)$ for $Q$ -value estimation. Finally, combining off- and on-policy objectives with a factor $\\beta\\in(0,1)$ , the optimization problem can be represented as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\phi_{1},\\phi_{2}\\in\\Phi}\\underbrace{\\beta\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f_{\\phi_{1}})+(1-\\beta)\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f_{\\phi_{2}})}_{=:\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})$ represents the overall training objective. To minimize the objective w.r.t the policy parameters $\\theta$ , we use a policy gradient approach and derive the policy gradient in Appendix A.5, formally represented as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})=\\underset{x\\sim p_{\\alpha}}{\\mathbb{E}}~\\underset{(\\tau,\\tau^{\\prime})\\sim\\pi_{*}|x\\cdot\\pi_{\\theta}|x}{\\mathbb{E}}\\bigg[-\\beta\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)+(1-\\beta)\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)\\bigg]}\\\\ &{\\mathrm{s.t.}\\quad\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)=\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\underset{y\\sim\\pi_{\\theta}(\\cdot|y_{\\le t})}{\\mathbb{E}}\\left[\\nabla\\log\\pi_{\\theta}(y|y_{\\le t})f_{\\phi_{1}}(y_{\\le t},y)\\right];}\\\\ &{\\qquad\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)=\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\nabla\\log\\pi_{\\theta}(y_{t+1}^{\\prime}|y_{\\le t}^{\\prime})\\hat{Q}_{f_{\\phi_{2}}}(y_{\\le t}^{\\prime},y_{t+1}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{Q}_{f_{\\phi_{2}}}:\\mathcal{y}\\times\\mathcal{V}\\to\\mathbb{R}$ denotes the empirical $Q$ -value defined in Eq. (21). Besides, we use stochastic gradient ascent (SGA) to maximize the objective of $\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})$ w.r.t. parameters of the on-policy $Q$ -value function $\\phi_{1}$ and parameters of the off-policy $Q$ -value function $\\phi_{2}$ . ", "page_idx": 5}, {"type": "text", "text": "Training procedure. The goal is to achieve an equilibrium between minimizing the objective w.r.t. the parameters of student policy $\\theta\\in\\Theta$ and maximizing the objective w.r.t. the parameters of on-policy and off-policy $Q$ -value functions $\\phi_{1},\\phi_{2}\\,\\in\\,\\Phi$ , formally defined as $\\mathrm{min}_{\\theta}\\,\\mathrm{max}_{\\phi_{1},\\phi_{2}}\\,\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})$ ", "page_idx": 5}, {"type": "text", "text": "(Eq. (9)). To this end, we use an adversarial training strategy in Algorithm 1, by starting from a student model fine-tuned on a dataset $\\mathcal{D}_{x y}$ . In the training algorithm, we iteratively maximize the objective w.r.t. the parameters of $Q$ -value functions $f_{\\phi_{1}},f_{\\phi_{2}}$ and simultaneously minimize the objective w.r.t. the parameters of student policy $\\pi_{\\theta}$ . In each iteration of policy updating, we first perform $N$ steps of stochastic gradient ascent (SGA) w.r.t. the parameters of $Q$ -value functions $\\phi_{1},\\phi_{2}$ . Then, the parameters of student policy $\\theta$ are updated by stochastic gradient descent (SGD) with the estimated policy gradient with sampling policy gradients. ", "page_idx": 6}, {"type": "text", "text": "3.4 Convergence Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further provide a convergence analysis for the algorithm proposed in $\\S3.3$ . To deal with the challenges of non-convexity by certain reward structures, the algorithm is expected to obtain an $\\epsilon$ -accurate stationary point of the policy parameters $\\theta_{*}\\in\\Theta$ , satisfying that $\\mathbb{E}[\\|\\bar{\\nabla}\\mathcal{L}(\\theta_{*})\\|^{2}]\\le\\epsilon.$ . We focus on policy optimization and directly use the optimized off-/on-policy $Q$ -value functions $f_{*}^{\\mathrm{off}}=$ arg maxf $\\mathcal{L}^{\\mathrm{off}}(\\theta_{*},f)$ and $f_{*}^{\\mathrm{on}}=\\underset{*}{\\arg\\operatorname*{max}}_{f}\\,\\mathcal{L}_{\\mathrm{~\\tiny~\\textnormal~{\\bf~x}~}}^{\\mathrm{on}}(\\theta_{*},f)$ , respectively. We use $\\mathcal{L}(\\theta):=\\mathcal{L}(\\theta,f_{*}^{\\mathrm{off}},f_{*}^{\\mathrm{on}})$ (def. in Eq. (9)) for simplicity in this section. We start with the following standard assumption [44]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. Suppose that the optimized $Q$ -value functions and the parameterized policy $\\pi_{\\theta}$ satisfy the following conditions: ", "page_idx": 6}, {"type": "text", "text": "(i) The uniformly boundness of off/on-policy $Q$ -value functions optimized by Algorithm $^{\\,l}$ , i.e., $\\|f_{*}^{\\mathrm{off}}\\|_{\\infty},\\|f_{*}^{\\mathrm{on}}\\|_{\\infty}\\leq1$ .   \n(ii) The $B$ -Lipschitzness and the $L$ -smoothness of the parameterized policy, i.e., for any stateaction pair $(\\boldsymbol{y}_{\\le t},\\boldsymbol{y}_{t+1})\\in\\mathcal{Y}\\times\\mathcal{V}$ at any time step $t\\in\\{0,1,\\ldots,T-1\\}$ , $\\|\\nabla\\log\\pi_{\\theta}(y_{t+1}|\\pmb{y}_{\\le t})\\|\\le B$ , for any $\\theta\\in\\Theta$ , (11) $\\begin{array}{r}{\\|\\nabla\\log\\pi_{\\theta_{1}}(y_{t+1}|y_{\\le t})-\\nabla\\log\\pi_{\\theta_{2}}(y_{t+1}|y_{\\le t})\\|\\le L\\|\\theta_{1}-\\theta_{2}\\|,}\\end{array}$ for any $\\theta_{1},\\theta_{2}\\in\\Theta$ (12) ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Convergence rate of Algorithm 1 to stationary points). Let $\\{\\theta_{k}\\}_{1\\le k\\le K}$ be the sequence of parameters of the policy $\\pi_{\\theta_{k}}$ given by Algorithm 1. Let the learning rate $\\begin{array}{r}{\\eta=\\frac{2}{3B}\\sqrt{\\frac{1-\\gamma}{3K}}}\\end{array}$ . Under Assumption $^{\\,l}$ and the condition $\\begin{array}{r}{\\gamma\\le\\operatorname*{min}\\{1-\\frac{\\eta L}{2},2-\\frac{1}{\\beta}\\}.}\\end{array}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\|\\nabla{\\mathcal{L}}(\\theta_{k})\\|^{2}\\right]\\leq{\\mathcal{O}}\\left({\\frac{1}{\\sqrt{K}}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix A.6 for the complete derivation. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 illustrates that the output gradien\u221at norm square by Algorithm 1 can converge to a neighborhood around zero with the rate of $1/\\sqrt{K}$ . Furthermore, leveraging a sufficient number of training iterations $\\mathcal{O}(\\epsilon^{-2})$ , Algorithm 1 can obtain an $\\epsilon$ -accurate stationary point. This leads to the following corollary on the computational complexity of the training procedure. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 (Computational complexity of Algorithm 1). We formalize the policy as a softmax function $\\pi_{\\theta}$ with a linear transformation: softmax $(\\theta\\pmb{y}_{\\le t})$ for any $\\pmb{y}_{\\leq t}\\in\\mathbb{R}^{H}$ , where $\\theta\\in\\mathbb{R}^{\\lceil\\nu\\vert\\times H}$ and $H$ denotes the hidden size. Then, to obtain an \u03f5-accurate stationary point by Algorithm $^{\\,l}$ , the complexity of gradient computation is $\\mathcal{O}(\\epsilon^{-2}T|\\mathcal{V}|H(N+T+|\\mathcal{V}|))$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix A.7 for the complete derivation. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 shows that Algorithm 1 has a polynomial computational complexity w.r.t $\\epsilon^{-2}$ , $N,|\\mathcal{V}|,$ $H$ and $T$ , to obtain an $\\epsilon_{}$ -accurate stationary point for optimizing the training objective in Eq. (9). ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider task-agnostic instruction-following experiments and task-specific experiments, including text summarization, machine translation, and commonsense reasoning. We compare our approach with various KD baselines, including: SFT, which fine-tunes the student model on the supervised dataset $\\mathcal{D}_{x y}$ ; KD [16], which uses KL divergence on the supervised dataset $\\mathcal{D}_{x y}$ ; SeqKD [20], which applies SFT to the student model with teacher-generated outputs; ImitKD [24], which uses KL divergence on the student-generated outputs; MiniLLM [14], which uses RKL divergence with a policy gradient method; GKD [2], which uses JS divergence with an on-policy method; and DistiLLM [21], which uses an adaptive training method for off-policy optimization of a skew KL divergence. Additionally, we focus on step-wise distance optimization for KD and compare it with a range of well-known methods, including KL divergence, RKL divergence, JS divergence, and TV distance, as discussed by Wen et al. [38]. All the reported results are the average across three random seeds. ", "page_idx": 6}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/c97d7c79c2f85b1f4361b939f2cab77b383604a780c8824e5d80bf4e31c2678a.jpg", "table_caption": ["Table 1: Comparison with state-of-the-art KD methods on the instruction-following dataset using fine-tuned OpenLLaMA-7B as the teacher and fine-tuned OpenLLaMA-3B as the student. We format the best, the second best and worse than SFT results. The results based on GPT-2 are available in Appendix C.1. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1 Task-Agnostic Distillation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental Setup. We follow the previous works [14, 21] for the implementation of the instructionfollowing experiment, aiming to evaluate the distilled model\u2019s ability to handle diverse tasks presented in the form of instructions. We construct the training data from databricks-dolly-15k [8], where we randomly select 15K samples for training and equally split 500 samples for validation and testing. We evaluate the trained model on five instruction-following datasets: DollyEval, SelfInst [36], VicunaEval [6], S-NI [37], and UnNI [17]. Following the previous works [14, 21], we also add the OpenWebText [13] corpus, consisting of long-document plain text, for joint training with a language modeling task. This has been shown to effectively improve the performance of instruction tuning [14]. The evaluation metrics include ROUGE-L [25] and GPT-4 feedback with the same prompts as in [21]. More details on experimental setup refer to Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Main results. Table 1 illustrates the instruction-following performances. Compared with the SFT baseline, which indicates the student model without KD, KD and SeqKD hardly improve the performances. This indicates that using only supervised datasets or teacher-generated outputs does not benefit the KD of large language models. In contrast, utilizing the student-generated outputs with KL divergence [2], RKL divergence [14], and JS divergence [2] shows effectiveness for KD in the instruction-following task. State-of-the-art methods [14, 2, 21] tend to combine the studentgenerated outputs with the teacher-generated output or supervised dataset to further improve the results of KD. This shows that a mixture optimization of both on-policy and off-policy objectives can effectively improve the KD performance of large language models on the instruction-following task. In particular, we use an adversarial moment-matching method and optimize both on-policy and off-policy objectives for KD, thus achieving the best results on five test datasets with both GPT-4 feedback and ROUGE-L evaluations. ", "page_idx": 7}, {"type": "text", "text": "4.2 Task-Specific Distillation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental Setup. We evaluated the KD models on three tasks consisting of text summarization, machine translation, and reasoning. For the text summarization task, we follow Ko et al. [21] to conduct experiments on the SAMSum [12] dataset. For the machine translation tasks, we follow Ko et al. [21] to conduct experiments on the IWSLT\u201917 (en-de) [5] dataset. For the commonsense reasoning task, we conduct experiments on the StrategyQA dataset [11]. For all of the task-specific experiments, we use T5-XL [29] as the teacher model and T5-Large/-Base/-Small as the student model. For the machine translation experiments, we employ a multilingual pretrained model, mT5 [42], to build the methods. For evaluation, we use ROUGE-L [25], BLEU [27], and accuracy as the performance metrics on SAMSum, IWSLT\u201917 (en-de), and StrategyQA, respectively. More details about the experimental setup refer to Appendix B. ", "page_idx": 7}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/fbd0aa790fab02710858f970af9d36b8967571774c396055181ad29026b88171.jpg", "table_caption": ["Table 2: Comparison with the state-of-the-art KD methods on text summarization, machine translation and commonsense reasoning datasets. We report the ROUGE-L, BLEU and accuracy for SAMSum, IWSLT\u201917 (en-de) and StrategyQA, respectively. We format the best, the second best and worse than SFT results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Main results. Table 2 displays the performances on three task-specific datasets. Since the original work of MiniLLM [14] does not consider these tasks, we thus do not make comparisons with MiniLLM. The performance trend is similar to the instruct-following results, revealing that KD of large language models for specific tasks also benefits from the combination of on-policy objectives with student-generated outputs and off-policy objectives with teacher-generated outputs or supervised datasets. Additionally, we observe that student models of different sizes all benefit from the KD methods to improve performance. Overall, our approach achieves the best results on all three task-specific datasets for student models of different sizes. This demonstrates the effectiveness of an adversarial moment-matching approach for KD of large language models on specific tasks. ", "page_idx": 8}, {"type": "text", "text": "4.3 Analysis on Step-Wise Distance Optimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comparison with distribution matching. We make comparisons with different step-wise distribution distances with a uniform formulation of Definition 2, considering the on-policy, offpolicy objectives as well as the joint form. Results on four tasks with a default combination factor $\\beta~=~0.5$ are shown in Figure 2. More instruct-following results are available in Appendix C.2 and results with different values of off-/on-policy combination factor are available in Appendix C.5. Compared with the KL divergence, RKL divergence, JS divergence and total variation ", "page_idx": 8}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/d652fa943fa1cbf864724036e6bd8657015e997c694811e9ab87db75526d220b.jpg", "img_caption": ["Figure 2: Performance of difference step-wise distribution distances. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "distance, the proposed moment-matching distance achieves the best results under both the on-policy and off-policy training objectives, which shows that the proposed moment-matching approach is effective for KD of large language models. Besides, we observe that using a joint objective of both on-policy and off-policy can further significantly improve the performances. This shows that both on-policy and off-policy moment-matching objectives contribute to the minimization of the imitation gap and can thus benefit the KD of large language models. ", "page_idx": 8}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/7564bbd213b23fc9fd0d5af23b66e3e6d7e07f36c98fef69e592aae950c2ee70.jpg", "img_caption": ["Figure 3: Adversarial training procedure for optimizing the on-policy and off-policy momentmatching distances $d_{\\mathrm{MM}}^{\\mathrm{on}}$ , $d_{\\mathrm{MM}}^{\\mathrm{off}}$ on the instruction-following dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Adversarial training procedure. We present the training loss and moment-matching distance against the adversarial training steps. As depicted in Figure 3 (a), the training loss initially increases within the first 0-1,000 steps, indicating that initially, the $Q$ -value functions are stronger than the policy in maximizing the loss function $\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})$ in Eq. (9). Concurrently, the policy gradient method contributes to minimizing the training loss, which eventually converges to a much lower stable value. Additionally, both the on-policy and off-policy moment-matching distances $d_{\\mathrm{MM}}^{\\mathrm{on}}$ and $d_{\\mathrm{MM}}^{\\mathrm{off}}$ decrease and eventually reach a low value with only minor fluctuations. For more results and details on experimental setups, please refer to Appendix C.3. ", "page_idx": 9}, {"type": "text", "text": "Moment-matching distance optimization. We further illustrate the on-policy moment-matching distance $d_{\\mathrm{MM}}^{\\mathrm{on}}$ and the off-policy moment-matching distance $d_{\\mathrm{MM}}^{\\mathrm{off}}$ (defined in Definition 3) optimized by different step-wise distances in Figure 3 (b) and (c), respectively. Interestingly, we observe that the total variation (TV) distance obtains the second-best results on average for both on-policy and off-policy distances. This finding suggests a similarity between the formulations of TV distance and moment-matching distances to some extent, as supported by the theoretical result of Theorem 1. Across all instruction-following test sets, our approach effectively optimizes both on-policy and off-policy moment-matching distances more than other step-wise distribution distances used in KD, including KL divergence, RKL divergence, JS divergence, and TV distance. This observation also underscores the effectiveness of our policy gradient methods. Extensive results on the task-specific datasets are available in Appendix C.4. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigated a moment-matching approach for knowledge distillation of large language models. Specifically, we formulated knowledge distillation from a perspective of imitation learning and derived both on-policy and off-policy bounds for the imitation gap between the teacher model and student model via moment-matching distance. Additionally, we proposed an adversarial training algorithm to simultaneously estimate and minimize the joint objective of on-policy and off-policy moment-matching distances. In experiments, we evaluated the proposed algorithm on four instruction-following datasets and three task-specific datasets, comparing it with a range of state-of-the-art KD methods as well as four well-studied step-wise distribution distances for KD of auto-regressive models. Results demonstrate that our approach can effectively leverage the policy gradient method to optimize the moment-matching distance and achieve the best results across all datasets. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. The proposed adversarial training algorithm requires additional computational steps for the inner-loop gradient ascent, which may result in increased time complexity. Moreover, the proposed approach necessitates auxiliary networks to build the $Q$ -value functions, which may incur additional memory costs. Besides, the experiments are conducted with limited LLM architectures, such as OpenLLaMA and T5. Therefore, in future work, we aim to enhance the time and memory efficiency of our approach, and evaluate the proposed approach on a wider range of architectures. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We thank the anonymous reviewers for their helpful comments and suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024.   \n[3] James Bagnell, Sham M Kakade, Jeff Schneider, and Andrew Ng. Policy search by dynamic programming. Advances in Neural Information Processing Systems, 16, 2003.   \n[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \n[5] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00fcker, Katsuitho Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the iwslt 2017 evaluation campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation, pages 2\u201314, 2017.   \n[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023.   \n[7] Kamil Ciosek. Imitation learning by reinforcement learning. In International Conference on Learning Representations, 2021.   \n[8] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023.   \n[9] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.   \n[10] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https://github. com/openlm-research/open_llama, 2023.   \n[11] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013361, 2021.   \n[12] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, 2019.   \n[13] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.   \n[14] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[15] Yongchang Hao, Yuxin Liu, and Lili Mou. Teacher forcing recovers reward functions for text generation. Advances in Neural Information Processing Systems, 35:12594\u201312607, 2022.   \n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409\u201314428, 2023.   \n[18] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267\u2013 274, 2002.   \n[19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[20] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2016.   \n[21] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined distillation for large language models. In Forty-first International Conference on Machine Learning, 2024.   \n[22] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[23] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning, pages 20852\u201320867. PMLR, 2023.   \n[24] Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei. Autoregressive knowledge distillation through imitation learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6121\u20136133, 2020.   \n[25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out, pages 74\u201381, 2004.   \n[26] Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations. In 9th International Conference on Learning Representations, 2021.   \n[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.   \n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):1\u201324, 2019.   \n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[30] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[31] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, and Gert RG Lanckriet. On integral probability metrics,\\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.   \n[32] Gokul Swamy, Sanjiban Choudhury, J Bagnell, and Steven Z Wu. Sequence model imitation learning with unobserved contexts. Advances in Neural Information Processing Systems, 35:17665\u201317676, 2022.   \n[33] Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching: A game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning, pages 10022\u201310032. PMLR, 2021.   \n[34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[36] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484\u201313508, 2023.   \n[37] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on $1600+$ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.   \n[38] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10817\u201310834, 2023.   \n[39] Qingyang Wu, Lei Li, and Zhou Yu. Textgail: Generative adversarial imitation learning for text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14067\u201314075, 2021.   \n[40] Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. Rethinking kullbackleibler divergence in knowledge distillation for large language models. arXiv preprint arXiv:2404.02657, 2024.   \n[41] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024.   \n[42] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, 2021.   \n[43] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[44] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586\u20133612, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Similar to the proof of Performance Difference Lemma (PDL) [18, 3, 33], we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\underset{-\\frac{\\mathbb{E}_{\\theta}}{\\sum_{k=1}^{\\mathbb{E}_{\\theta}}}}{\\sum_{k=1}^{\\mathbb{E}_{\\theta}}}\\left[\\sum_{\\ell=0}^{\\infty}f(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})\\right]-\\underset{x\\in\\mathcal{I}_{\\mathbb{R}_{n}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}|V^{n,\\theta}(x)|}\\\\ &{=\\underset{-\\frac{\\mathbb{E}_{\\theta}}{\\sum_{k=1}^{\\mathbb{E}_{\\theta}}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}\\left[\\sum_{\\ell=0}^{\\infty}(r(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})+V^{n,\\theta}(y_{\\mathbb{S}^{k}})-V^{n,\\theta}(y_{\\mathbb{S}^{k}}))\\right]-\\underset{x\\in\\mathcal{I}_{\\mathbb{R}_{n}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}|V^{n,\\theta}(x)|}\\\\ &{=\\underset{-\\frac{\\mathbb{E}_{\\theta}}{\\sum_{k=1}^{\\mathbb{E}_{\\theta}}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}\\left[\\sum_{\\ell=0}^{\\infty}(r(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})+\\gamma V^{n}(y_{\\mathbb{S}^{k}+1})-V^{n,\\theta}(y_{\\mathbb{S}^{k}}))\\right]}\\\\ &{=\\underset{-\\frac{\\mathbb{E}_{\\theta}}{\\sum_{k=1}^{\\mathbb{E}_{\\theta}}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}\\left[\\sum_{\\ell=0}^{\\infty}(r(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})+\\gamma V^{n}(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})-\\Gamma^{n,\\theta}(y_{\\mathbb{S}^{k}+1})\\right]-V^{n,\\theta}(y_{\\mathbb{S}^{k}})\\right]\\Bigg]}\\\\ &{\\overset{(a)}{\\underset{x\\in\\mathcal{I}_{\\mathbb{R}_{n}}}{\\sum_{\\ell=1}^{\\mathbb{E}_{\\theta}}}}\\left[\\sum_{\\ell=0}^{\\infty}(r(y_{\\mathbb{S}^{k}}\\cdot y_{k+1})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(i)$ follows from Bellman equation and noting that the transition probability $T(\\cdot\\vert y_{\\leq t},y_{t+1})$ is deterministic in an auto-regressive text generation problem. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Similar to the proof of Proposition 1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\mathbf{r}_{S})=J(\\mathbf{r}_{S})}\\\\ &{=-\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{s}(\\mathbf{r}_{S};\\mathbf{r}_{S};\\mathbf{b}+)\\right]+\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\mathbf{b}^{\\prime}(\\mathbf{r}_{S})\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{s}\\left(\\mathbf{r}^{\\prime}(\\mathbf{\\bar{r}}_{S};\\mathbf{r}_{S})-(\\gamma(\\mathbf{g}_{S};\\mathbf{r}_{S})+)^{s}\\mathbf{F}^{\\prime\\prime}(\\mathbf{g}_{S})\\right)\\right]+\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\mathbf{b}^{\\prime\\prime}(\\mathbf{r})\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{s}\\left(\\mathbf{r}^{\\prime}(\\mathbf{\\bar{r}}_{S};\\mathbf{\\bar{r}}_{S})-(\\gamma(\\mathbf{g}_{S};\\mathbf{r}_{S})+)^{s}\\mathbf{F}^{\\prime\\prime}(\\mathbf{g}_{S};\\mathbf{r}_{S})\\right)\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{s}\\left(\\mathbf{r}^{\\prime}(\\mathbf{\\bar{r}}_{S};\\mathbf{\\bar{r}}_{S})-\\left((\\gamma(\\mathbf{g}_{S};\\mathbf{r}_{S})+)^{s}\\mathbf{\\bar{F}}_{\\mathbf{R}_{n}+\\mathbf{r}_{S}}-\\gamma(\\mathbf{\\bar{r}}_{S};\\mathbf{r}_{S})\\right)\\right)\\right]}\\\\ &{=\\underbrace{\\mathbb{E}_{\\mathbf{R}_{n}}}_{\\geq0}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\gamma^{s}\\left(\\mathbf{r}^{\\prime}(\\mathbf{\\bar{r}}_{S};\\mathbf{\\bar{r}}_{S})-\\left((\\gamma(\\mathbf{g}_{S};\\mathbf{r}_{S})+)^{s}\\mathbf{\\bar{F}}_{\\mathbf{R}_{\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.3 Existing Step-Wise Distribution Distance for KD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 4 (Step-wise distribution distances for distillation [38]). Following Wen et al. [38], we define four groups of well-studied probability distribution distances as follows, ", "page_idx": 14}, {"type": "text", "text": "\u2022 Total variation (TV) distance. The token-level TV distance can be defined as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\pi_{\\theta}(\\cdot|{\\boldsymbol y}_{\\le t}),\\pi_{*}(\\cdot|{\\boldsymbol y}_{\\le t})):=\\sum_{y\\in\\mathcal{V}}\\left|\\pi_{*}(y|{\\boldsymbol y}_{\\le t})-\\pi_{\\theta}(y|{\\boldsymbol y}_{\\le t})\\right|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Kullback\u2013Leibler (KL) divergence. The token-level $K L$ divergence between the teacher policy $\\pi_{*}$ and the student policy $\\pi_{\\theta}$ can be defined as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\pi_{\\theta}(\\cdot|y_{\\leq t}),\\pi_{*}(\\cdot|y_{\\leq t})):=\\sum_{y\\in\\mathcal{V}}\\pi_{*}(y|y_{\\leq t})\\log\\frac{\\pi_{*}(y|y_{\\leq t})}{\\pi_{\\theta}(y|y_{\\leq t})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Reverse Kullback\u2013Leibler (RKL) divergence. The token-level RKL divergence between the teacher policy $\\pi_{*}$ and the student policy $\\pi_{\\theta}$ can be defined as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{RKL}(\\pi_{\\theta}(\\cdot|y_{\\leq t}),\\pi_{*}(\\cdot|y_{\\leq t})):=\\sum_{y\\in\\mathcal{V}}\\pi_{\\theta}(y|y_{\\leq t})\\log\\frac{\\pi_{\\theta}(y|y_{\\leq t})}{\\pi_{*}(y|y_{\\leq t})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Jenson\u2013Shannon (JS) divergence. The token-level $J S$ divergence between the teacher policy $\\pi_{*}$ and the student policy $\\pi_{\\theta}$ can be defined based on the $K L$ divergence and RKL divergence as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{JS}\\big(\\pi_{\\theta}(\\cdot|y_{\\leq t}),\\pi_{*}(\\cdot|y_{\\leq t})\\big):=\\frac{1}{2}\\mathrm{KL}\\big(\\pi_{*},\\frac{\\pi_{\\theta}+\\pi_{*}}{2}\\big)+\\frac{1}{2}\\mathrm{RKL}\\big(\\pi_{\\theta},\\frac{\\pi_{\\theta}+\\pi_{*}}{2}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We first derive an upper bound for the on-policy moment-matching objective of Eq. (3). Set $\\mathcal{F}_{Q_{*}}=\\{f:\\|f\\|_{\\infty}\\leq1\\}$ , and by the definition of $\\mathcal{L}(\\pi_{\\theta},f)$ in Eq. (3), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f:\\|f\\|\\infty}{\\operatorname*{sup}}\\le1^{\\mathcal{L}^{\\mathrm{on}}}(\\pi_{\\theta},f)}\\\\ &{=\\underset{f:\\|f\\|\\infty\\le1}{\\operatorname*{sup}}\\underset{\\tau\\sim\\pi_{\\theta}\\vert\\pi}{\\overset{\\mathbb{E}}{\\sum}}\\left[\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\left(\\underset{y\\sim\\pi_{*}(\\cdot\\vert y_{\\le t})}{\\mathbb{E}}[f(y_{\\le t},y)]-f(y_{\\le t},y)\\right)\\right]}\\\\ &{=\\underset{f:\\|f\\|\\infty\\le1}{\\operatorname*{sup}}\\underset{\\tau\\sim\\pi_{\\theta}\\vert\\pi}{\\overset{\\mathbb{E}}{\\sum}}\\left[\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\left(\\underset{y\\sim\\pi_{*}(\\cdot\\vert y_{\\le t})}{\\mathbb{E}}[f(y_{\\le t},y)]-\\underset{y\\sim\\pi_{\\theta}(\\cdot\\vert y_{\\le t})}{\\mathbb{E}}\\left[f(y_{\\le t},y)\\right]\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f:\\|f\\|_{\\infty}\\leq1}{\\operatorname*{sup}}\\mathcal L^{\\mathrm{on}}(\\pi_{\\theta},f)}\\\\ &{\\overset{(i)}{\\leq}\\underset{\\underset{\\tau\\sim\\pi_{\\theta}\\mid n}{\\mathbb{K}}}{\\underbrace{\\mathbb{K}}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\underset{f:\\|f\\|_{\\infty}\\leq1}{\\operatorname*{sup}}\\left(\\underset{y\\sim\\pi_{*}(\\cdot\\vert y_{\\leq t})}{\\mathbb{K}}\\Big[f(y_{\\leq t},y)\\Big]-\\underset{y\\sim\\pi_{\\theta}(\\cdot\\vert y_{\\leq t})}{\\mathbb{K}}\\big[f(y_{\\leq t},y)\\big]\\right)\\right]}\\\\ &{\\overset{(i i)}{=}\\underset{\\underset{\\tau\\sim\\pi_{\\theta}\\mid n}{\\mathbb{K}}}{\\underbrace{\\mathbb{K}}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\sum_{y\\in\\mathcal V}\\left\\vert\\pi_{*}(y\\vert y_{\\leq t})-\\pi_{\\theta}(y\\vert y_{\\leq t})\\right\\vert\\right]\\overset{(i i i)}{=}d_{\\mathrm{TV}}^{\\mathrm{on}}(\\pi_{\\theta},\\pi_{*})\\ (\\mathrm{Def.~}4),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(i)$ follows from Jensen\u2019s inequality, $(i i)$ follows from [31] and $(i i i)$ follows from the definition of TV distance. ", "page_idx": 14}, {"type": "text", "text": "Similarly, we can bound the off-policy version of Eq. (2) as follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\substack{f:\\|f\\|_{\\infty}\\leq1\\,\\,\\frac{\\kappa}{\\tau\\sim\\pi_{*}|\\pi}}}\\,\\left[\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f)\\right]\\leq\\operatorname*{l}_{\\substack{\\tau\\sim\\pi_{*}|\\pi}}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\sum_{y\\in\\mathcal{V}}\\left|\\pi_{*}(y|y_{\\leq t})-\\pi_{\\theta}(y|y_{\\leq t})\\right|\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=d_{\\mathrm{TV}}^{\\mathrm{off}}(\\pi_{\\theta},\\pi_{*})\\ \\mathrm{(Def.~}4),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof of Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "A.5 Derivation of Policy Gradient in Eq. (10) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on the definition of training objective in Eq. (9), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})=\\beta\\nabla\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f_{\\phi_{1}})+(1-\\beta)\\nabla\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f_{\\phi_{2}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on the definition of $\\mathcal{L}^{\\mathrm{off}}(\\pi_{\\theta},f_{\\phi_{1}})$ in Eq. (2), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{L}^{\\mathrm{eqf}}(\\pi_{\\theta},f_{\\phi_{l}})}\\\\ &{=\\nabla\\underbrace{\\frac{\\mathrm{R}}{\\kappa_{\\mathrm{srest}}}}_{\\substack{\\mathrm{srest}_{*}}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\left(f(y_{\\le t},y_{t+1})-\\displaystyle\\sum_{y>\\pi_{\\theta}(\\cdot|y_{\\le t})}\\mathbb{E}_{f}(y_{\\le t},y)\\right)\\right]}\\\\ &{=-\\underbrace{\\frac{\\mathrm{R}}{\\mathrm{Re}}}_{\\substack{\\mathrm{srest}_{*}}}\\sum_{\\substack{\\mathrm{tr}=0}}^{T-1}\\gamma^{t}\\nabla_{y\\le t}\\mathbb{E}_{\\ge\\varepsilon}\\left[f_{\\phi_{1}}(y_{\\le t},y)\\right]}\\\\ &{=-\\underbrace{\\frac{\\mathrm{R}}{\\mathrm{Re}}}_{\\substack{\\mathrm{srest}_{*}}}\\sum_{\\substack{\\mathrm{tr}=0}}^{T-1}\\gamma^{t}\\sum_{\\substack{y\\in\\mathcal{C}}}\\pi_{\\theta}(y|y_{\\le t})\\nabla\\log\\pi_{\\theta}(y|y_{\\le t})f_{\\phi_{1}}(y_{\\le t},y)}\\\\ &{=-\\underbrace{\\frac{\\mathrm{R}}{\\mathrm{Re}}}_{\\substack{\\mathrm{srest}_{*}}}\\sum_{\\substack{\\mathrm{tr}=0}}^{T-1}\\gamma^{t}\\underbrace{\\mathbb{E}}_{\\psi\\in\\mathcal{V}}(\\nabla\\log\\pi_{\\theta}(y|y_{\\le t})f_{\\phi_{1}}(y_{\\le t},y))}\\\\ &{=-\\underbrace{\\frac{\\mathrm{R}}{\\mathrm{Re}}}_{\\substack{\\mathrm{srest}_{*}}}\\sum_{\\substack{\\mathrm{tr}=0}}^{T}\\gamma^{t}\\underbrace{\\mathbb{E}}_{\\psi\\circ\\pi_{\\theta}(\\cdot|y_{\\le t})}\\left[\\nabla\\log\\pi_{\\theta}(y|y_{\\le t})f_{\\phi_{1}}(y_{\\le t},y)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, based on the definition of $\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f_{\\phi_{2}})$ in Eq. (3), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{L}^{\\mathrm{on}}(\\pi_{\\theta},f_{\\phi_{2}})}\\\\ &{=\\!\\!\\nabla_{\\frac{n}{\\tau\\sim\\eta_{\\theta}|\\alpha}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\left({\\scriptstyle\\underbrace{\\mathbb{E}}_{|y\\le t}}_{\\tau\\sim\\pi_{*}(\\cdot\\vert y_{\\le t})}\\left[f_{\\phi_{2}}(y_{\\le t},y)\\right]-f_{\\phi_{2}}(y_{\\le t},y_{t+1})\\right)\\right]}\\\\ &{\\overset{\\quad\\mathrm{U}}{=}\\!\\!\\underbrace{\\mathbb{E}}_{\\frac{n}{\\tau\\sim\\eta_{\\theta}|\\alpha}}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}\\nabla\\log\\pi_{\\theta}(y_{t}|y_{\\le t})\\displaystyle\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}\\left({\\scriptstyle\\underbrace{\\mathbb{E}}_{|y\\le t^{\\prime}}}_{y\\sim\\pi_{*}(\\cdot\\vert y_{\\le t^{\\prime}})}\\left[f_{\\phi_{2}}(y_{\\le t^{\\prime}},y)\\right]-f_{\\phi_{2}}(y_{\\le t^{\\prime}},y_{t^{\\prime}+1})\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(i)$ follows from a standard derivation of gradient policy (c.f. [22]). Set ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{Q}_{f_{\\phi_{2}}}(y_{\\leq t},y_{t+1})=\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}\\left(\\underset{y\\sim\\pi_{*}(\\cdot\\vert y_{\\leq t^{\\prime}})}{\\mathbb{E}}\\left[f_{\\phi_{2}}(y_{\\leq t^{\\prime}},y)\\right]-f_{\\phi_{2}}(y_{\\leq t^{\\prime}},y_{t^{\\prime}+1})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any draw of trajectory $\\tau\\sim\\pi_{\\theta}|\\pmb{y}_{0}=\\pmb{x},\\pmb{x}\\sim p_{\\pmb{x}}$ in Eq. (20). ", "page_idx": 15}, {"type": "text", "text": "Combining Eq. (18) with Eq. (19) and Eq. (20), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})=-\\,\\beta\\underset{\\tau\\sim\\pi_{\\phi_{*}}\\mid x}{\\mathbb{E}}\\left[\\overset{T-1}{\\underset{t=0}{\\sum}}\\gamma^{t}\\underset{y\\sim\\pi_{\\theta}(\\cdot\\mid y_{\\le t})}{\\mathbb{E}}\\left[\\nabla\\log\\pi_{\\theta}(y\\vert y_{\\le t})f_{\\phi_{1}}(y_{\\le t},y)\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,(1-\\beta)\\underset{\\tau\\sim\\pi_{\\theta}\\mid x}{\\mathbb{E}}\\left[\\overset{T-1}{\\underset{t=0}{\\sum}}\\gamma^{t}\\nabla\\log\\pi_{\\theta}(y_{t+1}\\vert y_{\\le t})\\hat{Q}_{f_{\\phi_{2}}}(y_{\\le t},y_{t+1})\\right]\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Supposing that $\\tau\\sim\\pi_{*}|x$ and $\\tau^{\\prime}\\sim\\pi_{\\theta}|\\pmb{x}$ are independent, then by rearranging the terms, we obtain the final formulation of policy gradient, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla\\mathcal{L}(\\pi_{\\theta},f_{\\phi_{1}},f_{\\phi_{2}})=\\underset{x\\sim p_{\\alpha}}{\\mathbb{E}}\\underset{\\tau_{*}\\mid x\\sim\\theta}{\\mathbb{E}}\\Bigg[-\\beta\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\underset{y\\sim\\pi_{\\theta}(\\cdot\\mid y\\leq t)}{\\mathbb{E}}\\left[\\nabla\\log\\pi_{\\theta}(y\\vert y_{\\leq t})f_{\\phi_{1}}(y_{\\leq t},y)\\right]}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad+\\left(1-\\beta\\right)\\underset{t=0}{\\overset{T-1}{\\sum}}\\gamma^{t}\\nabla\\log\\pi_{\\theta}(y_{t+1}^{\\prime}\\vert y_{\\leq t}^{\\prime})\\hat{Q}_{f_{\\phi_{2}}}(y_{\\leq t}^{\\prime},y_{t+1}^{\\prime})\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the derivation of policy gradient in Eq. (10). ", "page_idx": 16}, {"type": "text", "text": "A.6 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 1. Let $\\hat{\\nabla}\\mathcal{L}(\\theta)=-\\beta\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)+(1-\\beta)\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)$ denote the empirical policy gradient given any trajectories $\\pmb{x}\\sim p_{\\pmb{x}},\\tau\\sim\\pi_{*}|\\pmb{x},\\tau^{\\prime}\\sim\\pi_{\\theta}|\\pmb{x}$ . Under Assumption $^{\\,l}$ , $\\hat{\\nabla}\\mathcal{L}(\\theta)$ is $\\frac{3B}{1\\!-\\!\\gamma}$ -Lipschitz $w.r t.\\mathrm{~}\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. By triangle inequality, we have for any $\\begin{array}{r}{{\\pmb x}\\sim p_{{\\pmb x}},\\tau\\sim\\pi_{*}|{\\pmb x},\\tau^{\\prime}\\sim\\pi_{\\theta}|{\\pmb x},}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\nabla}\\mathcal{L}(\\theta)\\|\\leq\\beta\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)\\right\\|+(1-\\beta)\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the formulation of off-policy gradient $\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)\\|$ in Eq. (10) under the condition of optimized off-policy $Q$ -value functions $f_{*}^{\\mathrm{off}}$ by Algorithm 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)\\right\\|=\\left\\|\\sum_{t=0}^{T-1}\\gamma^{t}\\underset{y\\sim\\pi_{\\theta}(\\cdot|y_{\\leq t})}{\\mathbb{E}}\\left[\\nabla\\log\\pi_{\\theta}(y|y_{\\leq t})f_{*}^{\\mathrm{off}}(y_{\\leq t},y)\\right]\\right\\|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Jensen\u2019s inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Big\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)\\Big\\|\\leq\\sum_{t=0}^{T-1}\\gamma_{\\,\\,y\\sim\\pi_{\\theta}(\\cdot|y_{\\leq t})}^{t}\\left[\\big\\|\\nabla\\log\\pi_{\\theta}(y|y_{\\leq t})\\big\\|\\,\\big|f_{*}^{\\mathrm{off}}(y_{\\leq t},y)\\big|\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Assumption 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta)\\right\\|\\leq B\\sum_{t=0}^{T-1}\\gamma^{t}\\leq\\frac{B}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we can bound the on-policy gradient $\\|\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)\\|$ by Jensen\u2019s inequality as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Big\\|\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\boldsymbol{\\theta})\\Big\\|\\leq\\sum_{t=0}^{T-1}\\gamma^{t}\\,\\big\\|\\nabla\\log\\pi_{\\boldsymbol{\\theta}}(y_{t+1}|y_{\\leq t})\\big\\|\\,\\Big|\\hat{Q}_{f_{*}^{\\mathrm{on}}}(y_{\\leq t},y_{t+1})\\Big|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on the definition of $\\hat{Q}_{f_{*}^{\\mathrm{on}}}(y_{\\leq t},y_{t+1})$ in Eq. (21) and by Jensen\u2019s inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\vert\\hat{Q}_{f_{*}^{\\mathrm{on}}}(y_{\\leq t},y_{t+1})\\right\\vert=\\left\\vert\\displaystyle\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}\\left(\\displaystyle\\sum_{y\\sim\\pi_{*}(\\cdot\\vert y_{\\leq t^{\\prime}})}\\left[f_{*}^{\\mathrm{on}}(y_{\\leq t^{\\prime}},y)\\right]-f_{*}^{\\mathrm{on}}(y_{\\leq t^{\\prime}},y_{t^{\\prime}+1})\\right)\\right\\vert}\\\\ {\\leq\\displaystyle\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}\\left(\\displaystyle\\sum_{y\\sim\\pi_{*}(\\cdot\\vert y_{\\leq t^{\\prime}})}\\left[\\vert f_{*}^{\\mathrm{on}}(y_{\\leq t^{\\prime}},y)\\vert\\right]+\\vert f_{*}^{\\mathrm{on}}(y_{\\leq t^{\\prime}},y_{t^{\\prime}+1})\\vert\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, by Assumption 1 (i) that $\\|f_{*}^{\\mathrm{on}}\\|_{\\infty}\\leq1$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\hat{Q}_{f_{*}^{\\mathrm{on}}}(\\pmb{y}_{\\le t},y_{t+1})\\right|\\le2\\sum_{t^{\\prime}=t}^{T-1}\\gamma^{t^{\\prime}-t}\\le\\frac{2}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau^{\\prime},\\theta)\\right\\|\\le\\frac{2B}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Coming back to the bound of $\\|\\hat{\\nabla}\\mathcal{L}(\\theta)\\|$ in Eq. (22) and combining it with Eq. (23) and Eq. (24) and under the assumption that $\\begin{array}{r}{\\gamma\\le2-\\frac1\\beta}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\hat{\\nabla}\\mathcal{L}(\\theta)\\|\\le\\frac{\\beta B}{1-\\gamma}+\\frac{2(1-\\beta)B}{(1-\\gamma)^{2}}\\le\\frac{3\\beta B}{1-\\gamma}\\le\\frac{3B}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof of Lemma 1. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. Under Assumption $^{\\,I}$ , the objective function ${\\mathcal{L}}(\\theta)$ satisfies that for any $\\theta,\\theta^{\\prime}\\in\\Theta_{\\mathrm{g}}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)\\leq\\mathcal{L}(\\theta^{\\prime})+\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}),\\theta-\\theta^{\\prime}\\rangle+\\frac{6B\\|\\theta-\\theta^{\\prime}\\|+\\frac{1}{2}L\\|\\theta-\\theta^{\\prime}\\|^{2}}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Under the definition of policy gradient in Eq. (10), for any $\\theta_{1},\\theta_{2}\\,\\in\\,\\Theta$ , supposing that $\\tau\\sim\\bar{\\pi}_{\\ast}|{\\pmb x},\\tau_{1}^{\\prime}\\sim\\pi_{\\theta_{1}}|{\\pmb x}$ and $\\tau_{2}^{\\prime}\\sim\\pi_{\\theta_{2}}|{\\pmb x}$ are independent, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\nabla\\mathcal{L}(\\theta_{1})-\\nabla\\mathcal{L}(\\theta_{2})\\right\\rVert}\\\\ &{=\\left\\lVert\\underbrace{\\mathbb{E}}_{\\alpha\\sim p_{\\alpha_{1}}\\,(\\tau,\\tau_{1}^{\\prime},\\tau_{2}^{\\prime})\\sim}\\,\\left[-\\beta\\left(\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{2})\\right)+(1-\\beta)\\left(\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau_{1}^{\\prime},\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau_{2}^{\\prime},\\theta_{2})\\right)\\right]\\right\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, by Jensen\u2019s inequality and triangle inequality, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lVert\\nabla\\mathcal{L}(\\theta_{1})-\\nabla\\mathcal{L}(\\theta_{2})\\right\\rVert}\\\\ &{\\le\\underset{x\\sim p_{\\alpha}}{\\mathbb{E}}\\frac{\\mathbb{E}}{(\\tau,\\tau_{1}^{\\prime},\\tau_{2}^{\\prime})\\sim}\\left[\\beta\\left\\lVert\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{2})\\right\\rVert+(1-\\beta)\\left\\lVert\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau_{1}^{\\prime},\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{on}}(\\tau_{2}^{\\prime},\\theta_{2})\\right\\rVert\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Based on the definition of off-policy gradient in Eq. (10) and using Jensen\u2019s inequality, we have for any $\\pmb{x}\\sim p_{\\pmb{x}},\\tau\\sim\\pi_{*}|\\pmb{x}.$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{2})\\right\\|}\\\\ &{\\underset{\\leq\\sum_{t=0}^{J-1}}{\\sum}\\!\\!\\!r^{t}\\!\\!\\left\\|_{y\\sim\\pi_{\\theta_{1}}(\\cdot\\vert y_{\\leq t})}\\!\\!\\left[\\nabla\\log\\pi_{\\theta_{1}}(y\\vert y_{\\leq t})f_{*}^{\\mathrm{off}}(y_{\\leq t},y)\\right]\\!\\!-\\!\\underset{y\\sim\\pi_{\\theta_{2}}(\\cdot\\vert y_{\\leq t})}{\\mathbb{E}}\\!\\!\\left[\\nabla\\log\\pi_{\\theta_{2}}(y\\vert y_{\\leq t})f_{*}^{\\mathrm{off}}(y_{\\leq t},y)\\right]\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, by triangle inequality, we have for any $t\\in\\{0,1,\\ldots,T-1\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\underset{y\\sim\\pi_{\\theta_{1}}(\\cdot\\vert y_{\\le t})}{\\mathbb{E}}[\\nabla\\log\\pi_{\\theta_{1}}(y\\vert y_{\\le t})f_{*}^{\\mathrm{off}}(y_{\\le t},y)]-\\underset{y\\sim\\pi_{\\theta_{2}}(\\cdot\\vert y_{\\le t})}{\\mathbb{E}}[\\nabla\\log\\pi_{\\theta_{2}}(y\\vert y_{\\le t})f_{*}^{\\mathrm{off}}(y_{\\le t},y)]\\right\\|}\\\\ &{=\\left\\|\\underset{y\\in\\mathcal{V}}{\\sum}\\left(\\pi_{\\theta_{1}}(y\\vert y_{\\le t})\\nabla\\log\\pi_{\\theta_{1}}(y\\vert y_{\\le t})-\\pi_{\\theta_{2}}(y\\vert y_{\\le t})\\nabla\\log\\pi_{\\theta_{2}}(y\\vert y_{\\le t})\\right)f_{*}^{\\mathrm{off}}(y_{\\le t},y)\\right\\|}\\\\ &{\\le\\left\\|\\underset{y\\in\\mathcal{V}}{\\sum}\\left(\\pi_{\\theta_{1}}(y\\vert y_{\\le t})-\\pi_{\\theta_{2}}(y\\vert y_{\\le t})\\right)\\nabla\\log\\pi_{\\theta_{1}}(y\\vert y_{\\le t})f_{*}^{\\mathrm{off}}(y_{\\le t},y)\\right\\|=:\\left(\\right.\\mathrm{1})}\\\\ &{\\quad\\left.+\\left\\|\\underset{y\\in\\mathcal{V}}{\\sum}\\pi_{\\theta_{2}}(y\\vert y_{\\le t})\\left(\\nabla\\log\\pi_{\\theta_{1}}(y\\vert y_{\\le t})-\\nabla\\log\\pi_{\\theta_{2}}(y\\vert y_{\\le t})\\right)f_{*}^{\\mathrm{off}}(y_{\\le t},y)\\right\\|=:\\left(\\mathrm{II}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Jensen\u2019s inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathrm{I})\\leq\\sum_{y\\in\\mathcal{V}}|\\pi_{\\theta_{1}}(y|y_{\\leq t})-\\pi_{\\theta_{2}}(y|y_{\\leq t})|\\left\\|\\nabla\\log\\pi_{\\theta_{1}}(y|y_{\\leq t})\\right\\|\\left|f_{*}^{\\mathrm{off}}(y_{\\leq t},y)\\right|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, following from Assumption 1 and the fact that $|\\pi_{\\theta_{1}}(y|\\pmb{y}_{\\le t})-\\pi_{\\theta_{2}}(y|\\pmb{y}_{\\le t})|\\le\\pi_{\\theta_{1}}(y|\\pmb{y}_{\\le t})+$ $\\pi_{\\theta_{2}}(y|y_{\\leq t})$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathrm{I})\\leq B\\sum_{y\\in\\mathcal{V}}\\left(\\pi_{\\theta_{1}}(y|{\\pmb y}_{\\leq t})+\\pi_{\\theta_{2}}(y|{\\pmb y}_{\\leq t})\\right)=2B\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, by Jensen\u2019s inequality and following from Assumption 1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{II})\\leq\\displaystyle\\sum_{y\\in\\mathcal{V}}\\pi_{\\theta_{2}}(y|y_{\\leq t})\\left\\|\\nabla\\log\\pi_{\\theta_{1}}(y|y_{\\leq t})-\\nabla\\log\\pi_{\\theta_{2}}(y|y_{\\leq t})\\right\\|\\left|\\int_{*}^{\\mathrm{off}}({y_{\\leq t}},y)\\right|}\\\\ &{\\qquad\\leq\\!L\\|\\theta_{1}-\\theta_{2}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, coming back to Eq. (26), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\lVert\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{1})-\\hat{\\mathcal{G}}^{\\mathrm{off}}(\\tau,\\theta_{2})\\right\\rVert\\le(2B+L\\|\\theta_{1}-\\theta_{2}\\|)\\sum_{t=0}^{T-1}\\gamma^{t}\\le\\frac{2B+L\\|\\theta_{1}-\\theta_{2}\\|}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we have for any $\\begin{array}{r}{{\\pmb x}\\sim p_{{\\pmb x}},\\tau_{1}^{\\prime}\\sim\\pi_{\\theta_{1}}|{\\pmb x},\\tau_{2}^{\\prime}\\sim\\pi_{\\theta_{2}}|{\\pmb x},}\\end{array}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\cdots}\\\\ &{=\\displaystyle\\prod_{t=0}^{T-1}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Coming back to Eq. (25) and combining Eq. (27) and Eq. (28) and assume that $\\begin{array}{r}{\\gamma\\le2-\\frac1\\beta}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}(\\theta_{1})-\\nabla\\mathcal{L}(\\theta_{2})\\|\\le\\frac{6B+L\\|\\theta_{1}-\\theta_{2}\\|}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we have for any $\\theta,\\theta\\in\\Theta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta)-\\mathcal{L}(\\theta^{\\prime})-\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}),\\theta-\\theta^{\\prime}\\rangle}\\\\ &{\\le|\\mathcal{L}(\\theta)-\\mathcal{L}(\\theta^{\\prime})-\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}),\\theta-\\theta^{\\prime}\\rangle|}\\\\ &{\\le\\displaystyle\\left|\\int_{t\\in(0,1)}\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}+t(\\theta-\\theta^{\\prime})),\\theta-\\theta^{\\prime}\\rangle d t-\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}),\\theta-\\theta^{\\prime}\\rangle\\right|}\\\\ &{\\le\\displaystyle\\int_{t\\in(0,1)}\\|\\nabla\\mathcal{L}(\\theta^{\\prime}+t(\\theta-\\theta^{\\prime}))-\\nabla\\mathcal{L}(\\theta^{\\prime})\\|\\|\\theta-\\theta^{\\prime}\\|d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, by Eq. (29) and set $\\theta_{1}=\\theta^{\\prime}+t(\\theta-\\theta^{\\prime})$ and $\\theta_{2}=\\theta^{\\prime}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\theta)-\\mathcal{L}(\\theta^{\\prime})-\\langle\\nabla\\mathcal{L}(\\theta^{\\prime}),\\theta-\\theta^{\\prime}\\rangle}\\\\ &{\\leq\\displaystyle\\int_{t\\in(0,1)}\\frac{6B\\|\\theta-\\theta^{\\prime}\\|+L t\\|\\theta-\\theta^{\\prime}\\|^{2}}{1-\\gamma}d t=\\frac{6B\\|\\theta-\\theta^{\\prime}\\|+\\frac{1}{2}L\\|\\theta-\\theta^{\\prime}\\|^{2}}{1-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We prove Theorem 2 as follows. ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\theta_{t}$ , $\\theta_{t+1}$ , $t\\,\\in\\,\\{0,1,\\ldots,T-1\\}$ be adjacent parameters of policy $\\pi_{\\theta_{t}},\\,\\pi_{\\theta_{t+1}}$ given by Algorithm 1. Then, using Lemma 2 by setting $\\bar{\\theta}=\\bar{\\theta}_{k+1},\\bar{\\theta^{\\prime}}=\\theta_{k}$ for any $k\\in\\{0,1,\\ldots,K-1\\}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k})+\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\theta_{k+1}-\\theta_{k}\\rangle+\\frac{6B\\|\\theta_{k+1}-\\theta_{k}\\|+\\frac{1}{2}L\\|\\theta_{k+1}-\\theta_{k}\\|^{2}}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following from the updating role $\\theta_{k+1}=\\theta_{k}-\\eta\\hat{\\nabla}\\mathcal{L}(\\theta_{k})$ and Lemma 1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\theta_{k+1}-\\theta_{k}\\|\\leq\\frac{3\\eta B}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We further assume that $\\begin{array}{r}{\\gamma\\leq1-\\frac{\\eta L}{2}}\\end{array}$ , and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{6B\\|\\theta_{k+1}-\\theta_{k}\\|+\\frac{1}{2}L\\|\\theta_{k+1}-\\theta_{k}\\|^{2}}{1-\\gamma}\\leq\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k})-\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\eta\\hat{\\nabla}\\mathcal{L}(\\theta_{k})\\rangle+\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking expectations on both sides, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{L}(\\theta_{k+1})]\\leq\\mathbb{E}[\\mathcal{L}(\\theta_{k})]-\\eta\\mathbb{E}[\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\mathbb{E}_{x\\sim p_{x}}\\mathbb{E}_{(\\tau,\\tau^{\\prime})\\sim\\pi_{*}|x\\cdot\\pi_{\\theta}|x}[\\hat{\\nabla}\\mathcal{L}(\\theta_{k})]\\rangle]+\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging the terms, and making average on $k\\in\\{0,1,\\ldots,K-1\\}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\|\\nabla\\mathcal{L}(\\theta_{k})\\|^{2}\\right]\\leq\\displaystyle\\frac{1}{K\\eta}\\sum_{k=0}^{K-1}\\left(\\mathbb{E}[\\mathcal{L}(\\theta_{k})]-\\mathbb{E}[\\mathcal{L}(\\theta_{k+1})]\\right)+\\displaystyle\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{K\\eta}\\left(\\mathbb{E}[\\mathcal{L}(\\theta_{0})]-\\mathbb{E}[\\mathcal{L}(\\theta_{K})]\\right)+\\displaystyle\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}}\\leq\\displaystyle\\frac{4}{K\\eta(1-\\gamma)}+\\frac{27\\eta B^{2}}{(1-\\gamma)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality follows from the fact that $\\begin{array}{r}{{\\mathcal{L}}(\\theta)\\leq\\frac{2}{1-\\gamma}}\\end{array}$ for any $\\theta\\in\\Theta$ under Assumption 1 (i). ", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r}{\\eta=\\frac{2}{3B}\\sqrt{\\frac{1-\\gamma}{3K}}}\\end{array}$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\|\\nabla\\mathcal{L}(\\theta_{k})\\|^{2}\\right]\\leq\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\|\\nabla\\mathcal{L}(\\theta_{k})\\|^{2}\\right]\\leq\\frac{6B}{1-\\gamma}\\sqrt{\\frac{3}{(1-\\gamma)K}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof of Theorem 2. ", "page_idx": 19}, {"type": "text", "text": "A.7 Proof of Corollary 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Let the convergence rate in Eq. (30) satisfy that $\\begin{array}{r}{\\frac{6B}{1-\\gamma}\\sqrt{\\frac{3}{(1-\\gamma)K}}\\le\\epsilon}\\end{array}$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nK\\ge\\frac{108B^{2}}{(1-\\gamma)^{3}\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which indicates that when the iteration number of policy updating satisfies $K:=\\mathcal{O}(\\epsilon^{-2})$ , it can reach an $\\epsilon$ -accurate stationary point to optimize the objective in Eq. (9), such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\|\\nabla{\\mathcal{L}}(\\theta_{k})\\|^{2}\\right]\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For simplicity, we define the policy as a softmax function with a linear transformation of $\\pmb{y}_{\\leq t}\\in\\mathbb{R}^{H}$ with $\\theta\\in\\mathbb{R}^{|\\mathcal{V}|\\times H}$ . Formally, for any trajectory $\\tau$ and any timestep $t\\in\\{0,1,\\ldots,T-1\\}$ , we have the probability of any $y\\in\\mathcal{V}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{\\boldsymbol{\\theta}}(y|\\boldsymbol{y}_{\\le t})=\\frac{\\exp(\\theta_{y}\\pmb{y}_{\\le t})}{\\sum_{y^{\\prime}\\in\\mathcal{V}}\\exp(\\theta_{y^{\\prime}}\\pmb{y}_{\\le t})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the following, we will analyze the computational complexity in each policy updating iteration. First, we find that each inner-loop step of $Q$ -value function updating has a gradient computation complexity of $\\mathcal{O}(T|\\mathcal{V}|H)$ given the linear formulation of $Q$ -value functions. Accordingly, $N$ inner-loop steps in each policy updating iteration have a computational complexity of $\\mathcal{O}(N T|\\mathcal{V}|H)$ . Second, for policy gradient computation, since computational complexity of $\\nabla\\log\\pi_{\\boldsymbol{\\theta}}(y|\\mathbf{\\boldsymbol{y}}_{<T})$ is $\\mathcal{O}(|\\boldsymbol{\\mathcal{V}}|H)$ , the computation complexity of policy gradient computation is $\\mathcal{O}(T|\\mathcal{V}|H(T+|\\mathcal{V}|))$ . Overall, the total gradient computational complexity is $\\mathcal{O}(\\epsilon^{-2}T|\\bar{\\mathcal{V}}|H(N+T+|\\dot{\\mathcal{V}}|))$ , which completes the proof of Corollary 1. ", "page_idx": 20}, {"type": "text", "text": "B Experimental Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We use NVIDIA A40 GPUs with 40GB RAM to conduct all the experiments. ", "page_idx": 20}, {"type": "text", "text": "B.1 Instruction-Following Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Base models. We conduct experiments on both GPT-2 [28] and OpenLLaMA [10]. For the GPT-2 experiments, we use GPT-2 XL2 with 1.5B parameters to construct the teacher policy and GPT- $\\cdot2^{3}$ with 117M parameters to construct the student policy. For the OpenLLaMA experiments, we use OpenLLaMA- $\\cdot7{\\bf B}^{4}$ with 6.7B parameters to construct the teacher policy and OpenLLaMA- $\\mathbf{\\nabla}3\\mathbf{B}^{5}$ with 2.7B parameters to construct the student model. ", "page_idx": 20}, {"type": "text", "text": "Training details. We fine-tune the OpenLLaMA-7B teacher model and the OpenLLaMA-3B student models on the corresponding supervised dataset with 10,000 steps. The GPT-2 teacher and student models use the fine-tuned checkpoints by Gu et al. [14]. For the implementation of compared baselines, we use the code by Ko et al. [21] and re-run the results. The optimization protocol for KD training largely follows the previous work [14, 21]. In particular, we search for the learning rates among a finite set for each experiment to obtain the best result. The batch size for each experiments is seleted to make full use of the 40GB RAM of an A40 GPU. To handle the adversarial training, we choose the number of adversarial steps $K=5$ and the adversarial control factor $\\alpha=0.1$ based on the development experiments. We use a default off-/on-policy combination factor $\\beta=0.5$ for main experiments while exploring other values for analysis. The hyperparameters for training are listed in Table 3. ", "page_idx": 20}, {"type": "text", "text": "B.2 Task-Specific Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Base models. For the text summarization and commonsense reasoning experiments, we use T5- $\\mathrm{XL}^{6}$ with 2.8B parameters to construct the teacher policy and construct the student policy with T5-Large7 (770M parameters), T5-Base8 (220M parameters) and $\\mathrm{T5-Small^{9}}$ (60M parameters). ", "page_idx": 20}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/0a1581398353dfbff9c987d1fa3bfde9a5d24c5a9cfa57456af96c3d605e054e.jpg", "table_caption": ["Table 3: Hyperparameters for instruction-following experiments. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "For the machine translation experiments, we use mT5-XL [42] to construct the teacher policy and mT5-Large/-Base/-Small to construct the student policy. ", "page_idx": 21}, {"type": "text", "text": "Training details. We initialize the corresponding teacher and student models using 10,000-step-finetuning checkpoints on the SAMSum dataset, 80,000-step-fine-tuning checkpoints on the IWSLT\u201917 (en-de) dataset and 3,000-step-fine-tuning checkpoints on the StrategyQA dataset. We largely follow Ko et al. [21] to set the hyperparameters for training. In particular, we search for the learning rate from a preset range to obtain the best result for each baseline and our method. The batch size is selected to make full use of the RAM of GPUs. We use a relatively larger maximum number of training steps for IWSLT\u201917 (en-de) experiments to satisfy sufficient convergences for the machine translation task. We use beam search for the evaluation of the IWSLT\u201917 (en-de) dataset. ", "page_idx": 21}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/331678798593297feb457e804cfed205eee268ccefa164c8614242a8a888cf13.jpg", "table_caption": ["Table 4: Hyperparameters for three task-specific experiments. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/19ca6f1d7604015c0108725c530cbbbb8553d35004c188bd05687232d6c7bdfd.jpg", "table_caption": ["Table 5: Comparison with state-of-the-art KD methods on the instruction-following dataset using fine-tuned GPT-2 XL (1.5B) as the teacher model and fine-tuned GPT-2 (0.1B) as the student model. We format the best, the second best and worse than SFT results. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/f569d4c907b92a27e29c9eb5138814c06f19ccaa98c1106aa6eec8da0c9a421c.jpg", "img_caption": ["Figure 4: Performance of difference step-wise distribution distances on five instruction-following datasets using OpenLLaMA-7B $\\rightarrow$ OpenLLaMA-3B. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Results Based on GPT-2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition to the experimental results based on OpenLLaMA for instruction-following tasks, we also conduct experiments based on GPT-2. Results are illustrated in Table 5. Compared with current state-of-the-art KD approaches, our method achieves the best results on five datasets with both GPT-4 feedback and ROUGE-L evaluations. ", "page_idx": 22}, {"type": "text", "text": "C.2 Comparisons on Step-Wise Distribution Distance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 4 and Figure 5 illustrate performance comparison with well-studied step-wise distribution distance, including KL, RKL, JS divergences and TV distances. Results show that the optimization of proposed moment-matching objectives outperforms other step-wise distribution distances via either on-policy distillation or off-policy distillation. Besides, jointly using on-policy and off-policy moment-matching further improves the performances and achieves the best results on five instructionfollowing datasets with KD from the OpenLLaMA-7B to OpenLLaMA-3B model, and achieves the best results on three task-specific datasets with KD from the (m)T5-XL to (m)T5-Base model. ", "page_idx": 22}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/d12c8b9129dcfb95a5bc68cd041632f0ef3dfa7605c3c1e83c9fb8e1709bb4b7.jpg", "img_caption": ["Figure 5: Performance of difference step-wise distribution distances on three task-specific datasets using $(\\mathrm{m})\\mathrm{T}5\\mathrm{-}\\mathrm{XL}\\rightarrow(\\mathrm{m})\\mathrm{T}5\\mathrm{-}\\mathrm{Base}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/d942e2f1a1d43889943107b9eca197bca5cfcb1d22c8154e35d9faec23237c3c.jpg", "img_caption": ["Figure 6: Training loss and $d_{\\mathrm{MM}}^{\\mathrm{on}}$ , $d_{\\mathrm{MM}}^{\\mathrm{off}}$ against training step on four datasets. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.3 Adversarial Training Procedure ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure 6 illustrates the training loss and on-/off-policy moment-matching distances against the training steps on the instruction-following dataset and three task-specific datasets. We can observe that the training losses on four datasets have a similar trend, increasing at the beginning and then converging to a relatively lower level. The trend of loss function aligns with the characteristics of adversarial training with gradient descent ascent. In contrast, both the on-policy moment-matching distance $d_{\\mathrm{MM}}^{\\mathrm{on}}$ and the off-policy moment-matching distance $d_{\\mathrm{MM}}^{\\mathrm{off}}$ reduce as the number of training steps increases, which shows the effectiveness of our adversarial training approach for moment-matching. ", "page_idx": 23}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/a229f78d5f0df9196d3034c8e76d9c117e7239b44e010bc327df984a3e4a7eb5.jpg", "img_caption": ["Figure 7: Moment-matching via distribution-matching on the instruction-following dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/6f7987308d5e2e17924c8abc0b28c724888a1710e1cf1b184d658701585b2508.jpg", "img_caption": ["(a) On-policy moment-matching(b) On-policy moment-matching(c) On-policy moment-matching distance on SAMSum. distance on IWSLT\u201917 (en-de). distance on StrategyQA. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "0VeSCjRDBy/tmp/12bcd9e7df1798f1a3d4ad6331268be47006a887e3c863399f8d5dc2e8529b52.jpg", "img_caption": ["(d) Off-policy moment-matching(e) Off-policy moment-matching(f) Off-policy moment-matching distance on SAMSum. distance on IWSLT\u201917 (en-de). distance on StrategyQA. ", "Figure 8: Moment-matching via distribution-matching on three task-specific datasets. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.4 Moment-Matching via Distribution Matching ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We investigate how the distribution-matching methods via KL, RKL, JS divergences or TV distance can optimize the moment-matching distance in Figure 7 and Figure 8. Results show that the proposed adversarial training algorithm is more effective in minimizing the moment-matching distance than the distribution-matching methods. ", "page_idx": 24}, {"type": "text", "text": "C.5 Analysis on the Off-/On-Policy Combination Factor $\\beta$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We study the impact of on-policy and off-policy objectives with the combination factor $\\beta~\\in$ $\\{0.00,0.{\\dot{5}}0,0.90,\\dot{0}.99,1.00\\}$ in Eq. (9), which denotes a linear combination coefficient of the on-policy and off-policy objectives. We observe that if $\\beta=0$ , only the on-policy objective contributes to policy learning. As it increases from 0 to 1, the influence of off-policy objective increases while that of the on-policy objective decreases. Finally, when $\\beta=1$ , only the off-policy objective contributes to policy learning. We conduct experiments across four datasets. Specifically, we evaluate ROUGE-L for OpenLLaMA2-3B on the DollyEval dataset, ROUGE-L for T5-base on the SAMSum dataset, accuracy for T5-base on the IWSLT\u201917 dataset and accuracy for T5-base on the StrategyQA dataset. Results in Table 6 show that a combination of on-policy and off-policy objectives outperforms using either on-policy or off-policy objectives only across four datasets. ", "page_idx": 24}, {"type": "table", "img_path": "0VeSCjRDBy/tmp/27f68200ef5b9805635628a270954aef8da6e019c9c0ead1827b51d71a8b23fa.jpg", "table_caption": ["Table 6: Effects of the off-/on-policy combination factor $\\beta$ on four datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The claim of contributions in the abstract and introduction have been fully reflected in the sections of Methods and Experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Limitations are discussed in section of the conclusion ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 26}, {"type": "text", "text": "Justification: Complete proofs for the theoretical results are available in Appendix A. All proofs are based on Assumption 1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 26}, {"type": "text", "text": "Justification: Detailed experimental setups such as datasets, models and hyperparameters used in implementing proposed algorithms are all described in detail. See Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 27}, {"type": "text", "text": "Justification: All the datasets used in this work are publicly available. The code and implementation details are released at this GitHub URL: https://github.com/ jiachenwestlake/MMKD. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide the details of experimental settings in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 28}, {"type": "text", "text": "Justification: All experimental results have error bars. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 28}, {"type": "text", "text": "Justification: Available in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The research is conducted with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work mainly focuses on algorithm design and performance improvement, which has no relationship to societal impacts. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper has cited the original papers that produced the models, code packages or datasets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]