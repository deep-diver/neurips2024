[{"figure_path": "0VeSCjRDBy/figures/figures_1_1.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares different approaches for knowledge distillation in large language models.  It contrasts distribution-matching-based distillation (minimizing KL, RKL, TV distance between teacher and student probability distributions) with the proposed action-value moment-matching distillation. The latter approach focuses on matching the action-value moments of the teacher's policy from both on-policy (student-generated data) and off-policy (teacher-generated data) perspectives, aiming for better knowledge transfer.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_1_2.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares the proposed action-value moment-matching distillation method to existing distribution-matching methods for knowledge distillation of large language models.  It illustrates the differences in how the student policy learns from the teacher policy (both on-policy, where the student generates its own outputs, and off-policy, where the teacher generates the outputs) by comparing the optimization of probability distribution distances (KL, RKL, TV) with moment-matching of action-value functions.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_8_1.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different approaches to knowledge distillation of large language models.  The top two diagrams illustrate traditional distribution-matching methods (on-policy and off-policy), which attempt to align the probability distributions of the teacher and student models. The bottom two diagrams show the proposed action-value moment-matching method, which focuses on aligning the moments of the action-value functions, offering a potentially more robust and effective approach to knowledge transfer.  The use of both on-policy and off-policy perspectives is highlighted as a key aspect of the proposed method.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_9_1.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different distillation methods: on-policy and off-policy distribution matching, and on-policy and off-policy action-value moment matching.  The on-policy methods utilize student-generated outputs for training, while off-policy methods use teacher-generated outputs. The key difference is that moment matching focuses on aligning the action-value functions (Q-functions) which measure the quality of decisions, instead of directly minimizing the distance between probability distributions of teacher and student predictions.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_22_1.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different distillation methods: on-policy distribution matching, off-policy distribution matching, on-policy Q-value moment matching, and off-policy Q-value moment matching.  It highlights the key difference between distribution-matching approaches (which minimize distances like KL divergence) and the proposed moment-matching approach (which focuses on aligning the action-value moments of the teacher and student policies).  The diagram uses visual representations of policies (\u03c0\u03b8, \u03c0*) and Q-functions to illustrate the differences.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_23_1.jpg", "caption": "Figure 3: Adversarial training procedure for optimizing the on-policy and off-policy moment-matching distances dMM, dMM against training step.", "description": "This figure shows the results of the adversarial training procedure. The leftmost graph shows the training loss and the moment-matching distances against the training steps. The other two graphs show the on-policy and off-policy moment-matching distances on the test sets.  The results demonstrate that the adversarial training effectively optimizes both the on-policy and off-policy moment-matching distances, leading to improved performance.", "section": "3.3 Adversarial Training Algorithm"}, {"figure_path": "0VeSCjRDBy/figures/figures_23_2.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares the proposed action-value moment-matching distillation method with traditional distribution matching methods for knowledge distillation in large language models.  It illustrates the different approaches for both on-policy (using student-generated data) and off-policy (using teacher-generated data) scenarios. The key difference is that the proposed method matches the moments of the action-value functions (Q-functions), rather than directly minimizing the distance between probability distributions.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_24_1.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different distillation methods: on-policy and off-policy distribution matching, and on-policy and off-policy Q-value moment matching.  Distribution matching methods attempt to minimize the distance between the probability distributions of teacher and student model outputs. In contrast, the proposed method (Q-value moment matching) focuses on matching the moments of the action-value functions, representing the quality of token-level predictions, rather than directly comparing the distributions.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_24_2.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different distillation methods: on-policy and off-policy distribution matching, and on-policy and off-policy Q-value moment matching. The on-policy methods use student-generated outputs, while the off-policy methods use teacher-generated outputs.  The moment-matching approach, proposed by the authors, focuses on aligning the action-value moments instead of probability distributions, aiming to better capture the teacher's knowledge.", "section": "3 Method"}, {"figure_path": "0VeSCjRDBy/figures/figures_24_3.jpg", "caption": "Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \u03c0\u03b8 and \u03c0* denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc.", "description": "This figure compares four different distillation methods: on-policy and off-policy distribution matching, and on-policy and off-policy Q-value moment matching.  The figure illustrates how the proposed moment-matching method differs from traditional distribution matching methods by focusing on aligning action-value moments rather than probability distributions.  The teacher and student policies (\u03c0* and \u03c0\u03b8, respectively) are highlighted to show the flow of information in each method.", "section": "3 Method"}]