[{"figure_path": "IlIDNMvwmX/figures/figures_3_1.jpg", "caption": "Figure 1: Forward and backward propagation of the M-HT model. (a)-(c): mathematical relationship between the M-HT model and vanilla IF model. (d)-(e): surrogate gradient calculation for the M-HT model.", "description": "This figure illustrates the mathematical relationship between the multi-hierarchical threshold (M-HT) model and the vanilla integrate-and-fire (IF) model, as well as the surrogate gradient calculation for the M-HT model.  Panels (a)-(c) show how the M-HT model simulates the behavior of the vanilla IF model over multiple time steps, enabling information integration. (d) and (e) depict the surrogate gradient calculation method used for the M-HT model, addressing the non-differentiability of the Heaviside step function in the vanilla IF model's firing mechanism.  The use of a surrogate gradient allows for backpropagation during training, even though the actual firing process is non-differentiable.", "section": "4 Methodology"}, {"figure_path": "IlIDNMvwmX/figures/figures_4_1.jpg", "caption": "Figure 2: The STBP learning framework based on the LM-HT model. (a): vanilla STBP training. (b): STBP training with the LM-HT model. (c): direct training of quantized ANNs. (d): hybrid training with the LM-HT model, here R-I Curve denotes Rate-Input Curve.", "description": "This figure illustrates the STBP learning framework using the LM-HT model and compares it to the vanilla STBP training, direct training of quantized ANNs, and a hybrid training approach.  Panel (a) shows the standard vanilla STBP training. Panel (b) depicts the STBP training process with the LM-HT model, which incorporates a learnable multi-hierarchical threshold model and a temporal-global information matrix. Panel (c) demonstrates the direct training of quantized ANNs. Finally, panel (d) presents a hybrid training method that combines the strengths of the LM-HT model and quantized ANNs training. The rate-input (R-I) curves are shown for the LM-HT model and the QCFS ANN model.", "section": "4.3 The Learnable Multi-hierarchical Threshold (LM-HT) Model"}, {"figure_path": "IlIDNMvwmX/figures/figures_5_1.jpg", "caption": "Figure 3: Reparameterization procedure of the LM-HT model.", "description": "This figure illustrates the reparameterization process of transforming the LM-HT model (Learnable Multi-hierarchical Threshold model) into a vanilla LIF (Leaky Integrate-and-Fire) model for more efficient hardware deployment. The LM-HT model uses a multi-hierarchical threshold mechanism to integrate information over multiple timesteps within a single step. This figure shows how the LM-HT model's parameters (weights, biases, Temporal-Global Information Matrix (T-GIM), and leaky parameters) are transformed to equivalent parameters in a vanilla LIF model, allowing for a lossless conversion that maintains accuracy while simplifying the model's structure for hardware implementation.", "section": "4.5 Reparameterize the LM-HT model to vanilla LIF model"}, {"figure_path": "IlIDNMvwmX/figures/figures_16_1.jpg", "caption": "Figure 3: Reparameterization procedure of the LM-HT model.", "description": "This figure illustrates how the multi-hierarchical threshold (M-HT) model can be reparameterized into a vanilla single-threshold LIF model.  The process involves transforming the multi-step input current weighting scheme of the M-HT model into a single-step LIF model. This transformation allows for more flexible hardware deployment of the model, as many neuromorphic hardware platforms are designed to work with LIF neurons, which only use a single threshold for spiking behavior. The figure shows two stages: the SNN training stage and the SNN inference stage. Both stages demonstrate how the T-GIM matrix and the learnable parameters (\u03bb and \u03a9) are used for this transformation. The transformation ensures that there is no accuracy loss when switching from the M-HT model to the LIF model.", "section": "4.5 Reparameterize the LM-HT model to vanilla LIF model"}]