[{"figure_path": "ODbTlAs0Oj/figures/figures_0_1.jpg", "caption": "Figure 1: M\u00b3GPT can handle core motion comprehension and generation tasks, including text-to-motion, motion-to-text, music-to-dance, dance-to-music, motion prediction, and motion in-between. The motion sequences within the dashed-line areas are masked in the input.", "description": "This figure illustrates the six core motion-related tasks that the proposed M\u00b3GPT model can perform.  These tasks demonstrate both comprehension and generation capabilities. The figure visually shows examples of each task by using different modalities as input. Masked motion sequences in some cases highlight the in-filling abilities of the model.  Text, music and motion data are used as input and output modalities.", "section": "1 Introduction"}, {"figure_path": "ODbTlAs0Oj/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the M\u00b3GPT framework. M\u00b3GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\u00b3GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.", "description": "This figure shows a detailed overview of the M\u00b3GPT framework's architecture and training process.  M\u00b3GPT leverages multimodal tokenizers to convert raw motion and music data into discrete tokens, which are then processed by a motion-aware language model (LLM). The training is a three-stage process: 1) Training of Multimodal Tokenizers: This stage focuses on training separate tokenizers for text, music, and motion data. These tokenizers convert the raw data into discrete tokens that can be understood by the LLM. 2) Pre-training LLM:  In this stage, the LLM is pre-trained on a large dataset of multimodal data, enabling it to understand the relationships between different modalities (text, music, motion).  3) Instruction Tuning LLM: This stage fine-tunes the pre-trained LLM using task-specific instructions, allowing it to perform various motion-related tasks, such as text-to-motion, music-to-dance, and motion prediction.", "section": "3 Method"}, {"figure_path": "ODbTlAs0Oj/figures/figures_9_1.jpg", "caption": "Figure 3: Qualitative results for long-term dance and music-text conditioned dance generation of M\u00b3GPT.", "description": "This figure shows two examples of M\u00b3GPT's zero-shot generalization capabilities in dance generation. (a) demonstrates long-duration dance generation, where M\u00b3GPT generates a coherent dance sequence from a single music input by recursively generating short segments. (b) demonstrates music and text-conditioned dance generation, where M\u00b3GPT generates a dance sequence that both synchronizes with a music input and incorporates actions described in a text input.", "section": "4 Experiments"}, {"figure_path": "ODbTlAs0Oj/figures/figures_14_1.jpg", "caption": "Figure 2: An overview of the M\u00b3GPT framework. M\u00b3GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\u00b3GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.", "description": "This figure illustrates the architecture of the M\u00b3GPT framework, highlighting the three main components: multimodal tokenizers (for compressing raw motion and music data into discrete tokens), a motion-aware language model (LLM) that understands and generates motion tokens, and a three-stage training process. The training process consists of multimodal tokenizer training, modality alignment pre-training, and instruction tuning to improve the model's ability to comprehend and generate motion.", "section": "3 Method"}, {"figure_path": "ODbTlAs0Oj/figures/figures_16_1.jpg", "caption": "Figure 2: An overview of the M\u00b3GPT framework. M\u00b3GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\u00b3GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.", "description": "This figure shows the overall framework of M\u00b3GPT, which comprises three main components: multimodal tokenizers, a motion-aware language model, and a three-stage training process.  The tokenizers are responsible for converting raw motion and music data into discrete tokens, allowing seamless integration with text. The language model leverages these tokens to perform various motion-related tasks. The three-stage training process includes training the tokenizers, pre-training the model for modality alignment, and finally fine-tuning it using instructions. This iterative process enables M\u00b3GPT to effectively handle diverse motion-related tasks.", "section": "3 Method"}, {"figure_path": "ODbTlAs0Oj/figures/figures_19_1.jpg", "caption": "Figure 2: An overview of the M\u00b3GPT framework. M\u00b3GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\u00b3GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.", "description": "This figure presents a visual overview of the M\u00b3GPT framework, detailing its three core components: multimodal tokenizers, a motion-aware language model, and the three-stage training process.  The tokenizers are responsible for converting raw data (text, music, motion) into a unified representation. The motion-aware language model then processes these unified representations, learning to generate motion data given various inputs. The training process is depicted in three stages:  First, the multimodal tokenizers are trained. Second, a modality-alignment pre-training phase occurs to align and unify representations.  Finally, an instruction tuning phase further refines the model's ability to follow instructions for generating motion.", "section": "3 Method"}, {"figure_path": "ODbTlAs0Oj/figures/figures_20_1.jpg", "caption": "Figure 2: An overview of the M\u00b3GPT framework. M\u00b3GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\u00b3GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.", "description": "This figure illustrates the architecture of the M\u00b3GPT framework, which comprises three main components: multimodal tokenizers (for converting raw motion and music data into discrete tokens), a motion-aware language model (based on LLMs, to understand and generate motion tokens), and a three-stage training process (multimodal tokenizers training, modality-alignment pre-training, and instruction tuning).  The diagram visually depicts the flow of data through these components and highlights the interactions between them during training and inference.", "section": "3 Method"}]