[{"figure_path": "ODbTlAs0Oj/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music\u00b3, respectively.", "description": "This table compares several recent multimodal and multitask methods focusing on motion comprehension and generation.  It assesses their capabilities across six key tasks: text-to-motion (T2M), motion-to-text (M2T), music-to-dance (A2D), dance-to-music (D2A), motion-to-motion (including prediction and in-between sequences, M2M), and unconstrained generation of motion, text, and music (Random M, Random T, Random A). A checkmark (\u2713) indicates a method's ability to perform each task.", "section": "Related Work"}, {"figure_path": "ODbTlAs0Oj/tables/tables_7_1.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results to demonstrate the effectiveness of synergy learning and joint optimization of the language model (LLM) and motion de-tokenizer in the M\u00b3GPT model.  It compares the performance of different model configurations across two tasks: text-to-motion and music-to-dance. The configurations vary in terms of whether or not joint optimization and additional auxiliary tasks (T2D and A2T) are included.  Metrics like FID, Diversity, R-Precision, and BAS are used to assess performance. The table shows the improved results when both joint optimization and synergy learning are applied, indicating the effectiveness of the M\u00b3GPT's multitask learning strategy.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music\u00b3, respectively.", "description": "This table compares several state-of-the-art multimodal and multitask methods for motion comprehension and generation.  It assesses their capabilities across six tasks: text-to-motion (T2M), motion-to-text (M2T), music-to-dance (A2D), dance-to-music (D2A), motion-to-motion (M2M - encompassing prediction and in-betweening), and unconstrained generation of motion, text, and music.  The table helps illustrate the unique capabilities of M\u00b3GPT (the authors' proposed method) in handling multiple modalities and tasks simultaneously.", "section": "Related Work"}, {"figure_path": "ODbTlAs0Oj/tables/tables_8_2.jpg", "caption": "Table 3: Comparison results on Motion-X [29] dataset. The evaluation metrics are computed using the encoder introduced in Appendix A. Empty columns of previous methods indicate that they can not handle the task. Instruction-tuned only T2M indicates the model that is initially pre-trained on multiple tasks, followed by instruction tuning solely on text-to-motion task.", "description": "This table compares the performance of M\u00b3GPT with other state-of-the-art models on the Motion-X dataset for four tasks: text-to-motion, motion-to-text, motion prediction, and motion in-between.  The metrics used for evaluation are R TOP1, FID, Div, R TOP3, Bleu@4, CIDEr, and MPJPE. The table highlights that M\u00b3GPT achieves competitive performance, particularly when fine-tuned on a single task, indicating the effectiveness of its multitask training approach.", "section": "4.3 Comparisons with State-of-the-arts"}, {"figure_path": "ODbTlAs0Oj/tables/tables_8_3.jpg", "caption": "Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music, respectively.", "description": "This table compares several state-of-the-art multimodal and multitask methods for motion comprehension and generation. It evaluates their performance across six core tasks: text-to-motion (T2M), motion-to-text (M2T), music-to-dance (A2D), dance-to-music (D2A), motion-to-motion (including prediction and in-between), and unconstrained generation of motion, text, and music.  The table helps to illustrate the capabilities and limitations of each approach in handling different modalities and task combinations.", "section": "Related Work"}, {"figure_path": "ODbTlAs0Oj/tables/tables_8_4.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table compares different training strategies for the M\u00b3GPT model on two tasks: Text-to-Motion and Music-to-Dance.  It shows the impact of jointly optimizing the Language Model (LLM) and motion detokenizer, and the effect of adding auxiliary tasks (Text-to-Dance and Music-to-Text) to enhance synergy between the tasks. The results are evaluated using standard metrics for each task, such as FID, Diversity, and R-Precision, allowing a direct comparison of various training methods. The table highlights that joint optimization and the inclusion of auxiliary tasks lead to significant improvements in performance.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music\u00b3, respectively.", "description": "This table compares several state-of-the-art multimodal and multitask methods in motion comprehension and generation.  It assesses their capabilities across six key tasks: text-to-motion (T2M), motion-to-text (M2T), music-to-dance (A2D), dance-to-music (D2A), motion-to-motion (M2M, including prediction and in-between), and unconstrained generation of motion, text, and music. The table helps to illustrate the relative strengths and weaknesses of each method in handling different types of multimodal motion tasks.", "section": "Related Work"}, {"figure_path": "ODbTlAs0Oj/tables/tables_16_1.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results to show the effectiveness of synergy learning and joint optimization of LLM and motion de-tokenizer. It compares the performance of different model variations on Text-to-Motion and Music-to-Dance tasks, highlighting the impact of adding auxiliary tasks (T2D, A2T) and joint optimization of the motion de-tokenizer.  The metrics used are R-TOP1, FID, Div for text-to-motion and FIDk, Divk, BAS for music-to-dance.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music\u00b3, respectively.", "description": "This table compares several existing multimodal and multitask methods for motion comprehension and generation, highlighting their capabilities across various tasks.  The tasks include text-to-motion (T2M), motion-to-text (M2T), music-to-dance (A2D), dance-to-music (D2A), motion-to-motion (including prediction and in-betweening - M2M), and unconstrained generation of motion, text, and music (Random M, Random T, Random A).  A checkmark indicates the method's ability to perform each specific task.", "section": "Related Work"}, {"figure_path": "ODbTlAs0Oj/tables/tables_17_2.jpg", "caption": "Table 3: Comparison results on Motion-X [29] dataset. The evaluation metrics are computed using the encoder introduced in Appendix A. Empty columns of previous methods indicate that they can not handle the task. Instruction-tuned only T2M indicates the model that is initially pre-trained on multiple tasks, followed by instruction tuning solely on text-to-motion task.", "description": "This table presents a comparison of various methods on the Motion-X dataset for several motion-related tasks.  It shows the performance of different methods across four core tasks: text-to-motion, motion-to-text, motion prediction, and motion in-between.  The metrics used for evaluation include R-Precision, FID, Diversity, Bleu, CIDEr, and BertScore, reflecting different aspects of motion generation and comprehension quality.  The \"Instruction-tuned only T2M\" column highlights the performance when the model is initially pre-trained on multiple tasks and then fine-tuned specifically for the text-to-motion task.", "section": "4.3 Comparisons with State-of-the-arts"}, {"figure_path": "ODbTlAs0Oj/tables/tables_17_3.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results, comparing different training strategies for the M\u00b3GPT model on two tasks: Text-to-Motion and Music-to-Dance. It shows the impact of joint optimization of the Language Model (LLM) and motion de-tokenizer, and the synergy gained by including auxiliary tasks (Text-to-Dance and Music-to-Text) in the training process.  The results are presented in terms of several metrics for each task, showing the performance improvements from various model configurations.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_17_4.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results focusing on two aspects: the effectiveness of joint optimization of LLM and motion de-tokenizer and the synergy learning of multitasks.  It compares the performance of different model variations across text-to-motion and music-to-dance tasks, highlighting the impact of joint optimization and the inclusion of auxiliary tasks (text-to-dance and music-to-text) on the overall results.  The metrics used include FID, diversity (Div), and R-precision for text-to-motion, and FIDk, diversity (Divk), and BAS for music-to-dance, reflecting both the quality and diversity of the generated motions/dances.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_18_1.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results, comparing different training methods and their effects on text-to-motion and music-to-dance tasks. It shows the impact of jointly optimizing the language model (LLM) and motion detokenizer, as well as the effect of including synergy learning via auxiliary tasks (text-to-dance and music-to-text). The results are evaluated using standard metrics for each task, and the comparison highlights the benefits of the proposed training strategies in enhancing both fidelity and diversity of generated motions and dances.", "section": "4.2 Ablation Studies"}, {"figure_path": "ODbTlAs0Oj/tables/tables_18_2.jpg", "caption": "Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X [29]) and Music-to-Dance (AIST++ [24]). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. Trained single task refers to a model trained and tested on a single task. Pre-trained and Instruction-tuned indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\u2191) indicate that higher values are better. The arrows (\u2193) indicate that smaller values are better. Bold indicates the best result.", "description": "This table presents the ablation study results to show the effectiveness of the synergy learning and joint optimization of LLM and motion de-tokenizer on two tasks: Text-to-Motion and Music-to-Dance.  It compares different model configurations, showing the performance impact of adding auxiliary tasks (T2D and A2T), re-optimizing the motion de-tokenizer, and the effects of pre-training and instruction-tuning stages.  The results, in terms of metrics like R-Precision, FID, and Diversity, highlight the impact of these choices on the accuracy and quality of motion and dance generation.", "section": "4.2 Ablation Studies"}]