[{"figure_path": "tRRWoa9e80/figures/figures_1_1.jpg", "caption": "Figure 1: Current state-of-the-art T2I models often struggle with semantic binding in generated images according to textual prompts. For example, hats and sunglasses are placed on incorrect objects. We introduce a novel method ToMe to address these challenges.", "description": "This figure shows the results of several state-of-the-art text-to-image models on prompts containing two subjects with related attributes.  The models struggle to correctly associate the attributes (hats and sunglasses) with the intended subjects (dog and cat).  The authors' proposed method, ToMe, is presented as a solution to this \"semantic binding\" problem.", "section": "1 Introduction"}, {"figure_path": "tRRWoa9e80/figures/figures_3_1.jpg", "caption": "Figure 2: We generate images with various input prompts in (a): \u201ca cat wearing sunglasses and a dog wearing a hat\u201d; the single-token embedding [dog]; the end token [EOT] . (b) After that, we compute the probability of containing \u201csunglasses\u201d in the generated images in subfigure.", "description": "This figure shows the results of generating images with different input prompts. (a) shows the images generated when using three different prompts: the full sentence, the [dog] token only, and the [EOT] token only. This illustrates how different parts of the prompt influence the generated image and the impact of the end-of-text token. (b) shows a bar chart visualizing the detection score probabilities of \"sunglasses\" within the generated images. The results show that using the full sentence achieves the highest detection score. This indicates that the full sentence provides the most complete information, leading to better object association in the generated images.", "section": "3.1 Text Embedding Analysis"}, {"figure_path": "tRRWoa9e80/figures/figures_4_1.jpg", "caption": "Figure 3: (a) Image generations with the property of token additivity. All images are generated by the prompt template \u201ca photo of a {object}.\u201d (b) PCA plot for additivity of text embeddings.", "description": "This figure demonstrates the semantic additivity property of CLIP text embeddings.  Part (a) shows image generations using prompts combining object and attribute tokens, illustrating how adding tokens for attributes results in the expected visual changes to the generated image. Part (b) is a PCA plot showing the vector representations of different text embeddings, highlighting the linear relationship between embeddings of words and their combinations, visually supporting the claim of semantic additivity.", "section": "3.1 Text Embedding Analysis"}, {"figure_path": "tRRWoa9e80/figures/figures_5_1.jpg", "caption": "Figure 4: ToMe is composed of two parts: one with Token Merging and end token substitution, and the other token updating part with two auxiliary losses for iterative composite token update.", "description": "This figure illustrates the ToMe (Token Merging) framework, which is a two-part process.  The first part involves token merging and end token substitution.  This merges relevant tokens (e.g., object and attributes) into a single composite token to ensure that all related information shares the same cross-attention map, improving semantic alignment. The second part involves iterative composite token updates using two auxiliary losses: an entropy loss to ensure diverse attention and a semantic binding loss to align the composite token's prediction with that of the original phrase.  This iterative update further refines the composite token embedding, leading to a more accurate generation.", "section": "3.2 ToMe: Token Merging"}, {"figure_path": "tRRWoa9e80/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparison among various T2I generation methods with complex prompts.", "description": "This figure compares the results of several text-to-image (T2I) generation methods on complex prompts involving multiple objects and attributes.  The goal is to show how well each method handles semantic binding \u2013 the task of correctly associating objects with their attributes and other related objects. The figure visually demonstrates the strengths and weaknesses of different models in accurately representing the relationships specified in the text prompts.  It highlights the superiority of the proposed ToMe method in achieving more accurate and faithful image generation compared to existing state-of-the-art methods such as SDXL, Playground v2, ELLA 1.5, Ranni 1.5, and SynGen XL.", "section": "4 Experiments"}, {"figure_path": "tRRWoa9e80/figures/figures_8_1.jpg", "caption": "Figure 4: ToMe is composed of two parts: one with Token Merging and end token substitution, and the other token updating part with two auxiliary losses for iterative composite token update.", "description": "This figure illustrates the two main components of the ToMe model.  The first part shows the token merging and end token substitution processes.  Relevant tokens are grouped into composite tokens to enhance semantic alignment. The second part depicts iterative updates of composite tokens, guided by two auxiliary loss functions (entropy loss and semantic binding loss) to refine the layout determination and generation integrity in the initial stages of text-to-image synthesis. ", "section": "3 Methods"}, {"figure_path": "tRRWoa9e80/figures/figures_8_2.jpg", "caption": "Figure 8: Additional applications of semantic additivity in text embedding.", "description": "This figure demonstrates the versatility of the Token Merging (ToMe) method by showcasing its application beyond semantic binding.  It shows three scenarios:\n\n1. **Add objects:**  Adding new objects to existing scenes, modifying the composition. An example here is adding glasses to an elephant.\n2. **Remove objects:** Removing objects from the scene, demonstrating control over image composition. For instance, a woman's earrings are removed.\n3. **Eliminate generation bias:** Reducing biases by removing stereotypical attributes from existing object depictions.  An illustration is modifying the depiction of a queen with cats and a nurse.", "section": "Methods"}, {"figure_path": "tRRWoa9e80/figures/figures_16_1.jpg", "caption": "Figure 9: Additional statistical analyses, all statistical values are averaged results from 100 images. (a) An example of DetScore visualization. (b) By fusing the dog and hat token, we obtain dog*, and the generated images often include a hat. The DetScore value for dog* is close to the DetScore value obtained using the complete prompt \u201ca dog wearing a hat\u201d. (c) We calculated the entropy of the cross-attention maps for each token and found that tokens positioned later in the sequence generally have higher entropy, indicating that their cross-attention maps are more dispersed.", "description": "This figure presents statistical analysis supporting the concept of token additivity in text embeddings.  Subfigure (a) shows example DetScore visualizations (probability of object detection). Subfigure (b) demonstrates that combining \"dog\" and \"hat\" embeddings (creating \"dog*\") results in a higher probability of detecting \"hat\" in generated images, similar to using the full phrase \"dog wearing hat\". Subfigure (c) displays entropy calculations for each token in a prompt, illustrating that tokens later in the sequence have higher entropy, suggesting a less focused cross-attention map.", "section": "3.1 Text Embedding Analysis"}, {"figure_path": "tRRWoa9e80/figures/figures_17_1.jpg", "caption": "Figure 4: ToMe is composed of two parts: one with Token Merging and end token substitution, and the other token updating part with two auxiliary losses for iterative composite token update.", "description": "This figure illustrates the Token Merging (ToMe) method proposed in the paper. ToMe consists of two main parts: (a) Token Merging and End Token Substitution and (b) Iterative Composite Token Update. Part (a) shows how relevant tokens are aggregated into a single composite token to ensure semantic alignment.  Part (b) illustrates the use of two auxiliary losses (entropy loss and semantic binding loss) to refine the composite token embeddings in the initial stages of image generation, improving overall generation integrity. The figure uses a visual representation to explain the process, detailing the flow of information and the key components.", "section": "3 Methods"}, {"figure_path": "tRRWoa9e80/figures/figures_17_2.jpg", "caption": "Figure 7: Cross-Attention maps visualization with various configurations.", "description": "This figure visualizes the cross-attention maps generated by different configurations of the proposed method ToMe in comparison with the baseline SDXL model. The input prompt is \"a cat wearing sunglasses and a dog wearing hat\". Config A shows the baseline SDXL model, which demonstrates attribute binding errors due to the misalignment of cross-attention maps. Config B shows the results of ToMe with token merging, indicating an improvement in alignment. Config E shows the result with the entropy loss, further concentrating the focus. Config C, incorporating both token merging and entropy loss, achieves the best semantic binding performance.", "section": "3.2 ToMe: Token Merging"}, {"figure_path": "tRRWoa9e80/figures/figures_18_1.jpg", "caption": "Figure 12: Ablation study of our proposed end token substitution (ETS) technique, with the input prompt \u201ca boy wearing hat and a dog wearing sunglasses\u201d", "description": "This figure demonstrates the effect of the proposed End Token Substitution (ETS) technique on the quality of image generation.  The leftmost image shows the output of SDXL (a standard model) which struggles with the semantic binding issue in the prompt. The middle images show the results of the proposed ToMe method *without* ETS. Even without ETS, ToMe improves the image quality compared to SDXL.  Finally, the rightmost image shows the results of the proposed ToMe method *with* ETS. ETS addresses potential issues caused by the end token [EOT] that might interfere with correct attribute assignment, further improving the image quality.", "section": "3.2 ToMe: Token Merging"}, {"figure_path": "tRRWoa9e80/figures/figures_18_2.jpg", "caption": "Figure 4: ToMe is composed of two parts: one with Token Merging and end token substitution, and the other token updating part with two auxiliary losses for iterative composite token update.", "description": "This figure illustrates the Token Merging (ToMe) framework, which is the core of the proposed method.  It shows two main components: (a) Token Merging and End Token Substitution, which combines relevant tokens into composite tokens to ensure semantic alignment; and (b) Iterative Composite Token Update, which refines these composite tokens using an entropy loss and a semantic binding loss to improve generation integrity. The diagram visually represents the process of creating composite tokens from individual tokens in a sentence and the iterative update of these composite tokens. ", "section": "3.2 ToMe: Token Merging"}, {"figure_path": "tRRWoa9e80/figures/figures_19_1.jpg", "caption": "Figure 14: Additional semantic binding results. Our method not only achieves good results in object binding but is also effective for composite binding of objects and their adjective attributes.", "description": "This figure shows several examples of images generated by both SDXL and the proposed ToMe method. Each row shows a different prompt, demonstrating the method's ability to handle complex scenarios involving multiple objects and their attributes.  The results highlight ToMe's improved performance in accurately binding objects to their attributes and in correctly representing the relationships between multiple objects within a scene.", "section": "Additional Results"}, {"figure_path": "tRRWoa9e80/figures/figures_20_1.jpg", "caption": "Figure 15: User study with 20 participants, we ask users to rate the semantic binding into four levels.", "description": "This figure displays the results of a user study assessing the semantic binding capabilities of different text-to-image generation methods. Twenty participants evaluated the quality of image generation using four levels (1-4), with Level 1 representing perfect semantic binding and Level 4 representing images with significant errors. The pie charts show the distribution of scores across the four levels for different methods: SDXL, ToMe (the proposed method), SynGen, ELLA, and Ranni.  The study demonstrates ToMe's superior performance in achieving accurate semantic binding compared to other approaches.", "section": "Additional Results"}]