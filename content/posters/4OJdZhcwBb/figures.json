[{"figure_path": "4OJdZhcwBb/figures/figures_1_1.jpg", "caption": "Figure 1: A count of hyperparameters for different reinforcement learning algorithms proposed over the last decade. We include value-based, policy-gradient, and model-based methods. The counts do not include hyperparameters controlling the network architectures, such as number of layers, activation functions, etc. See Appendix B for details on how hyperparameters were counted.", "description": "This figure shows the number of hyperparameters used in several reinforcement learning algorithms over time.  It demonstrates the significant increase in the number of hyperparameters from early algorithms like DQN to more recent ones like DreamerV3. The figure highlights the increasing complexity of modern reinforcement learning algorithms and the growing challenge of hyperparameter tuning.", "section": "1 Introduction"}, {"figure_path": "4OJdZhcwBb/figures/figures_3_1.jpg", "caption": "Figure 2: Left: The distributions of performance (AUC) over 625 hyperparameter settings for the PPO algorithm in Swimmer and Halfcheetah Brax environments. Right: The same distributions after applying score normalization. Each data point is the mean AUC across runs. Each run consisted of 3M steps of agent-environment interaction.", "description": "This figure shows the distribution of performance (AUC) for 625 hyperparameter settings in two environments, Swimmer and Halfcheetah, before and after normalization. The left panel shows the raw AUC distributions which differ significantly in scale and range. The right panel shows the distributions after a percentile-based normalization, making them comparable across environments.", "section": "3 Hyperparameter Sensitivity"}, {"figure_path": "4OJdZhcwBb/figures/figures_4_1.jpg", "caption": "Figure 3: The distributions of environment normalized scores for 625 hyperparameter settings of the PPO algorithm in the Swimmer and Halfcheetah environments. The red stars indicate the normalized environment scores of a hyperparameter setting, which does well in Halfcheetah but poorly in Swimmer. The blue stars indicate the normalized scores of the hyperparameter setting, which maximizes the mean of the normalized environment scores across both environments.", "description": "This figure shows the distributions of environment-normalized scores for 625 different hyperparameter settings of the PPO algorithm across two environments: Swimmer and Halfcheetah.  It highlights the variability in performance across environments, even with the same algorithm and similar hyperparameter settings. The red stars represent a setting that performs exceptionally well in Halfcheetah but poorly in Swimmer, illustrating the challenge of finding universally good hyperparameter settings. In contrast, the blue stars represent a setting that achieves good performance in both environments, indicating a more robust configuration.", "section": "3.1 Sensitivity Metric"}, {"figure_path": "4OJdZhcwBb/figures/figures_5_1.jpg", "caption": "Figure 4: The performance-sensitivity plane for algorithmic evaluation. The center point indicates the hyperparameter sensitivity and performance of a reference point algorithm. The x-axis is the hyperparameter sensitivity metric as defined in equation 2. The y-axis is the per-environment tuned score (first term in equation 2). The diagonal line is the identity line shifted to intersect the reference point algorithm. The plane is then divided into 5 shaded regions that represent spaces of algorithms of varying qualities relative to the baseline.", "description": "This figure shows a two-dimensional plane used for evaluating reinforcement learning algorithms. The x-axis represents the hyperparameter sensitivity, indicating how much an algorithm's performance relies on per-environment hyperparameter tuning. The y-axis represents the per-environment tuned score, showing the average normalized performance when hyperparameters are tuned for each environment. The plane is divided into five regions, each representing a different combination of performance and sensitivity.  Algorithms in the top-left quadrant (Region 1) are ideal, showing high performance and low sensitivity. Regions 2, 3, and 4 represent trade-offs between performance and sensitivity, while Region 5 shows low performance and high sensitivity, indicating less desirable algorithms.", "section": "3.2 Sensitivity Analysis"}, {"figure_path": "4OJdZhcwBb/figures/figures_7_1.jpg", "caption": "Figure 5: Performance-sensitivity plane with unnormalized PPO as the center reference point. Variants of PPO plotted. The x-axis indicates hyperparameter sensitivity as defined in equation 2. The y-axis represents the per-environment tuned score (first term in the sensitivity calculation of equation 2). Hyperparameter sensitivity and per-environment tuned score metrics were computed from a 200 run sweep of 625 hyperparameter settings across 5 Brax Mujoco environments (Ant, Halfcheetah, Hopper, Swimmer, and Walker2d). Error bars show the endpoints of 10,000 sample 95% bootstrap confidence intervals around both the performance and hyperparameter sensitivity metrics (two dimensions).", "description": "This figure shows the performance-sensitivity plane, a visualization tool to analyze the relationship between an algorithm's performance and its sensitivity to hyperparameter tuning across different environments. The x-axis represents hyperparameter sensitivity, while the y-axis shows the per-environment tuned performance. Each point on the plot represents a variant of the PPO algorithm, with error bars indicating 95% confidence intervals. This visualization helps to understand how different normalization variants affect both the performance and sensitivity of PPO.", "section": "4 Sensitivity Experiments"}, {"figure_path": "4OJdZhcwBb/figures/figures_8_1.jpg", "caption": "Figure 6: Normalized performance scores as a function of the number of hyperparameters tuned per environment. The subplots compare PPO to the PPO variants studied. The x-axis indicates the size of the subset of hyperparameters being tuned. The y-axis is the average normalized score across the environment distribution. Each dot indicates the normalized score obtained by tuning the most performant subset of hyperparameters of each size. The curve is an interpolation between the dots. The dashed line indicates the point at which the curve reaches 95% of peak performance. LB is an abbreviation for lower bounded, zm is an abbreviation for zero-mean.", "description": "This figure shows the relationship between the number of hyperparameters tuned and the normalized performance scores for different variants of the PPO algorithm.  Each subplot represents a different PPO variant, and shows how performance improves as more hyperparameters are tuned. The dashed line in each subplot indicates the point where tuning the hyperparameters yields 95% of peak performance. This helps to quantify the *effective hyperparameter dimensionality*, showing how many hyperparameters really need tuning to achieve most of the performance gain.", "section": "5 Effective Hyperparameter Dimensionality"}, {"figure_path": "4OJdZhcwBb/figures/figures_13_1.jpg", "caption": "Figure 7: Performance-sensitivity planes shown are formed by leaving out each of the five environments. Error bars are 95% confidence intervals obtained from 1000 sample bootstraps.", "description": "This figure shows five performance-sensitivity plots. Each plot is generated by removing one of the five environments (Ant, Halfcheetah, Hopper, Swimmer, Walker2d) from the original dataset used to create the main performance-sensitivity plot (Figure 5 in the paper). The x-axis represents the hyperparameter sensitivity, and the y-axis represents the per-environment tuned performance. Each point in the plots represents a specific variant of the PPO algorithm, with error bars indicating the 95% confidence intervals. By comparing these plots to the main performance-sensitivity plot, one can analyze how the removal of a specific environment affects the overall sensitivity and performance of the different PPO variants. This analysis helps to understand the robustness of the algorithms across different environments and how much reliance they have on hyperparameter tuning in each environment.", "section": "Sensitivity Experiments"}, {"figure_path": "4OJdZhcwBb/figures/figures_13_2.jpg", "caption": "Figure 5: Performance-sensitivity plane with unnormalized PPO as the center reference point. Variants of PPO plotted. The x-axis indicates hyperparameter sensitivity as defined in equation 2. The y-axis represents the per-environment tuned score (first term in the sensitivity calculation of equation 2). Hyperparameter sensitivity and per-environment tuned score metrics were computed from a 200 run sweep of 625 hyperparameter settings across 5 Brax Mujoco environments (Ant, Halfcheetah, Hopper, Swimmer, and Walker2d). Error bars show the endpoints of 10,000 sample 95% bootstrap confidence intervals around both the performance and hyperparameter sensitivity metrics (two dimensions).", "description": "This figure shows the performance-sensitivity plane for different variants of the Proximal Policy Optimization (PPO) algorithm.  The x-axis represents the hyperparameter sensitivity, measuring how much an algorithm's performance relies on per-environment hyperparameter tuning. The y-axis represents the per-environment tuned performance. Each point represents a PPO variant, and the error bars indicate 95% confidence intervals based on 10,000 bootstrap samples. The plot helps visualize the trade-off between performance and sensitivity for various normalization techniques used in PPO.", "section": "4 Sensitivity Experiments"}]