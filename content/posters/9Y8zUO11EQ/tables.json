[{"figure_path": "9Y8zUO11EQ/tables/tables_3_1.jpg", "caption": "Table 1: Characterization of different attributes of SWT-BENCH instance.", "description": "This table provides a statistical overview of the SWT-BENCH dataset, showing the distribution and range of key characteristics.  It includes information about the issue text (number of words), codebase size (number of files and lines of code), existing tests (number of fail-to-pass, fail-to-fail, pass-to-pass, pass-to-fail tests, total number of tests, and code coverage), and golden tests (number of fail-to-pass, pass-to-pass tests added, removed, and number of files and lines of code edited).  These statistics help in understanding the complexity and scale of the benchmark dataset and provide context for evaluating the performance of different test generation methods.", "section": "3.2 Benchmark Overview"}, {"figure_path": "9Y8zUO11EQ/tables/tables_7_1.jpg", "caption": "Table 2: Rate of well-formed patches (W), successful tests (S), potentially reproducing initially failing tests (F\u2192\u00d7), reproducing fail-to-pass tests (F \u2192 P), and correct but unhelpful pass-to-pass tests (P \u2192 P), in %.", "description": "This table presents the performance of different test generation methods on the SWT-BENCH dataset.  The metrics evaluated are: \n- **W (Patch Well-Formedness):** Percentage of instances where a well-formed patch was generated.\n- **S (Success Rate):** Percentage of instances where the generated tests successfully reproduced the issue.\n- **F\u2192\u00d7 (Fail-to-Any):** Percentage of instances where at least one test failed before the patch and transitioned to any state after the patch was applied.\n- **F\u2192P (Fail-to-Pass):** Percentage of instances where at least one test failed before the patch and passed after.\n- **P\u2192P (Pass-to-Pass):** Percentage of instances where a test passed both before and after the patch application.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_7_2.jpg", "caption": "Table 3: Change Coverage AC [%] as defined in \u00a73.3 aggregated over all instances, S instances and non S instances (\u00acS).", "description": "This table presents the change coverage (AC) of generated tests, categorized by whether the tests successfully reproduced the issue (S) or not (\u00acS).  The change coverage metric measures the proportion of executable lines in the code patch that are covered by the generated tests.  The table shows the overall change coverage (ACall), the coverage for successful instances (ACS), and the coverage for unsuccessful instances (AC\u00acS) for each method.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_7_3.jpg", "caption": "Table 4: Comparison of different underlying LLMs for SWE-AGENT, all in %. ", "description": "This table presents the performance comparison of different Large Language Models (LLMs) when used as the underlying model for SWE-AGENT.  It shows the impact of the LLM choice on the success rate (S), well-formedness of the generated patch (W), the rate of potentially reproducing initially failing tests (F\u2192x), and the change coverage (\u0394C). The results demonstrate the sensitivity of SWE-AGENT's performance to the choice of LLM.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_7_4.jpg", "caption": "Table 5: Performance of ZEROSHOTPLUS, given the test file to change, none (-), the golden (\u2714) or an incorrect (\u2717) code patch, and the files retrieved via BM-25 (r), or modified by the golden (\u2714) or incorrect patch (\u2717).", "description": "This table presents the results of the ZEROSHOTPLUS method under different conditions. It shows the impact of providing different information (test files, golden patches, incorrect patches, and files retrieved using BM25) to the model on the success rate (S), well-formedness of the patch (W), the rate of tests that fail on the original codebase but pass after the patch is applied (F\u2192P), and the change coverage (\u0394C). The results indicate that providing the test files to change has a significant impact on the performance of the model.  Using a golden or incorrect patch with the files changed by the patch improved performance.", "section": "5.3 Code Repair and Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_8_1.jpg", "caption": "Table 6: Overlap in solved instances of SWE-BENCH and SWT-BENCH.", "description": "This table shows the overlap between instances solved by SWE-BENCH and SWT-BENCH for two different methods: ZEROSHOTPLUS and SWE-AGENT.  It demonstrates the low correlation between success on the two benchmarks. The p-values indicate there's no statistical significance in the correlation.", "section": "5.3 Code Repair and Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_8_2.jpg", "caption": "Table 7: Performance of ZEROSHOTPLUS on PRs before/after GPT-4 knowledge cutoff (KC = 30th April 2023) in %.", "description": "This table presents the performance of the ZEROSHOTPLUS method on pull requests (PRs) created before and after the GPT-4 knowledge cutoff date (KC).  It shows the success rate (S), the proportion of well-formed patches (W),  the rate of tests that initially failed and still failed after the patch (F\u2192F), the rate of tests that initially failed and passed after the patch (F\u2192P), the rate of tests that initially passed and still passed after the patch (P\u2192P), and the average number of API calls (\u0394Call) for each category. The data is used to investigate the impact of data contamination on the performance of the model.", "section": "5.4 Test Generation Success and Instance Characteristics"}, {"figure_path": "9Y8zUO11EQ/tables/tables_14_1.jpg", "caption": "Table 8: Comparison of ZEROSHOTPLUS for different T on GPT-4 (95% CI, n = 25).", "description": "This table presents the ablation study on the temperature parameter T for the ZEROSHOTPLUS method using GPT-4.  It shows the impact of varying the temperature on several key metrics:  the rate of well-formed patches (W), the success rate (S), the rate of potentially reproducing initially failing tests (F\u2192\u00d7), the rate of reproducing fail-to-pass tests (F\u2192P), the rate of correct but unhelpful pass-to-pass tests (P\u2192P), and the average change coverage (\u0394C). The results are reported with 95% confidence intervals (CI) based on 25 samples (n=25).  The table helps understand the effect of temperature on the model's performance and stability.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_15_1.jpg", "caption": "Table 9: Cost of different LLMs running SWE-AGENT on SWT-BENCH Lite in USD", "description": "This table presents the cost of using different large language models (LLMs) with the SWE-AGENT on the SWT-BENCH Lite dataset.  It shows the monetary cost associated with each LLM, providing a comparison of the financial resources required for different model choices in the experimental setup.", "section": "5.1 Experimental Setup"}, {"figure_path": "9Y8zUO11EQ/tables/tables_15_2.jpg", "caption": "Table 10: Cost of running different methods on SWT-BENCH Lite using GPT-4 in USD", "description": "This table presents the cost, in USD, of running different test generation methods on the SWT-BENCH Lite benchmark using the GPT-4 language model.  The methods compared include various zero-shot prompting techniques (ZEROSHOT, ZEROSHOTPLUS, PASS@5), the state-of-the-art LIBRO method, and three code agent approaches (AIDER, AUTOCODEROVER, SWE-AGENT, SWE-AGENT+). The costs reflect the expenses incurred by using the language model for each method.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/tables/tables_16_1.jpg", "caption": "Table 11: Average execution time t per instance", "description": "This table presents the average execution time for different test generation methods.  The methods include ZEROSHOTPLUS, LIBRO, SWE-AGENT, SWE-AGENT+, and AUTOCODEROVER. The execution times are measured in seconds (s) and minutes and seconds (m:ss).  The table shows that zero-shot methods are much faster than agent-based methods, and that agent-based methods have execution times on the order of several minutes.", "section": "5.1 Experimental Setup"}]