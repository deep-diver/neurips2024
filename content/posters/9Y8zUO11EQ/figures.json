[{"figure_path": "9Y8zUO11EQ/figures/figures_1_1.jpg", "caption": "Figure 1: Evaluation of an SWT-BENCH instance. Given an issue description in natural language and the corresponding codebase, the task is to generate tests that reproduce the issue. We considered a test to reproduce the issue if it fails on the codebase before the pull request (PR) is accepted, i.e., before the golden patch is applied, but passes after. We call this a fail-to-pass test (F \u2192 P).", "description": "This figure illustrates how SWT-BENCH evaluates generated tests.  An SWT-BENCH instance consists of a natural language description of a bug and the codebase before the bug fix (Pre PR).  The goal is to generate tests that will fail on the Pre PR codebase (because they expose the bug), but pass after the bug fix is applied (Post PR). A successful test is labeled 'F \u2192 P' (fail-to-pass), indicating that it identified the bug.", "section": "A Benchmark for Test Generation"}, {"figure_path": "9Y8zUO11EQ/figures/figures_3_1.jpg", "caption": "Figure 2: Distribution of SWT-BENCH instances over GitHub repositories.", "description": "This pie chart shows the distribution of the 1983 instances of SWT-BENCH across 12 different GitHub repositories.  The largest portion belongs to the 'django' repository, indicating a higher concentration of Python projects using this framework in the dataset. The other repositories represent a variety of popular Python projects, each contributing a varying number of instances to the benchmark. This visualization helps to understand the diversity of the codebases included in SWT-BENCH and their relative representation in the dataset.", "section": "3.2 Benchmark Overview"}, {"figure_path": "9Y8zUO11EQ/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of change coverage AC of the generated tests T, given the original code base R, the golden patch X*, and the golden tests T*. ", "description": "This figure illustrates how change coverage (AC) is calculated for generated tests. It shows the original codebase (R), the codebase after applying the golden patch (R o X*), and the generated tests (T). The golden tests are represented by T*. The yellow lines indicate lines added by the patch, while the pink lines indicate lines removed or modified. The change coverage (AC) is calculated as the ratio of lines in the patch that are executed by the generated tests to the total number of lines in the patch.", "section": "3.3 Metrics"}, {"figure_path": "9Y8zUO11EQ/figures/figures_5_1.jpg", "caption": "Figure 4: Comparison of the default unified diff format (left) and our fault-tolerant version (right).", "description": "This figure compares the standard unified diff format used in Git with a fault-tolerant version proposed in the paper.  The standard format is very sensitive to small errors and requires precise line numbers and verbatim code snippets. The fault-tolerant format is designed to be more robust to errors introduced by large language models and allows for entire functions or classes to be inserted, replaced, or deleted, making it easier for the models to generate correct patches.", "section": "4.2 A Code Diff Format for Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/figures/figures_8_1.jpg", "caption": "Figure 5: Distribution of success rate (S) across issue description lengths in # tokens", "description": "This figure shows the relationship between the length of issue descriptions (measured in the number of tokens) and the success rate (S) of different test generation methods.  It reveals a general trend where longer descriptions lead to higher success rates, although the improvement plateaus for very long descriptions. This suggests that more information in longer descriptions aids the test generation process, but excessively long descriptions might contain irrelevant information that hinders performance.  The chart highlights the relative performance of different methods across various description length ranges.", "section": "5.4 Test Generation Success and Instance Characteristics"}, {"figure_path": "9Y8zUO11EQ/figures/figures_9_1.jpg", "caption": "Figure 6: Overlap in instances solved by the four best performing methods.", "description": "This figure shows a Venn diagram illustrating the overlap in the number of instances successfully solved by four different test generation methods: LIBRO, AutoCodeRover, Aider, and SWE-Agent+.  It highlights the complementary nature of these methods, demonstrating that combining them results in a higher number of solved instances compared to using any single method alone. The numbers within each section of the Venn diagram indicate the number of instances uniquely or commonly solved by the corresponding methods.", "section": "Method Complimentarity"}, {"figure_path": "9Y8zUO11EQ/figures/figures_14_1.jpg", "caption": "Figure 8: Ablation on the number of samples and API calls for LIBRO and code agents resp.", "description": "This figure shows the results of ablation studies on the number of samples and API calls used in the LIBRO and code agent methods for automatic test generation.  The left plot shows the effect of varying the number of LIBRO samples on the well-formedness (W) and success rate (S) of generated tests. The right plot illustrates the impact of the number of API calls made by code agents on these metrics. Both plots show that increasing samples and API calls initially improves performance, but this improvement saturates after a certain point.  The plots highlight the trade-off between computational cost (number of samples/API calls) and test generation performance.", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/figures/figures_14_2.jpg", "caption": "Figure 8: Ablation on the number of samples and API calls for LIBRO and code agents resp.", "description": "This figure shows the ablation study results on the number of samples and API calls for different test generation methods. The left subplot shows the well-formedness (W) and success rate (S) for LIBRO with varying numbers of samples.  The right subplot shows W and S for SWE-AGENT, SWE-AGENT+, AUTOCODEROVER, and AIDER with varying numbers of API calls. The plots illustrate the impact of these hyperparameters on the performance of each method.  The horizontal dashed lines are reference lines showing the performance at the default hyperparameter values (5 samples for LIBRO and 20 API calls for code agents).", "section": "5.2 Automatic Test Generation"}, {"figure_path": "9Y8zUO11EQ/figures/figures_14_3.jpg", "caption": "Figure 2: Distribution of SWT-BENCH instances over GitHub repositories.", "description": "This figure shows the distribution of the 1983 instances in the SWT-BENCH dataset across different GitHub repositories.  The size of each pie slice corresponds to the number of instances from that repository.  It illustrates the variety and complexity of real-world projects represented in the benchmark.", "section": "3.2 Benchmark Overview"}, {"figure_path": "9Y8zUO11EQ/figures/figures_22_1.jpg", "caption": "Figure 3: Illustration of change coverage AC of the generated tests T, given the original code base R, the golden patch X*, and the golden tests T*. ", "description": "This figure illustrates how change coverage (AC) is calculated.  AC measures the portion of the codebase modified by the golden patch that is covered by the generated tests.  It shows the original code (R), the golden patch (X*), and the golden tests (T*). It highlights that the coverage calculation includes both lines of code added and lines of code removed (or modified) in the patch, and considers only the lines executed by either the original tests (S) or the golden tests (T*) on both the original and patched codebase.", "section": "3.3 Metrics"}]