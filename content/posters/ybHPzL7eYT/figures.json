[{"figure_path": "ybHPzL7eYT/figures/figures_0_1.jpg", "caption": "Figure 1: Large Spatial Model takes two unposed images as input and reconstructs an explicit radiance field, capturing geometry, appearance, and semantics in real time. This yields high performance in versatile tasks such as view synthesis, depth prediction, and open-vocabulary 3D segmentation.", "description": "This figure illustrates the Large Spatial Model (LSM) pipeline.  Two unposed images are fed into the LSM, which then generates a 3D radiance field in 0.1 seconds. This radiance field represents the scene's geometry, appearance, and semantics. From this radiance field, the LSM can produce a variety of outputs, including novel views (NVS), depth maps, and semantic segmentations at a speed exceeding 100 FPS. This demonstrates the model's efficiency and versatility in performing various 3D vision tasks simultaneously.", "section": "Abstract"}, {"figure_path": "ybHPzL7eYT/figures/figures_3_1.jpg", "caption": "Figure 2: Network Architecture. Our method utilizes input images from which pixel-aligned point maps are regressed using a generic Transformer. A set of semantic anitrosopic 3D Gaussians incorporating geometry, appearance, and semantics are then predicted employing another point-based Transformer that facilitates local context aggregation and hierarchical fusion. It is supervised end-to-end, minimizing the loss function through comparisons against ground truth and rasterized label maps on new views. During the inference stage, our approach is capable of predicting the scene representation without requiring camera parameters, enabling real-time semantic 3D reconstruction.", "description": "This figure illustrates the architecture of the Large Spatial Model (LSM).  It shows the process of taking input images, performing dense geometry prediction using a Transformer, aggregating features at the point level with cross-view and cross-modal attention, and finally using a rasterizer to produce a semantic 3D reconstruction. The model is trained end-to-end, generating pixel-aligned point maps and semantic anisotropic 3D Gaussians, allowing for real-time reconstruction without the need for camera parameters at inference time.", "section": "3 Methods"}, {"figure_path": "ybHPzL7eYT/figures/figures_6_1.jpg", "caption": "Figure 3: Visualization of the 3D Feature Field. We present examples of features rendered from novel viewpoints, illustrating how our method converts 2D features into a consistent 3D, facilitating versatile and efficient segmentation. Visualizations are generated using PCA [59].", "description": "This figure shows the visualization of 3D feature fields rendered from novel viewpoints using PCA. It demonstrates how the model converts 2D image features into a consistent 3D representation, which enables versatile and efficient semantic segmentation.", "section": "4.2 Semantic 3D Reconstruction"}, {"figure_path": "ybHPzL7eYT/figures/figures_7_1.jpg", "caption": "Figure 4: Novel-View Synthesis (NVS) Comparisons. We evaluate scene-level reconstruction by comparing our method to approaches that require per-scene optimization, such as NeRF-DFF and Feature-3DGS, which predicts both RGB and segmentation, and the generalizable 3D Gaussian Splatting method (pixelSplat). Notably, these methods require a pre-processing step to obtain camera poses using off-the-shelf SfM. Through end-to-end, data-driven training, our method achieves comparable visual quality to these approaches while reconstructing the 3D radiance field in a single feed-forward pass.", "description": "This figure compares the novel view synthesis results of the proposed Large Spatial Model (LSM) with three other state-of-the-art methods: NeRF-DFF, Feature-3DGS, and pixelSplat.  The comparison highlights that LSM achieves comparable visual quality to the other methods, but without requiring a pre-processing step (Structure from Motion) to estimate camera poses, showing the efficiency of the LSM approach.", "section": "4.2 Semantic 3D Reconstruction"}, {"figure_path": "ybHPzL7eYT/figures/figures_8_1.jpg", "caption": "Figure 5: Language-based 3D Segmentation Comparison. We visualize the segmentation results across four unseen scenes and observe that our method performs comparably to NeRF-DFF and Feature-3DGS. This indicates that LSM effectively lifts 2D feature maps into high-quality 3D feature fields.", "description": "This figure compares the performance of the proposed Large Spatial Model (LSM) against other state-of-the-art methods for language-based 3D semantic segmentation on four unseen scenes.  It visually demonstrates the model's ability to produce accurate and detailed 3D semantic segmentations comparable to other top-performing techniques, highlighting its capacity to effectively translate 2D features into high-quality 3D feature fields for improved semantic understanding.", "section": "4.2 Semantic 3D Reconstruction"}]