[{"figure_path": "ybHPzL7eYT/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative Comparison in 3D Tasks. We report novel-view synthesis, depth estimation quality, and open-vocabulary segmentation accuracy. Our method eliminates the need for any preprocessing in 3D tasks, while achieving performance comparable to other baselines that rely on SfM to obtain camera parameters and poses.", "description": "This table presents a quantitative comparison of the Large Spatial Model (LSM) against other state-of-the-art methods on three key 3D vision tasks: novel-view synthesis, depth estimation, and open-vocabulary segmentation.  It highlights LSM's ability to achieve comparable or better performance without requiring any preprocessing steps such as Structure-from-Motion (SfM), which other methods rely on.  The metrics used include mIoU, accuracy, relative depth error, PSNR, SSIM, and LPIPS.", "section": "4 Experiments"}, {"figure_path": "ybHPzL7eYT/tables/tables_7_1.jpg", "caption": "Table 2: Ablation Study on Our Design Choices. We refer to the model that integrates cross-view attention for multi-view geometry with point-wise aggregation for future refinement as the baseline configuration (Exp #1). Implementing cross-modal attention to fuse geometry encoder features enhances both the rendering quality of new views and the segmentation accuracy (Exp #2). Additionally, incorporating features from frozen 2D semantic backbone into the fusion process (Exp #3) for consistent feature field amalgamation, and multi-scale fusion enhances hierarchical information flow (Exp #4), substantially improving language-based semantic 3D segmentation. Segmentation metrics use LSeg results as ground-truth in this table.", "description": "This table presents the results of an ablation study evaluating the impact of different design choices on the Large Spatial Model (LSM).  The study investigates the contributions of cross-view attention, cross-modal attention (fusing geometry and semantic features), and multi-scale fusion to the model's performance on semantic segmentation and novel view synthesis.  The baseline model (Exp #1) uses cross-view attention and point-wise aggregation. Subsequent experiments add cross-modal attention and multi-scale fusion, demonstrating their positive effects on model accuracy.", "section": "4.3 Ablation Studies"}, {"figure_path": "ybHPzL7eYT/tables/tables_8_1.jpg", "caption": "Table 3: Inference Time per Module. Breakdown of inference time for each module for analysis.", "description": "This table breaks down the inference time for each module of the Large Spatial Model (LSM).  It shows the time taken for Dense Geometry Prediction, Point-wise Aggregation, and Feature Lifting, as well as the total inference time. This information helps in understanding the computational cost of each module within the overall system.", "section": "4.3 Ablation Studies"}, {"figure_path": "ybHPzL7eYT/tables/tables_9_1.jpg", "caption": "Table 4: Performance Comparison on Replica Dataset. LSM operates without ground-truth camera parameters, achieving decent PSNR and low relative depth error, while also enabling semantic understanding within a unified framework.", "description": "This table compares the performance of LSM with other methods (pixelSplat and Splatter Image) on the Replica dataset, focusing on metrics such as mIoU (mean Intersection over Union), rel (relative depth error), and PSNR (Peak Signal-to-Noise Ratio).  A key aspect highlighted is that LSM does not require offline Structure-from-Motion (SfM) for camera parameter estimation, unlike the other methods. The table demonstrates LSM's ability to achieve competitive results without this preprocessing step, emphasizing its efficiency and generalizability.", "section": "4 Experiments"}]