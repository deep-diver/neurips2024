[{"figure_path": "d1XrZ4EINV/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline of letting LLM generate code and self-debug.", "description": "This figure illustrates the process of an LLM generating code and then self-debugging it.  A user provides a programming task description to the LLM, which generates an initial code solution. This solution is then executed against unit tests. If it passes, the process ends.  If it fails, the LLM is queried again, either directly for a refined solution or for a code explanation and subsequent refinement. This iterative process continues until the LLM produces a correct solution or the maximum number of iterations is reached.", "section": "1 Introduction"}, {"figure_path": "d1XrZ4EINV/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the overall workflow of the LEDEX framework. It starts with data collection, where wrong solutions are generated by LLMs and then verified for correctness. The correct explanations and refinements are collected and used for supervised fine-tuning. Finally, reinforcement learning is used to further improve the quality of the generated code explanations and refinements. ", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_3_1.jpg", "caption": "Figure 3: The CodeBLEU scores, unit test cases passing rate, sentiment similarity of wrong code explanations, final refinement code reward, and the explanation reward of the training data.", "description": "This figure shows the distributions of various metrics used in the reward design of the reinforcement learning (RL) component in LEDEX.  Specifically, it presents histograms visualizing the CodeBLEU scores (code similarity), unit test passing rates (execution success), sentiment similarity scores of code explanations (explanation quality), the final refinement code reward, and the explanation reward.  Each histogram allows comparison of these scores for correct versus incorrect code explanations and refinements.", "section": "2.3 Reinforcement learning"}, {"figure_path": "d1XrZ4EINV/figures/figures_6_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure presents a flowchart illustrating the LEDEX framework's four main stages: (1) Data Collection, where code explanations and refinements are generated and filtered; (2) Data Verification, where the quality of collected data is checked; (3) Supervised Fine-Tuning (SFT), where the model is trained on the verified dataset; and (4) Reinforcement Learning (RL) Training, where the model's performance is further improved using reinforcement learning techniques.  Each stage is depicted with corresponding actions and data flows, clarifying the process of collecting and using high-quality data to train LLMs for self-debugging.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_14_1.jpg", "caption": "Figure 1: Pipeline of letting LLM generate code and self-debug.", "description": "This figure shows a pipeline illustrating how an LLM generates code, and then self-debugs it. First, a user provides a programming task description to the LLM. The LLM generates a solution (code), which is then verified through execution against unit tests. If the code fails the tests, feedback is collected. The feedback informs a new query prompting the LLM to refine the code. This process may iterate until the LLM produces a correct solution or a maximum number of iterations is reached.  Two prompt designs are highlighted. One is asking directly for code refinement; another is a chain-of-thought approach that first requests an explanation of the error in the wrong code, before asking for refinement.", "section": "1 Introduction"}, {"figure_path": "d1XrZ4EINV/figures/figures_14_2.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the pipeline of the LEDEX framework. It starts with data collection, where wrong solutions are generated and verified.  The verified data is then used for supervised fine-tuning and reinforcement learning to train the LLMs. The trained LLMs generate code explanations and refinements. Finally, the model is evaluated on various benchmarks.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_15_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the overall workflow of the LEDEX framework. It starts with data collection, where wrong code solutions are generated by LLMs, and then these are filtered and verified. High-quality data is then used for supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance LLMs' self-debugging capabilities. The final output is LLMs that can better self-debug and explain code.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the overview of the LEDEX framework, which consists of four main stages: 1. Data Collection and Verification; 2. Supervised Fine-tuning; 3. Reinforcement Learning Training; and 4. Data Validation. The framework uses an automated pipeline to collect high-quality code explanation and refinement data, then leverages supervised fine-tuning and reinforcement learning to enhance LLMs' self-debugging capabilities, resulting in more accurate code refinements and insightful code explanations. The data verification step ensures high quality and reduces noise in the training data. The figure uses icons and arrows to show the data flow and connections between these stages.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure presents a visual overview of the LEDEX framework.  It shows the different stages involved, starting with data collection and verification, followed by supervised fine-tuning and reinforcement learning. Each stage is represented by a block with a brief description of the process and the data flow between the stages. The figure provides a concise representation of the LEDEX pipeline, illustrating how the framework collects high-quality code explanation and refinement data and then uses this data to train LLMs for better self-debugging and code explanation.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the overview of the LEDEX framework. It illustrates the four main steps involved in the process: 1. Data Collection, which involves querying LLMs and verifying responses; 2. Data Verification; 3. Supervised Fine-Tuning, which uses the collected data to train the model; and 4. Reinforcement Learning Training, which further enhances the model's performance.  The diagram visually depicts the flow of data and the different stages of training.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_19_2.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure presents a flowchart illustrating the LEDEX framework.  The framework comprises four main stages: (1) Data Collection, where code explanation and refinement trajectories are generated and filtered via execution verification; (2) Data Verification, where the quality of collected data is validated; (3) Supervised Fine-Tuning (SFT), where the model is fine-tuned using the high-quality dataset; and (4) Reinforcement Learning (RL), where the model's performance is further enhanced using reinforcement learning with a novel reward design. The flowchart visually depicts the data flow and processing steps within the LEDEX framework, providing a clear overview of the entire process.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_20_1.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure shows the overview of the LEDEX framework, which includes four main steps: (1) Data Collection: querying LLMs and verifying responses; (2) Data Verification: filtering data based on execution; (3) Supervised Fine-tuning: using high-quality data for training; and (4) Reinforcement Learning Training: further optimizing with novel reward mechanisms.", "section": "2 Approach"}, {"figure_path": "d1XrZ4EINV/figures/figures_20_2.jpg", "caption": "Figure 2: Overview of LEDEX.", "description": "This figure presents a flowchart illustrating the LEDEX framework.  It outlines the stages involved in data collection (querying LLMs for wrong solutions, filtering via execution verification), supervised fine-tuning (using the collected dataset), and reinforcement learning (using a novel reward design). Each stage is visually depicted, helping to explain the steps involved in the automated pipeline that improves LLMs' self-debugging capabilities.", "section": "2 Approach"}]