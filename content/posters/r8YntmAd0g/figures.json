[{"figure_path": "r8YntmAd0g/figures/figures_4_1.jpg", "caption": "Figure 1: An illustration of the auto-correlation (\u03c4) and power spectrum density P(v) of {\u2207F(xt)} and wt where \u2207f decays proportional to \u03c4\u00b2 and wt is a white noise. (c) illustrates how an ideal low-pass filters out the high-frequency noise and keeps the low-frequency signal.", "description": "This figure demonstrates the concept of frequency domain analysis applied to gradient updates in differentially private optimization.  Panel (a) shows the auto-correlation functions for both the gradient (blue) and noise (red) across different time lags (\u03c4). The gradient exhibits high correlation for small lags, decaying as the lag increases, indicating a low-frequency signal.  In contrast, the noise is only correlated with itself at a lag of zero (white noise). Panel (b) shows the power spectral density (PSD) corresponding to the auto-correlation functions in (a). The gradient's PSD peaks at low frequencies, confirming its low-frequency nature. The noise, however, has a flat PSD across all frequencies. Panel (c) illustrates how an ideal low-pass filter can effectively separate the gradient signal from the high-frequency noise by allowing only the low-frequency components to pass through.", "section": "A signal processing perspective"}, {"figure_path": "r8YntmAd0g/figures/figures_8_1.jpg", "caption": "Figure 2: The recorded PSD of Gaussian noise {wt}, and the stochastic gradients of SGD and LP-SGD of ResNet-50 training on CIFAR-10 dataset.", "description": "The figure shows the power spectral density (PSD) plots of the Gaussian noise and the stochastic gradients.  The PSD of the Gaussian noise is flat across all frequencies, indicating white noise. In contrast, the PSD of the stochastic gradients shows a concentration of power at lower frequencies, indicating that the gradients are not white noise but exhibit correlation across iterations. Applying the low-pass filter to the gradients reduces the high-frequency components and enhances the signal-to-noise ratio. The low-pass filter is applied to suppress noise components, which makes the gradient have a better signal-to-noise ratio. The figure demonstrates the effectiveness of the proposed low-pass filter in reducing the impact of DP noise by separating the signal (gradient) from the noise in the frequency domain.", "section": "Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_8_2.jpg", "caption": "Figure 2: The recorded PSD of Gaussian noise {wt}, and the stochastic gradients of SGD and LP-SGD of ResNet-50 training on CIFAR-10 dataset.", "description": "This figure shows the Power Spectral Density (PSD) plots for the Gaussian noise (wt) and the stochastic gradients obtained using both standard SGD and the proposed LP-SGD method during the training of a ResNet-50 model on the CIFAR-10 dataset.  The PSD of the noise shows a relatively flat distribution across all frequencies, indicating white noise characteristics. In contrast, the PSDs of the gradients from both methods are concentrated around lower frequencies, signifying that the gradient signal has less power at higher frequencies. Notably, the LP-SGD gradient PSD demonstrates a more pronounced suppression of high-frequency components, indicating that the low-pass filter effectively reduces high-frequency noise without significant alteration of the main signal.", "section": "Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_8_3.jpg", "caption": "Figure 3: Comparision between DPSGD and LP-DPSGD for pre-training on different datasets.", "description": "This figure compares the performance of DPSGD and LP-DPSGD (DPSGD with the proposed low-pass filter) during the pre-training phase on three different datasets: CIFAR-10, CIFAR-100, and MNIST.  Each subfigure shows the test accuracy over epochs for both algorithms on a specific dataset. The results illustrate that LP-DPSGD consistently outperforms DPSGD across all three datasets, demonstrating the effectiveness of the low-pass filter in improving the performance of DP optimizers.", "section": "Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_8_4.jpg", "caption": "Figure 4: Comparision between DP optimizers w and w/o low-pass filters for pre-training with different e's on CIFAR-10 dataset.", "description": "This figure compares the performance of three different differentially private (DP) optimizers (DPSGD, DPAdam, and DPGaLore) with and without the low-pass filter (LP) proposed in the paper.  The comparison is done across various privacy budgets (epsilon values) during the pre-training phase on the CIFAR-10 dataset.  The results show how the low-pass filter improves the test accuracy of each DP optimizer for different levels of privacy protection.", "section": "Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_12_1.jpg", "caption": "Figure 5: Illustration of the low-pass filter.", "description": "This figure illustrates how the low-pass filter works in the context of differentially private (DP) optimization. The input is the clipped gradients, which are then added to DP noise. Finally, the low-pass filter processes the noisy gradient to reduce the noise.", "section": "A.2 Low-pass filter"}, {"figure_path": "r8YntmAd0g/figures/figures_15_1.jpg", "caption": "Figure 6: The time and frequency response of the filters used in the paper.", "description": "This figure visualizes the time and frequency responses of various filters used in the paper.  Specifically, it illustrates the auto-correlation coefficients (time response) and power spectral density (frequency response) for several filter designs:  SGD (no filter), Momentum-SGD, and first and second-order filters. This helps to understand how the different filter designs affect the balance between attenuating high-frequency noise and preserving low-frequency signal components of the gradients during training.", "section": "C.1 Hyper-parameter choice"}, {"figure_path": "r8YntmAd0g/figures/figures_16_1.jpg", "caption": "Figure 2: The recorded PSD of Gaussian noise {wt}, and the stochastic gradients of SGD and LP-SGD of ResNet-50 training on CIFAR-10 dataset.", "description": "This figure shows the power spectral density (PSD) plots for Gaussian noise and the stochastic gradients obtained from training a ResNet-50 model on the CIFAR-10 dataset using both standard SGD and the proposed LP-SGD (low-pass filter SGD) methods.  The PSD is a way to visualize how the power of the signal is distributed across different frequencies.  In the context of this paper, the low-frequency components correspond to the actual gradient signal, while the high-frequency components are associated with the noise added for differential privacy.  The figure visually demonstrates how the LP-SGD method effectively suppresses the noise, which is particularly useful in differential privacy applications where the goal is to protect sensitive data while still obtaining accurate gradients.", "section": "Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_17_1.jpg", "caption": "Figure 7: Comparision between DPSGD LP-DPSGD for pre-training different models on CIFAR-10 dataset with  = 8.", "description": "The figure compares the performance of DPSGD and LP-DPSGD (DPSGD with the low-pass filter) on four different models (5-layer CNN, Vit-small, EfficientNet, and ResNet-50) for pre-training on the CIFAR-10 dataset with a privacy budget (epsilon) of 8.  Each subplot shows the test accuracy over epochs for a specific model, illustrating how the low-pass filter improves accuracy. The results highlight the consistent improvement in performance offered by the low-pass filter across diverse model architectures.", "section": "6.2 Numerical results"}, {"figure_path": "r8YntmAd0g/figures/figures_17_2.jpg", "caption": "Figure 8: LP-DPSGD for pre-training on CIFAR-10 with different filter coefficients.", "description": "This figure compares the performance of the LP-DPSGD optimizer on the CIFAR-10 dataset using different low-pass filter coefficients.  Subfigure (a) shows the results for filters described in Table 2 of the paper. Subfigure (b) shows the results for filters described in Table 4 of the paper.  The different filter configurations demonstrate varying impacts on the training process and final test accuracy. The results illustrate the effectiveness of appropriately chosen low-pass filter in enhancing the performance of differentially private optimizers by reducing the noise introduced by the privacy mechanism.", "section": "Numerical experiments"}, {"figure_path": "r8YntmAd0g/figures/figures_18_1.jpg", "caption": "Figure 9: DPLPSGD for pre-training on CIFAR-10 with different clipping strategies.", "description": "This figure shows the impact of different clipping strategies on the performance of the DP-LPSGD optimizer during pre-training on the CIFAR-10 dataset.  Four different clipping methods are compared: Flat Clip (clipping the entire gradient vector), Layer Clip (clipping each layer's gradient separately), Flat Norm (normalizing the entire gradient vector before clipping), and Layer Norm (normalizing each layer's gradient before clipping). The x-axis represents the training epoch, and the y-axis represents the test accuracy. The results indicate that Flat Norm significantly outperforms other clipping strategies.", "section": "Numerical experiments"}, {"figure_path": "r8YntmAd0g/figures/figures_18_2.jpg", "caption": "Figure 10: LP-DPAdamBC for pre-training on CIFAR-10 with different learning rates.", "description": "The figure shows the impact of different learning rates on the test accuracy of the LP-DPAdamBC optimizer during the pre-training phase on the CIFAR-10 dataset.  It illustrates how the learning rate affects the convergence speed and the final test accuracy.  A learning rate that is too high may lead to instability and prevent convergence to a good solution, while a learning rate that is too low will cause slow convergence and might not reach a good final accuracy. The optimal learning rate provides the balance between these two effects, leading to fast convergence and high accuracy.", "section": "Numerical experiments"}, {"figure_path": "r8YntmAd0g/figures/figures_18_3.jpg", "caption": "Figure 11: LP-DPSGD with and without learning rate scheduler", "description": "This figure compares the performance of the LP-DPSGD optimizer with and without a learning rate scheduler (Cosine-Annealing with warmup). The results show that when the number of epochs is large, using a learning rate scheduler improves the training performance. However, in the early stages of training, the scheduler slows down convergence.", "section": "Numerical experiments"}, {"figure_path": "r8YntmAd0g/figures/figures_19_1.jpg", "caption": "Figure 12: DPSGD and LP-DPSGD for fine-tuning ViT on CIFAR-10 with different e's.", "description": "This figure shows the test accuracy of DPSGD and LP-DPSGD for fine-tuning a Vision Transformer (ViT) model on the CIFAR-10 dataset with varying privacy budgets (epsilon).  The x-axis represents the privacy parameter epsilon, and the y-axis represents the test accuracy. The blue line represents LP-DPSGD, which incorporates a low-pass filter, while the red line represents standard DPSGD.  The figure demonstrates the performance comparison of both methods under different levels of privacy protection. ", "section": "6.2 Numerical results"}]