[{"type": "text", "text": "Boosting Transferability and Discriminability for Time Series Domain Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingyang Liu1, Xinyang Chen1 , Yang $\\mathbf{S}\\mathbf{h}\\mathbf{u}^{2\\boxplus}$ , Xiucheng $\\mathbf{Li^{\\mathbf{l}\\boxtimes}}$ , Weili Guan3, Liqiang Nie1 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen) 2School of Data Science and Engineering, East China Normal University 3School of Electronics and Information Engineering, Harbin Institute of Technology (Shenzhen) mingyangliu1024@gmail.com, yshu@dase.ecnu.edu.cn {chenxinyang,lixiucheng,guanweili,nieliqiang}@hit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unsupervised domain adaptation excels in transferring knowledge from a labeled source domain to an unlabeled target domain, playing a critical role in time series applications. Existing time series domain adaptation methods either ignore frequency features or treat temporal and frequency features equally, which makes it challenging to fully exploit the advantages of both types of features. In this paper, we delve into transferability and discriminability, two crucial properties in transferable representation learning. It\u2019s insightful to note that frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains. Based on the findings, we propose Adversarial CO-learning Networks (ACON), to enhance transferable representation learning through a collaborative learning manner in three aspects: (1) Considering the multi-periodicity in time series, multi-period frequency feature learning is proposed to enhance the discriminability of frequency features; (2) Temporalfrequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; (3) Domain adversarial learning is conducted in the correlation subspaces of temporal-frequency features instead of original feature spaces to further enhance the transferability of both features. Extensive experiments conducted on a wide range of time series datasets and five common applications demonstrate the state-of-the-art performance of ACON. Code is available at https://github.com/mingyangliu1024/ACON. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series classification has achieved significant success in the deep learning era by leveraging discriminative features learned from extensive labeled data [18]. However, the presence of distribution shift may arise when deploying the model, potentially impeding the generalization ability of deep models [31]. Unsupervised domain adaptation [13], offering the potential to transfer knowledge from a labeled source domain to an unlabeled target domain, emerges as a promising solution. ", "page_idx": 0}, {"type": "text", "text": "Existing domain adaptation methods tailored for time series primarily focus on learning domaininvariant temporal features [30, 41, 29], yielding promising results. Recently, the significance of frequency features for enhancing domain-invariant representation has also been recognized [16]. However, frequency features and temporal features are treated equally, and their distinct properties are overlooked, leading to the inability to fully leverage both types of features to boost transfer learning. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we analyze the two most important properties of features in transfer learning: transferability and discriminability, to investigate the characteristics of the frequency features and temporal features. We find that under the premise of adopting advanced backbones in state-of-the-art works [31, 16], frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Based on the findings, we propose Adversarial CO-learning Networks (ACON) to maximize the potential of temporal features and frequency features in terms of both transferability and discriminability in a collaborative learning manner. Firstly, to fully leverage the properties of multi-periodicity in time series, we propose multi-period frequency feature learning to further enhance the discriminability of frequency features. Secondly, we propose temporal-frequency domain mutual learning to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain. Specifically, to harness the potent discriminability of frequency features within the domain, we enable the transfer of knowledge from frequency features to temporal features within the source domain via knowledge distillation. To leverage the strong transferability of temporal features across domains, we facilitate the transfer of knowledge from temporal features to frequency features in the target domain through knowledge distillation. Thirdly, we propose to learn transferable representations via domain adversarial learning in temporal-frequency correlation subspace instead of the original temporal feature space. The temporal-frequency correlation subspace not only possesses the properties of the original temporal feature space and original frequency feature space but also incorporates the correlation between the two types of features. Learning transferable representations in the temporal-frequency correlation subspace can further enhance the transferability of features. Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We uncover the characteristics wherein temporal features and frequency features cannot be equally treated in transfer learning. Specifically, we observe that frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains through empirical findings.   \n\u2022 We design ACON, which enhances UDA in three key aspects: a multi-period feature learning module to enhance the discriminability of frequency features, a temporal-frequency domain mutual learning module to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain, and a domain adversarial learning module in temporal-frequency correlation subspace to further enhance transferability of features.   \n\u2022 Experiments conducted on a wide range of time series datasets and five common applications verify the effectiveness of ACON. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "General Unsupervised Domain Adaptation Methods Unsupervised domain adaptation leverages the labeled source domain to predict the labels of a different but related, unlabeled target domain. It finds wide applications in computer vision [46, 15, 8] and natural language processing [40, 39, 44]. Existing UDA methods can be classified into three categories: (1) Methods based on adversarial training aim to learn domain-invariant representations via the game between the feature extractor and the domain discriminator. Widely used methods include DANN [13], CDAN [26] and DIRT-T [34]. (2) Methods based on statistical divergence aim to reduce the domain discrepancy by minimizing domain discrepancy in a latent feature space. Widely used methods include DAN [25], DeepCoral [36] and HoMM [5]. (3) Methods based on self-training produce pseudo-labels on unlabeled data and use confident pseudo-labels together with the labeled data to train the model. Widely used methods include PFAN[6], CST [22] and AdaMatch [2]. However, these methods are generally designed and do not fully leverage the properties of time series. Although these methods can be applied to time series through tailored feature extractors, they often obtain suboptimal performance and UDA algorithm specially designed for time series is needed. ", "page_idx": 1}, {"type": "text", "text": "Unsupervised Domain Adaptation for Time Series To date, a few methods have been tailored to unsupervised domain adaptation for time series data. VRADA [30] is the first UDA method for multivariate time series that uses adversarial learning for reducing domain discrepancy. In VRADA, a variational recurrent neural network (VRNN) [10] is trained in an adversarial way to learn domain-invariant temporal features. CoDATS [41] builds upon VRADA but uses a convolutional neural network for the feature extractor, proposing a solution for multi-source domain adaptation in time series classification. SASA [3] adopts LSTM [33] as feature extractors to capture the domaininvariant association, and aligns sparse associative structure between source and target domain via the minimization of maximum mean discrepancy (MMD) [38]. AdvSKM [23] modifies MMD to make it more suitable for time series data. CLUDA [29] learns contextual representation via contrastive learning, and aligns features between source and target domain via adversarial training. RAINCOAT [16] is the first to introduce frequency features into domain adaptation, aligning temporal features and frequency features respectively via Sinkhorn divergence. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Research gap In general, in terms of representation learning, most methods only focus on the temporal domain or assume that the temporal domain and the frequency domain are independent of each other, hindering the full utilization of two types of features. In terms of feature adaptation, existing works only focus on aligning temporal features or adopting simple statistical divergence to align frequency features, ignoring the different properties of the temporal features and frequency features in transfer learning. In terms of evaluation, the existing evaluations are conducted on several datasets of limited scale in a few specific tasks, and more general evaluations are needed. ", "page_idx": 2}, {"type": "text", "text": "3 Transferability and Discriminability in Time Series ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we study the UDA problem for time series classification. In time series classification problem, the model receives a set of $n$ labeled samples $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}$ , where $i$ -th sample $\\mathbf{x}_{i}\\in\\mathbb{R}^{C\\times T}$ contains observation of $C$ variates over $T$ time steps. We allow for both univariate and multivariate time series. In UDA setup, we are given $n_{s}$ labeled samples from a source domain $\\hat{P}=\\{(\\mathbf{x}_{i}^{s},\\mathbf{y}_{i}^{s})\\}_{i=1}^{n_{s}}$ and $n_{t}$ unlabeled samples from a target domain $\\hat{Q}=\\{(\\mathbf{x}_{i}^{t})\\}_{i=1}^{n_{t}}$ , which are sampled from different distributions $P$ and $Q$ . Superscripts $s$ and $t$ are adopted to distinguish the source domain and the target domain. UDA for time series aims to learn a time series classification model with labeled source data $\\hat{P}$ and unlabeled target data $\\hat{Q}$ , which can make accurate predictions on the target domain. ", "page_idx": 2}, {"type": "text", "text": "In addition to the source domain and target domain in UDA, time series naturally can be represented in the temporal domain and frequency domain. By Fast Fourier Transform (FFT), the raw time series input $\\mathbf{x}_{i}$ in the temporal domain can be transformed to corresponding frequency input $\\mathbf{v}_{i}$ in the frequency domain: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}=\\mathrm{FFT}\\left(\\mathbf{x}_{i}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the complex variable $\\mathbf{v}_{i}\\,\\in\\,\\mathbb{C}^{C\\times\\lfloor\\frac{T}{2}\\rfloor}$ contains observation of $C$ variates over $\\left\\lfloor{\\frac{T}{2}}\\right\\rfloor$ different frequencies. Due to the conjugacy of frequency domain, we only consider the frequencies within $\\textstyle\\{1,{\\overset{\\cdot}{\\dots}},\\lfloor{\\frac{T}{2}}\\rfloor\\}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Discriminability of frequency feature ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "cIBSsXowMr/tmp/d2af4b5868a47b40fab6e10b55ede8b567d2ba8f9695edf88998301f94dbd5e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Discriminability of frequency feature: (a) The Electroencephalography (EEG) signal and corresponding frequency data of two classes in the CAP dataset: Wake and Rapid Eye Movement (REM). (b) Classification on the source domain: Temporal domain vs. Frequency domain. (c) Source-only and DANN: Temporal domain vs. Frequency domain. ", "page_idx": 2}, {"type": "text", "text": "As Figure 1(a) presented, compared to the uniform distribution of temporal data for different classes, the frequency data for different classes shows distinct differences in the dominant frequencies and peaks, which holds more discriminative information. To further investigate the discriminability of frequency features, we perform the single data domain classification task in the frequency domain and temporal domain respectively on all five data domains of the CAP [37, 14] dataset. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In order to minimize the impact of specific model structures, we adopt 3-layer 1D-CNN, a generic structure as the temporal feature extractor, and 1-layer linear as the frequency feature extractor, which have both widely validated for their effectiveness in existing time series analysis methods [23, 16, 42, 43]. We only retain the low-frequency data to ensure that the temporal feature extractor and the frequency feature extractor have comparable parameter quantities. For the classifiers, we uniformly use 1-layer linear. As Figure 1(b) shown, with a simple feature extractor, the frequency classification outperforms the temporal classification, demonstrating that the frequency features have better discriminability. More analysis results on different datasets are included in Appendix C.1. ", "page_idx": 3}, {"type": "text", "text": "3.3 Transferability of temporal feature ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Another key criterion that characterizes the performance of domain adaptation is transferability [7]. Transferability indicates the ability to learn invariant features across domains. Since the frequency features have better discriminability within the source domain, it is natural to raise the question: Will the frequency features also have better discriminability in the target domain? ", "page_idx": 3}, {"type": "text", "text": "We investigate this problem starting with the comparison of four methods: (1) Source-only-F, a model trained in the frequency domain without UDA. (2) Source-only-T, a model trained in the temporal domain without UDA. (3) DANN-F, a model aligning the source features and the target features in the frequency domain via DANN. (4) DANN-T. a model aligning the source features and the target features in the temporal domain via DANN. Figure 1(c) shows the accuracy in the target domains of four source-target domain pairs from the CAP dataset. Compared with Figure 1(b), the frequency classification, which has better discriminability performance in the source domain, actually slightly underperforms in the target domain. It indicates that better discriminability in the source domain does not necessarily imply better discriminability in the target domain. Compared with Source-only methods, the gap between DANN-F and DANN-T is further exacerbated. This suggests that the temporal feature extractor more easily learns domain-invariant features. More analysis results on different datasets are included in Appendix C.2. ", "page_idx": 3}, {"type": "text", "text": "The above analysis reveals two insights for time series domain adaptation: With better discriminability but worse transferability, domain adaptation in the frequency domain obtains suboptimal performance; while with better transferability, domain adaptation in the temporal domain has the potential to achieve superior performance under the guidance of more discriminative information. ", "page_idx": 3}, {"type": "text", "text": "4 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the above observations, our motivation is to simultaneously leverage the strong discriminability of frequency features and the strong transferability of temporal features to enhance domain adaptation. This inspires us to learn domain-invariant temporal and frequency features in a collaborative learning manner. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 illustrates the overall structure of our Adversarial CO-learning Networks (ACON). To avoid confusion, subscripts $T$ and $F$ are adopted to distinguish the temporal domain and the frequency domain. Specifically, in the temporal domain, we have a temporal feature extractor with temporal input $\\mathbf{f}=\\psi_{T}(\\mathbf{x})$ and a temporal classifier $\\hat{\\mathbf{y}}_{T}=g_{T}(\\mathbf{f})$ ; while in the frequency domain, we have a frequency feature extractor with frequency input $\\mathbf{z}=\\psi_{F}(\\mathbf{v})$ and a frequency classifier $\\hat{{\\bf y}}_{F}=g_{F}({\\bf z})$ . Additionally, we have a domain discriminator $g_{D}$ , which is trained to distinguish the source feature and the target feature. In the following, we will introduce three main contributions in ACON: multi-period frequency feature learning in Section 4.1, temporal-frequency domain mutual learning in Section 4.2, and domain adversarial learning in temporal-frequency correlation subspace in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.1 Multi-period frequency feature learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The real-world time series usually present multi-periodicity, which is reflected in the frequency domain as the presence of a few dominant frequencies with significantly larger amplitudes. Data from different periods can have different discriminative patterns. Based on this, before performing FFT, we segment the raw time series according to the top- $\\boldsymbol{\\cdot}$ significant periods, enhancing the ", "page_idx": 3}, {"type": "image", "img_path": "cIBSsXowMr/tmp/67337f516b281c678fdbb614a24a952e2c027c5256f3e1d4d49659a021df02dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The architecture of ACON. ACON models temporal data (blue) and frequency data (green) simultaneously. Left part: Segment raw frequency data by period to capture different discriminative patterns. Middle part: Align distributions in temporal-frequency correlation subspace via adversarial training. Right part: Mutual learning between the temporal domain and frequency domain. ", "page_idx": 4}, {"type": "text", "text": "discriminability of the frequency domain. Additionally, by period-based segmentation, the noises brought by meaningless high frequencies are effectively filtered out [4, 45]. ", "page_idx": 4}, {"type": "text", "text": "To capture the overall multi-periodicity, before training, we randomly sample mini-batches from the training set to perform FFT and select the frequencies with the top- $\\cdot\\mathbf{k}$ amplitudes $\\{f_{1},\\ldots,f_{k}\\}$ . Given the frequency $f_{j}$ , the corresponding period is $\\begin{array}{r}{\\bar{p}_{j}=\\lceil\\frac{T}{f_{j}}\\rceil}\\end{array}$ . For each selected period $p_{j}$ in $\\{p_{1},\\ldots,p_{k}\\}$ and frequency $f_{j}$ in the corresponding $\\{f_{1},\\ldots,f_{k}\\}$ , we perform the following transform on input $\\mathbf{x}_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}_{i}^{j}=\\operatorname{Reshape}_{p_{j}}(\\mathbf{x}_{i}),\\quad j\\in\\{1,\\dots,k\\},}\\\\ &{\\mathbf{v}_{i}^{j}=\\operatorname{Avg}\\left(\\operatorname{FFT}\\left(\\mathbf{X}_{i}^{j}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{X}_{i}^{j}\\in\\mathbb{R}^{C\\times f_{j}\\times p_{j}}$ , $\\mathbf{v}_{i}^{j}\\in\\mathbb{C}^{C\\times\\lfloor\\frac{p_{j}}{2}\\rfloor}$ is averaged from $f_{j}$ dimensions by $\\mathrm{Avg}({\\cdot})$ . In other words, we perform FFT on each segment obtained by segmenting $\\mathbf{x}_{i}$ with period $p_{j}$ , and average the FFT results across segments to obtain the distribution $\\mathbf{v}_{i}^{j}$ over the frequencies within $\\{1,\\ldots,\\lfloor\\frac{p_{j}}{2}\\rfloor\\}$ . In this way, we obtain the overall frequency pattern for each period. To keep the discriminative patterns derived from different periods, we concatenate the different $\\mathbf{v}_{i}^{j}$ , obtaining $\\mathbf{v}_{i}$ as the frequency input corresponding to the temporal input $\\mathbf{x}_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}=\\mathbf{v}_{i}^{1}\\oplus\\ldots\\oplus\\mathbf{v}_{i}^{k},\\quad j\\in\\{1,\\ldots,k\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We extend the source sample set $\\hat{P}$ and the target sample set $\\hat{Q}$ to the frequency domain: $\\hat{P}=$ $\\{(\\mathbf{x}_{i}^{s},\\mathbf{v}_{i}^{s},\\mathbf{y}_{i}^{s})\\}_{i=1}^{n_{s}}$ and $\\hat{Q}=\\{(\\mathbf{x}_{i}^{t},\\mathbf{v}_{i}^{t})\\}_{i=1}^{n_{t}}$ . To learn features in both real part and imaginary part of complex frequency data, we adopt a complex-valued linear layer as the frequency feature extractor $\\psi_{F}$ . Since the phase generally does not provide strong discriminative information, we only retain the amplitudes of each frequency to construct the frequency domain feature $\\mathbf{z}_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}_{i}=\\mathrm{Amp}\\left(\\psi_{F}(\\mathbf{v}_{i})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{Amp}(\\cdot)$ denotes the calculation of amplitude values. For multivariate time series, we convert $\\mathbf{v}_{i}$ into a single-channel vector by concatenating across different variates. ", "page_idx": 4}, {"type": "text", "text": "4.2 Temporal-frequency domain mutual learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Discriminability and transferability are two key criteria that characterize the goodness of feature representations to enable domain adaptation. In Section 3, we reveal that the frequency features are more discriminative within the source domain, while the temporal features are more transferable across domains. Based on this discovery, we propose temporal-frequency domain mutual learning, aiming to leverage the respective advantages of the temporal domain and frequency domain. ", "page_idx": 4}, {"type": "text", "text": "The essence of domain mutual learning relies on how to transfer knowledge between the temporal domain and frequency domain. Inspired by model distillation, where the knowledge is transferred by matching the predictions between the teacher and student via the Kullback Leibler (KL) divergence [17], we focus mutual learning on the alignment between the temporal predictions and the frequency predictions. The KL divergence between two predictions $p_{1}$ and $p_{2}$ is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{K L}(p_{1}||p_{2})=\\sum_{m=1}^{C}p_{1}^{m}\\mathrm{log}\\frac{p_{1}^{m}}{p_{2}^{m}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The KL divergence is asymmetric, that is, $D_{K L}(p_{1}||p_{2})$ emphasizes aligning $p_{2}$ to $p_{1}$ , while $D_{K L}(p_{2}||p_{1})$ emphasizes aligning $p_{1}$ to $p_{2}$ . Based on the asymmetry, we use different alignment strategies in the source domain and target domain. Specifically, in the source domain, the frequency model serves as a more discriminative teacher, helping the temporal model make more accurate predictions; conversely, in the target domain, the temporal model acts as a more transferable teacher, assisting the frequency model in learning domain-invariant representations. We achieve temporalfrequency domain mutual learning by minimizing the KL Divergence. Formally, domain mutual learning is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{M_{s}}(\\psi_{T},g_{T})=\\mathbb{E}_{(\\mathbf{x}_{i}^{s},\\mathbf{v}_{i}^{s})\\sim\\hat{P}}[D_{K L}(\\hat{\\mathbf{y}}_{F}^{s}||\\hat{\\mathbf{y}}_{T}^{s})],}\\\\ {\\mathcal{L}_{M_{t}}(\\psi_{F},g_{F})=\\mathbb{E}_{(\\mathbf{x}_{i}^{t},\\mathbf{v}_{i}^{t})\\sim\\hat{Q}}[D_{K L}(\\hat{\\mathbf{y}}_{T}^{t}||\\hat{\\mathbf{y}}_{F}^{t})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{y}}_{F}^{s}$ and $\\hat{\\mathbf{y}}_{T}^{s}$ refer to the frequency prediction and temporal prediction in the source domain respectively; while $\\hat{\\mathbf{y}}_{F}^{t}$ and $\\hat{\\mathbf{y}}_{T}^{t}$ refer to the frequency prediction and temporal prediction in the target domain respectively. By aligning $\\hat{\\mathbf{y}}_{T}^{s}$ to $\\hat{\\mathbf{y}}_{F}^{s}$ , the training of the temporal feature extractor and classifier is guided with more discriminative information; by aligning $\\bar{\\hat{\\mathbf{y}}}_{F}^{t}$ to $\\hat{\\mathbf{y}}_{T}^{t}$ , the transferable knowledge contained in the temporal features is transferred to frequency domain. ", "page_idx": 5}, {"type": "text", "text": "4.3 Domain adversarial learning in temporal-frequency correlation subspace ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Domain adversarial learning [13] is one of the most popular transferable representation learning methods, and it can be employed to learn transferable representation in time series. The key to the effectiveness of the method lies in how to fully utilize two types of features to learn transferable representations. Given time series in temporal domain and frequency domain, domain adversarial learning can be formulated as a minimax optimization problem with three competitive loss terms: (a) $\\mathcal{L}_{C_{T}}$ on the temporal feature extractor $\\psi_{T}$ and classifier $g_{T}$ , which is minimized to guarantee lower source risk of the temporal classifier; (b) $\\mathcal{L}_{C_{F}}$ on the frequency feature extractor $\\psi_{F}$ and classifier $g_{F}$ , which is minimized to guarantee lower source risk of the frequency classifier; (c) $\\mathcal{L}_{D}$ on the temporal feature extractor $\\psi_{T}$ , the frequency feature extractor $\\psi_{F}$ and the domain discriminator $g_{D}$ , which is minimized over $g_{D}$ but maximized over $\\psi_{T}$ and $\\psi_{F}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{C_{T}}(\\psi_{T},g_{T})=\\mathbb{E}_{(\\mathbf{x}_{i}^{s},\\mathbf{y}_{i}^{s})\\sim\\hat{P}}[\\ell\\left(g_{T}\\left(\\psi_{T}\\left(\\mathbf{x}_{i}^{s}\\right)\\right),\\mathbf{y}_{i}^{s}\\right)],}\\\\ &{\\quad\\mathcal{L}_{C_{F}}(\\psi_{F},g_{F})=\\mathbb{E}_{(\\mathbf{v}_{i}^{s},\\mathbf{y}_{i}^{s})\\sim\\hat{P}}[\\ell\\left(g_{F}\\left(\\psi_{F}\\left(\\mathbf{v}_{i}^{s}\\right)\\right),\\mathbf{y}_{i}^{s}\\right)],}\\\\ &{\\mathcal{L}_{D}(\\psi_{T},\\psi_{F},g_{D})=-\\mathbb{E}_{(\\mathbf{x}_{i}^{s},\\mathbf{v}_{i}^{s})\\sim\\hat{P}}\\mathrm{log}[g_{D}\\left(\\psi_{T}\\left(\\mathbf{x}_{i}^{s}\\right),\\psi_{F}\\left(\\mathbf{v}_{i}^{s}\\right)\\right)]}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\mathbb{E}_{(\\mathbf{x}_{i}^{t},\\mathbf{v}_{i}^{t})\\sim\\hat{Q}}\\mathrm{log}[1-g_{D}\\left(\\psi_{T}\\left(\\mathbf{x}_{i}^{t}\\right),\\psi_{F}\\left(\\mathbf{v}_{i}^{t}\\right)\\right)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell$ denotes cross-entropy loss. Different from standard domain adversarial learning, where there is only one type of feature, domain adversarial learning in time series needs to consider the temporal features and frequency features simultaneously. A simple strategy is to concatenate the temporal feature $\\mathbf{f}$ and the frequency feature ${\\bf z}$ . However, with the concatenation strategy, the adversarial game between the domain discriminator and the feature extractors can be viewed as two independent components: the game between $g_{D}$ and $\\psi_{T}$ and the game between $g_{D}$ and $\\psi_{F}$ . With the worse transferability, $\\mathbf{z}$ provides $g_{D}$ with rich domain-label relevant information. In this case, $g_{D}$ only needs to focus on the game with $\\psi_{F}$ , ignoring the domain adversarial learning in the temporal domain. ", "page_idx": 5}, {"type": "text", "text": "To achieve co-alignment in the temporal domain and frequency domain, we propose domain adversarial learning in temporal-frequency correlation subspace. The temporal-frequency correlation subspace not only possesses statistical characteristics of the original temporal feature subspace and original frequency feature subspace but also reflects the correlation between temporal features and frequency features. Reducing the discrepancy of the temporal-frequency correlation subspace not only reduces the discrepancy in the cross-domain temporal and frequency features but also decreases the differences in cross-domain temporal-frequency correlations. ", "page_idx": 5}, {"type": "text", "text": "Formally, the vectors in temporal-frequency correlation subspace can be calculated as the outer product $\\otimes$ between the temporal feature f and the frequency feature ${\\bf z}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{f}\\otimes\\mathbf{z}=\\left[\\mathbf{z}\\left[1\\right]\\cdot\\mathbf{f},\\,\\mathbf{z}\\left[2\\right]\\cdot\\mathbf{f},\\,.\\,.\\,.\\,,\\mathbf{z}\\left[l\\right]\\cdot\\mathbf{f}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{z}\\in\\mathbb{R}^{1\\times l}$ . By adjusting the order of dimensions, $\\mathbf{z}\\otimes$ f is equivalent to f $\\otimes\\mathbf{z}$ . Considering the sparsity of the frequency domain and the modeling of long-length time series, the direct outer product leads to dimension explosion and the sparsity in temporal-frequency feature subspace. We address the problem by performing average pooling over ${\\bf z}$ . Average pooling, which calculates the average value for the amplitudes of neighboring frequencies, yields dense frequency features with smaller dimensions. With outer product and average pooling, the adversarial loss $\\mathcal{L}_{D}$ is formulated as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(\\mathbf{x}_{i},\\mathbf{v}_{i})=\\psi_{T}\\left(\\mathbf{x}_{i}\\right)\\otimes\\mathbf{P}\\left(\\psi_{F}\\left(\\mathbf{v}_{i}\\right)\\right),\\qquad\\qquad}\\\\ {\\mathcal{L}_{D}\\!\\left(\\psi_{T},\\psi_{F},g_{D}\\right)=-\\mathbb{E}_{(\\mathbf{x}_{i}^{\\mathfrak{s}},\\mathbf{v}_{i}^{\\mathfrak{s}})\\sim\\hat{P}}\\mathrm{log}\\big[g_{D}\\left(h\\left(\\mathbf{x}_{i}^{\\mathfrak{s}},\\mathbf{v}_{i}^{\\mathfrak{s}}\\right)\\right)\\big]-\\mathbb{E}_{(\\mathbf{x}_{i}^{\\mathfrak{t}},\\mathbf{v}_{i}^{\\mathfrak{t}})\\sim\\hat{Q}}\\mathrm{log}\\big[1-g_{D}\\left(h\\left(\\mathbf{x}_{i}^{\\mathfrak{t}},\\mathbf{v}_{i}^{\\mathfrak{t}}\\right)\\right)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $h(\\cdot)$ denotes the mapping from the inputs of the overall model to the inputs of the domain discriminator, and $\\mathrm{{P}}\\left(\\cdot\\right)$ denotes the calculation of average pooling. ", "page_idx": 6}, {"type": "text", "text": "4.4 Overview ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "During alignment, our method trains the temporal feature extractor $\\psi_{T}$ and classifier $g_{T}$ by minimizing the loss $\\mathcal{L}_{C_{T}}$ and trains the frequency feature extractor $\\psi_{F}$ and classifier $g_{F}$ by minimizing the loss $\\mathcal{L}_{C_{F}}$ using the source sample set $\\hat{P}$ . Additionally, our method promotes mutual learning between the temporal domain and frequency domain by minimizing the loss $\\mathcal{L}_{M_{s}}$ on the source domain and the loss $\\mathcal{L}_{M_{t}}$ on the target domain. Meanwhile, our method aligns distributions of the source domain and target domain in the temporal-frequency correlation subspace. With two gradient reversal layers between the two feature extractors and the domain discriminator, the adversarial training is achieved by minimizing the loss $\\mathcal{L}_{D}$ . To simplify notation, we denote $\\theta_{F}$ as parameters containing $\\psi_{F}$ and $g_{F}$ , and $\\theta_{T}$ is parameters containing $\\psi_{T}$ and $g_{T}$ . The minimax optimization problem is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta_{F},\\theta_{T}}{\\mathrm{min}}\\,\\mathcal{L}_{C_{F}}(\\theta_{F})+\\mathcal{L}_{C_{T}}(\\theta_{T})+\\mathcal{L}_{M_{s}}(\\theta_{T})+\\mathcal{L}_{M_{t}}(\\theta_{F})-\\mathcal{L}_{D}(\\psi_{T},\\psi_{F},g_{D}),}\\\\ &{\\underset{g_{D}}{\\mathrm{min}}\\,\\mathcal{L}_{D}(\\psi_{T},\\psi_{F},g_{D}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets We conduct extensive experiments using a wide range of time series datasets. (1) Experiments using benchmark datasets in sensor-based human activity recognition (HAR) task: UCIHAR [1], HHAR [35] and WISDM[20]. For HHAR, we first split domains from the perspective of participants, denoted as HHAR-P [16, 31] dataset. Then, we split domains from the perspective of devices, denoted as HHAR-D [12] datasets. (2) Experiments using the benchmark dataset in sleep stage classification (SSC) task: CAP [14, 37]. (3) Experiments using EMG [24, 27] dataset in gesture recognition (GR) task. (4) Experiments using PCL [32, 9, 21, 19] dataset in motor imagery classification (MIC) task. (5) Experiments using FD [31] dataset in machine fault diagnosis (MFD) task. For each dataset, following the existing DA methods on time series [2, 16], we randomly sample 10 source-target domain pairs for evaluation. If the dataset has less than 10 pairs, we evaluate all available domain pairs. Further details, processing and domain splits are included in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Baselines (1) We report the performance of a model without UDA (Source-only) in the temporal domain to show the overall contribution of UDA methods. (2) We implement the following stateof-the-art baselines for UDA of time series data: CODATS[41], AdvSKM[23], CLUDA[29] and RAINCOAT[16]. (3) We additionally implement general unsupervised DA methods: CDAN [26], DeepCoral [36], AdaMatch [2], HoMM [5] and DIRT-T [34]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation We report accuracy and Macro-F1 Score calculated using target test datasets. Accuracy is computed by dividing the number of correctly classified samples by the total number of samples. Macro-F1 Score is calculated using the unweighted mean of all the per-class F1 scores. ", "page_idx": 6}, {"type": "text", "text": "Implementation We adopt the implementation of AdaTime [31] as a benchmarking suite for domain adaptation on time series data, using 1D-CNN as the temporal feature extractor and 1-lyer complex-valued linear as the frequency feature extractor. We use the same feature extractor across all algorithms, ensuring a fair comparison. In all experiments, we use the prediction of the temporal classifier to calculate accuracy and Macro-F1 Score. More experimental details are provided in Appendix B. ", "page_idx": 6}, {"type": "table", "img_path": "cIBSsXowMr/tmp/2bb32975a4916e387ffcb0fd67eb20facdd34bfedf298af465f1130199086d8b.jpg", "table_caption": ["Table 1: Average Accuracy $(\\%)$ on Eight Datasets and Five Applications for UDA "], "table_footnote": ["5.2 Results "], "page_idx": 7}, {"type": "text", "text": "Table 1 shows the average accuracy of each method on all datasets and tasks. Overall, our method has won 5 out of 5 tasks and 8 out of 8 datasets (2 metrics). Specifically, our method improves accuracy by $2.75\\%$ on GR task, $5.20\\%$ on SSC task, $1.31\\%$ on MI task, $9.86\\%$ on HAR task and $1.30\\%$ on MFD task over the advanced baseline on each dataset respectively. ", "page_idx": 7}, {"type": "text", "text": "Due to the limited pages, we report the results for selected source-target domain pairs with metric accuracy on the representative datasets EMG (GR task), CAP (SSC task) and HHAR-P (HAR task). More accuracy results are given in Table 12-16. Average macro-f1 score results are given in Table 17. Full macro-f1 score results are given in Table 18-25. ", "page_idx": 7}, {"type": "table", "img_path": "cIBSsXowMr/tmp/2be86652813492e4d68b50b1e3037455bdbb9df60a67f9e0f53c9175c27aacce.jpg", "table_caption": ["Table 2: Accuracy $(\\%)$ on CAP for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2 presents the results on CAP dataset. CAP contains over 40,000 samples of 3000 time steps, so adaptation on it is more challenging. Our method outperforms general DA and time series DA methods on 9 out of 10 source-target domain pairs, achieving an average improvement of ${\\bf5.20\\,\\%}$ over the advanced baseline, DIRT-T. Table 3 presents the results on HHAR-P dataset. Our method significantly outperforms RAINCOAT, the state-of-the-art DA method for time series, by $10.15\\%$ . ", "page_idx": 7}, {"type": "table", "img_path": "cIBSsXowMr/tmp/bb6ea29833a824ad4dbc35d753a9024aaf76441edb0a66e5cf5afcdb0f397381.jpg", "table_caption": ["Table 3: Accuracy $(\\%)$ on HHAR-P for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cIBSsXowMr/tmp/9e04e38fe5ea2322eefd769bf82704598c385e44ec2a88abb876c95e04b2b5e0.jpg", "table_caption": ["Table 4: Ablation studies: Average Accuracy $(\\%)$ on UCIHAR, HHAR-P and WISDM. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation Study We conduct ablation experiments on three datasets, UCIHAR, HHAR-P and WISDM. For each datasets, we select the same 10 source-target domain pairs as mentioned in Section 5.1. The ablation results (average accuracy of 10 domain pairs) are presented in Table 4. We can observe that all learning modules in the proposed method are effective. Further discussions are included in Appendix C.3. ", "page_idx": 8}, {"type": "image", "img_path": "cIBSsXowMr/tmp/f5e08d744da48b40850ab9207a1844718697c8bc1ac5d4e9a278b2b082f25930.jpg", "img_caption": ["(a) Sensitivity analysis on UCIHAR(b) Sensitivity analysis on HHAR (c) Sensitivity analysis on WISDM "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Sensitivity Analysis on three different datasets: (a) UCIHAR (b) HHAR-P (c) WISDM.   \nRAINCOAT: The advanced baseline that achieves suboptimal performance on the three datasets. ", "page_idx": 8}, {"type": "text", "text": "Sensitivity Analysis It\u2019s worth noting that our total loss in Equation (10) does not include any hyperparameters. In other words, we have not tuned any trade-offs, but set them to 1 for the involved datasets in this paper, giving equal weight to all components of the total loss. In UDA setup, how to search the optimal trade-offs without access to labeled target samples is still an open problem. Considering that, we choose to set all the trade-offs to 1, as it is the most intuitive choice. Without tuning the trade-offs, our proposed ACON still achieves significant improvements. To further investigate the sensitivity of ACON to the potential trade-offs, we update Equation (10) as: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{min}_{\\theta_{F},\\theta_{T}}\\mathcal{L}_{C_{F}}(\\theta_{F})+\\mathcal{L}_{C_{T}}(\\theta_{T})+\\lambda_{M_{s}}\\mathcal{L}_{M_{s}}(\\theta_{T})+\\lambda_{M_{t}}\\mathcal{L}_{M_{t}}(\\theta_{F})-\\lambda_{D}\\mathcal{L}_{D}(\\psi_{T},\\psi_{F},g_{D})},}}\\\\ {{\\displaystyle\\operatorname*{min}_{g_{D}}\\lambda_{D}\\mathcal{L}_{D}(\\psi_{T},\\psi_{F},g_{D}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where hyperparameters $\\lambda_{D}$ , $\\lambda_{M_{s}}$ and $\\lambda_{M_{t}}$ control the contribution of each component. We investigate the sensitivity of the model to the hyperparameters $\\lambda_{D},\\lambda_{M_{s}}$ and $\\lambda_{M_{t}}$ on the same 10 source-target domain pairs as mentioned in Section 5.1 of three different datasets. We vary the currently investigated hyperparameter in the range of $0.6\\sim1.4$ and fix other hyperparameters at 1. ACON- $\\lambda_{D}$ refers that the currently investigated hyperparameter is $\\lambda_{D}$ , and others are analogous. From Figure 3, we observe that the performance of ACON is quite stable to the hyperparameters in Equation (11). Although setting all the trade-offs to 1 may not achieve the optimal performance, ACON still significantly outperforms the advanced baseline. It implies that without the demand for careful hyperparameter tuning, ACON has the potential to achieve superior performance on a wider range of datasets. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, the phenomenon is revealed\u2014\u2014that temporal features exhibit better transferability across domains, whereas frequency features tend to be more discriminative within a specific domain. Based on the findings, Adversarial CO-learning Networks (ACON) is proposed to boost the transferability and discriminability in a collaborative learning manner. Specifically, multi-period feature learning is proposed to enhance the discriminability of frequency features; temporal-frequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; domain adversarial learning in temporal-frequency correlation subspace is proposed to further enhance transferability of features. ACON achieves state-of-the-art performance on a wide range of time series datasets. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (62306085, 62206074, 62406112, 62476071, 62236003), Shenzhen College Stability Support Plan (GXWD20231130151329002, GXWD20220811173233001, GXWD20220817144428005). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition using smartphones. In Esann, 2013.   \n[2] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alexey Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. In ICLR, 2021.   \n[3] Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang, and Zhenjie Zhang. Time series domain adaptation via sparse associative structure alignment. In AAAI, 2021.   \n[4] Chris Chatfield and Haipeng Xing. The analysis of time series: an introduction with R. 1981.   \n[5] Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng, Xinyu Jin, and Xian-Sheng Hua. Homm: Higher-order moment matching for unsupervised domain adaptation. In AAAI, 2020.   \n[6] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised domain adaptation. In CVPR, 2019.   \n[7] Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. In ICML, 2019.   \n[8] Xinyang Chen, Sinan Wang, Jianmin Wang, and Mingsheng Long. Representation subspace distance for domain adaptation regression. In ICML, 2021.   \n[9] Hohyun Cho, Minkyu Ahn, Sangtae Ahn, Moonyoung Kwon, and Sung Chan Jun. Eeg datasets for motor imagery brain\u2013computer interface. GigaScience, 2017.   \n[10] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In NeurIPS, 2015.   \n[11] Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad Javad Darvishi Bayazi, Pooneh Mousavi, Guillaume Dumas, and Irina Rish. Woods: Benchmarks for out-of-distribution generalization in time series. TMLR.   \n[12] Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad Javad Darvishi Bayazi, Pooneh Mousavi, Guillaume Dumas, and Irina Rish. Woods: Benchmarks for out-of-distribution generalization in time series. TMLR, 2023.   \n[13] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 2016.   \n[14] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. Circulation, 2000.   \n[15] Xiaoqing Guo, Chen Yang, Baopu Li, and Yixuan Yuan. Metacorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation. In CVPR, 2021.   \n[16] Huan He, Owen Queen, Teddy Koker, Consuelo Cuevas, Theodoros Tsiligkaridis, and Marinka Zitnik. Domain adaptation for time series under feature and label shifts. In ICML, 2023.   \n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS Deep Learning Workshop, 2014.   \n[18] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and PierreAlain Muller. Deep learning for time series classification: a review. Data mining and knowledge discovery, 2019.   \n[19] Vinay Jayaram and Alexandre Barachant. Moabb: trustworthy algorithm benchmarking for bcis. Journal of neural engineering, 2018.   \n[20] Jennifer R Kwapisz, Gary M Weiss, and Samuel A Moore. Activity recognition using cell phone accelerometers. ACM SIGKDD, 2011.   \n[21] Min-Ho Lee, O-Yeon Kwon, Yong-Jeong Kim, Hong-Kyung Kim, Young-Eun Lee, John Williamson, Siamac Fazli, and Seong-Whan Lee. Eeg dataset and openbmi toolbox for three bci paradigms: An investigation into bci illiteracy. GigaScience, 2019.   \n[22] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In NeurIPS, 2021.   \n[23] Qiao Liu and Hui Xue. Adversarial spectral kernel matching for unsupervised time series domain adaptation. In IJCAI, 2021.   \n[24] Sergey Lobov, Nadia Krilova, Innokentiy Kastalskiy, Victor Kazantsev, and Valeri A. Makarov. Latent factors limiting the performance of semg-interfaces. Sensors, 2018.   \n[25] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015.   \n[26] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In NeurIPS, 2018.   \n[27] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. In ICLR, 2022.   \n[28] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. In ICLR, 2023.   \n[29] Yilmazcan Ozyurt, Stefan Feuerriegel, and Ce Zhang. Contrastive learning for unsupervised domain adaptation of time series. In ICLR, 2022.   \n[30] Sanjay Purushotham, Wilka Carvalho, Tanachat Nilanon, and Yan Liu. Variational recurrent adversarial deep domain adaptation. In ICLR, 2017.   \n[31] Mohamed Ragab, Emadeldeen Eldele, Wee Ling Tan, Chuan-Sheng Foo, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, and Xiaoli Li. Adatime: A benchmarking suite for domain adaptation on time series data. ACM TKDD, 2023.   \n[32] Gerwin Schalk, Dennis J McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R Wolpaw. Bci2000: a general-purpose brain-computer interface (bci) system. IEEE Transactions on biomedical engineering, 2004.   \n[33] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NeurIPS, 2015.   \n[34] Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018.   \n[35] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kj\u00e6rgaard, Anind Dey, Tobias Sonne, and Mads M\u00f8ller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In ACM SenSys, 2015.   \n[36] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016.   \n[37] Mario Giovanni Terzano, Liborio Parrino, Adriano Sherieri, Ronald Chervin, Sudhansu Chokroverty, Christian Guilleminault, Max Hirshkowitz, Mark Mahowald, Harvey Moldofsky, Agostino Rosa, et al. Atlas, rules, and recording techniques for the scoring of cyclic alternating pattern (cap) in human sleep. Sleep medicine, 2001.   \n[38] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.   \n[39] Rui Wang, Masao Utiyama, Andrew Finch, Lemao Liu, Kehai Chen, and Eiichiro Sumita. Sentence selection and weighting for neural machine translation domain adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018.   \n[40] Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen, and Eiichiro Sumita. Instance weighting for neural machine translation domain adaptation. In EMNLP, 2017.   \n[41] Garrett Wilson, Janardhan Rao Doppa, and Diane J Cook. Multi-source deep domain adaptation with weak supervision for time-series sensor data. In KDD, 2020.   \n[42] Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with $10k$ parameters. In ICLR, 2023.   \n[43] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In NeurIPS, 2023.   \n[44] Wangjie You, Pei Guo, Juntao Li, Kehai Chen, and Min Zhang. Efficient domain adaptation for non-autoregressive machine translation. In ACL, 2024.   \n[45] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022.   \n[46] Yang Zou, Zhiding Yu, B.V.K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Detailed Statistics ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We conduct extensive experiments using a wide range of time series datasets. The detailed statistics for each dataset is included in Table 5. For EMG dataset, we use the processed version released by DIVERSIFY [28]. For PCL, CAP and HHAR-D datasets, we use the processed versions released by WOODS [11]. For UCIHAR, HHAR-P, WISDM and FD datasets, we use the processed versions released by AdaTime [31]. ", "page_idx": 12}, {"type": "table", "img_path": "cIBSsXowMr/tmp/c913c8f1c0fff42b2c83eb0271b28d778bb494fe4416a36fff509218a8ec2c9c.jpg", "table_caption": ["Table 5: Summary of datasets. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.2 Dataset Processing ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Each domain of datasets is randomly divided into $80\\%$ training, and $20\\%$ testing. We follow [31], apply ${\\cal Z}_{}$ -score normalization to both the training and testing splits of the data, using the following equation: ", "page_idx": 12}, {"type": "equation", "text": "$$\nx_{i}^{n o r m a l i z e}=\\frac{x_{i}-x^{m e a n}}{x^{s t d}},\\quad i=1,2,\\ldots,N\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $N=N_{s}$ for the source domain data and $N=N_{t}$ for the target domain data. Note that both the training and testing splits are normalized based on the training set statistics only. ", "page_idx": 12}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The experiments were conducted on a single NVIDIA GeForce696 RTX 4090 with 24GiB of memory. As shown in Figure 3, without tuning the trade-offs of training loss, ACON still achieves significant improvements. Here we report other key hyperparameters for ACON in Table 6. Additional hyperparameters can be found in our code. In all experiments, we adopt 3-layer 1D-CNN as the temporal feature extractor (the specific structure is kept consistent with the existing works [31, 16]). For frequency feature extraction, we adopt a 1-layer complex-valued linear as the frequency feature extractor. ", "page_idx": 12}, {"type": "table", "img_path": "cIBSsXowMr/tmp/a57b634e0bca7d20746d1ff775338081b61d5b7c37f1cbdd673aea24b8d22fd1.jpg", "table_caption": ["Table 6: Key hyperparameters for ACON. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Further Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Discriminability of Frequency Feature ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Table 7, we report the average accuracy of classification experiments under the setting of Section 3.2 using five different datasets: UCIAHR[1], HHAR-P[35], WISDM[20], CAP[14, 37] and FD[31]. For each dataset, we collect all the domains involved in the selected 10 domain pairs as mentioned in Section 5.1, and perform the classification task on them. ", "page_idx": 12}, {"type": "text", "text": "Table 7: Classification Accuracy $(\\%)$ in the source domain: Temporal domain vs. Frequency domain. ", "page_idx": 13}, {"type": "table", "img_path": "cIBSsXowMr/tmp/9f586176127039c31d1ccf50a8ef4aea831d4ddf0953922907e0962aabcb8ea9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C.2 Transferability of Temporal Feature ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Table 8, we report the average accuracy of classification experiments under the setting of Section 3.3 using three different datasets: UCIAHR[1], HHAR-P[35], WISDM[20], CAP[14, 37] and FD[31]. For each dataset, we perform the domain adaptation and classification task on the selected 10 domain pairs as mentioned in Section 5.1. ", "page_idx": 13}, {"type": "table", "img_path": "cIBSsXowMr/tmp/922198b8dcceaaf7e60cab5ae6fdd23a5f6ca7e6fd1bd3f1aba198c03b203704.jpg", "table_caption": ["Table 8: Classification Accuracy $(\\%)$ in the target domain: Temporal domain vs. Frequency domain. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C.3 Ablation Study ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.3.1 Ablation Study on different modules ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We conduct ablation experiments on three datasets, UCIHAR, HHAR-P and WISDM. For each datasets, we select the same 10 source-target domain pairs as mentioned in Section 5.1. The ablation results (average accuracy of 10 domain pairs) are presented in Table 9. We verify the effectiveness of all learning modules in the proposed method by answering the following questions. ", "page_idx": 13}, {"type": "text", "text": "Can multi-period frequency feature learning enhance the discriminability of frequency feature?By comparing the 2nd and 3rd rows, we can observe that with multi-period frequency feature learning, the model makes more accurate predictions on the target domain. Meanwhile, compared with 1st row, even with multi-period frequency feature learning, the performance on the target domain is still inferior to the classification on the temporal domain, which is consistent with our conclusion in Section 3.3 that the temporal features have better transferability. ", "page_idx": 13}, {"type": "text", "text": "Can aligning distributions in temporal-frequency subspace effectively learn domain-invariant features? By comparing the 1st, 3rd and 4th rows, we can observe that distribution alignment significantly improves the performance. It indicates that by aligning the distributions in the temporalfrequency subspace, the model learns more domain-invariant features. ", "page_idx": 13}, {"type": "text", "text": "Can temporal-frequency domain mutual learning leverage the respective advantages? By comparing the 1st, 3rd and 5th rows, we can observe that the model with domain mutual learning outperforms the model using only the temporal domain or only the frequency domain. It demonstrates that via domain mutual learning, the temporal domain and frequency domain successfully transfer meaningful knowledge, leveraging their respective advantages. ", "page_idx": 13}, {"type": "text", "text": "Can the different modules mutually promote each other? By comparing the 3rd, 4th, 5th and 6th rows, we can observe that with all modules, the model achieves the optimal performance. Specifically, with multi-period frequency feature learning, the frequency domain transfers more discriminative knowledge to the temporal domain; with aligning distribution in temporal-frequency subspace, the temporal domain transfers more transferable knowledge to the frequency domain; with aligning distribution in temporal-frequency subspace and the temporal domain as a more tranferbale teacher, both the temporal domain and frequency domain learn domain-invariant features; with domain mutual learning, the frequency domain and the temporal domain enhance each other in the transfer progress, achieving a synergistic effect. ", "page_idx": 13}, {"type": "text", "text": "C.3.2 Ablation Study on different feature fusion strategies ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we further investigate the impact of different strategies to fuse temporal features and frequency features. We divide the training framework into three modules: feature extraction module ( $\\psi_{T}$ and $\\psi_{F}$ ), classification module ( $g_{T}$ and $g_{F}$ ), and alignment module $(g_{D})$ . In the feature extraction module, ACON contains a temporal feature extractor and a frequency feature extractor. In the classification module, ACON contains a temporal feature classifier and a frequency feature classifier and adopts mutual learning to leverage the respective advantages of the temporal domain and frequency domain. In the alignment module, ACON calculates the outer product of temporal features and frequency features and performs alignment in temporal-frequency correlation subspace. ", "page_idx": 13}, {"type": "table", "img_path": "cIBSsXowMr/tmp/397a3df44eb373feb283eabdc5c5f208bd0fdecee84e7aad14c5f34d82e23073.jpg", "table_caption": ["Table 9: Ablation study on different modules: Average Accuracy $(\\%)$ on UCIHAR, HHAR-P and WISDM. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "With the same feature extraction module and alignment module, in classification module, we compare 3 different strategies: (1)\u201cconcat\u201d: use a classifier to perform classification on the concatenation of temporal features and frequency features (the 5th row in Table 10). (2)\u201c2-classifiers\u201d: use two classifiers to perform classification in the temporal domain and the frequency domain respectively (the 4th row in Table 10). (3) \u201cmutual learning\u201d: the temporal-frequency domain mutual learning used in ACON, which is based on $^{\\bullet\\bullet}2$ -classifiers\u201d (the 6th row in Table 10). ", "page_idx": 14}, {"type": "text", "text": "With the same feature extraction module and classification module, in alignment module, we compare 2 different strategies: (1) f $\\bigoplus\\mathbf{z}$ : align the concatenation of temporal features and frequency features between the source domain and target domain (the 3rd row in Table 10). (2) f $\\otimes\\mathbf{z}$ : align the outer product of temporal features and frequency features between the source domain and target domain, i.e., the alignment in temporal-frequency correlation subspace used in ACON (the 6th row in Table 10). ", "page_idx": 14}, {"type": "text", "text": "DANN-T refers to only performing alignment in the temporal domain. DANN-F refers to only performing alignment in the frequency domain. DANN-TF refers to performing alignment in both the temporal domain and the frequency domain. ", "page_idx": 14}, {"type": "table", "img_path": "cIBSsXowMr/tmp/06113b5de7cdf649749809eb97a2f0635a62febedcbe20c29952ae307478aab9.jpg", "table_caption": ["Table 10: Ablation study on different feature fusion strategies: Average Accuracy $(\\%)$ on UCIHAR, HHAR-P and WISDM. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "In the classification module, by comparing the 4th, 5th and 6th rows in Table 10, we can observe that concatenation features do not achieve better performance. Intuitively, when we use the concatenation feature for classification, the final prediction is a simple average of the temporal prediction and the frequency prediction. Ideally, the extracted features can only maintain suboptimal discriminability and suboptimal transferability. In contrast, our mutual learning allows the frequency domain and temporal domain to become teachers with different advantages, and transfer knowledge to each other. In this way, each other\u2019s secondary quantities (e.g. the estimates of the probabilities of the next most likely classes) are transferred [17], enhancing the discriminability and transferability. ", "page_idx": 14}, {"type": "text", "text": "In the alignment module, by comparing the 3rd and 6th rows in Table 10, we can observe that with the concatenation features, DANN significantly underperforms our method. Intuitively, when we use the concatenation features for alignment, $\\mathbf{f}_{F}$ with the worse transferability provides rich domain-label relevant information for the domain discriminator $g_{D}$ . In this case, $g_{D}$ only needs to focus on the game with $\\psi_{F}$ in the frequency domain, ignoring the domain adversarial learning in the temporal domain. Based on the experimental performance and intuition, we discard the simple fusion and opt for co-alignment in the temporal-frequency correlation subspace. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Although ACON boosts transferability and discriminability for time series Domain adaptation, like existing DA methods, it is still unstable enough in time series with relatively large variances. This is a problem that needs to be solved urgently in the future. ", "page_idx": 15}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We investigate how to boost Transferability and discriminability for domain adaptation in time series classification. We reveal that the frequency features are more discriminative, while the temporal features are more transferable. Upon this, we propose multi-period frequency feature learning, domain mutual learning, and distribution alignment in temporal-frequency feature subspace. The purpose of our research is to advance the research progress in the relevant community without any negative social impact. ", "page_idx": 15}, {"type": "text", "text": "F Full Resluts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present all experimental results in this section. Notably, our model achieves superior performance, yielding improvements of more than $6\\%$ in terms of accuracy and $4\\%$ in terms of Macro-F1 across 8 cross-domain time series datasets and 5 common applications on average. ", "page_idx": 15}, {"type": "table", "img_path": "cIBSsXowMr/tmp/525467fd2773ea016c1786420a21dd1860d1f58e0a1c33de4f82c67144d49a1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cIBSsXowMr/tmp/bfbcd7b16f30f3bd9936817379f43504accfc80c496eda585c9e90a1f1f7f13c.jpg", "table_caption": ["Table 13: Accuracy $(\\%)$ on FD for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "cIBSsXowMr/tmp/a176f5638aecdda744483c4477f17bb7482101453cd04bedbeb561d35eccd139.jpg", "table_caption": ["Table 14: Accuracy $(\\%)$ on UCIHAR for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "cIBSsXowMr/tmp/c3a021e4a90517aae981a5e453d4ba30b8cbda52e332a302a5649c64ea172fec.jpg", "table_caption": ["Table 15: Accuracy $(\\%)$ on WISDM for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "cIBSsXowMr/tmp/ace11862aff95386c6673e2bb8e50fce4c2ba73b8f7358ff48b2e48541875c7b.jpg", "table_caption": ["Table 16: Accuracy $(\\%)$ on HHAR-D for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "cIBSsXowMr/tmp/b97e3a36f7a86b56f941e0087b2bfff3fa2f4add2ae3f8db346d3f4e6a7b7563.jpg", "table_caption": ["Table 17: Average Macro-F1 Score on Eight Datasets and Five Applications for UDA "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cIBSsXowMr/tmp/82fc2be9bb18bec3e317206684f14dfe1ed7dc96612c962bf5898593522dc8bb.jpg", "table_caption": ["Table 18: Macro-F1 Score on EMG for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cIBSsXowMr/tmp/8585ea59cf377c04440bbdcedf47b5ef22fa160d47a2fd038123b5fe201173f8.jpg", "table_caption": ["Table 19: Macro-F1 Score on CAP for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cIBSsXowMr/tmp/3bb748cf68e2f610f78635de5898edbff870b8e7fb4f474342ecdb417217aba8.jpg", "table_caption": ["Table 20: Macro-F1 Score on PCL for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cIBSsXowMr/tmp/40cced5e41a1e297c135d16818fb5690d58df416a3420d35a6e2c83427e4eb7e.jpg", "table_caption": ["Table 21: Macro-F1 Score on FD for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cIBSsXowMr/tmp/40265722bad36a1ff2f8178c0ebe5c9b1a7aa04489a445c74bbf1e13a1f0a5e1.jpg", "table_caption": ["Table 22: Macro-F1 Score on UCIHAR for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cIBSsXowMr/tmp/a7df0c3a476ac56523393d774312eac5b658cc2f9e97a90b2359366bfffc8eb0.jpg", "table_caption": ["Table 23: Macro-F1 Score on HHAR-P for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cIBSsXowMr/tmp/1e9c48de230481f1f12b65c89993e4004cc780427dd3cd4df09f2412bffb861f.jpg", "table_caption": ["Table 24: Macro-F1 Score on WISDM for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cIBSsXowMr/tmp/2e0cb809edf5f92ee93fc09ad32d0fc6efa269a8b4cd733df49bfa600b93bc71.jpg", "table_caption": ["Table 25: Macro-F1 Score on HHAR-D for unsupervised domain adaptation. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 20}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 20}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 20}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 20}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 20}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We include detailed information in Section 1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The limitations are included in Appendix D. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The theory assumptions are included in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include the detailed experimental settings in Appendix B. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Code is available at the anonymous link: https://anonymous.4open.   \nscience/r/ACON. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 22}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We include the detailed experimental settings in Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The results are included in Appendix F. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the experiments in this paper are conducted on a single NVIDIA GeForce RTX 4090 with 24GiB of memory. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In every respect in the paper, we follow the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Broader impacts is included in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All data, models, and code in the paper respect the license. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]