[{"figure_path": "g6nn2AijDp/tables/tables_6_1.jpg", "caption": "Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP colum cell indicates 9 different visual patterns (details in Appendix. C).", "description": "This table presents the results of hallucination evaluation on discriminative benchmarks using six different large multimodal models (LMMs) and six decoding methods.  The discriminative benchmarks, POPE and MMVP, assess hallucination by focusing on the correctness of model predictions on object-level and subtle visual detail questions respectively. The table shows precision, F1-score, and accuracy for the POPE benchmark, and accuracy for individual visual pattern categories within the MMVP benchmark.  The results are broken down by model, parameter count, and decoding method, showing how CODE compares to other state-of-the-art methods.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_7_1.jpg", "caption": "Table 2: GPT-aided evaluation results among 6 LMMs and decoding methods on generative benchmarks (LLaVA-QA90 [42]: score ratio for GPT answer / MMHal-Bench [52]: score rated by GPT).", "description": "This table presents the results of GPT-aided evaluations of six different Large Multimodal Models (LMMs) using six different decoding methods on two generative benchmarks: LLaVA-QA90 and MMHal-Bench.  LLaVA-QA90 uses a score ratio based on GPT-4 ratings of model responses compared to ground truth, covering conversation, detailed description, and complex reasoning question types. MMHal-Bench provides a GPT-4 hallucination score (0-7, higher is better) across eight question types assessing various aspects of visual understanding.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_7_2.jpg", "caption": "Table 3: Ablation study on  \u03b1t (DR) and \u03b2t (AIC) for (abbreviated) LV1.5 [40], LV-N [41], IVL1.5 [8] on two benchmarks [53, 42]. We report overall scores for the benchmarks. X indicates fixed hyper-parameter for \u03b1 and \u03b2.", "description": "This ablation study investigates the impact of the dynamic restriction (DR) and adaptive information constraint (AIC) components of the CODE model on two benchmarks: MMVP and LLaVA-Bench.  It evaluates three different LLM models: LLaVA-1.5, LLaVA-NeXT, and InternVL 1.5.  The results show the performance of each model under different combinations of DR and AIC (enabled/disabled), with the final row representing the full CODE model where both techniques are enabled.", "section": "4.2 Analyses on CODE"}, {"figure_path": "g6nn2AijDp/tables/tables_8_1.jpg", "caption": "Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP column cell indicates 9 different visual patterns (details in Appendix. C).", "description": "This table presents the results of hallucination evaluation on discriminative benchmarks (POPE, MMVP) for six different Large Multimodal Models (LMMs) using six different decoding methods (greedy, beam search, nucleus, OPERA, VCD, and CODE).  The performance is measured using precision, F1-score, and accuracy for POPE and average accuracy for MMVP.  For MMVP, emojis represent different visual patterns used in the evaluation.  The table highlights the performance improvements achieved by the CODE method compared to baselines.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_8_2.jpg", "caption": "Table 4: Computational analysis on decoding throughput and latency among CD-based methods. We compare three different model sizes.", "description": "This table presents the results of a computational analysis comparing three different contrastive decoding methods (VCD, OPERA, and CODE) across three different model sizes (7B, 14B, and 34B parameters).  For each model size and method, the throughput (tokens processed per second) and latency (milliseconds per token) are reported. The results show the relative efficiency of each method in terms of speed and resource utilization.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_13_1.jpg", "caption": "Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP column cell indicates 9 different visual patterns (details in Appendix. C).", "description": "This table presents the performance of different decoding methods (Greedy, Beam, Nucleus, Opera, VCD, and CODE) on several Large Multimodal Models (LMMs) across two discriminative benchmarks: POPE and MMVP.  POPE evaluates object-level hallucinations, while MMVP assesses the models' ability to discern subtle visual differences in image pairs. The results are shown in terms of precision, F1-score, and accuracy for POPE, and accuracy for each of the nine visual patterns in MMVP, along with an average accuracy across all patterns.  The table allows comparison of the proposed CODE method against standard decoding methods on various state-of-the-art LMMs.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_14_1.jpg", "caption": "Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP column cell indicates 9 different visual patterns (details in Appendix. C).", "description": "This table presents the results of hallucination evaluation on discriminative benchmarks (POPE, MMVP) across six different Large Multi-modal Models (LMMs) and six decoding methods.  Each LMM is evaluated using three metrics for POPE and multiple metrics for MMVP.  The MMVP results use emojis to represent performance across nine different visual patterns, as detailed in Appendix C of the paper.  The table allows for a comparison of the effectiveness of various decoding strategies in mitigating hallucinations in different LMMs.", "section": "4.1 Evaluation Results"}, {"figure_path": "g6nn2AijDp/tables/tables_15_1.jpg", "caption": "Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP column cell indicates 9 different visual patterns (details in Appendix. C).", "description": "This table presents the results of hallucination evaluation using discriminative benchmarks (POPE, MMVP) on six different Large Multimodal Models (LMMs) and six decoding methods.  For each LMM and decoding method, the precision, F1 score, and accuracy are shown for the POPE benchmark, while the average accuracy across nine visual patterns are provided for the MMVP benchmark. The emoji icons in the MMVP section indicate the nine different visual patterns assessed in that benchmark, allowing for a detailed comparison of hallucination mitigation techniques across various LMM architectures and decoding strategies.", "section": "4.1 Evaluation Results"}]