[{"figure_path": "g6nn2AijDp/figures/figures_1_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering DEscription Contrastive Decoding) method.  The process starts with visual content (an image of a dog catching a frisbee) which is processed by a Large Multimodal Model (LMM). The LMM generates a comprehensive description of the image. Then, during the decoding phase, the LMM generates an ongoing response to a user question. The CODE method contrasts the logit information from the visual content and the generated comprehensive description. This contrasting helps the model to generate more contextually appropriate and correct responses, reducing hallucinations by suppressing inconsistent words in the generated text (e.g., replacing \"catching\" with \"hit\" in this example).", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_3_1.jpg", "caption": "Figure 2: The comparison is based on two benchmarks (MMVP [53]: multiple choice/LLaVA-Bench [42]: description-level). The plain and dotted bars indicate the results for the models that use self-generated descriptions as visual input replacements and original model with actual visual contents, respectively.", "description": "This figure compares the performance of models using self-generated descriptions as visual input replacements versus the original models using actual visual content. The comparison is performed on two benchmarks: MMVP (multiple choice) and LLaVA-Bench (description-level).  The bar chart displays the results, with plain bars representing models using self-generated descriptions and dotted bars representing original models. This visualization helps to illustrate the impact of using self-generated descriptions as a source of visual information.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_5_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering DEscription Contrastive Decoding) method.  CODE leverages self-generated descriptions to improve the accuracy of LMMs' responses.  The model generates a detailed description of the input image (d). During decoding, it contrasts this description with the visual content (v) to produce logits. By contrasting the likelihoods from both visual and textual information, the model generates more accurate and contextually appropriate responses, reducing hallucinations.  An example is shown where the inconsistent word \"catching\" is corrected to \"hit\" based on the contrastive decoding.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_8_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering DEscription Contrastive Decoding) method.  CODE uses a large multimodal model (LMM) to generate a detailed description of an image. This description is then used as a contrasting reference during the decoding phase. The LMM recursively outputs logits based on both the original visual content and the self-generated description. By comparing these logits, CODE adjusts the information flow and improves response alignment with the actual visual content, reducing hallucinations.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_13_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering Description Contrastive Decoding) method.  The process starts with a Large Multimodal Model (LMM) receiving visual content (v) and a user's instruction. The LMM first generates a detailed description of the visual content (d) independently. Then, during the decoding phase of generating a response, CODE contrasts the logit information from both the visual content and the self-generated description. By dynamically adjusting the information flow based on this comparison, CODE aims to improve response accuracy and consistency with the visual content, mitigating hallucination issues.  The example shows how the model shifts from an incorrect prediction ('catching') to a correct one ('hit') by leveraging the contrastive information.", "section": "3 Proposed Method"}, {"figure_path": "g6nn2AijDp/figures/figures_14_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering Description Contrastive Decoding) method.  It shows how a large multimodal model (LMM) generates a detailed description of an image (v). This description (d) acts as a contrastive reference during decoding. The LMM then recursively generates logits based on both the image (v) and its self-generated description (d). By contrasting these logits, CODE aims to produce more accurate responses aligned with the actual visual content, correcting inconsistencies like replacing \"catching\" with \"hit\" in a response regarding an image of a dog.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_15_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering Description Contrastive Decoding) method.  CODE uses a large multimodal model (LMM) to generate a detailed description of an image. This description acts as a \"visual counterpart\" during the decoding process. The LMM then recursively outputs logits based on both the original visual input and the self-generated description. By contrasting these logits, CODE aims to improve response accuracy and coherence by suppressing inconsistent words and ensuring that the generated response aligns with the actual visual content. The example shows how the word \"catching\" is corrected to \"hit\" because the self-generated description provides a more accurate contextual understanding of the image.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_15_2.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering DEscription Contrastive Decoding) method.  CODE leverages self-generated descriptions as contrasting references during the decoding phase of Large Multimodal Models (LMMs) to reduce hallucinations. The model generates a detailed description of the input image (d), which is then used alongside the original image content (v) to generate the final response. By comparing the log probabilities from both sources, CODE refines the output, prioritizing responses that align with the actual visual content and suppressing hallucinatory words.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_16_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (COuntering DEscription Contrastive Decoding) method.  CODE uses a large multimodal model (LMM) to generate a detailed description of an image.  This description is then used as a contrastive reference during the decoding phase, where the model compares logits from the original image (v) and the self-generated description (d). This contrastive process helps to correct and improve the response alignment with the actual visual content, reducing hallucinations by suppressing inconsistent words. The example shown highlights how the word \"catching\" is corrected to \"hit\", based on the contrastive information.", "section": "3 Proposed Method"}, {"figure_path": "g6nn2AijDp/figures/figures_17_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the overall decoding procedure of the CODE method.  It shows how a Large Multimodal Model (LMM) generates a detailed description of an image (visual content). This description is then used as a contrasting reference during the decoding process. The LMM recursively outputs logits (probabilities) for each token based on both the original visual content and the self-generated description. By contrasting these logits, CODE aims to produce more accurate and contextually relevant responses that align with the image content, correcting any inconsistencies or hallucinations.", "section": "3 Proposed Method"}, {"figure_path": "g6nn2AijDp/figures/figures_18_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering DEscription Contrastive Decoding) method.  It shows how a Large Multimodal Model (LMM) generates a detailed description of an image (d). This description is then used contrastively during the decoding phase, where the model's next-token predictions are adjusted based on comparing logits from the actual visual content (v) and the self-generated description (d). This process helps to correct inconsistencies and improve the alignment of the model's response with the actual visual content, reducing hallucinations. The example highlights how the word 'catching' is replaced with 'hit' because the self-generated description provides a contrasting view.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_19_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering DEscription Contrastive Decoding) method.  CODE uses a large multimodal model (LMM) to generate a detailed description of an image.  This description acts as a contrasting reference during the decoding process for a user's question about the image. The LMM produces logits (probabilities) for the next word based on both the original visual input and the self-generated description. By contrasting these logits, CODE aims to improve the accuracy and coherence of the LMM's response by suppressing inaccurate or hallucinated words.", "section": "3 Proposed Method"}, {"figure_path": "g6nn2AijDp/figures/figures_20_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering Description Contrastive Decoding) method.  It shows how a large multimodal model (LMM) generates a detailed description of an image. This description is then used as a contrasting reference during the decoding process. The model recursively outputs logits (probabilities) for each word based on both the visual content and self-generated description. By comparing these probabilities, CODE aims to select words that are consistent with both the image and the model's understanding, reducing hallucinations.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_21_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering DEscription Contrastive Decoding) method.  The process begins with a Large Multimodal Model (LMM) generating a comprehensive description of the input image. This description serves as a contrast to the image itself during the decoding process.  The LMM recursively outputs logits (predicted probabilities) for the next token based on both the image and its self-generated description. By comparing the likelihoods, the model refines its response to align better with the actual visual content. The example shown highlights how contrasting the log-likelihoods corrects an inconsistent word from \"catching\" to \"hit\", improving accuracy.", "section": "3 Proposed Method"}, {"figure_path": "g6nn2AijDp/figures/figures_22_1.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering DEscription Contrastive Decoding) method.  CODE uses a large multimodal model (LMM) to generate a detailed description of an image. This description acts as a contrastive reference during the decoding process. The model generates logits (probabilities) for the next word based on both the original image and its self-generated description. By comparing these probabilities, CODE refines the model's response to be more accurate and consistent with the actual image content, reducing hallucinations.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}, {"figure_path": "g6nn2AijDp/figures/figures_22_2.jpg", "caption": "Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each v and d. By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching\u2192hit).", "description": "This figure illustrates the CODE (Countering DEscription Contrastive Decoding) method.  The process begins with a Large Multimodal Model (LMM) receiving visual content (v). The LMM generates a comprehensive self-description (d) of the visual content.  The model then uses both the visual content (v) and its self-description (d) to predict the next token in the response generation process. By contrasting the likelihoods from both v and d, the model improves the response accuracy and consistency, reducing hallucinations and making it more aligned with the actual image content. An example is provided showing how inconsistent words, such as \"catching\" being corrected to \"hit\" based on the visual context.", "section": "3.1 Comprehensive Image Description as Visual Counterpart"}]