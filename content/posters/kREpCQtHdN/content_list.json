[{"type": "text", "text": "Identifying Latent State-Transition Processes for Individualized Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuewen Sun Biwei Huang Yu Yao Donghuo Zeng   \nMBZUAI & CMU UCSD USYD KDDI Research   \nXinshuai Dong Songyao Jin Boyang Sun Roberto Legaspi CMU UCSD MBZUAI KDDI Research Kazushi Ikeda Peter Spirtes Kun Zhang KDDI Research CMU MBZUAI & CMU ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, the application of reinforcement learning (RL) involving interactions with individuals has seen significant growth. These interactions, influenced by individual-specific factors ranging from personal preferences to physiological differences, can causally affect state transitions, such as health conditions in healthcare or learning progress in education. Consequently, different individuals may exhibit different state-transition processes. Understanding these individualized state-transition processes is crucial for optimizing individualized policies. In practice, however, identifying these state-transition processes is challenging, especially since individual-specific factors often remain latent. In this paper, we establish the identifiability of these latent factors and present a practical method that effectively learns these processes from observed state-action trajectories. Our experiments on various datasets show that our method can effectively identify the latent state-transition processes and help learn individualized RL policies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) [46] involves training agents to make decisions by interacting with the environment. The agent observes its current state, takes an action, and transitions to a new state with a reward. Such a sequence of moving from one state to another is known as a state-transition process. ", "page_idx": 0}, {"type": "text", "text": "Individualized RL focuses on adapting the policy for each individual. It has recently seen increasing application in various sectors, including healthcare [89, 57, 21], education [68, 2, 14], and e-commerce [53, 87, 1]. The individual-specific factors [60], which embed the unique characteristics of each individual, play a crucial role in causally influencing the transitions between states. Such factors range from individual preferences and past experiences to physiological differences. For example, in the realm of education, different learning styles can affect how two students with the same prior knowledge learn from a tutorial. In healthcare, differences in genetic makeup can affect how two hypertension patients respond to identical treatments. Understanding individual-specific factors is essential for designing better RL systems that provide more individualized and effective decisions [41, 28, 4, 65]. With knowledge of learning styles, the RL agents can recommend personalized tutorials, such as animated content for visual learners or hands-on exercises for kinesthetic learners. Similarly, in healthcare, such knowledge of genetic makeup can help agents suggest treatment plans tailored to their specific needs, leading to improved health outcomes. ", "page_idx": 0}, {"type": "image", "img_path": "kREpCQtHdN/tmp/c68848fcd34aebcd65112a6baad66bcff8785d6177d1f8842321c04aee3221ce.jpg", "img_caption": ["Figure 1: Comparisons of different state-transition processes. Latent variables are colored in grey. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, these individual-specific factors are not always observable, which poses challenges to understanding the Latent Individualized State-Transition (LIST) processes, as shown in Figure 1(a). The latent individual-specific factors are unique (e.g., learning styles, genetic makeup, etc.) to each individual and have a time-invariant influence on the state-transition process. This raises the question: can we guarantee the identifiability of the latent factors? ", "page_idx": 1}, {"type": "text", "text": "Such identifiability is easier to achieve if the observations are either i.i.d., or i.i.d. given side information (e.g., domain index, time index, etc.) by exploiting sparsity [97], variability [47], or functional complexity [43]. To the best of our knowledge, only a few studies have attempted to uncover the identifiability of latent factors from temporal observations. These methods focus on the time-varying latent factors, which is very different from our work on time-invariant latent factors. Specifically, existing works [86, 85, 7] assume the time-varying latent variables without considering the influence of actions in the generative process (see Figure 1(b)). Factored MDPs[15] incorporate actions into the process but still assume that the latent factors change over time (see Figure 1(c)). Thus the results from existing work cannot be applied in our setting. Intuitively, this is because the timeinvariant latent factors cannot provide the variability that many current methods rely on to achieve identifiability. It remains unknown how to derive the identifiability of the latent individual-specific factors, together with the latent state-transition processes, from the observed states and actions. ", "page_idx": 1}, {"type": "text", "text": "Recent advances in finite mixture models [77, 67] have proven strong identifiability results by exploiting group information in nonparametric settings. By assuming that observations in the same group are known to come from the same component, the mixture of probability measures can be uniquely identified under proper assumptions. Inspired by these works, we establish the identifiability of the latent factors by leveraging group information from the data, making it easier to distinguish different underlying components. We propose both finite latent, nonparametric setting and infinite latent, parametric setting and develop a theoretically grounded framework that effectively learns these processes from observed state-action trajectories. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the Individualized Markov Decision Processes (iMDPs), a novel framework that integrates latent individual-specific factors $\\kappa$ into state-transition processes. We consider $\\kappa$ as a latent individual-specific factor, allowing it to influence each state in the decision process and to vary across different individuals. \u2022 Our work provides theoretical guarantees and new insights for learning state-transition processes with latent factors. When $\\kappa$ is finite, we consider two scenarios to derive the identifiability even if the processes are nonparametric. For infinite $\\kappa$ , we show that identifiability is guaranteed in the post-nonlinear case. To the best of our knowledge, this is the first work to provide a theoretical guarantee for the identification of latent individual-specific factors from observed transitions. \u2022 We propose a practical generative-based method that can effectively estimate the latent individualspecific factors. Empirical results on various datasets demonstrate the method\u2019s effectiveness not only in inferring these factors but also in learning individualized policies. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Individualized Machine-Learning Applications Recently, machine learning has created highly individualized solutions across various domains. In healthcare, algorithms support individualized interventions for physical activity, weight loss, and diabetes management [89, 57, 18, 17]. In finance, it provides accurate stock predictions for stock market activities [56]. Education is benefiting from individualized ICT systems that address the individual learning needs of students [16, 40]. Furthermore, transportation has seen the development of individualized car-following strategies [69] that improve driving safety and efficiency. Meanwhile, entertainment platforms such as YouTube and TikTok are using it to provide individualized video recommendations [6, 33]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning with Latent State-Transition Processes In the field of RL, various models explore the state transition dynamics with latent variables. One such approach is Partially Observable Markov Decision Processes (POMDPs) [64], where the full information about the state is unknown. In POMDPs, observations are generated from the latent states, which do not match our individual latent setting. For example, block MDPs [92, 96] assume that there is a fixed and unknown mapping from observations to the latent states. Factored MDPs [35, 15], which provide the partial identifiability of latent factors, assume that the latent factors evolve over time following a Markov process. On the other hand, there exists a piece of work focusing on estimating state transitions with time-invariant latent factors. Models such as contextual MDPs [29, 60, 65], latent MDPs [51, 50] and multitask RL [73, 24] consider similar scenarios with our latent individual-specific factors. However, these works lack theoretical guarantees on the identifiability of the latent factors thus it is hard for them to guarantee individualized decision-making. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a population with $M$ individuals that can be divided into $G$ groups, whereas the exact group membership is unknown. We introduce iMDP to model individualized decision-making, where observed individual uniqueness is represented by $u$ , and latent group-level properties are embedded by individual-specific factors $\\kappa$ . Specifically, each individual has a unique value of $u$ , and the cardinality of $u$ equals $M$ . In contrast, individuals in the same group share the same value of $\\kappa$ and have different values across different groups, and the cardinality of $\\kappa$ equals $G$ . For each individual, the value of $\\kappa$ is determined and has a time-invariant influence on the state-transition process. Suppose all individuals share the same state and action spaces. The iMDP is defined as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (iMDP). An iMDP consists of a tuple $\\langle S,A,R,\\{s_{0}^{m}\\}_{m=1}^{M},\\{u_{m}\\}_{m=1}^{M},\\{\\mathbb{T}_{m}\\}_{m=1}^{M}\\rangle$ , where $M$ is the number of individuals; $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are the state and action spaces, respectively; $R\\in\\mathbb{R}$ is the immediate reward received after transitioning from s to $s^{\\prime}$ via $a_{i}$ , i.e., $r=R_{a}(s,s^{\\prime})$ for current state $s\\in S$ , new state $s^{\\prime}\\in\\mathcal{S}$ and action $a\\in A$ . ", "page_idx": 2}, {"type": "text", "text": "For the $m^{t h}$ MDP, $u_{m}$ is the unique index identifying each individual. $s_{0}^{m}$ is the individualized initial state. $\\mathbb{T}_{m}\\ \\in\\ \\mathbb{R}^{|S|\\times|A|\\times|S|}$ is the individualized state transition probability, i.e., $\\mathbb{T}_{m}:=$ $\\mathbb{P}_{m}(s^{\\prime}|s,a,\\kappa_{m})$ . Here $\\kappa_{m}$ is the latent individual-specific factor with cardinality $G$ . Thus, the joint distribution of any adjacent state-action pairs $(s,a,s^{\\prime})$ can be specified by $u$ and $\\kappa$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}(s,a,s^{\\prime}|u)=\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a|u).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Data Generation Process Here we introduce the latent individualized state-transition processes based on iMDP. For individual $m$ , the observed states $\\mathbf{s}_{t}^{m}$ satisfy the following generation process: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{i,t}^{m}=f_{i}(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\kappa_{m},\\epsilon_{i,t}^{m}),\\quad\\mathrm{for}\\quad i=1,\\ldots,d_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{s}_{t}^{m}=(s_{0,t}^{m},...\\,,s_{d_{s},t}^{m})^{\\top}\\in\\mathbb{R}^{d_{s}}$ represents the state, and $\\mathbf{a}_{t}^{m}=(a_{0,t}^{m},\\ldots,a_{d_{a},t}^{m})^{\\top}\\in\\mathbb{R}^{d_{a}}$ the action at time $t$ , with $d_{s}$ and $d_{a}$ as the dimensions of state and action, respectively. $\\epsilon_{i,t}^{m}$ denotes independent noise term. $\\kappa$ characterizes the group-level property across individuals, and the transition function $f$ is identical across individuals, which is consistent with Eq. (1). During interaction with the environment, the trajectory $\\tau_{m}=\\{\\mathbf{s}_{0}^{m},\\mathbf{a}_{0}^{m},\\mathbf{s}_{1}^{m},\\dots,\\mathbf{s}_{T}^{m}\\}$ is recorded as a sequence of observed state-action tuples, where $T$ denotes the trajectory length. ", "page_idx": 2}, {"type": "text", "text": "Objectives In this work, we focus on the individualized RL agents with latent state-transition processes. Our objectives are twofold: 1) to identify latent individual-specific factors $\\kappa$ from observed trajectories, and 2) to derive individualized policies for each agent and realize policy adaptation for newcomers. Consider the example of hypertension diagnosis in healthcare. Treating all patients identically may lead to varied outcomes due to the dynamics of state transitions influenced by latent $\\kappa$ . Therefore, accurate identification of $\\kappa$ from the population provides crucial dynamic background knowledge. Once $\\kappa$ is uncovered, we can categorize patients into different groups and provide individualized treatments for each patient, which is consistent with our second goal. ", "page_idx": 2}, {"type": "text", "text": "4 Identifiability Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider two conditions that ensure the identifiability of latent individualized state-transition processes using either (1) group determinacy assumption or (2) functional constraints. The corresponding identifiability is established in the following theorems. ", "page_idx": 3}, {"type": "text", "text": "Finite Latent Condition Suppose the value of $\\kappa$ is finite; we first provide the definition of groupwise identifiability. For the detailed assumptions discussion and proofs, please see Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Group-wise Identifiability). Let $\\{\\tau_{m}\\}_{m=1}^{M}$ be sequences of observed states and actions collected from $G$ groups under a fixed policy, following the true latent individualized statetransition processes described in Eq. (2). A learned generative model $(\\hat{f},\\hat{\\kappa},\\hat{\\epsilon})$ is observational equivalent to $(f,\\kappa,\\epsilon)$ if the joint distribution $\\mathbb{P}_{\\hat{f},\\hat{\\kappa},\\hat{\\epsilon}}(s,a,s^{\\prime})$ matches $\\mathbb{P}_{f,\\kappa,\\epsilon}(s,a,s^{\\prime})$ everywhere. We say that the latent individualized state-transition processes are group-wise identifiable if observational equivalence can always lead to the identifiability of latent individual-specific factors across the population up to the invertible transformation $g$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\hat{f},\\hat{\\kappa},\\hat{\\epsilon}}(s,a,s^{\\prime})=\\mathbb{P}_{f,\\kappa,\\epsilon}(s,a,s^{\\prime})\\iff\\hat{\\kappa}=g(\\kappa).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 4.1 (Group Determinacy). Consider a finite mixture model $\\begin{array}{r}{\\sum_{g=1}^{G}\\pi_{g}\\delta_{\\kappa_{g}}(\\kappa)}\\end{array}$ , where $\\pi_{g}$ represents mixing proportions with $\\textstyle\\sum_{g=1}^{G}\\pi_{g}=1$ , and $\\delta_{\\kappa_{g}}$ is the Dirac function centered at $\\kappa_{g}$ . Each unique value of \u03ba corresponds to a specific group in the population, with $\\delta_{\\kappa_{g}}(\\kappa)=1\\;i f\\kappa=\\kappa_{g}$ and $\\boldsymbol{O}$ otherwise, and the number of individuals per group is greater than $2G-1$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.1 indicates that identifiability can be derived from the finite mixture model perspective using group information. We consider two scenarios to establish identifiability under finite latent conditions. Theorem 4.1 considers finite samples and specifies a minimum trajectory length with assumptions on initial states $\\{\\mathbf{s}_{0}^{m}\\}_{m=1}^{M}$ . Theorem 4.2 guarantees asymptotic identifiability for sufficiently long trajectories without initial state constraints. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. Assume the LIST processes in Eq. (2). Suppose the distributions of initial states within the same groups are the same for all individuals. Under Assumption 4.1, the identifiability of the individual-specific factor $\\kappa$ is guaranteed. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.2. Assume the LIST processes in Eq. (2). Suppose the distribution of initial state varies across individuals and the trajectory length is sufficiently long, i.e., there exist two different individuals in the same group have overlap condition $\\mathbb{P}(s,a|u\\,=\\,\\bar{u}_{i})\\,=\\,\\mathbb{P}(s,a|u\\,=\\,u_{j})$ ), $i\\ne j$ . Under Assumption 4.1, the identifiability of $\\kappa$ is asymptotically guaranteed. ", "page_idx": 3}, {"type": "text", "text": "Infinite Latent Condition The following theorem shows that under certain functional constraints, the identifiability of individual-specific latent variables can be extended to multiple and even infinite latent factors. Specifically, we consider the post-nonlinear temporal model [93] and allow multiple instances of $\\kappa$ to influence the state transition dynamics. The identifiability and the cardinality is decided upon the rank conditions of specific covariance submatrices derived from the observed data. In addition, the empirical results in Section 6 show that even when there are multiple latent factors with infinite cardinality, our estimation framework (see Section 5) still encourages the identification of latent factors. For the detailed model description and proof, please see Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.3. Consider a trajectory collected from the post-nonlinear temporal model (Definition C.1) with $d_{s}$ -dimensional observed states over time $t=1,\\dots,T_{\\!}$ . Let m latent group factors $\\kappa_{j}$ , $j=1,\\dots,m_{\\!}$ , have direct causal influence on all states, and $S_{t}=\\{s_{1,t},s_{2,t},\\ldots,s_{n,t}\\}$ represent the set of all state variables at time $t.$ . These latent factors, as well as the state-transition process, can be identified if and only if for every $i=m+2,\\ldots,T-(m+1),$ , there exist pairs of minimal rank sets (Definition C.2) $(\\mathbf{A_{i}},\\mathbf{B_{i}})$ , defined as $\\mathbf{A_{i}}=\\mathcal{R}_{i,i^{-}}$ and $\\mathbf{B_{i}}=\\mathcal{R}_{i,i^{+}}$ , where $i^{-}<i<i^{+}$ , that satisfy: ", "page_idx": 3}, {"type": "text", "text": "5 Estimation and Policy Learning Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a two-stage approach to generate individualized policies. Our method achieves two objectives: (1) constructing an estimation framework to recover the latent group factors from a collection of individual trajectories, and (2) implementing individualized policy learning to facilitate policy adaptation for new individuals. ", "page_idx": 4}, {"type": "text", "text": "Overview The proposed method is carefully designed to meet the requirements of the identifiability theorems. As specified in Definition 4.1, identifiability is ensured iff observational equivalence can always lead to latent factor equivalence. This motivates us to use a generative model to achieve latent factor estimation and ensure the learned distribution closely aligns with the true observed distribution. ", "page_idx": 4}, {"type": "text", "text": "Theorems 4.1 and 4.2 provide the guarantee that such an alignment is asymptotically accurate. We adopt the variational autoencoder [45] architecture to estimate latent group factors and classify individuals into different groups in an unsupervised manner. The proposed estimation framework supports the identifiability theorem discussed in Section 4. As demonstrated in Figure 2, the sequence of individual trajectories with the required size is represented in the discrete embedding space, which is consistent with the assumptions proposed in the theorems. A detailed pseudocode is provided in Appendix I, and a comprehensive realization of each component is available in Appendix H. ", "page_idx": 4}, {"type": "text", "text": "5.1 Latent Estimation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Latent Inference via Quantized Encoding The group determinacy assumption suggests the existence of the latent individual-specific factor $\\kappa$ . Since $\\kappa$ is time-invariant and influences each state in the transition process, we first use a sequential encoder to capture the high-level representation $z_{m}$ , based on the input from all states across each trajectory $\\mathbf{s}_{0:T}^{m}$ . We then apply a vector quantization layer [76] to discretize the latent space and estimate the latent factor $\\hat{\\kappa}_{m}$ . This quantization ensures that the learned representation aligns with our assumptions about the group characteristics of the latent factors, making it suitable for our objectives. ", "page_idx": 4}, {"type": "text", "text": "Specifically, to capture the temporal dependency from the sequential observations, sequential neural networks such as Conv1D [52] or Long Short-Term Memory (LSTM) [31] are used as the encoder, represented as $z_{m}\\,=\\,g(\\mathbf{s}_{0}^{m},\\cdot\\cdot\\cdot,\\mathbf{s}_{T}^{m})$ , where $g$ is the encoder function. Conv1D processes each subsequence $\\mathbf{s}_{t:t+H}^{m}$ through a series of 1D convolution layers. It traverses the entire sequence to capture local temporal patterns, yielding a feature map $o_{t}^{m}$ at each time step $t$ , where $o_{t}^{m}\\,=$ $\\mathrm{ConviD}(\\mathbf{s}_{t:t+H}^{m})$ . On the other hand, LSTM processes each $\\mathbf{s}_{t}^{m}$ sequentially. It updates the hidden states $h_{t}$ by aggregating information over time. At each time step, the hidden state $h_{t}^{m}$ and the cell state $c_{t}^{m}$ are updated as $h_{t}^{m},c_{t}^{m}=\\mathrm{LSTM}(h_{t-1}^{m},c_{t-1}^{m},\\mathbf{s}_{t}^{m})$ . After processing the whole trajectory sequentially, the final hidden state of the LSTM and the final output of the Conv1D layer serve as the high-level representation $z_{m}$ . ", "page_idx": 4}, {"type": "text", "text": "Given that $z_{m}$ produces continuous latent representations, which are incompatible with our requirements, we use a vector quantization layer to discretize the latent space and approximate the latent factor. It maps the continuous representation to the nearest vector in a predefined embedding dictionary $E$ , thereby translating the continuous representations into a discrete latent space. Specifically, the embedding dictionary consists of a set of vectors $E=\\{e_{1},e_{2},\\ldots,e_{G}\\}$ , each representing a distinct group in the discrete embedding space. The assignment of a dictionary vector $e_{i}$ to $z_{m}$ is realized by finding the nearest neighbor in the dictionary as $\\begin{array}{r}{\\hat{\\kappa}_{m}=\\arg\\operatorname*{min}_{e_{i}}\\|z_{m}-e_{i}\\|_{2}}\\end{array}$ , where $\\hat{\\kappa}_{m}$ represents the quantized vector that is the closest embedding $e_{i}$ to the continuous representation $z_{m}$ . ", "page_idx": 4}, {"type": "text", "text": "Latent Optimization via Conditional Reconstruction To effectively estimate latent factors in an unsupervised manner, reconstruction is important because it ensures that the distribution learned by the model closely matches the true observed distribution. According to Definition 4.1, the estimated latent factor thus closely approximates the true latent factor. Given the nature of the transition processes, a conditional decoder is designed with the state-action pairs $(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m})$ as conditions to guide the reconstruction of $\\hat{\\bf s}_{t}^{m}$ . These conditions, together with the estimated latent factors $\\hat{\\kappa}_{m}$ , serve as inputs to the decoder. The accuracy of the reconstruction is quantitatively evaluated by its reconstruction likelihood $p_{\\mathrm{Recon}}\\big(\\hat{\\mathbf{s}}_{t}^{m}|\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m}\\big)$ , where $p_{\\mathrm{{Recon}}}$ denotes the reconstruction distribution. It provides a probabilistic measure of how accurately $\\hat{\\bf s}_{t}^{m}$ reconstructs $\\mathbf{s}_{t}^{m}$ and a quantitative evaluation of the model\u2019s reconstruction accuracy. ", "page_idx": 4}, {"type": "image", "img_path": "kREpCQtHdN/tmp/a4f37e3bb9aea74002f1519da001a1bb1d7043d62ae9213644e0705d0a0973b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: (a) Latent estimation framework takes each trajectory $\\mathbf{s}_{0:T}$ as input through a quantized encoder to estimate the latent factor $\\hat{\\kappa}$ . A conditional decoder uses $(\\mathbf{s}_{t-1},\\mathbf{a}_{t-1})$ as the condition and $\\hat{\\kappa}$ as the input to reconstruct $\\hat{\\mathbf{s}}_{t}$ . (b) After assigning the estimated latent factors to each trajectory, the policy learning framework incorporates the latents as augmented labels to optimize the RL policy. For new individuals, the initial policy is adapted according to their group affiliation, allowing for individualized policy adaptation for newcomers. ", "page_idx": 5}, {"type": "text", "text": "Training Objectives The parameters are optimized according to the following ELBO objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ELBO}}=\\mathcal{L}_{\\mathrm{Recon}}+\\alpha\\mathcal{L}_{\\mathrm{Quant}}+\\beta\\mathcal{L}_{\\mathrm{Commit}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ and $\\beta$ are weights for the corresponding loss components. Specifically, (1) Reconstruction loss $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Recon}}=\\sum_{t}\\|\\mathbf{s}_{t}^{m}-\\mathbf{De}(\\mathrm{En}(\\mathbf{s}_{0:T}^{m}),\\mathbf{s}_{t-1}^{\\bar{m}},\\mathbf{a}_{t-1}^{m^{-}})\\|^{2}}\\end{array}$ , where En and De are the encoder and decoder, measures the  discrepancy between the reconstructed state $\\hat{\\bf s}_{t}^{m}$ and the original state $\\mathbf{s}_{t}^{m}$ . (2) Quantization loss assesses the discrepancy between the encoder output $z_{m}$ and the discretized representation $e_{m}$ , formulated as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Quant}}=\\overline{{\\sum_{i}\\left\\|\\mathbf{sg}[z_{m,i}]-e_{m,i}\\right\\|^{2}}}}\\end{array}$ . Since the quantization step is undifferentiable, we use the stop-gradient operation $\\mathrm{sg}[\\cdot]$ to enable gradient-based optimization, which updates the dictionary without affecting the encoder parameters. 3) Commitment loss is designed to minimize the discrepancy between $z_{m}$ and $e_{m}$ , ensuring that $z_{m}$ aligns more closely with the embedding space and formulated as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Commit}}=\\sum_{i}\\|z_{m,i}-\\mathbf{sg}\\bar{[}e_{m,i}\\|^{2}}\\end{array}$ . By applying the stop gradient to $z_{m,i}$ , it ensures that the gradients from this loss do not change the dictionary vectors but rather optimize the encoder. ", "page_idx": 5}, {"type": "text", "text": "5.2 Policy Learning Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The estimation network is pre-trained offline. When a new individual arrives, we estimate $\\kappa$ and adapt the policy simultaneously, and the policy is adapted through new interactions. Specifically, ", "page_idx": 5}, {"type": "text", "text": "Latent-based Policy Individualization The estimated latent individual-specific factors $\\hat{\\kappa}$ , together with the offline trajectories over all individuals, are used to learn the individualized policy $\\pi_{\\kappa}^{*}$ . We view the estimated factors as an augmented component of the policy input and adjust the policy training objective to match the unique characteristics of each individual. ", "page_idx": 5}, {"type": "text", "text": "Take Q-learning [58] as an example. In the individualized process, the latent factor is augmented as a policy input represented as $\\mu_{\\pi}(\\mathbf{s}_{t};\\theta^{\\mu})\\rightarrow\\mu_{\\pi}^{m}(\\mathbf{s}_{t}^{m},\\hat{\\kappa}_{m};\\theta^{\\mu})$ , where $\\theta^{\\mu}$ represents the parameters of the policy network. The policy model, by incorporating latent factors, can more effectively adapt to the unique characteristics of each individual The training objective is updated accordingly as $\\begin{array}{r}{\\mathcal{I}(\\theta^{\\mu})=\\hat{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}Q\\left(\\mathbf{s}_{t},\\mu_{\\pi}^{m}(\\mathbf{s}_{t}^{m},\\hat{\\kappa}_{m};\\theta^{\\mu});\\theta^{Q}\\right)\\right]}\\end{array}$ , where $\\gamma$ is the discount factor and $Q$ is the $\\mathrm{\\DeltaQ}$ value. Such individualization improves the adaptability of the policies to various environments, and our framework is general enough to be integrated with many RL algorithms. ", "page_idx": 5}, {"type": "text", "text": "Policy Adaptation for New Individual The policy adaptation involves two steps: initializing the policy based on the individualized policy $\\pi_{\\kappa}^{*}$ and fine-tuning the policy through new interactions. For a new individual from group $g_{n}$ , we first estimate its group factor $\\hat{\\kappa}_{n}$ and then initialize the policy network $\\pi_{\\mathrm{new}}$ by directly transferring the parameters from \u03c0\u03ba\u2217=\u03ba\u02c6 . Specifically, the new agent updates $\\pi_{\\mathrm{new}}$ based on the collected trajectory $D_{n}$ by maximizing the expected reward as $\\pi_{\\mathrm{new}}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{a}_{t})\\in D_{n}}(R_{\\mathbf{a}_{t}}(\\mathbf{s}_{t}))$ . The dataset $D_{n}$ is further augmented with new observations $({\\bf s}_{t},{\\bf a}_{t},R_{t},{\\bf s}_{t+1})$ from the new individual under the policy $\\pi_{\\mathrm{new}}$ , which is critical for accurately estimating the latent factor of the new individual. During this process, the policy is iteratively improved to better fit the specific characteristics of the new individual. ", "page_idx": 5}, {"type": "text", "text": "6 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics To measure the latent identification, we quantify the correlation between the estimated and true latent factors by: (1) Pearson Correlation Coefficient (PCC) for single latent, which quantifies the linear correlation between individual estimated and true factors, and (2) Kernel Canonical Correlation Analysis (KCCA) for multiple latents, which evaluates the correlation between sets of estimated and true factors. An absolute value close to 1 indicates a strong correlation and better latent recovery. To evaluate the control performance, we measure the adaptation performance using: (3) jumpstart, which records the improvement in initial performance when a learning agent leverages knowledge from source tasks, and (4) accumulative reward, which indicates the learning quality over the learning process. (5) initial and final reward, which measures the initial performance benefited from policy adaptation and the performance after the full training process. ", "page_idx": 6}, {"type": "text", "text": "Baselines For estimation evaluation, our baselines include: (1) disentangled sequential autoencoder [88], which disentangles the latent representations into static and dynamic parts instead of considering the global influence of $\\kappa$ . (2) Population-level component, which embeds the latent factors using population data rather than on an individual basis. For policy evaluation, our baselines include: (3) aligned latent models [22], which jointly optimizes a latent-space model and a policy to achieve high returns. (4) Soft Actor-Critic (SAC) [26], which uses entropy as part of the objective function to encourage exploration and improve robustness. (5) Deep Deterministic Policy Gradient (DDPG) [58], which combines deterministic policy gradients with deep neural networks to effectively handle continuous action spaces. (6) Dueling Double Deep Q-Network (D3QN) [81], which introduces a dueling architecture for value function estimation and improves value estimation. (7) Rainbow DQN [30], which combines prioritized experience replay and dueling network architectures to improve performance and learning stability. ", "page_idx": 6}, {"type": "text", "text": "6.1 Evaluation on Latent Estimation Framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Synthetic Experiments We first run experiments on the synthetic dataset to demonstrate the effectiveness of the estimation framework, which is manually generated following the post-nonlinear model. We design three types of latent factors $\\kappa$ , each either satisfying or violating the required assumptions. Case 1: $\\kappa$ is a finite latent factor following the categorical distribution $\\operatorname{Cat}(0.1,0.2,0.3,0.4)$ with cardinality equal to 4. Case 2: \u03bas are three-dimensional finite latent factors, and each factor follows the categorical distributions $\\mathrm{Cat}(0.2,0.8)$ , $\\mathrm{Cat}(0.2,0.3,0.5)$ , $\\operatorname{Cat}(0.1,0.2,0.3,0.4)$ , with cardinality equal to 2, 3, 4, respectively. Case 3: \u03bas are three-dimensional infinite latent factors, and each factor follows the Gaussian distribution ${\\mathcal{N}}(0,1)$ , uniform distribution $\\mathrm{Uniform}(0,1)$ , and exponential distribution $\\operatorname{Exp}(1)$ , respectively. We synthetically generate 40 unique trajectories, each representing an individual, with a maximum length of 20 per trajectory. ", "page_idx": 6}, {"type": "image", "img_path": "kREpCQtHdN/tmp/7503d909b9c54cfa6b3db2a3aa4aff1f257354aaf213f0e9cb7c16b1fe9222a3.jpg", "img_caption": ["Figure 3: Synthetic results. (a) Comparisons of PCC trajectories in Case 1. (b-c) Scatterplot of the canonical variables in Case 2 and 3. (d) Identifiability performance responses of the sample size. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "For case 1, we use PCC to measure the estimation performance and report the training curve in Figure 3(a), where the shaded regions represent the standard deviation. The comparative results show that our method can recover the true latent factors, which outperforms other baselines. In particular, the population-level component overlooks the underlying components between different groups, thus failing to identify the individual-specific factor. Furthermore, although the disentangled sequential autoencoder achieves compromised identifiability in the early training stage by considering the static part in the latent space, it fails to reach full identifiability because it overlooks the individualized transition processes, leading to worse recovery performance over time. For cases 2 and 3, we use KCCA to quality the correlation and visualize the scatter plots in Figure 3(b) and Figure 3(c). These results highlight the strong identifiability in the infinite cases and verify the claim in Theorem 4.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Moreover, we slightly violate the required sample size assumption in Theorem 4.1 and report the change in the training curve under different population sizes in Figure 3(d), represented by the dashed lines. The result shows that satisfying the sample sufficiency assumption is necessary to recover the latent factor. In addition, we evaluate the effect of the trajectory length as described in the Theorem 4.2. The findings, shown as solid lines, indicate that increasing the sample size would apparently improve the identifiability performance, which is consistent with the proposed theorem. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study The contributions of the different components in the latent estimation framework are reported in Table 1. We build on the autoencoder framework with a quantization layer and add each component sequentially to the previous module. Incorporating a sequential encoder significantly improves the identifiability, which is important for the accurate recovery of latent factors. In the implementation, we use a noise estimator during optimization to minimize bias and improve identifiability. The results suggest that the noise estimator contributes to fine-tuning the overall performance of the model, allowing for more accurate and reliable recovery of latent individual-specific factors. ", "page_idx": 7}, {"type": "table", "img_path": "kREpCQtHdN/tmp/0e6c43d6653b6424472733661316410e4cfab71bd3922eda48eb7657dbbc9761.jpg", "table_caption": ["Table 1: Contribution of each module. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "PersuasionForGood Corpus We further evaluate our framework on the real-world dataset, PersuasionForGood corpus [80], which is widely used for analyzing persuasion strategies [66, 8, 90]. It consists of 1017 person-to-person dialogues and 32 personality traits of each participant. In each dialogue, the persuader tries to convince the persuadee to donate to a charity. In the iMDP context, the state refers to the persuadee\u2019s response, the action refers to the persuader\u2019s utterance, and the reward is the final donation. Since this offline dataset lacks real-time interactions necessary for assessing control performance, we instead use it to identify the latent personality of each individual. We use BERT [11] as the backbone to embed each utterance into a 768-dimensional feature representation and then use an LSTM encoder followed by the quantization layer to recover the latent personalities. The CCA results under different latent dimensions are shown in Figure 4(a), demonstrating that our method can achieve remarkable performance on the real dataset by appropriately fine-tuning the latent dimensions. ", "page_idx": 7}, {"type": "image", "img_path": "kREpCQtHdN/tmp/5bcce7e6d65ed741f250ddbc57b50b15f625475fe3e7b7a18d95c56f5eb76ad9.jpg", "img_caption": ["Figure 4: (a) Canonical correlation with respect to the latent dimensions in the PersuasionForGood corpus. (b-d) Accumulative reward curves in Pendulum, HeartPole, and Half Cheetah, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Evaluation on Policy Learning Framework ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Pendulum Pendulum [5] is a continuous control task for RL study with the goal of swinging up and stabilizing in an upright position. The states are the x-y coordinates and angular velocity, and the action is the torque applied to the pendulum. For simplicity, we choose DDPG as the foundational optimization algorithm and manually create 20 individualized environments. In these environments, the gravity $g$ is randomly drawn from a categorical distribution over the set $\\{3,\\ldots,12\\}$ . The performance of the policy adaptation is evaluated on a new individual with $g=10$ . ", "page_idx": 7}, {"type": "text", "text": "We compare our method against several baselines: (1) SAC; (2) DDPG without prior knowledge; (3) aligned latent models; (4) pre-trained DDPG incorporating knowledge from given individuals, termed cross-individual transfer; (5) individualized policy incorporating randomly defined group embedding, termed random group embedding. The training curves over accumulative reward are reported in Figure 4(b), showing that the proposed method outperforms other baselines in both jumpstart and accumulative reward. Specifically, methods that benefit from population knowledge (our method and cross-individual transfer) outperform non-transfer methods, indicating that the pre-trained policy would accelerate the learning process. However, since cross-individual transfer ignores the individualspecific information, such mixed policy knowledge yields worse initial performance compared to the individualized policies derived from our method. ", "page_idx": 8}, {"type": "text", "text": "HeartPole HeartPole [59] is a discrete healthcare environment that explores the long-term health outcomes of short-term decisions. The six-dimensional states represent different health conditions, including alertness, hypertension, intoxication, time since sleep, time elapsed, and work done. Actions can be chosen from work, coffee, alcohol, and sleep. We create 100 individualized scenarios and assign each patient with individual characteristics, such as coffee tolerance, hypertension risk, and alcohol tolerance, according to a categorical distribution over the set $\\{0.6,0.8,1.2\\}$ . The adaptation performance is evaluated on a new individual with all indices set to 1. ", "page_idx": 8}, {"type": "text", "text": "We compare our method against the following baselines: (1) D3QN without prior knowledge, (2) Rainbow DQN, (3) cross-individual transfer with D3QN, and (4) random group embedding. The training curves over accumulative reward are shown in Figure 4(c), and our method outperforms other baselines in both jumpstart and accumulative reward. Interestingly, although inappropriate source domain knowledge may harm the control performance (see cross-individual transfer), the result of random group embedding indicates that group embedding knowledge can encourage the performance of generalization. The group structure, together with properly estimated group information, jointly allows our method to converge better and faster than other baselines. ", "page_idx": 8}, {"type": "text", "text": "Half Cheetah Half Cheetah [74] is a Mujoco-based task aiming to control a 2D bipedal robot. The agent consists of 9 links and 8 joints, and the goal is to apply torque to the joints to make the cheetah run forward as fast as possible. We introduced 50 individualized settings with the gravity $g$ following a categorical distribution with probabilities $p=0.2$ and corresponding $g$ values from $\\{8,8.5,\\ldots,10\\}$ . The adaptation performance is evaluated on a new individual with $g=9.8$ . We compare our method with (1) DDPG without prior knowledge, (2) aligned latent models, (3) crossindividual transfer with DDPG, and (4) random group embedding. The comparative results are shown in Figure 4(d). We found that inappropriate source domain data can degrade the control performance (see cross-individual transfer), but the integration of group embedding facilitates generalization, allowing our method to outperform baselines in terms of convergence speed and efficiency. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work focuses on learning latent state transitions from observed state-action trajectories, ensuring identifiability even in the presence of latent individual-specific factors. To the best of our knowledge, this study provides novel identifiability guarantees in several settings that have not been addressed by others. Despite these contributions, our approach has three major limitations. (1) It currently does not account for instantaneous causal influences within $\\mathbf{s}_{t}$ . However, this problem could be mitigated by adjusting the temporal resolution of the data and explicitly modeling causal dependencies within states. Integrating causal graphical models or advanced inference techniques for handling instantaneous causal relationships would enhance our framework. (2) It lacks a nonparametric proof for scenarios where latent factors are continuous, while our empirical results suggest that the approach may be adaptable to these more general conditions. (3) The proposed model does not account for time-varying latent factors. Establishing theoretical identifiability is highly non-trivial and further constraints would be needed. ", "page_idx": 8}, {"type": "text", "text": "These limitations present key areas for future research. In addition, practical concerns such as privacy, robustness, and reliability are essential for real-world applications. For privacy, de-identification techniques such as removing direct identifiers (e.g., names, postal codes), applying data perturbation, and pseudonymization could help mitigate risks. The exploration of differential privacy techniques is also a promising direction for ensuring privacy and security in practical applications. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. BH is supported by NSF DMS-2428058. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender systems: A survey. ACM Computing Surveys, 55(7):1\u201338, 2022.   \n[2] Jonathan Bassen, Bharathan Balaji, Michael Schaarschmidt, Candace Thille, Jay Painter, Dawn Zimmaro, Alex Games, Ethan Fast, and John C Mitchell. Reinforcement learning for the adaptive scheduling of educational activities. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1\u201312, 2020. [3] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023.   \n[4] Andrew Bennett, Nathan Kallus, Lihong Li, and Ali Mousavi. Off-policy evaluation in infinitehorizon reinforcement learning with latent confounders. In International Conference on Artificial Intelligence and Statistics, pages 1999\u20132007. PMLR, 2021. [5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. [6] Qingpeng Cai, Ruohan Zhan, Chi Zhang, Jie Zheng, Guangwei Ding, Pinghua Gong, Dong Zheng, and Peng Jiang. Constrained reinforcement learning for short video recommendation. arXiv preprint arXiv:2205.13248, 2022.   \n[7] Guangyi Chen, Yifan Shen, Zhenhao Chen, Xiangchen Song, Yuewen Sun, Weiran Yao, Xiao Liu, and Kun Zhang. Caring: Learning temporal causal representation under non-invertible generation process. arXiv preprint arXiv:2401.14535, 2024. [8] Maximillian Chen, Weiyan Shi, Feifan Yan, Ryan Hou, Jingwen Zhang, Saurav Sahay, and Zhou Yu. Seamlessly integrating factual information and social content with persuasive dialogue. arXiv preprint arXiv:2203.07657, 2022. [9] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 1\u20134, 2009.   \n[10] Florent Delgrange, Ann Nowe, and Guillermo A P\u00e9rez. Wasserstein auto-encoded mdps: Formal verification of efficiently distilled rl policies with many-sided guarantees. arXiv preprint arXiv:2303.12558, 2023.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[12] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. A versatile causal discovery framework to allow causally-related hidden variables. arXiv preprint arXiv:2312.11001, 2023.   \n[13] F. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip connections. ArXiv Preprint ArXiv:1710.05268, 2017.   \n[14] Bisni Fahad Mon, Asma Wasf,i Mohammad Hayajneh, Ahmad Slim, and Najah Abu Ali. Reinforcement learning in education: A literature review. In Informatics, volume 10, page 74. MDPI, 2023.   \n[15] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for nonstationary reinforcement learning. Advances in Neural Information Processing Systems, 35:31957\u201331971, 2022.   \n[16] Apple WP Fok, Hau-San Wong, and YS Chen. Hidden markov model based characterization of content access patterns in an e-learning environment. In 2005 ieee international conference on multimedia and expo, pages 201\u2013204. IEEE, 2005.   \n[17] Evan M Forman, Michael P Berry, Meghan L Butryn, Charlotte J Hagerman, Zhuoran Huang, Adrienne S Juarascio, Erica M LaFata, Santiago Onta\u00f1\u00f3n, J Mick Tilford, and Fengqing Zhang. Using artificial intelligence to optimize delivery of weight loss treatment: Protocol for an efficacy and cost-effectiveness trial. Contemporary Clinical Trials, 124:107029, 2023.   \n[18] Evan M Forman, Stephanie G Kerrigan, Meghan L Butryn, Adrienne S Juarascio, Stephanie M Manasse, Santiago Onta\u00f1\u00f3n, Diane H Dallal, Rebecca J Crochiere, and Danielle Moskow. Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss? Journal of behavioral medicine, 42:276\u2013290, 2019.   \n[19] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning (ICML), 2019.   \n[20] D. Ghosh, A. Gupta, and S. Levine. Learning actionable representations with goal conditioned policies. ICLR, 2019.   \n[21] Susobhan Ghosh, Raphael Kim, Prasidh Chhabria, Raaz Dwivedi, Predrag Klasjna, Peng Liao, Kelly Zhang, and Susan Murphy. Did we personalize? assessing personalization by an online reinforcement learning algorithm using resampling. arXiv preprint arXiv:2304.05365, 2023.   \n[22] Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Simplifying model-based rl: learning representations, latent-space models, and policies with one objective. arXiv preprint arXiv:2209.08466, 2022.   \n[23] K. Gregor, G. Papamakarios, F. Besse, L. Buesing, and T. Weber. Temporal difference variational auto-encoder. arXiv preprint arXiv:1806.03107, 2018.   \n[24] Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altch\u00e9, R\u00e9mi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In International Conference on Machine Learning, pages 3875\u20133886. PMLR, 2020.   \n[25] D. Ha and J. Schmidhuber. World models. In Advances in Neural Information Processing Systems, 2018.   \n[26] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[27] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.   \n[28] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.   \n[29] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.   \n[30] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[31] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.   \n[32] Jesse Hoey, Pascal Poupart, Craig Boutilier, and Alex Mihailidis. Pomdp models for assistive technology. In Assistive Technologies: Concepts, Methodologies, Tools, and Applications, pages 120\u2013140. IGI Global, 2014.   \n[33] William Hoiles, Vikram Krishnamurthy, and Kunal Pattanayak. Rationally inattentive inverse reinforcement learning explains youtube commenting behavior. The Journal of Machine Learning Research, 21(1):6879\u20136917, 2020.   \n[34] Harold Hotelling. Relations between two sets of variates. In Breakthroughs in statistics: methodology and distribution, pages 162\u2013190. Springer, 1992.   \n[35] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. Adarl: What, where, and how to adapt in transfer reinforcement learning. arXiv preprint arXiv:2107.02729, 2021.   \n[36] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark Glymour, and Kun Zhang. Latent hierarchical causal structure discovery with rank constraints. Advances in Neural Information Processing Systems, 35:5549\u20135561, 2022.   \n[37] Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P Xing, and Clark Glymour. Specific and shared causal relation modeling and mechanism-based clustering. Advances in Neural Information Processing Systems, 32, 2019.   \n[38] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. Advances in neural information processing systems, 29, 2016.   \n[39] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 859\u2013868. PMLR, 2019.   \n[40] Xiaoyuan Ji, Hu Ye, Jianxin Zhou, Yajun Yin, and Xu Shen. An improved teaching-learningbased optimization algorithm and its application to a combinatorial optimization problem in foundry industry. Applied Soft Computing, 57:504\u2013516, 2017.   \n[41] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, \u00b7 \u00b7 \u00b7 , and H. Michalewski. Model-based reinforcement learning for Atari. arXiv preprint arXiv:1903.00374, 2019.   \n[42] M. Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes fliters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.   \n[43] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207\u20132217. PMLR, 2020.   \n[44] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[45] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[46] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999.   \n[47] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande, and Kun Zhang. Partial identifiability for domain adaptation. arXiv preprint arXiv:2306.06510, 2023.   \n[48] R.G. Krishnan, U. Shalit, and D. Sontag. Deep kalman fliters. arXiv preprint arXiv:1511.05121, 2015.   \n[49] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016.   \n[50] Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Rl for latent mdps: Regret guarantees and a lower bound. Advances in Neural Information Processing Systems, 34:24523\u201324534, 2021.   \n[51] Jeongyeol Kwon, Yonathan Efroni, Shie Mannor, and Constantine Caramanis. Prospective side information for latent mdps. arXiv preprint arXiv:2310.07596, 2023.   \n[52] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.   \n[53] Yu Lei and Wenjie Li. Interactive recommendation with user-specific deep reinforcement learning. ACM Transactions on Knowledge Discovery from Data (TKDD), 13(6):1\u201315, 2019.   \n[54] T. Lesort, N. D\u00edaz-Rodr\u00edguez, J. F. Goudou, and D. Filliat. State representation learning for control: An overview. Neural Networks, 108:379\u2013392, 2018.   \n[55] Minne Li, Mengyue Yang, Furui Liu, Xu Chen, Zhitang Chen, and Jun Wang. Causal world models by unsupervised deconfounding of physical dynamics. arXiv preprint arXiv:2012.14228, 2020.   \n[56] Zhige Li, Derek Yang, Li Zhao, Jiang Bian, Tao Qin, and Tie-Yan Liu. Individualized indicator for all: Stock-wise technical indicator optimization with stock embedding. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 894\u2013902, 2019.   \n[57] Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1):1\u201322, 2020.   \n[58] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.   \n[59] Vadim Liventsev, Aki H\u00e4rm\u00e4, and Milan Petkovi\u00b4c. Towards effective patient simulators. Frontiers in artificial intelligence, 4:798659, 2021.   \n[60] Chaochao Lu, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Deconfounding reinforcement learning in observational settings. arXiv preprint arXiv:1812.10576, 2018.   \n[61] Zhiyao Luo, Mingcheng Zhu, Fenglin Liu, Jiali Li, Yangchen Pan, Jiandong Zhou, and Tingting Zhu. Dtr-bench: An in silico environment and benchmark platform for reinforcement learning based dynamic treatment regime. arXiv preprint arXiv:2405.18610, 2024.   \n[62] S. Mahadevan and M.. Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research (JMLR), 8:2169\u20132231, 2007.   \n[63] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Annual review of statistics and its application, 6:355\u2013378, 2019.   \n[64] Kevin P Murphy. A survey of pomdp solution techniques. environment, 2(10), 2000.   \n[65] Aliz\u00e9e Pace, Hugo Y\u00e8che, Bernhard Sch\u00f6lkopf, Gunnar R\u00e4tsch, and Guy Tennenholtz. Delphic offline reinforcement learning under nonidentifiable hidden confounding. arXiv preprint arXiv:2306.01157, 2023.   \n[66] Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, and Yajing Sun. Do you know my emotion? emotion-aware strategy recognition towards a persuasive dialogue system. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 724\u2013739. Springer, 2022.   \n[67] Alexander Ritchie, Robert A Vandermeulen, and Clayton Scott. Consistent estimation of identifiable nonparametric mixture models from grouped observations. Advances in Neural Information Processing Systems, 33:11676\u201311686, 2020.   \n[68] Doaa Shawky and Ashraf Badawi. Towards a personalized learning experience using reinforcement learning. Machine learning paradigms: Theory and application, pages 169\u2013187, 2019.   \n[69] Dongjian Song, Bing Zhu, Jian Zhao, Jiayi Han, and Zhicheng Chen. Personalized car-following control based on a hybrid of reinforcement learning and supervised learning. IEEE Transactions on Intelligent Transportation Systems, 2023.   \n[70] Seth Sullivant, Kelli Talaska, and Jan Draisma. Trek separation for gaussian graphical models. 2010.   \n[71] Yuewen Sun, Erli Wang, Biwei Huang, Chaochao Lu, Lu Feng, Changyin Sun, and Kun Zhang. Acamda: Improving data efficiency in reinforcement learning through guided counterfactual data augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 15193\u201315201, 2024.   \n[72] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[73] Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. Advances in neural information processing systems, 30, 2017.   \n[74] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012.   \n[75] Momchil S Tomov, Eric Schulz, and Samuel J Gershman. Multi-task reinforcement learning in humans. Nature Human Behaviour, 5(6):764\u2013773, 2021.   \n[76] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[77] Robert A Vandermeulen and Clayton D Scott. On the identifiability of mixture models from grouped samples. arXiv preprint arXiv:1502.06644, 2015.   \n[78] Thanh Vinh Vo, Pengfei Wei, Wicher Bergsma, and Tze Yun Leong. Causal modeling with stochastic confounders. In International Conference on Artificial Intelligence and Statistics, pages 3025\u20133033. PMLR, 2021.   \n[79] Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably efficient causal reinforcement learning with confounded observational data. Advances in Neural Information Processing Systems, 34:21164\u201321175, 2021.   \n[80] Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. Persuasion for good: Towards a personalized persuasive dialogue system for social good. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5635\u20135649, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[81] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995\u20132003. PMLR, 2016.   \n[82] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. NeurIPS, 2015.   \n[83] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4):715\u2013770, 2002.   \n[84] Xinghao Yang, Weifeng Liu, Wei Liu, and Dacheng Tao. A survey on canonical correlation analysis. IEEE Transactions on Knowledge and Data Engineering, 33(6):2349\u20132368, 2019.   \n[85] Weiran Yao, Guangyi Chen, and Kun Zhang. Temporally disentangled representation learning. Advances in Neural Information Processing Systems, 35:26492\u201326503, 2022.   \n[86] Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, and Kun Zhang. Learning temporally causal latent processes from general temporal data. arXiv preprint arXiv:2110.05428, 2021.   \n[87] Chunli Yin and Jinglong Han. Dynamic pricing model of e-commerce platforms based on deep reinforcement learning. CMES-Computer Modeling in Engineering & Sciences, 127(1), 2021.   \n[88] Li Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In International Conference on Machine Learning, pages 5670\u20135679. PMLR, 2018.   \n[89] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit Hochberg. Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system. Journal of medical Internet research, 19(10):e338, 2017.   \n[90] Donghuo Zeng, Roberto S Legaspi, Yuewen Sun, Xinshuai Dong, Kazushi Ikeda, Peter Spirtes, and Kun Zhang. Counterfactual reasoning using predicted latent personality dimensions for optimizing persuasion outcome. In International Conference on Persuasive Technology, pages 287\u2013300. Springer, 2024.   \n[91] A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations for reinforcement learning without reconstruction. ICLR, 2021.   \n[92] Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference on Machine Learning, pages 11214\u201311224. PMLR, 2020.   \n[93] Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. arXiv preprint arXiv:1205.2599, 2012.   \n[94] M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Self-supervised visual planning with temporal skip connections. ICML, 2019.   \n[95] M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. J. Johnson, and S. Levine. Solar: deep structured representations for model-based reinforcement learning. arXiv preprint arXiv:1808.09105, 2018.   \n[96] Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In International Conference on Machine Learning, pages 26517\u201326547. PMLR, 2022.   \n[97] Yujia Zheng, Ignavier $\\mathrm{Ng}$ , and Kun Zhang. On the identifiability of nonlinear ica: Sparsity and beyond. Advances in Neural Information Processing Systems, 35:16411\u201316422, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Materials for \u201cIdentifying Latent State-Transition Processes for Individualized Reinforcement Learning\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Notation and Terminology ", "page_idx": 15}, {"type": "text", "text": "We summarize the notations used throughout the paper in the following table. ", "page_idx": 15}, {"type": "table", "img_path": "kREpCQtHdN/tmp/c2078b42ed3b0eeedcfa6a2f8785440a40b64ba1a6358f85366318c8fca68ea2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Identifiability Theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given the identifiability theorems, we first provide intuitive explanations for each assumption and discuss their relevance to real-world applications. Then, we provide the proof. Finally, we introduce some preliminaries related to our theorems, which are essential for the proof. ", "page_idx": 15}, {"type": "text", "text": "B.1 Preliminaries for Theorem 4.1 and 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1.1 Markov Property ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The first-order Markov property implies that the transition probability to the next state depends only on the current state, uninfluenced by the sequence of previous states. Specifically, ", "page_idx": 15}, {"type": "text", "text": "Definition B.1 (First-order Markov Property [72]). A stochastic process $\\{X_{t}:t\\in\\mathcal{N}\\}$ has the firstorder Markov property if, for each set of times $t,t\\!-\\!1,\\ldots,0$ and corresponding state $x_{t},x_{t-1},...,x_{0}$ in the state space, the following conditional independence property holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(X_{t}=x_{t}|X_{t-1}=x_{t-1},X_{t-2}=x_{t-2},\\ldots,X_{0}=x_{0})=\\mathbb{P}(X_{t}=x_{t}|X_{t-1}=x_{t-1})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first-order Markov property implies that the transition probability to the next state depends only on the current state, uninfluenced by the previous states. In the context of the state transition process, it possesses the first-order Markov property. Mathematically, it can be represented as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{s}_{t}|\\mathbf{s}_{t-1},\\mathbf{a}_{t-1},\\mathbf{s}_{t-2},\\mathbf{a}_{t-2},\\dots,\\mathbf{s}_{0},\\mathbf{a}_{0})=\\mathbb{P}(\\mathbf{s}_{t}|\\mathbf{s}_{t-1},\\mathbf{a}_{t-1}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbb{P}(\\mathbf{s}_{t}|\\mathbf{s}_{t-1},\\mathbf{a}_{t-1})$ is the transition probability from $(\\mathbf{s}_{t-1},\\mathbf{a}_{t-1})$ to the state $\\mathbf{s}_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "B.1.2 Finite Mixture Model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A finite mixture model is used for modeling a total population that comprises unobserved or hidden groups. Each of these groups is assumed to follow its own distinct probability distribution. In this context, the overall population model is expressed as a weighted sum of these individual distributions [63]. Specifically, ", "page_idx": 15}, {"type": "text", "text": "Definition B.2 (Finite Mixture Models [77]). A finite mixture model is a probability law based on a finite number of probability measures, $\\mu_{1},\\ldots,\\mu_{m}.$ , and a discrete distribution $\\omega_{1},\\ldots,\\omega_{m}$ . $A$ realization of a mixture model is generated by generating a component at random $k$ , $1\\leq k\\leq m$ , and then drawing from $\\mu_{k}\\sim\\mathcal{P}$ . Then, the mixture measure $\\mathcal{P}$ is defined as a weighted sum of probability measures $\\mu_{i}$ with weights $w_{i}$ . Specifically, ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\mathcal P}=\\sum_{i=1}^{m}w_{i}\\delta_{\\mu_{i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Discussions on Assumptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Group Determinacy The distinct values of the latent factor $\\kappa$ categorize the population into separate groups, with each group characterized by its unique probability distribution and denoted as $\\dot{\\sum_{g=1}^{G}\\pi_{g}\\delta_{\\kappa_{g}}}(\\kappa)$ . The mixture formulation implies that the latent factor $\\kappa$ serves as a categorical variable, with each unique value explicitly specifying a distinct group within the population. Such a formulation facilitates the identification and analysis of heterogeneous subpopulations within the finite mixture model. ", "page_idx": 16}, {"type": "text", "text": "The idea of group determinacy is important in real-world applications. Take personalized education as an example. For each student $m$ , $\\mathbf{s}^{m}$ represents their current knowledge state, $\\mathbf{a}^{m}$ denotes their personalized learning action, and the function $f$ determines the unique educational trajectory for each student. The latent factor $\\kappa^{m}$ influences how a student\u2019s learning progresses over time. It can be based on factors such as learning style preferences that help to logically group students. Specifically, one group might consist of visual learners who excel in interactive, graphically-oriented subjects, while another group might include students who prefer textual information and excel in reading and writing-intensive subjects. Each group exhibits its own set of learning outcomes and patterns, allowing educators to personalize teaching methods and materials to effectively meet the different needs of each group. ", "page_idx": 16}, {"type": "text", "text": "Sample Sufficiency In a finite mixture model with $G$ groups, each group requires sufficient observations to identify the latent group factor $\\kappa$ . This assumption provides a minimum number of observation samples, which is $2G-1$ observations in each group. Such a threshold ensures that we have enough information and variability in the observed data to distinguish the characteristics of each group. This assumption helps to identify the unique characteristics of each individual, which is critical for identifiability. ", "page_idx": 16}, {"type": "text", "text": "Sample sufficiency indicates that sufficient data are needed to achieve identifiability, which is a fundamental assumption in many analytical models. For example, in the context of nonlinear ICA using auxiliary variables [39], it is necessary to have at least $2n+1$ values for the auxiliary variables to ensure sufficient variability and guarantee identifiability. Similarly, for successful disentanglement with minimal change [47], at least $2n+1$ domain embeddings are required to ensure identifiability. Intuitively, without sufficient data to provide us with relevant information about the parameters, it is impossible to determine the values of these parameters. ", "page_idx": 16}, {"type": "text", "text": "Infinite Samples and Overlapping Conditions Asymptotic identifiability refers to the property that a model becomes identifiable as the sample size goes to infinity. In practical terms, this means that given an infinite amount of data, one would be able to consistently estimate the parameters of the model. In the context of finite mixture models, if there are enough samples for each individual, then the corresponding components can be identified directly from each individual [37]. ", "page_idx": 16}, {"type": "text", "text": "The overlapping condition requires the existence of at least two different individuals within the same group of the population, who have identical conditional probabilities. This assumption is crucial as it ensures that the model accounts for overlapping behavioral responses between different individuals, which is a common phenomenon in heterogeneous populations. Consider personalized education as an example, where students come from different academic backgrounds and have different levels of prior knowledge. Despite this initial heterogeneity, it is possible for two students in the same learning group to have the same probability of successfully completing a task. ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 4.1 and Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first show that the individualized transition processes can be viewed as a finite mixture model with grouped samples and then derive the identifiability under two scenarios. ", "page_idx": 17}, {"type": "text", "text": "B.3.1 Necessary Lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma B.1 addresses the identifiability of mixture models from grouped samples. It implies that with sufficient data per group, each component of the mixture model can be determined without ambiguity from the observed data. Specifically, suppose we have a mixture model consisting of $G$ different probability distributions, that is, $G$ components $c_{1},c_{2},\\ldots,c_{G}$ . Each component $c_{i}$ corresponds to a unique probability density function $\\mathbb{P}_{i}(\\cdot)$ . These components are mixed together, with each component having a mixing weight $\\pi_{i}$ , satisfying $\\pi_{i}\\geq0$ and iG=1 \u03c0i = 1. Now suppose we have G observation groups $g_{1},g_{2},\\ldots,g_{G}$ , with each group $g_{i}$ conta ining observations that are independent and identically distributed drawn from the same component $c_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma B.1 (Identifiability of Mixture Models from Grouped Samples [77]). Suppose we have observations from a mixture model and that they are grouped such that observations in the same group are known to be drawn from the same component. Denote by $G$ the number of groups. If there are at least $2G-1$ observations per group, any mixture of $G$ probability measures can be uniquely identified. ", "page_idx": 17}, {"type": "text", "text": "B.3.2 Proof of Identifiability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The observation model for each group $g_{i}$ with a unique latent factor $\\kappa_{i}$ can be defined as $\\begin{array}{r}{\\mathbb{P}(s,\\stackrel{.}{a},s^{\\prime}|\\kappa_{i})=\\int\\mathbb{P}(s,a,s^{\\prime},u|\\kappa_{i})d u.}\\end{array}$ , where $u$ denotes individual-specific factors within the group $g_{i}$ . Since $u$ comprises both $\\kappa$ and the initial state $z$ , we have $\\mathbb{P}(u)\\,=\\,\\mathbb{P}(u)\\mathbb{P}(\\kappa|u)\\,=\\,\\mathbb{P}(u,\\kappa)\\,=$ $\\mathbb{P}(u|\\kappa)\\mathbb{P}(\\kappa)$ . In this work, we consider two scenarios to provide identifiability from the observed temporal data collected from the population. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.1 Assumptions in theorem 4.1 and Eq. 2 ensure that individuals within the same group share identical joint distributions. Suppose the observations can be grouped into $G$ finite components, then the joint distribution can be factorized as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(s,a,s^{\\prime})=\\int\\mathbb{P}(s,a,s^{\\prime}|u)\\mathbb{P}(u)d u}\\\\ {=\\int\\mathbb{P}(\\alpha)\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a|u)d u}\\\\ {=\\int\\int\\mathbb{P}(u|s)\\mathbb{P}(\\kappa^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a|u)d u d s}\\\\ {=\\int\\int\\mathbb{P}(\\kappa)\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a|u)\\mathbb{P}(u|\\kappa)d u d\\kappa}\\\\ {=\\int\\int\\mathbb{P}(\\kappa)\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a,u|v)d u d\\kappa}\\\\ {=\\int\\int\\mathbb{P}(\\kappa)\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a,u|\\kappa)d u d\\kappa}\\\\ {=\\int\\mathbb{P}(\\kappa)\\mathbb{P}(s^{\\prime}|s,a,\\kappa)\\mathbb{P}(s,a|\\kappa)d u}\\\\ {=\\int\\mathbb{P}(\\kappa)\\mathbb{P}(s^{\\prime},s,a|\\kappa)d u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This formulation asserts that the joint distribution of $\\mathbb{P}(s,a,s^{\\prime})$ for the entire population can be modeled as a mixture model governed by the respective $\\kappa_{i}$ values. Since sample sufficiency ensures that the sample size of observations within each group is greater than $2G-1$ , then the identifiability of $\\kappa$ is guaranteed by Lemma B.1. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.2 According to assumptions in theorem 4.2, if each individual\u2019s trajectory is sufficiently long, the conditional model $\\mathbb{P}(s^{\\prime}|s,\\bar{a})$ , as well as $\\mathbb{P}(s,a)$ for the individual, would become asymptotically identifiable. Consider an extreme scenario where each individual is treated as a distinct group. Asymptotically, it is possible to identify the mixture distribution of the population. However, some individuals may share the same $\\kappa$ and can be grouped together. Then, we need to find the similarity between different individuals and merge them into the same group. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "An intuitive merging criterion is as follows. Asymptotically, it can be inferred that for some particular value of $(\\mathbf{s},\\mathbf{a})=(\\mathbf{s}^{*},\\mathbf{a}^{*})$ , the probability of ${\\bf{s}}^{\\prime}$ given $(\\mathbf{s}^{*},\\mathbf{a}^{*})$ will be the same across individuals in the same group. Define $t^{j}$ as the time of the $j$ -th occurrence of $(\\mathbf{s}^{*},\\mathbf{a}^{*})$ . We can then define the collection of variables $\\mathbf{X}^{j}$ as $\\mathbf{X}^{j}=\\{\\mathbf{s}_{t^{j}+1},t^{j}=^{\\bullet}\\},..\\,.\\}$ , representing the state at time $t+1$ given the fixed state and action $(\\mathbf{s}^{*},\\mathbf{a}^{*})$ at time $t$ . In this way, for any $j$ , $\\mathbf{X}^{j}$ sampled from a particular group $j$ . Then, the identifiability is ensured by Lemma B.1. ", "page_idx": 18}, {"type": "text", "text": "Remark B.1. Prior work [37] used a Gaussian mixture model as a prior on the coefficients, while the latent confounder variable, denoted as $Z$ , was constrained to a binary state, thereby indicating group membership for a given individual. We extend this foundation by generalizing the latent confounder to a set of discrete values and considering a nonparametric model for broader applications. ", "page_idx": 18}, {"type": "text", "text": "C Further Discussion on Identifiability Theorem ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recent work [12, 36] provides the necessary and sufficient conditions for the identifiability of certain latent structural patterns, but it rules out the case of triangle structure involving latent variables. In this paper, we extend their work to the temporal case and provide the identifiability of the latent group factor $\\kappa$ , where $\\kappa$ can be either continuous or discrete. Furthermore, we allow multiple instances of $\\kappa$ to influence the state transition dynamics. ", "page_idx": 18}, {"type": "text", "text": "C.1 Problem Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Theorem 4.3, we aim to identify the latent group factors based on a post-nonlinear temporal causal model, as shown in Figure 5. We assume the existence of a learnable and invertible embedding mapping $f$ , which is able to preserve the causal structure intrinsic to the state $s$ . Specifically, ", "page_idx": 18}, {"type": "text", "text": "Definition C.1 (Post-nonlinear Temporal Causal Models). Consider a scenario where $d_{s}$ observed states from the $k$ -th individual are denoted as $\\mathbf{s}_{t}^{k}=(s_{1,t}^{k},\\ldots,s_{d_{s},t}^{k})^{\\mathrm{T}}$ , which is a direct observation of an embedded representation $f(s)$ , alongside $m$ unobserved group factors $\\boldsymbol{\\kappa}=(\\kappa_{1},\\dots,\\kappa_{m})^{\\mathrm{T}}$ . The state transition dynamics satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{i,t+1}^{k}=f^{-1}\\left(\\sum_{j\\in\\mathcal{P}_{i}}\\alpha_{i j}f\\bigl(s_{j,t}^{k}\\bigr)+\\sum_{j\\in\\mathcal{L}_{i}}\\beta_{i j}a_{j,t}^{k}+\\sum_{j=1}^{m}\\lambda_{j}\\kappa_{j}+\\epsilon_{i,t+1}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $i=1,\\hdots,n$ . Here, $\\alpha_{i j}$ and $\\beta_{i j}$ represent causal coefficients that quantify the influence of the state $f(s_{j,t})$ and the action $a_{j,t}$ on $f(s_{i,t+1})$ , respectively. $\\mathcal{P}_{i}$ and ${\\mathcal{L}}_{i}$ denote the sets of direct state and action that influences $s_{i,t+1}^{k}$ (or $f(s_{i,t+1}^{k}))$ . Actions are considered to be stochastic. The coefficients $\\lambda_{j}$ are individual-specific and show variation across individuals. The random noise term $\\epsilon_{i,t+1}^{k}$ is independent of $s_{j,t}$ and $a_{j,t}$ for all $j\\in\\mathcal{N}^{+}$ to account for unmeasured influences. ", "page_idx": 18}, {"type": "image", "img_path": "kREpCQtHdN/tmp/f95a38b40c701de81624df55c809e5f9b1509338b6b2db49304def010ed8ab00.jpg", "img_caption": ["Figure 5: Post-nonlinear temporal causal model. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Definition C.2 (Minimal Rank Set). Let ${{S}_{t}}\\,=\\,\\{{{s}_{1,t}},{{s}_{2,t}},...\\,,{{s}_{d_{s},t}}\\}$ represent the set of all state variables in the system at any time $t=1,\\dots,T$ , and let $\\mathcal{L}=\\{\\kappa_{1},\\kappa_{2},...\\,,\\kappa_{m}\\}$ represent the set of latent variables, where $m$ and $d_{s}$ are the numbers of latent and state dimensions, respectively. A subset $\\mathcal{R}_{t,t^{-}}\\subseteq S_{t}\\cup S_{<t}$ (or $\\mathcal{R}_{t,t^{+}}\\subseteq\\mathcal{S}_{t}\\cup\\mathcal{S}_{>t},$ ) with cardinality $r$ , is called a minimal rank set if it satisfies the following conditions: ", "page_idx": 18}, {"type": "text", "text": "(i) The bottleneck set, defined as $B=\\mathcal{L}\\cup S_{t}$ for any given time $t,$ , can t-separate (see Definition C.5) any pair of minimal rank sets $(\\mathcal{R}_{t,t^{-}},\\mathcal{R}_{t,t^{+}})$ , where $t^{-}<t<t^{+}$ . ", "page_idx": 19}, {"type": "text", "text": "(ii) There does not exist a subset $\\mathcal{R}_{t,t^{\\pm}}^{\\prime}\\subset\\mathcal{R}_{t,t^{\\pm}}$ with $|\\mathcal{R}_{t,t^{\\pm}}^{\\prime}|<|\\mathcal{R}_{t,t^{\\pm}}|$ that can satisfy condition (i). ", "page_idx": 19}, {"type": "text", "text": "The set $\\mathcal{R}_{t,t^{\\pm}}$ is considered minimal in the sense that it is the smallest cardinality subset of observed state variables that includes a bottleneck set and disjoint state variables, capable of representing the essential separation status within the system. An illustrative example of a minimal rank set (see Figure 6) is shown in the yellow area, and a bottleneck set is depicted in the green area. ", "page_idx": 19}, {"type": "text", "text": "C.2 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The underlying intuition of Theorem 4.3 is that, in the absence of latent variables, rank information should align with what Conditional Independence (CI) skeleton (see Definition C.6) provides; if not, then there must exist at least one latent variable. ", "page_idx": 19}, {"type": "text", "text": "C.2.1 Necessary Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following lemma indicates that the rank of the covariance matrix (see Definition C.3) $\\Sigma_{\\mathbf{A},\\mathbf{B}}$ between any two sets of variables $\\mathbf{A}$ and $\\mathbf{B}$ is less than or equal to the sum of cardinalities of any trek-separating (see Definition C.5) sets $\\mathbf{C_{A}}$ and $\\mathbf{C_{B}}$ . The equality holds for generic covariance matrices consistent with the graph $\\mathcal{G}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.1 (Trek Separation for Directed Graphical Models [70]). The submatrix $\\Sigma_{\\mathbf{A},\\mathbf{B}}$ has rank less than or equal to $r$ for all covariance matrices consistent with the graph $\\mathcal{G}$ if and only if there exist subsets $\\mathbf{C_{A}},\\mathbf{C_{B}}\\subset V(\\mathcal{G})$ with $|\\mathbf{C_{A}}|+|\\mathbf{C_{B}}|\\,\\leq\\,r$ such that $\\mathbf{C_{A}},\\mathbf{C_{B}}$ t-separates A from $\\mathbf{B}$ . Consequently, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{rank}(\\Sigma_{\\mathbf{A},\\mathbf{B}})\\leq\\mathrm{min}\\{|\\mathbf{C_{A}}|+|\\mathbf{C_{B}}|:(\\mathbf{C_{A}},\\mathbf{C_{B}})\\,t{\\boldsymbol{-s e p a r a t e s}}\\,\\mathbf{A}\\,f r o m\\,\\mathbf{B}\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and equality holds for generic covariance matrices consistent with $\\mathcal{G}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2 (Identifiability of Linear Regression Models). Consider a linear regression model with a response variable $Y$ and $p$ predictors $X_{1},X_{2},\\ldots,X_{p}$ . The linear relationship is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nY=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+...+\\beta_{p}X_{p}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta_{0},\\beta_{1},\\ldots,\\beta_{p}$ are the regression coefficients and $\\varepsilon$ is the error term. The matrix representation can be expressed as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{Y}=\\mathbf{X}\\beta+\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{Y}$ is the response vector, $\\mathbf{X}$ is the design matrix including predictors, $\\beta$ is the vector of regression coefficients, and $\\varepsilon$ is the vector of error terms. For the regression coefficients $\\beta$ to be identifiable, the design matrix $\\mathbf{X}$ must have full column rank, meaning no predictor is a perfect linear combination of the others. This ensures that the matrix $\\mathbf{X}^{T}\\mathbf{X}$ is invertible, allowing for the unique estimation of $\\beta$ through: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\hat{\\boldsymbol{\\beta}}}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma C.3 (Identifiability of Factor Analysis). Consider a factor analysis model with p observations for each of $n$ individuals and $k$ common factors $(k<p)$ . The relationship is defined by the factor loading matrix $L\\in\\mathbb{R}^{p\\times k}$ and the factor matrix $F\\in\\mathbb{R}^{k\\times n}$ . Specifically, ", "page_idx": 19}, {"type": "equation", "text": "$$\nX=L F+\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $X\\,\\in\\,\\mathbb{R}^{p\\times n}$ is the observation matrix and $\\varepsilon\\,\\in\\,\\mathbb{R}^{p\\times n}$ is the error term matrix. The factor loading matrix $L$ and the factor matrix $F$ are unique up to an orthogonal transformation. Specifically, for any orthogonal matrix $Q$ , if we set $L^{\\prime}=L Q$ and ${\\cal F}^{\\prime}=Q^{T}{\\cal F}$ , the transformed matrices $L^{\\prime}$ and $F^{\\prime}$ also satisfy the model criteria. ", "page_idx": 19}, {"type": "text", "text": "C.2.2 Proof of Structure Identifiability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Suppose latent factors exist and influence the embedding of the observed states $f(s)$ , which preserve the causal structure intrinsic to the states $s$ . According to Lemma C.1, the rank of $\\Sigma_{\\mathbf{A_{i},B_{i}}}$ should be less than or equal to $\\operatorname*{min}\\{|\\mathbf{C_{A_{i}}}|+|\\mathbf{C_{B_{i}}}|\\}$ . In the absence of latent factors, according to the CI skeleton, the minimal configuration to t-separate $\\mathbf{A_{i}}$ from $\\mathbf{B_{i}}$ is by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(\\{f\\big(s_{1,i}\\big),\\dots,f\\big(s_{d_{s},i}\\big)\\big\\},\\emptyset\\big)\\quad\\mathrm{or}\\quad\\big(\\emptyset,\\big\\{f\\big(s_{1,i}\\big),\\dots,f\\big(s_{d_{s},i}\\big)\\big\\}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, the rank of the covariance matrix is $\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{\\mathrm{i}},\\mathbf{B}_{\\mathrm{i}}})=|\\{f(s_{1,i}),\\dots,f(s_{d_{s},i})\\}|+|\\emptyset|=$ $d_{s}$ . If the calculated rank is greater than $d_{s}$ , it implies the presence of latent variables accounting for the unexplained variance since the observed variables alone would not result in such rank deficiency. ", "page_idx": 20}, {"type": "text", "text": "In scenarios with latent factors, the maximum rank deficiency observed across covariance submatrices, representing the discrepancy between the expected and actual ranks, establishes a lower bound for the number of latent variables. Considering the minimal t-separation of $\\mathbf{A_{i}}$ and $\\mathbf{B_{i}}$ occurs via ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\big(\\{f(s_{1,i}),\\dots,f(s_{d_{s},i}),\\kappa_{1},\\dots,\\kappa_{m}\\},\\emptyset\\big)\\quad\\mathrm{or}\\quad(\\emptyset,\\{f(s_{1,i}),\\dots,f(s_{d_{s},i}),\\kappa_{1},\\dots,\\kappa_{m}\\}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the rank of the covariance matrix is $\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{\\mathrm{i}},\\mathbf{B}_{\\mathrm{i}}})=|\\{f(s_{1,i}),\\dots,f(s_{d_{s},i}),\\kappa_{1},\\dots,\\kappa_{m}\\}|\\,+$ $|\\emptyset|=m+d_{s}$ . By iteratively computing the rank of $\\operatorname{rank}(\\Sigma_{\\mathbf{A_{i}},\\mathbf{B_{i}}})$ , a consistent value corroborates the existence of latent factors influencing all observed states. Furthermore, the count of latent factors can be deduced by $m=\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{\\mathrm{i}},\\mathbf{B}_{\\mathrm{i}}})\\bar{-d}_{s}$ . ", "page_idx": 20}, {"type": "text", "text": "In conclusion, under the conditions of the theorem, if the observed rank deficiency in the covariance matrix of observed variables cannot be explained by the observed variables alone, it implies the existence of latent variables. Furthermore, the number of such latent variables can be inferred from the extent of the rank deficiency. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.2.3 Proof of Parameter Identifiability ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. For each individual $k$ , consider the proposed model at any time $t$ and $t+1$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad f(s_{i,t}^{k})=\\displaystyle\\sum_{j\\in\\mathcal{P}_{i}}\\alpha_{i j}f\\big(s_{j,t-1}^{k}\\big)+\\displaystyle\\sum_{j\\in\\mathcal{L}_{i}}\\beta_{i j}a_{j,t-1}^{k}+\\displaystyle\\sum_{j=1}^{m}\\lambda_{j}\\kappa_{j}+\\epsilon_{i,t}^{k},}\\\\ &{f\\big(s_{i,t+1}^{k}\\big)=\\displaystyle\\sum_{j\\in\\mathcal{P}_{i}}\\alpha_{i j}f\\big(s_{j,t}^{k}\\big)+\\displaystyle\\sum_{j\\in\\mathcal{L}_{i}}\\beta_{i j}a_{j,t}^{k}+\\displaystyle\\sum_{j=1}^{m}\\lambda_{j}\\kappa_{j}+\\epsilon_{i,t+1}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Subtracting these two equations, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(s_{i,t+1}^{k})-f(s_{i,t}^{k})=\\sum_{j\\in\\mathcal{P}_{i}}\\alpha_{i j}\\big(f(s_{j,t}^{k})-f(s_{j,t-1}^{k})\\big)+\\sum_{j\\in\\mathcal{L}_{i}}\\beta_{i j}(a_{j,t}^{k}-a_{j,t-1}^{k})+(\\epsilon_{i,t+1}^{k}-\\epsilon_{i,t}^{k}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $x_{i,t+1}^{k}=f(s_{i,t+1}^{k})-f(s_{i,t}^{k}),y_{i,t+1}^{k}=a_{i,t+1}^{k}-a_{i,t}^{k}$ , and $\\eta_{i,t+1}^{k}=\\epsilon_{i,t+1}^{k}-\\epsilon_{i,t}^{k}$ . Substituting these, the model transforms to: ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{i,t+1}^{k}=\\sum_{j\\in\\mathcal{P}_{i}}\\alpha_{i j}x_{j,t}^{k}+\\sum_{j\\in\\mathcal{L}_{i}}\\beta_{i j}y_{j,t}^{k}+\\eta_{i,t+1}^{k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In that case, the identifiability of $\\alpha$ and $\\beta$ can be directly derived by Lemma C.2. We further assume that the $m$ latent factors follow the Normal distribution. Drawing on methodologies used in factor analysis C.3, then $\\lambda$ is orthogonal-wise identifiable. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.3 Examples For Theorem 4.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we present four illustrative examples to describe cases where identifiability is achieved. For the sake of simplicity, we define $X$ as $X=f(s)$ and omit the terms $a$ and $\\epsilon$ from our illustration for simplicity, under the assumption that they are random and independent variables. ", "page_idx": 20}, {"type": "text", "text": "Example 1 In Example 1, as shown in Figure 6, there is only one latent factor and onedimensional states. The bottleneck set is $(X_{3},\\kappa)$ , and the pairs of minimal rank sets are $(\\mathbf{A},\\mathbf{B})=$ $\\left(\\left(X_{1},X_{2},X_{3}\\right)\\right.$ , $(X_{3},X_{4},X_{5}))$ . According to the causal graph, the minimal way to t-separate A from $\\mathbf{B}$ is either $(\\{\\kappa,X_{3}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa,\\bar{X_{3}}\\})$ . Consequently, the rank of the covariance matrix is $\\mathrm{rank}({\\Sigma}_{\\bf A,B})=|\\{\\kappa,X_{3}\\}|+|\\emptyset|=2$ . According to Theorem 2, the fact that $\\operatorname{rank}(\\Sigma_{\\mathbf{A},\\mathbf{B}})=2>1$ indicates the presence of the latent factor. Consequently, the number of latent variables can be deduced as $m^{\\overset{\\cdot}{=}}\\operatorname{rank}(\\Sigma_{\\mathbf{A},\\mathbf{B}})-1=2-1=1$ . ", "page_idx": 20}, {"type": "image", "img_path": "kREpCQtHdN/tmp/c3a6c0162f4034c426c9d3c2b4265b4622e2603a62aa01b9a6e11989c1608f57.jpg", "img_caption": ["Figure 10: Examples that illustrate different identifiable cases. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Example 2 As shown in Figure 7, there are $m$ latent factors and one-dimensional states. The bottleneck sets are $((X_{m+2},\\kappa_{1},\\ldots,\\kappa_{m}),\\ldots,(X_{T-m-1},\\kappa_{1},\\ldots,\\kappa_{m}$ )). Suppose $T\\,=\\,2m+4$ , then the pairs of minimal rank sets are $(\\mathbf{A_{1}},\\mathbf{B_{1}})=((X_{1},\\ldots,X_{m+2})$ , $(X_{m+2},\\ldots,X_{2m+3}))$ and $(\\mathbf{A_{2}},\\mathbf{B_{2}})=((X_{2},\\ldots,X_{m+3}),(X_{m+3},\\ldots,X_{2m+4}))$ ). According to the graph, the minimal configuration to t-separate $\\mathbf{A_{1}}$ from $\\mathbf{B_{1}}$ is either $(\\{\\kappa_{1},\\hdots,\\kappa_{m},X_{m+2}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa_{1},\\dots,\\kappa_{m},X_{m+2}\\})$ with $\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{1},\\mathbf{B}_{1}})\\,=\\,|\\bigl\\{\\kappa_{1},\\dots,\\kappa_{m},X_{m+2}\\bigr\\}|+|\\emptyset|\\,=\\,m+1$ . The minimal configuration to tseparate $\\mathbf{A_{2}}$ from $\\mathbf{B_{2}}$ is either $(\\{\\kappa_{1},\\ldots,\\kappa_{m},X_{m+3}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa_{1},\\dots,\\kappa_{m},X_{m+3}\\})$ , resulting in r $\\mathrm{ank}(\\Sigma_{\\mathbf{A}_{2},\\mathbf{B}_{2}})=|\\{\\kappa_{1},\\dots,\\kappa_{m},X_{m+3}\\}|+|\\emptyset|=m+1.\\ \\Lambda$ According to Theorem 2, the fact that $\\operatorname{rank}(\\Sigma_{\\mathbf{A_{1}},\\mathbf{B_{1}}})=\\operatorname{rank}(\\Sigma_{\\mathbf{A_{2}},\\mathbf{B_{2}}})=m+1>1$ indicates the presence of the latent factor. Consequently, the number of latent variables can be deduced as $m=\\mathrm{rank}\\big(\\Sigma_{\\mathbf{A_{i}},\\mathbf{B_{i}}}\\big)-1=m+1-1=m$ . ", "page_idx": 21}, {"type": "text", "text": "Example 3 As shown in Figure 8, there is one latent factor and two-dimensional states. The bottleneck sets are $((X_{13}^{\\bar{1}},X_{23},\\kappa),(X_{14},X_{24},\\kappa))$ , and one possible pairs of minimal rank sets are $(\\mathbf{A_{1}},\\mathbf{B_{1}})\\;\\;=\\;\\;((X_{11},X_{12},X_{13},X_{23}),$ $\\left(X_{13},X_{23},X_{24},X_{25}\\right))$ and $\\begin{array}{r l}{\\left(\\mathbf{A_{2}},\\mathbf{B_{2}}\\right)}&{{}=}\\end{array}$ $\\left(\\left(X_{12},X_{13},X_{14},X_{24}\\right)\\right.$ , $(X_{14},X_{24},X_{25},X_{26}))$ . According to the causal graph, the minimal configuration to t-separate $\\mathbf{A_{1}}$ from $\\mathbf{B_{1}}$ is either $(\\{\\dot{\\kappa},X_{13},X_{23}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa,X_{13},X_{23}\\})$ . Consequently, the rank of the covariance matrix is $\\mathrm{rank}(\\Sigma_{\\mathbf{A_{1}},\\mathbf{B_{1}}})\\,=\\,|\\{\\kappa,X_{13},X_{23}\\}|+|\\emptyset|\\,=\\,3$ . Similarly, the minimal configuration to t-separate $\\mathbf{A_{2}}$ from $\\mathbf{B_{2}}$ is either $(\\{\\kappa,X_{14},X_{24}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa,X_{14},X_{24}\\})$ , resulting in $\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{2},\\mathbf{B}_{2}})=|\\{\\kappa,X_{14},X_{24}\\}|+|\\emptyset|=3$ . According to Theorem 2, the fact that $\\mathrm{rank}(\\Sigma_{\\mathbf{A_{1},B_{1}}})=\\mathrm{rank}(\\Sigma_{\\mathbf{A_{2},B_{2}}})=3>2$ indicates the presence of the latent factor. Consequently, the number of latent variables can be deduced as $m=\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{\\mathrm{i}},\\mathbf{B}_{\\mathrm{i}}})-2=3-2=1.$ . ", "page_idx": 21}, {"type": "text", "text": "Example 4 As shown in Figure 9, there are two latent factors and two-dimensional states. The bottleneck sets are $\\left(\\left(X_{14},X_{24},\\kappa_{1},\\kappa_{2}\\right)\\right.$ , $\\left(X_{15},X_{25},\\kappa_{1},\\kappa_{2}\\right)$ , and one possible pairs of minimal rank sets are $(\\mathbf{A_{1}},\\mathbf{B_{1}})\\:=\\:((X_{11},X_{12},X_{13},X_{14},X_{24})$ , $\\left(X_{14},X_{24},X_{25},X_{26},X_{27}\\right))$ and $(\\mathbf{A_{2}},\\mathbf{B_{2}})\\ =$ $\\left(\\left(X_{12},X_{13},X_{14},X_{15},X_{25}\\right)\\right.$ , $(X_{15},X_{25},X_{26},X_{27},X_{28})$ ). According to the graph, the minimal configuration to t-separate $\\mathbf{A_{1}}$ from $\\mathbf{B_{1}}$ is either $(\\{\\kappa_{1},\\kappa_{2},X_{14},X_{24}\\},\\emptyset)$ or $(\\bar{\\emptyset},\\{\\kappa_{1},\\kappa_{2},X_{14},X_{24}\\})$ . Consequently, the rank of the covariance matrix is $\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{1},\\mathbf{B}_{1}})=|\\{\\kappa_{1},\\kappa_{2},X_{14},X_{24}\\}|+|\\emptyset|=4$ . Similarly, the minimal configuration to t-separate $\\mathbf{A_{2}}$ from $\\mathbf{B_{2}}$ is either $(\\{\\kappa_{1},\\kappa_{2},X_{15},X_{25}\\},\\emptyset)$ or $(\\emptyset,\\{\\kappa_{1},\\dot{\\kappa}_{2},X_{15},X_{25}\\})$ with $\\mathrm{rank}({\\Sigma}_{\\bf A_{2},\\bf B_{2}})=|\\{\\kappa_{1},\\kappa_{2},X_{15},X_{25}\\}|+|\\emptyset|=4$ . According to Theorem 2, the fact that $\\mathrm{rank}(\\Sigma_{\\mathbf{A_{1},B_{1}}})=\\mathrm{rank}(\\Sigma_{\\mathbf{A_{2},B_{2}}})=4>2$ indicates the presence of the latent factor. Consequently, the number of latent variables can be deduced as $m=\\mathrm{rank}(\\Sigma_{\\mathbf{A}_{\\mathrm{i}},\\mathbf{B}_{\\mathrm{i}}})-2=$ $4-2=2$ . ", "page_idx": 21}, {"type": "text", "text": "C.4 Related Definitions of Theorem 4.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.4.1 Covariance Matrix of Random Vector ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this discussion, we introduce the concept of the covariance matrix within the framework of latent variable models. By examining the properties of the covariance matrix, such as its rank, we are able to identify signs of latent variables\u2014rank deficiencies, which serve as a measure of the cardinality of the minimal set of latent variables required to explain the observed dependencies. Such rank deficiencies indicate the presence of latent variables that extend beyond the observable scope. ", "page_idx": 22}, {"type": "text", "text": "Consider a directed acyclic graph (DAG), denoted as $\\mathcal{G}$ , whose vertices $V(\\mathcal{G})$ form the set $[m]:=$ $\\{1,2,\\ldots,m\\}$ . Each node $i$ in $\\mathcal{G}$ is associated with a random variable $X_{i}$ and an independent error term $\\epsilon_{i}\\sim\\mathcal{N}(0,\\phi_{i})$ with $\\phi_{i}>0$ . The DAG structure imposes a recursive relationship among the variables, where the value of $X_{j}$ can be expressed as a linear combination of the variables $X_{i}$ of its parent vertices $\\mathtt{p a}(j)$ , alongside the error term $\\epsilon_{j}$ and regression coefficients $\\lambda_{i j}$ that correspond to the edges $i\\rightarrow j$ in $\\mathcal{G}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{j}=\\sum_{i\\in\\mathrm{pa}(j)}\\lambda_{i j}X_{i}+\\epsilon_{j}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathtt{p a}(j)$ denotes the set of parent nodes of vertex $j$ , where a parent node $i$ is one that has an edge leading to $j$ in $\\mathcal{G}$ . From this recursive sequence of regressions, one can solve for the covariance matrix $\\Sigma$ of the jointly normal random vector $\\mathbf{X}$ , which is defined as follows. ", "page_idx": 22}, {"type": "text", "text": "Definition C.3 (Covariance Matrix of Random Vector [70]). The covariance matrix of the random vector is given by the matrix factorization ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Sigma=\\Lambda^{-\\top}\\Phi\\Lambda^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where matrix $\\Phi$ is defined as a diagonal matrix with the variances of the error terms as its diagonal elements: $\\Phi\\,=\\,\\mathrm{diag}(\\phi_{1},.\\,.\\,.\\,,\\phi_{m})$ . The matrix $M$ is an $m\\times m$ upper triangular matrix where $M_{i j}\\,=\\,\\lambda_{i j}$ if $i\\,\\rightarrow\\,j$ is an edge in $\\mathcal{G}$ , and $M_{i j}\\,=\\,0$ otherwise. Thus the matrix $\\Lambda$ is defined as $\\Lambda=I-M$ , where $I$ is the $m\\times m$ identity matrix. ", "page_idx": 22}, {"type": "text", "text": "Specifically, given two subsets $\\mathbf{\\partial}_{\\mathbf{\\tau}},\\mathbf{B}\\subset[m],\\Sigma_{\\mathbf{A},\\mathbf{B}}=(\\sigma_{a b})_{a\\in\\mathbf{A},b\\in\\mathbf{B}}$ is defined as the submatrix of covariance with row index set A and column index set $\\mathbf{B}$ . ", "page_idx": 22}, {"type": "text", "text": "C.4.2 Trek and Trek Separation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The concepts of Trek and Trek Separation precede a crucial need to address the presence of latent variables and intricate dependency structures that are not directly observable. The Trek represents a particular path that interconnects variables within a graph, even if they are not directly linked, while Trek Separation delineates a criterion to ascertain whether two sets of variables are independent, conditional on a set of other variables. Below, we give the formation definitions of these two concepts. ", "page_idx": 22}, {"type": "text", "text": "Definition C.4 (Trek [70]). A trek in $\\mathcal{G}$ from i to $j$ is an ordered pair of directed paths $(P_{1},P_{2})$ where $P_{1}$ has sink i, $P_{2}$ has sink $j$ , and both $P_{1}$ and $P_{2}$ have the same source $k$ . The common source $k$ is called the top of the trek, denoted top $\\left(P_{1},P_{2}\\right)$ . Note that one or both of $P_{1}$ and $P_{2}$ may consist of a single vertex, that is, a path with no edges. A trek $(P_{1},P_{2})$ is simple if the only common vertex among $P_{1}$ and $P_{2}$ is the common source $:o p(P_{1},P_{2})$ . We let $\\tau(i,j)$ and $s(i,j)$ denote the sets of all treks and all simple treks from i to $j$ , respectively. ", "page_idx": 22}, {"type": "text", "text": "Definition C.5 (Trek Separation [70]). Let A, B, $\\mathbf{C_{A}}$ and $\\mathbf{C_{B}}$ be four subsets of $V(\\mathcal{G})$ which need not be disjoint. We say that the pair $(\\mathbf{C_{A}},\\mathbf{C_{B}})$ trek separates (or $t$ -separates) A from $\\mathbf{B}$ if for every trek $(P_{1},P_{2})$ from a vertex in A to a vertex in $\\mathbf{B}$ , either $P_{1}$ contains a vertex in $\\mathbf{C_{A}}$ or $P_{2}$ contains $a$ vertex in $\\mathbf{C_{B}}$ . ", "page_idx": 22}, {"type": "text", "text": "C.4.3 Conditional Independence Skeleton ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The Conditional Independence (CI) skeleton in graphical models refers to a structure that represents the conditional independence among observed variables. The CI skeleton can be used to infer the existence of latent variables. If the observed data suggests dependencies not represented in the CI skeleton, it may indicate hidden factors at play. The formal definition is given as follows. ", "page_idx": 22}, {"type": "text", "text": "Definition C.6 (Conditional Independence Skeleton [12]). A CI skeleton of X is an undirected graph where the edge between $X_{1}$ and $X_{2}$ exists if and only if there does not exist a set of observed variables $\\mathbf{C}$ such that $X_{1},X_{2}\\notin\\mathbf{C}$ and $X_{1}$ \u22a5\u22a5 $X_{2}|\\mathbf{C}$ . ", "page_idx": 22}, {"type": "text", "text": "D Background ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Reinforcement Learning In RL, an agent learns to make decisions by interacting with the environment. The agent receives rewards for taking actions in the environment and uses this feedback to learn optimal behavior. It is often modeled as a Markov Decision Process (MDP) represented by a tuple $\\left\\langle{S,A,\\mathbb{P},R,\\gamma}\\right\\rangle$ , where $\\boldsymbol{S}$ denotes a finite set of states representing different situations an agent might encounter, $\\mathcal{A}$ a finite set of actions representing different decisions an agent can make, $\\mathbb{P}$ a state transition function defining the probability of transitioning to a new state $s^{\\prime}$ given a current state $s$ and action $a$ , denoted as $\\bar{\\mathbb{P}}(s^{\\prime}|s,a)$ , $R$ a reward function assigning a scalar value to each state-action pair $(s,a)$ , representing the immediate reward received after performing action $a$ in state s. $\\gamma\\in[0,1]$ is the discount factor, representing the agent\u2019s consideration for future rewards. The agent\u2019s goal is to learn an optimal policy $\\pi^{*}$ , which defines the optimal set of actions in different states to maximize the expected cumulative discounted reward over the long run. Developing this optimal policy involves estimating value functions such as the action-value function, defined as $\\begin{array}{r}{\\dot{Q^{\\pi}}(s,a)\\dot{=}\\;\\mathbb{\\dot{E}}_{\\pi}\\big[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}|S_{0}\\;\\overset{\\texttt{c}}{=}\\;s,A_{0}\\;=\\;a\\big]}\\end{array}$ , which represents the expected reward of taking action $a$ in state $s$ following policy $\\pi$ . The pursuit of optimal policy $\\pi^{*}$ involves maximizing the value functions over all possible state-action pairs: $\\pi^{*}=\\arg\\operatorname*{max}_{\\pi}Q^{\\pi}(s,a)$ . ", "page_idx": 23}, {"type": "text", "text": "Variational Autoencoder Variational Autoencoders (VAEs) [44] are a class of generative models in deep learning, adept at unsupervised learning of complex data distributions. Rooted in the framework of Bayesian inference, VAEs are designed to approximate probability density functions of input data. The architecture of a VAE consists of two primary components: an encoder $q_{\\phi}(z|x)$ and a decoder $p_{\\theta}(x|z)$ . The encoder maps input data $x$ to a latent space, represented by a probability distribution, typically Gaussian, with parameters $\\mu$ and $\\sigma$ signifying the mean and standard deviation, respectively. The decoder reconstructs the input data from a sampled latent representation $z$ . ", "page_idx": 23}, {"type": "text", "text": "The distinct feature of VAEs lies in their probabilistic approach. The encoder outputs parameters of a latent distribution, from which a sample $z$ is drawn: ", "page_idx": 23}, {"type": "equation", "text": "$$\nz\\sim q_{\\phi}(z|x)=\\mathcal{N}(z;\\mu,\\sigma^{2}I)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The decoder then attempts to reconstruct the input from this latent sample. VAEs optimize the Evidence Lower Bound (ELBO) objective, which balances two aspects: the reconstruction quality and the regularization of the latent space. The traditional ELBO is given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{ELBO}=\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]-\\mathrm{KL}[q_{\\phi}(z|x)||p(z)]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, the first term measures the reconstruction quality, while the second term, the Kullback-Leibler (KL) divergence, imposes a regularization by encouraging the latent distribution $q_{\\phi}(z|x)$ to be close to a prior $p(z)$ , typically assumed to be a standard normal distribution ${\\mathcal{N}}(0,I)$ . VAEs, through this optimization, are capable of generating new data points that are similar to the input data, making them highly valuable in applications like image generation, denoising, and anomaly detection within the domain of unsupervised learning. ", "page_idx": 23}, {"type": "text", "text": "E Detailed Related Work ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Individualized Machine-Learning Applications In the modern era, the power of machine learning has been harnessed to create highly individualized solutions across a myriad of domains. In the realm of health and wellness, machine learning aids in tailoring interventions for increasing physical activity [89, 57], promoting weight loss [18, 17], improving adherence for diabetes [89]. For the elderly, personalized algorithms assist in both technology adaptation and specialized care for conditions [32]. The financial sector benefits from machine learning\u2019s prowess in optimizing technical indicators, making stock market predictions more precise and individualized [56]. In the educational landscape, Information and Communication Technology (ICT) leverages machine learning to offer personalized education systems such as adaptive e-learning interfaces [16] and individualized tutorial planning [40]. Furthermore, the transportation sector sees advancements with car-following control strategies tailored for individual drivers [69]. Multimedia platforms, such as YouTube and TikTok, are enhancing user experiences by offering video content recommendations fine-tuned to individual preferences using reinforcement learning [6, 33]. These examples merely scratch the surface, emphasizing the vast and diverse applications of individualized machine learning in today\u2019s world. ", "page_idx": 23}, {"type": "text", "text": "Reinforcement Learning for Latent State-Transition Processes RL has witnessed significant advancements in recent years, particularly with the integration of latent variable models to capture the underlying dynamics of environments. A primary focus in this domain is learning low-dimensional, latent Markovian representations from observed data [54, 48, 42, 25, 82, 95, 49, 62, 19, 23, 20, 91]. Common strategies for state representation learning include reconstructing the observation, learning a forward model, or learning an inverse model. Additionally, prior knowledge, such as temporal continuity [83], can be leveraged to constrain the state space. Numerous studies have proposed methods to estimate the underlying state-transition process from high-dimensional input sequences [82, 13, 25, 27, 94, 19, 41, 28]. Using the learned world model, agents can engage in model-based RL or planning. Furthermore, these methods encode structural constraints, ensuring the sufficiency and minimality of the estimated state representations from both generative and selection processes. Recently, several studies [60, 55, 78, 79, 4, 65] have aimed to estimate the state-transition process in the presence of latent confounders. A handful of work [60, 65] can be viewed as addressing similar settings involving individual-specific factors. However, to the best of our knowledge, we have yet to identify a systemic approach that offers a clear identifiability result for the state-transition process when individual-specific factors are present. ", "page_idx": 24}, {"type": "text", "text": "Comparisons with Related Works Contextual MDPs [29] consider the general contextual influence on transition probabilities and rewards. However, the context variables are assumed to be partially observable and do not guarantee the identifiability of the context variables. In our case, when the latent factors are finite, our method guarantees group-wise identifiability even when the transition processes are nonparametric. In the cases of infinite latent factors, identification could be achieved under proper assumptions. ", "page_idx": 24}, {"type": "text", "text": "Multi-task RL [75] involves learning policies for a variety of tasks simultaneously. The goal of the agent is to perform well on all these tasks, which may have similar or different objectives. often involves sharing information between tasks to improve learning efficiency and policy performance. Instead of focusing on policy optimization for all tasks, our work identifies latent individual-specific factors that implicitly influence the decision-making process. These factors indicate the unique properties of each individual, providing explanatory clues for policy adaptation. ", "page_idx": 24}, {"type": "text", "text": "Meta-RL [3] trains a learning model on a variety of tasks so that it can efficiently apply what it has learned to new tasks. Unlike our method, it does not assume a time-invariant latent factor, has no guarantee of identifiability, and does not provide a clear clue of adaptation. While iMDP captures how an individual\u2019s belonging to a certain group affects their interactions within an environment, allowing for individualized policy adaptation. Moreover, iMDP provides a guarantee of identifiability and develops a corresponding estimation framework that potentially offers better interpretability. ", "page_idx": 24}, {"type": "text", "text": "Factored MDP [92] and Factored Non-stationary MDP [24] assume there are no unobserved confounders in the state transitions. Block MDP [10], POMDP [38], and Latent MDP [86] consider latent states/spaces, but such latent factors do not influence each state in the transition process. Specifically, existing works [92, 10, 24] usually focus on latent variables that are time-varying. When they are time-varying, they can benefit from many recent advances in nonlinear ICA to achieve strong identifiability results [38, 86]. However, the identifiability of time-invariant latent confounders, though not well-studied, has numerous applications. The aforementioned settings differ significantly from our work since we consider a latent group factor that influences each state in the state transition process, and the proposed individualized transition process is motivated by numerous applications. We provide theoretical results that when individual-specific factors are finite, our method ensures the identifiability of the entire latent state-transition process, even in the case of nonparametric transitions. This establishes novel theoretical insights for learning state-transition processes with latent factors. ", "page_idx": 24}, {"type": "text", "text": "F Experiment Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1 Evaluation Metrics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Pearson Correlation Coefficient Pearson Correlation Coefficient (PCC) [9] is a statistical measure that quantifies the degree of linear relationship between two variables. It provides a value between -1 and 1, where 1 implies a perfect positive linear relationship, -1 implies a perfect negative linear relationship, and 0 implies no linear relationship between the variables. The equation for calculating the Pearson Correlation Coefficient $r$ between two variables $X$ and $Y$ is as follows: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\nr={\\frac{n(\\sum x y)-(\\sum x)(\\sum y)}{\\sqrt{[n\\sum x^{2}-(\\sum x)^{2}][n\\sum y^{2}-(\\sum y)^{2}]}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $n$ is the number of paired samples, $\\sum{x y}$ is the sum of the product of paired scores, $\\sum x$ and $\\sum y$ are the sums of the $x$ scores and $y$ scores respectively, $\\textstyle\\sum x^{\\frac{\\imath}{2}}$ and $\\sum y^{2}$ are the sums of the squared $x$ scores and $y$ scores respectively. ", "page_idx": 25}, {"type": "text", "text": "Canonical Correlation Analysis Canonical Correlation Analysis (CCA) [34] is designed to identify bases for two sets of variables in order to maximize the mutual correlations between the projections onto these bases. In our work, CCA is used as an evaluation metric to validate that the recovered latent variable is meaningfully related to the ground truth latent variable, thus proving the relevance of the estimated representations. Let $X$ and $Y$ be the two sets of observed variables. This algorithm starts by centering the columns of $X$ and $Y$ so that they have zero mean. Then the covariance matrices $C_{X X}=$ $X^{\\top}X,{\\bar{C}}_{Y Y}=Y^{\\top}Y$ , and $C_{X Y}=X^{\\top}Y$ are calculated. After that, the canonical correlations are obtained by solving the following generalized eigenvalue problem: $C_{X X}^{-1}C_{X Y}C_{Y Y}^{-1}C_{Y X}\\nu=\\lambda\\nu$ . The square roots of the eigenvalues $\\lambda$ indicate the canonical correlations between the linear combinations of $X$ and $Y$ . The corresponding eigenvectors $\\nu$ and $u\\,=\\,C_{X Y}\\nu$ are the canonical weights used to construct the canonical variables. Finally, the canonical variables of $X$ and $Y$ are $U\\,=\\,X\\nu$ and $V=Y u$ , respectively, representing the linear combinations of the original variables that are maximally correlated. The correlation of the primary pair of canonical variables is the highest, followed by the secondary pair, and so on. When employing CCA as an evaluation metric, a higher canonical correlation indicates a stronger and more relevant relationship between the recovered latent variable and the ground truth latent variable. ", "page_idx": 25}, {"type": "text", "text": "To extend the capability of CCA for analyzing nonlinear relationships, Kernel Canonical Correlation Analysis (KCCA) [84] is employed in the experiment which uses kernel functions to map the original variables into a higher-dimensional feature space. This allows for the capture of more complex, nonlinear correlations between the variables, thus potentially increasing the robustness and relevance of the relationships discovered in scenarios where linear methods fall short. ", "page_idx": 25}, {"type": "text", "text": "F.2 Dataset Descriptions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Synthetic Data Generation Processes In this paper, we created three synthetic datasets: Case 1 corresponds to a finite latent factor that satisfies our assumptions, and Case 2 and Case 3 allow for multiple finite and infinite latent variables. The dimensions of states and actions are set to 3 and 2, respectively. The actions taken are generated randomly, following a uniform distribution $\\mathrm{Uniform}(\\bar{0},1)$ . The noise term follows a mean-zero Gaussian distribution. The mixing function $f$ corresponds to the post-nonlinear model [93], where $f_{1}$ represents the nonlinear effect, and $f_{2}$ denotes the invertible post-nonlinear distortion on $\\mathbf{s}_{t}$ , embodied by a randomly initialized three-layer MLP with Tanh activation function. The data generation process follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{s}_{t}=f_{2}\\big(f_{1}\\big(\\mathbf{s}_{t-1},\\mathbf{a}_{t-1},\\kappa\\big),\\epsilon_{t}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "PersuasionForGood The PersuasionForGood dataset reveals the mechanics of persuasion in the context of charitable giving. It contains 1017 dialogues from 1285 participants in which one participant, called the persuader (ER), tries to convince the other participant, called the persuadee (EE), to donate to a charity. An example dialog is shown in Figure 11. All participants underwent personality assessments, which included detailed participant-level information such as demographics, Big Five personality traits, moral foundations, and so on, allowing for a multifaceted analysis of persuasion strategies and allowing us to use the labeled 32-dimensional personalities of each persuader as the ground-truth latent factor in our experiments. ", "page_idx": 25}, {"type": "text", "text": "We use this dataset to evaluate the performance of our estimation framework. We compare the estimated factors to the documented personality traits of persuaders. By examining the interactions between participants with different backgrounds and personalities, we aim to identify underlying patterns that could create effective persuasive agents. Specifically, we use BERT embeddings to generate a 768-dimensional feature vector for each dialog utterance. This process starts with tokenization, segmenting words into smaller units. BERT then processes these tokens to produce contextual embeddings. ", "page_idx": 25}, {"type": "table", "img_path": "kREpCQtHdN/tmp/80e0e075767b89767dab32fe7f99058b6cdff73695cb4371777b1216abb1d5ac.jpg", "table_caption": ["Figure 11: A sample persuasive dialog between persuader (ER) and persuadee (EE) from the PersuasionForGood corpus, along with the Big Five personality test scores, including Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Pendulum The pendulum environment, provided by OpenAI Gym, is a classic control task used for the evaluation RL models. This environment presents a continuous control task where the agent must learn to control a frictionless pendulum with the goal of swinging it to the highest point and keeping it in the inverted position. The pendulum starts at a random position, and the goal is to bring it to a standstill at the inverted position with the least amount of effort. The system is characterized by a continuous action space, representing the torque applied to the pendulum\u2019s fulcrum. For a pendulum of length $l$ and mass $m$ , subject to gravity $g$ and a control input $u$ , the equations of motion can be described by the following second-order nonlinear ordinary differential equations $\\dot{\\theta}=\\omega$ , $\\begin{array}{r}{\\dot{\\omega}=-\\frac{g}{l}\\sin(\\theta)+\\frac{\\dot{u}}{m l^{2}}}\\end{array}$ , where $\\theta$ is the angle of the pendulum from the vertical upright position, and $\\omega$ is the angular velocity of the pendulum. The state of the pendulum at any time $t$ can be represented as $\\vec{s_{t}}=\\left[\\cos\\theta_{t},\\sin\\theta_{t},\\dot{\\omega_{t}}\\right]$ , action represents the torque applied to the free end of the pendulum in the range $a_{t}\\in[-2,2]$ , and the reward function is defined as: $r_{t}=-(\\theta_{t}^{2}+0.1*\\omega_{t}^{2}+\\dot{0}.001*a_{t}^{2})$ . ", "page_idx": 26}, {"type": "text", "text": "The goal of RL algorithms is to determine an optimal control policy $\\pi^{*}$ that minimizes the effort to swing and balance the pendulum upright, typically by minimizing a cost function defined over states and actions. Each episode provides a continuous stream of observations, actions, and rewards, allowing the development and evaluation of algorithms capable of learning effective control policies in continuous action spaces. In academic studies, the Pendulum environment serves as a benchmark to investigate the effectiveness of RL algorithms in handling continuous control tasks. ", "page_idx": 26}, {"type": "text", "text": "HeartPole HeartPole provides a straightforward scenario for assessing healthcare treatment, highlighting the complex interplay between productivity, health, and decision-making strategies. It simulates a professional\u2019s quest for increased productivity and examines the long-term health impacts of short-term choices, such as insufficient sleep, and intake of coffee and alcohol. The states include alertness, hypertension, intoxication, time since last sleep, total elapsed time, and total work done. ", "page_idx": 26}, {"type": "text", "text": "A productivity function and a heart attack risk function are defined over these variables, rewarding incremental productivity while imposing a significant penalty for heart attacks. Every thirty minutes, the agent evaluates the current state and chooses from a set of actions: work, drink coffee (which increases alertness and hypertension), drink alcohol (which decreases alertness while increasing hypertension and intoxication), or sleep (time-consuming but essential to reduce hypertension and intoxication to maintain alertness). ", "page_idx": 26}, {"type": "text", "text": "Half Cheetah Half Cheetah is an integral part of the Mujoco physics engine, designed to simulate the agility and mechanics of a cheetah through a 2D robotic model. This model consists of 9 body parts, including a torso, two front and two back thighs, shins and feet, connected by 8 joints to provide fluid motion reminiscent of a cheetah\u2019s natural gait. The primary goal for this robot is to achieve maximum forward speed while maintaining stability, mirroring the efficiency and speed of its biological counterpart. ", "page_idx": 27}, {"type": "text", "text": "The observational data in this environment includes both the position and velocity of each segment of the half cheetah, methodically ordered with all position data provided before velocity information. This systematic ordering allows for a detailed understanding of the dynamics of the robot at any given time. Actions within this simulation are defined by the torque applied to the joints, which directly affects its acceleration and motion patterns. The reward function for Half Cheetah is designed to encourage rapid forward motion and operational efficiency and consists of two main components: a forward motion reward proportional to the increase in the robot\u2019s horizontal displacement over time and a control cost penalty for unnecessary control effort and external force application. This reward structure is carefully designed to encourage the optimization of forward motion, with an emphasis on reducing control effort and mitigating forces that may interfere with the robot\u2019s streamlined motion. ", "page_idx": 27}, {"type": "text", "text": "F.3 Additional Experiment Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Ablation Study: Variability in number of samples Here, we vary the number of samples to further verify this effectiveness. The data generation process is the same as Case 1, except that we change the number of samples to $\\{100,\\bar{15}0,200,3\\bar{00},500,800,1000\\}$ . The comparison results shown in Figure 14 indicate that our method can achieve consistently good recovery performance under different numbers of individuals. This further confirms that the identifiability of our framework is guaranteed by the mathematical relationship between the trajectory length and the number of groups, which is constrained by the sample sufficiency assumption under the conditions given in Theorem 4.1. Moreover, Figures 14(b) and 14(c) show the successful recovery of the latent group factor, validated by high-frequency similarity and a remarkable PCC value, confirming the ability of our method to skillfully recover latent variables in practical pendulum tasks. ", "page_idx": 27}, {"type": "text", "text": "Ablation Study: Variability in initial states We conduct additional experiments to verify how the variability in initial states across individuals affects performance. The initial state distributions are defined with two types: normal and uniform. For the normal distribution, the means are set to [0, 1, 1] and the standard deviations are set to [1, 2, 1], respectively. For the uniform distribution, the range for each dimension is defined with lower bounds [0, -1, 1] and upper bounds [1, 1, 1.5]. The experiment results in Figure 15 show that although the initial states have high variability, the estimated values of the latent factors corresponding to the 200 individuals are ultimately highly classified and can be divided into 4 groups. ", "page_idx": 27}, {"type": "text", "text": "Ablation Study: Consideration of transformer as encoder Our framework is flexible enough to integrate various encoders and decoders, depending on the application tasks. To demonstrate this flexibility, we incorporated Transformers into our framework and conducted a comparative analysis against the existing models. The result, shown in Figure 15(c), indicates that while both frameworks achieve identifiability, the Transformer-based encoder demonstrates faster convergence compared to our previous approach. ", "page_idx": 27}, {"type": "text", "text": "Added Experiment: Inventory Inventory management [71] is an important real-world problem that aims to keep inventories of goods at optimal levels to minimize inventory costs while maximizing revenue from demand fulfillment. We tested the performance of our algorithm on the inventory with state dimensions of 50, 100, and 200 and added additional baselines (8) Meta gradient RL, (9) Multitask RL, (10) Policy distillation, and (11) Non-policy adaptation to verify the model. The experimental results in Figure 12 show that our framework outperforms other algorithms in terms of initial reward and final reward. ", "page_idx": 27}, {"type": "text", "text": "Added Experiment: AhnChemo AhnChemoEnv [61] is designed to simulate cancer treatment through chemotherapy, allowing realistic modeling of tumor growth and response to treatment. We create different groups with PK/PD variation. The experimental results in Figure 13 show that our framework outperforms other algorithms in terms of initial and final reward. Our method achieves the highest initial and final rewards compared to the baselines. Specifically, it shows a significant jump-start compared to non-policy adaptation, validating the effectiveness of our adaptation approach. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "The meta-gradient method optimizes the hyperparameters of the learning algorithm by calculating the gradient of the learning process, allowing rapid adaptation to new tasks as they change. However, due to the continuous adjustment of learning strategies during training, it converges more slowly and the adaptation effect is less significant compared to our algorithm. Multitask RL improves learning efficiency by sharing model strategies across different tasks. This requires first training policies on multiple tasks, which can be time-consuming (and even risky) during exploration. Moreover, identifying which new task corresponds to a previously trained task can be challenging. Our algorithm addresses this by estimating directly without requiring prior knowledge. Policy distillation transfers the knowledge of already trained teacher models to a student model, allowing the student to perform well across multiple tasks. However, this approach highly relies on the performance of the teacher models; insufficiently trained teacher models can negatively impact the final performance. Our algorithm does not depend on the source policy performance; subsequent policy optimization is based on the new environment, leading to better final performance. ", "page_idx": 28}, {"type": "text", "text": "F.4 Training Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The estimation framework is trained using AdamW optimizer for a maximum of 200 epochs and early stops if the validation ELBO loss does not decrease for ten epochs. A learning rate of 0.001 and a mini-batch size of 32 are used. We used three random seeds in each experiment and reported the mean performance with standard deviation averaged across random seeds. We used a machine with the following CPU specifications: 11th Gen Intel(R) Core(TM) i7-11800H $\\textcircled{a}2.30\\mathrm{GHz}$ with 16 logical processors. The machine has one GeForce RTX 3080 GPU with 32GB GPU memory. ", "page_idx": 28}, {"type": "image", "img_path": "kREpCQtHdN/tmp/f2b973d5d8a4fb91dec2986f08e77bf03350a0fc1633f21e5234dbb23a9381ea.jpg", "img_caption": ["(a) Reward curve with $d_{s}=50$ . (b) Reward curve with $d_{s}=100$ . (c) Reward curve with $d_{s}=200$ . ", "Figure 12: Results in Inventory. We evaluated the performance of different methods under different state dimensions. Our algorithm scales well to high-dimensional cases and outperforms other baselines in terms of initial reward and final reward. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "G Impact Statement ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Our work has a significant impact on ethics, society, and future applications. We emphasize the importance of individualized policies in systems and advocate for a deeper understanding and respect for individual differences. Tailoring interventions to different individuals has the potential to improve user experience and outcomes in healthcare, education, and other areas. This approach avoids a one-size-fits-all policies. Our method can greatly improve individualized services, transforming the delivery of educational content, the management of healthcare, and the recommendation of products. This makes these services more effective and aligned with individual needs. However, the implementation of this method requires careful consideration of privacy and data security, as personalized systems require the collection and analysis of personal data. Maintaining user trust, preventing misuse, and ensuring ethical use of such data are of utmost importance. ", "page_idx": 28}, {"type": "image", "img_path": "kREpCQtHdN/tmp/5bea295a9a6aae77741b5cc313e112ea57451e15b1e796cf0583dd46daae6140.jpg", "img_caption": ["(a) Accumulative reward curves. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "kREpCQtHdN/tmp/b7108ba8f33172b693fd4da1e55343db29ddfee8719b526208dcff6df3a9be0b.jpg", "img_caption": ["(b) Initial and final reward under different benchmarks. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 13: Results in AhnChemoEnv. We evaluated the performance of our method against several baselines, including (1) meta gradient RL, (2) multitask RL, (3) policy distillation, and (4) non-policy adaptation. Our method outperforms these benchmarks and achieves superior performance in terms of initial reward and final reward. ", "page_idx": 29}, {"type": "image", "img_path": "kREpCQtHdN/tmp/1e7a1009f12ed597ce20bbecee60a5e7d2fc3b0a44b7456814fd7a464a3bc80d.jpg", "img_caption": ["Figure 14: (a) PCC trajectory comparisons under different numbers of individuals. (b-c) Successful recovery of the latent group factor in the Pendulum. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "H Estimation Framework Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The proposed framework is customized based on the requirements of the identifiability theorems given in Section 4. We would like to emphasize that our proposed framework differs from the traditional VAE and model-based RL in three main aspects: (1) Our framework uses a quantization layer to discretize the continuous latent representations. This mapping of continuous latent representations to an embedding dictionary is well suited to the group determinacy requirement. (2) Our decoder reconstructs individualized state transition processes to simulate the data generation process, incorporating additional conditions as well as the estimated latent factor. (3) We further extract latent factors for each individual as additional information to facilitate individual policy learning. The detailed implementations of each component are summarized below. ", "page_idx": 29}, {"type": "text", "text": "Encoder For any individual $m$ , the Conv1D layer transforms an input sequence $\\mathbf{s}_{t}^{m}$ , using learned kernel fliters. These fliters slide over the sequence to produce a feature map, denoting the response of the fliter at each position. Mathematically, the transformation by a single filter in the Conv1D layer at time $t$ is described as $\\mathbf{o}_{t}=\\sigma\\left(W*\\mathbf{s}_{t:H+t}^{m}+b\\right)$ , where $\\mathbf{o}_{t}$ is the feature map, $W$ the kernel to be learned during training, $^*$ the convolution operation, $\\mathbf{s}_{t:H+t}^{m}$ the input sub-sequence from time $t$ to $t+H$ , where $H$ is the size of the kernel. $\\sigma$ is the activation function, and $b$ is the bias term to be learned during training. The layer may contain multiple such fliters, each learning different features of the input sequence. The resulting feature maps serve as a transformed representation $z_{m}$ , which embeds the information about the latent group factor $\\kappa$ . ", "page_idx": 29}, {"type": "image", "img_path": "kREpCQtHdN/tmp/6d4d5171d5010db0f74083087942cee9e4e9ae916ea1219af24b2dcb39db3cc1.jpg", "img_caption": ["(a) Initial states with high vari- (b) Estimated latent factor. (c) PCC trajectories comparison. ability. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 15: (a-b) Evaluation on variability in initial states. The estimated values of $\\kappa$ are highly clustered into four classes. (c) Incorporating Transformers. The transformer encoder achieves faster convergence compared to the original framework. ", "page_idx": 30}, {"type": "text", "text": "As for the LSTM, let the hidden states and cell states of the LSTM at time $t$ denote as $h_{t}$ and $c_{t}$ , respectively. Then, the LSTM updates are given by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{t}=\\sigma(W_{f}\\cdot[\\mathbf{s}_{t}^{m},h_{t-1}]+b_{f}),}\\\\ &{i_{t}=\\sigma(W_{i}\\cdot[\\mathbf{s}_{t}^{m},h_{t-1}]+b_{i}),}\\\\ &{\\tilde{c}_{t}=\\operatorname{tanh}(W_{c}\\cdot[\\mathbf{s}_{t}^{m},h_{t-1}]+b_{c}),}\\\\ &{c_{t}=f_{t}\\odot c_{t-1}+i_{t}\\odot\\tilde{c}_{t},}\\\\ &{o_{t}=\\sigma(W_{o}\\cdot[\\mathbf{s}_{t}^{m},h_{t-1}]+b_{o}),}\\\\ &{h_{t}=o_{t}\\odot\\operatorname{tanh}(c_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\sigma$ is the sigmoid activation function, $\\odot$ element-wise multiplication. $W_{f},W_{i},W_{c},W_{o}$ and $b_{f},b_{i},b_{c},b_{o}$ are the weight matrices and bias terms to be learned during training. $f_{t},i_{t},\\tilde{c}_{t},c_{t},o_{t}$ and $h_{t}$ are the forget gate, input gate, candidate cell state, cell state, output gate, and hidden state at time $t$ , respectively. The final hidden state of LSTM $h_{T}$ , after the sequential processing of the entire trajectory, serves as the representative $z_{m}$ that embeds the information about the latent group factor $\\kappa$ . ", "page_idx": 30}, {"type": "text", "text": "Quantization Layer Let the output of the encoder be a continuous latent representation denoted as $z_{m}\\,\\in\\,\\mathbb{R}$ , and define an embedding dictionary $E$ consisting of $G$ vectors, where each vector represents a unique discrete category: $E=\\{e_{1},e_{2},\\ldots,e_{G}\\}$ , where $e_{i}\\in\\mathbb{R}$ . The quantized vector $\\hat{\\kappa}_{m}$ is obtained by mapping $z_{m}$ to the nearest dictionary vector. The mapping can be expressed mathematically as $\\begin{array}{r}{\\hat{\\boldsymbol{\\kappa}}_{m}=\\arg\\operatorname*{min}_{\\boldsymbol{e}_{i}\\in E}\\left|\\left|z_{m}-\\boldsymbol{e}_{i}\\right|\\right|_{2}}\\end{array}$ . Subsequently, the quantized output is the vector from the dictionary that is closest to the encoder output. Thus, the continuous representation $z_{m}$ is effectively mapped to a discrete $\\hat{\\kappa}_{m}$ by finding the nearest neighbor in the dictionary, aligning the representation learning with the discrete nature of the latent variable. ", "page_idx": 30}, {"type": "text", "text": "Decoder Suppose $\\mathbf{s}_{t-1}^{m}$ and $\\mathbf{a}_{t-1}^{m}$ as the true previous state and action, respectively. Let $\\hat{\\kappa}_{m}$ be the approximated latent group factor for the $m$ -th individual. The inputs to the conditional decoder are a combination of the aforementioned variables: $\\mathrm{Input}_{t}=(\\mathbf{s}_{t-1},\\mathbf{a}_{t-1},\\hat{\\kappa}_{m})$ . The output of the decoder is the reconstructed next state, $\\hat{\\mathbf{s}}_{t}$ , which is a function of the decoder input: $\\hat{\\mathbf{s}}_{t}=\\mathrm{De}(\\mathrm{Input}_{t})$ . The reconstruction likelihood measures how closely the reconstructed state matches the true subsequent state, which is defined as $\\mathcal{L}_{\\mathrm{Recon}}=p_{\\mathrm{Recon}}(\\mathbf{s}_{t}^{\\dot{m}}|\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m})$ . The objective in this process is to optimize the decoder parameters to maximize the reconstruction likelihood max ${\\mathcal{L}}_{\\mathrm{Recon}}$ so that the reconstructed state $\\hat{\\mathbf{s}}_{t}$ is as close as possible to the true next state $\\mathbf{s}_{t}$ . ", "page_idx": 30}, {"type": "text", "text": "I Algorithm ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The pseudocode for the proposed algorithm is presented in Algorithm 1 and Algorithm 2. ", "page_idx": 30}, {"type": "text", "text": "1: Input: $\\{f_{\\mathrm{Env}}^{m}\\}_{m=1}^{M}$ : individualized environments; Encoder: encoder; Quantization: embed  \nding dictionary; Decoder: decoder; : policy network   \n2: Output: $\\{\\hat{\\kappa}_{g}\\}_{g=1}^{G}$ : estimated group factor; $\\bar{\\{\\pi_{m}^{*}\\}}_{m=1}^{M}$ : optimized individualized policy   \n4: ## Main loop   \n5: Main( $f_{\\mathrm{Env}}$ , Encoder, Quantization, Decoder, $\\pi$ )   \n7: 6: Encoder, Quantization, Decoder, $\\mathcal{H}\\gets\\{\\tau_{m}\\}_{m=1}^{M}$ # lect individual trajectories by interaction with $\\pi\\sim\\mathrm{N}(0,\\mathrm{I})$ # Randomly initialize the network $\\{f_{\\mathrm{Env}}^{m}\\}_{m=1}^{M}$   \n8: for each individual $m$ do   \n9: $z_{m}=\\mathrm{Encoder}(\\mathbf{s}_{0:T}^{m})$ # Capture the high-level representations   \n10: $\\hat{\\kappa}_{m}=\\mathrm{Quantization}(z_{m})$ # Vector quantization   \n11: for each state $\\mathbf{s}_{t}^{m}$ in the trajectory do   \n12: $\\hat{\\mathbf{s}}_{t}^{m}=\\operatorname{Decoder}(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m})$ # Reconstruct the next state   \n13: end for   \n14: end for   \n15: return $\\{\\pi_{g}^{*}\\}_{g=1}^{G}=\\mathrm{PolicyLearning}(\\mathcal{H},\\{\\hat{\\kappa}_{g}\\}_{g=1}^{G})\\,\\#$ Optimize the individualized policies   \n16:   \n17: EncoderFunction $(\\mathbf{s}_{0:T}^{m})$   \n18: if dataset is synthetic then   \n19: for each $t$ in $\\mathbf{s}_{0:T}^{m}$ do   \n20: $o_{t}^{m}\\gets\\mathbf{\\mathrm{Conv1D}}(\\mathbf{s}_{t:t+H}^{m})$   \n21: end for   \n22: else if dataset is corpus then   \n23: Initialize $h_{0}^{m},c_{0}^{m}$   \n24: for each $t$ in $\\mathbf{s}_{0:T}^{m}$ do   \n25: $h_{t}^{m},c_{t}^{m}\\gets\\bar{\\mathrm{LSTM}}(h_{t-1}^{m},c_{t-1}^{m},\\mathbf{s}_{t}^{m};\\theta)$   \n26: end for   \n27: end if   \n28: return $z_{m}\\leftarrow$ Final output of Conv1D or final hidden state of LSTM   \n29:   \n30: QuantizationFunction $(z_{m})$   \n31: Initialize $E=\\{e_{1},e_{2},\\ldots,e_{G}\\}$ , $d_{\\operatorname*{min}}=\\infty$   \n32: for each $e_{i}$ in $E$ do   \n33: if $\\|z_{m}-e_{i}\\|_{2}<d_{\\operatorname*{min}}$ then   \n34: Update $d_{\\mathrm{min}}$ and $\\hat{\\kappa}_{m}\\gets e_{i}$   \n35: end if   \n36: end for   \n37: return $\\hat{\\kappa}_{m}$   \n38:   \n39: DecoderFunction $(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m})$   \n40: Combine inputs to reconstruct $\\hat{\\mathbf{s}}_{t}^{m}\\gets\\mathrm{Decoder}(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m})$   \n41: return Reconstructed state $\\hat{\\bf s}_{t}^{m}$   \n42:   \n43: PolicyLearningFunction $(\\mathcal{H},\\{\\hat{\\kappa}_{g}\\}_{g=1}^{G})$   \n44: for each individual $m$ do   \n45: Update policy input to $\\mu_{\\pi}\\bigl(\\mathbf{s}_{t};\\theta^{\\mu}\\bigr)\\to\\mu_{\\pi}^{m}\\bigl(\\mathbf{s}_{t}^{m},\\hat{\\kappa}^{m};\\theta^{\\mu}\\bigr)$   \n4467:: U $\\begin{array}{r}{\\mathcal{I}(\\theta^{\\mu})=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}Q\\left(\\mathbf{s}_{t},\\mu_{\\pi}^{m}(\\mathbf{s}_{t}^{m},\\hat{\\kappa}_{m};\\theta^{\\mu});\\theta^{Q}\\right)\\right]}\\end{array}$   \n48: Optimize $\\mu_{\\pi}^{m}$ for individual $m$   \n49: end for   \n50: return Optimized individual policy $\\mu_{\\pi}^{*}$ ", "page_idx": 31}, {"type": "text", "text": "Algorithm 2 Training Process with Extended ELBO Objective. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1: Initialize parameters of the encoder Encoder and decoder Decoder   \n2: Initialize weights $\\alpha$ and $\\beta$   \n3: repeat   \n4: for each individual $m$ do   \n5: Compute encoded representation: $z_{m}\\leftarrow\\mathrm{Encoder}(\\mathbf{s}_{0:T}^{m})$   \n6: Estimate individual-specific factor: $\\hat{\\kappa}_{m}\\gets\\mathrm{Quantization}(z_{m})$   \n7: Compute reconstructed state: $\\hat{\\mathbf{s}}_{t}^{m}\\gets\\mathrm{Decoder}(\\mathbf{s}_{t-1}^{m},\\mathbf{a}_{t-1}^{m},\\hat{\\kappa}_{m})$   \n8: Calculate $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Recon}}=\\sum_{t}\\|\\mathbf{s}_{t}^{m}-\\hat{\\mathbf{s}}_{t}^{m}\\|^{2}}\\end{array}$   \n9: Calculate $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Quant}}=\\sum_{i}\\|\\mathrm{sg}[z_{m,i}]-e_{m,i}\\|^{2}}\\end{array}$ , $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Commit}}=\\sum_{i}\\|e_{m,i}-\\mathbf{sg}[z_{m,i}]\\|^{2}}\\end{array}$   \n10: Compute extended  ELBO objective: $\\mathcal{L}_{\\mathrm{ELBO}}=\\mathcal{L}_{\\mathrm{Recon}}+\\alpha\\mathcal{L}_{\\mathrm{Quant}}+\\beta\\mathcal{L}_{\\mathrm{Commit}}$   \n11: Update parameters to minimize $\\mathcal{L}_{\\mathrm{ELBO}}$   \n12: end for   \n13: until convergence ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We list the contributions and scope in both abstract and introduction. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: See conclusion. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Section 4 and Appendix B and C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide details in both the main content and appendix, with code link Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The access to the data is referred and code is linked. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Details are in appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Error bars are provided in the figures. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Provided in the appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Checked. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See appendix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 36}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Not related to our work. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All are referred. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Not related to our work. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Not related to our work. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: Not related to our work. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]