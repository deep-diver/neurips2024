[{"type": "text", "text": "Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ipsita Ghosh Department of Computer Science University of North Carolina at Charlotte ighosh2@charlotte.edu ", "page_idx": 0}, {"type": "text", "text": "Abiy Tasissa   \nDepartment of Mathematics Tufts University   \nAbiy.Tasissa@tufts.edu ", "page_idx": 0}, {"type": "text", "text": "Christian K\u00fcmmerle Department of Computer Science University of North Carolina at Charlotte kuemmerle@charlotte.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of finding suitable point embedding or geometric configurations given only Euclidean distance information of point pairs arises both as a core task and as a sub-problem in a variety of machine learning applications. In this paper, we aim to solve this problem given a minimal number of distance samples. To this end, we leverage continuous and non-convex rank minimization formulations of the problem and establish a local convergence guarantee for a variant of iteratively reweighted least squares (IRLS), which applies if a minimal random set of observed distances is provided. As a technical tool, we establish a restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank- $^r$ matrices given random Euclidean distance measurements, which might be of independent interest for the analysis of other non-convex approaches. Furthermore, we assess data efficiency, scalability and generalizability of different reconstruction algorithms through numerical experiments with simulated data as well as real-world data, demonstrating the proposed algorithm\u2019s ability to identify the underlying geometry from fewer distance samples compared to the state-of-the-art. ", "page_idx": 0}, {"type": "text", "text": "The Matlab code can be found at github_EDG-IRLS ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Euclidean Distance Geometry (EDG) problems have applications spanning diverse domains, from comprehending protein structures through molecular conformations [MMLL19, MMWL22], prediction of molecular conformations in computational chemistry [DPRV15, SWBB97] and aiding dimensional reduction in machine learning [TSL00] to facilitating localization in sensor networks [FQX19], coupled with its role in solving partial differential equations on manifolds [LL17]. This emphasizes its broad impact in computational sciences. [LLMM14] proposes embedding entities based on Distance Geometry Problems (DGP), where object positions are determined based on a subset of pairwise distances or inner products which significantly reduces computational complexity compared to traditional word embedding methods. ", "page_idx": 0}, {"type": "text", "text": "Problem 1. Mathematically, consider a collection of $n$ points $\\mathbf{p}_{i}$ in an $r$ -dimensional Euclidean space with coordinates $\\mathbf{P}=[\\mathbf{p}_{1},\\mathbf{\\dot{p}}_{2},...,\\mathbf{p}_{n}]\\in\\mathbb{R}^{r\\times n}$ whose pairwise squared Euclidean distances are given by $d_{i j}^{2}=||{\\bf p}_{i}-{\\bf p}_{j}||^{2}$ for each $1\\leq i\\neq j\\leq n$ where $||\\cdot||$ denotes the Euclidean norm. Given only partial information of the $\\{d_{i j}\\}$ such that only a subset of cardinality $m<n(n-1)/2$ is known, the goal is to reconstruct the geometry of the points, that is to recover the point coordinates $\\mathbf{P}$ . ", "page_idx": 0}, {"type": "text", "text": "If all the distances between the points are provided, the problem is known as multidimensional scaling [LT24, GCKG23], and closed solution formula exists. However, this is not the case for the incomplete setup which is the focus of this work, and in which only partial information of the pairwise distances is available. ", "page_idx": 1}, {"type": "text", "text": "For instance, AlphaFold $[\\mathrm{SEJ^{+}20}$ , $\\mathbf{J}\\mathbf{E}\\mathbf{P}^{+}21\\,\\mathrm{~.~}$ ] has shown effectiveness in predicting the threedimensional structure of protein given as input its amino acid sequence. AlphaFold2, which uses an attention mechanism-based transformer architecture $[\\mathrm{VSP^{+}17}]$ , is trained on known sequences and structures from the Protein Data Bank $[\\mathrm{HMBFGG^{+}00}]$ and determines the distances between the $C_{\\alpha}$ atoms of all residue pairs in a protein. Subsequently, as a subproblem, this distance information is used to predict the protein\u2019s structure by identifying the foldings within the protein molecule. In the context of this subproblem, where the input is the predicted pairwise distances, a linear mapping can be used to predict geometric coordinates. However, many of the predicted distances might not be accurate, which is why low confidence regions of the resulting structures, as indicated by the predicted local-distance difference test (pLDDT) $[\\mathbf{J}\\mathbf{E}\\mathbf{P}^{+}21]$ , could be masked, and a new structure could subsequently be re-computed based on the distance entries of the medium-to-high confidence regions using high-accuracy solution algorithms for Problem 1. ", "page_idx": 1}, {"type": "text", "text": "In the context of the Problem 1, we have access to a subset of entries of this $\\mathbf{D}=[d_{i j}^{2}]$ matrix, defined by the index set $\\Omega\\subset\\{1,\\ldots,n\\}^{2}$ . Now, we formulate the Gram matrix $\\mathbf{X}^{0}=\\mathbf{P}^{\\top}\\mathbf{P}$ , residing in the set of real-valued symmetric matrices $S_{n}$ . This matrix has a lower rank $r$ compared to the symmetric distance matrix $\\mathbf{D}\\,\\,\\in\\,S_{n}=\\{\\mathbf{X}\\in\\mathbb{R}^{n\\times n}:\\mathbf{X}=\\mathbf{X}^{\\top}\\}$ , which has a rank of $r+2$ [DPRV15]. ", "page_idx": 1}, {"type": "text", "text": "To make the solution translation invariant, the centroid of the points must be the origin, i.e., $\\textstyle\\sum_{\\ell=1}^{n}p_{\\ell}\\,=\\,0$ . To ensure this, the Gram matrix should satisfy also satisfy $\\mathbf{X^{0}\\cdot1}\\,=\\,0$ , where $\\mathbf{1}\\in\\mathbb{R}^{n}$ is the vector of all ones. Based on these observations, we can frame Problem 1 as an instance of the computationally challenging NP-hard rank minimization [Rec11] problem defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in S_{n},\\mathbf{X}\\cdot\\mathbf{1}=0,\\mathbf{X}\\succeq\\mathbf{0}}\\ \\mathrm{rank}(\\mathbf{X})\\quad\\mathrm{~subject~to~}\\mathbf{X}_{i,i}+\\mathbf{X}_{j,j}-2\\mathbf{X}_{i,j}=\\mathbf{D}_{i,j}\\quad\\mathrm{~for~all~}(i,j)\\in\\Omega,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "incorporating also the positive semi-definiteness constraint $\\mathbf{X}\\succeq\\mathbf{0}$ , which comes from the observation that $\\dot{\\mathbf{X}}^{0}=\\check{\\mathbf{P}}^{\\intercal}\\mathbf{P}$ is also PSD. ", "page_idx": 1}, {"type": "text", "text": "From an optimization perspective, this rank-minimization problem is highly complex due to its nonconvexity and non-smoothness. Being an NP-hard problem [TL18, Hen95, MW97], it is extremely difficult to solve directly. Consequently, a significant amount of existing research [Rec11, DR16, CR09, CW18, Gro11, TL18] has focused on minimizing the convex envelope of the rank function, known as the nuclear norm, instead. However, it was observed that for related low-rank matrix completion problem, such convex relaxations are not as data-efficient as other formulations [ALMT14, TW13], while posing also computational limitations [CC14]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study the question of, given incomplete distance information Problem 1, how many samples are necessary to ensure accurate recovery? [TL18] established convergence with $O(\\nu r n\\log^{2}n)$ uniformly random samples for solving (1) using a nuclear norm surrogate. While there are numerous results for non-convex methods in other low-rank recovery problems [KV21, CLC19, Van13], there are currently only few non-convex algorithm specifically designed for EDG problems available [TL18, SLCT23, ZCZ22, NKKS19, LS24]. To the best of our knowledge, only [NKKS19, LS24] provide convergence guarantees in the non-convex setting. Unlike generic lowrank matrix recovery problems [RFP10], EDG problems share the complexities of low-rank matrix completion problems, such as the absence of a direct uniform null space property [RXH11] or a restricted isometry property [RFP10]. Additionally, the underlying measurement basis in EDG problems is not orthonormal, making it impossible to directly use the analysis of standard matrix completion to provide guarantees for techniques using the Gram matrix-based low-rank modelling (1) of Problem 1. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we propose and analyze an algorithm for the EDG problem based on the iteratively reweighted least squares framework (MatrixIRLS, see Section 3), for which we show that $m=$ $\\Omega(\\nu r\\bar{n}\\log(n))$ (where $\\nu$ is the coherence factor) randomly sampled distances are sufficient to guarantee local convergence to the ground truth, ${\\bf X}^{0}$ with a quadratic rate as shown in Theorem 4.3, from which the geometry of the points $\\mathbf{P}$ is trivially recovered. The sample complexity assumption of Theorem 4.3 matches the lower bound of low-rank matrix completion problems as established in [CT10]. In Section 4, we construct a dual basis (Lemma 4.4) that spans $S_{n}$ which enables us to show the restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank- $^r$ in Theorem 4.5 for our approach, which can be of independent interest for the analysis of other nonconvex algorithms. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "While the convergence statement of Theorem 4.3 only applies in a local neighborhood of a ground truth, the indicated data-efficiency of the proposed method is numerically validated through different experiments in Section 5 on synthetic and real data in comparison to the state-of-the-art methods. Furthermore, we demonstrate that MatrixIRLS method is robust to ill-conditioned data, further highlighting its flexibility. In the Supplementary material, we discuss the limitations in Appendix B. Additionally, we provide proofs related to the the theoretical results of Section 4 in Appendices B and C. We further discuss the numerical considerations of our experiments in Appendix E. We discuss about the computational complexity of the algorithm in detail in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Early research on EDG problems focused on establishing its mathematical properties like defining the conditions under which a distance matrix can be represented in Euclidean space[Gow85, YH38, YH38, Alf05].A comprehensive overview of EDG applications, including molecular conformations, wireless sensor networks, robotics, and manifold learning, is available in [Lib20]. Motivated by the molecular conformation problem, [BJ95] relates this Euclidean distance matrix completion to a graph realization problem by showing that such matrices can be completed if the graph is chordal. Other than graph theoretic approaches, as a technical tool, various optimization strategies [Hen95, MW97, Tro00] have been deployed to solve EDG problems. [AKW99] proposes a primaldual interior point algorithm that solves an equivalent semi-definite programming problem. However, none these works provide theoretical reconstruction guarantees in the incomplete setup of Problem 1. ", "page_idx": 2}, {"type": "text", "text": "While providing an accurate modelling of Problem 1, the rank minimization formulation (1) poses challenges due to its non-convex and non-smooth nature. There is a mature existing literature [DR16, CR09, CW18, Gro11] around replacing the rank function, $\\operatorname{rank}(\\mathbf{X})$ , with the sums of its singular values $\\sigma_{i}(\\mathbf{X})$ (also known as the nuclear norm). Building on the rank minimization formulation (1) of the EDG problem, [TL18] minimizes the convex nuclear norm surrogate of the inferred Gram matrix. They propose a dual basis approach that enables a theoretical guarantee for this type of nuclear norm minimization formulation of the EDG problem. ", "page_idx": 2}, {"type": "text", "text": "From a practical point of view, it is well-known that the convex approach is computationally intensive, as the arithmetic complexity is cubic in $n$ , the dimension of $\\mathbf{X}^{0}$ [CC14]. It also tends to demand more data samples than non-convex alternatives, making it less efficient in terms of data [ALMT14, TW13]. ", "page_idx": 2}, {"type": "text", "text": "To mitigate these issues, recent studies have shifted focus to non-convex methods such as matrix factorization [SL16, MWCC20, ZCZ22]. These methods optimize a function involving a data-fit objective regularized by squared Frobenius norms of the factor matrices, which are computationally more feasible and data-efficient. Few \u201cnon-convex\u201d algorithms are based on matrix factorization like the work in [BM03]. ", "page_idx": 2}, {"type": "text", "text": "Based on a similar formulation, ScaledSGD [ZCZ22] is a preconditioned stochastic gradient descent method aimed at robustness for ill-conditioned problems. Additionally, some of the most effective techniques for low-rank matrix completion involve minimizing smooth objectives on the Riemannian manifold of fixed-rank matrices, providing scalability and the potential to reconstruct the matrix with fewer samples, although they lack strong performance guarantees [Van13, WCCL20, BA15, BNZ21]. ReiEDG [SLCT23] is a Riemannian-based gradient descent strategy utilizing the sampling operator on $\\Omega$ , which is non-convex approach to solving the EDG problem. However, it does not provide convergence guarantees. To the best of our knowledge, [NKKS19, LS24] are the non-convex approaches for solving the EDG problem in the Gram-matrix-based low-rank modeling (1) of Problem 1. The work in [NKKS19] proposes an algorithm based on Riemannian optimization over a manifold and provides convergence guarantees. These guarantees are derived from extended Wolfe conditions. However, it is not explicitly detailed how these convergence guarantees depend on problem parameters such as the sampling model and the number of samples (see Remark III.8 in [NKKS19]). Similarly, the study in [LS24] employs a Riemannian framework and provides local convergence guarantees under Bernoulli sampling. Nonetheless, [LS24] does not clarify whether the proposed algorithm achieves local linear convergence. In contrast to these studies, the algorithm proposed in this paper achieves local quadratic convergence under uniform sampling of the distances. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To handle the non-smoothness and non-convexity of rank minimization problems of the type (1), iteratively reweighted least squares (IRLS) algorithms take a different route than methods based on [BM03] or Riemannian methods by minimizing a sequence of quadratic majorizing functions of smoothed rank surrogates [DDFG10, FRW11, MF10, KS18, KV21]. IRLS algorithms have been studied extensively over the years, as indicated by [Law61, GR97, Bur12]. In the context of lowrank matrix completion, IRLS algorithms are known to be among the most data-efficient methods available, while being amenable to a rigorous convergence analysis [FRW11, MF10, KS18, KV21]. Most recently, [KV21] provided an improvement on previous instantiations of the IRLS framework [FRW11, MF10, KS18] for low-rank optimization problems by providing an improved reweighting strategy, for which the authors show a local convergence guarantee that is applicable for low-rank matrix completion, given random entrywise samples of minimal sample complexity. The algorithm we propose in Section 3 is similar to [KV21, Algorithm 1] and Theorem 4.3 follows partially the proof strategy of a related result in [KV21]. However, the setup of Problem 1 does not allow a direct adaptation of both the implementation and analysis of [KV21] due to the non-orthogonality of the measurement basis. ", "page_idx": 3}, {"type": "text", "text": "3 MatrixIRLS for Euclidean Distance Geometry ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a detailed outline and description of the iteratively reweighted least squares method in the context of the EDG reconstruction problem. To this end, we define preliminaries for stating the algorithm. MatrixIRLS, defined in Algorithm 1 below, can be interpreted as a hybrid of a smoothing method [Che12] and a Majorization-Minimization algorithm [SBP17]. In particular, the proposed algorithm minimizes smoothed log-det objectives defined as $\\begin{array}{r}{F_{\\epsilon}(\\mathbf{X}):=\\sum_{i=1}^{n}f_{\\epsilon}(\\sigma_{i}(\\mathbf{X}))}\\end{array}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\epsilon}(\\sigma)={\\left\\{\\log|\\sigma|,\\right.}\\qquad\\qquad\\qquad\\qquad\\mathrm{if}\\ \\sigma\\geq\\epsilon,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a continuously differentiable function with $\\epsilon^{-2}$ -Lipschitz gradient [KV21]. ", "page_idx": 3}, {"type": "text", "text": "We can decompose $\\mathbf{X}$ by $\\mathbf{X}=\\mathbf{U}\\,\\mathrm{dg}(\\gamma\\sigma)\\mathbf{U}^{\\top}$ , where $\\gamma\\in\\{+1,-1\\}$ , It is clear that the optimization landscape of $F_{\\epsilon}$ crucially depends on the smoothing parameter $\\epsilon$ . Instead of minimizing $F_{\\epsilon}$ directly, our method minimizes, for $k\\in\\mathbb{N}$ , $\\epsilon_{k}>0$ and $\\mathbf{X}^{(k)}$ a quadratic model that is related to the secondorder Taylor expansion of the function $f_{\\epsilon}$ at the current iterate and its information is encoded in a weight operator [KV21] defined below in Definition 3.1. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 ([KV21]). $\\textbf{X}\\in\\ S_{n}$ be a matrix with singular value decomposition $\\begin{array}{r l}{\\mathbf{X}^{(k)}}&{{}=}\\end{array}$ $\\mathbf{U}\\,\\mathrm{dg}(\\gamma\\sigma)\\mathbf{U}^{\\top}$ , where $\\gamma\\in\\{+1,-1\\}$ i.e., $\\mathbf{U}\\in S_{n}$ are orthonormal matrices. We define the weight operator core matrix $\\mathbf{H}_{\\mathbf{X},\\epsilon}\\in S_{n}$ of $\\mathbf{X}$ for smoothing parameter $\\varepsilon>0$ such that ", "page_idx": 3}, {"type": "text", "text": "$(\\mathbf{H}_{\\mathbf{X},\\varepsilon})_{i j}:=\\left(\\operatorname*{max}(\\sigma_{i},\\epsilon)\\operatorname*{max}(\\sigma_{j},\\epsilon)\\right)^{-1}$ for each $i,j\\in\\{1,\\ldots,n\\}$ and the weight operator $W_{\\mathbf{X},\\varepsilon}~;$ $S_{n}\\to S_{n}$ , which maps any $\\mathbf{Z}\\in S_{n}$ to ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{\\mathbf{X},\\varepsilon}(\\mathbf{Z}):=\\mathbf{U}\\left[\\mathbf{H}_{\\mathbf{X},\\varepsilon}\\circ(\\mathbf{U}^{\\top}\\mathbf{Z}\\mathbf{U})\\right]\\mathbf{U}^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\circ$ denotes the entrywise ore Hadamard product of two matrices. ", "page_idx": 3}, {"type": "text", "text": "Our method is designed to provide iterates that satisfy the constraints of the formulation (1) at each iteration. They can be encoded using the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. Given $n\\,\\in\\,\\mathbb{N}$ , let $\\mathbb{I}=\\{\\alpha=(i,j)\\mid1\\leq i<j\\leq n\\}$ be the index set of upper triangular indices. ", "page_idx": 3}, {"type": "text", "text": "We define the operator basis $\\left\\{\\mathbf{w}_{\\alpha}\\right\\}_{\\alpha\\in\\mathbb{I}\\cup\\{(i,i):i\\in\\{1,\\dots,n\\}\\}}$ where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{w}_{\\alpha}=\\left\\{\\begin{array}{l l}{\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}+\\mathbf{e}_{j}\\mathbf{e}_{j}^{\\top}-\\mathbf{e}_{i}\\mathbf{e}_{j}^{\\top}-\\mathbf{e}_{j}\\mathbf{e}_{i}^{\\top},}&{\\mathrm{~if~}\\alpha=(i,j)\\in\\mathbb{I},}\\\\ {\\frac{1}{2}(\\mathbf{e}_{i}\\mathbf{1}^{\\top}+\\mathbf{1}\\mathbf{e}_{i}^{\\top}),}&{\\mathrm{~if~}\\alpha=(i,i)\\mathrm{~for~some~}i\\in\\{1,\\ldots,n\\},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{e}_{i}\\in\\mathbb{R}^{n}$ is the $i$ -th standard basis vector. ", "page_idx": 3}, {"type": "text", "text": "Given a set (or multiset) of indices $\\Omega=\\{\\alpha_{1},\\ldots,\\alpha_{m}\\}\\subset\\mathbb{I}$ of cardinality $m=|\\Omega|$ , we define the measurement operator $A=A_{\\Omega}:S_{n}\\rightarrow\\mathbb{R}^{m+n}$ which maps $\\mathbf{X}\\in S_{n}$ to ${\\mathcal{A}}(\\mathbf{X})$ whose $\\ell$ -th coordinate is defined as $\\mathcal{A}(\\mathbf{X})_{\\ell}=\\langle\\mathbf{w}_{\\alpha_{\\ell}},\\mathbf{X}\\rangle$ for $\\ell\\leq m$ and as $A(\\mathbf{X})_{\\ell}=\\langle\\mathbf{w}_{(\\ell-m,\\ell-m)},\\mathbf{X}\\rangle$ for $\\ell>m$ . ", "page_idx": 4}, {"type": "text", "text": "The basis $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha}$ of Definition 3.2 can be considered as an extended definition of the primal basis used in [TL18, LT24], but additionally is able to encode the constraint $\\mathbf{X}\\cdot\\mathbf{1}=\\mathbf{0}$ which guarantees that the Gram matrix corresponds to points whose centroid is located at the origin. Accordingly, we can define the constraint set corresponding to Gram matrices of points that are centered and simultaneously satisfy the pairwise distance constraints of Problem 1 as $\\{\\mathbf{X}\\,\\in\\,S_{n}\\,:\\,A(\\mathbf{X})\\,=\\,[\\mathbf{D}_{\\Omega};\\mathbf{0}]\\}$ . The algorithm below minimizes the quadratic, majorizing model of the objective $F_{\\epsilon_{k}}(\\cdot)$ given $k$ , while satisfying the measurement operator based on the sampled distances. Equivalently, we can define the main computational step of the method as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{X}^{(k+1)}=\\underset{\\mathrm{s.t.}\\;A\\left(\\mathbf{X}\\right)=\\left[\\mathbf{D}_{\\Omega};\\mathbf{0}\\right]}{\\arg\\operatorname*{min}}\\langle\\mathbf{X},W_{k}(\\mathbf{X})\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{k}\\,:\\,S_{n}\\,\\mapsto\\,S_{n}$ is defined as $W_{k}:=W_{\\mathbf{X}^{(k)},\\epsilon_{k}}$ with the definition of the weight operator Definition 3.1 above. With this preparation, we provide an outline of MatrixIRLS in Algorithm 1 below. Equation (7) provides suitable update rule for the smoothing parameter sequence $(\\epsilon_{k})_{k\\in\\mathbb{N}}$ that enables the computation of only $r=O(\\widetilde{r})$ singular triplets of each algorithmic iterate [KV21]. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 MatrixIRLS for Euclidean Distance Geometry Problems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Index pairs $\\Omega\\subset\\mathbf{1}$ , distances $\\mathbf{D}_{\\Omega}=(d_{i j})_{(i,j)\\in\\Omega}$ , rank estimate $\\widetilde r$ . Output: $\\mathbf{X}^{(k)}$ after   \nsuitable stopping condition.   \nInitialize $k=0$ , $\\epsilon_{0}=\\infty$ and $W_{0}=\\mathrm{Id}$ .   \nfor $k=1,2,\\dots$ , do Solve weighted least squares: Solve (5) by $\\mathbf{X}^{(k)}=W_{k-1}^{-1}\\mathcal{A}^{*}\\left(\\mathcal{A}W_{k-1}^{-1}\\mathcal{A}^{*}\\right)^{-1}\\mathbf{y}$ (6) Update smoothing: Compute $\\widetilde r+1$ -th singular value of $\\mathbf{X}^{(k)}$ to update $\\begin{array}{r}{\\epsilon_{k}=\\operatorname*{min}\\left(\\epsilon_{k-1},\\sigma_{\\widetilde{r}+1}(\\mathbf{X}^{(k)})\\right).}\\end{array}$ (7) Update weight operator: For $r_{k}:=|\\{i\\in[n]:\\sigma_{i}(\\mathbf{X}^{(k)})>\\epsilon_{k}\\}|$ , compute the first $r_{k}$ singular values $\\sigma_{i}^{(k)}:=\\sigma_{i}(\\mathbf{X}^{(k)})$ and matrices $\\mathbf{U}^{(k)}\\in\\mathbb{R}^{n\\times r_{k}}$ with leading $r_{k}$ left singular vectors of $\\mathbf{X}^{(k)}$ to update $W_{k}=W_{\\mathbf{X}^{(k)},\\epsilon_{k}}$ defined in Definition 3.1.   \nend for ", "page_idx": 4}, {"type": "text", "text": "3.1 Computational Considerations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The computational complexity of this above algorithm can be computed from the steps (6) and (7). In our numerical implementation, we largely follow the tangent space formulation of the weighted least squares step (6), cf. [KV21, Section 3], which involves the solution of an order $O(n r_{k})=O(n r)$ linear system if $\\widetilde r=r$ is chosen as the ground truth rank. An additional difficulty we overcame in the provided refere n ce implementation arises from the fact that $A.A^{*}$ is not the identity. The per-iteration time complexity of our method is dominated by $O((m r+r^{2}n)\\mathbf{N}_{i n n e r}^{0})$ , where $\\mathrm{N}_{i n n e r}^{\\tilde{0}}$ is the number of inner iteration bound of the iterative linear system solver. Detailed FLOPs calculation is shown in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss about the local convergence of MatrixIRLS in Section 4.1 and establish RIP restricted to the tangent space of the manifold of symmetric rank- $^r$ matrices at the ground truth Gram matrix $\\mathbf{X}^{0}$ in Section 4.2. ", "page_idx": 4}, {"type": "text", "text": "4.1 Local Convergence Analysis of Algorithm 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "It is well-known in the literature on low-rank matrix completion that for an entrywise measurement basis, recovery from generic measurements is more difficult if most information of the low-rank matrix is concentrated in few entries, and this observation is typically captured by the notion of incoherence [CR09, Rec11, Che15]. For the purpose of the EDG problem of interest, we use the following coherence notion, which has appeared in similar form in [TL18]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Coherence for Gram matrices in the EDG problem, [TL18]). Let $\\mathbf{X}\\in S_{n}$ be of rank $r$ . Let $T=T_{\\mathbf{X}}=\\{\\mathbf{Z}\\mathbf{X}+\\mathbf{X}\\mathbf{Z}^{\\top}:\\mathbf{Z}\\in\\mathbb{R}^{n\\times n}\\}$ be the tangent space onto the rank-r manifold $\\mathcal{M}_{r}=\\{\\mathbf{Z}\\in S_{n}:\\mathrm{rank}(\\mathbf{Z})=r\\}$ at $\\mathbf{X}$ . We say that $\\mathbf{X}$ has coherence $\\nu$ with respect to the basis $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}}$ of the subspace $\\{\\mathbf{X}\\in\\dot{S}_{n}:\\mathbf{X1}=\\mathbf{0}\\}$ if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\alpha\\in\\mathbb{I}}\\sum_{\\beta\\in\\mathbb{I}}\\langle\\mathcal{P}_{T}\\mathbf{w}_{\\alpha},\\mathbf{w}_{\\beta}\\rangle^{2}\\leq2\\nu\\frac{r}{n}\\quad\\mathrm{~and~}\\quad\\operatorname*{max}_{\\alpha\\in\\mathbb{I}}\\sum_{\\beta\\in\\mathbb{I}}\\langle\\mathcal{P}_{T}\\mathbf{v}_{\\alpha},\\mathbf{w}_{\\beta}\\rangle^{2}\\leq4\\nu\\frac{r}{n},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{P}_{T}:S_{n}\\rightarrow S_{n}$ denotes the projection operator onto $T$ and $\\{\\mathbf{v}\\}_{\\alpha\\in\\mathbf{I}}$ is a dual basis of $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}}$ , which means that $\\langle\\mathbf{w}_{\\alpha},\\mathbf{v}_{\\beta}\\rangle=\\delta_{\\alpha,\\beta}$ for each $\\alpha,\\beta\\in\\mathbb{I}$ ( $\\delta_{\\alpha,\\beta}=1$ for $\\alpha=\\beta$ and equal to 0 otherwise). Remark 4.2. In [TL18, Definition 1], the coherence constant $\\nu$ was required to satisfy a third condition (see [TL18, (Ineq. 14)]). However, this condition is not needed for our proofs, which is why we can use the weaker definition of Definition 4.1. Similar improvements for the standard basis incoherence notion were achieved in [Che15]. In [TL18, Lemma 21], it was shown that up constants, the definition above is equivalent to a coherence condition with respect to the standard basis [Rec11, Che15]. ", "page_idx": 5}, {"type": "text", "text": "Following the conventional sampling approach in the existing literature [Rec11, Che15, CWB08], the index set is $\\boldsymbol{\\Omega}=(i_{\\ell},j_{\\ell})_{\\ell=1}^{m}\\subset\\mathbf{I}$ contains $m$ samples drawn uniformly at random without replacement. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Local convergence of MatrixIRLS for EDG with Quadratic Rate). Let $\\mathbf{X}^{0}\\in S_{n}$ be a matrix of rank $r$ that is $\\nu$ -incoherent, and let $\\mathcal{A}:S_{n}\\rightarrow\\mathbb{R}^{m+\\bar{n}}$ be the measurement operator corresponding to an index set $\\Omega\\subset\\mathbb{I}$ of size $m=|\\Omega|$ that is drawn uniformly without replacement. There exist constants $C^{*}$ , $\\widetilde{C}$ and $C$ such that the following holds. (a) If the sample complexity fulflils $m\\geq C\\nu r n\\log n$ , and $i f(b)$ the output matrix $\\mathbf{X}^{(k)}\\in\\breve{S}_{n}$ of the $k$ -th iteration of MatrixIRLS for EDG with inputs $\\mathbf{y}=\\mathcal{A}(\\mathbf{X}^{0})$ and $\\widetilde r=r$ updates the smoothing parameter in (7) such that $\\epsilon_{k}=$ $\\sigma_{r+1}(\\mathbf{X}^{(k)})$ and fulflils $\\begin{array}{r}{\\|\\mathbf{X}^{(k)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\lesssim\\frac{m^{\\frac{3}{2}}}{C^{\\ast}\\kappa L^{2}(\\log n)^{\\frac{3}{2}}\\sqrt{n-r}}\\sigma_{r}(\\mathbf{X}^{0})}\\end{array}$ \u2272C\u2217\u03baL2(lomg 2n)32 \u221an\u2212r\u03c3r(X0) where \u03ba = \u03c31(X0)/\u03c3r(X0) is the condition number of ${\\bf X}^{0}$ , then the local convergence rate is quadratic in the sense that \u2225X(k+1)\u2212X0\u2225S\u221e\u2264min(\u00b5\u2225X(k)\u2212X0\u22252S\u221e, \u2225X(k)\u2212X0\u2225S\u221e) with \u00b5 = C2L mlog n 4(1+6\u03ba1)\u03c3r(X0) and furthermore $\\mathbf{X}^{(k+\\ell)}\\xrightarrow{\\ell\\to\\infty}\\mathbf{X}^{0}$ with high probability. ", "page_idx": 5}, {"type": "text", "text": "(The values of the constants $C^{*}=10^{5},\\widetilde{C}=21\\sqrt{5},C=4900$ are explicitly derived in the Supplementary material.) ", "page_idx": 5}, {"type": "text", "text": "In other words, Theorem 4.3 indicates Algorithm 1 converges to the ground truth with high probability with a sample complexity of $\\Omega(\\nu r n\\log n)$ , if initialized close to the ground truth Gram matrix. We refer to Appendix $\\mathrm{D}$ for its proof. ", "page_idx": 5}, {"type": "text", "text": "Our theorem\u2019s sample complexity requirement aligns with the lower bound for generic low-rank matrix completion problems, as established in [CT10]. We note that this theorem only provides a local convergence guarantee for Algorithm 1. This is in line with the strongest known results for IRLS algorithms optimizing non-convex objectives [For10, KV21, PKV22]. We provide numerical evidence in Section 5 that the minimal sample complexity assumption (a) of Theorem 4.3 indeed captures the generic reconstruction ability of the method.To the best of our knowledge, Theorem 4.3 represents the first convergence guarantee for any algorithm for Problem 1 that applies at the optimal order $\\Omega(\\nu r n\\log n)$ of provided pairwise distances, and furthermore, the first theoretical guarantee for any non-convex optimization framework for Problem 1. ", "page_idx": 5}, {"type": "text", "text": "4.2 Dual Basis Construction and Local Restricted Isometry Property on Tangent Spaces ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While a convergence result similar to Theorem 4.3 had been previously obtained for an IRLS algorithm for low-rank matrix completion [KV21], an adaptation of the proof of [KV21] to the EDG setting is not possible due to non-orthogonality of the basis $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha}$ of Definition 3.2. ", "page_idx": 5}, {"type": "text", "text": "In order to prove Theorem 4.3, we establish a restricted isometry property of a suitably defined sampling operator (see (9)) with respect to the tangent space of the manifold of symmetric rank- $^r$ matrices at the ground truth Gram matrix ${\\bf X}^{0}={\\bf P}^{\\dagger}{\\bf P}$ . To formulate this sampling operator and the respective RIP condition, we construct a dual basis to the measurement basis $\\bar{\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha}}\\bar{\\}$ of Definition 3.2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4 (Dual Basis Construction). Let $n\\in\\mathbb{N}$ , $\\mathbb{I}=\\{(i,j)~|~1\\le i<j\\le n\\}$ be the index set and $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}\\cup\\mathbb{I}_{D}}$ be the primal basis of Definition 3.2 with $\\mathbb{I}_{D}=\\{(i,i):i\\in\\{1,\\ldots,n\\}\\}$ . If ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{v}_{\\alpha}=\\left\\{{\\bf-}\\frac{1}{2}(\\mathbf{a}_{i}\\mathbf{a}_{j}^{\\top}+\\mathbf{a}_{j}\\mathbf{a}_{i}^{\\top}),\\quad i f\\alpha=(i,j)\\in\\mathbb{I},\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{{\\bf a}_{i}={\\bf e}_{i}-\\frac{1}{n}{\\bf1}}\\end{array}$ for $i\\in\\{1,\\ldots,n\\}$ , then $\\{\\mathbf{v}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}\\cup\\mathbb{I}_{D}}$ is a dual basis with respect to $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}\\cup\\mathbb{I}_{D}}$ , i.e., $\\{\\mathbf{v}_{\\alpha}\\}_{\\alpha}$ and $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha}$ are bi-orthogonal. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4 extends the dual basis construction of [TL18, LT24], in which the duality of $\\{\\mathbf{v}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}}$ with respect to $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}}$ was shown. The proof of Lemma 4.4 is detailed in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "Unlike the basis pair of [TL18, LT24], our bases span the entire space of symmetric matrices $S_{n}$ (see Appendix B.2), which enables us to show the following restricted isometry property. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5 (Restricted Isometry Property for Sampling Operator $\\mathcal{Q}_{\\Omega}$ ). Let $L\\,=\\,n(n\\,-\\,1)/2$ , $0<\\epsilon\\le\\frac{1}{2}$ , and $\\Omega\\subset\\mathbb{I}$ be a multiset of size m sampled independently with replacement. Define the sampling operator $\\mathcal{Q}_{\\Omega}:S_{n}\\rightarrow S_{n}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{\\Omega}(\\mathbf{X}):=\\frac{L}{m}\\sum_{\\alpha\\in\\Omega\\cup(i,i)_{i=1}^{n}}\\langle\\mathbf{X},\\mathbf{w}_{\\alpha}\\rangle\\mathbf{v}_{\\alpha}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{w}_{\\alpha}$ and $\\mathbf{v}_{\\alpha}$ as in (4) and (8), respectively. Let ${\\mathbf{X}}^{0}\\,\\in\\,S_{n}$ be a $\\nu$ -incoherent matrix whose tangent space onto the manifold $\\mathcal{M}_{r}$ of symmetric rank- $^r$ matrices is denoted as $T_{0}=T\\mathbf{\\vec{x}}^{\\mathrm{0}}$ . Let $\\mathcal P_{T_{0}}:S_{n}\\rightarrow S_{n}$ be the projection operator associated to $T_{0}$ . Then $\\left\\|\\mathcal{P}_{T_{0}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\right\\|_{S_{\\infty}}^{-*}\\le\\epsilon$ holds with probability at least $\\textstyle1-{\\frac{2}{n}}$ provided that ", "page_idx": 6}, {"type": "equation", "text": "$$\nm\\ge(49/\\epsilon^{2})\\nu n r\\log n.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The dual basis as discussed in Section 3 along with the extension for the diagonal entries together spans the space of $n\\times n$ symmetric matrices. This construction has been crucial in proving the RIP restricted to the tangent space $T_{\\mathbf{X}}$ of rank constrained smooth manifold $\\mathcal{M}_{r}$ . This construction could also be valuable for analyzing other non-convex algorithms. A detailed proof of Theorem 4.5 is provided in Appendix C This proof is achieved by using concentration inequalities like the Matrix Bernstein inequality theorem C.1 in multiple lemmas stated and proved in detail in the supplemental material. ", "page_idx": 6}, {"type": "text", "text": "Since the existing literature for nonconvex approaches for solving Problem 1 lacks this property, this approach of establishing RIP by restricting it to the tangent space can be useful for the analysis of other nonconvex methods. The RIP condition, originally introduced in the context of compressed sensing [Can08], is a fundamental assumption in the literature on low-rank matrix recovery ([ZL12, Che15, KV21, CZ13]). A tangent-space restricted RIP has been has been useful for analyzing the convergence and performance properties of other non-convex methods [GLM16, LZ23, LHLZ20, ZLTW18] in a non-EDG setting. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of MatrixIRLS, Algorithm 1, for instances of the EDG reconstruction Problem 1 in terms of data efficiency across multiple datasets in comparison to other state-of-the-art methods in the literature. We compare the performance of MatrixIRLS with three other algorithms: (a) ALM, an augmented Lagrangian method that minimizes the non-convex formulation defined by $\\operatorname*{min}_{\\mathbf{P}\\in\\mathbb{R}^{r\\times n}}\\operatorname{Tr}(\\mathbf{\\bar{P}^{\\intercal}P})$ subject to $\\mathcal{R}_{\\Omega}(\\mathbf{P}^{\\top}\\mathbf{P})=\\mathcal{R}_{\\Omega}(\\mathbf{X}^{0})$ , ", "page_idx": 6}, {"type": "text", "text": "with ${\\mathcal{R}}_{\\Omega}$ being equal to the sampling operator $\\mathcal{Q}_{\\Omega}$ restricted to $\\Omega$ , which is based on a Burer-Monteiro factorization of the Gram matrix, and which has been studied in the numerical experiments of [TL18], (b) ScaledSGD [ZCZ22] which is a preconditioned stochastic gradient descent method designed to be robust with respect to ill-conditioned problems, (c) RieEDG [SLCT23] which is a Riemannian-based gradient descent approach based on the sampling operator (9) restricted to $\\Omega$ . The choice of these algorithms is based on their robustness to noise as claimed in the respective papers. ", "page_idx": 6}, {"type": "text", "text": "5.1 Synthetic Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider a synthetic data, where we select $n=500$ points $\\mathbf{P}^{0}=[\\mathbf{p}_{1},\\quad.\\,.\\,.\\,,\\mathbf{p}_{n}]\\in\\mathbb{R}^{r\\times n}$ from a standard Gaussian distribution at random such that $(\\mathbf{p}_{i})_{j}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,1)$ for all $i,j$ , which defines the ground truth Gram matrix $\\mathbf{X}^{0}=\\mathbf{P}^{0\\top}\\mathbf{P}^{0}$ . We are provided with $m=|\\Omega|$ Euclidean distances, where the point index pair set $\\Omega\\,\\subset\\,\\{(i,j)\\in[n]\\times[\\bar{n}],i<j\\}$ is sampled uniformly at random. This is parametrized by the oversampling factor $\\begin{array}{r}{\\rho=\\frac{m}{n r-r(r-1)/2}}\\end{array}$ where the denominator is the degrees of freedom (discussed in Appendix E.4). To understand the efficiency of the algorithm over a range of ranks $r$ and across different oversampling factors $\\rho$ , we conduct phase transition experiments for all the above mentioned algorithms. We define a successful recovery as the case of the relative Procrustes distance $d_{\\mathrm{Procrustes}}(\\mathbf{P}_{r e c},\\mathbf{P}^{0})$ (discussed in Appendix E.3) between the recovered matrix $\\mathbf{P}_{r e c}$ and ground truth coordinate matrix $\\mathbf{P}^{0}$ does not exceed a tolerance threshold $\\mathrm{{tol}_{r e c}=10^{-3}}$ . We chose the Procrustes distance at it is a shape preserving distance that accounts also for differences in scaling and alignment between the reconstructed geometries. We observe the performance of the algorithms as shown in the Figure 1 for ranks between 2 to 5 and a oversampling factor ranging from 1 to 4 over 24 instances. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In Figure 1, each entry on the figure represents the probability of success of an algorithm for the given rank ground truth rank $r$ and oversampling factor $\\rho$ over 24 instances.1 In terms of the recovery of the ground truth, we notice a comparable performance for the MatrixIRLS and ALM. However, the other two algorithms RieEDG and ScaledSGD, for the given success tolerance, the recovery of a ground truth is only possible if more samplies corresponding to a larger oversampling factor are provided, for any of the ranks $r$ considered. This emphasizes that MatrixIRLS is able to achieve state-of-the-art data efficiency. ", "page_idx": 7}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/efdcd80703edfda67d376c0659e9db04528b8bff857128d6f9c327af02d95e4b.jpg", "img_caption": ["Figure 1: Success probabilities for recovery for different algorithms, given Gaussian ground truths $\\mathbf{X}^{0}$ of different ranks, computed across 24 instances. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In order to understand the scalability of the proposed algorithm, we have provided the time of completion of MatrixIRLS in Table 1. The first two rows shows that for fixed oversampling factor $\\rho=$ 3 , we are able to recover the points in the magnitude of $10^{4}$ , is just 13.7 minutes. So to further test the scalability, we also looked at the time to completion when less number of samples are provided ( $\\rho=2.5)$ ). In that setup $n=10000$ takes 57.5 minutes to recover with high precision. ", "page_idx": 7}, {"type": "text", "text": "5.2 Ill-conditioned Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now assess the performance of the different EDG methods on ill-conditioned data. Ill-conditioning in the point matrix in particular arises in situations where, for example, $r$ -dimensional points are approximately following an $r-1$ -dimensional geometry, a situation which often arises when the available distance information if affected by outliers [ZCZ22]. ", "page_idx": 7}, {"type": "text", "text": "Similar to the setup above, we construct Gram matrices $\\mathbf{X}^{0}\\,\\in\\,S_{n}$ of rank $r\\,=\\,5$ with condition number $\\kappa=\\sigma_{1}(\\dot{\\mathbf{X^{0}}})/\\sigma_{r}(\\mathbf{X^{0}})=10^{5}$ corresponding to $n=400$ random data points $\\mathbf{P}^{0}\\,\\in\\,\\mathbb{R}^{r\\times n}$ generated with random orthogonal singular vectors and singular values $\\sigma_{i}(\\mathbf{X}^{0})$ interpolated between $\\kappa$ and 1 with decay of order $\\breve{O}(i^{-2})$ . Figure 2 shows the success probability of different algorithms for ill-conditioned ground truths. ", "page_idx": 7}, {"type": "text", "text": "To have a deeper insight into the algorithm\u2019s performance, Figure 3 shows the per-iterate relative reconstructed error of the four algorithms on this ill-conditioned data at an oversampling factor of $\\rho=2$ for a representative problem instance. This reconstructed error refers to the Procrustes distance between the ground truth $\\mathbf{P}^{0}$ and the recovered $\\mathbf{P}^{k}$ at each iteration $k$ . We observe that for ill-conditioned data, MatrixIRLS is the only method that can recover the ground truth up to a reasonable precision, achieving a relative error of $\\approx10^{-8}$ after around 35 iterations (top of Figure 3), whereas the other methods do not achieve errors below $10^{-3}$ even after 100000 iterations. While one iteration of MatrixIRLS typically has a longer runtime than an iteration of any of the other algorithms due to its second-order nature, we also provide a visualization of the observed runtimes of the methods in the bottom of Figure 3. It can be seen that MatrixIRLS is able reconstruct the geometry of the challenging problem in around 200 seconds up to a high precision. Additionally, we run a study on the algorithms\u2019 behavior across different oversampling factors between $\\rho=1$ and $\\rho=4$ for 24 random problem instances, visualized in Figure 4. The box plots indicate visualize median relative Procrustes error together with the relevant $25\\%$ and $75\\%$ quantiles of the observed error distribution. We observe that MatrixIRLS has consistent convergence for ill-conditioned data even for lower oversampling rates as long as $\\rho\\geq1.5$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/81dd00bf48181663656c102b4fdd88f8e25833c651746213d174d7a59ee6a831.jpg", "img_caption": ["Figure 2: Empirical success probabilities for recovery for different algorithms, given ill-conditioned ground truths $\\mathbf{X}^{0}$ of different ranks, computed across 8 instances. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/5c96d0f3e3faf0b3a7ea4fe93d248850b5fbab477234a44aa691250593001183.jpg", "img_caption": ["Figure 3: Top: Relative error plot. Bottom: Runtime Figure 4: Performance of EDG completion algorithms across iterates for one instance. over 24 instances on ill-conditioned data. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/abf54d53af24f707fd1ad4364706464e583657804e684f583b58de34f9476e90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Real Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To assess the performance of Algorithm 1 in realistic setups, we consider the task of molecular conformation, i.e., we aim to reconstruct the 3D structure of a molecule from partial information about the distances of its atoms. For our experiments, we determine the structures of a protein molecule (1BPM) from the Protein Data Bank $[\\mathrm{HMBFGG^{+}00}]$ , which is collected using $\\boldsymbol{\\mathrm{X}}$ -ray diffraction experiments or nuclear magnetic resonance (NMR) spectroscopy. ", "page_idx": 8}, {"type": "text", "text": "The goal of this experiment is to reconstruct the point matrix $\\mathbf{P}^{0}$ from $|\\Omega|$ distance samples as defined in Problem 1. For the 1BPM protein data the rank of the input matrix is 3. We conduct the experiments corresponding to oversampling factors $\\rho$ between 1 and 4. Like the previous setup, here successful recovery refers to the case where the relative Procrustes distance, that is the metric $d_{\\mathrm{Procrustes}}(\\mathbf{P}_{r e c},\\mathbf{P}^{0})$ between the recovered matrix $\\mathbf{P}_{r e c}$ and ground truth coordinate matrix $\\mathbf{P}^{0}$ does not exceed a tolerance threshold $\\mathrm{tol}_{\\mathrm{rec}}$ across 24 independent realizations. The error analysis for this experiment is shown in Figures $7\\mathbf{a}$ and 7b in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "It is evident that MatrixIRLS and ALM have comparable results in recovering the geometry of the data given lesser samples, where as algorithms like ScaledSGD or RieEDG are unable to reconstruct geometries even when the oversampling rate is as high as $\\rho=4$ . In the fig. 5, we see a convergence with high precision for MatrixIRLS from $\\rho\\sim2.5$ for Protein which means that it recovered the ground truth with around $0.5\\%$ samples. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/d2a1551b2842adf1221532587c6c78e4533c0af1b36f2e9d69586472c478903e.jpg", "img_caption": ["Figure 5: Reconstruction of Protein 1BPM molecule Figure 6: Reconstruction of Protein 1BPM molecule by MatrixIRLS with $0.5\\%$ samples by MatrixIRLS with $0.6\\%$ samples "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In the Figure 6 the reconstructed protein in blue is aligned with the ground truth structure of the protein in pink. We can see that when the oversampling is 3.5, which is when there is $0.6\\%$ samples available, the reconstruction exactly matches with the ground truth. ", "page_idx": 9}, {"type": "text", "text": "In order to understand the runtime of the different algorithms Table 2, reports the reconstruction times for each of the four algorithms applied to the 1BPM Protein data (with datapoints $n=3672$ ) in a low-data regime with oversampling factor of $\\rho=3$ . . It can be seen that MatrixIRLS is with 7.08 minutes around 3 times faster than ALM, which needed 23.04 minutes until convergence. While these times certainly depend on the precise choice of stopping criteria for each algorithm (we used the ones indicated in Appendix E ), this shows that the proposed method is competitive in terms of clock time. ", "page_idx": 9}, {"type": "table", "img_path": "Yu7H8ZOuI2/tmp/3a766db5ea636d4fe37a36efc564b0882fdde79db6667d28aa46198ccbf04a03.jpg", "table_caption": [], "table_footnote": ["Table 1: Execution time of MatrixIRLS vs problem size $n$ for Gaussian data with oversampling factor $\\rho=3$ "], "page_idx": 9}, {"type": "table", "img_path": "Yu7H8ZOuI2/tmp/4491fe1a18daebe8c78149b7723c9a00dd4f050d24b6c91ad6f949044d99efe5.jpg", "table_caption": [], "table_footnote": ["Table 2: Runtime comparison for different geometry reconstruction algorithms from partially known pairwise distances for 1BPM Protein data $(n=3672,r=3)$ ), oversampling factor $\\rho=3$ "], "page_idx": 9}, {"type": "text", "text": "For the implementation of the other algorithms, we use the authors\u2019 code for the respective approaches (discussed in Appendix E). We include another set of experiments on US cities data [UU20], in the Appendix E. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the challenge of reconstructing suitable geometric configurations using minimal Euclidean distance samples. By leveraging continuous and non-convex rank minimization formulations, we develop a variant of the iteratively reweighted least squares (IRLS) algorithm and establish a local convergence guarantee under the condition of a minimal random set of observed distances. Our contribution also includes the proof of a restricted isometry property (RIP) restricted to the tangent space of the manifold of symmetric rank- ${\\bf\\nabla}r$ matrices, a result which might be of independent interest for the analysis of other non-convex methods. As future work further analysis on the global convergence can be established. Through numerical validation we conclude that the algorithm is able to achieve state-of-the-art data efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abiy Tasissa acknowledges partial support from the National Science Foundation through grant DMS-2208392. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "ADSVF23] A. Andreella, R. De Santis, A. Vesely, and L. Finos. Procrustes-based distances for exploring between-matrices similarity. Statistical Methods & Applications, 32(3):867\u2013882, 2023.   \n[AKW99] A. Y. Alfakih, A. K. Khandani, and H. Wolkowicz. Solving Euclidean Distance Matrix Completion Problems Via Semidefinite Programming. Computational Optimization and Applications, 12:13\u201330, 1999. [Alf05] A. Y. Alfakih. On the uniqueness of Euclidean distance matrix completions: the case of points in general position. Linear Algebra and its Applications, 397:265\u2013 277, 2005.   \n[ALMT14] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase transitions in convex programs with random data. Information and Inference: A Journal of the IMA, 3(3):224\u2013294, 2014. [BA15] N. Boumal and P.-A. Absil. Low-rank matrix completion via preconditioned optimization on the Grassmann manifold. Linear Algebra and its Applications, 15(475):200\u2013239, 2015. [BJ95] M. Bakonyi and C. R. Johnson. The Euclidian Distance Matrix Completion Problem. SIAM J. Matrix Anal. Appl., 16:646\u2013654, 1995. [BM03] S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.   \n[BNZ21] J. Bauch, B. Nadler, and P. Zilber. Rank 2r iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries. SIAM J. Math. Data Sci., 3(1):439\u2013465, 2021. [Bou20] N. Boumal. An introduction to optimization on smooth manifolds. Available online at http: // sma. epfl. ch/ \\~nboumal/ book/ IntroOptimManifolds_ Boumal_ 2020. pdf , November, 2020. [Bur12] C. S. Burrus. Iterative reweighted least squares. OpenStax CNX. Available online: http://cnx. org/contents/92b90377-2b34-49e4-b26f-7fe572db78a1, 12, 2012. [Can08] E. J. Cand\u00e8s. The restricted isometry property and its implications for compressed sensing. Comptes Rendus Mathematique, 346(9):589\u2013592, 2008. [CC14] Y. Chen and Y. Chi. Robust Spectral Compressed Sensing via Structured Matrix Completion. IEEE Trans. Inf. Theory, 60(10):6576\u20136601, 2014. [Che12] X. Chen. Smoothing methods for nonsmooth, nonconvex minimization. Math. Program., 134(1):71\u201399, 2012. [Che15] Y. Chen. Incoherence-Optimal Matrix Completion. IEEE Trans. Inf. Theory, 61(5):2909\u20132923, 2015. [CLC19] Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview. IEEE Trans. Signal Process., 67(20):5239\u20135269, 2019. [CR09] E. J. Cand\u00e8s and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math., 9(6):717\u2013772, 2009. [CT10] E. J. Cand\u00e8s and T. Tao. The Power of Convex Relaxation: Near-Optimal Matrix Completion. IEEE Trans. Inf. Theory, 56(5):2053\u20132080, 2010. [CW18] J.-F. Cai and K. Wei. Exploiting the Structure Effectively and Efficiently in LowRank Matrix Recovery. Processing, Analyzing and Learning of Images, Shapes, and Forms, 19:21 pp., 2018. [CWB08] E. Cand\u00e8s, M. B. Wakin, and S. Boyd. Enhancing Sparsity by Reweighted \u21131- Minimization. The Journal of Fourier Analysis and Applications, 14:877\u2013905, 2008. [CZ13] T. T. Cai and A. Zhang. Sharp RIP bound for sparse signal and low-rank matrix recovery. Applied and Computational Harmonic Analysis, 35(1):74\u201393, 2013.   \n[DDFG10] I. Daubechies, R. DeVore, M. Fornasier, and C. G\u00fcnt\u00fcrk. Iteratively Reweighted Least Squares Minimization for Sparse Recovery. Commun. Pure Appl. Math., 63:1\u201338, 2010.   \n[DPRV15] I. Dokmanic, R. Parhizkar, J. Ranieri, and M. Vetterli. Euclidean Distance Matrices: Essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12\u201330, 2015. [DR16] M. A. Davenport and J. Romberg. An Overview of Low-Rank Matrix Recovery From Incomplete Observations. IEEE J. Sel. Topics Signal Process., 10:608\u2013622, 2016. [For10] M. Fornasier. Numerical Methods for Sparse Recovery. In M. Fornasier, editor, Theoretical Foundations and Numerical Methods for Sparse Recovery, volume 9 of Radon Series on Computational and Applied Mathematics, pages 93\u2013200. De Gruyter, Berlin, 2010. [FQX19] J. Fliege, H.-D. Qi, and N. Xiu. Euclidean distance matrix optimization for sensor network localization. In Cooperative Localization and Navigation, pages 99\u2013126. CRC Press, 2019. [FRW11] M. Fornasier, H. Rauhut, and R. Ward. Low-rank Matrix Recovery via Iteratively Reweighted Least Squares Minimization. SIAM J. Optim., 21(4):1614\u20131640, 2011.   \n[GCKG23] B. Ghojogh, M. Crowley, F. Karray, and A. Ghodsi. Multidimensional Scaling, Sammon Mapping, and Isomap, pages 185\u2013205. Springer International Publishing, Cham, 2023. [GLM16] R. Ge, J. D. Lee, and T. Ma. Matrix Completion has No Spurious Local Minimum. In Advances in Neural Information Processing Systems (NIPS), pages 2973\u20132981, 2016. [Gow85] J. C. Gower. Properties of Euclidean and non-Euclidean distance matrices. Linear Algebra and its Applications, 67:81\u201397, 1985. [GR97] I. F. Gorodnitsky and B. D. Rao. Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm. IEEE Trans. Signal Process., pages 600\u2013616, 1997. [Gro11] D. Gross. Recovering Low-Rank Matrices From Few Coefficients in Any Basis. IEEE Trans. Inf. Theory, 57(3):1548\u20131566, 2011. [Hen95] B. Hendrickson. The Molecule Problem: Exploiting Structure in Global Optimization. SIAM Journal on Optimization, 5(4):835\u2013857, 1995.   \nBFGG $^+00_{.}$ ] J. W. Helen M. Berman, Z. Feng, T. N. B. Gary Gilliland, I. N. S. Helge Weissig, and P. E. Bourne. The Protein Data Bank. Nucleic Acids Research, Volume 28, Issue 1, 1 January 2000, Pages 235\u2013242, 2000. [HS52] M. R. Hestenes and E. Stiefel. Methods of Conjugate Gradients for Solving Linear Systems. Journal of research of the National Bureau of Standards, 49(1), 1952. $[\\mathbf{J}\\mathbf{E}\\mathbf{P}^{+}21]$ J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. \u017d\u00eddek, A. Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, 2021. [KS18] C. K\u00fcmmerle and J. Sigl. Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix Recovery. J. Mach. Learn. Res., 19(47):1\u201349, 2018. [KV21] C. K\u00fcmmerle and C. M. Verdun. A scalable second order method for ill-conditioned matrix completion from few samples. In International Conference on Machine Learning, pages 5872\u20135883, 2021. [Law61] C. Lawson. Contributions to the Theory of Linear Least Maximum Approximation. Ph. D. Thesis, Univ. of Calif., Los Angeles, 1961.   \n[LHLZ20] Y. Luo, W. Huang, X. Li, and A. R. Zhang. Recursive Importance Sketching for Rank Constrained Least Squares: Algorithms and High-order Convergence. arXiv preprint arXiv:2011.08360, 2020. [Lib20] L. Liberti. Distance geometry and data science. Top, 28(2):271\u2013339, 2020. [LL17] R. Lai and J. Li. Solving Partial Differential Equations on Manifolds From Incomplete Inter-Point Distance, 2017.   \n[LLMM14] L. Liberti, C. Lavor, N. Maculan, and A. Mucherino. Euclidean Distance Geometry and Applications. SIAM Review, 56(1):3\u201369, 2014. [LS24] Y. Li and X. Sun. Sensor Network Localization via Riemannian Conjugate Gradient and Rank Reduction. IEEE Transactions on Signal Processing, 2024. [LT24] S. Lichtenberg and A. Tasissa. A dual basis approach to multidimensional scaling. Linear Algebra and its Applications, 682:86\u201395, 2024. [LZ23] Y. Luo and A. R. Zhang. Low-rank Tensor Estimation via Riemannian GaussNewton: Statistical Optimality and Second-Order Convergence. The Journal of Machine Learning Research, 24(1):18274\u201318321, 2023. [Meu06] G. Meurant. The Lanczos and Conjugate Gradient Algorithms: From Theory to Finite Precision Computations. Society for Industrial and Applied Mathematics\u201e 2006. [MF10] K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application to system identification. In Proceedings of the American Control Conference, pages 2953\u20132959. IEEE, 2010. [Mir60] L. Mirsky. Symmetric Gauge Functions And Unitarily Invariant Norms. The Quarterly Journal of Mathematics, 11(1):50\u201359, 1960. [MM15] C. Musco and C. Musco. Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition. In Advances in Neural Information Processing Systems (NIPS), pages 1396\u20131404, 2015.   \n[MMLL19] T. Malliavin, A. Mucherino, C. Lavor, and L. Liberti. Systematic Exploration of Protein Conformational Space Using a Distance Geometry Approach. Journal of Chemical Information and Modeling, 59, 08 2019.   \n[MMWL22] M. Masters, A. H. Mahmoud, Y. Wei, and M. A. Lill. Deep Learning Model for Flexible and Efficient Protein-Ligand Docking. In ICLR2022 Machine Learning for Drug Discovery, 2022. [MW97] J. J. Mor\u00e9 and Z. Wu. Global Continuation for Distance Geometry Problems. SIAM Journal on Optimization, 7(3):814\u2013836, 1997.   \n[MWCC20] C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution. Foundations of Computational Mathematics, 20:451\u2013632, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[NKKS19] L. T. Nguyen, J. Kim, S. Kim, and B. Shim. Localization of IoT networks via lowrank matrix completion. IEEE Transactions on Communications, 67(8):5833\u20135847, 2019. ", "page_idx": 13}, {"type": "text", "text": "[PKV22] L. Peng, C. K\u00fcmmerle, and R. Vidal. Global Linear and Local Superlinear Convergence of IRLS for Non-Smooth Robust Regression. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 28972\u201328987, 2022. [Rec11] B. Recht. A Simpler Approach to Matrix Completion. J. Mach. Learn. Res., 12:3413\u20133430, 2011. [RFP10] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization. SIAM Rev., 52(3):471\u2013 501, 2010.   \n[RXH11] B. Recht, W. Xu, and B. Hassibi. Null space conditions and thresholds for rank minimization. Mathematical programming, 127:175\u2013202, 2011.   \n[SBP17] Y. Sun, P. Babu, and D. P. Palomar. Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning. IEEE Trans. Signal Process., 65(3):794\u2013816, 2017.   \n$[\\mathrm{SEJ}^{+}20]$ A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. \u017d\u00eddek, A. W. Nelson, A. Bridgland, et al. Improved protein structure prediction us3ing potentials from deep learning. Nature, 577(7792):706\u2013710, 2020. [SL16] R. Sun and Z.-Q. Luo. Guaranteed Matrix Completion via Non-Convex Factorization. IEEE Transactions on Information Theory, 62(11):6535\u20136579, November 2016.   \n[SLCT23] C. M. Smith, S. P. Lichtenberg, H. Cai, and A. Tasissa. Riemannian Optimization for Euclidean Distance Geometry. In OPT 2023: Optimization for Machine Learning, 2023. [Ste06] M. Stewart. Perturbation of the SVD in the presence of small singular values. Linear Algebra Appl., 419(1):53\u201377, 2006.   \n[SWBB97] D. C. Spellmeyer, A. K. Wong, M. J. Bower, and J. M. Blaney. Conformational analysis using distance geometry methods. Journal of Molecular Graphics and Modelling, 15(1):18\u201336, 1997. [TL18] A. Tasissa and R. Lai. Exact reconstruction of euclidean distance geometry problem using low-rank matrix completion. IEEE Transactions on Information Theory, 65(5):3124\u20133144, 2018. [Tro00] M. W. Trosset. Distance matrix completion by numerical optimization. Computational Optimization and Applications, 17:11\u201322, 2000. [Tro11] J. A. Tropp. User-Friendly Tail Bounds for Sums of Random Matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, August 2011. [TSL00] J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000. [TW13] J. Tanner and K. Wei. Normalized Iterative Hard Thresholding for Matrix Completion. SIAM J. Sci. Comput., 35(5):S104\u2013S125, 2013. [UU20] U.S. Geological Survey and U.S. Census Bureau. US Cities Dataset. Available at https://simplemaps.com/data/us-cities, 2020. [Van13] B. Vandereycken. Low-Rank Matrix Completion by Riemannian Optimization. SIAM J. Optim., 23(2), 2013.   \n[VSP+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[WCCL20] K. Wei, J.-F. Cai, T. F. Chan, and S. Leung. Guarantees of Riemannian optimization for low rank matrix completion. Inverse Probl. Imaging, 14(2):233\u2013265, 2020. [Woo50] M. A. Woodbury. Inverting modified matrices. Memorandum report, 42(106):336, 1950. [YH38] G. M. Young and A. S. Householder. Discussion of a set of points in terms of their mutual distances. Psychometrika, 3:19\u201322, 1938. [ZCZ22] J. Zhang, H.-M. Chiu, and R. Y. Zhang. Accelerating SGD for Highly IllConditioned Huge-Scale Online Matrix Completion. Advances in Neural Information Processing Systems, 35:37549\u201337562, 2022. [ZL12] Y.-B. Zhao and D. Li. Reweighted $\\ell_{1}$ -minimization for sparse solutions to underdetermined linear systems. SIAM J. Optim., 22(3):1065\u20131088, 2012.   \n[ZLTW18] Z. Zhu, Q. Li, G. Tang, and M. B. Wakin. Global Optimality in Low-Rank Matrix Optimization. IEEE Trans. Signal Process., 66(13):3614\u20133628, 2018. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For our method, the limited convergence radius leads to the convergence guarantee locally. Additionally, the runtime per iteration is larger compared to other non-convex methods [ZCZ22, TL18, SLCT23] but our method achieves convergence in less than a tenth of the number of iterations of the other methods. For some datasets like the Protein (1BPM) $[\\mathrm{HMBFGG^{+}00}]$ , ALM has a superior performance for oversampling factor $(\\rho)$ between 1 and 1.5. ", "page_idx": 15}, {"type": "text", "text": "B Dual Basis Construction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the proof the bi-orthogonality of the basis defined in Lemma 4.4 with respect to the measurement basis Definition 3.2, and furthermore establish properties of the basis pair in Appendix B.2 that will be useful later on. ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4.4. For simplicity of the proof, we denote the two cases of Definition 3.2, separately.   \nThis helps us to divide the proof into multiple subcases. ", "page_idx": 15}, {"type": "text", "text": "With $\\begin{array}{r}{L\\ =\\ \\frac{n(n-1)}{2}}\\end{array}$ , let $\\mathbf{V}\\ =\\ \\left[v_{(1,2)},\\quad\\ldots,\\quad v_{(n-1,n)}\\right]\\ \\in\\ \\mathbb{R}^{n^{2}\\times L}$ be a matrix whose columns enumerate the vectorized dual basis elements v\u03b1 where \u03b1 = (i, j) \u2208 I, and let VE \u2208 Rn2\u00d7n consist of vectorized dual basis vectors v(i,i) with i \u2208 {1, . . . , n}. Similarly, W \u2208 Rn2\u00d7L and $\\mathbf{W}_{E}\\,\\in\\,\\mathbb{R}^{n^{2}\\times n}$ are defined using vectorized basis elements $\\{\\mathbf{w}_{\\alpha}\\}_{\\alpha\\in\\mathbb{I}\\cup\\mathbb{I}_{D}}$ . As establishing duality between the two basis sets is equivalent to bi-orthogonality of the basis elements, it remains to show that the extended basis matrices $\\tilde{\\mathbf{V}}=[\\mathbf{V}|\\mathbf{V}_{E}]\\in\\mathbb{R}^{n^{2}\\times L+n}$ and $\\tilde{\\mathbf{W}}=[\\mathbf{W}|\\mathbf{W}_{E}]\\in\\mathbb{R}^{n^{2}\\times L+n}$ satisfy the relationship ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{W}}^{\\top}\\tilde{\\mathbf{V}}=\\left[\\mathbf{W}^{\\top}\\mathbf{V}\\quad\\mathbf{W}^{\\top}\\mathbf{V}_{E}\\right]=\\mathrm{Id},}\\\\ {\\mathbf{W}_{E}^{\\top}\\mathbf{V}\\quad\\mathbf{W}_{E}^{\\top}\\mathbf{V}_{E}\\right]=\\mathrm{Id},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where Id is the identity matrix. ", "page_idx": 15}, {"type": "text", "text": "In coordinate-wise notation, this means that for $\\alpha=(i,j)$ and $\\beta=(k,l)$ , with $1\\leq i\\leq j\\leq n$ and $1\\leq k\\leq l\\leq n$ , we must show that $\\langle\\widetilde{\\bf w}_{\\alpha},\\widetilde{\\bf v}_{\\beta}\\rangle=\\delta_{\\alpha,\\beta}$ where $\\tilde{\\mathbf{w}}_{\\alpha}$ and $\\tilde{\\mathbf{v}}_{\\beta}$ are columns of $\\tilde{\\bf W}$ and $\\tilde{\\textbf{V}}$ respectively. ", "page_idx": 15}, {"type": "text", "text": "From Claim 3.1 of [LT24], it is clear that $\\mathbf{V}$ is the dual of $\\mathbf{W}$ . We require to show the biorthogonality for the extended parts. ", "page_idx": 15}, {"type": "text", "text": "First we show that, for $\\alpha$ of the form $(i,i)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{w}_{\\alpha},\\mathbf{v}_{\\alpha}\\rangle=\\langle\\mathbf{w}_{i,i},\\mathbf{v}_{i,i}\\rangle}\\\\ &{\\qquad\\qquad=\\frac{1}{2}\\langle\\mathbf{e}_{i}\\cdot1^{\\top}+1\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}-\\mathbf{a}_{i}\\mathbf{a}_{i}^{\\top}\\rangle}\\\\ &{\\qquad\\qquad=\\frac{1}{2}(\\langle\\mathbf{e}_{i}\\cdot1^{\\top},\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}\\rangle+\\langle1\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}\\rangle-\\langle\\mathbf{e}_{i}\\cdot1^{\\top},\\mathbf{a}_{i}\\mathbf{a}_{i}^{\\top}\\rangle-\\langle1\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{a}_{i}\\mathbf{a}_{i}^{\\top}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let us decompose the summand $\\langle\\mathbf{e}_{i}\\cdot1^{\\top},\\mathbf{a}_{i}\\mathbf{a}_{i}^{\\top}\\rangle$ in detail, so that we can derive the remaining calculations in a similar manner, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf e_{i}\\cdot1^{\\top},\\mathbf a_{i}\\mathbf a_{i}^{\\top}\\rangle=\\langle\\mathbf e_{i}\\cdot1^{\\top},\\mathbf e_{i}\\mathbf e_{i}^{\\top}\\rangle-\\langle\\mathbf e_{i}\\cdot1^{\\top},\\frac1n\\mathbf e_{i}^{\\top}\\rangle-\\langle\\mathbf e_{i}\\cdot1^{\\top},\\frac1n\\mathbf e_{i}1^{\\top}\\rangle+\\langle\\mathbf e_{i}\\cdot1^{\\top},\\frac1n^{1}\\cdot1^{\\top}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=1-\\displaystyle\\frac1n-1+\\frac1n}\\\\ &{\\qquad\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting this value back in (10), we can complete the rest of the calculation in a similar manner and finally arrive at the following, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{\\alpha},\\mathbf v_{\\alpha}\\rangle=\\langle\\mathbf w_{i,i},\\mathbf v_{i,i}\\rangle}\\\\ &{\\qquad\\qquad=\\frac{1}{2}(\\langle\\mathbf e_{i}\\cdot\\mathbf1^{\\top},\\mathbf e_{i}\\mathbf e_{i}^{\\top}\\rangle+\\langle\\mathbf1\\cdot\\mathbf e_{i}^{\\top},\\mathbf e_{i}\\mathbf e_{i}^{\\top}\\rangle-\\langle\\mathbf e_{i}\\cdot\\mathbf1^{\\top},\\mathbf a_{i}\\mathbf a_{i}^{\\top}\\rangle+\\langle\\mathbf1\\cdot\\mathbf e_{i}^{\\top},\\mathbf a_{i}\\mathbf a_{i}^{\\top}\\rangle}\\\\ &{\\qquad\\qquad=\\frac{1}{2}(1+1-0-0)}\\\\ &{\\qquad\\qquad=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, let us look at the case where $\\alpha=(i,i)$ , and $\\beta=(j,j)$ and $i\\neq j$ . Using a similar calculation we can show that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{\\alpha},\\mathbf v_{\\beta}\\rangle=\\langle\\mathbf w_{i,i},\\mathbf v_{j,j}\\rangle}\\\\ &{\\quad\\quad\\quad=\\frac{1}{2}\\langle\\mathbf e_{i}\\cdot\\mathbf1^{\\top}-\\mathbf1\\cdot\\mathbf e_{i}^{\\top},\\mathbf e_{j}\\mathbf e_{j}^{\\top}-\\mathbf a_{j}\\mathbf a_{j}^{\\top}\\rangle}\\\\ &{\\quad\\quad\\quad=\\frac{1}{2}(\\langle\\mathbf e_{i}\\cdot\\mathbf1^{\\top},\\mathbf e_{j}\\mathbf e_{j}^{\\top}\\rangle-\\langle1\\cdot\\mathbf e_{i}^{\\top},\\mathbf e_{j}\\mathbf e_{j}^{\\top}\\rangle-\\langle\\mathbf e_{i}\\cdot\\mathbf1^{\\top},\\mathbf a_{j}\\mathbf a_{j}^{\\top}\\rangle+\\langle1\\cdot\\mathbf e_{i}^{\\top},\\mathbf a_{j}\\mathbf a_{j}^{\\top}\\rangle}\\\\ &{\\quad\\quad\\quad=\\frac{1}{2}(0-0-0+0)}\\\\ &{\\quad\\quad\\quad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, between the ${\\mathbf w}_{\\alpha}$ and $\\mathbf{v}_{\\beta}$ where $\\alpha=(i,j)$ with $1\\leq i<j\\leq n$ and $\\beta=(k,k)$ with $1\\leq k\\leq n$ , we can show a similar relation by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\mathbf{w}_{\\alpha},\\mathbf{v}_{\\beta}\\right)=\\left(\\mathbf{w}_{i},\\mathbf{v}_{i},\\mathbf{k}\\right)}\\\\ {=\\frac{1}{2}\\left(\\mathbf{e}_{i},+\\mathbf{e}_{j},-\\mathbf{e}_{i,j},-\\mathbf{e}_{\\beta,i},\\mathbf{e}_{k}\\mathbf{e}_{k}^{\\top}-\\mathbf{a}_{k}\\mathbf{e}_{i}^{\\top}\\right)}\\\\ {=\\frac{1}{2}\\left(\\delta_{i,k}+\\delta_{j,k}-0-0-\\left(\\delta_{i,k}-\\frac{1}{n}\\right)\\left(\\delta_{i,k}-\\frac{1}{n}\\right)-\\left(\\delta_{j,k}-\\frac{1}{n}\\right)\\left(\\delta_{j,k}-\\frac{1}{n}\\right)\\right)}\\\\ {+\\frac{1}{2}\\left(\\delta_{i,k}-\\frac{1}{n}\\right)\\left(\\delta_{j,k}-\\frac{1}{n}\\right)}\\\\ {=\\frac{1}{2}\\left(\\delta_{i,k}+\\delta_{j,k}-\\delta_{i,k}\\delta_{i,k}+\\frac{1}{n}\\delta_{i,k}+\\frac{1}{n}\\delta_{j,k}-\\frac{1}{n^{2}}-\\delta_{j,k}\\delta_{j,k}+\\frac{1}{n}\\delta_{j,k}+\\frac{1}{n}\\delta_{j,k}\\right)}\\\\ {+\\frac{1}{2}\\left(-\\frac{1}{n^{2}}+2\\delta_{i,k}\\delta_{j,k}-2\\frac{1}{n}\\delta_{j,k}-2\\delta_{i,k,k}+\\frac{2}{n^{2}}\\right)}\\\\ {=\\frac{1}{2}\\left(2\\delta_{i,k}\\delta_{j,k}\\right)}\\\\ {=\\delta_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we need to show for ${\\bf w}_{\\alpha}$ with $\\alpha\\,=\\,(i,i)$ for $1\\leq\\,i\\,\\leq\\,n$ and $\\mathbf{v}_{\\beta}$ where $\\beta\\,=\\,(k,l)$ with $1\\leq k<l\\leq n$ that the respective dot product is 0. This is verified by the calculation ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\langle\\mathbf{w}_{\\alpha},\\mathbf{v}_{\\beta}\\rangle=-{\\frac{1}{4}}(\\mathbf{e}_{i}\\cdot1^{\\top}+\\mathbf{1}\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{e}_{k}\\mathbf{e}_{i}^{\\top}+\\mathbf{e}_{i}\\mathbf{e}_{k}^{\\top})}\\\\ &{=-{\\frac{1}{4}}((\\mathbf{e}_{i}\\cdot1^{\\top},\\mathbf{e}_{k}\\mathbf{e}_{i}^{\\top})+\\langle\\mathbf{e}_{i}\\cdot1^{\\top},\\mathbf{e}_{i}\\mathbf{e}_{k}^{\\top}\\rangle+\\langle\\mathbf{1}\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{e}_{k}\\mathbf{e}_{i}^{\\top}\\rangle+\\langle\\mathbf{1}\\cdot\\mathbf{e}_{i}^{\\top},\\mathbf{e}_{i}\\mathbf{e}_{k}^{\\top}\\rangle)}\\\\ &{=-{\\frac{1}{4}}\\left(\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}\\right)}\\\\ &{=-{\\frac{1}{4}}\\left(\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}\\right)}\\\\ &{=-{\\frac{1}{4}}\\left(4\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}+\\delta_{i k}\\delta_{i l}\\right)}\\\\ &{=-{\\frac{1}{4}}(4\\delta_{i l}\\delta_{i l})}\\\\ &{=-\\delta_{i k}\\delta_{i l}}\\\\ &{=-\\delta_{k}}\\\\ &{=0}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Properties of Dual Basis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma B.1. $\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}$ maps $n\\times n$ symmetric matrices to itself. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\mathbf{x}$ be a vectorized representation of a symmetric matrix $\\mathbf{X}$ . Then, using the dual basis expansion, $\\mathbf{x}$ can be represented as: $\\mathbf{x}=\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}\\mathbf{x}$ . We now apply the the operator $\\bar{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}$ to $\\mathbf{x}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T})\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}\\mathbf{x}=\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\mathbf{T}}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last equality follows from the fact that $\\tilde{\\textbf{V}}$ is dual to $\\tilde{\\bf W}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. For orthonormal basis $\\{\\mathbf{w}_{\\alpha}\\}$ and $\\{\\mathbf{v}_{\\alpha}\\}$ as defined in Definition 3.2 , the spectral norm of $\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}$ is $^{\\,l}$ , that is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}\\right\\|_{S_{\\infty}}=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By definition, $\\|\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}\\|_{S_{\\infty}}=\\operatorname*{max}_{\\|x\\|_{2}=1}\\|\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}\\mathbf{x}\\|_{2}$ . Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{T}\\mathbf{x}\\|_{2}^{2}=\\mathbf{x}^{T}\\tilde{\\mathbf{V}}(\\tilde{\\mathbf{W}}^{T}\\tilde{\\mathbf{W}})\\tilde{\\mathbf{V}}^{T}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By construction of the dual basis, $(\\tilde{\\mathbf{W}}^{T}\\tilde{\\mathbf{W}})=(\\tilde{\\mathbf{V}}^{T}\\tilde{\\mathbf{V}})^{-1}$ . Therefore, $\\tilde{\\mathbf{V}}(\\tilde{\\mathbf{W}}^{T}\\tilde{\\mathbf{W}})\\tilde{\\mathbf{V}}^{T}$ is an orthogonal projection operator onto the column space of $\\tilde{\\textbf{V}}$ . If $\\mathbf{x}$ is a vectorized representation of any symmetric matrix, using Lemma B.1 the projection operator is an identity operator. Hence, its operator norm is 1. ", "page_idx": 17}, {"type": "text", "text": "C Sampling Operator Bounds and Proof of Tangent Space Restricted Isometry Properties ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we first state the matrix Bernstein [Tro11] concentration inequality for rectangular matrices, which is repeatedly used in the proofs further in the section. We then provide a proof for the bound of our sampling operator $\\mathcal{Q}_{\\Omega}$ and then we conclude this section by proving the RIP restricted to the tangent space of manifold of symmetric rank- $^r$ matrices at the ground truth ${\\bf X}^{0}$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem C.1 (Matrix Bernstein for rectangular matrices [Tro11, Theorem 1.6]). Consider the finite sequence $\\{\\mathbf{Z_{k}}\\}$ of independent, random matrices with dimension $d_{1}\\times d_{2}$ . Assume that each random matrix satisfies ", "page_idx": 17}, {"type": "text", "text": "Define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma^{2}:=\\operatorname*{max}\\left\\lbrace\\left\\|\\sum_{k}\\mathbb{E}[\\mathbf{Z_{k}}\\mathbf{Z_{k}^{*}}]\\right\\|,\\left\\|\\sum_{k}\\mathbb{E}[\\mathbf{Z_{k}^{*}}\\mathbf{Z_{k}}]\\right\\|\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for all $t\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\left\\{\\left\\|\\sum_{k}\\mathbf{Z_{k}}\\right\\|\\geq t\\right\\}\\leq(d_{1}+d_{2})\\cdot\\exp\\left(\\frac{-t^{2}/2}{\\sigma^{2}+R t/3}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.2 (Spectral Norm Bound for Sampling Operator $\\mathcal{Q}_{\\Omega}$ ). Let n be the dimension of the input matrix and let $\\boldsymbol{\\Omega}=(i\\ell,j_{\\ell})_{\\ell=1}^{m}\\subset\\mathbf{I}$ be a multiset of double indices fulfilling $m<L=n(n-1)/2$ that are sampled independently with replacement. Consequently, we have that with probability of at least $\\textstyle1-{\\frac{2}{n^{2}}}$ , the sampling operator $\\mathcal{Q}_{\\Omega}:S_{n}\\rightarrow S_{n}$ of (9) ", "page_idx": 17}, {"type": "text", "text": "fulfills ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\parallel Q_{\\Omega}\\parallel_{S_{\\infty}}\\le20L\\sqrt{\\frac{\\log n}{m}}+1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We define, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\Omega}^{\\,\\prime}=\\mathbf{V}\\mathbf{S}_{\\Omega}\\mathbf{W}^{\\top}}\\\\ {=\\displaystyle\\sum_{\\alpha\\in\\Omega}\\mathbf{v}_{\\alpha}\\mathbf{w}_{\\alpha}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So from (13), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\Omega}=\\displaystyle\\frac{L}{m}\\,\\mathcal{R}_{\\Omega}^{'},}\\\\ &{\\mathcal{Q}_{\\Omega}=\\mathcal{R}_{\\Omega}+\\mathbf{V}_{E}\\mathbf{W}_{E}^{\\top}}\\\\ &{\\mathcal{Q}_{\\Omega}^{'}=\\mathcal{R}_{\\Omega}^{'}+\\displaystyle\\frac{L}{m}\\mathbf{V}_{E}\\mathbf{W}_{E}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In order to provide a bound for $\\mathcal{Q}_{\\Omega}$ , we first evaluate a bound for ${\\mathcal{R}}_{\\Omega}$ . We would use concentration inequality of Theorem C.1, for computing the bound of $\\mathcal{Q}_{\\Omega}$ ", "page_idx": 18}, {"type": "text", "text": "Let us define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{z}_{k}=\\frac{L}{m}\\mathbf{v}_{\\alpha{k}}\\mathbf{w}_{\\alpha{k}}^{\\top}-\\frac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We compute the expectation of $\\mathbf{z}_{k}$ by, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{z}_{k}]=\\mathbb{E}[\\frac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\frac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top}]}\\\\ &{\\phantom{\\quad}\\;=\\displaystyle\\frac{1}{L}\\sum_{k=1}^{L}\\frac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\frac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top}}\\\\ &{\\phantom{\\quad}\\;=\\displaystyle\\frac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top}-\\frac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top}}\\\\ &{\\phantom{\\quad}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now let us compute the bound for $\\mathbf{z}_{k}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathbf z}_{k}\\|_{S_{\\infty}}=\\left\\|\\frac{L}{m}{\\mathbf v}_{\\alpha_{k}}{\\mathbf w}_{\\alpha_{k}}^{\\top}-\\frac{1}{m}{\\mathbf V}{\\mathbf W}^{\\top}\\right\\|_{S_{\\infty}}}\\\\ {\\displaystyle~~~~~~~\\leq\\left\\|\\frac{L}{m}{\\mathbf v}_{\\alpha_{k}}{\\mathbf w}_{\\alpha_{k}}^{\\top}\\right\\|_{S_{\\infty}}+\\frac{1}{m}\\left\\|{\\mathbf V}{\\mathbf W}^{\\top}\\right\\|_{S_{\\infty}}}\\\\ {\\displaystyle~~~~~~~\\leq\\left\\|\\frac{L}{m}{\\mathbf v}_{\\alpha_{k}}{\\mathbf w}_{\\alpha_{k}}^{\\top}\\right\\|_{S_{\\infty}}+\\frac{1}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use triangle inequality in the first inequality and since the spectral norm of $\\mathbf{V}\\mathbf{W}^{\\top}$ is 1, we arrive at the second inequality.   \nNow, we can further bound the spectral norm of $\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}$ by, ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\|_{S_{\\infty}}=\\operatorname*{max}\\frac{\\langle\\mathbf{V},\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{W}\\rangle}{||\\mathbf{V}||_{2}||\\mathbf{W}||_{2}}}\\\\ &{\\phantom{\\frac{1}{\\theta}}\\leq\\frac{||\\mathbf{v}_{\\alpha_{k}}||_{2}^{2}}{||\\mathbf{v}_{\\alpha_{k}}||_{2}}\\frac{||\\mathbf{w}_{\\alpha_{k}}||_{2}^{2}}{||\\mathbf{w}_{\\alpha_{k}}||_{2}}}\\\\ &{\\phantom{\\frac{1}{\\theta}}\\leq1\\cdot4=4}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So from (16), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vert\\vert\\mathbf{z}_{k}\\vert\\vert_{S_{\\infty}}\\leq\\frac{4L+1}{m}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We take the product of $\\mathbf{z}_{k}$ and $\\mathbf{z}_{k}^{\\ast}$ as, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{k}^{*}\\mathbf{z}_{k}=(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})^{\\top}(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})}\\\\ &{\\quad\\quad=(\\cfrac{L}{m}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{W}\\mathbf{V}^{\\top})(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})}\\\\ &{\\quad\\quad=\\cfrac{L^{2}}{m^{2}}(\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top})-\\cfrac{L}{m^{2}}(\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathbf{V}\\mathbf{W}^{\\top})-\\cfrac{L}{m^{2}}(\\mathbf{W}\\mathbf{V}^{\\top}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top})}\\\\ &{\\quad\\quad+\\cfrac{1}{m^{2}}(\\mathbf{W}\\mathbf{V}^{\\top}\\mathbf{V}\\mathbf{W}^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now taking expectation on both side of the above relation we get; ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|{\\mathbb E}[\\mu_{t}^{n},\\mathbf{u}]\\right|_{\\mathcal{S}_{n}}}&{=\\left\\|\\left[\\frac{L}{n}\\overline{{\\left(w_{n}w_{n}^{*}\\gamma_{n}w_{n}w_{n}^{*}\\right)}}\\right]+{\\mathbb E}\\left[\\frac{L}{n^{2}}(w_{n}w_{n}^{*}\\mathbf{v}(\\mathbf{v}))\\right]\\right\\|_{\\mathcal{S}_{n}}}\\\\ &{\\quad+\\left\\|\\frac{L}{n^{2}}(\\mathbf{W}^{*}\\gamma_{n}w_{n}^{*}\\mathbf{v}_{n}w_{n}^{*})\\right\\|_{\\mathcal{S}_{n}}+\\left\\|\\frac{1}{n^{2}}(\\mathbf{W}^{*}\\mathbf{V}^{*}\\mathbf{W}^{*})\\right\\|_{\\mathcal{S}_{n}}}\\\\ &{\\quad+\\left\\|\\frac{L}{n^{2}}(\\mathbf{W}^{*}\\gamma_{n}w_{n}^{*}\\mathbf{v}_{n}w_{n}^{*})\\right\\|_{\\mathcal{S}_{n}}+\\left\\|\\frac{L}{n^{2}}\\left[\\overline{{\\left(w_{n}w_{n}^{*}\\gamma_{n}w_{n}^{*}\\right)}}\\right]\\right\\|_{\\mathcal{S}_{n}}}\\\\ &{\\quad+\\left\\|\\frac{L}{n^{2}}\\left[\\overline{{\\left(w_{n}w_{n}^{*}\\gamma_{n}w_{n}w_{n}^{*}\\right)}}\\right]\\right\\|_{\\mathcal{S}_{n}}+\\left\\|\\frac{L}{n^{2}}\\left[\\overline{{\\left(w_{n}^{*}\\gamma_{n}w_{n}^{*}\\gamma_{n}^{*}\\right)}}\\right]\\right\\|_{\\mathcal{S}_{n}}}\\\\ &{\\quad+\\left\\|\\frac{L^{2}}{n^{2}}\\left[\\frac{L}{L^{2}}\\overline{{\\left(w_{n}w_{n}^{*}\\gamma_{n}w_{n}w_{n}^{*}\\right)}}\\right]\\right\\|_{\\mathcal{S}_{n}}+\\left\\|\\frac{L}{n}\\overline{{\\left(\\frac{L}{n^{2}}\\overline{{\\left(w_{n}^{*}\\gamma_{n}w_{n}^{*}\\right)}}\\right)}}\\right\\|_{\\mathcal{S}_{n}}}\\\\ &{\\quad+\\left\\|\\frac{L}{n^{2}}\\left[\\overline{{\\left(w_{n}w_{n}^{*}\\gamma_{n}w \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since expectation is a linear operator we can preserve the equality in the first equality, then we use triangle inequality in the first inequality. We further use that the spectral norm of $\\bar{\\mathbf{W}}\\mathbf{V}^{\\top}$ is 1 and arrive at the second inequality. Further using (17) we bound the expression in the last inequality. As per Theorem C.1 we now compute the product of $\\mathbf{z}_{k}$ and $\\mathbf{z}_{k}^{\\ast}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{k}\\mathbf{z}_{k}^{*}=(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})^{\\top}}\\\\ &{\\quad\\quad=(\\cfrac{L}{m}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{V}\\mathbf{W}^{\\top})(\\cfrac{L}{m}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}-\\cfrac{1}{m}\\mathbf{W}\\mathbf{V}^{\\top})}\\\\ &{\\quad\\quad=\\cfrac{L^{2}}{m^{2}}(\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top})-\\cfrac{L}{m^{2}}(\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{W}\\mathbf{V}^{\\top})}\\\\ &{\\quad\\quad-\\cfrac{L}{m^{2}}(\\mathbf{V}\\mathbf{W}^{\\top}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top})+\\cfrac{1}{m^{2}}(\\mathbf{V}\\mathbf{W}^{\\top}\\mathbf{W}\\mathbf{V}^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now taking expectation on both side of the above relation we get; ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\|_{S_{x}}=}&{\\left\\|\\frac{L^{2}}{m^{2}}[\\frac{1}{L}\\frac{L}{\\sum_{k=1}^{K}[\\mathbf{o}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}]}]\\right\\|_{S_{\\alpha_{n}}}}\\\\ &{+\\left\\|\\frac{1}{L}\\frac{L}{k-1}\\frac{L}{m^{2}}(\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{W}\\mathbf{V}^{\\top})\\right\\|_{S_{\\alpha_{n}}}}\\\\ &{+\\left\\|\\frac{1}{L}\\frac{L}{m^{2}}(\\mathbf{v}\\mathbf{W}^{\\top}\\mathbf{w}(\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top})\\right\\|_{S_{\\alpha_{n}}}+\\frac{1}{m^{2}}}\\\\ &{\\leq\\frac{L^{2}}{m^{2}}\\operatorname*{max}\\left\\|\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\right\\|_{S_{\\alpha_{n}}}+\\left\\|\\frac{L}{m^{2}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathbf{W}\\mathbf{V}^{\\top}\\right\\|_{S_{\\alpha_{n}}}}\\\\ &{+\\left\\|\\frac{L}{m^{2}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathbf{W}\\mathbf{V}^{\\top}\\right\\|_{S_{\\alpha_{n}}}+\\frac{1}{m^{2}}}\\\\ &{\\leq\\frac{16L^{2}+2L+1}{m^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So, now by Matrix Bernstein Inequality as stated Theorem C.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}(\\mathbf{z}_{k})=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The value of the bound of $\\mathbf{z}_{k}$ is denoted by $\\mathbf{R}$ in Theorem C.1. So R in this case is ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\mathbf{z}_{k}||_{S_{\\infty}}\\leq R=\\frac{4L+1}{m}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Further $\\sigma$ is defined by $\\begin{array}{r}{\\operatorname*{max}[\\sum_{k}\\mathbb{E}({\\mathbf{z}_{k}\\mathbf{z}_{k}^{*}}),\\sum_{k}\\mathbb{E}({\\mathbf{z}_{k}^{*}\\mathbf{z}_{k}})]}\\end{array}$ as per Theorem C.1. We have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sigma^{2}=m\\cdot{\\frac{16L^{2}+2L+1}{m^{2}}}={\\frac{16L^{2}+2L+1}{m}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So as per Theorem C.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall t>0,P(\\lvert\\lvert\\sum_{k=1}^{L}\\mathbf z_{k}\\rvert\\rvert_{S_{\\infty}}\\geq t)\\leq2n\\exp(\\frac{\\frac{-t^{2}}{2}}{\\sigma^{2}+\\frac{R t}{3}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Recalling the definition of $\\mathbf{z}_{k}$ from (14) ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\sum_{k=1}^{L}\\mathbf{z}_{k}||_{S_{\\infty}}\\leq t\\implies||\\frac{L}{m}\\,\\mathbf{\\mathcal{Q}}_{\\Omega}'\\!-\\!\\mathbf{V}\\mathbf{W}^{\\top}||_{s_{\\infty}}\\leq t\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\mathbf{\\nabla}Q_{\\Omega}\\|_{s_{\\infty}}=\\|\\frac{L}{m}\\,\\mathcal{Q}_{\\Omega}\\mathbf{\\dot{\\omega}}\\|_{s_{\\infty}}}}\\\\ &{\\quad\\quad\\leq\\|\\frac{L}{m}\\,\\mathcal{Q}_{\\Omega}\\mathbf{\\dot{\\omega}}^{\\prime}\\!-\\!\\mathbf{V}\\mathbf{W}^{\\top}\\vert\\vert_{s_{\\infty}}+\\vert|\\mathbf{V}\\mathbf{W}^{\\top}\\vert\\vert_{s_{\\infty}}}\\\\ &{\\quad\\quad\\leq t+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with a probability $\\begin{array}{r}{1-2n\\exp(\\frac{-t^{2}}{\\sigma^{2}+\\frac{R t}{3}})}\\end{array}$ . We have arrived at the above inequality from the definition of $\\mathcal{Q}_{\\Omega}$ and that the spectral norm of the extended basis of the dual pair, $\\mathbf{V}_{E}\\mathbf{W}_{E}^{\\top}$ is 1. Since L = n(n\u22121) so, $\\begin{array}{r}{L^{2}\\leq\\frac{n^{2}L}{2}}\\end{array}$ . We can further simplify the denominator by the following, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\left(\\sigma^{2}+\\displaystyle\\frac{R t}{3}\\right)=\\displaystyle\\frac{16L^{2}+2L+1}{m}+\\displaystyle\\frac{4L+1}{3m}t}}\\\\ {\\displaystyle\\le\\displaystyle\\frac{16L^{2}}{m}+\\displaystyle\\frac{2L}{m}+\\displaystyle\\frac{L}{m}+\\displaystyle\\frac{2L t}{m}}\\\\ {\\displaystyle\\le\\displaystyle\\frac{L}{m}(16L+3+2t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We arrive at the first inequality by using $\\textstyle{\\frac{1}{m}}\\leq{\\frac{L}{m}}$ and $\\textstyle{\\frac{4L+1}{3m}}\\leq{\\frac{6L}{3m}}$   \nSo using this relation, let us assume that $\\textstyle t=20L{\\sqrt{\\frac{\\log n}{m}}}$ .   \nwhere Further let us simply Equation (23), by using this value of t. Since by Theorem 4.3, , so , hence we can say simplify Equation (23), further b $m\\geq K n\\log n$ , $K=C\\nu r$ $\\frac{\\log n}{m}\\leq\\frac{1}{K n}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(\\sigma^{2}+\\frac{R t}{3}\\right)\\le\\frac{L}{m}(16L+3+2t)}\\quad}&{}\\\\ &{\\leq\\frac{L}{m}\\left(16L+3+2\\cdot20L\\sqrt{\\frac{\\log n}{m}}\\right)}\\\\ &{\\leq\\frac{L}{m}\\left(16L+3+2\\cdot20L\\sqrt{\\frac{1}{K n}}\\right)}\\\\ &{\\leq\\frac{L}{m}\\left(60L\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "TWhee anr, riwvee  caat nt hdee tdhuircde  itnhee qfuoallliotwy ibnug  ufrsionmg  tEhqeu eatxiporne s(s2i4o)n, for $\\mathrm{L}$ and using $\\frac{l o g n}{m}\\leq\\frac{1}{K n}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(\\frac{-\\frac{t^{2}}{2}}{\\sigma^{2}+\\frac{R t}{3}}\\right)=\\exp\\left({\\frac{-\\frac{20^{2}L^{2}\\log n}{2m}}{\\frac{60L^{2}}{8m^{2}}}}\\right)}\\\\ &{\\phantom{\\exp x}\\leq\\exp\\left(-\\frac{400}{120}\\log n\\right)}\\\\ &{\\phantom{\\exp x}=\\exp(3.33\\log(\\frac{1}{n}))}\\\\ &{\\phantom{\\exp x}\\leq\\exp(\\log(\\frac{1}{n^{3}}))}\\\\ &{\\phantom{\\exp x}=\\frac{1}{n^{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So the probability in (21) becomes $2n\\cdot{\\frac{1}{n^{3}}}$ If we use (22), then we can conclude that that the spectral norm of $\\mathcal{Q}_{\\Omega}$ is bounded by $\\left(20L\\sqrt{\\frac{\\log n}{m}}+1\\right)$ logm n+ 1 . Hence, we can say that the spectral norm of Q\u2126is bounded by $\\left(20L\\sqrt{\\frac{\\log n}{m}}+1\\right)$ logm n+ 1 with a probability of 1 \u2212n22 . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Now we prove the restricted isometry property of the sampling operator $\\mathcal{Q}_{\\Omega}$ with respect to the tangent space of the rank- $^r$ matrix manifold at the ground truth, as stated in Theorem 4.5. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 4.5. Define $\\begin{array}{r}{z_{k}=\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}.}\\end{array}$ Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(\\mathbf{z}_{k})=\\mathbb{E}\\left(\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\right)}\\\\ &{\\quad\\quad=\\displaystyle\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left(\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\ell}\\mathbf{v}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\right)}\\\\ &{\\quad\\quad=\\displaystyle\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}}\\\\ &{\\quad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we calculate the spectral norm of $\\mathbf{z}_{k}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\mathbf{z}_{k}\\right\\rVert_{S_{\\infty}}=\\left\\lVert\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{S_{\\infty}}}\\\\ &{\\leq\\left\\lVert\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{S_{\\infty}}+\\frac{1}{m}\\left\\lVert\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{S_{\\infty}}}\\\\ &{\\leq\\frac{L}{m}\\lVert\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\rVert_{F}\\lVert\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\rVert_{F}+\\frac{1}{m}}\\\\ &{\\leq\\frac{2L}{m}\\sqrt{\\frac{2\\nu r}{n}}\\sqrt{\\frac{2\\nu r}{n}}+\\frac{1}{m}}\\\\ &{\\leq\\frac{4\\nu r L}{m n}+\\frac{1}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The third inequality follows from the coherence bounds as in Definition 4.1 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{k}^{*}\\mathbf{z}_{k}=(\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}})^{\\top}(\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}})}\\\\ &{\\mathrm{~\\~\\}=(\\frac{L^{2}}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}})-(\\frac{L}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}})}\\\\ &{\\mathrm{~\\~\\}-(\\frac{L}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}})+\\frac{1}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We use that $\\mathcal{P}_{T_{0}}^{2}\\,=\\,\\mathcal{P}_{T_{0}}$ in the second equality to further calculate the expectation of the above equality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})=\\mathbb{E}(\\frac{L^{2}}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}})-\\mathbb{E}((\\frac{L}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{v}^{\\top}\\mathcal{P}_{T_{0}})}\\\\ &{\\phantom{\\mathrm{\\tiny{()}}}-\\mathbb{E}((\\frac{L}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}})))+\\mathbb{E}(\\frac{1}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}})}\\\\ &{\\phantom{\\mathrm{\\tiny{()}}}=\\mathbb{E}(\\frac{L^{2}}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}})-\\frac{1}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}}\\\\ &{\\phantom{\\mathrm{\\tiny{()}}}-\\frac{1}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}+\\frac\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $\\begin{array}{r}{\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}=I d}\\end{array}$ , we can adjust summand at the seocond equality. ", "page_idx": 22}, {"type": "text", "text": "Defining the random operator $\\begin{array}{r}{\\widetilde{\\mathbf{z}}_{k}:=\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}}\\end{array}$ , we see that $\\mathbf{z}_{k}$ from above satisfies $\\mathbf{z}_{k}=$ $\\begin{array}{r}{\\widetilde{\\mathbf{z}}_{k}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}}\\end{array}$ and therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\widetilde{\\mathbf{z}}_{k}]=\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With this definition, and for the fact that positive semi-definite matrices satisfies ", "page_idx": 22}, {"type": "text", "text": "$\\|A-B\\|_{S_{\\infty}}\\leq\\operatorname*{max}(\\|A\\|_{S_{\\infty}}\\,,\\|B\\|_{S_{\\infty}})$ , we can bound the spectral norm of the expectation in the following w\u221eay ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})\\right\\|_{S_{\\infty}}=\\left\\|\\mathbb{E}\\left[\\widetilde{\\mathbf{z}}_{k}^{*}\\widetilde{\\mathbf{z}}_{k}\\right]-\\frac{1}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\mathcal{P}_{T_{0}}\\mathbf{V}\\mathbf{W}^{\\top}\\mathcal{P}_{T_{0}}\\right\\|_{S_{\\infty}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{max}\\left(\\left\\|\\mathbb{E}\\left[\\widetilde{\\mathbf{z}}_{k}^{*}\\widetilde{\\mathbf{z}}_{k}\\right]\\right\\|_{S_{\\infty}},\\frac{1}{m^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any $\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$ , it follows from the definition of $\\widetilde{\\mathbf{z_{k}}}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{z}_{k}(\\mathbf{M})=\\frac{L}{m}\\langle\\mathbf{v}_{\\alpha_{k}},\\mathcal{P}_{T_{0}}(\\mathbf{M})\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha_{k}})=\\frac{L}{m}\\langle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha_{k}}),\\mathbf{M}\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha_{k}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{z}_{k}^{*}(\\mathbf{M})=\\frac{L}{m}\\langle\\mathbf{w}_{\\alpha_{k}},\\mathcal{P}_{T_{0}}(\\mathbf{M})\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha_{k}})=\\frac{L}{m}\\langle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha_{k}}),\\mathbf{M}\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha_{k}})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widetilde{\\mathbf{z}}_{k}^{*}\\widetilde{\\mathbf{z}}_{k}({\\mathbf{M}})=\\frac{L}{m}\\langle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha_{k}}),\\widetilde{\\mathbf{z}}_{k}({\\mathbf{M}})\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha{}_{k}})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~=\\frac{L^{2}}{m^{2}}\\langle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha{}_{k}}),\\langle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha{}_{k}}),\\mathbf{M}\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha{}_{k}})\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha{}_{k}})}\\ ~}\\\\ {{\\displaystyle~~~~~~~=\\frac{L^{2}}{m^{2}}\\lVert\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\alpha{}_{k}})\\rVert_{F}^{2}\\langle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha{}_{k}}),\\mathbf{M}\\rangle\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha{}_{k}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now using the incoherence condition such as Definition 4.1 and [TL18, Lemma 22]. we can argue that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\mathbb{E}\\left[\\widetilde{\\mathbf{z}}_{k}^{*}\\widetilde{\\mathbf{z}}_{k}\\right]\\right\\rVert_{S_{\\infty}}\\leq\\frac{L^{2}}{m^{2}}\\operatorname*{max}_{\\ell=1}^{L}\\left\\lVert\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\ell})\\right\\rVert_{F}^{2}\\left\\lVert\\mathbb{E}\\left[\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\right]\\right\\rVert_{S_{\\infty}}}\\\\ &{\\ =\\frac{L^{2}}{m^{2}}\\operatorname*{max}_{\\ell=1}^{\\mathrm{max}}\\left\\lVert\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\ell})\\right\\rVert_{F}^{2}\\left\\lVert\\frac{L}{L}\\sum_{\\ell=1}^{L}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\ell}\\mathbf{v}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{S_{\\infty}}}\\\\ &{\\ \\leq\\frac{L^{2}}{m^{2}}\\operatorname*{max}_{\\ell=1}^{L}\\left\\lVert\\mathcal{P}_{T_{0}}(\\mathbf{w}_{\\ell})\\right\\rVert_{F}^{2}\\operatorname*{max}_{\\ell=1}^{L}\\left\\lVert\\mathcal{P}_{T_{0}}(\\mathbf{v}_{\\alpha_{k}})\\right\\rVert_{s_{\\infty}}^{2}}\\\\ &{\\ \\leq\\frac{L^{2}}{m^{2}}(2\\nu\\frac{r}{n})\\frac{1}{L}(8\\nu\\frac{r}{n})}\\\\ &{=\\frac{L}{m^{2}}\\mathrm{l}6\\frac{\\nu^{2}r^{2}}{n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we get from (27) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})\\|_{S_{\\infty}}\\leq\\operatorname*{max}\\left(\\frac{L}{m^{2}}(16\\frac{\\nu^{2}r^{2}}{n^{2}}),\\frac{1}{m^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So by Triangle Inequality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})\\right\\|_{S_{\\infty}}\\leq\\sum_{k=1}^{m}\\|\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})\\|_{S_{\\infty}}\\leq m\\cdot\\frac{L}{m^{2}}\\bigg(16\\frac{\\nu^{2}r^{2}}{n^{2}}\\bigg)=\\frac{L}{m}\\bigg(16\\frac{\\nu^{2}r^{2}}{n^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we need to compute similar bounds for $\\mathbb{E}[{\\mathbf{z}}_{k}{\\mathbf{z}}_{k}^{*}]$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\mathbb{E}[\\widetilde{\\mathbf{z}}_{k}\\widetilde{\\mathbf{z}}_{k}^{\\ast}]\\right\\rVert_{s_{\\infty}}=\\left\\lVert\\mathbb{E}\\left[\\frac{L^{2}}{m^{2}}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\mathbf{v}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\mathbf{w}_{\\alpha_{k}}^{\\top}\\mathcal{P}_{T_{0}}\\right]\\right\\rVert_{s_{\\infty}}}\\\\ &{=\\left\\lVert\\frac{L^{2}}{m^{2}}\\sum_{\\ell=1}^{L}\\frac{1}{L}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\ell}\\mathbf{v}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\ell}\\mathbf{w}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{s_{\\infty}}}\\\\ &{\\le\\frac{L}{m}\\operatorname*{max}{\\lVert\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\ell}\\mathbf{v}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\ell}\\mathbf{w}_{\\ell}^{\\top}\\mathcal{P}_{T_{0}}\\rVert_{s_{\\infty}}}}\\\\ &{\\le\\frac{L}{m}\\operatorname*{max}{\\lVert\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\alpha_{k}}\\rVert_{s_{\\infty}}^{2}\\operatorname*{max}{\\lVert\\mathcal{P}_{T_{0}}\\mathbf{v}_{\\alpha_{k}}\\rVert_{s_{\\infty}}^{2}}}}\\\\ &{\\le\\frac{L}{m^{2}}(2\\nu_{\\mathfrak{n}}^{2})(8\\nu_{\\mathfrak{n}}^{2})}\\\\ &{=\\frac{L}{m^{2}}{16}\\mathfrak{b}\\frac{\\nu^{2}r^{2}}{n^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So similar to (29) we apply Triangle Inequality on (30) and get, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\right\\|_{S_{\\infty}}\\leq\\sum_{k=1}^{m}\\|\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\|_{S_{\\infty}}\\leq m\\cdot\\frac{L}{m^{2}}\\bigg(16\\frac{\\nu^{2}r^{2}}{n^{2}}\\bigg)=\\frac{L}{m}\\bigg(16\\frac{\\nu^{2}r^{2}}{n^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we can approximate this bound in the following way, ", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{L=\\frac{n(n-1)}{2}}\\end{array}$ , so $\\begin{array}{r}{L\\leq\\frac{n^{2}}{2}}\\end{array}$ this bound can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\right\\|_{S_{\\infty}}\\leq\\frac{L}{m}\\bigg(16\\frac{\\nu^{2}r^{2}}{n^{2}}\\bigg)\\leq8\\frac{\\nu^{2}r^{2}}{m}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Comparing the equations (31) and (32), we can argue that the bound on (31) is larger than that of (32) as $\\begin{array}{l}{{\\frac{L}{2}}}\\end{array}$ is larger than $\\nu r$ . So ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\right\\|_{S_{\\infty}}\\leq8\\frac{\\nu^{2}r^{2}}{m}\\leq4\\frac{\\nu r L}{m}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So, now by Matrix Bernstein Inequality as stated Theorem C.1, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}(\\mathbf{z}_{k})=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The value of the bound of $\\mathbf{z}_{k}$ is denoted by $\\mathbf{R}$ in Theorem C.1. So R in this case is ", "page_idx": 24}, {"type": "equation", "text": "$$\n||\\mathbf{z}_{k}||_{S_{\\infty}}\\leq R=\\frac{4\\nu r L}{m n}+\\frac{1}{m}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further $\\sigma$ is defined by max $\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*}),\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})]$ as per Theorem C.1. We have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}=\\operatorname*{max}\\left[\\left\\Vert\\displaystyle\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}\\mathbf{z}_{k}^{*})\\right\\Vert_{S_{\\infty}},\\left\\Vert\\displaystyle\\sum_{k=1}^{m}\\mathbb{E}(\\mathbf{z}_{k}^{*}\\mathbf{z}_{k})\\right\\Vert_{S_{\\infty}}\\right]}\\\\ &{\\sigma^{2}=\\displaystyle\\frac{L}{m}\\!\\left(\\frac{4\\nu r}{n}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So as per Theorem C.1, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall t>0,P(\\lvert\\lvert\\sum_{k=1}^{L}\\mathbf z_{k}\\rvert\\rvert_{S_{\\infty}}\\geq t)\\leq2n\\exp(\\frac{\\frac{-t^{2}}{2}}{\\sigma^{2}+\\frac{R t}{3}})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we would calculate the bound for the above probability, In this scenario, since $\\begin{array}{r}{L=\\frac{n(n-1)}{2}}\\end{array}$ , we can bound $\\textstyle(\\sigma^{2}+{\\frac{R\\epsilon}{3}})$ by the following inequality in the second, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\sigma^{2}+\\frac{R\\epsilon}{3}\\right)=\\frac{L}{m}\\left(\\frac{4\\nu r}{n}\\right)+\\left(\\frac{4\\nu r L}{m n}+\\frac{1}{m}\\right)\\frac{\\epsilon}{3}}&{{}}\\\\ {=\\left(\\frac{4\\nu r L}{n m}\\right)\\left(1+\\frac{\\epsilon}{3}\\right)+\\frac{\\epsilon}{3m}}&{{}}\\\\ {\\leq\\left(\\frac{8\\nu r L}{n m}\\right)+\\frac{\\epsilon}{3m}}&{{}}\\\\ {\\leq\\left(\\frac{9\\nu r L}{n m}\\right)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "sWece ounsde $\\frac{\\epsilon}{3}$ eiqs ulaelsits yt.han 1 in the first inequality and the the second summand is less than $\\frac{\\nu r L}{n m}$ in the Here further we used that $\\epsilon\\leq\\frac{1}{2}$ and $\\nu\\geq1,\\,r\\geq1$ and bound the probability as follows, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2n\\exp\\left(\\frac{-\\frac{\\epsilon^{2}}{2}}{\\sigma^{2}+\\frac{R\\epsilon}{3}}\\right)\\leq2n\\exp\\left(\\frac{-\\frac{\\epsilon^{2}}{2}}{\\frac{9\\nu r L}{n m}}\\right)}&{}\\\\ {=2n\\exp\\left(\\frac{-\\epsilon^{2}n m}{18\\nu r L}\\right)}&{}\\\\ {\\leq2n\\exp\\left(\\frac{-49\\nu r n^{2}\\log n}{18\\nu r L}\\right)}&{}\\\\ {\\leq2n\\exp\\left(\\frac{-98\\log n}{18}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So, given that $\\begin{array}{r}{m\\geq\\frac{49\\nu n r}{\\epsilon^{2}}\\log n}\\end{array}$ holds true, we can arrive at the first inequality above, further, since $\\begin{array}{r}{L=\\frac{n(n-1)}{2}}\\end{array}$ so we can say, $L\\leq n^{2}/2$ hence we get the second inequality. We can bound this further by, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2n\\exp\\left(\\frac{-\\frac{\\epsilon^{2}}{2}}{\\sigma^{2}+\\frac{R\\epsilon}{3}}\\right)\\leq2n\\exp\\left(\\frac{-98\\log n}{18}\\right)}\\\\ &{\\phantom{2n\\exp x}\\leq2n\\exp\\left(-4\\log n\\right)}\\\\ &{\\phantom{2n\\exp x}\\leq2n\\exp\\left(\\log\\left(\\frac{1}{n}\\right)^{2}\\right)}\\\\ &{\\phantom{2n\\exp x}=\\frac{2}{n^{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence we can conclude from here that $||\\sum_{k=1}^{m}\\mathbf{z}_{k}||_{S_{\\infty}}\\leq\\epsilon)$ holds with a probability of $\\textstyle1-{\\frac{2}{n^{3}}}$ . Now we can derive the bound $\\left\\|\\mathcal{P}_{T_{0}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\right\\|_{S_{\\infty}}\\le\\epsilon$ in the statement of Theorem 4.5 by the following argument, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\displaystyle\\sum_{k=1}^{m}\\mathbf{z}_{k}\\right\\rvert_{s_{\\infty}}=\\left\\lVert\\displaystyle\\sum_{\\ell=1}^{L}(\\frac{L}{m}\\mathcal{P}_{T_{0}}\\mathbf{w}_{\\ell}\\mathbf{v}_{\\alpha_{\\ell}}^{\\top}\\mathcal{P}_{T_{0}}-\\frac{1}{m}\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}})\\right\\rVert_{s_{\\infty}}}&{}\\\\ {\\displaystyle=\\left\\lVert\\mathcal{P}_{T_{0}}\\,\\mathcal{R}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}\\right\\rVert_{s_{\\infty}}}&{}\\\\ {\\displaystyle=\\left\\lVert\\mathcal{P}_{T_{0}}\\,\\mathcal{R}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{0}}+\\mathcal{P}_{T_{0}}\\mathbf{w}_{E}\\mathbf{v}_{E}^{\\top}\\mathcal{P}_{T_{0}}-(\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}+\\mathcal{P}_{T_{0}}\\mathbf{w}_{E}\\mathbf{v}_{E}^{\\top}\\mathcal{P}_{T_{0}})\\right\\rVert_{S_{\\infty}}}&{}\\\\ {\\displaystyle=\\left\\lVert\\mathcal{P}_{T_{0}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\right\\rVert_{S_{\\infty}}\\le\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We get the get implication by adjusting the equation with addition and subtraction of $\\mathcal{P}_{T_{0}}\\mathbf{w}_{E}\\mathbf{v}_{E}^{\\top}\\mathcal{P}_{T_{0}}$ because $\\mathcal{P}_{T_{0}}\\mathbf{W}\\mathbf{V}^{\\top}\\mathcal{P}_{T_{0}}+\\mathcal{P}_{T_{0}}\\mathbf{w}_{E}\\mathbf{v}_{E}^{\\top}\\mathcal{P}_{T_{0}}=\\mathcal{P}_{T_{0}}\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}\\mathcal{P}_{T_{0}}$ and $\\tilde{\\mathbf{W}}\\tilde{\\mathbf{V}}^{\\top}\\mathcal{P}_{T_{0}}=\\mathcal{P}_{T_{0}}$ So, we have proved the assertion of Theorem 4.5 is true with a probability of $\\textstyle1-{\\frac{2}{n^{3}}}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Following Proposition 3.1 in [RXH11], if we change the sampling model to sampling without replacement we should have the same bound with same failure probability. ", "page_idx": 25}, {"type": "text", "text": "D Proof of Local Quadratic Convergence ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we provide the proof of the local convergence theorem as stated in Theorem 4.3. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.1. Let $0<\\epsilon\\le\\frac{1}{2}$ , let ${\\mathbf{X}}^{0}\\in S_{n}$ be a $\\nu$ -incoherent matrix. Let $\\mathcal P_{T_{0}}:S_{n}\\rightarrow S_{n}$ be the projection operator associated to $T_{0}$ . Then assume that the following three conditions hold: ", "page_idx": 25}, {"type": "text", "text": "(a) For $\\mathcal{Q}_{\\Omega}:S_{n}\\rightarrow S_{n}$ be defined as in (13) from m independent uniformly sampled locations, we have : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\parallel Q_{\\Omega}\\parallel_{S_{\\infty}}\\parallel_{\\Omega}\\parallel_{\\Omega}\\parallel_{\\Omega}\\parallel_{\\Omega}\\parallel_{\\Omega}\\parallel_{\\Omega}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(b) The tangent space $T_{0}=T\\mathbf{\\vec{x}}^{\\mathrm{{0}}}$ onto the rank- $^r$ manifold $T_{0}=T_{\\mathbf{X}^{0}}\\mathbf{\\mathcal{M}}_{r}$ fulfills : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{T_{0}}\\stackrel{{}_{\\textstyle()}}{\\longrightarrow}\\mathcal{P}_{\\Omega}\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\right\\|_{S_{\\infty}}\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(c) The spectral norm distance between $\\mathbf{X}$ and $\\mathbf{X}^{0}$ fulfills: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{X}-\\mathbf{X}^{0}\\big\\|_{S_{\\infty}}\\leq\\frac{\\epsilon}{\\left(20L\\sqrt{\\frac{\\log n}{m}}+1\\right)}\\sigma_{r}(\\mathbf{X}^{0})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the tangent space $T=T\\mathbf{x}$ onto the rank- $r$ manifold at $\\mathbf{X}$ fulfills: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{T}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T}-\\mathcal{P}_{T}\\right\\|_{S_{\\infty}}\\leq4\\varepsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma D.1 is a literal adaptation of [KV21, Lemma B.3] to the operator $\\boldsymbol{\\mathcal{Q}}_{\\Omega}^{*}$ , which is why we omit its proof. Similarly, we can use Lemmas B.8 and B.9 of [KV21] in the same way for our proofs. ", "page_idx": 26}, {"type": "text", "text": "Lemma D.2. Let $\\mathbf{X}^{0}\\,\\in\\,S_{n}$ be a matrix of rank $r$ that is $\\nu$ -incoherent, and let $\\Omega\\,=\\,(i_{\\ell},j_{\\ell})_{\\ell=1}^{m}$ be a random index set of cardinality $|\\Omega|=m$ that is sampled uniformly without replacement, $o r,$ alternatively, sampled independently with replacement. There exists constants $C,\\widetilde{C},C_{1}$ such that $i f$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nm\\geq C\\nu r n\\log n\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then, with probability at least $\\textstyle1-{\\frac{2}{n^{2}}}$ , the following holds: For each matrix $\\mathbf{X}^{(k)}\\in S_{n}$ fulfilling ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathbf{X}^{(k)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\le C_{1}\\frac{\\sqrt{m}}{L\\sqrt{\\log n}}\\sigma_{r}(\\mathbf{X}^{0}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "it follows that the projection $\\mathcal{P}_{T_{k}}:S_{n}\\rightarrow S_{n}$ onto the tangent space $T_{k}:=T_{\\mathcal{T}_{r}(\\mathbf{X}^{(k)})}\\mathcal{M}_{r}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}-\\mathcal{P}_{T_{k}}\\right\\|_{S_{\\infty}}\\leq\\frac{2}{5},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and furthermore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\eta\\|_{F}\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for each matrix $\\eta\\in\\ker\\mathcal{Q}_{\\Omega}$ in the null space of the operator $\\mathcal{Q}_{\\Omega}:S_{n}\\rightarrow S_{n}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma D.2. Assume that there are $m$ locations $\\Omega=(i_{\\ell},j_{\\ell})_{\\ell=1}^{m}$ in $n\\times n$ sampled independently uniformly with replacement, where $m$ fulfills (38) with $C:=49/\\varepsilon^{2}$ and $\\varepsilon=0.1$ . By Lemma C.2, it follows that the corresponding operator $\\mathcal{Q}_{\\Omega}$ from Lemma C.2 fulfills ", "page_idx": 26}, {"type": "equation", "text": "$$\n||\\,\\mathcal{Q}_{\\Omega}\\,\\|_{S_{\\infty}}\\leq\\Big(20L\\sqrt{\\frac{\\log n}{m}}+1\\Big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "on an event called ${\\bf e}\\Omega$ , which occurs with a probability of at least $\\textstyle1-{\\frac{2}{n}}$ , and by Theorem 4.5, the tangent space $T_{0}=T_{\\mathbf{X}^{0}}\\mathbf{\\mathcal{M}}_{r}$ corresponding to the $\\mu_{0}$ -incoherent rank- $^r$ matrix ${\\bf X}^{0}$ fulfills ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{T_{0}}\\boldsymbol{\\mathcal{Q}}_{\\Omega}^{*}\\mathcal{P}_{T_{0}}-\\mathcal{P}_{T_{0}}\\right\\|_{S_{\\infty}}\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "on an event called ${\\bf e}_{\\Omega,T_{0}}$ , which occurs with a probability of at least $1\\mathrm{~-~}n^{-2}$ . Let $\\tilde{\\epsilon}\\ =\\ \\textstyle{\\frac{1}{10}}$ . If $\\mathbf{X}^{(k)}\\in\\mathbb{R}^{n\\times n}$ is such that $\\|\\mathbf{X}^{(k)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\le\\widetilde{\\xi}\\sigma_{r}(\\mathbf{X}^{0})$ with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\xi}=\\frac{\\widetilde{\\epsilon}}{\\left(20L\\sqrt{\\frac{\\log n}{m}}+1\\right)}\\leq\\frac{1}{\\left(20\\cdot10L\\sqrt{\\frac{\\log n}{m}}\\right)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "it follows by Lemma D.1 that on the event $E_{\\Omega}\\cap E_{\\Omega,T_{0}}$ , the tangent space $T_{k}:=\\mathbf{X}^{(k)}$ onto the rank- $r$ manifold at $\\mathbf{X}^{(k)}$ fulfills ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{T_{k}}\\boldsymbol{\\mathcal{Q}}_{\\Omega}^{*}\\mathcal{P}_{T_{k}}-\\mathcal{P}_{T_{k}}\\right\\|_{S_{\\infty}}\\leq4\\tilde{\\epsilon}=\\frac{2}{5}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, we claim that on the event $E_{\\Omega}\\cap E_{\\Omega,T_{0}}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\eta\\|_{F}\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any for each matrix $\\eta\\in\\ker\\mathcal{Q}_{\\Omega}$ in the null space of the operator $\\mathcal{Q}_{\\Omega}:S_{n}\\rightarrow S_{n}$ . ", "page_idx": 26}, {"type": "text", "text": "Let $\\eta\\in\\ker\\mathcal{Q}_{\\Omega}$ . ", "page_idx": 26}, {"type": "text", "text": "Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}^{2}=\\langle\\mathcal{P}_{T_{k}}(\\eta),\\mathcal{P}_{T_{k}}(\\eta)\\rangle}\\\\ &{\\qquad\\qquad=\\left\\langle\\mathcal{P}_{T_{k}}(\\eta),\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}(\\eta)\\right\\rangle+\\left\\langle\\mathcal{P}_{T_{k}}(\\eta),\\mathcal{P}_{T_{k}}(\\eta)-\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}(\\eta)\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left\\langle\\mathcal{P}_{T_{k}}(\\eta),\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}(\\eta)\\right\\rangle+\\|\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}\\left\\|\\mathcal{P}_{T_{k}}-\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}\\right\\|_{S_{\\infty}}\\|\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\left\\langle\\mathcal{P}_{T_{k}}(\\eta),\\mathcal{P}_{T_{k}}\\,\\mathcal{Q}_{\\Omega}^{*}\\,\\mathcal{P}_{T_{k}}(\\eta)\\right\\rangle+4\\epsilon\\left\\|\\mathcal{P}_{T_{k}}(\\eta)\\right\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using (42) in the last inequality, implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|P_{\\Lambda}(\\eta)\\|_{F}^{2}\\le\\frac{1}{1-\\xi_{\\epsilon}}\\bigl(\\mathcal{P}_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}\\circ_{\\Lambda}^{\\circ}P_{\\Lambda}(\\eta)\\bigr)}&{}\\\\ {\\le\\frac{1}{1-\\xi_{\\epsilon}}\\bigl\\langle\\mathcal{P}_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}\\circ_{\\Lambda}^{\\circ}P_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle P_{\\Lambda}(\\eta)^{2},\\mathcal{Q}_{\\Lambda}^{\\circ}P_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle Q_{\\Lambda}\\mathcal{P}_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}(\\eta)^{2}\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle Q_{\\Lambda}\\mathcal{P}_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle\\mathcal{P}_{\\Lambda}\\circ_{\\Lambda}^{\\circ}P_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle\\mathcal{P}_{\\Lambda}\\circ_{\\Lambda}^{\\circ}P_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle\\mathcal{P}_{\\Lambda}(\\eta),\\mathcal{P}_{\\Lambda}\\circ_{\\Lambda}\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\langle\\mathcal{P}_{\\Lambda}(\\eta),\\Pi_{\\{\\Phi}}\\circ_{\\Lambda}\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\rangle}&{}\\\\ {\\le\\frac{1}{1-4\\epsilon}\\bigl\\|\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\|_{F}\\bigl\\|\\mathcal{P}_{\\Lambda}\\bigr\\|_{\\infty}\\le\\bigl\\|\\Phi_{\\Lambda}^{\\circ}\\,\\mathcal{P}_{\\Lambda}(\\eta)\\bigr\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Dividing by $\\Vert\\mathcal{P}_{T_{k}}(\\eta)\\Vert_{F}$ on both sides we get, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}\\leq\\frac{1}{1-4\\epsilon}\\|\\mathcal{P}_{T_{k}}\\|_{S_{\\infty}}\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since spectral norm of $\\mathcal{P}_{T_{k}}$ . is bounded by 1. Furthermore, we used that $\\epsilon\\leq\\frac{1}{10}$ in the last inequality. Since $\\eta\\in\\ker\\mathcal{Q}_{\\Omega}$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n0=\\|\\,\\mathcal{Q}_{\\Omega}(\\eta)\\|_{F}=\\left\\|\\mathcal{Q}_{\\Omega}\\left(\\mathcal{P}_{T_{k}}(\\eta)+\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\right)\\right\\|_{F}\\geq\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}-\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}}(\\eta)\\|_{F}\\leq\\|\\,\\mathcal{Q}_{\\Omega}\\,\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F}\\leq\\Big(20L\\sqrt{\\frac{\\log n}{m}}+1\\Big)\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used (40) in the last inequality. Inserting this above, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\eta\\|_{F}^{2}=\\|\\mathcal P_{T_{k}}(\\eta)\\|_{F}^{2}+\\|\\mathcal P_{T_{k}^{\\perp}}(\\eta)\\|_{F}^{2}\\leq\\left(4\\Big(20L\\sqrt{\\frac{\\log n}{m}}+1\\Big)^{2}+1\\right)\\|\\mathcal P_{T_{k}^{\\perp}}(\\eta)\\|_{F}^{2}}\\\\ &{\\qquad\\leq5\\Big(21L\\sqrt{\\frac{\\log n}{m}}\\Big)^{2}\\|\\mathcal P_{T_{k}^{\\perp}}(\\eta)\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since ${\\frac{L}{m}}\\log n>1$ So we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\eta\\|_{F}\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta)\\|_{F}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for the constant $\\widetilde{C}$ defined by, ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\widetilde{C}}:={\\sqrt{5}}\\cdot21\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, we observe that for $\\begin{array}{r}{C_{1}:=\\frac{1}{20\\tilde{C}}}\\end{array}$ where $C_{1}$ is the constant of (39), it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widetilde{\\xi}\\leq\\frac{1}{\\left(10\\cdot20L\\frac{\\log n}{m}\\right)}=\\frac{C_{1}\\sqrt{m}}{L\\sqrt{\\log n}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "implying that the two statements of Lemma D.2 are satisfied on the event $E_{\\Omega}\\cap E_{\\Omega,T_{0}}$ if (39) holds. By the above mentioned probability bounds and a union bound, $E_{\\Omega}\\cap E_{\\Omega,T_{0}}$ occurs with a probability of at least $1-2n^{-2}$ , finishing the proof for the sampling with replacement model. By the argument of Proposition 3 of [Rec11], the result extends to the model of sampling locations drawn uniformly at random without replacement, with the same probability bound. This concludes the proof of Lemma D.2. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "The following lemma will also play a role in the proof of Theorem 4.3. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.3. Let $C,{\\widetilde{C}},C_{1}$ be the constants of Lemma $D.2$ and $\\mu_{0}$ be the incoherence factor of a rank- $r$ matrix ${\\bf X}^{0}$ . $I\\!f$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nm\\geq C\\nu r n\\log n\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and if $\\boldsymbol{\\eta}^{(k)}=\\mathbf{X}^{(k)}-\\mathbf{X}^{0}$ fulfills ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\eta^{(k)}\\|_{S_{\\infty}}\\le\\xi\\sigma_{r}(\\mathbf{X}^{0}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\xi:=\\operatorname*{min}\\left(\\frac{C_{1}\\sqrt{m}}{L\\sqrt{\\log n}},\\frac{10C_{1}\\sqrt{m}}{8L\\sqrt{r}\\kappa\\sqrt{\\log n}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then, on the event of Lemma $D.2$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\eta^{(k)}\\|_{S_{\\infty}}\\leq2\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}(\\sqrt{n-r})\\sigma_{r+1}(\\mathbf{X}^{(k)})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First, we compute that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta^{(k)})\\|_{F}\\leq\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\mathbf{X}^{(k)})\\|_{F}+\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\mathbf{X}^{0})\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\displaystyle\\sum_{i=r+1}^{d}\\sigma_{i}^{2}(\\mathbf{X}^{(k)})}+\\left\\|\\mathbf{U}_{\\perp}^{(k)}\\mathbf{U}_{\\perp}^{(k)*}\\mathbf{X}^{0}\\mathbf{V}_{\\perp}^{(k)}\\mathbf{V}_{\\perp}^{(k)*}\\right\\|_{F}}\\\\ &{\\qquad\\leq\\sqrt{n-r}\\sigma_{r+1}(\\mathbf{X}^{(k)})+\\|\\mathbf{U}_{\\perp}^{(k)*}\\mathbf{U}_{0}\\|_{S_{\\infty}}\\|\\Sigma_{0}\\|_{F}\\|\\mathbf{V}_{0}^{*}\\mathbf{V}_{\\perp}^{(k)}\\|_{S_{\\infty}}}\\\\ &{\\qquad\\leq\\sqrt{n-r}\\sigma_{r+1}(\\mathbf{X}^{(k)})+\\frac{2\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}}{(1-\\zeta)^{2}\\sigma_{r}^{2}(\\mathbf{X}^{0})}\\sqrt{r}\\sigma_{1}(\\mathbf{X}^{0})}\\\\ &{\\qquad=\\sqrt{n-r}\\sigma_{r+1}(\\mathbf{X}^{(k)})+\\frac{2\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}}{(1-\\zeta)^{2}\\sigma_{r}(\\mathbf{X}^{0})}\\sqrt{r}\\kappa,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $0\\,<\\,\\zeta\\,<\\,1$ such that $\\|\\mathbf{X}^{(k)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\le\\zeta\\sigma_{r}(\\mathbf{X}^{0})$ , using Lemma D.4 twice in the fourth inequality and $\\|\\mathbf{AB}\\|_{F}\\leq\\|\\bar{\\mathbf{A}}\\|_{S_{\\infty}}\\|\\mathbf{B}\\|_{F}$ all matrices $\\mathbf{A}$ and $\\mathbf{B}$ , referring to the notations of lemma $B.8$ in [KV21](see belo w) for $\\mathbf{U}_{0},\\pmb{\\Sigma}_{0},\\mathbf{V}_{0},\\mathbf{U}_{\\perp}^{(k)}$ and ${\\bf V}_{\\perp}^{(k)}$ . ", "page_idx": 28}, {"type": "text", "text": "Using Lemma D.2 for $\\boldsymbol{\\eta}^{(k)}\\;=\\;\\mathbf{X}^{(k)}\\;-\\;\\mathbf{X}^{0}$ , we obtain on the event on which the statement of Lemma D.2 holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\eta^{(k)}\\|_{S_{\\infty}}\\leq\\|\\eta^{(k)}\\|_{F}\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\|\\mathcal{P}_{T_{k}^{\\perp}}(\\eta^{(k)})\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\left(\\sqrt{n-r}\\sigma_{r+1}(\\mathbf{X}^{(k)})+\\frac{8\\sqrt{r}\\kappa\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}}{\\sigma_{r}(\\mathbf{X}^{0})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}\\left(\\sqrt{n-r}\\sigma_{r+1}(\\mathbf{X}^{(k)})+\\frac{8\\cdot10C_{1}\\sqrt{m}\\sqrt{r}\\kappa}{8L\\sqrt{\\log n}\\sqrt{r}\\kappa}\\|\\eta^{(k)}\\|_{S_{\\infty}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "since $\\begin{array}{r}{C_{1}=\\frac{1}{20\\tilde{C}}}\\end{array}$ , after rearranging we get, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(1-\\frac{1}{2}\\right)\\|\\eta^{(k)}\\|_{S_{\\infty}}\\leq\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}(\\sqrt{n-r})\\sigma_{r+1}(\\mathbf{X}^{(k)})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies the statement of this lemma. ", "page_idx": 29}, {"type": "text", "text": "Lemma D.4 (Wedin\u2019s bound [Ste06]). Let $\\mathbf{X}$ and $\\widehat{\\mathbf{X}}$ be two matrices of the same size and their singular value decompositions ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\bf X}=({\\bf U}\\quad{\\bf U}_{\\perp})\\left(\\begin{array}{c c}{{\\bf\\Sigma}}&{{\\bf0}}\\\\ {{\\bf0}}&{{\\bf\\Sigma}_{\\perp}}\\end{array}\\right)\\left({\\bf V}_{\\perp}^{*}\\right)\\quad\\,a n d\\quad{\\widehat{\\bf X}}=\\left({\\widehat{\\bf U}}\\quad{\\widehat{\\bf U}}_{\\perp}\\right)\\left(\\begin{array}{c c}{{{\\widehat{\\bf\\Sigma}}}}&{{\\bf0}}\\\\ {{\\bf0}}&{{{\\widehat{\\bf\\Sigma}}_{\\perp}}}\\end{array}\\right)\\left(\\begin{array}{c}{{{\\widehat{\\bf V}}^{*}}}\\\\ {{{\\widehat{\\bf V}}_{\\perp}^{*}}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the submatrices have the sizes of corresponding dimensions. Suppose that $\\delta,\\alpha$ satisfying $0<\\delta\\leq\\alpha$ are such that $\\alpha\\leq\\sigma_{\\operatorname*{min}}(\\Sigma)$ and $\\sigma_{\\operatorname*{max}}(\\widehat{\\Sigma}_{\\perp})<\\alpha-\\delta$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathbf{U}}_{\\bot}^{*}\\mathbf{U}\\|_{S_{\\infty}}\\leq\\sqrt{2}\\frac{\\|\\mathbf{X}-\\widehat{\\mathbf{X}}\\|_{S_{\\infty}}}{\\delta}\\,a n d\\,\\|\\widehat{\\mathbf{V}}_{\\bot}^{*}\\mathbf{V}\\|_{S_{\\infty}}\\leq\\sqrt{2}\\frac{\\|\\mathbf{X}-\\widehat{\\mathbf{X}}\\|_{S_{\\infty}}}{\\delta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now using the above lemma let us conclude the proof for the theorem, ", "page_idx": 29}, {"type": "text", "text": "Proof of Theorem 4.3. Let $k\\;=\\;k_{0}$ and $\\mathbf{X}^{(k)}$ be the $k$ -th iterate of MatrixIRLS fro EDG with the parameters stated in Theorem 4.3. Under the sampling model of Theorem 4.3, if the number of samples $m$ fulfills $m\\geq C\\nu r n\\log n$ , where $C$ is the constant of Lemma D.2, we know from Lemma D.2 ", "page_idx": 29}, {"type": "text", "text": "if furthermore $\\boldsymbol{\\eta}^{(k)}:=\\mathbf{X}^{(k)}-\\mathbf{X}^{0}$ fulfills ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\eta^{(k)}\\|_{S_{\\infty}}\\le\\xi\\sigma_{r}(\\mathbf{X}^{0})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi\\leq{\\frac{C_{1}{\\sqrt{m}}}{L{\\sqrt{\\log n}}}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "holds with a probability of at least $1-2n^{-2}$ . Then, by lemma $B.9$ of [KV21], ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{X}^{(k+1)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\leq\\left(\\frac{\\widetilde{C}^{2}L\\log n}{m}\\right)\\epsilon_{k}^{2}\\|W^{(k)}(\\mathbf{X}^{0})\\|_{S_{1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We denote the event that this is fulfilled by $E$ . Furthermore, on this event, if $\\xi\\leq1/2$ in (48) and denoting the condition number by $\\kappa=\\sigma_{1}(\\dot{\\mathbf{X}}^{0})/\\sigma_{r}(\\mathbf{X}^{0})$ , it follows from ,lemma $B.8$ of [KV21], that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{X}^{(k+1)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}\\leq\\left(\\frac{\\widetilde{C}^{2}L\\log n}{m}\\right)4\\sigma_{r}(\\mathbf{X}^{0})^{-1}\\left(\\epsilon_{k}^{2}+4\\epsilon_{k}\\|\\eta^{(k)}\\|_{S_{\\infty}}\\kappa+2\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}\\kappa\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, if $\\mathbf{X}_{r}^{(k)}\\in\\mathbb{R}^{n\\times n}$ denotes the best rank- $^r$ approximation of $\\mathbf{X}^{(k)}$ in any unitarily invariant norm, we estimate that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\epsilon_{k}\\leq\\sigma_{r+1}(\\mathbf{X}^{(k)})=\\|\\mathbf{X}^{(k)}-\\mathbf{X}_{r}^{(k)}\\|_{S_{\\infty}}\\leq\\|\\mathbf{X}^{(k)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}=\\|\\eta^{(k)}\\|_{S_{\\infty}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Inserting these two bounds into (50), we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\eta^{(k+1)}\\|_{S_{\\infty}}=\\|\\mathbf{X}^{(k+1)}-\\mathbf{X}^{0}\\|_{S_{\\infty}}=\\left(\\frac{\\widetilde{C}^{2}L\\log n}{m}\\right)4\\sigma_{r}(\\mathbf{X}^{0})^{-1}\\left(1+6\\kappa\\right)\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, if, additionally, (48) is satisfied for ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi\\leq\\left(\\frac{m}{\\widetilde{C}^{2}L\\log n}\\right)\\frac{1}{4(1+6\\kappa)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we conclude that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\eta^{(k+1)}\\|_{S_{\\infty}}<\\|\\eta^{(k)}\\|_{S_{\\infty}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and also, we observe a quadratic decay in the spectral error such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\eta^{(k+1)}\\|_{S_{\\infty}}\\leq\\mu\\|\\eta^{(k)}\\|_{S_{\\infty}}^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with a constant $\\begin{array}{r}{\\mu=\\left(\\frac{m}{\\widetilde{C}^{2}L\\log n}\\right)\\frac{1}{4(1+6\\kappa)\\sigma_{r}(\\mathbf{X}^{0})}}\\end{array}$ 4(1+6\u03ba1)\u03c3r(X0). This shows condition b of Theorem 4.3. ", "page_idx": 30}, {"type": "text", "text": "To show the remaining statement, we can use Lemma D.3 to show that if $\\mathbf{X}^{(k)}$ is close enough to $\\mathbf{X}^{0}$ , we can ensure that the $(r+1)$ -st singular value $\\sigma_{r+1}(\\mathbf{X}^{(k)})$ of the current iterate is strictly decreasing. More precisely, assume now the stricter assumption of ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\eta^{(k)}\\|_{S_{\\infty}}\\leq\\frac{\\sqrt{m}\\sqrt{r}}{12\\widetilde{C}L(\\log n)\\sqrt{n-r}}\\xi\\sigma_{r}(\\mathbf{X}^{0})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In fact, if $\\xi$ fulfills (49) and (51), we can conclude that on the event $E$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{r+1}(\\mathbf{X}^{(k+1)})\\leq\\|\\eta^{(k+1)}\\|_{S_{\\infty}}\\leq\\|\\eta^{(k+1)}\\|_{S_{\\infty}}}&{}\\\\ {\\leq\\left(\\frac{\\widetilde{C}^{2}L\\log n}{m}\\right)4\\sigma_{r}(\\mathbf{X}^{0})^{-1}\\left(1+6\\kappa\\right)\\|\\eta^{(k)}\\|_{S_{\\infty}}\\cdot\\|\\eta^{(k)}\\|_{S_{\\infty}}}&{}\\\\ {<\\left(\\frac{\\widetilde{C}^{2}L\\log n}{m}\\right)4\\sigma_{r}(\\mathbf{X}^{0})^{-1}\\left(1+6\\kappa\\right)\\frac{\\sqrt{m}\\sqrt{r}}{12\\widetilde{C}L(\\log n)\\sqrt{n-r}}}&{}\\\\ {\\cdot\\xi\\sigma_{r}(\\mathbf{X}^{0})2\\widetilde{C}L\\sqrt{\\frac{\\log n}{m}}(\\sqrt{n-r})\\sigma_{r+1}(\\mathbf{X}^{(k)})}&{}\\\\ {\\leq\\sigma_{r+1}(\\mathbf{X}^{(k)})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "using Lemma D.3 for one factor $\\|\\eta^{(k)}\\|_{S_{\\infty}}$ and (52) for the other factor $\\|\\eta^{(k)}\\|_{S_{\\infty}}$ in the third inequality, and (51) in the last inequality. Taking the update rule (7) for the smoothing parameter into account, this implies that $\\epsilon_{k+1}\\,=\\,\\sigma_{r+1}({\\bf X}^{(k+1)})$ , which ensures that the first statement of Theorem 4.3 is fulfilled likewise for iteration $k+1$ . By induction, this implies that $\\mathbf{X}^{(k+\\ell)}\\xrightarrow{\\ell\\rightarrow\\infty}\\mathbf{X}^{0}$ , which finishes the proof of Theorem 4.3. ", "page_idx": 30}, {"type": "text", "text": "E Numerical Considerations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1 Experiments on more real data ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "As a continuation of Section 5, we provide the error analysis for the 1BPM protein data whose corresponding recovery visualizations are found in Figures 7a and 7b. The figure Figure 7a shows the Procrustes distance between the recovered matrix from the samples provided and the ground-truth for both datasets. Similar to the phase transition diagram of the Gaussian data, we observe the probability of success observed over these 24 instances in fig. 7b. ", "page_idx": 30}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/51c24805d7c6bedce92730f7fd24c07647049e8ccbe6f5a70db9835f73765ac5.jpg", "img_caption": ["(a) The relative Procrustes error for all the algorithms for different(b) The probability of success for all the algorithms for different overoversampling rates for1BPM protein data sampling rates for 1BPM protein data. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Next, we evaluate the performance of MatrixIRLS (Algorithm 1), on the US cities dataset [UU20] in comparison to the aforementioned algorithms. In this setup, we are given $m=|\\Omega|$ Euclidean distances $\\sqrt{(\\lambda_{i}-\\lambda_{j})^{2}+(\\phi_{i}-\\phi_{j})^{2}}$ between vectors containing longitude and latitude values $\\lambda_{i},\\lambda_{j}$ and $\\phi_{i},\\phi_{j}$ of $n=2920$ cities in the United States, whose squares can be arranged in an incomplete distance matrix $\\mathbf{D}\\in S_{n}$ that serves as an input for the reconstruction algorithms. Like the previous setup of Section 5, the set $\\Omega\\subset\\mathbb{I}$ of point index pairs is sampled uniformly at random, and we consider different choices of $m$ parameterized by the oversampling factor $\\rho$ . Here, for the US cities the rank of the input matrix is 2. In Figure 8a we observe a similar pattern to that of Figure 7a, for the US cities data. The success probability Figure 8b also shows a similar pattern to that Figure 4. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/fa14e9b4b58e438d4439cc0329735ccb13099d3e32793140bc17e64f15e95ad5.jpg", "img_caption": ["(a) The relative Procrustes error for all the algorithms for different(b) The probability of success for all the algorithms for different overoversampling rates for US cities data sampling rates for US cities ", "Figure 8 "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "We visualize the reconstruction of the US cities data as shown in Figure 9. ", "page_idx": 31}, {"type": "image", "img_path": "Yu7H8ZOuI2/tmp/ba3ecd5ce7c540b5c13d27d196f8c5c70a72dd95c98602316f30cdde7d790f41.jpg", "img_caption": ["Figure 9: Visualizations of the recovery of US cities data by MatrixIRLS for oversampling rates 1.5, 2.5, 3.5 in $9\\mathrm{a},9\\mathrm{b},9\\mathrm{c}$ respectively "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "In this paper, we have adapted the authors\u2019 codes of ScaledSGD [ZCZ22], ALM [TL18] and MatrixIRLS [KV21] from their respective githubs. The code of RieEDG [SLCT23][KV21], has been obtained from the author through personal communication. ", "page_idx": 31}, {"type": "text", "text": "E.2 Choice of parameters ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 MatrixIRLS: The algorithm is configured with the following options: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The input parameter for MatrixIRLS includes the number of outer IRLS iterations $N^{0}$ , tahlge onriutmhbme $t o l_{i n n e r}^{0}$ e.r  iFtoerr altairognes $N_{i n n e r}^{0}$ s,  tlhike et otlheer aUnScec,i tiwehsi cdha tias  athned  sPtrooptpeiinng  dcartiat $N\\,=\\,400$ although the algorithm converges within 120 iterations. We run the experiments with $N_{i n n e r}^{0}=2000$ and $t o l_{i n n e r}=\\overline{{10}}^{-10}$ . In a smaller setup of the synthetic data we perform the experiments with $n\\,=\\,500$ points with same parameters. although convergence of MatrixIRLS is observed much faster. To put more emphasis on the per iteration error, we study the experiment on per-iterate analysis with more iterations. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Augmented Lagrangian Method: For the Augmented Lagrangian method(ALM) we have same number iterations which is parametrized as f0irstorder for the Barzilai-Borwein gradient method in the code. There are 3 stopping criteria for the BB gradient method which are parametrised by xtol,ftol and gtol on the iterate, functional value of the iterate and the gradient of the function respectively. As mentioned in [TL18], the relative change in ", "page_idx": 31}, {"type": "text", "text": "Energy is the stopping criterion for the algorithm and the tolerance for that measure is set at $10^{-1\\breve{0}}$ for our experiments. Similar to the above setup, for larger dataset, more iterations are observed so that we can achieve a fair comparison between all the algorithms. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Scaled SGD: This method uses the learning rate and the number of iterations $N_{f i r s t o r d e r}^{0}$ as the parameters. For all the experiments the value of N f0irstorder are kept constant across the methods. however, based on the dimension of the input which is $n$ , the iterations are modified to achieve a fair comparison. The learning rate is set at 0.2, which is based on the respective paper [ZCZ22]. \u2022 Riemmanian Gardient: This method has parameters, number of iterations which is same $N_{f i r s t o r d e r}^{0}$ , the thresholding tolerance which has same value as the $t o l_{i n n e r}$ . ", "page_idx": 32}, {"type": "text", "text": "E.3 Distance metric ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The success of proabbility in the experiments of Section 5, is with respect to the Procrustes distance. The Procrustes analysis uses similarity transformations like scaling, rotation and maps into a common reference frame [ADSVF23]. If we consider our problem, this distance is considered with the ground truth as the reference frame and is defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Q}\\in\\mathbb{R}^{r\\times r},\\mathbf{t}\\in\\mathbb{R}^{r}}\\|\\mathbf{Q}\\cdot\\left(\\mathbf{P_{0}}+\\mathbf{t}\\mathbf{1}^{\\top}\\right)-\\mathbf{P}_{r e c}\\|_{F}\\mathrm{~subject~to~}\\mathbf{Q}^{\\top}\\mathbf{Q}=\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "E.4 Degrees of freedom ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We count the degrees of freedom in the spectral decomposition of a rank-r symmetric matrix: $\\mathbf{A}\\,=\\,\\mathbf{U}\\mathbf{D}\\mathbf{U}^{\\top}$ where $\\mathbf{U}$ is an orthogonal matrix and $\\mathbf{D}$ is a diagonal matrix consisting of the eigenvalues of $\\mathbf{A}$ on the diagonal. In $\\mathbf{U}$ there are total $r$ unit norm constraints and there are total $\\frac{r(\\bar{r}-1)}{2}$ constraints to the orthogonality of the columns vectors of $\\mathbf{U}$ which follow from $\\langle u_{i},u_{j}\\rangle=0$ for $i\\neq j$ and $i\\in\\{1,2,...,r\\}$ and $j^{'}\\in\\{1,2,...,r\\}$ . Therefore, the total number of constraints is $\\begin{array}{r}{\\frac{r(r-1)}{2}+r=\\frac{r(r+1)}{2}}\\end{array}$ r(r2+1). The total degrees of freedom in D is r. Hence, the total degrees of freedom of A is: ", "page_idx": 32}, {"type": "equation", "text": "$$\nn r+r-(r+{\\frac{r(r+1)}{2}})=n r-{\\frac{r(r-1)}{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "F Computational Complexity ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For a symmetric matrix $\\mathbf{X}\\in S_{n}$ , we write its eigendecomposition (with in magnitude decaying eigenvalues) such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{X}=[\\mathbf{U}\\quad\\mathbf{U}_{\\perp}]\\left[\\mathbf{A}\\quad\\mathbf{\\Omega}_{0}\\right]\\left[\\mathbf{U}_{\\perp}^{\\top}\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\textbf{U}\\,\\in\\,\\mathbb{R}^{n\\times r}$ , $\\mathbf{U_{\\perp}}\\;\\;\\in\\;\\;\\mathbb{R}^{n\\times(n-r)}$ are matrices with orthonormal columns and $\\textrm{\\textbf{A}}=$ $\\mathrm{diag}(\\gamma\\sigma_{1},\\dots\\gamma\\sigma_{r})$ and $\\mathbf{\\Lambda}_{\\mathbf{\\lambda}\\perp}:=\\mathrm{dg}(\\gamma\\sigma_{r+1},\\ldots\\gamma\\sigma_{d})$ diagonal matrices with entries $\\lambda_{i}=\\sigma_{i}\\gamma_{i}$ where $\\sigma_{i}$ is the $i$ -th singular value of $\\mathbf{X}$ and $\\gamma_{i}\\in\\{\\pm1\\}$ a sign. Furthermore, we denote by ${\\mathcal{T}}_{r}(\\mathbf{X})$ the best rank- $r$ approximation of $\\mathbf{X}$ , which can be written such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{T}_{r}(\\mathbf{X}):=\\underset{\\mathbf{Z}:\\mathrm{rank}(\\mathbf{Z})\\leq r}{\\arg\\operatorname*{min}}\\,\\|\\mathbf{Z}-\\mathbf{X}\\|=\\mathbf{U}\\mathbf{A}\\mathbf{U}^{\\top},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\|\\cdot\\|$ can be any unitarily invariant norm, due to the Eckardt-Young-Mirsky theorem [Mir60], using the established notation. ", "page_idx": 32}, {"type": "text", "text": "The computational complexity of Algorithm 1 is dominated by the solution of the weighted least squares problem (5) and the computational of spectral information used in the update of the smoothing parameter $\\epsilon_{k}$ of (7) and the weight operator update (7) (see also Definition 3.1). ", "page_idx": 32}, {"type": "text", "text": "For solving (5), we consider detailed in Appendix F.1, based on solving a linear system with the dimensionality of tangent space $T_{T_{r_{k}}(\\mathbf{X}^{(k)})}\\mathcal{M}_{r_{k}}$ . The tangent space formulation can be derived from (6) via the Sherman-Morrison-Woodbury [Woo50] formula using the weight operator structure. ", "page_idx": 32}, {"type": "text", "text": "In order to update the smoothing parameter and the weight operator we compute the SVD of the iterate by approximating top singular vectors and corresponding values following Randomized Block Krylov Methods [MM15]. This computation takes $O(((m+n r)r\\log n)$ . ", "page_idx": 32}, {"type": "text", "text": "F.1 Tangent Space Implementation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Let $\\mathcal{M}_{r}:=\\{\\mathbf{X}\\in S_{n}:\\operatorname{rank}(\\mathbf{X})=r\\}$ the manifold of symmetric rank- $^r$ matrices and let $\\mathbf{X}\\in S_{n}$ be as in (53). In this case, given $r\\in\\mathbb N$ , we can write the tangent space of $\\mathcal{M}_{r}$ at ${\\mathcal{T}}_{r}(\\mathbf{X})$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T:=T_{T_{r}(\\mathbf{x})}\\mathcal{M}_{r}:=\\bigg\\{[\\mathbf{U}\\,\\mathbf{U}_{\\perp}]\\bigg[\\mathbb{R}^{r\\times r}\\mathbb{R}^{r\\times(n-r)}\\bigg][\\mathbf{U}\\,\\mathbf{U}_{\\perp}]^{*}\\bigg\\}}\\\\ &{\\quad=\\bigg\\{[\\mathbf{U}\\mathbf{U}_{\\perp}]\\bigg[\\mathbf{M}_{1}\\,\\mathbf{M}_{2}^{\\top}\\bigg][\\mathbf{U}\\mathbf{U}_{\\perp}]^{*}:\\mathbf{M}_{1}\\in S_{r},\\mathbf{M}_{2}\\in\\mathbb{R}^{r\\times(n-r)},\\mathrm{~arbitrary}\\bigg\\}}\\\\ &{\\quad=\\big\\{\\mathbf{U}\\Gamma_{1}\\mathbf{U}^{\\top}+\\mathbf{U}\\Gamma_{2}^{\\top}\\left(\\mathbf{I}-\\mathbf{U}\\mathbf{U}^{\\top}\\right)+(\\mathbf{I}-\\mathbf{U}\\mathbf{U}^{\\top})\\Gamma_{2}\\mathbf{U}^{\\top}:\\Gamma_{1}\\in S_{r},\\Gamma_{2}\\in\\mathbb{R}^{n\\times r}\\big\\}}\\\\ &{\\quad=\\big\\{\\mathbf{U}\\Gamma_{1}\\mathbf{U}^{\\top}+\\mathbf{U}\\Gamma_{2}^{\\top}+\\Gamma_{2}\\mathbf{U}^{\\top}:\\Gamma_{1}\\in S_{r},\\Gamma_{2}\\in\\mathbb{R}^{n\\times r},\\mathbf{U}^{\\top}\\Gamma_{2}=\\mathbf{0}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "see also [Van13], [Bou20, Chapter 7.5]. ", "page_idx": 33}, {"type": "text", "text": "If $\\mathcal{P}_{T}:S_{n}\\rightarrow S_{n}$ is the orthogonal projection operator that projects symmetric matrices onto $T$ as used in Theorem 4.5, we note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{P}_{T}(\\mathbf{X})=\\mathbf{UU}^{\\top}\\mathbf{XUU}^{\\top}+\\mathbf{UU}^{\\top}\\mathbf{X}(\\mathbf{I}-\\mathbf{UU}^{\\top})+(\\mathbf{I}-\\mathbf{UU}^{\\top})\\mathbf{XUU}^{\\top},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which can be decomposed such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{P}_{T}(\\mathbf{X})=P_{T}P_{T}^{*}(\\mathbf{X}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the action of $P_{T}:\\mathbb{R}^{r(n+r)}\\rightarrow S_{n}$ can be described as ", "page_idx": 33}, {"type": "equation", "text": "$$\nP_{T}(\\gamma):=\\mathbf{U}\\Gamma_{1}\\mathbf{U}^{\\top}+\\mathbf{U}\\Gamma_{2}^{\\top}+\\Gamma_{2}\\mathbf{U}^{\\top}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\Gamma_{1}\\in S_{r}$ being the result of an $(r^{2}\\times1)$ to $(r\\times r)$ reshaping of the first $r^{2}$ coordinates of $\\gamma$ and $\\Gamma_{2}\\in\\mathbb{R}^{n\\times r}$ the result of an $(r n\\times1)$ to $(n\\times r)$ reshaping of the remaining coordinates of $\\gamma$ . Further, we used the notation of the adjoint operator $P_{T}^{*}:S_{n}\\rightarrow\\mathbb{R}^{r(n+r)}$ of $P_{T}$ which maps $\\mathbf{X}\\in S_{n}$ to the vectorization $\\gamma$ of $[\\Gamma_{1},\\Gamma_{2}]:=\\big\\{\\mathbf{U}^{\\top}\\mathbf{X}\\mathbf{U},(\\mathbf{I}-\\mathbf{\\bar{U}U}^{\\top})\\mathbf{X}\\mathbf{U}\\big\\}$ . ", "page_idx": 33}, {"type": "text", "text": "Due to the fact that the weight operator $W_{k}=W_{\\mathbf{X}^{(k)},\\epsilon_{k}}$ of Definition 3.1 associated to the iterate $\\mathbf{X}^{(k)}\\in S_{n}$ and the smoothing parameter $\\epsilon_{k}>0$ is self-adjoint and invertible, we can write its inverse as ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{k}^{-1}=P_{T_{k}}\\mathbf{D}_{k}^{-1}P_{T_{k}}^{*}+\\epsilon_{k}^{2}(\\mathbf{I}-P_{T_{k}}P_{T_{k}}^{*}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathbf{D}_{k}^{-1}\\qquad\\in\\qquad\\mathbb{R}^{r_{k}(n+r_{k})\\times r_{k}(n+r_{k})}$ is a diagonal matrix with entries $\\operatorname*{max}(\\sigma_{i}(\\mathbf{X}^{(k)}),\\epsilon_{k})\\operatorname*{max}(\\sigma_{j}(\\mathbf{X}^{(k)}),\\epsilon_{k})$ , where either $i$ or $j$ are smaller or equal than $r_{k}$ , see also [KV21, Eq. (12)], and $T_{k}=T_{T_{r_{k}}(\\mathbf{X}^{(k)})}\\mathcal{M}_{r_{k}}$ is the tangent space at $\\boldsymbol{\\mathcal{T}}_{r_{k}}(\\mathbf{X}^{(k)})$ onto the rank- $\\cdot r_{k}$ manifold. ", "page_idx": 33}, {"type": "text", "text": "Recall from (7) that the the defining equation for the next iterate $\\mathbf{X}^{(k+1)}$ given $W_{k}$ is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{X}^{(k+1)}=W_{k}^{-1}\\mathcal{A}^{*}\\left(A W_{k}^{-1}\\mathcal{A}^{*}\\right)^{-1}\\mathbf{y}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using (56), we see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathcal{A}W_{k}^{-1}\\mathcal{A}^{*}\\right)=\\mathcal{A}\\left(P_{T_{k}}\\mathbf{D}_{S_{k}}^{-1}P_{T_{k}}^{*}+\\epsilon_{k}^{2}\\left(\\mathbf{I}-P_{T_{k}}P_{T_{k}}^{*}\\right)\\right)\\mathcal{A}^{*}}\\\\ &{\\quad\\quad\\quad\\quad=\\mathcal{A}\\left(P_{T_{k}}\\left(\\mathbf{D}_{S_{k}}^{-1}-\\epsilon_{k}^{2}\\mathbf{I}_{S_{k}}\\right)P_{T_{k}}^{*}\\right)\\mathcal{A}^{*}+\\epsilon_{k}^{2}\\mathcal{A}\\mathcal{A}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and, using the Sherman-Morrison-Woodbury [Woo50] formula, we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathring{A}(W^{(k)})^{-1}A^{*}\\Big)^{-1}=\\epsilon_{k}^{-2}(A A^{*})^{-1}}&{}\\\\ {-\\,\\epsilon_{k}^{-2}\\left(A A^{*}\\right)^{-1}A P_{T_{k}}\\left(\\epsilon_{k}^{2}\\mathbf{C}^{-1}+P_{T_{k}}^{*}A^{*}\\left(A A^{*}\\right)^{-1}A P_{T_{k}}\\right)^{-1}P_{T_{k}}^{*}A^{*}\\left(A A^{*}\\right)^{-1}}\\\\ &{=\\epsilon_{k}^{-2}(A A^{*})^{-1}-\\epsilon_{k}^{-2}\\left(A A^{*}\\right)^{-1}A P_{T_{k}}\\mathbf{M}^{-1}P_{T_{k}}^{*}A^{*}\\left(A A^{*}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with linear system matrix $\\begin{array}{r l r}{\\mathbf{M}}&{{}:=}&{\\left(\\epsilon_{k}^{2}\\mathbf{C}^{-1}+P_{T_{k}}^{*}\\boldsymbol{A}^{*}\\left(\\boldsymbol{A}\\boldsymbol{A}^{*}\\right)^{-1}\\boldsymbol{A}P_{T_{k}}\\right)}\\end{array}$ , noting that $\\mathbf{C_{\\mathrm{~}}}:=$ $\\left(\\mathbf{D}_{k}^{-1}-\\epsilon_{k}^{2}\\mathbf{I}\\right)$ is invertible since $(\\mathbf{D}_{k}^{-1})_{i i}\\,>\\,\\epsilon_{k}^{2}$ for all diagonal indices. The observation of (59) can be turned an efficient implementation of the weighted least squares computing $\\mathbf{X}^{(k+1)}$ , which is presented in Algorithm 2. It can be shown that Algorithm 2 indeed computes $\\mathbf{X}^{(k+1)}$ implicitly, cf. Lemma F.1. We omit its proof, which follows the proof of [KV21, Lemma A.1]. ", "page_idx": 33}, {"type": "text", "text": "Input: Set $\\Omega$ , observation distances $\\mathbf{D}_{\\Omega}~=~(d_{i j})_{(i,j)\\in\\Omega}$ , singular vectors $\\textbf{U}\\in\\mathbb{R}^{d_{1}\\times r_{k}}$ , $\\mathbf{V}^{(k)}\\in\\mathbb{R}^{n\\times r_{k}}$ and singular values $\\sigma_{1}^{(k)},\\ldots,\\sigma_{r_{k}}^{(k)}$ , smoothing parameter $\\epsilon_{k}$ , projection $\\gamma_{k}^{(0)}=$ $P_{T_{k}}^{*}P_{T_{k-1}}(\\gamma_{k-1})\\ \\in\\ \\mathbb{R}^{r_{k}(n+r_{k})}$ of solution $\\boldsymbol{\\gamma}_{k-1}\\,\\in\\,\\mathbb{R}^{r_{k-1}(n+r_{k-1})}$ of linear system (60) for previous iteration $k-1$ . ", "page_idx": 34}, {"type": "text", "text": "1: Compute $\\mathbf{h}_{k}^{0}:=P_{T_{k}}^{*}A^{*}\\left(A A^{*}\\right)^{-1}\\mathbf{y}-\\left(\\epsilon_{k}^{2}\\left(\\mathbf{D}_{k}^{-1}-\\epsilon_{k}^{2}\\mathbf{I}\\right)^{-1}+P_{T_{k}}^{*}A^{*}\\left(A A^{*}\\right)^{-1}A P_{T_{k}}\\right)\\gamma_{k}^{(0)}\\in\\mathcal{A}_{*}\\times\\mathcal{V}.$ $\\mathbb{R}^{r_{k}(n+r_{k})}$ .   \n2: Solve ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{M}\\Delta\\gamma_{k}=\\mathbf{h}_{k}^{0}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for $\\Delta\\gamma_{k}\\in S_{k}$ by the conjugate gradient method [HS52, Meu06], where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\epsilon_{k}^{2}\\left(\\mathbf{D}_{k}^{-1}-\\epsilon_{k}^{2}\\mathbf{I}\\right)^{-1}+P_{T_{k}}^{*}\\boldsymbol{A}^{*}\\left(\\boldsymbol{A}\\boldsymbol{A}^{*}\\right)^{-1}\\boldsymbol{A}P_{T_{k}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "3: Compute \u03b3k = \u03b3k $\\gamma_{k}=\\gamma_{k}^{(0)}+\\Delta\\gamma_{k}$ . 4: Compute residual $\\mathbf{r}_{k+1}:=\\left(A A^{\\ast}\\right)^{-1}\\left(\\mathbf{y}-A P_{T_{k}}(\\gamma_{k})\\right)\\in\\mathbb{R}^{m}.$ Output: rk+1\u2208Rm and \u03b3k\u2208Rrk(n+rk). ", "page_idx": 34}, {"type": "text", "text": "Lemma F.1. If $\\mathbf{\\dot{r}}_{k+1}\\in\\mathbb{R}^{m}$ and $\\gamma_{k}\\in\\mathbb{R}^{r_{k}(n+r_{k})}$ is the output of Algorithm 2, then $\\mathbf{X}^{(k+1)}$ as in step (6) of Algorithm $^{\\,I}$ fulfills ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{(k+1)}={\\cal{A}}^{*}({\\mathbf{r}}_{k+1})+{\\mathcal{P}}_{T_{k}}({\\boldsymbol\\gamma}_{k})}\\\\ &{\\quad\\quad\\quad={\\cal{A}}^{*}({\\mathbf{r}}_{k+1})+{\\mathbf{U}}\\Gamma_{1}{\\mathbf{U}}^{\\top}+{\\mathbf{U}}\\Gamma_{2}^{\\top}+\\Gamma_{2}{\\mathbf{U}}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\Gamma_{1}\\in\\mathbb{R}^{r_{k}\\times r_{k}}$ is the matricization of the first $r_{k}^{2}$ elements of $\\gamma_{k}$ and $\\Gamma_{2}\\in\\mathbb{R}^{n\\times r_{k}}$ the matricization of the reamining elements. ", "page_idx": 34}, {"type": "text", "text": "Algorithm 3 Implementation of U\u22a4A\u2217: Rm+n \u2192Rr\u00d7n ", "page_idx": 34}, {"type": "text", "text": "Input: Input vector $\\mathbf{y}=[y_{1},y_{2},....,y_{m+n}]$ , $\\boldsymbol{\\Omega}=(i_{\\ell},j_{\\ell})_{\\ell=1}^{m}\\subset\\mathbf{I}$ be a multiset of double indices. $\\begin{array}{r}{\\mathbf{S_{1}}=\\sum_{\\ell=1:(i_{\\ell},j)\\in\\Omega}^{m}}\\end{array}$ for some j y\u2113ei\u2113eiT\u2113 $\\begin{array}{r}{\\mathbf{S_{2}}=\\sum_{\\ell=1:(i,j_{\\ell})\\in\\Omega}^{m}}\\end{array}$ for some i $\\mathbf{y}_{\\ell}\\mathbf{e}_{j_{\\ell}}\\mathbf{e}_{j_{\\ell}}^{T}$ $\\begin{array}{r}{\\mathbf{S_{3}}=\\sum_{\\ell=1:(i_{\\ell},j_{\\ell})\\in\\Omega}^{m}\\mathbf{y}_{\\ell}\\mathbf{e}_{i_{\\ell}}\\mathbf{e}_{j_{\\ell}}^{T}}\\end{array}$ $\\mathbf{S}=\\mathbf{S}_{1}+\\mathbf{S}_{2}-\\mathbf{S}_{3}-\\mathbf{S}_{3}^{T}$ . saved as a sparse matrix. V1 = U\u22a4S \u25b74rm flops.   \nV2 = 12U\u22a4\u00b7 [ym+1, ..., ym+n]\u22a4\u00b7 1\u22a4 \u25b7rn flops.   \nV3 = 21U\u22a4\u00b7 1 \u00b7 [ym+1, ..., ym+n] $\\mathsf{\\Delta}_{\\mathsf{D}}\\mathsf{\\Pi}_{\\mathsf{D}}$ flops V = V1 + V2 + V3   \nOutput: V Total of $r m+3r n$ flops. ", "page_idx": 34}, {"type": "text", "text": "Algorithm 4 Implementation of $A A^{*}:\\mathbb{R}^{m+n}\\rightarrow\\mathbb{R}^{m+n}$ ", "page_idx": 34}, {"type": "text", "text": "Input: $\\boldsymbol{\\Omega}=(i\\ell,j_{\\ell})_{\\ell=1}^{m}\\subset\\mathbf{I}$ be a multiset of double indices fulfilling $m<L=n(n-1)/2$ that are sampled independently with replacement, the vector $\\mathbf{y}=[y_{1},y_{2},.....,y_{m+n}]$ . Load S from Algorithm 3 which computes $A^{*}(\\mathbf{y})$ . ", "page_idx": 34}, {"type": "text", "text": "T1 = (ei\u22a4\u2113Sei\u2113)\u2113m=1:(i\u2113,j)\u2208\u2126for some j T2 = (ej\u22a4\u2113Sej\u2113)\u2113m=1:(i,j\u2113)\u2208\u2126for some i. ", "page_idx": 34}, {"type": "text", "text": "Output: $[\\mathbf{T}_{5};\\mathbf{T}_{6}]$ ", "page_idx": 34}, {"type": "text", "text": "Total of $4m+4n$ flops ", "page_idx": 34}, {"type": "text", "text": "Input: Input vector $\\mathbf{y}=[y_{1},y_{2},.....,y_{m+n}]\\in\\mathbb{R}^{m+n}$ , index set $\\Omega$ , left singular vector matrix   \n$\\mathbf{U}\\in\\mathbb{R}^{n\\times r}$ . $\\mathbf{A}_{1}=\\mathbf{V}\\in\\mathbb{R}^{r\\times n}$ from Algorithm 3. $\\circ\\,O(r(m+n))$ flops $\\Gamma_{1}=\\mathbf{A}_{1}\\mathbf{U}\\in\\mathbb{R}^{r\\times r}$ . $\\mathsf{\\Delta}\\mathsf{\\Delta}\\mathsf{D}(r^{2}n)$ flops $\\mathbf{M}=\\Gamma_{1}\\mathbf{U}^{\\top}\\in\\mathbb{R}^{r\\times n}$ . $\\mathsf{\\Delta}\\mathsf{\\Delta}\\mathsf{D}(r^{2}n)$ flops $\\Gamma_{2}=(\\mathbf{A}_{1}^{\\top}-\\mathbf{M}^{\\top})$   \nOutput: $\\{\\Gamma_{1},\\Gamma_{2}\\}$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma F.1 shows that an iterate $\\mathbf{X}^{(k+1)}$ can be represented via only $m+r_{k}(n+r_{k})$ parameters. In the remainder of this discussion, we will assume that $r_{k}=r$ , which is the case in most cases in practice if the rank estimate $\\widetilde{r}$ of Algorithm 1 is chosen as $\\widetilde r=r$ . ", "page_idx": 35}, {"type": "text", "text": "To quantify the computational cost of Algorithm 2, we assume a fixed number $\\mathrm{N}_{i n n e r}^{0}$ of CG iterations solving (60). When applying the system matrix $\\mathbf{M}$ via matrix-vector multplication, we observe that its first summand $\\epsilon_{k}^{2}\\left(\\mathbf{D}_{k}^{-1}-\\epsilon_{k}^{2}\\mathbf{I}\\right)^{-1}$ is diagonal and thus results in $O(r(n+r))$ flops per CG iteration. To quantify the matrix-vector multiplication cost of its second summand $P_{T_{k}}^{*}\\mathcal{A}^{*}\\left(\\mathcal{A}A^{*}\\right)^{-1}\\mathcal{A}P_{T_{k}}$ , we define below algorithms that efficiently implement the application of the operators $P_{T}^{*}\\mathcal{A}^{*}:\\mathbb{R}^{m+n}\\rightarrow$ $\\mathbb{R}^{r(n+r)}$ (Algorithm 5), $\\boldsymbol{A}\\boldsymbol{A}^{\\ast}:\\mathbb{R}^{m+n}\\rightarrow\\mathbb{R}^{m+n}$ (Algorithm 4) and $A P_{T}\\,:\\,\\mathbb{R}^{r(n+r)}\\,\\rightarrow\\,\\mathbb{R}^{m+n}$ (Algorithm 6). As an auxiliary function, we also provide an implementation of $\\mathbf{U}^{\\top}\\mathcal{A}^{*}:\\mathbb{R}^{m+n}\\rightarrow$ $\\dot{\\mathbb{R}}^{r\\times n}$ (Algorithm 3) below. ", "page_idx": 35}, {"type": "text", "text": "The application of $(A A^{*})^{-1}:\\mathbb{R}^{m+n}\\,\\rightarrow\\,\\mathbb{R}^{m+n}$ can be achieved by an inexact iterative solver that applies Algorithm 4 a fixed number $\\tilde{N}$ of iterations, which costs $O((m+n)\\tilde{N})$ . Since the time complexity of Algorithm 5 and Algorithm 6 are $O(r^{2}n+r m)$ , we obtain one matrix vector multiplication with M from Algorithm 2in a time complexity of $O({\\dot{r}}^{2}n+r m+(m+n){\\tilde{N}})$ . ", "page_idx": 35}, {"type": "text", "text": "Since $\\mathrm{N}_{i n n e r}^{0}$ CG iterations are used, we obtain a total time complexity of $O(\\ensuremath{\\mathbf{N}}_{i n n e r}^{0}(r^{2}n\\!+\\!r m\\!+\\!(m\\!+$ $n)\\tilde{N})$ ) for Algorithm 2. In our experiments, we observe that a small number $\\dot{N}=10$ of iterations for solving the system associated to $(\\bar{\\mathcal{A}}A^{*})^{-1}$ is sufficient to obtain high-accuracy solutions. ", "page_idx": 35}, {"type": "text", "text": "This breakdown of the computational costs of each algorithm is shown in Algorithm 3 to Algorithm 6. ", "page_idx": 35}, {"type": "text", "text": "Algorithm 6 Implementation of APT : Rr(n+r) \u2192Rm+n ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "Yu7H8ZOuI2/tmp/fe2017cc112f3edf0ef8992314533bb3676db68eeeca5aa21899220aa8d1c89d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Main claims of the paper are weaved in Abstract and Introduction. There is a separate subsection in the Introduction dedicated to discuss the contributions of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: In the Appendix B of this paper, we mention the limitation of this paper. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Every lemma/ theorems stated in the paper (Section 4 and appendices C and D) include the assumptions in the respective statements. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The related link to the anonymized Github respository is provided in the Appendix E ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All references of code and reference code are provided in the paper. [TODO] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Experimental details are provided in Section 5 and discussion on the choice of parameters is provided in the Appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: In the Section 5, the experiments are performed on multiple independant realizations and respective boxplots of error are shown in Appendix E. All the figures provide statistical significance of the experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The experiements were performed on a single node of a compute cluster equipped with dual 24-core Intel Xeon Gold 6248R CPUs, utilizing 32 parallel tasks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: In the introduction and in related work we highlight the impact of the work. These provide foundational algorithmic research to reconstruction algorithms which has applications in molecular conformation, sensor network localization. There is no potential negative docial impact of this work. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper uses data that are publicly available. The data sources are cited properly in Section 5. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Throughout the paper we cite the different information that we used from the existing literature. Additionally, in the Appendix E, we provide links to the Github repositories that we used for reference. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The link to the anonymized version of the repository containing our experiments have been provided in Appendix E. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: NA ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects.r Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]