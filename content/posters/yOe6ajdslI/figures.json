[{"figure_path": "yOe6ajdslI/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of given data in our problem setting. The triangle, circle, and cross represents negative data, positive data in the training distribution, and positive data in the test distribution, respectively. Orange and gray represents labeled and unlabeled data, respectively. Our setting assumes that the distribution of negative data does not change (triangles) but that of positive data can vary (circles and crosses).", "description": "This figure illustrates the data used in the proposed AUC maximization method under positive distribution shift. It shows the different types of data: labeled positive data in the training set, unlabeled positive and negative data in the training set, and unlabeled positive data in the test set. The key assumption is that the negative data distribution remains the same between training and testing, while the positive data distribution may change.", "section": "1 Introduction"}, {"figure_path": "yOe6ajdslI/figures/figures_8_1.jpg", "caption": "Figure 3: Results in the case where true and input class-priors on the training distribution \u03c0tr can be different: average test AUCs and standard errors over different class-priors on the test distribution \u03c0te within {0.1, 0.2, 0.3} with true training class-prior \u03c0tr = 0.1 when changing the input class-prior on the training distribution.", "description": "This figure shows the results of an ablation study on the impact of the input class prior (\u03c0tr) on the test AUC of the proposed method.  The study was conducted across four datasets (MNIST, FashionMNIST, SVHN, CIFAR10) and with three different test class priors (\u03c0te). The x-axis represents different input class priors used in the experiment, while the y-axis represents the test AUC.  Error bars indicate standard errors. The results indicate that the performance of the proposed method is influenced by how well the input class prior matches the true class prior.", "section": "5 Experiments"}, {"figure_path": "yOe6ajdslI/figures/figures_8_2.jpg", "caption": "Figure 3: Results in the case where true and input class-priors on the training distribution \u03c0tr can be different: average test AUCs and standard errors over different class-priors on the test distribution \u03c0te within {0.1, 0.2, 0.3} with true training class-prior \u03c0tr = 0.1 when changing the input class-prior on the training distribution.", "description": "This figure shows the performance of the proposed method when the input class prior on the training data is different from the true class prior.  The x-axis represents the input class prior, and the y-axis represents the average test AUC.  Each subplot corresponds to a different dataset (MNIST, FashionMNIST, SVHN, and CIFAR10). Error bars represent standard errors. The results show that the performance of the proposed method is relatively robust to differences between the input and true class priors, especially when the input class prior is larger than the true class prior.", "section": "5 Experiments"}, {"figure_path": "yOe6ajdslI/figures/figures_9_1.jpg", "caption": "Figure 5: Average test AUCs and standard errors of the proposed method over different class-priors on the test distribution when changing the number of labeled positive data in the training distribution NP.", "description": "This figure shows how the performance of the proposed method changes when varying the number of labeled positive data (NP) in the training dataset across four different datasets (MNIST, FashionMNIST, SVHN, CIFAR10).  Each subplot represents a dataset, illustrating the average test AUC and its standard error. The x-axis represents the number of labeled positive data points in the training set, and the y-axis represents the average test AUC.  The error bars indicate the standard deviation, showing the variability of the results. The figure demonstrates the impact of the amount of labeled positive data on the performance of the proposed AUC maximization method.", "section": "5.4 Results"}, {"figure_path": "yOe6ajdslI/figures/figures_16_1.jpg", "caption": "Figure 6: Dynamics of training loss, validation loss, and test AUC when (\u03c0tr, \u03c0te) = (0.1, 0.3). Each column represents the results of Ours, Ours w/ nn, and Ours w/ b, respectively from left to right. The value of b = (1 \u2013 \u03c0tr)(1 \u2013 \u03c0te)/2 is 0.315 for all datasets.", "description": "This figure displays the training loss, validation loss, and test AUC over epochs for three variations of the proposed method: the original method, the method with non-negative loss correction, and the method with a tighter loss bound.  It shows how these metrics evolve during training for four different datasets (MNIST, Fashion MNIST, SVHN, CIFAR10) and demonstrates the impact of the loss correction techniques on overfitting and model performance. The constant b, used in the tighter bound, is also shown.", "section": "4 Proposed Method"}]