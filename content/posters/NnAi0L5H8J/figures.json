[{"figure_path": "NnAi0L5H8J/figures/figures_1_1.jpg", "caption": "Figure 1: Margin violations in the instance space and the label space. (a) and (b) depict the attention scores of ELIMIPL and MIPLMA for the same test bag in the FMNIST-MIPL dataset. Orange and blue colors indicate attention scores assigned to positive and negative instances, respectively. (c)-(f) show the highest predicted probabilities for candidate labels (green) and non-candidate labels (blue) by ELIMIPL or MIPLMA in the CRC-MIPL-Row dataset. (c) and (e) correspond to the same training bag, while (d) and (f) refer to another training bag.", "description": "This figure illustrates the problem of margin violations in both instance and label space that existing MIPL algorithms suffer from. Subfigures (a) and (b) compare the attention scores given to positive and negative instances by ELIMIPL and MIPLMA, respectively. Subfigures (c) to (f) compare the highest predicted probabilities given to candidate and non-candidate labels by both algorithms across two different training bags.  The comparison highlights how MIPLMA addresses the issue of margin violation by increasing the margin between positive and negative instances as well as candidate and non-candidate labels.", "section": "1 Introduction"}, {"figure_path": "NnAi0L5H8J/figures/figures_2_1.jpg", "caption": "Figure 2: The MIPLMA framework processes an input comprising the multi-instance bag X\u2081 = {Xi,1, Xi,2,, X\u2081,9} and the candidate label set S\u2081 = {2, 3, 5, 7}, where La and Lm represent the dynamic disambiguation loss and the margin distribution loss, respectively.", "description": "This figure illustrates the architecture of the proposed MIPLMA algorithm.  It shows how multi-instance bags (X\u1d62) and their associated candidate label sets (S\u1d62) are processed. The process starts with extracting instance-level features (H\u1d62) using a feature extractor (\u03c8). A margin-aware attention mechanism aggregates these features into a bag-level representation (z\u1d62), dynamically adjusting margins for attention scores. Simultaneously, predicted probabilities (p\u1d62) are generated and their margins are adjusted using a margin distribution loss (L\u2098) and a dynamic disambiguation loss (L\u2090).  These losses guide the classifier in accurately predicting the true label from the candidate set. ", "section": "2 The Proposed Approach"}, {"figure_path": "NnAi0L5H8J/figures/figures_8_1.jpg", "caption": "Figure 1: Margin violations in the instance space and the label space. (a) and (b) depict the attention scores of ELIMIPL and MIPLMA for the same test bag in the FMNIST-MIPL dataset. Orange and blue colors indicate attention scores assigned to positive and negative instances, respectively. (c)-(f) show the highest predicted probabilities for candidate labels (green) and non-candidate labels (blue) by ELIMIPL or MIPLMA in the CRC-MIPL-Row dataset. (c) and (e) correspond to the same training bag, while (d) and (f) refer to another training bag.", "description": "This figure shows examples of margin violations in both instance and label spaces during the training process of two multi-instance partial-label learning (MIPL) algorithms, ELIMIPL and the proposed MIPLMA.  The top row illustrates how attention scores assigned to positive and negative instances can be very close (a,b), whereas the bottom row depicts scenarios where the model assigns its highest predicted probability to a non-candidate label. MIPLMA aims to mitigate such violations.", "section": "1 Introduction"}, {"figure_path": "NnAi0L5H8J/figures/figures_15_1.jpg", "caption": "Figure 1: Margin violations in the instance space and the label space. (a) and (b) depict the attention scores of ELIMIPL and MIPLMA for the same test bag in the FMNIST-MIPL dataset. Orange and blue colors indicate attention scores assigned to positive and negative instances, respectively. (c)-(f) show the highest predicted probabilities for candidate labels (green) and non-candidate labels (blue) by ELIMIPL or MIPLMA in the CRC-MIPL-Row dataset. (c) and (e) correspond to the same training bag, while (d) and (f) refer to another training bag.", "description": "This figure demonstrates the margin violations in both instance and label spaces by comparing ELIMIPL and MIPLMA. The first row shows the attention scores assigned to positive and negative instances for a test bag in FMNIST-MIPL dataset. The second row shows the highest predicted probabilities for candidate and non-candidate labels by ELIMIPL and MIPLMA on two different training bags in CRC-MIPL-Row dataset. It visually illustrates how MIPLMA effectively addresses the margin violation issues, which were commonly observed in previous MIPL algorithms.", "section": "1 Introduction"}, {"figure_path": "NnAi0L5H8J/figures/figures_18_1.jpg", "caption": "Figure 1: Margin violations in the instance space and the label space. (a) and (b) depict the attention scores of ELIMIPL and MIPLMA for the same test bag in the FMNIST-MIPL dataset. Orange and blue colors indicate attention scores assigned to positive and negative instances, respectively. (c)-(f) show the highest predicted probabilities for candidate labels (green) and non-candidate labels (blue) by ELIMIPL or MIPLMA in the CRC-MIPL-Row dataset. (c) and (e) correspond to the same training bag, while (d) and (f) refer to another training bag.", "description": "This figure illustrates the problem of margin violations in both instance and label spaces that existing MIPL algorithms often suffer from.  The top row shows attention scores for instances within a bag, highlighting how ELIMIPL assigns similar attention scores to positive and negative instances, while MIPLMA better distinguishes them. The bottom row shows predicted probabilities for candidate and non-candidate labels, demonstrating that ELIMIPL can assign higher probability to a non-candidate label than a candidate label, while MIPLMA shows better separation.", "section": "1 Introduction"}, {"figure_path": "NnAi0L5H8J/figures/figures_19_1.jpg", "caption": "Figure A3: The classification accuracies (mean and std) of MIPLMA with varying \u03bb on Birdsong-MIPL dataset (r \u2208 {1, 2, 3}). The diameter of the circle represents the relative standard deviation.", "description": "This figure shows how the classification accuracy of the MIPLMA algorithm varies with different values of the hyperparameter \u03bb (lambda) on the Birdsong-MIPL dataset for different numbers of false positive labels (r).  The plot indicates that the optimal value of \u03bb for achieving the highest accuracy depends on the number of false positives. The error bars represent the standard deviations of the accuracy across multiple runs.", "section": "Additional Experiment Results"}, {"figure_path": "NnAi0L5H8J/figures/figures_19_2.jpg", "caption": "Figure 1: Margin violations in the instance space and the label space. (a) and (b) depict the attention scores of ELIMIPL and MIPLMA for the same test bag in the FMNIST-MIPL dataset. Orange and blue colors indicate attention scores assigned to positive and negative instances, respectively. (c)-(f) show the highest predicted probabilities for candidate labels (green) and non-candidate labels (blue) by ELIMIPL or MIPLMA in the CRC-MIPL-Row dataset. (c) and (e) correspond to the same training bag, while (d) and (f) refer to another training bag.", "description": "The figure visualizes margin violations in instance and label spaces, comparing ELIMIPL and MIPLMA.  Subfigures (a) and (b) show attention scores (positive instances in orange, negative in blue) for a test bag in the FMNIST-MIPL dataset, highlighting MIPLMA's improved margin. Subfigures (c) to (f) display the highest predicted probabilities for candidate (green) vs. non-candidate (blue) labels in two different training bags from the CRC-MIPL-Row dataset. These demonstrate MIPLMA's superior ability to maintain larger margins between candidate and non-candidate probabilities.", "section": "1 Introduction"}]