[{"figure_path": "7uWzoGn4kv/tables/tables_7_1.jpg", "caption": "Table 1: Comparison on the zero-shot transfer over EgoMCQ, EK100-MIR, EK100-CLS, and EGTEA. HelpingHands* refers to our re-produced results with TSF-B backbone using provided source code [4].", "description": "This table presents a comparison of the proposed HENASY model's performance against several state-of-the-art (SOTA) video-language models on four zero-shot transfer tasks.  The tasks cover various aspects of egocentric video understanding: multi-choice question answering (EgoMCQ), video-text retrieval (EK100-MIR), action classification (EK100-CLS), and temporal action localization (EGTEA).  The metrics used vary depending on the task, but generally involve measures of accuracy and ranking performance.  The table highlights HENASY's competitive performance compared to existing methods, especially in the context of zero-shot transfer where the model is not fine-tuned for the specific tasks.", "section": "5.2 Benchmarks and Evaluation Protocols"}, {"figure_path": "7uWzoGn4kv/tables/tables_7_2.jpg", "caption": "Table 2: Comparison on the visual & textual representation over EgoNLQ and EgoMQ. Grey indicates result we obtained using provided pre-trained checkpoint that.", "description": "This table compares the performance of different methods on two egocentric video tasks: EgoNLQ (episodic memory task involving free-form text queries) and EgoMQ (episodic memory task that involves identifying and categorizing action instances). The metrics used for evaluation are mIoU (mean Intersection over Union) at thresholds of 0.3 and 0.5 for EgoNLQ and Recall@K (R1@0.5, R5@0.5) and mAP for EgoMQ.  The results show that the proposed method ('Ours') achieves state-of-the-art performance on these benchmarks, surpassing previous approaches such as SlowFast, EgoVLP, LaViLa, and HelpingHands.", "section": "5.2 Benchmarks and Evaluation Protocols"}, {"figure_path": "7uWzoGn4kv/tables/tables_8_1.jpg", "caption": "Table 3: Ablation results on multi-grained losses.", "description": "This table presents the ablation study results of the HENASY model, focusing on the impact of different loss functions.  It shows the performance of the model on three different benchmark datasets (EgoMCQ, EK100-MIR, EK100-CLS) when trained with various combinations of four loss functions:  Lego (egocentric contrastive loss), LNEC (noun-entity contrastive loss), LVEC (verb-entities contrastive loss), and Lproj (projection loss). Each row represents a different training configuration, indicated by checkmarks (\u2713) and crosses (\u2717) to show which losses were included. The results demonstrate the contribution of each loss component to the overall model performance.", "section": "5.4 Ablation Studies"}, {"figure_path": "7uWzoGn4kv/tables/tables_9_1.jpg", "caption": "Table 4: Ablation results on model design.", "description": "This table presents the ablation study results on the model design of HENASY. It shows the performance of the model on various tasks (EgoMCQ, EK100-MIR, and EK100-CLS) under different model design settings. Specifically, it compares the performance when using average pooling instead of an entity-aware decoder, when using only a self-attention decoder without the entity-aware decoder, and when removing the bootstrapping stage.  The results demonstrate the impact of each component on the overall performance and the effectiveness of the proposed design choices.", "section": "5.4 Ablation Studies"}, {"figure_path": "7uWzoGn4kv/tables/tables_9_2.jpg", "caption": "Table 5: Comparison on computational complexity and memory cost.", "description": "This table compares the computational complexity and memory costs of HENASY and HelpingHands.  It shows that HENASY has a higher computational cost (GFLOPs) and uses more parameters and GPU memory during both training and inference. However, it achieves a significantly faster inference time (1.02 seconds vs. 2.87 seconds). The difference in inference time is primarily attributed to the different decoder architectures used by the two models; HENASY uses a non-autoregressive decoder while HelpingHands employs an autoregressive decoder, which inherently limits parallelization and increases computation time.", "section": "5 Experiments"}, {"figure_path": "7uWzoGn4kv/tables/tables_14_1.jpg", "caption": "Table 6: Comparison with HelpingHands on visual grounding task.", "description": "This table compares the performance of the proposed HENASY model and the HelpingHands model on a visual grounding task.  The metric used is mean Intersection over Union (mIoU), which measures the overlap between the predicted segmentation mask and the ground truth mask.  The results show that HENASY significantly outperforms HelpingHands, indicating superior visual grounding capabilities.", "section": "5.4 Ablation Studies"}]