{"references": [{"fullname_first_author": "Kristen Grauman", "paper_title": "Ego4D: Around the world in 3,000 hours of egocentric video", "publication_date": "2022-06-01", "reason": "This paper introduces Ego4D, a large-scale egocentric video dataset crucial for training and evaluating the HENASY model, providing the foundation for its empirical results."}, {"fullname_first_author": "Dima Damen", "paper_title": "Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100", "publication_date": "2022-01-01", "reason": "This paper presents EpicKitchens-100, another significant egocentric video dataset used for benchmarking the HENASY model, demonstrating its performance on diverse tasks and datasets."}, {"fullname_first_author": "Kevin Qinghong Lin", "paper_title": "Egocentric video-language pretraining", "publication_date": "2022-12-01", "reason": "This paper proposes a foundational egocentric video-language model (EgoVLP) that HENASY builds upon, providing a baseline for comparison and demonstrating the advancement of HENASY."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a crucial visual-language model which forms the basis of many video-language models, including HENASY, and its dual-encoder architecture."}, {"fullname_first_author": "Chuhan Zhang", "paper_title": "Helping hands: An object-aware ego-centric video recognition model", "publication_date": "2023-10-01", "reason": "This paper introduces the HelpingHands model, a direct competitor of HENASY, enabling detailed comparison and highlighting HENASY's improvements and unique aspects."}]}