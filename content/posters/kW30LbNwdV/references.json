{"references": [{"fullname_first_author": "Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2017-06-06", "reason": "This paper is foundational for adversarial training, a core concept extended in the current research."}, {"fullname_first_author": "Goldblum", "paper_title": "Adversarially robust distillation", "publication_date": "2020-00-00", "reason": "This paper introduces adversarial robustness distillation (ARD), a key technique directly built upon in this work."}, {"fullname_first_author": "Xu", "paper_title": "To be robust or to be fair: Towards fairness in adversarial training", "publication_date": "2021-00-00", "reason": "This paper addresses adversarial robust fairness, a central problem the current work aims to improve."}, {"fullname_first_author": "Zhang", "paper_title": "Theoretically principled trade-off between robustness and accuracy", "publication_date": "2019-00-00", "reason": "This paper provides theoretical insights into the robustness-accuracy trade-off, which is relevant to the fairness problem studied."}, {"fullname_first_author": "Zhu", "paper_title": "Reliable adversarial distillation with unreliable teachers", "publication_date": "2021-00-00", "reason": "This paper offers another perspective on knowledge distillation in adversarial settings, providing a related approach for comparison."}]}