[{"type": "text", "text": "Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Shiji Zhao1, Ranjie Duan2, Xizhe Wang1, Xingxing Wei1\u2217 1Institute of Artificial Intelligence, Beihang University, Beijing, China 2Security Department, Alibaba Group, Hangzhou, China {zhaoshiji123,xizhewang,xxwei}@buaa.edu.cn, ranjieduan@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adversarial Training (AT) has been widely proved to be an effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD) has demonstrated its superior performance in improving the robustness of small student models with the guidance of large teacher models. However, both AT and ARD encounter the robust fairness problem: these models exhibit strong robustness when facing part of classes (easy class), but weak robustness when facing others (hard class). In this paper, we give an in-depth analysis of the potential factors and argue that the smoothness degree of samples\u2019 soft labels for different classes (i.e., hard class or easy class) will affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Based on the above finding, we propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of Knowledge Distillation (KD). Specifically, ABSLD adaptively reduces the student\u2019s error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples\u2019 soft labels during the training process, and the smoothness degree of soft labels is controlled by assigning different temperatures in KD to different classes. Extensive experiments demonstrate that ABSLD outperforms state-of-theart AT, ARD, and robust fairness methods in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have achieved great success in various tasks, e.g., classification [10], detection [6], and segmentation [24]. However, DNNs are vulnerable to adversarial attacks [30; 35; 33; 34], where adding small perturbations to the input examples will lead to misclassification. To enhance the robustness of DNNs, Adversarial Training (AT) [20; 41; 32; 14] is proposed and has been proven to be an effective method to defend against adversarial examples. To further improve the robustness, Adversarial Robustness Distillation (ARD) [7] as a variant of AT is proposed and aims to transfer the robustness of the large models into the small models based on Knowledge Distillation (KD), and further researches [44; 45; 43; 12; 42] show the excellent performance of ARD. ", "page_idx": 0}, {"type": "text", "text": "Although AT and ARD can remarkably improve the adversarial robustness, some researches [2; 31; 39; 19; 28; 38] demonstrate the robust fairness problem: these models perform strong robustness on part of classes (easy class) but show high vulnerability on others (hard class). This phenomenon will raise further attention to class-wise security. Specifically, an overall robust model appears to be relatively safe for model users, however, the robust model with poor robust fairness will lead to attackers targeting vulnerable classes of the model, which leads to significant security risks to potential applications. Different from simply improving the overall robustness, some methods are proposed to address the robust fairness problem in AT and ARD [39; 36; 38; 19; 28] (i.e., improving the worst-class robustness as much as possible without sacrificing too much overall robustness). However, the robust fairness problem still exists and requires further to be explored. ", "page_idx": 0}, {"type": "image", "img_path": "kW30LbNwdV/tmp/0a415e15b5ed40da09b4677d6f40c40b930d9b5b8a62c2e301a803508840ee23.jpg", "img_caption": ["Figure 1: The comparison between the sample-based fair adversarial training and our label-based fair adversarial training. For the former ideology in (a), the trained model\u2019s bias is avoided by re-weighting the sample\u2019s importance according to the different contribution to fairness. For the latter ideology in (b), the trained model\u2019s bias is avoided by re-temperating the smoothness degree of soft labels for different classes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For that, we give an in-depth analysis of the potential factors to influence robust fairness in the optimization objective function. From the perspective of the training sample, the sample itself has a certain degree of biased behavior, which is mainly reflected in the different learning difficulties and various vulnerabilities to adversarial attacks. For this reason, previous works apply the reweighting ideology to achieve robust fairness for different types of classes in the optimization process [39; 36; 38]. However, as another important factor in the optimization objective function, the role of the labels applied to guide the model\u2019s training is ignored. Labels can be divided into two types, including one-hot labels and soft labels, where the soft labels are widely studied [29; 11] and have been proven effective in improving the performance of DNNs. Inspired by this, we try to explore robust fairness from the perspective of samples\u2019 soft labels. Interestingly, we first find that the smoothness degree of soft labels for different classes (i.e., hard and easy class) can affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Intuitively speaking, sharper soft labels mean larger supervision intensity, while smoother soft labels mean smaller supervision intensity, so it is helpful to improve robust fairness by assigning sharp soft labels for hard classes and smooth soft labels for easy classes. ", "page_idx": 1}, {"type": "text", "text": "Based on the above finding, we further propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of knowledge distillation. ABSLD can adaptively adjust the smoothness degree of soft labels by re-temperating the teacher\u2019s soft labels for different classes, and each class has its own teacher\u2019s temperatures based on the student\u2019s error risk. For instance, when the student performs more error risk in some classes, ABSLD will compute sharp soft labels by assigning lower temperatures, and the student\u2019s learning intensity for these classes will relatively increase compared with other classes. After the optimization, the student\u2019s robust error risk gap between different classes will be reduced. The code can be found in https://github.com/zhaoshiji123/ABSLD. ", "page_idx": 1}, {"type": "text", "text": "Our contribution can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We explore the labels\u2019 effects on the adversarial robust fairness of DNNs, which is different from the existing sample perspective. To the best of our knowledge, we are the first one to find that the smoothness degree of samples\u2019 soft labels for different types of classes can affect the robust fairness from both empirical observation and theoretical analysis. \u2022 We propose the Anti-Bias Soft Label Distillation (ABSLD) to enhance the adversarial robust fairness within the framework of knowledge distillation. Specifically, we re-temperate the teacher\u2019s soft labels to adjust the class-wise smoothness degree and further reduce the student\u2019s error risk gap between different classes in the training process. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We empirically verify the effectiveness of ABSLD. Extensive experiments on different datasets and models demonstrate that our ABSLD can outperform state-of-the-art methods against a variety of attacks in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Adversarial Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To defend against the adversarial examples, Adversarial Training (AT) [20; 41; 32; 13; 25] is regarded as an effective method to obtain robust models. AT can be formulated as a min-max optimization problem as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}E_{(x,y)\\sim\\mathcal{D}}[\\operatorname*{max}_{\\delta\\in\\Omega}\\mathcal{L}(f(x+\\delta;\\theta),y)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f(\\cdot;\\theta)$ represents a deep neural network with weight $\\theta$ , $D$ represents a data distribution with clean example $x$ and the ground truth label $y$ . $\\mathcal{L}$ represents the optimization loss function, e.g. the cross-entropy loss. $\\delta$ represents the adversarial perturbation, and $\\Omega$ represents a bound, which can be defined as $\\Omega=\\{\\delta:\\bar{||}\\delta||\\leq\\epsilon\\}$ with the maximum perturbation scale $\\epsilon$ . To further improve the performance, some variant methods of AT appear including regularization [21; 41; 32], using additional data [27; 22], and optimizing iteration process [15; 23]. Different from the above methods for improving the overall robustness, in this paper, we focus on solving the robust fairness problem. ", "page_idx": 2}, {"type": "text", "text": "2.2 Adversarial Robustness Distillation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation [11] as a training method can effectively transfer the large model\u2019s knowledge into the small model\u2019s knowledge, which has been widely applied in different areas. To enhance the adversarial robustness of small DNNs, Goldblum et al. [7] first propose Adversarial Robustness Distillation (ARD) by applying the clean prediction distribution of strong robust teacher models to guide the adversarial training of student models. Zhu et al. [44] argue that the prediction of the teacher model is not so reliable, and composite with unreliable teacher guidance and student introspection during the training process. RSLAD [45] applies the teacher clean prediction distribution as the guidance to train both clean examples and adversarial examples. MTARD [43; 42] applies clean teacher and adversarial teacher to enhance both accuracy and robustness, respectively. AdaAD [12] adaptively searches for optimal match points by directly applying the teacher adversarial prediction distribution in the inner maximization. In this paper, we explore how to enhance robust fairness within the framework of knowledge distillation. ", "page_idx": 2}, {"type": "text", "text": "2.3 Adversarial Robust Fairness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Some researchers address the robust fairness problem from different views [39; 18; 36; 38; 19; 37; 28] and improve the fairness without losing too much robustness. The most intuitive idea is to give different weights to the sample of different classes in the optimization process, and Xu et al. [39] propose Fair Robust Learning (FRL), which adjusts the loss weight and the adversarial margin based on the prediction accuracy of different classes. Ma et al. [19] finds the trade-off exists between robustness and fairness and propose Fairly Adversarial Training to mitigate this phenomenon by adding a regularization loss to control the variance of class-wise adversarial error risk. Sun et al. [28] propose Balance Adversarial Training (BAT) to achieve both source-class fairness (different difficulties in generating adversarial examples from each class) and target-class fairness (disparate target class tendencies when generating adversarial examples). Wu et al. [37] argue that the maximum entropy regularization for the model\u2019s prediction distribution can help to achieve robust fairness. Wei et al. [36] propose Class-wise Calibrated Fair Adversarial Training (CFA) to address fairness by dynamically customizing adversarial configurations for different classes and modifying the weight averaging operation. To enhance the ARD robust fairness, Yue et al. [38] propose Fair-ARD by re-weighting different classes based on the vulnerable degree. Different from these sample-based fairness methods, we try to solve this problem from the perspective of samples\u2019 labels, by adjusting the class-wise smoothness degree of samples\u2019 soft labels in the optimization process. ", "page_idx": 2}, {"type": "image", "img_path": "kW30LbNwdV/tmp/137a88c0843c33328188ec8bafd6804332634d0b06d7645f777b05b145223353.jpg", "img_caption": ["(a) Class-wise and average Robustness(b) Class-wise and average Robustness of ResNet-18 on CIFAR-10. of MobileNet-v2 on CIFAR-10. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The class-wise and average robustness of DNNs guided by soft labels with the same smoothness degree (SSD) and different smoothness degree (DSD) for different classes, respectively. For the soft labels with different smoothness degrees, we use sharper soft labels for hard classes and use smoother soft labels for easy classes. We select two DNNs (ResNet-18 and MobileNet-v2) trained by SAT [20] on CIFAR-10. The robust accuracy is evaluated based on PGD. The checkpoint is selected based on the best checkpoint of the highest mean value of all-class average robustness and the worst class robustness following [36]. We see that blue lines and red lines have similar average robustness, but the worst robustness of blue lines are remarkably improved compared with red lines. ", "page_idx": 3}, {"type": "text", "text": "3 Robust Fairness via Smoothness Degree of Soft Labels ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As an important part of the model optimization, label information used to guide the model plays an important role. The label can be divided into one-hot labels and soft labels, where one-hot labels only contain one class\u2019s information and soft labels can be considered as an effective way to alleviate over-fitting and improve the performance [29]. Previous methods usually ignore the class-wise smoothness degree of soft labels, either applying the same smoothness degree [29], or uniformly changing the smoothness degree of soft labels for all the classes without deliberate adjustments [11]. Different from previous methods, we are curious about if we adjust the class-wise smoothness degree of soft labels, will it influence the class-wise robust fairness of the trained model? Intuitively speaking, different smoothness degree of soft labels denote different supervision intensity, which means that it is possible to achieve fairness by adjusting the smoothness degree of soft labels. Here we try to explore the relationship between class-wise smoothness degree of soft labels and the robust fairness from both empirical observation and theoretical analysis. ", "page_idx": 3}, {"type": "text", "text": "3.1 Empirical Observation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we focus on the impact of the class-wise smoothness degree of soft labels on adversarial training. First, we train the model with soft labels that have the same smoothness degree for all types of classes (smoothing coefficient2 is 0.2). Then we assign different smoothness degrees of soft labels for hard classes and easy classes: specifically, we manually use sharper soft labels for hard classes (smoothing coefficient is 0.05) and smoother soft labels for easy classes (smoothing coefficient is 0.35). We conduct the experiment based on the SAT [20] shown in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "The result shows that the class-wise smoothness degree of soft labels has an impact on class-wise robust fairness. When we apply the sharper smoothness degree of soft labels for hard classes and the smoother smoothness degree of soft labels for easy classes, the class-wise robust fairness problem can be alleviated. More specifically, for the two worst classes (class 4, 5), the robust accuracy of ResNet-18 guided by the soft label distribution with the same smoothness degree is $24.2\\%$ , and $30.9\\%$ , and the robust accuracy of ResNet-18 guided by the soft label distribution with different smoothness degree is $30.2\\%$ , and $39.1\\%$ , which exists an obvious improvement for the class-wise robust fairness, and the average robust accuracy has a slight improvement $(52.12\\%$ vs $52.36\\%$ ). Similar performance can also be observed in MobileNet-v2. This phenomenon indicates that appropriately assigning class-wise smoothness degrees of soft labels can be beneficial to achieve robust fairness. ", "page_idx": 3}, {"type": "text", "text": "3.2 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we try to theoretically analyze the impact of the smoothness degree of soft labels on class-wise fairness. Firstly, we want to analyze the model bias performance with the guidance of the soft label distribution with the same smoothness degree. Then we give Corollary 1 by extending the prediction distribution of binary linear classifier into the prediction distribution of DNNs based on the theoretical analysis in [39] and [19]. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. A dataset $(x,y)\\sim\\mathcal{D}$ contains 2 classes (hard class $c_{+}$ and easy class $c_{-}$ ). Based on the label distribution $y$ , the soft label distribution with same smoothness degree $P_{\\lambda1}=\\{p_{c_{+}}^{\\lambda1},p_{c_{-}}^{\\lambda1}\\}$ can be generated and satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n1>p_{c_{-}}^{\\lambda1}(x_{c_{-}})=p_{c_{+}}^{\\lambda1}(x_{c_{+}})>0.5,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If a DNN model $f$ is optimized by minimizing the average optimization error risk in $\\mathcal{D}$ with the guidance of the equal soft labels $\\bar{P}_{\\lambda1}=\\{p_{c_{+}}^{\\lambda1},p_{c_{-}}^{\\lambda1}\\}$ , and obtain the relevant parameter $\\theta_{\\lambda1}$ , where the optimization error risk is measured by Kullback\u2013Leibler divergence loss $(K L)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x;\\theta_{\\lambda1})=\\arg\\operatorname*{min}_{f}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}(K L(f(x;\\theta_{\\lambda1});P_{\\lambda1})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then the error risks (the expectation that samples are wrongly predicted by the model) for classes $c_{+}$ and $c_{-}$ have a relationship as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(f(x_{c_{+}};\\theta_{\\lambda1}))>R(f(x_{c_{-}};\\theta_{\\lambda1})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the error risks can be defined: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(f(x_{c_{+}};\\theta_{\\lambda1}))=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}(C E(f(x_{c_{+}};\\theta_{\\lambda1});y_{c_{+}})),}\\\\ &{R(f(x_{c_{-}};\\theta_{\\lambda1}))=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}(C E(f(x_{c_{-}};\\theta_{\\lambda1});y_{c_{-}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Corollary 1 demonstrates that when optimizing hard and easy classes with equal intensity, the model will inevitably be biased, and this bias mainly comes from the characteristics of the sample itself and is not related to the optimization method. Based on the Corollary 1, we can further analyze the performance differences with the guidance of the different types of soft labels. Here we provide Theorem 1 about the relationship between class-wise smoothness degree of soft labels and fairness. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Following the setting in Corollary $^{\\,l}$ , for a dataset $\\mathcal{D}$ containing 2 classes ( $c_{+}$ and $c_{-,}$ ), two soft label distribution $(P_{\\lambda1}=\\{p_{c_{+}}^{\\lambda1},p_{c_{-}}^{\\dot{\\lambda1}}\\}$ and $P_{\\lambda2}=\\{p_{c_{+}}^{\\lambda2},p_{c_{-}}^{\\lambda2}\\})$ exist, where $P_{\\lambda2}$ have $a$ correct prediction distribution but have a limited different class-wise smoothness degree of soft labels $(v_{1}>0,\\,v_{2}>0)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{1>p_{c_{+}}^{\\lambda2}(x_{c_{+}})=\\!p_{c_{+}}^{\\lambda1}(x_{c_{+}})+v_{1}>p_{c_{+}}^{\\lambda1}(x_{c_{+}})=}}\\\\ {{p_{c_{-}}^{\\lambda1}(x_{c_{-}})>p_{c_{-}}^{\\lambda2}(x_{c_{-}})=p_{c_{-}}^{\\lambda1}(x_{c_{-}})-v_{2}>0.5,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then the model is trained with the guidance of the soft label distribution $P_{\\lambda1}$ and soft label distribution $P_{\\lambda2}$ and obtains the trained model parameters $\\theta_{\\lambda1}$ and $\\theta_{\\lambda2}$ , respectively. If the model parameter $\\theta_{\\lambda2}$ still satisfies: $R(f(x_{c_{+}};\\theta_{\\lambda2}))>\\bar{R(f(x_{c_{-}};\\theta_{\\lambda2}))}$ , then the model\u2019s error risk for hard classes $c_{+}$ and easy classes $c_{-}$ has a relationship as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR(f(x_{c_{+}};\\theta_{\\lambda1}))-R(f(x_{c_{-}};\\theta_{\\lambda1}))>R(f(x_{c_{+}};\\theta_{\\lambda2}))-R(f(x_{c_{-}};\\theta_{\\lambda2})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 can be found in Appendix A.1. Based on Theorem 1, the class-wise smoothness degree of soft labels theoretically has an impact on class-wise robust fairness. If the soft label distribution with different smoothness degree $P_{\\lambda2}$ is applied to guide the model training, where the sharper smoothness degree of soft labels for hard classes and smoother smoothness degree of soft labels for easy classes, the model will appear smaller error risk gap between easy and hard class compared with the soft label distribution with same smoothness degree $P_{\\lambda1}$ , which demonstrates better robust fairness. The Theorem 1 theoretically demonstrates that if we appropriately adjust the class-wise smoothness degree of soft labels, the model can achieve class-wise robust fairness. ", "page_idx": 4}, {"type": "text", "text": "4 Anti-Bias Soft Label Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Overall Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the above finding, adjusting the class-wise smoothness degree of soft labels can be regarded as a potential way to obtain robust fairness. Then we consider introducing this ideology into Knowledge Distillation (KD), which has been proven to be an effective method to improve the robustness of small models [44; 45; 43; 12; 42]. Since the core idea of KD is to use the teacher\u2019s soft labels to guide the student\u2019s optimization process, we can adjust the class-wise smoothness degree of soft labels and obtain the student with both strong robustness and fairness. ", "page_idx": 5}, {"type": "text", "text": "Here we propose the Anti-Bias Soft Label Distillation (ABSLD) to obtain a student with adversarial robust fairness. We formulate the optimization objective function for ABSLD as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f_{s}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\big(\\mathcal{L}_{a b s l d}\\big(\\tilde{\\boldsymbol{x}},\\boldsymbol{x};f_{s},\\boldsymbol{f}_{t}^{\\prime}\\big)\\big),}\\\\ &{s.t.\\;R(f_{s}(\\tilde{\\boldsymbol{x}}_{k}))=\\displaystyle\\frac{1}{C}\\sum_{i=1}^{C}R(f_{s}(\\tilde{\\boldsymbol{x}}_{i})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{i}$ and $\\tilde{x}_{i}$ are the clean examples and adversarial examples of the $i$ -th class, $f_{s}$ denotes the student model, $f_{t}^{'}$ denotes the teacher model with Anti-Bias Soft Labels, $\\mathbf{C}$ is the total number of classes, $\\mathcal{L}_{a b s l d}$ is the loss function, and $R(f_{s}(\\tilde{x}_{k}))$ denotes the robust error risk of $k$ -th class in student model $f_{s}$ . Here we apply the Cross-Entropy loss $C E(f_{s}(\\tilde{x}_{k}),y)$ as the evaluation criterion of the optimization error risk $R(f_{s}(\\tilde{x}_{k}))$ following [19]. ", "page_idx": 5}, {"type": "text", "text": "4.2 Re-temperate Teacher\u2019s Soft Labels ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to adjust the class-wise smoothness degree of soft labels in ARD, two options exist: one is to use student feedback to update the teacher parameter in the process of optimizing students, but this option requires retraining the teacher model, which may bring the pretty optimization difficulty and computational overhead; the other is to directly adjust the smoothness degree of soft labels for different classes. Inspired by [42], we apply the temperature as a means of directly controlling the smoothness degree of soft labels during the training process. Here, we provide Theorem 2 about the relationship between the teacher\u2019s temperature and the student\u2019s class-wise error risk gap. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. If the teacher $f_{t}^{'}$ has a correct prediction distribution, the teacher temperature $\\tau_{c+}^{t}$ of hard class $c+$ is positively correlated with the error risk gap for student $f_{s}$ , and the teacher temperature $\\tau_{c-}^{t}$ of easy class $c-$ is negatively correlated with the error risk gap for student $f_{s}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 can be found in Appendix A.2. In particular, just as the conclusion in [38]: The teacher has a more correct prediction distribution than the student even in the worst classes, which means Theorem 2 holds in most cases. Theorem 2 demonstrates that the different temperatures can influence the student robust fairness: when the student\u2019s error risk of $k$ -th class is larger than the average error risk, we think that this type of class is relatively hard compared with others, then the teacher temperature for $k$ -th class will reduce and the smoothness degree of soft labels will be sharper, and the optimization gap between teacher distribution and student distribution in $k$ -th class will corresponding increase, leading to stronger learning intensity for $k$ -th class and final reduce the student\u2019s class-wise optimization error risk gap. ", "page_idx": 5}, {"type": "text", "text": "To achieve the above optimization goal, we adjust the teacher\u2019s $k$ -th class temperature $\\tilde{\\tau}_{k}^{t}$ for the guidance of adversarial examples as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\tau}_{k}^{t}=\\tilde{\\tau}_{k}^{t}-\\beta\\cdot\\frac{R(f_{s}(\\tilde{x}_{k}))-\\frac{1}{C}\\sum_{i=1}^{C}R(f_{s}(\\tilde{x}_{i}))}{m a x(|R(f_{s}(\\tilde{x}_{k}))-\\frac{1}{C}\\sum_{i=1}^{C}R(f_{s}(\\tilde{x}_{i}))|)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ is the learning rate, $m a x(.)$ denotes taking the maximum value, and $|.|$ represents taking the absolute value, $m a x(|.|)$ is applied for regularization to maintain the stability of optimization. The update operation in Eq.(10) can change the teacher temperature $\\tilde{\\tau}_{k}^{t}$ based on the gap between the student\u2019s $k$ -th class error risk $R(f_{s}(\\tilde{x}_{k}))$ and the average error risk $\\begin{array}{r}{\\frac{1}{C}\\sum_{i=1}^{C}R(f_{s}(\\tilde{x}_{i}))}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Meanwhile, according to [39], both clean and adversarial examples exist the fairness problems and can affect each other, so it is necessary to achieve fairness for both types of data. Since clean and ", "page_idx": 5}, {"type": "text", "text": "Require: the train dataset $\\mathcal{D}$ , Student $f_{s}$ with random initial weight $\\theta_{s}$ and temperature $\\tau_{s}$ , pretrained robust teacher $f_{t}$ , the initial temperature $\\tau_{y}^{t}$ and $\\tilde{\\tau}_{y}^{t}$ for the teacher\u2019s soft labels for clean examples $x$ and adversarial examples $\\tilde{x}$ , where $y=\\{1,...,C\\}$ , the max training epochs max-epoch. 1: for 0 to max-epoch do   \n2: for $k$ in $\\boldsymbol{y}^{\\bar{}}=\\{1,\\dots,C\\}$ do   \n3: $R(f_{s}(\\tilde{x}_{k}))=0$ , $R(f_{s}(x_{k}))=0$ . // Initialize the clean and robust error risk for each class. 4: end for   \n5: for Every minibatch $\\left({x,y}\\right)$ in $\\mathcal{D}$ do   \n6: $\\tilde{x}=a r g m a x\\,K L(f_{s}(\\tilde{x};\\tau^{s}),f_{t}^{'}(x;\\tilde{\\tau}_{y}^{t}))$ . // Get adversarial examples with teacher\u2019s soft labels. $||\\tilde{x}\\!-\\!x||\\!\\leq\\!\\epsilon$   \n7: $\\theta_{s}=\\theta_{s}-\\eta\\cdot\\nabla_{\\theta}\\mathcal{L}_{a b s l d}(\\tilde{x},x;f_{s},f_{t}^{'})$ . // Update student weight $\\theta_{s}$ with teacher\u2019s soft labels. 8: $R(f_{s}(\\tilde{x}_{y}))=R(f_{s}(\\tilde{x}_{y}))+C E(f_{s}(\\tilde{x}),y)$ . // Calculate robust error risk for each class. 9: $R(f_{s}(x_{y}))=R(f_{s}(x_{y}))+C E(f_{s}(x),y)$ . // Calculate clean error risk for each class. 10: end for   \n11: for $k$ in $y=\\{1,\\ldots,C\\}$ do   \n12: Update $\\widetilde{\\tau}_{k}^{t}$ and $\\tau_{k}^{t}$ based on Eq.(10). // Re-temperate teacher\u2019s soft labels for x\u02dc and $_x$ . 13: end for   \n14: end for ", "page_idx": 6}, {"type": "text", "text": "", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "adversarial examples of the same classes may have different error risks during the training process, it is unreasonable to use the same set of class temperatures to adjust both clean and adversarial examples. Here we simultaneously optimize the student\u2019s clean error risk $R(f_{s}(x_{k}))$ and the student\u2019s robust error risk $R(f_{s}(\\tilde{x}_{k}))$ , in other words, we apply two different sets of teacher temperatures: $\\tau_{k}^{t}$ and $\\tilde{\\tau}_{k}^{t}$ , for the adjustment of the teacher\u2019s soft labels for clean and adversarial examples, respectively. ", "page_idx": 6}, {"type": "text", "text": "Then we extend the Anti-Bias Soft Label Distillation based on [45] and the loss function $\\mathcal{L}_{a b s l d}$ in Eq.(8) can be formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{\\xi}_{a b s l d}(\\tilde{x},x;f_{s},f_{t}^{'})=\\alpha\\frac{1}{C}\\sum_{i=1}^{C}K L(f_{s}(\\tilde{x}_{i};\\tau^{s}),f_{t}^{'}(x_{i};\\tilde{\\tau}_{i}^{t}))+(1-\\alpha)\\frac{1}{C}\\sum_{i=1}^{C}K L(f_{s}(x_{i};\\tau^{s}),f_{t}^{'}(x_{i};\\tau_{i}^{t}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $K L$ represents Kullback\u2013Leibler divergence loss, $\\alpha$ is the trade-off parameter between accuracy and robustness, $f(x;\\tau)$ denotes model $f$ predicts the output probability of $x$ with temperature $\\tau$ in the final softmax layer. It should be mentioned that the teacher is frozen and we apply the teacher\u2019s predicted soft labels $f_{t}^{'}(x_{k};\\tilde{\\tau}_{k}^{t})$ for $k$ -th class to generate adversarial examples $\\tilde{x}_{k}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{x}_{k}=\\operatorname*{\\Delta}_{||\\tilde{x}_{k}-x_{k}||\\leq\\epsilon}K L\\big(f_{s}(\\tilde{x}_{k};\\tau^{s}),f_{t}^{'}(x_{k};\\tilde{\\tau}_{k}^{t})\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and the complete process can be viewed in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct our experiments on three datasets: CIFAR-10 [16], CIFAR-100, and Tiny-ImageNet [17].   \nThe results about CIFAR-100 and Tiny-ImageNet are in Appendix A.4 and A.5, respectively. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We consider the standard training method and eight state-of-the-art methods as comparison methods: AT methods: SAT [20], and TRADES [41]; ARD methods: RSLAD [45], and AdaAD [12]; Robust Fairness methods: FRL [39], BAT [28], CFA [36], and Fair-ARD [38]. ", "page_idx": 6}, {"type": "text", "text": "Student and Teacher Networks. For the student model, here we consider two networks for CIFAR10 and CIFAR-100 including ResNet-18 [10] and MobileNet-v2 [26]. For the teacher model, we follow the setting in [45], and we select WiderResNet-34-10 [40] trained by [41] for CIFAR-10 and WiderResNet-70-16 trained by [9] for CIFAR-100. The teacher\u2019s performance is in Appendix A.7. ", "page_idx": 6}, {"type": "text", "text": "Training Setting. For ABSLD, we train the model using the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.1, a momentum of 0.9, and a weight decay of 2e-4. The learning rate $\\beta$ of temperature is initially set as 0.1. For CIFAR-10 and CIFAR-100, we set the training epochs to 300. The learning rate is divided by 10 at the 215-th, 260-th, and 285-th epochs; We set the batch size to 128 for both CIFAR-10 and CIFAR-100 following [45]. For the inner maximization, we use a 10-step PGD with a random start size of 0.001 and a step size of 2/255, and the perturbation is bounded to the $L_{\\infty}$ norm $\\epsilon=8/255$ . The more training setting can be found in Appendix A.3. ", "page_idx": 6}, {"type": "table", "img_path": "kW30LbNwdV/tmp/c0761f990d088c28acb82d268f710afcdc080d1f69740fe9c6610a042ee42588.jpg", "table_caption": ["Table 1: Result in average robustness( $\\%)$ (Avg.\u2191), worst robustness $(\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on CIFAR-10 of ResNet-18. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "kW30LbNwdV/tmp/a2f1597ff1589778f5dd6a079b193a90be1962748a77f0a070c10fcdc2796380.jpg", "table_caption": ["Table 2: Result in average robustness( $\\%)$ (Avg.\u2191), worst robustness $\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on CIFAR-10 of MobileNet-v2. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Metrics. We apply two metrics to evaluate the robust fairness: Normalized Standard Deviation $(\\mathrm{NSD}^{3})$ [38] and the worst-class robustness [36]. NSD can reflect robust fairness while also considering the average robustness. The smaller standard deviation means better fairness, and the larger average means better robustness, so the smaller NSD means better comprehensive performance in terms of fairness and robustness. The worst-class robustness is easy to understand, and a larger value means better fairness. For CIFAR-10, we directly report the worst class robust accuracy; For CIFAR-100 and Tiny-ImageNet, due to the poor performance of the worst class robustness and only 100 images (CIFAR-100) or 50 images (Tiny-ImageNet) for each class in the test set, we follow the operation in CFA [36] and report the worst $10\\%$ class robust accuracy. Besides, we also report the average robustness as a reference. The attack setting for evaluation can be found in Appendix A.3. ", "page_idx": 7}, {"type": "text", "text": "5.2 Robust Fairness Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performances of ResNet-18 and MobileNet-v2 trained by our ABSLD and other baseline methods under the various attacks are shown in Table 1, Table 2 for CIFAR-10. The results demonstrate that ABSLD achieves the state-of-the-art worst-class robustness on CIFAR-10. For ResNet-18 on CIFAR-10, ABSLD improves the worst class robustness by $2.0\\%$ , $3.2\\%$ , $2.4\\%$ , and $2.9\\%$ compared with the best baseline method against the FGSM, PGD, ${\\mathrm{CW}}_{\\infty}$ , and AA. Moreover, ABSLD shows relevant superiority on MobileNet-v2 compared with other methods. ", "page_idx": 7}, {"type": "text", "text": "Moreover, ABSLD can also show the best comprehensive performance of fairness and robustness (NSD) on CIFAR-10. For ResNet-18 on CIFAR-10, ABSLD reduces the NSD by 0.028, 0.032, 0.017, and 0.024 compared with the best baseline method against the FGSM, PGD, ${\\cal C}{\\sf W}_{\\infty}$ , and AA. The result indicates that although the trade-off between robustness and fairness still exists as [19] say, we obtain the highest robust fairness while sacrificing the least average robustness. ", "page_idx": 7}, {"type": "text", "text": "Meanwhile, we visualize the class-wise robustness in Figure 3, and the result shows that the robustness of harder classes (class 3, 4, 5, 6) have different levels of improvement, which demonstrates that our method is beneficial to the overall robust fairness but not only to the worst class. Moreover, combined with Figure 2, we can find that the trend of class-wise bias is similar in different training strategies, indicating that the bias may be sourced from the dataset itself, which further confirms Corollary 1. ", "page_idx": 7}, {"type": "image", "img_path": "kW30LbNwdV/tmp/a143767d0b02a9936ae03d9c7e80d72aa24dfc9652953070a7a5c1fd771d9f8a.jpg", "img_caption": ["Figure 3: The class-wise robustness (PGD) of models guided by RSLAD and ABSLD on CIFAR-10. We can see that the harder classes\u2019 robustness (class 3, 4, 5, 6) of ABSLD (blue lines) have different levels of improvement compared with RSLAD (red lines). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In particular, we compare ABSLD with Fair-ARD [38], which is an adaptive re-weighting method on ARD. From the results, ABSLD has better robust fairness performance, which means that our proposed re-temperating method has superiority compared to the re-weighting method. ", "page_idx": 8}, {"type": "image", "img_path": "kW30LbNwdV/tmp/063c1972b316a1b6cbb7639eba9239efbb7362e036a4f3e6a0588582741b2ee4.jpg", "img_caption": ["Figure 4: Ablation study for Baseline,Figure 5: Standard deviationFigure 6: Standard deviation Baseline $+\\mathrm{ABSLD}_{a d v}$ , and ABSLD. of class-wise clean optimiza-of class-wise adversarial optition error risk. mization error risk. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To certify the effectiveness of our method, we perform ablation experiments on every component of ABSLD. First, based on the baseline method [45], we re-temperate the teacher\u2019s soft labels for the adversarial examples but do not re-temperate the teacher\u2019s soft labels for the clean examples (Baseline $+\\mathrm{ABSLD}_{a d v}$ ); then we re-temperate the teacher\u2019s soft labels for both the adversarial examples and clean examples (ABSLD). The results are shown in Figure 4. The results demonstrate the effectiveness of our ABSLD, and pursuing fairness for clean examples can also help robust fairness for adversarial examples as claimed in [39]. ", "page_idx": 8}, {"type": "text", "text": "Meanwhile, to verify the optimization effect of our method, we visualize the standard deviation of class-wise optimization error risk in the training process (the optimization error risk is normalized by dividing the mean), which can reflect the optimization gap between different classes. We visualize the standard deviation of both the clean and adversarial optimization error risk, and the results are shown in Figure 5 and Figure 6. We can notice that the standard deviation of the baseline increases as the training epoch increases, which demonstrates that the baseline pays more attention to reducing the error risk of easy class, but the error risk of hard class is ignored, eventually leading to robust unfairness. On the contrary, our ABSLD can remarkably reduce the standard deviation of student\u2019s class-wise optimization error risk, which demonstrates the effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we comprehensively explored the potential factors that influence robust fairness in the model optimization process. We first found that the smoothness degrees of soft labels for different classes can be applied to eliminate the robust fairness based on empirical observation and theoretical analysis. Then we proposed Anti-Bias Soft Label Distillation (ABSLD) to address the robust fairness problem by adjusting the class-wise smoothness degree of soft labels. We adjusted the teacher\u2019s soft labels by assigning different temperatures to different classes based on the performance of student\u2019s class-wise error risk. A series of experiments proved that ABSLD was superior to state-of-the-art methods in the comprehensive metric (NSD) of robustness and fairness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Alibaba Group through Alibaba Reasearch Intern Program, the Project of the National Natural Science Foundation of China (No.62076018), and the Fundamental Research Funds for the Central Universities. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Andriushchenko, M., Croce, F., Flammarion, N., Hein, M.: Square attack: a query-efficient black-box adversarial attack via random search. In: European Conference on Computer Vision. pp. 484\u2013501. Springer (2020)   \n[2] Benz, P., Zhang, C., Karjauv, A., Kweon, I.S.: Robustness may be at odds with fairness: An empirical study on class-wise accuracy. In: NeurIPS 2020 Workshop on Pre-registration in Machine Learning. pp. 325\u2013342. PMLR (2021)   \n[3] Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In: 2017 ieee symposium on security and privacy (sp). pp. 39\u201357. IEEE (2017)   \n[4] Croce, F., Hein, M.: Minimally distorted adversarial examples with a fast adaptive boundary attack. In: International Conference on Machine Learning. pp. 2196\u20132205. PMLR (2020)   \n[5] Croce, F., Hein, M.: Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In: International conference on machine learning. pp. 2206\u20132216. PMLR (2020)   \n[6] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 1440\u20131448 (2015)   \n[7] Goldblum, M., Fowl, L., Feizi, S., Goldstein, T.: Adversarially robust distillation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 3996\u20134003 (2020)   \n[8] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014)   \n[9] Gowal, S., Qin, C., Uesato, J., Mann, T., Kohli, P.: Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593 (2020)   \n[10] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016)   \n[11] Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 2(7) (2015)   \n[12] Huang, B., Chen, M., Wang, Y., Lu, J., Cheng, M., Wang, W.: Boosting accuracy and robustness of student models via adaptive adversarial distillation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 24668\u201324677 (2023)   \n[13] Jia, X., Zhang, Y., Wei, X., Wu, B., Ma, K., Wang, J., Cao, X.: Prior-guided adversarial initialization for fast adversarial training. In: European Conference on Computer Vision. pp. 567\u2013584. Springer (2022)   \n[14] Jia, X., Zhang, Y., Wei, X., Wu, B., Ma, K., Wang, J., Cao Sr, X.: Improving fast adversarial training with prior-guided knowledge. arXiv preprint arXiv:2304.00202 (2023)   \n[15] Jia, X., Zhang, Y., Wu, B., Ma, K., Wang, J., Cao, X.: Las-at: adversarial training with learnable attack strategy. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13398\u201313408 (2022)   \n[16] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)   \n[17] Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS 231N 7(7), 3 (2015)   \n[18] Li, B., Liu, W.: Wat: improve the worst-class robustness in adversarial training. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 14982\u201314990 (2023)   \n[19] Ma, X., Wang, Z., Liu, W.: On the tradeoff between robustness and fairness. Advances in Neural Information Processing Systems 35, 26230\u201326241 (2022)   \n[20] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017)   \n[21] Pang, T., Yang, X., Dong, Y., Xu, K., Zhu, J., Su, H.: Boosting adversarial training with hypersphere embedding. Advances in Neural Information Processing Systems 33, 7779\u20137792 (2020)   \n[22] Rebuff,i S.A., Gowal, S., Calian, D.A., Stimberg, F., Wiles, O., Mann, T.A.: Data augmentation can improve robustness. Advances in Neural Information Processing Systems 34, 29935\u201329948 (2021)   \n[23] Rice, L., Wong, E., Kolter, Z.: Overftiting in adversarially robust deep learning. In: International Conference on Machine Learning. pp. 8093\u20138104. PMLR (2020)   \n[24] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015)   \n[25] Ruan, S., Dong, Y., Su, H., Peng, J., Chen, N., Wei, X.: Improving viewpoint robustness for visual recognition via adversarial training. arXiv preprint arXiv:2307.11528 (2023)   \n[26] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4510\u20134520 (2018)   \n[27] Sehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., Mittal, P.: Robust learning meets generative models: Can proxy distributions improve adversarial robustness? arXiv preprint arXiv:2104.09425 (2021)   \n[28] Sun, C., Xu, C., Yao, C., Liang, S., Wu, Y., Liang, D., Liu, X., Liu, A.: Improving robust fariness via balance adversarial training. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 15161\u201315169 (2023)   \n[29] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016)   \n[30] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013)   \n[31] Tian, Q., Kuang, K., Jiang, K., Wu, F., Wang, Y.: Analysis and applications of class-wise robustness in adversarial training. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. pp. 1561\u20131570 (2021)   \n[32] Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., Gu, Q.: Improving adversarial robustness requires revisiting misclassified examples. In: International Conference on Learning Representations (2019)   \n[33] Wei, X., Guo, Y., Yu, J.: Adversarial sticker: A stealthy attack method in the physical world. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022)   \n[34] Wei, X., Guo, Y., Yu, J., Zhang, B.: Simultaneously optimizing perturbations and positions for black-box adversarial patch attacks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022)   \n[35] Wei, X., Wang, S., Yan, H.: Efficient robustness assessment via adversarial spatial-temporal focus on videos. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)   \n[36] Wei, Z., Wang, Y., Guo, Y., Wang, Y.: Cfa: Class-wise calibrated fair adversarial training. In: CVPR. pp. 8193\u20138201 (2023)   \n[37] Wu, Z., Gao, H., Zhang, S., Gao, Y.: Understanding the robustness-accuracy tradeoff by rethinking robust fairness (2021)   \n[38] Xinli, Y., Mou, N., Qian, W., Lingchen, Z.: Revisiting adversarial robustness distillation from the perspective of robust fairness. NeurIPS (2023)   \n[39] Xu, H., Liu, X., Li, Y., Jain, A., Tang, J.: To be robust or to be fair: Towards fairness in adversarial training. In: International conference on machine learning. pp. 11492\u201311501. PMLR (2021)   \n[40] Zagoruyko, S., Komodakis, N.: Wide residual networks. arXiv preprint arXiv:1605.07146 (2016)   \n[41] Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., Jordan, M.: Theoretically principled trade-off between robustness and accuracy. In: International conference on machine learning. pp. 7472\u20137482. PMLR (2019)   \n[42] Zhao, S., Wang, X., Wei, X.: Mitigating the accuracy-robustness trade-off via multi-teacher adversarial distillation. arXiv preprint arXiv:2306.16170 (2023)   \n[43] Zhao, S., Yu, J., Sun, Z., Zhang, B., Wei, X.: Enhanced accuracy and robustness via multiteacher adversarial distillation. In: Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part IV. pp. 585\u2013602. Springer (2022)   \n[44] Zhu, J., Yao, J., Han, B., Zhang, J., Liu, T., Niu, G., Zhou, J., Xu, J., Yang, H.: Reliable adversarial distillation with unreliable teachers. arXiv preprint arXiv:2106.04928 (2021)   \n[45] Zi, B., Zhao, S., Ma, X., Jiang, Y.G.: Revisiting adversarial robustness distillation: Robust soft labels make student better. In: International Conference on Computer Vision (2021) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 The proof of Theorems 1 in Sec. 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For the initial state of the DNN model $f$ with random distribution $I$ , the DNN model has no preference for any examples. So for the dataset containing hard class $c_{+}$ and easy class $c_{-}$ , we have a relationship between class-wise error risk as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nR(f(x_{c_{+}};\\theta_{I}))=R(f(x_{c_{-}};\\theta_{I})),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "in other words, the model has the same error risk for easy class $c_{-}$ and hard class $c_{+}$ , so we set the model\u2019s prediction distribution $f(x;\\theta_{I})=\\{p_{c_{-}}^{I}(x),p_{c_{+}}^{I}(\\bar{x})\\}$ and has the relationship as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}(p^{I}(x))=\\mathbb{E}(p_{c_{+}}^{I}(x_{c_{+}}))=\\mathbb{E}(p_{c_{-}}^{I}(x_{c_{-}}))=0.5.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then we analyze the process of knowledge distillation, we assume that DNN model $f$ is optimized with the guidance of the soft label distribution with the same smoothness degree $P_{\\lambda1}$ , which satisfies: ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{c_{-}}^{\\lambda1}(x_{c_{-}})=p_{c_{+}}^{\\lambda1}(x_{c_{+}})>0.5>p_{c_{-}}^{\\lambda1}(x_{c_{+}})=p_{c_{+}}^{\\lambda1}(x_{c_{-}}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "And the updated parameter $\\theta_{\\lambda1}$ guided by the soft label distribution $P_{\\lambda1}$ can formulated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{\\lambda1}=\\theta_{I}-\\eta\\cdot\\frac{\\partial K L(f(x;\\theta_{I}),P_{\\lambda1})}{\\partial\\theta},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "here we divide the partial derivative of the optimization function $K L$ with respect to the student parameter $\\theta$ into two parts: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial K L(f(x;\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}=\\sum_{c=1}^{C=2}\\frac{\\partial z_{c}(x)}{\\partial\\theta}\\cdot\\frac{\\partial K L(f(x;\\theta_{I}),P_{\\lambda1})}{\\partial z_{c}(x)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $z_{c}(x)$ denotes the $c$ -th dimension output logits of the model before the softmax layer (denote the prediction logits of $c$ -th class). ", "page_idx": 12}, {"type": "text", "text": "The first part $\\frac{\\partial z_{c}(x)}{\\partial\\theta}$ can be regarded as the impact of the class itself on model weight optimization, in a practical sense, this part reflects the inner relationship between DNN and sample, more specifically, it can reflect the difficulty of the sample itself for the model and not relate to the optimization object. ", "page_idx": 12}, {"type": "text", "text": "The second par t \u2202KL(f\u2202(zxc;(\u03b8xI)),P\u03bb1) can be regarded as the impact of the optimization object on the different classes. Although adjusting the optimization goal is not enough to change the bias derived from the sample itself, it can be adjusted to make the model perform as fair as possible. ", "page_idx": 12}, {"type": "text", "text": "Here we further extend the partial derivative of optimization object $K L(f(x;\\theta_{I}),P_{\\lambda1})$ toward the prediction logit $z_{c}(x)$ of class $c$ and obtain: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial K L(f(x;\\theta_{I}),P_{\\lambda1})}{\\partial z_{c}(x)}}\\\\ &{=\\displaystyle\\sum_{i=1}^{c=2}\\frac{\\partial p_{i}(x)}{\\partial z_{c}(x)}\\cdot\\frac{\\partial K L(f(x;\\theta_{I}),P_{\\lambda1})}{\\partial p_{i}(x)}}\\\\ &{=\\displaystyle\\sum_{i=1}^{c=2}p_{i}^{\\lambda1}(x)p_{c}^{I}(x)-p_{c}^{\\lambda1}(x)}\\\\ &{=p_{c}^{I}(x)-p_{c}^{\\lambda1}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "so we can easily obtain: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(\\frac{\\partial K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c-}(x_{c-})})=\\mathbb{E}(\\frac{\\partial K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c+}(x_{c+})})}\\\\ &{\\;=\\mathbb{E}(p_{c}^{I}(x_{c})-p_{c-}^{\\lambda1}(x_{c-}))=\\mathbb{E}(p_{c}^{I}(x_{c})-p_{c+}^{\\lambda1}(x_{c+})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(\\frac{\\partial K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c-}(x_{c+})})=\\mathbb{E}(\\frac{\\partial K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c+}(x_{c-})})}\\\\ &{\\ =\\mathbb{E}(p_{c}^{I}(x_{c})-p_{c-}^{\\lambda1}(x_{c+}))=\\mathbb{E}(p_{c}^{I}(x_{c})-p_{c+}^{\\lambda1}(x_{c-})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on the Corollary 3.1, the DNN model with parameter $\\theta_{\\lambda1}$ has different the error risk of the hard class $c+$ and the easy class $c-$ , so we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(f(x_{c+};\\theta_{\\lambda1}))>R(f(x_{c-};\\theta_{\\lambda1})),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "from the optimization perspective, the optimization gradient toward the model parameter for different classes directly influences the class error risk, more specifically, the easy class error risk is less than the hard class error risk, so the gradient expectation of the easy class is higher than the gradient expectation of hard class, and the proof is as follows: ", "page_idx": 13}, {"type": "text", "text": "In the initial state, we assume that the model is a uniform distribution, and in this case, the error optimization risk is the same for the easy and hard classes: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(K L(f(x_{c-};\\theta_{I}),P_{\\lambda1}))=\\mathbb{E}(K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "then we will simplify the optimization of the model into a one-step gradient iteration process, which means we only consider the initial state before optimization and the last state after optimization. The model parameter is updated from initial $\\theta_{I}$ to optimized $\\theta_{o p t}$ , since easy classes perform better than hard classes after the optimization process, the easy class error risk is smaller than hard class error risk: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(K L(f(x_{c-};\\theta_{o p t}),P_{\\lambda1}))<\\mathbb{E}(K L(f(x_{c+};\\theta_{o p t}),P_{\\lambda1}))}\\\\ &{\\ <\\mathbb{E}(K L(f(x_{c-};\\theta_{I}),P_{\\lambda1}))=\\mathbb{E}(K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "based on the above result, we can have the result as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(|K L(f(x_{c-};\\theta_{o p t}),P_{\\lambda1})-K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})|)}\\\\ &{\\ >\\mathbb{E}(|K L(f(x_{c+};\\theta_{o p t}),P_{\\lambda1})-K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "at this time, we assume that model\u2019s parameter $\\theta$ is differentiable and continuous, then we garsa fdoilelnotw se:xpectation of the partial derivative $\\mathbb{E}\\big(\\frac{\\partial K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}\\big)$ and $\\mathbb{E}\\big(\\frac{\\partial K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}\\big)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}(\\frac{\\partial K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta})}\\quad}&{}\\\\ &{\\approx\\mathbb{E}(\\frac{K L(f(x_{c-};\\theta_{o p t}),P_{\\lambda1})-K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\theta_{o p t}-\\theta_{I}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}(\\frac{\\partial K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta})}\\quad}&{}\\\\ &{\\approx\\mathbb{E}(\\frac{K L(f(x_{c+};\\theta_{o p t}),P_{\\lambda1})-K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\theta_{o p t}-\\theta_{I}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "combined with the relationship between easy class error risk and hard class error risk, we can obtain the result that the gradient absolute value expectation of partial derivative about the easy class\u2019s optimization goal toward the model parameter $(\\mathbb{E}(|\\frac{\\partial K L(f(\\bar{x_{c-}};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}|))$ ) is higher than the gradient absolute value expectation of partial derivative about the hard class\u2019s optimization goal toward the model parameter $(\\mathbb{E}(|\\frac{\\partial K L(f(\\bar{x_{c+}};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}|))$ ) as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}(|\\frac{\\partial K L(f(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}|)>\\mathbb{E}(|\\frac{\\partial K L(f(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial\\theta}|),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "so we can obtain the assumption: If the model is a uniform distribution before the optimization process and the easy class error risk is less than the hard class error risk after optimization process, then the gradient expectation absolute value of partial derivative about the easy class\u2019s optimization goal toward the model parameter is higher than the gradient expectation absolute value of partial derivative about the hard class\u2019s optimization goal toward the model parameter. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "T\u2202hKeLn( fw(xec c+a;\u03b8nI )d,Pec\u03bb1o)uple the gradient into two parts based on the class types: \u2202KL(f(xc\u2212;\u03b8I),P\u03bb1)and \u2202\u03b8 ", "page_idx": 14}, {"type": "text", "text": "Here we take the hard class as an example, the gradient can be divided into two parts: $\\frac{\\partial z_{c+}(x_{c+})}{\\partial\\theta}\\frac{\\partial K L(f_{s}(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c+}}$ and $\\frac{\\partial z_{c-}(x_{c+})}{\\partial\\theta}\\frac{\\partial\\bar{K}L(f_{s}(x_{c+}\\bar{;}\\theta_{I}),P_{\\lambda1})}{\\partial z_{c-}}$ , where the first part represents the ability to make hard class\u2019s samples more like hard class, and the second part represents the ability to make hard class\u2019s samples less like easy class. The final prediction performance of hard class $c+$ is influenced by both parts of the optimization gradient due to the softmax operation. ", "page_idx": 14}, {"type": "text", "text": "Based on the above analysis, the relationship between the sum value of the above two parts of gradient for hard class $(\\nabla_{f\\sim\\lambda1}^{+})$ and easy class $(\\bar{\\nabla_{f\\sim\\lambda1}^{-}})$ can be assumed under a more stringent condition as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{f\\sim\\lambda1}^{-}=\\mathbb{E}(|\\frac{\\partial z_{c-}\\left(x_{c-}\\right)}{\\partial\\theta}\\frac{\\partial K L\\left(f_{s}\\left(x_{c-};\\theta_{I}\\right),P_{\\lambda1}\\right)}{\\partial z_{c-}}|+|\\frac{\\partial z_{c+}\\left(x_{c-}\\right)}{\\partial\\theta}\\frac{\\partial K L\\left(f_{s}\\left(x_{c-};\\theta_{I}\\right),P_{\\lambda1}\\right)}{\\partial z_{c+}}|)>}\\\\ &{\\nabla_{f\\sim\\lambda1}^{+}=\\mathbb{E}(|\\frac{\\partial z_{c-}\\left(x_{c+}\\right)}{\\partial\\theta}\\frac{\\partial K L\\left(f_{s}\\left(x_{c+};\\theta_{I}\\right),P_{\\lambda1}\\right)}{\\partial z_{c-}}|+|\\frac{\\partial z_{c+}\\left(x_{c+}\\right)}{\\partial\\theta}\\frac{\\partial K L\\left(f_{s}\\left(x_{c+};\\theta_{I}\\right),P_{\\lambda1}\\right)}{\\partial z_{c+}}|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combined with Eq.(19), Eq.(20) and Eq.(28), we have a relationship about the derivative of $z_{c}$ toward model parameter $\\theta$ , which can reflect the bias of sample for the model: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}(|\\frac{\\partial z_{c_{-}}(x_{c-})}{\\partial\\theta}|+|\\frac{\\partial z_{c+}(x_{c-})}{\\partial\\theta}|)>\\mathbb{E}(|\\frac{\\partial z_{c_{-}}(x_{c+})}{\\partial\\theta}|+|\\frac{\\partial z_{c_{+}}(x_{c+})}{\\partial\\theta}|).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "After the analysis of the model characteristic with the guidance of soft label distribution $P_{\\lambda1}$ , we try to compare the model characteristic difference between soft label distribution $P_{\\lambda1}$ and soft label distribution $P_{\\lambda2}$ . Initially, we can obtain the following relationship for the probabilities of different distributions: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{p_{c_{+}}^{\\lambda2}(x_{c_{+}})=p_{c_{+}}^{\\lambda1}(x_{c_{+}})+v_{1}>p_{c_{+}}^{\\lambda1}(x_{c_{+}})=}\\\\ {p_{c_{-}}^{\\lambda1}(x_{c_{-}})>p_{c_{-}}^{\\lambda2}(x_{c_{-}})=p_{c_{-}}^{\\lambda1}(x_{c_{-}})-v_{2}>0.5,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then we further analyze the model class-wise error risk gap guided by label distribution. The total optimization gradient for different classes can reflect the bias degree, so we get the class optimization gradient gap guided by label distribution $P_{\\lambda1}$ and label distribution $P_{\\lambda2}$ , respectively: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad B_{f\\sim\\lambda1}=\\nabla_{f\\sim\\lambda1}^{+}-\\nabla_{f\\sim\\lambda1}^{+}}\\\\ &{=\\mathbb{E}(|\\frac{\\partial z_{c-}(x_{c-})}{\\partial\\theta}\\frac{\\partial K L(f_{s}(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c-}}|+|\\frac{\\partial z_{c+}(x_{c-})}{\\partial\\theta}\\frac{\\partial K L(f_{s}(x_{c-};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c+}}|-}\\\\ &{\\quad|\\frac{\\partial z_{c-}(x_{c+})}{\\partial\\theta}\\frac{\\partial K L(f_{s}(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c-}}|-|\\frac{\\partial z_{c+}(x_{c+})}{\\partial\\theta}\\frac{\\partial K L(f_{s}(x_{c+};\\theta_{I}),P_{\\lambda1})}{\\partial z_{c+}}|)}\\\\ &{=\\mathbb{E}(|\\frac{\\partial z_{c-}(x_{c-})}{\\partial\\theta}|(p_{c-}^{\\lambda_{1}}(x_{c-})-p_{c}^{I})+|\\frac{\\partial z_{c+}(x_{c-})}{\\partial\\theta}|(p_{c}^{I}-p_{c+}^{\\lambda_{1}}(x_{c-}))-}\\\\ &{\\quad|\\frac{\\partial z_{c-}(x_{c+})}{\\partial\\theta}|(p_{c}^{I}-p_{c-}^{\\lambda_{1}}(x_{c+}))-|\\frac{\\partial z_{c+}(x_{c+})}{\\partial\\theta}|(p_{c+}^{\\lambda_{1}}(x_{c+})-p_{c}^{I})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad B_{f\\sim\\lambda2}=\\nabla_{f\\sim\\lambda2}^{-}-\\nabla_{f\\sim\\lambda2}^{+}}\\\\ &{=\\!\\mathbb{E}(|\\frac{\\partial z_{c-}\\left(x_{c-}\\right)}{\\partial\\theta}|(p_{c-}^{\\lambda_{2}}(x_{c-})-p_{c}^{I})+|\\frac{\\partial z_{c+}\\left(x_{c-}\\right)}{\\partial\\theta}|(p_{c}^{I}-p_{c+}^{\\lambda_{2}}(x_{c-}))-}\\\\ &{\\quad|\\frac{\\partial z_{c-}\\left(x_{c+}\\right)}{\\partial\\theta}|(p_{c}^{I}-p_{c-}^{\\lambda_{2}}(x_{c+}))-|\\frac{\\partial z_{c+}\\left(x_{c+}\\right)}{\\partial\\theta}|(p_{c+}^{\\lambda_{2}}(x_{c+})-p_{c}^{I})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "if the model parameter $\\theta_{\\lambda2}$ still satisfies: $R(f(x_{c_{+}};\\theta_{\\lambda2}))>R(f(x_{c_{-}};\\theta_{\\lambda2}))$ , then we can obtain $B_{f\\sim\\lambda2}>0$ , and the relationship between $B_{f\\sim\\lambda1}$ and $B_{f\\sim\\lambda2}$ is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad B_{f\\sim\\lambda1}-B_{f\\sim\\lambda2}}\\\\ &{=\\!\\mathbb{E}(|\\frac{\\partial z_{c-}\\left(x_{c-}\\right)}{\\partial\\theta}|(p_{c-}^{\\lambda_{1}}(x_{c-})-p_{c-}^{\\lambda_{2}}(x_{c-}))+|\\frac{\\partial z_{c+}\\left(x_{c-}\\right)}{\\partial\\theta}|(p_{c+}^{\\lambda_{2}}(x_{c-})-p_{c+}^{\\lambda_{1}}(x_{c-}))}\\\\ &{\\quad-\\,|\\frac{\\partial z_{c-}\\left(x_{c+}\\right)}{\\partial\\theta}|(-p_{c-}^{\\lambda_{2}}(x_{c+})+p_{c-}^{\\lambda_{1}}(x_{c+}))-|\\frac{\\partial z_{c+}\\left(x_{c+}\\right)}{\\partial\\theta}|(p_{c+}^{\\lambda_{2}}(x_{c+})-p_{c+}^{\\lambda_{1}}(x_{c+})))}\\\\ &{=\\!\\mathbb{E}(|\\frac{\\partial z_{c-}\\left(x_{c-}\\right)}{\\partial\\theta}|v_{2}+|\\frac{\\partial z_{c+}\\left(x_{c-}\\right)}{\\partial\\theta}|v_{2}+|\\frac{\\partial z_{c-}\\left(x_{c+}\\right)}{\\partial\\theta}|v_{1}+|\\frac{\\partial z_{c+}\\left(x_{c+}\\right)}{\\partial\\theta}|v_{1})>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so we can have the following conclusion: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{f\\sim\\lambda1}>B_{f\\sim\\lambda2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on the above results, the model total gradient expectation gap of different classes trained by soft label distribution $P_{\\lambda1}$ is larger than gradient expectation gap between different classes trained by soft label distribution $P_{\\lambda2}$ , so the bias degree of model trained by the soft label distribution $P_{\\lambda1}$ is greater than by the bias degree of model trained by the guidance of soft label distribution $P_{\\lambda2}$ , we can get the conclusion as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(f(x_{c_{+}};\\theta_{\\lambda1}))-R(f(x_{c_{-}};\\theta_{\\lambda1}))>R(f(x_{c_{+}};\\theta_{\\lambda2}))-R(f(x_{c_{-}};\\theta_{\\lambda2})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then the Theorem 1 is proved. ", "page_idx": 15}, {"type": "text", "text": "A.2 The proof of Theorems 2 in Sec. 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on the correct prediction distribution assumption about the teacher model, we can obtain the relationship of probability toward $k$ -th class as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}(p_{k}^{t}(x_{k};\\tau_{k}^{t}))>\\!\\mathbb{E}(p_{c\\neq k}^{t}(x_{k};\\tau_{k}^{t})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we extend the temperature into the mathematical formula of the prediction probability, we can obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{k}^{t}(x_{k};\\tau_{k}^{t})=\\frac{e x p(z_{k}(x_{k})/\\tau_{k}^{t})}{\\sum_{j=1}^{C}e x p(z_{j}(x_{k})/\\tau_{k}^{t})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $z_{k}(x)$ denotes the $k$ -th dimension output logits of model before softmax layer. Since $\\begin{array}{r}{\\sum_{j=1}^{C}e x p(z_{j}(x_{k})/\\tau_{k}^{t})}\\end{array}$ is applied as a normalization application, here we mainly focus the change $e x p(z(x_{k})/\\tau_{k}^{t})$ with the temperature $\\tau_{k}^{t}$ . $e x p(\\cdot)$ is a monotonic increasing function, so based on the Eq.(36), we can easily obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}(z_{k}(x_{k}))>\\mathbb{E}(z_{c\\neq k}(x_{k})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "here we mainly focus the change $e x p(z(x_{k})/\\tau_{k}^{t})$ with the temperature $\\tau_{k}^{t}$ . We assume the $\\tau_{k}^{t}$ increase into $\\tau_{k}^{t}\\!+\\!\\Delta_{k}^{\\tau}(\\Delta_{k}^{\\tau}>0)$ , then we have the partial derivative of $e x p(z(x_{k})/\\bar{\\tau}_{k}^{t})\\!-e x p(z(x_{k})/(\\bar{\\tau}_{k}^{t}\\!+\\!\\Delta_{k}^{\\tau}))$ with respect to $z(x_{k})$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial(e x p(z(x_{k})/\\tau_{k}^{t})-e x p(z(x_{k})/(\\tau_{k}^{t}+\\Delta_{k}^{\\tau})))}{\\partial(z(x_{k}))},}\\\\ &{-\\frac{(e x p(z(x_{k})/\\tau_{k}^{t})}{\\tau_{k}^{t}}-\\frac{\\left(e x p(z(x_{k})/(\\tau_{k}^{t}+\\Delta_{k}^{\\tau}))\\right)}{\\tau_{k}^{t}+\\Delta_{k}^{\\tau}}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so we have the conclusion as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{e x p(z_{k}(x_{k})/\\tau_{k}^{t})-e x p(z_{k}(x_{k})/(\\tau_{k}^{t}+\\Delta_{k}^{\\tau}))>}\\\\ {e x p(z_{c\\neq k}(x_{k})/\\tau_{k}^{t})-e x p(z_{c\\neq k}(x_{k})/(\\tau_{k}^{t}+\\Delta_{k}^{\\tau})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then we can have relationship between the prediction distribution of different temperature as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}(p_{k}^{t}(x_{k};\\tau_{k}^{t}))-\\mathbb{E}(p_{c\\neq k}^{t}(x_{k};\\tau_{k}^{t}))>}\\\\ &{\\qquad\\mathbb{E}(p_{k}^{t}(x_{k};\\tau_{k}^{t}+\\Delta_{k}^{\\tau}))-\\mathbb{E}(p_{c\\neq k}^{t}(x_{k};\\tau_{k}^{t}+\\Delta_{k}^{\\tau})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on the above analysis, we assume that the teacher\u2019s temperature of the easy class and hard class increase into $\\tau_{c-}^{t}+\\Delta_{c-}^{\\tau}$ and $\\tau_{c+}^{t}+\\Delta_{c+}^{\\tau}$ and obtain teacher model with soft label prediction $f_{t}^{'}$ , then following the Derivation in Theorem 1, we can obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad B_{f_{s}\\sim f_{t}}-B_{f_{s}\\sim f_{t}^{t}}}\\\\ &{=\\!\\mathbb{E}(|\\frac{\\partial z_{c-}(x_{c-})}{\\partial\\theta}|(p_{c-}^{t}(x_{c-};\\tau_{c-}^{t}))-p_{c-}^{t}(x_{c-};\\tau_{c-}^{t}+\\Delta_{c-}^{\\tau}))}\\\\ &{\\quad+\\,|\\frac{\\partial z_{c+}(x_{c-})}{\\partial\\theta}|(p_{c+}^{t}(x_{c-};\\tau_{c-}^{t}+\\Delta_{c-}^{\\tau})-p_{c+}^{t}(x_{c-};\\tau_{c-}^{t}))}\\\\ &{\\quad-\\,|\\frac{\\partial z_{c-}(x_{c+})}{\\partial\\theta}|(p_{c-}^{t}(x_{c+};\\tau_{c-}^{t})-p_{c-}^{t}(x_{c+};\\tau_{c+}^{t}+\\Delta_{c+}^{\\tau}))}\\\\ &{\\quad-\\,|\\frac{\\partial z_{c+}(x_{c+})}{\\partial\\theta}|(p_{c+}^{t}(x_{c+};\\tau_{c+}^{t}+\\Delta_{c+}^{\\tau})-p_{c+}^{t}(x_{c+};\\tau_{c-}^{t}))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can further analyze the relationship between the temperature and the error risk gap. Here we assume that the teacher\u2019s temperature of easy class $\\tau_{c-}^{k}$ is unchanged $(\\Delta_{c-}^{\\tau}=0)$ ), the teacher\u2019s temperature of hard class $\\tau_{c+}^{k}$ increases into $\\tau_{c+}^{k}+\\Delta_{c+}^{\\tau}$ $(\\Delta_{c+}^{\\tau}>0)$ ), we can obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\nB_{f_{s}\\sim f_{t}}<B_{f_{s}\\sim f_{t}^{\\prime}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "following the analysis in Appendix A.1, DNN model $f_{s}$ is optimized with the guidance of the teacher\u2019s soft label distribution $f_{t}$ and $f_{t}^{'}$ , and can obtain the model parameter $\\theta_{t}$ and $\\theta_{t}^{'}$ , respectively. We can get the conclusion as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR(f_{s}(x_{c_{+}};\\theta_{t}))-R(f_{s}(x_{c_{-}};\\theta_{t})<R(f(x_{c_{+}};\\theta_{t}^{'}))-R(f(x_{c_{-}};\\theta_{t}^{'})).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we assume that the teacher\u2019s temperature of hard class $\\tau_{c+}^{k}$ is unchanged $\\langle\\Delta_{c+}^{\\tau}\\,=\\,0\\rangle$ ), the teacher\u2019s temperature of easy class \u03c4 ck\u2212increases into \u03c4 ck\u2212+ \u2206c\u03c4\u2212(\u2206c\u03c4\u2212> 0), we can obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}_{f_{s}\\sim f_{t}}>\\mathcal{B}_{f_{s}\\sim f_{t}^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and following the analysis in Appendix A.1, we can get the conclusion as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR(f_{s}(x_{c_{+}};\\theta_{t}))-R(f_{s}(x_{c_{-}};\\theta_{t})>R(f(x_{c_{+}};\\theta_{t}^{'})-R(f(x_{c_{-}};\\theta_{t}^{'})).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on the above results, we can obtain the teacher temperature $\\tau_{c+}^{t}$ of hard class $c+$ is positively correlated with the error risk gap for student $f_{s}$ , and the teacher temperature $\\tau_{c-}^{t}$ of easy class $c-$ is negatively correlated with the error risk gap for student $f_{s}$ . ", "page_idx": 16}, {"type": "text", "text": "Then the Theorem 2 is proved. ", "page_idx": 16}, {"type": "text", "text": "A.3 Additional Experimental Setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed in [36], the worst class robust accuracy changes drastically when the average robustness convergence, so we follow [36] and select the best checkpoint of the highest mean value of all-class average robustness and the worst class robustness (where the worst class for CIFAR-10 and the worst $10\\%$ class for CIFAR-100) in all baselines and our method for a fair comparison. ", "page_idx": 16}, {"type": "text", "text": "For ABSLD, to maintain stability when adjusting the teacher prediction distribution, we hold the student temperature $\\tau^{s}$ constant, and the student\u2019s optimization error risk for each class can be compared under the same standard. The teacher temperature of $\\tau_{k}^{t}$ and $\\tilde{\\tau}_{k}^{t}$ are initially set as 1 for all the classes. The student temperature $\\tau_{s}$ is set to constant 1 without additional adjustment. With additional instruction, the maximum and minimum values of temperature are 5 and 0.5, respectively; For ResNet-18 on CIFAR-100, the maximum and minimum values of temperature are 3 and 0.8, respectively. All the experiments are conducted in a single GeForce RTX 3090, and our ABSLD takes approximately one GPU day for training a model. ", "page_idx": 16}, {"type": "text", "text": "For the baselines, we strictly follow original setting if without additional instruction. For FRL [39], we use the Reweight+Remargin under the threshold of 0.05. For CFA [36], we select the best version (TRADES $+$ CFA) as reported in the original article. For the training of CFA on CIFAR-100, we set the fairness threshold to 0.02 for FAWA operation based on the worst- $10\\%$ class robustness. For Fair-ARD [38], we select the best version (Fair-ARD on CIFAR-10 and Fair-RSLAD on CIFAR-100) as reported in the original article. Due to the different strategy of selecting checkpoint, the baselines may have slight differences from the results in their original papers. ", "page_idx": 16}, {"type": "table", "img_path": "kW30LbNwdV/tmp/31cdcc810dfdd4c3404454d133224801b9b8cccd3da0b3ae6142912cff645558.jpg", "table_caption": ["Table 3: Result in average robustness( $\\%)$ (Avg.\u2191), worst- $10\\%$ robustness $\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on CIFAR-100 of ResNet-18. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "kW30LbNwdV/tmp/e117c71e0a42d86240247be0371ad6e6b1fc7e4a979e3e824addda36c86cf30f.jpg", "table_caption": ["Table 4: Result in average robustness $\\%)$ (Avg.\u2191), worst- $10\\%$ robustness $\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on CIFAR-100 of MobileNet-v2. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Following previous studies [45; 42; 38], we evaluate the trained model against white-box adversarial attacks: FGSM [8], PGD [20], ${\\mathrm{CW}}_{\\infty}$ [3]. For PGD, we apply 20 steps with a step size of 2/255; For $\\mathrm{CW}_{\\infty}$ , we apply 30 steps with a step size of 2/255. Meanwhile, we apply a strong attack: AutoAttack (AA) [5] to evaluate the robustness, which includes four attacks: Auto-PGD (APGD), Difference of Logits Ratio (DLR) attack, FAB-Attack [4], and the black-box Square Attack [1]. The maximum perturbation of all generated adversarial examples is 8/255. ", "page_idx": 17}, {"type": "text", "text": "A.4 The Robustness on CIFAR-100 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The performances of ResNet-18 and MobileNet-v2 trained by our ABSLD and other baseline methods under the various attacks are shown in Table 3, Table 4 for CIFAR-100. ", "page_idx": 17}, {"type": "text", "text": "The results demonstrate that ABSLD achieves the state-of-the-art worst-class robustness on CIFAR100. For ResNet-18 on CIFAR-100, ABSLD improves the worst- $10\\%$ class robustness by $2.1\\%$ , $2.1\\%$ , $1.2\\%$ , and $0.7\\%$ compared with the best baseline method against the FGSM, PGD, ${\\mathrm{CW}}_{\\infty}$ , and AA. Moreover, ABSLD shows relevant superiority on MobileNet-v2 compared with other methods. ", "page_idx": 17}, {"type": "text", "text": "Moreover, ABSLD can also show the best comprehensive performance of fairness and robustness (NSD) on CIFAR-100. For ResNet-18 on CIFAR-100, ABSLD reduces the NSD by 0.05, 0.063, 0.077, and 0.078 compared with the best baseline method against the FGSM, PGD, ${\\mathrm{CW}}_{\\infty}$ , and AA. ", "page_idx": 17}, {"type": "text", "text": "A.5 The Robustness on Tiny-ImageNet ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We select the subset of ImageNet: Tiny-ImageNet as the additional dataset. We train with PreActResNet-18 with 100 epochs, while other settings are the same as CIFAR-100. For our ABSLD, the teacher model is PreActResNet-34 trained by TRADES [41], and the maximum and minimnum values of temperature are 1.1 and 0.9. We select RSLAD [45] (the baseline method) and CFA[36] (the second-best method proven in Table 1 and Table 3) as the comparison method. The results in Table 5 show ABSLD has the best performance under the metric of the worst class robustness and NSD under different attacks, verifying our effectiveness and generalization. ", "page_idx": 17}, {"type": "text", "text": "Table 5: Result in average robustness( $\\%)$ (Avg.\u2191), worst robustness $\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on Tiny-ImageNet of PreActResNet-18. ", "page_idx": 18}, {"type": "table", "img_path": "kW30LbNwdV/tmp/4a0dc04741b604f457a2e93ae5a49f4b54eea17f1715629faa7d4baff4f3a2ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.6 The Necessity of Adaptive Adjustment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To demonstrate the effectiveness of our self-adaptive temperature adjustment strategy, we manually retemperate with the static temperature for different classes based on the prior knowledge. Specifically, We set the teacher temperature for difficult class to be small $(\\tau_{k}^{t}=0.5)$ ) and set the teacher temperature for easy class to be large $(\\tau_{k}^{t}=5)$ ), which is the minimum and maximum values of temperature, respectively. The experiment in Table 6 shows that our adaptive strategy has a better performance than this manual strategy on CIFAR-10. Moreover, the manual strategy needs to be carefully designed and lacks operability for more complex datasets, e.g., 100 classes on CIFAR-100 or 200 classes on Tiny-ImageNet, so we finally apply the adaptive adjustment strategy for ABSLD. ", "page_idx": 18}, {"type": "text", "text": "Table 6: Result in average robustness( $\\%)$ (Avg.\u2191), worst robustness $(\\%)$ (Worst\u2191), and normalized standard deviation (NSD\u2193) on CIFAR-10 of ResNet-18. ", "page_idx": 18}, {"type": "table", "img_path": "kW30LbNwdV/tmp/7877d2e1f4c918d2561511a3cc65a2f11964a9b17fc6fbe67866c8b3bcb3f334.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.7 The Robustness of Teacher Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we report the performance of Teacher models, we select WiderResNet-34-10 [40] trained by [41] for CIFAR-10 and WiderResNet-70-16 trained by [9] for CIFAR-100 following [45; 43]. For Tiny-ImageNet, the teacher model is PreActResNet-34 trained by TRADES [41]. The performance is shown in Table 7. ", "page_idx": 18}, {"type": "table", "img_path": "kW30LbNwdV/tmp/47156c46600a86e5c0136f60ae0a1e1977f6ae1ceaed7ab3bc089f6933d424d6.jpg", "table_caption": ["Table 7: Robustness $(\\%)$ of the teachers in our experiments. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.8 Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "At present, although we have improved the fairness of model adversarial robustness with minimal cost, the overall robustness remains unchanged or slightly decreases compared to previous methods in some cases, and how to solve the trade-off between robustness and fairness is one of the directions that will be further explored in the future. Meanwhile, our method is based on the adjustment towards the smoothness degree of soft labels, and cannot be directly applied to adversarial training methods based on one-hot labels but needs to be combined with label smoothing operations. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Sec. 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Appendix A.8. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Appendix A.1 and Appendix A.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Sec. 5 and Appendix A.3. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The code is available at supplemental material, and the code will be open access after revision. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Sec. 5 and Appendix A.3. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Since adversarial training and adversarial robustness distillation are timeconsuming, we only ran one time. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Appendix A.3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not find any discrepancieswith the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code (new assets) can be found in supplemental material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]