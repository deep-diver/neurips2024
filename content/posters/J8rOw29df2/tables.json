[{"figure_path": "J8rOw29df2/tables/tables_1_1.jpg", "caption": "Table 1: Bounds on uniform meta-stability \u03b2 for different families of learning problems. Here, \u03b7 is the step-size for GD for task-specific learning, \u03b3 is the step-size for GD for meta-parameter learning, m is the number of tasks during training, n is the number of training data for the task at test time.", "description": "This table presents the theoretical bounds on uniform meta-stability (\u03b2) for different meta-learning algorithms and loss function types.  It shows how the stability of the algorithms (a measure of their sensitivity to changes in the data) depends on various hyperparameters (step sizes \u03b7 and \u03b3, regularization parameter \u03bb), the number of training tasks (m), and the number of training examples per task (n).  Different rows represent different algorithm choices (using regularized empirical risk minimization or gradient descent for task-specific learning) and assumptions about the loss function (convex and smooth, weakly convex and non-smooth).", "section": "2 Results"}, {"figure_path": "J8rOw29df2/tables/tables_3_1.jpg", "caption": "Table 1: Bounds on uniform meta-stability \u03b2 for different families of learning problems. Here, \u03b7 is the step-size for GD for task-specific learning, \u03b3 is the step-size for GD for meta-parameter learning, m is the number of tasks during training, n is the number of training data for the task at test time.", "description": "This table presents theoretical bounds on uniform meta-stability (\u03b2) for different types of loss functions used in meta-learning algorithms.  The bounds depend on several hyperparameters: the step size for gradient descent in both the task-specific and meta-learning stages (\u03b7 and \u03b3 respectively), the number of training tasks (m), and the number of training examples per task (n). The table helps to understand how these hyperparameters and the properties of the loss function (convexity, smoothness, Lipschitz continuity) impact the stability of the algorithm.", "section": "2 Algorithm Stability Analysis"}, {"figure_path": "J8rOw29df2/tables/tables_8_1.jpg", "caption": "Table 1: Bounds on uniform meta-stability \u03b2 for different families of learning problems. Here, \u03b7 is the step-size for GD for task-specific learning, \u03b3 is the step-size for GD for meta-parameter learning, m is the number of tasks during training, n is the number of training data for the task at test time.", "description": "This table summarizes the upper bounds on the uniform meta-stability (\u03b2) for four different scenarios of convex and non-convex loss functions. Each row represents a setting where the loss function has different properties (e.g., convex and Lipschitz, or weakly convex and Lipschitz) and different optimization methods (e.g., regularized empirical risk minimization (RERM) or gradient descent (GD)) are used for task-specific learning. The bounds for uniform meta-stability are expressed in terms of the step-sizes (\u03b7, \u03b3), the number of tasks (m), and the number of training data per task (n).", "section": "2 Bounding Transfer Risk"}]