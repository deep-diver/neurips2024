[{"type": "text", "text": "Data Debugging is NP-hard for Classifiers Trained with SGD ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Data debugging is to find a subset of the training data such that the model obtained   \n2 by retraining on the subset has a better accuracy. A bunch of heuristic approaches   \n3 are proposed, however, none of them are guaranteed to solve this problem effec  \n4 tively. This leaves an open issue whether there exists an efficient algorithm to find   \n5 the subset such that the model obtained by retraining on it has a better accuracy.   \n6 To answer this open question and provide theoretical basis for further study on   \n7 developing better algorithms for data debugging, we investigate the computational   \n8 complexity of the problem named DEBUGGABLE. Given a machine learning   \n9 model $\\mathcal{M}$ obtained by training on dataset $D$ and a test instance $\\left(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}}\\right)$ where   \n10 $\\mathcal{M}(\\mathbf{x}_{\\mathrm{test}})\\neq y_{\\mathrm{test}}$ , DEBUGGABLE is to determine whether there exists a subset $D^{\\prime}$ of   \n11 $D$ such that the model $\\mathcal{M}^{\\prime}$ obtained by retraining on $D^{\\prime}$ satisfies $\\mathcal{M}^{\\prime}(\\mathbf{x}_{\\mathrm{test}})=y_{\\mathrm{test}}$ .   \n12 To cover a wide range of commonly used models, we take SGD-trained linear   \n13 classifier as the model and derive the following main results. (1) If the loss function   \n14 and the dimension of the model are not fixed, DEBUGGABLE is NP-complete   \n15 regardless of the training order in which all the training samples are processed   \n16 during SGD. (2) For hinge-like loss functions, a comprehensive analysis on the   \n17 computational complexity of DEBUGGABLE is provided; (3) If the loss function is a   \n18 linear function, DEBUGGABLE can be solved in linear time, that is, data debugging   \n19 can be solved easily in this case. These results not only highlight the limitations of   \n20 current approaches but also offer new insights into data debugging. ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Given a machine learning model, data debugging is to find a subset of the training data such that   \n23 the model will have a better accuracy if retrained on that subset [1]. Data debugging serves as a   \n24 popular method of both data cleaning and machine learning interpretation. In the context of data   \n25 cleaning, data debugging (a.k.a. training data debugging [2] or data cleansing [1]) can be used   \n26 to improve the quality of the training data by removing the flaws leading to mispredictions [3\u20135].   \n27 When it comes to ML interpretation, data debugging locates the part of the training data responsible   \n28 for unexpected predictions of an ML model. Therefore it is also studied as a training data-based   \n29 (a.k.a. instance-based [6]) interpretation, which is crucial for helping system developers and ML   \n30 practitioners to debug ML system by reporting the harmful part of training data [7].   \n31 To solve the data debugging problem, existing researches adopt a two-phase score-based heuristic   \n32 approach [2]. In the first phase, a score representing the estimated impact on the model accuracy is   \n33 assigned to each training sample in the training data. It is hoped that the harmful part of training   \n34 data gets a lower score than the other part. In the second phase, training samples with lower scores   \n35 are removed greedily and the model is retrained on the modified training data. The two phases are   \n36 carried out iteratively until a well-trained model is obtained. Most of the related works focus on   \n37 developing algorithms to estimate the scores efficiently in the first phase [8\u201316], but rarely study the   \n38 effectiveness of the entire two-phase approach.   \n39 Since it is computationally intractable to estimate the score for all possible subsets of the training   \n40 data, it is often assumed that the score representing the impact of a subset is approximately equal   \n41 to the sum of the scores of each individual training samples from the subset. However, Koh et. al.   \n42 [10] showed this is not always the case. For a bunch of subsets sampled from the training data,   \n43 they empirically studied the difference between the estimated impact and the actual impact of each   \n44 subset by taking influence functions as the scoring method. The estimated impact is calculated by   \n45 summing up the score by influence function of each training samples in the subset, and the actual   \n46 impact is measured by the improvement of accuracy of the model retrained after removing the subset   \n47 from training data. They found that the estimated impact tends to underestimate the actual impact.   \n48 Removing a large number of training samples could result in a large deviation between estimated   \n49 and actual impacts. Although an upper bound of the deviation under certain assumptions has been   \n50 derived, it is still unknown whether the deviation can be reduced or eliminated efficiently.   \n51 The above deviation also poses challenges to the effectiveness of the entire approach. Suppose the   \n52 influence function is adopted as the scoring method, the accuracy of the model is not guaranteed   \n53 to improve due to the deviation reported in [10] if a large group of training samples are removed   \n54 during each iteration. Moreover, there is no theoretical analysis for the effectiveness of the greedy   \n55 approach in the second phase. Even if only one training sample is removed during each iteration   \n56 of the two-phase approach, the accuracy of the model is still not guaranteed to be improved. The   \n57 effectiveness of the entire two-phase approach is therefore not assured. This leaves the following   \n58 open problem:   \n59 Problem 1.1. Is there an efficient algorithm to find the subset of the training data, such that the   \n60 model obtained by retraining on it has a better accuracy?   \n61 The computational complexity results presented in this paper demonstrate that it is unlikely to solve   \n62 the data debugging problem efficiently in polynomial time. To figure out its hardness, we study the   \n63 problem DEBUGGABLE which is the decision version of data debugging when the test set consists of   \n64 only one instance. Formally, DEBUGGABLE is defined as follows:   \n65 Problem 1.2 (DEBUGGABLE). Given a classifier $\\mathcal{M}$ , its training data $T$ , a test instance $\\left(\\mathbf{x},y\\right)$ . Is   \n66 there a $T^{\\prime}\\subseteq T$ , such that $\\mathcal{M}$ predicts $y$ on $\\mathbf{x}$ if retrained on $T^{\\prime}$ ?   \n67 Basically, we prove that DEBUGGABLE is NP-complete, which means data debugging is unlikely   \n68 to be solved in polynomial time. This result answers the open question mentioned above directly,   \n69 this is, the large deviation of estimated impacts [10] cannot be reduced or eliminated efficiently. This   \n70 is because if the impact of a subset of the training data could be accurately estimated as the sum of   \n71 the impact of each training sample in the subset, data debugging can be solved in polynomial time,   \n72 which is impossible unless $\\scriptstyle\\mathrm{P=NP}.$   \n73 Although DEBUGGABLE is generally intractable, we still hope to develop efficient algorithms tailored   \n74 to specific cases. Thus it is necessary to figure out the root cause of the hardness for DEBUGGABLE.   \n75 Previous research are always conducted based on the belief that the complexity of data debugging is   \n76 due to the chosen model architecture is complicated. However, we show that at least for models trained   \n77 by stochastic gradient descent (SGD), the hardness stems from the hyper-parameter configuration   \n78 selected for the SGD training, which was not yet aware of by previous work. To cover a wide range of   \n79 commonly used machine learning models, we take linear classifiers as the model and show that even   \n80 for linear classifiers, DEBUGGABLE is NP-hard as long as they are trained by SGD. Moreover, we   \n81 provided a comprehensive analysis on hyper-parameter configurations that affect the computational   \n82 complexity of DEBUGGABLE, including the loss function, the model dimension and the training   \n83 order. Training order, a.k.a. training data order [17] or order of training samples [18], refers to the   \n84 order in which each training sample is considered during the SGD. Detailed complexity results are   \n85 shown in Table 1. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "86 Our contribution can be concluded as follows: ", "page_idx": 1}, {"type": "text", "text": "87 \u2022 We studied the computational complexity of data debugging and showed that data debugging   \n88 is NP-hard for linear classifiers in the general setting for all possible training orders.   \n89 \u2022 We studied the complexity of DEBUGGABLE when the loss is fixed as the hinge-like   \n90 function. For 2 or higher dimension, DEBUGGABLE is NP-complete when the training order ", "page_idx": 1}, {"type": "table", "img_path": "unMRtFfNhF/tmp/39072e6c6781f43fba596b932a6527655c610ab28f353990cbd750d20f99e6bb.jpg", "table_caption": ["Table 1: Computational complexity of the data debugging problem "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "is adversarially chosen; For one-dimensional cases, DEBUGGABLE can be NP-hard when the interception $\\beta<0$ , and is solvable in linear time when $\\beta\\geq0$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 We proved that DEBUGGABLE is solvable in linear time when the loss function is linear. ", "page_idx": 2}, {"type": "text", "text": "94 Moreover, we have a discussion on the implications of these complexity results for machine learning   \n95 interpretability and data quality, as well as limitations of score-based greedy methods. Our results   \n96 suggest the further study as follows. (1) It is better to characterize the training sample and find the   \n97 criterion which can be used to decide the existence of efficient algorithms; (2) Designing algorithms   \n98 with CSP-solver is a potential way to solve data debugging more efficiently than the brute-force one;   \n99 (3) Developing random algorithms is a potential way to solve data debugging successfully with high   \n100 probability. ", "page_idx": 2}, {"type": "text", "text": "101 1.1 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 The solution of data debugging has applications in database query results reliability enhancement   \n103 [2, 19], training data cleaning [1] and machine learning interpretation[9, 8, 10, 20, 21]. Existing   \n104 works on data debugging mainly adopt a two-phase approach, which scores the training samples in the   \n105 first phase and greedily deletes training samples with lower scores in the second phase. Most of the   \n106 research focus on the first phase. There are mainly two ways of scoring adopted for data debugging in   \n107 practice. Leave-one-out (LOO) retraining is a widely studied way, which evaluates the contribution of   \n108 a training sample through the difference in the model\u2019s accuracy trained without that training sample.   \n109 To avoid the cost of model retraining, Koh and Liang took influence functions as an approximation of   \n110 LOO [8]. After that, various extensions and improvements of the influence function based method   \n111 are proposed, such as Fisher kernel [9], influence function for group impacts [10], second-order   \n112 approximations [11] and scalable influence functions [12]. Another way is Shapley-based scoring,   \n113 where the impact of a training sample is measured by its average marginal contribution to all subsets   \n114 of the training data [13]. Since Shapley-base scoring suffers from expensive computational cost [22],   \n115 recent works focus on techniques that efficiently estimate the Shapley value, including Monte-Carlo   \n116 sampling [13], group testing [14, 15] and using proxy models such as $k$ -NN [16, 3]. However,   \n117 those methods do not admit any theoretical guarantee on the effectiveness. This paper discusses the   \n118 limitations of the above methods and suggests some future directions on data debugging. ", "page_idx": 2}, {"type": "text", "text": "119 2 Preliminaries and Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 Linear classifiers. Formally, a (binary) linear classifier is a function $\\lambda_{\\mathbf{w}}:\\mathbb{R}^{d}\\rightarrow\\{-1,1\\}$ , where $d$ is   \n121 called its dimension and $\\mathbf{w}\\in\\mathbb{R}^{d}$ its parameter. Without loss of generality, the bias term of a linear   \n122 classifier is set as zero in this paper. All vectors in this paper are assumed to be column vectors. For   \n123 an input $\\mathbf{x}$ , the value of $\\lambda_{\\mathbf{w}}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda_{\\mathbf{w}}(\\mathbf{x})={\\binom{1}{-1}}\\quad{\\mathrm{if~}}\\mathbf{w}^{\\top}\\mathbf{x}\\geq0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 We denote the class of linear models as $\\Lambda$ . ", "page_idx": 2}, {"type": "text", "text": "125 Training data. A training sample is a pair $(\\mathbf{x},y)$ in which $\\mathbf{x}\\in\\mathbb{R}^{d}$ is the input and $y\\in\\{-1,1\\}$ is   \n126 the label of $\\mathbf{x}$ . The training data is a multiset of training samples. We employ $\\mathbf{w}\\xrightarrow{T}\\mathbf{w}^{\\prime}$ to denote   \n127 that the parameter $\\ensuremath{\\mathbf{w}}^{\\prime}$ is obtained by training the parameter $\\mathbf{w}$ on the training data $T$ , and employ   \n128 \u2212(x\u2212,\u2212y)\u2192w\u2032 to denote that w\u2032 is obtained by training w on the training sample (x, y).   \n129 Loss functions and learning rates. Binary linear classifiers typically use unary functions on yw $^\\top\\mathbf{x}$   \n130 as their loss functions [23]. Therefore we only consider loss functions of the form $\\mathcal{L}:y\\mathbf{w}^{\\top}\\bar{\\mathbf{x}}\\mapsto\\mathbb{R}$   \n131 for the rest of the paper. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "132 The linear loss is in the form of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{1in}}(y\\mathbf{w}^{\\top}\\mathbf{x})=-\\alpha(y\\mathbf{w}^{\\top}\\mathbf{x}+\\boldsymbol{\\beta}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 The hinge-like loss function is defined as the following form ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{hinge}}(y\\mathbf{w}^{\\top}\\mathbf{x})={\\left\\{\\begin{array}{l l}{-\\alpha(y\\mathbf{w}^{\\top}\\mathbf{x}+{\\boldsymbol{\\beta}}),}&{y\\mathbf{w}^{\\top}\\mathbf{x}<{\\boldsymbol{\\beta}}}\\\\ {0,}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 We call $\\beta$ as the interception of $\\mathcal{L}_{\\mathrm{hinge}}$ . We represent the learning rate of a model using a vector   \n135 $\\pmb{\\eta}=(\\eta_{1},\\dots,\\eta_{d})$ , where $\\eta_{i}~\\geq~0$ and each parameter $w_{i}$ can be updated with the corresponding   \n136 learning rate $\\eta_{i}$ .   \n137 Stochastic gradient descent. The stochastic gradient descent (SGD) method updates parameter w   \n138 from its initial value $\\mathbf{w}^{(0)}$ through several epochs. During each epoch, the SGD goes through the   \n139 entire set of training samples in some training order through several iterations. The training order is   \n140 defined as a sequence of training samples, in the form of $\\left(\\mathbf{x}_{1},y_{1}\\right)\\ldots\\left(\\mathbf{x}_{n},y_{n}\\right)$ . For $1\\leq i<j\\leq n$ ,   \n141 $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ is considered before $(\\mathbf{x}_{j},y_{j})$ during the SGD. We use $w_{i}$ to denote the $i$ -th coordinate of w.   \n142 We also use $\\mathbf{w}^{(e,k)}$ to denote the value of w at the end of $k$ -th iteration of epoch $e$ and use $\\mathbf{w}^{(e)}$ to   \n143 denote the value of w after the end of epoch $e$ . Assuming $(\\mathbf{x},y)$ to be the training sample considered   \n144 at iteration $k$ , the stochastic gradient descent (SGD) method updates parameter $w_{i}$ for each $i$ by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{i}^{(e,k)}\\leftarrow w_{i}^{(e,k-1)}-\\eta_{i}\\cdot\\frac{\\partial\\mathcal{L}(y(\\mathbf{w}^{(e,k-1)})^{\\top}\\mathbf{x})}{\\partial w_{i}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 In other words, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{w}^{(e,k)}\\leftarrow\\mathbf{w}^{(e,k-1)}-\\pmb{\\eta}\\otimes\\nabla\\mathcal{L}(y(\\mathbf{w}^{(e,k-1)})^{\\top}\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 where \u03b7 \u2297\u2207L = (\u03b71 \u2202\u2202wL1 , . . . , \u03b7d \u2202\u2202wLd ) is the Hadamard product. We say a training sample x   \n147 is activated at iteration $k$ during epoch $e$ if $\\nabla\\mathcal{L}\\big(\\boldsymbol{y}\\big(\\mathbf{w}^{(e,k-1)}\\big)^{\\top}\\mathbf{x}\\big)\\,\\neq\\,0$ . The SGD terminates at   \n148 the end of epoch $e$ if $\\|\\mathbf{w}^{(e-1)}\\,-\\,\\mathbf{w}^{(e)}\\|\\;<\\;\\varepsilon$ for threshold $\\varepsilon$ or $e$ reached some predetermined   \n149 value. We denote $\\mathbf{w}^{*}\\;=\\;\\mathbf{w}^{(e)}$ . A linear classifier trained by SGD with the meta-parameters   \n150 mentioned above is denoted as $\\mathtt{S G D}_{\\Lambda}(\\mathcal{L},\\eta,\\varepsilon,T)=\\lambda_{\\mathbf{w}^{*}}$ . With a slight abuse of notation, we define   \n151 $\\mathtt{S G D}_{\\Lambda}(\\mathcal{L},\\eta,\\varepsilon,T,\\mathbf{x})=\\lambda_{\\mathbf{w}^{*}}(\\mathbf{x})$ . We also use $\\mathtt{S G D}_{\\Lambda}(T,\\mathbf{x})$ to avoid cluttering when the context is clear.   \n152 Problem definition. With the above definitions, DEBUGGABLE for SGD-trained linear classifiers   \n153 can be formalized as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "154 ", "page_idx": 3}, {"type": "table", "img_path": "unMRtFfNhF/tmp/c68d0aac297d82fc73a1856addc8a74a5b4326f4bfeb6926139732f7f3254a77.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "155 We say $\\mathtt{S G D}_{\\Lambda}(\\mathcal{L},\\eta,\\varepsilon,T)$ is debuggable on $\\left(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}}\\right)$ if $(\\mathcal{L},\\mathbf{w}^{(0)},\\pmb{\\eta},\\varepsilon,T,\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}})$ is a yes-instance   \n156 of DEBUGGABLE-LIN, and not debuggable on $\\left(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}}\\right)$ otherwise. ", "page_idx": 3}, {"type": "text", "text": "157 3 Results for Unfixed Loss Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "158 In this section, we prove the NP-hardness of DEBUGGABLE-LIN. Intuitively, DEBUGGABLE-LIN is   \n159 to determine whether there exists a subset $T^{\\prime}\\subseteq T$ where activated training samples within $T^{\\prime}$ drive   \n160 the parameter w toward the region defined by $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$ . The activation of training samples   \n161 depends on the complex interaction between the training data and the model. ", "page_idx": 3}, {"type": "text", "text": "162 Theorem 3.1. DEBUGGABLE-LIN is NP-hard for all training orders. ", "page_idx": 3}, {"type": "text", "text": "163 We only show the proof sketch and leave the details in the appendix. ", "page_idx": 3}, {"type": "text", "text": "165 ", "page_idx": 4}, {"type": "table", "img_path": "unMRtFfNhF/tmp/baf0b24f71ba480c0f02d8f4d058d8e2f04161378c885ae7ee2060f78778a775.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "166 For example, $\\varphi_{1}=(x_{1}\\lor x_{2}\\lor x_{3})\\land(x_{2}\\lor x_{3}\\lor x_{4})$ is a yes-instance because $(x_{1},x_{2},x_{3},x_{4})=$   \n167 (T,F,F,T) is an 1-in-3 assignment; $\\varphi_{2}=(x_{1}\\vee x_{2}\\vee x_{3})\\wedge(x_{2}\\vee x_{3}\\vee x_{4})\\wedge(x_{1}\\vee x_{2}\\vee x_{4})\\wedge(x_{1}\\vee x_{3}\\vee x_{4})$   \n168 is a no-instance.   \n169 Given a 3-CNF formula $\\varphi$ , our goal is to construct a configuration of the training process, such that   \n170 the resulting model outputs the correct answer if and only if its training data $T^{\\prime}$ encodes an 1-in-3   \n171 assignment $\\nu$ of $\\varphi$ . This can be done by carefully designing the encoding so that for each $x_{i}\\in\\varphi$ ,   \n172 $\\nu(x_{i})=\\mathrm{TRUE}$ if and only if $\\mathbf{t}_{x_{i}}\\in T^{\\prime}$ . Finally, we can construct some $T$ with $T\\supseteq T^{\\prime}\\cup\\{\\mathbf{t}_{x_{i}}|x_{i}\\in\\varphi\\}$ ,   \n173 such that some classifier trained on $T$ is a yes-instance of DEBUGGABLE-LIN if and only if $\\varphi$ is a   \n174 yes-instance of MONOTONE 1-IN-3 SAT, thereby finishing our proof.   \n175 The reduction. Suppose $\\varphi$ has $m$ clauses and $n$ variables, let $N=n+2m+1$ . We set the dimension   \n176 of the linear classifier to $N$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "177 The input. Each coordinate of the input is named as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}=(x_{c_{1}},\\hdots,x_{c_{m}},x_{x_{1}},\\hdots,x_{x_{n}},x_{b_{1}},\\hdots,x_{b_{m}},x_{\\mathrm{dump}})^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 We also use $x_{i}$ to denote the $i$ -th coordinate of $\\mathbf{x}$ . ", "page_idx": 4}, {"type": "text", "text": "179 The parameters. Each coordinate of the parameter is named as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{w}=(w_{c_{1}},\\hdots,w_{c_{m}},w_{x_{1}},\\hdots,w_{x_{n}},w_{b_{1}},\\hdots,w_{b_{m}},w_{\\mathrm{dumny}})^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "180 We also use $w_{i}$ to denote the $i$ -th coordinate of w. Each $w_{x_{j}}$ represents the truth value of variable $x_{j}$ ,   \n181 where 1 represents TRUE and $^{-1}$ represents FALSE. Similarly, each $w_{c_{j}}$ represents the truth value of   \n182 clause $c_{j}$ based on the value of its variables. $w_{b_{j}}$ and $w_{\\mathsf{d u m m y}}$ are used for convenience of proof. ", "page_idx": 4}, {"type": "text", "text": "183 The initial value of the parameter is set to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{w}^{(0)}=(\\overbrace{\\frac{1}{2},\\ldots,\\frac{1}{2}}^{m},\\overbrace{-1,\\ldots,-1}^{n},\\overbrace{-1,\\ldots,-1}^{m},1)^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 Loss function. We denote $U(x_{0},\\delta):=\\{x|x_{0}-\\delta<x<x_{0}+\\delta\\}$ as the $\\delta$ -neighborhood of $x_{0}$ and   \n185 define $U(\\pm x_{0},\\delta)=U(x_{0},\\delta)\\cup U(-x_{0},\\delta)$ . We define the local ramp function as ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{x_{0},\\delta}(x)=\\left\\{\\!\\!\\begin{array}{l l}{0}&{,x\\leq x_{0}-\\delta;}\\\\ {x-x_{0}+\\delta}&{,x\\in U(x_{0},\\delta);}\\\\ {2\\delta}&{,x\\geq x_{0}+\\delta.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 The loss function is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{12N}{5}r_{-5,0.01}(y\\mathbf{w}^{\\top}\\mathbf{x})-r_{-\\frac{1}{2},0.26}(y\\mathbf{w}^{\\top}\\mathbf{x})-\\frac{1}{1000N}\\sum_{x_{0}\\in\\{\\pm1,\\pm3\\}}r_{x_{0},0.01}(y\\mathbf{w}^{\\top}\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "187 $\\mathcal{L}$ is monotonically decreasing with derivatives ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{i}}=\\left\\{\\begin{array}{l l}{-\\frac{12N}{5}\\cdot y x_{i}}&{\\mathrm{~,~}y\\mathbf{w}^{\\top}\\mathbf{x}\\in U(-5,0.01);}\\\\ {-y x_{i}}&{\\mathrm{~,~}y\\mathbf{w}^{\\top}\\mathbf{x}\\in U(-\\frac{1}{2},0.26);}\\\\ {-\\frac{1}{1000N}y x_{i}}&{\\mathrm{~,~}y\\mathbf{w}^{\\top}\\mathbf{x}\\in\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},0.01);}\\\\ {0}&{\\mathrm{~,~}\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "188 Learning rate. The learning rate for SGD is set to be ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\eta}=(\\stackrel{m}{5},\\stackrel{m}{\\dots},\\stackrel{m}{5},\\stackrel{n}{\\cfrac{1}{6N}},\\stackrel{1}{\\dots},\\stackrel{1}{\\cfrac{1}{6N}},\\stackrel{m}{2000N,\\dots,2000N,1)^{\\top}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "189 Training data. We define two gadgets, ${\\tt v a r}(i)$ and clause $(i,i_{1},i_{2},i_{3})$ , as illustrated in Table 2 and   \n190 3. All the unspecified coordinates are set to zero. We use $T_{0}$ to denote the training data. var $(i)$   \n191 is contained in $T_{0}$ if and only if $x_{i}\\in\\varphi$ , and clause $(i,i_{1},i_{2},i_{3})$ is contained in $T_{0}$ if and only if   \n192 $c_{i}=\\left(x_{i_{1}}\\vee x_{i_{2}}\\vee x_{i_{3}}\\right)\\in\\varphi.$ .   \n193 Threshold and instance. The threshold $\\varepsilon$ can be any fixed value in $\\mathbb{R}_{+}$ . The instance is defined as   \n194 $\\left(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}}\\right)$ , where $y_{\\mathrm{{test}}}=1$ and ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}_{\\mathrm{test}}=(\\overbrace{1,\\ldots,1}^{m},\\overbrace{0,\\ldots,0}^{n+m},\\overbrace{2}^{-11m+5})^{\\top}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195 The following reduction works for all possible training orders. Intuitively, during the training process,   \n196 each ${\\tt v a r}\\left(i\\right)$ in the training data will set $w_{x_{i}}$ to around 1 (that is, mark $x_{i}$ as TRUE) in the first epoch,   \n197 and each clause $(i,i_{1},i_{2},i_{3})$ will set $w_{c_{i}}$ to near $\\textstyle{\\frac{11}{2}}$ in the second epoch, if and only if exactly one   \n198 of $w_{x_{i_{1}}},w_{x_{i_{2}}},w_{x_{i_{3}}}$ is near 1 and the others near $-1$ (that is, mark $c_{i}$ as satisfied if exactly one of   \n199 its literals is TRUE and the others FALSE). The training process terminates at the end of the second   \n200 epoch. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "201 4 Results for Fixed Loss Functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "202 We have proved the NP-hardness for DEBUGGABLE-LIN when the loss function is not fixed. In   \n203 this section, we study the complexity when the loss function is fixed as linear and hinge-like   \n204 functions. Assuming that SGD terminates after only one epoch with a fixed order, we will show   \n205 that DEBUGGABLE-LIN is solvable in linear time for linear loss. For hinge-like loss functions,   \n206 DEBUGGABLE-LIN can be solved in linear time only when the dimension $d=1$ and the interception   \n207 $\\beta\\geq0$ . For the rest cases, DEBUGGABLE-LIN becomes NP-hard. ", "page_idx": 5}, {"type": "text", "text": "208 4.1 The Easy Case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "209 We start with the linear loss function $\\mathcal{L}=-\\alpha(y\\mathbf{w}^{\\top}\\mathbf{x}+\\beta)$ , with which all the training data are   \n210 activated and $\\begin{array}{r}{\\mathbf{w}^{*}=\\mathbf{w}^{*}(T)=\\mathbf{w}^{(0)}+\\sum_{(\\mathbf{x},y)\\in T}\\alpha y\\pmb{\\eta}\\otimes\\mathbf{x}}\\end{array}$ . Since $y_{\\mathrm{test}}\\in\\{-1,1\\}$ , DEBUGGABLE-LIN   \n211 is equivalent to deciding whether ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{T^{\\prime}\\subseteq T}\\{y_{\\mathrm{test}}(\\mathbf w^{*}(T^{\\prime}))^{\\top}\\mathbf x_{\\mathrm{test}}\\}>0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "212 A training sample $\\left(\\mathbf{x},y\\right)$ is \u201cgood\u201d if $y_{\\mathrm{test}}(\\alpha y\\pmb{\\eta}\\otimes\\mathbf{x})^{\\top}\\mathbf{x}_{\\mathrm{test}}\\,>\\,0$ and \u201cbad\u201d otherwise. The good   \n213 training-sample assessment (GTA) algorithm, as shown in Algorithm 1, deals with this situation by   \n214 greedily picking all \u201cgood\u201d training samples. ", "page_idx": 5}, {"type": "text", "text": "215 Denoting $T^{*}$ as the set of all good data in $T$ , it follows that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\mathrm{test}}(\\mathbf{w}^{*}(T^{*}))^{\\top}\\mathbf{x}_{\\mathrm{test}}=y_{\\mathrm{test}}(\\mathbf{w}^{(0)})^{\\top}\\mathbf{x}_{\\mathrm{test}}+\\displaystyle\\sum_{(\\mathbf{x},y)\\in T^{*}}y_{\\mathrm{test}}(\\alpha y\\pmb{\\eta}\\otimes\\mathbf{x})^{\\top}\\mathbf{x}_{\\mathrm{test}}}\\\\ {\\geq y_{\\mathrm{test}}(\\mathbf{w}^{(0)})^{\\top}\\mathbf{x}_{\\mathrm{test}}+\\displaystyle\\sum_{(\\mathbf{x},y)\\in T^{\\prime}}y_{\\mathrm{test}}(\\alpha y\\pmb{\\eta}\\otimes\\mathbf{x})^{\\top}\\mathbf{x}_{\\mathrm{test}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "216 for all $T^{\\prime}\\subseteq T$ . Hence $\\begin{array}{r}{\\operatorname*{max}_{T^{\\prime}\\subseteq T}\\{y_{\\mathrm{test}}(\\mathbf{w}^{*}(T^{\\prime}))^{\\top}\\mathbf{x}_{\\mathrm{test}}\\}=y_{\\mathrm{test}}(\\mathbf{w}^{*}(T^{*}))^{\\top}\\mathbf{x}_{\\mathrm{test}}}\\end{array}$ and DEBUGGABLE  \n217 LIN can be solved by GTA in linear time. The following theorem is straightforward. ", "page_idx": 5}, {"type": "text", "text": "218 Theorem 4.1. DEBUGGABLE-LIN is linear time solvable for linear loss functions. ", "page_idx": 6}, {"type": "image", "img_path": "unMRtFfNhF/tmp/0a594813c74a3b75e9dbf1055440880b2818c38d39a361d10d7d2fcfee23effb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "220 GTA is still effective for one-dimensional classifiers trained with hinge-like losses when $\\beta\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "221 Theorem 4.2. DEBUGGABLE-LIN is linear time solvable for hinge-like loss functions, when $d=1$   \n222 and $\\beta\\geq0$ .   \n223 Proof. It suffices to prove that if $\\exists T^{\\prime}\\subseteq T$ such that $\\mathsf{S G D}_{\\Lambda}(T^{\\prime},x_{\\mathrm{test}})=y_{\\mathrm{test}}$ , $\\mathrm{SGD}_{\\Lambda}(T^{*},x_{\\mathrm{test}})=y_{\\mathrm{test}}.$   \n224 a) Suppose all the data in $T^{*}$ are activated, we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{y_{\\mathrm{test}}w^{*}(T^{*})x_{\\mathrm{test}}=y_{\\mathrm{test}}w^{(0)}x_{\\mathrm{test}}+\\sum_{\\substack{(x,y)\\in T^{*}}}y_{\\mathrm{test}}\\alpha y\\eta x x_{\\mathrm{test}}}}\\\\ &{\\ge y_{\\mathrm{test}}w^{(0)}x_{\\mathrm{test}}+\\sum_{\\substack{(x,y)\\in T^{\\prime}\\cap T^{*}}}y_{\\mathrm{test}}\\alpha y\\eta x x_{\\mathrm{test}}+\\sum_{\\substack{(x,y)\\in T^{*}\\backslash T^{*}}}y_{\\mathrm{test}}\\alpha y\\eta x x_{\\mathrm{test}}}\\\\ &{=y_{\\mathrm{test}}w^{*}(T^{\\prime})x_{\\mathrm{test}}\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "225 b) Suppose $(x,y)\\in T^{*}$ is the first inactivated data during the training phase, and $w$ is the current   \n226 parameter, we have $y w x>\\beta$ . Since $\\alpha\\eta\\cdot(x y)\\cdot(x_{\\mathrm{test}}y_{\\mathrm{test}})\\bar{\\geq}\\ 0$ , we have $(x_{\\mathrm{test}}y_{\\mathrm{test}})\\cdot w\\ge0$ . Let $T^{\\prime\\prime}$ be   \n227 the set of training data appeared before $(x,y)$ , we have $y_{\\mathrm{test}}w^{*}(T^{*})x_{\\mathrm{test}}\\geq y_{\\mathrm{test}}w^{*}(T^{\\prime\\prime})x_{\\mathrm{test}}\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "228 4.2 The Hard Case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "229 The gradient of training data may not always be activated and could be affected by the training order.   \n230 When the training order is adversarially chosen, the following theorem shows that DEBUGGABLE-LIN   \n231 is NP-hard for all $d\\geq2$ and $\\beta\\in\\mathbb{R}$ .   \n232 Theorem 4.3. If the training order is adversarially chosen and $d\\geq2$ , DEBUGGABLE-LIN is NP-hard   \n233 for each hinge-like loss function at every constant learning rate.   \n234 Proof sketch. Since the result can be easily extended for all $d\\,>\\,2$ by padding the other $d\\mathrm{~-~}2$   \n235 dimensions with zeros, we only prove for the case of $d=2$ . We assume $\\beta\\geq-1$ and leave the   \n236 $\\beta<-1$ case to the appendix. To avoid cluttering, we further assume $\\eta=1$ and $\\alpha=1$ . The proof   \n237 can be easily generalized by appropriately re-scaling the constructed vectors. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "238 We build a reduction from the subset sum problem, which is well-known to be NP-hard: ", "page_idx": 6}, {"type": "text", "text": "239 ", "page_idx": 6}, {"type": "table", "img_path": "unMRtFfNhF/tmp/18d2c5231afd54282af24aa22351ab7cf6cfe7cc813d06ddd6064ba576bcfea6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "240 Suppose $n=|S|$ , $m=\\operatorname*{max}_{a\\in S}\\{a\\}$ , $\\gamma=\\operatorname*{max}\\{\\beta,1\\}$ and $S=\\{a_{1},a_{2},\\dots,a_{n}\\}$ . We further assume   \n241 $n>1$ . Let the training data be ", "page_idx": 7}, {"type": "equation", "text": "$$\nT=\\left\\{(\\mathbf{x}_{1},y_{1}),(\\mathbf{x}_{2},y_{2}),\\ldots,(\\mathbf{x}_{n},y_{n})\\right\\}\\cup\\left\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "242 w\u221ahere $\\begin{array}{r}{\\mathbf{x}_{i}y_{i}\\;=\\;(\\frac{\\sqrt{\\gamma}}{n+1},3\\sqrt{\\gamma}a_{i})}\\end{array}$ for all $1\\;\\leq\\;i\\;\\leq\\;n,{\\bf x}_{c}y_{c}\\;=\\;((18n^{2}m^{2}\\:-\\:2)\\sqrt{\\gamma},-3t\\sqrt{\\gamma}),{\\bf x}_{b}y_{b}\\;=$   \n243 $(\\sqrt{\\gamma},-\\sqrt{\\gamma}),{\\bf x}_{a}y_{a}\\,=\\,(\\sqrt{\\gamma},\\sqrt{\\gamma})$ . Let $\\mathbf{w}^{(0)}\\,=\\,(-18n^{2}m^{2}\\sqrt{\\gamma},0)$ . Let the test instance $(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}})$   \n244 satisfy $\\mathbf{x}_{\\mathrm{test}}y_{\\mathrm{test}}=(1,0)$ . ", "page_idx": 7}, {"type": "text", "text": "245 Let the training order be $(\\mathbf{x}_{1},y_{1}),(\\mathbf{x}_{2},y_{2}),\\ldots,(\\mathbf{x}_{n},y_{n}),(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a}).$ ", "page_idx": 7}, {"type": "text", "text": "24 6 For each 1 \u2264i < n, suppose w(0)\u2212T \u2212\u2229\u2212{(\u2212x\u2212i,\u2212y\u2212i)\u2212|1\u2212\u2264\u2212j\u2212\u2264\u2212i}\u2192 , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle y_{i+1}\\mathbf{w}_{i}^{\\top}\\mathbf{x}_{i+1}\\leq\\frac{\\sqrt{\\gamma}}{n+1}(-18n^{2}m^{2}\\sqrt{\\gamma}+\\frac{\\sqrt{\\gamma}i}{n+1})+3\\sqrt{\\gamma}a_{i+1}\\sum_{j=1}^{i}3\\sqrt{\\gamma}a_{j}}\\\\ {\\displaystyle\\leq\\gamma\\left(-\\frac{n-1}{n+1}\\cdot9n m^{2}+\\frac{n}{(n+1)^{2}}\\right)<-1\\leq\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "247 This means all the $T\\setminus\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\}$ can be activated. Thus the resulting parameter   \n248 trained by $T\\setminus\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\}$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}=\\mathbf{w}^{(0)}+\\sum_{i=1}^{n}\\mathbf{x}_{i}y_{i}=\\left(-18n^{2}m^{2}\\sqrt{\\gamma}+\\frac{\\sqrt{\\gamma}|T^{*}|}{n+1},3\\sqrt{\\gamma}\\sum_{i=1}^{n}a_{i}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "249 It now suffices to prove that for all $S^{\\prime}\\;\\subseteq\\;S,\\;\\sum_{a\\in S^{\\prime}}a\\;=\\;t$ if and only if $\\exists T^{\\prime}\\subseteq T$ such that   \n250 $\\mathbf{w}:\\mathbf{w}^{(0)}\\xrightarrow{T^{\\prime}}\\mathbf{w}$ satisfies $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$ .   \n251 If: Suppose $\\exists S^{\\prime}\\subseteq S$ such that $\\textstyle\\sum_{a\\in S}a=t$ , we prove that $\\exists T^{\\prime}\\subseteq T$ such that $y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$   \n252 for $\\mathbf{w}^{*}$ satisfying $\\textbf{w}^{(0)}\\xrightarrow{T^{\\prime}}\\textbf{w}^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "253 Let $T^{*}=\\{(\\mathbf{x}_{i},y_{i})|a_{i}\\in S^{\\prime}\\}$ , $T^{\\prime}=T^{*}\\cup\\left\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\right\\}$ . We have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}=(-18n^{2}m^{2}{\\sqrt{\\gamma}}+{\\frac{{\\sqrt{\\gamma}}|T^{*}|}{n+1}},3{\\sqrt{\\gamma}}\\sum_{a_{i}\\in S^{\\prime}}a_{i})=(-18n^{2}m^{2}{\\sqrt{\\gamma}}+{\\frac{{\\sqrt{\\gamma}}|T^{*}|}{n+1}},3{\\sqrt{\\gamma}}t).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "254 And therefore $\\begin{array}{r}{y_{c}\\mathbf{w}_{c}^{\\top}\\mathbf{x}_{c}=\\gamma\\left((-18n^{2}m^{2}+\\frac{|T^{*}|}{n+1})(18n^{2}m^{2}-2)-9t^{2}\\right)<-1\\leq\\beta,\\mathbf{s}}\\end{array}$ o ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}\\xrightarrow[]{}\\mathbf{w}_{b}=\\mathbf{w}_{c}+\\mathbf{x}_{c}y_{c}=(\\sqrt{\\gamma}(\\frac{|T^{*}|}{n+1}-2),0).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "255 Note that $y_{b}\\mathbf{w}_{b}^{\\top}\\mathbf{x}_{b}=\\gamma(\\frac{|T^{*}|}{n+1}-2)<-1\\leq\\beta$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{b}\\xrightarrow[]{(\\mathbf{x}_{b},y_{b})}\\mathbf{w}_{a}=\\mathbf{w}_{b}+\\mathbf{x}_{a}y_{a}=(\\sqrt{\\gamma}(\\frac{|T^{*}|}{n+1}-1),-\\sqrt{\\gamma})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "256 Note also that $\\begin{array}{r}{y_{a}\\mathbf{w}_{a}^{\\top}\\mathbf{x}_{a}=\\gamma(\\frac{|T^{*}|}{n+1}-2)<-1\\leq\\beta}\\end{array}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{a}\\xrightarrow[n\\]{(\\mathbf{x}_{a},y_{a})}\\mathbf{w}^{*}=\\mathbf{w}_{a}+\\mathbf{x}_{a}y_{a}=(\\frac{|T^{*}|\\sqrt{\\gamma}}{n+1},0)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "257 Therefore, $\\begin{array}{r}{y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}=\\frac{|T^{*}|\\sqrt{\\gamma}}{n+1}>0.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "258 Only if: For each $T^{\\prime}\\subseteq T$ , let $T^{*}\\,=\\,T^{\\prime}\\setminus\\{({\\bf x}_{c},y_{c}),({\\bf x}_{b},y_{b}),({\\bf x}_{a},y_{a})\\}$ . If $y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$ for   \n259 $\\mathbf{w}^{*}$ satisfying $\\textbf{w}^{(0)}\\xrightarrow{T^{\\prime}}\\textbf{w}^{*}$ , we prove that $\\exists S^{\\prime}\\subseteq S$ such that $\\textstyle\\sum_{a\\in S^{\\prime}}a=t$ . We first show that for   \n260 each $T^{\\prime}\\subseteq T$ , if $\\mathbf{w}(\\mathbf{w}^{(0)}\\;\\;\\xrightarrow{T^{\\prime}}\\;\\mathbf{w})$ satisfying $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$ , we have $\\forall k\\,\\in\\,\\{a,b,c\\},(\\mathbf{x}_{k},y_{k})\\,\\in$   \n261 $T^{\\prime},y_{k}\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{k}<\\gamma$ , where $\\mathbf{w}^{(0)}\\;\\xrightarrow{T^{*}}\\mathbf{w}_{c}\\xrightarrow{(\\mathbf{x}_{c},y_{c})}\\mathbf{w}_{b}\\xrightarrow[]{}\\mathbf{w}_{a}$ . Otherwise, suppose $\\exists k\\in\\{a,b,c\\}$   \n262 such that $(\\mathbf{x}_{k},y_{k})\\notin T^{\\prime}$ or $y_{k}\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{k}\\ge\\gamma$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\ny_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}\\leq\\sqrt{\\gamma}(\\frac{|T^{*}|}{n+1}-1)<0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "263 which contradicts to the fact that $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}\\geq0$ . ", "page_idx": 8}, {"type": "text", "text": "264 Let $S^{\\prime}=\\{a_{i}|(\\mathbf{x}_{i},y_{i})\\in T^{*}\\}$ and $\\textstyle t^{\\prime}=\\sum_{a\\in S^{\\prime}}a_{i}$ , it suffices to prove $t^{\\prime}=t$ . Notice that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbf{w}^{(0)}\\xrightarrow{T^{*}\\cap\\{(\\mathbf{x}_{i},y_{i})|1\\leq j\\leq i\\}}\\mathbf{w}_{c}=(\\sqrt{\\gamma}(-18n^{2}m^{2}+\\frac{|T^{*}|}{n+1}),3\\sqrt{\\gamma}\\displaystyle\\sum_{a_{i}\\in S^{\\prime}}a_{i})}&{}&\\\\ {=(\\sqrt{\\gamma}(-18n^{2}m^{2}+\\frac{|T^{*}|}{n+1}),3\\sqrt{\\gamma}t^{\\prime})}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "265 Hence $\\begin{array}{r}{y_{c}\\mathbf{w}_{c}^{\\top}\\mathbf{x}_{c}=\\gamma(-18n^{2}m^{2}+\\frac{|T^{*}|}{n+1})(18n^{2}m^{2}-2)-9\\gamma t t^{\\prime}<-1\\leq\\beta,}\\end{array}$ , thus ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}\\xrightarrow[]{}\\mathbf{w}_{b}=\\mathbf{w}_{c}+\\mathbf{x}_{c}y_{c}=(\\sqrt{\\gamma}(\\frac{|T^{*}|}{n+1}-2),3\\sqrt{\\gamma}(t^{\\prime}-t))\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "266 (1) If $t^{\\prime}\\le t-1$ , we have $\\begin{array}{r}{y_{b}\\mathbf{w}_{b}^{\\top}\\mathbf{x}_{b}=\\gamma\\left(\\frac{|T^{*}|}{n+1}-2+3(t-t^{\\prime})\\right)>\\gamma\\geq\\beta}\\end{array}$ , a contradiction. ", "page_idx": 8}, {"type": "text", "text": "267 (2) If $t^{\\prime}\\geq t+1$ , we have $y_{a}\\mathbf{w}_{a}^{\\top}\\mathbf{x}_{a}=\\gamma\\left(\\frac{|T^{*}|}{n+1}-2+3(t^{\\prime}-t)\\right)>\\gamma\\geq\\beta$ , another contradiction.   \n268 Therefore $t^{\\prime}=t$ , and this completes the proof. ", "page_idx": 8}, {"type": "text", "text": "269 Moreover, DEBUGGABLE-LIN is NP-hard even when $d=1$ and $\\beta<0$ . ", "page_idx": 8}, {"type": "text", "text": "270 Theorem 4.4. If the training order is adversarially chosen and $d=1$ , DEBUGGABLE-LIN remains   \n271 NP-hard for each hinge-like loss function with $\\beta<0$ at every constant learning rate.   \n272 Remarks. The training order in this section can be arbitrary as long as the last three training   \n273 samples are $(\\mathbf{x}_{c},y_{c}),\\,(\\bar{\\mathbf{x}_{b}},y_{b}),(\\mathbf{x}_{a},y_{a})$ , respectively. All the training samples are \u201cgood\u201d since for   \n274 each $(\\mathbf{x},y)\\in T$ we have $\\mathbf{x}^{\\top}\\mathbf{x}_{\\mathrm{test}}y y_{\\mathrm{test}}>0$ . This implies that DEBUGGABLE-LIN is NP-hard even if   \n275 all the training data are \u201cgood\u201d training samples, and exemplifies why the GTA algorithm fails for   \n276 higher dimensions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "277 5 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "278 In this paper, we provided a comprehensive analysis on the complexity of DEBUGGABLE. We focus   \n279 on the linear classifier that is trained using SGD, as it is a key component in the majority of popular   \n280 models.   \n281 Since DEBUGGABLE is a special case of data debugging, the above results proved the intractability   \n282 of data debugging and therefore gives a negative answer to Problem 1.1 declared in the introduction.   \n283 The complexity results also demonstrated that it is not accurate to estimate the impact of subset of   \n284 training data by summing up the score of each training samples in the subset, as long as the scores   \n285 can be calculated in polynomial time.   \n286 In Section 4, a training sample is said to be \u201cgood\u201d if it can help the resulting model to predict   \n287 correctly on the test instance. That is, it can increase $y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{\\dot{x}}_{\\mathrm{test}}$ . However, in our proof we   \n288 showed that DEBUGGABLE remains NP-hard even if all training samples are \u201cgood\u201d. This suggests   \n289 that the quality of a training sample does not depend only on some properties of itself but also on   \n290 the interaction between the rest of the training data, which should be taken into consideration when   \n291 developing data cleaning approaches.   \n292 Moreover, the NP-hardness of DEBUGGABLE implies that, it is in general intractable to figure out the   \n293 causality between even the prediction of a linear classifier and its training data. This may be seem   \n294 surprising since linear classifiers have long been considered \u201cinherently interpretable\u201d. As warned   \n295 in [25], a method being \u201cinherently interpretable\u201d needs to be verified before it can be trusted, the   \n296 concept of interpretability must be rigorously defined, or at least its boundaries specified.   \n297 Our results suggests the following directions for future research. Firstly, characterizing the training   \n298 sample may be helpful in designing efficient algorithms for data debugging; Secondly, designing   \n299 algorithms using CSP-solver is a potential way to solve data debugging more efficiently than the brute  \n300 force algorithms; Finally, developing random algorithms is a potential way to solve data debugging   \n301 successfully with high probability. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "302 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "303 [1] Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data Cleansing for Models Trained with SGD.   \n304 Curran Associates Inc., Red Hook, NY, USA, 2019.   \n305 [2] Weiyuan Wu, Lampros Flokas, Eugene Wu, and Jiannan Wang. Complaint-driven training data debugging   \n306 for query 2.0. pages 1317\u20131334, 06 2020. doi: 10.1145/3318464.3389696.   \n307 [3] Bojan Karla\u0161, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data   \n308 debugging with shapley importance over end-to-end machine learning pipelines, 2022.   \n309 [4] Felix Neutatz, Binger Chen, Ziawasch Abedjan, and Eugene Wu. From cleaning before ml to cleaning for   \n310 ml. IEEE Data Eng. Bull., 44:24\u201341, 2021. URL https://api.semanticscholar.org/CorpusID:   \n311 237542697.   \n312 [5] Peng Li, Xi Rao, Jennifer Blase, Yue Zhang, Xu Chu, and Ce Zhang. Cleanml: A study for evaluating the   \n313 impact of data cleaning on ml classification tasks. In 2021 IEEE 37th International Conference on Data   \n314 Engineering (ICDE), pages 13\u201324, 2021. doi: 10.1109/ICDE51399.2021.00009.   \n315 [6] Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. If influence functions are   \n316 the answer, then what is the question? In Proceedings of the 36th International Conference on Neural   \n317 Information Processing Systems, NIPS \u201922, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN   \n318 9781713871088.   \n319 [7] Romila Pradhan, Jiongli Zhu, Boris Glavic, and Babak Salimi. Interpretable data-based explanations for   \n320 fairness debugging. In Proceedings of the 2022 International Conference on Management of Data,   \n321 SIGMOD \u201922, page 247\u2013261, New York, NY, USA, 2022. Association for Computing Machinery.   \n322 ISBN 9781450392495. doi: 10.1145/3514221.3517886. URL https://doi.org/10.1145/3514221.   \n323 3517886.   \n324 [8] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In   \n325 Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page   \n326 1885\u20131894. JMLR.org, 2017.   \n327 [9] Rajiv Khanna, Been Kim, Joydeep Ghosh, and Oluwasanmi Koyejo. Interpreting black box predictions   \n328 using fisher kernels. In International Conference on Artificial Intelligence and Statistics, 2018. URL   \n329 https://api.semanticscholar.org/CorpusID:53085397.   \n330 [10] Pang Wei Koh, Kai-Siang Ang, Hubert Hua Kian Teo, and Percy Liang. On the accuracy of influence   \n331 functions for measuring group effects. In Neural Information Processing Systems, 2019. URL https:   \n332 //api.semanticscholar.org/CorpusID:173188850.   \n333 [11] Samyadeep Basu, Xuchen You, and Soheil Feizi. On second-order group influence functions for black  \n334 box predictions. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920.   \n335 JMLR.org, 2020.   \n336 [12] Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. FastIF: Scalable influence   \n337 functions for efficient model interpretation and debugging. In Marie-Francine Moens, Xuanjing Huang,   \n338 Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods   \n339 in Natural Language Processing, pages 10333\u201310350, Online and Punta Cana, Dominican Republic,   \n340 November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.808.   \n341 URL https://aclanthology.org/2021.emnlp-main.808.   \n342 [13] Amirata Ghorbani and James Y. Zou. Data shapley: Equitable valuation of data for machine learning.   \n343 ArXiv, abs/1904.02868, 2019. URL https://api.semanticscholar.org/CorpusID:102350503.   \n344 [14] R. Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nicholas Hynes, Nezihe Merve G\u00fcrel, Bo Li,   \n345 Ce Zhang, Dawn Xiaodong Song, and Costas J. Spanos. Towards efficient data valuation based on the   \n346 shapley value. ArXiv, abs/1902.10275, 2019. URL https://api.semanticscholar.org/CorpusID:   \n347 67855573.   \n348 [15] Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn   \n349 Song. Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification?   \n350 In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8235\u20138243,   \n351 2021. doi: 10.1109/CVPR46437.2021.00814.   \n352 [16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas   \n353 Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. Proc.   \n354 VLDB Endow., 12(11):1610\u20131623, jul 2019. ISSN 2150-8097. doi: 10.14778/3342263.3342637. URL   \n355 https://doi.org/10.14778/3342263.3342637.   \n356 [17] Jeremy Mange. Effect of training data order for machine learning. In 2019 International Conference   \n357 on Computational Science and Computational Intelligence (CSCI), pages 406\u2013407, 2019. doi: 10.1109/   \n358 CSCI49370.2019.00078.   \n359 [18] Ernie Chang, Hui-Syuan Yeh, and Vera Demberg. Does the order of training samples matter? improving   \n360 neural data-to-text generation with curriculum learning. ArXiv, abs/2102.03554, 2021. URL https:   \n361 //api.semanticscholar.org/CorpusID:231846815.   \n362 [19] Yejia Liu, Weiyuan Wu, Lampros Flokas, Jiannan Wang, and Eugene Wu. Enabling sql-based training   \n363 data debugging for federated learning. Proceedings of the VLDB Endowment, 15:388\u2013400, 02 2022. doi:   \n364 10.14778/3494124.3494125.   \n365 [20] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding   \n366 the origins of bias in word embeddings, 2019.   \n367 [21] Hao Wang, Berk Ustun, and Flavio P. Calmon. Repairing without retraining: Avoiding disparate impact   \n368 with counterfactual distributions, 2019.   \n369 [22] Xiaotie Deng and Christos H. Papadimitriou. On the complexity of cooperative solution concepts. Math.   \n370 Oper. Res., 19:257\u2013266, 1994. URL https://api.semanticscholar.org/CorpusID:12946448.   \n371 [23] Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine   \n372 learning. Annals of Data Science, 9, 04 2022. doi: 10.1007/s40745-020-00253-5.   \n373 [24] Erik D. Demaine, William Gasarch, and Mohammad Hajiaghayi. Computational Intractability: A Guide to   \n374 Algorithmic Lower Bounds. MIT Press, 2024.   \n375 [25] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and   \n376 evaluate faithfulness? In Annual Meeting of the Association for Computational Linguistics, 2020. URL   \n377 https://api.semanticscholar.org/CorpusID:215416110.   \n378 [26] Victor Parque. Tackling the subset sum problem with fixed size using an integer representation scheme.   \n379 In 2021 IEEE Congress on Evolutionary Computation (CEC), pages 1447\u20131453, 2021. doi: 10.1109/   \n380 CEC45853.2021.9504889. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "381 A Detailed Proofs for Section 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "382 Notations. Given some orderings $\\{o_{e}\\}$ of training data, where $o_{\\mathbf{t}}^{e}$ as the order of $\\mathbf{t}$ in epoch $e$ . We   \n383 use $w_{x_{i}}^{(e,l)}$ to denote the value of $w_{x_{i}}$ after the $l$ -th iteration in epoch $e$ . We also denote $\\mathbf{x_{t}}$ and $y_{\\mathbf{t}}$ as   \n384 the feature and the label of training data $\\mathbf{t}$ , respectively. We denote $\\mathbf{t}^{(e,l)}$ as the training sample being   \n385 considered during epoch $e$ , iteration $l$ .   \n386 Lemma A.1. Suppose $T\\ \\subseteq\\ T_{0}$ is the training data and let $T_{l,r}^{e}\\;=\\;\\{{\\bf t}^{(e,l)},{\\bf t}^{(e,l+1)},\\ldots,{\\bf t}^{(e,r)}\\}$   \n387 be the set of consecutive training samples considered during epoch $e$ from iteration $l$ to $r$ . For   \n388 $1\\leq l\\leq r\\leq|T|$ , if $\\mathtt{c l a u s e}(\\gamma,i_{1},i_{2},i_{3})\\not\\in T_{l,r}^{e}$ , then $w_{c_{\\gamma}}^{(e,l-1)}=\\bar{w_{c_{\\gamma}}^{(e,r)}}$ ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "389 Proof. For each $\\mathbf{t}\\in T_{l,r}^{e}$ , we have $(x_{\\mathbf{t}})_{c_{\\gamma}}=0$ . Therefore ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\mathcal{L}}{\\partial c_{\\gamma}}\\right|_{\\mathbf{t}}\\Biggr|\\leq\\operatorname*{max}\\left\\{\\left|-\\frac{12N}{5}y x_{c_{\\gamma}}\\right|,|-y x_{c_{\\gamma}}|,\\left|-\\frac{1}{1000N}y x_{c_{\\gamma}}\\right|,0\\right\\}=0\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "390 Hence $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial c_{\\gamma}}\\bigg\\rvert_{\\mathbf{t}}=0}\\end{array}$ , and ", "page_idx": 11}, {"type": "equation", "text": "$$\nw_{c_{\\gamma}}^{(e,r)}=w_{c_{\\gamma}}^{(e,l-1)}-\\eta_{c_{\\gamma}}\\sum_{\\mathbf{t}\\in T_{l,r}^{e}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial c_{\\gamma}}\\right|_{\\mathbf{t}}=w_{c_{\\gamma}}^{(e,l-1)}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "391 Similarly, $(x_{\\mathbf{t}})_{b_{\\gamma}}=0$ , and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left|\\left.\\frac{\\partial\\mathcal{L}}{\\partial b_{\\gamma}}\\right|_{\\mathbf{t}}\\right|\\leq\\operatorname*{max}\\left\\{\\left|-\\frac{12N}{5}y x_{b_{\\gamma}}\\right|,|-y x_{b_{\\gamma}}|,\\left|-\\frac{1}{1000N}y x_{b_{\\gamma}}\\right|,0\\right\\}=0\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "392 Hence $\\begin{array}{r}{\\left.\\frac{\\partial\\mathcal{L}}{\\partial b_{\\gamma}}\\right|_{\\mathbf{t}}=0}\\end{array}$ , and ", "page_idx": 11}, {"type": "equation", "text": "$$\nw_{b_{\\gamma}}^{(e,r)}=w_{b_{\\gamma}}^{(e,l-1)}-\\eta_{b_{\\gamma}}\\sum_{\\mathbf{t}\\in T_{l,r}^{e}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial b_{\\gamma}}\\right|_{\\mathbf{t}}=w_{b_{\\gamma}}^{(e,l-1)}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "393 ", "page_idx": 11}, {"type": "text", "text": "394 Lemma A.2. Suppose $T\\subseteq T_{0}$ is the training data and $T_{l}:=\\{\\mathbf{t}^{(1,1)},\\dots,\\mathbf{t}^{(1,l)}\\}$ . $\\forall1\\leq i\\leq n,1\\leq$   \n395 $l\\leq|T|$ , $\\begin{array}{r}{w_{x_{i}}^{(1,l)}\\in U(1,\\frac{l+1}{6000N^{2}})}\\end{array}$ if va $\\tau(i)\\!\\in\\!T_{l}$ ; Otherwise $\\begin{array}{r}{w_{x_{i}}^{(1,l)}\\in U(-1,\\frac{l+1}{6000N^{2}})}\\end{array}$ . ", "page_idx": 11}, {"type": "text", "text": "396 Proof. We prove this lemma by induction. ", "page_idx": 11}, {"type": "text", "text": "397 Basic Case: Note that for all $1\\leq i\\leq n$ , $w_{x_{i}}^{(0)}=-1$ = \u22121, and for all 1 \u2264\u03b3 \u2264m, wc\u03b3 $1\\leq\\gamma\\leq m,w_{c_{\\gamma}}^{(0)}=1/2,w_{b_{\\gamma}}^{(0)}=-1$ .   \n398 We denote $\\mathbf{t}=\\mathbf{t}^{(1,1)}$ to avoid cluttering. For any fixed $i$ : ", "page_idx": 11}, {"type": "text", "text": "399 (1) If $\\mathbf{t}=\\mathtt{v a r}(i)$ . We have $y_{\\mathbf{t}}(\\mathbf{w}^{(0)})^{\\top}\\mathbf{x_{t}^{\\prime}}=5w_{x_{i}}^{(0)}=-5$ , hence ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}=-\\frac{12N}{5}y_{\\mathbf{t}}(x_{\\mathbf{t}})_{i}=-12N\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "400 and ", "page_idx": 11}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(1,1)}=w_{x_{i}}^{(0)}-\\eta_{x_{i}}\\left.{\\frac{\\partial{\\mathcal{L}}}{\\partial w_{x_{i}}}}\\right|_{\\mathbf{t}}=-1-{\\frac{1}{6N}}\\left(-{\\frac{12N}{5}}\\right)=1\\in U(1,{\\frac{2}{6000N^{2}}})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "401 (2) If $\\mathbf{t}=\\mathtt{c l a u s e}(\\gamma,i,i^{\\prime},i^{\\prime\\prime})$ . We have ", "page_idx": 11}, {"type": "equation", "text": "$$\ny_{\\mathbf{t}}(\\mathbf{w}^{(0)})^{\\top}\\mathbf{x}_{\\mathbf{t}}^{\\prime}=w_{x_{i}}^{(0)}+w_{x_{i^{\\prime}}}^{(0)}+w_{x_{i^{\\prime\\prime}}}^{(0)}+w_{c_{\\gamma}}^{(0)}+\\frac{1}{2}w_{b_{\\gamma}}^{(0)}=-3\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "402 hence ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\bigg\\vert_{\\mathbf{t}}=-\\frac{1}{1000N}y_{\\mathbf{t}}(x_{\\mathbf{t}})_{x_{i}}=-\\frac{1}{1000N}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "403 and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{w_{x_{i}}^{(1,1)}=w_{x_{i}}^{(0)}-\\eta_{x_{i}}\\left.\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\sf t}=-1-\\displaystyle\\frac{1}{6N}\\left(-\\frac{1}{1000N}\\right)}\\\\ {=-1+\\displaystyle\\frac{1}{6000N^{2}}\\in U(-1,\\frac{2}{6000N^{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "404 (3) Otherwise, $w_{x_{i}}$ will not be updated. Therefore $\\begin{array}{r}{w_{x_{i}}^{(1,1)}=w_{x_{i}}^{(0)}=-1\\in U(-1,\\frac{2}{6000N^{2}})}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "405 Hence this lemma is true for $l=1$ . ", "page_idx": 12}, {"type": "text", "text": "406 Induction Step: Suppose the lemma is true for $l<|T|$ . We prove that this lemma remains true for   \n407 $l+1$ . We denote $\\mathbf{t}=\\mathbf{t}^{(1,l+1)}$ to avoid cluttering. This makes sense since $l+1\\leq|T|$ and thus $\\mathbf{t}\\in T$ .   \n408 For any fixed $i$ : ", "page_idx": 12}, {"type": "text", "text": "409 (1) If $\\mathbf{t}=\\mathtt{v a r}(i)$ , then $\\mathtt{v a r}(i)\\not\\in T_{l}$ because there are at most one var $(i)$ in $T$ for each $i$ . ", "page_idx": 12}, {"type": "text", "text": "410 Therefore w(x1i, $\\begin{array}{r}{w_{x_{i}}^{(1,l)}~\\in~U(-1,\\frac{l+1}{6000N^{2}})}\\end{array}$ . We have $y_{\\mathbf{t}}(\\mathbf{w}^{(1,l)})^{\\top}\\mathbf{x_{t}^{\\prime}}~=~5w_{x_{i}}^{(1,l)}~\\in~U(-5,0.01)$ , and   \n411 $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\bigg\\vert_{\\mathbf{t}}=-\\frac{12N}{5}y_{\\mathbf{t}}(x_{\\mathbf{t}})_{i}=-12N}\\end{array}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{x_{i}}^{(1,l+1)}=w_{x_{i}}^{(1,l)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}=w_{x_{i}}^{(1,l)}-\\frac{1}{6N}\\left(-\\frac{12N}{5}\\right)}\\\\ &{\\qquad\\qquad=w_{x_{i}}^{(1,l)}+2\\in U(1,\\frac{l+2}{6000N^{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "412 (2) If $\\mathbf{t}=\\mathtt{c l a u s e}(\\gamma,i,i^{\\prime},i^{\\prime\\prime})$ . In this case, clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{1,l}^{1}$ and by Lemma A.1 we have   \n413 $w_{c_{\\gamma}}^{(1,l)}=w_{c_{\\gamma}}^{(0)},w_{b_{\\gamma}}^{(1,l)}=w_{b_{\\gamma}}^{(0)}.$ wb(\u03b30) . From the induction hypothesis we have ", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(1,l)},w_{x_{i^{\\prime}}}^{(1,l)},w_{x_{i^{\\prime\\prime}}}^{(1,l)}\\in U(\\pm1,\\frac{l+1}{6000N^{2}})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "414 and thus ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle y_{\\mathbf{t}}(\\mathbf{w}^{(1,l)})^{\\top}\\mathbf{x}_{\\mathbf{t}}^{\\prime}=w_{x_{i}}^{(1,l)}+w_{x_{i^{\\prime}}}^{(1,l)}+w_{x_{i^{\\prime\\prime}}}^{(1,l)}+w_{c_{\\gamma}}^{(1,l)}+\\frac{1}{2}w_{b_{\\gamma}}^{(1,l)}}\\\\ &{\\displaystyle=w_{x_{i}}^{(1,l)}+w_{x_{i^{\\prime}}}^{(1,l)}+w_{x_{i^{\\prime\\prime}}}^{(1,l)}}\\\\ &{\\displaystyle\\in\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},\\frac{3(l+1)}{6000N^{2}})\\subseteq\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},0.01)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "415 We have $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\bigg\\vert_{\\mathbf{t}}=-\\frac{1}{1000N}}\\end{array}$ and wxi $\\begin{array}{r}{w_{x_{i}}^{(1,l+1)}=w_{x_{i}}^{(1,l)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}=w_{x_{i}}^{(1,l)}+\\frac{1}{6000N^{2}}}\\end{array}$ 60010N2 . Consider the   \n416 following cases: ", "page_idx": 12}, {"type": "text", "text": "\u2022 If $\\mathtt{v a r}(i)\\in\\;T_{l}$ , then $\\mathrm{var}(i)\\!\\in\\;T_{l+1}$ and $\\begin{array}{r}{w_{x_{i}}^{(1,l)}~\\in~U(1,\\frac{l+1}{6000N^{2}})}\\end{array}$ . Therefore $w_{x_{i}}^{(1,l+1)}\\;\\in$ $\\begin{array}{r}{U(1,\\frac{l+2}{6000N^{2}})}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "\u2022 If $\\mathtt{v a r}(i)\\not\\in\\;T_{l}$ , then $\\mathrm{var}(i)\\notin\\;T_{l+1}$ and $w_{x_{i}}^{(1,l)}\\;\\in\\;U(-1,\\frac{l+1}{6000N^{2}})$ 1,60l0+01N2 ). Therefore $w_{x_{i}}^{(1,l+1)}\\in$ $\\begin{array}{r}{U(-1,\\frac{l+2}{6000N^{2}})}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "421 (3) Otherwise, $w_{x_{i}}$ will not be updated, and $w_{x_{i}}^{(1,l+1)}=w_{x_{i}}^{(1,l)}$ w(x1i,l). If var(i)\u2208Tl then var(i)\u2208Tl+1 and   \n422 $\\begin{array}{r}{w_{x_{i}}^{(1,l+1)}\\in U(1,\\frac{l+2}{6000N^{2}})}\\end{array}$ ; Otherwise $\\mathtt{v a r}(i)\\not\\in T_{l+1}$ and $\\begin{array}{r}{w_{x_{i}}^{(1,l+1)}\\in U(-1,\\frac{l+2}{6000N^{2}})}\\end{array}$   \n423 Hence if the lemma is true for $l<|T|$ , it is also true for $l+1$ . Therefore, the lemma is true for all   \n424 $1\\leq l\\leq|T|$ . \u53e3   \n425 Corollary A.1. Suppose $T\\subseteq T_{0}$ is the training data. $\\forall1\\leq i\\leq n,1\\leq l\\leq|T|$ , if var $(i)\\in T$ , then   \n426 $\\begin{array}{r}{w_{x_{i}}^{(1)}\\in U(1,\\frac{1}{6000N})}\\end{array}$ . Otherwise $\\begin{array}{r}{w_{x_{i}}^{(1)}\\in U(-1,\\frac{1}{6000N})}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "427 Proof. Note that $w_{x_{i}}^{(1)}=w_{x_{i}}^{(1,|T|)}$ and $N=2m+n+1$ . By Lemma A.2, if va $\\mathbf{r}(i)\\in T$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(1,|T|)}\\in U(1,\\frac{|T|+1}{6000N^{2}})\\subseteq U(1,\\frac{m+n+1}{6000N^{2}})\\subseteq U(1,\\frac{1}{6000N})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "428 If $\\mathtt{v a r}(i)\\not\\in T$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(1,|T|)}\\in U(-1,\\frac{|T|+1}{6000N^{2}})\\subseteq U(-1,\\frac{m+n+1}{6000N^{2}})\\subseteq U(-1,\\frac{1}{6000N})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "429 ", "page_idx": 13}, {"type": "text", "text": "430 Lemma A.3. Suppose $T\\subseteq T_{0}$ is the training data. $\\forall1\\leq\\gamma\\leq m$ , if $\\exists1\\leq i_{1},i_{2},i_{3}\\leq n$ such that   \n431 clause $(\\gamma,i_{1},i_{2},i_{3})\\in T$ , then $\\begin{array}{r}{w_{b_{\\gamma}}^{(1)}=0,w_{c_{\\gamma}}^{(1)}=\\frac{1}{2}+\\frac{1}{200N}}\\end{array}$ ; Otherwise, $w_{b_{\\gamma}}^{(1)}=-1,w_{c_{\\gamma}}^{(1)}=\\frac{1}{2}$ . ", "page_idx": 13}, {"type": "text", "text": "432 Proof. (1) If such $\\mathbf{t}_{\\gamma}=\\mathtt{c l a u s e}\\left(\\gamma,i_{1},i_{2},i_{3}\\right)$ exists in $T$ , by Lemma A.2 we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{x_{i_{1}}}^{(1,\\sigma_{t_{\\gamma}}^{1})}+w_{x_{i_{2}}}^{(1,\\sigma_{t_{\\gamma}}^{1})}+w_{x_{i_{3}}}^{(1,\\sigma_{t_{\\gamma}}^{1})}\\in\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},\\frac{3(\\sigma_{t_{\\gamma}}^{1}+1)}{6000N^{2}})\\subseteq\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},0.01)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "433 By Lemma A.1 we have wc\u03b3 $w_{c_{\\gamma}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}\\,=\\,w_{c_{\\gamma}}^{(0)}$ and wb $w_{b_{\\gamma}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}\\,=\\,w_{b_{\\gamma}}^{(0)}$ wb(\u03b30) because clause(\u03b3, \u00b7, \u00b7, \u00b7)\u0338\u2208   \n434 $T_{1,o_{\\mathbf{t}_{\\gamma}}-1}^{1}$ . Hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\mathbf{t}_{\\gamma}}(\\mathbf{w}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)})^{\\top}\\mathbf{x}_{\\mathbf{t}_{\\gamma}}^{\\prime}=w_{x_{i_{1}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{x_{i_{2}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{x_{i_{3}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{c_{\\gamma}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+\\frac{1}{2}w_{b_{\\gamma}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=w_{x_{i_{1}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{x_{i_{2}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{x_{i_{3}}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1})}+w_{c_{\\gamma}}^{(1,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\in\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},0.01)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "435 We have $\\begin{array}{r}{\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{c_{\\gamma}}}\\right|_{\\mathbf{t}_{\\gamma}}=-\\frac{1}{1000N}}\\end{array}$ 1000N , and ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{c_{\\gamma}}^{(1,o_{\\mathsf{t}_{\\gamma}}^{1})}=w_{c_{\\gamma}}^{(1,o_{\\mathsf{t}_{\\gamma}}^{1}-1)}-\\eta_{c_{\\gamma}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{c_{\\gamma}}}\\right|_{\\mathsf{t}_{\\gamma}}=\\frac{1}{2}+5\\times\\frac{1}{1000N}=\\frac{1}{2}+\\frac{1}{200N}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "436 Similarly, $\\begin{array}{r}{\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{b_{\\gamma}}}\\right|_{\\mathbf{t}_{\\gamma}}=-\\frac{1}{2000N}}\\end{array}$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\nw_{b_{\\gamma}}^{(1,o_{{\\tt t}_{\\gamma}}^{1})}=w_{b_{\\gamma}}^{(1,o_{{\\tt t}_{\\gamma}}^{1}-1)}-\\eta_{b_{\\gamma}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{c_{\\gamma}}}\\right|_{{\\tt t}_{\\gamma}}=-1-2000N\\times(-\\frac{1}{2000N})=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "437 Note also that clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{o_{\\mathbf{t}_{\\gamma}},|T|}^{1}$ , by Lemma A.1 we have ", "page_idx": 13}, {"type": "text", "text": "439 (2) If such $\\mathbf{t}_{\\gamma}=\\mathtt{c l a u s e}(\\gamma,i_{1},i_{2},i_{3})$ does not exist in $T$ , by Lemma A.1 we have $w_{c_{\\gamma}}^{(1)}=w_{c_{\\gamma}}^{(0)}=\\frac{1}{2}$   \n440 and wb(\u03b31) $w_{b_{\\gamma}}^{(1)}=w_{b_{\\gamma}}^{(0)}=-1$ . \u53e3   \n441 Lemma A.4. Suppose $T\\subseteq T_{0}$ and $C_{l}$ be the number of clause() in $T_{1,l}^{2}$ . $\\forall1\\leq i\\leq n,1\\leq l\\leq|T|$ ,   \n442 wxi $\\begin{array}{r}{w_{x_{i}}^{(2,l)}\\in U(1,\\frac{C_{l}+1/2}{6N})}\\end{array}$ if ${\\mathtt{v a r}}(i)\\!\\in\\!T$ ; Otherwise $\\begin{array}{r}{w_{x_{i}}^{(2,l)}\\in U(-1,\\frac{C_{l}+1/2}{6N})}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "443 Proof. Similar to the proof of A.2, we prove this lemma by induction. ", "page_idx": 13}, {"type": "text", "text": "444 Basic Case: Note that for all 1 \u2264i \u2264n, w(x1i) = U(\u00b11,60010N ), and for all 1 \u2264\u03b3 \u2264m, wc(1\u03b3)   \n445 $\\bigl\\{\\frac12,\\frac12+\\frac{1}{200N}\\bigr\\},w_{b_{\\gamma}}^{(1)}\\in\\{-1,0\\}$ . We denote $\\mathbf{t}=\\mathbf{t}^{(2,1)}$ to avoid cluttering. For any fixed $i$ : ", "page_idx": 13}, {"type": "text", "text": "446 (1) If $\\mathbf{t}=\\mathtt{v a r}(i),C_{1}=0$ . By Corollary A.1, $\\begin{array}{r}{w_{x_{i}}^{(1)}=U(1,\\frac{1}{6000N})}\\end{array}$ . We have ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{\\mathbf{t}}(\\mathbf{w}^{(1)})^{\\top}\\mathbf{x}_{\\mathbf{t}}^{\\prime}=5w_{x_{i}}^{(1)}\\in U(5,\\frac{1}{1200N})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "447 hence $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\Big|_{\\mathbf{t}}=0}\\end{array}$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,1)}=w_{x_{i}}^{(1)}\\in U(1,\\frac{1}{6N})=U(1,\\frac{C_{l}+1/2}{6N})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "448 (2) If $\\mathbf{t}=\\mathtt{c l a u s e}(\\gamma,i,i^{\\prime},i^{\\prime\\prime}),C_{1}=1$ . By Lemma A.3, we have $\\begin{array}{r}{w_{c_{\\gamma}}^{(1)}=\\frac{1}{2}+\\frac{1}{200N}}\\end{array}$ and $w_{b_{\\gamma}}^{(1)}=0$ .   \n449 Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\mathbf{t}}(\\mathbf{w}^{(1)})^{\\top}\\mathbf{x}_{\\mathbf{t}}^{\\prime}=w_{x_{i}}^{(1)}+w_{x_{i^{\\prime}}}^{(1)}+w_{x_{i^{\\prime\\prime}}}^{(1)}+w_{c_{\\gamma}}^{(1)}+\\frac{1}{2}w_{b_{\\gamma}}^{(1)}}\\\\ &{\\qquad\\qquad\\qquad=w_{x_{i}}^{(1)}+w_{x_{i^{\\prime}}}^{(1)}+w_{x_{i^{\\prime\\prime}}}^{(1)}+\\frac{1}{2}-\\frac{1}{200N}}\\\\ &{\\qquad\\qquad\\in\\qquad\\bigcup\\qquad U(x_{0},0.01)}\\\\ &{\\qquad\\qquad\\qquad x_{0}\\in\\{\\frac{1}{2}\\pm1,\\frac{1}{2}\\pm3\\}}\\\\ &{,-y x_{x_{i}}\\}=\\{-1,0\\},\\mathrm{~and~}\\eta_{x_{i}}\\ \\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\Big|_{\\mathbf{t}}\\in\\{-\\frac{1}{6N},0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "450 hence \u2202wxi ", "page_idx": 14}, {"type": "text", "text": "451 By Corollary A.1, if ${\\mathtt{v a r}}(i)\\!\\in\\!T$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,1)}=w_{x_{i}}^{(1)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathsf{t}}\\in U(1,\\frac{3/2}{6N})=U(1,\\frac{C_{l}+1/2}{6N})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "452 If $\\mathtt{v a r}(i)\\not\\in T$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,1)}=w_{x_{i}}^{(1)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}\\in U(-1,\\frac{3/2}{6N})=U(-1,\\frac{C_{l}+1/2}{6N})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "453 (3) Otherwise, $w_{x_{i}}$ will not be updated and $C_{1}\\leq1$ . Therefore if ${\\mathtt{v a r}}(i)\\!\\in\\!T$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,1)}=w_{x_{i}}^{(1)}\\in U(1,\\frac{3/2}{6N})\\subseteq U(1,\\frac{C_{l}+1/2}{6N})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "454 If $\\mathtt{v a r}(i)\\not\\in T$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,1)}=w_{x_{i}}^{(1)}\\in U(-1,\\frac{3/2}{6N})\\subseteq U(-1,\\frac{C_{l}+1/2}{6N})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "455 Hence this lemma is true for $l=1$ . ", "page_idx": 14}, {"type": "text", "text": "456 Induction Step: Suppose the lemma is true for $l<|T|$ . We prove that this lemma remains true for   \n457 $l+1$ . We denote $\\mathbf{t}=\\mathbf{t}^{(2,l+1)}$ to avoid cluttering. This makes sense since $l+1\\leq|T|$ and thus $\\mathbf{t}\\in T$   \n458 For any fixed $i$ : ", "page_idx": 14}, {"type": "text", "text": "459 (1) If $\\mathbf{t}=\\mathtt{v a r}(i)$ , $C_{l+1}=C_{l}$ . By Corollary A.1, $\\begin{array}{r}{w_{x_{i}}^{(2,l)}\\in U(1,\\frac{C_{l}+1/2}{6N})}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "460 We have $y_{\\mathbf{t}}(\\mathbf{w}^{(2,l)})^{\\top}\\mathbf{x_{t}^{\\prime}}\\;=\\;5w_{x_{i}}^{(2,l)}\\;\\in\\;U(5,1/6)$ and $\\begin{array}{r}{\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\bf t}\\;=\\;0.1}\\end{array}$ = 0.Hence w(x2i, $w_{x_{i}}^{(2,l+1)}\\;=\\;w_{x_{i}}^{(2,l)}\\;\\in$   \n461 $\\begin{array}{r}{U(1,\\frac{C_{l+1}+1/2}{6N})}\\end{array}$ .   \n462 (2) If $\\mathbf{t}=\\mathtt{c l a u s e}\\left(\\gamma,i,i^{\\prime},i^{\\prime\\prime}\\right),C_{l+1}=C_{l}+1$ . In this case, clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{1,l}^{2}$ and by Lemma   \n463 A.1 and Lemma A.3 we have $\\begin{array}{r}{w_{c_{\\gamma}}^{(2,l)}=w_{c_{\\gamma}}^{(1)}=\\frac{1}{2}+\\frac{1}{200N},w_{b_{\\gamma}}^{(2,l)}=w_{b_{\\gamma}}^{(1)}=0}\\end{array}$ . From the induction   \n464 hypothesis we have $\\begin{array}{r}{w_{x_{i}}^{(2,l)},w_{x_{i^{\\prime}}}^{(2,l)},w_{x_{i^{\\prime\\prime}}}^{(2,l)}\\in U(\\pm1,\\frac{C_{l}+1/2}{6N})}\\end{array}$ . Noting that ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{C_{l}+1/2}{6N}\\le\\frac{m+1/2}{6N}=\\frac{m+1/2}{\\left(n+2(m+1/2)\\right)}\\le\\frac{1}{12}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "465 we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\mathbf{t}}(\\mathbf{w}^{(2,l)})^{\\top}\\mathbf{x}_{\\mathbf{t}}^{\\prime}=w_{x_{i}}^{(2,l)}+w_{x_{i^{\\prime}}}^{(2,l)}+w_{x_{i^{\\prime\\prime}}}^{(2,l)}+w_{c_{\\gamma}}^{(2,l)}+\\frac{1}{2}w_{b_{\\gamma}}^{(2,l)}}\\\\ &{\\phantom{x x}=w_{x_{i}}^{(2,l)}+w_{x_{i^{\\prime}}}^{(2,l)}+w_{x_{i^{\\prime\\prime}}}^{(2,l)}+\\frac{1}{2}+\\frac{1}{200N}}\\\\ &{\\phantom{x x}\\in\\{\\displaystyle\\bigcup_{x_{0}\\in\\{\\frac{1}{2}\\}\\leq1,\\frac{1}{2}\\leq3\\}}U\\left(x_{0},\\frac{3(C_{l}+1/2)}{6N}+\\frac{1}{200N}\\right)}\\\\ &{\\phantom{x x}\\leq\\sum_{x_{0}\\in\\{\\frac{1}{2}\\}\\leq1,\\frac{1}{2}\\leq3\\}U(x_{0},0.26)}\\\\ &{\\phantom{x x}\\in\\{\\displaystyle\\bigcup_{x_{0}\\in\\{\\frac{1}{2}\\}\\leq1,\\frac{1}{2}\\leq3\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "466 And thus t \u2208{0, \u2212yxxi} = {\u22121, 0}, and \u03b7xi \u2202\u2202wLxi   t $\\begin{array}{r}{\\eta_{x_{i}}\\ \\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\bigg|_{\\mathbf{t}}\\in\\{-\\frac{1}{6N},0\\}}\\end{array}$ . \u2202wxi ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z p}\\operatorname{Corollary}\\mathrm{~A.1,if~}\\mathtt{v a r}(i)\\in T,\\,w_{x_{i}}^{(2,l+1)}=w_{x_{i}}^{(l)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}\\in U(1,\\frac{C_{l}+3/2}{6N})=U(1,\\frac{C_{l+1}+1/2}{6N});}\\\\ &{\\mathfrak{f}\\operatorname{var}(i)\\notin T,\\,w_{x_{i}}^{(2,l+1)}=w_{x_{i}}^{(l)}-\\eta_{x_{i}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{x_{i}}}\\right|_{\\mathbf{t}}\\in U(-1,\\frac{C_{l}+3/2}{6N})=U(-1,\\frac{C_{l+1}+1/2}{6N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "46 9 (3) Otherwise, $w_{x_{i}}$ will not be updated. We have $C_{l+1}\\leq C_{l}+1\\ w_{x_{i}}^{(2,l+1)}=w_{x_{i}}^{(2,l)}$ (x2,l). If var(i)\u2208T   \n470 then wxi $\\begin{array}{r}{w_{x_{i}}^{(2,l+1)}\\in U(1,\\frac{C_{l+1}+1/2}{6N})}\\end{array}$ , Cl+61N+1/2); If var(i)\u0338\u2208T then w(x2i, $\\begin{array}{r}{w_{x_{i}}^{(2,l+1)}\\in U(-1,\\frac{C_{l+1}+1/2}{6N})}\\end{array}$   \n471 Hence if the lemma is true for $l<|T|$ , it is also true for $l+1$ . Therefore, the lemma is true for all   \n472 $1\\leq l\\leq|T|$ . \u53e3   \n473 Corollary A.2. Suppose $T\\subseteq T_{0}$ is the training data. $\\forall1\\leq\\,i\\leq\\,n$ , if $\\mathtt{v a r}(i)\\in\\;T$ , then $w_{x_{i}}^{(2)}\\in$   \n474 $U(1,0.1)$ . Otherwise $w_{x_{i}}^{(2)}\\in U(-1,0.1)$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "475 Proof. Note that $w_{x_{i}}^{(2)}=w_{x_{i}}^{(2,|T|)}$ and $C_{|T|}\\leq m$ . By Lemma A.4, if va $\\mathsf{c}\\left(i\\right)\\in T$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(2,|T|)}\\in U(1,\\frac{C_{|T|}+1/2}{6N})\\subseteq U(1,\\frac{m+1/2}{6N})\\subseteq U(1,\\frac{1}{12})\\subseteq U(1,0.1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "476 If $\\mathtt{v a r}(i)\\not\\in T$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{x_{i}}^{(1,|T|)}\\in U(-1,\\frac{C_{|T|}+1/2}{6N})\\subseteq U(-1,\\frac{m+1/2}{6N})\\subseteq U(-1,\\frac{1}{12})\\subseteq U(-1,0.1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "477 ", "page_idx": 15}, {"type": "text", "text": "478 Lemma A.5. Suppose $T\\subseteq T_{0}$ is the training data. $\\forall1\\leq i\\leq m$ , if $\\exists1\\leq i_{1},i_{2},i_{3}\\leq n$ such that   \n479 clause $(i,i_{1},i_{2},i_{3})\\in T$ , then   \n480 1. $w_{b_{j}}^{(2)}=1000N$ ;   \n481 2. $\\begin{array}{r}{w_{c_{j}}^{(2)}=\\frac{11}{2}+\\frac{1}{200N}}\\end{array}$ if exactly one of $\\mathtt{v a r}(i_{1}),\\,\\mathtt{v a r}(i_{2}),\\,\\mathtt{v a r}(i_{3})$ is in $T$ . Otherwise $w_{c_{j}}^{(2)}=$   \n482 $\\textstyle{\\frac{1}{2}}+{\\frac{1}{200N}}$ ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "483 Otherwise, $w_{b_{i}}^{(2)}=-1,w_{c_{i}}^{(2)}=\\frac{1}{2}$ ", "page_idx": 15}, {"type": "text", "text": "484 Proof. (1) If such $\\mathbf{t}_{\\gamma}=\\mathtt{c l a u s e}\\left(\\gamma,i_{1},i_{2},i_{3}\\right)$ exists in $T$ , by Lemma A.4 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{x_{i_{1}}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})},w_{x_{i_{2}}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})},w_{x_{i_{3}}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}\\in U(\\pm1,\\frac{m+1/2}{6N})\\subseteq U(\\pm1,\\frac{1}{12N})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "485 By Lemma A.1 we have $\\begin{array}{r}{w_{c_{\\gamma}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}\\,=\\,w_{c_{\\gamma}}^{(1)}\\,=\\,\\frac{1}{2}\\,+\\,\\frac{1}{200N}}\\end{array}$ and $w_{b_{\\gamma}}^{(2,o_{\\dagger_{\\gamma}}^{1}-1)}\\,=\\,w_{b_{\\gamma}}^{(1)}\\,=\\,0$ because   \n486 clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{1,o_{\\mathbf{t}_{\\gamma}}-1}^{1}$ . Consider the following two cases: ", "page_idx": 15}, {"type": "text", "text": "487 (a) If exactly one of $\\mathtt{v a r}(i_{1}),\\mathtt{v a r}(i_{2}),\\mathtt{v a r}(i_{3})$ is in $T$ , by Corollary A.2 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{y_{\\mathbf{t}_{\\gamma}}(\\mathbf{w}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)})^{\\top}\\mathbf{x}_{\\mathbf{t}_{\\gamma}}^{\\prime}=w_{x_{i_{1}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{2}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{3}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{c_{\\gamma}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+\\frac{1}{2}w_{b_{\\gamma}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}}\\\\ &{\\qquad\\qquad\\qquad=w_{x_{i_{1}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{2}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{3}}}^{(2,\\sigma_{\\mathbf{t}_{\\gamma}}^{1}-1)}+\\frac{1}{2}+\\frac{1}{200N}}\\\\ &{\\qquad\\qquad\\in U(-\\frac{1}{2},\\frac{3}{12N}+\\frac{1}{200N})\\subseteq U(-\\frac{1}{2},0.26)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "488 Hence $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial w_{c_{\\gamma}}}\\Big|_{\\mathbf{t}_{\\gamma}}=-1}\\end{array}$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{c_{\\gamma}}^{(2,o_{\\mathsf{t}_{\\gamma}}^{1})}=w_{c_{\\gamma}}^{(2,o_{\\mathsf{t}_{\\gamma}}^{1}-1)}-\\eta_{c_{\\gamma}}\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{c_{\\gamma}}}\\right|_{\\mathsf{t}_{\\gamma}}=\\frac{1}{2}+\\frac{1}{200N}+5=\\frac{11}{2}+\\frac{1}{200N}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "489 Similarly, ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{b_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=w_{b_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1}-1)}-\\eta_{b_{\\gamma}}\\left.{\\frac{\\partial{\\mathcal{L}}}{\\partial w_{b_{\\gamma}}}}\\right|_{\\mathbf{t}_{\\gamma}}=1000N\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "490 Note also that clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{o_{\\mathbf{t}_{\\gamma}},|T|}^{1}$ , by Lemma A.1 we have $w_{c_{\\gamma}}^{(2)}=w_{c_{\\gamma}}^{(2,|T|)}=w_{c_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=$   \n491 $\\textstyle{\\frac{11}{2}}\\mathrm{~-~}{\\frac{1}{200N}}$ and wb $w_{b_{\\gamma}}^{(2)}=w_{b_{\\gamma}}^{(2,|T|)}=w_{b_{\\gamma}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1})}=1000N$ . ", "page_idx": 16}, {"type": "text", "text": "492 (b) Otherwise, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle y_{\\mathbf{t}_{\\gamma}}(\\mathbf{w}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)})^{\\top}\\mathbf{x}_{\\mathbf{t}_{\\gamma}}^{\\prime}=w_{x_{i_{1}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{2}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{3}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{c_{\\gamma}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+\\frac{1}{2}w_{b_{\\gamma}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}}\\\\ {\\displaystyle=w_{x_{i_{1}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{2}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+w_{x_{i_{3}}}^{(2,o_{\\mathbf{t}_{\\gamma}}^{1}-1)}+\\frac{1}{2}+\\frac{1}{200N}}&\\\\ {\\displaystyle\\in\\bigcup_{x_{0}\\in\\{-\\frac{7}{2},\\frac{1}{2},\\frac{5}{2}\\}}U(x_{0},\\frac{3}{12N}+\\frac{1}{200N})\\subseteq\\bigcup_{x_{0}\\in\\{-\\frac{7}{2},\\frac{1}{2},\\frac{5}{2}\\}}U(x_{0},0.26)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "493 Hence $\\begin{array}{r}{\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{c\\gamma}}\\right|_{\\mathbf{t}_{\\gamma}}=\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{b_{\\gamma}}}\\right|_{\\mathbf{t}_{\\gamma}}=0}\\end{array}$ = 0, so wc\u03b3 $\\begin{array}{r}{w_{c_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=w_{c_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1}-1)}=\\frac{1}{2}+\\frac{1}{200N},w_{b_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=w_{b_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1}-1)}=0.}\\end{array}$   \n494 Note also that clause $(\\gamma,\\cdot,\\cdot,\\cdot)\\not\\in T_{o_{\\mathbf{t}_{\\gamma}},|T|}^{1}.$ , by Lemma A.1 we have $w_{c_{\\gamma}}^{(2)}=w_{c_{\\gamma}}^{(2,|T|)}=w_{c_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=$   \n495 12 +2010N and wb $w_{b_{\\gamma}}^{(2)}=w_{b_{\\gamma}}^{(2,|T|)}=w_{b_{\\gamma}}^{(2,o_{\\mathfrak{t}_{\\gamma}}^{1})}=0.$ .   \n496 (2) If such $\\mathbf{t}_{\\gamma}=\\mathtt{c l a u s e}(\\gamma,i_{1},i_{2},i_{3})$ does not exist in $T$ , by Lemma A.1 and Lemma A.3 we have   \n497 $w_{c_{\\gamma}}^{(2)}=w_{c_{\\gamma}}^{(1)}=\\frac{1}{2}$ and $w_{b_{\\gamma}}^{(2)}=w_{b_{\\gamma}}^{(1)}=-1$ . \u53e3   \n498 Moreover, w reaches its fixpoint at the end of the second epoch and will no longer be updated.   \n499 Lemma A.6. $\\mathbf{w}^{(2)}=\\mathbf{w}^{(3)}$ .   \n500 Proof. Suppose $\\mathbf{w}^{(2)}\\;\\neq\\;\\mathbf{w}^{(3)}$ , then there exists $1\\,\\leq\\,i\\,\\leq\\,N$ such that $w_{i}^{(2)}\\neq w_{i}^{(3)}$ , and there   \n501 are some training sample $\\mathbf{t}$ in the training data such that $\\left.\\frac{\\partial\\mathcal{L}}{\\partial w_{i}^{(2)}}\\right|_{\\mathbf{t}}\\neq\\mathrm{~0~}$ . Let $\\mathbf{t}\\ =\\ (\\mathbf{x_{t}},y_{\\mathbf{t}})$ and   \n502 $\\begin{array}{r}{\\mathbb{I}=U(-5,0.01)\\cup U(-\\frac{1}{2},0.26)\\cup\\left(\\bigcup_{x_{0}\\in\\{\\pm1,\\pm3\\}}U(x_{0},0.01)\\right)}\\end{array}$ . By (2) we have $y_{\\mathbf{t}}(\\mathbf{w}^{(2)})^{\\top}\\mathbf{x_{t}}^{\\prime}\\in\\mathbb{I}.$ .   \n503 At least one of the following is true: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "1. $\\exists1\\;\\leq\\;i\\;\\leq\\;n,\\mathbf{t}\\;=\\;\\operatorname{var}(i)$ . According to lemma A.2, $y_{\\mathbf{t}}(\\mathbf{w}^{(2)})^{\\top}\\mathbf{x_{t}}^{\\prime}~=~y w_{x_{i}}^{(2)}x_{i}~\\in$ $U(5,0.5)\\subseteq\\mathbb{R}\\setminus\\mathbb{I}$ , contradicting to $y_{\\mathbf{t}}(\\mathbf{w}^{(2)})^{\\top}\\mathbf{x_{t}}^{\\prime}\\in\\mathbb{I}$ . ", "page_idx": 16}, {"type": "text", "text": "506 2. $\\exists1\\leq i\\leq m$ and $1\\,\\leq\\,i_{1},i_{2},i_{3}\\,\\leq\\,n$ , such that $\\mathbf{t}\\,=\\,{\\mathsf{c}}{\\mathsf{l a u s e}}(i,i_{1},i_{2},i_{3})$ . According to   \n507 lemma A.5, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle y_{\\mathbf{t}}(\\mathbf{w}^{(2)})^{\\top}\\mathbf{x_{t}}^{\\prime}=w_{b_{i}}^{(2)}+w_{c_{i}}^{(2)}+w_{x_{i_{1}}}^{(2)}+w_{x_{i_{2}}}^{(2)}+w_{x_{i_{3}}}^{(2)}}\\\\ &{\\phantom{\\displaystyle\\geq}1000N+\\frac{1}{2}+\\frac{1}{200N}+3\\times(-1-0.1)}\\\\ &{\\phantom{\\displaystyle\\geq1000-3.3\\geq996}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "508 ", "page_idx": 16}, {"type": "text", "text": "We have $y_{\\mathbf{t}}(w^{(2)})^{\\top}\\mathbf{x_{t}}^{\\prime}\\notin\\mathbb{I}$ , another contradiction. ", "page_idx": 16}, {"type": "text", "text": "509 Therefore $\\mathbf{w}^{(2)}\\,=\\,\\mathbf{w}^{(3)}$ , w reaches its fixpoint at the end of the second epoch. In other words,   \n510 w\u2217= w(2). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "511 We are now ready to give a rigorous proof of theorem 3.1. ", "page_idx": 16}, {"type": "text", "text": "512 Proof of theorem 3.1. It only suffices to prove the correctness of the reduction in section 3. ", "page_idx": 17}, {"type": "text", "text": "513 If. Suppose $\\varphi\\in$ MONOTONE 1-IN-3 SAT, then there is a truth assignment $\\nu(\\cdot)$ that assigns exactly   \n514 one variable in each clause of $\\varphi$ is true. Let $\\Delta=\\{{\\sf v a r}(i)|\\nu(x_{i})={\\sf F A L S E}\\}$ . Let $\\mathbf{w}^{\\prime}$ be the parameter   \n515 of $\\mathsf{S G D}_{\\Lambda}(T_{0}\\setminus\\Delta)$ . By Lemma A.5, $\\begin{array}{r}{(w^{\\prime})_{c_{\\gamma}}=\\frac{11}{2}\\stackrel{\\cdot}{+}\\frac{1}{200N}}\\end{array}$ for all $1\\leq\\gamma\\leq m$ , hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\mathbf{w}^{\\prime}\\right)^{\\top}\\mathbf{x}_{\\mathrm{test}}=\\sum_{\\gamma=1}^{m}w_{c_{\\gamma}}^{\\prime}\\geq\\frac{11m}{2}+\\frac{-11m+5}{2}=\\frac{5}{2}>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "516 and $\\lambda_{\\mathbf{w}^{\\prime}}(\\mathbf{x}_{\\mathrm{test}})=1$ , thus $\\mathrm{SGD}_{\\Lambda}(T_{0})$ is thus debuggable. ", "page_idx": 17}, {"type": "text", "text": "517 Only if. Suppose $\\mathrm{SGD}_{\\Lambda}(T_{0})$ is debuggable, there will be a $\\Delta$ such that $\\mathtt{S G D}_{\\Lambda}(T_{0},\\mathbf{x}_{\\mathrm{test}})=y_{\\mathrm{test}}$ . We   \n518 denote $\\mathbf{w}^{\\prime}$ as the parameter trained by SGD on $T_{0}\\setminus\\Delta$ . We have $\\lambda_{\\mathbf{w}^{\\prime}}(\\mathbf{x}_{\\mathrm{test}})=1$ and $(\\mathbf{w}^{\\prime})^{\\top}\\mathbf{x}_{\\mathrm{test}}\\geq0$ .   \n519 By Lemma A.5, $\\begin{array}{r}{\\dot{w}_{c_{\\gamma}}^{\\prime}=\\{\\frac{1}{2}+\\frac{1}{200N},\\frac{\\dot{1}1}{2}+\\frac{1}{200N}\\}}\\end{array}$ . Suppose $\\begin{array}{r}{w_{c^{*}}=\\frac{1}{2}+\\frac{1}{200N}}\\end{array}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf w^{\\prime})^{\\top}\\mathbf x_{\\mathrm{test}}=w_{c^{*}}+\\displaystyle\\sum_{c_{\\gamma}\\neq c^{*}}w_{c_{\\gamma}}}\\\\ &{\\phantom{(\\mathbf w^{\\prime})^{\\top}\\mathbf x_{\\mathrm{test}}}\\leq\\displaystyle\\frac{11}{2}(m-1)+\\frac1{2}+\\frac{m}{200N}-\\frac{11m}{2}+\\frac{5}{2}}\\\\ &{\\phantom{(\\mathbf w^{\\prime})^{\\top}\\mathbf x_{\\mathrm{test}}}=-\\frac{5}{2}+\\frac{m}{200N}}\\\\ &{\\phantom{(\\mathbf w^{\\prime})^{\\top}\\mathbf x_{\\mathrm{test}}}\\leq-\\frac{5}{2}+\\frac{1}{200}=-2.495<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "520 leading to a contradiction. ", "page_idx": 17}, {"type": "text", "text": "521 As a consequence, $\\begin{array}{r}{w_{c_{\\gamma}}^{\\prime}~=~\\frac{11}{2}\\,+\\,\\frac{1}{200N}}\\end{array}$ for all $1~\\leq~\\gamma~\\leq~m$ . By Lemma A.5, exactly one of   \n522 $\\mathtt{v a r}(i_{1}),\\mathtt{v a r}(i_{2}),\\mathtt{v a r}(i_{3})$ is in $T_{0}\\setminus\\Delta$ for each $c_{\\gamma}=(x_{i_{1}}\\vee x_{i_{2}}\\vee x_{i_{3}})$ . Consider a truth assignment $\\nu$   \n523 that maps every $x_{i}$ to FALSE where $\\mathtt{v a r}(i)\\!\\in\\!\\Delta$ , and maps the rest to TRUE. Then $\\nu$ assigns exactly   \n524 one variable true in each $c_{\\gamma}=(x_{i_{1}}\\vee x_{i_{2}}\\vee x_{i_{3}})$ if and only if exactly one of $\\mathtt{v a r}(i_{1}),\\mathtt{v a r}(i_{2}),\\mathtt{v a r}(i_{3})$   \n525 is in $T_{0}\\setminus\\Delta$ . Hence $\\nu$ is a truth assignment that assigns true to exactly one variable in each clause of   \n526 $\\varphi$ , and thus $\\varphi$ is a yes-instance of MONOTONE 1-IN-3 SAT. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "527 B Detailed Proofs for Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "528 B.1 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "529 Proof. We build a reduction from the SUBSET SUM problem with a fixed size, which is NP-hard as a   \n530 particular case of the class of knapsack problems [26]. Formally, it is defined as: ", "page_idx": 17}, {"type": "text", "text": "531 ", "page_idx": 17}, {"type": "table", "img_path": "unMRtFfNhF/tmp/31f0021bf6a3f8060e923bed9ec427f7d385948b1c7e5e758beb60f1986e0a5e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "532 The ordered training data $T$ is constructed as ", "page_idx": 17}, {"type": "equation", "text": "$$\nT=\\{(x_{1},y_{1}),(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\}\\cup\\{(x_{a},y_{a})\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "533 where xiyi = 32 +3 aai\u2208S a for all 1 \u2264i \u2264n and xaya = 1 +6 a1\u2208S a. Let $\\eta=1,\\alpha=1,\\beta=-1$ ,   \n534 w(0) = \u22121 \u221223k \u22123 at\u2208S a and let the test instance $\\left\\langle x_{\\mathrm{test}},y_{\\mathrm{test}}\\right\\rangle$ satisfy $x_{\\mathrm{{test}}}y_{\\mathrm{{test}}}=1$ . It now suffices   \n535 to prove that $\\exists S^{\\prime}\\subseteq S$ such that $|S^{\\prime}|\\;=\\;k$ and $\\textstyle\\sum_{a\\in S^{\\prime}}a\\,=\\,t$ if and only if $\\exists T^{\\prime}\\subseteq T$ such that   \n536 $w:w^{(0)}\\xrightarrow{T^{\\prime}}w$ satisfies $y_{\\mathrm{test}}w x_{\\mathrm{test}}>0$ .   \n537 If: Suppose $\\exists S^{\\prime}\\subseteq S$ such that $|S^{\\prime}|=k$ and $\\textstyle\\sum_{a\\in S}a=t$ . Let $T^{*}=\\{(x_{i},y_{i})|a_{i}\\in S^{\\prime}\\}$ , we prove   \n538 that $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}>0$ for $w^{*}$ satisfying $w^{(0)}$ $\\xrightarrow{T^{\\prime}=T^{*}\\cup\\{(x_{a},y_{a})\\}}w^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{w^{(0)}+\\displaystyle\\sum_{a_{i}\\in S^{\\prime}}x_{i}y_{i}=-1-\\frac{2}{3}k-\\frac{t}{3\\sum_{a\\in S}a}+\\displaystyle\\sum_{a_{i}\\in S^{\\prime}}\\left(\\frac{2}{3}+\\frac{a_{i}}{3\\sum_{a\\in S}a}\\right)}}\\\\ {{=-1-\\frac{2}{3}k-\\displaystyle\\frac{t}{3\\sum_{a\\in S}a}+\\displaystyle\\sum_{a_{i}\\in S^{\\prime}}\\frac{2}{3}+\\frac{\\sum_{a\\in S^{\\prime}}a}{3\\sum_{a\\in S}a}=-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and \u22001 \u2264i \u2264n, xiyi > 23, for each 1 \u2264i < n, suppose w(0)\u2212T \u2212\u2212\u2229\u2212{(\u2212x\u2212j,\u2212yj\u2212)\u2212|1\u2212\u2264\u2212j\u2212\u2264\u2212i}\u2192 , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{i}x_{i+1}y_{i+1}<\\left(w^{(0)}+\\sum_{a_{j}\\in S^{\\prime}}x_{j}y_{j}-\\frac{2}{3}\\right)\\cdot\\frac{2}{3}<-\\frac{10}{9}<\\beta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "539 That is, each training sample in $T^{*}$ is activated. Then for $w^{(0)}\\xrightarrow{T^{*}}w_{a}$ , we have $w_{a}=-1$ . Then,   \n540 since $\\begin{array}{r}{y_{a}w_{a}x_{a}=-(1+\\frac{1}{6\\sum_{a\\in S}a})<\\beta}\\end{array}$ and wa $\\boldsymbol{w_{a}}\\xrightarrow[]{(x_{a},y_{a})}\\boldsymbol{w}^{*}$ \u2192w\u2217we have w\u2217= wa + xaya =6 a1\u2208S a.   \n541 Therefore, ytestw\u2217xtest = 6 a1\u2208S a .   \n542 Only if: For each $T^{\\prime}\\subseteq T$ , let $T^{*}\\,=\\,T^{\\prime}\\setminus\\left\\{(x_{a},y_{a})\\right\\}$ and $c(T^{*})$ be the set of training samples in   \n543 $T^{*}$ that are activated. If $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}\\;\\geq\\;0$ for $w^{*}$ satisfying $w^{(0)}~\\xrightarrow{T^{\\prime}}w^{*}$ , we prove that the set   \n544 $S^{\\prime}=\\{a_{i}|(x_{i},y_{i})\\in c(T^{*})\\}$ satisfies $|S^{\\prime}|=k$ and $\\textstyle\\sum_{a\\in S^{\\prime}}a=t$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "545 We first show that $y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}\\,<\\,0$ for $w^{(0)}\\xrightarrow{c(T^{*})}w_{a}$ . Otherwise, suppose $y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}\\,\\geq\\,0$ we 546 have $w_{a}~\\ge~0$ . Let $(x,y)$ be the last training sample of $c(T^{\\prime})$ , since ${\\frac{2}{3}}\\ <\\ x y\\ \\leq\\ 1$ , we have 547 $w^{\\prime}\\geq w_{a}-x y\\geq-1$ for $w^{\\prime}\\xrightarrow{(x,y)}w_{a}$ . Thus $y w^{\\prime}x\\geq\\beta$ , which contradicts to the definition of $c(T^{*})$ We next show that $\\left|S^{\\prime}\\right|=k$ . Suppose $|S^{\\prime}|\\le k-1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{a}=w^{(0)}+\\displaystyle\\sum_{(x_{i},y_{i})\\in c(T^{*})}x_{i}y_{i}=-1-\\frac{2}{3}k-\\frac{t}{3\\sum_{a\\in S}a}+\\displaystyle\\sum_{a_{i}\\in S^{\\prime}}\\frac{2}{3}+\\frac{\\sum_{a\\in S^{\\prime}}a}{3\\sum_{a\\in S}a}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad<-1-\\frac{2}{3}k+\\frac{2}{3}(k-1)+\\frac{1}{3}=-\\frac{4}{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "548 Thus w\u2217\u2264wa + xaya < \u221234 + (1 +6 a1\u2208S a) and then $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}<0$ , which contradicts to   \n549 the fact that $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}\\geq0$ . Therefore $|S^{\\prime}|\\geq k$ . ", "page_idx": 18}, {"type": "text", "text": "Suppose $|S^{\\prime}|\\geq k+1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{a}=w^{(0)}+\\sum_{(x_{i},y_{i})\\in c(T^{*})}x_{i}y_{i}\\geq-1-\\frac{2}{3}k-\\frac{1}{3}+\\frac{2}{3}(k+1)=-\\frac{2}{3}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "550 Then yawaxa \u2265(\u221232) \u00b7 (1 +6 a1\u2208S , that is, $\\left(x_{a},y_{a}\\right)$ is not activated and $w^{*}=w_{a}$   \n551 Then since $y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}<0$ , we have $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}=y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}<0$ , which contradicts to the fact that   \n552 $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}\\geq0$ . Therefore $\\left|S^{\\prime}\\right|=k$ . ", "page_idx": 18}, {"type": "text", "text": "It remains to prove that $\\textstyle\\sum_{a\\in S^{\\prime}}a=t$ . Otherwise, suppose $\\textstyle\\sum_{a\\in S^{\\prime}}a\\leq t-1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{w_{a}=w^{(0)}+\\displaystyle\\sum_{(x_{i},y_{i})\\in c(T^{*})}x_{i}y_{i}\\leq-1-\\frac{2}{3}k-\\displaystyle\\frac{t}{3\\sum_{a\\in S}a}+\\frac{2}{3}k+\\displaystyle\\frac{t-1}{3\\sum_{a\\in S}a}}}\\\\ {{=-1-\\displaystyle\\frac{1}{3\\sum_{a\\in S}a}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "553 Thus ytestw\u2217xtest \u2264 ytest(wa + xaya)xtest \u2264 \u22126 a1\u2208S a , which contradicts to the fact that   \n554 $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}\\geq0$ . Therefore $\\textstyle\\sum_{a\\in S^{\\prime}}a\\geq t$ . ", "page_idx": 18}, {"type": "text", "text": "Suppose $\\textstyle\\sum_{a\\in S^{\\prime}}a\\geq t+1$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{a}=w^{(0)}+\\sum_{\\substack{(x_{i},y_{i})\\in c(T^{*})}}x_{i}y_{i}\\geq-1-\\frac{2}{3}k-\\frac{t}{3\\sum_{a\\in S}a}+\\frac{2}{3}k+\\frac{t+1}{3\\sum_{a\\in S}a}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{y_{a}w_{a}x_{a}\\geq(-1+\\frac{1}{3\\sum_{a\\in S}a})\\cdot(1+\\frac{1}{6\\sum_{a\\in S}a})}}\\\\ {{\\qquad\\geq-1+\\frac{1}{6\\sum_{a\\in S}a}+\\frac{1}{18(\\sum_{a\\in S}a)^{2}}\\geq\\beta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "555 That is, $\\left(x_{a},y_{a}\\right)$ is not activated and $w^{*}=w_{a}$ . Then since $y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}<0$ , we have $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}=$   \n556 $y_{\\mathrm{test}}w_{a}x_{\\mathrm{test}}<0$ , which contradicts to the fact that $y_{\\mathrm{test}}w^{*}x_{\\mathrm{test}}\\geq0$ . Therefore $\\textstyle\\sum_{a\\in S^{\\prime}}a=t$ . ", "page_idx": 19}, {"type": "text", "text": "557 B.2 Proof of Theorem 4.3 for $\\beta<-1$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "558 Proof. To avoid cluttering, we still assume $\\eta=1$ and $\\alpha\\,=\\,1$ . The proof can be generalized by   \n559 appropriately re-scaling the constructed vectors.   \n560 Let $M\\,=\\,-\\beta(n+2)+9\\beta n m^{2}(n+1)+3$ . Suppose $n\\;=\\;|S|\\;>\\;1$ , $m\\;=\\;\\operatorname*{max}_{a\\in S}\\{a\\}$ and   \n561 $S=\\{a_{1},a_{2},\\dots,a_{n}\\}$ . We further assume $n>1$ . Let the ordered set of training samples be ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\left\\{(\\mathbf{x}_{1},y_{1}),(\\mathbf{x}_{2},y_{2}),\\ldots,(\\mathbf{x}_{n},y_{n})\\right\\}\\cup\\left\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "562 where $\\begin{array}{r}{\\mathbf{x}_{i}y_{i}\\;=\\;(\\frac{1}{n+1},-3\\beta a_{i})}\\end{array}$ for all $\\begin{array}{r}{1\\;\\leq\\;i\\;\\leq\\;n,\\mathbf{x}_{c}y_{c}\\;=\\;(M+\\frac{3}{2}\\beta\\,-\\,1,\\beta(3t\\,-\\,\\frac{1}{2})),\\mathbf{x}_{b}y_{b}\\;=}\\end{array}$   \n563 $\\begin{array}{r}{(1,-1),{\\bf x}_{a}y_{a}\\;=\\;(-\\frac{3}{2}\\beta,-\\frac{3}{2}\\beta)}\\end{array}$ . Let $\\mathbf{w}^{(0)}\\,=\\,(-M,0)$ . Let the test instance $\\left(\\mathbf{x}_{\\mathrm{test}},y_{\\mathrm{test}}\\right)$ satisfy   \n564 $\\mathbf{x}_{\\mathrm{test}}y_{\\mathrm{test}}=(1,0)$ . ", "page_idx": 19}, {"type": "text", "text": "5 65 For each 1 \u2264i < n, suppose w(0)\u2212T \u2212\u2229\u2212{(\u2212x\u2212i,\u2212y\u2212i)\u2212|1\u2212\u2264\u2212j\u2212\u2264\u2212i}\u2192 , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{i+1}\\mathbf{w}_{i}^{\\top}\\mathbf{x}_{i+1}\\leq-M\\cdot\\displaystyle\\frac{1}{n+1}+\\displaystyle\\frac{i}{(n+1)^{2}}+9\\beta^{2}a_{i+1}\\sum_{j=1}^{i}a_{j}}\\\\ &{\\qquad\\qquad\\leq-M\\cdot\\displaystyle\\frac{1}{n+1}+\\displaystyle\\frac{n}{(n+1)^{2}}+9\\beta^{2}n m^{2}<\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "566 This means all the $(\\mathbf{x}_{i},y_{i})\\in T\\setminus\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\}$ can be activated and thus the resulting   \n567 parameter trained by $T\\setminus\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}=\\mathbf{w}^{(0)}+\\sum_{i=1}^{n}\\mathbf{x}_{i}y_{i}=\\left(-M+\\frac{|T^{*}|}{n+1},-3\\beta\\sum_{i=1}^{n}a_{i}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "568 It now suffices to prove that for all $S^{\\prime}\\,\\subseteq\\,S,\\ \\sum_{a\\in S^{\\prime}}a\\,=\\,t$ if and only if $\\exists T^{\\prime}\\subseteq T$ such that   \n569 $\\mathbf{w}:\\mathbf{w}^{(0)}\\xrightarrow{T^{\\prime}}\\mathbf{w}$ such that $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$ .   \n570 If: Suppose $\\exists S^{\\prime}\\subseteq S$ such that $\\textstyle\\sum_{a\\in S}a=t$ , we prove that $\\exists T^{\\prime}\\subseteq T$ such that $y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}>0$   \n571 for $\\mathbf{w}^{*}$ satisfying $\\mathbf{w}^{(0)}\\xrightarrow{T^{*}}\\mathbf{w}^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "572 Let $T^{*}=\\{(\\mathbf{x}_{i},y_{i})|a_{i}\\in S^{\\prime}\\}$ , $T^{\\prime}=T^{*}\\cup\\left\\{(\\mathbf{x}_{c},y_{c}),(\\mathbf{x}_{b},y_{b}),(\\mathbf{x}_{a},y_{a})\\right\\}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}=(-M+\\frac{|T^{*}|}{n+1},-3\\beta\\sum_{a_{i}\\in S^{\\prime}}a_{i})=(-M+\\frac{|T^{*}|}{n+1},-3\\beta t)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "573 And $\\begin{array}{r}{y_{c}\\mathbf w_{c}^{\\top}\\mathbf x_{c}=(-M+\\frac{|T^{*}|}{n+1})(M+\\frac32\\beta-1)-3t\\beta^{2}(3t-\\frac12)<\\beta,}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}\\xrightarrow[]{(\\mathbf{x}_{c},y_{c})}\\mathbf{w}_{b}=\\mathbf{w}_{c}+\\mathbf{x}_{c}y_{c}=(\\frac{|T^{*}|}{n+1}+\\frac{3}{2}\\beta-1,-\\frac{1}{2}\\beta)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "574 Note that $\\beta<-1$ , we have $\\begin{array}{r}{y_{b}\\mathbf{w}_{b}^{\\top}\\mathbf{x}_{b}=\\frac{|T^{*}|}{n+1}+2\\beta<(\\beta+\\frac{|T^{*}|}{n+1})+\\beta<\\beta}\\end{array}$ , and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{w}_{b}\\xrightarrow[]{(\\mathbf{x}_{b},y_{b})}\\mathbf{w}_{a}=\\mathbf{w}_{b}+\\mathbf{x}_{a}y_{a}=(\\frac{|T^{*}|}{n+1}+\\frac{3}{2}\\beta,-\\frac{1}{2}\\beta-1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "575 Note also that $\\begin{array}{r}{y_{a}\\mathbf{w}_{a}^{\\top}\\mathbf{x}_{a}=\\frac{3}{2}(-\\beta)(\\frac{|T^{*}|}{n+1}-1+\\beta)<\\beta}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{w}_{a}\\xrightarrow[]{(\\mathbf{x}_{a},y_{a})}\\mathbf{w}^{*}=\\mathbf{w}_{a}+\\mathbf{x}_{a}y_{a}=(\\frac{|T^{*}|}{n+1},-2\\beta-1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "576 Therefore, $\\begin{array}{r}{y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}=\\frac{|T^{*}|}{n+1}\\geq0}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "577 Only if: For each $T^{\\prime}\\subseteq T$ , let $T^{*}\\,=\\,T^{\\prime}\\setminus\\{({\\bf x}_{c},y_{c}),({\\bf x}_{b},y_{b}),({\\bf x}_{a},y_{a})\\}$ , if $y_{\\mathrm{test}}(\\mathbf{w}^{*})^{\\top}\\mathbf{x}_{\\mathrm{test}}$ for $\\mathbf{w}^{*}$   \n578 satisfying $\\textbf{w}^{(0)}\\xrightarrow{T^{\\prime}}\\textbf{w}^{*}$ , we prove that $\\exists S^{\\prime}\\subseteq S$ such that $\\textstyle\\sum_{a\\in S^{\\prime}}a=t$ . We first show that for   \n579 each $T^{\\prime}\\subseteq T$ , if $\\mathbf{w}(\\mathbf{w}^{(0)}\\;\\;\\xrightarrow{T^{\\prime}}\\;\\mathbf{w})$ satisfying $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}\\geq0$ , we have $\\forall k\\,\\in\\,\\{a,b,c\\},(\\mathbf{x}_{k},y_{k})\\,\\in$   \n580 $T^{\\prime},y_{k}\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{k}<\\beta$ , where $\\mathbf{w}^{(0)}\\;\\xrightarrow{T^{*}}\\mathbf{w}_{c}\\xrightarrow{(\\mathbf{x}_{c},y_{c})}\\mathbf{w}_{b}\\xrightarrow[]{}\\mathbf{w}_{a}$ . Otherwise, suppose $\\exists k\\in\\{a,b,c\\}$   \n581 such that $(\\mathbf{x}_{k},y_{k})\\not\\in T^{\\prime}$ or $y_{k}\\mathbf{w}_{k}^{\\top}\\mathbf{x}_{k}\\geq\\beta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}\\leq-M+\\frac{|T^{*}|}{n+1}+M+\\frac{3}{2}\\beta-1+1-\\frac{3}{2}\\beta-\\operatorname*{min}\\left\\{1,M+\\frac{3}{2}\\beta-1,-\\frac{3}{2}\\beta\\right\\}}\\\\ {\\displaystyle=\\frac{|T^{*}|}{n+1}-1<0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "582 which contradicts to the fact that $y_{\\mathrm{test}}\\mathbf{w}^{\\top}\\mathbf{x}_{\\mathrm{test}}\\geq0$ . ", "page_idx": 20}, {"type": "text", "text": "583 Let $S^{\\prime}=\\{a_{i}|(\\mathbf{x}_{i},y_{i})\\in T^{*}\\}$ and $\\textstyle t^{\\prime}=\\sum_{a\\in S^{\\prime}}a_{i}$ , it suffices to prove $t^{\\prime}=t$ . Notice that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbf{w}^{(0)}\\xrightarrow{T^{*}}\\mathbf{w}_{c}=(-M+\\frac{\\vert T^{*}\\vert}{n+1},-3\\beta\\sum_{a_{i}\\in S^{\\prime}}a_{i})}\\\\ {=(-M+\\frac{\\vert T^{*}\\vert}{n+1},-3\\beta t^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "584 Hence $\\begin{array}{r}{y_{c}\\mathbf{w}_{c}^{\\top}\\mathbf{x}_{c}=(-M+\\frac{|T^{*}|}{n+1})(M+\\frac{3}{2}\\beta-1)-3t^{\\prime}\\beta^{2}(3t-\\frac{1}{2})<\\beta,}\\end{array}$ , thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{w}_{c}\\xrightarrow[]{}\\mathbf{w}_{b}=\\mathbf{w}_{c}+\\mathbf{x}_{c}y_{c}=(\\frac{|T^{*}|}{n+1}+\\frac32\\beta-1,-3\\beta(t^{\\prime}-t)-\\frac12\\beta)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "585 (1) If $t^{\\prime}\\le t-1$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{b}\\mathbf{w}_{b}^{\\top}\\mathbf{x}_{b}=\\frac{\\left|T^{*}\\right|}{n+1}-1+2\\beta+3\\beta(t^{\\prime}-t)}\\\\ &{\\qquad\\qquad\\geq\\frac{\\left|T^{*}\\right|}{n+1}-(1+\\beta)>0>\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "586 a contradiction. Hence $\\begin{array}{r}{\\mathbf{w}_{a}=\\mathbf{w}_{b}\\xrightarrow{(\\mathbf{x}_{b},y_{b})}\\mathbf{w}_{a}=(\\frac{|T^{*}|}{n+1}+\\frac{3}{2}\\beta,-3\\beta(t^{\\prime}-t)-\\frac{1}{2}\\beta-1).}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "587 (2) If $t^{\\prime}\\geq t+1$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle y_{a}{\\bf w}_{a}^{\\top}{\\bf x}_{a}=-\\frac{3\\beta}{2}\\left(\\frac{|T^{*}|}{n+1}-1+\\beta-3\\beta(t^{\\prime}-t)\\right)}}\\\\ {~~}\\\\ {{\\displaystyle\\geq-\\frac{3\\beta}{2}\\left(\\frac{|T^{*}|}{n+1}-1-2\\beta\\right)}}\\\\ {~~}\\\\ {{\\displaystyle>-\\frac{3\\beta}{2}\\left(\\frac{|T^{*}|}{n+1}+1\\right)>0>\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "588 another contradiction. Therefore $t^{\\prime}=t$ , and this completes the proof. ", "page_idx": 20}, {"type": "text", "text": "589 ", "page_idx": 20}, {"type": "text", "text": "590 C Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "591 It is important to emphasize that the complexity results in section 4 requires the training order to   \n592 be adversarially chosen. The complexity of DEBUGGABLE for randomly chosen training order is   \n593 unclear and needs to be figured out in the future research. ", "page_idx": 20}, {"type": "text", "text": "594 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "595 1. Claims   \n596 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n597 paper\u2019s contributions and scope?   \n598 Answer: [Yes]   \n599 Justification: The main results are discussed in section 3 and section 4.   \n600 Guidelines:   \n601 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n602 made in the paper.   \n603 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n604 contributions made in the paper and important assumptions and limitations. A No or   \n605 NA answer to this question will not be perceived well by the reviewers.   \n606 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n607 much the results can be expected to generalize to other settings.   \n608 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n609 are not attained by the paper.   \n610 2. Limitations   \n611 Question: Does the paper discuss the limitations of the work performed by the authors?   \n612 Answer: [Yes]   \n613 Justification: See section C in the appendix.   \n614 Guidelines:   \n615 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n616 the paper has limitations, but those are not discussed in the paper.   \n617 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n618 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n619 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n620 model well-specification, asymptotic approximations only holding locally). The authors   \n621 should reflect on how these assumptions might be violated in practice and what the   \n622 implications would be.   \n623 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n624 only tested on a few datasets or with a few runs. In general, empirical results often   \n625 depend on implicit assumptions, which should be articulated.   \n626 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n627 For example, a facial recognition algorithm may perform poorly when image resolution   \n628 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n629 used reliably to provide closed captions for online lectures because it fails to handle   \n630 technical jargon.   \n631 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n632 and how they scale with dataset size.   \n633 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n634 address problems of privacy and fairness.   \n635 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n636 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n637 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n638 judgment and recognize that individual actions in favor of transparency play an impor  \n639 tant role in developing norms that preserve the integrity of the community. Reviewers   \n640 will be specifically instructed to not penalize honesty concerning limitations.   \n641 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 21}, {"type": "text", "text": "642 43 a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "660 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n661 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n662 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "697 5. Open access to data and code ", "page_idx": 22}, {"type": "text", "text": "698 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n699 tions to faithfully reproduce the main experimental results, as described in supplemental   \n700 material?   \n701 Answer: [NA]   \n702 Justification: This paper does not include experiments requiring code.   \n703 Guidelines:   \n704 \u2022 The answer NA means that paper does not include experiments requiring code.   \n705 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n706 public/guides/CodeSubmissionPolicy) for more details.   \n707 \u2022 While we encourage the release of code and data, we understand that this might not be   \n708 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n709 including code, unless this is central to the contribution (e.g., for a new open-source   \n710 benchmark).   \n711 \u2022 The instructions should contain the exact command and environment needed to run to   \n712 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n713 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n714 \u2022 The authors should provide instructions on data access and preparation, including how   \n715 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n716 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n717 proposed method and baselines. If only a subset of experiments are reproducible, they   \n718 should state which ones are omitted from the script and why.   \n719 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n720 versions (if applicable).   \n721 \u2022 Providing as much information as possible in supplemental material (appended to the   \n722 paper) is recommended, but including URLs to data and code is permitted.   \n723 6. Experimental Setting/Details   \n724 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n725 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n726 results?   \n727 Answer: [NA]   \n728 Justification: This paper does not include experiments.   \n729 Guidelines:   \n730 \u2022 The answer NA means that the paper does not include experiments.   \n731 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n732 that is necessary to appreciate the results and make sense of them.   \n733 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n734 material.   \n735 7. Experiment Statistical Significance   \n736 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n737 information about the statistical significance of the experiments?   \n738 Answer: [NA]   \n739 Justification: This paper does not include experiments.   \n740 Guidelines:   \n741 \u2022 The answer NA means that the paper does not include experiments.   \n742 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n743 dence intervals, or statistical significance tests, at least for the experiments that support   \n744 the main claims of the paper.   \n745 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n746 example, train/test split, initialization, random drawing of some parameter, or overall   \n747 run with given experimental conditions).   \n748 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n749 call to a library function, bootstrap, etc.)   \n750 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n751 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n752 of the mean.   \n753 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n754 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n755 of Normality of errors is not verified.   \n756 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n757 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n758 error rates).   \n759 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n760 they were calculated and reference the corresponding figures or tables in the text.   \n761 8. Experiments Compute Resources   \n762 Question: For each experiment, does the paper provide sufficient information on the com  \n763 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n764 the experiments?   \n765 Answer: [NA]   \n766 Justification: This paper does not include experiments.   \n767 Guidelines:   \n768 \u2022 The answer NA means that the paper does not include experiments.   \n769 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n770 or cloud provider, including relevant memory and storage.   \n771 \u2022 The paper should provide the amount of compute required for each of the individual   \n772 experimental runs as well as estimate the total compute.   \n773 \u2022 The paper should disclose whether the full research project required more compute   \n774 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n775 didn\u2019t make it into the paper).   \n776 9. Code Of Ethics   \n777 Question: Does the research conducted in the paper conform, in every respect, with the   \n778 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n779 Answer: [Yes]   \n780 Justification: They authors have made sure that the research conducted in the paper conform   \n781 with the NeurIPS Code of Ethics.   \n782 Guidelines:   \n783 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n784 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n785 deviation from the Code of Ethics.   \n786 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n787 eration due to laws or regulations in their jurisdiction).   \n788 10. Broader Impacts   \n789 Question: Does the paper discuss both potential positive societal impacts and negative   \n790 societal impacts of the work performed?   \n791 Answer: [NA]   \n792 Justification: The impacts are discussed in section 5   \n793 Guidelines:   \n794 \u2022 The answer NA means that there is no societal impact of the work performed.   \n795 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n796 impact or why the paper does not address societal impact.   \n797 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n798 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n799 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n800 groups), privacy considerations, and security considerations.   \n801 \u2022 The conference expects that many papers will be foundational research and not tied   \n802 to particular applications, let alone deployments. However, if there is a direct path to   \n803 any negative applications, the authors should point it out. For example, it is legitimate   \n804 to point out that an improvement in the quality of generative models could be used to   \n805 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n806 that a generic algorithm for optimizing neural networks could enable people to train   \n807 models that generate Deepfakes faster.   \n808 \u2022 The authors should consider possible harms that could arise when the technology is   \n809 being used as intended and functioning correctly, harms that could arise when the   \n810 technology is being used as intended but gives incorrect results, and harms following   \n811 from (intentional or unintentional) misuse of the technology.   \n812 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n813 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n814 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n815 feedback over time, improving the efficiency and accessibility of ML).   \n816 11. Safeguards   \n817 Question: Does the paper describe safeguards that have been put in place for responsible   \n818 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n819 image generators, or scraped datasets)?   \n820 Answer: [NA]   \n821 Justification: This paper only provides theoretical results and poses no such risks.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the paper poses no such risks.   \n824 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n825 necessary safeguards to allow for controlled use of the model, for example by requiring   \n826 that users adhere to usage guidelines or restrictions to access the model or implementing   \n827 safety filters.   \n828 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n829 should describe how they avoided releasing unsafe images.   \n830 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n831 not require this, but we encourage authors to take this into account and make a best   \n832 faith effort.   \n833 12. Licenses for existing assets   \n834 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n835 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n836 properly respected?   \n837 Answer: [NA]   \n838 Justification: This paper does not use existing assets.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not use existing assets.   \n841 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n842 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n843 URL.   \n844 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n845 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n846 service of that source should be provided.   \n847 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n848 package should be provided. For popular datasets, paperswithcode.com/datasets   \n849 has curated licenses for some datasets. Their licensing guide can help determine the   \n850 license of a dataset.   \n851 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n852 the derived asset (if it has changed) should be provided.   \n853 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n854 the asset\u2019s creators.   \n855 13. New Assets   \n856 Question: Are new assets introduced in the paper well documented and is the documentation   \n857 provided alongside the assets?   \n858 Answer: [NA]   \n859 Justification: This paper does not release new assets.   \n860 Guidelines:   \n861 \u2022 The answer NA means that the paper does not release new assets.   \n862 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n863 submissions via structured templates. This includes details about training, license,   \n864 limitations, etc.   \n865 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n866 asset is used.   \n867 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n868 create an anonymized URL or include an anonymized zip file.   \n869 14. Crowdsourcing and Research with Human Subjects   \n870 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n871 include the full text of instructions given to participants and screenshots, if applicable, as   \n872 well as details about compensation (if any)?   \n873 Answer: [NA]   \n874 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n875 Guidelines:   \n876 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n877 human subjects.   \n878 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n879 tion of the paper involves human subjects, then as much detail as possible should be   \n880 included in the main paper.   \n881 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n882 or other labor should be paid at least the minimum wage in the country of the data   \n883 collector.   \n884 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n885 Subjects   \n886 Question: Does the paper describe potential risks incurred by study participants, whether   \n887 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n888 approvals (or an equivalent approval/review based on the requirements of your country or   \n889 institution) were obtained?   \n890 Answer: [NA]   \n891 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n892 Guidelines:   \n893 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n894 human subjects.   \n895 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n896 may be required for any human subjects research. If you obtained IRB approval, you   \n897 should clearly state this in the paper.   \n898 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n899 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n900 guidelines for their institution.   \n901 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n902 applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]