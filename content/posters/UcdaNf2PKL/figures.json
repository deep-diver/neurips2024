[{"figure_path": "UcdaNf2PKL/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of classic and all-in-one video restoration. (a) aims to develop a specific model for each degradation to handle the corrupted video, assuming that the degradation types and levels are the same and known for all frames. In contrast, (b) intends to handle videos containing time-varying unknown degradations through a unified model, which is more practical and challenging.", "description": "This figure compares two approaches to video restoration: classic and all-in-one.  The classic approach uses a separate model for each known degradation type. In contrast, the all-in-one approach uses a single unified model to handle multiple unknown degradation types, especially those that change over time (time-varying unknown degradations or TUD). The all-in-one approach is presented as more practical and challenging.", "section": "1 Introduction"}, {"figure_path": "UcdaNf2PKL/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of the pixel shift issue. We compute the optical flow between two consecutive frames with time-invariant and time-varying degradations. Several directional vectors are visualized as red arrows to indicate the estimated pixel alignments between the two frames. One could observe that time-varying degradations lead to less accurate estimations compared to time-invariant degradations, causing a larger and more complex pixel shift after alignment.", "description": "This figure shows how time-varying degradations affect pixel alignment in video frames.  The optical flow (movement of pixels between frames) is calculated for clean frames, noisy frames with static degradations, and noisy frames with dynamic degradations. The arrows represent the calculated optical flow vectors.  The results show that time-varying degradations result in a larger and more complex pixel shift compared to time-invariant degradations. This highlights the difficulty of aligning frames with dynamic degradation, making it a significant challenge for video restoration.", "section": "2.1 Video Restoration"}, {"figure_path": "UcdaNf2PKL/figures/figures_3_1.jpg", "caption": "Figure 3: Architecture overview. (a) Overall architecture of our AverNet, which is mainly composed of propagation blocks. Each block consists of a (b) PGA module for spatially aligning features across frames with time-varying degradations, and a (c) PCE module for enhancing the features of current frame with unknown degradations. (d) PGI modules endow PGA and PCE with the capacity of conditioning on degradations by means of input-conditioned prompts. For simplicity, the superscripts j in (b) are omitted. In (c), the past feature is from the last time of propagation, and Ik refers to the indices of key frames.", "description": "This figure presents the overall architecture of AverNet, which is composed of propagation blocks.  Each block includes two core modules:  Prompt-Guided Alignment (PGA) and Prompt-Conditioned Enhancement (PCE). The PGA module aligns features across frames affected by time-varying degradations using prompt-guided deformable convolutions. The PCE module enhances features of the current frame with unknown degradations, also using prompts.  A Prompt Generation & Integration (PGI) module generates and integrates input-conditioned prompts to guide both PGA and PCE.", "section": "3 Proposed Method"}, {"figure_path": "UcdaNf2PKL/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative results on the \"tractor\" video from DAVIS-test (t = 12), from which one could observe that existing methods leave residual noise or artifacts in the results. In contrast, our method obtains clearer results that are closer to GT.", "description": "This figure compares the video restoration results of several methods on the \"tractor\" video from the DAVIS-test dataset, where the degradation variation interval is 12 frames. The methods include WDiffusion, TransWeather, AirNet, PromptIR, BasicVSR++, Shift-Net, RVRT, and AverNet (the proposed method).  The ground truth (GT) is also shown for comparison. The figure visually demonstrates that AverNet produces significantly clearer results with fewer artifacts compared to the other methods, showing its effectiveness in recovering videos from time-varying unknown degradations.", "section": "4.2 Comparison Experiments"}, {"figure_path": "UcdaNf2PKL/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results on the \"tractor\" video from DAVIS-test (t = 12), from which one could observe that existing methods leave residual noise or artifacts in the results. In contrast, our method obtains clearer results that are closer to GT.", "description": "This figure shows a qualitative comparison of video restoration results on the \"tractor\" video from the DAVIS-test dataset, where the time-varying degradation interval is set to 12.  It compares the results of several state-of-the-art methods (WDiffusion, TransWeather, AirNet, PromptIR, BasicVSR++, Shift-Net, RVRT) against the proposed AverNet and the ground truth (GT).  The results highlight AverNet's ability to produce clearer results with fewer artifacts and noise compared to the other methods, demonstrating its effectiveness in restoring videos with time-varying unknown degradations.", "section": "4.2 Comparison Experiments"}, {"figure_path": "UcdaNf2PKL/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative results on the \"tractor\" video from DAVIS-test (t = 12), from which one could observe that existing methods leave residual noise or artifacts in the results. In contrast, our method obtains clearer results that are closer to GT.", "description": "This figure compares the video restoration results of different methods on the \"tractor\" video from the DAVIS-test dataset. The variation interval (t) is set to 12.  The figure shows that existing methods (WDiffusion, TransWeather, AirNet, PromptIR, BasicVSR++, Shift-Net, and RVRT) produce results with noticeable artifacts and residual noise. In contrast, the proposed AverNet method achieves cleaner results that are closer to the ground truth (GT).", "section": "4.2 Comparison Experiments"}, {"figure_path": "UcdaNf2PKL/figures/figures_14_2.jpg", "caption": "Figure 4: Qualitative results on the \"tractor\" video from DAVIS-test (t = 12), from which one could observe that existing methods leave residual noise or artifacts in the results. In contrast, our method obtains clearer results that are closer to GT.", "description": "This figure compares the video restoration results of different methods on the \"tractor\" video from the DAVIS-test dataset, using a time-varying degradation interval (t=12).  The figure shows that existing methods like WDiffusion, TransWeather, AirNet, PromptIR, BasicVSR++, Shift-Net, and RVRT all exhibit artifacts, noise, or other issues in their restorations. In contrast, the proposed AverNet method produces a cleaner result that is much closer to the ground truth (GT). This demonstrates the effectiveness of AverNet in handling time-varying unknown degradations.", "section": "4 Experiment"}, {"figure_path": "UcdaNf2PKL/figures/figures_14_3.jpg", "caption": "Figure 8: Qualitative results on the \"subway\" video from DAVIS-test in the noise&blur degradation combination, from which one could observe that the results of existing methods are blurry. In contrast, the results of our method have clearer outlines and tones that are more similar to the GT.", "description": "This figure shows a comparison of video restoration results on the \"subway\" video clip from the DAVIS-test dataset.  The video was degraded with a combination of noise and blur.  The results from several state-of-the-art video restoration methods are presented, alongside the ground truth. AverNet, the proposed method, provides significantly clearer and more detailed results compared to other methods.", "section": "4.2 Comparison Experiments"}]