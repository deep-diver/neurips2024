[{"figure_path": "woRFmNJiLp/tables/tables_2_1.jpg", "caption": "Table 1: Toxicity before and after native alignment of Arabic data: smaller scores are better.", "description": "This table presents the results of a toxicity analysis conducted on 8,000 Arabic data points before and after applying the native alignment technique.  It shows the scores for four toxicity categories (Harassment, Hate, Sexual, Violence) as assessed by OpenAI's Moderation API. The \"Improvement\" row shows the percentage reduction in toxicity scores after native alignment, demonstrating the effectiveness of the method in reducing harmful content.", "section": "2.2 Preliminary Analysis on Alignment Data"}, {"figure_path": "woRFmNJiLp/tables/tables_5_1.jpg", "caption": "Table 2: Evaluation of base models in a few-shot setting. The best-performing model overall is highlighted in bold, while the top-performing model within each group is underlined.", "description": "This table compares the performance of several large language models (LLMs) on various benchmarks, including ArabicMMLU, EXAMS, ACVA (clean and all), and AraTrust.  The models are categorized into three groups based on their parameter size (less than 10B, greater than 10B, and closed-source models).  The table shows the performance of each model on each benchmark, with the best overall score highlighted in bold and the top performer within each group underlined. This allows for comparison across different model architectures and sizes, demonstrating the relative strengths and weaknesses of each model in various aspects, such as knowledge, Arabic localization, and trustworthiness.", "section": "3.2 Results and Analysis"}, {"figure_path": "woRFmNJiLp/tables/tables_9_1.jpg", "caption": "Table 2: Evaluation of base models in a few-shot setting. The best-performing model overall is highlighted in bold, while the top-performing model within each group is underlined.", "description": "This table presents the performance of various base LLMs on several benchmarks, including ArabicMMLU, EXAMS, ACVA, and AraTrust.  The models are evaluated using a few-shot setting. The best overall performing model is highlighted in bold, and the best performing model within each group (models with fewer than 10B parameters, models with more than 10B parameters, and closed-source models) is underlined.  The table helps compare the performance of different models, highlighting the effectiveness of the proposed 'native alignment' approach.", "section": "3.2 Results and Analysis"}, {"figure_path": "woRFmNJiLp/tables/tables_16_1.jpg", "caption": "Table 2: Evaluation of base models in a few-shot setting. The best-performing model overall is highlighted in bold, while the top-performing model within each group is underlined.", "description": "This table presents the performance of various baseline language models on four different benchmark tasks: ArabicMMLU, EXAMS, ACVA clean, and ACVA all.  Each model's performance is measured by its average score across these four benchmarks. The models are grouped based on the number of parameters they have.  The table helps to establish a baseline for comparison against the newly developed models presented in the paper that incorporate the native alignment technique. The best overall performer is highlighted in bold, with the best within each parameter group underlined.", "section": "3.2 Results and Analysis"}, {"figure_path": "woRFmNJiLp/tables/tables_16_2.jpg", "caption": "Table 5: Comparisons of MMLU between native alignment and data cleaning.", "description": "This table presents a comparison of the performance of three different models on the MMLU benchmark. The three models are: (1) a baseline model trained with the original dataset; (2) a model trained using a data cleaning approach (RefinedWeb); and (3) a model trained using a native alignment approach. The results in this table show the average scores for each model across four different categories of questions: Humanities, STEM, Social Science, and Other.  The table shows the average scores for each of the four categories. The results demonstrate the benefits of using the native alignment approach for improved model performance. ", "section": "E.2 Experiment Results and Analysis"}, {"figure_path": "woRFmNJiLp/tables/tables_17_1.jpg", "caption": "Table 6: Comparison data quality assessment results based on different seed data selection strategies", "description": "This table presents a comparison of data quality assessment results based on different seed data selection strategies for native alignment.  The assessments cover five aspects: Format, Accuracy of Information, Content Moderation, Advertisement Removal, and Level of Detail.  Each aspect is scored on a scale of 1-10. The table shows scores for three seed selection approaches:\n\n* **high-ppl:** High perplexity scores after data rewriting (indicating significant changes).\n* **low-ppl:** Low perplexity scores after rewriting (indicating minimal changes).\n* **random (x3):** Three separate experiments using randomly selected seed data; the average and standard deviation are presented.", "section": "2.2 Preliminary Analysis on Alignment Data"}]