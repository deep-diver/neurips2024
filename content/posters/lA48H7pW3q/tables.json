[{"figure_path": "lA48H7pW3q/tables/tables_7_1.jpg", "caption": "Table 1: Result on Flickr30k and MS-COCO with varied method. sc denotes shortcut, we evaluate CLIP and VSE++ w/wo shortcut on i2t and i2i task. QUEST outperforms InfoNCE and achieve superior performance compare with other baselines in most cases. \u2020denote use of ltd.", "description": "This table presents the performance comparison of different methods on image-text retrieval tasks using the Flickr30k and MS-COCO datasets.  The methods compared include InfoNCE, InfoNCE with Latent Target Decoding (LTD), InfoNCE with Implicit Feature Modification (IFM), and the proposed QUEST method. The table shows the Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and Recall Sum (RSUM) metrics for both image-to-text (i2t) and text-to-image (t2i) retrieval tasks. Results are presented for both scenarios with and without synthetic shortcuts ('sc' column) introduced to simulate easy-to-learn features.  The table demonstrates that the QUEST method consistently outperforms the baseline methods across various metrics and scenarios.", "section": "3.2 Performance Evaluation"}, {"figure_path": "lA48H7pW3q/tables/tables_8_1.jpg", "caption": "Table 1: Result on Flickr30k and MS-COCO with varied method. sc denotes shortcut, we evaluate CLIP and VSE++ w/wo shortcut on i2t and i2i task. QUEST outperforms InfoNCE and achieve superior performance compare with other baselines in most cases. \u2020denote use of ltd.", "description": "This table presents the performance comparison of different methods on image-text retrieval tasks using Flickr30k and MS-COCO datasets.  The methods compared include InfoNCE, InfoNCE+LTD, InfoNCE+IFM, and the proposed QUEST method.  Results are shown with and without synthetic shortcuts added to the data.  The table shows Recall@1, Recall@5, Recall@10, and RSUM values for image-to-text (i2t) and text-to-image (t2i) tasks on both datasets.  QUEST consistently outperforms other methods, particularly when mitigating shortcut learning.", "section": "3.2 Performance Evaluation"}, {"figure_path": "lA48H7pW3q/tables/tables_16_1.jpg", "caption": "Table 1: Result on Flickr30k and MS-COCO with varied method. sc denotes shortcut, we evaluate CLIP and VSE++ w/wo shortcut on i2t and i2i task. QUEST outperforms InfoNCE and achieve superior performance compare with other baselines in most cases. \u2020denote use of ltd.", "description": "This table presents the performance comparison of different models (InfoNCE, InfoNCE+LTD, InfoNCE+IFM, and QUEST) on image-to-text (i2t) and text-to-image (t2i) retrieval tasks using two datasets: Flickr30k and MS-COCO.  The results show the Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and Recall Sum (RSUM) metrics for each model and dataset, both with and without synthetic shortcuts added to the data.  The table highlights that the proposed QUEST model consistently outperforms the baselines, demonstrating its effectiveness in multimodal contrastive learning.", "section": "3.2 Performance Evaluation"}, {"figure_path": "lA48H7pW3q/tables/tables_19_1.jpg", "caption": "Table 4: Image audio retrieval results on FMA and GTZAN datasets.", "description": "This table presents the results of image-to-audio and audio-to-image retrieval experiments using two different datasets, FMA and GTZAN.  For each dataset and retrieval task, it shows the performance of two methods: InfoNCE and QUEST. The performance is measured using Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and the Recall sum (RSUM).  The table compares the performance of the baseline InfoNCE method with the proposed QUEST method, demonstrating the improvement achieved by QUEST. This evaluation assesses the effectiveness of the proposed approach on two different audio datasets and different retrieval tasks.", "section": "3.2 Performance Evaluation"}, {"figure_path": "lA48H7pW3q/tables/tables_20_1.jpg", "caption": "Table 5: Text-audio retrieval results on CLOTHO and AUDIOCAPS datasets.", "description": "This table presents the performance of InfoNCE and QUEST methods on text-to-audio and audio-to-text retrieval tasks, using the CLOTHO and AUDIOCAPS datasets.  The results are reported in terms of Recall@1, Recall@5, Recall@10, and RSUM (Recall Sum), providing a comprehensive evaluation of the retrieval accuracy for both methods and datasets.", "section": "3.2 Performance Evaluation"}]