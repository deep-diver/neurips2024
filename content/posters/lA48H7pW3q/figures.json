[{"figure_path": "lA48H7pW3q/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Our QUEST outperforms baselines 97.95% on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The Lsic narrows the gap between shared representations, while LP-UIC pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss Lcos is employed to constrain the area.", "description": "This figure shows the performance comparison of QUEST against other baselines on various image caption retrieval tasks using the CLIP model.  Part (a) presents a bar chart visualizing the Recall@1 and RSUM values across different datasets, showcasing QUEST's superior performance.  Part (b) illustrates the quaternion embedding space created by QUEST.  This space is designed to effectively separate shared and unique information, using constraints to pull shared representations closer, push unique representations further apart, and maintain orthogonality between shared and unique information.", "section": "1 Introduction"}, {"figure_path": "lA48H7pW3q/figures/figures_2_1.jpg", "caption": "Figure 2: Feature suppression in multi-view contrastive learning. We define I(XA; XB; Y) as task-related shared information, I(XA;Y|XB) and I(XB;Y|XA) as task-related unique information related to task Y in modalities XA and XB,respectively. Contrastive losses, such as InfoNCE, tend to maximize the task-related shared information while suppressing the task-related unique information in each modality. Left: before training with InfoNCE. Right: after training with InfoNCE.", "description": "This figure illustrates the problem of feature suppression in multi-view contrastive learning.  Before training with InfoNCE (left), there is a balance between shared and unique information for each modality. After training with InfoNCE (right), the model prioritizes maximizing shared information across modalities, leading to the suppression of unique, modality-specific information.", "section": "2 Methods"}, {"figure_path": "lA48H7pW3q/figures/figures_2_2.jpg", "caption": "Figure 3: Framework of QUEST. The unique decoder is utilized to extra view-specific unique information and this process is guided by the proposed constraints and penalization.", "description": "This figure illustrates the QUEST framework's architecture.  It shows how the model processes data from two modalities (M<sub>i</sub> and M<sub>j</sub>).  Each modality has an encoder that extracts general features. These features are then passed to separate shared and unique decoders. The shared decoders focus on information common to both modalities, while the unique decoders extract modality-specific information.  The framework uses constraints and self-penalization to optimize the extraction and integration of both shared and unique information, preventing the model from over-relying on easily learned shared information (shortcut learning) and ensuring that unique information is adequately represented. The figure also highlights the use of quaternion embedding space and orthogonal constraints to further improve performance.", "section": "2 Overview"}, {"figure_path": "lA48H7pW3q/figures/figures_9_1.jpg", "caption": "Figure 1: (a) Our QUEST outperforms baselines 97.95% on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The Lsic narrows the gap between shared representations, while LP-UIC pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss Lcos is employed to constrain the area.", "description": "This figure shows a comparison of the QUEST model's performance against other baseline models (a), highlighting a significant performance improvement (97.95% on average).  Part (b) illustrates the proposed quaternion embedding space which is designed to effectively separate and align shared and unique information from multiple modalities using constraints and self-penalization techniques.  The visualization demonstrates how the constraints work to improve the separation of information.", "section": "1 Introduction"}, {"figure_path": "lA48H7pW3q/figures/figures_15_1.jpg", "caption": "Figure 1: (a) Our QUEST outperforms baselines 97.95% on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The L<sub>SIC</sub> narrows the gap between shared representations, while L<sub>P-UIC</sub> pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss L<sub>cos</sub> is employed to constrain the area.", "description": "Figure 1(a) shows the superior performance of QUEST (the proposed method) against existing baselines.  This improvement is observed when using task-related unique information during training and evaluating the results on downstream tasks. Figure 1(b) illustrates how QUEST builds a quaternion embedding space. It uses constraints and self-penalization to better align both shared and unique representations from different modalities. The orthogonalization loss is applied to ensure that shared information does not overly affect the unique information.", "section": "1 Introduction"}]