[{"type": "text", "text": "QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qi Song1\u2217, Tianxiang $\\mathbf{Gong}^{2*}$ , Shiqi Gao2, Haoyi $\\mathbf{Zhou^{1,3}\\dagger}$ , Jianxin $\\mathbf{L}\\mathbf{i}^{2,3}$ 1School of Software, Beihang University 2School of Computer Science and Engineering, Beihang University 3Zhongguancun Laboratory, Beijing {songqi23, gongtx, gaoshiqi, haoyi, lijx}@buaa.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model\u2019s ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and suboptimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework named QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of $97.95\\%$ on the CLIP model. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal Contrastive Learning (MCL) has demonstrated robust representation capabilities and generalizability and effectively transferring to various downstream tasks (e.g. cross-modal retrieval [40, 39, 33], image captioning [37, 72, 73]). However, simply applying contrastive learning in multimodal scenarios presents significant challenges. $\\pmb{\\mathrm{\\Sigma}}$ In particular, contrastive learning treats all negative samples equally, ignoring the potential semantic relationships between negative samples and the anchor. $\\pmb{\\varphi}$ Besides, current contrastive learning methods focus on maximizing mutual information between two views [68, 70] while ignoring unique information [41]. In multi-view scenarios, the assumption that modalities share substantial task-related information often does not hold, especially in complex datasets with minimal inter-modal overlap. $\\pmb{\\wp}$ Meanwhile, recent studies [56, 75] indicate that contrastive learning often neglects significant portions of input information, leading to feature suppression [1, 8] and shortcut learning [25, 57], where models minimize loss through the simplest path (e.g. shared information [3]), sacrificing deeper learning. These issues are prevalent in multimodal [45, 54, 32] and multi-view tasks [76, 82, 50, 43]. Recent approaches focus on preserving more unique information, including reconstruction regularization [77, 4], implicit feature modification [58], and factorized representation [41], among others. However, these methods either overly introduce noise which may harm downstream tasks, or rely on certain assumptions (e.g., augmentation [41, 65, 45]). Additionally, we find that these methods do not explicitly distinguish unique information and still optimize using contrastive learning, making it difficult to avoid the model learning shortcuts [68, 29]. This raises the question: can we explicitly extract both task-related unique and shared information without introducing too much noise? ", "page_idx": 0}, {"type": "image", "img_path": "lA48H7pW3q/tmp/fc104c479f8a2880397c27d37f76d87b17b7cfd3e5e5905bd18bd725a714e536.jpg", "img_caption": ["Figure 1: (a) Our QUEST outperforms baselines $97.95\\%$ on average when trained with task-related unique information and evaluated on downstream tasks on the CLIP model. (b) We build the quaternion embedding space, which aligns shared and unique representations from different modalities through the application of constraints and self-penalization. The $\\mathcal{L}_{\\mathrm{SIC}}$ narrows the gap between shared representations, while ${\\mathcal{L}}_{\\mathrm{P-UIC}}$ pulls the plane spanned by intra-modality shared and unique representations closer. Furthermore, Orthogonalization loss $\\mathcal{L}_{c o s}$ is employed to constrain the area. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we proposes a novel contrastive framework called QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization, designed to enhance the extraction and integration of both shared and unique information across multimodal data. Our primary motivation is to develop a mechanism that effectively captures unique information through a novel quaternion multimodal embedding space, as illustrated in Figure 1(b). This embedding space aims to pull shared representations closer while aligning the unique representations with the shared representation on a common plane. We achieve this by leveraging the properties of the normal vector from the cross-product to diversified unique representation. Consequently, our approach aligns commonalities across modalities while preserving the distinctive unique features of each modality. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we first split a network into three components: an encoder, a shared decoder, and a unique decoder. The encoder learns general features with little bias toward specific tasks, while the shared decoder and unique decoder learn agreement and discriminative information, respectively. We build contrastive loss to constrain learning of shared information. To avoid the unique decoder degenerating into the shared decoder, we propose novel contrastive objectives and orthogonal constraints to optimize the quaternion vector space. Finally, self-penalization is used to prevent shared information from overly affecting quaternion vector space optimization. Our framework seeks to mitigate shortcut learning, offering a more nuanced, task-related learning paradigm. Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel framework to efficiently extract shared and unique information across multimodal data. To avoid the degeneration of the unique decoder, we propose an algorithm that utilizes quadruple embedding to constrain unique information from different views in a plane space.   \n\u2022 We consider that traditional CL overly relies on shared information due to data bias, causing failures with negative samples containing shared information related to the positive sample. Meanwhile, to prevent shared information from dominating the extraction of unique information, we introduce a self-penalization mechanism to dynamically reweight the distribution of negative samples, which penalizes hard negative samples. We provide theoretical analysis to show how this penalization effectively improves the extraction of unique information.   \n\u2022 We achieve state-of-the-art on popular datasets (e.g. MS-COCO [9] and Flickr30k [80]) compared to the baseline, demonstrating the general effectiveness of QUEST. Additionally, experiment results on synthetic shortcut datasets outperform baselines $97.95\\%$ on average for CLIP, verifying the efficacy of QUEST. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For different modalities $\\{\\mathcal{M}_{i}\\}_{i=1}^{K}$ , given one modality, denoted as $\\mathcal{M}_{i}$ , along with its corresponding set of views $\\{x_{i}^{j}\\}_{j=1}^{N_{i}},N_{i}\\ge1$ , for one modality $\\mathcal{M}_{1}$ encoder parameterized by $\\Theta_{1}$ , represented as $\\mathcal{F}_{\\mathcal{M}_{1}}(\\cdot;\\Theta_{1})$ , and another modality $\\mathcal{M}_{2}$ encoder parameterized by $\\Theta_{2}$ , denoted as $\\mathcal{F}_{\\mathcal{M}_{2}}(\\cdot;\\Theta_{2})$ . These encoders process the sample from modal $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ , for those modalities with multiple views, like modal one $\\mathcal{M}_{1}$ and each of its views through their respective encoder, resulting in corresponding general representations HjM1 = FM1(xj1; \u03981) and H jM = FM2(xj2; \u03982). However, as illustrated in Figure 2, the InfoNCE loss maximizes task-related features shared across all modalities during training (i.e. $I(X_{A}^{\\prime};X_{B}^{\\prime};Y))$ , while simultaneously suppressing the unique task-related features of each individual modality(i.e. $I(X_{A};Y|X_{B})$ and $I(X_{B};Y|X_{A}))$ . This process ultimately results in the loss of unique information. Therefore, the general representations are then separated into task-related shared and unique features through different decoders, i.e., the representations of different modalities ${\\bf{H}}_{{\\mathcal{M}}_{1}}^{j},{\\bf{H}}_{{\\mathcal{M}}_{2}}^{j}$ are inputted separately into different decoders $\\mathcal{G}_{\\mathcal{M}_{i}}(\\cdot)$ parameterized by $\\Phi_{i}$ . For the complete notations, refer to Appendix C, Table 6. ", "page_idx": 2}, {"type": "image", "img_path": "lA48H7pW3q/tmp/ec4b926dc58256a7ce76b29e3684d3f15343a4dcaf2b8d7e9924679efd004e56.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Feature suppression in multi-view contrastive learning. We define $I(X_{A};X_{B};Y)$ as taskrelated shared information, $I(X_{A};Y|X_{B})$ and $I(X_{B};Y|X_{A})$ as task-related unique information related to task $Y$ in modalities $X_{A}$ and $X_{B}$ ,respectively. Contrastive losses, such as InfoNCE, tend to maximize the task-related shared information while suppressing the task-related unique information in each modality. Left: before training with InfoNCE. Right: after training with InfoNCE. ", "page_idx": 2}, {"type": "text", "text": "2.2 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In multi-view scenarios, the relationships between different modalities are many-to-many (e.g., for image retrieval, multiple captions can refer to the same image). Within a single modality, different views contain task-related unique and shared information. Additionally, task-related shared information may also exist among negative samples (as indicated by the red shading in Figure 3). Therefore, optimizing solely for shared information while ignoring unique information is suboptimal. ", "page_idx": 2}, {"type": "text", "text": "To address the challenge of overlooking unique information inherent to different perspectives in multimodal scenarios, we introduce the effective framework called QUEST: Quadruple Multimodal Contrastive Learning with Constraints and SelfPenalization. This framework extends existing contrastive learning methods by incorporating a fourpartite architecture specifically designed to enhance the capture and integration of distinctive modalspecific features. The overall architecture is shown in Figure 3. According to [84, 62, 34], from the perspective of neural network architecture, shallow layers learn low-level and general features while deeper layers learn task-biased high-level semantic features. Therefore, we define the encoder output as $H$ , which contains shared $S$ and unique $U$ information related to the task, $H\\supseteq S\\cup U$ . Consequently, we shared shallow layers for general representation to reduce computational cost and optimize the shared decoder and unique decoder for $S$ and $U$ , respectively. ", "page_idx": 2}, {"type": "image", "img_path": "lA48H7pW3q/tmp/c5f4d30b78dde3f0c7f7813203bb893610afcbc58bafb3f88734ebe21c4db335.jpg", "img_caption": ["Figure 3: Framework of QUEST. The unique decoder is utilized to extra view-specific unique information and this process is guided by the proposed constraints and penalization. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.3 Quadruple InfoNCE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For each modality, the input data $\\mathbf{X}_{i}$ (sampled from views) undergoes transformation by a modalityspecific encoder $\\mathcal{F}_{\\mathcal{M}_{i}}(\\cdot)$ , producing an intermediary general representation denoted as $\\mathbf{H}_{\\mathcal{M}_{i}}$ . Consequently, we introduce two decoders: a shared information decoder $\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathrm{s}}(\\cdot)$ and a unique information decoder $\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathbf{u}}(\\cdot)$ . These decoders are tasked with disentangling the shared and unique components from the representation $\\mathbf{H}_{\\mathcal{M}_{i}}$ , respectively. Let $\\Theta_{i}$ represent the parameters of a shared encoder, while $\\Phi_{i}$ and $\\Psi_{i}$ symbolize the decoders for shared and unique information, respectively. The representation can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}_{i}^{\\mathbf{u}}=\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathbf{u}}(\\mathbf{H}_{\\mathcal{M}_{i}};\\boldsymbol{\\Phi}_{i})=\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathbf{u}}(\\mathcal{F}_{\\mathcal{M}_{i}}(\\mathbf{X}_{i};\\boldsymbol{\\Theta}_{i});\\boldsymbol{\\Phi}_{i}),}\\\\ {\\mathbf{Z}_{i}^{\\mathbf{s}}=\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathbf{s}}(\\mathbf{H}_{\\mathcal{M}_{i}};\\boldsymbol{\\Psi}_{i})=\\mathcal{G}_{\\mathcal{M}_{i}}^{\\mathbf{s}}(\\mathcal{F}_{\\mathcal{M}_{i}}(\\mathbf{X}_{i};\\boldsymbol{\\Theta}_{i});\\boldsymbol{\\Psi}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Shared Information Constraint. In multimodal and multi-view scenarios, data from different modalities and views often encapsulate shared information vital for model training. We conduct a shared Information Constraint (SIC) to maximize the lower bound of MI between representations from different views to encourage the shared decoder to learn agreement related to the task. This constraint is optimized by computing the InfoNCE loss between $\\mathbf{Z}_{i}^{\\mathbf{s}}$ and $\\mathbf{Z}_{j}^{\\mathbf{s}}$ ), defined as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SIC}}=\\sum_{i,j}\\mathbb{1}_{M_{i}\\neq M_{j}}\\mathbb{E}_{\\mathbf{Z}_{i}^{\\mathrm{s}}}\\left[-\\log\\frac{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{s}},\\mathbf{Z}_{j}^{\\mathrm{s}+})/\\tau)}{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{s}},\\mathbf{Z}_{j}^{\\mathrm{s}+})/\\tau)+\\sum_{k=1}^{m}\\mathbb{1}_{\\bar{\\mathbf{y}}^{-}}\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{s}},\\mathbf{Z}_{j k}^{\\mathrm{s}}-)/\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Unique Information Constraint. In contrast to shared information, unique information is modalityspecific and task-related, providing essential insights for downstream tasks. To preserve this, we introduce a Unique Information Constraint (UIC), extracting the unique information that exists within different views, which are unrelated to each other yet relevant to the task. Relying solely on Shared Information Constraint (SIC) is insufficient to preserve this information [41, 45, 3]. Strict constraints, such as directly enforcing consistency of distributions across diverse views, may lead to the Unique Information Constraint converging to the SIC, particularly in scenarios with identical input representations, loss functions, and network structures. To address this issue, we implement a less stringent constraint, aiming to maximize the similarity between the spaces formed by unique and shared information across different modalities. Firstly, we derive the representation space of normal vectors for shared and unique embedding spaces through cross-product calculations, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{i}^{\\mathbf{n}}=\\mathbf{Z}_{i}^{\\mathbf{s}}\\times\\mathbf{Z}_{i}^{\\mathbf{u}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the newly projected space, our objectives aim to maximize the alignment of unique representation from different modalities within the plane spanned by the shared representation, the magnitude can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{Z}_{i}^{\\mathrm{s}}\\times\\mathbf{Z}_{i}^{\\mathrm{u}})\\cdot(\\mathbf{Z}_{j}^{\\mathrm{s}}\\times\\mathbf{Z}_{j}^{\\mathrm{u}})=\\|\\mathbf{Z}_{i}^{\\mathrm{s}}\\|\\|\\mathbf{Z}_{i}^{\\mathrm{u}}\\|\\sin\\alpha\\|\\mathbf{Z}_{j}^{\\mathrm{s}}\\|\\|\\mathbf{Z}_{j}^{\\mathrm{u}}\\|\\sin\\beta\\cos\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sin\\alpha$ and $\\sin\\beta$ represent the sine similarity between the shared and unique representation across different modalities, and $\\cos\\gamma$ represents the cosine similarity of the normal vector. We maximize sin $\\alpha$ and $\\sin\\beta$ via orthogonalized cosine loss $\\mathcal{L}_{\\mathrm{cos}}$ and contrastive loss to maximize $\\cos\\gamma$ the unique information constraint can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{UC}}=\\displaystyle\\sum_{i,j}\\mathbb{1}_{\\mathcal{M}_{i}\\ne\\mathcal{M}_{j}}\\mathbb{E}_{\\mathbf{Z}_{i}^{\\mathrm{n}}}\\left[-\\log\\frac{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})/\\tau)}{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})/\\tau)+\\sum_{k=1}^{m}\\mathbb{1}_{\\hat{\\mathbf{y}}^{-}}\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j k}^{\\mathrm{n}^{-}})/\\tau)}\\right]}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i}\\sum_{j}\\frac{\\mathbf{Z}_{i j}^{\\mathrm{s}}\\cdot\\mathbf{Z}_{i j}^{\\mathrm{u}}}{\\|\\mathbf{Z}_{i j}^{\\mathrm{s}}\\|\\|\\mathbf{Z}_{i j}^{\\mathrm{u}}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By pulling positive samples closer and pushing negative samples away, we constrain the shared and unique representations of different modalities to coexist within the same spatial plane as much as practicable. Concurrently, we employ $\\mathcal{L}_{\\mathrm{cos}}$ to maintain the diversity of unique information and ensure that $\\mathbf{Z}_{i}^{\\mathbf{s}}\\neq\\mathbf{Z}_{i}^{\\mathbf{u}}$ . It is worth noting that we do not impose any constraints on the unique representations of different modalities, as these may be inherently unrelated. ", "page_idx": 4}, {"type": "text", "text": "Comparison. Applying standard InfoNCE to extract unique information leads to suboptimal generalization, as demonstrated in Section 3.3. To address this, we propose UIC with indirect vectors ${\\mathbf Z}^{n}$ , where the cross-product operation fundamentally alters gradient propagation patterns compared to pair-wised InfoNCE. Specifically, given $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\log\\frac{\\exp(Z_{a}\\cdot Z_{b}^{+}/\\tau)}{\\sum_{i=0}^{N}\\exp(Z_{a}\\cdot Z_{i}/\\tau)}}\\end{array}$ iN=0 expa(Zba \u00b7Zi/\u03c4) [70, 83], the gradient with respect to anchor embedding $Z_{a}$ as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\frac{\\partial\\mathcal{L}_{\\mathrm{InfoNCE}}}{\\partial Z_{a}}=\\frac{1}{\\tau}(Z_{b}^{+}-\\sum_{i=0}^{N}\\beta_{i}Z_{b i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u03b2i = iNe=x0 pe(xZpa(\u00b7ZZai\u00b7/Z\u03c4i)/\u03c4) and Zb+ is random sampled from positive set {Zb1 , ...Zbj}. Under the assumption that different views hold both shared and unique information, we have $I(Z_{b}^{j};Y)=$ $I(Z_{b}^{j};Z_{a};Y)+I(Z_{b}^{j};Y|Z_{a})$ , where $I(Z_{b}^{j};Z_{a};Y)$ represents shared information between two views and $I(Z_{b}^{j};Y|Z_{a})$ represents unique information for $j$ th view. With sufficient training iterations, the unique information tends towards noise as shared information dominates the accumulated gradient (first term in Eq. (6)). This is consistent with the conclusion of MI [41]. Assume that the features obtained from the encoder consist of shared features $S$ which correspond to the anchor and unique features $U$ , represented as $Z_{b}^{j}\\,=\\,(S\\cup U^{j})$ . Traditional contrastive learning defines an additive model $Z_{b}^{j}\\,=\\,(S+U^{j})$ whereas $Z_{b}^{j}=(S\\times U^{j})$ in our model. Intuitively, there exists $\\zeta$ satisfes $\\zeta\\,=\\,Z_{a}\\cdot(S\\times U^{1})\\,=\\,\\ldots\\,=\\,Z_{a}\\cdot(S\\times U^{j})$ . Therefore, UIC is a weaker constraint that ensures quaternion vectors between different views lie on the same plane as much as possible. If we use both SIC and UIC simultaneously, SIC will pull the shared representations of different views closer, while UIC will ensure that the unique representations of different views lie on the same plane as much as possible, rather than measuring their cosine similarity, as unique information is uncorrelated. We conducted extensive experiments to verify this (Section 3.3 for more details). ", "page_idx": 4}, {"type": "text", "text": "2.4 Shared Information Guided Constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Contrastive learning fundamentally operates by optimizing vector representations to minimize distances between positive pairs while maximizing distances between negative pairs in the embedding space. However, a critical limitation inherent in conventional approaches stems from their undifferentiated treatment of all samples within a batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ as negative examples. This indiscriminate categorization may inadvertently cause the model to overlook potential semantic relationships (as illustrated by the red shading in Figure 3), despite their shared semantic content (e.g., different image captions containing identical substrings in image-text retrieval tasks). Such misclassification significantly impairs the model\u2019s representation capacity. We refer to these cases as \"hard negative samples. While existing methods attempt to address this issue through clustering-based approaches, their two-stage nature limits practical adoption. Leveraging our dual-branch architecture, we propose a more effective solution that directly utilizes the output of the shared decoder as a supervision signal for the penalty term. ", "page_idx": 4}, {"type": "text", "text": "Considering our objectives is to optimize the shared information decoder through $\\mathcal{L}_{\\mathrm{SIC}}$ and unique information decoder through ${\\mathcal{L}}_{\\mathrm{UIC}}$ , and the shared information also affects the optimization of unique information as shown in Eq. (4), we attempt to use the intra-model shared information similar to penalization to guide the optimization of unique information. Unlike soft label [49, 17, 16, 47, 60, 24] which aim to mitigate the strict constraints of one-hot labels, preventing overconfidence by retaining more potential positive samples, our method aims to impose stricter constraints to suppress shared information in the process of learning unique information. Specifically, the more shared information between the anchor and all the negative samples, the greater the encouragement in learning unique information between the anchor and the positive sample. Formally, for shared representation $\\mathbf{Z}_{i}^{\\mathrm{s}}$ and $\\mathbf{Z}_{j}^{\\mathrm{s}}$ , the weighted similarity matrix can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}=\\exp(\\lambda[\\mathbf{S}-\\mathrm{diag}(\\mathbf{S})+\\mathbf{I}]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{S}=\\mathbf{Z}_{i}^{\\mathrm{s}}\\mathbf{Z}_{j}^{\\mathrm{s}}^{T}$ is the similarity matrix, $\\mathbf{I}$ is identity matrix, $\\lambda\\in\\mathbb R$ is learnable weight parameter. Next, the weighted similarity matrix $\\mathcal{P}\\in\\mathbb{R}^{N\\times N}$ is utilized as penalization to supervise the optimization of unique information satisfied $\\mathscr{P}_{i j,i\\neq j}\\propto s(z_{i}^{s},z_{j}^{s})$ , the penalized UIC can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{P.UIC}}=\\displaystyle\\sum_{i,j}\\mathbb{1}_{\\mathcal{M}_{i}\\ne\\mathcal{M}_{j}}\\mathbb{E}_{\\mathbf{Z}_{i}^{\\mathrm{n}}}\\left[-\\log\\frac{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})/\\tau)}{\\exp(s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})/\\tau)+\\sum_{k=1}^{m}\\mathbb{1}_{\\hat{\\mathbf{y}}^{-}}\\exp(\\mathcal{P}_{k}\\cdot s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j k}^{\\mathrm{n}^{-}})/\\tau)}\\right]}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i}\\mathcal{L}_{\\mathrm{cos.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Gradient Analysis. For simplicity, we set $\\begin{array}{r}{\\lambda=\\sum_{k=0}^{m}\\exp(\\mathcal{P}_{k}\\cdot s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j k}^{\\mathbf{n}})/\\tau)}\\end{array}$ and ignore the second term, we reformulate Eq. (8) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathcal{L}}_{\\mathrm{P:UIC}}=\\mathbb{E}_{{\\mathbf{Z}}_{i}^{\\mathbf{n}}}\\left[-\\log\\frac{\\exp(\\mathcal{P}^{+}\\cdot s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j}^{\\mathbf{n}^{+}})/\\tau)}{\\exp(\\mathcal{P}^{+}\\cdot s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j}^{\\mathbf{n}^{+}})/\\tau)+\\sum_{k=1}^{m}\\mathbb{1}_{\\hat{\\mathbf{y}}^{-}}\\exp(\\mathcal{P}_{k}\\cdot s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j k}^{\\mathbf{n}^{-}})/\\tau)}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{{\\mathbf{Z}}_{i}^{\\mathbf{n}}}\\left[\\log\\lambda-\\frac{\\mathcal{P}^{+}\\cdot s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j}^{\\mathbf{n}^{+}})}{\\tau}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The gradient can be calculated as (Appendix D.2 for details): ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\frac{\\partial\\widetilde{\\mathcal{L}}_{\\mathrm{P}^{\\mathrm{}}\\mathrm{UIC}}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}=\\frac{\\mathcal{P}^{+}}{\\tau}\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}-\\frac{1}{\\lambda\\tau}\\sum_{k=0}^{m}\\mathcal{P}_{k}\\exp\\left(\\frac{\\mathcal{P}_{k}s^{k}}{\\tau}\\right)\\frac{\\partial s^{k}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $s^{+}\\,=\\,s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})$ and $s^{k}\\,=\\,s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j k}^{\\mathbf{n}^{-}})$ . Intuitively, $\\mathcal{P}$ represents the belief mass based on shared information, and the larger $P_{k}$ indicates more shared information between hard positive samples, which also influences the optimization of the unique decoder as in Eq. (4). When $\\mathcal{P}_{k}=0$ (i.e, $c(\\mathbf{Z}_{i},\\mathbf{Z}_{j k})\\approx0)$ , it will degenerate to the original InfoNCE loss. For hard negatives samples hold shared information where $\\overline{{s}}(\\mathbf{Z}_{i}^{\\mathbf{s}},\\mathbf{Z}_{j}^{\\mathbf{s}})>\\underline{{0}}$ , we increase the gradient using a penalty term which ensures that even if $\\mathbf{Z}_{i}^{\\mathrm{s}}$ is relatively large in Eq. (4), the lower loss reinforce other terms to constrain the overall value. From mutual information perspectives (Appendix D.3 for details), we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nI(Z_{i},Z_{j})\\geq H^{\\tilde{P}}(Z_{j}|Z_{i})-H(Z_{j}|Z_{i})+\\log N-\\widetilde{\\mathcal{L}}_{\\mathrm{P-UIC}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When all negative samples $k$ satisfy $s(\\mathbf{Z}_{i},\\mathbf{Z}_{j k})\\approx0$ , we obtain $H^{\\tilde{P}}(Z_{j}|Z_{i})=H(Z_{j}|Z_{i})$ . Subsequently, as the shared information between hard positive samples increases, $H^{\\tilde{P}}(Z_{j}|Z_{i})$ correspondingly increases, thereby elevating both the lower bound of mutual information and the confidence level, which consequently facilitates the learning of unique information. ", "page_idx": 5}, {"type": "text", "text": "2.5 Training Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In summary, we apply (1) SIC (Eq. 2) to keep shared information relevance between different modalities, (2) UIC (Eq. 5) to build quadruple embedding space by maximizing the alignment of normal vector spanned by shared representation and unique representation, and the shared information is jointly optimized by SIC and UIC, (3) Self-penalization (Eq. 7) to amplify the effect of false positive samples in unique information optimization. The overall objective can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{QUEST}}=\\mathcal{L}_{\\mathrm{SIC}}+\\mathcal{L}_{\\mathrm{P-UIC}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baselines and Setup. Shortcut learning refers to the process in deep learning model training where the model completes tasks (such as classification, retrieval, etc.) by learning simple and discriminatory features while ignoring the semantic and more complex features of the data. This can result in poor model performance on downstream tasks. Latent target decoding (LTD) [4], and implicit feature modification (IFM) [58] are two methods that mitigate shortcut learning by reducing feature suppression. LTD reconstructs the caption representations in the latent space of a Sentence-BERT model, allowing the encoder to mitigate feature suppression via correct mapping. IFM perturbs discriminatory features through encoders and removes part of these features to avoid learning shortcuts, which is implemented as a dual loss combined with the InfoNCE loss. We provide source code of our paper. 2. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Image Caption Retrieval(ICR) retrieves the most relevant sample in another modality by using a sample of one modality as a query. In this task, there are two retrieval modes: text-to-image (t2i) and image-to-text (i2t). We evaluated our method in the ICR task using CLIP [52] and $\\mathrm{VSE++}$ [23] models on Flickr30k [80] and MS-COCO [42] datasets. ", "page_idx": 6}, {"type": "text", "text": "Bleeker et al. [3] proposed synthetic shortcuts for the vision-language framework. This allows us to evaluate whether vision language (VL) models capture easy-to-learn discriminatory features or task-related information. We add MNIST Images to the top of pictures and appending corresponding numbers at the end of their respective captions. This controlled approach preserves the original information from both modalities and increases explicit mutual information between them. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. To evaluate the model\u2019s performance on the Flickr30k and MS-COCO-Caption datasets, the Recall $@\\,\\mathrm{K}$ (i.e. $\\mathbf{R}\\otimes1$ , $\\mathbf{R}@5$ , $\\mathbf{R}\\@10$ , which refers to the proportion of instances where the correct answer appears among the top K returned results out of all instances) and recall sum (RSUM) were selected as evaluation metrics for both i2t and t2i retrieval. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We select ViT-B/32 as the visual backbone for CLIP and resnet152 for the $\\mathrm{VSE++}$ model when we evaluate them on the caption retrieval task using Flickr30k and MS-COCO dataset. We fine-tuned the pre-trained CLIP on downstream tasks and trained $\\mathrm{VSE++}$ from scratch with our method. ", "page_idx": 6}, {"type": "text", "text": "Alternatives of Unique Decoder. The key of multi-view assumption lies in the extraction of unique information, current methods achieve this by employing a single-layer MLP, ensuring orthogonality with shared information or through data augmentation and factorized loss. In our framework, a single-layer MLP is used for the ResNet backbone, while a two-layer Transformer is implemented for the Transformer backbone. Detailed methodologies are provided in the Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "3.2 Performance Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "QUEST vs. Vanilla InfoNCE. QUEST outperforms the vanilla InfoNCE, as shown in Table 1. On the Flickr30k test set, QUEST yields $R@1$ improvements of (2.4, 1.5) for CLIP and (1.1, 3.4) for $\\mathrm{VSE++}$ in i2t and t2i tasks.. The corresponding RSUM metrics increase by 3.2 and 12.1. On MS-COCO, QUEST achieves $R@1$ gains of (1.6, 0.9) for CLIP and (3.1, 3.2) for $\\mathrm{VSE++}$ in i2t and t2i tasks, with corresponding RSUM improvements of 8.1 and 17.3. Additionaly, QUEST exhibits faster convergence to the optimal solution. ", "page_idx": 6}, {"type": "text", "text": "Experiment on Synthetic Shortcuts. To assess the effectiveness of our proposed QUEST mitigating feature suppression in Contrastive Learning, we use synthetic shortcuts [3] by injecting easy-to-learn and discriminatory shared information into the image-text training dataset. We then evaluate the model\u2019s performance on downstream tasks with and without these synthetic shortcuts to determine if the presence of shortcuts causes the suppression of other task-related information and an over-reliance on shortcut features. Our baselines\u2019 results are consistent with [3]. ", "page_idx": 6}, {"type": "text", "text": "As shown in Table 1, adding shortcuts leads to performance degradation across all models to some degree, indicating that the models have not learned sufficient shared and unique information. However, our method outperforms LTD and IFM, indicating it captures task-related information more effectively in downstream tasks (evaluation without shortcuts). ", "page_idx": 6}, {"type": "text", "text": "CLIP Performance Enhancement. Our method significantly enhances the CLIP model\u2019s performance on the Flick $\\cdot30\\mathbf{k}$ and MS-COCO datasets. Compared to InfoNCE, we observe substantial $\\mathbf{R}\\mathcal{@}1$ improvements in both i2t and t2i tasks, with RSUM increases of 91.6 and 240 on Flickr30k and MS-COCO, respectively. Our approach also outperforms previous SOTA methods, surpassing $\\scriptstyle{\\mathcal{L}}_{\\mathrm{InfoNCE+IFM}}$ on Flickr30k and showing sinificant performance improvements against $\\mathcal{L}_{\\mathrm{InfoNCE}}$ on MS-COCO. ", "page_idx": 6}, {"type": "table", "img_path": "lA48H7pW3q/tmp/48aae2866ad8fe9f8f80472405c3d2be0b116eea232c1d21aac7c1c556e377c9.jpg", "table_caption": ["Table 1: Result on Flickr30k and MS-COCO with varied method. sc denotes shortcut, we evaluate CLIP and $\\mathrm{VSE++}$ w/wo shortcut on i2t and i2i task. QUEST outperforms InfoNCE and achieve superior performance compare with other baselines in most cases. \u2020denote use of ltd. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparative Analysis of $\\mathbf{VSE++}$ Improvements. $\\mathrm{VSE++}$ also shows improvements on both datasets, with increases in $R@1$ for i2t and $_{\\mathrm{t}2\\mathrm{i}}$ tasks compared to $\\mathcal{L}_{\\mathrm{InfoNCE}}$ . On MS-COCO, $\\mathrm{VSE++}$ outperforms $\\mathcal{L}_{\\mathrm{InfoNCE+IFM}}$ , enhancing $R@1$ and rsum metrics. However, $\\mathcal{L}_{\\mathrm{InfoNCE+LTD}}$ achieves better performance on Flick $\\cdot30\\mathrm{k}$ , excelling in text modality reconstruction and enhancement. This discrepancy may be attributed to $\\mathrm{VSE++}$ \u2019s GRU text encoder, which appears to be less effective in capturing modality-independent information on smaller datasets like Flick $\\cdot30\\mathbf{k}$ . ", "page_idx": 7}, {"type": "text", "text": "3.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We also evaluate the effects of two constraints (shared information constraint $\\mathcal{L}_{\\mathrm{SIC}}$ and unique information constraint $\\mathcal{L}_{\\mathrm{UIC}})$ and self-penalization on the performance of our proposed QUEST method. These variants, denoted as $\\mathcal{L}_{\\mathrm{SIC}}$ , ${\\mathcal{L}}_{\\mathrm{UIC}}$ , $\\mathscr{L}_{\\mathrm{P}}.$ -UIC, are evaluated through modality-specific decoder objectives in our ablation studies. The evaluation is conducted on the ICR task, comparing the performance differences between CLIP and $\\mathrm{VSE++}$ models on the Flick ${\\sqrt{30\\mathbf{k}}}$ and MS-COCO datasets. ", "page_idx": 7}, {"type": "text", "text": "Decoder Configuration and Objective Function Assignment. As shown in Table 2, both $\\mathcal{L}_{\\mathrm{SIC}}$ and ${\\mathcal{L}}_{\\mathrm{UIC}}$ individually exhibit performance drops compared to the full QUEST. CLIP and $\\mathrm{VSE++}$ perform poorly when only using unique information constraints, achieving 55.1 and 40.7 on COCO and 80.8 and 47.8 on Flickr $\\cdot30\\mathrm{k}$ , respectively, suggesting that capturing unique information alone is insufficient for improvement. Furthermore, the results demonstrate that self-penalization enhances the performance of QUEST, preventing overconfidence and retaining more potential positive samples. CLIP and $\\mathrm{VSE++}$ have better performance when using $\\mathcal{L}_{\\mathrm{P}}$ -UIC instead of ${\\mathcal{L}}_{\\mathrm{UIC}}$ . In addition, our QUEST method outperforms the approach utilizing $\\mathcal{L}_{\\mathrm{SIC+UIC}}$ . The ablation study aligns with our theoretical framework, confirming that simultaneously capturing shared and unique information in multi-view contrastive learning, along with the application of self-penalization, leads to significant improvements in downstream tasks. ", "page_idx": 7}, {"type": "text", "text": "Unique Decoder Degeneration. Our architecture includes a unique decoder module. To evaluate its impact on the task, we replaced this unique decoder with a shared decoder to determine if performance improvements were due to parameter changes. As shown in Table 2, using dual shared decoders $(\\mathcal{L}_{\\mathrm{SIC+SIC}})$ did not enhance performance and even resulted in a decline compared to a single shared decoder $(\\mathcal{L}_{\\mathrm{SIC}})$ . We refer to this decline as the degradation of the unique decoder. ", "page_idx": 7}, {"type": "text", "text": "3.4 Evaluation on More Modalities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted extensive experiments on three modalities: image, text and audio. For image-audio evaluations, we leveraged the FMA [19, 20] and GTZAN [69] datasets, while text-audio experiments were evaluated on CLOTHO [21] and AUDIOCAPS [36] datasets. As shown in Figure 4, our pro", "page_idx": 7}, {"type": "table", "img_path": "lA48H7pW3q/tmp/0f6401972976d042e13de627852292607cd9e01989a8389a8a9f87b0a4e5162a.jpg", "table_caption": ["Table 2: Ablation study on image caption retrieval task with different training objectives. D1 and D2 denote decoders in the architecture. Decoder with all \u0017beneath are omitted, while those with \u0013indicate optimization with corresponding objective functions. Bold and underlined numbers indicate the best and second-best results, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Performance comparison of InfoNCE and QUEST methods with additional audio modality on image-to-audio (i2a) and audio-to-image (a2i) retrieval tasks across FMA, GTZAN, CLOTHO, and AUDIOCAPS datasets. ", "page_idx": 8}, {"type": "text", "text": "posed methods outperformed the baseline. For simplicity, we employed the almost simplest decoder structure (linear layers) and did not implement modality fusion or any cross-modal interactions.We believe that enhancing the architecture, strengthening cross-modal interactions, employing a larger batch size and incorporating pre-training would yield even higher performance. ", "page_idx": 8}, {"type": "text", "text": "3.5 Case Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "On Flickr30k, $\\mathcal{L}_{\\mathrm{InfoNCE}}$ over-privileges shared information, particularly for synthetic shortcut data, while $\\mathcal{L}$ QUEST emphasizes task-relevant semantic information. In the text-to-image retrieval case, $\\mathcal{L}_{\\mathrm{InfoNCE}}$ leverages modality-shared information like \"gold\" and \"bicycle\" for retrieval. When shared information is explicitly injected across modalities through shortcuts, $\\mathcal{L}_{\\mathrm{InfoNCE}}$ pays less attention to the semantic information in queries. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Multimodal Contrastive Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the field of multimodal contrastive learning, substantial progress has been achieved in harnessing the synergistic effects of multimodal data [38, 46, 11, 74]. Multimodal contrastive learning mainly concentrates on developing methods to semantically align data from diverse modalities through contrastive learning techniques, including optimizing contrastive objectives to enhance agreement between paired data [52, 33, 79, 15], improving the selection of positive and negative sample [85, 35, 64], optimize training mechanism [28, 12] and designing innovative network architectures [7, 6, 26, 10]. Nevertheless, our work is orthogonal to most of the aforementioned studies. Our shared branch approach can be integrated with existing training methods such as MoCo [28] and SimCLR [7]. ", "page_idx": 8}, {"type": "image", "img_path": "lA48H7pW3q/tmp/687e0cbb0f5ef22999318e89f5cb21d8d2bc64f3ec67da21bbcf6bf2ba35b5ca.jpg", "img_caption": ["Figure 5: Case Study: (a) Image-to-text retrieval , where the results of $\\mathcal{L}_{\\mathrm{QUEST}}$ and $\\mathcal{L}_{\\mathrm{InfoNCE}}$ are denoted by italics and underlines, respectively. (b) Text-to-image retrieval, where red and green borders indicate the top-5 retrievals using $\\mathcal{L}_{\\mathrm{QUEST}}$ , while blue borders represent those using $\\mathcal{L}_{\\mathrm{InfoNCE}}$ . The upper and lower sections in both (a) and (b) demonstrate scenarios with and without shortcuts, respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.2 Shortcuts Learning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Shortcut Learning refers to the tendency of deep neural networks to exploit simple but potentially unreliable features (i.e., \"shortcuts\") in data for decision-making, rather than learning more complex but reliable features [25]. This phenomenon can lead to poor model performance on out-of-distribution data and is particularly common in multimodal retrieval [3] and VQA tasks [55, 18]. Robinson et al. [58] proposed strategically adjusting the feature distribution of positive and negative sample pairs to achieve implicit feature modification in contrastive learning, guiding models to learn more robust feature representations. Sanchez et al. [59] propose maximizing mutual information to capture data attributes in shared and exclusive representations, while minimizing it between them to enforce disentanglement. LTD [4] introduces an additional decoder to reconstruct input text descriptions in the latent space of a universal sentence encoder, preventing image and text encoders from suppressing predictive features. More recently, some works strive to enhance the estimation of mutual information through the utilization of stricter bounds [30, 41, 13, 51] or the introduction of regularization constraints [45], consequently preserving unique information more effectively. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce QUEST, a framework utilizing specialized decoders to extract both unique and shared information via shared information-guided constraints and self-penalization. This study addresses the challenges of imbalanced negative samples and task-related unique feature suppression in Multimodal Contrastive Learning. Our method optimizes shared and unique representations simultaneously, outperforming state-of-the-art methods in preserving unique information and enhancing contrastive learning. Unlike traditional approaches that employ direct dot products to minimize distances between positive samples, QUEST leverages quaternions for the indirect optimization of unique and shared information. However, the application of cross products in high-dimensional spaces is limited, complicating the control of high-dimensional representations and reducing theoretical interpretability. For further discussion on these limitations, see the appendix (Appendix E). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the grants from the Natural Science Foundation of China (62202029), and Young Elite Scientists Sponsorship Program by CAST (No. 2023QNRC001). Thanks for the computing infrastructure provided by Beijing Advanced Innovation Center for Big Data and Brain Computing. This work was also sponsored by CAAI-Huawei MindSpore Open Fund. Haoyi Zhou is the corresponding author. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] Mido Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael G. Rabbat, and Nicolas Ballas. The hidden uniform cluster prior in self-supervised learning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[2] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in Neural Information Processing Systems, 32, 2019.   \n[3] Maurits Bleeker, Mariya Hendriksen, Andrew Yates, and Maarten de Rijke. Demonstrating and reducing shortcuts in vision-language representation learning. Transactions on Machine Learning Research, 2024.   \n[4] Maurits J. R. Bleeker, Andrew Yates, and Maarten de Rijke. Reducing predictive feature suppression in resource-constrained contrastive image-caption retrieval. Trans. Mach. Learn. Res., 2023, 2023.   \n[5] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, pages 77\u201391. PMLR, 2018.   \n[6] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. Contrastive learning of global and local features for medical image segmentation with limited annotations. Advances in Neural Information Processing Systems, 33:12546\u201312558, 2020.   \n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pages 1597\u20131607. PMLR, 2020.   \n[8] Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. Advances in Neural Information Processing Systems, 34:11834\u201311845, 2021.   \n[9] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015.   \n[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n[11] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. Pali: A jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[12] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9640\u20139649, 2021.   \n[13] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A contrastive log-ratio upper bound of mutual information. In International Conference on Machine Learning, pages 1779\u20131788. PMLR, 2020.   \n[14] Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. CoRR, abs/1810.08810, 2018.   \n[15] Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, and Yale Song. Robust contrastive learning against noisy views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16670\u201316681, 2022.   \n[16] Sanghyuk Chun, Wonjae Kim, Song Park, Minsuk Chang, and Seong Joon Oh. Eccv caption: Correcting false negatives by collecting machine-and-human-verified image-caption associations for ms-coco. In European Conference on Computer Vision, pages 1\u201319. Springer, 2022.   \n[17] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. Probabilistic embeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8415\u20138424, 2021.   \n[18] Corentin Dancette, Remi Cadene, Damien Teney, and Matthieu Cord. Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1574\u20131583, 2021.   \n[19] Micha\u00ebl Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. FMA: A dataset for music analysis. In Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, pages 316\u2013323, 2017.   \n[20] Micha\u00ebl Defferrard, Sharada P. Mohanty, Sean F. Carroll, and Marcel Salath\u00e9. Learning to recognize musical genre from audio: Challenge overview. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis, editors, Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, pages 1921\u20131922. ACM, 2018.   \n[21] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 736\u2013740. IEEE, 2020.   \n[22] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep learning in healthcare. Nature medicine, 25(1):24\u201329, 2019.   \n[23] Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. VSE $^{++}$ : improving visual-semantic embeddings with hard negatives. In British Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, September 3-6, 2018, page 12. BMVA Press, 201.   \n[24] Chen Feng and Ioannis Patras. Adaptive soft contrastive learning. In 2022 26th International Conference on Pattern Recognition (ICPR), pages 2721\u20132727. IEEE, 2022.   \n[25] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[26] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.   \n[27] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems, 29, 2016.   \n[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[29] Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. Advances in Neural Information Processing Systems, 33:9995\u201310006, 2020.   \n[30] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[31] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daum\u00e9 III, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In Proceedings of the 2019 CHI conference on human factors in computing systems, pages 1\u201316, 2019.   \n[32] Keli Huang, Botian Shi, Xiang Li, Xin Li, Siyuan Huang, and Yikang Li. Multi-modal sensor fusion for auto driving perception: A survey. CoRR, abs/2202.02703, 2022.   \n[33] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.   \n[34] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):4037\u20134058, 2020.   \n[35] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:21798\u201321809, 2020.   \n[36] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT, 2019.   \n[37] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 7241\u20137259. Association for Computational Linguistics, 2022.   \n[38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, pages 19730\u201319742. PMLR, 2023.   \n[39] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:9694\u20139705, 2021.   \n[40] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX 16, pages 121\u2013137. Springer, 2020.   \n[41] Paul Pu Liang, Zihao Deng, Martin Q Ma, James Y Zou, Louis-Philippe Morency, and Ruslan Salakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740\u2013755. Springer, 2014.   \n[43] Yijie Lin, Yuanbiao Gou, Xiaotian Liu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Dual contrastive prediction for incomplete multi-view representation learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4447\u20134461, 2022.   \n[44] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23592\u201323601, 2023.   \n[45] Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, and Tarek Abdelzaher. Focal: Contrastive learning for multimodal time-series sensing signals in factorized orthogonal latent space. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13\u201323, 2019.   \n[47] Jie Ma, Chuan Wang, Yang Liu, Liang Lin, and Guanbin Li. Enhanced soft label for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1185\u20131195, 2023.   \n[48] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1\u201335, 2021.   \n[49] Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO. In Paola Merlo, J\u00f6rg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 2855\u20132870. Association for Computational Linguistics, 2021.   \n[50] Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S Melo, Ana Paiva, and Danica Kragic. Geometric multimodal contrastive representation learning. In International Conference on Machine Learning, pages 17782\u201317800. PMLR, 2022.   \n[51] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171\u20135180. PMLR, 2019.   \n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[54] Valentin Radu, Nicholas D Lane, Sourav Bhattacharya, Cecilia Mascolo, Mahesh K Marina, and Fahim Kawsar. Towards multimodal deep learning for activity recognition on mobile devices. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, pages 185\u2013188, 2016.   \n[55] Ishaan Singh Rawal, Alexander Matyasko, Shantanu Jaiswal, Basura Fernando, and Cheston Tan. Dissecting multimodality in videoqa transformer models by impairing modality fusion. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024.   \n[56] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? Advances in Neural Information Processing Systems, 34:4974\u20134986, 2021.   \n[57] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? Advances in Neural Information Processing Systems, 34:4974\u20134986, 2021.   \n[58] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 4974\u20134986, 2021.   \n[59] Eduardo Hugo Sanchez, Mathieu Serrurier, and Mathias Ortner. Learning disentangled representations via mutual information estimation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16, pages 205\u2013221. Springer, 2020.   \n[60] Yuanjun Shi, Linzhi Wu, and Minglai Shao. Adaptive end-to-end metric learning for zero-shot crossdomain slot filling. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6291\u20136301, 2023.   \n[61] Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi. Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis. IEEE Journal of Biomedical and Health Informatics, 22(5):1589\u20131604, 2017.   \n[62] Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin Taylor. Layer-specific adaptive learning rates for deep networks. In 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA), pages 364\u2013368. IEEE, 2015.   \n[63] Karthik Sridharan and Sham M. Kakade. An information theoretic framework for multi-view learning. In Rocco A. Servedio and Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008, pages 403\u2013414. Omnipress, 2008.   \n[64] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[65] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? Advances in Neural Information Processing Systems, 33:6827\u20136839, 2020.   \n[66] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In Algorithmic Learning Theory, pages 1179\u20131206. PMLR, 2021.   \n[67] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised learning from a multi-view perspective. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[68] Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[69] George Tzanetakis. Automatic musical genre classification of audio signals. In ISMIR 2001, 2nd International Symposium on Music Information Retrieval, Indiana University, Bloomington, Indiana, USA, October 15-17, 2001, Proceedings, 2001.   \n[70] A\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.   \n[71] Michael Veale, Max Van Kleek, and Reuben Binns. Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making. In Proceedings of the 2018 chi conference on human factors in computing systems, pages 1\u201314, 2018.   \n[72] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. Trans. Mach. Learn. Res., 2022, 2022.   \n[73] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.   \n[74] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. CoRR, abs/2208.10442, 2022.   \n[75] Tete Xiao, Xiaolong Wang, Alexei A. Efros, and Trevor Darrell. What should not be contrastive in contrastive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[76] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. CoRR, abs/1304.5634, 2013.   \n[77] Jie Xu, Shuo Chen, Yazhou Ren, Xiaoshuang Shi, Hengtao Shen, Gang Niu, and Xiaofeng Zhu. Selfweighted contrastive learning among multiple views for mitigating representation degeneration. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[78] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Languageaware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18155\u201318165, 2022.   \n[79] Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, and Yann LeCun. Decoupled contrastive learning. In European Conference on Computer Vision, pages 668\u2013684. Springer, 2022.   \n[80] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Trans. Assoc. Comput. Linguistics, 2:67\u201378, 2014.   \n[81] Olaf Zawacki-Richter, Victoria I Mar\u00edn, Melissa Bond, and Franziska Gouverneur. Systematic review of research on artificial intelligence applications in higher education\u2013where are the educators? International Journal of Educational Technology in Higher Education, 16(1):1\u201327, 2019.   \n[82] Changqing Zhang, Zongbo Han, Huazhu Fu, Joey Tianyi Zhou, Qinghua Hu, et al. Cpm-nets: Cross partial multi-view networks. Advances in Neural Information Processing Systems, 32, 2019.   \n[83] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X. Pham, Chang D. Yoo, and In So Kweon. How does simsiam avoid collapse without negative samples? A unified understanding with self-supervised contrastive learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[84] Linfeng Zhang, Xin Chen, Junbo Zhang, Runpei Dong, and Kaisheng Ma. Contrastive deep supervision. In European Conference on Computer Vision, pages 1\u201319. Springer, 2022.   \n[85] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10306\u201310315, 2021. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Broder Impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Multimodal contrastive learning, as an emerging learning paradigm, enhances the model\u2019s performance in understanding and processing multimodal data by establishing connections and comparisons between multiple modalities. With the increasing demand for processing multimodal data, widespread application of large models, and rapid development of embodied intelligence, the application of multimodal contrastive learning in the real world requires more consideration of safety and application. ", "page_idx": 15}, {"type": "text", "text": "Safety Privacy and data security [5] have exacerbated concerns in the context of sensitive data processing, such as facial recognition and personal identification information in multimodal learning. We adopted open-source datasets to avoid incorporating private information into the training process as much as possible. Concurrently, the training of multimodal algorithms often exhibits biases [48, 27, 14] that can contribute to discrimination or unfair treatment of certain groups, reducing the fairness and inclusivity. Our approach may require bias detection or the application of bias reduction methods [71] in real-world applications to ensure that multimodal algorithms maintain unbiased ethics. ", "page_idx": 15}, {"type": "text", "text": "Application Multimodal learning, particularly when integrated into embodied intelligence, has farreaching impacts across various domains. In healthcare [22, 61], it enhances advanced diagnostic tools and personalized treatments, which also requires specific measures to assure patient privacy and data security. In education [81, 31], multimodal AI can personalize learning experiences, potentially reducing educational disparities, which requires an unbiased AI. Embodied intelligence [53] in social interaction facilitates accessibility and provides companionship for vulnerable populations, especially as pre-training then fine-tuning becomes the mainstream training paradigm, our method helps models learn more modal-independent unique information; unlike models such as CLIP, which focus too much on shared information. ", "page_idx": 15}, {"type": "text", "text": "Future work Our research mainly focuses on extracting unique information in modalities from different views. In terms of model architecture, we have implemented a unique decoder to extract unique information and train the model with the quadruple loss with constraints. Our approach and Bleeker [3] et.al both adopt the supervised learning method. Future work could explore extracting unique information through self-supervised learning (SSL) approaches. One potential direction for this exploration involves augmenting the input data. Liang et al. [41] proposed FactorCL to factorize information into shared information and unique information by augmentation to implement SSL. Furthermore, in the era of large language models (LLM), leveraging the powerful capabilities of LLMs to explicitly extract existing data to unique and shared information between different views, as shown in Fig 6. By combining this with the original data, training can be conducted on this explicitly separated information for enhanced model performance. ", "page_idx": 15}, {"type": "image", "img_path": "lA48H7pW3q/tmp/0870334a82fcc44e38246a2fa5446687cf3141838cb2c47ef2007715ab0c2860.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Shared and unique Information in Multimodal Multi-view Scenario. A single image can be described from multiple viewpoints, each containing shared information and distinctive details unique to the specific perspective. ", "page_idx": 15}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Flickr30k is a benchmark commonly used in computer vision (CV) and natural language processing (NLP), The tasks applicable to this dataset involving image-caption and multimodal learning, like image understanding, visual question answering and generating text descriptions for images. Flickr30k contains 31783 images collected from the Flickr platform, each with five corresponding descriptive captions, totalling 158,915 captions. ", "page_idx": 16}, {"type": "text", "text": "Microsoft Common Objects in Context (MS-COCO) is a large-scale dataset containing over 328000 images, and each image is paired with at least five detailed captions annotated by humans for other CV tasks like segmentation; MS-COCO also provides segmentation masks, key points and relationships between objects. The images within the MS-COCO cover a multitude of object categories, activities, and scenes, representing a broad spectrum of settings and contexts. ", "page_idx": 16}, {"type": "text", "text": "Free Music Archive (FMA) is an extensive, open-access dataset designed for Music Information Retrieval (MIR) research, encompassing 917 GiB of audio data (equivalent to 343 days of playback) from 106,574 Creative Commons-licensed tracks. This diverse collection, spanning 16,341 artists, 14,854 albums, and 161 hierarchically organized genres, provides researchers with high-quality, full-length audio flies, pre-computed features, and rich metadata including track and user information, tags, and textual descriptions. FMA\u2019s comprehensive nature makes it ideal for various MIR tasks such as genre classification, artist identification, and music recommendation, while its predefined train/validation/test splits and subsets of varying sizes facilitate reproducible research and benchmarking in the field. Code, data, and usage examples are available at https://github.com/mdeff/fma. ", "page_idx": 16}, {"type": "text", "text": "GTZAN is a benchmark dataset widely used in Music Information Retrieval (MIR) and audio signal processing research, particularly for tasks involving musical genre classification and audio feature extraction. The dataset comprises 1,000 audio excerpts, each 30 seconds in duration, equally distributed across 10 distinct musical genres: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock. All audio samples in GTZAN are standardized to $22{,}050\\ \\mathrm{Hz}$ sampling rate, mono channel, and 16-bit resolution in WAV format, facilitating consistent analysis and algorithm development. Despite some noted limitations, GTZAN remains a valuable resource for evaluating and comparing various approaches in automatic music genre recognition and related MIR tasks. ", "page_idx": 16}, {"type": "text", "text": "Clotho is a diverse audio captioning dataset comprising 4981 audio samples (15-30 seconds each) from Freesound, paired with 24,905 crowdsourced captions (8-20 words each). Designed to facilitate general audio content description using free text, Clotho emphasizes perceptual diversity by providing multiple captions per audio and excluding visual or contextual cues during annotation. The dataset\u2019s post-processing, including removal of unique words and speech transcription, enhances its suitability for developing and evaluating audio captioning systems. ", "page_idx": 16}, {"type": "text", "text": "AudioCaps is a seminal dataset for audio captioning, comprising 46,000 audio clips from AudioSet with human-authored descriptions. This large-scale corpus has become the benchmark for evaluating audio captioning models, catalyzing advancements in audio representation and multimodal learning. AudioCaps has facilitated the development of innovative architectures such as the Audio Captioning Transformer and retrieval-augmented models, significantly contributing to audio-language research. Its impact extends beyond captioning, influencing broader studies in audio-visual integration and inspiring more comprehensive datasets in the field. ", "page_idx": 16}, {"type": "text", "text": "B.2 Experimental Settings ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "lA48H7pW3q/tmp/339109e3f00e6f58f5589783a0b56f335e15872b39f78872c313885828e0d469.jpg", "table_caption": ["Table 3: Multimodal Model Training Details. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Detailed training settings In our experiments, we conducted a comprehensive comparison of multimodal model training strategies using the Flickr30k and MS-COCO datasets. We used $\\mathrm{VSE++}$ and CLIP, each employing distinct training configurations tailored to their respective architectures and optimization requirements. ", "page_idx": 16}, {"type": "text", "text": "For the $\\mathrm{VSE++}$ model, we utilized a training regime consisting of 30 epochs with a batch size of 128. The optimization process was driven by the Adam optimizer with a learning rate set at $2\\times10^{-4}$ . Notably, no warmup steps were employed, and the learning rate was adjusted using a stepLR scheduler. This configuration was consistently applied across both the Flickr30k and MS-COCO datasets. The visual encoder for $\\mathrm{VSE++}$ was based on ResNet152, while the text encoding was handled by a GRU, resulting in a total parameter count of 67 million. ", "page_idx": 17}, {"type": "text", "text": "Moreover, the CLIP model was trained over five epochs with a larger batch size of 256. Optimization was performed using the AdamW optimizer with a significantly lower learning rate of $2\\stackrel{\\cdot}{\\times}10^{-5}$ . In contrast to $\\mathrm{VSE++}$ , CLIP incorporated 100 warmup steps and utilized a cosine annealing scheduler for learning rate adjustment. This setup was also uniformly applied to both datasets. The visual encoder for CLIP was a ViT-B/32, and text encoding was managed by a Transformer, leading to a substantially larger parameter count of 152 million. ", "page_idx": 17}, {"type": "text", "text": "Unique Decoder Implementation Details We used CLIP(ViT/B-32 backbone) and $\\mathrm{VSE++}$ (resnet152 backbone); we chose a single-layer MLP for $\\mathrm{VSe}{+}{+}$ and two-layer Transformers for CLIP. Both of them use nine layers as a unique start layer and 0 for no unique start layer. We choose the hyperparameters alpha_ $\\textsl{t}$ as 0.08 on most experiments and set positive_sample to false. ", "page_idx": 17}, {"type": "text", "text": "B.3 More Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The ${\\mathcal{L}}_{\\mathrm{UIC}}$ algorithm enhances contrastive loss in representation learning through a novel embedding space involving quadruplets. This approach primarily comprises two stages: the computation of a similarity map from quadruple embeddings and the subsequent calculation of cross-entropy loss based on this map. The method leverages shared and unique embeddings to construct expressive feature representations, further augmented by three-dimensional vectors generated via cross-products, thereby intensifying the discriminative power of embeddings. ", "page_idx": 17}, {"type": "text", "text": "As shown in 1, the algorithm firstly processes four embedding vectors:xshared, $x_{\\mathrm{unique}}$ , yshared, and $y_{\\mathrm{unique}}$ , representing shared and unique features across two data sets. Padding may be applied to ensure dimensional consistency (Divisible by 3) across embeddings. The GETSIMMAP function, shown in 2 then calculates the similarity map between $\\boldsymbol{x}_{\\mathrm{shared}}$ and $y_{\\mathrm{shared}}$ , reflecting the similarity between shared feature embeddings. This computation involves normalizing the embeddings, obtaining a preliminary similarity matrix via dot products, and adjusting the similarity values through exponential weighting and diagonal normalization. ", "page_idx": 17}, {"type": "text", "text": "Upon obtaining the similarity map, the algorithm transforms $x_{\\mathrm{shared}}$ and $x_{\\mathrm{{unique}}}$ (along with their $y$ counterparts) into three-dimensional vectors, or triplet embeddings, via cross products. This geometric transformation aims to further enhance the distinctiveness of embeddings. The triplet embeddings are then normalized to ensure numerical stability during dot product calculations. Ultimately, the algorithm calculates the dot product of $x_{\\mathrm{{triplet}}}$ and $y_{\\mathrm{triplet}}$ , scales the result, and applies element-wise multiplication with the similarity map to produce the final logits. These logits, after scaling and absolute value adjustments, are used to compute the cross-entropy loss. ", "page_idx": 17}, {"type": "text", "text": "We also use orthogonal cosine embedding loss 3 to quantify the similarity between pairs of input vectors, facilitating the training of models on tasks that distinguish between similar and dissimilar data points. The objective of this loss computation is to minimize the discrepancy between predicted logits and actual labels, thereby optimizing the representational capacity of the embeddings. ", "page_idx": 17}, {"type": "text", "text": "The synthetic shortcuts experiment enables a quantitative analysis of the model\u2019s reliance on shortcuts when they are present and its ability to capture shared and task-relevant unique information. The results demonstrate that our proposed method effectively mitigates the feature suppression phenomenon, contributing to improved performance on downstream tasks compared to previous approaches. ", "page_idx": 17}, {"type": "text", "text": "Generalization Capability Across Multiple Modalities To evaluate the generalization capability of our proposed approach on more modalities, we conducted comprehensive experiments across various modalities, including visual, textual, and acoustic domains. Specifically, we performed image-audio retrieval experiments utilizing the FMA (Free Music Archive) and GTZAN datasets, with quantitative results presented in Table 4. For text-audio retrieval evaluations, we leveraged the CLOTHO and AUDIOCAPS datasets, with comparative performance metrics detailed in Table 5. ", "page_idx": 17}, {"type": "text", "text": "The empirical results demonstrate that our proposed models consistently outperformed the baseline approaches across all experimental configurations. For simplicity, we employed the almost simplest ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 LUIC loss calculation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Require: Latent representation xshared, xunique, yshared, yunique.   \nEnsure: $\\mathcal{L}_{\\mathrm{UIC}}$   \n1: function CALCULATE LUIC(xshared, xunique, yshared, yunique)   \n2: $B,C\\gets$ shape of xshared   \n3: if padding is enabled in config then   \n4: $\\begin{array}{r l}&{\\ a d\\_s i z e\\leftarrow(3-C\\ \\mathrm{~mod~3})\\ \\mathrm{~mod~3}}\\\\ &{\\ x_{s h a r e d}\\leftarrow\\mathrm{pad}\\ x_{s h a r e d}\\mathrm{~with~}p a d\\_s i z e}\\\\ &{\\ x_{u n i q u e}\\leftarrow\\mathrm{pad}\\ x_{u n i q u e}\\mathrm{~with~}p a d\\_s i z e}\\\\ &{\\ y_{s h a r e d}\\leftarrow\\mathrm{pad}\\ y_{s h a r e d}\\mathrm{~with~}p a d\\_s i z e}\\\\ &{\\ y_{u n i q u e}\\leftarrow\\mathrm{pad}\\ y_{u n i q u e}\\mathrm{~with~}p a d\\_s i z e}\\end{array}$   \n5:   \n6:   \n7:   \n8:   \n9: end if   \n10: s $\\therefore m_{-}m a p\\gets\\mathrm{GETSIMMAP}(x_{s h a r e d},y_{s h a r e d})$   \n11: mini_heads $\\leftarrow$ integer division of $C$ by 3   \n12: participated_dims $\\leftarrow$ mini_heads $\\times\\,3$   \n13: $x_{s h a r e d}\\leftarrow$ reshape xshared[:, : participated_dims] to $(B,-1,3)$   \n14: $x_{u n i q u e}\\leftarrow$ reshape $x_{u n i q u e}[:,:$ : participated_dims] to $(B,-1,3)$   \n15: yshared $\\leftarrow$ reshape yshared[:, : participated_dims] to $(B,-1,3)$   \n16: yunique $\\leftarrow$ reshape yunique[:, : participated_dims] to $(B,-1,3)$   \n17: $x_{u i c}\\gets$ cross product of $x_{s h a r e d}$ and $x_{u n i q u e}$ along dimension 2, then reshape to $(B,-1)$   \n18: $y_{u i c}\\leftarrow$ cross product of $y_{s h a r e d}$ and $y_{u n i q u e}$ along dimension 2, then reshape to $(B,-1)$   \n19: $x_{u i c}\\gets$ normalize $x_{u i c}$ along the last dimension   \n20: $y_{u i c}\\leftarrow$ normalize $y_{u i c}$ along the last dimension   \n21: logits \u2190xuic \u00d7 yuTic   \n22: logits $\\leftarrow$ absolute value of logits   \n23: logi $s\\gets s c a l e\\times l o g i t s\\times s i m\\_m a p$   \n24: num_logit $s\\leftarrow B$   \n25: $l a b e l s\\leftarrow$ range from 0 to num_logits \u22121   \n26: $\\mathcal{L}_{\\mathrm{QUAD}}\\leftarrow\\mathrm{CE}$ loss of logits with labels $+\\,\\mathrm{CE}$ loss of logitsT with labels   \n27: $\\mathcal{L}_{\\mathrm{cos}}\\gets\\mathrm{GETCOSLOSS}(y_{s h a r e d},y_{u n i q u e})+\\mathrm{GETCOSLOSS}(x_{s h a r e d},x_{u n i q u e})$   \n28: $\\mathcal{L}_{\\mathrm{UIC}}\\leftarrow\\mathcal{L}_{\\mathrm{QUAD}}+\\mathcal{L}_{\\mathrm{cos}}$   \n29: return LUIC   \n30: end function ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Calculate similarity map ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Require: latent representations $x$ , $y$   \nEnsure: similarity map sim_map   \n1: function $\\mathrm{GETSIMMAP}(x,y)$   \n2: $x\\gets$ normalize $x$ along the last dimension   \n3: $y\\leftarrow$ normalize $y$ along the last dimension   \n4: sim $\\therefore m a p\\gets x\\times y^{T}$   \n5: sim_map $\\leftarrow$ clamp sim_map between 0 and 1   \n6: sim_ $m a p\\gets\\exp(s i m\\_m a p)$   \n7: fill the diagonal of sim_map with 1   \n8: return sim_map   \n9: end function   \nRequire: unique_information $x_{\\mathrm{unique}}$ , shared_information xshared   \nEnsure: $\\mathcal{L}_{\\mathrm{cos}}$   \n1: function GETCOSLOSS(xunique, xshared)   \n2: $\\epsilon\\gets1e-12$   \n3: product ${}_{-}s u m\\gets(x_{\\mathrm{unique}},x_{\\mathrm{shared}}).\\mathrm{sum}(d i m=1)$   \n4: matnitud $z_{-}s q u a r e_{1}\\gets(x_{\\mathrm{unique}},x_{\\mathrm{unique}}).\\mathrm{sum}(d i m=1)+\\epsilon$   \n5: matnitude $\\dots s q u a r e_{2}\\gets(x_{\\mathrm{shared}},x_{\\mathrm{shared}}).\\mathrm{sum}(d i m=1)+\\epsilon$   \n6: denominator $\\cdot\\gets\\sqrt{m a g\\_s q u a r e1\\times m a g\\_s q u a r e2}$   \n7: $c o s\\gets$ product_sum/denominator   \n8: $z e r o s\\leftarrow$ tensor of zeros with same shape as $x_{\\mathrm{{unique}}}$ along dimension 0   \n9: $p o s\\_l o s s\\gets1-c o s$   \n10: $n e g\\_l o s s\\gets\\mathrm{clamp}(\\left|c o s-0.0\\right|,\\mathrm{min}=0)$   \n11: target $\\leftarrow$ tensor of $^ \u1e0a -1 \u1e0c$ with same shape as $x_{\\mathrm{unique}}$ along dimension 0   \n12: Initialize loss_pos and loss_neg with the same shape as target, pos_loss, and zeros   \n13: for each index $i$ in target do   \n14: if target[i] is 1 then   \n15: $l o s s\\_p o s[i]\\gets p o s\\_l o s s[i]$   \n16: else   \n17: $l o s s\\_p o s[i]\\gets z e r o s[i]$   \n18: end if   \n19: if target[i] is -1 then   \n20: $l o s s\\_\\dot{n}e g[i]\\gets n e g\\_l o s s[i]$   \n21: else   \n22: $l o s s\\_n e g[i]\\gets z e r o s[i]$   \n23: end if   \n24: end for   \n25: $l o s s\\gets l o s s\\_p o s+l o s s\\_n e g$   \n26: return mean of loss   \n27: end function ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "lA48H7pW3q/tmp/b3ef88ed782481b62bbebffd09b6cfcc1084464570e3c898b43a86e62218a8a9.jpg", "table_caption": ["Table 4: Image audio retrieval results on FMA and GTZAN datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "decoder structure (linear layers) and did not implement modality fusion or any cross-modal interactions. We believe that enhancing the architecture, strengthening cross-modal interactions, employing a larger batch size and incorporating pre-training would yield even higher performance. ", "page_idx": 19}, {"type": "text", "text": "Compute Resources. All experiments in this paper are run on a single NVIDIA A100 GPU. The implementation is based on PyTorch 2.0.1. It takes about 2 hours to train the CLIP-based model on Flickr30K for 5 epochs, and the maximum training time for other experiments does not exceed 12 GPU hours. ", "page_idx": 19}, {"type": "table", "img_path": "lA48H7pW3q/tmp/e5bf3483904de85bad4bf94617b8109e1bda1aadc1c65df24d64e620eb5c138e.jpg", "table_caption": ["Table 5: Text audio retrieval results on CLOTHO and AUDIOCAPS datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Notation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 6: Notation used in the paper. ", "page_idx": 21}, {"type": "text", "text": "Symbol Description   \n$\\mathcal{M}_{i}$ Modality i   \n$\\boldsymbol{x}_{i}^{j}$ The $i$ th sample(view) of modality $j$   \nHjMi Representations as the encoder\u2019s output   \n$\\mathbf{Z}_{i}^{\\mathrm{s}}$ and $\\mathbf{Z}_{i}^{\\mathbf{u}}$ denote the decoder\u2019s output representations for shared and unique $\\mathbf{Z}_{i}^{\\mathbf{s}},\\mathbf{Z}_{i}^{\\mathbf{u}}$ information, respectively.   \n$\\mathbf{Z}_{i}^{\\mathbf{n}}$ The cross product of $\\mathbf{Z}_{i}^{\\mathrm{s}}$ and $\\mathbf{Z}_{i}^{\\mathbf{u}}$   \nLInfoNCE InfoNCE loss   \nLInfoNCE+LTD Loss that combines InfoNCE and LTD   \nLInfoNCE+IFM Loss that combines InfoNCE and IFM   \nLSIC Shared information constraint loss   \nLUIC Unique information constraint loss   \nLP-UIC Penalized LUIC   \nL P-UIC Reformulated LP-UIC, ignore second item   \nLcos Orthogonalized cosine loss   \nLQUEST Overall Quardruple InfoNCE Loss   \nS The similarity matrix between shared embedding   \n$\\mathcal{P}$ The weighted similarity matrix between shared embedding $\\mathbf{Z}_{i}^{\\mathbf{s}}$ and $\\mathbf{Z}_{j}^{\\mathbf{s}}$   \n$\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ Batch of image-caption pairs   \n$\\tau$ temperature coefficient   \n$X_{A}$ random variables from modility $\\mathcal{M}_{A}$   \n$\\mathcal{P}$ the weighted similarity matrix, utilized as penalization   \nitMs oldataelintty $\\mathcal{M}$ encoder parametrised by resentation $\\Theta$ akes sample $X_{\\mathcal{M}}^{j}$ as input and returns $\\mathcal{F}_{\\mathcal{M}}(\\cdot;\\Theta)$   \nrep $\\mathbf{H}_{\\mathcal{M}}^{j}=\\mathcal{F}_{\\mathcal{M}}(x^{j};\\theta)$   \nrMetoudranlsi tiyts $\\mathcal{M}$ ednetc roedperre speanrtaamtieotnr $\\phi$ sentations $\\mathbf{H}_{\\mathcal{M}}^{j}$ as input and $\\mathcal{G}_{\\mathcal{M}}(\\cdot;\\Phi)$   \n$\\mathbf{Z}_{\\mathcal{M}}^{j}=\\mathcal{G}_{\\mathcal{M}}(\\mathbf{H}_{\\mathcal{M}}^{j};\\boldsymbol{\\Phi})$   \n$s(\\cdot,\\cdot)$ Scoring function   \n$H(Z_{j}|Z_{i})$ The conditional entropy of $Z_{j}$ given $Z_{i}$   \nH P\u02dc (Zj|Zi) Conditional entropy under the penalized term $\\tilde{P}$   \n$I(Z_{i},Z_{j})$ Mutual information between $Z_{i}$ and $Z_{j}$ ", "page_idx": 21}, {"type": "text", "text": "D Analysis. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Mutual Information ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In many machine learning tasks, it is often observed that different views simultaneously harbour both task-relevant shared information and unique information (e.g., image captioning [38, 40, 72] and referring expression segmentation [78, 44]). To this end, we work with a dual-encoder architecture. ", "page_idx": 22}, {"type": "text", "text": "Contrastive learning with multiple views obtains mutual information between different modalities by learning the similarity among different views. [70] et al. introduced a lower bound on mutual information, known as InfoNCE, which is based on the concept of Noise Contrastive Estimation (NCE); it compares the compatibility of different views by maximizing the mutual information of positive pairs and minimizing the mutual information of negative pairs, learning to extract the consistent representation across different modal. It can be defined as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nI_{\\mathrm{InfoNCE}}(X;Y)=\\mathbb{E}_{(x,y)\\sim p(x,y)}\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\log\\frac{\\exp f(x_{i},y_{i})}{\\frac{1}{N}\\sum_{j=1}^{N}\\exp f(x_{i},y_{j})}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In this equation, $x_{i}$ and $y_{i}$ represent paired samples from the joint distribution of the two random variables under consideration, while $y_{j}$ represents samples from the marginal distribution of one of the variables. The function $f(x,y)$ is a learnable function, often parameterized by a neural network, that aims to distinguish between the paired samples and the independently sampled ones. ", "page_idx": 22}, {"type": "text", "text": "In the realm of vison-language contrastive learning, the InfoNCE loss function [70] enhances the similarity between positive sample pairs relative to negative ones. Mathematically, the InfoNCE loss is articulated as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\log\\frac{\\exp(\\sin(x,y^{+})/\\tau)}{\\sum_{i=0}^{K}\\exp(\\sin(x,y_{i})/\\tau)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\sin(x,y)$ calculates the similarity between samples $x$ and $y$ , typically computed through dot product or cosine similarity. $y^{+}$ denotes the positive sample corresponding to $x$ , and $\\{y_{i}\\}_{i=0}^{K}$ is a set including one positive and $K$ negative samples. The parameter $\\tau$ serves as a temperature coefficient, modulating the scale of similarity scores. ", "page_idx": 22}, {"type": "text", "text": "InfoNCE loss prevails in many contrastive learning algorithms and performs well in applications. Multi-view redundancy [66, 2, 30, 63, 65, 67] assumes that there exists duplicated information across varied views or representations. Contrastive losses like InfoNCE loss tend to maximize the mutual features between different views while suppressing task-relevant features that may be used in downstream tasks in multi-view representation learning. ", "page_idx": 22}, {"type": "text", "text": "Mutual information (MI), often denoted as $I(X;Y)$ , is a fundamental concept in information theory that quantifies the statistical dependence between two random variables, $X$ and $Y$ . It measures the reduction in uncertainty about one variable when the value of the other is known and is defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nI(x;y)=\\mathbb{E}_{(x,y)\\sim p(x,y)}[\\log\\frac{p(x,y)}{p(x)p(y)}]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $p(x,y)$ is the joint distribution, and $p(x)$ and $p(y)$ are the marginal distributions of $X$ and $Y$ ", "page_idx": 22}, {"type": "text", "text": "In machine learning, particularly in deep learning, MI is often used as an objective function or regularization component to promote or constrain the interdependence among variables. However, the precise quantification of MI is only attainable in limited instances, as it requires the closed form of the density function and the tractable logarithm density ratio between the joint and marginal distributions. In most machine learning applications, practitioners only have access to samples from the joint distribution, making the direct computation of MI infeasible. ", "page_idx": 22}, {"type": "text", "text": "The InfoNCE bound has several desirable properties that make it an attractive choice for estimating mutual information. Firstly, it is a lower bound on the true mutual information, $I(X;Y)\\ge I_{\\mathrm{NCE}}$ , which means that maximizing the InfoNCE bound leads to an increase in the true mutual information. ", "page_idx": 22}, {"type": "text", "text": "Secondly, the bound is computationally tractable and can be efficiently optimized using standard gradient-based methods, such as stochastic gradient descent. ", "page_idx": 23}, {"type": "text", "text": "As shown in Fig 2, multimodal mutual information can be divided into shared information $I(X_{A};X_{B};Y)$ and task-relevant unique information $(I(x_{A};Y|X_{B})$ and $I(X_{B};Y|X_{A}))$ . Information useful for the task can be represented as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\tau}=H(Y)-H(Y|X_{A},X_{B})}\\\\ &{\\quad=I(X_{A};X_{B};Y)+I(X_{A};Y|X_{B})+I(X_{B};Y|X_{A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2 Gradient Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the simplified loss function Eq. (9), we first calculate the partial derivative of $\\mathbf{Z}_{i}^{\\mathbf{s}}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\widetilde{\\mathcal{L}}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}=\\frac{1}{\\lambda}\\frac{\\partial\\lambda}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}-\\frac{\\mathcal{P}^{+}}{\\tau}\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $s^{+}=s(\\mathbf{Z}_{i}^{\\mathrm{n}},\\mathbf{Z}_{j}^{\\mathrm{n}^{+}})$ . Define $s^{k}=s(\\mathbf{Z}_{i}^{\\mathbf{n}},\\mathbf{Z}_{j k}^{\\mathbf{n}^{-}})$ , then we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\tilde{\\mathcal{L}}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}=\\frac{1}{\\lambda}\\left[\\exp\\left(\\frac{\\mathcal{P}^{+}s^{+}}{\\tau}\\right)\\cdot\\frac{\\mathcal{P}^{+}}{\\tau}\\cdot\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}+\\sum_{k=1}^{m}\\exp\\left(\\frac{\\mathcal{P}_{k}s^{k}}{\\tau}\\right)\\cdot\\frac{\\mathcal{P}_{k}}{\\tau}\\cdot\\frac{\\partial s^{k}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}\\right]-\\frac{\\mathcal{P}^{+}}{\\tau}\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}}\\\\ &{\\displaystyle\\qquad=-\\frac{\\mathcal{P}^{+}}{\\tau}\\left(1-\\frac{\\exp\\left(\\frac{\\mathcal{P}^{+}s^{+}}{\\tau}\\right)}{\\lambda}\\right)\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}+\\frac{1}{\\lambda\\tau}\\sum_{k=1}^{m}\\mathcal{P}_{k}\\exp\\left(\\frac{\\mathcal{P}_{k}s^{k}}{\\tau}\\right)\\frac{\\partial s^{k}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}}\\\\ &{\\displaystyle\\qquad=-\\frac{\\mathcal{P}^{+}}{\\tau}\\frac{\\partial s^{+}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}+\\frac{1}{\\lambda\\tau}\\sum_{k=0}^{m}\\mathcal{P}_{k}\\exp\\left(\\frac{\\mathcal{P}_{k}s^{k}}{\\tau}\\right)\\frac{\\partial s^{k}}{\\partial\\mathbf{Z}_{i}^{\\mathrm{n}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r c l}{\\lambda}&{=}&{\\sum_{k=0}^{m}\\exp(\\mathcal{P}\\cdot s(\\mathbf{Z}_{i}^{n},\\mathbf{Z}_{j k}^{n})/\\tau)}\\end{array}$ . For energy function $s(\\mathbf{Z}_{i},\\mathbf{Z}_{j})\\;\\;=\\;\\;{\\hat{\\mathbf{Z}}}_{i}\\,\\cdot\\,{\\hat{\\mathbf{Z}}}_{j}\\;\\;=$ $\\mathbf{Z}_{i}\\cdot\\mathbf{Z}_{j}/(\\lVert\\mathbf{Z}_{i}\\rVert\\lVert\\mathbf{Z}_{j}\\rVert)$ , the derived gradient on the vector $\\mathbf{Z}_{i}$ is shown as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial s(\\mathbf{Z}_{i},\\mathbf{Z}_{j})}{\\partial\\mathbf{Z}_{i}}=\\frac{1}{\\|\\mathbf{Z}_{i}\\|\\|\\mathbf{Z}_{j}\\|}\\left(\\mathbf{Z}_{j}-\\frac{\\mathbf{Z}_{i},\\mathbf{Z}_{j}}{(\\|\\mathbf{Z}_{i}\\|)^{2}}\\mathbf{Z}_{i}\\right)}\\\\ &{\\qquad\\qquad=\\frac{\\mathbf{Z}_{j}}{\\|\\mathbf{Z}_{i}\\|\\|\\mathbf{Z}_{j}\\|}-s(\\mathbf{Z}_{i}\\mathbf{Z}_{j})\\frac{\\mathbf{Z}_{i}}{\\|\\mathbf{Z}_{i}\\|^{2}}.}\\\\ &{\\qquad\\qquad=\\frac{1}{\\|\\mathbf{Z}_{i}\\|}(\\hat{\\mathbf{Z}}_{j}-s(\\mathbf{Z}_{i},\\mathbf{Z}_{j})\\hat{\\mathbf{Z}}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.3 Mutual Information Estimation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let $Z_{i}$ and $Z_{i}$ represent the compact vector representation from different modalities input; the classical mutual information between $Z_{i}$ and $Z_{j}$ can be defined as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nI(Z_{i},Z_{j})=\\sum_{Z_{i},Z_{j}}p(Z_{i},Z_{j})\\log\\frac{p(Z_{i},Z_{j})}{p(Z_{i})p(Z_{j})}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to [70], the density ratio $\\frac{p(Z_{i},Z_{j})}{p(Z_{i})p(Z_{j})}$ is expressed as $\\exp(s(Z_{i},Z_{j})/\\tau)$ , where $s(Z_{i},Z_{j})$ is the similarity score and $\\tau$ is the temperature parameter. The penalty term $\\mathcal{P}$ is defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}=\\exp(\\lambda[\\mathbf{Z}_{i}^{\\mathrm{s}}\\mathbf{Z}_{j}^{T}-\\mathrm{diag}(\\mathbf{Z}_{i}^{\\mathrm{s}}\\mathbf{Z}_{j}^{T})+\\mathbf{I}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The penalty term does not affect the numerator but amplifies the denominator in Eq. (8) thus the energy function in Eq. (8) can be reformulated as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\exp(s(Z_{i},Z_{j})/\\tau)\\cdot\\frac{1}{\\tilde{\\mathcal{P}}}=\\exp(s(Z_{i},Z_{j})/\\tau-\\log\\tilde{\\mathcal{P}})\\propto\\frac{\\tilde{p}(Z_{j}|Z_{i})}{p(Z_{j})}\\quad\\mathrm{where}\\quad\\tilde{P}\\propto\\mathcal{P}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we derive the lower bound of MI, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{C}_{\\mathrm{PUC}}=-\\mathrm{EcLe}\\left[\\frac{\\widetilde{F}(Z_{i}|\\widetilde{Z}_{i})}{\\widetilde{F}(Z_{i}|\\widetilde{Z}_{i})}\\frac{\\widetilde{F}(Z_{i}|Z_{i})}{\\widetilde{P}(Z_{i})}\\right]}\\\\ &{\\qquad=\\mathrm{EcLe}\\left[1+\\frac{p}{\\widetilde{F}(Z_{i}|\\widetilde{Z}_{i})}\\frac{\\widetilde{F}}{p!}\\frac{\\widetilde{F}(Z_{i}|Z_{i})}{p!}\\right]}\\\\ &{\\qquad\\approx\\mathrm{EcLe}\\left[1+\\frac{p}{\\widetilde{F}(Z_{i}|\\widetilde{Z}_{i})}\\frac{\\widetilde{F}}{p!}\\frac{\\widetilde{F}(Z_{i}|Z_{i})}{p!q!}\\right]}\\\\ &{\\qquad\\approx\\mathrm{EcLe}\\left[1+\\frac{p}{\\widetilde{F}(Z_{i}|Z_{i})}(N-1)\\mathbb{E}_{Z_{i}}\\frac{\\widetilde{F}(Z_{i}|Z_{i})}{p!}\\right],}\\\\ &{\\qquad=\\mathrm{EcLe}\\left[1+\\frac{p}{\\widetilde{F}(Z_{i}|Z_{i})}\\frac{\\widetilde{F}(Z_{i})}{p!}(N-1)\\right]}\\\\ &{\\qquad\\ge\\mathrm{EcLe}\\left[\\frac{p}{\\widetilde{F}(Z_{i}|Z_{i})}\\frac{\\widetilde{F}(Z_{i})}{p!}\\right]}\\\\ &{\\qquad=H^{\\widetilde{F}}(Z_{i}|Z_{i})-H(Z_{i})+\\log N}\\\\ &{\\qquad=H^{\\widetilde{F}}(Z_{i}|Z_{i})-I(Z_{i},Z_{i})-H(Z_{i}|Z_{i})+\\log N}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E Limitation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Dimension restrictions. The limitations of the cross-product in the context of QupleInfoNCE\u2019s unique information extraction are theoretically constrained. When contrastive learning is applied to both unique and shared representations, it can lead to model degradation, and a trade-off choice is the cross-product, which is a weaker constraint. In low-dimensional space, it is straightforward to prove the properties of orthogonal vector space, yet the extension of the cross-product to high-dimensional space is challenging. The behaviour of high-dimensional space representation is difficult to control, resulting in our theory lacking sufficient interpretability in high-dimensional space. This highlights the importance of developing more sophisticated methods for high-dimensional data analysis in the future. ", "page_idx": 24}, {"type": "text", "text": "The Extraction of Unique Information. On the synthetic shortcuts dataset, our framework achieved significant performance, validating the positive role of unique information in preventing model shortcuts. However, within the entire framework, the core lies in identifying unique information, which is essentially a task-dependent definition. Our experiment indicates that pre-trained models (such as CLIP [52]) typically yield higher unique information beneftis due to their extensive generic representations. Non-pre-trained models (such as $\\mathrm{VSE++}$ [23]) struggle to discern unique information, tending to shortcuts and losing vital unique information. In this study, we introduced an additional unique information decoder to capture unique information, which incorporated an extra gradient branch, suggesting exploration space remains for single-stream unique information extraction. This also proposes future research: how to more effectively extract unique information from a single stream to reduce computational complexity while maintaining model performance. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction are consistent with the contributions and scope detailed in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See Section 2 and Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix B.1 and Appendix B.2. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See the \"supplementary.zip\" file. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See Section 3 and Appendix B.1 Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See Section 3.2. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Appendix B.3. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Appendix A. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 28}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper have state which version of the asset is used. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Paper does not release new assets. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]