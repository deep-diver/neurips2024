[{"figure_path": "Ul3lDYo3XQ/tables/tables_3_1.jpg", "caption": "Table 1: Functions for an exemplary customer service QA agent. Among them, [Reflection] and [PredictAnswer] are trivial functions, as the executor passes control immediately back to the LLM to start generating result tokens.", "description": "This table lists the functions used by the customer service QA agent. Each function is designed to execute a set of operations, including memory I/O, tool usage, and interaction with the environment.  The functions are invoked by the LLM agent by predicting their names. The executor then interprets these instructions to activate the corresponding modules.  The table shows the function name and implementation details for a total of nine functions.", "section": "2.1 RL formulation of agent"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_4_1.jpg", "caption": "Table 2: An example of an information table for the headphones group.", "description": "This table shows an example of the information table that is created for each product group in the ProductQA dataset.  Each row represents a single product, listing its ID, title, price, brand, headphone type, cable type, audio transmission method, and audio output mode.  This table demonstrates the type of structured product data used to generate questions for the ProductQA benchmark. The full table includes information for multiple products in the headphone group.", "section": "3 The ProductQA dataset"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_4_2.jpg", "caption": "Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.", "description": "This table exemplifies the different question types included in the ProductQA dataset. Fact-QA involves straightforward factual questions about product features. Search-QA requires the agent to find suitable products based on user needs. Reasoning-QA demands more complex reasoning to answer questions, showcasing the dataset's comprehensive evaluation of diverse LLM agent capabilities.", "section": "3 The ProductQA dataset"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_6_1.jpg", "caption": "Table 4: Results on ProductQA. Here, X-prompt represents directly prompting model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents prompting or PPO training. We report results on short and long answers, respectively. The seeking advice cost is c = 0.3. Results are averaged over six test tasks. See Table 14 for individual product group performance.", "description": "This table presents the results of experiments conducted on the ProductQA dataset. It compares the performance of several methods: directly prompting GPT-3.5 and GPT-4, and using the AGILE framework with different LLMs (Vicuna) and training methods (prompting and PPO). The table shows the advice rate, accuracy (on both short and long answers), and total score for each method. The results are averaged across six test tasks from the ProductQA dataset, and detailed results for each task are available in another table.  The seeking advice cost is set to 0.3.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_7_1.jpg", "caption": "Table 5: Ablation studies for disabling reflection, memory, seeking advice, tool use, or RL training. Here, non-adapt-advice means that seeking advice is invoked for the first K sessions of the trajectory, where K equals to the number of [SeekAdvice] performed by agile-vic13b-ppo. See Table 15 for ablation results on individual product groups.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of individual components (reflection, memory, seeking advice, tool use) and RL training on the overall performance of the AGILE agent.  The \"non-adapt-advice\" condition forces the agent to seek advice for a specific number of initial sessions (the same number as the full model used the advice-seeking function), testing whether adaptive advice seeking is beneficial. The results are shown as the advice rate, accuracy, and total score, highlighting the relative importance of each component.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_7_2.jpg", "caption": "Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training. The seeking advice cost is c = 0.4.", "description": "This table presents the results of the MedMCQA experiment. It compares the performance of different models, including prompting GPT-3.5 and GPT-4 directly versus incorporating them into the AGILE framework with different components (reflection, memory, seeking advice) enabled or disabled.  The \"seeking advice cost\" (c) is set to 0.4.  The table shows the advice rate (percentage of times the agent sought advice), accuracy (percentage of correct answers), and total score (a combined metric of accuracy and advice cost). The table highlights the improved performance of the AGILE framework, particularly when using PPO training and all components.", "section": "4.3 Results on MedMCQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_8_1.jpg", "caption": "Table 7: Results on the HotPotQA full dev dataset. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training. The seeking advice cost is c = 0.3.", "description": "This table presents the results of the AGILE model and several baselines on the HotPotQA dataset.  It compares the performance of prompting GPT-4 directly (ReAct-gpt4-prompt) and the AGILE agent using Vicuna-13B as the LLM (agile-vic13b-ppo), with and without different components (advice-seeking and RL training). The table shows advice rate, accuracy (using both exact match and GPT-4 evaluation), and the total score, demonstrating the effectiveness of the AGILE framework.", "section": "4.4 Results on HotPotQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_8_2.jpg", "caption": "Table 8: Related work on LLM agents. AGILE stands out as the pioneering work that trains the entire agent using reinforcement learning, incorporating proactive human advice-seeking.", "description": "This table compares AGILE with other related works on Large Language Model (LLM) agents.  It highlights key features such as the specific LLM used, whether Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) were employed, the use of memory and tools, reflection capabilities, and the presence of proactive human-agent interaction.  The table shows that AGILE is unique in its end-to-end reinforcement learning approach and its incorporation of proactive human advice-seeking.", "section": "5 Related work"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_9_1.jpg", "caption": "Table 9: Benchmarks for evaluating LLM agents. ProductQA features long trajectories, tool use, long-term knowledge accumulation, and cross-task capabilities.", "description": "This table compares various existing benchmarks for evaluating LLM agents against the newly proposed ProductQA benchmark.  It highlights the unique features of ProductQA, such as its ability to evaluate agents' performance across long trajectories, tool usage, long-term knowledge retention, and multiple tasks. The table shows that while existing benchmarks assess some of these capabilities, none comprehensively evaluates all of them, underscoring ProductQA's novel contributions.", "section": "Related work"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_19_1.jpg", "caption": "Table 10: Robustness of RL training. Here, w/o RL represents the agent trained solely by imitation learning. agile-vic13b-ppo-X stands for the X-th RL experiment. The table presents the average and standard deviation across multiple RL training runs.", "description": "This table shows the results of multiple independent trials of Proximal Policy Optimization (PPO) training to evaluate the robustness of reinforcement learning (RL) in improving the agent's performance.  The \"w/o RL\" row represents the agent trained only using imitation learning, serving as a baseline. The other rows show the results of three separate PPO training runs (agile-vic13b-ppo-1, -2, -3). The table reports the average advice rate, accuracy, and total score across these runs, along with the standard deviation for each metric. The final column shows the relative improvement in the total score achieved by RL compared to the imitation-learning-only baseline.", "section": "C Supplementary experimental results on RL training"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_19_2.jpg", "caption": "Table 11: Improvement of PPO training. The training data for agile-vic13b-sft includes trajectories from GPT-4 agent. The training data for agile-vic13b-random is constructed by randomly assigning [SeekAdvice] to 25% of the data. agile-vic13b-ppo and agile-vic13b-ppo-random are initialized from agile-vic13b-sft and agile-vic13b-sft-random, respectively, and both are trained with PPO.", "description": "This table shows the results of experiments that investigate the impact of PPO training on the AGILE agent's performance under different scenarios.  The first two rows represent the baseline model trained with supervised fine-tuning (SFT) and then further optimized with PPO. The next two rows show results where the initial SFT training data includes a 25% random introduction of advice-seeking actions, demonstrating the model's ability to learn in more general scenarios. The last two rows demonstrate the impact of adjusting the cost of advice-seeking, revealing the agent's adaptive capabilities.", "section": "C.3 Impact of PPO training"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_20_1.jpg", "caption": "Table 12: Statistics of the ProductQA dataset. # Products indicates the number of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.", "description": "This table presents the statistical distribution of the ProductQA dataset.  It breaks down the dataset into 26 different product categories, indicating the number of products within each category and the count of questions categorized as Fact-QA (factual questions), Search-QA (questions requiring searches), and Reasoning-QA (questions that require higher-level reasoning). This provides insights into the dataset's composition and complexity.", "section": "3 The ProductQA dataset"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_20_2.jpg", "caption": "Table 13: Training statistics for each experiment.", "description": "This table presents the training time and computational resources used for the three tasks: ProductQA, MedMCQA, and HotPotQA.  It shows the number of NVIDIA H800 GPUs used and the training time for both supervised fine-tuning (SFT) and reinforcement learning (RL) for each task.  The information is valuable in understanding the computational cost and scaling aspects of the AGILE framework.", "section": "4 Experiments"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_21_1.jpg", "caption": "Table 14: Detail performance of our methods and other baselines on six test product groups of ProductQA. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting or PPO training. The Short and Long stand for the results evaluated on short answers and long answers, respectively. The seeking advice cost is c = 0.3. The best total scores are highlighted in bold.", "description": "This table provides a detailed comparison of the performance of various methods (direct prompting of GPT-3.5 and GPT-4, AGILE framework with GPT-3.5 and GPT-4, and AGILE framework with Vicuna-7B and Vicuna-13B using PPO) across six product categories within the ProductQA dataset.  The metrics used for comparison include advice rate, accuracy (for both short and long answers), and total score, with the best total score for each product category highlighted.  The seeking advice cost was set to 0.3 for all experiments.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_22_1.jpg", "caption": "Table 5: Ablation studies for disabling reflection, memory, seeking advice, tool use, or RL training. Here, non-adapt-advice means that seeking advice is invoked for the first K sessions of the trajectory, where K equals to the number of [SeekAdvice] performed by agile-vic13b-ppo. See Table 15 for ablation results on individual product groups.", "description": "This table presents the ablation study results for the AGILE model. It shows the impact of removing individual components (reflection, memory, advice-seeking, tools, and RL training) on the model's performance.  The \"Non-adapt advice\" row shows the results when advice-seeking is forced for only the first K sessions (where K is the number of advice-seeking actions taken by the full model).  The results are given in terms of advice rate, accuracy, and total score for six product groups. The table highlights the importance of each component for achieving strong performance.  Comparing the results for the full model to the ablation results shows the individual contributions of each module.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_22_2.jpg", "caption": "Table 16: Performance of the model (agile-vic13b-ppo) trained on different seeking advice cost settings.", "description": "This table shows the performance of the agile-vic13b-ppo model on ProductQA with different seeking advice costs (0.5, 0.4, 0.3, 0.2, and 0.1). For each cost, it presents the advice rate (the percentage of times the agent sought advice) and the accuracy (the percentage of correct answers). The results are broken down by product group (Camera Cases, Leggings, All Pans, Rollerball Pens, Motherboards, Rifle Scopes) and averaged across all groups.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_24_1.jpg", "caption": "Table 14: Detail performance of our methods and other baselines on six test product groups of ProductQA. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting or PPO training. The Short and Long stand for the results evaluated on short answers and long answers, respectively. The seeking advice cost is c = 0.3. The best total scores are highlighted in bold.", "description": "This table presents a detailed comparison of the performance of different methods (including the proposed AGILE framework and several baselines) on six product categories from the ProductQA dataset.  It shows the advice rate (percentage of times the agent sought advice), accuracy (percentage of correct answers), and the total score for both short and long answers. The comparison helps to evaluate the effectiveness of the proposed approach relative to baselines and the impact of various components (prompting, RL training, etc.).", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_25_1.jpg", "caption": "Table 14: Detail performance of our methods and other baselines on six test product groups of ProductQA. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting or PPO training. The Short and Long stand for the results evaluated on short answers and long answers, respectively. The seeking advice cost is c = 0.3. The best total scores are highlighted in bold.", "description": "This table presents a detailed comparison of the performance of different methods (direct prompting with GPT-3.5 and GPT-4, AGILE framework with GPT-3.5 and GPT-4, and AGILE with Vicuna-7B and Vicuna-13B) across six product categories from the ProductQA dataset.  It shows the advice rate, accuracy (for short and long answers), and overall total score for each method.  The best total scores are highlighted to easily identify the top-performing approach within each product category.", "section": "4.2 Results on ProductQA"}, {"figure_path": "Ul3lDYo3XQ/tables/tables_27_1.jpg", "caption": "Table 12: Statistics of the ProductQA dataset. # Products indicates the number of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.", "description": "This table presents the statistical information of the ProductQA dataset.  It shows the distribution of the data across different product categories, providing the number of products in each category, as well as the number of question-answer pairs that fall under Fact-QA, Search-QA, and Reasoning-QA. This breakdown helps to understand the composition and complexity of the ProductQA dataset, useful for evaluating different aspects of question-answering capabilities.", "section": "3 The ProductQA dataset"}]