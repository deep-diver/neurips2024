[{"figure_path": "rF1YRtZfoJ/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of the proposed CLAP4CLIP method against various baselines and state-of-the-art continual learning approaches on five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The table shows the average and last-step accuracy across three runs for each method.  The best and second-best performing methods are highlighted in bold and blue, respectively. It also references Appendix Table 12 for a statistical analysis of the results demonstrating the significance of the differences between the compared methods.", "section": "4 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparison on the CDCL setting [19]. All CLIP-based methods use the ViT-L/14 backbone.", "description": "This table compares the performance of different continual learning methods on a challenging cross-dataset continual learning (CDCL) setting. The CDCL setting involves training the model sequentially on CIFAR100 and ImageNet100 datasets and testing the model on both datasets after training.  The table shows the average and last-step accuracy for various methods, including the proposed CLAP4CLIP approach and several baselines.  The ViT-L/14 backbone is used for all CLIP-based methods to ensure fair comparison. The results highlight the effectiveness of the proposed method in handling cross-dataset continual learning scenarios.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_9_1.jpg", "caption": "Table 4: PhNDD performances averaged over 3 runs on CIFAR100. Best scores for each variant are in bold.", "description": "This table presents the results of post-hoc novel data detection (PhNDD) experiments conducted on the CIFAR100 dataset.  Three metrics are reported: AUROC (Area Under the Receiver Operating Characteristic curve), AUPR (Area Under the Precision-Recall curve), and FPR95 (False Positive Rate at 95% True Positive Rate).  The table compares the performance of several methods, including the baseline Continual-CLIP and variants of the proposed CLAP4CLIP method (with and without variational inference), as well as combinations of CLAP4CLIP with the CoOp method.  The best performance for each variant is highlighted in bold, showing that the proposed methods, especially those incorporating variational inference, significantly improve upon the baseline in terms of novel data detection.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_17_1.jpg", "caption": "Table 5: Benchmark datasets and their details.", "description": "This table lists five benchmark datasets used in the paper's experiments: CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB. For each dataset, it provides the number of training instances, the number of testing instances, the number of classes, the number of tasks, and a link to the dataset.", "section": "4 Experiments"}, {"figure_path": "rF1YRtZfoJ/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the average performance of several continual learning methods across five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The \"Avg\" column represents the average accuracy across all incremental tasks, while \"Last\" shows the accuracy on the final task.  The table highlights the best-performing method in bold for each dataset and includes a reference to Appendix Table 12 for statistical significance testing.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the performance of several continual learning methods on five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The table shows the average accuracy across three runs for each method, with the best performance highlighted in bold and the second-best in blue.  It also provides references to supplementary materials for statistical significance testing.", "section": "4 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of different continual learning methods on five benchmark datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  For each dataset, the table shows the average accuracy and the accuracy on the last task across three runs for each method.  The best performing method for each dataset is highlighted in bold, and the second best is highlighted in blue.  Methods compared include several CLIP-based methods (Continual-CLIP, CoOp, MaPLe, AttriCLIP, CLIP-Adapter, VPT, and the proposed CLAP4CLIP method with various prompting methods), vision-only methods (DualPrompt, L2P, CODA-P, and PROOF), and one baseline continual learning method (iCaRL). The table provides a comprehensive overview of the performance of various continual learning methods on various datasets.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_20_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of the proposed CLAP4CLIP method against various other continual learning methods across five different datasets: CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB.  For each dataset, it shows the average and last-step accuracy achieved by each method, with the best scores highlighted in bold.  It also indicates the statistical significance of the results using standard deviations.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_20_2.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of the proposed CLAP4CLIP method against various baseline and state-of-the-art continual learning methods across five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  For each dataset and method, it shows the average and last-step accuracy across three runs.  Bold text indicates the best performance for each dataset, while blue text indicates the second-best performance.  It also references an appendix table for statistical significance testing of the results.", "section": "4 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_20_3.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the average performance of various continual learning methods across five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The \"Avg\" column shows the average accuracy across all incremental learning steps, while \"Last\" represents the accuracy achieved on the final step.  The table highlights the best-performing method for each dataset in bold and the second-best in blue.  For a more detailed statistical analysis, including standard deviation scores, the reader is referred to Appendix Table 12.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_21_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the average performance of several continual learning methods (including baselines and state-of-the-art methods) across five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, VTAB).  The results are averaged over three runs for each method, and the best-performing method for each dataset is shown in bold.  The table also notes where the performance results for certain methods were sourced from another paper ([39]), and references an appendix table for statistical significance details.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_22_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of the proposed CLAP4CLIP method against various existing continual learning and vision-language model finetuning methods across five benchmark datasets.  The metrics used for comparison are average accuracy and last-step accuracy across ten incremental tasks, providing a comprehensive assessment of performance and forgetting.  The table also highlights the statistical significance of the results.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_22_2.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of the proposed CLAP4CLIP method against several baseline and state-of-the-art continual learning methods.  The average and last-step accuracy are reported across three runs for five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The best-performing method in each category is highlighted in bold, and the second-best is in blue.  It shows that CLAP4CLIP consistently outperforms other methods.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_24_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table presents a comparison of various continual learning methods on five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The performance is measured by the average accuracy across all tasks and the accuracy on the last task.  It also includes results from several baselines.  The best and second-best performing methods are highlighted. A reference to Appendix Table 12 is provided for details on statistical significance testing.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_24_2.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the average performance of various continual learning methods across five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  For each dataset and method, it shows the average accuracy across three runs and the accuracy on the last incremental task.  The best and second-best performances are highlighted.  Statistical significance is discussed in the appendix.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_27_1.jpg", "caption": "Table 20: Entropy-based exemplar selection results for different methods on CIFAR100.", "description": "This table presents the results of exemplar selection using entropy on the CIFAR100 dataset.  It compares the average and last-task accuracy achieved by several continual learning methods, including CoOp, Clip-Adapter, and the authors' method (both with and without Variational Inference).  The 'Avg' column shows the average accuracy across all tasks, while 'Last' shows the accuracy on the final task. The goal is to show how well each method selects exemplars which will aid in preventing catastrophic forgetting.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_28_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the performance of various continual learning methods on five different datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  For each dataset and method, average and last-step accuracy are shown.  The table also indicates which methods achieved the best and second-best results for each dataset.  It refers the reader to Appendix Table 12 for statistical details regarding the significance of the results.", "section": "4 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_28_2.jpg", "caption": "Table 2: Performance comparison on the CDCL setting [19]. All CLIP-based methods use the ViT-L/14 backbone.", "description": "This table compares the performance of different continual learning methods on a challenging cross-dataset continual learning setting (CDCL).  The CDCL setting involves training sequentially on two datasets, CIFAR100 and ImageNet100, and evaluating performance on both.  The table highlights that the proposed CLAP4CLIP method outperforms several baselines and state-of-the-art continual learning methods, demonstrating its effectiveness in handling the challenges posed by cross-dataset continual learning.", "section": "4.1 Results"}, {"figure_path": "rF1YRtZfoJ/tables/tables_30_1.jpg", "caption": "Table 1: Performance comparison of different methods averaged over three runs. Best scores are in bold. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from [39]. See App. Table 12 for statistical significance of these results using std. dev. scores.", "description": "This table compares the average performance of several continual learning methods across five benchmark datasets (CIFAR100, ImageNet100, ImageNet-R, CUB200, and VTAB).  The \"Avg\" column shows the average accuracy across all incremental learning steps, and the \"Last\" column shows the accuracy at the final step.  Bold values represent the best performance for each dataset, and blue values represent the second-best performance.  The table highlights the superior performance of the proposed CLAP4CLIP method compared to existing baselines.  Appendix Table 12 provides additional statistical details regarding the significance of the results.", "section": "4.1 Results"}]