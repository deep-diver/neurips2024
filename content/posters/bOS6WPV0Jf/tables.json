[{"figure_path": "bOS6WPV0Jf/tables/tables_9_1.jpg", "caption": "Table 1: Results on multi-environment datasets, evaluated on test data using three model selection criteria. ID: validation with averaged performance on training data. Worst: validation with the worst performance across training environments. Oracle: validation with performance on sampled test set.", "description": "This table presents the results of the experiments conducted on multi-environment datasets. Three different model selection criteria were used for evaluation: in-distribution (ID) validation, worst-environment (Worst) validation, and oracle validation.  The table compares the performance (RMSE for ACSIncome and Worst-U/R Pearson for PovertyMap) of various methods, including MC-Pseudolabel and several baselines, across these criteria. The results highlight the performance differences between the models under different evaluation scenarios, showing which models are more robust to distribution shifts.", "section": "6 Experiments"}, {"figure_path": "bOS6WPV0Jf/tables/tables_9_2.jpg", "caption": "Table 2: Single-environment results.", "description": "This table presents the results of experiments conducted in a single-environment setting, where algorithms are trained on a single source distribution and tested on a target dataset with distribution shift.  The table shows the performance of different methods, including the proposed MC-Pseudolabel algorithm and several baselines, using the ID (in-distribution) and Oracle (out-of-distribution) validation metrics.  The performance is measured using RMSE (Root Mean Squared Error), a common metric for regression tasks.", "section": "6 Experiments"}, {"figure_path": "bOS6WPV0Jf/tables/tables_17_1.jpg", "caption": "Table 1: Results on multi-environment datasets, evaluated on test data using three model selection criteria. ID: validation with averaged performance on training data. Worst: validation with the worst performance across training environments. Oracle: validation with performance on sampled test set.", "description": "This table presents the results of experiments conducted on three multi-environment datasets (ACSIncome, PovertyMap, VesselPower).  Three model selection criteria were used to evaluate the performance of different algorithms: In-distribution (ID) validation using the average performance across training data; Worst-environment validation using the worst performance across all training environments; and Oracle validation which uses performance on a sampled test set. The table shows the performance (RMSE for ACSIncome and VesselPower, Worst-U/R Pearson for PovertyMap) for each algorithm under each selection criterion.  The lower RMSE and higher Worst-U/R Pearson values indicate better performance.", "section": "6 Experiments"}, {"figure_path": "bOS6WPV0Jf/tables/tables_19_1.jpg", "caption": "Table 4: Hyperparameters for model architecture.", "description": "This table lists the hyperparameters used for the model architectures in the experiments.  It shows the architecture (linear model, MLP, and Resnet18-MS), hidden layer dimensions, optimizer (Adam), weight decay, loss function (MSE), learning rate, and batch size for each dataset (Simulation, ACSIncome, VesselPower, PovertyMap).  Note that the learning rate and batch size for ResNet18-MS follow the settings used in the WILDS benchmark.", "section": "6 Experiments"}, {"figure_path": "bOS6WPV0Jf/tables/tables_19_2.jpg", "caption": "Table 1: Results on multi-environment datasets, evaluated on test data using three model selection criteria. ID: validation with averaged performance on training data. Worst: validation with the worst performance across training environments. Oracle: validation with performance on sampled test set.", "description": "This table presents the results of experiments conducted on three multi-environment datasets (ACSIncome, PovertyMap, and VesselPower).  Three different model selection criteria were used to evaluate the performance of various methods: In-distribution (ID) validation, Worst-case (Worst) validation across training environments, and Oracle validation.  The table compares the performance (RMSE for ACSIncome, Worst-U/R Pearson for PovertyMap) of MC-PseudoLabel against several baseline methods under each criterion.", "section": "6 Experiments"}]