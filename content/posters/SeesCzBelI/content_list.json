[{"type": "text", "text": "Removing Length Bias in RLHF Is Not Enough ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Reinforcement Learning from Human Feedback (RLHF) has become an essential   \n2 technique for enhancing pretrained large language models (LLMs) to generate   \n3 responses that align with human preferences and societal values. While RLHF has   \n4 shown promise, the training of reward models (RMs) still faces the challenge of   \n5 reward hacking, motivating recent works to prevent RMs from finding shortcuts   \n6 that bypass the intended optimization objectives by identifying simplistic patterns,   \n7 especially response length. Besides the issue of length bias, our work firstly reveal   \n8 that prompt-template bias learned by RMs can also cause reward hacking when   \n9 dealing with marginal samples, resulting in LLMs preferring to generate responses   \n10 in a specific format after RLHF fine-tuning, regardless of the format requested in the   \n11 prompt. To this end, we propose a low-cost but effective method, namely Prompt   \n12 Bias Calibration (PBC), to estimate the prompt-template bias term during reward   \n13 modeling, which can be utilized to calibrate reward scores in the following RL   \n14 fine-tuning process. Then, we show that our PBC method can be flexibly combined   \n15 with existing algorithms of removing length bias, leading to a further improvement   \n16 in the aspect of enhancing the quality of generated responses. Experiments results   \n7 show that the performance of our PBC method and its extensions have significantly   \n18 surpassed the original implementation of RLHF. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Reinforcement Learning from Human Feedback (RLHF) has become a critical technique to enable   \n21 pretrained large language models (LLMs) to follow human instructions, understand human intent,   \n22 and also generate responses that align with human preferences and societal values [1\u20134]. Specifically,   \n23 RLHF usually trains a reward model (RM) to act as the proxy of human preferences, and then   \n24 utilize online reinforcement learning (RL) algorithms to fine-tune the language models for generating   \n25 responses that can achieve higher expectation rewards, leading to the success of ChatGPT and also   \n26 many other AI systems [5, 6]. Although the paradigm of RLHF has simplified human data collection,   \n27 as acquiring human ratings is much easier than collecting demonstrations for supervised fine-tuning   \n28 (SFT), it still requires huge amount of human-annotated preference pairs to train well-performing   \n29 RMs in practice, motivating recent researches to seek novel alignment methods to bypass RM   \n30 training [2\u20134]. However, the pipeline of original RLHF is still the primary choice of most industrial   \n31 applications, because well-trained RMs can provide a certain level of generalization ability [7].   \n32 Besides the expensive cost of collecting numerous human-annotated preference pairs, another heavily   \n33 criticized issue of RLHF could be the phenomenon of reward hacking [8], where the over-optimized   \n34 RMs tend to find some shortcuts to bypass its intended optimization objective, through identifying   \n35 some simple patterns to distinguish between good and bad responses [9]. The most widely studied   \n36 pattern in reward hacking could be the sentence (response) length, and these trained RMs can utilize   \n37 the preference among human raters for longer responses to achieve reward hacking, despite the actual   \n38 quality of response does not improve with the increase of response length [10]. Thus, to mitigate   \n39 reward hacking, recent works has primarily focused on estimating the length bias term in the reward   \n40 scoring process, so that it can be removed in the subsequent RL fine-tuning procedure to further   \n41 improve the quality of generated response after RLHF process [11, 12].   \n42 Besides the issue of length bias, in the practice of applying RLHF to industrial products, we have   \n43 observed that the original implementation of RLHF tends to make LLMs prefer generating responses   \n44 in a specific format. This observation motivates us to investigate the underlying causes and seek a   \n45 cost-effective solution to address this issue. The main contributions are summarized as follows:   \n46 \u2022 We are the first to reveal the existence of prompt-template bias in RMs trained with the   \n47 original preference loss, and theoretically analyze the cause of prompt-template bias issue,   \n48 along with its corresponding potential risks on the entire RLHF process;   \n49 \u2022 To mitigate the reward hacking caused by prompt-template bias, we develop a Prompt Bias   \n50 Calibration (PBC) method, which will firstly estimate the prompt-template bias term during   \n51 the reward scoring process, and then remove it in the subsequent RL fine-tuing process;   \n52 \u2022 We show that the developed PBC method can be flexibly combined with existing methods   \n53 of removing length bias, leading to a further improvement in the aspect of enhancing the   \n54 quality of generated responses;   \n55 \u2022 Experimental results show that our developed PCB method and its extensions can achieve   \n56 promising performance improvements compared to the original implementation of RLHF. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "57 2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "58 Reward models (RMs) have become the dominant tool for aligning the LLM\u2019s responses with user   \n59 preferences or task-specific requirements [1, 9]. In this section, we will firstly review the training   \n60 procedure of reward models in Sec. 2.1, including analyzing the causes of length bias and prompt   \n61 bias in existing RMs, and also illustrate how these RMs are used for alignment in Sec. 2.2, especially   \n62 RLHF fine-tuning processes. ", "page_idx": 1}, {"type": "text", "text": "63 2.1 Reward Model Training ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 The usual optimization goal of a reward model is to minimize the loss under the Bradley\u2013Terry model   \n65 [13] on the dataset of pair-wise comparisons of model responses, denoted as $(x,y^{+},y^{-})\\in\\mathcal{D}$ where   \n66 $x$ indicates the input prompt, $y^{+}$ and $y^{-}$ are the chosen and rejected responses respectively. Then,   \n67 the objective function can be formulated as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{R M}(\\theta)=-\\mathbb{E}_{(x,y^{+},y^{-})\\sim\\mathcal{D}}\\left[\\log(\\sigma(r_{\\theta}(x,y^{+})-r_{\\theta}(x,y^{-}))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "68 where $r_{\\theta}(x,y)$ denotes the reward model that takes the prompt $x$ and response $y$ as input to predict a   \n69 scalar reward with trainable parameters $\\theta$ ; $\\sigma$ denotes the sigmoid function.   \n70 Length Bias: Denote $r_{\\theta^{*}}(x,y)$ as the \u201cgold standard\u201d reward model [9] with the optimal parameters   \n71 $\\theta^{*}$ , it reflects human\u2019s intrinsic ranking preferences and can play a role of human rater to provide gold   \n72 reward signal for each prompt-response pair. However, due to the subjectivity of ranking preferences   \n73 and flaws in rating criteria, there is a phenomenon where human raters prefer longer responses that   \n74 appear to be more detailed or better formatted, but their actual quality does not improve [10]. Thus,   \n75 the \u201cgold standard\u201d reward model for rating preference data can often be biased and thus we can   \n76 decompose it to disentangle the actual reward from the spurious reward [11], formulated as ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nr_{\\theta^{*}}(x,y)=r_{\\theta^{*}}^{Q}(x,y)+r_{\\theta^{*}}^{L}(x,y),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "77 where $r_{\\theta^{\\ast}}^{Q}(x,y)$ is the actual reward gains brought by improving the quality of response $y$ ; $r_{\\theta^{\\ast}}^{L}(x,y)$   \n78 is the spurious reward gains of increasing response length, whose patterns are much easier to identify.   \n79 Thus, with length bias in the \u201cgold standard\u201d $r_{\\theta^{\\ast}}(x,y)$ , during the training of reward model, $r_{\\theta}(x,y)$   \n80 can easily find shortcuts to bypass its intended optimization objective, through identifying simple   \n81 patterns, such as sentence (response) length, to distinguish between good and bad responses, leading   \n82 to the phenomenon of \u201creward hacking\u201d caused by length bias [10]. Without increasing the cost   \n83 of rating higher quality preference data, it becomes increasingly important and beneficial to study   \n84 mitigating the impact of length bias in the process of reward modeling.   \n85 Prompt Bias: the prompt bias in reward modeling derives from the underdetermination of Bardley  \n86 Terry model [13]. For any reward model $r_{\\theta^{\\prime}}(x,y)$ learned from the preference loss defined in Eq. (1),   \n87 whose target is optimized to approximate the \u201cgold standard\u201d $r_{\\theta^{\\ast}}(x,y)$ , there always exists an   \n88 equivalent reward model $r_{\\theta}(x,y)$ that satisfies ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "SeesCzBelI/tmp/71846c9d0993dafa75c5e00397c74c67172e0c488672419d77914f3e308b99dd.jpg", "img_caption": ["Figure 1: Comparison of the RM training process using the original preference loss and our developed PBC method respectively, where the latter employs $\\bar{u_{c}}(x)$ to approximate the prompt-template bias, providing unbiased reward scores with lower variance for the subsequent RL fine-tuning. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nr_{\\theta}(x,y):=r_{\\theta^{\\prime}}(x,y)+C(x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 where $C(x)$ is a prompt-dependent constant referred to as prompt bias, leading to the same loss value   \n90 as $\\mathcal{L}(\\boldsymbol{\\theta})=\\mathcal{L}(\\boldsymbol{\\theta}^{'})$ . Due to the fact that there is no constraint on $C(x)$ in the original preference loss   \n91 as defined in Eq. (1), the issue of prompt bias has been criticized in the scenario of reward model   \n92 ensembles [8], where different reward models tend to choose different values for $C(x)$ , making the   \n93 statistics of the set of reward scores meaningless.   \n94 As shown in Fig. 1, it has been widely reported that the prompt bias will result in a certain gap in the   \n95 mean values of the set of prompt-response pairs under different prompts. However, in our research,   \n96 we find that this gap is more likely caused by the prompt-template bias, as discussed in Section 3.1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 2.2 RLHF Fine-tuning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 Given the trained reward model $r_{\\theta}(x,y)$ as the proxy of human preferences, Reinforcement Learning   \n99 from Human Feedback (RLHF) tends to utilize an online reinforcement learning method, typically   \n100 proximal policy optimization (PPO) [14], trains a policy language model $\\pi_{\\phi}^{R L}$ to maximize expected   \n101 reward, while staying close to its initial policy \u03c0\u03d5SF T, which is finetuned on supervised data (prompt  \n102 response pairs). Through measuring the distance from the initial policy with Kullback-Leibler (KL)   \n103 divergence, the optimization objective of RLHF fine-tuning can be formulated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}^{R L}(\\phi)=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\pi_{\\phi}^{R L}}}\\left[r_{\\theta}(x,y)+\\beta\\log\\left[\\pi_{\\phi}^{R L}(y|x)/\\pi^{S F T}(y|x)\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 where $\\beta$ is the hyper-parameter to control the strength of the KL divergence term. ", "page_idx": 2}, {"type": "text", "text": "105 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106 In this section, we will firstly investigate the cause of prompt-template bias and then theoretically   \n107 analyze its potential risks when dealing with marginal samples during reward modeling, as shown in   \n108 Sec. 3.1, and then illustrate our low-cost but effective method to estimate the prompt-template bias   \n109 term during RM training in Sec. 3.2, which can be utilized to calibrate reward scores in the following   \n110 RL fine-tuning process. At last, in Sec. 3.3, we show that our Prompt Bias Calibration (PBC) method   \n111 can be flexibly combined with recent popular methods of removing length bias, leading to a further   \n112 improvement in the aspect of enhancing the quality of generated responses.   \n114 In this part, we will first illustrate the cause of prompt-template bias during RM training. Formally,   \n115 given a set of prompt-response pairs, denoted as $\\mathcal{D}_{a}=\\{x_{a},y_{a}^{(i)}\\}_{i=1}^{N_{a}}$ , with the same user prompt $x_{a}$ ,   \n116 e.g. \u201cwriting an academic paper on the field of computer science\u201d, and $\\{y_{a}^{(i)}\\}_{i=1}^{N_{a}}$ denoting the set   \n117 of collected academic papers to satisfy the request of $x_{a}$ , the prompt bias term, specifically $C(x_{a})$ ,   \n118 learned by RMs is supposed to not affect the preference order within $\\mathcal{D}_{a}$ , as discussed in Section 2.1.   \n119 However, in the practice of RM training, the reward score is usually predicted by a LLM that takes   \n120 the concatenation of the prompt and response as input, making it challenging for RMs to learn a bias   \n121 term that focuses solely on the prompt $x$ while disregarding variations in the subsequent response $y$ .   \n122 During the training process to order the pairs within $\\mathcal{D}_{a}$ , we find that RMs trained with the original   \n123 preference loss in Eq. (1) are more likely to introduce a joint bias term across the entire sequence of   \n124 concatenating the prompt and response, formulated as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{\\theta}(x_{a},y_{a}):=r_{\\theta^{'}}(x_{a},y_{a})+C(x_{a},\\overline{{y}}_{a}),\\quad\\overline{{y}}_{a}=\\frac{1}{N_{a}}\\sum_{i=1}^{N}y_{a}^{(i)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 where ${\\overline{{y}}}_{a}$ can be considered the average response of the response set $\\{y_{a}^{(i)}\\}_{i=1}^{N_{a}}$ , and it will embody   \n126 the common characteristics found within these collected responses, such as the format of academic   \n127 paper; $C(x_{a},\\overline{{y}}_{a})$ denotes the joint bias on the entire sequence of the prompt $x_{a}$ associated with the   \n128 average response $\\overline{{y}}_{a}$ in the format of academic paper; $r_{\\theta}(x_{a},y_{a})$ is still supposed to approximate the   \n129 \u201cgold standard\u201d provided by $r_{\\theta^{*}}(x_{a},y_{a})$ , leading to $\\mathbb{E}_{\\mathcal{D}_{a}}\\left[r_{\\theta^{\\prime}}(x_{a}^{\\,\\cdot},y_{a})\\right]\\approx\\bar{\\mathbb{E}}_{\\mathcal{D}_{a}}^{\\,\\cdot}\\left[r_{\\theta^{*}}(x_{a},\\bar{y_{a}})\\right]$ .   \n130 Considering the average response $\\overline{y}$ can be treated as a standard template of the response to the   \n131 prompt $x$ , we define the joint bias $C(x,{\\overline{{y}}})$ as prompt-template bias. Then, we highlight the properties   \n132 of prompt-template bias as follows: 1) the original preference loss in Eq. (1) imposes no constraints   \n133 on $C(x,{\\overline{{y}}})$ , because its value will not influence the outcome of the preference loss and also not affect   \n134 the preference order within the prompt-response pairs collected for the same prompt $x$ ; 2) $C(x,{\\overline{{y}}})$   \n135 will reduce to the original prompt bias $C(x,-)$ when no common characteristics can be found across   \n136 all of these collected responses, indicating the diversity of $\\{y^{(i)}\\}_{i=1}^{N}$ is sufficiently high. With these   \n137 properties in mind, we assume that the prompt-template bias $C(x,{\\overline{{y}}})$ can essentially meet most of the   \n138 properties of the original prompt bias $C(x,-)$ as discussed in Section 2.1. Thus, we suppose $C(x,{\\overline{{y}}})$   \n139 can be considered as a broader definition of prompt bias in the actual RM training, because it is more   \n140 likely to be learned by RMs in practice, given the fact that preference pairs are extremely scarce and   \n141 the diversity of responses collected for the same prompt is often insufficient.   \n142 After defining prompt-template bias, we will theoretically investigate the impact of introducing   \n143 $C(x,{\\overline{{y}}})$ during RM training on the entire RLHF process. Assume that there exist two sets of prompt  \n144 response pairs, denoted as $\\mathcal{D}_{a}=\\{x_{a},y_{a}^{(i)}\\}_{i=1}^{N_{a}}$ =a1 and Db = {xb, yb(i )}iN=b1, where xa and xb indicate   \n145 different categories of prompts, $e.g.\\ x_{a}$ requests \u201cwriting an academic paper on theme and   \n146 requests \u201cwriting a brief on theme $\\pmb{b}\\,^{,\\bullet}$ , and $\\{y_{a}^{(i)}\\}_{i=1}^{N_{a}}$ and $\\{y_{b}^{(i)}\\}_{i=1}^{N_{b}}$ denote the collected responses   \n147 for answering the prompt $x_{a}$ and $x_{b}$ respectively. After RM training, due the fact that there is no   \n148 constraint on $C(x,{\\overline{{y}}})$ in the preference loss defined in Eq. (1), the discrepancies of prompt biases   \n149 between these two previously mentioned sets of prompt-response pairs, specifically $\\mathcal{D}_{a}$ and $\\mathcal{D}_{b}$ , could   \n150 be extremely large, e.g. $C(\\dot{x}_{a},\\overline{{y}}_{a})>>C(x_{b},\\overline{{y}}_{b})$ , leading to ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(x_{a},y_{a})\\sim\\mathcal{D}_{a}}\\left[r_{\\theta}(x_{a},y_{a})\\right]>>\\mathbb{E}_{(x_{b},y_{b})\\sim\\mathcal{D}_{b}}\\left[r_{\\theta}(x_{b},y_{b})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "115512 rwehwearre $r_{\\theta}(x_{a},y_{a})=r_{\\theta^{\\prime}}(x_{a},y_{a})+C(x_{a},{\\overline{{y}}}_{a})$ caonrd $r_{\\theta}(x_{b},y_{b})=r_{\\theta^{\\prime}}(x_{b},y_{b})+C(x_{b},\\overline{{y}}_{b})$ $\\{r_{\\theta^{\\prime}}(x_{a},y_{a}^{(i)})\\}_{i=1}^{N_{a}}$ $\\{r_{\\theta^{\\prime}}(x_{b},y_{b}^{(i)})\\}_{i=1}^{N_{b}}$ nrbeisapseecd  \n153 tively, should exhibit similar mean values, e.g. $\\mathfrak{T}_{\\mathcal{D}_{a}}\\left[r_{\\theta^{\\prime}}(x_{a},y_{a})\\right]\\approx\\mathbb{E}_{\\mathcal{D}_{b}}\\left[r_{\\theta^{\\prime}}(x_{b},y_{b})\\right]$ , and will make   \n154 little impact on the comparison of expectation terms in Eq. (6). We highlight that the discrepancies of   \n155 prompt bias terms, specifically the gap between $C(x_{a},\\overline{{y}}_{a})$ and $C(x_{b},\\overline{{y}}_{b})$ , won\u2019t affect preference   \n156 ordering within categories, but can cause disaster when dealing with some marginal samples, like   \n157 \u201can academic paper on theme $\\pmb{b}\\\"$ denoted as $y_{a b}$ , or $^{\\ast}a$ brief on theme a\u201d denoted as $y_{b a}$ .   \n158 To facilitate an intuitive analysis, we take the marginal sample \u201can academic paper on theme $\\pmb{b}\\,^{,\\bullet}$ ,   \n159 denoted as $y_{a b}$ , as an example, and the reward scores for prompt-response pairs corresponding to the   \n160 prompt $x_{b}$ may exhibit the following preference orders: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{\\theta}(x_{b},y_{a b})=r_{\\theta^{\\prime}}(x_{b},y_{a b})+C(x_{b},{\\overline{{y}}}_{a})>r_{\\theta^{\\prime}}(x_{b},y_{b})+C(x_{b},{\\overline{{y}}}_{b})=r_{\\theta}(x_{b},y_{b}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "SeesCzBelI/tmp/3a17c466f6cc19b01116483640f674784d1826a5941c1f2235c060e024a3c0ec.jpg", "img_caption": ["Figure 2: Network architecture design for the RM trained using the LBPC method incorporates a prompt bias head on the last token of the prompt $x$ designed to predict $C^{Q}(x,{\\overline{{y}}})$ and $C^{L}(x,{\\dot{\\overline{{y}}}})$ , and a reward score head on the last token of the response intended to predict $r_{\\theta}^{Q}(x,\\overline{{y}})$ and $r_{\\theta}^{L}(x,\\overline{{y}})$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "161 which can be achieved as long as $r_{\\theta^{\\prime}}(x_{b},y_{a b})\\approx r_{\\theta^{\\prime}}(x_{b},y_{b})$ and $C(x_{b},\\overline{{y}}_{a})>C(x_{b},\\overline{{y}}_{b})$ . The first   \n162 condition $r_{\\theta^{\\prime}}(x_{b},y_{a b})\\approx r_{\\theta^{\\prime}}(x_{b},y_{b})$ can be achieved because both the response $y_{a b}$ and $y_{b}$ meet the   \n163 description of theme $b$ and are similar on a semantic level. The second inequality is highly likely   \n164 to be achieved when there is a reward model that has a bias towards preferring the sentence in the   \n165 format of $a$ over $b$ , specifically $C(x_{a},\\overline{{y}}_{a})>>C(x_{b},\\overline{{y}}_{b})$ .   \n166 Finally, we highlight that the phenomena of inequality in Eq. (7), caused by prompt-template bias   \n167 $C(x,{\\overline{{y}}})$ , is commonly encountered in the deployment process of RLHF in real-world applications,   \n168 especially text creation. For example, if responses are collected solely for the style requested in each   \n169 prompt during RM training, the reward model can lead to a bias towards particular styles as shown in   \n170 Fig. 3(a). Then, once such marginal samples, e.g $\\cdot\\left(x_{b},y_{a b}\\right)$ , are generated by LLMs during the RL   \n171 fune-tuning process and also satisfy the inequality $\\dot{r}_{\\theta}(x_{b},y_{a b})>r_{\\theta}(x_{b},y_{b})$ as shown in Table 1, the   \n172 entire RL fine-tuning process, typically PPO, will be biased and results in a LLM that only generates   \n173 responses in a specific format, regardless of the format you request in the prompt. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "174 3.2 Calibrating prompt-template bias in RLHF ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "175 To mitigate the impact of the prompt-template bias issue on the RLHF process, the most straight  \n176 forward solution in industry could be to collect a more diverse set of response candidates for each   \n177 prompt. However, this approach is time-consuming and may even require a lot of human interventions   \n178 for response collection, motivating us to develop a low-cost but effective method to alleviate the issue   \n179 of prompt-template bias during RM training.   \n180 The developed Prompt Bias Calibration (PBC) method mainly includes two steps: 1) estimating the   \n181 prompt-template bias term in the reward scoring process with minimal additional computational cost;   \n182 2) removing prompt-template bias in the subsequent RLHF fine-tuning process to ensure that the   \n183 resulting LLM does not have a tendency to generate responses in a specific format. As shown in   \n184 Fig. 1, to approximate the prompt-template bias term $C(\\Bar{x},\\overline{{y}})$ in Eq. (5), we choose to apply a linear   \n185 layer on the last token of the prompt sentence to predict prompt-template bias, denoted as $\\boldsymbol u_{c}(\\boldsymbol x)$ , and   \n186 then add the following regularization term on the original preference loss, formulated as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{c}^{R M}(\\theta)=\\mathbb{E}_{(x,y^{+},y^{-})\\sim\\mathcal{D}}\\left[\\|r_{\\theta}(x,y^{+})-u_{c}(x)\\|_{2}^{2}+\\|r_{\\theta}(x,y^{-})-u_{c}(x)\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "187 where $u_{c}(x)$ is supposed to approximate the mean value of reward scores of the prompt-response   \n188 pairs given the same prompt $x$ . We note that there will be a hyper-parameter $\\eta_{c}$ to be multiplied on   \n189 the regularization term in the final loss to promise the accuracy of RMs, leading to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{p b c}^{R M}(\\theta)=\\mathcal{L}^{R M}(\\theta)+\\eta_{c}\\cdot\\mathcal{L}_{c}^{R M}(\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "190 The benefits of such a design in the PBC method include the following folds: 1) approximating   \n191 $C(x,{\\overline{{y}}})$ by adding a linear layer to the last hidden layer of LLMs results in almost no additional   \n192 computational cost; 2) during the autoregressive scoring process of LLM-based RMs, $C(x,{\\overline{{y}}})$ can   \n193 serve as an intermediate signal guidance of the prompt sequence, thereby enabling RMs to focus   \n194 more on the differences between chosen/rejected responses in the subsequent reward scoring process;   \n195 3) we can use unbiased reward scores to guide the follow RLHF fine-tuning process, formulated as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nr_{\\theta^{\\prime}}(x,y)=r_{\\theta}(x,y)-u_{c}(x)\\approx r_{\\theta}(x,y)-C(x,{\\overline{{y}}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "196 which has been proven effective for penalizing reward uncertainty, improving robustness, encouraging   \n197 improvement over baselines, and reducing variance in PPO fune-tuning [15]. ", "page_idx": 5}, {"type": "text", "text": "198 3.3 Jointly calibrating length and prompt-template bias in RLHF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 To simultaneously calibrate length and prompt-template bias in RLHF, the developed PBC method can   \n200 be flexibly combined with existing methods of removing length bias, whose main idea is to separately   \n201 approximate the \u201cgold standard\u201d reward model after disentangling shown in Eq. (2), formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nr_{\\theta}(x,y)=r_{\\theta}^{Q}(x,y)+r_{\\theta}^{L}(x,y),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where $r_{\\theta}^{Q}(x,y)$ is supposed to approximate the actual reward $r_{\\theta^{*}}^{Q}(x,y);r_{\\theta}^{L}(x,y)$ is used to approxi  \n203 mate the spurious reward brought by length bias, specifically $r_{\\theta^{*}}^{L}(x,y)$ . Then, for those methods of   \n204 removing length bias [11, 12], the original preference loss in Eq. (1) can be equivalently expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{R M}(\\theta)=-\\mathbb{E}_{(x,y^{+},y^{-})\\sim\\mathcal{D}}\\left[\\log(\\sigma(r_{\\theta}^{Q}(x,y^{+})+r_{\\theta}^{L}(x,y^{+})-r_{\\theta}^{Q}(x,y^{-})-r_{\\theta}^{L}(x,y^{-}))\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "205 where $r_{\\theta}^{Q}(x,y)$ and $r_{\\theta}^{L}(x,y)$ can be modeled with two different LLMs [12] or two different heads in   \n206 the same LLM [11]. To remove length bias in Eq. (12), recent work proposes to add constraints on   \n207 the preference loss to reduce the correlation between the confounding factor, e.g. response length, and   \n208 actual reward $r_{\\theta}^{Q}(x,y)$ , while increasing its correlation with spurious reward $r_{\\theta}^{L}(x,y)$ , formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{l}^{R M}(\\theta)=C o r r(r_{\\theta}^{Q}(x,y),L(x,y))-C o r r(r_{\\theta}^{L}(x,y),L(x,y))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 where the confounding factor $L(x,y)$ can be either specifically defined as response length $L(y)$ in   \n210 [11], or use Products-of-Experts framework for estimation [12].   \n211 To model the scoring process of the reward model more accurately, which simultaneously considers   \n212 the concepts of length and prompt bias, we combine the definition of reward model in Eq. (3) and   \n213 Eq. (11), achieving a more precise definition of reward scoring process, formulated as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nr_{\\theta}(x,y)=r_{\\theta^{\\prime}}(x,y)+C(x,{\\overline{{y}}})=r_{\\theta^{\\prime}}^{Q}(x,y)+C^{Q}(x,{\\overline{{y}}})+r_{\\theta^{\\prime}}^{L}(x,y)+C^{L}(x,{\\overline{{y}}})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 where $C^{Q}(x,{\\overline{{y}}})$ and $C^{L}(x,{\\overline{{y}}})$ indicate the component of prompt-template bias in actual and spurious   \n215 rewards, respectively; the unbiased overall reward $r_{\\theta^{\\prime}}(x,y)=r_{\\theta^{\\prime}}^{Q}(x,y)+r_{\\theta^{\\prime}}^{L}(x,y)$ and the overall   \n216 prompt-template bias term ${\\cal C}(x,\\overline{{{y}}})\\,=\\,{\\cal C}^{Q}(x,\\overline{{{y}}})+{\\cal C}^{L}(x,\\overline{{{y}}})$ . Then we can propose Length and   \n217 Prompt Bias Calibration (LPBC) method, as shown in Fig. 2, which can estimate $\\dot{\\mathcal{L}}_{l}^{R M}(\\theta,\\tau)$ with a   \n218 conditioned correlation method, defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{l}^{R M}(\\theta)=C o r r(r_{\\theta}^{Q}(x,y)-C^{Q}(x,\\overline{{y}}),L(y;x))-C o r r(r_{\\theta}^{L}(x,y)-C^{L}(x,\\overline{{y}}),L(y;x))}\\\\ &{\\qquad\\qquad=C o r r(r_{\\theta^{\\prime}}^{Q}(x,y),L(y;x))-C o r r(r_{\\theta^{\\prime}}^{L}(x,y),L(y;x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "219 where the confounding factor $L(y;x):=L(x,y)-L(x)$ can be estimated with the response length. ", "page_idx": 5}, {"type": "text", "text": "220 Through combining the disentangled preference loss in Eq. (12), the prompt-bias regularization term   \n221 in Eq. (8) and also the length-bias conditional correlation term in Eq. (15), the final loss of LBPC   \n222 method can be formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l p b c}^{R M}(\\theta)=\\mathcal{L}^{R M}(\\theta)+\\eta_{c}\\cdot\\mathcal{L}_{c}^{R M}(\\theta)+\\eta_{l}\\cdot\\mathcal{L}_{l}^{R M}(\\theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 where $\\eta_{c}$ and $\\eta_{l}$ are hyper-parameters to control the importance of regularization terms, which can be   \n224 adjusted according to the accuracy of trained RMs on the validation dataset. ", "page_idx": 5}, {"type": "table", "img_path": "SeesCzBelI/tmp/78765fb94eaa879502347748dba708a32083bb43c4a7affd500ef6282565f3fd.jpg", "table_caption": ["Table 1: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "225 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "226 4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "227 Datasets. For intuitively understanding the issue of prompt-template bias in RLHF and also qual  \n228 itatively evaluating the effectiveness of our method, we manually construct a training dataset for   \n229 text creation applications, where each prompt requires creation in a special style according to the   \n230 theme. Then, a small validation set is also constructed, in which only responses that meet the stylistic   \n231 requirements of each prompt are collected. We name this dataset as RM-Template, which can be used   \n232 to measure the severity of the prompt-template bias issue during RM training.   \n233 Further, to make quantitative comparisons with other baseline methods, we conduct experiments on   \n234 RM-Static dataset [16], which has been released on Huggingface [17] and consists of 76K preference   \n235 pairs. After randomly shuffling, we choose 40K preference pairs for RM training, 6K preference   \n236 pairs for RM evaluation, and the rest prompt-response pairs for the subsequent PPO fune-tuning. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "237 The dataset statics of these datasets have been exhibited in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "238 Model & Training. For model selection, we choose Llama-2-7b [18] as our base model, which   \n239 is relatively lightweight, and has been open-sourced on Huggingface [17]. For RM training, we   \n240 fine-tune all the parameters of RMs initialized with the pretrained weights of Llama-2-7b. For PPO   \n241 fine-tuning, we also initialize the actor model with pretrained Llama-2-7b and the critic model with   \n242 RMs trained with various preference losses.   \n243 For model training, all experiments are implemented with DeepSpeed-Chat framework [19] and   \n244 Huggingface Transformers [20], running on 4 NVIDIA A100 80GB GPUs. For the hyper-parameter   \n245 setting, we set $\\eta_{c}=0.05$ and $\\eta_{l}=0.05$ in Eq. (16) for all our proposed methods, and have listed the   \n246 rest hyper-parameters in Appendix A.4, such as learning rate, weight decay, batch size etc. AdamW   \n247 [21] is adopted for optimizing all the model parameters without freezing anything or using adapters.   \n248 Evaluation Metrics. For quantitative comparison, we follow the evaluation procedure of Instruct  \n249 Eval [22] to test the actor models, which has been aligned with biased/de-biased RMs with PPO   \n250 fine-tuning, on Massive Multitask Language Understanding (MMLU) [23], DROP [24], BIG-Bench   \n251 Hard (BBH) [25], and TruthfulQA (TQA) [26] benchmarks respectively, evaluating the model\u2019s   \n252 ability on the aspects of multi-task solving, math reasoning, and response trustworthy. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "SeesCzBelI/tmp/8f98220b0f2c20cfdfc716be9ccc7f99035a3e2ed69cbd9a09fe26f1ce55f7c7.jpg", "img_caption": ["Figure 3: The comparison of statistics of the reward scores predicted by RMs trained with (a) the original preference loss and (b) our developed PBC method, across different categories of promptresponse pairs in the validation set of the manually constructed RM-Template dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "SeesCzBelI/tmp/7119cab8f17a1fe3c6f2e68b0636089a4659b05422662614d86de77e0e26e756.jpg", "table_caption": ["Table 2: Performance comparison of LLMs aligned with RMs trained with various methods. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "253 4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "254 Qualitative Evaluation. To intuitively evaluate the effectiveness of our method, we exhibit the   \n255 statistics (mean and standard deviation) of the reward scores predicted by RMs trained with the   \n256 original preference loss in Eq. (1) and our PBC method in Eq. (9), across different categories of   \n257 prompt-response pairs in the validation set of the RM-Template dataset. The results depicted in   \n258 Fig.3(c) demonstrate that calibrating prompt-template bias with the PBC method leads to a gradual   \n259 reduction in the variance of the mean values of reward distributions across different categories. The   \n260 most noticeable observation is that the vanilla RM tends to give an extremely high reward score to   \n261 prompt-response pairs in the format of tech article, but the RM trained with the PBC method can   \n262 calibrate the reward distribution for tech articles to make it more close with that of other categories.   \n263 Then, we evaluate the performance of RMs trained with various methods on handling marginal   \n264 samples defined in Section 3.1. Specifically, given the prompt randomly selected from the validation   \n265 set of RM-Template dataset, we use GPT-4 [6] to generate responses in various formats according   \n266 to the theme described in the prompt. Then, we use RMs trained with various preference losses to   \n267 rank these responses. From the showcase in Table. 1, we can find that the vanilla RM tend to assign   \n268 a higher reward score to the response in the format of tech article, caused by the prompt-template   \n269 bias issue shown in Fig. d3(a). After removing this bias with our PBC or LPBC methods, the RM   \n270 can provide a relatively fair ranking for these prompt-response pairs, where LPBC method can even   \n271 mitigate the affect of length bias during comparing poetry with other categories (the length of poetry   \n272 is generally shorter than other literary forms). More showcases can be found in Appendix A.6.   \n273 Quantitative Comparison. For the quantitative com  \n274 parison in Table 2, we utilize PPO fine-tuning process   \n275 to align Llama-2-7b with the RMs trained with vari  \n276 ous methods. From the results, we can find that our   \n277 developed PBC method can lead to performance im  \n278 provements compared to the original implementation   \n279 of RLHF; directly combining PBC with other meth  \n280 ods of removing length bias, e.g. ODIN [11], can help   \n281 them to achieve further performance improvement; the   \n282 well-designed LPBC achieves the best performance and   \n283 surpasses the rough combination of PBC and ODIN.   \n284 To make a comprehensive comparison, we follow the experimental setting described in ODIN [11],   \n285 and use GPT-4 as the judge to compare two responses generated by LLMs aligned with RMs trained   \n286 with various methods. Specifically, we take the LLM aligned with LPBC-based RM as model A, and   \n287 compare it against other LLMs aligned with RM trained with ODIN, PCB, ODIN $+.$ PBC, respectively.   \n288 From the results shown in Fig. 4, we can find that the win rate of LPBC is significantly higher than   \n289 that of other baseline models, with ODIN $^+$ PBC being the most challenging competitor as model B. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "SeesCzBelI/tmp/40ad7e47496fb27dcddd715de4a704e3db0aba13bed1cc43f362a496944a93d8.jpg", "img_caption": ["Figure 4: Win rates comparison (judged by GPT-4) of LLMs aligned with RMs trained with LBPC and other methods. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "SeesCzBelI/tmp/c51047987f94e28223944d779daac1f0eefe1da0fd6f70d6a17fa0f3a9893ac7.jpg", "img_caption": ["Figure 5: Ablation studies on the various settings of hyper-parameter $\\eta_{c}$ and $\\eta_{l}$ in LPBC method. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "290 4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "291 To investigate the robustness of our developed LPBC method, we conduct ablation studies on the   \n292 hyper-parameter settings of LPBC method, specifically $\\eta_{c}$ and $\\eta_{l}$ in Eq. (16). With various settings   \n293 of $\\eta_{c}\\in\\{0.01,0.05,0.\\bar{1}\\}$ and $\\eta_{l}\\in\\{0.01,0.0\\dot{5},0.1\\}$ , we can have total 9 RMs trained with various   \n294 hyper-parameter settings of LPBC methods. From the accuracy curves shown in Fig.5(a), we can   \n295 find the introducing constraints to the original preference loss indeed affects the performance of RM   \n296 accuracy, and this performance loss increases with the importance weight of the constraint terms.   \n297 However, at the limited cost of sacrificing RM accuracy, the performance of the LLM aligned the RM   \n298 trained with LPBC method has improved to some extent on MMLU and DROP as shown in Fig. 5(b)   \n299 and 5(c) respectively. Note that the performance of the LPBC method in Table. 2 is not the optimal,   \n300 as it is achieved with $\\eta_{c}=\\eta_{l}=0.05$ , demonstrating no cherry-picking of hyperparameters.. ", "page_idx": 8}, {"type": "text", "text": "301 5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "302 The prevalence of length bias in RLHF have been widely criticized as indicative of reward hacking   \n303 [9, 10], and numerous recent studies have delved into strategies aimed at mitigating the tendency   \n304 for length increase during the fine-tuning process of RLHF [11, 12, 27]. Typically, Shen et al. [12]   \n305 innovatively apply the Productof-Experts (PoE) technique to separate reward modeling from the   \n306 influence of sequence length, which adopts a smaller reward model to learn the biases in the reward   \n307 and a larger reward model to learn the true reward. Utilizing similar disentangling ideas, Chen et al.   \n308 [11] jointly train two linear heads on shared feature representations to predict the rewards, one trained   \n309 to correlate with length, and the other trained to focus more on the actual content quality. Ryan et al.   \n310 [27] firstly study the length problem in the DPO setting, showing significant exploitation in DPO   \n311 and linking it to out-of-distribution bootstrapping. As for the prompt bias issue, although it has been   \n312 criticized in the scenario of reward model ensembles [8], no studies have yet attempted to analyze its   \n313 cause and influence on RLHF. We emphasize that our work is the first to flil this gap by proposing a   \n314 low-cost yet effective method to mitigate the reward hacking induced by prompt-template bias. ", "page_idx": 8}, {"type": "text", "text": "315 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "316 In this paper, we demonstrate that prompt-template bias in RMs can lead to LLMs, which, after RL   \n317 fine-tuning, generate responses exclusively in a specific format, irrespective of the variations in the   \n318 prompt request. Thus, we propose a low-cost but effective PBC method, to estimate the prompt  \n319 template bias term during reward modeling, which can be utilized to calibrate reward scores in the   \n320 following RL fine-tuning process. Then, we show that our PBC method can be flexibly combined   \n321 with existing algorithms of removing length bias, leading to a further improvement in the aspect of   \n322 enhancing the quality of generated responses. Experimental results show that the performance of   \n323 PBC method and its extensions have significantly surpassed the original implementation of RLHF. ", "page_idx": 8}, {"type": "text", "text": "324 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "325 [1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,   \n326 Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to   \n327 follow instructions with human feedback. Advances in neural information processing systems,   \n328 35:27730\u201327744, 2022.   \n329 [2] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and   \n330 Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.   \n331 Advances in Neural Information Processing Systems, 36, 2024.   \n332 [3] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:   \n333 Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \n334 [4] Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou. Rela  \n335 tive preference optimization: Enhancing llm alignment through contrasting responses across   \n336 identical and diverse prompts. arXiv preprint arXiv:2402.10958, 2024.   \n337 [5] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,   \n338 Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly   \n339 capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n340 [6] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni   \n341 Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4   \n342 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n343 [7] Ziniu Li, Tian Xu, and Yang Yu. Policy optimization in rlhf: The impact of out-of-preference   \n344 data. arXiv preprint arXiv:2312.10584, 2023.   \n345 [8] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour, DJ Dvi  \n346 jotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or   \n347 herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint   \n348 arXiv:2312.09244, 2023.   \n349 [9] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.   \n350 In International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023.   \n351 [10] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating   \n352 length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.   \n353 [11] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng   \n354 Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: disentangled reward mitigates   \n355 hacking in RLHF. CoRR, abs/2402.07319, 2024.   \n356 [12] Wei Shen, Rui Zheng, WenYu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing   \n357 Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human   \n358 feedback. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association   \n359 for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 2859\u2013   \n360 2873. Association for Computational Linguistics, 2023.   \n361 [13] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the   \n362 method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n363 [14] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal   \n364 policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n365 [15] Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang Liu. Improv  \n366 ing reinforcement learning from human feedback using contrastive rewards. arXiv preprint   \n367 arXiv:2403.07708, 2024.   \n368 [16] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn   \n369 Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless   \n370 assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,   \n371 2022.   \n372 [17] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony   \n373 Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transform  \n374 ers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n375 [18] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n376 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \n377 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n378 [19] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia   \n379 Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al.   \n380 Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales.   \n381 arXiv preprint arXiv:2308.01320, 2023.   \n382 [20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony   \n383 Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State  \n384 of-the-art natural language processing. In Proceedings of the 2020 conference on empirical   \n385 methods in natural language processing: system demonstrations, pages 38\u201345, 2020.   \n386 [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint   \n387 arXiv:1711.05101, 2017.   \n388 [22] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic   \n389 evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.   \n390 [23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and   \n391 Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint   \n392 arXiv:2009.03300, 2020.   \n393 [24] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gard  \n394 ner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.   \n395 arXiv preprint arXiv:1903.00161, 2019.   \n396 [25] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won   \n397 Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big  \n398 bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,   \n399 2022.   \n400 [26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic   \n401 human falsehoods. arXiv preprint arXiv:2109.07958, 2021.   \n402 [27] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from   \n403 quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "404 A Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "405 A.1 Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "406 The main limitation of this work is that there are no theoretical proof to promise RM can provide an   \n407 accurate preference order when handling marginal samples, e.g., responses that satisfy the theme of   \n408 the user prompt but in various formats. Moreover, the constraints added by our developed method   \n409 to the preference loss will lead to a decrease in the accuracy of the RM, and to some extent, limit   \n410 the capability of the RM. Therefore, how to remove the prompt-template bias without scarifying the   \n411 accuracy of RM is a worthwhile problem for future research. ", "page_idx": 11}, {"type": "text", "text": "412 A.2 Border Impact ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "413 The most significant positive impact of this work is that by removing the prompt-template bias,   \n414 our method can mitigate the LLM\u2019s tendency to prefer generating responses in specific formats   \n415 after RLHF fine-tuning. Furthermore, our developed method can improve the quality of responses   \n416 generated by LLMs after alignment, compared to the original RLHF. The discovery of prompt  \n417 template bias may lead to another stream of research focused on investigating, estimating, and   \n418 removing this bias from RM training.   \n419 The negative impact could be that our method can be used for enhancing the capabilities of LLMs. If   \n420 LLMs enpowered by our methods are misunderstood, it could lead to unexpected troubles, but this is   \n421 also a common issue with all of current pretrained LLMs. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "422 A.3 License ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "423 We highlight that Llama-2-7b is licensed under the LLAMA 2 Community License, and RM-Static   \n424 dataset is licensed the Huggingface hub. Our work follows the license of CC BY-NC 4.0. ", "page_idx": 11}, {"type": "text", "text": "425 A.4 Hyper-parameter Settings ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "426 RM Training. The hyper-parameter settings of RM training under the DeepSpeedChat framework   \n427 has been listed in Table. 3.   \n428 PPO Fine-tuning. The hyper-parameter settings of PPO fine-tuning under the DeepSpeedChat   \n429 framework has been listed in Table. 4. ", "page_idx": 11}, {"type": "table", "img_path": "SeesCzBelI/tmp/8668c49bd511b4f1b3d432e34fd6c3a48003857eb875978c26d5010fb3f0decd.jpg", "table_caption": ["Table 3: The hyper-parameter settings of RM training. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "430 A.5 Dataset Statics ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "431 The dataset statics of RM-Template and RM-Static used in our experiments have been summarized as   \n432 follows:   \n433 RM-Template. RM-Template is a manually constructed dataset for measuring the severity of the   \n434 prompt-template bias issue and evaluating the effectiveness of the method developed for alleviating   \n435 the issue of prompt-template bias. In this dataset, each prompt requires responses to be created in a   \n436 specific format according to the theme. There are a total of 50K prompt-response pairs, encompassing   \n437 20 categories of format requirements in the responses.   \n438 RM-Static. The RM-Static dataset is provided by Hugging Face and is primarily used for training   \n439 reward models after supervised fine-tuning. It is a branch of the hh-static dataset and contains both   \n440 training and testing parts. Features of the dataset include: 1) prompt: A string type representing the   \n441 user\u2019s input; 2) response: A string type representing the assistant\u2019s answer. 3) chosen: A string type   \n442 representing the selected answer. 4) rejected: A string type representing the rejected answer. The   \n443 training set contains approximately 76K rows of data and the testing set contains approximately 5.1K   \n444 rows of data. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "SeesCzBelI/tmp/c519a8d7fa28824a77a79c9d6745d64cae50703c4db9023f5fb4a84e5a2fb0e2.jpg", "table_caption": ["Table 4: The hyper-parameter settings of PPO fine-tuning. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "445 A.6 More Showcases ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "446 More showcases of the preference order predicted by RMs trained with various methods, have been   \n447 listed in the Table 5 and Table 6. ", "page_idx": 12}, {"type": "table", "img_path": "SeesCzBelI/tmp/a383be24639788b0bd72c0f29e7ebbf97f0ad2419184d0cb2d71105ffcf8b173.jpg", "table_caption": ["Table 5: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Table 6: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4. ", "page_idx": 13}, {"type": "table", "img_path": "SeesCzBelI/tmp/b51b0b3126fa7590282da2614a1be49f1c814c2d5bbe301675585f4953bf1ac1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "448 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Justification: Yes, the claims in abstract and introduction has already reflected the paper\u2019s contribution on the field of RLHF. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "449   \n450   \n451   \n452   \n453   \n454   \n455   \n456   \n457   \n458   \n459   \n460   \n461   \n462   \n463   \n464   \n465   \n466   \n467   \n468 ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Yes, the discussion about limitation can be found in Appendix. ", "page_idx": 13}, {"type": "text", "text": "469 Guidelines:   \n470 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n471 the paper has limitations, but those are not discussed in the paper.   \n472 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n473 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n474 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n475 model well-specification, asymptotic approximations only holding locally). The authors   \n476 should reflect on how these assumptions might be violated in practice and what the   \n477 implications would be.   \n478 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n479 only tested on a few datasets or with a few runs. In general, empirical results often   \n480 depend on implicit assumptions, which should be articulated.   \n481 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n482 For example, a facial recognition algorithm may perform poorly when image resolution   \n483 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n484 used reliably to provide closed captions for online lectures because it fails to handle   \n485 technical jargon.   \n486 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n487 and how they scale with dataset size.   \n488 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n489 address problems of privacy and fairness.   \n490 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n491 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n492 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n493 judgment and recognize that individual actions in favor of transparency play an impor  \n494 tant role in developing norms that preserve the integrity of the community. Reviewers   \n495 will be specifically instructed to not penalize honesty concerning limitations.   \n496 3. Theory Assumptions and Proofs   \n497 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n498 a complete (and correct) proof?   \n499 Answer: [Yes]   \n500 Justification: We have included the theoretical analysis of the cause of prompt-template bias.   \n501 Guidelines:   \n502 \u2022 The answer NA means that the paper does not include theoretical results.   \n503 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n504 referenced.   \n505 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n506 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n507 they appear in the supplemental material, the authors are encouraged to provide a short   \n508 proof sketch to provide intuition.   \n509 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n510 by formal proofs provided in appendix or supplemental material.   \n511 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n512 4. Experimental Result Reproducibility   \n513 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n514 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n515 of the paper (regardless of whether the code and data are provided or not)?   \n516 Answer: [Yes]   \n517 Justification: We have included the implementation details in the main manuscript and also   \n518 provide the hyper-parameter setting in the Appendix   \n519 Guidelines:   \n520 \u2022 The answer NA means that the paper does not include experiments.   \n1 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n22 well by the reviewers: Making the paper reproducible is important, regardless of   \n23 whether the code and data are provided or not.   \n4 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n25 to make their results reproducible or verifiable.   \n26 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n7 For example, if the contribution is a novel architecture, describing the architecture fully   \n8 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n29 be necessary to either make it possible for others to replicate the model with the same   \n0 dataset, or provide access to the model. In general. releasing code and data is often   \n31 one good way to accomplish this, but reproducibility can also be provided via detailed   \n2 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n3 of a large language model), releasing of a model checkpoint, or other means that are   \n4 appropriate to the research performed.   \n35 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n6 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n37 nature of the contribution. For example   \n38 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n39 to reproduce that algorithm.   \n0 (b) If the contribution is primarily a new model architecture, the paper should describe   \nthe architecture clearly and fully.   \n2 (c) If the contribution is a new model (e.g., a large language model), then there should   \n43 either be a way to access this model for reproducing the results or a way to reproduce   \n4 the model (e.g., with an open-source dataset or instructions for how to construct   \n5 the dataset).   \n46 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n47 authors are welcome to describe the particular way they provide for reproducibility.   \n48 In the case of closed-source models, it may be that access to the model is limited in   \n49 some way (e.g., to registered users), but it should be possible for other researchers   \n50 to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The code has been included in the supplemental material and the dataset for the main experimental results is public. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 15}, {"type": "text", "text": "576 \u2022 Providing as much information as possible in supplemental material (appended to the   \n577 paper) is recommended, but including URLs to data and code is permitted.   \n578 6. Experimental Setting/Details   \n579 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n580 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n581 results?   \n582 Answer: [Yes]   \n583 Justification: Have included the training and test details in the experimental settings.   \n584 Guidelines:   \n585 \u2022 The answer NA means that the paper does not include experiments.   \n586 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n587 that is necessary to appreciate the results and make sense of them.   \n588 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n589 material.   \n590 7. Experiment Statistical Significance   \n591 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n592 information about the statistical significance of the experiments?   \n593 Answer: [NA]   \n594 Justification: We report the average performance in our experiments, and we are willing to   \n595 release the training and evalution log in W&B if it is required.   \n596 Guidelines:   \n597 \u2022 The answer NA means that the paper does not include experiments.   \n598 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n599 dence intervals, or statistical significance tests, at least for the experiments that support   \n600 the main claims of the paper.   \n601 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n602 example, train/test split, initialization, random drawing of some parameter, or overall   \n603 run with given experimental conditions).   \n604 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n605 call to a library function, bootstrap, etc.)   \n606 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n607 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n608 of the mean.   \n609 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n610 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n611 of Normality of errors is not verified.   \n612 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n613 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n614 error rates).   \n615 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n616 they were calculated and reference the corresponding figures or tables in the text.   \n617 8. Experiments Compute Resources   \n618 Question: For each experiment, does the paper provide sufficient information on the com  \n619 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n620 the experiments?   \n621 Answer: [Yes]   \n622 Justification: 4\\*A100   \n623 Guidelines:   \n624 \u2022 The answer NA means that the paper does not include experiments.   \n625 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n626 or cloud provider, including relevant memory and storage.   \n627 \u2022 The paper should provide the amount of compute required for each of the individual   \n628 experimental runs as well as estimate the total compute.   \n629 \u2022 The paper should disclose whether the full research project required more compute   \n630 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n631 didn\u2019t make it into the paper).   \n632 9. Code Of Ethics   \n633 Question: Does the research conducted in the paper conform, in every respect, with the   \n634 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n635 Answer: [Yes]   \n636 Justification: Yes, it is   \n637 Guidelines:   \n638 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n639 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n640 deviation from the Code of Ethics.   \n641 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n642 eration due to laws or regulations in their jurisdiction).   \n643 10. Broader Impacts   \n644 Question: Does the paper discuss both potential positive societal impacts and negative   \n645 societal impacts of the work performed?   \n646 Answer: [Yes]   \n647 Justification: Have discussed the broader impact in the Appendix   \n648 Guidelines:   \n649 \u2022 The answer NA means that there is no societal impact of the work performed.   \n650 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n651 impact or why the paper does not address societal impact.   \n652 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n653 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n654 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n655 groups), privacy considerations, and security considerations.   \n656 \u2022 The conference expects that many papers will be foundational research and not tied   \n657 to particular applications, let alone deployments. However, if there is a direct path to   \n658 any negative applications, the authors should point it out. For example, it is legitimate   \n659 to point out that an improvement in the quality of generative models could be used to   \n660 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n661 that a generic algorithm for optimizing neural networks could enable people to train   \n662 models that generate Deepfakes faster.   \n663 \u2022 The authors should consider possible harms that could arise when the technology is   \n664 being used as intended and functioning correctly, harms that could arise when the   \n665 technology is being used as intended but gives incorrect results, and harms following   \n666 from (intentional or unintentional) misuse of the technology.   \n667 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n668 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n669 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n670 feedback over time, improving the efficiency and accessibility of ML).   \n671 11. Safeguards   \n672 Question: Does the paper describe safeguards that have been put in place for responsible   \n673 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n674 image generators, or scraped datasets)?   \n675 Answer: [NA]   \n676 Justification: The safeguards of our model should be the same as Llama released by META.   \n677 Guidelines:   \n678 \u2022 The answer NA means that the paper poses no such risks.   \n679 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n680 necessary safeguards to allow for controlled use of the model, for example by requiring   \n681 that users adhere to usage guidelines or restrictions to access the model or implementing   \n682 safety filters.   \n683 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n684 should describe how they avoided releasing unsafe images.   \n685 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n686 not require this, but we encourage authors to take this into account and make a best   \n687 faith effort. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "688 12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "689 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n690 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n691 properly respected?   \n692 Answer: [Yes]   \n693 Justification: Yes, their licenses can be found in Huggingface website and we have also   \n694 highlight it in our Appendix.   \n695 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "711 13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "712 Question: Are new assets introduced in the paper well documented and is the documentation   \n713 provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "714 Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "715 Justification: No new asset   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper does not release new assets.   \n718 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n719 submissions via structured templates. This includes details about training, license,   \n720 limitations, etc.   \n721 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n722 asset is used.   \n723 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n724 create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "725 14. Crowdsourcing and Research with Human Subjects ", "page_idx": 18}, {"type": "text", "text": "726 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n727 include the full text of instructions given to participants and screenshots, if applicable, as   \n728 well as details about compensation (if any)?   \n729 Answer: [No]   \n730 Justification: No research with human subjects ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "731   \n732   \n733   \n734   \n735   \n736   \n737   \n738   \n739   \n740   \n741   \n742   \n743   \n744   \n745   \n746   \n747   \n748   \n749   \n750   \n751   \n752   \n753   \n754   \n755   \n756   \n757   \n758 ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No research with human subjects Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]