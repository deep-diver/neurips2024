[{"figure_path": "UqvEHAnCJC/tables/tables_8_1.jpg", "caption": "Table 1: Evaluation metrics of OLLM and baselines on Wikipedia and arXiv. OLLM performs particularly well in modelling semantics, and remains competitive syntactically and structurally.", "description": "This table presents the quantitative results of the experiments conducted on Wikipedia and arXiv datasets to evaluate the performance of OLLM against several baseline methods.  The metrics used to assess the quality of the generated ontologies are Literal F1, Fuzzy F1, Continuous F1, Graph F1, and Motif Distance.  Higher values for Literal F1, Fuzzy F1, Continuous F1, and Graph F1 generally indicate better performance, while a lower Motif Distance value indicates better structural integrity. The table shows that OLLM outperforms other methods, particularly in semantic similarity (Fuzzy F1 and Continuous F1), while remaining competitive in syntactic and structural similarity. The results are separated for Wikipedia (in-domain evaluation) and arXiv (out-of-domain evaluation).", "section": "5.3 Results"}, {"figure_path": "UqvEHAnCJC/tables/tables_17_1.jpg", "caption": "Table 1: Evaluation metrics of OLLM and baselines on Wikipedia and arXiv. OLLM performs particularly well in modelling semantics, and remains competitive syntactically and structurally.", "description": "This table presents the performance of the OLLM model and several baseline methods on two datasets: Wikipedia and arXiv.  The performance is measured using five metrics: Literal F1, Fuzzy F1, Continuous F1, Graph F1, and Motif Distance.  Higher scores are better for all metrics except Motif Distance, where a lower score is better. The table highlights that OLLM generally outperforms the baselines, especially in terms of semantic similarity (Fuzzy F1 and Continuous F1), while maintaining competitive performance on syntactic and structural similarity.  This demonstrates the effectiveness of OLLM in building accurate ontologies.", "section": "5.3 Results"}, {"figure_path": "UqvEHAnCJC/tables/tables_19_1.jpg", "caption": "Table 1: Evaluation metrics of OLLM and baselines on Wikipedia and arXiv. OLLM performs particularly well in modelling semantics, and remains competitive syntactically and structurally.", "description": "This table presents a comparison of the performance of OLLM and several baseline methods on two datasets: Wikipedia and arXiv.  The performance is evaluated using five different metrics: Literal F1, Fuzzy F1, Continuous F1, Graph F1, and Motif Distance.  Each metric assesses a different aspect of ontology quality, such as semantic similarity (Fuzzy F1) or structural similarity (Graph F1). The table highlights OLLM's superior performance in semantic modelling, while maintaining competitive results in terms of syntax and structure.", "section": "5.3 Results"}, {"figure_path": "UqvEHAnCJC/tables/tables_19_2.jpg", "caption": "Table 1: Evaluation metrics of OLLM and baselines on Wikipedia and arXiv. OLLM performs particularly well in modelling semantics, and remains competitive syntactically and structurally.", "description": "This table presents the quantitative results of the experiments conducted on Wikipedia and arXiv datasets.  It compares the performance of the proposed method, OLLM, against several baseline methods across five evaluation metrics: Literal F1, Fuzzy F1, Continuous F1, Graph F1, and Motif Distance.  Higher scores are better for the first four metrics, while a lower score is better for Motif Distance. The results demonstrate that OLLM achieves state-of-the-art performance in terms of semantic similarity (Fuzzy F1 and Continuous F1) while maintaining relatively strong performance across other metrics.", "section": "5.3 Results"}]