{"references": [{"fullname_first_author": "A. Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2018-00-00", "reason": "This paper is foundational in the field of adversarial attacks against deep learning models, providing a comprehensive overview of the topic and introducing several key concepts and techniques."}, {"fullname_first_author": "I. J. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014-12-00", "reason": "This paper is seminal work in the area of adversarial machine learning, introducing the concept of adversarial examples and demonstrating their impact on deep learning models."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), a fundamental method for generating high-quality samples from complex probability distributions, which is the basis of the diffusion models used in the target paper."}, {"fullname_first_author": "C. Chi", "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion", "publication_date": "2023-03-00", "reason": "This paper introduces diffusion policies, which are directly used in this paper, and are the primary focus of the attack."}, {"fullname_first_author": "H. Salman", "paper_title": "Raising the cost of malicious AI-powered image editing", "publication_date": "2023-02-00", "reason": "This paper provides insights into the vulnerabilities of diffusion models to adversarial attacks, particularly focusing on image manipulation tasks, which is relevant to the adversarial attacks explored in the main paper."}]}