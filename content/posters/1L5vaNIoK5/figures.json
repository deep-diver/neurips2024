[{"figure_path": "1L5vaNIoK5/figures/figures_1_1.jpg", "caption": "Figure 1: Adversarial Attacks against Diffusion Policy: We aim to attack robots controlled with visual-based DP, unveiling hidden threats to the safe application of diffusion-based policies. (a) By hacking the visual inputs, we can fool the diffusion process into generating wrong actions \u03c4 (in red). We propose Diffusion Policy Attacker(DP-Attacker), which can effectively attack the DP by (b) hacking the global camera inputs I using small visual perturbations under both online and offline settings or (c) attaching an adversarial patch into the environment. The online settings use current visual inputs at t-th timestep It to generate time-variant perturbations \u03b4t, while the offline settings use only offline data ID to generate time-invariant perturbations \u03b4.", "description": "This figure illustrates three different attack methods against diffusion-based policies.  (a) shows a high-level overview of the attack, where incorrect actions are generated by manipulating the visual input. (b) details the process of applying adversarial perturbations to the camera input, categorized by online (dynamic, adapting to each frame) and offline (static, applied consistently).  (c) depicts an attack using a physical adversarial patch placed in the environment to perturb the robot's visual input, again resulting in incorrect actions.", "section": "1 Introduction"}, {"figure_path": "1L5vaNIoK5/figures/figures_3_1.jpg", "caption": "Figure 2: Design Space of DP-Attacker: the tree above shows the design space of DP-Attacker, which can be adapted to various kinds of attack scenarios, including global attacks (hacking cameras) vs patched attacks (hacking the physical environment); offline vs online; targeted vs untargeted.", "description": "This figure illustrates the different attack strategies implemented by the DP-Attacker.  DP-Attacker is categorized into two main approaches: Global Attacks and Patched Attacks. Global Attacks involve manipulating the camera input, either offline (using a pre-computed perturbation) or online (generating perturbations in real-time).  Both types of global attacks can be targeted (aiming for a specific bad action) or untargeted (simply trying to degrade performance).  Patched Attacks involve placing physical adversarial patches in the robot's environment.  These too can be targeted or untargeted and are implemented offline.", "section": "4 Methods"}, {"figure_path": "1L5vaNIoK5/figures/figures_6_1.jpg", "caption": "Figure 3: Global Attack (Online): We visualize the global attacks in Algorithm 1 within both the PushT and Can environments. Specifically, we present action rollouts for four types of observations: clean observations, observations perturbed with random Gaussian noise, and our optimized perturbations (both untargeted and targeted). While the DPs show robustness to random perturbations, they are vulnerable to adversarial samples generated using DP-Attacker.", "description": "This figure visualizes the results of online global attacks on two robotic manipulation tasks: PushT and Can. It compares four scenarios: clean observations, observations with added random Gaussian noise, observations with untargeted adversarial perturbations generated by DP-Attacker, and observations with targeted adversarial perturbations. The results show that while the diffusion policies are robust to random noise, they are vulnerable to the adversarial perturbations crafted by DP-Attacker, indicating a significant decrease in performance for both targeted and untargeted attacks.", "section": "5.1 Global Attack"}, {"figure_path": "1L5vaNIoK5/figures/figures_6_2.jpg", "caption": "Figure 4: Physical Adversarial Patches: we show the patches optimized by Algorithm 2, attaching it in the physical scene will effectively lower the success rate of the target diffusion policy.", "description": "This figure shows three examples of physical adversarial patches generated by the DP-Attacker algorithm. Each patch is designed to target a specific robotic manipulation task (Can, Square, Toolhang). The patches are small, visually inconspicuous, and designed to be robust to changes in lighting and viewing angle. Attaching the patch to the environment fools the diffusion policy into making incorrect actions, significantly reducing task success rate.", "section": "Experiments"}, {"figure_path": "1L5vaNIoK5/figures/figures_7_1.jpg", "caption": "Figure 5: Difference in Encoded Feature Vector: we calculate the distance between the clean feature vector and the attacked feature vector. DP-Attacker perturb the feature vector significantly compared to naive random noise attack.", "description": "This figure displays violin plots showing the distribution of L2 distances between encoded feature vectors.  The clean feature vectors are compared against those obtained after applying random noise and DP-Attacker perturbations.  The results demonstrate that DP-Attacker significantly alters the encoded feature vector compared to the simple addition of random noise. This highlights how the adversarial attacks effectively modify the feature representation to deceive the diffusion model.", "section": "5.2 Patched Attack"}, {"figure_path": "1L5vaNIoK5/figures/figures_8_1.jpg", "caption": "Figure 6: We used DP-Attacker with different attack strengths to run rollouts. We report the average distance between the predicted action sequence and the target action sequence (a sequence of a duplicate target coordinate). The metric is calculated at each inference during the rollout.", "description": "This figure shows the results of using DP-Attacker with varying attack strengths (\u03b4 = 0.05, 0.06, 0.07) on two different model checkpoints: PushT (CNN) and CAN (PH CNN).  The graphs plot the mean distance between the actions generated by the diffusion policy and the target action sequence over the course of a rollout (environment steps).  The results show how well DP-Attacker can manipulate the generated actions towards the target, with stronger attacks leading to a closer match overall.", "section": "5.3 Quantitative Results on Targeted Attacks"}, {"figure_path": "1L5vaNIoK5/figures/figures_14_1.jpg", "caption": "Figure 1: Adversarial Attacks against Diffusion Policy: We aim to attack robots controlled with visual-based DP, unveiling hidden threats to the safe application of diffusion-based policies. (a) By hacking the visual inputs, we can fool the diffusion process into generating wrong actions \u03c4t (in red). We propose Diffusion Policy Attacker(DP-Attacker), which can effectively attack the DP by (b) hacking the global camera inputs It using small visual perturbations under both online and offline settings or (c) attaching an adversarial patch into the environment. The online settings use current visual inputs at t-th timestep It to generate time-variant perturbations \u03b4t, while the offline settings use only offline data ID to generate time-invariant perturbations \u03b4.", "description": "This figure illustrates three different attack methods against diffusion policies (DPs) used for robot control.  (a) shows the general concept of the attack, where manipulating the visual input (camera image) causes the DP to generate incorrect actions (shown in red).  (b) details the \"global attack\", where small visual perturbations are added to the input image, either online (changing per timestep) or offline (a single static perturbation).  (c) illustrates the \"patch attack\", where a physical patch placed in the environment deceives the robot's vision system, resulting in the DP generating incorrect actions.", "section": "1 Introduction"}, {"figure_path": "1L5vaNIoK5/figures/figures_15_1.jpg", "caption": "Figure 4: Physical Adversarial Patches: we show the patches optimized by Algorithm 2, attaching it to the physical scene will effectively lower the success rate of the target diffusion policy.", "description": "This figure shows three examples of adversarial patches generated by Algorithm 2.  These patches are designed to be placed in the robot's environment to deceive the diffusion policy. The patches, shown in the top row, are tailored to specific tasks and applied to the physical scene.  The bottom row shows that attaching these patches to the environment will significantly reduce the success rate of the target diffusion policy.  Each patch was designed using the method in Algorithm 2 to attack a specific pre-trained diffusion policy model.", "section": "5 Experiments"}, {"figure_path": "1L5vaNIoK5/figures/figures_16_1.jpg", "caption": "Figure 3: Global Attack (Online): We visualize the global attacks in Algorithm 1 within both the PushT and Can environments. Specifically, we present action rollouts for four types of observations: clean observations, observations perturbed with random Gaussian noise, and our optimized perturbations (both untargeted and targeted). While the DPs show robustness to random perturbations, they are vulnerable to adversarial samples generated using DP-Attacker.", "description": "This figure visualizes the results of online global attacks on two robotic manipulation tasks: PushT and Can.  It compares the robot's actions under four conditions: (1) clean observations (no attack); (2) observations with added random Gaussian noise; (3) observations with untargeted adversarial perturbations generated by DP-Attacker; and (4) observations with targeted adversarial perturbations.  The results show that while the diffusion policies are robust against random noise, they are significantly affected by the adversarial perturbations crafted by DP-Attacker.", "section": "5.1 Global Attack"}, {"figure_path": "1L5vaNIoK5/figures/figures_16_2.jpg", "caption": "Figure 3: Global Attack (Online): We visualize the global attacks in Algorithm 1 within both the PushT and Can environments. Specifically, we present action rollouts for four types of observations: clean observations, observations perturbed with random Gaussian noise, and our optimized perturbations (both untargeted and targeted). While the DPs show robustness to random perturbations, they are vulnerable to adversarial samples generated using DP-Attacker.", "description": "This figure visualizes the results of online global attacks using the DP-Attacker algorithm on two different robotic manipulation tasks: PushT and Can.  Four scenarios are shown for each task: (1) clean observations (no attack), (2) observations with added random Gaussian noise, (3) observations with untargeted adversarial perturbations generated by DP-Attacker, and (4) observations with targeted adversarial perturbations generated by DP-Attacker. The results demonstrate that while the Diffusion Policies (DPs) are robust to random noise, they are highly susceptible to carefully crafted adversarial perturbations produced by DP-Attacker, highlighting the vulnerability of DPs to these types of attacks.", "section": "5.1 Global Attack"}]