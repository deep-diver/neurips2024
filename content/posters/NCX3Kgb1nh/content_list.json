[{"type": "text", "text": "Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gabriel Rioux Apoorva Nitsure Mattia Rigotti Center for Applied Mathematics MIT-IBM Watson AI Lab MIT-IBM Watson AI Lab Cornell University IBM Research IBM Research ", "page_idx": 0}, {"type": "text", "text": "Kristjan Greenewald MIT-IBM Watson AI Lab IBM Research ", "page_idx": 0}, {"type": "text", "text": "Youssef Mroueh MIT-IBM Watson AI Lab IBM Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents\u2019 preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In choice theory, economics, finance, and models benchmarking, agents are faced with stochastic prospects that are (eventually) multivariate random variables which they wish to order according to a utility or risk measure of interest. To formalize such a notion of ordering of stochastic quantities, the concept of stochastic dominance can be utilized. ", "page_idx": 0}, {"type": "text", "text": "In the univariate case, a standard notion of order is given by First order Stochastic Dominance (FSD) which can be expressed in terms of the quantiles of the underlying random variables [Ogryczak and Ruszczynski, 2002]. To wit, a random variable $X$ dominates another random variable $Y$ in FSD, if it has larger quantiles than $Y$ across all percentiles. A weaker notion of FSD, called almost stochastic dominance was introduced in del Barrio et al. [2018]. Their approach is based on optimal transport (OT) and consists of measuring a ratio which quantifies how close $X$ is to dominating $Y$ . del Barrio et al. [2018] further lay the groundwork for principled statistical analysis of almost FSD by establishing a central limit theorem for the empirical ratio as well as consistency of the bootstrap. Dror et al. [2018], Ulmer et al. [2022], Nitsure et al. [2023] used the almost FSD testing framework in benchmarking Large Language models to make statistically significant decisions regarding which model to select when these models are evaluated with a metric of interest on test data. ", "page_idx": 0}, {"type": "text", "text": "Our main motivation is to extend the testing framework for almost FSD of del Barrio et al. [2018] to the multivariate case as to enable applications with dependencies between metrics. For instance, the problem of multivariate portfolio selection in financial applications has been treated via a reduction to univariate orders [Kouaissah, 2021]. Another application of interest is that of multimetric benchmarking which is central nowadays in ranking and selecting Large Languages Models [Bommasani et al., 2023, Chang et al., 2023, Huang et al., 2023, MosaicLM, 2023, Wolf, 2023, Zhang and Hardt, 2024]. Current approaches such as Nitsure et al. [2023] use aggregation techniques to reduce the ordering to the univariate case thereby ignoring dependencies between metrics. ", "page_idx": 1}, {"type": "text", "text": "Our starting point is the so-called standard multivariate stochastic order (see Chapter 6 in Shaked and Shanthikumar, 2007). This order can be expressed in terms of couplings between random vectors. This insight allows us to follow the approach of del Barrio et al. [2018] and define an almost multivariate FSD via OT in Section 2. This notion of stochastic dominance can be defined using multivariate violation ratios that are expressed as optimal transport problems with smooth costs [Manole and Niles-Weed, 2024, Hundrieser et al., 2022, Groppe and Hundrieser, 2023]. Given that empirical OT suffers from the curse of dimensionality, we resort in Section 3 to entropic regularization [Cuturi, 2013] to alleviate that issue and hence define Entropic Multivariate Violation Ratios. We establish in Section 3 convergence of these entropic violation ratios as the regularization parameter goes to zero, as well as a central limit theorem and bootstrapping consistency in Section 4 using the functional delta method [R\u00f6misch, 2006]. We highlight that the delta method has seen general success for proving limit theorems with entropic OT [Hundrieser et al., 2024, Goldfeld et al., 2024a,b]. Armed with this theory, we propose a new framework for hypothesis testing of multivariate stochastic dominance and apply it to multi-metric benchmarking of LLMs. Multivariate FSD captures the dependencies between the metrics in this setting and leads to a more robust ordering. ", "page_idx": 1}, {"type": "text", "text": "Notation The indicator function of a set $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ is denoted $\\mathbb{1}_{A}(x)$ and takes the value 1 if $x\\in A$ and 0 otherwise. We also adopt the following shorthand notation, $\\mathbb{R}_{+}=[0,\\infty)$ , the maximum of two numbers $a,b\\in\\mathbb{R}$ is denoted by $a\\vee b$ , and for vectors $x,y\\in\\mathbb{R}^{d}$ , we write $x\\leq y$ to indicate that $x_{i}\\leq y_{i}$ for every $i\\in\\{1,\\ldots,d\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Throughout, $\\mathcal{P}(\\mathbb{R}^{d})$ is the set of all probability measures on $\\mathbb{R}^{d}$ . A measure $\\eta\\in\\mathcal{P}(\\mathbb{R}^{d})$ is said to be sub-Gaussian with parameter $\\tau^{2}\\geq0$ with respect to the 1-norm provided $\\mathbb{E}_{\\eta}\\left[\\exp\\!\\left(\\|X\\|_{1}^{2}\\!/\\!2\\tau^{2}\\right)\\right]\\leq2$ . Convergence in distribution of random variables is denoted by $\\stackrel{d}{\\to}$ (in the sense of Hoffmann-J\u00f8rgensen when necessary, see Chapter 1 in van der Vaart and Wellner, 1996). ", "page_idx": 1}, {"type": "text", "text": "2 Optimal transport and stochastic order ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 FSD and Almost Stochastic Dominance in One Dimension ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To properly motivate our results on multivariate FSD, we first review some theory for the univariate setting. For random variables $X,Y$ , it is said that $X$ dominates $Y$ in the stochastic order (denoted $X\\ \\succcurlyeq\\ Y)$ if $\\mathbb{P}(X\\leq\\,t)\\,\\leq\\,\\mathbb{P}(Y\\leq\\,t)$ for every $t\\,\\in\\,\\mathbb{R}$ . Formally, this means that the inequality FSD   \n$Y\\le X$ generally holds for a given instantiation of these random variables. This condition can be cast, equivalently, as $F_{Y}^{-1}(t)\\le\\bar{F_{X}^{-1}}(t)$ for every $t\\in(0,1)$ , where $F_{X}^{-1}(t)$ and $F_{Y}^{-1}(t)$ are the quantile functions for $X$ and $Y$ respectively. With this formulation in mind, del Barrio et al. [2018] propose the following index of almost stochastic dominance; ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathcal{W}_{2}}(\\mu,\\nu)=\\frac{\\int_{0}^{1}(F_{Y}^{-1}(t)-F_{X}^{-1}(t))_{+}^{2}d t}{\\mathcal{W}_{2}^{2}(\\mu,\\nu)},\\;\\mathrm{where}\\;\\mathcal{W}_{2}^{2}(\\mu,\\nu)=\\int_{0}^{1}(F_{X}^{-1}(t)-F_{Y}^{-1}(t))^{2}d t,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "$\\mu=\\operatorname{law}(X),\\nu=\\operatorname{law}(Y)$ , and, for $z\\in\\mathbb{R}$ , $(z)_{+}^{2}=(0\\vee z)^{2}$ denotes the squared hinge loss. Here, the numerator captures the degree to which $X$ fails to dominate $Y$ whereas the denominator serves as a normalizing constant so that $\\varepsilon_{\\mathcal{W}_{2}(\\mu,\\nu)}\\in[0,1]$ . Indeed, as $(x-y)_{+}^{2}+(y-x)_{+}^{2}=(x-y)^{2}$ , we see that $\\varepsilon\\nu_{2}(\\mu,\\nu)=0$ precisely when $F_{Y}^{-1}(t)\\leq F_{X}^{-1}(t)$ for a.e. $t\\in(0,1)$ whereas $\\varepsilon\\boldsymbol{\\mathcal{W}}_{2}\\left(\\mu,\\nu\\right)=1$ when the opposite inequality holds. del Barrio et al. [2018] then propose to test the null hypothesis $\\varepsilon_{\\mathcal{W}_{2}}(\\mu,\\nu)\\,\\leq\\,\\varepsilon_{0}$ for some $\\varepsilon_{0}$ sufficiently close to 0, corresponding to the case where $X$ almost dominates $Y$ in the stochastic order, versus the alternative hypothesis $\\varepsilon\\mathcal{W}_{2}\\left(\\mu,\\nu\\right)>\\varepsilon_{0}$ . To this end, they provide a central limit theorem for the statistic $\\varepsilon_{\\mathcal{W}_{2}}$ and propose to construct the rejection region via bootstrap estimation of the limiting variance. Similar results were obtained for a notion of almost second order stochastic dominance in Nitsure et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "We highlight that the index $\\varepsilon\\nu_{2}$ can be connected to OT. Indeed, by Theorem 2.9 in Santambrogio [2015], the numerator and the denominator in $\\varepsilon_{\\mathcal{W}_{2}}(\\mu,\\nu)$ can be written, respectively, as $\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\;\\textstyle\\int(y-x)_{+}^{2}d\\pi(x,y)$ , and $\\begin{array}{r}{\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int(y-x)^{2}\\ddot{d}\\pi(x,y)}\\end{array}$ , where $\\Pi(\\mu,\\nu)$ denotes the set of all couplings of $(\\mu,\\nu)$ . These problems are (univariate) instances of the well-studied OT problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\sf O T}_{c}(\\mu,\\nu):=\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int c d\\pi,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $c:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ is a given cost function. We highlight that the costs considered herein are such that an optimal coupling always exists (see Theorem 4.1 in Villani, 2009). This connection will serve as our basis for extending the notion of almost stochastic dominance to the multivariate setting. ", "page_idx": 2}, {"type": "text", "text": "2.2 Multivariate FSD and its Relaxation via Optimal Transport with Compatible Costs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the sequel, we provide a general framework for assessing multivariate almost FSD using a purely OT-based methodology. Following Chapter 6 and Theorem 6.B.1. in Shaked and Shanthikumar [2007], given the random vectors $X,Y\\in\\dot{\\mathbb{R}}^{d}$ , we say that $X$ dominates $Y$ in the usual stochastic order (denoted $X\\ \\succcurlyeq\\ Y)$ provided that there exists a coupling $({\\hat{X}},{\\hat{Y}})$ of $(X,Y)$ satisfying $\\mathbb{P}(\\hat{X}\\geq\\hat{Y})=1$ FSD ", "page_idx": 2}, {"type": "text", "text": "(i.e. for each $i=1,\\dots,d,\\hat{X}_{i}\\geq\\hat{Y}_{i}$ with probability one). This condition can be cast as follows: ", "page_idx": 2}, {"type": "text", "text": "Lemma 1. Letting $\\mu$ (resp. \u03bd) denote the law of $X$ (resp. $Y$ ), $X\\underset{F\\mathrm{S}D}{\\approx}Y\\,i f\\,0\\mathsf{T}_{c}(\\mu,\\nu)=0,$ , where $c:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ is the cost function $c(x,y)=\\mathbb{1}_{\\{x\\leq y\\}}(x,y)$ . ", "page_idx": 2}, {"type": "text", "text": "Evidently, the cost $\\mathbb{1}_{\\{x\\leq y\\}}(x,y)$ in Lemma 1 can be replaced by any nonnegative cost function $c(x,y)$ satisfying $c(x,y)=0$ if and only if $x\\leq y$ and it still holds that $X\\ \\succcurlyeq\\ Y\\$ if $0\\mathsf{T}_{c}(\\mu,\\nu)=0$ . FSD ", "page_idx": 2}, {"type": "text", "text": "We denote the set of all such costs by $\\mathfrak{C}_{\\le}$ ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (OT Costs Compatible with Multivariate FSD). The set of all cost functions which are compatible with multivatiate FSD in the sense that $0\\mathsf{T}_{c}(\\mathrm{law}(X),\\mathrm{law}(Y))=0$ implies that $X\\ \\succcurlyeq\\ Y\\$ ", "page_idx": 2}, {"type": "text", "text": "A simple recipe for generating cost functions in $\\mathfrak{C}_{\\le}$ is to take any univariate function $h:\\mathbb{R}\\to\\mathbb{R}_{+}$ with the property that $h^{-1}(\\{0\\})=(-\\infty,0]$ and define $\\begin{array}{r}{c(x,y)=\\sum_{i=1}^{d}h(y_{i}-x_{i})}\\end{array}$ . For notational simplicty, we write ${0}{\\sf T}_{h}$ to denote the OT cost with this type of cost function even when the aforementioned property does not hold. The results presented in the following sections require some additional smoothness assumptions on the function $h$ discussed above which we summarize presently. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Smooth Costs). The function $h:\\mathbb{R}\\to\\mathbb{R}_{+}$ satisfies the smoothness condition $\\mathrm{(SC_{d})}$ ) $i f$ $h$ is Lipschitz continuous with constant $L\\geq0$ (that is, $|h(x)-h(y)|\\leq L|x-y|$ for every $x,y\\in\\mathbb{R},$ ) and, for $k=\\lfloor d/2\\rfloor+1$ , $h$ is $k$ -times continuously differentiable with derivatives of order $s\\leq k$ satisfying $|h^{(s)}(x)|\\le C_{k}(1+|x|)^{p_{k}}$ for some $C_{k}<\\infty$ and $p_{k}>1$ which may depend on $k$ . ", "page_idx": 2}, {"type": "text", "text": "We now discuss some examples of cost functions of interest. ", "page_idx": 2}, {"type": "text", "text": "Example 1 (Examples of OT Costs). 1) The function $h(z)=e^{-1/z}$ for $z\\in(0,\\infty)$ and 0 otherwise is known to be smooth (see Example 1.3 in Tu, 2011) and satisfies $h^{-1}(\\{0\\})=(-\\infty,0]$ . It is easy to see that all derivatives of $h$ are 0 on $(-\\infty,0].$ , and decay to 0 at infinity (and hence are bounded on $\\mathbb{R}$ ) so that $h$ satisfies $\\mathrm{(SC_{d})}$ for any $d\\in\\mathbb{N}$ , and induces a cost function in $\\mathfrak{C}_{\\le}$ . ", "page_idx": 2}, {"type": "text", "text": "2) The squared hinge function $h(z)=(z)_{+}^{2}$ considered in Section 2.1 has linear growth, but is nonsmooth. Although this function can be smoothed using e.g. mollification as introduced in Friedrichs [1944], this will result in a cost c which is not an element of $\\mathfrak{C}_{\\le}$ and may be costly to implement due to the convolution operation used in mollification. ", "page_idx": 2}, {"type": "text", "text": "3) The logistic function $h(z)=\\log(1+e^{\\beta z})$ for $\\beta>0$ has linear growth and derivative $h^{\\prime}(z)=$ $\\begin{array}{r}{\\beta\\frac{e^{\\beta z}}{1+e^{\\beta z}}=\\beta\\varsigma(\\beta z)}\\end{array}$ , where $\\begin{array}{r}{\\varsigma(z)=\\frac{1}{1+e^{-z}}}\\end{array}$ is the sigmoid function. As $\\varsigma^{\\prime}(z)=\\varsigma(z)(1-\\varsigma(z))$ , it is easy to see that all derivatives of $h$ are bounded on so that the assumption $\\mathrm{(SC_{d})}$ is satisfied for any $d\\in\\mathbb{N}$ . Although $h$ does not induce a cost in $\\mathfrak{C}_{\\le}$ , it is increasing and decays to 0 faster than $e^{\\beta z}$ as $z\\rightarrow-\\infty$ . Moreover, if the induced cost satisfies $c(x,y)\\leq\\varepsilon_{0}$ , then $\\mathrm{max}_{i=1}^{d}(y_{i}-x_{i})\\leq$ $\\textstyle{\\frac{1}{\\beta}}\\log(e^{\\varepsilon_{0}}-1)$ . Thus, $\\begin{array}{r}{i f\\mathsf{O}\\mathsf{T}_{c}(\\mu,\\nu)=\\int_{\\{c\\leq\\varepsilon_{0}\\}}c d\\pi^{\\star}+\\int_{\\{c>\\varepsilon_{0}\\}}c d\\pi^{\\star}=\\varepsilon}\\end{array}$ for an optimal plan $\\pi^{\\star}$ and some small $\\varepsilon>0$ , one has that $\\begin{array}{r}{\\int_{\\{c>\\varepsilon_{0}\\}}c d\\pi^{\\star}\\leq\\varepsilon}\\end{array}$ so that $\\pi^{\\star}(\\{c>\\varepsilon_{0}\\})\\leq\\varepsilon/\\varepsilon_{0}$ i.e. $\\pi^{\\star}$ assigns at least mass $1-\\varepsilon/\\varepsilon_{0}$ to points $(x,y)$ for which $\\begin{array}{r}{\\operatorname*{max}_{i=1}^{d}(y_{i}-x_{i})\\le\\frac{1}{\\beta}\\log(e^{\\varepsilon_{0}}-1)}\\end{array}$ . Hence, for large $\\beta$ , $c$ enforces similar properties to a cost in ${\\mathfrak{C}}_{+}$ . Note also that $h$ can be viewed as a smooth surrogate for the $0/1$ loss for large $\\beta$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "At this point, a multivariate analogue to the univariate almost stochastic domination could be defined by analogy with Section 2.1. However, we highlight two major impasses which make the entropically regularized index considered in the following sections a far more palatable option in dimension $d\\,>\\,1$ . First, it is well-known that the expected rate of convergence of empirical OT generally suffers from the curse of dimensionality in statistical estimation; scaling as $\\bar{n^{-1/d}}$ (cf. e.g. Manole and Niles-Weed, 2024). Although Hundrieser et al. [2022] improve these rates as to depend on the minimum of the intrinsic dimensions of $\\mu,\\nu$ in place of $d$ , entropic optimal transport exhibits a preferable parametric rate of convergence. Next, solving the OT problem numerically between two finitely discrete distributions supported on $N$ points requires solving a linear program in $N^{2}$ variables which can be prohibitive for even moderately sized problems. ", "page_idx": 3}, {"type": "text", "text": "3 Entropic Regularization of OT with Multivariate FSD Compatible Costs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before defining the regularized index, we first provide some background on entropic optimal transport (EOT) with a cost $c:\\overline{{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}}}$ . EOT is defined by regularizing the OT problem (1) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal O}\\mathsf{T}_{c,\\lambda}(\\mu,\\nu)=\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int c d\\pi+\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi||\\mu\\otimes\\nu),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda\\geq0$ is a regularization parameter and $\\mathsf{D}_{\\mathsf{K L}}$ is the Kullback-Leibler divergence defined by $\\begin{array}{r}{\\mathsf{D}_{\\mathsf{K L}}(\\rho,\\eta)=\\int\\log\\left(\\frac{d\\rho}{d\\eta}\\right)d\\rho}\\end{array}$ if $\\rho$ is absolutely continuous with respect to $\\eta$ and $\\mathsf{D}_{\\mathsf{K L}}(\\rho,\\eta)=+\\infty$ otherwise. When $\\lambda=0$ , we recover the standard OT problem. If $c\\in L^{1}(\\mu\\otimes\\nu)$ , (2) admits a unique solution and is paired in strong duality with the problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\varphi\\in L^{1}(\\mu),\\psi\\in L^{1}(\\nu)}\\int\\varphi d\\mu+\\int\\psi d\\nu-\\lambda\\int e^{\\frac{\\varphi(x)+\\psi(y)-c(x,y)}{\\lambda}}d\\mu\\otimes\\nu(x,y)+\\lambda.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Solutions to (3) are known to be almost surely unique up to additive constants (i.e. if $(\\varphi,\\psi),(\\varphi^{\\prime},\\psi^{\\prime})$ solve (3), $\\varphi=\\varphi^{\\prime}+C\\,\\mu$ -almost surely and $\\psi=\\psi^{\\prime}-C\\:\\nu$ -almost surely for some constant $C\\in\\mathbb{R}$ ) and are uniquely determined for $\\mu$ -a.e. $x$ and $\\nu$ -a.e. $y$ by the so-called Schr\u00f6dinger system ", "page_idx": 3}, {"type": "equation", "text": "$$\ne^{-\\varphi(x)/\\lambda}=\\int e^{\\frac{\\psi(y)-c(x,y)}{\\lambda}}d\\nu(y),\\quad e^{-\\psi(y)/\\lambda}=\\int e^{\\frac{\\varphi(x)-c(x,y)}{\\lambda}}d\\mu(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which implies that $\\begin{array}{r}{\\int e^{\\frac{\\varphi(x)+\\psi(y)-c(x,y)}{\\lambda}}d\\mu\\otimes\\nu(x,y)=1}\\end{array}$ . EOT potentials satisfying (4) on the whole space are known to exist and are unique up to additive constants (see Lemma 6). We refer the reader to Nutz [2021] for a comprehensive introduction on EOT including all stated results. ", "page_idx": 3}, {"type": "text", "text": "Entropic regularization of OT problems was introduced in the seminal work of Cuturi [2013] as a means to accelerate computation by utilizing Sinkhorn-Knopp\u2019s matrix scaling algorithm which can be efficiently implemented on GPUs. Interestingly, entropic regularization also alleviates the curse of dimensionality rates in statistical estimation inherent to standard OT; for instance Genevay et al. [2019] and Mena and Niles-Weed [2019] show that the plug-in estimator for the EOT cost with fixed $\\lambda>0$ and cost $c(x,y)=\\|x-y\\|^{2}$ achieves a parametric expected rate of convergence with a dimension dependent constant (see also Groppe and Hundrieser, 2023, Stromme, 2023 for related results with the dimension replaced by the minimum intrinsic dimension of $\\mu$ and $\\nu$ ); del Barrio et al. [2023] and Goldfeld et al. [2024b] further establish a central limit theorem in this setting. ", "page_idx": 3}, {"type": "text", "text": "Given that EOT is meant to approximate OT, we derive the properties of $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ and its solutions as $\\lambda\\downarrow0$ . First, we quantify the rate of convergence of $\\mathsf{O T}_{h,\\lambda}$ to $\\mathsf{O T}_{0,\\lambda}$ under mild conditions, then show that solutions of $\\mathsf{O T}_{h,\\lambda}$ converge to solutions of $\\mathsf{O T}_{h,0}$ as $\\lambda\\downarrow0$ in a suitable sense. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Stability as $\\lambda\\downarrow0,$ ). Let $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d})$ have finite first moment, h be a function satisfying $\\mathrm{(SC_{d})}$ , and fix $\\delta>0$ . Then, ", "page_idx": 3}, {"type": "text", "text": "1. for any $\\lambda\\in[0,1)$ satisfying $\\lfloor\\lambda^{-d}\\rfloor\\geq\\left(20/\\delta\\right)^{d}$ , we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{0\\leq0{\\mathsf{T}}_{h,\\lambda}(\\mu,\\nu)-0{\\mathsf{T}}_{h,0}(\\mu,\\nu)\\leq d\\lambda\\log(1/\\lambda)+\\displaystyle\\frac{4L\\lambda}{\\delta}(5C_{\\delta}+20d)}}\\\\ {{\\omega r\\,C_{\\delta}=\\sum_{j=1}^{d}\\int|x_{j}|^{1+\\delta}d\\mu_{0}(x)\\wedge\\sum_{j=1}^{d}\\int|x_{j}|^{1+\\delta}d\\mu_{1}(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2. $i f\\pi_{\\lambda}$ is the unique solution to $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ for $\\lambda>0$ , then there exists a subsequence of $(\\pi_{\\lambda})_{\\lambda\\downarrow0}$ which converges weakly to a solution of $\\mathsf{O T}_{h,0}(\\mu,\\nu)$ . ", "page_idx": 4}, {"type": "text", "text": "We highlight that the implications of Theorem 1 require only the Lipschitz condition imposed in $\\mathrm{(SC_{d})}$ . The proof of the first result follows that of Theorem 3.3 in Eckstein and Nutz [2023] which controls the error of approximating the OT cost and OT plan using discretizations of the measures at play. The main novelty in our approach is to provide explicit constants and a simple argument showing that the rate at which a general measure on $\\mathbb{R}^{d}$ can be approximated by a finitely discrete measure on at most $n$ points under the 1-Wasserstein distance scales at worst as $n^{-1/d}$ for sufficiently large $n$ . The second statement is proved using the machinery of $\\Gamma$ -convergence (see Maso, 1993). Complete proofs are provided in Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "4 Entropic Multivariate FSD Violation Ratio and Testing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By analogy with the univariate case described in Section 2.1, we consider a normalized index of stochastic order violation given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varepsilon_{h,\\lambda}(\\mu,\\nu)=\\frac{0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we adopt the convention that $\\varepsilon_{h,\\lambda}(\\mu,\\nu)=0$ whenever $0{\\sf T}_{\\bar{h},\\lambda}(\\mu,\\nu)=0$ . Here, $0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)$ is the EOT problem with cost $\\begin{array}{r}{c({\\boldsymbol x},{\\boldsymbol y})=\\sum_{i=1}^{d}h(y_{i}-x_{i})+h(x_{i}-y_{i})}\\end{array}$ . This cost function is induced by the function $\\bar{h}(z)=h(z)+h(-z)$ and hence satisfies $\\mathrm{(SC_{d})}$ provided that $h$ satisfies $\\mathrm{(SC_{d})}$ . Moreover, $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)\\leq0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)$ by construction so that $\\varepsilon_{h,\\lambda}(\\mu,\\nu)\\in[0,1]$ yielding a normalized index. The corresponding notion of entropic multivariate almost stochastic dominance can thus be defined. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Entropic Multivariate Almost FSD). We define $(h,\\lambda,\\varepsilon_{0})-F S D_{;}$ , the entropic multivariate almost FSD via the violation ratio as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu\\underbrace{\\succ}_{(h,\\lambda,\\varepsilon_{0})-F S D}\\nu\\;i f\\varepsilon_{h,\\lambda}(\\mu,\\nu)\\leq\\varepsilon_{0}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In light of Theorem 1, lim\u03bb\u21930 \u03b5h,\u03bb(\u00b5, \u03bd) = OOTT\u00afhh,,00((\u00b5\u00b5,,\u03bd\u03bd)) with the convention that this latter quantity is zero when $0\\mathsf T_{\\bar{h},0}(\\mu,\\nu)=0$ . In the case that $h$ is the squared hinge function described in Example 1 and $d=1$ , we recover the univariate index from Section 2.1. As aforementioned, the squared hinge function is not sufficiently smooth to enable us to characterize the asymptotic fluctuations of the empirical index in arbitrary dimensions. See Example 1 which lists other examples of costs satisfying $\\mathrm{(SC_{d})}$ ; this condition is sufficient for the following statistical developments. We underscore that the choice of cost influences the notion of stochastic dominance reflected by the violation ratio. In applications where a practitioner wishes to formulate a domain-specific notion of dominance, a data-driven approach can be employed by replacing the fixed cost $h$ in the previous development by a collection of costs $(h_{i,\\theta_{i}})_{i=1}^{d}$ for each dimension where $\\theta_{i}$ is the corresponding parameter (e.g. $\\beta$ in the logistic function) and optimizing over these parameters. The results presented herein readily adapt to this setting, but, for simplicity, we restrict our attention to the case of a fixed $h$ troughout. ", "page_idx": 4}, {"type": "text", "text": "4.1 Statistical Properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now lay the groundwork for performing principled statistical inference with the empirical estimator of the entropic index $\\varepsilon_{h,\\lambda}$ . Namely, we establish the asymptotic properties of the plug-in estimator $\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n},\\hat{\\nu}_{n})$ , where $\\begin{array}{r}{\\hat{\\mu}_{n}\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{X_{i}},\\hat{\\nu}_{n}\\,=\\,\\frac{1}{n}\\sum_{j=1}^{n}\\delta_{Y_{j}}}\\end{array}$ are the empirical distributions from $n$ independent observations, $(X_{i})_{i=1}^{n}$ and $(Y_{j})_{j=1}^{n}$ of $\\mu$ and $\\nu$ respectively. Furthermore, we establish consistency of the bootstrap procedure. To this end, given sets of $n$ iterations observations of $\\mu$ and $\\nu$ , $(X_{i})_{i=1}^{n}$ and $(Y_{j})_{j=1}^{n}$ as above and sets $(X_{i}^{B})_{i=1}^{n}$ and $(Y_{j}^{B})_{j=1}^{n}$ of $n$ independent samples from $\\textstyle{\\hat{\\mu}}_{n}$ and $\\hat{\\nu}_{n}$ , $\\begin{array}{r}{\\hat{\\mu}_{n}^{B}:=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{X_{i}^{B}}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\nu}_{n}^{B}:=\\frac{1}{n}\\sum_{j=1}^{n}\\delta_{Y_{j}^{B}}}\\end{array}$ are the corresponding bootstrap empirical distributions. $\\mathbb{P}^{B}$ denotes the conditional probability given the data. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Limit distribution and bootstrapping). Assume that $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d})$ are sub-Gaussian with a shared parameter $\\tau^{2}>0$ and that $h$ satisfies $\\mathrm{(SC_{d})}$ . Let $(\\varphi_{h},\\psi_{h})$ and $(\\varphi_{\\bar{h}},\\psi_{\\bar{h}})$ be any pairs of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ and $0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)$ respectively satisfying the Schr\u00f6dinger system (4) on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ . Then, $i f{\\mit0}\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)>0$ , ", "page_idx": 5}, {"type": "text", "text": "1. ${\\sqrt{n}}\\left(\\varepsilon_{h,\\lambda}({\\hat{\\mu}}_{n},{\\hat{\\nu}}_{n})-\\varepsilon_{h,\\lambda}(\\mu,\\nu)\\right)\\ \\stackrel{d}{\\to}\\ N(0,\\sigma^{2})$ , a mean-zero Gaussian with variance $\\sigma^{2}\\mathbf{\\Sigma}=$ $\\begin{array}{r}{\\mathrm{var}_{\\mu}\\left(\\frac{1}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}\\varphi_{h}-\\frac{0\\top_{h,\\lambda}(\\mu,\\nu)}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\varphi_{\\bar{h}}\\right)+\\mathrm{var}_{\\nu}\\left(\\frac{1}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}\\psi_{h}-\\frac{0\\top_{h,\\lambda}(\\mu,\\nu)}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\psi_{\\bar{h}}\\right).}\\end{array}$ 2. $I f\\sigma^{2}>0,\\operatorname*{sup}_{t\\in\\mathbb{R}}\\left|\\mathbb{P}^{B}\\left(\\sqrt{n}(\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n}^{B},\\hat{\\nu}_{n}^{B})-\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n},\\hat{\\nu}_{n}))\\leq t\\right)-\\mathbb{P}(N(0,\\sigma^{2})\\leq t)\\right|\\overset{\\mathbb{P}}{\\rightarrow}0.$ ", "page_idx": 5}, {"type": "text", "text": "Observe that the limiting distribution in Theorem 2 is non-pivotal in the sense that the variance depends on the population distributions $(\\mu,\\nu)$ rendering direct estimation of the limiting variance highly non-trivial. The bootstrap consistency result in the second point enables us to establish confidence intervals for $\\varepsilon_{h,\\lambda}(\\mu,\\nu)$ . Explicitly, if $\\zeta_{\\beta}$ denotes the smallest value of $t\\in\\mathbb R$ for which $\\mathbb{P}^{B}\\left(\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n}^{B},\\hat{\\nu}_{n}^{B})\\leq t\\right)\\geq1-\\beta$ for any $\\beta\\in(0,1)$ , then $\\varepsilon_{h,\\lambda}(\\mu,\\nu)\\in[0,2\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n},\\hat{\\nu}_{n})-\\zeta_{\\alpha}]$ with probability approaching $1-\\alpha$ for any $\\alpha\\in(0,1)$ due to Lemma 23.3 in Van der Vaart [2000]. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 is based on the functional delta method [R\u00f6misch, 2006] which extends the standard delta method to functionals defined on normed vector spaces, following the framework of Goldfeld et al. [2024b]. Formally, this approach consists of showing that the functional mapping $\\tau^{2}$ -sub-Gaussian distributions $(\\eta,\\rho)$ to $\\varepsilon_{h,\\lambda}(\\eta,\\rho)$ is directionally differentiable at $(\\mu,\\nu)$ and Lipschitz continuous in a suitable sense and that the relevant potentials lie in a space of sufficiently smooth functions using the a\u221assumption $\\mathrm{(SC_{d})}$ . Smoothness of the potentials is crucial to ensure that the empirical processes $\\sqrt{n}(\\hat{\\mu}_{n}-\\mu)$ , $\\sqrt{n}(\\hat{\\nu}_{n}-\\nu)$ converge when treated as functionals on the aforementioned space of smooth functions. Complete details are included in Appendix C.3, and a primer on the functional delta method can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Remark 1 (On Theorem 2). 1) We note that the condition that $0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)>0$ in Theorem 2 is satisfied except in certain degenerate settings. Indeed, for a general nonnegative cost $c$ , $0\\mathsf{T}_{c,\\lambda}(\\mu,\\nu)=0$ if and only if $\\int c d\\mu\\otimes\\bar{\\nu}=0$ as follows from the fact that $\\mathsf{D}_{\\mathsf{K L}}(\\pi\\|\\mu\\otimes\\nu)\\geq0$ with equality if and only if $\\ '\\pi=\\mu\\otimes\\nu$ . In particular, ${0\\sf T}_{\\bar{h},\\lambda}(\\mu,\\nu)=0$ if and only if ${\\dot{h}}(x_{i}-y_{i})=h(y_{i}-x_{i})=0$ for every $x\\in\\operatorname{spt}(\\mu)$ and $y\\,\\in\\,\\mathrm{spt}(\\nu)$ and every $i\\in\\{1,\\ldots,d\\}$ . If $h$ is chosen as to generate a cost function which is compatible with multivariate $F S D$ (recall Definition $^{l}$ ), $h^{-1}(\\{0\\})=(-\\infty,0]\\,s o$ that ${0\\sf T}_{\\bar{h},\\lambda}(\\mu,\\nu)=0$ if and only if \u00b5 and $\\nu$ are point masses at some shared $a\\in\\mathbb{R}^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "2) Theorem 2 is presented in the balanced case with empirical measures from n samples. In the case where $\\textstyle{\\hat{\\mu}}_{n}$ and $\\hat{\\nu}_{m}$ are empirical measures from $n\\not=m$ samples with $\\textstyle{\\frac{n}{n+m}}\\;\\to\\;s\\;\\in\\;(0,1)$ , $\\begin{array}{r}{\\mathrm{var}_{\\mu}\\left(\\frac{1-s}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}\\varphi_{h}-\\frac{s0\\top_{h,\\lambda}(\\mu,\\nu)}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\varphi_{\\bar{h}}\\right)+\\mathrm{var}_{\\nu}\\left(\\frac{1-s}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}\\psi_{h}-\\frac{s0\\top_{h,\\lambda}(\\mu,\\nu)}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\psi_{\\bar{h}}\\right)}\\end{array}$ nn+mm in place of  n and \u03c3s2 = ce of $\\sigma^{2}$ ", "page_idx": 5}, {"type": "text", "text": "4.2 Multivariate FSD Hypothesis Testing In ML Models Benchmarking ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the statistical properties of the violation ratio in hand, we now consider using the violation ratio in the context of statistical testing for $(h,\\lambda,\\varepsilon_{0})-$ FSD. Consider two $d$ -dimensional distributions $\\mu,\\,\\nu$ . In our application, these will correspond to the distributions of performance of two language models\u2019 responses evaluated on $d$ metrics. Given $n,m$ samples from $\\mu,\\,\\nu$ respectively, we can apply Theorem 2 to create statistically valid tests comparing $\\mu$ and $\\nu$ . Similarly to Nitsure et al. [2023], we consider both absolute and relative testing (see Nitsure et al., 2023 for a complete discussion). ", "page_idx": 5}, {"type": "text", "text": "Absolute testing The most straightforward application of Theorem 2 is to specify a desired threshold   \n$\\varepsilon_{0}$ and consider the following hypothesis test for $(h,\\lambda,\\varepsilon_{0})-$ FSD: $H_{0}:\\mu\\eqno\\nu$ versus the $(h,\\lambda,\\dot{\\varepsilon}_{0})\\!-\\!\\mathrm{FSD}$   \nalternative $H_{1}:\\mu\\qquad\\succcurlyeq\\qquad\\nu.$ Note that $\\nu$ dominating $\\mu$ would be tested separately. Given a desired $(h,\\lambda,\\varepsilon_{0})\\!-\\!\\mathrm{FSD}$ ", "page_idx": 5}, {"type": "text", "text": "confidence $1-\\alpha$ , the central limit theorem and bootstrap results in Theorem 2 suggest rejecting $H_{0}$ if ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varepsilon_{h,\\lambda}(\\hat{\\mu}_{n},\\hat{\\nu}_{m})\\leq\\varepsilon_{0}+\\sqrt{\\frac{m+n}{m n}}\\sigma_{B}(\\hat{\\mu}_{n},\\hat{\\nu}_{m})\\Phi^{-1}(\\alpha),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\sigma_{B}^{2}$ is the bootstrapped variance (See Algorithm 1) and $\\Phi$ is the CDF of the standard normal distribution. By Theorem 2, this test will be asymptotically valid. ", "page_idx": 6}, {"type": "text", "text": "Relative testing A downside of absolute testing is that it requires specifying a threshold $\\varepsilon_{0}$ . This threshold can be meaningful in pairwise comparisons, but when multiple distributions (e.g. multiple language models evaluations) are being compared for ranking purposes, it is difficult to determine a priori what threshold to use to ensure that the distributions can be separated. As in Nitsure et al. [2023], we therefore also present a relative test that compares each of $k$ random vectors with measures $\\mu_{1},\\ldots,\\mu_{k}$ using a one-versus-all violation ratio. First, consider all pairs of violations ratios between the k measures: \u03b5i(jh, $\\varepsilon_{i j}^{(h,\\lambda)}\\,=\\,\\varepsilon_{h,\\lambda}(\\mu_{i},\\mu_{j})$ for $i,j\\,\\in\\,\\{1\\,.\\,.\\,k\\},i\\,\\neq\\,j$ . Let $M=(\\mu_{1},\\cdot\\cdot\\cdot\\mu_{k})$ , and define the one-versus-all violation ratio of the dominance of $\\mu_{i}$ on all other variables $\\mu_{j},j\\neq i$ : $\\begin{array}{r}{\\varepsilon_{i}^{(h,\\lambda)}(M)=\\frac{1}{k-1}\\sum_{j\\neq i}\\varepsilon_{i j}^{(h,\\lambda)}}\\end{array}$ \u03b5i(jh,\u03bb). We can then define relative stochastic dominance (h, \u03bb)-R-FSD as $\\mu_{i_{1}}\\underset{R-\\mathrm{FSD}}{\\asymp}\\mu_{i_{2}}\\,.\\,.\\,.\\,\\underset{R-\\mathrm{FSD}}{\\asymp}\\mu_{i_{k}}\\,\\iff\\,\\varepsilon_{i_{1}}^{(h,\\lambda)}(M)\\,\\leq\\,\\cdots\\,\\leq\\,\\varepsilon_{i_{k}}^{(h,\\lambda)}(M)$ . Here the most dominating model is the one with the lowest one-versus-all violation ratio. Testing for relative dominance of $\\mu_{i}$ on $\\mu_{j}$ we can then compare their one-versus-all ratios via the following statistic: $\\Delta\\varepsilon_{i j}^{(\\ell)}(M)=$ $\\varepsilon_{i}^{(\\ell)}(M)-\\varepsilon_{j}^{(\\ell)}(M)$ .To test for $(h,\\lambda)$ -R-FSD of $\\mu_{i}$ versus $\\mu_{j}$ then, we have the null hypothesis $H_{0}:\\Delta\\varepsilon_{i j}(M)\\geq0$ versus the alternative $H_{1}:\\Delta\\varepsilon_{i j}(M)<0$ . It is possible to extend the central limit theorem and bootstrapping results in Theorem 2 to this relative statistic under an independence assumption (omitted for brevity). Let $\\hat{M}_{n}=(\\hat{\\mu}_{1,n},\\dots\\hat{\\mu}_{k,n})$ be the empirical measures for $n$ samples from each distribution. As in the absolute case, we then reject $H_{0}$ with a confidence $1\\mathrm{~-~}\\alpha$ if: $\\begin{array}{r}{\\Delta\\varepsilon_{i_{1},i_{2}}(\\hat{M}_{n})\\,\\leq\\,\\sqrt{\\frac{1}{n}}\\sigma_{B,\\mathrm{relative}}(i_{1},i_{2})\\Phi^{-1}(\\alpha)}\\end{array}$ where $\\sigma_{B,\\mathrm{relative}}^{2}(i_{1},i_{2})$ is the bootstrapped variance (see Algorithm 1 for the variance expression). ", "page_idx": 6}, {"type": "text", "text": "Multitesting and Ranking To apply the above multivariate FSD violation ratio hypothesis tests to ranking of multiple distributions, we follow the approach of Nitsure et al. [2023]. We aggregate the set of all pairwise tests, ensure multitesting statistical validity via Family-Wise Error Rate (FWER) control, and, if needed, aggregate the pairwise test results into a numerical ranking. Our approach is described below and summarized in Algorithm 1. The overall complexity of performing one pairwise test is dominated by the cost of computing the EOT cost with $h$ and $\\bar{h}$ . To this end, the Sinkhorn algorithm is used [Cuturi, 2013], which computes EOT between distributions on $N$ points with a complexity of $O(\\bar{N}^{2}K(d)+N^{2})$ , where $K(\\bar{d})$ denotes the cost of complexity of computing the cost $c(x,y)$ between $x,y\\in\\mathbb{R}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Ranking multiple distributions In our experiments below, we seek to use the pairwise tests above to obtain a statistically valid ranking of a set of $k$ random vectors $X^{(i)}$ with measures $\\mu^{(i)}$ , e.g. samples of $X^{(i)}$ can be the per-sample evaluation metrics for a set of language models. To rank $k$ models at a specified significance level $\\alpha$ , we first test all $k^{2}-k$ pairs $(\\mu^{(i)},\\mu^{(j)})$ , $i\\neq j$ , employing a FWER correction (see next paragraph) to guarantee a valid control on the overall false rejection rate. This yields a set of trinary outcomes for each $(i,j)$ pair, with 1 if the null is rejected in the positive direction, $^-1$ if the null is rejected in the negative direction, and 0 if the null is not rejected. These pairwise rankings are then combined into a single rank using a simple Borda count [de Borda, 1781] rank aggregation algorithm. ", "page_idx": 6}, {"type": "text", "text": "Multitesting FWER control When running a family of $T$ tests each at a significance level $1-\\alpha$ , the true probability that at least one test falsely rejects the null scales with $T$ . If the output of all $T$ tests needs to be trusted simultaneously, instead it is desirable that the probability of any test falsely rejects the null is less than or equal to the specified $\\alpha$ . Achieving this requires adjusting, or \u201ccorrecting\u201d the significance level of each of the $T$ tests, in a process called Family-Wise Error Rate (FWER) control. In the present work, we use the Bonferroni correction, which sets the significance level of the ith test to $1-\\alpha_{i}$ with $\\alpha_{i}=\\alpha/T$ . Note that while the Bonferroni correction is known to be pessimistic, we choose it as it sets uniform significance levels for all tests, as opposed to other strategies such as the Holm correction [Holm, 1979] which is tighter but yields highly nonuniform statistical power across the family of tests. Exploring sensible ways to employ nonuniform FWER control approaches in the context of performance ranking is an interesting avenue for future work. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All experiments were run on NVIDIA A100 80GB GPUs using PyTorch [Ansel et al., 2024] (v.2.3.0, BSD-3 license) and the Python Optimal Transport package [Flamary et al., 2021] (v.0.9.3, MIT license) to compute optimal transport distances with and without regularization. Code for these experiments is available at https://github.com/IBM/stochastic-order-eval. ", "page_idx": 7}, {"type": "text", "text": "5.1 Synthetic Data Experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we analyze our method on a synthetic toy dataset that enables us to parametrically control the level of the multivariate stochastic dominance between two random variables. Given a dimension $d$ , a parameter $p\\in[0,1]$ , and mean and variance parameters $\\mu,\\sigma^{2}$ , our synthetic dataset is generated by sampling from the multivariate random variables $X,Y\\in\\mathbb{R}^{d}$ : ", "page_idx": 7}, {"type": "text", "text": "\u2022 $X_{i}\\sim{\\mathcal{N}}(\\mu,\\sigma^{2})\\;{\\mathrm{for~}}i=1,\\ldots,d$ \u2022 Yi = Xi + (2 \u00b7 Bi(p) \u22121)Ui with $B_{i}(p)=\\mathrm{{Bernoulli}}(p)\\in\\{0,1\\}$ and $U_{i}=\\mathrm{Uniform}(0,1)$ . ", "page_idx": 7}, {"type": "text", "text": "These variables $X$ and $Y$ are designed in such a way that $p$ parametrizes the dominance of $Y$ over $X$ .   \nIn particular, $\\ensuremath{\\boldsymbol{X}}\\ensuremath{\\mathrm{~\\boldmath~\\suc~\\psi~}}\\succcurlyeq\\ensuremath{\\mathrm{~\\boldmath~\\chi~}}\\ensuremath{\\boldsymbol{Y}}$ if $p<0.5$ , and $Y\\quad\\ \\succcurlyeq\\ \\ \\quad X$ if $p>0.5$ . $(h,\\lambda)\\!-\\!\\operatorname{FSD}$ $(h,\\lambda)\\!-\\!\\mathrm{FSD}$ ", "page_idx": 7}, {"type": "text", "text": "As a baseline for our synthetic experiments, we also compute the violation ratio for the standard FSD framework using unregularized OT (EMD), i.e. $\\varepsilon_{\\mathrm{hinge,0}}$ , as a function of $p$ for fixed $d\\,=\\,5$ , $\\mu=0$ , $\\sigma^{2}=1.0$ and $N=100$ samples from $X$ and $Y$ . We then investigate how well this baseline is approximated by $\\varepsilon_{\\log,\\lambda>0}$ , the entropically regularized ratio with a logistic cost as in Example 1. Fig. 1 shows that as the entropic regularization parameter $\\lambda$ decreases towards 0 and as the gain of the logistic cost $\\beta$ increases, $\\varepsilon_{\\log,\\lambda>0}$ converges towards $\\varepsilon_{\\mathrm{hinge,0}}$ across all values of $p\\in[0,1]$ . In all cases, multivariate FSD violation ratio predicts linearly $p$ , indicating that it is captures well the FSD violations. This experiment indicates that, to best approximate the standard FSD violation ratio, $\\lambda$ should be taken as small as possible (c.f. Theorem 1) and $\\beta$ should be taken as large as possible (c.f. Example 1). There is, in practice, a tradeoff that must be made when computing the regularized index, as Sinkhorn\u2019s algorithm requires the matrix $e^{-C/\\lambda}$ , where $C$ is the matrix of pairwise costs. As such, if the ratio $\\beta/\\lambda$ is too large, numerical underflow will occur and the algorithm will fail. As a rule of thumb, it is recommended to set $\\beta$ first and increase the value of $\\lambda$ if instability occurs. ", "page_idx": 7}, {"type": "image", "img_path": "NCX3Kgb1nh/tmp/e2639f6e92d16bee8f01babba036bc5c5c064a29c8891ffb0e6b2fed56b6de3a.jpg", "img_caption": ["Figure 1: Convergence of $\\varepsilon_{\\log,\\lambda>0}$ towards $\\varepsilon_{\\mathrm{hinge,0}}$ in the synthetic dataset introduced in this section. Left panel: for a fixed parameter $\\beta=8$ of the logistic cost, $\\varepsilon_{\\log,\\lambda>0}$ converge towards $\\varepsilon_{\\mathrm{hinge,0}}$ as $\\lambda$ is decreased toward 0. Right panel: for a fixed entropic regularization parameter $\\lambda=0.1$ , $\\varepsilon_{\\mathrm{log},\\lambda}$ converges towards $\\varepsilon_{\\mathrm{hinge,0}}$ as the gain of the logistic cost $\\beta$ increases. All simulations were generated for $d=5$ , $\\mu=0$ , $\\sigma^{2}\\,=\\,1.0$ and $N=100$ . Points and error bars indicate average and standard deviation across 100 repetitions. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We now assess the power of our proposed test with as a function of the number of samples and the dimension. We consider the same setup as the previous experiment and set $p=0.65$ so that ", "page_idx": 7}, {"type": "image", "img_path": "NCX3Kgb1nh/tmp/d57da862ee91dfa18ce3435979791ce30666803f23b4714bc5e7ed912ea45c84.jpg", "img_caption": ["$Y\\qquad\\succcurlyeq\\qquad X$ . We then estimate the type I and type II error of the relative test statistic by $(h,\\lambda,0.5)\\!-\\!\\mathrm{FSD}$ averaging across 100 repetitions, these results are compiled in Fig. 2. ", "Figure 2: Type I and type II error of the relative test statistic as a function of the sample size $n$ in dimension $\\dot{d}\\in\\{10,20\\dot{,}\\dot{5}0\\}$ . Here, $\\beta=8$ and $\\lambda=0.01d$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "NCX3Kgb1nh/tmp/d656c1f1166c3d68620098cb1c8bb213e56762367e0bb18ae97716e0f3b1385f.jpg", "img_caption": ["Figure 3: Mix Instruct Results: Comparison of Multivariate FSD to Reduction to univariate FSD with aggregation across the dimensions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 LLM Benchmarking ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test our method on real world scenarios, we have chosen a current topic of significant interest to the community: LLM Benchmarking. We show through our experiments that our method can provide a more holistic ranking of LLMs evaluated on different metrics as opposed to present strategies which involve mean win rate. To demonstrate our method\u2019s application to LLM Benchmarking, we have conducted assessments on two different sets of data. ", "page_idx": 8}, {"type": "text", "text": "Mixinstruct For our first evaluation we use the dataset from Jiang et al. [2023] (MIT license) that consists of responses from 12 different instruction following LLMs, with each response evaluated on 9 metrics such as BLEU, ROUGE, BERTScore, BARTScore, etc. The data has a train (100K rows) and test $^\\mathrm{5k}$ rows) split where each row consists of an instruction, input sentence, the expected output from users, as well as the responses of a set of different LLMs with their decoding parameters and evaluation scores on different metrics. However for the test set, Jiang et al. [2023] also did a pairwise evaluation of the responses from the models by asking ChatGPT which response was better. We use this test set and generate a ranking of the LLMs using Entropic Multivariate FSD (Algorithm 1), where for each LLM we construct an empirical measure on $\\mathbb{R}^{9}$ using $n$ samples varying from 100 to 5000. We then compute the pairwise ratios for these empirical distributions using the logistic loss with $\\beta=0.2$ , the regularization parameter $\\lambda=0.1$ , and utilize the relative testing procedure from Section 4.2 to rank the 12 LLMS (see Fig. 4 for the ranking obtained with $n=5000$ in appendix Appendix B). The confidence intervals are then generated using 1000 bootstrap repetitions. Finally, we compare the resulting ranking with the (univariate ranking) provided by ChatGPT scoring (which serves as a human proxy) as a function of the sample size $n$ using Kendall Tau similarity. These results are presented in Fig. 3. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We then compare different methods that reduce multivariate ordering to univariate FSD via aggregation. The first method is a portfolio aggregation with Independent Copula P(IC) [Nitsure et al., 2023], where a dimension is normalized with a global univariate CDF across all models and a geometric mean is performed across all dimensions. A univariate FSD is then applied on the resulting univariate random variables. The second method, referred to as portfolio aggregation with Empirical Copula P(EC) [Ruschendorf, 1976, Ulan et al., 2021], estimates a global multivariate CDF across all models, and then assigns to each evaluation vector the value of its CDF. Similarly, a univariate FSD is applied on this one dimensional data. ", "page_idx": 9}, {"type": "text", "text": "Results We see from Fig. 3 that the multivariate FSD, is sample efficient and has the highest Kendall tau rank similarity with GPT score. We hypothesize that this thanks to its ability to capture dependencies between the metrics. The independent copula P(IC), ignores the dependencies and hence lags a little behind but is still sample efficient. Whilst the empirical copula P(EC) captures the dependencies, it suffers from the curse of dimension and is not sample efficient. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed entropic multivariate FSD violation ratio as a statistic for assessing multivariate first order dominance. We addressed the convergence of these ratios as the entropic regularization goes to zero and established a central limit theorem and bootstrap consistency for this statistic. These statistical properties were leveraged in a framework for multivariate FSD testing which was applied to multi-metrics benchmarking machine learning models, showing its benefits in capturing the metric dependencies. Casting testing for stochastic order as an optimal transport problem with a smooth cost and devising an entropic regularization to ensure beneficial statistical and computational properties is an interesting framework that we envision to be useful and versatile for other stochastic orders. For instance the $\\mu$ -first order dominance of Galichon and Henry [2012] uses optimal transport maps as multivariate quantiles [Carlier et al., 2014] and defines a $\\mu$ -stochastic dominance; our entropic violation ratio framework can be extended to that case and, upon proving central limit theorems on the OT potentials, will lead to similar central limit theorems to the one presented in this work. Similarly, for the multivariate Lorenz order [Fan et al., 2024] that is of interest when the agent making the choice is risk averse. The Lorenz order can be expressed in terms of optimal transport maps and can be extended to our statistical testing framework using the tools introduced in this paper. We leave these developments for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "G. Rioux is partially supported by the NSERC postgraduate fellowship PGSD-567921-2022. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Birkh\u00e4user Basel, Basel, 2005. URL https://doi.org/10.1007/978-3-7643-8722-8.   \nJ. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929\u2013947, 2024.   \nP. Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.   \nR. Bommasani, P. Liang, and T. Lee. Holistic evaluation of language models. Annals of the New York Academy of Sciences, 2023.   \nG. Carlier, V. Chernozhukov, and A. Galichon. Vector quantile regression: an optimal transport approach. arXiv preprint arXiv:1406.4643, 2014.   \nY. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.   \nG. Constantine and T. Savits. A multivariate Faa di Bruno formula with applications. Transactions of the American Mathematical Society, 348(2):503\u2013520, 1996.   \nM. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/ file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf.   \nJ.-C. de Borda. M\u00e9moire sur les \u00e9lections au scrutin. Histoire de l\u2019Acad\u00e9mie Royale des Sciences, 1781.   \nE. del Barrio, J. A. Cuesta-Albertos, and C. Matr\u00e1n. An optimal transportation approach for assessing almost stochastic order. In E. Gil, E. Gil, J. Gil, and M. \u00c1. Gil, editors, The Mathematics of the Uncertain: A Tribute to Pedro Gil, pages 33\u201344. Springer International Publishing, Cham, 2018. URL https://doi.org/10. 1007/978-3-319-73848-2_3.   \nE. del Barrio, A. G. Sanz, J.-M. Loubes, and J. Niles-Weed. An improved central limit theorem and fast convergence rates for entropic transportation costs. SIAM Journal on Mathematics of Data Science, 5(3): 639\u2013669, 2023. URL https://doi.org/10.1137/22M149260X.   \nR. Dror, G. Baumer, S. Shlomov, and R. Reichart. The hitchhiker\u2019s guide to testing statistical significance in natural language processing. In Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1383\u20131392, 2018.   \nR. M. Dudley. Uniform central limit theorems, volume 142. Cambridge university press, 2014.   \nS. Eckstein and M. Nutz. Convergence rates for regularized optimal transport via quantization. Mathematics of Operations Research, Articles in Advance, pages 1\u201318, 2023. URL https://doi.org/10.1287/moor. 2022.0245.   \nY. Fan, M. Henry, B. Pass, and J. A. Rivero. Lorenz map, inequality ordering and curves based on multidimensional rearrangements, 2024.   \nR. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko, A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-451.html.   \nG. B. Folland. Real analysis: modern techniques and their applications, volume 40. John Wiley & Sons, 1999.   \nK. O. Friedrichs. The identity of weak and strong extensions of differential operators. Transactions of the American Mathematical Society, 55:132\u2013151, 1944.   \nA. Galichon and M. Henry. Dual theory of choice with multivariate risks. Journal of Economic Theory, 147(4): 1501\u20131516, 2012.   \nA. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyr\u00e9. Sample complexity of sinkhorn divergences. In K. Chaudhuri and M. Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 1574\u2013 1583. PMLR, 16\u201318 Apr 2019. URL https://proceedings.mlr.press/v89/genevay19a.html.   \nZ. Goldfeld, K. Kato, G. Rioux, and R. Sadhu. Limit theorems for entropic optimal transport maps and sinkhorn divergence. Electronic Journal of Statistics, 18(1):980\u20131041, 2024a.   \nZ. Goldfeld, K. Kato, G. Rioux, and R. Sadhu. Statistical inference with regularized optimal transport. Information and Inference: A Journal of the IMA, 13(1):iaad056, 2024b.   \nS. Graf and H. Luschgy. Foundations of Quantization for Probability Distributions. Springer Berlin Heidelberg, Berlin, Heidelberg, 2000. URL https://doi.org/10.1007/BFb0103945.   \nM. Groppe and S. Hundrieser. Lower complexity adaptation for empirical entropic optimal transport. arXiv preprint arXiv:2306.13580, 2023.   \nS. Holm. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pages 65\u201370, 1979.   \nY. Huang, Q. Zhang, L. Sun, et al. Trustgpt: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507, 2023.   \nS. Hundrieser, T. Staudt, and A. Munk. Empirical optimal transport between different measures adapts to lower complexity. arXiv preprint arXiv:2202.10434, 2022.   \nS. Hundrieser, M. Klatt, and A. Munk. Limit distributions and sensitivity analysis for empirical entropic optimal transport on countable spaces. The Annals of Applied Probability, 34(1B):1403 \u2013 1468, 2024. doi: 10.1214/23-AAP1995. URL https://doi.org/10.1214/23-AAP1995.   \nD. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.   \nR. W. Keener. Theoretical Statistics: Topics for a Core Course. Springer New York, New York, NY, 2010. URL https://doi.org/10.1007/978-0-387-93839-4.   \nN. Kouaissah. Using multivariate stochastic dominance to enhance portfolio selection and warn of financial crises. The Quarterly Review of Economics and Finance, 80:480\u2013493, 2021.   \nT. Manole and J. Niles-Weed. Sharp convergence rates for empirical optimal transport with smooth costs. The Annals of Applied Probability, 34(1B):1108\u20131135, 2024.   \nG. D. Maso. An introduction to \u0393-convergence. Birkh\u00e4user Boston, Boston, MA, 1993. URL https: //doi.org/10.1007/978-1-4612-0327-8.   \nG. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 5acdc9ca5d99ae66afdfe1eea0e3b26b-Paper.pdf.   \nMosaicLM. Llm evaluation scores. https://www.mosaicml.com/llm-evaluation, 2023.   \nS. Nietert, Z. Goldfeld, and K. Kato. Smooth $p$ -Wasserstein distance: structure, empirical approximation, and statistical applications. In International Conference on Machine Learning, pages 8172\u20138183. PMLR, 2021.   \nA. Nitsure, Y. Mroueh, M. Rigotti, K. Greenewald, B. Belgodere, M. Yurochkin, J. Navratil, I. Melnyk, and J. Ross. Risk assessment and statistical significance in the age of foundation models. arXiv preprint arXiv:2310.07132, 2023.   \nM. Nutz. Introduction to entropic optimal transport. Lecture notes, Columbia University, 2021.   \nW. Ogryczak and A. Ruszczynski. Dual stochastic dominance and related mean-risk models. SIAM Journal on Optimization, 13(1):60\u201378, 2002.   \nW. R\u00f6misch. Delta method, infinite dimensional. In Encyclopedia of Statistical Sciences. John Wiley & Sons, Ltd, 2006. URL https://doi.org/10.1002/0471667196.ess3139.   \nL. Ruschendorf. Asymptotic Distributions of Multivariate Rank Order Statistics. The Annals of Statistics, 4(5): 912 \u2013 923, 1976.   \nF. Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling. Springer International Publishing, Cham, 2015. URL https://doi.org/10.1007/978-3-319-20828-2.   \nM. Shaked and J. G. Shanthikumar. Stochastic Orders. Springer New York, New York, NY, 2007. URL https://doi.org/10.1007/978-0-387-34675-5.   \nA. Shapiro. On concepts of directional differentiability. Journal of optimization theory and applications, 66: 477\u2013487, 1990.   \nA. Shapiro. Asymptotic analysis of stochastic programs. Annals of Operations Research, 30(1):169\u2013186, Dec. 1991.   \nA. J. Stromme. Minimum intrinsic dimension scaling for entropic optimal transport. arXiv preprint arXiv:2306.03398, 2023.   \nL. W. Tu. An Introduction to Manifolds. Springer New York, New York, NY, 2011. URL https://doi.org/ 10.1007/978-1-4419-7400-6.   \nM. Ulan, W. L\u00f6we, M. Ericsson, and A. Wingkvist. Copula-based software metrics aggregation. Software Quality Journal, 29(4):863\u2013899, 2021. URL https://doi.org/10.1007/s11219-021-09568-9.   \nD. Ulmer, C. Hardmeier, and J. Frellsen. deep-significance-easy and meaningful statistical significance testing in the age of neural networks. arXiv preprint arXiv:2204.06815, 2022.   \nA. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.   \nA. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer New York, New York, NY, 1996. URL https://doi.org/10.1007/978-1-4757-2545-2.   \nV. S. Varadarajan. On the convergence of sample probability distributions. Sankhy\u00afa: The Indian Journal of Statistics (1933-1960), 19(1/2):23\u201326, 1958.   \nC. Villani. Optimal Transport: Old and New. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. URL https://doi.org/10.1007/978-3-540-71050-9. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "E. B. . C. F. . N. H. S. H. N. L. N. R. O. S. L. T. T. Wolf. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open_llm_leaderboard, 2023. G. Zhang and M. Hardt. Inherent trade-offs between diversity and stability in multi-task benchmarks, 2024. ", "page_idx": 12}, {"type": "text", "text": "A Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 describes our multitesting-based ranking procedure, for both the absolute and relative testing frameworks. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Multivariate Stochastic Order Multi-testing (relative and absolute) 1: Input: $\\mu_{1},...,\\mu_{k}$ , $k$ models we want to rank corresponding to empirical measure $\\,p_{1}~=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\Dot{\\delta}_{x_{i}^{1}},\\,.\\,.\\,.\\,p_{k}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{x_{i}^{k}}}\\end{array}$ , Threshold: $\\tau$ . 2: Input: Desired $h,\\lambda,B$ number of bootstraps, $m=K^{2}$ number of comparisons, significance level $\\alpha$ . 3: Cache the bootstraps samples 4: for $j=1$ to $k$ do 5: $p_{j}^{0}\\leftarrow p_{j}$ 6: for $b=1$ to $B$ do 7: $p_{j}^{b}\\leftarrow$ RESAMPLEWITHREPLACEMENT $(p_{j},n)$ 8: Compute all violation ratios 9: for $b=0$ to $B$ do   \n10: for $i=1$ to $k$ do   \n11: for $j=1$ to $k$ do   \n12: if $i\\neq j$ then   \n13: Cost matrix $[C_{h}]_{k,l}\\gets h([p_{i}^{b}]_{k}-[p_{j}^{b}]_{l})$ . {Use $h(z)=\\log(1+e^{\\beta z})$ . $[p_{i}^{b}]_{k}$ is the kth sample in $\\boldsymbol{p}_{i}^{b}.$ .}   \n14: $[C_{\\bar{h}}]_{k,l}\\gets[C_{h}]_{k,l}+[C_{h}]_{l,k}$ .   \n15: $\\Pi_{i,j,b}\\leftarrow\\mathrm{Sinkhorn}(C_{h},\\lambda,p_{i}^{b},p_{j}^{b}),\\bar{\\Pi}_{i,j,b}\\leftarrow\\mathrm{Sinkhorn}(C_{\\bar{h}},\\lambda,p_{i}^{b},p_{j}^{b}).$ {Sinkhorn alg. with costs $C_{h},C_{\\bar{h}}$ and entropic reg. \u03bb.}   \n16: ${\\mathsf{O T}}_{h,\\lambda}(p_{i}^{b},p_{j}^{b})\\gets\\mathrm{Trace}(C_{h}^{\\top}\\Pi_{i,j,b})+\\lambda\\bar{D}_{\\mathsf{K L}}(\\bar{\\Pi}_{i,j,b}\\bar{\\|}p_{i}^{b}\\otimes p_{j}^{b})$ .   \n17: ${\\mathsf{O T}}_{\\bar{h},\\lambda}(p_{i}^{b},p_{j}^{b})\\gets\\mathrm{Trace}(C_{\\bar{h}}^{\\top}\\bar{\\Pi}_{i,j,b})+\\lambda D_{\\mathsf{K L}}(\\bar{\\Pi}_{i,j,b}||p_{i}^{b}\\otimes p_{j}^{b})$ .   \n18: \u03b5b,i,j \u2190\u03b5h,\u03bb(pib, pjb) = OOTT\u00afhh,,\u03bb\u03bb((ppiib,,ppjjb)) in (5).   \n19: $\\varepsilon_{b,i,i}=0,\\forall\\,b,i$   \n20: Compute the sum statistics   \n21: for $b=0$ to $B$ do   \n22: for $i=1$ to $k$ do   \n23: $\\begin{array}{r}{\\varepsilon_{b}^{i}\\leftarrow\\frac{1}{k-1}\\sum_{j}\\varepsilon_{b,i,j}}\\end{array}$   \n24: Compute the relative statistics   \n25: $\\Delta\\varepsilon_{b}^{i,\\bar{j}}=\\varepsilon_{b}^{i}-\\varepsilon_{b}^{j},\\forall b,i,j$   \n26: Compute the Bootstrap Variance   \n27: for $i=1$ to $k$ do   \n28: for $j=1$ to $k$ do   \n29: $\\begin{array}{r}{\\Bar{\\sigma_{i j}}=\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B}(\\Delta\\varepsilon_{b}^{i,j}-\\mathbf{M}\\mathrm{EAN}(\\Delta\\varepsilon_{b}^{i,j},b))^{2}}}\\\\ {\\sigma_{i j}^{\\mathrm{abs}}=\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B}(\\varepsilon_{b,i,j}-\\mathbf{M}\\mathbf{EAN}(\\varepsilon_{b,i,j},b))^{2}}}\\end{array}$   \n30:   \n31: Compute the test   \n32: $\\mathrm{Win}_{i j}=\\mathrm{Win}_{i j}^{\\mathrm{abs}}=0$   \n33: for $i=1$ to $k$ do   \n34: for $j=1$ to $k$ do   \n35: if $i\\neq j$ and $\\begin{array}{r}{\\Delta\\varepsilon_{0}^{i,j}-\\frac{1}{\\sqrt{n}}\\sigma_{i j}\\Phi^{-1}(\\alpha/k^{2})\\leq0}\\end{array}$ then   \n36: $\\mathrm{Win}_{i j}=1$ {with confidence level $1-\\alpha/k^{2}\\}$   \n37: if $i\\neq j$ and $\\begin{array}{r}{\\varepsilon_{0.i,j}\\,-\\,\\frac{1}{\\sqrt{n}}\\sigma_{i j}^{\\mathrm{abs}}\\Phi^{-1}(\\alpha/k^{2})\\leq\\tau}\\end{array}$ then   \n38: $\\mathrm{Win}_{i j}^{\\mathrm{abs}}=1$ {with confidence level $1-\\alpha/k^{2}$ } $\\mathrm{rank}=\\mathrm{BORDA}(\\mathrm{Win})$ {with confidence level $1-\\alpha)$ } $\\mathrm{rank_{abs}=B O R D A(W i n^{a b s})}$ {with confidence level $1-\\alpha\\}$   \n39: Return rank, $\\mathrm{rank_{abs}}$ ", "page_idx": 13}, {"type": "table", "img_path": "NCX3Kgb1nh/tmp/82e9832e05b9f2843c71faf8cf58340f1c8d985bbe38d12a8a99adc7f8ae7ee5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: This table ranks the 12 models tested in the LLM benchmarking experiment using $n=5000$ samples according to their one-versus-all violation ratio, $\\begin{array}{r}{\\varepsilon_{i}^{(h,\\lambda)}(M)=\\frac{\\bar{1}}{k-1}\\sum_{j\\neq i}\\varepsilon_{i j}^{(h,\\lambda)}}\\end{array}$ \u03b5(h,\u03bb)where \u03b5(h, $\\varepsilon_{i j}^{(h,\\lambda)}$ is the pairwise violation ration of model $i$ compared with model $j$ (lower is better). ", "page_idx": 14}, {"type": "text", "text": "B Ranking from the Mix Instruct experiment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C Proofs of main results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Observe that the set $\\{(x,y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}:x\\leq y\\}$ is closed, as any limit point, $(x,y)$ , of a sequence $\\{(x_{n},y_{n})\\}_{n\\in\\mathbb{N}}$ satisfying $x_{n}\\leq y_{n}$ is such that $x\\leq y$ as the relevant inequalities are preserved in the limit. It follows that $\\mathbb{1}_{\\{x\\leq y\\}}(x,y)$ is a lower semicontinuous function and hence there exists a coupling $\\bar{\\pi}\\in\\Pi(\\mu,\\nu)$ for which $\\begin{array}{r}{\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int\\mathbb{1}_{\\{x\\leq y\\}}(x,y)d\\pi(x,y)=\\int\\mathbb{1}_{\\{x\\leq y\\}}(x,y)d\\bar{\\pi}(x,y)}\\end{array}$ for any choice of $\\mu,\\nu\\in\\mathbb{R}^{d}$ (cf. e.g. Theorem 4.1 in Villani, 2009). ", "page_idx": 14}, {"type": "text", "text": "Assume that $\\begin{array}{r}{\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int\\mathbb{1}_{\\left\\{x\\leq y\\right\\}}(x,y)d\\pi(x,y)=0}\\end{array}$ such that there exists a coupling $\\bar{\\pi}$ for which $\\begin{array}{r}{0=\\int\\mathbb{1}_{\\{x\\leq y\\}}(x,y)d\\bar{\\pi}(x,y)=\\mathbb{P}_{(\\hat{X},\\hat{Y})\\sim\\bar{\\pi}}(\\hat{X}\\leq\\hat{Y})=1-\\mathbb{P}_{(\\hat{X},\\hat{Y})\\sim\\bar{\\pi}}(\\hat{X}>\\hat{Y})}\\end{array}$ i.e. $\\mathbb{P}_{(\\hat{X},\\hat{Y})\\sim\\bar{\\pi}}(\\hat{X}\\geq$ $\\hat{Y})\\,\\geq\\,\\mathbb{P}_{(\\hat{X},\\hat{Y})\\sim\\bar{\\pi}}(\\hat{X}>\\hat{Y})\\,=\\,1$ which, in light of Theorem 6.B.1. in [Shaked and Shanthikumar, 2007], implies that $X\\ \\succcurlyeq Y$ . ", "page_idx": 14}, {"type": "text", "text": "C.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Throughout, we fix the metric $\\mathsf{d}:((x,y),(x^{\\prime},y^{\\prime}))\\in\\mathbb{R}^{2d}\\times\\mathbb{R}^{2d}\\mapsto\\|x-x^{\\prime}\\|_{1}+\\|y-y^{\\prime}\\|_{1}$ on $\\mathbb{R}^{2d}$ . We first show that costs induced by functions $h$ satisfying $\\mathrm{(SC_{d})}$ ) are Lipschitz continuous with respect to d with constant $L$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. For any $\\begin{array}{r}{\\langle(x,y),(x^{\\prime},y^{\\prime})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d},\\,|c(x,y)-c(x^{\\prime},y^{\\prime})|\\leq L\\mathsf{d}((x,y),(x^{\\prime},y^{\\prime})).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. As $\\begin{array}{r}{c(x,y)\\,=\\,\\sum_{i=1}^{d}h(y_{i}\\,-\\,x_{i})}\\end{array}$ for $(x,y)\\,\\in\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}$ and $h$ is Lipschitz continuous with constant $L\\geq0$ , we have that, for any $x,y,x^{\\prime},y^{\\prime}\\in\\mathbb{R}^{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|c(x,y)-c(x^{\\prime},y^{\\prime})|\\leq\\sum_{i=1}^{d}|h(y_{i}-x_{i})-h(y_{i}^{\\prime}-x_{i}^{\\prime})|\\leq L\\sum_{i=1}^{d}|y_{i}-y_{i}^{\\prime}+x_{i}^{\\prime}-x_{i}|}}\\\\ &{}&{\\leq L(\\|y-y^{\\prime}\\|_{1}+\\|x-x^{\\prime}\\|_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3. For any choice of $(\\mu,\\nu),(\\mu^{\\prime},\\nu^{\\prime})\\in\\mathcal{P}(\\mathbb{R}^{d})\\times\\mathcal{P}(\\mathbb{R}^{d})$ and $\\pi\\in\\Pi(\\mu,\\nu),\\pi^{\\prime}\\in\\Pi(\\mu^{\\prime},\\nu^{\\prime})$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\int c d(\\pi-\\pi^{\\prime})\\right|\\leq L\\mathsf{W}_{1}(\\pi,\\pi^{\\prime}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathsf{W}_{1}(\\pi,\\pi^{\\prime})=0\\mathsf{T}_{\\mathsf{d}}(\\pi,\\pi^{\\prime})$ is the 1-Wasserstein distance for the metric d. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $(\\mu,\\nu),(\\mu^{\\prime},\\nu^{\\prime})\\in\\mathcal{P}(\\mathbb{R}^{d})\\!\\times\\!\\mathcal{P}(\\mathbb{R}^{d})$ be arbitrary and fix any $\\pi\\in\\Pi(\\mu,\\nu)$ and $\\pi^{\\prime}\\in\\Pi(\\mu^{\\prime},\\nu^{\\prime})$ . Let $\\gamma\\in\\Pi(\\dot{\\pi},\\pi^{\\prime})$ be arbitrary and consider ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left|\\int c(x,y)d\\pi(x,y)-\\int c(x^{\\prime},y^{\\prime})d\\pi(x^{\\prime},y^{\\prime})\\right|=\\left|\\int c(x,y)-c(x^{\\prime},y^{\\prime})d\\gamma(x,y,x^{\\prime},y^{\\prime})\\right|,}}\\\\ &{\\leq\\int|c(x,y)-c(x^{\\prime},y^{\\prime})|\\,d\\gamma(x,y,x^{\\prime},y^{\\prime}),}\\\\ &{\\leq L\\int\\mathrm{d}((x,y),(x^{\\prime},y^{\\prime}))d\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As $\\gamma$ is arbitrary, it follows that $\\begin{array}{r}{\\left|\\int c d(\\pi-\\pi^{\\prime})\\right|\\le0\\mathsf{T}_{\\mathsf{d}}(\\pi,\\pi^{\\prime})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 1 is based on the framework developed in [Eckstein and Nutz, 2023] and, in particular, the proof of Theorem 3.1 therein. As a byproduct of their proof technique, it is demonstrated that, for any $\\pi\\in\\Pi(\\mu,\\nu)$ , there exists a coupling $\\pi^{\\prime}\\in\\Pi(\\mu,\\nu)$ which is independent of $c$ satisfying ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int c d\\pi^{\\prime}-\\int c d\\pi\\leq2L\\left(0\\mathsf{T}_{|\\cdot|,0}(\\mu^{n},\\mu)\\wedge0\\mathsf{T}_{|\\cdot|,0}(\\nu^{n},\\nu)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "provided that the cost satisfies the condition (7) (see Definition 3.1 in [Eckstein and Nutz, 2023] for a weaker condition) for any choice of $\\mu^{n},\\bar{\\nu}^{n}\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})$ , the set of all probability measures on $\\mathbb{R}^{d}$ supported on at most $n$ points. We note that $\\mathsf{O T}_{|\\cdot|,0}$ is simply the 1-Wasserstein distance for the distance induced by the 1-norm and recall that $\\big|\\cdot\\big|$ has Lipschitz constant 1 due to the reverse triangle inequality (recall the notation $\\mathsf{O T}_{h,\\lambda}$ from Section 3). It is then shown in their proof that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int c d\\pi^{\\prime}-\\int c d\\pi\\leq4L C\\lambda,\\quad{\\sf D}_{\\sf K L}(\\pi^{\\prime}||\\mu\\otimes\\nu)\\leq\\frac{1}{\\alpha}\\log\\left(\\frac{1}{\\lambda}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "provided that there exists $\\mu^{n},\\nu^{n}\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})$ satisfying $\\big(0\\mathsf{T}_{|\\cdot|,0}(\\mu^{n},\\mu)\\wedge0\\mathsf{T}_{|\\cdot|,0}(\\nu^{n},\\nu)\\big)\\leq C n^{-\\alpha}$ for $n=\\lfloor\\lambda^{-1/\\alpha}\\rfloor$ for some $\\alpha\\in(0,1]$ and $C\\geq0$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Fix $\\delta\\,>\\,0$ and assume that $\\eta\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d})$ is not mutually singular with respect to the Lebesgue measure. Then, there exists a measure $\\eta^{n}\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})$ satisfying ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathsf{O T}}_{|\\cdot|,0}(\\eta^{n},\\eta)\\leq n^{-1/d}\\left(\\frac{5}{\\delta}\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}+\\frac{20d}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for every $n\\geq\\left(20/\\delta\\right)^{d}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, note that if $\\begin{array}{r}{\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}=\\infty}\\end{array}$ , the inequality holds vacuously. We hence assume that $\\begin{array}{r}{\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}<\\infty}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Fix $\\delta\\,>\\,0$ . Then, by Lemmas 6.6 and 6.7 in [Graf and Luschgy, 2000], there exists constants $C_{1},C_{2},C_{3}>0$ depending on $\\delta$ for which ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\rho\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})}0\\mathsf{T}_{|\\cdot|,0}(\\rho,\\eta)\\leq n^{-1/a}\\left(2C_{1}\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}+2d C_{2}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for every $n\\geq(2C_{3})^{d}$ . Here $\\begin{array}{r}{C_{1}=\\frac{5}{2\\delta}}\\end{array}$ and $C_{2},C_{3}>0$ can be chosen as any constants satisfying \u0393(2)\u0393\u0393((\u03b4\u03b4nn)\u22121)=\u03b4n1\u22121 \u22645Cn2 for every n \u2265C53 . Observe that C2 = C3 = 1\u03b40 s atisfy these conditions. ", "page_idx": 15}, {"type": "text", "text": "We conclude by noting that the infimum in (10) is achieved by Theorem 4.12 in [Graf and Luschgy, 2000]. \u5382 ", "page_idx": 15}, {"type": "text", "text": "Lemma 5. Fix $\\delta>0$ and $\\eta\\in\\mathcal{P}(\\mathbb{R}^{d})$ . Then, there exists a measure $\\eta^{n}\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})$ satisfying ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathsf{O T}}_{|\\cdot|,0}(\\eta^{n},\\eta)\\leq n^{-1/d}\\left(\\frac{5}{\\delta}\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}+\\frac{20d}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for every $n\\geq\\left({20}/{\\delta}\\right)^{d}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Again, if $\\begin{array}{r}{\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}=\\infty}\\end{array}$ , the inequality trivially holds. ", "page_idx": 16}, {"type": "text", "text": "Assume that $\\begin{array}{r}{\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}\\,<\\,\\infty}\\end{array}$ and, for $t\\,\\geq\\,0$ , let $g_{t}:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ denote the density of an isotropic mea n-zero normal distribution with covariance $t^{2}\\mathrm{Id}$ . Define the probability measure $\\eta\\ast g_{t}$ via ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\eta\\ast g_{t}(A)=\\displaystyle\\iint\\mathbb{1}_{A}(x+y)g_{t}(y)d y d\\eta(x)=\\displaystyle\\int\\left(\\int\\mathbb{1}_{A}(z)g_{t}(z-x)d z\\right)d\\eta(x)}&{{}}&{}\\\\ {=\\displaystyle\\int\\mathbb{1}_{A}(z)\\left(\\int g_{t}(z-x)d\\eta(x)\\right)d z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any Borel measurable set $A\\subset\\mathbb{R}^{d}$ . From the above display, $\\eta\\ast g_{t}$ has density $z\\;\\in\\;\\mathbb{R}^{d}\\;\\mapsto\\;$ $\\begin{array}{r}{\\int g_{t}(\\dot{z}-x)d\\eta(x)}\\end{array}$ with respect to the Lebesgue measure. ", "page_idx": 16}, {"type": "text", "text": "It follows from a minor modification of Lemma 7.1.10 in [Ambrosio et al., 2005], that, for any $1\\leq p<\\infty$ , $\\begin{array}{r}{\\mathsf{O T}_{|\\cdot|^{\\mathsf{p}},0}(\\eta\\ast g_{t},\\eta)\\le t^{p}\\int\\|z\\|_{1}^{p}g_{t}(z)d z<\\infty}\\end{array}$ as normal distributions have finite absolute moments of all orders. Hence, $\\begin{array}{r}{\\operatorname*{lim}_{t\\downarrow0}0\\boldsymbol{\\mathsf{T}}_{|\\cdot|,0}(\\eta*g_{t},\\eta)=0}\\end{array}$ and $\\mathbb{E}_{\\eta\\ast g_{t}}|X_{i}|^{1+\\delta}\\to\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}$ as $t\\downarrow0$ (see Theorem 6.9 in [Villani, 2009]). ", "page_idx": 16}, {"type": "text", "text": "Now, let $\\eta^{n}\\in\\mathcal{P}^{n}(\\mathbb{R}^{d})$ be such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\cal O}\\mathsf{T}_{|\\cdot|,0}(\\eta^{n},\\eta*g_{t})\\leq n^{-1/{d}}\\left(\\frac{5}{\\delta}\\sum_{i=1}^{d}\\mathbb{E}_{\\eta*g_{t}}|X_{i}|^{1+\\delta}+\\frac{20d}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for every $n\\geq\\left(20/\\delta\\right)^{d}$ as in Lemma 4. It follows for the triangle inequality for Wasserstein distances (see Chapter 6 in Villani, 2009) that ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\sf O T}_{|\\cdot|,0}(\\eta^{n},\\eta)\\leq{\\sf O T}_{|\\cdot|,0}(\\eta^{n},\\eta*g_{t})+{\\sf O T}_{|\\cdot|,0}(\\eta,\\eta*g_{t}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The claimed result follows by applying the upper bound from the penultimate display and taking the limit $t\\downarrow0$ on both sides of the resulting inequality. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 provides the worst case scaling for $0\\mathsf{T}_{|\\cdot|,0}(\\eta^{n},\\eta)$ . As noted in the text, it is anticipated that $d$ can be replaced by a suitable notion of intrinsic dimension for $\\eta$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 1. We begin with part 1. The lower bound $0\\,\\le\\,0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)-0\\mathsf{T}_{h,0}(\\mu,\\nu)$ is due to the fact that the Kulback-Leibler divergence is non-negative. As for the upper bound, letting $\\pi\\in\\Pi(\\mu,\\nu)$ be an optimal plan for $0\\mathsf{T}_{h,0}(\\mu,\\nu)$ , such a plan always exists due to Theorem 4.1 in [Villani, 2009]. It follows from (9) and Lemma 5 that there exists a plan $\\pi^{\\prime}\\in\\Pi(\\mu,\\nu)$ satisfying ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int c d\\pi^{\\prime}+\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi^{\\prime}||\\mu\\otimes\\nu)-0\\mathsf{T}_{h,0}(\\mu,\\nu)\\leq d\\lambda\\log\\left(\\frac{1}{\\lambda}\\right)+4L\\lambda\\left(\\frac{5}{\\delta}\\sum_{i=1}^{d}\\mathbb{E}_{\\eta}|X_{i}|^{1+\\delta}+\\frac{20d}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "provided that $\\lfloor\\lambda^{-d}\\rfloor\\ \\geq\\ (20/\\delta)^{d}$ . By minimizing both sides of the above display with respect to $\\bar{\\pi}^{\\prime}\\in\\Pi(\\mu,\\nu)$ , Part 1 readily follows. ", "page_idx": 16}, {"type": "text", "text": "The proof of Part 2 will follow from the fact that the set of all couplings $\\Pi(\\mu,\\nu)$ is tight and hence admits a limit point in the weak topology by Prokhorov\u2019s theorem (see Lemma 4.4 in Villani, 2009) and Corollary 7.20 in [Maso, 1993] once we establish that the functionals ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{F}_{\\lambda}:\\pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})\\mapsto\\left\\{\\int c d\\pi+\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi||\\mu\\otimes\\nu),\\begin{array}{l l}{\\mathrm{if}~\\pi\\in\\Pi(\\mu,\\nu),}\\\\ {\\mathrm{otherwise},}\\end{array}\\right.}\\\\ &{\\mathsf{F}:\\pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})\\mapsto\\left\\{\\int c d\\pi,\\begin{array}{l l}{\\mathrm{if}~\\pi\\in\\Pi(\\mu,\\nu),}\\\\ {\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "are such that $\\mathsf{F}_{\\lambda}\\,\\Gamma$ -converges to F when treated as functionals on the separable metric space $\\mathcal{P}(\\mathbb{R}^{d}\\times$ $\\mathbb{R}^{d}),{\\mathsf{d}}^{\\prime})$ , where ${\\mathsf{d}}^{\\prime}$ is the L\u00e9vy-Prokhorov metric which metrizes the weak convergence of probability distributions (cf. e.g. p.72 in [Billingsley, 2013]), we refer the reader to [Maso, 1993] as a standard reference on $\\Gamma.$ -convergence. In light of Proposition 8.1 in [Maso, 1993] it suffices to show that: ", "page_idx": 16}, {"type": "text", "text": "1. for every $\\pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ and any sequence $\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})\\ni\\pi_{\\lambda}\\to\\pi$ with respect to ${\\mathsf{d}}^{\\prime}$ , $\\mathsf{F}(\\pi)\\leq\\operatorname*{lim}\\operatorname*{inf}_{\\lambda\\downarrow0}\\mathsf{F}_{\\lambda}(\\pi_{\\lambda})$ . ", "page_idx": 16}, {"type": "text", "text": "2. for every $\\pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ there exists a sequence $\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})\\ni\\pi_{\\lambda}\\to\\pi$ with respect to ${\\mathsf{d}}^{\\prime}$ satisfying $\\mathsf{F}({\\boldsymbol\\pi})=\\operatorname*{lim}_{\\lambda\\downarrow0}\\mathsf{F}_{\\lambda}({\\boldsymbol\\pi}_{\\lambda})$ . ", "page_idx": 17}, {"type": "text", "text": "We start by proving the first statement. Fix $\\pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ and any sequence $(\\pi_{\\lambda})_{\\lambda\\downarrow0}\\subset\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ converging to $\\pi$ with respect to $\\mathsf{W}_{1}$ . By Lemma 4.4 in [Villani, 2009], $\\Pi(\\mu,\\nu)$ is tight and hence precompact with respect to ${\\mathsf{d}}^{\\prime}$ by Prokhorov\u2019s theorem. It is easy to see that $\\Pi(\\mu,\\nu)$ is closed under the weak convergence such that it is in fact compact with respect to ${\\mathsf{d}}^{\\prime}$ . It follows that if $\\pi\\not\\in\\Pi(\\mu,\\nu)$ , $\\pi_{\\lambda}\\notin\\Pi(\\mu,\\nu)$ for every $\\lambda$ sufficiently small, hence $\\mathsf{F}_{\\lambda}(\\pi_{\\lambda})\\,\\to\\,+\\infty\\,=\\,\\mathsf{F}(\\pi)$ as $\\lambda\\downarrow0$ . Now, if $\\pi\\in\\Pi(\\mu,\\nu)$ , compactness of $\\Pi(\\mu,\\nu)$ implies that $\\pi_{\\lambda}\\in\\Pi(\\mu,\\nu)$ for every $\\lambda$ sufficiently small. As $\\mu,\\nu$ have finite first moments and $c$ is Lipschitz continuous with constant $L$ , any $\\gamma\\in\\Pi(\\mu,\\nu)$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int|c(x,y)|d\\gamma(x,y)\\leq\\int|c(0,0)|+L(\\|x\\|_{1}+\\|y\\|_{1})d\\gamma(x,y)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=|c(0,0)|+L\\int\\|x\\|_{1}d\\mu(x)+L\\int\\|y\\|_{1}d\\nu(y)<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Conclude from Lemma 5.1.7 in [Ambrosio et al., 2005] that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{F}_{\\lambda}(\\pi_{\\lambda})=\\int c d\\pi_{\\lambda}+\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi||\\mu\\otimes\\nu)\\geq\\int c d\\pi_{\\lambda}\\to\\int c d\\pi=\\mathsf{F}(\\pi),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "proving the first condition. ", "page_idx": 17}, {"type": "text", "text": "As for the second condition, if $\\pi\\quad\\notin\\ \\ \\Pi(\\mu,\\nu)$ , the constant sequence $\\pi_{\\lambda}~~=~\\pi$ satisfies $\\sf F_{\\lambda}(\\pi_{\\lambda})\\;=\\;\\sf F_{\\lambda}(\\pi)\\;=\\;\\sf F(\\pi)\\;=\\;+\\infty$ . If $\\pi~\\in~\\Pi(\\mu,\\nu)$ , let $\\mu^{n},\\nu^{n}\\,\\,\\in\\,\\,{\\mathcal{P}}^{n}(\\mathbb{R}^{d})$ be such that ${\\sf O T}_{|\\cdot|,0}(\\mu^{n},\\mu),{\\sf O T}_{|\\cdot|,0}(\\nu^{n},\\nu)\\,\\to\\,0$ as $n\\,\\rightarrow\\,\\infty$ (i.e. $\\mu^{n}$ and $\\nu^{n}$ converge weakly to $\\mu$ and $\\nu$ respectively and $\\mathbb{E}_{\\mu^{n}}[\\|X\\|_{1}]\\,\\to\\,\\mathbb{E}_{\\mu}[\\|X\\|_{1}],\\mathbb{E}_{\\nu^{n}}[\\|Y\\|_{1}]\\,\\to\\,\\mathbb{E}_{\\nu}[\\|Y\\|_{1}])$ . For instance, the empirical versions of $\\mu$ and $\\nu$ constructed from independent samples satisfy this condition almost surely (see Theorem 3 in [Varadarajan, 1958] for weak convergence, convergence of the moments is due to the law of large numbers). By (8) and the surrounding discussion, there exists $\\pi_{\\lambda}\\in\\Pi(\\mu,\\nu)$ satisfying ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int c d\\pi_{\\lambda}-\\int c d\\pi\\le4L\\left(0\\mathsf{T}_{\\mid\\cdot\\mid,0}\\left(\\mu^{\\lfloor\\lambda^{-1}\\rfloor},\\mu\\right)\\wedge0\\mathsf{T}_{\\mid\\cdot\\mid,0}\\left(\\nu^{\\lfloor\\lambda^{-1}\\rfloor},\\nu\\right)\\right)\\to0\\quad\\mathrm{as}\\;\\lambda\\downarrow0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From the proof of Theorem 3.1 in [Eckstein and Nutz, 2023], $\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi_{\\lambda}||\\mu\\otimes\\nu)\\leq\\lambda\\log(\\lfloor\\lambda^{-1}\\rfloor)\\to0$ as $\\lambda\\downarrow0$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{W}_{1}(\\pi_{\\lambda},\\pi)\\leq2\\left({\\mathsf{O T}}_{|\\cdot|,0}\\left(\\mu^{\\lfloor\\lambda^{-1}\\rfloor},\\mu\\right)\\wedge{\\mathsf{O T}}_{|\\cdot|,0}\\left(\\nu^{\\lfloor\\lambda^{-1}\\rfloor},\\nu\\right)\\right)\\to0\\quad\\mathrm{as}\\;\\lambda\\downarrow0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{F}_{\\lambda}(\\pi_{\\lambda})=\\int c d\\pi_{\\lambda}+\\lambda\\mathsf{D}_{\\mathsf{K L}}(\\pi_{\\lambda}||\\mu\\otimes\\nu)\\to\\int c d\\pi=\\mathsf{F}(\\pi),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and, as $\\mathsf{W}_{1}(\\pi_{\\lambda},\\pi)\\to0$ , $\\pi_{\\lambda}$ converges weakly to $\\pi$ (see [Villani, 2009]).This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first establish some useful properties of optimal potentials for this problem, there exists a pair ", "page_idx": 17}, {"type": "text", "text": "Lemma 6. Fix $\\lambda>0$ and sub-Gaussian distributions $\\iota,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d})$ with a shared constant $\\tau^{2}>0$ . Then, for any choice of $h$ satisfying $\\mathrm{(SC_{d}]}$ ), there exists a unique pair of continuous optimal potentials $(\\varphi,\\psi)$ for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ for which the Schr\u00f6dinger system (4) holds at all points $(x,y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ and $\\varphi(0)=0$ . Moreover, this pair of potentials satisfies the estimates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\varphi(x)|\\le C_{d,L,\\tau,h(0)}(1+\\|x\\|_{1}),\\quad|\\psi(y)|\\le C_{d,L,\\tau,h(0)}(1+\\|y\\|_{1}),}\\\\ &{|D^{\\alpha}\\varphi(x)|\\le C_{d,k,\\tau,\\lambda,p_{k},L,h(0)}(1+\\|x\\|_{1})^{k p_{k}},\\quad|D^{\\alpha}\\psi(y)|\\le C_{d,k,\\tau,\\lambda,p_{k},L,h(0)}(1+\\|y\\|_{1})^{k p_{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any multi-index ${\\boldsymbol{\\alpha}}\\in\\ensuremath{\\mathbb{N}}_{0}^{d}$ of order $|\\alpha|\\leq k=\\lfloor d/2\\rfloor+1$ with the constants $k,L,p_{k}$ from the $\\mathrm{(SC_{d})}$ ) condition. The quantities in the subscripts of the constants indicate what parameters the constants depend on. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $(\\varphi_{h},\\psi_{h})$ be an arbitrary pair of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ so that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int\\varphi_{h}d\\mu+\\int\\psi_{h}d\\nu={\\mathsf{O}}\\mathsf{T}_{h,\\lambda}(\\mu_{0},\\mu_{1}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and, setting $\\begin{array}{r}{\\mathsf{C}=\\int\\psi_{h}d\\nu-\\frac{1}{2}0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\end{array}$ , we have that $(\\varphi_{h}^{\\prime},\\psi_{h}^{\\prime})=(\\varphi_{h}+\\mathsf C,\\psi_{h}-\\mathsf C)$ is another pair of optimal potentials, and $\\begin{array}{r}{\\int\\varphi_{h}^{\\prime}d\\mu=\\int\\psi_{h}^{\\prime}d\\nu=\\frac{1}{2}0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "We further consider the functions defined on $\\mathbb{R}^{d}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{h}^{\\prime\\prime}(x)=-\\lambda\\log\\left(\\int e^{\\frac{\\psi_{h}^{\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)\\right),\\quad\\psi_{h}^{\\prime\\prime}(y)=-\\lambda\\log\\left(\\int e^{\\frac{\\varphi_{h}^{\\prime\\prime}(x)-c(x,y)}{\\lambda}}d\\mu(x)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will show that $(\\varphi_{h}^{\\prime\\prime},\\psi_{h}^{\\prime\\prime})$ are optimal potentials satisfying the claimed bounds. ", "page_idx": 18}, {"type": "text", "text": "It follows by Jensen\u2019s inequality and Lemma 2 that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{h}^{\\prime\\prime}(x)\\leq\\int c(x,y)-\\psi_{h}^{\\prime}(y)d\\nu(y)\\leq\\underbrace{c(0,0)}_{=h(0)}+L\\underbrace{\\int\\|y\\|_{1}d\\nu(y)}_{\\leq\\sqrt{4\\tau^{2}}}+L\\|x\\|_{1}-\\frac{1}{2}\\underbrace{0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}_{\\geq0},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "observing that $2\\,\\geq\\,\\mathbb{E}_{\\nu}[\\exp(\\|X\\|_{1}^{2}/2\\tau^{2})]\\,\\geq\\,\\mathbb{E}_{\\nu}\\big[\\|X\\|_{1}^{2}\\big/2\\tau^{2}\\big]$ such that $\\int\\|\\,\\cdot\\,\\|_{1}d\\nu\\,\\leq\\,\\sqrt{4\\tau^{2}}$ (again by Jensen\u2019s inequality). It follows that $\\varphi_{h}^{\\prime\\prime}(x)\\leq C_{d,L,\\tau,h(0)}(1+\\|x\\|_{1})$ where $C_{d,L,\\tau,h(0)}$ depends on $d,L,\\tau$ , and the value of $h(0)$ . The same bound evidently holds for $\\psi_{h}^{\\prime\\prime}$ on $\\mathbb{R}^{d}$ and for $\\psi_{h}^{\\prime}$ on the support of $\\nu$ . Applying this bound and Lemma 2, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\varphi_{h}^{\\prime\\prime}(x)\\leq\\lambda\\log\\left(\\int e^{\\frac{C_{d,L,\\tau}(1+\\|y\\|_{1})-h(0)+L\\|x\\|_{1}+L\\|y\\|_{1}}{\\lambda}}d\\nu(y)\\right)\\leq L\\|x\\|_{1}+C_{d,L,\\tau,h(0)}^{\\prime},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we have used the fact that $\\begin{array}{r}{\\mathbb{E}_{\\nu}[e^{t\\|X\\|_{1}}]\\le\\mathbb{E}_{\\nu}\\left[e^{\\frac{\\tau^{2}t^{2}}{2}+\\frac{\\|X\\|_{1}^{2}}{2\\tau^{2}}}\\right]\\le2e^{\\frac{\\tau^{2}t^{2}}{2}}}\\end{array}$ for any $t\\in\\mathbb R$ as follows from Young\u2019s inequality and the sub-Gaussian assumption. The same argument implies that $\\psi_{h}^{\\prime\\prime}$ satisfies an analogous bound. ", "page_idx": 18}, {"type": "text", "text": "To see that $(\\varphi_{h}^{\\prime\\prime},\\psi_{h}^{\\prime\\prime})$ is a pair of optimal potentials, observe that Jensen\u2019s inequality yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int(\\varphi_{h}^{\\prime}-\\varphi_{h}^{\\prime\\prime})d\\mu+\\int(\\psi_{h}^{\\prime}-\\psi_{h}^{\\prime\\prime})d\\nu\\leq\\lambda\\log\\int e^{\\frac{\\varphi_{h}^{\\prime}-\\varphi_{h}^{\\prime\\prime}}{\\lambda}}d\\mu+\\lambda\\log\\int e^{\\frac{\\psi_{h}^{\\prime}-\\psi_{h}^{\\prime\\prime}}{\\lambda}}d\\nu}\\\\ {=\\lambda\\log\\int e^{\\frac{\\varphi_{h}^{\\prime}(x)+\\psi_{h}^{\\prime}(y)-c(x,y)}{\\lambda}}d\\mu\\otimes\\nu(x,y)}\\\\ {+\\,\\lambda\\log\\int e^{\\frac{\\varphi_{h}^{\\prime\\prime}(x)+\\psi_{h}^{\\prime}(y)-c(x,y)}{\\lambda}}d\\mu\\otimes\\nu(x,y)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as follows from the fact that $(\\varphi_{h}^{\\prime},\\psi_{h}^{\\prime})$ satisfy (4). Conclude that $\\begin{array}{r}{\\int\\varphi_{h}^{\\prime\\prime}d\\mu+\\int\\psi_{h}^{\\prime\\prime}d\\nu\\geq0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\end{array}$ such that equality must hold (indeed $\\begin{array}{r}{\\int e^{\\frac{\\varphi_{h}^{\\prime\\prime}(x)+\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\mu\\otimes\\nu(x,y)=1}\\end{array}$ by construction, so the left hand side of the ineqalit is the objective value in the dual form of the EOT problem) and, by strict concavity of the logarithm, $\\varphi_{h}^{\\prime\\prime}=\\bar{\\varphi}_{h}^{\\prime}\\,\\mu\\cdot$ -a.e. and $\\psi_{h}^{\\prime\\prime}=\\psi_{h}^{\\prime}$ $\\nu$ -a.e. such that $(\\bar{\\varphi_{h}^{\\prime\\prime}},\\psi_{h}^{\\prime\\prime})$ are indeed optimal potentials for this problem. ", "page_idx": 18}, {"type": "text", "text": "Now, consider the potentials $(\\varphi,\\psi)_{.}=(\\varphi_{h}^{\\prime\\prime}-\\varphi_{h}^{\\prime\\prime}(0),\\psi_{h}^{\\prime\\prime}+\\varphi_{h}^{\\prime\\prime}(0))$ , which satisfy (4) on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ $\\varphi(0)=0$ . The bounds for $\\varphi_{h}^{\\prime\\prime}$ and $\\psi_{h}^{\\prime\\prime}$ evidently carry over to $\\varphi$ and $\\psi$ so that there exists a constant C\u2032d\u2032,L,\u03c4,h(0) < \u221edepending only on d, L, \u03c4, and h(0) for which ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\varphi(x)|\\leq C_{d,L,\\tau,h(0)}^{\\prime\\prime}(1+\\|x\\|_{1}),\\quad|\\psi(y)|\\leq C_{d,L,\\tau,h(0)}^{\\prime\\prime}(1+\\|y\\|_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for every $(x,y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ . Now, suppose that $(\\Bar\\varphi,\\Bar\\psi)$ is any other pair of potentials for which (4) holds on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ and $\\bar{\\varphi}(0)=0$ . As discussed in Section 3, one has that $(\\varphi,\\psi)$ and $(\\Bar\\varphi,\\Bar\\psi)$ coincide $\\mu$ - and $\\nu$ -a.e. so that, for every $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\ne^{-\\frac{\\bar{\\varphi}(x)}{\\lambda}}=\\int e^{\\frac{\\bar{\\psi}(y)-c(x,y)}{\\lambda}}d\\nu(y)=\\int e^{\\frac{\\psi(y)-c(x,y)}{\\lambda}}d\\nu(y)=e^{-\\frac{\\varphi(x)}{\\lambda}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which shows that ${\\bar{\\varphi}}=\\varphi$ , the equality of $\\psi$ and $\\bar{\\psi}$ follows analogously. ", "page_idx": 18}, {"type": "text", "text": "We now establish the bounds on the derivatives. By the multivariate Fa\u00e0 di Bruno formula (cf. e.g. Constantine and Savits, 1996), for any multi-index $\\alpha\\in\\mathbb{N}_{0}^{d}$ of order $\\vert\\alpha\\vert\\geq1$ , the derivative $-D^{\\alpha}\\varphi_{h}^{\\prime\\prime}(x)$ (assuming it exists) can be expressed as a linear combination of products of derivatives of the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\prod_{j=1}^{|\\alpha|}\\left(\\frac{D_{x}^{\\beta_{j}}\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}{\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}\\right)^{k_{j}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta_{j}\\,\\in\\,\\mathbb{N}_{0}^{d}$ and $k_{j}\\,\\in\\,\\ensuremath{\\mathbb{N}}_{0}$ satisfy $\\begin{array}{r}{\\sum_{j=1}^{|\\alpha|}\\beta_{j}k_{j}\\,=\\,\\alpha}\\end{array}$ . By the dominated convergence theorem, the derivative commutes with the integral sign and, applying the same formula to the derivative $D_{x}^{\\beta_{j}}e^{\\frac{-c(x,y)}{\\lambda}}$ , we see that it can be expressed as a linear combination of products of the form ", "page_idx": 19}, {"type": "equation", "text": "$$\ne^{-\\frac{c(x,y)}{\\lambda}}\\prod_{m=1}^{|\\beta_{j}|}\\left(-\\frac{1}{\\lambda}D_{x}^{\\gamma_{m}}\\sum_{i=1}^{d}h(y_{i}-x_{i})\\right)^{l_{m}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where \u03b3m \u2208N0d, lm \u2208N0 with |m\u03b2j=|1 $\\begin{array}{r}{\\sum_{m=1}^{\\vert\\beta_{j}\\vert}\\gamma_{m}l_{m}=\\beta_{j}}\\end{array}$ , and we assume that all relevant derivatives exist. As $h$ satisfies $\\mathrm{(SC_{d})}$ , it is $k$ -times continuously differentiable for $k=\\lfloor d/2\\rfloor+1$ with derivatives of order $s\\leq k$ satisfying $|h^{(s)}(x)|\\,\\le\\,C_{k}(1+|x|)^{p_{k}}$ for some $C_{k}<\\infty$ and $p_{k}>1$ which may depend on $k$ . From this assumption, (12) is well defined provided that $|\\beta_{j}|\\leq k$ and we observe that $\\begin{array}{r l}&{\\left|\\bar{D}_{x}^{\\gamma_{m}}\\sum_{i=1}^{d}h(y_{i}-x_{i})\\right|\\leq\\sum_{i=1}^{\\bar{d}}C_{k}(1+|y_{i}-x_{i}|)^{p_{k}}\\leq\\sum_{i=1}^{d}C_{k}2^{p_{k}-1}(1+|y_{i}-x_{i}|^{p_{k}})\\,\\mathrm{art}}\\\\ &{\\mathrm{so~that}\\left|D_{x}^{\\beta_{j}}e^{-\\frac{c(x,y)}{\\lambda}}\\right|\\mathrm{~can~be~bounded~as~}e^{-\\frac{c(x,y)}{\\lambda}}C_{d,k,p_{k},\\lambda}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}\\mathrm{~by~appealin}}\\end{array}$ s $p_{k}>1$ , g to (12). Returning to (11), we infer that $D^{\\alpha}\\varphi_{h}^{\\prime\\prime}$ is well-defined for any $|\\alpha|\\leq k$ and that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|D_{x}^{\\beta_{j}}\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}d\\nu(y)}\\right|=\\left|\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)}{\\lambda}}D_{x}^{\\beta_{j}}e^{-\\frac{c(x,y)}{\\lambda}}d\\nu(y)\\right|}&{}\\\\ {\\qquad\\qquad\\qquad\\qquad\\leq C_{d,k,p_{k},\\lambda}\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will split this integral into the regions $\\|y\\|_{1}<\\tau$ and $\\|y\\|_{1}\\geq\\tau$ for some $\\tau>0$ which will be chosen later in the proof. ", "page_idx": 19}, {"type": "text", "text": "In this first region, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\{\\|y\\|_{1}<\\tau\\}}e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y)\\leq(1+\\tau+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by the triangle inequality so that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{D_{x}^{\\beta_{j}}\\int_{\\{\\|y\\|_{1}<\\tau\\}}e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}{\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}\\leq(1+\\tau+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the second region, we have from Lemma 2 and the first part of the proof that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y)}\\\\ &{\\leq\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-h(0)+L\\|x\\|_{1}+L\\|y\\|_{1}}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y)}\\\\ &{\\leq\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}+2L\\|y\\|_{1}}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To bound this integral, note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}\\leq(1+\\|y\\|_{1}+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}\\leq2^{p_{k}|\\beta_{j}|-1}\\left((1+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}+\\|y\\|_{1}^{p_{k}|\\beta_{j}|}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}+2L\\|y\\|_{1}}{\\lambda}}d\\nu(y)=e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}}{\\lambda}}\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{2L\\|y\\|_{1}}{\\lambda}}d\\nu(y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}}{\\lambda}}\\left(\\int e^{\\frac{4L\\|y\\|_{1}}{\\lambda}}d\\nu(y)\\right)^{\\frac{1}{2}}\\left(\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}1d\\nu(y)\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by the Cauchy-Schwarz inequality. Similarly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}+2L\\|y\\|_{1}}{\\lambda}}\\|y\\|_{1}^{p_{k}|\\beta_{j}|}d\\nu(y)}\\\\ &{\\leq e^{\\frac{L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}}{\\lambda}}\\left(\\displaystyle\\int e^{\\frac{4L\\|y\\|_{1}}{\\lambda}}d\\nu(y)\\right)^{\\frac{1}{2}}\\left(\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}\\|y\\|_{1}^{2p_{k}|\\beta_{j}|}d\\nu(y)\\right)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, e4L\u2225\u03bby\u22251d\u03bd(y) \u22642e162L\u03bb2\u03c4 due to the bound $\\begin{array}{r}{\\mathbb{E}_{\\nu}[e^{t\\|x\\|_{1}}]\\le\\mathbb{E}_{\\nu}\\left[e^{\\frac{\\tau^{2}t^{2}}{2}+\\frac{\\|X\\|_{1}^{2}}{2\\tau^{2}}}\\right]\\le2e^{\\frac{\\tau^{2}t^{2}}{2}}}\\end{array}$ which holds for every $t\\geq0$ . Further, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}1d\\nu(y)\\leq e^{-\\frac{\\tau^{2}}{4\\tau^{2}}}\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}e^{\\frac{\\|y\\|_{1}^{2}}{4\\tau^{2}}}d\\nu(y)\\leq\\sqrt{2}e^{-\\frac{\\tau^{2}}{4\\tau^{2}}},\\qquad}\\\\ {\\int_{\\{\\|y\\|_{1}\\geq\\tau\\}}\\|y\\|_{1}^{2p_{k}|\\beta_{j}|}d\\nu(y)\\leq e^{-\\frac{\\tau^{2}}{4\\tau^{2}}}\\int e^{\\frac{\\|y\\|_{1}^{2}}{4\\tau^{2}}}\\|y\\|_{1}^{2p_{k}|\\beta_{j}|}d\\nu(y)\\leq\\sqrt{2}e^{-\\frac{\\tau^{2}}{4\\tau^{2}}}\\left(\\int\\|y\\|_{1}^{4p_{k}|\\beta_{j}|}d\\nu(y)\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have applied the Cauchy-Schwarz inequality in the second line. By sub-Gaussianity, $\\begin{array}{r l}&{2\\quad\\geq\\quad\\mathbb{E}_{\\nu}[e^{\\|X\\|_{1}^{2}/2\\tau^{2}}]\\quad\\geq\\quad\\mathbb{E}_{\\nu}\\left[\\frac{\\|X\\|_{1}^{2p}}{(2\\tau^{2})^{p}p!}\\right]}\\\\ &{\\sqrt{2}(2\\tau^{2})^{p_{k}|\\beta_{j}|}\\sqrt{(2p_{k}|\\beta_{j}|)!}.}\\end{array}$ for any $\\begin{array}{r l r}{p}&{{}\\in}&{\\mathbb{N}}\\end{array}$ so that $\\begin{array}{r l}{\\sqrt{\\mathbb{E}_{\\nu}\\left[\\|X\\|_{1}^{4p_{k}|\\beta_{j}|}\\right]}}&{{}\\leq}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Combining all of these bounds, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\{\\|y\\|_{1}\\geq r\\}}e^{\\frac{\\psi_{k}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}(1+\\|y-x\\|_{1})^{p_{k}|\\beta_{j}|}d\\nu(y)}\\\\ &{\\leq2^{p_{k}|\\beta_{j}|-1}(1+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}e^{\\frac{L\\sqrt{4r^{2}}+L\\|x\\|_{1}}{\\lambda}}\\sqrt{2e^{\\frac{4L^{2}r^{2}}{\\lambda^{2}}}e^{-\\frac{\\tau^{2}}{8\\tau^{2}}}}\\left(2^{1/4}+\\sqrt{\\sqrt{2}(2\\tau^{2})^{p_{k}|\\beta_{j}|}\\sqrt{(2p_{k}|\\beta_{j}|)!}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The denominator can be bounded as $\\begin{array}{r}{\\left(\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)\\right)^{-1}=e^{\\frac{\\varphi_{h}^{\\prime\\prime}(x)}{\\lambda}}\\leq e^{\\frac{h(0)+L\\sqrt{4\\tau^{2}}+L\\|x\\|_{1}}{\\lambda}}}\\end{array}$ as follows from the first part of the proof. The ratio we wish to bound thus includes the exponential term $e^{\\frac{h(0)+2L\\sqrt{4\\tau^{2}}+2L\\|x\\|_{1}}{\\lambda}+\\frac{4L^{2}\\tau^{2}}{\\lambda^{2}}-\\frac{\\tau^{2}}{8\\tau^{2}}}$ 8\u03c4\u03c42 , which we equate to 1 by setting $\\begin{array}{r}{\\tau^{2}=\\frac{8\\tau^{2}}{\\lambda}(h(0)+2L\\sqrt{4\\tau^{2}}+}\\end{array}$ $\\begin{array}{r}{2L\\|x\\|_{1})+\\frac{32L^{2}\\tau^{4}}{\\lambda^{2}}=C_{L,\\tau,\\lambda,h(0)}+\\frac{16\\tau^{2}}{\\lambda}L\\|x\\|_{1}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "All in all, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{D_{x}^{\\beta_{j}}\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}{\\int e^{\\frac{\\psi_{h}^{\\prime\\prime}(y)-c(x,y)}{\\lambda}}d\\nu(y)}\\right|}\\\\ &{\\leq C_{d,k,p_{k},\\lambda}\\left(1+\\left(C_{L,\\tau,\\lambda,h(0)}+16\\tau^{2}\\lambda^{-1}L\\|x\\|_{1}\\right)^{1/2}+\\|x\\|_{1}\\right)^{p_{k}|\\beta_{j}|}}\\\\ &{+\\,C_{d,k,p_{k},\\lambda}2^{p_{k}|\\beta_{j}|-1}(1+\\|x\\|_{1})^{p_{k}|\\beta_{j}|}\\sqrt{2}\\left(2^{1/4}+\\sqrt{\\sqrt{2}(2\\tau^{2})^{p_{k}|\\beta_{j}|}\\sqrt{(2p_{k}|\\beta_{j}|)!}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the products in (11) can thus be bounded as $C_{d,k,\\tau,\\lambda,p_{k},L,h(0)}(1+\\|x\\|_{1})^{p_{k}|\\alpha|}$ , for any $|\\alpha|\\leq k$ . The same argument establishes analogous bounds for $\\psi_{h}^{\\prime\\prime}$ . As $(\\varphi,\\psi)$ coincides with $(\\varphi_{h}^{\\prime\\prime},\\psi_{h}^{\\prime\\prime})$ up to additive constants, the derivative bounds evidently transfer over. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "In what follows, we always choose the unique solution for any given EOT problem described in Lemma 6. ", "page_idx": 21}, {"type": "text", "text": "As aforementioned, our approach to proving limit distributions is based on the functional delta method (see Appendix $\\mathrm{D}$ for a summary of the method). As $\\bar{h}$ also satisfies $\\mathrm{(SC_{d})}$ with Lipschitz constant $2L$ , $C_{k}$ replaced by $2C_{k}$ , and $\\bar{h}(0)=2h(0)$ , there exists a constant $C_{d,h,\\tau}$ for which all bounds from Lemma 6 hold simultaneously for some choice of potentials $(\\varphi_{h},\\psi_{h})$ for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ and $(\\varphi_{\\bar{h}},\\psi_{\\bar{h}})$ for $0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)$ . We thus instantiate the function classes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\tau,h}=\\left\\{f\\in\\mathcal{C}^{k}(\\mathbb{R}^{d}):|D^{\\alpha}f(x)|\\leq C_{d,h,\\tau}(1+\\|x\\|_{1})^{k p_{k}}\\mathrm{~for~}k=\\lfloor d/2\\rfloor+1,\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\forall\\alpha\\in\\mathbb{N}_{0}^{d}\\mathrm{~with~}|\\alpha|\\leq k,\\forall x\\in\\mathbb{R}^{d}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{\\tau,h}^{\\oplus}=\\left\\{f\\oplus g:f,g\\in\\mathcal{F}_{\\tau,h}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $f\\oplus g$ is understood as the function $(x,y)\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\mapsto f(x)+g(y)$ . We further consider the class of probability distributions ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{P}}_{\\tau}^{\\otimes}=\\left\\{\\mu\\otimes\\nu:\\mu,\\nu\\in{\\mathcal{P}}(\\mathbb{R}^{d}){\\mathrm{~and~are~}}\\tau^{2}{\\mathrm{-sub-Gaussian}}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $\\tau>0$ . Throughout, we will treat $\\mathcal{P}_{\\tau}^{\\otimes}$ as a subset of $\\ell^{\\infty}(\\mathcal{F}_{\\tau,h}^{\\oplus})$ , the Banach space of bounded real functions on $\\mathcal{F}_{\\tau,h}^{\\oplus}$ endowed with the supremum norm $\\begin{array}{r}{{\\|\\ell\\|}_{\\infty,\\mathcal{F}_{\\tau,h}^{\\oplus}}=\\operatorname*{sup}_{f\\oplus g\\in\\mathcal{F}_{\\tau,h}^{\\oplus}}|\\ell(f\\oplus g)|}\\end{array}$ ; the action of $\\mu\\otimes\\nu\\in\\mathcal P_{\\tau}^{\\otimes}$ on $f\\oplus g$ is given by $\\begin{array}{r}{\\mu\\otimes\\nu(f\\oplus g)=\\int f d\\mu+\\int g d\\nu}\\end{array}$ . It is easy to see that $\\mu\\otimes\\nu$ defines a bounded function on this function class due to the growth bounds inherent to $\\mathcal{F}_{\\tau,h}$ . Our approach is similar to that of Proposition 1 in [Goldfeld et al., 2024b] and, in particular, Section 6 of that work. Namely, we prove a type of directional differentiability and Lipschitz continuity for the EOT cost. ", "page_idx": 21}, {"type": "text", "text": "Lemma 7. Fix $\\lambda\\,>\\,0$ and sub-Gaussian distributions $\\mu,\\nu,\\rho,\\eta\\in\\mathcal{P}(\\mathbb{R}^{d})$ with constant $\\tau^{2}\\,>\\,0$ . Then, for any choice of optimal potentials $(\\varphi,\\psi)$ solving $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ and satisfying (4) on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\downarrow0}\\frac{\\mathsf{O T}_{h,\\lambda}(\\mu+t(\\rho-\\mu),\\nu+t(\\eta-\\nu))-\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}{t}=\\int\\varphi d(\\rho-\\mu)+\\int\\psi d(\\eta-\\nu).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For $t\\in[0,1]$ , let $\\mu_{t}:=\\mu+t(\\rho-\\mu)$ and $\\nu_{t}:=\\nu+t(\\eta-\\nu)$ and, let $\\left(\\varphi^{(\\mu_{t},\\nu_{t})},\\psi^{(\\mu_{t},\\nu_{t})}\\right)$ denote the unique pair of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu_{t},\\nu_{t})$ satisfying $\\varphi^{(\\mu_{t},\\nu_{t})}(0)\\;=\\;0$ and solving the Schr\u00f6dinger system on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ (see Lemma 6) and likewise for $(\\varphi^{(\\mu_{t},\\nu)},\\psi^{(\\mu_{t},\\nu)})$ and $\\bar{(\\varphi^{(\\mu,\\nu)},\\psi^{(\\mu,\\nu)})}$ . Observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\mathsf{T}_{h,\\lambda}(\\mu_{t},\\nu_{t})-0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\\\ &{=0\\mathsf{T}_{h,\\lambda}(\\mu_{t},\\nu_{t})-0\\mathsf{T}_{h,\\lambda}(\\mu_{t},\\nu)+0\\mathsf{T}_{h,\\lambda}(\\mu_{t},\\nu)-0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\\\ &{\\le\\displaystyle\\int\\varphi^{(\\mu_{t},\\nu_{t})}d\\mu_{t}+\\int\\psi^{(\\mu_{t},\\nu_{t})}d\\nu_{t}-\\int\\varphi^{(\\mu_{t},\\nu_{t})}d\\mu_{t}-\\int\\psi^{(\\mu_{t},\\nu_{t})}d\\nu}\\\\ &{+\\lambda\\displaystyle\\int e^{\\frac{\\varphi(\\mu_{t},\\nu_{t})}{\\lambda}(x)+\\frac{\\psi(\\mu_{t},\\nu_{t})}{\\lambda}(y)-c(x,y)}d\\mu_{t}\\otimes\\nu(x,y)-\\lambda}\\\\ &{+\\displaystyle\\int\\varphi^{(\\mu_{t},\\nu)}d\\mu_{t}+\\int\\psi^{(\\mu_{t},\\nu)}d\\nu-\\int\\varphi^{(\\mu_{t},\\nu)}d\\mu-\\int\\psi^{(\\mu_{t},\\nu)}d\\nu}\\\\ &{+\\lambda\\displaystyle\\int e^{\\frac{\\varphi(\\mu_{t},\\nu)}{\\lambda}(x)+\\psi^{(\\mu_{t},\\nu)}(y)-c(x,y)}d\\mu\\otimes\\nu(x,y)-\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we recall that the potentials satisfy the relevant Schr\u00f6dinger systems (4) such that $\\begin{array}{r}{\\int e^{\\frac{\\varphi^{(\\mu_{t},\\nu_{t})}(x)+\\psi^{(\\mu_{t},\\nu_{t})}(y)-c(x,y)\\big}{\\lambda}}d\\mu_{t}(x)\\equiv1}\\end{array}$ and \u03c6(\u00b5t,\u03bd)(x)+\u03c8(\u03bb\u00b5t,\u03bd)(y)\u2212c(x,y)d\u03bd(y) \u22611 on Rd so that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{O T}_{h,\\lambda}(\\mu_{t},\\nu_{t})-\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}\\\\ &{=\\mathsf{O T}_{h,\\lambda}(\\mu_{t},\\nu_{t})-\\mathsf{O T}_{h,\\lambda}(\\mu_{t},\\nu)+\\mathsf{O T}_{h,\\lambda}(\\mu_{t},\\nu)-\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}\\\\ &{\\le\\displaystyle\\int\\varphi^{(\\mu_{t},\\nu_{t})}d\\mu_{t}+\\int\\psi^{(\\mu_{t},\\nu_{t})}d\\nu_{t}-\\int\\varphi^{(\\mu_{t},\\nu_{t})}d\\mu_{t}-\\int\\psi^{(\\mu_{t},\\nu_{t})}d\\nu}\\\\ &{+\\displaystyle\\int\\varphi^{(\\mu_{t},\\nu)}d\\mu_{t}+\\int\\psi^{(\\mu_{t},\\nu)}d\\nu-\\int\\varphi^{(\\mu_{t},\\nu)}d\\mu-\\int\\psi^{(\\mu_{t},\\nu)}d\\nu}\\\\ &{=t\\displaystyle\\int\\varphi^{(\\mu_{t},\\nu)}d(\\rho-\\mu)+t\\displaystyle\\int\\psi^{(\\mu_{t},\\nu_{t})}d(\\eta-\\nu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and, analogously, ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathsf{O T}}_{h,\\lambda}(\\mu_{t},\\nu_{t})-{\\mathsf{O T}}_{h,\\lambda}(\\mu,\\nu)\\geq t\\int\\varphi^{(\\mu,\\nu)}d(\\rho-\\mu)+t\\int\\psi^{(\\mu_{t},\\nu)}d(\\eta-\\nu).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It suffices, therefore, to show pointwise convergence of all relevant potentials to $\\left(\\varphi^{(\\mu,\\nu)},\\psi^{(\\mu,\\nu)}\\right)$ in the limit $t\\downarrow0$ . Here we will only show convergence of $\\left(\\varphi^{\\left(\\mu_{t},\\nu_{t}\\right)},\\psi^{\\left(\\mu_{t},\\nu_{t}\\right)}\\right)$ , convergence of the other set of potentials follows analogously. For convenience set ${(\\varphi_{t},\\psi_{t})=(\\varphi^{(\\mu_{t},\\nu_{t})},\\psi^{(\\mu_{t},\\nu_{t})})}$ ", "page_idx": 22}, {"type": "text", "text": "Let $[0,1]\\ni t_{n}\\downarrow0$ be arbitrary and fix a subsequence $t_{n^{\\prime}}$ . By Lemma 6 we can apply the Arzel\u00e0- Ascoli theorem (cf. e.g. Theorem 4.44 in Folland, 1999) to infer that $(\\varphi_{t},\\psi_{t})$ converges to a pair of continuous functions $(\\varphi,\\psi)$ uniformly on compact sets and, in particular, pointwise along a further subsequence $t_{n^{\\prime\\prime}}$ . From Lemma 6, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e^{\\frac{\\varphi_{t_{n}\\prime\\prime}(x)+\\psi_{t_{n}\\prime\\prime}(y)-c(x,y)}{\\lambda}}\\leq e^{\\frac{C_{d,L,\\tau}(2+\\|x\\|_{1}+\\|y\\|_{1})-c(0,0)+L\\|x\\|_{1}+L\\|y\\|_{1}}{\\lambda}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where this final term is integrable with respect to any product of sub-Gaussian measures. By the dominated convergence theorem, for any $\\bar{(x,y)}\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e^{-\\frac{\\varphi_{t_{n^{\\prime\\prime}}}(x)}{\\lambda}}=\\displaystyle\\int e^{\\frac{\\psi_{t_{n^{\\prime\\prime}}}(y)-c(x,y)}{\\lambda}}d(\\nu+t_{n^{\\prime\\prime}}(\\eta-\\nu))(y)\\to\\displaystyle\\int e^{\\frac{\\psi(y)-c(x,y)}{\\lambda}}d\\nu(y),}\\\\ &{e^{-\\frac{\\psi_{t_{n^{\\prime\\prime}}}(x)}{\\lambda}}=\\displaystyle\\int e^{\\frac{\\varphi_{t_{n^{\\prime\\prime}}}(x)-c(x,y)}{\\lambda}}d(\\mu+t_{n^{\\prime\\prime}}(\\rho-\\mu))(x)\\to\\displaystyle\\int e^{\\frac{\\varphi(x)-c(x,y)}{\\lambda}}d\\mu(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "as $t_{n^{\\prime\\prime}}\\downarrow0$ such that the pair $(\\varphi,\\psi)$ satisfies the Schr\u00f6dinger system (4) pointwise and hence is a pair of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ and $\\varphi(0)\\,=\\,\\mathrm{lim}_{t_{n^{\\prime\\prime}}\\downarrow0}\\,\\varphi_{t_{n^{\\prime\\prime}}}(0)\\,=\\,0$ , whence $\\left(\\varphi,\\psi\\right)\\;=$ $\\left(\\varphi^{(\\mu,\\nu)},\\psi^{(\\mu,\\nu)}\\right)$ . Combining (13) and (14), we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t_{n^{\\prime\\prime}}\\downarrow0}\\frac{0\\mathsf{T}_{h,\\lambda}(\\mu_{t_{n^{\\prime\\prime}}},\\nu_{t_{n^{\\prime\\prime}}})-0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{t_{n^{\\prime\\prime}}}=\\int\\varphi^{(\\mu,\\nu)}d(\\rho-\\mu)+\\int\\psi^{(\\mu,\\nu)}d(\\eta-\\nu),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and, as the limit is independent of the choice of original subsequence it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t,\\downarrow0}\\frac{\\mathsf{O T}_{h,\\lambda}(\\mu_{t},\\nu_{t})-\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}{t}=\\int\\varphi^{(\\mu,\\nu)}d(\\rho-\\mu)+\\int\\psi^{(\\mu,\\nu)}d(\\eta-\\nu).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This limit is invariant under the transformation $(\\varphi^{(\\mu,\\nu)},\\psi^{(\\mu,\\nu)})\\mapsto(\\varphi^{(\\mu,\\nu)}+C,\\psi^{(\\mu,\\nu)}-C)$ , proving the claim. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 8. Fix $\\lambda>0$ and arbitrary sub-Gaussian distributions $\\rho,\\eta,\\rho^{\\prime},\\eta^{\\prime}\\in\\mathcal{P}(\\mathbb{R}^{d})$ with a shared constant $\\tau^{2}>0$ . Then, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\rho,\\eta)-\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})|\\le\\|\\rho\\otimes\\eta-\\rho^{\\prime}\\otimes\\eta^{\\prime}\\|_{\\infty,\\mathcal{F}_{\\tau,h}^{\\oplus}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $(\\varphi^{(\\rho,\\eta)},\\psi^{(\\rho,\\eta)}),\\;(\\varphi^{(\\rho^{\\prime},\\eta^{\\prime})},\\psi^{(\\rho^{\\prime},\\eta^{\\prime})})$ , and ${(\\varphi^{(\\rho^{\\prime},\\eta)},\\psi^{(\\rho^{\\prime},\\eta)})}$ be the optimal potentials for ${\\mathsf{O T}}_{h,\\lambda}(\\rho,\\eta)$ , $0\\mathsf{T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})$ , and $0\\mathsf{T}_{h,\\lambda}(\\rho^{\\prime},\\eta)$ described in Lemma 6. Then, by analogy with (13) and (14) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{O T}_{h,\\lambda}(\\rho,\\eta)-\\mathsf{O T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})\\le\\displaystyle\\int\\varphi^{(\\rho,\\eta^{\\prime})}d(\\rho-\\rho^{\\prime})+\\int\\psi^{(\\rho,\\eta)}d(\\eta-\\eta^{\\prime}),}\\\\ &{\\mathsf{O T}_{h,\\lambda}(\\rho,\\eta)-\\mathsf{O T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})\\ge\\displaystyle\\int\\varphi^{(\\rho^{\\prime},\\eta^{\\prime})}d(\\rho-\\rho^{\\prime})+\\int\\psi^{(\\rho,\\eta^{\\prime})}d(\\eta-\\eta^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\displaystyle|0\\mathsf{T}_{h,\\lambda}(\\rho,\\eta)-0\\mathsf{T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})|\\le\\left|\\int\\varphi^{(\\rho^{\\prime},\\eta)}d(\\rho^{\\prime}-\\rho)+\\int\\psi^{(\\rho^{\\prime},\\eta^{\\prime})}d(\\eta^{\\prime}-\\eta)\\right|}&{}\\\\ {\\displaystyle\\bigvee\\left|\\int\\varphi^{(\\rho,\\eta)}d(\\rho^{\\prime}-\\rho)+\\int\\psi^{(\\rho^{\\prime},\\eta)}d(\\eta^{\\prime}-\\eta)\\right|.}&{}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As the constants from Lemma 6 (and hence the constants defining $\\mathcal{F}_{\\tau,h}^{\\oplus})$ are independent of the choice of sub-Gaussian distributions (rather they depend only on the sub-Gaussian constant), $\\big(\\varphi^{(\\rho^{\\prime},\\eta)},\\psi^{(\\rho^{\\prime},\\eta^{\\prime})}\\big)$ and $\\big(\\varphi^{(\\rho,\\eta)},\\psi^{(\\rho^{\\prime},\\eta)}\\big)$ are elements of F\u03c4,h so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\rho,\\eta)-\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\rho^{\\prime},\\eta^{\\prime})|\\le\\|\\rho\\otimes\\eta-\\rho^{\\prime}\\otimes\\eta^{\\prime}\\|_{\\infty,\\mathcal{F}_{\\tau,h}^{\\oplus}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 9. Fix $\\lambda>0$ and sub-Gaussian distributions $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^{d})$ with a shared constant $\\tau^{2}>0$ . Then, $\\sqrt{n}({\\hat{\\mu}}_{n}\\otimes{\\hat{\\nu}}_{n}-\\mu\\otimes\\nu)\\ {\\stackrel{d}{\\to}}\\ G_{\\mu\\otimes\\nu}$ in $\\ell^{\\infty}(\\mathcal{F}_{\\tau,h}^{\\oplus})$ , where $G_{\\mu\\otimes\\nu}$ is a tight Gaussian process in $\\ell^{\\infty}(\\mathcal{F}_{\\tau,h}^{\\oplus})$ for which $G_{\\mu\\otimes\\nu}(f\\oplus g)=N(0,\\mathrm{var}_{\\mu}(f)+\\mathrm{var}_{\\nu}(g))$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. From the proof of Lemma 27 in [Goldfeld et al., 2024b] (see also Lemma 8 in Nietert et al., 2021), we \u221asee that the funct\u221aion class $\\mathcal{F}_{\\tau,h}$ is $\\mu$ -Donsker and $\\nu$ -Donsker (i.e. the associated empirical processes $\\sqrt{n}(\\hat{\\mu}_{n}\\!-\\!\\mu)$ and $\\sqrt{n}(\\hat{\\nu}_{n}\\!-\\!\\nu)$ converge weakly to tight mean-zero Brownian bridge processes in $\\ell^{\\infty}(\\mathcal{F}_{\\tau,h})$ with respective covariance functions given by $(f,g)\\,\\in\\,{\\mathcal{F}}_{\\tau,h}\\,\\times{\\mathcal{F}}_{\\tau,h}\\,\\mapsto\\,{\\dot{\\mathrm{cov}}}_{\\mu}(f,g)$ and $(f,g)\\,\\in\\,{\\mathcal{F}}_{\\tau,h}\\,\\times{\\mathcal{F}}_{\\tau,h}\\,\\mapsto\\,\\operatorname{cov}_{\\nu}(f,g)$ respectively) provided that $\\begin{array}{r}{\\sum_{r=1}^{\\infty}r^{d+k p_{k}-1}\\mathbb{P}_{\\mu}(\\|X\\|\\geq}\\end{array}$ $r-1)^{1/2}<\\infty$ for $k=\\lfloor{d\\mathord{\\left/{\\vphantom{d\\cdot2}}\\right.\\kern-\\nulldelimiterspace}2}\\rfloor+1$ and likewise for $\\nu$ . By the Chernoff bound, $\\mathbb{P}_{\\mu}(\\|X\\|\\geq r-1)\\leq$ $\\mathbb{E}_{\\mu}\\left[e^{t\\|X\\|}\\right]e^{-t(r-1)}$ for any $t\\,>\\,0$ . The standard inequality $\\|z\\|\\leq\\|z\\|_{1}$ for any $z\\,\\in\\,\\mathbb{R}^{d}$ yields $\\mathbb{E}_{\\mu}\\left[e^{t\\|X\\|}\\right]\\leq\\mathbb{E}_{\\mu}\\left[e^{t\\|X\\|_{1}}\\right]\\leq2e^{\\frac{t^{2}\\tau^{2}}{2}}$ (recall the proof of Lemma 6). It readily follows that the sum $\\begin{array}{r}{\\sum_{r=1}^{\\infty}r^{d+k p_{k}-1}\\mathbb{P}_{\\mu}(\\|X\\|\\geq r-1)^{1/2}}\\end{array}$ is finite, establishing Donskerness of the class with respect to $\\mu$ and hence $\\nu$ by the same argument. ", "page_idx": 23}, {"type": "text", "text": "By independence of the samples $(X_{i})_{i=1}^{n}$ and $(Y_{j})_{j=1}^{n}$ we have by Example 1.4.6 in [van der Vaart and Wellner, 1996], Lemma 3.2.4 in [Dudley, 2014], and Donskerness of the class that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sqrt{n}(\\hat{\\mu}_{n}-\\mu),\\sqrt{n}(\\hat{\\nu}_{n}-\\nu))\\xrightarrow{d}(G_{\\mu},G_{\\nu})\\mathrm{~in~}\\ell^{\\infty}(\\mathcal{F}_{\\tau,h})\\times\\ell^{\\infty}(\\mathcal{F}_{\\tau,h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $G_{\\mu}$ and $G_{\\nu}$ are independent tight $\\mu$ - and $\\nu$ -Brownian bridge processes. As the map $(\\ell,\\ell^{\\prime})\\in$ $\\ell^{\\infty}(\\mathscr{F}_{\\tau,h})\\,\\times\\,\\ell^{\\infty}(\\mathscr{F}_{\\tau,h})\\,\\mapsto\\,\\ell\\otimes\\ell^{\\prime}\\,\\in\\,\\ell^{\\infty}(\\mathscr{F}_{\\tau,h}^{\\oplus})$ is continuous (indeed $\\|\\ell\\otimes\\ell^{\\prime}\\|_{\\infty,\\mathcal{F}_{\\tau,h}^{\\oplus}}\\,\\le\\,\\|\\ell\\|_{\\mathcal{F}_{\\tau,h}}\\,+$ $\\|\\ell^{\\prime}\\|_{\\mathcal{F}_{\\tau,h}})$ , we have by the continuous mapping theorem that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{n}(\\widehat{\\mu}_{n}-\\mu)\\otimes\\sqrt{n}(\\widehat{\\nu}_{n}-\\nu))=\\sqrt{n}\\left(\\widehat{\\mu}_{n}\\otimes\\widehat{\\nu}_{n}-\\mu\\otimes\\nu\\right)\\overset{d}{\\rightarrow}G_{\\mu\\otimes\\nu}\\;\\mathrm{in}\\;\\ell^{\\infty}(\\mathcal{F}_{\\tau,h}^{\\oplus}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $G_{\\mu\\otimes\\nu}(f_{0}\\oplus f_{1})=G_{\\mu}(f_{0})+G_{\\nu}(f_{1})$ for any $f_{0}\\oplus f_{1}\\in\\mathcal{F}_{\\tau,h}^{\\oplus}$ F\u03c4\u2295,h, proving the claim. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2. Throughout, we fix some $\\bar{\\tau}>\\tau$ and observe that if $\\mu,\\nu$ are $\\tau^{2}$ -sub-Gaussian, then they are also $\\bar{\\tau}^{2}$ -sub-Gaussian. From the proof of Proposition 1 in [Goldfeld et al., 2024b] (see also Remark 4 of the same reference), Lemmas 7 and 8 together imply that the functional $\\dot{\\rho}\\otimes\\eta\\in\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}\\mapsto\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\rho,\\eta)$ is Hadamard directionally differentiable at $\\mu\\otimes\\nu$ tangentially to $\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ (treated as a convex subset of $\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}))$ with derivative $\\gamma\\in\\mathcal{T}_{\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}}(\\mu\\otimes\\nu)\\mapsto\\gamma(\\varphi\\oplus\\psi)$ , where $(\\varphi,\\psi)$ denote any pair of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ satisfying (4) on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , and the derivative is defined on the tangent cone to $\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ at $\\mu\\otimes\\nu$ which is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{T_{\\mathcal{P}_{\\tau}^{\\otimes}}(\\mu\\otimes\\nu):=\\left\\{\\gamma\\in\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}):\\exists\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}\\supset(\\rho_{n}\\otimes\\eta_{n})_{n\\in\\mathbb{N}}\\to\\mu\\otimes\\nu\\;\\mathrm{in}\\;\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus})\\;\\mathrm{and}\\;t_{n}\\downarrow0\\right.}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\mathrm{s.t.}\\;\\gamma=\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\frac{\\rho_{n}\\otimes\\eta_{n}-\\mu\\otimes\\nu}{t_{n}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "see Appendix D for precise definitions. Note that we have identified $0\\mathsf{T}_{h,\\lambda}(\\rho,\\eta)$ with a functional on $\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ ; such an identification is well-defined in light of the discussion following Proposition 1 in [Goldfeld et al., 2024b]. ", "page_idx": 23}, {"type": "text", "text": "The same implications hold for the functional $\\rho\\otimes\\eta\\in\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}\\mapsto\\mathsf{O}\\mathsf{T}_{\\bar{h},\\lambda}(\\rho,\\eta)$ in light of the discussion preceding Lemma 7, with corresponding derivative $\\gamma\\in{\\mathcal{T}}_{{\\mathcal{P}}_{\\bar{\\tau}}^{\\otimes}}(\\mu\\otimes\\nu)\\mapsto\\gamma({\\bar{\\varphi}}\\oplus{\\bar{\\psi}})$ , where $(\\Bar\\varphi,\\Bar\\psi)$ is any pair of optimal potentials for $0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)$ satisfying (4) on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ . ", "page_idx": 24}, {"type": "text", "text": "Note that $\\varepsilon_{h,\\lambda}(\\mu,\\nu)=f\\circ\\left(\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu),0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)\\right)$ for $f:(x,y)\\in\\mathbb{R}\\times\\mathbb{R}\\mapsto x/y$ such that the chain rule (cf. e.g. Proposition 3.6 in Shapiro, 1990) guarantees that $\\rho\\otimes\\eta\\in\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}\\mapsto\\varepsilon_{h,\\lambda}(\\rho,\\eta)$ is Hadamard directionally differentiable at $\\mu\\otimes\\nu$ tangentially to $\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ with derivative ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigl(\\varepsilon_{h,\\lambda}\\bigr)_{\\mu\\otimes\\nu}^{\\prime}:\\gamma\\in\\mathcal{T}_{\\mathcal{P}_{\\tau}^{\\otimes}}(\\mu\\otimes\\nu)\\mapsto\\frac{1}{\\mathsf{O T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\gamma(\\varphi\\oplus\\psi)-\\frac{\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\gamma(\\bar{\\varphi}\\oplus\\bar{\\psi}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is notably linear as a function of $\\gamma$ . It will be useful to rewrite this expression in terms of a single evaluation of $\\gamma$ . To this end, observe that if $f_{1}\\oplus f_{2},g_{1}\\oplus g_{2}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ , then so too is $(\\alpha f_{1}\\!-\\!\\beta g_{1})\\oplus(\\alpha\\bar{f_{2}}\\!-$ $\\beta g_{2})$ for any $\\alpha,\\beta\\in\\mathbb{R}$ with $|\\alpha|+|\\beta|\\leq1$ . Moreover, setting $\\begin{array}{r}{\\dot{M}=\\frac{1}{\\mathsf{O T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\,\\bigvee\\frac{\\mathsf{O T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}}\\end{array}$ (which is assumed to be nonzero by construction), we have that $\\begin{array}{r}{0\\leq\\frac{1}{2M}\\frac{1}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}\\lor\\frac{1}{2M}\\frac{0\\top_{h,\\lambda}(\\mu,\\nu)}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\leq\\frac{1}{2}}\\end{array}$ so that we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{0\\top_{\\bar{h},\\lambda}(\\mu,\\nu)}(\\varphi\\oplus\\psi)-\\frac{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)^{2}}(\\bar{\\varphi}\\oplus\\bar{\\psi})}\\\\ &{\\,\\,=2M\\left(\\frac{1}{2M}\\frac{1}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\varphi\\oplus\\frac{1}{2M}\\frac{1}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\psi\\right)}\\\\ &{\\phantom{m m m m m m m m m m m m m}-2M\\left(\\frac{1}{2M}\\frac{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)^{2}}\\bar{\\varphi}\\oplus\\frac{1}{2M}\\frac{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)^{2}}\\bar{\\psi}\\right)}\\\\ &{\\,\\,=2M\\left(\\frac{1}{2M}\\frac{1}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\varphi-\\frac{1}{2M}\\frac{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)^{2}}\\bar{\\varphi}\\right.}\\\\ &{\\phantom{m m m m m m m m m m}\\left.\\Phi\\,\\frac{1}{2M}\\frac{1}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}\\psi-\\frac{1}{2M}\\frac{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{\\mathsf{O}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)^{2}}\\bar{\\psi}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the final term in brackets is an element of $\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ . Further, for any $\\gamma\\in\\mathcal{T}_{\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}}(\\mu\\otimes\\nu)$ , there exists a sequence $(\\rho_{n}\\otimes\\eta_{n})_{n\\in\\mathbb{N}}\\subset\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ which converges to $\\mu\\mathcal{B}\\nu$ in $\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus})$ and a sequence $t_{n}\\downarrow0$ for which $\\begin{array}{r}{\\gamma=\\operatorname*{lim}_{n\\to\\infty}\\frac{\\rho_{n}\\otimes\\eta_{n}-\\mu\\otimes\\nu}{t_{n}}}\\end{array}$ . Thus, if $f_{0}\\oplus f_{1},g_{0}\\oplus g_{1}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ are such that $f_{0}+g_{0}\\oplus f_{1}+g_{1}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\gamma(f_{0}\\oplus f_{1})+\\gamma(g_{0}\\oplus g_{1})}}\\\\ &{=\\operatorname*{lim}_{n\\to\\infty}t_{n}^{-1}\\left(\\int f_{0}d\\rho_{n}+\\int f_{1}d\\eta_{n}-\\int f_{0}d\\rho-\\int f_{1}d\\eta\\right)}\\\\ &{+\\operatorname*{lim}_{n\\to\\infty}t_{n}^{-1}\\left(\\int g_{0}d\\rho_{n}+\\int g_{1}d\\eta_{n}-\\int g_{0}d\\rho-\\int g_{1}d\\eta\\right)}\\\\ &{=\\operatorname*{lim}_{n\\to\\infty}t_{n}^{-1}\\left(\\int f_{0}+g_{0}d\\rho_{n}+\\int f_{1}+g_{1}d\\eta_{n}-\\int f_{0}+g_{0}d\\rho-\\int f_{1}+g_{1}d\\eta\\right)}\\\\ &{=\\gamma(f_{0}+g_{0}\\oplus f_{1}+g_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Likewise, if $\\alpha f_{0}\\oplus\\alpha f_{1}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ for some $\\alpha\\in\\mathbb R$ , and $f_{0}\\oplus f_{1}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma(\\alpha f_{0}\\oplus\\alpha f_{1})=\\alpha\\operatorname*{lim}_{n\\to\\infty}t_{n}^{-1}\\left(\\int f_{0}d\\rho_{n}+\\int f_{1}d\\eta_{n}-\\int f_{0}d\\rho-\\int f_{1}d\\eta\\right)=\\alpha\\gamma(f_{0}\\oplus f_{1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With this, (15) can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\varepsilon_{h,\\lambda})_{\\mu\\otimes\\nu}^{\\prime}:\\gamma\\in{\\mathcal{T}}_{{\\mathcal{P}}_{\\overline{{\\tau}}}^{\\otimes}}(\\mu\\otimes\\nu)\\mapsto}}\\\\ {{2M\\gamma\\left(\\displaystyle\\frac{1}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\varphi-\\displaystyle\\frac{0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\bar{\\varphi}\\oplus\\displaystyle\\frac{1}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\psi-\\displaystyle\\frac{0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\bar{\\psi}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given the above differentiability result and Lemma 9, part 1 of Theorem 2 will follow from the functional delta method (see Lemma 10 in Appendix D) upon showing that $\\hat{\\mu}_{n}\\otimes\\hat{\\nu}_{n}\\in\\mathcal{P}_{\\bar{\\tau}}^{\\otimes}$ with probability approaching one and noting that $G_{\\mu\\otimes\\nu}\\in{\\mathcal{T}}_{{\\mathcal{P}}_{\\bar{\\tau}}^{\\otimes}}(\\mu\\otimes\\nu)$ with probability one as follows from the portmanteau theorem. To this end, note that, by the law of large numbers, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{\\mu}_{n}}\\left[\\exp(\\|X\\|_{1}^{2}/2\\tau^{2})\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\exp(\\|X_{i}\\|_{1}^{2}/2\\tau^{2})\\rightarrow\\mathbb{E}_{\\mu}\\left[\\exp(\\|X\\|_{1}^{2}/2\\tau^{2})\\right]\\leq2^{\\frac{\\tau^{2}}{\\tau^{2}}}<2.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "almost surely such that $\\textstyle{\\hat{\\mu}}_{n}$ is $\\bar{\\tau}^{2}$ -sub-Gaussian with probability approaching one. The same deliberations imply that $\\hat{\\nu}_{n}$ share the same property. ", "page_idx": 25}, {"type": "text", "text": "By applying the delta method, we obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{n}\\big(\\varepsilon_{h,\\lambda}\\big(\\hat{\\mu}_{n},\\hat{\\nu}_{n}\\big)-\\varepsilon_{h,\\lambda}(\\mu,\\nu)\\big)\\stackrel{d}{\\to}(\\varepsilon_{h,\\lambda})_{\\mu\\otimes\\nu}^{\\prime}\\,(G_{\\mu\\otimes\\nu}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and using the explicit expression for the derivative from (16), we see that ${\\left({\\varepsilon_{h,\\lambda}}\\right)_{\\mu\\otimes\\nu}^{\\prime}}\\left({{G_{\\mu\\otimes\\nu}}}\\right)$ is equal in distribution to $2M N(0,v^{2}+w^{2})$ , where $\\begin{array}{r}{v^{2}\\,=\\,\\mathrm{var}_{\\mu}\\left(\\frac{1}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\varphi-\\frac{\\mathsf{\\bar{\\Pi}}_{0}\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\bar{\\varphi}\\right)}\\end{array}$ and $\\begin{array}{r}{w^{2}=\\mathrm{var}_{\\nu}\\left(\\frac{1}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)}\\psi-\\frac{0\\mathsf{T}_{h,\\lambda}(\\mu,\\nu)}{2M0\\mathsf{T}_{\\bar{h},\\lambda}(\\mu,\\nu)^{2}}\\bar{\\psi}\\right)}\\end{array}$ ; the $2M$ terms in the variance and multiplying the normal distribution evidently cancel out to give the desired formula for the limiting variance. ", "page_idx": 25}, {"type": "text", "text": "As for the bootstrap consistency result, since (15) is linear, it follows from Corollary 1 in [Goldfeld et al., 2024b] that $\\bar{(\\rho,\\eta)}\\mapsto\\varepsilon_{h,\\dot{\\lambda}}(\\rho,\\eta)$ is Hadamard directionally differentiable at $\\mu\\otimes\\nu$ tangentially to $\\operatorname{spt}(G_{\\mu\\otimes\\nu})$ . As in the proof of Lemma 9, the class $\\mathcal{F}_{\\bar{\\tau}}$ is $\\mu_{-}$ - and $\\nu$ -Donsker such that the bootstrapped empirical processes $\\sqrt{n}(\\hat{\\mu}_{n}^{B}-\\hat{\\mu}_{n})$ and $\\sqrt{n}(\\hat{\\nu}_{n}^{B}-\\hat{\\nu}_{n})$ are asymptotically measurable and converge conditionally in distribution to the $\\mu{}_{\\rightmoon}$ - and $\\nu$ -Brownian bridge processes $G_{\\mu}$ and $G_{\\nu}$ respectively (see Chapter 3.6 in van der Vaart \u221aand Wellner, 19\u221a96). By Lemma 1.4.4 and Example 1.4.6 in [van der Vaart and Wellner, 1996], $(\\sqrt{n}(\\hat{\\mu}_{n}^{B}-\\hat{\\mu}_{n}),\\sqrt{n}(\\hat{\\nu}_{n}^{B}-\\hat{\\nu}_{n}))$ is asymptotically measurable and converges conditionally in distribution to $\\left(G_{\\mu},G_{\\nu}\\right)$ as elements of $\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau}})\\times\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau}})$ . As the map $(\\ell,\\ell^{\\prime})\\in\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau}})\\times\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau}})\\mapsto\\ell\\otimes\\ell^{\\prime}\\in\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus})$ is continuous, $(\\sqrt{n}(\\hat{\\mu}_{n}^{B}-\\hat{\\mu}_{n})\\otimes\\sqrt{n}(\\hat{\\nu}_{n}^{B}-\\hat{\\nu}_{n}))$ $\\ell^{\\infty}(\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus})$ otwihcealrley $G_{\\mu\\otimes\\nu}(f_{0}\\oplus f_{1})=G_{\\mu}(f_{0})+G_{\\nu}(f_{1})$ nfaolrl ya niny $f_{0}\\oplus f_{1}\\in\\mathcal{F}_{\\bar{\\tau},h}^{\\oplus}$ $G_{\\mu\\otimes\\nu}$ as elements of ", "page_idx": 25}, {"type": "text", "text": "Bootstrap consistency then follows from Theorem 23.9 in [Van der Vaart, 2000] by applying the logic from the first half of the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D The Functional Delta Method ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our strategy for deriving limit distributions and consistency of the bootstrap is based upon the functional delta method, which generalizes the standard delta method for functions of simple random variables. This section provides a brief introduction to the functional delta method following the exposition of [R\u00f6misch, 2006]. Throughout, convergence in distribution is understood in the sense of Hoffmann-J\u00f8rgensen when necessary (cf. e.g. Chapter 1 in [van der Vaart and Wellner, 1996]). ", "page_idx": 25}, {"type": "text", "text": "Much like the delta method which, identifies the distributional limit of $\\sqrt{n}(g(X_{n})\\,-\\,g(\\mu))$ as $N(0,\\sigma^{2}(g^{\\prime}(\\mu))^{2})$ $n\\to\\infty$ provided that ${\\sqrt{n}}(X_{n}-\\mu)\\ {\\xrightarrow{d}}\\ N(0,\\sigma^{2})$ and that $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ is differentiable at $\\mu$ (see Proposition 8.14 in Keener, 2010), the functional delta method establishes the limit distribution of a functional $f:\\Theta\\subset D\\rightarrow\\mathbb{R}$ , where $D$ is a normed vector space. In this setting, the surrogate for the derivative in the standard delta method is the Hadamard directional derivative. ", "page_idx": 25}, {"type": "text", "text": "Definition 4 (Hadamard directional derivative [R\u00f6misch, 2006, Shapiro, 1990]). Let $D$ be a normed vector space and fix a non-empty set $\\Theta\\subset D$ . The tangent cone to $\\Theta$ at $\\theta\\in\\Theta$ is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{\\Theta}(\\theta):=\\left\\{h\\in D:h=\\operatorname*{lim}_{n\\to\\infty}\\frac{\\theta_{n}-\\theta}{t_{n}},\\,f o r\\,s o m e\\,\\theta_{n}\\in\\Theta,\\theta_{n}\\to\\theta,t_{n}\\downarrow0\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A functional $f:\\Theta\\to\\mathbb{R}$ is Hadamard directionally differentiable at $\\theta\\in\\Theta$ tangentially to $\\Theta$ if there exists a map $f_{\\theta}^{\\prime}:\\mathcal{T}_{\\Theta}(\\theta)\\rightarrow\\mathbb{R}$ satisfying ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}{\\frac{f(\\theta+t_{n}h_{n})-f(\\theta)}{t_{n}}}=f_{\\theta}^{\\prime}(h),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any $h\\in\\mathcal{T}_{\\Theta}(\\theta),\\,t_{n}\\downarrow0,$ , and $h_{n}\\to h$ in $D$ with $\\theta+t_{n}h_{n}\\in\\Theta$ . ", "page_idx": 25}, {"type": "text", "text": "This notion of differentiability is compatible with distributional convergence of random elements of $D$ in the sense that the following generalization of the delta method holds. ", "page_idx": 26}, {"type": "text", "text": "Lemma 10 (Functional delta method [R\u00f6misch, 2006, Shapiro, 1991]). Fix a probability space $(\\Omega,\\Sigma,{\\mathbb{P}})$ and let $D$ be a normed vector space and $f:\\Theta\\subset D\\rightarrow\\mathbb{R}$ be Hadamard directionally differentiable at $\\theta\\in\\Theta$ tangentially to ${\\mathcal{T}}_{\\Theta}(\\theta)$ with derivative $f_{\\theta}^{\\prime}:\\mathcal{T}_{\\Theta}(\\theta)\\rightarrow\\mathbb{R}$ . Let $T_{n}:\\Omega\\to\\Theta$ be maps such that $r_{n}(T_{n}-\\theta)\\stackrel{d}{\\rightarrow}T$ for some norming sequence $r_{n}\\to\\infty$ and a measurable map $T:\\Omega\\to T_{\\Theta}(\\theta)\\subset D$ . Then $r_{n}\\big(f(T_{n})-f(\\theta)\\big)\\stackrel{d}{\\to}f_{\\theta}^{\\prime}(T)$ and, $i f\\Theta$ is convex, $r_{n}\\left(f(T_{n})-f(\\theta)\\right)-$ $f_{\\theta}^{\\prime}\\big(r_{n}(T_{n}-\\theta)\\big)\\rightarrow0$ in outer probability. ", "page_idx": 26}, {"type": "text", "text": "Whilst Lemma 10 is sufficient to derive limit distributions, bootstrap consistency typically requires the following notion of full Hadamard differentiability (see e.g. Theorem 23.9 in [Van der Vaart, 2000] or Theorem 3.9.11 in van der Vaart and Wellner, 1996). A functional $f:D\\rightarrow\\mathbb{R}$ is said to be Hadamard differentiable at $\\theta$ tangentially to a vector subspace $D_{0}\\subset D$ if there exists a continuous linear functional $f_{\\theta}^{\\prime}:D_{0}\\mapsto\\mathbb{R}$ satisfying (17) for any $h\\in\\mathcal{T}_{\\Theta}(\\theta),\\,t_{n}\\neq0,\\,t_{n}\\to0$ , and $h_{n}\\to h$ in $\\mathfrak{D}$ with $\\theta+t_{n}h_{n}^{\\textrm{\\scriptsize e}}\\Theta$ . The following lemma enables a connection between full and directional Hadamard differentiability. ", "page_idx": 26}, {"type": "text", "text": "Lemma 11 (Lemma 2 in [Goldfeld et al., 2024b]). If $f:\\Theta\\subset D\\rightarrow\\mathbb{R}$ is Hadamard directionally differentiable at $\\theta\\in\\Theta$ tangentially to ${\\mathcal{T}}_{\\Theta}(\\theta)$ and $f_{\\theta}^{\\prime}$ is linear on a subspace $D_{0}\\subset T_{\\Theta}(\\theta)$ , then $f$ is Hadamard differentiable at $\\theta$ tangentially to $\\mathfrak{D}_{0}$ . ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction make the following claims: 1) we introduce a statistic for accessing a notion of multivariate almost stochastic dominance 2) we establish a CLT and bootstrapping for this statistic 3) we use this statistic to compare the relative performance of models. The first two points are addressed in Section 4 whereas Section 4.2 clarifies the testing framework and our approach is validated empirically in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: As we mention in the discussion section, a limitation of our work is that does not treat the case where choices are risk averse. However, we indicated how these situations can be addressed by combining our statistical testing framework with multivariate Lorenz order. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The complete proofs of all results are provided in the appendices along with a short outline in the main text. All relevant assumptions are also clearly stated. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Algorithm 1, included in the appendix, provides the relevant pseudocode to implement the proposed statistical framework. The descriptions of the numerical experiments provide all required details to reproduce the results provided. A complete code package is also included in the submission. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 28}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: As highlighted in the previous point, the code is included in the submission, and all relevant experimental details are provided. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All relevant experimental details are provided in the main text. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All experiments are provided with error bars and are computed using the bootstrap as justified by the main theoretical results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have provided details on the type of hardware and computation time needed in the experiment section. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work provides a theoretical framework for assessing a statistical property of multivariate data. The application explored in this work is to benchmarking of different models according to a vector of metrics. Such a framework can be broadly useful for identifying ethical concerns in models, but in itself poses no such concerns. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The statistical framework proposed in this paper is used to compare the outputs of different models based on metrics of interest. On its own, this framework has no social impact. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: No new data or models were released in conjunction with this paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All relevant details regarding existing assets are provided in the main text. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No new assets were released in conjunction with this paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects was included in this paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing or research with human subjects was included in this paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]