[{"type": "text", "text": "Transformers are Minimax Optimal Nonparametric In-Context Learners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Juno Kim1,2\u2217 Tai Nakamaki1 Taiji Suzuki1,2 1University of Tokyo 2Center for Advanced Intelligence Project, RIKEN \u2217junokim@g.ecc.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In-context learning (ICL) of large language models has proven to be a surprisingly effective method of learning a new task from only a few demonstrative examples. In this paper, we study the efficacy of ICL from the viewpoint of statistical learning theory. We develop approximation and generalization error bounds for a transformer composed of a deep neural network and one linear attention layer, pretrained on nonparametric regression tasks sampled from general function spaces including the Besov space and piecewise $\\gamma$ -smooth class. We show that sufficiently trained transformers can achieve \u2013 and even improve upon \u2013 the minimax optimal estimation risk in context by encoding the most relevant basis representations during pretraining. Our analysis extends to high-dimensional or sequential data and distinguishes the pretraining and in-context generalization gaps. Furthermore, we establish information-theoretic lower bounds for meta-learners w.r.t. both the number of tasks and in-context examples. These findings shed light on the roles of task diversity and representation learning for ICL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated remarkable capabilities in understanding and generating natural language data. In particular, the phenomenon of in-context learning (ICL) has recently garnered widespread attention. ICL refers to the ability of pretrained LLMs to perform a new task by being provided with a few examples within the context of a prompt, without any parameter updates or fine-tuning. It has been empirically observed that few-shot prompting is especially effective in large-scale models (Brown et al., 2020) and requires only a couple of examples to consistently achieve high performance (Garc\u00eda et al., 2023). In contrast, Raventos et al. (2023) demonstrate that sufficient pretraining task diversity is required for the emergence of ICL. However, we still lack a comprehensive understanding of the statistical foundations of ICL and few-shot prompting. ", "page_idx": 0}, {"type": "text", "text": "A vigorous line of research has been directed towards understanding ICL of single-layer linear attention models pretrained on the query prediction loss of linear regression tasks (Garg et al., 2022; Aky\u00fcrek et al., 2023; Zhang et al., 2023; Ahn et al., 2023; Mahankali et al., 2023; Wu et al., 2024). It has been shown that the global minimizer of the $L^{2}$ pretraining loss implements one step of GD on a least-squares linear regression objective (Mahankali et al., 2023) and is nearly Bayes optimal (Wu et al., 2024). Moreover, risk bounds with respect to the context length (Zhang et al., 2023) and number of tasks (Wu et al., 2024) have been obtained. ", "page_idx": 0}, {"type": "text", "text": "Other works have examined ICL of more complex multi-layer transformers. Bai et al. (2023); von Oswald et al. (2023) give specific transformer constructions which simulate GD in context, however it is unclear how such meta-algorithms may be learned. Another approach is to study learning with representations, where tasks consist of a fixed nonlinear feature map composed with a varying linear function. Guo et al. (2023) empirically found that trained transformers exhibit a separation where lower layers transform the input and upper layers perform linear ICL. Recently, Kim and Suzuki (2024) analyzed a model consisting of a shallow neural network followed by a linear attention layer and proved that the MLP component learns to encode the true features during pretraining. However, they assumed the infinite task and sample size limit and did not study generalization capabilities. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this paper, we analyze the optimality of ICL from the perspective of statistical learning theory. Our object of study is a transformer consisting of a deep neural network with $N$ - dimensional output followed by one linear attention layer. The model is pretrained on $n$ input-output samples from $T$ nonparametric regression tasks, generated from a suitably decaying distribution on a general function space. Compared to previous works, we take a crucial step towards understanding practical multi-layer transformers by incorporating the representation learning capabilities of the DNN module. From a more abstract perspective, this work can also be situated as a nonlinear extension of meta-learning. Our contributions are highlighted below. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a general framework for upper bounding the in-context estimation error of the empirical risk minimizer in terms of the approximation error of the neural network and separate in-context and pretraining generalization gaps depending on $n,T$ , respectively. \u2022 In the Besov space setting, we show that ICL achieves nearly minimax optimal risk $\\textstyle n^{-{\\frac{2\\alpha}{2\\alpha+d}}}$ when $T$ is sufficiently large. Since LLMs are pretrained on vast amounts of data in practice, $T$ can be taken to be nearly infinite, justifying the emergence of ICL at large scales. We extend the optimality guarantees to nearly dimension-free rates in the anisotropic Besov space and also to learning sequential data with deep transformers in the piecewise $\\gamma.$ -smooth function class. \u2022 We show that ICL can improve upon the a priori optimal rate when the task class basis resides in a coarser Besov space by learning to encode informative basis representations, emphasizing the importance of pretraining on diverse tasks. \u2022 We also derive information-theoretic lower bounds for the minimax risk in both $n,T$ , rigorously confirming that ICL is jointly optimal when $T$ is large (Besov space setting), while any metalearning method is jointly suboptimal when $T$ is small (coarser space setting). This separation aligns with empirical observations of a task diversity threshold (Raventos et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "The paper is structured as follows. In Section 2, the regression tasks and transformer model are defined in an abstract setting. In Section 3, we present the general framework for estimating the ICL approximation and generalization error. In Section 4, we specialize to the Besov-type and piecewise $\\gamma$ -smooth class settings and show that transformers can achieve or exceed the minimax optimal rate in context. In Section 5, we derive minimax lower bounds. All proofs are deferred to the appendix; moreover, we provide numerical experiments validating our results in Appendix E. ", "page_idx": 1}, {"type": "text", "text": "1.1 Other Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Meta-learning. The theoretical setting of ICL is closely related to meta-learning, where the goal is to infer a shared representation $\\psi^{\\circ}$ with samples from a set of transformed tasks $\\beta_{i}^{\\top}\\psi^{\\circ}$ . When $\\psi^{\\circ}$ is linear, fast rates have been established by Tripuraneni et al. (2020); Du et al. (2021), while the nonlinear case has been studied by Meunier et al. (2023) where $\\psi^{\\circ}$ is a feature projection into a reproducing kernel Hilbert space. Our results can be viewed as extending this body of work to function spaces of generalized smoothness with a specific deep transformer architecture. ", "page_idx": 1}, {"type": "text", "text": "Optimal rates for DNNs. Our analysis extends established optimality results for classes of DNNs in ordinary supervised regression settings to ICL. Suzuki (2019) has shown that deep feedforward networks with the ReLU activation can efficiently approximate functions in the Besov space and thus achieve the minimax optimal rate. This has been extended to the anisotropic Besov space (Suzuki and Nitanda, 2021), convolutional neural networks for infinite-dimensional input (Okumoto and Suzuki, 2022), and transformers for sequence-to-sequence functions (Takakura and Suzuki, 2023). We remark that a work in progress (Imaizumi, 2024) also studies the sample complexity of ICL of transformers in a Sobolev space setting. ", "page_idx": 1}, {"type": "text", "text": "Pretraining dynamics for ICL. While how ICL arises from optimization is not fully understood, there are encouraging developments in this direction. Zhang et al. (2023) has shown for one layer of linear attention that running GD on the population risk always converges to the global optimum. This was extended to incorporate a linear output layer by Zhang et al. (2024), and to softmax attention by Huang et al. (2023); Li et al. (2024); Chen et al. (2024). Kim and Suzuki (2024) considered a compound transformer equivalent to ours with a shallow MLP component and proved that the loss landscape becomes benign in the mean-field limit, deriving convergence guarantees for the corresponding gradient dynamics. These analyses indicate that the attention mechanism, while highly nonconvex, may possess structures favorable for gradient-based optimization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Nonparametric Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we analyze the ability of a transformer to solve nonparametric regression problems in context when pretrained on examples from a family of regression tasks, which we describe below. Let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be the input space ( $d$ is allowed to be infinite), $\\mathcal{P}_{\\mathcal{X}}$ a probability distribution on $\\mathcal{X}$ , and $(\\psi_{j}^{\\circ})_{j=1}^{\\infty}$ a fixed countable subset of $L^{2}(\\mathcal{P}_{\\mathcal{X}})$ . A regression function $F_{\\beta}^{\\circ}:\\mathcal{X}\\rightarrow\\mathbb{R}$ is randomly generated for each task by sampling the sequence of coefficients $\\beta\\in\\mathbb{R}^{\\infty}$ from a distribution $\\mathcal{P}_{\\beta}$ on $\\mathcal{B}(\\mathbb{R}^{\\infty})$ ; the class of tasks ${\\mathcal{F}}^{\\circ}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}^{\\circ}=\\left\\{F_{\\beta}^{\\circ}=\\sum_{j=1}^{\\infty}\\beta_{j}\\psi_{j}^{\\circ}\\;\\big|\\;\\beta\\in\\mathrm{supp}\\,\\mathcal{P}_{\\beta}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "endowed with the induced distribution. Each task prompt contains $n$ example input-response pairs $\\{(x_{k},y_{k})\\}_{k=1}^{n}$ . The covariates $x_{k}$ are i.i.d. drawn from $\\mathcal{P}_{\\mathcal{X}}$ and the responses are generated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{k}=F_{\\beta}^{\\circ}(x_{k})+\\xi_{k},\\quad1\\leq k\\leq n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the noise $\\xi_{k}$ is assumed to be i.i.d. with mean zero and $|\\xi_{k}|\\leq\\sigma$ almost surely.1 In addition, we independently generate a query token $\\tilde{x}$ and corresponding output $\\tilde{y}$ in the same manner. ", "page_idx": 2}, {"type": "text", "text": "We proceed to state our assumptions for the regression model. Informally, we suppose a relaxed version of sparsity and orthonormality of $\\psi_{j}^{\\circ}$ and suitable decay rates for the basis expansion. These will be subsequently verified for specific function spaces of interest with their natural decay rate. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (relaxed sparsity and orthonormality of basis functions). For $N\\in\\mathbb{N}$ , there exist integers $\\underline{{N}}^{\\star}<\\bar{N}\\lesssim\\bar{N}$ with $\\bar{\\bar{N}}-\\underline{{{\\bar{N}}}}+1=N$ such that $\\psi_{\\underline{{N}}}^{\\circ},\\cdots\\,,\\psi_{\\bar{N}}^{\\circ}$ are independent and $\\psi_{1}^{\\circ},\\cdots\\,,\\psi_{\\underline{{N}}-1}^{\\circ}$ are all contained in the linear span of $\\psi_{\\underline{{N}}}^{\\circ},\\cdots\\,,\\psi_{\\bar{N}}^{\\circ}$ .\u00af Moreover, there exist $r,C_{1},C_{2},C_{\\infty}>0$ s\u00afuch that and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\Psi,N}:=\\left(\\mathbb{E}_{x\\sim\\mathcal{P}_{X}}[\\psi_{j}^{\\circ}(x)\\psi_{k}^{\\circ}(x)]\\right)_{j,k=\\underline{{N}}}^{\\bar{N}}\\,s a t i s f i e s\\,C_{1}{\\mathbf{I}}_{N}\\preceq\\Sigma_{\\Psi,N}\\preceq C_{2}{\\mathbf{I}}_{N}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left\\|\\sum_{j=\\underline{{N}}}^{\\bar{N}}(\\psi_{j}^{\\circ})^{2}\\right\\|_{L^{\\infty}(\\mathcal{P}_{X})}^{1/2}\\leq C_{\\infty}N^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Denoting the $\\bar{N}$ -basis approximation of $F_{\\beta}^{\\circ}$ as $\\begin{array}{r}{F_{\\beta,\\bar{N}}^{\\circ}:=\\sum_{j=1}^{\\bar{N}}\\beta_{j}\\psi_{j}^{\\circ}}\\end{array}$ , by Assumption 1 there exist \u2018aggregated\u2019 coefficients $\\bar{\\beta}_{\\underline{{N}}},\\cdot\\cdot\\cdot\\,,\\bar{\\beta}_{\\bar{N}}$ uniquely determined by $\\beta$ such that $\\begin{array}{r}{F_{\\beta,\\bar{N}}^{\\circ}=\\sum_{j=\\underline{{N}}}^{\\bar{N}}\\bar{\\beta}_{j}\\psi_{j}^{\\circ}}\\end{array}$ . We define two types of coefficient covariance matrices ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\beta,\\bar{N}}:=\\big(\\mathbb{E}_{\\beta}[\\beta_{j}\\beta_{k}]\\big)_{j,k=1}^{\\bar{N}}\\in\\mathbb{R}^{\\bar{N}\\times\\bar{N}}\\quad\\mathrm{~and~}\\quad\\Sigma_{\\bar{\\beta},N}:=\\big(\\mathbb{E}_{\\beta}[\\bar{\\beta}_{j}\\bar{\\beta}_{k}]\\big)_{j,k=N}^{\\bar{N}}\\in\\mathbb{R}^{N\\times N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (decay of $\\beta$ ). For $s,B>0$ it holds that $\\|F_{\\beta}^{\\circ}\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}\\leq B$ for all $F_{\\beta}^{\\circ}\\in\\mathcal{F}^{\\circ}$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|F_{\\beta}^{\\circ}-F_{\\beta,N}^{\\circ}\\|_{L^{2}(\\mathcal{P}_{x})}^{2}\\lesssim N^{-2s}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "uniformly over $\\beta\\in\\operatorname{supp}\\mathcal{P}_{\\beta}$ . Furthermore, $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ is bounded for all $N$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\n0\\prec\\Sigma_{\\beta,\\bar{N}}\\prec\\mathrm{diag}\\left[(j^{-2s-1}(\\log j)^{-2})_{j=1}^{\\bar{N}}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 2.1. In the simple case where $(\\psi_{j}^{\\circ})_{j=1}^{\\infty}$ is a basis for $L^{2}(\\mathcal{P}_{\\mathcal{X}})$ , we may set $\\underline{{{N}}}=1,\\bar{N}=N$ so that the dependency condition of Assumption 1 is trivially satisfied, moreover, $\\Sigma_{\\bar{\\beta},N}\\,=\\,\\Sigma_{\\beta,N}$ and boundedness of $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ automatically follows from (4). However, the assumptions in the stated form also allow for hierarchical bases with dependencies such as wavelet systems. We also note that (3) and (4) entail basically the same rate but are not equivalent: the uniform bound $|\\beta_{j}|^{2}\\lesssim j^{-2s-1}(\\log j)^{-2}$ along with Assumption 1 implies (3). The $(\\bar{\\log{\\j}})^{-2}$ term can be replaced with any $g(j)$ such that $\\scriptstyle\\sum_{j=1}^{\\infty}j^{-1}g(j)$ is convergent. ", "page_idx": 2}, {"type": "text", "text": "2.2 In-Context Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now describe our transformer model, which takes $n$ context pairs $\\mathbf{X}=(x_{1},\\cdot\\cdot\\cdot\\,,x_{n})\\in\\mathbb{R}^{d\\times n}$ , $\\pmb{y}=(y_{1},\\cdot\\cdot\\cdot\\,,y_{n})^{\\top}\\in\\mathbb{R}^{n}$ and a query token $\\tilde{x}$ as input and returns a prediction for the corresponding output. The covariates are first passed through a nonlinear representation or feature mapping $\\phi:$ $\\vec{\\mathcal{X}}\\stackrel{=}{\\to}\\mathbb{R}^{N}$ , which we assume belongs to a sufficiently powerful class of estimators ${\\mathcal F}_{N}$ . Specifically: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (expressivity of ${\\mathcal F}_{N_{.}}$ ). $\\|\\phi(x)\\|_{2}\\leq B_{N}^{\\prime}$ for some $B_{N}^{\\prime}>0$ for all $x\\in\\mathcal{X}$ , $\\phi\\in\\mathcal{F}_{N}$ . Moreover for some $\\delta_{N}>0$ , there exist $\\phi_{\\underline{{N}}}^{*},\\cdot\\cdot\\cdot\\,,\\phi_{\\bar{N}}^{*}\\in\\mathcal{F}_{N}$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\underline{{{N}}}\\leq j\\leq\\bar{N}}\\bigl|\\|\\psi_{j}^{\\circ}-\\phi_{j}^{*}\\bigr\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}\\leq\\delta_{N}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By choosing ${\\mathcal{F}}_{N}$ and $\\delta_{N}$ to satisfy the above assumption, we will be able to utilize established approximation and generalization guarantees for families of deep neural networks in Section 4. ", "page_idx": 3}, {"type": "text", "text": "The extracted representations $\\phi(\\mathbf{X})=(\\phi(x_{1}),\\cdot\\cdot\\cdot\\,,\\phi(x_{n}))$ are then mapped to a scalar output via a linear attention layer parametrized by a matrix $\\Gamma\\in\\cal S_{N}$ for $\\mathcal{S}_{N}\\subset\\mathbb{R}^{N\\times N}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{\\check{e}}_{\\Theta}(\\mathbf{X},\\pmb{y},\\tilde{x}):=\\frac{1}{n}\\sum_{k=1}^{n}y_{k}\\phi(x_{k})^{\\top}\\Gamma^{\\top}\\phi(\\tilde{x})=\\left\\langle\\frac{\\Gamma\\phi(\\mathbf{X})y}{n},\\phi(\\tilde{x})\\right\\rangle,\\quad\\mathrm{where}~\\Theta=(\\Gamma,\\phi)\\in\\mathcal{S}_{N}\\times\\mathcal{F}_{N}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, the output is constrained to lie on $[-\\bar{B},\\bar{B}]$ by applying $\\mathrm{clip}_{\\bar{B}}(u):=\\operatorname*{max}\\{\\operatorname*{min}\\{u,\\bar{B}\\},-\\bar{B}\\}$ , yielding $f_{\\Theta}(\\mathbf{X},\\pmb{y},\\tilde{x}):=\\mathrm{clip}_{\\bar{B}}(\\check{f}_{\\Theta}(\\mathbf{X},\\pmb{y},\\tilde{x}))$ . We set $S_{N}=\\{\\Gamma\\in\\mathbb{R}^{N\\times N}\\mathrm{~}|\\mathrm{~}0\\preceq\\Gamma\\preceq C_{3}\\mathbf{I}_{N}\\}$ for some $C_{3}>0$ and fix $\\bar{B}=\\bar{B}$ for simplicity. ", "page_idx": 3}, {"type": "text", "text": "The above setup is a restricted reparametrization of linear attention widely used in theoretical analyses (see e.g. Zhang et al., 2023; Wu et al., 2024, for more details), where the values only refer to $\\textit{\\textbf{y}}$ and the query and key matrices are consolidated into one matrix $\\Gamma$ . The form is equivalent to one step of GD with matrix step size and has been shown to be optimal for a single layer of linear attention for linear regression tasks (Ahn et al., 2023; Mahankali et al., 2023). The placement of the attention layer after the DNN module $\\phi$ is justified by the observation that lower layers of trained transformers act as data representations on top of which upper layers perform ICL (Guo et al., 2023). ", "page_idx": 3}, {"type": "text", "text": "During pretraining, the model is presented with $T$ prompts $\\{(\\mathbf{X}^{(t)},\\pmb{y}^{(t)},\\tilde{x}^{(t)})\\}_{t=1}^{T}$ where the tasks $F_{\\beta^{(t)}}^{\\circ}\\in\\mathcal{F}^{\\circ}$ , $\\beta^{(t)}\\sim\\mathcal{P}_{\\beta}$ and tokens $\\mathbf{X}^{(t)}=(x_{1}^{(t)},\\cdot\\cdot\\cdot\\,,x_{n}^{(t)})$ , $\\pmb{y}^{(t)}=(y_{1}^{(t)},\\cdot\\cdot\\cdot\\,,y_{n}^{(t)})^{\\top}$ , $\\tilde{.}^{(t)}$ and $\\tilde{y}^{(t)}$ are independently generated as described in Section 2.1, and is trained to minimize the empirical risk ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\Theta}=\\operatorname*{arg\\,min}_{\\Theta\\in{\\cal S}_{N}\\times{\\mathcal{F}_{N}}}\\widehat{R}(\\Theta),\\quad\\widehat{R}(\\Theta)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\widetilde{y}^{(t)}-f_{\\Theta}({\\mathbf{X}^{(t)}},{\\pmb y}^{(t)},\\tilde{x}^{(t)})\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our goal is to verify the efficiency of ICL as a learning algorithm and show that learning the optimal $\\widehat{\\Theta}$ allows the transformer to solve new random regression problems $y=F_{\\beta}^{\\circ}(x)+\\xi$ for $F_{\\beta}^{\\circ}\\in\\mathcal{F}^{\\circ}$ in context. To this end, we evaluate the convergence of the mean-squared risk or estimation error, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{R}(\\widehat{\\Theta}):=\\mathbb{E}_{(\\mathbf{X}^{(t)},\\mathbf{y}^{(t)},\\tilde{x}^{(t)},\\tilde{y}^{(t)})_{t=1}^{T}}[R(\\widehat{\\Theta})],\\quad R(\\Theta):=\\mathbb{E}_{\\mathbf{X},\\mathbf{y},\\tilde{x},\\beta}\\left[(F_{\\beta}^{\\circ}(\\tilde{x})-f_{\\Theta}(\\mathbf{X},\\mathbf{y},\\tilde{x}))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that we do not study whether the transformer always converges to\u0398 ; the training dynamics of a DNN is already a very difficult problem. For the attention layer, see th e discussion in Section 1.1. ", "page_idx": 3}, {"type": "text", "text": "3 Risk Bounds for In-Context Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we outline our framework for analyzing the in-context estimation error $\\bar{R}(\\widehat{\\Theta})$ . Some additional definitions are in order. The $\\epsilon$ -covering number $\\mathcal{N}(\\mathcal{C},\\rho,\\epsilon)$ of a metric space $\\mathcal{C}$ equipped with a metric $\\rho$ for $\\epsilon>0$ is defined as the minimal number of balls in $\\rho$ with radius $\\epsilon$ needed to cover $\\mathcal{C}$ (van der Vaart and Wellner, 1996). The $\\epsilon$ -covering entropy or metric entropy is given as $\\mathcal{V}(\\mathcal{F},\\rho,\\epsilon):=\\log\\mathcal{N}(\\mathcal{F},\\rho,\\epsilon)$ . The $\\epsilon$ -packing number $\\mathcal{M}(\\epsilon,\\mathcal{C},\\rho)$ is given as the maximal cardinality of a $\\epsilon$ -separated set $\\{c_{1},\\dots,c_{M}\\}\\subseteq{\\mathcal{C}}$ such that $\\rho(c_{i},c_{j})\\ge\\delta$ for all $i\\neq j$ . The transformer model class is defined as $\\mathcal{T}_{N}:=\\{f_{\\Theta}\\,\\vert\\,\\Theta\\in\\mathcal{S}_{N}\\times\\mathcal{F}_{N}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "To bound the overall risk, we first decompose into the approximation and generalization gaps. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Schmidt-Hieber (2020), Lemma 4, adapted). There exists a universal constant $C$ such that for any $\\epsilon>0$ such that $\\mathcal{V}(\\mathcal{T}_{N},\\Vert\\cdot\\Vert_{L^{\\infty}},\\epsilon)\\ge1,$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\leq2\\operatorname*{inf}_{\\Theta\\in\\mathcal{S}_{N}\\times\\mathcal{F}_{N}}R(\\Theta)+C\\left(\\frac{B^{2}+\\sigma^{2}}{T}\\,\\mathcal{V}(\\mathcal{T}_{N},\\|\\cdot\\|_{L^{\\infty}},\\epsilon)+(B+\\sigma)\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. The convergence rate of the empirical risk minimizer is established for a fixed regression problem $y\\,=\\,f^{\\circ}(\\bar{z})+\\xi$ in Schmidt-Hieber (2020) when $\\xi$ is Gaussian; we modify the proof to incorporate bounded noise in Appendix B.1. The ICL setup can be reduced to the ordinary case as follows. We consider the entire batch $(\\beta,{\\mathbf{X}},\\xi_{1:n},\\tilde{x})$ including the hidden coefficient $\\beta$ as a single datum $z$ with output $\\tilde{y}$ . The true function is given as $f^{\\circ}(z)=\\bar{F}_{\\beta}^{\\circ}(\\tilde{x})$ and the model class is taken to be $\\mathcal{T}_{N}$ implicitly concatenated with the generative process $(\\beta,\\mathbf{X},\\xi_{1:n},\\tilde{x})\\mapsto(\\mathbf{X},\\pmb{y},\\tilde{x})$ . Then $R(\\Theta)$ , $\\mathcal{V}(\\mathcal{T}_{N},||\\cdot||_{L^{\\infty}},\\epsilon)$ and $T$ agree with the ordinary $L^{\\bar{2}}$ risk, model class entropy and sample size. ", "page_idx": 4}, {"type": "text", "text": "Here, the second term is the pretraining generalization error dependent on the number of tasks $T$ ; the in-context generalization error dependent on the prompt length $n$ manifests as part of the first term. This separation allows us to compare the relative difficulty of the two types of learning. ", "page_idx": 4}, {"type": "text", "text": "Bounding approximation error. In order to bound the first term, we analyze the risk of the choice ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\left(\\Gamma^{*},\\phi^{*}\\right):=\\left(\\left(\\Sigma_{\\Psi,N}+\\textstyle\\frac1n\\Sigma_{\\bar{\\beta},N}^{-1}\\right)^{-1},\\phi_{\\underline{{N}}:\\bar{N}}^{*}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi^{*}$ is given as in Assumption 3 for a suitable $\\delta_{N}$ to be determined. The definition of $\\Gamma^{*}$ approximately generalizes the global optimum $\\begin{array}{r}{\\Gamma=\\left((1+\\frac{1}{n})\\Lambda+\\frac{1}{n}\\mathrm{tr}(\\Lambda)\\mathbf{I}_{d}\\right)^{-1}}\\end{array}$ for the Gaussian linear regression setup where $x\\sim\\mathcal{N}(0,\\Lambda)$ (Zhang et al., 2023). Since $\\Sigma_{\\Psi,N}\\,\\succeq\\,C_{1}{\\bf I}_{N}$ we have $\\Gamma^{*}\\preceq C_{1}^{-1}{\\bf I}_{N}$ and hence we may assume $\\Gamma^{*}\\in\\cal S_{N}$ by replacing $C_{3}$ with $C_{3}\\vee C_{1}^{-1}$ if necessary. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2. Under Assumptions 1-3, it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\Theta\\in{\\mathcal S}_{N}\\times{\\mathcal F}_{N}}R(\\Theta)\\le R(\\Theta^{*})\\lesssim\\frac{N^{2r}}{n}\\log N+\\frac{N^{4r}}{n^{2}}\\log^{2}N+\\frac{N}{n}+N^{-2s}+N^{2}\\delta_{N}^{4}+N^{2r+1}\\delta_{N}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof is presented throughout Appendix A. The overall scheme is to approximate $F_{\\beta}^{\\circ}(\\tilde{x})$ by its truncation $F_{\\beta,\\bar{N}}^{\\circ}(\\tilde{x})$ and the finite basis $\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}$ by $\\phi_{\\underline{{N}};\\bar{N}}^{*}$ , which incurs errors $N^{-2s}$ and the terms pertaining to $\\delta_{N}$ , respectively. The first t\u00afhree term\u00afs arise from the concentration of the $n$ token representations $\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{k})$ . All hidden constants are at most polynomial in problem parameters. ", "page_idx": 4}, {"type": "text", "text": "Bounding generalization error. To estimate the metric entropy of $\\mathcal{T}_{N}$ , we first reduce to the metric entropy of the representation class ${\\mathcal F}_{N}$ . Here, $\\|\\cdot\\|_{L^{\\infty}}$ refers to the essential supremum over the support of $\\mathcal{P}_{\\mathcal{X}}$ and also over all $N$ components for ${\\mathcal F}_{N}$ . The proof is given in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3. Under Assumptions 1-3, there exists $D>0$ such that for all $\\epsilon$ sufficiently small, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{V}(\\mathcal{T}_{N},\\|\\cdot\\|_{L^{\\infty}},\\epsilon)\\lesssim N^{2}\\log\\frac{B_{N}^{\\prime2}}{\\epsilon}+\\mathcal{V}\\left(\\mathcal{F}_{N},\\|\\cdot\\|_{L^{\\infty}},\\frac{\\epsilon}{D B_{N}^{\\prime}\\sqrt{N}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 Minimax Optimality of In-Context Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Besov Space and DNNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now apply our theory to study the sample complexity of ICL when ${\\mathcal F}_{N}$ consists of (clipped, see (6)) deep neural networks. These can be also seen as simplified transformers with attention layers and skip connections removed. To be precise, we define the set of DNNs with depth $L$ , width $W$ , sparsity $S$ , norm bound $M$ and ReLU activation $\\eta(x)=x\\vee0$ (applied element-wise) as $\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{DNN}}(L,W,S,M)=\\displaystyle\\bigg\\{(\\mathbf{W}^{(L)}\\eta+b^{(L)})\\circ\\cdot\\cdot\\circ(\\mathbf{W}^{(1)}x+b^{(1)})\\bigg|\\mathbf{W}^{(1)}\\in\\mathbb{R}^{W\\times d},\\mathbf{W}^{(\\ell)}\\in\\mathbb{R}^{W\\times W},}\\\\ &{N^{(L)}\\in\\mathbb{R}^{W},b^{(\\ell)}\\in\\mathbb{R}^{W},b^{(L)}\\in\\mathbb{R},\\displaystyle\\sum_{\\ell=1}^{L}\\lVert\\mathbf{W}^{(\\ell)}\\rVert_{0}+\\lVert b^{(\\ell)}\\rVert_{0}\\leq S,\\displaystyle\\operatorname*{max}_{1\\leq\\ell\\leq L}\\lVert\\mathbf{W}^{(\\ell)}\\rVert_{\\infty}\\vee\\lVert b^{(\\ell)}\\rVert_{\\infty}\\leq M\\bigg\\}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The Besov space is a very general class of functions including the H\u00f6lder and Sobolev spaces which captures spatial inhomogeneity in smoothness, and provides a natural setting in which to study the expressive power of deep neural networks (Suzuki, 2019). Here, we fix $\\mathcal{X}=[0,1]^{d}$ for simplicity. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Besov space). For $2\\,\\leq\\,p\\,\\leq\\,\\infty,0\\,<\\,q\\,\\leq\\,\\infty$ , fractional smoothness $\\alpha\\,>\\,0$ and $r=\\lfloor\\alpha\\rfloor+1$ , the $r$ th modulus of $f\\in L^{p}(\\mathcal{X})$ is defined using the difference operator $\\Delta_{h}^{r},h\\in\\mathbb{R}^{d}$ as $\\begin{array}{r}{w_{r,p}(f,t):=\\operatorname*{sup}_{\\|h\\|_{2}\\leq t}\\|\\Delta_{h}^{r}(f)\\|_{p},\\quad\\Delta_{h}^{r}(f)(x)=1_{\\{x,x+r h\\in\\mathcal{X}\\}}\\sum_{j=0}^{r}{\\binom{r}{j}}(-1)^{r-j}f(x+j h)}\\end{array}$ . Also, the Besov (quasi-)norm is given as $\\|\\cdot\\|_{B_{p,q}^{\\alpha}}=\\|\\cdot\\|_{L^{p}}+|\\cdot|_{B_{p,q}^{\\alpha}}$ where ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f|_{B_{p,q}^{\\alpha}}:=\\left\\{\\left(\\int_{0}^{\\infty}t^{-q\\alpha}w_{r,p}(f,t)^{q}\\frac{\\mathrm{d}t}{t}\\right)^{1/q}\\right.\\ \\ q<\\infty}\\\\ {\\left.\\operatorname*{sup}_{t>0}t^{-\\alpha}w_{r,p}(f,t)\\right.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ q=\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the Besov space is defined as $B_{p,q}^{\\alpha}(\\mathcal{X})=\\{f\\in L^{p}(\\mathcal{X})\\mid\\Vert f\\Vert_{B_{p,q}^{\\alpha}}<\\infty\\}.$ . We write $\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ for the unit ball in $(B_{p,q}^{\\alpha}(\\mathcal{X}),\\|\\cdot\\|_{B_{p,q}^{\\alpha}})$ . ", "page_idx": 5}, {"type": "text", "text": "We have that the H\u00f6lder space $C^{\\alpha}(\\mathcal{X})=B_{\\infty,\\infty}^{\\alpha}(\\mathcal{X})$ for order $\\alpha>0,\\alpha\\notin\\mathbb{N}$ and the Sobolev space $W_{2}^{m}(\\mathcal{X})=B_{2,2}^{m}(\\mathcal{X})$ for $m\\in\\mathbb{N}$ as well as the embeddings $B_{p,1}^{m}(\\mathcal{X})\\hookrightarrow W_{p}^{m}(\\mathcal{X})\\hookrightarrow B_{p,\\infty}^{m}(\\mathcal{X})$ ; if $\\alpha>d/p,\\,B_{p,q}^{\\alpha}(\\chi)_{.}$ compactly embeds into the space of continuous functions on $\\mathcal{X}$ . See Triebel (1983); Gin\u00e9 and Nickl (2015) for more details. The difficulty of learning a regression function in the Besov is quantified by the minimax risk; the following rate is classical. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2 (Donoho and Johnstone (1998)). The minimax risk for an estimator ${\\widehat{f}}_{n}$ with n i.i.d. samples $D_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ over $\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\widehat{f}_{n}:\\mathcal{D}_{n}\\to\\mathbb{R}}\\operatorname*{sup}_{f^{\\circ}\\in\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))}\\mathbb{E}_{\\mathcal{D}_{n}}[\\|f^{\\circ}-\\widehat{f}_{n}\\|_{L^{2}(\\mathcal{X})}^{2}]\\asymp n^{-\\frac{2\\alpha}{2\\alpha+d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A natural basis system for $B_{p,q}^{\\alpha}(\\mathcal{X})$ is formed by the $B$ -splines, which can be seen as a type of wavelet decomposition or multiresolution analysis (DeVore and Popov, 1988). As B-splines are piecewise polynomials, they can be efficiently approximated by DNNs with at most log depth (Suzuki, 2019). ", "page_idx": 5}, {"type": "text", "text": "Definition 4.3 (B-spline wavelet basis). The tensor product B-spline of order $m\\in\\mathbb{N}$ satisfying $m>\\alpha+1-1/p$ , at resolution $k\\in\\mathbb{Z}_{\\geq0}^{d}$ and location $\\begin{array}{r}{\\ell\\in I_{k}^{d}=\\prod_{i=1}^{d}[-m:2^{k_{i}}]}\\end{array}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{k,\\ell}^{d}(x)=\\prod_{i=1}^{d}\\iota_{m}(2^{k_{i}}x_{i}-\\ell_{i}),\\quad\\mathrm{where}\\quad\\iota_{m}(x)=(\\underbrace{\\iota_{*\\,l*}\\cdot\\cdot\\cdot_{*\\,\\ell}}_{m+1})(x),\\quad\\iota(x)=1_{\\{x\\in[0,1]\\}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When k1 = \u00b7 \u00b7 \u00b7 = kd, we abuse notation and write \u03c9kd,\u2113for k \u2208Z\u22650 in place of \u03c9(dk,\u00b7\u00b7\u00b7 ,k),\u2113. ", "page_idx": 5}, {"type": "text", "text": "4.2 Estimation Error Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To apply our framework, we set the task class as the unit ball $\\mathcal{F}^{\\circ}=\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ and take as basis $\\{\\psi_{j}^{\\circ}\\ |\\ j\\in\\mathbb{N}\\}=\\{2^{k d/2}\\omega_{k,\\ell}^{d}\\ |\\ k\\in\\mathbb{Z}_{\\geq0},\\ell\\in I_{k}^{d}\\}$ the set of all B-spline wavelets ordered primarily by increasing $k$ and scaled to counteract the dilation in $x$ . Abusing notation, we also write $\\beta_{k,\\ell}$ to denote the coefficient in $\\beta$ corresponding to $2^{k d/2}\\omega_{k,\\ell}^{d}$ . The set of B-splines at each resolution are independent, while those of lower resolution can always be decomposed into a linear sum of B-splines of higher resolution satisfying certain decay rates, which we prove in Proposition C.10. ", "page_idx": 5}, {"type": "text", "text": "From this setup, in Appendix C.1.1, we verify Assumptions 1 and 2 with $r=1/2,s=\\alpha/d$ under: Assumption 4. $\\mathcal{F}^{\\circ}=\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ , $\\alpha>d/p$ and $\\mathcal{P}_{\\mathcal{X}}$ has positive Lebesgue density $\\rho_{\\mathcal{X}}$ bounded above and below on $\\mathcal{X}$ . Also, all coefficients $\\beta_{k,\\ell}$ are independent and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}]=0,\\quad0<\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}^{2}]\\lesssim2^{-k(2\\alpha+d)}k^{-2},\\quad\\forall k\\ge0,\\,\\,\\ell\\in I_{k}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can check that we have not given ourselves an easier learning problem with (5): the assumed variance decay rate is tight (up to the logarithmic factor $k^{-2}$ ) in the sense that any $f\\in\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ can indeed be expanded into a sum of wavelets with the same coefficient decay when averaged over $\\ell\\,\\in\\,I_{k}^{d}$ . See Lemma C.1 and the following discussion. We also obtain the following in-context approximation and entropy bounds in Appendix C.1.2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4. For any $\\delta_{N}>0$ , Assumption 3 is satisfied by taking ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal F_{N}}=\\{\\Pi_{B_{N}^{\\prime}}\\circ\\phi\\;|\\;\\phi=(\\phi_{j})_{j=1}^{N},\\phi_{j}\\in{\\mathcal F}_{\\mathrm{DNN}}(L,W,S,M)\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Pi_{B_{N}^{\\prime}}$ is the projection in $\\mathbb{R}^{N}$ to the centered ball of radius $B_{N}^{\\prime}={\\cal O}(\\sqrt{N})$ and each $\\phi_{j}$ is $a$ ReLU network such that $L=O(\\log N+\\log\\delta_{N}^{-1})$ and $W,S,M=O(1)$ . Also, the metric entropy of FN is bounded as V(FN, \u2225\u00b7\u2225L\u221e, \u03f5) \u2272N log\u03b4NN\u03f5. ", "page_idx": 6}, {"type": "text", "text": "Hence Assumptions 1-3 all follow from Assumption 4, and we conclude in Appendix C.1.3: Theorem 4.5 (minimax optimality of ICL in Besov space). Under Assumption 4, if $:n\\gtrsim N\\log N_{+}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim N^{-\\frac{2\\alpha}{d}}\\left(\\underset{\\mathrm{error}}{\\overset{\\mathrm{DNN}}{\\mathrm{~nproximation}}}\\right)+\\frac{N\\log N}{n}\\left(\\underset{\\mathrm{error}}{\\overset{\\mathrm{in-context}}{\\mathrm{generalization}}}\\right)+\\frac{N^{2}\\log N}{T}\\left(\\underset{\\mathrm{error}}{\\overset{\\mathrm{pretraining}}{\\mathrm{generalization}}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Hence if $T\\gtrsim n^{\\frac{2\\alpha+2d}{2\\alpha+d}}$ and $N\\,\\asymp\\,n^{\\frac{d}{2\\alpha+d}}$ , in-context learning achieves the minimax optimal rate $\\textstyle n^{-{\\frac{2\\alpha}{2\\alpha+d}}}$ up to a log factor. ", "page_idx": 6}, {"type": "text", "text": "The first term arises from the $N$ -term truncation and oracle approximation error of the DNN module, and is equal to the $N$ -term optimal error (Du\u02dcng, 2011a). The second and third term each correspond to the in-context and pretraining generalization gap. With regard to $N$ , we see that $n\\,=\\,\\bar{\\widetilde{\\Omega}}(N)$ is enough to learn the basis expansion in context, while $T\\,=\\,\\widetilde\\Omega(N^{2})$ is necessary to learn the attention layer. However if $T/N^{\\bar{}{}}=o(n)$ , the third term dominates and the overall complexity scales suboptimally as $\\textstyle T^{-}{\\frac{\\alpha}{\\alpha+d}}$ , illustrating the importance of sufficient pretraining. This also aligns with the task diversity threshold observed by Raventos et al. (2023). Since the amount of training data for LLMs is practically infinite in practice, our result justifies the effectiveness of ICL at large scales with only a small number of in-context samples. ", "page_idx": 6}, {"type": "text", "text": "A limitation of ICL. In the regime $1\\le p<2$ , the approximation error is strictly worse without an adaptive representation scheme and the resulting rate is suboptimal (see Remark C.3). While DNNs can adapt to task smoothness in supervised settings (Suzuki, 2019), ICL and any other metalearning methods are fundamentally constrained to non-adaptive representations since they cannot update at inference time, and hence are bounded below by the best linear approximation rate or Kolmogorov width, which is strictly worse than the minimax optimal rate when $p<2$ . Indeed, for any $N$ -dimensional subspace $\\mathcal{L}_{N}\\subset B_{p,q}^{\\alpha}(\\mathcal{X})$ it holds that (Vyb\u00edral, 2008) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\mathcal{L}_{N}}\\operatorname*{sup}_{f^{\\circ}\\in\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))}\\operatorname*{inf}_{\\ell_{n}\\in\\mathcal{L}_{N}}\\Vert f^{\\circ}-\\ell_{n}\\Vert_{L^{2}(\\mathcal{P}_{\\mathcal{X}})}\\gtrsim N^{-\\alpha/d+(1/p-1/2)+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 4.6. The $N^{2}\\log N$ term in the pretraining generalization gap is due to the covering bound of the attention matrix $\\Gamma$ , while the entropy of the DNN class is only $N\\log N$ . Hence the task diversity requirement may be lessened to the latter by considering low-rank structure or approximation of attention heads (Bhojanapalli et al., 2020; Chen et al., 2021). ", "page_idx": 6}, {"type": "text", "text": "4.3 Avoiding the Curse of Dimensionality ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The above rate inevitably suffers from the curse of dimensionality as $d$ appears in the exponent of the optimal rate. We also consider the anisotropic Besov space (Nikol\u2019skii, 1975), a generalization allowing for different degrees of smoothness $(\\alpha_{1},\\cdot\\cdot\\cdot,\\alpha_{d})$ in each coordinate. Then the optimal rate is nearly dimension-free in the sense that the rate only depends on $d$ through the quantity $\\widetilde{\\alpha}:=(\\textstyle\\sum_{i}\\alpha_{i}^{\\bar{-}1})^{-1}$ , and becomes independent of dimension if only a few directions are important i.e. h ave small $\\alpha_{i}$ . Rigorous definitions, statements and proofs are provided in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Extending Theorem 4.5, we show that ICL again attains near-optimal estimation error in the anisotropic Besov space, circumventing the curse of dimensionality and theoretically establishing the efficiacy of in-context learning in high-dimensional settings. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.7 (informal version of Theorem C.7). For the anisotropic Besov space of smoothness $\\left(\\alpha_{1},\\cdot\\cdot\\cdot,\\alpha_{d}\\right)$ , assume variance decay (4) with $s=\\widetilde{\\alpha}>1/p$ . If $T\\gtrsim n N$ and N \u224dn2 \u03b1 +1 , in-context learning achieves the minimax optimal rate $n^{-\\frac{2\\tilde{\\alpha}}{2\\tilde{\\alpha}+1}}$ up to a log factor. ", "page_idx": 6}, {"type": "text", "text": "4.4 Learning a Coarser Basis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Thus far, we have demonstrated the importance of sufficient pretraining to achieve optimal risk; as another application of our framework, we illustrate how pretraining can actively mitigate the complexity of in-context learning. Consider the case where $(\\psi_{j}^{\\circ})_{j=1}^{\\infty}$ is no longer the B-spline basis of $B_{p,q}^{\\alpha}(\\mathcal{X})$ but instead is chosen from some wider function space, say the unit ball of $B_{p,q}^{\\tau}(\\mathcal{X})$ for a smaller smoothness $\\tau<\\alpha$ . Without knowledge of the basis, the sample complexity of learning any regression function $F_{\\beta}^{\\circ}$ is a priori lower bounded by the minimax rate $n^{-\\frac{\\bar{2+}}{2\\tau+d}}$ by Proposition 4.2. For ICL, this difficulty manifests as an increase in the metric entropy of the class ${\\mathcal{F}}_{N}$ which must be powerful enough to approximate $\\psi_{1:N}^{\\circ}$ (Corollary C.12), giving rise to the modified risk bound: ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.8 (ICL for coarser basis). Suppose $\\alpha>\\tau>d/p$ , the basis $(\\psi_{j}^{\\circ})_{j=1}^{\\infty}\\subset\\mathbb{U}(B_{p,q}^{\\tau}(\\mathcal{X}))$ and Assumptions 1, 2 hold with $r=1/2,s=\\alpha/d$ . Then if $n\\gtrsim N\\log N_{.}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim N^{-\\frac{2\\alpha}{d}}+\\frac{N\\log N}{n}+\\frac{N^{1+\\frac{\\alpha}{\\tau}+\\frac{d}{\\tau}}\\log^{3}N}{T}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Hence if $T\\gtrsim n^{1+\\frac{d}{2\\alpha+d}\\frac{\\alpha+d}{\\tau}}$ and $N\\asymp n^{\\frac{d}{2\\alpha+d}}$ , the risk converges as $n^{-{\\frac{2\\alpha}{2\\alpha+d}}}\\log n$ ", "page_idx": 7}, {"type": "text", "text": "The pretraining generalization gap is now dominated by the higher complexity $\\begin{array}{r}{N^{1+\\frac{\\alpha}{\\tau}+\\frac{d}{\\tau}}\\log^{3}N}\\end{array}$ of the DNN class and strictly worse compared to $N^{2}\\log\\dot{N}$ for Theorem 4.5. The required number of tasks also suffers and the exponent is no longer $\\textstyle{\\frac{2\\alpha+2d}{2\\alpha+d}}\\in(1,2)$ but scales as $O(d)$ . Nevertheless, observe that the burden of complexity is entirely carried by $T$ ; with sufficient pretraining, the third term can be made arbitrarily small and the ICL risk again attains $n^{-{\\frac{2\\alpha}{2\\alpha+d}}}$ . Hence ICL improves upon the a priori lower bound $n^{-\\frac{2\\tau}{2\\tau+d}}$ at inference time by encoding information on the coarser basis during pretraining. We remark that the result is also readily adapted to the anisotropic setting. ", "page_idx": 7}, {"type": "text", "text": "4.5 Sequential Input and Transformers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider a more complex setting where the inputs $x\\in[0,1]^{d\\times\\infty}$ are bidirectional sequences of tokens (e.g. entire documents) and $\\phi$ is itself a transformer network.2 In this infinite-dimensional setting, transformers can still circumvent the curse of dimensionality and in fact achieve near-optimal sample complexity due to their parameter sharing and feature extraction capabilities (Takakura and Suzuki, 2023). Our goal in this section is to extend this guarantee to ICL of trained transformers. ", "page_idx": 7}, {"type": "text", "text": "For sequential data, it is natural to suppose the smoothness w.r.t. each coordinate can vary depending on the input. For example, the position of important tokens in a sentence will change if irrelevant strings are inserted. To this end, we adopt the piecewise $\\gamma$ -smooth function class introduced by Takakura and Suzuki (2023), which allows for arbitrary bounded permutations among input tokens; see Appendix D.1 for definitions. Also borrowing from their setup, we consider multi-head sliding window self-attention layers with window size $U$ , embedding dimension $D$ , number of heads $H$ with key, query, value matrices $K^{(h)},Q^{(h)}\\in\\mathbb{R}^{D\\times d}$ , $V^{(h)}\\in\\mathbb{R}^{d\\times d}$ and norm bound $M$ defined as3 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{Aun}}(U,D,H,M)=\\displaystyle\\bigg\\{g:\\mathbb{R}^{d\\times\\infty}\\rightarrow\\mathbb{R}^{d\\times\\infty}\\ \\bigg|\\ \\operatorname*{max}_{1\\leq h\\leq H}\\|K^{(h)}\\|_{\\infty}\\vee\\|Q^{(h)}\\|_{\\infty}\\vee\\|V^{(h)}\\|_{\\infty}\\leq M,}\\\\ &{\\qquad\\qquad\\qquad\\qquad g(x)_{i}=x_{i}+\\displaystyle\\sum_{h=1}^{H}V^{(h)}x_{i-U:i+U}\\mathrm{Softmax}\\left((K^{(h)}x_{i-U:i+U})^{\\top}Q^{(h)}x_{i}\\right)\\bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We also consider a linear embedding layer $\\operatorname{Enc}(x)=E x+P$ , $E\\in\\mathbb{R}^{D\\times d}$ with absolute positional encoding $P\\in\\mathbb{R}^{D}$ of bounded norm. Then the class of depth $J$ transformers is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{\\mathrm{TF}}(J,U,D,H,L,W,S,M):=\\big\\{f_{J}\\circ g_{J}\\circ\\cdots\\circ f_{1}\\circ g_{1}\\circ\\mathrm{Enc~}|\\ ||E||\\leq M,\\big.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad f_{i}\\in\\mathcal{F}_{\\mathrm{DNN}}(L,W,S,M),\\ g_{i}\\in\\mathcal{F}_{\\mathrm{Atm}}(U,D,H,M)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our result, proved in Appendix D.2, reads: ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.9 (informal version of Theorem D.1). Suppose ${\\mathcal{F}}^{\\circ}$ consists of functions on $[0,1]^{d\\times\\infty}$ of bounded piecewise $\\gamma$ -smooth and $L^{\\infty}$ -norm with smoothness $\\alpha\\in\\mathbb{R}_{>0}^{d\\times\\infty}$ , and let $\\gamma$ be mixed or anisotropic smoothness with $\\alpha^{\\dagger}=\\operatorname*{max}_{i,j}\\alpha_{i j}$ or $(\\sum_{i,j}\\alpha_{i j}^{-1})^{-1}$ , respectively. Under suitable regularity and decay assumptions, by taking ${\\mathcal{F}}_{N}$ to be a class of clipped transformers it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim N^{-2\\alpha^{\\dagger}}+\\frac{N\\log N}{n}+\\frac{N^{2\\vee(1+1/\\alpha^{\\dagger})}\\operatorname{polylog}(N)}{T}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence if $T\\gtrsim n N^{1\\vee1/\\alpha^{\\dagger}}$ and $N\\asymp n^{\\frac{1}{2\\alpha^{\\dagger}+1}}$ , ICL achieves the rate $n^{-\\frac{2\\alpha^{\\dagger}}{2\\alpha^{\\dagger}+1}}\\,\\mathrm{polylog}(n).$ . ", "page_idx": 8}, {"type": "text", "text": "This matches the optimal rate in finite dimensions independently of the (possibly infinite) length of the input or context window. The dynamical feature extraction ability of attention layers in the $\\mathcal{F}_{\\mathrm{TF}}$ class is essential in dealing with input-dependent smoothness, further justifying the efficiacy of ICL of sequential data. ", "page_idx": 8}, {"type": "text", "text": "5 Minimax Lower Bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide lower bounds for the minimax rate in both $n,T$ by extending the theory of Yang and Barron (1999), which can be leveraged to yield results stronger than optimality in merely $n$ . The bound is purely information-theoretic and hence applies to not just ICL but any meta-learning scheme for the regression problem of Section 2.1 from the data $\\bar{\\mathcal{D}_{n,T}}=\\{(\\mathbf{X}^{(t)},\\pmb{y}^{(t)})\\}_{t=1}^{T+1}$ , where the index $T+1$ corresponds to the test task. ", "page_idx": 8}, {"type": "text", "text": "For this section we assume that the noise (1) is i.i.d. Gaussian, $\\xi_{k}\\sim\\mathcal{N}(0,\\sigma^{2})$ , instead of bounded; while the exact shape of the noise distribution is not important, having restricted support may convey additional information and affect the minimax rate. We also suppose for simplicity that the support of $\\mathcal{P}_{\\beta}$ is included in $\\mathcal{B}:=\\{\\beta\\in\\mathbb{R}^{\\infty}\\mid\\vert\\beta_{j}\\vert^{2}\\lesssim j^{-2s-1}(\\log j)^{-2}$ , $j\\in\\mathbb N\\}$ and that the aggregated coefficients $\\bar{\\beta}_{j}$ for $B\\le j\\le\\bar{N}$ satisfy $\\mathbb{E}_{\\beta}[\\bar{\\beta}_{j}^{2}]\\le\\sigma_{\\beta}^{2}$ for some $\\sigma_{\\beta}$ dependent on $N$ . The proof of the following statement is given in Appendix F.1. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.1. For $\\varepsilon_{n,1},\\varepsilon_{n,2},\\delta_{n}\\,>\\,0,$ , let $Q_{1}$ and $Q_{2}$ be the $\\varepsilon_{n,1}.$ - and $\\varepsilon_{n,2}$ -covering numbers of ${\\mathcal{F}}_{N}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ respectively, and $M$ be the $\\delta_{n}$ -packing number of ${\\mathcal{F}}^{\\circ}$ . Suppose that the following conditions are satisfied: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2\\sigma^{2}}\\left(n(T+1)\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}+C_{2}n\\varepsilon_{n,2}^{2}\\right)\\leq\\log Q_{1}+\\log Q_{2}\\leq\\frac{1}{8}\\log M,\\quad4\\log2\\leq\\log M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then the minimax rate is lower bounded as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\widehat f:\\mathcal D_{n,T}\\to\\mathbb R}\\operatorname*{sup}_{f^{\\circ}\\in\\mathcal F^{\\circ}}\\mathbb{E}_{\\mathcal D_{n,T}}[\\|\\widehat f-f^{\\circ}\\|_{L^{2}(\\mathcal P_{\\mathcal X})}^{2}]\\geq\\frac{1}{4}\\delta_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finally, Proposition 5.1 is applied to obtain concrete lower bounds for the settings studied in Section 4 throughout Appendices F.2-F.4. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.2 (minimax lower bound). The minimax rates in the previous regression settings are lower bounded as follows. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(i)}\\ B e s o\\nu\\ s p a c e\\ (S e c t i o n\\ 4.2);\\ \\operatorname*{inf}_{\\hat{f}}\\operatorname*{sup}_{f^{\\circ}}\\mathbb{E}_{\\mathcal{D}_{n,T}}[\\|\\widehat{f}-f^{\\circ}\\|^{2}]\\gtrsim n^{-\\frac{2\\alpha}{2\\alpha+d}},}\\\\ &{\\mathrm{(ii)}\\ C o a r s e r\\ b a s i s\\ (S e c t i o n\\ 4.4);\\ \\operatorname*{inf}_{\\hat{f}}\\operatorname*{sup}_{f^{\\circ}}\\mathbb{E}_{\\mathcal{D}_{n,T}}[\\|\\widehat{f}-f^{\\circ}\\|^{2}]\\gtrsim n^{-\\frac{2\\alpha}{2\\alpha+d}}+\\left(n T\\right)^{-\\frac{2\\tau}{2\\tau+d}},}\\\\ &{\\mathrm{(iii)}\\ S e q u e n t i a l\\ i n p u t\\ (S e c t i o n\\ 4.5);\\ \\operatorname*{inf}_{\\hat{f}}\\operatorname*{sup}_{f^{\\circ}}\\mathbb{E}_{\\mathcal{D}_{n,T}}[\\|\\widehat{f}-f^{\\circ}\\|^{2}]\\gtrsim n^{-\\frac{2\\alpha^{\\dagger}}{2\\alpha^{\\dagger}+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "These results match the upper bounds for (i), (iii) and show that ICL is provably jointly optimal in $n,T$ in the \u2018large $T'$ regime. Moreover, we can check for the coarser basis setting that insufficient pretraining $T=O(1)$ indeed leads to the worse complexity n\u22122\u03c4+d , while the faster rate n\u22122\u03b1+d is retrieved when $T\\stackrel{\\cdot}{\\sim}n^{\\frac{(\\alpha-\\tau)\\,d}{(2\\alpha+d)\\tau}}$ . This aligns with the discussion in Section 4.4, showing that ICL is provably suboptimal in the \u2018small $T'$ regime. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Remark 5.3. The obtained upper and lower bounds in the coarser basis setting are not tight as $T$ varies, hence it remains to be shown whether there exists a meta-learning algorithm that attains the lower bound (ii). The task diversity threshold for optimal learning suggested by the bounds are also different (n1+ $(n^{1+\\frac{d(\\alpha+d)}{(2\\alpha+d)}}$ (2\u03b1+d) v.s. n(2\u03b1+d)\u03c4 ); it would be interesting for future work to resolve this gap. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we performed a learning-theoretic analysis of ICL of a transformer consisting of a DNN and a linear attention layer pretrained on nonparametric regression tasks. We developed a general framework for bounding the in-context estimation error of the empirical risk minimizer in terms of both the number of tasks and samples, and proved that ICL can achieve nearly minimax optimal rates in the Besov space, anisotropic Besov space and $\\gamma.$ -smooth class. We also demonstrated that ICL can improve upon the a priori optimal rate by learning informative representations during pretraining. We supplemented our analyses with corresponding minimax lower bounds jointly in $n,T$ and also performed numerical experiments validating our findings. Our work opens up interesting approaches of adapting classical learning theory to study emergent phenomena of foundation models. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our transformer model is limited to a single layer of linear self-attention and does not consider more complex in-context learning behavior which may arise in transformers with multiple attention layers. Moreover, the obtained upper and lower bounds are not tight in certain regimes, suggesting future research directions for meta-learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JK was partially supported by JST CREST (JPMJCR2015). TS was partially supported by JSPS KAKENHI (24K02905, 20H00576) and JST CREST (JPMJCR2115). We would like to express our gratitude to Masaaki Imaizumi for valuable and insightful discussions on the topic in relation to his work in progress (Imaizumi, 2024). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.   \nE. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is incontext learning? Investigations with linear models. In International Conference on Learning Representations, 2023.   \nY. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: provable incontext learning with in-context algorithm selection. In ICML Workshop on Efficient Systems for Foundation Models, 2023.   \nS. Bhojanapalli, C. Yun, A. S. Rawat, S. J. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention models. In International Conference on Machine Learning, 2020.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.   \nB. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. R\u00e9. Scatterbrain: unifying sparse and low-rank attention approximation. In Advances in Neural Information Processing Systems, 2021.   \nS. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for in-context learning: emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.   \nR. A. DeVore and V. A. Popov. Interpolation of Besov spaces. Transactions of the American Mathematical Society, 305(1):397\u2013414, 1988.   \nD. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of Statistics, 26(3):879\u2013921, 1998.   \nS. Du, W. Hu, S. Kakade, J. Lee, and Q. Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations, 2021.   \nD. D\u02dcung. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34:1\u201341, 2011a.   \nD. Du\u02dcng. B-spline quasi-interpolant representations and sampling recovery of functions with mixed smoothness. Journal of Complexity, 27(6):541\u2013567, 2011b.   \nX. Garc\u00eda, Y. Bansal, C. Cherry, G. F. Foster, M. Krikun, F. Feng, M. Johnson, and O. Firat. The unreasonable effectiveness of few-shot learning for machine translation. In International Conference on Machine Learning, 2023.   \nS. Garg, D. Tsipras, P. Liang, and G. Valiant. What can Transformers learn in-context? A case study of simple function classes. In Advances in Neural Information Processing Systems, 2022.   \nE. Gin\u00e9 and R. Nickl. Mathematical foundations of infinite-dimensional statistical models. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.   \nT. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y. Bai. How do Transformers learn in-context beyond simple functions? A case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.   \nS. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces. Neural Networks, 123:343\u2013361, 2020.   \nY. Huang, Y. Cheng, and Y. Liang. In-context convergence of Transformers. arXiv preprint arXiv:2310.05249, 2023.   \nM. Imaizumi. Statistical analysis on in-context learning, 2024. Personal communication.   \nJ. Kim and T. Suzuki. Transformers learn nonlinear features in context: nonconvex mean-field dynamics on the attention landscape. In International Conference on Machine Learning, 2024.   \nH. Li, M. Wang, S. Lu, X. Cui, and P.-Y. Chen. Training nonlinear Transformers for efficient in-context learning: a theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607, 2024.   \nA. Mahankali, T. B. Hashimoto, and T. Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \nD. Meunier, Z. Li, A. Gretton, and S. Kpotufe. Nonlinear meta-learning can guarantee faster rates. arXiv preprint arXiv:2307.10870, 2023.   \nS. M. Nikol\u2019skii. Approximation of functions of several variables and imbedding theorems, volume 205 of Grundlehren der mathematischen Wissenschaften. Springer Berlin, 1975.   \nY. Nishimura and T. Suzuki. Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= EW8ZExRZkJ.   \nS. Okumoto and T. Suzuki. Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness. In International Conference on Learning Representations, 2022.   \nA. Raventos, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. In Advances in Neural Information Processing Systems, 2023.   \nJ. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. The Annals of Statistics, 48(4), 2020.   \nT. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality. In International Conference on Learning Representations, 2019.   \nT. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space. In Advances in Neural Information Processing Systems, 2021.   \nS. J. Szarek. Nets of Grassmann manifold and orthogonal group. Proceedings of Banach Space Workshop, pages 169\u2013186, 1981.   \nS. Takakura and T. Suzuki. Approximation and estimation ability of Transformers for sequenceto-sequence functions with infinite dimensional input. In International Conference on Machine Learning, 2023.   \nH. Triebel. Theory of function spaces. Monographs in mathematics. Birkh\u00e4user Verlag, 1983.   \nH. Triebel. Entropy numbers in function spaces with mixed integrability. Revista Matematica Complutense, 24:169\u2013188, 2011.   \nN. Tripuraneni, C. Jin, and M. Jordan. Provable meta-learning of linear representations. In International Conference on Machine Learning, 2020.   \nJ. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1\u20132):1\u2013230, May 2015. ISSN 1935-8237.   \nA. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes: with applications to statistics. Springer, New York, 1996.   \nJ. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2023.   \nJ. Vyb\u00edral. Function spaces with dominating mixed smoothness, volume 30 of Lectures in Mathematics. European Mathematical Society, 2006.   \nJ. Vyb\u00edral. Widths of embeddings in function spaces. Journal of Complexity, 24(4):545\u2013570, 2008.   \nJ. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. L. Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In International Conference on Learning Representations, 2024.   \nY. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564\u20131599, 1999.   \nD. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94: 103\u2013114, 2016.   \nR. Zhang, S. Frei, and P. L. Bartlett. Trained Transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \nR. Zhang, J. Wu, and P. L. Bartlett. In-context learning of a linear Transformer block: beneftis of the MLP component and one-step GD initialization. arXiv preprint arXiv:2402.14951, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction 1.1 Other Related Works 2 ", "page_idx": 12}, {"type": "text", "text": "2 Problem Setup 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "2.1 Nonparametric Regression 3   \n2.2 In-Context Learning . . 4 ", "page_idx": 12}, {"type": "text", "text": "3 Risk Bounds for In-Context Learning 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "4 Minimax Optimality of In-Context Learning 5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "4.1 Besov Space and DNNs . 5   \n4.2 Estimation Error Analysis . . 6   \n4.3 Avoiding the Curse of Dimensionality . . . 7   \n4.4 Learning a Coarser Basis . . . 8   \n4.5 Sequential Input and Transformers . 8 ", "page_idx": 12}, {"type": "text", "text": "5 Minimax Lower Bounds 9 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "6 Conclusion 10 ", "page_idx": 12}, {"type": "text", "text": "A Proof of Proposition 3.2 15 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Decomposing Approximation Error 15   \nA.2 Bounding Term (8) 16   \nA.3 Bounding Term (9) . . . 17   \nA.4 Bounding Terms (10)-(12) 19   \nA.5 Proof of Lemma A.1 20 ", "page_idx": 12}, {"type": "text", "text": "B Proofs on Metric Entropy 22 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Modified Proof of Theorem 3.1 22   \nB.2 Proof of Lemma 3.3 . 22   \nB.3 Proof of Lemma B.1 23 ", "page_idx": 12}, {"type": "text", "text": "C Details on Besov-type Spaces 24 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Besov Space . . . 24   \nC.1.1 Verification of Assumptions 24   \nC.1.2 Proof of Lemma 4.4 27   \nC.1.3 Proof of Theorem 4.5 . 28   \nC.2 Anisotropic Besov Space . . . 28   \nC.2.1 Definitions and Results . . 28   \nC.2.2 Proof of Theorem C.7 29   \nC.3 Wavelet Refinement . . 31   \nC.4 Proof of Corollary 4.8 . . 32 ", "page_idx": 12}, {"type": "text", "text": "D Details on Sequential Input 33 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 Definitions and Results 33 ", "page_idx": 12}, {"type": "text", "text": "D.2 Proof of Theorem D.1 34 ", "page_idx": 13}, {"type": "text", "text": "E Numerical Experiments 36 ", "page_idx": 13}, {"type": "text", "text": "F Proofs of Minimax Lower Bounds 37 ", "page_idx": 13}, {"type": "text", "text": "F.1 Proof of Proposition 5.1 . . 37   \nF.2 Lower Bound in Besov Space . . 39   \nF.3 Lower Bound with Coarser Basis . . 39   \nF.4 Lower Bound in Piecewise $\\gamma$ -smooth Class 39 ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Decomposing Approximation Error ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Theta^{*}=(\\Gamma^{*},\\phi^{*})=\\left(\\left(\\Sigma_{\\Psi,N}+\\frac1n\\Sigma_{\\bar{\\beta},N}^{-1}\\right)^{-1},\\phi_{\\underline{{N}}:\\bar{N}}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We introduce some additional notation in the following fashion. For brevity, we write $\\bar{N}:\\infty$ instead of $(\\bar{N}+1):\\infty$ as an exception. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi^{*}=(\\phi_{\\mathcal{N}:\\bar{N}}^{*}(x_{1}),\\cdots,\\phi_{\\mathcal{N}:\\bar{N}}^{*}(x_{n}))\\in\\mathbb{R}^{N\\times n},\\quad\\xi=(\\xi_{1},\\ldots,\\xi_{n})^{\\top}\\in\\mathbb{R}^{n},}\\\\ &{\\Psi^{\\circ}=(\\psi^{\\circ}(x_{1}),\\ldots,\\psi^{\\circ}(x_{n}))\\in\\mathbb{R}^{\\infty\\times n},\\quad\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}=(\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{1}),\\cdots,\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{n}))\\in\\mathbb{R}^{N\\times n},}\\\\ &{\\Psi_{\\mathcal{N}:\\infty}^{\\circ}=(\\psi_{N:\\infty}^{\\circ}(x_{1}),\\cdots,\\psi_{N:\\infty}^{\\circ}(x_{n}))\\in\\mathbb{R}^{\\infty\\times n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since clipping $\\check{f}_{\\Theta}$ does not make its difference with $F_{\\beta}^{\\circ}\\in[-B,B]$ larger, it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\Theta^{*})=\\mathbb{E}\\left[\\big(F_{\\beta}^{\\circ}(\\tilde{x})-f_{\\Theta^{*}}(\\mathbf{X},y,\\tilde{x})\\big)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\big(F_{\\beta}^{\\circ}(\\tilde{x})-\\tilde{f}_{\\Theta^{*}}(\\mathbf{X},y,\\tilde{x})\\big)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq2\\mathbb{E}\\left[\\Big(F_{\\beta}^{\\circ}(\\tilde{x})-F_{\\beta,\\bar{N}}^{\\circ}(\\tilde{x})\\Big)^{2}\\right]+2\\mathbb{E}\\left[\\Big(F_{\\beta,\\bar{N}}^{\\circ}(\\tilde{x})-\\tilde{f}_{\\Theta^{*}}(\\mathbf{X},y,\\tilde{x})\\Big)^{2}\\right]}\\\\ &{\\qquad\\qquad\\lesssim N^{-2s}+\\mathbb{E}\\left[\\bigg(F_{\\beta,\\bar{N}}^{\\circ}(\\tilde{x})-\\phi^{*}(\\tilde{x})^{\\top}\\frac{\\Gamma^{*}\\Phi^{*}y}{n}\\bigg)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "due to Assumption 2 and ${\\bar{N}}\\asymp N$ . Expanding the attention output as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi^{*}(\\tilde{x})^{\\top}\\frac{\\Gamma^{*}\\Phi^{*}y}{n}=(\\phi^{*}(\\tilde{x})-\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x})+\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x}))^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Psi_{{X};\\tilde{X}}^{o}+\\Psi_{{B};\\tilde{X}}^{o})y}{n}}&{}\\\\ {=(\\phi^{*}(\\tilde{x})-\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x}))^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Psi_{{X};\\tilde{X}}^{o})y}{n}}&{}\\\\ {+\\left(\\phi^{*}(\\tilde{x})-\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x})\\right)^{\\top}\\Gamma^{*}\\frac{\\Psi_{{X};\\tilde{X}}^{o}y}{n}}&{}\\\\ {+\\left.\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Psi_{{X};\\tilde{X}}^{o})y}{n}\\right.}&{}\\\\ {+\\left.\\psi_{{X};\\tilde{X}}^{o}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{{X};\\tilde{X}}^{o}y}{n},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the final term further as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}y}{n}=\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\Psi^{\\circ\\top}\\beta+\\xi)}{n}}\\\\ &{=\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ\\top}\\bar{\\beta}_{\\underline{{N}}:\\bar{N}}}{n}+\\psi_{1:N}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\Psi_{\\bar{N}:\\infty}^{\\circ\\top}\\beta_{\\bar{N}:\\infty}+\\xi)}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(F_{\\beta,\\bar{N}}^{o}(\\tilde{x})-\\phi^{*}(\\tilde{x})^{\\top}\\frac{\\Gamma^{*}\\Phi^{*}y}{n}\\right)^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}\\left[\\left(F_{\\beta,\\bar{N}}^{o}(\\tilde{x})-\\psi_{\\underline{{N}}:\\bar{N}}^{o}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{o}\\Psi_{\\underline{{N}}:\\bar{N}}^{o\\top}\\bar{\\beta}_{\\underline{{N}}:\\bar{N}}}{n}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,\\mathbb{E}\\left[\\left(\\psi_{N;N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{*}\\frac{\\psi_{N;N}^{0}(\\Psi_{N;N}^{\\top}\\beta_{N;\\infty}+\\xi)}{n}\\right)^{2}\\right]}\\\\ &{\\quad+\\,\\mathbb{E}\\left[\\left((\\phi^{*}(\\bar{x})-\\psi_{N;N}^{0}(\\bar{x}))^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Psi_{N;N}^{0})y}{n}\\right)^{2}\\right]}\\\\ &{\\quad+\\,\\mathbb{E}\\left[\\left((\\phi^{*}(\\bar{x})-\\psi_{N;N}^{0}(\\bar{x}))^{\\top}\\Gamma^{*}\\frac{\\Psi_{N;N}^{0}\\bar{y}}{n}\\right)^{2}\\right]}\\\\ &{\\quad+\\,\\mathbb{E}\\left[\\left(\\psi_{N;N}^{0}(\\bar{x})^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Psi_{N;N}^{0})y}{n}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We proceed to control each term separately, from which the statement of Proposition 3.2 will follow. ", "page_idx": 15}, {"type": "text", "text": "A.2 Bounding Term (8) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since $F_{\\beta,\\bar{N}}^{\\circ}(\\tilde{x})=\\psi_{1:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\beta_{1:\\bar{N}}=\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\bar{\\beta}_{\\underline{{N}}:\\bar{N}}$ , we can introduce a $(\\Gamma^{*})^{-1}$ factor to bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(F_{\\beta,N}^{\\circ}(\\bar{x})-\\psi_{\\beta,N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{\\bullet}\\frac{\\Psi_{N,N}^{\\circ}\\Psi_{N,N}^{\\circ}\\tilde{D}_{N,N}^{\\circ}}{n}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(F_{\\beta,N}^{\\circ}(\\bar{x})-\\psi_{\\beta,N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{\\bullet}\\left(\\frac{\\Psi_{N,N}^{\\circ}\\Psi_{N,N}^{\\circ\\top}}{n}-(\\Gamma^{\\bullet})^{-1}+(\\Gamma^{\\bullet})^{-1}\\right)\\bar{\\mathcal{J}}_{\\mathcal{B};N}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(\\psi_{\\beta,N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{\\bullet}\\left(\\frac{\\Psi_{N,N}^{\\circ}\\Psi_{N,N}^{\\circ\\top}}{n}-\\Sigma_{\\theta,N}-\\frac{1}{n}\\Sigma_{\\beta,N}^{-1}\\right)\\bar{\\mathcal{J}}_{\\mathcal{B};N}\\right)^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left(\\psi_{\\beta,N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{\\bullet}\\left(\\frac{\\Psi_{N,N}^{\\circ}\\Psi_{N,N}^{\\circ\\top}}{n}-\\Sigma_{\\theta,N}\\right)\\bar{\\mathcal{J}}_{\\mathcal{B};N}\\right)^{2}\\right]}\\\\ &{\\qquad+2\\mathbb{E}\\left[\\left(\\psi_{\\beta,N}^{\\circ}(\\bar{x})^{\\top}\\Gamma^{\\bullet}\\frac{\\mathbf{L}_{\\beta,N}^{\\circ}\\bar{\\mathcal{J}}_{\\mathcal{B};N}}{n}\\bar{\\mathcal{J}}_{\\mathcal{B};N}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denote the operator norm (with respect to $L^{2}$ norm of vectors) by $\\lVert\\cdot\\rVert_{\\mathrm{op}}$ . For (13), noting that $\\Sigma_{\\Psi,N}$ is positive definite, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\psi_{\\mathcal{R},\\mathcal{R}}^{\\circ}(\\lambda)^{\\top}\\Gamma\\left(\\frac{\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\circ}(\\lambda)\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}{\\mathcal{R}}-\\Sigma_{\\mathcal{R},\\mathcal{R}}\\right)\\bar{\\beta}_{\\mathcal{R},\\mathcal{R}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(\\psi_{\\mathcal{R},\\mathcal{R}}^{\\circ}(\\lambda)^{\\top}\\Gamma^{-1}\\frac{\\Gamma^{2}}{\\mathcal{R}\\beta_{\\mathcal{R},\\mathcal{R}}^{\\star}}\\left(\\sum_{v_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}^{-1/q}\\frac{\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}{\\mathcal{R}}\\sum_{v_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}^{-1/q}-\\mathbf{I}_{N}\\right)\\Sigma_{\\mathcal{R},\\mathcal{R}}^{1/q}\\bar{\\beta}_{\\mathcal{R},\\mathcal{R}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{X,\\mathcal{R}}\\left[\\bar{\\beta}_{\\mathcal{R},\\mathcal{R}}^{\\top}\\Sigma_{\\mathcal{R},\\mathcal{R}}^{\\top}\\left(\\sum_{v_{\\mathcal{R},\\mathcal{R}}^{\\in}}^{-1/q}\\frac{\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}{\\mathcal{R}}-\\Sigma_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}\\bar{\\beta}_{\\mathcal{R},\\mathcal{R}}\\right)\\Sigma_{\\mathcal{R},\\mathcal{R}}^{1/q}\\Gamma\\bar{\\Phi}_{\\mathcal{R},\\mathcal{R}}(\\lambda)\\psi_{\\mathcal{R},\\mathcal{R}}^{\\circ}(\\lambda)\\right]^{\\top}}\\\\ &{\\qquad\\times\\Gamma^{-1}\\frac{\\Gamma^{2}}{\\mathcal{R}\\beta_{\\mathcal{R},\\mathcal{R}}^{\\top}}\\left(\\sum_{v_{\\mathcal{R},\\mathcal{R}}^{\\in}}^{-1/q}\\frac{\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}\\Psi_{\\mathcal{R},\\mathcal{R}}^{\\nabla\\times}}{\\mathcal{R}}-\\Sigma_{\\mathcal{R},\\mathcal{R}}^{\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim\\mathbb{E}_{\\mathbf{X}}\\left[\\mathrm{Tr}\\left[\\left(\\Sigma_{\\Psi,N}^{-1/2}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ\\top}}{n}\\Sigma_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}\\right)^{2}\\Sigma_{\\Psi,N}^{1/2}\\Sigma_{\\bar{\\beta},N}^{1/2}\\Sigma_{\\Psi,N}^{1/2}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "due to the independence of $\\mathbf{X},{\\tilde{x}}$ and $\\beta$ . For the last inequality, we have used the fact that both the $\\Sigma_{\\Psi,N}^{1/2}\\Gamma^{*}\\Sigma_{\\Psi,N}\\bar{\\Gamma^{*}}\\Sigma_{\\Psi,N}^{1/2}$ term and the matrix multiplied to it are positive semi-definite, and the former is bounded above as $\\precsim\\mathbf{I}_{N}$ by Assumption 1. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, we utilize the following result proved in Appendix A.5: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Lemma}\\;\\mathbf{A.1.}\\;\\mathbb{E}_{\\mathbf{X}}\\left[\\left\\|\\Sigma_{\\Psi,N}^{-1/2}\\frac{\\Psi_{N:N}^{\\circ}\\Psi_{N:N}^{\\circ\\top}}{n}\\Sigma_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}\\right\\|_{\\mathrm{op}}^{2}\\right]\\lesssim\\frac{N^{2r}}{n}\\log N+\\frac{N^{4r}}{n^{2}}\\log^{2}N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows that (13) is bounded as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\psi_{N:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\left(\\frac{\\Psi_{N:\\bar{N}}^{0}\\Psi_{N:\\bar{N}}^{0\\top}}{n}-\\Sigma_{\\Psi,N}\\right)\\bar{\\beta}_{\\bar{N}:\\bar{N}}\\right)^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}_{\\mathbf{X}}\\left[\\left\\|\\Sigma_{\\Psi,N}^{-1/2}\\frac{\\Psi_{N:\\bar{N}}^{0}\\Psi_{N:\\bar{N}}^{0\\top}}{n}\\Sigma_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}\\right\\|_{\\mathrm{op}}^{2}\\right]\\mathrm{Tr}\\left[\\Sigma_{\\Psi,N}^{1/2}\\Sigma_{\\bar{\\beta},N}\\Sigma_{\\Psi,N}^{1/2}\\right]}\\\\ &{\\lesssim\\left(\\frac{N^{2r}}{n}\\log N+\\frac{N^{4r}}{n^{2}}\\log^{2}N\\right)\\|\\Sigma_{\\Psi,N}\\|_{\\mathrm{op}}\\,\\mathrm{Tr}(\\Sigma_{\\bar{\\beta},N})}\\\\ &{\\lesssim\\frac{N^{2r}}{n}\\log N+\\frac{N^{4r}}{n^{2}}\\log^{2}N}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ is bounded by Assumption 2. ", "page_idx": 16}, {"type": "text", "text": "Moreover for (14), we compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\nu_{S}^{*}(\\Delta)^{*}\\Gamma\\frac{\\sum_{i=1}^{K-1}\\Delta}{\\Delta}\\beta_{S,i}\\right)^{*}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\{\\mathcal{J}_{T}^{*}\\frac{\\sum_{k=1}^{K-1}\\sum_{i=1}^{K}\\gamma_{S,i}}{\\Delta}(\\beta)_{i}^{*}\\mathcal{J}_{S,S}^{*}(\\Delta)^{*}\\Gamma^{*}\\frac{\\sum_{k=1}^{K-1}\\beta_{S,i}}{\\Delta}\\beta_{S,i}\\right\\}\\right.}\\\\ &{=\\mathbb{E}\\left[\\left.\\sum_{k=1}^{K}\\mathrm{Er}\\frac{\\sum_{i=1}^{K}\\gamma_{S,i}}{\\sum_{k=1}^{K}\\sum_{i=1}^{K}\\beta_{S,i}}(\\Delta)_{i}^{*}\\mathrm{Er}^{*}(\\Delta)^{*}\\mathrm{Er}^{*}\\frac{\\sum_{k=1}^{K-1}\\beta_{S,i}}{\\Delta}\\beta_{S,i}\\overline{{\\Delta}}\\overline{{\\beta}}_{S,S}^{*}\\right]\\right\\}}\\\\ &{=\\mathbb{E}\\left[\\left.\\sum_{k=1}^{K-1}\\mathrm{Er}\\frac{\\sum_{i=1}^{K}\\gamma_{S,i}}{\\sum_{k=1}^{K}\\left\\langle\\gamma_{S,S}^{*}(\\Delta)_{i}^{*}\\Delta(\\Delta)_{i}^{*}\\mathrm{Er}^{*}(\\Delta)^{*}\\right\\rangle}\\right]\\subset\\frac{\\sum_{k=1}^{K-1}\\mathrm{Er}}{\\Delta}\\mathrm{Er}\\left\\{\\left.\\beta_{S,S}^{*}(\\Delta)_{i}^{*}\\mathrm{Er}^{*}\\right\\}\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\sum_{k=1}^{K}\\sum_{i=1}^{K}\\gamma_{S,i}}{\\sum_{k=1}^{K-1}\\mathrm{Er}\\left\\{\\alpha_{S,i}\\mathrm{Er}^{*}\\frac{\\sum_{k=1}^{K-1}\\Delta}{\\Delta}\\mathrm{Er}\\right\\}}\\right]}\\\\ &{\\leq\\frac{1}{n}\\mathbb{T}\\left[\\left(\\Delta_{\\phi,K}+\\frac{1}{\\Delta}\\sum_{i=1}^{K-1}\\right)\\Gamma^{*}\\Xi_{S,K}\\right]\\mathrm{T}\\Bigg[}\\\\ &{\\left.=\\frac{1}{n}\\mathrm{Tr}\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Bounding Term (9) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since the sequence of covariates $x_{1},\\cdot\\cdot\\cdot\\,,x_{n}$ and noise $\\xi_{1},\\cdot\\cdot\\cdot,\\xi_{n}$ are each i.i.d., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\Psi_{\\bar{N}:\\infty}^{\\circ\\top}\\beta_{\\bar{N}:\\infty}+\\xi)}{n}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Xi}=\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top}\\Gamma^{*}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(x_{i})(\\beta_{{\\boldsymbol{N}};{\\infty}}^{\\top}\\psi_{{\\boldsymbol{S}};{\\infty}}^{o}(x_{i})+\\xi_{i})\\right)^{2}\\right]}\\\\ &{\\mathbf{\\Xi}=\\frac{1}{n^{2}}\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top}\\Gamma^{*}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(x_{i})\\beta_{{\\boldsymbol{N}};{\\infty}}^{\\top}\\psi_{{\\boldsymbol{S}};{\\infty}}^{o}(x_{i})+\\displaystyle\\sum_{i=1}^{n}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top}\\Gamma^{*}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(x_{i})\\xi_{i}\\right)^{2}\\right]}\\\\ &{\\le\\frac{1}{n}\\mathbb{E}\\left[\\left(\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top}\\Gamma^{*}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(x)\\beta_{{\\boldsymbol{N}};{\\infty}}^{\\top}\\psi_{{\\boldsymbol{S}};{\\infty}}^{o}(x)\\right)^{2}\\right]}\\\\ &{\\phantom{2p c}+\\frac{n-1}{n}\\mathbb{E}\\left[\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top}\\Gamma^{*}\\psi_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(x)\\beta_{{\\boldsymbol{N}};{\\infty}}^{\\top}\\psi_{{\\boldsymbol{S}};{\\infty}}^{o}(x)\\psi_{{\\boldsymbol{N}};{\\infty}}^{o}(x)\\mathcal{V}_{{\\boldsymbol{S}};{\\boldsymbol{\\tilde{N}}}}^{o}(\\boldsymbol{\\tilde{x}})^{\\top} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for independent samples $x,x^{\\prime},\\tilde{x}\\sim\\mathcal{P}_{\\mathcal{X}}$ . We now bound the three terms separately below. ", "page_idx": 17}, {"type": "text", "text": "For (15), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\mathbb{E}\\left[\\left(\\psi_{N;\\hat{N}}^{0}(\\hat{x})^{\\top}\\Gamma^{*}\\psi_{N;\\hat{N}}^{0}(x)\\beta_{N;\\hat{N}\\omega}^{\\top}\\psi_{N;\\omega}^{0}(x)\\right)^{2}\\right]}\\\\ &{=\\frac{1}{n}\\mathbb{E}\\left[\\psi_{N;\\hat{N}}^{0}(x)^{\\top}\\Gamma^{*}\\psi_{N;\\hat{N}}^{0}(\\hat{x})\\psi_{N;\\hat{N}}^{0}(\\hat{x})^{\\top}\\Gamma^{*}\\psi_{N;\\hat{N}}^{0}(x)\\psi_{N;\\hat{N}\\omega}^{0}(x)^{\\top}\\beta_{\\hat{N};\\infty}\\beta_{\\hat{N};\\infty}^{\\top}\\psi_{N;\\omega}^{0}(x)\\right]}\\\\ &{=\\frac{1}{n}\\mathbb{E}_{x}\\left[\\psi_{N;\\hat{N}}^{0}(x)^{\\top}\\Gamma^{*}\\Sigma_{\\hat{N},N}\\Gamma^{*}\\psi_{N;\\hat{N}}^{0}(x)\\psi_{N;\\hat{N}\\omega}^{0}(x)^{\\top}\\mathbb{E}_{\\beta}\\left[\\beta_{\\hat{N};\\infty}\\beta_{\\hat{N};\\infty}^{\\top}\\right]\\psi_{N;\\infty}^{0}(x)\\right]}\\\\ &{\\lesssim\\frac{1}{n}\\mathbb{E}_{x}\\left[\\|\\psi_{N;\\hat{N}}^{0}(x)\\|^{2}\\psi_{N;\\infty}^{0}(x)^{\\top}\\mathbb{E}_{\\beta}\\left[\\beta_{\\hat{N};\\infty}\\beta_{N;\\infty}^{\\top}\\right]\\psi_{N;\\infty}^{0}(x)\\right]}\\\\ &{\\lesssim\\frac{1}{n}\\underbrace{\\operatorname*{sup}\\psi_{N}^{1}(\\psi_{N;\\hat{N}}^{0}(x)|^{2})\\cdot\\operatorname*{im}\\textrm{T r}(\\mathbb{E}_{\\beta}\\left[\\beta_{\\hat{N};M}\\beta_{\\hat{N};M}^{\\top}\\right]\\mathbb{E}_{x}\\left[\\psi_{N;M}^{0}(x)\\psi_{N;M}^{0}(x)^{\\top}\\right])}_{M\\setminus d_{\\hat{N};\\hat{N}}(M)}}\\\\ &{\\lesssim\\frac{1}{n}\\underbrace{\\operatorname*{sup}\\psi_{N}^{0}(\\psi_{N;\\hat{N}}^{0} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $\\begin{array}{r}{\\Sigma_{\\Psi,M}\\preceq C_{2}\\mathbf{I}_{N}}\\end{array}$ as $M\\to\\infty$ by Assumption 2. As $\\mathrm{sup}_{x}\\|\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)\\|^{2}\\lesssim N^{-2r}$ by (2) and the diagonal of $\\mathbb{E}_{\\beta}[\\beta_{\\bar{N}:M}\\beta_{\\bar{N}:M}^{\\top}]$ decays faster than $\\bar{N}^{-2s}M^{-1}(\\log M)^{-2}$ when $M>\\bar{N}$ due to (4), it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\operatorname*{sup}_{x\\in\\operatorname{supp}\\mathcal{P}_{X}}\\lVert\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)\\rVert^{2}\\cdot\\operatorname*{lim}_{M\\to\\infty}\\mathrm{Tr}\\left(\\mathbb{E}_{\\beta}\\left[\\beta_{\\bar{N}:M}\\beta_{\\bar{N}:M}^{\\top}\\right]\\right)\\lesssim\\frac{1}{n}N^{2r}N^{-2s}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for (16), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n-1}{n}\\mathbb{E}\\left[\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})^{\\top}\\Gamma^{*}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x})\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})^{\\top}\\Gamma^{*}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x}^{\\prime})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x}^{\\prime})\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left(\\psi_{\\mathcal{N};\\mathcal{N}}^{o}(\\boldsymbol{x})^{\\top}\\Gamma^{*}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x})\\right)^{\\top}\\left(\\psi_{\\mathcal{N};\\mathcal{N}}^{o}(\\boldsymbol{\\lambda})^{\\top}\\Gamma^{*}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x}^{\\prime})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x}^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{x,x^{\\prime},\\mathcal{N}}\\left[\\psi_{N;\\infty}^{o}(\\boldsymbol{x})^{\\top}\\beta_{\\mathcal{N};\\infty}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})^{\\top}\\Gamma^{*}\\Sigma_{\\Psi,N}\\Gamma^{*}\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x}^{\\prime})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x}^{\\prime})\\right]}\\\\ &{=\\mathbb{E}_{\\beta}\\left[\\mathbb{E}_{x}\\left[\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x})\\right]^{\\top}\\Gamma^{*}\\Sigma_{\\Psi,N}\\Gamma^{*}\\mathbb{E}_{x}\\left[\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})\\beta_{N;\\infty}^{\\top}\\psi_{N;\\infty}^{o}(\\boldsymbol{x}^{\\prime})\\right]\\right]}\\\\ &{\\lesssim\\mathbb{E}_{\\beta}\\left[\\left\\|\\mathbf{E}_{x}\\left[\\psi_{N;\\mathcal{N}}^{o}(\\boldsymbol{x})\\beta_{N;\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And for (17), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\sigma^{2}}{n}\\mathbb{E}\\left[\\left(\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\Gamma^{*}\\mathbb{E}_{\\bar{x}}\\left[\\psi_{\\underline{{N}};\\bar{N}}^{o}(\\tilde{x})\\psi_{\\underline{{N}};\\bar{N}}^{o}(\\tilde{x})^{\\top}\\right]\\Gamma^{*}\\mathbb{E}_{x}\\left[\\psi_{\\underline{{N}};\\bar{N}}^{o}(x)\\psi_{\\underline{{N}};\\bar{N}}^{0}(x)^{\\top}\\right]\\right]}\\\\ &{=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\underset{\\mathrm{positive~definite}}{\\underbrace{\\Gamma^{*}\\Sigma_{\\Psi,N}\\Gamma^{*}}}\\Sigma_{\\Psi,N}\\right]}\\\\ &{\\leq\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\Gamma^{*}\\Sigma_{\\Psi,N}\\Gamma^{*}\\left(\\Sigma_{\\Psi,N}+\\frac{1}{n}\\Sigma_{\\bar{\\beta},N}^{-1}\\right)\\right]}\\\\ &{=\\frac{\\sigma^{2}}{n}\\operatorname{Tr}\\left[\\Gamma^{*}\\Sigma_{\\Psi,N}\\right]\\leq\\frac{\\sigma^{2}N}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we obtain the following bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\psi_{N:\\bar{N}}^{\\circ}(\\tilde{x})^{\\top}\\Gamma^{*}\\frac{\\Psi_{N:\\bar{N}}^{\\circ}(\\Psi_{\\bar{N}:\\infty}^{\\circ\\top}\\beta_{\\bar{N}:\\infty}+\\xi)}{n}\\right)^{2}\\right]\\lesssim\\frac{N^{2r}N^{-2s}}{n}+N^{-2s}+\\frac{\\sigma^{2}N}{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.4 Bounding Terms (10)-(12) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For (10), we use the Cauchy-Schwarz inequality and Assumption 3 to bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left((\\phi^{*}(\\tilde{x})-\\psi_{N;N}^{o}(\\tilde{x}))^{\\top}\\Gamma^{*}\\frac{(\\Phi^{*}-\\Phi_{N;N}^{o})y}{n}\\right)^{2}\\right]}\\\\ &{\\leq\\frac{1}{n}\\frac{n}{i-1}\\mathbb{E}\\left[\\left((\\phi^{*}(\\tilde{x})-\\psi_{N;N}^{o}(\\tilde{x}))^{\\top}\\Gamma^{*}(\\phi^{*}(x_{i})-\\psi_{N;N}^{o}(x_{i}))y\\right)^{2}\\right]}\\\\ &{\\leq\\frac{1}{n}\\frac{n}{i-1}\\mathbb{E}\\left[\\|\\phi^{*}(\\tilde{x})-\\psi_{N;N}^{o}(\\tilde{x})\\|^{2}\\|\\Gamma^{*}(\\phi^{*}(x_{i})-\\psi_{N;N}^{o}(x_{i}))\\|^{2}y_{i}^{2}\\right]}\\\\ &{\\leq\\|\\Gamma^{*}\\|_{\\infty}^{2}\\bigg(\\underset{x\\leq i\\leq m_{N}}{\\leq}\\|\\phi^{*}(x)-\\psi_{N;N}^{o}(x)\\|^{2}\\bigg)^{2}\\mathbb{E}\\left[y^{2}\\right]}\\\\ &{\\leq\\|\\Gamma^{*}\\|_{\\infty}^{2}\\bigg(\\underset{N\\leq\\tilde{x}\\leq N}{\\operatorname*{max}}\\|\\phi_{j}^{*}-\\psi_{j}^{0}\\|_{L^{\\infty}(P_{x})}^{2}\\bigg)^{2}\\mathbb{E}\\left[y^{2}\\right]}\\\\ &{\\leq\\|\\Gamma^{*}\\|_{\\infty}^{2}\\bigg(N\\underset{N\\leq\\tilde{x}\\leq N}{\\operatorname*{max}}\\|\\phi_{j}^{*}-\\psi_{j}^{0}\\|_{L^{\\infty}(P_{x})}^{2}\\bigg)^{2}\\mathbb{E}\\left[y^{2}\\right]}\\\\ &{\\leq N^{2}\\delta_{N}^{2}(B^{2}+\\sigma^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly for (11) and (12), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left((\\phi^{*}(\\bar{x})-\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(\\bar{x}))^{\\top}\\Gamma^{*}\\frac{\\Psi_{\\mathcal{B}:\\bar{N}}^{o}(\\bar{x})}{n}\\right)^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left((\\phi^{*}(\\bar{x})-\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(\\bar{x}))^{\\top}\\Gamma^{*}\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(x_{i})y_{i}\\right)^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\|\\phi^{*}(\\bar{x})-\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(\\bar{x})\\|^{2}\\|\\Gamma^{*}\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(x_{i})\\|^{2}y_{i}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{x\\in\\mathrm{supp}}_{\\mathcal{P}_{\\mathcal{B}}}\\|\\phi^{*}(x)-\\psi_{{\\mathcal{B}}:\\bar{N}}^{o}(x)\\|^{2}\\|\\Gamma^{*}\\|_{\\phi_{\\mathcal{B}}}^{2}\\displaystyle\\sum_{x\\in\\mathrm{supp}}_{\\mathcal{P}_{\\mathcal{B}}}\\|\\psi_{\\mathcal{B}:\\bar{N}}^{o}(x)\\|^{2}\\mathbb{E}\\left[y^{2}\\right]}\\\\ &{\\lesssim N\\delta_{\\mathcal{B}}^{2}\\cdot N^{2}(B^{2}+\\sigma^{2})=N^{2r+1}\\delta_{\\mathcal{N}}^{2}(B^{2}+\\sigma^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left(\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}({\\tilde{x}})^{\\top}\\Gamma^{*}\\frac{\\left(\\Phi^{*}-\\Psi_{\\underline{{N}}:\\bar{N}}^{\\circ}\\right)y}{n}\\right)^{2}\\right]}\\\\ {\\displaystyle\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left(\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}({\\tilde{x}})^{\\top}\\Gamma^{*}(\\phi^{*}(x_{i})-\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{i}))y_{i}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\|\\psi_{\\underline{{N}};\\bar{N}}^{\\circ}(\\tilde{x})\\|^{2}\\|\\Gamma^{*}(\\phi^{*}(x_{i})-\\psi_{\\underline{{N}};\\bar{N}}^{\\circ}(x_{i}))\\|^{2}y_{i}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{x\\in\\mathrm{supp}\\,\\mathcal{P}_{\\mathcal{X}}}\\|\\psi_{\\underline{{N}};\\bar{N}}^{\\circ}(x)\\|^{2}\\|\\Gamma^{*}\\|_{\\mathrm{op}}^{2}\\displaystyle\\operatorname*{sup}_{x\\in\\mathrm{supp}\\,\\mathcal{P}_{\\mathcal{X}}}\\|(\\phi^{*}(x)-\\psi_{\\underline{{N}};\\bar{N}}^{\\circ}(x))\\|^{2}\\mathbb{E}\\left[y^{2}\\right]}\\\\ &{\\lesssim N^{2r+1}\\delta_{N}^{2}(B^{2}+\\sigma^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This concludes the proof of Proposition 3.2. ", "page_idx": 19}, {"type": "text", "text": "A.5 Proof of Lemma A.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We will make use of the following concentration bound and its corollary, proved at the end of this subsection. ", "page_idx": 19}, {"type": "text", "text": "Theorem A.2 (matrix Bernstein inequality). Let $\\mathbf{S}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{S}_{n}\\,\\in\\,\\mathbb{R}^{N\\times N}$ be independent random matrices such that $\\mathbb{E}[\\mathbf{S}_{i}]=0$ and $\\|\\mathbf{S}_{i}\\|_{\\mathrm{op}}\\leq L$ almost surely for all i. Define $\\textstyle\\mathbf{Z}=\\sum_{i=1}^{n}\\mathbf{S}_{n}$ and the matrix variance statistic $v(\\mathbf{Z})$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v(\\mathbf{Z})=\\operatorname*{max}\\left\\{\\|\\mathbb{E}[\\mathbf{Z}\\mathbf{Z}^{\\top}]\\|_{\\mathrm{op}},\\|\\mathbb{E}[\\mathbf{Z}^{\\top}\\mathbf{Z}]\\|_{\\mathrm{op}}\\right\\}=\\operatorname*{max}\\left\\{\\left\\|\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{S}_{i}\\mathbf{S}_{i}^{\\top}]\\right\\|_{\\mathrm{op}},\\left\\|\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{S}_{i}^{\\top}\\mathbf{S}_{i}]\\right\\|_{\\mathrm{op}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then it holds for all $t>0$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\|\\mathbf{Z}\\|_{\\mathrm{op}}\\geq t\\right)\\leq2N\\exp\\left(-{\\frac{t^{2}}{2v(\\mathbf{Z})+{\\frac{2}{3}}L t}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. See Tropp (2015), Theorem 6.1.1. ", "page_idx": 19}, {"type": "text", "text": "Corollary A.3. For matrices $\\mathbf{S}_{1},\\cdots,\\mathbf{S}_{n}$ satisfying the conditions of Theorem A.2, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\right]\\le\\frac{4v(\\mathbf{Z})}{n^{2}}(1+\\log2N)+\\frac{16L^{2}}{9n^{2}}(2+2\\log2N+(\\log2N)^{2}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will apply Corollary A.3 to the matrices ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i}:=\\boldsymbol{\\Sigma}_{\\Psi,N}^{-1/2}\\boldsymbol{\\psi}_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{i})\\boldsymbol{\\psi}_{\\underline{{N}}:\\bar{N}}^{\\circ}(x_{i})^{\\top}\\boldsymbol{\\Sigma}_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It is straightforward to verify that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{S}_{i}]=\\boldsymbol{\\Sigma}_{\\Psi,N}^{-1/2}\\boldsymbol{\\Sigma}_{\\Psi,N}\\boldsymbol{\\Sigma}_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{S}_{i}\\|_{\\mathrm{op}}\\lesssim\\|\\psi_{\\underline{{N}}:\\bar{N}}^{\\mathrm{o}}(x_{i})\\psi_{\\underline{{N}}:\\bar{N}}^{\\mathrm{o}}(x_{i})^{\\top}\\|_{\\mathrm{op}}+1=\\sum_{j=\\bar{N}}^{\\bar{N}}\\psi_{j}^{\\mathrm{o}}(x_{i})^{2}+1\\lesssim N^{2r}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "almost surely by Assumption 1. ", "page_idx": 19}, {"type": "text", "text": "Next, we evaluate the matrix variance statistic. Since each $\\mathbf{S}_{i}$ is symmetric, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v(\\mathbf{Z})=\\left\\|\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}[\\mathbf{S},\\mathbf{S}_{i}^{\\top}]\\right\\|_{\\mathrm{op}}}\\\\ &{\\displaystyle=\\left\\|\\frac{\\sqrt{1-1/2}}{i-1}\\left(\\mathbb{E}_{\\mathbf{X}}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})^{\\top}\\Sigma_{\\Psi,N}^{-1}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})^{\\top}\\Sigma_{\\Psi,N}^{-1/2}\\right]\\right.}\\\\ &{\\displaystyle~~~~~~-\\left.2\\mathbb{E}_{\\mathbf{X}}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})^{\\top}\\Sigma_{\\Psi,N}^{-1/2}\\right]+\\mathbf{I}_{N}\\right)\\right\\|_{\\mathrm{op}}}\\\\ &{\\displaystyle=\\left\\|\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbf{X}}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})^{\\top}\\Sigma_{\\Psi,N}^{-1}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x_{i})^{\\top}\\Sigma_{\\Psi,N}^{-1/2}\\right]-n\\mathbf{I}_{N}\\right\\|_{\\mathrm{op}}}\\\\ &{\\displaystyle\\leq n+n\\left\\|\\mathbb{E}_{\\mathbf{X}}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x)\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x)^{\\top}\\Sigma_{\\Psi,N}^{-1}\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x)\\psi_{{\\mathbb{X}};\\tilde{N}}^{\\circ}(x)^{\\top}\\Sigma_{ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for a single sample $x\\sim\\mathcal{P}_{\\mathcal{X}}$ . The second term is further bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\mathbb{E}_{x}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)^{\\top}\\Sigma_{\\Psi,N}^{-1}\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)^{\\top}\\Sigma_{\\Psi,N}^{-1/2}\\right]\\right\\Vert_{\\mathrm{op}}}\\\\ &{\\leq\\left\\Vert\\mathbb{E}_{x}\\left[\\Sigma_{\\Psi,N}^{-1/2}\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)\\psi_{\\mathcal{N};\\bar{N}}^{0}(x)^{\\top}\\Sigma_{\\Psi,N}^{-1/2}\\right]\\right\\Vert_{\\mathrm{op}}\\cdot\\left\\Vert\\psi_{\\mathcal{N};\\bar{N}}^{\\circ\\top}\\Sigma_{\\Psi,N}^{-1}\\psi_{\\mathcal{N};\\bar{N}}^{0}\\right\\Vert_{L^{\\infty}(\\mathcal{P}_{\\mathcal{N}})}}\\\\ &{\\lesssim\\|\\mathbf{I}_{N}\\|_{\\mathrm{op}}\\cdot\\bigg\\|\\displaystyle\\sum_{j=\\bar{N}}^{\\bar{N}}(\\psi_{j}^{0})^{2}\\bigg\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{N}})}}\\\\ &{\\lesssim N^{2r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "again by Assumption 1. ", "page_idx": 20}, {"type": "text", "text": "Hence we may apply Corollary A.3 with $v(\\mathbf{Z})\\lesssim n N^{2r},L\\lesssim N^{2r}$ to conclude: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}}\\left[\\left\\lVert\\sum_{\\Psi,N}^{-1/2}\\frac{\\Psi_{N:N}^{\\circ}\\Psi_{\\overline{{\\mathbf{\\Lambda}}}^{\\circ}:N}^{\\circ\\top}}{n}\\Sigma_{\\Psi,N}^{-1/2}-\\mathbf{I}_{N}\\right\\rVert_{\\mathrm{op}}^{2}\\right]=\\mathbb{E}\\left[\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{S}_{i}\\right\\rVert_{\\mathrm{op}}^{2}\\right]\\lesssim\\frac{N^{2r}}{n}\\log N+\\frac{N^{4r}}{n^{2}}\\log^{2}N.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary A.3. From Theorem A.2 and with the change of variables $\\lambda=t^{2}/n^{2}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}{\\left(\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\ge\\lambda\\right)}\\le2N\\exp{\\bigg(-\\frac{n^{2}\\lambda}{2v(\\mathbf{Z})+\\frac{2}{3}L n\\sqrt{\\lambda}}\\bigg)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the probability is also bounded above by 1, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\displaystyle\\left(\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\geq\\lambda\\right)\\leq1\\wedge2N\\exp\\Bigg(-\\frac{n^{2}\\lambda}{2(2v(\\mathbf{Z})\\vee\\frac{2}{3}L n\\sqrt{\\lambda})}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1\\wedge2N\\exp\\left(-\\frac{n^{2}\\lambda}{4v(\\mathbf{Z})}\\right)+1\\wedge2N\\exp\\bigg(-\\frac{3n\\sqrt{\\lambda}}{4L}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the expectation can be controlled as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\right]=\\int_{0}^{\\infty}\\operatorname*{Pr}\\left(\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\geq\\lambda\\right)\\mathrm{d}\\lambda}\\\\ {\\displaystyle\\leq\\int_{0}^{\\infty}1\\wedge2N\\exp\\left(-\\frac{n^{2}\\lambda}{4v(\\mathbf{Z})}\\right)\\mathrm{d}\\lambda+\\int_{0}^{\\infty}1\\wedge2N\\exp\\left(-\\,\\frac{3n\\sqrt{\\lambda}}{4L}\\right)\\mathrm{d}\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the first integral, we truncate at 4v(2Z)log 2N so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{\\infty}1\\wedge2N\\exp\\left(-\\frac{n^{2}\\lambda}{4v(\\mathbf{Z})}\\right)\\mathrm{d}\\lambda=\\lambda_{1}+\\int_{\\lambda_{1}}^{\\infty}2N\\exp\\left(-\\frac{n^{2}\\lambda}{4v(\\mathbf{Z})}\\right)\\mathrm{d}\\lambda}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\frac{4v(\\mathbf{Z})}{n^{2}}\\log2N+\\frac{4v(\\mathbf{Z})}{n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the second integral, we truncate at $\\begin{array}{r}{\\lambda_{2}:=\\left(\\frac{4L}{3n}\\log2N\\right)^{2}}\\end{array}$ so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{\\infty}1\\wedge2N\\exp\\left(-\\,\\frac{3n\\sqrt\\lambda}{4L}\\right)\\mathrm{d}\\lambda}\\\\ {\\displaystyle=\\lambda_{2}+\\int_{\\lambda_{2}}^{\\infty}2N\\exp\\left(-\\,\\frac{3n\\sqrt\\lambda}{4L}\\right)\\mathrm{d}\\lambda}\\\\ {\\displaystyle=\\lambda_{2}-\\frac{16L N}{3n}\\left(\\sqrt\\lambda+\\frac{4L}{3n}\\right)\\exp\\left(-\\,\\frac{3n\\sqrt\\lambda}{4L}\\right)\\Bigg|_{\\lambda=\\lambda_{2}}^{\\infty}}\\\\ {\\displaystyle=\\left(\\frac{4L}{3n}\\log2N\\right)^{2}+\\frac{8L}{3n}\\left(\\frac{4L}{3n}\\log2N+\\frac{4L}{3n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Adding up, we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{n^{2}}\\|\\mathbf{Z}\\|_{\\mathrm{op}}^{2}\\right]\\leq\\frac{4v(\\mathbf{Z})}{n^{2}}(1+\\log2N)+\\frac{16L^{2}}{9n^{2}}(2+2\\log2N+(\\log2N)^{2})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B Proofs on Metric Entropy ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Modified Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For a full proof of the original statement, we refer the reader to Section B.1 of Hayakawa and Suzuki (2020), which corrects some technical flaws in the original proof. Here we only outline the necessary modification to incorporate the bounded noise setting. ", "page_idx": 21}, {"type": "text", "text": "Denote an $\\epsilon$ -cover of the model space by $f_{1},\\cdot\\cdot\\cdot,f_{M}$ . The only step which relies on the normality of noise $\\xi_{1:n}$ is a concentration result for the random variables ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon_{j}:=\\frac{\\sum_{i=1}^{n}\\xi_{i}(f_{j}(x_{i})-f^{\\circ}(x_{i}))}{\\left[(\\sum_{i=1}^{n}(f_{j}(x_{i})-f^{\\circ}(x_{i}))^{2}\\right]^{1/2}},\\quad1\\leq j\\leq M,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where it is shown via the normality of $\\varepsilon_{j}$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{1\\leq j\\leq M}\\varepsilon_{j}^{2}\\right]\\leq4\\sigma^{2}(\\log M+1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will instead rely on Hoeffding\u2019s inequality. By writing $\\begin{array}{r}{\\varepsilon_{j}=w_{j}^{\\top}\\xi_{1:n}=\\sum_{i=1}^{n}w_{j,i}\\xi_{i}}\\end{array}$ where ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{j,i}=\\frac{f_{j}(x_{i})-f^{\\circ}(x_{i})}{\\left[(\\sum_{i=1}^{n}(f_{j}(x_{i})-f^{\\circ}(x_{i}))^{2}\\right]^{1/2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $|w_{j,i}\\xi_{i}|\\leq\\sigma|w_{j,i}|$ a.s. it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\vert\\varepsilon_{j}\\vert\\geq u\\right)\\leq2\\exp\\left(-{\\frac{2u^{2}}{\\sum_{i=1}^{n}(2\\sigma|w_{j,i}|)^{2}}}\\right)=2\\exp\\left(-{\\frac{u^{2}}{2\\sigma^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $u>0$ . Then the squared-exponential moment of each $\\varepsilon_{j}$ is bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(t\\varepsilon_{j}^{2})\\right]=1+\\displaystyle\\int_{1}^{\\infty}\\operatorname*{Pr}\\left(\\exp(t\\varepsilon_{j}^{2})\\geq\\lambda\\right)\\mathrm{d}\\lambda}\\\\ &{\\qquad\\qquad\\leq1+\\displaystyle\\int_{1}^{\\infty}2\\exp\\left(-\\frac{\\log\\lambda}{2\\sigma^{2}t}\\right)\\mathrm{d}\\lambda}\\\\ &{\\qquad\\qquad\\leq1+2\\displaystyle\\int_{1}^{\\infty}\\lambda^{-\\frac{1}{2\\sigma^{2}t}}\\,\\mathrm{d}\\lambda\\leq3}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by setting $\\textstyle t={\\frac{1}{4\\sigma^{2}}}$ . Hence via Jensen\u2019s inequality we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\exp\\left(t\\mathbb{E}\\left[\\operatorname*{max}_{1\\leq j\\leq M}\\varepsilon_{j}^{2}\\right]\\right)\\leq\\mathbb{E}\\left[\\operatorname*{max}_{1\\leq j\\leq M}\\exp(t\\varepsilon_{j}^{2})\\right]\\leq\\sum_{j=1}^{M}\\mathbb{E}\\left[\\exp(t\\varepsilon_{j}^{2})\\right]\\leq3M\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{1\\leq j\\leq M}\\varepsilon_{j}^{2}\\right]\\leq4\\sigma^{2}\\log3M,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "retrieving the original result up to a constant. ", "page_idx": 21}, {"type": "text", "text": "B.2 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let us take two functions $f_{\\Theta_{1}},f_{\\Theta_{2}}\\in\\mathcal{T}_{N}$ for $\\Theta_{i}=(\\Gamma_{i},\\phi_{i})$ , $i=1,2$ separated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\Gamma_{1}-\\Gamma_{2}\\|_{\\mathrm{op}}\\leq\\delta_{1},\\quad\\operatorname*{max}_{1\\leq j\\leq N}\\|\\phi_{1,j}-\\phi_{2,j}\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}\\leq\\delta_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f_{\\Theta_{1}}(\\mathbf{X},\\pmb{y},\\tilde{x})-f_{\\Theta_{2}}(\\mathbf{X},\\pmb{y},\\tilde{x})|}\\\\ &{\\ \\leq|\\check{f}_{\\Theta_{1}}(\\mathbf{X},\\pmb{y},\\tilde{x})-\\check{f}_{\\Theta_{2}}(\\mathbf{X},\\pmb{y},\\tilde{x})|}\\\\ &{\\ =\\left|\\left\\langle\\frac{\\Gamma_{1}\\phi_{1}(\\mathbf{X})\\pmb{y}}{n},\\phi_{1}(\\tilde{x})\\right\\rangle-\\left\\langle\\frac{\\Gamma_{2}\\phi_{2}(\\mathbf{X})\\pmb{y}}{n},\\phi_{2}(\\tilde{x})\\right\\rangle\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n}\\Big|\\phi_{1}\\big(\\widetilde x\\big)^{\\top}\\Gamma_{1}\\phi_{1}(\\mathbf X)y-\\phi_{2}(\\widetilde x)^{\\top}\\Gamma_{1}\\phi_{1}(\\mathbf X)y+\\phi_{2}(\\widetilde x)^{\\top}\\Gamma_{1}\\phi_{1}(\\mathbf X)y}\\\\ &{\\qquad-\\phi_{2}(\\widetilde x)^{\\top}\\Gamma_{2}\\phi_{1}(\\mathbf X)y+\\phi_{2}(\\widetilde x)^{\\top}\\Gamma_{2}\\phi_{1}(\\mathbf X)y-\\phi_{2}(\\widetilde x)^{\\top}\\Gamma_{2}\\phi_{2}(\\mathbf X)y\\Big|}\\\\ &{\\le\\displaystyle\\frac1n\\|\\phi_{1}(\\widetilde x)-\\phi_{2}(\\widetilde x)\\|\\|\\Gamma_{1}\\|_{\\infty}\\|\\phi_{1}(\\mathbf X)y\\|+\\displaystyle\\frac1n\\|\\phi_{2}(\\widetilde x)\\|\\|\\Gamma_{1}-\\Gamma_{2}\\|_{\\infty}\\|\\phi_{1}(\\mathbf X)y\\|}\\\\ &{\\qquad+\\displaystyle\\frac1n\\|\\phi_{2}(\\widetilde x)\\|\\|\\Gamma_{2}\\|_{\\infty}\\|\\phi_{1}(\\mathbf X)-\\phi_{2}(\\mathbf X)y\\|}\\\\ &{\\le\\left(\\displaystyle\\frac{\\sqrt N\\delta_{2}C_{2}}{n}+\\frac{B_{\\mathrm{N}}^{\\top}\\delta_{1}}{n}\\right)\\displaystyle\\sum_{i=1}^{n}\\|\\phi_{1}(x_{i})\\|\\|y_{i}\\|+\\displaystyle\\frac{B_{N}^{\\top}C_{2}}{n}\\displaystyle\\sum_{i=1}^{n}\\!\\|\\phi_{1}(x_{i})-\\phi_{2}(x_{i})\\||y_{i}}\\\\ &{\\le\\left(\\sqrt N\\delta_{2}C_{2}+B_{\\mathrm{N}}^{\\top}\\delta_{1}\\right)B_{N}^{\\top}(B+\\sigma)+B_{N}^{\\top}C_{2}\\sqrt{N}\\delta_{2}(B+\\sigma)}\\\\ &{=B_{N}^{\\top}(B+\\sigma)\\delta_{1}+2B_{N}^{\\top}(B+\\sigma)C_{2}\\sqrt{N}\\delta_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence to construct an $\\epsilon$ -cover $G_{T}$ of $\\mathcal{T}_{N}$ , it suffices to exhibit a $\\delta_{1}$ -cover $G_{S}$ of $\\mathcal{S}_{N}$ and a $\\delta_{2}$ -cover $G_{\\mathcal{F}}$ of ${\\mathcal F}_{N}$ for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\delta_{1}=\\frac{\\epsilon}{2B_{N}^{\\prime2}(B+\\sigma)},\\quad\\delta_{2}=\\frac{\\epsilon}{4B_{N}^{\\prime}(B+\\sigma)C_{2}\\sqrt{N}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and set $G_{\\mathcal{T}}=G_{\\mathcal{S}}\\times G_{\\mathcal{F}}$ . For the metric entropy, this implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}(\\mathcal{T}_{N},\\|\\cdot\\|_{L^{\\infty}},\\epsilon)\\leq\\mathcal{V}(S_{N},\\|\\cdot\\|_{\\mathrm{op}},\\delta_{1})+\\mathcal{V}(\\mathcal{F}_{N},\\|\\cdot\\|_{L^{\\infty}},\\delta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We further bound the metric entropy of $\\mathcal{S}_{N}$ with the following result, proved in Appendix B.3. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.1. For $\\delta\\leq{\\frac{1}{2}}\\,i t$ it holds that V(SN, \u2225\u00b7\u2225op, \u03b4) \u2272N 2 log 1. ", "page_idx": 22}, {"type": "text", "text": "Substituting into the above, we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{V}(T_{N},\\Vert\\cdot\\Vert_{L^{\\infty}},\\epsilon)\\lesssim N^{2}\\log\\frac{B_{N}^{\\prime2}}{\\epsilon}+\\mathcal{V}\\left(\\mathcal{F}_{N},\\Vert\\cdot\\Vert_{L^{\\infty}},\\frac{\\epsilon}{4B_{N}^{\\prime}(B+\\sigma)C_{2}\\sqrt{N}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The choice of $\\delta_{2}$ is not important as long as the metric entropy of ${\\mathcal{F}}_{N}$ is at most polynomial in $\\delta_{2}$ . ", "page_idx": 22}, {"type": "text", "text": "B.3 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let $\\Gamma_{1},\\Gamma_{2}\\in S_{N}$ and consider their diagonalizations ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Gamma_{i}=\\mathbf{U}_{i}\\mathbf{\\Lambda}_{i}\\mathbf{U}_{i}^{\\top},\\quad\\boldsymbol{\\Lambda}_{i}=\\mathrm{diag}(\\boldsymbol{\\lambda}_{i,1},\\cdot\\cdot\\cdot\\mathbf{\\Lambda},\\boldsymbol{\\lambda}_{i,N}),\\quad i=0,1,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{U}_{i}\\in\\mathcal{O}_{N}$ , the orthogonal group in dimension $N$ , and $0\\le\\lambda_{i,j}\\le C_{3}$ for each $1\\le j\\le N$ Assuming ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{U}_{1}-\\mathbf{U}_{2}\\|_{\\mathrm{op}}\\leq\\frac{\\delta}{4C_{3}},\\quad|\\lambda_{1,j}-\\lambda_{2,j}|\\leq\\frac{\\delta}{2}\\quad\\forall j\\leq N,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Gamma_{1}-\\Gamma_{2}\\|_{\\mathrm{op}}}\\\\ &{=\\|\\mathbf{U}_{1}\\Lambda_{1}\\mathbf{U}_{1}^{\\top}-\\mathbf{U}_{1}\\Lambda_{1}\\mathbf{U}_{2}^{\\top}+\\mathbf{U}_{1}\\Lambda_{1}\\mathbf{U}_{2}^{\\top}-\\mathbf{U}_{2}\\Lambda_{1}\\mathbf{U}_{2}^{\\top}+\\mathbf{U}_{2}\\Lambda_{1}\\mathbf{U}_{2}^{\\top}-\\mathbf{U}_{2}\\Lambda_{2}\\mathbf{U}_{2}^{\\top}\\|_{\\mathrm{op}}}\\\\ &{\\leq2\\|\\Lambda_{1}\\|_{L^{\\infty}}\\|\\mathbf{U}_{1}-\\mathbf{U}_{2}\\|_{\\mathrm{op}}+\\|\\Lambda_{1}-\\Lambda_{2}\\|_{L^{\\infty}}}\\\\ &{\\leq2C_{3}\\cdot\\displaystyle\\frac{\\delta}{4C_{3}}+\\displaystyle\\frac{\\delta}{2}=\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, the covering number of $\\mathcal{O}_{N}$ in operator norm is given by the following result. ", "page_idx": 22}, {"type": "text", "text": "Theorem B.2 (Szarek (1981), Proposition 6). There exist universal constants $c_{1},c_{2}>0$ such that for all $N\\in\\mathbb{N}$ and $\\delta\\in(0,2]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\frac{c_{1}}{\\delta}\\right)^{\\frac{N(N-1)}{2}}\\leq\\mathcal{N}(\\mathcal{O}_{N},\\|\\cdot\\|_{\\mathrm{op}},\\delta)\\leq\\left(\\frac{c_{2}}{\\delta}\\right)^{\\frac{N(N-1)}{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\mathcal{V}(S_{N},\\|\\cdot\\|_{\\mathrm{op}},\\delta)\\leq\\mathcal{V}\\left(\\mathcal{O}_{N},\\|\\cdot\\|_{\\mathrm{op}},\\frac{\\delta}{4C_{3}}\\right)+\\mathcal{V}\\left([0,C_{3}]^{N},\\|\\cdot\\|_{L^{\\infty}},\\frac{\\delta}{2}\\right)}\\\\ &{\\leq\\frac{N(N-1)}{2}\\log\\frac{C_{2}}{\\delta}+N\\log\\frac{2C_{3}}{\\delta}}\\\\ &{\\lesssim N^{2}\\log\\frac{1}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, we remark that if elements of the domain $\\mathcal{S}_{N}$ are not constrained to be symmetric, we can alternatively consider the singular value decomposition and separately bound entropy of the two rotation components, giving the same result up to constants. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C Details on Besov-type Spaces ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Besov Space ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1.1 Verification of Assumptions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first give some background on wavelet decomposition. The decay rate $s=\\alpha/d$ is intrinsic to the Besov space as shown by the following result, which allows us to translate between functions $f\\in B_{p,q}^{\\alpha}(\\bar{\\mathcal{X}})$ and their B-spline coefficient sequences. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.1 (DeVore and Popov (1988), Corollary 5.3). If $\\alpha\\,>\\,d/p$ and $m>\\alpha+1-1/p,$ , a function $f\\in L^{p}(\\mathcal{X})$ is in $B_{p,q}^{\\alpha}(\\mathcal{X})$ if and only if $f$ can be represented as ", "page_idx": 23}, {"type": "equation", "text": "$$\nf=\\sum_{k=0}^{\\infty}\\sum_{\\ell\\in I_{k}^{d}}\\tilde{\\beta}_{k,\\ell}\\omega_{k,\\ell}^{d}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "such that the coefficients satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Vert\\tilde{\\beta}\\Vert_{b_{p,q}^{\\alpha}}:=\\left[\\sum_{k=0}^{\\infty}\\left[2^{k(\\alpha-d/p)}\\bigg(\\sum_{\\ell\\in I_{k}^{d}}|\\tilde{\\beta}_{k,\\ell}|^{p}\\bigg)^{1/p}\\right]^{q}\\right]^{1/q}<\\infty,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with appropriate modifications if $p=\\infty$ or $q=\\infty$ . Moreover, the two norms $\\|\\tilde{\\beta}\\|_{b_{p,q}^{\\alpha}}$ and $\\|f\\|_{B_{p,q}^{\\alpha}}$ are equivalent. ", "page_idx": 23}, {"type": "text", "text": "In particular, this implies that for any $f\\,\\in\\,\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ the $p$ -norm average of $\\tilde{\\beta}_{k,\\ell}$ for $\\ell\\in I_{k}^{d}$ at resolution $k$ is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{|I_{k}^{d}|}\\sum_{\\ell\\in I_{k}^{d}}|\\tilde{\\beta}_{k,\\ell}|^{p}\\right)^{1/p}\\lesssim(2^{-k d})^{1/p}\\cdot2^{k(d/p-\\alpha)}\\|f\\|_{B_{p,q}^{\\alpha}}\\leq2^{-k\\alpha},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the coefficients $\\beta_{k,\\ell}=2^{-k d/2}\\tilde{\\beta}_{k,\\ell}$ w.r.t. the scaled basis $(2^{k d/2}\\omega_{k,\\ell}^{d})_{k,\\ell}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{|I_{k}^{d}|}\\sum_{\\ell\\in I_{k}^{d}}|\\beta_{k,\\ell}|^{p}\\right)^{1/p}\\lesssim(2^{k d})^{-\\alpha/d-1/2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus it is natural in a probabilistic sense to assume $\\mathbb{E}[\\beta_{k,\\ell}^{p}]^{1/p}\\,\\lesssim\\,(2^{k d})^{-\\alpha/d-1/2}$ . This will be the case if for instance we sample $(\\beta_{k,\\ell})_{\\ell\\in I_{k}^{d}}$ uniformly from the $p$ -norm ball (18). This matches Assumption $4\\;\\mathrm{up}$ to the logarithmic factor and hence the rate is nearly tight in variance, even though (18) only applies to the average over locations rather than each coefficient explicitly. See also the discussion in Lemma 2 of Suzuki (2019). ", "page_idx": 23}, {"type": "text", "text": "Assumption 1. We take $m$ to be even for simplicity. The wavelet system $(\\omega_{K,\\ell}^{d})_{\\ell\\in I_{K}^{d}}$ at each resolution $K$ is linearly independent; for any $g\\in L^{2}(\\mathcal{X})$ that can be expressed as ", "page_idx": 24}, {"type": "equation", "text": "$$\ng=\\sum_{\\ell\\in I_{K}^{d}}\\beta_{K,\\ell}2^{K d/2}\\omega_{K,\\ell}^{d}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have the quasi-norm equivalence (Du\u02dcng, 2011b, 2.15) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|g\\|_{2}\\asymp2^{-K d/2}\\Big(\\sum_{\\ell\\in I_{K}^{d}}2^{K d}\\beta_{K,\\ell}^{2}\\Big)^{1/2}=\\|(\\beta_{K,\\ell})_{\\ell\\in I_{K}^{d}}\\|_{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies that the covariance matrix $\\mathbb{E}_{x\\sim\\mathrm{Unif}([0,1]^{d})}[\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)^{\\top}]$ is bounded above and below. Since we assume $\\mathcal{P}_{\\mathcal{X}}$ has Lebesgue density bounded\u00af above an\u00afd below, it follows that $\\Sigma_{\\Psi,N}$ is uniformly bounded above and below for all $K\\ge0$ . ", "page_idx": 24}, {"type": "text", "text": "In contrast, any B-spline at a lower resolution $k<K$ can be exactly expressed as a linear combination of elements of $(\\omega_{K,\\ell}^{d})_{\\ell\\in I_{K}^{d}}$ by repeatedly applying the following relation. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.2 (refinement equation). For even m and $\\boldsymbol{r}=(r_{1},\\cdot\\cdot\\cdot\\,,r_{d})^{\\intercal}$ , $\\mathbf{1}=(1,\\cdot\\cdot\\cdot\\,,1)^{\\intercal}\\in\\mathbb{R}^{d}\\:i t$ holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\omega_{k,\\ell}^{d}=\\sum_{r_{1},\\cdots,r_{d}=0}^{m}2^{(-m+1)d}\\prod_{i=1}^{d}\\binom{m}{r_{i}}\\cdot\\omega_{k+1,2\\ell+r-\\frac{m}{2}{\\bf1}}^{d}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The relation for one-dimensional wavelets is given in equation (2.21) of Du\u02dcng (2011b), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\iota_{m}(x)=2^{-m+1}\\sum_{r=0}^{m}{\\binom{m}{r}}\\iota_{m}\\left(2x-r+\\frac{m}{2}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "from which it follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\omega_{k,\\ell}^{d}(x)=\\displaystyle\\prod_{i=1}^{d}\\iota_{m}(2^{k}x_{i}-\\ell_{i})}\\\\ &{\\qquad=\\displaystyle\\sum_{r_{1},\\dots,r_{d}=0}^{m}2^{(-m+1)d}\\displaystyle\\prod_{i=1}^{d}\\binom{m}{r_{i}}\\iota_{m}\\left(2^{k+1}x_{i}-2\\ell_{i}-r_{i}+\\frac{m}{2}\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{r_{1},\\dots,r_{d}=0}^{m}2^{(-m+1)d}\\displaystyle\\prod_{i=1}^{d}\\binom{m}{r_{i}}\\cdot\\omega_{k+1,2\\ell+r-\\frac{m}{2}\\mathbf{1}}^{d}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as was to be shown. ", "page_idx": 24}, {"type": "text", "text": "Therefore we select all B-splines at a fixed resolution $K$ to approximate the target tasks, ", "page_idx": 24}, {"type": "equation", "text": "$$\nN=\\left|I_{K}^{d}\\right|=(m+1+2^{K})^{d}\\asymp2^{K d}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underline{{{N}}}=\\sum_{k=0}^{K-1}|I_{k}^{d}|+1\\times N,\\quad\\bar{N}=\\sum_{k=0}^{K}|I_{k}^{d}|\\asymp N.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is straightforward to see that $0\\leq\\omega_{k,\\ell}^{d}(x)\\leq1$ for all $x\\in\\mathscr{X}$ and moreover the B-splines (extended to all $\\ell\\in\\mathbb{Z}^{d}$ ) form a partition of unity of $\\mathbb{R}^{d}$ at all resolutions: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in\\mathbb{Z}^{d}}\\omega_{k,\\ell}^{d}(x)\\equiv1,\\quad\\forall x\\in\\mathbb{R}^{d},\\quad\\forall k\\geq0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then for all $x\\in\\mathscr{X}$ we have the bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{j=\\underline{{N}}}^{\\bar{N}}\\psi_{j}^{\\circ}(x)^{2}=\\sum_{\\ell\\in I_{K}^{d}}2^{K d}{\\omega_{K,\\ell}^{d}}(x)^{2}\\leq2^{K d}\\sum_{\\ell\\in\\mathbb{Z}^{d}}{\\omega_{K,\\ell}^{d}}(x)=2^{K d}\\lesssim N,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and hence (2) holds with $r=1/2$ . ", "page_idx": 24}, {"type": "text", "text": "Assumption 2. For any $\\beta\\in\\operatorname{supp}\\mathcal{P}_{\\beta}$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|F_{\\beta}^{\\alpha}\\|_{L^{\\infty}(P_{x})}\\leq\\displaystyle\\sum_{k=0}^{\\infty}\\left\\|\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}\\cdot2^{k d/2}\\omega_{k,\\ell}^{d}\\right\\|_{L^{\\infty}(P_{x})}}\\\\ {\\ }&{\\leq\\displaystyle\\sum_{k=0}^{\\infty}2^{k d/2}\\operatorname*{max}_{\\ell\\in I_{k}^{d}}\\left\\|\\beta_{k,\\ell}\\right\\|\\cdot\\displaystyle\\sum_{\\ell\\in I_{k}^{d}}\\omega_{k,\\ell}^{d}\\right\\|_{L^{\\infty}(X)}}\\\\ {\\ }&{\\leq\\displaystyle\\sum_{k=0}^{\\infty}2^{k d(1/2+1/p)}\\left(\\frac{1}{|I_{k}^{d}|}\\sum_{\\ell\\in I_{k}^{d}}|\\beta_{k,\\ell}|^{p}\\right)^{1/p}\\left\\|\\sum_{\\ell\\in I_{k}^{d}}\\omega_{k,\\ell}^{d}\\right\\|_{L^{\\infty}(X)}}\\\\ {\\ }&{\\lesssim\\displaystyle\\sum_{k=0}^{\\infty}2^{k d(1/2+1/p)}\\cdot(2^{k d})^{-(2\\ell-1/2)}\\|F_{\\beta}^{\\alpha}\\|_{B_{p,q}^{\\infty}}}\\\\ {\\ }&{\\lesssim(1-2^{d/p-\\alpha})^{-1}=:B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, the convergence rate of the truncated approximation $F_{\\beta,\\bar{N}}^{\\circ}$ is determined by the decay rate of $\\beta$ in Lemma C.1 as follows (it does not matter whether we bound F \u03b2\u25e6, N\u00af or F \u03b2\u25e6,N since N\u00af \u224dN). We consider a truncation of all resolutions lower than $K$ so that $\\bar{N},N\\asymp2^{K d}$ . Then it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F_{\\beta}^{\\circ}-F_{\\beta,N}^{\\circ}\\|_{L^{2}(\\mathcal{P}_{\\mathcal{X}})}^{2}=\\displaystyle\\int_{\\mathcal{X}}\\Bigg(\\sum_{j=\\bar{N}+1}^{\\infty}\\beta_{j}\\psi_{j}^{\\circ}(x)\\Bigg)^{2}\\mathcal{P}_{\\mathcal{X}}(\\mathrm{d}x)=\\displaystyle\\sum_{j,k=\\bar{N}+1}^{\\infty}\\beta_{j}\\beta_{k}\\mathbb{E}_{x}[\\psi_{j}^{\\circ}(x)\\psi_{k}^{\\circ}(x)]}\\\\ &{\\le\\displaystyle\\operatorname*{lim}_{M\\rightarrow\\infty}C_{2}\\|\\beta_{\\bar{N}:M}\\|^{2}=C_{2}\\displaystyle\\sum_{k=K}^{\\infty}\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}^{2}}\\\\ &{\\lesssim\\displaystyle\\sum_{k=K}^{\\infty}|I_{k}^{d}|^{1-2/p}\\Bigg(\\sum_{\\ell\\in I_{k}^{d}}|\\beta_{k,\\ell}|^{p}\\Bigg)^{2/p}\\lesssim\\displaystyle\\sum_{k=K}^{\\infty}2^{k d}\\cdot\\big(2^{k d}\\big)^{-2\\alpha/d-1}}\\\\ &{=\\displaystyle\\frac{2^{-2\\alpha K}}{1-2^{-2\\alpha}}\\asymp N^{-2\\alpha/d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where for the last two inequalities we have used the inequality $\\|z\\|_{2}\\leq D^{1/2-1/p}\\|z\\|_{p}$ for $z\\in\\mathbb{R}^{D}$ and $p\\geq2$ in conjunction with (18). Thus our choice of $s=\\alpha/d$ is justified. Under this choice, (5) directly implies (4) as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\beta}[\\beta_{K,\\ell}^{2}]\\lesssim2^{-K d(2s+1)}K^{-2}\\asymp\\bar{N}^{-2s-1}(\\log\\bar{N})^{-2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds for the basis elements at each resolution $K$ , that is for those numbered between $N$ and $\\bar{N}$ . ", "page_idx": 25}, {"type": "text", "text": "Remark C.3. When $1\\le p<2$ , the truncation up to $\\bar{N}$ does not suffice to achieve the $N^{-2\\alpha/d}$ approximation rate, and basis elements must be judiciously selected from a wider resolution range. More concretely, a size $N$ subset of all wavelets up to resolution $K^{\\prime}\\;=\\;\\lceil K(1+\\nu^{-1})\\rceil$ where $\\nu=p\\alpha/2d-1/2>0$ must be used (Suzuki and Nitanda, 2021, Lemma 2). Hence the exponent is a factor of $1+\\nu^{-1}$ worse w.r.t. $N^{\\prime}\\asymp2^{K^{\\prime}d}$ , leading to the inevitable suboptimal rate. ", "page_idx": 25}, {"type": "text", "text": "To show boundedness of $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ , we analyze the composition of the aggregated coefficients $\\bar{\\beta}$ using the following result. ", "page_idx": 25}, {"type": "text", "text": "Corollary C.4. For any $0\\le k<k^{\\prime}$ there exists constants $\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\geq0$ for $\\ell\\in I_{k}^{d}$ , $\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}2^{k d/2}\\omega_{k,\\ell}^{d}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}2^{k^{\\prime}d/2}\\omega_{k^{\\prime},\\ell^{\\prime}}^{d},\\quad\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}=\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\beta_{k,\\ell}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds for all $(\\beta_{k,\\ell})_{\\ell\\in I_{k}^{d}}$ . Moreover, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\leq2^{(k-k^{\\prime})d/2},\\quad\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\leq2^{(k^{\\prime}-k)d/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The statement follows directly from the more general Proposition C.10, stated and proved in Appendix C.3 below, by restricting to wavelets with uniform resolution across dimensions. Using Corollary C.4, we can refine each lower resolution component of $F_{\\beta}^{\\circ}$ to resolution $K$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nF_{\\beta,\\bar{N}}^{\\circ}=\\sum_{j=1}^{\\bar{N}}\\beta_{j}\\psi_{j}^{\\circ}=\\sum_{k=0}^{K}\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}2^{k d/2}\\omega_{k,\\ell}^{d}=\\sum_{k=0}^{K}\\sum_{\\ell^{\\prime}\\in I_{K}^{d}}\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}\\beta_{k,\\ell}2^{K d/2}\\omega_{K,\\ell^{\\prime}}^{d}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus each aggregated coefficient, indexed here by $\\ell\\in I_{K}^{d}$ , can be expressed as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{K,\\ell}=\\sum_{k=0}^{K}\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}\\beta_{k,\\ell}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence it follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{\\beta}[\\bar{\\beta}_{K,\\ell}^{2}]=\\sum_{k=0}^{K}\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}^{2}\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}^{2}]\\lesssim\\sum_{k=0}^{K}\\left(\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}\\right)^{2}2^{-k(2\\alpha+d)}k^{-2}}\\\\ &{\\quad\\qquad\\leq\\displaystyle\\sum_{k=0}^{K}2^{(k-K)d}\\cdot2^{-k(2\\alpha+d)}k^{-2}}\\\\ &{\\quad\\lesssim2^{-K d}\\cdot\\displaystyle\\sum_{k=0}^{K}2^{-2k\\alpha}k^{-2}\\asymp N^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "from which we conclude that $\\begin{array}{r}{\\mathrm{Tr}(\\Sigma_{\\bar{\\beta},N})=\\sum_{\\ell\\in I_{K}^{d}}\\mathbb{E}_{\\beta}[\\bar{\\beta}_{K,\\ell}^{2}]\\lesssim N\\cdot N^{-1}}\\end{array}$ is uniformly bounded. ", "page_idx": 26}, {"type": "text", "text": "Finally for the verification of Assumption 3, see Appendix C.1.2. ", "page_idx": 26}, {"type": "text", "text": "C.1.2 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We use the following result to approximate each wavelet $\\omega_{K,\\ell}^{d}$ at resolution $K$ with the class (6). The proof, in turn, relies on the construction by Yarotsky (2016) of DNNs which efficiently approximates the multiplication operation. ", "page_idx": 26}, {"type": "text", "text": "Lemma C.5 (Suzuki (2019), Lemma 1). For all $\\delta\\ >\\ 0$ , there exists a ReLU neural network $\\tilde{\\omega}\\in\\mathcal{F}_{\\mathrm{DNN}}(L,W,S,M)$ with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=3+2\\left\\lceil\\log_{2}\\left(3^{d\\vee m}(1+d m^{-1/2}(2e)^{m+1})\\delta^{-1}\\right)+5\\right\\rceil\\left\\lceil\\log_{2}(d\\vee m)\\right\\rceil,}\\\\ &{W=W_{0}=6d m(m+2)+2d,\\qquad S=L W^{2},\\qquad M=2(m+1)^{m}}\\\\ &{\\mathrm{upp}\\,\\tilde{\\omega}\\subseteq[0,m+1]^{d}\\,a n d\\;\\|\\omega_{0,0}^{d}-\\tilde{\\omega}\\|_{L^{\\infty}(\\mathcal{X})}\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $\\delta$ is also dependent on $N$ . ", "page_idx": 26}, {"type": "text", "text": "Now consider $N$ identical copies of $\\tilde{\\omega}$ in parallel, where each module is preceded by the scaling $(x_{i})_{i=1}^{d}\\mapsto(2^{K}x_{i}-\\ell_{i})_{i=1}^{d}$ for $\\ell\\,\\in\\,I_{K}^{d}$ and whose output is scaled by $2^{K\\bar{d}/2}$ . In particular, these operations can be implemented by $K\\lesssim\\log N$ consecutive additional layers with norm bounded\u221a by a constant. Hence each module $\\phi_{\\underline{{N}}}^{*},\\cdots\\,,\\phi_{\\bar{N}}^{*}$ approximates the basis $2^{K d/2}\\omega_{K,\\ell}^{d}$ 2\u03c9dK,\u2113with 2Kd/2\u03b4 \u2272 N\u03b4 accuracy, and substituting $\\delta_{N}=\\sqrt{N}\\delta$ gives that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\psi_{j}^{\\circ}-\\phi_{j}^{*}\\|_{L^{\\infty}(\\mathcal{P}_{x})}\\leq\\delta_{N},\\quad\\underline{{N}}\\leq j\\leq\\bar{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $L\\lesssim\\log\\delta^{-1}+\\log N\\lesssim\\log\\delta_{N}^{-1}+\\log N$ . Note that the sparsity $S$ is only multiplied by a factor of $N$ since different modules do not share any connections. Moreover the target basis has 2-norm bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\psi_{\\underline{{N}}:\\bar{N}}^{\\circ}(x)\\|_{2}\\lesssim\\bigg(\\sum_{\\ell\\in I_{K}^{d}}2^{K d}\\omega_{K,\\ell}(x)^{2}\\bigg)^{1/2}\\leq\\bigg(2^{K d}\\sum_{j\\in\\mathbb{Z}^{d}}\\omega_{K,\\ell}(x)\\bigg)^{1/2}\\asymp\\sqrt{N},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we have again used the sparsity of $\\omega_{k,\\ell}^{d}$ at each resolution. Hence we may clip the magnitude of the vector output $\\phi$ by $B_{N}^{\\prime}$ and the approximation guarantee remains unchanged. ", "page_idx": 26}, {"type": "text", "text": "To bound the covering number of ${\\mathcal F}_{N}$ , we directly apply the following result. ", "page_idx": 26}, {"type": "text", "text": "Lemma C.6 (Suzuki (2019), Lemma 3). The covering number of $\\mathcal{F}_{\\mathrm{DNN}}$ is bounded as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\mathcal{F}_{\\mathrm{DNN}}(L,W,S,M),\\|\\cdot\\|_{L^{\\infty}},\\epsilon)\\leq\\left(\\frac{L(M\\vee1)^{L-1}(W+1)^{2L}}{\\epsilon}\\right)^{S}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since clipping the magnitude of the outputs does not increase the covering number, we conclude: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{V}(\\mathcal{F}_{N},\\left\\Vert\\cdot\\right\\Vert_{L^{\\infty}},\\epsilon)\\leq N\\cdot\\mathcal{V}(\\mathcal{F}_{\\mathrm{\u1e0aNN\u1e0c}}(L,W,S,M),\\left\\Vert\\cdot\\right\\Vert_{L^{\\infty}},\\epsilon)}\\\\ &{\\qquad\\qquad\\leqq S N\\log L+S L N\\log M+2S L N\\log(W+1)+S N\\log\\frac{1}{\\epsilon}}\\\\ &{\\qquad\\qquad\\lesssim{N\\log\\frac{N}{\\delta_{N}}+N\\log\\frac{1}{\\epsilon}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "C.1.3 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "By Lemma 3.3 and Lemma 4.4, the metric entropy of $\\mathcal{T}_{N}$ is bounded as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{V}(\\mathcal{T}_{N},\\lVert\\cdot\\rVert_{L^{\\infty}},\\epsilon)\\lesssim N^{2}\\log\\frac{N}{\\epsilon}+N\\log\\frac{N^{2}}{\\delta_{N}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining with Theorem 3.1 and Proposition 3.2 with $r=1/2$ and $s=\\alpha/d$ gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim\\frac{N}{n}\\log N+\\frac{N^{2}}{n^{2}}\\log^{2}N+N^{-2\\alpha/d}+N^{2}\\delta_{N}^{2}+\\frac{1}{T}\\left(N^{2}\\log\\frac{N}{\\epsilon}+N\\log\\frac{N^{2}}{\\delta_{N}\\epsilon}\\right)+\\epsilon.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Substituting $\\delta_{N}\\asymp N^{-1-\\alpha/d}$ and $\\epsilon\\asymp N^{-2\\alpha/d}$ yields the desired bound. ", "page_idx": 27}, {"type": "text", "text": "C.2 Anisotropic Besov Space ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.2.1 Definitions and Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For $1\\leq p,q\\leq\\infty$ , directional smoothness $\\alpha=(\\alpha_{1},\\cdot\\cdot\\cdot\\,,\\alpha_{d})\\in\\mathbb{R}_{>0}^{d}$ and $r=\\mathrm{max}_{i\\leq d}\\lfloor\\alpha_{i}\\rfloor+1$ , we define $\\|\\cdot\\|_{B_{p,q}^{\\alpha}}=\\|\\cdot\\|_{L^{p}}+|\\cdot|_{B_{p,q}^{\\alpha}}$ where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f|_{B_{p,q}^{\\alpha}}:=\\left\\{\\left(\\sum_{k=0}^{\\infty}\\left[2^{k}w_{r,p}(f,(2^{-k/\\alpha_{1}},\\cdot\\cdot\\cdot,2^{-k/\\alpha_{d}}))\\right]^{q}\\right)^{1/q}\\right.\\quad q<\\infty}\\\\ {\\left.\\operatorname*{sup}_{k\\geq0}2^{k}w_{r,p}(f,(2^{-k/\\alpha_{1}},\\cdot\\cdot\\cdot,2^{-k/\\alpha_{d}}))\\right.\\quad\\qquad q=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The anisotropic Besov space is defined as ", "page_idx": 27}, {"type": "equation", "text": "$$\nB_{p,q}^{\\alpha}(\\mathcal{X})=\\{f\\in L^{p}(\\mathcal{X})\\mid\\Vert f\\Vert_{B_{p,q}^{\\alpha}}<\\infty\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The definition reduces to the usual Besov space if $\\alpha_{1}=\\cdot\\cdot\\cdot=\\alpha_{d}$ ; see Vyb\u00edral (2006); Triebel (2011) for details. We also write $\\overline{{\\alpha}}=\\operatorname*{max}_{i}\\alpha_{i},\\underline{{\\alpha}}=\\operatorname*{min}_{i}\\alpha_{i}$ and the harmonic mean smoothness as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\alpha}:=\\Big(\\sum_{i=1}^{d}\\alpha_{i}^{-1}\\Big)^{-1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the anisotropic Besov space, we need to redefine the wavelet basis so that the sensitivity to resolution $k\\in\\mathbb{Z}_{\\geq0}$ differs for each component depending on $\\alpha$ . Define the quantities ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|k\\|_{\\underline{{\\alpha}}/\\alpha}:=\\sum_{i=1}^{d}\\lfloor k\\underline{{\\alpha}}/\\alpha_{i}\\rfloor,\\quad I_{k}^{d,\\alpha}:=\\prod_{i=1}^{d}\\{-m,-m+1,\\cdot\\cdot\\cdot,2^{\\lfloor k\\underline{{\\alpha}}/\\alpha_{i}\\rfloor}\\}\\subset{\\mathbb{Z}}^{d}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We then set for each $k\\geq0$ and $\\ell\\in I_{k}^{d,\\alpha}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\omega_{k,\\ell}^{d,\\alpha}(x):=\\omega_{(\\lfloor k\\underline{{\\alpha}}/\\alpha_{1}\\rfloor,\\cdots,\\lfloor k\\underline{{\\alpha}}/\\alpha_{d}\\rfloor),\\ell}^{d}(x)=\\prod_{i=1}^{d}\\iota_{m}(2^{\\lfloor k\\underline{{\\alpha}}/\\alpha_{i}\\rfloor}x_{i}-\\ell_{i}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and take the scaled basis ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\{\\psi_{j}^{\\circ}\\ |\\ j\\in\\mathbb{N}\\}=\\{2^{\\|k\\|_{\\underline{{\\alpha}}/\\alpha}/2}\\omega_{k,\\ell}^{d,\\alpha}\\ |\\ k\\in\\mathbb{Z}_{\\geq0},\\ell\\in I_{k}^{d,\\alpha}\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with the natural hierarchy induced by $k$ . ", "page_idx": 28}, {"type": "text", "text": "The minimax optimal rate for the anisotropic Besov space is equal to n\u22122 \u03b1   +1 (Suzuki and Nitanda, 2021, Theorem 4). Our result for in-context learning is as follows. ", "page_idx": 28}, {"type": "text", "text": "Theorem C.7 (minimax optimality of ICL in anisotropic Besov space). Let $\\alpha\\in\\mathbb{R}_{>0}^{d}$ with $\\widetilde{\\alpha}>1/p$ and $\\mathcal{F}^{\\circ}=\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ . Suppose that $\\mathcal{P}_{\\mathcal{X}}$ has positive Lebesgue density $\\rho_{\\mathcal{X}}$ bounded a b ove and below on $\\mathcal{X}$ . Also suppose that all coefficients are independent and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}]=0,\\quad\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}^{2}]\\lesssim2^{-k\\underline{{\\alpha}}(2+1/\\widetilde{\\alpha})}k^{-2},\\quad\\forall k\\ge0,~\\ell\\in I_{k}^{d,\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then for $n\\gtrsim N\\log N$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim N^{-2\\widetilde{\\alpha}}+\\frac{N\\log N}{n}+\\frac{N^{2}\\log N}{T}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence if $T\\gtrsim n N$ and $N\\asymp n^{\\frac{1}{2\\tilde{\\alpha}+1}}$ , in-context learning achieves the rate $n^{-\\frac{2\\tilde{\\alpha}}{2\\tilde{\\alpha}+1}}\\log n$ which is minimax optimal up to a polylog factor. ", "page_idx": 28}, {"type": "text", "text": "C.2.2 Proof of Theorem C.7 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The overall approach is similar to Appendix C.1. The decay rate of functions in the anisotropic Besov space is characterized by the following result which extends Lemma C.1. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.8 (Suzuki and Nitanda (2021), Lemma 2). If $\\widetilde{\\alpha}>1/p$ and $m>\\overline{{\\alpha}}+1-1/p,$ , a function $f\\in L^{p}(\\mathcal{X})$ is in $M B_{p,q}^{\\alpha}(\\mathcal{X})$ if and only if $f$ can be repre s ented as ", "page_idx": 28}, {"type": "equation", "text": "$$\nf=\\sum_{k\\in\\mathbb{Z}_{\\geq0}^{d}}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\tilde{\\beta}_{k,\\ell}\\omega_{k,\\ell}^{d,\\alpha}(x)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "such that the coefficients satisfy ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\tilde{\\beta}\\|_{b_{p,q}^{\\alpha}}:=\\Bigg[\\sum_{k=0}^{\\infty}\\Bigg[2^{k\\underline{{\\alpha}}-\\|k\\|_{\\alpha/\\alpha}/p}\\bigg(\\sum_{\\ell\\in I_{k}^{d,\\alpha}}|\\tilde{\\beta}_{k,\\ell}|^{p}\\bigg)^{1/p}\\Bigg]^{q}\\Bigg]^{1/q}\\lesssim\\|f\\|_{B_{p,q}^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, the two norms $\\|\\tilde{\\beta}\\|_{b_{p,q}^{\\alpha}}$ and $\\|f\\|_{B_{p,q}^{\\alpha}}$ are equivalent. ", "page_idx": 28}, {"type": "text", "text": "We again select all B-splines $(\\omega_{K,\\ell}^{d,\\alpha})_{\\ell\\in I_{K}^{d}}$ at each resolution $K$ to approximate the target functions. By repeatedly applying the refinement equation for one-dimensional wavelets as many times as needed to each dimension separately, we may express any $\\mathbf{B}$ -spline at a lower resolution $k<K$ as a linear combination of $(\\omega_{K,\\ell}^{d,\\alpha})_{\\ell\\in I_{K}^{d}}$ similarly to Lemma C.2. See Proposition C.10 for details. We thus have ", "page_idx": 28}, {"type": "equation", "text": "$$\nN=|I_{K}^{d,\\alpha}|=\\prod_{i=1}^{d}(m+1+2^{\\lfloor K\\underline{{\\alpha}}/\\alpha_{i}\\rfloor})\\asymp2^{\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\|k\\|_{\\underline{{\\alpha}}/\\alpha}=k\\underline{{\\alpha}}/\\widetilde{\\alpha}+O_{k}(1)$ always holds, it also follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\bar{N}=\\sum_{k=0}^{K}\\lvert I_{k}^{d,\\alpha}\\rvert+1\\asymp\\sum_{k=0}^{K}2^{\\lVert k\\rVert_{\\alpha/\\alpha}}\\asymp\\sum_{k=0}^{K}(2^{\\underline{{\\alpha}}/\\tilde{\\alpha}})^{k}\\asymp2^{K}\\underline{{\\alpha}}/\\tilde{\\alpha}\\asymp N\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and similarly $N\\asymp N$ . Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{j=\\underline{{N}}}^{\\bar{N}}\\psi_{j}^{\\circ}(x)^{2}\\leq2^{\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\omega_{K,\\ell}^{d,\\alpha}(x)^{2}\\leq2^{\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}\\asymp N\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and the scaled coefficients decay in average as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(\\displaystyle\\frac{1}{|I_{k}^{d,\\alpha}|}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}|\\beta_{k,\\ell}|^{p}\\bigg)^{1/p}\\lesssim\\bigg(\\displaystyle\\prod_{i=1}^{d}2^{\\lfloor k\\underline{{\\alpha}}/\\alpha_{i}\\rfloor}\\bigg)^{-1/p}2^{-\\|k\\|_{\\alpha/\\alpha}/2}\\bigg(\\sum_{\\ell\\in I_{k}^{d,\\alpha}}|\\tilde{\\beta}_{k,\\ell}|^{p}\\bigg)^{1/p}}&{}\\\\ {\\quad}&{\\lesssim2^{-\\|k\\|_{\\underline{{\\alpha}}/\\alpha}/2-k}\\underline{{\\alpha}}\\|f\\|_{B_{p,q}^{\\alpha}}}\\\\ {\\quad}&{\\lesssim N^{-(\\widetilde{\\alpha}+1/2)}\\|f\\|_{B_{p,q}^{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For Assumption 2, we can check that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F_{\\beta}^{\\circ}\\|_{L^{\\infty}(\\mathcal{P}_{x})}\\leq\\displaystyle\\sum_{k=0}^{\\infty}\\left\\|\\sum_{\\ell\\in I_{k}^{\\ell,\\alpha}}\\beta_{k,\\ell}\\cdot2^{\\|k\\|_{\\alpha/\\alpha}/2}\\omega_{k,\\ell}^{d,\\alpha}\\right\\|_{L^{\\infty}(\\mathcal{P}_{x})}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{k=0}^{\\infty}2^{(1/2+1/p)\\|k\\|_{\\alpha}/\\alpha}\\left(\\frac{1}{|I_{k}^{d,\\alpha}|}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}|\\beta_{k,\\ell}|^{p}\\right)^{1/p}}\\\\ &{\\qquad\\lesssim\\displaystyle\\sum_{k=0}^{\\infty}2^{(1/2+1/p)\\|k\\|_{\\alpha/\\alpha}}\\cdot2^{-\\|k\\|_{\\alpha/\\alpha}/2-k}}\\\\ &{\\qquad\\lesssim\\left(1-2^{\\alpha/\\widetilde\\alpha(1/p-\\widetilde\\alpha)}\\right)^{-1}=:B}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and for a resolution cutoff $K>0$ , $\\bar{N}\\asymp2^{\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}$ the truncation error satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F_{\\beta}^{\\circ}-F_{\\beta,\\bar{N}}^{\\circ}\\|_{L^{2}(\\mathcal{P}_{\\mathcal{X}})}^{2}\\lesssim\\displaystyle\\sum_{k=K+1}^{\\infty}\\displaystyle\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\beta_{k,\\ell}^{2}\\lesssim\\displaystyle\\sum_{k=K+1}^{\\infty}|I_{k}^{d,\\alpha}|^{1-2/p}\\bigg(\\displaystyle\\sum_{\\ell\\in I_{k}^{d,\\alpha}}|\\beta_{k,\\ell}|^{p}\\bigg)^{2/p}}\\\\ &{\\lesssim\\displaystyle\\sum_{k=K+1}^{\\infty}2^{\\|k\\|_{\\alpha/\\alpha}}\\cdot2^{-\\|k\\|_{\\alpha/\\alpha}-2k\\underline{{\\alpha}}}\\asymp2^{-2K}\\tilde{\\alpha}\\asymp N^{-2\\tilde{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus we may set $r=1/2,s=\\widetilde{\\alpha}$ and take the variance decay rate (20) as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}^{2}]\\lesssim2^{-\\|k\\|_{\\underline{{\\alpha}}/\\alpha}(2\\widetilde{\\alpha}+1)}k^{-2}\\asymp2^{-k\\underline{{\\alpha}}(2+1/\\widetilde{\\alpha}))}k^{-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For boundedness of $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ , we use the following result which is also obtained from Proposition C.10 by considering resolution vectors $(\\lfloor k\\underline{{\\alpha}}/\\alpha_{1}\\rfloor,\\cdot\\cdot\\cdot\\,,\\lfloor k\\underline{{\\alpha}}/\\alpha_{d}\\rfloor)$ and $(\\lfloor k^{\\prime}\\underline{{\\alpha}}/\\alpha_{1}\\rfloor,\\cdot\\cdot\\cdot\\,,\\lfloor k^{\\prime}\\underline{{\\alpha}}/\\alpha_{d}\\rfloor)$ . Corollary C.9. For any $0\\le k<k^{\\prime}$ there exists constants $\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\geq0$ for $\\ell\\in I_{k}^{d,\\alpha}$ , $\\ell^{\\prime}\\in I_{k^{\\prime}}^{d,\\alpha}$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\beta_{k,\\ell}2^{\\|k\\|_{\\underline{{\\alpha}}/\\alpha}/2}\\omega_{k,\\ell}^{d,\\alpha}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d,\\alpha}}\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}2^{\\|k^{\\prime}\\|_{\\underline{{\\alpha}}/\\alpha}/2}\\omega_{k^{\\prime},\\ell^{\\prime}}^{d,\\alpha},\\quad\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}=\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\beta_{k,\\ell^{\\prime},\\ell^{\\prime}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "holds for all $(\\beta_{k,\\ell})_{\\ell\\in I_{k}^{d,\\alpha}}$ . Moreover, it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\le2^{(\\|k\\|_{\\underline{{\\alpha}}/\\alpha}-\\|k^{\\prime}\\|_{\\underline{{\\alpha}}/\\alpha})/2},\\quad\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d,\\alpha}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\le2^{(\\|k^{\\prime}\\|_{\\underline{{\\alpha}}/\\alpha}-\\|k\\|_{\\underline{{\\alpha}}/\\alpha})/2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We apply Corollary C.9 to refine all components of $F_{\\beta,\\bar{N}}^{\\circ}$ to resolution $K$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{\\beta,\\bar{N}}^{\\circ}=\\sum_{k=0}^{K}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\beta_{k,\\ell}2^{\\Vert k\\Vert_{\\alpha/\\alpha}/2}\\omega_{k,\\ell}^{d,\\alpha}=\\sum_{k=0}^{K}\\sum_{\\ell^{\\prime}\\in I_{K}^{d,\\alpha}}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}\\beta_{k,\\ell}2^{\\Vert K\\Vert_{\\alpha/\\alpha}/2}\\omega_{K,\\ell^{\\prime}}^{d,\\alpha}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence it follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\beta}[\\bar{\\beta}_{K,\\ell}^{2}]=\\sum_{k=0}^{K}\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}^{2}\\mathbb{E}_{\\beta}[\\beta_{k,\\ell}^{2}]\\lesssim\\sum_{k=0}^{K}\\Bigg(\\sum_{\\ell\\in I_{k}^{d,\\alpha}}\\gamma_{k,K,\\ell,\\ell^{\\prime}}\\Bigg)^{2}2^{-k\\underline{{\\alpha}}(2+1/\\tilde{\\alpha})}k^{-2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{k=0}^{K}2^{\\|k\\|_{\\underline{{\\alpha}}/\\alpha}-\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}\\cdot2^{-k\\underline{{\\alpha}}(2+1/\\tilde{\\alpha})}k^{-2}}\\\\ {\\displaystyle\\lesssim2^{-\\|K\\|_{\\underline{{\\alpha}}/\\alpha}}\\cdot\\sum_{k=0}^{K}2^{-2k\\underline{{\\alpha}}}k^{-2}\\asymp N^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and we again conclude that $\\operatorname{Tr}(\\Sigma_{\\bar{\\beta},N})$ is uniformly bounded. ", "page_idx": 30}, {"type": "text", "text": "The rest of the proof proceeds similarly to the ordinary Besov space. ", "page_idx": 30}, {"type": "text", "text": "C.3 Wavelet Refinement ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this subsection, we present and prove an auxiliary result concerning the refinement of $\\mathbf{B}$ -spline wavelets and the recurrence relations satisfied by their coefficient sequences. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.10. For any $k,k^{\\prime}\\in\\mathbb{Z}_{\\geq0}^{d}$ such that $k^{\\prime}-k\\in\\mathbb{Z}_{\\geq0}^{d}$ there exists constants $\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\geq0$ for $\\ell\\in I_{k}^{d}$ , $\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}2^{\\Vert k\\Vert_{1}/2}\\omega_{k,\\ell}^{d}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}2^{\\Vert k^{\\prime}\\Vert_{1}/2}\\omega_{k^{\\prime},\\ell^{\\prime}}^{d},\\quad\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}=\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\beta_{k,\\ell^{\\prime}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds for all $(\\beta_{k,\\ell})_{\\ell\\in I_{k}^{d}}$ . Moreover, it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\leq2^{-\\|k^{\\prime}-k\\|_{1}/2},\\quad\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\leq2^{\\|k^{\\prime}-k\\|_{1}/2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We proceed by induction on $\\|\\boldsymbol k^{\\prime}-\\boldsymbol k\\|_{1}$ . When $k^{\\prime}=k+e_{j}$ for some $1\\leq j\\leq d$ , we can refine each $\\omega_{k,\\ell}^{d}$ using equation (2.21) of D\u02dcung (2011b) as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\omega_{k,\\ell}^{d}(x)=\\displaystyle\\prod_{i=1}^{d}\\iota_{m}(2^{k_{i}}x_{i}-\\ell_{i})}\\\\ &{\\qquad\\qquad=2^{-m+1}\\displaystyle\\prod_{i\\neq j}\\iota_{m}(2^{k_{i}}x_{i}-\\ell_{i})\\displaystyle\\sum_{r=0}^{m}\\binom{m}{r}\\iota_{m}\\left(2^{k_{j}+1}x_{j}-2\\ell_{j}-r+\\frac{m}{2}\\right)}\\\\ &{\\qquad\\qquad=2^{-m+1}\\displaystyle\\sum_{r=0}^{m}\\binom{m}{r}\\omega_{k+e_{j},\\ell+(\\ell_{j}+r-\\frac{m}{2})e_{j}}^{d}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\begin{array}{r}{\\ell+(\\ell_{j}+r-\\frac{m}{2})e_{j}}\\end{array}$ matches a given location vector $\\ell^{\\prime}\\in I_{k+e_{j}}^{d}$ if and only if $\\ell_{i}=\\ell_{i}^{\\prime}\\;(i\\neq j)$ and $\\begin{array}{r}{\\ell_{j}^{\\prime}=2\\ell_{j}+r-\\frac{m}{2}}\\end{array}$ , comparing coefficients in (21) yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma_{k,k+e_{j},\\ell,\\ell^{\\prime}}=2^{-m+1/2}{\\bf1}_{\\{\\ell_{i}=\\ell_{i}^{\\prime}(i\\neq j)\\}}{\\binom{m}{\\ell_{j}^{\\prime}-2\\ell_{j}+\\frac{m}{2}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, ${\\mathbf{1}}_{A}$ denotes the indicator function for condition $A$ . It follows that $\\gamma_{k,k+e_{j},\\ell,\\ell^{\\prime}}\\geq0$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k+e_{j},\\ell,\\ell^{\\prime}}\\leq\\displaystyle\\sum_{\\ell_{j}\\in\\mathbb{Z}}2^{-m+1/2}\\binom{m}{\\ell_{j}^{\\prime}-2\\ell_{j}+\\frac{m}{2}}\\leq2^{-1/2},}\\\\ &{\\displaystyle\\sum_{\\ell^{\\prime}\\in I_{k+e_{j}}^{d}}\\gamma_{k,k+e_{j},\\ell,\\ell^{\\prime}}\\leq\\displaystyle\\sum_{\\ell_{j}^{\\prime}\\in\\mathbb{Z}}2^{-m+1/2}\\binom{m}{\\ell_{j}^{\\prime}-2\\ell_{j}+\\frac{m}{2}}\\leq2^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "by considering parities. ", "page_idx": 30}, {"type": "text", "text": "Now suppose the claim holds for a fixed difference $\\|\\boldsymbol{k}^{\\prime}-\\boldsymbol{k}\\|_{1}$ . Applying the above derivation to further refine resolution $k^{\\prime}$ to $k^{\\prime\\prime}=k^{\\prime}+e_{j}$ for arbitrary $j$ gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in I_{k}^{d}}\\beta_{k,\\ell}2^{\\|k\\|_{1}/2}\\omega_{k,\\ell}^{d}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}2^{\\|k^{\\prime}\\|_{1}/2}\\omega_{k^{\\prime},\\ell^{\\prime}}^{d}=\\sum_{\\ell^{\\prime\\prime}\\in I_{k^{\\prime}+1}^{d}}\\bar{\\beta}_{k^{\\prime}+1,\\ell^{\\prime\\prime}}2^{(\\|k^{\\prime}\\|_{1}+1)/2}\\omega_{k^{\\prime}+e_{j},\\ell^{\\prime\\prime}}^{d}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\bar{\\beta}_{k^{\\prime}+e_{j},\\ell^{\\prime\\prime}}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}2^{-m+1/2}{\\bf1}_{\\{\\ell_{i}^{\\prime}=\\ell_{i}^{\\prime\\prime}(i\\neq j)\\}}\\binom{m}{\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}}\\bar{\\beta}_{k^{\\prime},\\ell^{\\prime}}}}\\\\ &{}&{=\\sum_{\\ell\\in I_{k}^{d}}\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}2^{-m+1/2}{\\bf1}_{\\{\\ell_{i}^{\\prime}=\\ell_{i}^{\\prime\\prime}(i\\neq j)\\}}\\binom{m}{\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\beta_{k,\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence we obtain the recurrence relation ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma_{k,k^{\\prime}+e_{j},\\ell,\\ell^{\\prime\\prime}}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}2^{-m+1/2}\\mathbf{1}_{\\{\\ell_{i}^{\\prime}=\\ell_{i}^{\\prime\\prime}\\,(i\\neq j)\\}}\\left(\\!\\!\\begin{array}{c}{{m}}\\\\ {{\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}}}\\end{array}\\!\\!\\right)\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "from which we verify that $\\gamma_{k,k^{\\prime}+e_{j},\\ell,\\ell^{\\prime\\prime}}\\geq0$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime}+e_{j},\\ell,\\ell^{\\prime\\prime}}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}2^{-m+1/2}{\\bf1}_{\\{\\ell_{i}^{\\prime}=\\ell_{i}^{\\prime\\prime}(i\\neq j)\\}}\\binom{m}{\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}}\\sum_{\\ell\\in I_{k}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}}}\\\\ &{}&{\\le2^{-m+1/2-\\|k^{\\prime}-k\\|_{1}/2}\\sum_{\\ell_{j}^{\\prime}\\in{\\cal Z}}\\binom{m}{\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}}=2^{-(\\|k^{\\prime}-k\\|_{1}+1)/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and furthermore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell^{\\prime\\prime}\\in I_{k^{\\prime}+e_{j}}^{d}}\\gamma_{k,k^{\\prime}+e_{j},\\ell,\\ell^{\\prime\\prime}}=\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\sum_{\\ell^{\\prime\\prime}\\in I_{k^{\\prime}+e_{j}}^{d}}2^{-m+1/2}\\mathbf{1}_{\\{\\ell_{i}^{\\prime}=\\ell_{i}^{\\prime\\prime}(i\\neq j)\\}}\\left(\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}\\right)\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}}\\\\ &{\\leq2^{-m+1/2}\\displaystyle\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\sum_{\\ell_{j}^{\\prime\\prime}\\in\\mathbb{Z}}\\left(\\ell_{j}^{\\prime\\prime}-2\\ell_{j}^{\\prime}+\\frac{m}{2}\\right)\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}}\\\\ &{\\leq2^{1/2}\\displaystyle\\sum_{\\ell^{\\prime}\\in I_{k^{\\prime}}^{d}}\\gamma_{k,k^{\\prime},\\ell,\\ell^{\\prime}}\\leq2^{(\\|k^{\\prime}-k\\|_{1}+1)/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 31}, {"type": "text", "text": "C.4 Proof of Corollary 4.8 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In order to approximate arbitrary $\\psi_{j}^{\\circ}\\in\\mathbb{U}(B_{p,q}^{\\tau}(\\mathcal{X}))$ , we need the following construction instead of Lemma C.5. Note that $N^{\\prime}$ corresponds to the number of B-splines used to approximate the target function and can be freely chosen to match the desired error, which however affects the covering number of ${\\mathcal F}_{N}$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma C.11 (Suzuki (2019), Proposition 1). Set $m\\in\\mathbb{N}$ , $m>\\tau+2-1/p$ and $\\nu=(p\\tau-d)/2d.$ For all $N^{\\prime}\\in\\mathbb{N}$ sufficiently large and $\\epsilon=N^{\\prime-\\tau/d}(\\log N^{\\prime})^{-1}$ , for any $f^{\\circ}\\in\\mathbb{U}(B_{p,q}^{\\tau}(\\mathcal{X}))$ there exists a ReLU network $\\tilde{f}\\in\\mathcal{F}_{\\mathrm{DNN}}(L,W,S,M)$ with ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{L=3+2\\left\\lceil\\log_{2}\\left(3^{d\\vee m}(1+d m^{-1/2}(2e)^{m+1})\\epsilon^{-1}\\right)+5\\right\\rceil\\lceil\\log_{2}(d\\vee m)\\rceil,}}\\\\ {{W=N^{\\prime}W_{0},\\qquad S=((L-1)W_{0}^{2}+1)N^{\\prime},\\qquad M=O(N^{\\prime1/\\nu+1/d})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "satisfying $\\|f^{\\circ}-\\tilde{f}\\|_{L^{\\infty}(\\mathcal{X})}\\le N^{\\prime-\\tau/d}$ . ", "page_idx": 31}, {"type": "text", "text": "Also note that from Assumption 1 it follows that $\\|\\psi_{j}\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}\\le C_{\\infty}N^{1/2}$ . Setting $N^{\\prime}\\asymp\\delta_{N}^{-d/\\tau}$ and applying the covering number bound in Lemma C.6, after some algebra we obtain the following counterpart to Lemma 4.4. ", "page_idx": 31}, {"type": "text", "text": "Corollary C.12. For any $\\delta_{N}>0$ , Assumption 3 is satisfied by taking ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\mathcal F_{N}}=\\{\\Pi_{B_{N}^{\\prime}}\\circ\\phi\\;|\\;\\phi=(\\phi_{j})_{j=1}^{N},\\phi_{j}\\in{\\mathcal F}_{\\mathrm{DNN}}(L,W,S,M)\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $B_{N}^{\\prime}=C_{\\infty}N^{1/2}$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\nL=O(\\log\\delta_{N}^{-1}),\\quad W=O(\\delta_{N}^{-d/\\tau}),\\quad S=O(\\delta_{N}^{-d/\\tau}\\log\\delta_{N}^{-1}),\\quad\\log M=O(\\log\\delta_{N}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also, the metric entropy of ${\\mathcal F}_{N}$ is bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{V}(\\mathcal{F}_{N},\\Vert\\cdot\\Vert_{L^{\\infty}},\\epsilon)\\lesssim N\\delta_{N}^{-d/\\tau}\\log\\frac{1}{\\delta_{N}}\\left(\\log\\frac{1}{\\epsilon}+\\log^{2}\\frac{1}{\\delta_{N}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then by combining with Lemma 3.3 and Proposition 3.2 via Theorem 3.1, it follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{V}(\\mathcal{T}_{N},\\Vert\\cdot\\Vert_{L^{\\infty}},\\epsilon)\\lesssim N^{2}\\log\\frac{N}{\\epsilon}+N\\delta_{N}^{-d/\\tau}\\log\\frac{1}{\\delta_{N}}\\left(\\log\\frac{N}{\\epsilon}+\\log^{2}\\frac{1}{\\delta_{N}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}(\\widehat\\Theta)\\lesssim\\displaystyle\\frac{N}{n}\\log N+\\frac{N^{2}}{n^{2}}\\log^{2}N+N^{-2\\alpha/d}+N^{2}\\delta_{N}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{N^{2}}{T}\\log\\frac{N}{\\epsilon}+\\frac{N}{T}\\delta_{N}^{-d/\\tau}\\log\\frac{1}{\\delta_{N}}\\left(\\log\\frac{N}{\\epsilon}+\\log^{2}\\frac{1}{\\delta_{N}}\\right)+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Substituting $\\delta_{N}\\asymp N^{-1-\\alpha/d}$ and $\\epsilon\\asymp N^{-2\\alpha/d}$ concludes the desired bound. ", "page_idx": 32}, {"type": "text", "text": "D Details on Sequential Input ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "D.1 Definitions and Results ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "$\\gamma$ -smooth class. We first define the $\\gamma$ -smooth function class introduced by Okumoto and Suzuki (2022). Let $r\\in\\mathbb{Z}_{0}^{d\\times\\infty}$ Z0d\u00d7\u221eand s \u2208 N\u00af0d\u00d7\u221e, where N\u00af = N \u222a{0} and the subscript 0 indicates restriction to the subset of elements with a finite number of nonzero components. Consider the orthonormal basis $(\\psi_{r})_{r}$ of $L^{2}([0,1]^{d\\times\\infty})$ given as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\psi_{r}(x)=\\prod_{i\\in\\,2}\\prod_{j=1}^{d}\\psi_{r_{i j}}(x_{i j}),\\quad\\psi_{r_{i j}}(x_{i j})=\\left\\{\\!\\!\\!\\begin{array}{l l}{{\\sqrt{2}\\cos(2\\pi r_{i j}x_{i j})}}&{{r_{i j}<0}}\\\\ {{1}}&{{r_{i j}=0\\;.}}\\\\ {{\\sqrt{2}\\sin(2\\pi r_{i j}x_{i j})}}&{{r_{i j}>0}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The frequency $s$ component $\\delta_{s}(f)$ of $f\\in L^{2}([0,1]^{d\\times\\infty})$ is defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\delta_{s}(f):=\\sum_{\\left\\lfloor2^{s_{i j}-1}\\right\\rfloor\\leq\\left\\vert r_{i j}\\right\\vert<2^{s_{i j}}}\\langle f,\\psi_{r}\\rangle\\psi_{r}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For a monotonically nondecreasing function $\\gamma:\\bar{\\mathbb{N}}_{0}^{d\\times\\infty}\\rightarrow\\mathbb{R}$ and $p\\geq2,q\\geq1$ , the $\\gamma.$ -smooth norm and function class are defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|f\\|_{\\mathcal{F}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}})}:=\\left(\\sum_{s\\in\\bar{\\mathbb{N}}_{0}^{d\\times\\infty}}2^{q\\gamma(s)}\\|\\delta_{s}(f)\\|_{p,\\mathcal{P}_{\\mathcal{X}}}^{q}\\right)^{1/q}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{p,q}^{\\gamma}(\\mathcal{P}_{\\boldsymbol{x}}):=\\big\\{f\\in L^{2}([0,1]^{d\\times\\infty})\\ |\\ ||f||_{\\mathcal{F}_{p,q}^{\\gamma}(\\mathcal{P}_{\\boldsymbol{x}})}<\\infty\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The $\\gamma$ -smooth class over finite-dimensional input space $[0,1]^{d\\times m}$ is similarly defined. ", "page_idx": 32}, {"type": "text", "text": "In particular, we consider two specific cases of $\\gamma$ for the component-wise smoothness parameter $\\alpha\\in\\mathbb{R}_{>0}^{d\\times\\infty}$ , for which we also define the corresponding degrees of smoothness $\\alpha^{\\dagger}\\in\\mathbb{R}_{>0}$ . Denote by $(\\tilde{\\alpha}_{j})_{j=1}^{\\infty}$ all components of sorted by ascending magnitude. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Mixed smoothness: \u03b3(s) = \u27e8\u03b1, s\u27e9, \u03b1\u2020 = \u03b1\u02dc1 = maxi,j \u03b1ij. \u2022 Anisotropic smoothness: $\\begin{array}{r}{\\gamma(s)=\\operatorname*{max}_{i,j}\\alpha_{i j}s_{i j},\\alpha^{\\dagger}=(\\sum_{i,j}\\alpha_{i j}^{-1})^{-1}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Furthermore, the weak $l^{\\eta}$ -norm of $\\alpha$ is defined as $\\|\\alpha\\|_{w l^{\\eta}}:=\\operatorname*{sup}_{j}j^{\\eta}\\tilde{\\alpha}_{j}^{-1}$ for $\\eta>0$ . ", "page_idx": 32}, {"type": "text", "text": "Piecewise $\\gamma$ -smooth class. The piecewise $\\gamma.$ -smooth class is an extension of the $\\gamma$ -smooth class allowing for arbitrary bounded permutations of the tokens of an input (Takakura and Suzuki, 2023). For a threshold $V\\,\\in\\,\\mathbb{N}$ and an index set $\\Lambda$ , let $\\{\\Omega_{\\lambda}\\}_{\\lambda\\in\\Lambda}$ be a disjoint partition of $\\operatorname{supp}\\mathcal{P}_{\\mathcal{X}}$ and $\\{\\pi_{\\lambda}\\}_{\\lambda\\in\\Lambda}$ a set of bijections from $[2V+1]$ to $\\left[-V:V\\right]$ . Further define the permutation operator $\\bar{\\Pi}:\\mathrm{supp}\\,\\mathcal{P}_{\\mathcal{X}}\\to\\mathbb{R}^{d\\times(2V+1)}$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Pi(x)=\\left(x_{\\pi_{\\lambda}(1)},\\cdot\\cdot\\cdot\\,,x_{\\pi_{\\lambda}(2V+1)}\\right)\\!,\\quad{\\mathrm{if~}}x\\in\\Omega_{\\lambda}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then the piecewise $\\gamma$ -smooth function class is defined as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{P}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}}):=\\big\\{g=f\\circ\\Pi\\ |\\ f\\in\\mathcal{F}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}}),\\|g\\|_{\\mathcal{P}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}})}<\\infty\\big\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|g\\|_{\\mathcal{P}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}})}:=\\Bigg(\\sum_{\\substack{s\\in\\bar{\\mathbb{N}}_{0}^{d\\times[-V:V]}}}2^{q\\gamma(s)}\\|\\delta_{s}(f)\\circ\\Pi\\|_{p,\\mathcal{P}_{\\mathcal{X}}}^{q}\\Bigg)^{1/q}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, we state the set of assumptions inherited from Takakura and Suzuki (2023). In particular, the importance function makes precise a notion of relative importance between tokens which is preserved by permutations. ", "page_idx": 33}, {"type": "text", "text": "Assumption 5 (smoothness and importance function). $1<q\\leq2$ and: ", "page_idx": 33}, {"type": "text", "text": "1. The smoothness parameter $\\alpha$ satisfies $\\|\\alpha\\|_{w l^{\\eta}}\\leq1$ and $\\alpha_{i j}=\\Omega(|i|^{\\eta})$ for some $\\eta>1$ . For mixed smoothness, we also require $\\tilde{\\alpha}_{1}<\\tilde{\\alpha}_{2}$ . 2. There exists a shift-equivariant map $\\mu\\,:\\,\\operatorname{supp}\\mathcal{P}_{\\mathcal{X}}\\,\\to\\,\\mathbb{R}^{\\infty}$ such that $\\mu_{0}\\;\\in\\;\\mathbb{U}(\\mathcal{F}_{\\infty,q}^{\\gamma})$ , $\\|\\mu_{0}\\|\\leq1$ and $\\Omega_{\\lambda}=\\{x\\in\\operatorname{supp}\\mathcal{P}_{X}\\mid\\mu(x)_{\\pi_{\\lambda}(1)}>\\cdot\\cdot\\cdot>\\mu(x)_{\\pi_{\\lambda}}$ \u03bb(2V +1)} for all $\\lambda\\in\\Lambda$ . $\\mu$ is moreover well-separated, that is $\\mu(x)_{\\pi_{\\lambda}(v)}-\\mu(x)_{\\pi_{\\lambda}(v+1)}\\geq C_{\\mu}v^{-\\varrho}$ for $C_{\\mu},\\varrho>0$ . ", "page_idx": 33}, {"type": "text", "text": "We focus on parameter ranges $1<q\\leq2$ and $\\eta>1$ strictly for simplicity of presentation, but the cases $q=1,q>2$ and $\\eta>0$ can be handled with some more analysis. Note that $\\eta>1$ ensures $\\alpha^{\\dagger}>\\bar{0}$ for anisotropic smoothness. ", "page_idx": 33}, {"type": "text", "text": "Additionally, the assumption pertaining to our ICL setup is stated as follows. ", "page_idx": 33}, {"type": "text", "text": "Assumption 6. For $r\\in\\mathbb{Z}_{0}^{d\\times\\infty}$ the coefficients $\\beta_{r}$ corresponding to $\\psi_{r}$ are independent and satisfy for $s\\in\\bar{\\mathbb{N}}_{0}^{d\\times\\infty}$ such that the frequency component $\\delta_{s}(f)$ contains the element $\\psi_{r}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}[\\beta_{r}]=0,\\quad\\mathbb{E}_{\\beta}[\\beta_{r}^{2}]\\lesssim2^{-(2+1/\\alpha^{\\dagger})\\gamma(s)}\\gamma(s)^{-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Also $\\begin{array}{r}{\\sum_{\\Psi,N}\\asymp\\mathbf{I}_{N}}\\end{array}$ holds, for example $\\mathcal{P}_{\\mathcal{X}}$ is bounded above and below with respect to the product measure $\\lambda^{d\\times\\infty}$ on $\\mathcal{B}([0,1]^{d\\times\\infty})$ of the uniform measure $\\lambda$ on $\\mathcal{B}([0,1])$ . ", "page_idx": 33}, {"type": "text", "text": "We then obtain the following result for ICL with transformers: ", "page_idx": 33}, {"type": "text", "text": "Theorem D.1 (minimax optimality of ICL for sequential input). Let $\\mathcal{F}^{\\circ}=\\{f\\in\\mathbb{U}(\\mathcal{P}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}}))$ | $\\|f\\|_{L^{\\infty}(\\mathcal{P}_{x})}\\,\\le\\,B\\}$ for some $B~>~0$ where $\\gamma$ corresponds to mixed or anisotropic smoothness. Suppose Assumptions 5 and $^{6}$ hold. Then for $n\\gtrsim N\\log N$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bar{R}(\\widehat{\\Theta})\\lesssim N^{-2\\alpha^{\\dagger}}+\\frac{N\\log N}{n}+\\frac{N^{2\\vee(1+1/\\alpha^{\\dagger})}\\operatorname{polylog}(N)}{T}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence if $T\\gtrsim n N^{1\\vee1/\\alpha^{\\dagger}}$ and $N\\asymp n^{\\frac{1}{2\\alpha^{\\dagger}+1}}$ , ICL achieves the rate $n^{-\\frac{2\\alpha^{\\dagger}}{2\\alpha^{\\dagger}+1}}{\\mathrm{~polylog}}(n).$ . ", "page_idx": 33}, {"type": "text", "text": "D.2 Proof of Theorem D.1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Since the system $(\\psi_{r})_{r}$ is orthonormal, we may take $\\underline{{{N}}}=1,\\bar{N}=N$ following Remark 2.1. We mainly utilize the following approximation and coverin\u00afg number bounds. ", "page_idx": 33}, {"type": "text", "text": "Theorem D.2 (Takakura and Suzuki (2023), Theorem 4.5). For a function $F^{\\circ}\\in\\mathbb{U}(\\mathcal{P}_{p,q}^{\\gamma}(\\mathcal{P}_{\\mathcal{X}}))$ , $\\|F^{\\circ}\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}\\leq B$ and any $K>0$ , there exists a transformer $\\widehat{F}\\in\\mathcal{F}_{\\mathrm{TF}}(J,U,D,H,L,\\operatorname{\\dot{W}},S,M)$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\widehat{F}_{0}-F^{\\circ}\\|_{L^{2}(\\mathcal{P}_{\\mathcal{X}})}\\lesssim2^{-K},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J\\lesssim K^{1/\\eta},\\quad\\log U\\lesssim\\log K\\vee\\log V,\\quad D\\lesssim K^{2(1+\\varrho)/\\eta}\\log V,\\quad H\\lesssim(\\log K)^{1/\\eta},}\\\\ &{L\\lesssim K^{2},\\quad W\\lesssim2^{K/\\alpha^{\\dagger}}K^{1/\\eta},\\quad S\\lesssim2^{K/\\alpha^{\\dagger}}K^{2+2/\\eta},\\quad\\log M\\lesssim K\\vee\\log\\log V.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Theorem D.3 (Takakura and Suzuki (2023), Theorem 5.3). For $\\epsilon>0$ and $B\\geq1$ it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}(\\mathcal{F}_{\\mathrm{TF}}(J,U,D,H,L,W,S,M),\\Vert\\cdot\\Vert_{L^{\\infty}},\\epsilon)\\lesssim J^{3}L(S+H D^{2})\\log\\left(\\frac{D H L W M}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To analyze the decay rate in the $\\gamma$ -smooth class, we approximate a function $f\\in L^{2}([0,1]^{d\\times\\infty})$ by the partial sum of its frequency components up to \u2018resolution\u2019 $K$ , measured via the $\\gamma$ function: ", "page_idx": 34}, {"type": "equation", "text": "$$\nR_{K}(f):=\\sum_{\\gamma(s)<K}\\delta_{s}(f).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The basis functions $\\psi_{r}$ are thus ordered primarily ordered by increasing $\\gamma(s)$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma D.4 (Okumoto and Suzuki (2022), Lemma 17). For $1\\le q\\le2$ it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|f-R_{K}(f)\\|_{L^{2}({\\mathcal{P}}_{\\mathcal{X}})}\\lesssim2^{-K}\\|f\\|_{{\\mathcal{F}}_{p,q}^{\\gamma}({\\mathcal{P}}_{\\mathcal{X}})}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that if $\\gamma(s)<K$ then $s_{i j}<K/a_{i j}\\lesssim K/\\left|i\\right|^{\\eta}$ for all $i,j$ for both types of smoothness and so $\\lVert s\\rVert_{0}\\lesssim d K^{1/\\eta}$ . In addition, the number of basis functions $\\psi_{r}$ used in the sum for $\\delta_{s}(f)$ is exactly $\\begin{array}{r}{2^{\\Vert s\\Vert_{1}}=\\prod_{i,j}2^{s_{i j}}}\\end{array}$ . Theorem D.3 of Takakura and Suzuki (2023) shows that the number of basis elements used in the sum $R_{K}(f)$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\nN\\asymp\\sum_{\\gamma(s)<K}2^{\\|s\\|_{1}}\\lesssim2^{K/\\alpha^{\\dagger}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for both mixed and anisotropic smoothness. Hence the $N$ -term approximation error decays as $N^{-\\alpha^{\\dagger}}$ so that the choice $s=\\alpha^{\\dagger}$ leading to the assumed variance bound $N^{-2\\alpha^{\\dagger}-1}\\asymp2^{-(2+1/\\dot{\\alpha}^{\\dagger})K}$ in (22) is justified. Moreover for large $K$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{\\gamma(s)<K}\\sum_{\\lfloor2^{s_{i j}}-1\\rfloor\\leq\\lfloor r_{i j}\\rfloor<2^{s_{i j}}}\\|\\psi_{r}\\|_{L^{\\infty}(\\mathcal{P}_{\\mathcal{X}})}^{2}\\leq\\sum_{\\gamma(s)<K}2^{\\|s\\|_{1}}(\\sqrt{2}^{\\|s\\|_{0}})^{2}\\lesssim2^{K/\\alpha^{\\dagger}+O(K^{1/\\eta})}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since $\\|r\\|_{0}\\,=\\,\\|s\\|_{0}$ , so that (2) of Assumption 1 is satisfied with $r\\,=\\,1/2$ . The second part of Assumption 1 holds since $\\left(\\psi_{r}\\right)_{r\\in\\mathbb{Z}_{0}^{d\\times\\infty}}$ is orthonormal w.r.t. $\\lambda^{d\\times\\infty}$ . Furthermore, the discussion thus far immediately extends to the piecewise $\\gamma$ -smooth class for any partition $\\{\\Omega_{\\lambda}\\}_{\\lambda\\in\\Lambda}$ by composing with the permutation operator $\\Pi$ . ", "page_idx": 34}, {"type": "text", "text": "We proceed to use Theorem D.2 to approximate each basis function $\\psi_{r}\\circ\\Pi$ up to resolution $K$ . Moreover, we can see from the proof of Lemma 17 of Okumoto and Suzuki (2022) that we do not need to account for the sup-norm scaling of $\\psi_{r}$ and thus it suffices to find the parameter $K^{\\prime}\\in\\mathbb{N}$ such that the approximation error $2^{-K^{\\prime}}\\asymp\\delta_{N}$ . Hence combining Theorems D.2 and D.3, we conclude that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma(\\mathcal{F}_{\\mathrm{TF}}(J,U,D,H,L,W,S,M),\\|\\cdot\\|_{L^{\\infty}},\\epsilon)\\lesssim K^{\\prime3/\\eta}K^{\\prime2}\\cdot2^{K^{\\prime}/\\alpha^{\\dagger}}K^{\\prime2+2/\\eta}\\cdot K^{\\prime}\\log\\frac{1}{\\epsilon}}\\\\ {\\lesssim\\left(\\frac{1}{\\delta_{N}}\\right)^{1/\\alpha^{\\dagger}}\\mathrm{polylog}\\left(N,\\frac{1}{\\delta_{N}}\\right)\\log\\frac{1}{\\epsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is sufficient to satisfy Assumption 3. Therefore, we can now apply our framework with $B_{N}^{\\prime}\\asymp N$ to obtain the bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}(\\widehat\\Theta)\\lesssim\\frac{N}{n}\\log N+\\frac{N^{2}}{n^{2}}\\log^{2}N+N^{-2\\alpha^{\\dagger}}+N^{2}\\delta_{N}^{2}}\\\\ &{\\qquad\\qquad+\\,\\frac{N^{2}}{T}\\log\\frac{N}{\\epsilon}+\\frac{1}{T}\\delta_{N}^{-1/\\alpha^{\\dagger}}\\,\\mathrm{polylog}\\left(N,\\frac{1}{\\delta_{N}}\\right)\\log\\frac{1}{\\epsilon}+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Substituting $\\delta_{N}\\asymp N^{-1-\\alpha^{\\dagger}}$ and \u03f5 \u224dN \u22122\u03b1 concludes the theorem. ", "page_idx": 34}, {"type": "image", "img_path": "hF6vatntqc/tmp/8ea431162a52221b15ab5d62c24ccae0e69e306bcae8c1dda63270929a2ac872.jpg", "img_caption": ["Figure 1: Architecture of the compared models. Each model contains two MLP components, all attention layers are single-head and LayerNorm is not included. (a),(b) implement the simplified reparametrization for attention, while all layers in (c) utilize the full embeddings. The input dimension is 8 and all hidden layer and DNN output widths are 32. The query prediction is read off the last entry of the output at the query position. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "hF6vatntqc/tmp/6b175f358b957d15b375cca02956963c3f309579d2ef55260c25cdf02ac9a99d.jpg", "img_caption": ["Figure 2: Training and test curves for the ICL pretraining objective. We use the Adam optimizer with a learning rate of 0.02 for all layers. For the task class we take $\\alpha=1$ , $p=q=\\infty$ , $T=n=512$ and generate samples from random combinations of order 2 wavelets. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "E Numerical Experiments ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we connect our theoretical contributions to practical transformers by conducting experiments verifying our results as well as justifying the simplified model setup and the empirical risk minimization assumption. We implement and compare the following toy models: (a) the simplified architecture studied in our paper; (b) the same model with linear attention replaced by softmax; and (c) a full transformer with 2 stacked encoder layers. The number of feedforward layers, widths of hidden layers, learning rate, etc. are set to equal for a fair comparison, see Figure 1 for details. ", "page_idx": 35}, {"type": "text", "text": "Figure 2 shows training (solid) and test loss curves (dashed) during pretraining. All 3 architectures exhibit similar behavior and converge to near zero training loss, justifying the use of our simplified model and also supporting the assumption that the empirical risk is minimized. Moreover, Figure 3 shows the converged losses over a wide range of $N,n,T$ values. We verify that increasing $N,n$ leads to decreasing train and test error, corresponding to the approximation error of Theorem 3.1. We also observe that increasing $T$ tends to improve the pretraining generalization gap up to a threshold, confirming our theoretical analysis of task diversity. Again, this behavior is consistent across the 3 architectures. We note that in the overparametrized regime when the number of total parameters $\\gtrsim n T$ , the trained model is likely not the empirical risk minimizer, which may also contribute to the large error for large $N$ or small $n,T$ . ", "page_idx": 35}, {"type": "image", "img_path": "hF6vatntqc/tmp/812030d9c4d482b55ececdada4dddadb93bbcfb9a10c07b41b07421dc494217e.jpg", "img_caption": ["Figure 3: Training and test losses of the three models after 50 epochs while varying (a) DNN width $N$ ; (b) number of in-context samples $n$ ; (c) number of tasks $T$ . For (a), the widths of all hidden layers also vary with $N$ . We take the median over 5 runs for robustness. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "F Proofs of Minimax Lower Bounds ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "F.1 Proof of Proposition 5.1 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we develop our framework for obtaining minimax lower bounds in the ICL setup by adapting the information-theoretic approach of Yang and Barron (1999). ", "page_idx": 36}, {"type": "text", "text": "Let $\\{(\\psi^{(j)},\\beta_{T+1}^{(j)})\\}_{j=1}^{M}$ be a $\\delta_{n}$ -packing of the class ${\\mathcal{F}}^{\\circ}$ with respect to the $L^{2}(\\mathcal{X})$ -norm such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\beta_{T+1}^{(j)\\top}\\psi^{(j)}-\\beta_{T+1}^{(j^{\\prime})\\top}\\psi^{(j^{\\prime})}\\|_{L^{2}(\\mathcal{X})}^{2}\\geq\\delta_{n}^{2},\\quad1\\leq j<j^{\\prime}\\leq M,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $M$ is the corresponding packing number. Then we have the following proposition as an application of Fano\u2019s inequality (Yang and Barron, 1999). ", "page_idx": 36}, {"type": "text", "text": "Proposition F.1. Let $\\Theta$ be a random variable uniformly distributed over $\\{(\\psi^{(j)},\\beta_{T+1}^{(j)})\\}_{j=1}^{M}$ . Then, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{f_{n}}:\\mathcal{D}_{n,T}\\to\\mathbb{R}}\\operatorname*{sup}_{f^{0}\\in\\mathcal{F}^{0}}\\mathbb{E}_{\\mathcal{D}_{n,T}}[\\Vert\\widehat{f}-f^{\\circ}\\Vert_{L^{2}(\\mathcal{P}_{X})}^{2}]\\ge\\frac{\\delta_{n}^{2}}{2}\\left(1-\\frac{\\mathbb{E}_{\\mathbf{X}}[I_{\\mathbf{X}^{(1:T+1)}}(\\Theta,y^{(1:T+1)})]+\\log2}{\\log M}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $I_{\\mathbf{X}^{(1:T+1)}}(\\Theta,y^{(1:T+1)})$ is the mutual information between $\\Theta,y^{(1:T+1)}$ for given $\\mathbf{X}^{(1:T+1)}$ ", "page_idx": 36}, {"type": "text", "text": "The mutual information $I_{\\mathbf{X}^{(1:T+1)}}(\\Theta,y^{(1:T+1)})$ is formulated more concretely as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{\\theta\\in\\mathrm{supp}\\,\\Theta}w(\\theta)\\int p(y^{(1:T+1)}|\\theta,\\mathbf{X}^{(1:T+1)})\\log\\left(\\frac{p(y^{(1:T+1)}|\\theta,\\mathbf{X}^{(1:T+1)})}{p_{w}(y^{(1:T+1)}|\\mathbf{X}^{(1:T+1)})}\\right)\\mathrm{d}y^{(1:T+1)},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $p({\\pmb y}|{\\pmb\\theta},{\\bf X})$ is the probability density of $\\textit{\\textbf{y}}$ conditioned on $\\theta,\\mathbf{X}$ and $p_{w}$ is the marginal distribution of $\\pmb{y}^{(1:T+1)}$ where $\\begin{array}{r}{w(\\cdot)\\equiv\\frac{1}{M}}\\end{array}$ is the probability mass function of $\\Theta$ (i.e., $p_{w}(\\cdot|\\mathbf{X}^{(1:T+1)})=$ $\\begin{array}{r}{\\sum_{\\theta\\in\\mathrm{supp}\\,\\Theta}w(\\theta)p(\\cdot|\\theta,\\mathbf{X}^{(1:T+1)}))}\\end{array}$ . We let $P_{y^{(t)}|\\theta}$ (and $P_{y^{(1:t)}|\\theta})$ be the distribution of $\\pmb{y}^{(t)}$ conditioned on $\\theta,\\mathbf{X}^{(t)}$ (and $\\mathbf{X}^{(1:t)}.$ ) respectively, and let ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\bar{P}}_{y^{(1:T+1)}}={\\frac{1}{M}}\\sum_{j=1}^{M}P_{y^{(1:T+1)}|\\theta^{(j)}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "be the marginal distribution of $\\pmb{y}^{(1:T+1)}$ conditioned on $\\mathbf{X}^{(1:T+1)}$ . ", "page_idx": 36}, {"type": "text", "text": "Next, we define the set $\\{\\tilde{\\psi}^{(j)}\\}_{j=1}^{Q_{1}}$ to be a $\\varepsilon_{n,1}$ -covering of ${\\mathcal F}_{N}$ w.r.t. the norm $d(\\psi,\\psi^{\\prime})\\;:=\\;$ $\\sqrt{\\mathbb{E}_{x}[\\|\\psi(x)-\\psi^{\\prime}(x)\\|^{2}]}$ with $\\varepsilon_{n,1}$ -covering number $Q_{1}$ , and $\\{\\tilde{\\beta}^{(j)}\\}_{j=1}^{Q_{2}}$ to be a $\\varepsilon_{n,2}$ -covering of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ w.r.t. the $L^{2}$ norm with $\\varepsilon_{n,2}$ -covering number $Q_{2}$ . By taking all combinations of $(\\tilde{\\psi}^{(j)},\\tilde{\\beta}^{(j^{\\prime})})$ for $1\\leq j\\leq Q_{1}$ and $1\\leq j^{\\prime}\\leq Q_{2}$ , we obtain the covering $\\{\\widetilde{\\theta}^{(j)}\\}_{j=1}^{Q}$ with respect to the quantity $\\varepsilon_{n}^{2}=\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}+C_{2}\\varepsilon_{n,2}^{2}$ where $Q=Q_{1}Q_{2}$ and each $\\tilde{\\theta}^{(j)}$ is given by $\\tilde{\\theta}^{(j)}=(\\tilde{\\psi}^{(j_{1})},\\tilde{\\beta}^{(j_{2})})$ for some indices $j_{1}$ and $j_{2}$ . ", "page_idx": 36}, {"type": "text", "text": "Then as in the discussion of Yang and Barron (1999), the mutual information is bounded by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{I_{\\mathbf{X}^{(1:T+1)}}(\\Theta,\\pmb{y}^{(1:T+1)})=\\displaystyle\\frac{1}{M}\\sum_{j=1}^{M}D(P_{\\pmb{y}^{(1:T+1)}|\\theta^{(j)}}\\|\\bar{P}_{\\pmb{y}^{(1:T+1)}})}\\\\ &{}&{\\leq\\displaystyle\\frac{1}{M}\\sum_{j=1}^{M}D(P_{\\pmb{y}^{(1:T+1)}|\\theta^{(j)}}\\|\\tilde{P}_{\\pmb{y}^{(1:T+1)}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $D(\\cdot\\|\\cdot)$ is the Kullback-Leibler divergence and $\\begin{array}{r}{\\tilde{P}_{y^{(1:T+1)}}\\,=\\,\\frac{1}{Q}\\sum_{j=1}^{Q}P_{y^{(1:T+1)}|\\tilde{\\theta}^{(j)}}}\\end{array}$ because $\\bar{P}_{y^{(1:T+1)}}$ minimizes the right hand side. If we let ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\kappa(j):=\\underset{1\\leq k\\leq Q}{\\arg\\operatorname*{min}}\\,D(P_{y^{(1:T+1)}|\\theta^{(j)}}\\,\\big\\|P_{y^{(1:T+1)}|\\tilde{\\theta}^{(k)}}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then each summand of the right-hand side is further bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\log Q+D\\big(P_{y^{(1:T+1)}|\\theta^{(j)}}\\|P_{y^{(1:T+1)}|\\tilde{\\theta}^{\\kappa(j)}}\\big).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, for $\\theta=(\\psi,\\beta^{(T+1)})$ it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p({\\boldsymbol{y}}^{(1:T+1)}|\\theta,\\mathbf{X}^{(1:T+1)})}}\\\\ {{\\displaystyle=\\prod_{t=1}^{T}p({\\boldsymbol{y}}^{(t)}|{\\boldsymbol{\\psi}},\\mathbf{X}^{(t)})\\cdot p({\\boldsymbol{y}}^{(T+1)}|{\\boldsymbol{\\psi}},{\\boldsymbol{\\beta}}^{(T+1)},\\mathbf{X}^{(T+1)})}}\\\\ {{\\displaystyle=\\prod_{t=1}^{T}p({\\boldsymbol{y}}^{(t)}|{\\boldsymbol{\\psi}},{\\boldsymbol{\\beta}}^{(t)},\\mathbf{X}^{(t)})p_{\\boldsymbol{\\beta}}({\\boldsymbol{\\beta}}^{(t)})\\mathrm{d}{\\boldsymbol{\\beta}}^{(t)}\\cdot p({\\boldsymbol{y}}^{(T+1)}|{\\boldsymbol{\\psi}},{\\boldsymbol{\\beta}}^{(T+1)},\\mathbf{X}^{(T+1)}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then the KL-divergence can be bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\big(P_{y^{(1:T+1)}|\\theta^{(j)}}\\|P_{y^{(1:T+1)}|\\tilde{\\theta}^{\\kappa(j)}}\\big)}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}D\\left(P_{y^{(t)}|\\psi^{(j)}}\\|P_{y^{(t)}|\\tilde{\\psi}^{(\\kappa(j))}}\\right)+D\\left(P_{y^{(T+1)}|\\psi^{(j)},\\beta_{T+1}^{(j)}}\\|P_{y^{(T+1)}|\\tilde{\\psi}^{(\\kappa(j))},\\tilde{\\beta}_{T+1}^{(\\kappa(j))}}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\int D\\left(P_{y^{(t)}|\\psi^{(j)},\\beta^{(t)}}\\|P_{y^{(t)}|\\tilde{\\psi}^{(\\kappa(j))},\\beta^{(t)}}\\right)p_{\\beta}(\\beta^{(t)})\\mathrm{d}\\beta^{(t)}}\\\\ &{\\qquad+D\\left(P_{y^{(T+1)}|\\psi^{(j)},\\beta_{T+1}^{(j)}}\\|P_{y^{(T+1)}|\\tilde{\\psi}^{(\\kappa(j))},\\tilde{\\beta}_{T+1}^{(\\kappa(j))}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the joint convexity of KL-divergence was used for the last inequality. Since the observation noise is assumed to be normally distributed, the integrand KL-divergence can be bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\nD\\left(P_{y^{(t)}|\\psi^{(j)},\\beta}\\|P_{y^{(t)}|\\tilde{\\psi}^{\\kappa(j)},\\beta}\\right)=\\sum_{i=1}^{n}\\frac{1}{2\\sigma^{2}}\\left(\\beta^{\\top}\\psi^{(j)}(x_{i}^{(t)})-\\beta^{\\top}\\tilde{\\psi}^{(\\kappa(j))}(x_{i}^{(t)})\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, its expectation with respect to ${\\boldsymbol{\\beta}},\\mathbf{X}^{(t)}$ becomes ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}^{(t)},\\beta}\\left[D\\left(P_{y^{(t)}|\\psi^{(j)},\\beta}\\|P_{y^{(t)}|\\tilde{\\psi}^{\\kappa(j)},\\beta}\\right)\\right]=\\frac{n\\sigma_{\\beta}^{2}}{2\\sigma^{2}}\\|\\psi^{(j)}-\\tilde{\\psi}^{\\kappa(j)}\\|_{L^{2}(\\mathcal{P}_{x})}^{2}\\leq\\frac{n\\sigma_{\\beta}^{2}}{2\\sigma^{2}}\\varepsilon_{n,1}^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the same manner, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\supset\\left(P_{y^{(T+1)}|\\psi^{(\\ell)},\\beta_{T+1}^{(\\ell)}}\\|P_{y^{(T+1)}|\\tilde{\\psi}^{\\kappa(\\ell)},\\tilde{\\beta}_{T+1}^{(\\kappa(j))}}\\right)}\\\\ &{\\displaystyle=\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\left(\\beta_{T+1}^{(j)\\top}\\psi^{(j)}(x_{i}^{(t)})-\\hat{\\beta}_{T+1}^{(\\kappa(j))\\top}\\tilde{\\psi}^{(\\kappa(j))}(x_{i}^{(t)})\\right)^{2}}\\\\ &{\\displaystyle\\le\\sum_{i=1}^{n}\\frac{1}{2\\sigma^{2}}\\left[\\left(\\beta_{T+1}^{(j)}-\\tilde{\\beta}_{T+1}^{(\\kappa(j))}\\right)^{\\top}\\tilde{\\psi}^{(\\kappa(j))}(x_{i}^{(t)})\\right]^{2}+\\displaystyle\\sum_{i=1}^{n}\\frac{1}{2\\sigma^{2}}\\left[\\beta_{T+1}^{(j)\\top}(\\psi^{(j)}(x_{i}^{(t)})-\\tilde{\\psi}^{(\\kappa(j))}(x_{i}^{(t)}))\\right]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The expectation of the right-hand side with respect to $\\mathbf{X}^{(T+1)},\\beta_{T+1}^{(j)}$ is bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{X}^{(T+1)},\\beta_{T+1}^{(j)}}\\left[D\\left(P_{y^{(T+1)}|\\psi^{(j)},\\beta_{T+1}^{(j)}}\\Vert P_{y^{(t)}|\\tilde{\\psi}^{\\kappa(j)},\\tilde{\\beta}_{T+1}^{(\\kappa(j))}}\\right)\\right]}\\\\ &{\\leq\\displaystyle\\frac{C_{2}n}{2\\sigma^{2}}\\Vert\\beta_{T+1}^{(j)}-\\tilde{\\beta}_{T+1}^{(\\kappa(j))}\\Vert^{2}+\\displaystyle\\frac{n}{2\\sigma^{2}}\\sigma_{\\beta}^{2}\\Vert\\psi^{(j)}-\\tilde{\\psi}^{(\\kappa(j))}\\Vert_{L^{2}(\\mathcal{P}_{X})}^{2}}\\\\ &{\\leq\\displaystyle\\frac{C_{2}n}{2\\sigma^{2}}\\varepsilon_{n,2}^{2}+\\displaystyle\\frac{n}{2\\sigma^{2}}\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}=\\displaystyle\\frac{n}{2\\sigma^{2}}\\varepsilon_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, the expected mutual information can be bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[I_{\\mathbf{X}^{(1:T+1)}}(\\Theta,y^{(1:T+1)})]\\le\\log Q_{1}+\\log Q_{2}+\\frac{n T}{2\\sigma^{2}}\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}+\\frac{n}{2\\sigma^{2}}\\varepsilon_{n}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Applying Proposition F.1 together with (7) concludes the proof. ", "page_idx": 37}, {"type": "text", "text": "F.2 Lower Bound in Besov Space ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Here, we derive the minimax lower bound when $\\mathcal{F}^{\\circ}\\,=\\,\\mathbb{U}(B_{p,q}^{\\alpha}(\\mathcal{X}))$ . Recall that in this setting $s=\\alpha/d$ . We fix a resolution $K$ and then consider the set of B-splines $\\omega_{K,\\ell}^{d},\\ell\\in I_{K}^{d}$ of cardinality $N^{\\prime}\\,\\asymp\\,2^{K d}$ . Considering the basis pairs $(\\omega_{K,1}^{d},\\omega_{K,2}^{d}),\\dots,(\\omega_{K,N^{\\prime}-1}^{d},\\omega_{K,N^{\\prime}}^{d})$ , we can determine which one is employed to construct the basis $\\psi^{(j)}$ . The Varshamov-Gilbert bound yields that for $\\Omega=\\{0,1\\}^{N^{\\prime}/2}$ , we can construct a subset $\\Omega^{\\prime}=\\{w_{1},\\dots,w_{2^{N^{\\prime}}/16}\\}\\subset\\Omega$ such that $|\\Omega|=2^{N^{\\prime}/16}$ and $w\\neq w^{\\prime}\\in\\Omega^{\\prime}$ has a Hamming distance not less than $N^{\\prime}/16$ . Using this $\\Omega^{\\prime}$ , we set $N=N^{\\prime}/2$ and $M=2^{N^{\\prime}/16}$ and define $(\\psi^{(j)})_{j=1}^{M}$ as )= \u03c9dK,2i\u22121 if wj,i = 0 and \u03c8i(j) $\\psi_{i}^{(j)}=\\omega_{K,2i}^{d}$ if $w_{j,i}=1$ . We use the same B-spline bases with resolution more than $K$ for $\\psi_{i}^{(j)}\\left(i\\geq N\\right)$ across all $j$ . ", "page_idx": 38}, {"type": "text", "text": "By the construction of $(\\psi^{(j)})$ , if we set $\\beta^{(1)}=(\\sigma_{\\beta},\\dots,\\sigma_{\\beta},0,0,\\dots)$ , then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\beta^{(1)\\top}\\psi^{(j)}-\\beta^{(1)\\top}\\psi^{(j^{\\prime})}\\|_{L^{2}(\\mathcal{P}_{\\mathcal{X}})}^{2}\\geq\\sigma_{\\beta}^{2}N/8.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, for $\\delta_{n}^{2}\\leq\\sigma_{\\beta}^{2}N/8\\lesssim1$ , the $\\delta_{n}$ -packing number is not less than $2^{N/8}$ . Moreover, the logarithmic $\\delta_{n}$ -packing number of $\\{\\beta^{\\top}\\psi^{(j)}\\ |\\ \\beta\\ \\in\\ B\\}$ for a fixed $j$ is $\\Theta(\\operatorname*{min}\\{\\delta_{n}^{-1/s},N\\log(1/\\delta_{n})\\})$ by the standard argument. ", "page_idx": 38}, {"type": "text", "text": "$\\log Q_{1}+\\log Q_{2}\\lesssim N$ $\\delta_{n}\\,=\\,N^{-s}$ $\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}\\leq\\delta_{n}^{2}$ $\\log M\\gtrsim N$ $\\varepsilon_{n,2}^{2^{'}}=C\\delta_{n}^{2}$ twhhee ruep $C$ e irs  bao cuonnds toafn tt. hTe hceonv, ebryi ncgh onousimnbge $C$ appropriately and $\\varepsilon_{n,1}\\lesssim N^{-1-s}$ (so that $\\log Q_{1}\\lesssim N)$ , as long as ", "page_idx": 38}, {"type": "equation", "text": "$$\nn T\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}+n\\delta_{n}^{2}\\lesssim\\log Q_{1}+\\log Q_{2}\\lesssim N\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "is satisfied, the minimax rate is lower bounded by $\\delta_{n}^{2}$ . Taking $N\\asymp n^{\\frac{1}{2s+1}}$ , we obtain the lower bound ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta_{n}^{2}\\gtrsim n^{-\\frac{2s}{2s+1}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "F.3 Lower Bound with Coarser Basis ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We consider a generalized setting where $\\mathcal{X}=\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\cdot\\cdot\\cdot\\times\\mathbb{R}^{d}$ and take $\\psi_{i}^{(j)}\\in\\mathbb{U}(B_{p,q}^{\\tau}(\\mathbb{R}^{d}))$ $(N{+}1)\\mathrm{times}$   \nand assume that $\\beta_{1}\\in[-1,1]$ and $\\beta_{j}\\in[-\\sigma_{\\beta},\\sigma_{\\beta}]$ where $\\sigma_{\\beta}^{2}=\\tilde{\\Theta}(N^{-2s-1})$ . Since the logarithmic $\\tilde{\\varepsilon}_{1}$ -covering and packing numbers of $\\mathbb{U}(B_{p,q}^{\\tau}(\\mathbb{R}^{d}))$ are $\\Theta(\\tilde{\\varepsilon}_{1}^{-d/\\tau})$ , those for the basis functions on $j\\,=\\,2,.\\,.\\,.\\,,N+1$ become $\\Theta(N\\tilde{\\varepsilon}_{1}^{-d/\\tau})$ , and those for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ are $\\begin{array}{r}{\\Theta(N\\log(1+\\frac{N\\sigma_{\\beta}^{2}}{\\varepsilon_{n,2}^{2}}))}\\end{array}$ . Therefore, by taking $\\varepsilon_{n,1}^{2}=N\\tilde{\\varepsilon}_{1}^{2}$ we see that ", "page_idx": 38}, {"type": "equation", "text": "$$\nn T(\\varepsilon_{n,1}^{2}+\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2})+n\\varepsilon_{n,2}^{2}\\lesssim\\varepsilon_{n,1}^{-d/\\tau}+N\\left(\\frac{\\varepsilon_{n,1}}{\\sqrt{N}}\\right)^{-d/\\tau}+N\\log\\left(1+\\frac{N\\sigma_{\\beta}^{2}}{\\varepsilon_{n,2}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "should be satisfied. Moreover, by taking $\\varepsilon_{n,1}^{(2\\tau+d)/\\tau}\\asymp1/n T$ and $\\varepsilon_{n,2}^{2}\\asymp N\\log(1+N^{-2s}/\\varepsilon_{n,2}^{2})/n$ we can balance both sides. In particular, we may set ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\varepsilon_{n,1}^{2}\\asymp(n T)^{-\\frac{2\\tau}{2\\tau+d}},\\quad\\varepsilon_{n,2}^{2}\\asymp\\frac{N}{n}\\wedge N^{-2s}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Taking the balance with respect to $N$ to maximize $\\varepsilon_{n,2}^{2}$ , we have $N\\asymp n^{\\frac{d}{2\\alpha+d}}$ and $\\varepsilon_{n,1}^{2}\\asymp(n T)^{-\\frac{2\\tau}{2\\tau+d}}$ . Therefore, the minimax rate is lower bounded as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta_{n}^{2}\\simeq(1+\\sigma_{\\beta}^{2})\\varepsilon_{n,1}^{2}+\\varepsilon_{n,2}^{2}\\simeq n^{-\\frac{2\\alpha}{2\\alpha+d}}+(n T)^{-\\frac{2\\tau}{2\\tau+d}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "F.4 Lower Bound in Piecewise $\\gamma$ -smooth Class ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Suppose that we utilize the basis functions up to resolution $K$ . Then, by the argument by Nishimura and Suzuki (2024), the number of basis functions $\\psi_{r}$ in the $K$ -th resolution is $\\bar{N^{\\prime}}\\asymp2^{K/a^{\\dagger}}$ . Moreover, the $\\delta_{n}$ -packing number of the $\\gamma$ -smooth class is also lower bounded by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{log}\\,M\\geq N^{\\prime}\\,\\mathrm{polylog}(\\delta_{n},N^{\\prime}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, by noticing the approximation error bound in Appendix D.2, we take $N^{\\prime}\\asymp\\delta_{n}^{-1/a^{\\dagger}}$ where the basis functions are chosen from the $K$ th resolution. As in the case of the Besov space, we construct $(\\psi^{(j)})_{j=1}^{M^{\\prime}}$ where $M^{\\prime}=2^{N^{\\prime}/16}$ and $\\psi^{(j)}(x)\\in\\mathbb{R}^{N}$ for $N=N^{\\prime}/2$ and $\\|\\psi^{(j)}-\\psi^{(\\bar{j}^{\\prime})}\\|_{L^{2}(P_{\\mathcal{X}})}^{2}\\geq N/8$ for $j\\neq j^{\\prime}$ . Following the same argument as in the Besov case, we need to take $\\varepsilon_{n,1}$ and $\\varepsilon_{n,2}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\nn T\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}+n\\varepsilon_{n,2}^{2}\\lesssim\\delta_{n}^{-1/a^{\\dagger}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "up to logarithmic factors. This is satisfied by taking $\\varepsilon_{n,2}^{2}=C\\delta_{n}^{2}\\asymp n^{-\\frac{2a^{\\dagger}}{2a^{\\dagger}+1}}$ with a constant $C$ and balancing $N$ so that $\\sigma_{\\beta}^{2}\\varepsilon_{n,1}^{2}=(n T)^{-\\frac{2a^{\\dagger}}{2a^{\\dagger}+1}}\\sigma_{\\beta}^{\\frac{2}{2a^{\\dagger}+1}}\\asymp T^{-1}n^{-\\frac{2a^{\\dagger}}{2a^{\\dagger}+1}}$ 1 \u224dT \u22121n\u22122a\u2020+1 . Combining this evaluation and (25) yields that the minimax lower bound is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\delta_{n}^{2}\\gtrsim n^{-\\frac{2\\alpha^{\\dagger}}{2a^{\\dagger}+1}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 40}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 40}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 40}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 40}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 40}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 40}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: See the Our Contributions paragraph for details. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: See the Limitations paragraph. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See Assumptions 1, 2, 3, 4, 5, 6. All proofs were provided in the Appendix. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See Appendix E, Figure 1. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 41}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: All experiments are toy simulations and data is i.i.d. random. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 42}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: See Appendix E, Figure 2. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The median over 5 runs is reported in Figure 3. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: All experiments are toy simulations. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 43}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: The research is theoretical and raises no ethical concerns. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The research is theoretical and raises no concerns on societal impact. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: [NA]   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]