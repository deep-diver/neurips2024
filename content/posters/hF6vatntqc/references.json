{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper empirically demonstrates the effectiveness of few-shot prompting in large language models, which is a central phenomenon explored in the current work."}, {"fullname_first_author": "A. Mahankali", "paper_title": "One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention", "publication_date": "2023-07-01", "reason": "This paper provides theoretical analysis and optimality results for single-layer linear attention models, offering a foundational basis for understanding the current work's multi-layer analysis."}, {"fullname_first_author": "R. Zhang", "paper_title": "Transformers learn linear models in-context", "publication_date": "2023-06-01", "reason": "This paper provides theoretical analysis of the in-context learning behavior of linear attention models, significantly influencing the design and direction of the current research."}, {"fullname_first_author": "T. Suzuki", "paper_title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality", "publication_date": "2019-01-01", "reason": "This paper establishes the optimal learning rates for deep neural networks in Besov spaces, providing crucial theoretical groundwork for analyzing the approximation capabilities of the DNN module in the current transformer model."}, {"fullname_first_author": "S. Takakura", "paper_title": "Approximation and estimation ability of Transformers for sequence-to-sequence functions with infinite dimensional input", "publication_date": "2023-01-01", "reason": "This paper provides insights into the approximation power of transformers for sequential data, which is essential for the analysis of transformers handling sequential inputs in the current work."}]}