[{"figure_path": "sOhFyFFnxT/figures/figures_6_1.jpg", "caption": "Figure 1: ODE results for learning rate \u03c4 = 0.04, \u0433 = 0.2 and four different noise levels, with d = 2. The columns represent \u03b7\u03c2 = \u03b7\u03c4 = 2, 1, 3, 4 respectively. At \u03b7 = 5 or higher, the generator is unable to learn anything. In all cases, the green and red represent the two diagonals of P, and the blue and yellow represent the two diagonals of Q. We see that the simulations do match the predicted ODE results.", "description": "This figure displays the results of Ordinary Differential Equation (ODE) simulations for various noise levels, comparing them to empirical simulations.  Four noise levels (\u03b7\u03c2 = \u03b7\u03c4 = 2, 1, 3, 4) are shown, illustrating the learning dynamics for a two-dimensional subspace (d=2) using a specific learning rate.  The plot demonstrates the convergence of the model's macroscopic state to values predicted by the ODE model, which suggests that the ODE accurately captures the essential dynamics of the generative adversarial network (GAN).  Importantly, it's noted that when the noise level (\u03b7) reaches 5 or above, the generator fails to learn effectively.", "section": "5 Simulations"}, {"figure_path": "sOhFyFFnxT/figures/figures_6_2.jpg", "caption": "Figure 1: ODE results for learning rate \u03c4 = 0.04, \u0433 = 0.2 and four different noise levels, with d = 2. The columns represent \u03b7\u03c2 = \u03b7\u03c4 = 2, 1, 3, 4 respectively. At \u03b7 = 5 or higher, the generator is unable to learn anything. In all cases, the green and red represent the two diagonals of P, and the blue and yellow represent the two diagonals of Q. We see that the simulations do match the predicted ODE results.", "description": "This figure shows the results of simulations performed to validate the ODE model presented in the paper.  It shows the cosine similarity over time for different noise levels (\u03b7\u03c2 = \u03b7\u03c4 = 2, 1, 3, 4). The results indicate a good match between the simulations and the ODE model's predictions.  For noise levels above 4, the generator fails to learn, illustrating a limitation of the model.", "section": "5 Simulations"}, {"figure_path": "sOhFyFFnxT/figures/figures_7_1.jpg", "caption": "Figure 3: The graph shows the Grassmann distance over time on the Olivetti Faces dataset, for Oja's method (Blue) and the GAN model (Orange), as well as the single-feature GAN model (Green). We use the same hyperparameters as all previous experiments, measured with respect to a full PCA decomposition which acts as a surrogate for the true subspace.", "description": "This figure compares the performance of three subspace learning methods: Oja's method, a multi-feature GAN, and a single-feature GAN, on the Olivetti Faces dataset.  The y-axis represents the Grassmann distance between the learned subspace and the true subspace (approximated by full PCA). The x-axis shows the training time.  The graph illustrates that the multi-feature GAN outperforms both Oja's method and the single-feature GAN in terms of capturing the true data subspace, achieving a lower Grassmann distance.", "section": "7 Real image subspace learning"}, {"figure_path": "sOhFyFFnxT/figures/figures_8_1.jpg", "caption": "Figure 4: We provide results on the Olivetti Faces dataset, a well-known dataset. We show the top 16 learned features for all approaches at 3 stages of training: after the 1st epoch, the 200th epoch, and the end of training. We train all approaches for 500 epochs, equivalent to approximately 50 timesteps of simulated training. It can be clearly seen that while Oja\u2019s method learns quicker than the GAN model, eventually the GAN model outperforms it. Additionally, we see that the features learned by the GAN model are much more diverse and meaningful than those learned by Oja\u2019s method (whose learned features are more similar). For the single-feature GAN model, we can see that the learning is significantly slower, and never approaches anywhere close to the other two results.", "description": "This figure compares the top 16 learned features of three different subspace learning methods (GAN with multi-feature discriminator, GAN with single-feature discriminator, and Oja\u2019s method) on the Olivetti Faces dataset at three different training stages (epoch 1, epoch 200, and final epoch).  It demonstrates the qualitative difference in feature learning between the methods, highlighting the superior performance and diversity of features learned by the GAN with a multi-feature discriminator.", "section": "7 Real image subspace learning"}, {"figure_path": "sOhFyFFnxT/figures/figures_11_1.jpg", "caption": "Figure 5: Comparison between the generator basis vectors learned by the multi-feature and single-feature discriminators on 36 features. The multi-feature model is trained for 1 epoch, while the single-feature model is trained for 5 epochs.", "description": "This figure compares the learned features of a multi-feature discriminator GAN model against a single-feature discriminator GAN model.  Both models aim to learn 36 features from the MNIST dataset. The multi-feature model achieves good results in a single epoch, whereas the single-feature model requires 5 epochs to reach a similar level of performance. The visualization shows that the multi-feature model learns clear and recognizable features, while the single-feature model produces many noisy or unclear features.", "section": "A.1 Comparisons with Single-Feature Discriminator and Oja's Method"}, {"figure_path": "sOhFyFFnxT/figures/figures_12_1.jpg", "caption": "Figure 6: After training both GAN and Oja's method on the MNIST dataset with 16 features, we use SVD to extract their learned features, and compare them here. We can see that despite Oja's method learning quicker than the GAN model (seen in previous analysis), the GAN features learned are a better representation of the data, while most of Oja's features do not resemble the data. Note that Oja learning an 8 as the first feature is due to the order of training samples seen.", "description": "This figure compares the features learned by GAN and Oja's method on the MNIST dataset.  The left side shows the top 16 features learned by the GAN model, while the right shows those learned by Oja's method.  Each image represents a single feature vector.  The caption highlights that GAN learns features that better represent the dataset despite Oja's method being faster. The order of features learned by Oja's method is influenced by the order of training samples.", "section": "A.1 Comparisons with Single-Feature Discriminator and Oja's Method"}, {"figure_path": "sOhFyFFnxT/figures/figures_12_2.jpg", "caption": "Figure 7: In the first figure, using PCA on the MNIST dataset, we obtain the top 16 features of the data, and use these features as an approximation for the true subspace U. We then track the Grassmann distance between the learned subspace and this approximation, for both our model trained for 1 epoch, and the sequential discriminator trained for 1 and 5 epochs. It is clearly seen that our discriminator learns much faster than the sequential discriminator, while at the same time obtaining a much lower distance, even when the sequential discriminator has sees 5 times as much data.", "description": "This figure compares the Grassmann distances for the multi-feature and single-feature discriminators. The Grassmann distance measures the similarity between two subspaces. The results show that the multi-feature discriminator learns a subspace that is much closer to the true subspace (as determined by PCA) than the single-feature discriminator, even when the single-feature discriminator is trained for five times as many epochs (iterations).", "section": "A.2 Grassmann distances"}]