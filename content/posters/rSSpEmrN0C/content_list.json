[{"type": "text", "text": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recently, many studies have demonstrated that exclusively incorporating OCR  \n2 derived text and spatial layouts with large language models (LLMs) can be highly   \n3 effective for document understanding tasks. However, existing methods that in  \n4 tegrate spatial layouts with text have limitations, such as producing overly long   \n5 text sequences or failing to fully leverage the autoregressive traits of LLMs. In   \n6 this work, we introduce Interleaving Layout and Text in a Large Language Model   \n7 (LayTextLLM) for document understanding. In particular, LayTextLLM projects   \n8 each bounding box to a single embedding and interleaves it with text, efficiently   \n9 avoiding long sequence issues while leveraging autoregressive traits of LLMs.   \n10 LayTextLLM not only streamlines the interaction of layout and textual data but   \n11 also shows enhanced performance in Key Information Extraction (KIE) and Visual   \n12 Question Answering (VQA). Comprehensive benchmark evaluations reveal signifi  \n13 cant improvements, with a $27.0\\%$ increase on KIE tasks and $24.1\\%$ on VQA tasks   \n14 compared to previous state-of-the-art document understanding MLLMs, as well as   \n15 a $15.5\\%$ improvement over other SOTA OCR-based LLMs on KIE tasks. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Recent research has increasingly focused on applying Large Language Models (LLMs) [1\u201317] to   \n18 document-oriented Visual Question Answering (VQA) and Key Information Extraction (KIE) scenar  \n19 ios. Efforts to build a text-sensitive MultiModal Large Language Models (MLLMs) based on existing   \n20 LLMs, particularly aimed at enhancing Visually Rich Document Understanding (VRDU), have made   \n21 significant progress [6, 12, 18]. Although existing MLLMs show promising results in document   \n22 understanding, they often encounter challenges related to image resolution. When the input image is   \n23 of low resolution, it is too blurry to extract visual features effectively. Conversely, high-resolution   \n24 images require additional computational resources to capture detailed textual information [12].   \n25 Concurrently, another line of research employs off-the-shelf OCR tools to extract text and spatial   \n26 layouts, which are then combined with LLMs to address VRDU tasks. These approaches assume that   \n27 most valuable information for document comprehension can be derived from the text and its spatial   \n28 layouts, viewing spatial layouts as \u201clightweight visual information\u201d [19]. Following this premise,   \n29 several studies [12, 20\u201323] have explored various approaches that integrate spatial layouts with text   \n30 for LLMs, achieving results that are competitive with, or even surpass, those of MLLMs.   \n31 The most natural method to incorporate layout information is by treating spatial layouts as tokens,   \n32 which allows for the seamless interleaving of text and layout into a unified text sequence [20, 22, 23].   \n33 For example, Perot et al. [20] employ format such as \u201cHARRISBURG 78|09\u201d to represent OCR text   \n34 and corresponding layout, where \u201cHARRISBURG\u201d is OCR text and \u201c78|09\u201d indicates the mean of   \n35 the horizontal and vertical coordinates, respectively. Similarly, He et al. [23] use \u201c[x_min, y_min,   \n36 x_max, y_max]\u201d to represent layout information. These approaches can effectively take advantage of   \n37 autoregressive characteristics of LLMs and is known as the \u201ccoordinate-as-tokens\u201d scheme [20]. In   \n38 contrast, DocLLM [19] explores interacting spatial layouts with text through a disentangled spatial   \n39 attention mechanism that captures cross-alignment between text and layout modalities.   \n40 However, we argue that both of the previous ap  \n41 proaches have limitations. As shown in Fig. 1,   \n42 coordinate-as-tokens significantly increases the   \n43 number of tokens. Additionally, to accurately   \n44 comprehend coordinates and enhance zero-shot   \n45 capabilities, this scheme often requires few  \n46 shot in-context demonstrations and large-scale   \n47 language models, such as ChatGPT Davinci  \n48 003 (175B) [23], which exacerbates issues re  \n49 lated to sequence length and GPU resource de  \n50 mands. Meanwhile, although DocLLM does not   \n51 increase sequence length and integrates spatial   \n52 layouts through attention, its generalizability is   \n53 limited. We believe that spatial cross attention   \n54 and masked span tasks in DocLLM cannot fully   \n55 utilize the autoregressive traits of LLMs.   \n56 To address these problems, this paper explores a   \n57 simple yet effective approach to enhance the in  \n58 teraction between spatial layouts and text \u2014 In  \n59 terleaving Layout and Text in a Large Language   \n60 Model (LayTextLLM) for document understand  \n61 ing. Adhering to the common practice of inter  \n62 leaving any modality with text [15, 24, 25], we   \nspecifically apply this principle to spatial lay  \n64 outs. In particular, we maps each bounding box   \n65 to a single embedding, which is then interleaved with its corresponding text. Then we propose a   \n66 tailored pre-training task\u2014Layout-aware Next Token Prediction\u2014a completely self-supervised task   \n67 that enhances the alignment between layout and textual modalities without using synthetic data.   \n68 Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly   \n69 improves performance on downstream document-related VQA and KIE tasks. As shown in Fig. 1,   \n70 LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even   \n71 reducing the sequence length compared to DocLLM. Our contributions can be listed as follows:   \n72 \u2022 We propose LayTextLLM for document understanding. To the best of the authors\u2019 knowl  \n73 edge, this is the first work to employ a unified embedding approach (Sec. 3.1.1) that   \n74 interleaves spatial layouts directly with textual data within a LLM. By representing each   \n75 bounding box with one token, LayTextLLM efficiently addresses sequence length issues   \n6 brought by coordiante-as-tokens while fully leveraging autoregressive traits for enhanced   \n77 document understanding.   \n78 \u2022 We propose two tailored training tasks: (1) Layout-aware Next Token Prediction (Sec. 3.2.1),   \n79 a completely self-supervised training task to enhance the alignment between layout and   \n80 textual modality; (2) Shuffled-OCR Supervised Fine-tuning task (Sec. 3.2.2) to better elicit   \n1 the model generalizability in downstream tasks.   \n82 \u2022 Comprehensive experimental results demonstrate quantitatively that LayTextLLM signifi  \n83 cantly outperforms previous state-of-the-art (SOTA) OCR-free MLLMs by a large margin in   \n4 zero-shot scenarios, particularly in KIE tasks with an improvement of $27.0\\%$ . Additionally,   \n85 we illustrate that LayTextLLM competes effectively or even surpasses previous SOTA OCR  \n86 based methods in both zero-shot and SFT scenarios. Specifically, it surpasses DocLLM by   \n87 $19.8\\%$ on VQA and $15.5\\%$ on KIE tasks (Sec. 4).   \n88 \u2022 Extensive ablations demonstrate the utility of the proposed component, with analysis show  \n89 ing that LayTextLLM not only improves performance but also reduces input sequence length   \n90 compared to current OCR-based models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/a256daa9cfcb6dd8f1319a38e6f62de8b960e667394528b64a743e79d88f52b4.jpg", "img_caption": ["Figure 1: The performance against input sequence length of different datasets across various OCRbased methods where data is from Tab. 2 and 5. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "91 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "92 2.1 OCR-based LLMs for Document Understanding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 Early document understanding methods [26\u201330] tend to solve the task in a two-stage manner, i.e., first   \n94 reading texts from input document images using off-the-shelf OCR engines and then understanding   \n95 the extracted texts. Considering the advantages of LLMs (e.g., high generalizability), some recent   \n96 methods endeavor to combine LLMs with OCR-derived results to solve document understanding.   \n97 For example, inspired by the \u201ccoordinate-as-tokens\u201d scheme [20], He et al. [23] propose to use   \n98 \u201c[x_min, y_min, x_max, y_max]\u201d to introduce the layout information, which can fuse the layout   \n99 information and texts into a unified text sequence and fully exploit the autoregressive merit of LLMs.   \n100 To reinforce the layout information while avoiding increasing the number of tokens, DocLLM [19]   \n101 designs a disentangled spatial attention mechanism to capture cross-alignment between text and   \n102 layout modalities. Recently, LayoutLLM [21] utilizes the pre-trained layout-aware model [31], to   \n103 insert the visual information, layout information and text information. However, the aforementioned   \n104 methods neither suffer from the computational overhead leading by the increasing tokens or hardly   \n105 take advantage of autoregressive characteristics of LLMs. Thus, it is an urgent problem to address   \n106 how to better incorporate layout information without significantly increasing the number of tokens. ", "page_idx": 2}, {"type": "text", "text": "107 2.2 OCR-free MLLMs for Document Understanding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "108 Another approach to solve document understanding tasks is the OCR-free method. Benefiting from   \n109 the end-to-end training framework, it involves processing the text content of documents directly,   \n110 without relying on OCR engines. Donut [32] first presents an OCR-free method through mapping   \n111 a text-rich document image into the desired answers. Pix2Struct [33] is trained to parse masked   \n112 screenshots of web pages into simplified HTML, where variable resolution inputs are supported.   \n113 While these approaches eliminate the need for OCR tools, they still necessitate task-specific fine  \n114 tuning. With the increasing popularity of LLMs/MLLMs [10\u201317], various methods are proposed to   \n115 solve the document understanding task through explicitly training models on visual text understanding   \n116 datasets and fine-tuning them with instructions to perform a zero-shot prediction. LLaVAR [34]   \n117 and UniDoc [10] are notable examples that expand upon the document-oriented VQA capabilities   \n118 of LLaVA [35] by incorporating document-based tasks. These models pioneer the use of MLLMs   \n119 for predicting texts and coordinates from document images, enabling the development of OCR  \n120 free document understanding methods. Additionally, DocPedia [9] operates document images in   \n121 the frequency domain, allowing for higher input resolution without increasing the input sequence   \n22 length. Recent advancements in this field, including mPLUG-DocOwl [18], Qwen-VL [6], and   \n123 TextMonkey [12], leverage publicly available document-related VQA datasets to further enhance   \n124 the document understanding capability. Although these OCR-free methods have exhibited their   \n125 advantages, they still struggle with the high-resolution input to reserve more text-related details. ", "page_idx": 2}, {"type": "text", "text": "126 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "127 In this section, we present our LayTextLLM. First, we introduce a innovative Spatial Layout Projector   \n128 (Sec. 3.1.1) converts four-dimensional layout coordinates into a single-token embedding. To reduce   \n129 parameter overhead, we apply Partial Low-Rank Adaptation (Sec. 3.1.2). We also introduce two   \n130 specific training tasks: Layout-aware Next Token Prediction (Sec. 3.2.1) to align layouts with   \n131 text during pre-training, and Shuffled-OCR Supervised Fine-tuning (Sec. 3.2.2) to enhance the   \n132 generalizability of the model. An illustration of our approach is shown in Fig. 2. ", "page_idx": 2}, {"type": "text", "text": "133 3.1 Model Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "134 LayTextLLM is built on the Llama2-7B-base model, which was originally designed to accept only   \n135 text inputs [36, 37]. To enable the model to interleave spatial layouts with text, we introduce a   \n136 novel Spatial Layout Projector. This projector converts OCR-derived coordinates into bounding box   \n137 tokens. We also adopt the Partial Low-Rank Adaptation, a minimally invasive method to incorporate   \n138 additional modalities while preserving the LLM\u2019s inherent knowledge intact. ", "page_idx": 2}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/4456febe8c88159b031f6fa8928eef28893d0d1461e53fd6fc45c2979055a732.jpg", "img_caption": ["Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens $(b^{i})$ with text tokens $(t^{i})$ , where the superscripts represent the sequence positions of the tokens. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "139 3.1.1 Spatial Layout Projector (SLP) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 A key innovation in LayTextLLM is the Spatial Layout Projector (SLP), which transforms a spatial   \n141 layout into a singular bounding box token. This enhancement enables the model to process both   \n142 spatial layouts and textual inputs simultaneously. To be specifically, each OCR-derived spatial layout   \n143 is represented by a bounding box defined by four-dimensional coordinates $\\left[x_{1},y_{1},x_{2},y_{2}\\right]$ , these   \n144 coordinates represent the normalized minimum and maximum horizontal and vertical extents of the   \n145 box, respectively. The SLP maps these coordinates into a high-dimensional space that the language   \n146 model can process as a single token. The process can be computed as $z=W\\cdot c+b$ , where $c\\in\\bar{\\mathbb{R}}^{4}$   \n147 is the vector of the bounding box coordinates. $W\\,\\in\\,\\mathbb{R}^{d\\times4}$ is a weight matrix with $d$ represents   \n148 the dimension of the embedding, $b\\in\\mathbb{R}^{d\\times1}$ is a bias vector, $z$ is the resulting bounding box token   \n149 represented as an $d$ -dimensional embedding. As illustrated in Fig. 2, the resulting bounding box   \n150 token $z$ will be interleaved with corresponding textual embeddings to put into LLMs. Note that the   \n151 SLP is shared by all bounding box tokens so very limited number of parameters are introduced.   \n152 Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single   \n153 token. This approach significantly reduces the number of input tokens and adheres to the practice   \n154 of interleaving any modality with text, effectively integrating layout and textual information into a   \n155 unified sequence. This allows the model to process both modalities simultaneously and coherently,   \n156 fully leveraging the autoregressive traits of LLMs. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "157 3.1.2 Layout Partial Low-Rank Adaptation ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/85091da2195bbf9f1df43f0405ed98fa784fe65663b1e64fad46682507e5fb0e.jpg", "img_caption": ["173 Figure 3: The illustration of P-LoRA, 174 adapted from [15]. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "After using the SLP to generate bounding box tokens and a tokenizer to produce text tokens, these two modalities are then communicated using a Layout Partial Low-Rank Adaptation (P-LoRA) module in LLMs. P-LoRA, introduced in InternLM-XComposer2 [15], is originally used to adapt LLMs to visual modality. It applies plug-in low-rank modules specified to the visual tokens, which adds minimal parameters while preserving the LLMs inherent knowledge. ", "page_idx": 3}, {"type": "text", "text": "Formally, as shown in Fig. 3 for a linear layer in the LLM, the original weights $W_{O}\\in\\mathbb{R}^{C_{o u t}\\times C_{i n}}$ and bias $B_{O}\\in\\mathbb{R}^{C_{o u t}}$ are specified for input and output dimensions $C_{i n}$ and $C_{o u t}$ . P-LoRA modifies this setup by incorporating two additional matrices, $W_{A}\\ \\in\\ \\mathbb{R}^{C_{r}\\times C_{i n}^{\\textbf{c}}}$ and $\\boldsymbol{W_{B}}^{\\mathrm{~\\scriptsize~\\perp~}}\\in\\mathrm{~\\mathbb{R}}^{\\check{C}_{o u t}\\times C_{r}}$ . These matrices are lower-rank, with $C_{r}$ being considerably smaller than both $C_{i n}$ and $C_{o u t}$ , and are specifically designed to interact with new modality tokens, which in our case are bounding box tokens. For example, given an input $x=$ ", "page_idx": 3}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/2a31cb0e45a569e863c7ce7886878c97d7c11dc99e8ac8eb7128aaee7b2a3962.jpg", "img_caption": ["Figure 4: Comparison of Layout-aware Next Token Prediction and normal Next Token Prediction. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "175 $[x_{b},x_{t}]$ comprising of bounding box tokens $(x_{b})$ and textual tokens $\\left(\\boldsymbol{x}_{t}\\right)$ is fed into the system, the   \n176 forward process is as follows, where $\\hat{x}_{t},\\hat{x}_{b}$ and $\\hat{x}$ are outputs: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\boldsymbol{x}}_{t}=W_{0}\\boldsymbol{x}_{t}+B_{0}}\\\\ &{\\hat{\\boldsymbol{x}}_{b}=W_{0}\\boldsymbol{x}_{b}+W_{B}W_{A}\\boldsymbol{x}_{b}+B_{0}}\\\\ &{\\hat{\\boldsymbol{x}}=[\\hat{\\boldsymbol{x}}_{b},\\hat{\\boldsymbol{x}}_{t}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 3.2 Training Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "178 LayTextLLM is trained with innovative layout-aware training procedure, which consists of two stages:   \n179 Layout-aware Next Token Prediction pre-training and Shuffled-OCR Supervised Fine-tuning. ", "page_idx": 4}, {"type": "text", "text": "180 3.2.1 Layout-aware Next Token Prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 Inspired by the next token prediction commonly used in current LLM pre-training [1\u20137], we propose   \n182 the Layout-aware Next Token Prediction (LNTP). Fig. 4 presents the contrast of the proposed Layout  \n183 aware Next Token Prediction and the conventional next token prediction task. The traditional next   \n184 token prediction (Fig. 4(a)) relies solely on the textual content, predicting each subsequent token   \n185 based on the prior sequence of tokens without considering their spatial layouts. Layout-aware next   \n186 token prediction (Fig. 4(b)), however, interleaves the spatial information encoded by SLP $(i.e.,\\,b^{i})$   \n187 with the text tokens (i.e., $t^{i}$ ). This integration considers both the content and its layout within the   \n188 document, leading to a richer, more precise understanding of both the structure and the content. ", "page_idx": 4}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/3f54faba79b2b00df2c244248da4c2bcda37009889757c6326ba54812e8de01c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Similarly, primary objective of LNTP is to maximize the likelihood of its predictions for the next token. Thus the loss function is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\frac{1}{T}\\sum_{i=1}^{T}\\log P\\left(t^{i}\\mid t^{1},t^{2},\\ldots,t^{i-1}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P\\left(t^{i}\\mid t^{1},t^{2},\\ldots,t^{i-1}\\right)$ represents the probability of $i^{t h}$ token $t^{i}$ given the sequence of preceding tokens $t^{1},t^{2},\\ldots,t^{i-1}$ , as predicted by the model. Note that we compute the loss only for text tokens, excluding bounding box tokens. During pre-training, our goal is to enhance the alignment between spatial layouts and textual modality, while preserving the LLM\u2019s inherent knowledge as much as possible. Thus, we freeze the LLMs and only update ", "page_idx": 4}, {"type": "text", "text": "201 It is important to note that the proposed Layout-aware Next Token Prediction is a completely self  \n202 supervised pre-training procedure, unlike previous works that require human annotations of document   \n203 structure data or synthetic data generated by larger LLMs such as GPT-4 [21]. Thus, LNTP facilitates   \n204 the creation of large-scale, high-fidelity pre-training datasets at minimal cost.   \n206 OCR engines typically process text from top to bottom and left to right. This order is also adopted as   \n207 the input sequence for current OCR-based LLMs [19, 21]. However, modern LLMs often exhibit   \n208 a strong inductive bias toward the positions of input tokens, influenced by designs such as Rotary   \n209 Position Embeddings (RoPE) [38]. Specifically, tokens that are close together in the input sequence   \n210 are likely to receive higher attention scores, which is advantageous for processing standard text   \n211 sequences. Such inductive bias brings cons and pros.   \n212 Consider the example illustrated in Fig. 5, where the OCR input text reads: \u201c ... Change, 1.30, $G S T\\%$ ,   \n213 Amt(RM), GST(RM), Total(RM), SR, 6, 17.64, 1.06, 18.70 ... \u201d. If the question posed is \u201cWhat is the   \n214 value of the field Change?\u201d (highlighted in a blue box), the model easily identifies \u201c $^{1.30^{\\,\\bullet}}$ as it is   \n215 closely positioned to the word \u201cChange\u201d in the sequence. However, for a more challenging query like   \n216 \u201cWhat is the value of the field Total(RM)?\u201d (highlighted in a red box), the model struggles to determine   \n217 the correct answer due to the presence of multiple subsequent numbers closed to \u201cTotal(RM)\u201d.   \n218 LayTextLLM integrates spatial layouts with textual data, reducing reliance on input sequence order.   \n219 Thus, we posit that shuffling the OCR input order could enhance the resilience of LayTextLLM in   \n220 discerning relevant information irrespective of token proximity in the sequence.   \n221 Specifically, we propose Shuffled-OCR Supervised Fine-tuning (SSFT) that randomly shuffles the   \n222 order of OCR-derived text in a certain proportion of examples. The range of exploration for the   \n223 shuffling ratio can be found in Tab. 7 and $20\\%$ shuffled ratio is applied. The training objective is   \n224 equivalent to predicting the next tokens, but in this scenario, only the tokens of the response are used   \n225 to compute loss. During SSFT, we unfreeze all parameters including those of LLMs. Experimental   \n226 results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making   \n227 it more robust to disruptions in input token order. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "228 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 Pre-training data In our training process, we exclusively use open-source data to facilitate replica  \n231 tion. We collect data from two datasets for pre-training: (1) IIT-CDIP Test Collection 1.0 [39] and   \n232 (2) DocBank [40]. The IIT-CDIP Test Collection 1.0 comprises an extensive repository of more than   \n233 16 million document pages. DocBank consists of 500K documents, each presenting distinct layouts   \n234 with a single page per document. For training efficiency, we choose to utilize the entire DocBank   \n235 dataset and only subsample 5 million pages from the IIT-CDIP collection 1.0.   \n236 SFT data For document-oriented VQA, we select Document Dense Description (DDD) and   \n237 Layout-aware SFT data used in Luo et al. [21], which are two synthetic datasets generated by   \n238 GPT-4. Besides, DocVQA [41], InfoVQA [42], ChartQA [43], VisualMRC [44] is included   \n239 following [12]. For KIE task, we select SROIE [45], CORD [46], FUNSD [47], POIE [48] datasets   \n240 following [12, 19, 21]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "241 4.2 Implementation Detail ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "242 The LLM component of LayTextLLM is initialized from the Llama2-7B-base [36], which is a widely  \n243 used backbone. Other parameters including SLP and P-LoRA are randomly initialized. During   \n244 pre-training, the LLM is frozen, and the parameters of SLP and P-LoRA modules are updated. During   \n245 SFT, all parameters are fine-tuned. Other detailed setup can be found in Appendix B.   \n246 We have configured the model with three versions of LayTextLLM for a side-by-side comparison   \n247 under different settings. Aligned with Luo et al. [21], the first version, LayTextLL $\\mathbf{M}_{z e r o}$ , is   \n248 trained exclusively with DDD and Layout-aware SFT data. Building upon this, and in alignment   \n249 with the setting of Liu et al. [12], we introduce the DocVQA, InfoVQA, and ChartQA training   \n250 sets to the dataset pool for our second version, termed LayTextLL $\\mathbf{M}_{v q a}$ . Finally, we incorporate   \n251 a comprehensive suite of KIE datasets\u2014FUNSD, CORD, POIE, SROIE, and VisualMRC\u2014as   \n252 described by Wang et al. [19], creating our most extensive version, LayTextLL $\\mathbf{\\nabla}M_{a l l}$ . Note that all   \n253 versions are based on the same pre-trained LayTextLLM weight. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/cf9fe3b845d5b38bdcc6fd49be6f89945da2c456ef840e3f83d4bf75c070e875.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison with SOTA OCR-free MLLMs. \u2217indicates the training set used. "], "page_idx": 6}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/c72b25e9c2bc3891b4c2153b9c2c5f10459fb8007451d660a2b5ea47a14ba8e9.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison with other OCR-based methods. \u2217indicates the training set used. "], "page_idx": 6}, {"type": "text", "text": "254 4.3 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 OCR-free baselines In the category of OCR-free MLLMs, we have chosen the following SOTA   \n256 models as our strong baselines due to their superior performance in both document-oriented VQA   \n257 and KIE tasks. These include UniDoc [10], DocPedia [9], Monkey [49], InternVL [50], InternLM  \n258 XComposer2 [15], TextMonkey, and TextMonkey $^{+}$ [12].   \n259 OCR-based baselines For OCR-based baseline models, we implemented a basic approach using   \n260 only OCR-derived text as input. This was done using two versions: Llama2-7B-base and Llama2-   \n261 7B-chat. We also adapted the coordinate-as-tokens scheme from He et al. [23] for these models,   \n262 resulting in two new variants: Llama2-7B-base $c c o o r$ and Llama2-7B-cha $\\because c o o r$ . It\u2019s important to note   \n263 that we did not employ the ICL strategy with these models, as it would significantly exceed their   \n264 maximum sequence length constraints. Additionally, we included results from a stronger baseline   \n265 using the ChatGPT Davinci-003 (175B) model [23], termed Davinci- $\\mathbf{003-175B}_{c o o r}$ . One other recent   \n266 SOTA OCR-based approach, DocLLM [19] is also considered in our analysis. Finally, LayoutLLM   \n267 and LayoutLL ${\\bf{\\nabla}}{\\bf{M}}_{C o T}$ [21], which integrates visual cues, text and layout is also included. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "268 4.4 Evaluation Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "269 To ensure a fair comparison with OCR-free methods, we adopted the accuracy metric, where a   \n270 response from the model is considered correct if it fully captures the ground truth. This approach   \n271 aligns with the evaluation criteria described by [9, 10, 12]. To further enhance the comparability with   \n272 other OCR-based methods, we conducted additional evaluations using original metrics specific to   \n273 certain datasets, such as F1 score [19, 23], ANLS [19, 21, 51] and CIDEr [19, 52]. ", "page_idx": 6}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/784e71e15feebf56e11d4be5cd6ab4bd5a7ea5872b3342e60ee4d01eb47432ee.jpg", "table_caption": [], "table_footnote": ["Table 3: Comparison with LayoutLLM. \u2212indicates that the cleaned test set used in Luo et al. [21]. "], "page_idx": 7}, {"type": "text", "text": "274 4.5 Quantitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "75 Comparison with SOTA OCR-free Methods The experimental results shown in Tab. 1 demon  \n76 strate the outstanding performance of the LayTextLLM series across various tasks. Note that the   \n77 results for ChartQA are reported in Appendix E due to concerns about fairness in comparison, as   \n78 the dataset does not include OCR-derived results and we used in-house OCR tools instead. Firstly,   \n79 LayTextL $\\boldsymbol{\\mathrm{LM}}_{z e r o}$ significantly outperforms previous SOTA OCR-free methods, such as TextMon  \n80 key [12], in zero-shot capabilities, even when these methods use the training set of the dataset. For   \n81 example, in the DocVQA and InfoVQA datasets, LayTextL $\\boldsymbol{\\mathrm{LM}}_{z e r o}$ achieves accuracies of $71.8\\%$ and   \n82 $33.8\\%$ , respectively, which are markedly higher than existing OCR-free methods such as TextMonkey   \n83 and InternLM-XComposer2. When fine-tuned with corresponding datasets, LayTextLLM shows even   \n84 greater performance improvements, particularly in document-oriented VQA datasets. Specifically,   \n85 its accuracies on DocVQA and InfoVQA increase to $77.4\\%$ and $66.1\\%$ , respectively, demonstrating   \n86 the model\u2019s strong ability to leverage task-specific data. Additionally, LayTextL $.\\mathrm{LM}_{z e r o}$ excels in   \n87 KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of $86.7\\%$ and   \n88 $66.1\\%$ , respectively. These results significantly surpass those of previous SOTA OCR-free model (i.e.,   \n89 TextMonkey $^+$ ) by margins of $40.5\\%$ and $34.1\\%$ , respectively. This significant performance gain is   \n90 likely due to these datasets containing low-resolution images that are too blurred for current MLLMs   \n91 to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.   \n92 Comparison with SOTA OCR-based Methods For comprehensive comparison, we have also   \n93 conducted correspinding experiments to align with OCR-based methods [19, 21]. The experimental   \n94 results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM   \n95 models compared to pure OCR-based SOTA methods such as DocLLM [19]. Specifically, when   \n96 comparing with DocLLM, LayTextLLM $z e r o$ demonstrates notably superior performance, with even its   \n97 zero-shot capabilities being competitive with supervised SFT approaches. We believe that the subpar   \n98 performance of DocLLM is likely due to its use of cross-attention and the masked span pre-training   \n99 tasks [53], which fail to leverage the autoregressive features of LLMs effectively. Similarly, when   \n00 contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLL $\\boldsymbol{.}\\boldsymbol{\\mathrm{M}}_{z e r o}$ again outperforms   \n01 significantly. This disparity in performance can be attributed to the following three reasons: (1) The   \n02 coordinate-as-tokens approach tends to introduce an excessive number of tokens, often exceeding the   \n03 pre-defined maximum length of Llama2-7B (i.e., 4096). Consequently, this leads to a lack of crucial   \n04 OCR information, resulting in hallucination and subpar performance. (2) When re-implementing the   \n05 coordinate-as-tokens method with Llama2-7B, we did not introduce the ICL strategy, as it would   \n06 contribute additional length to the input sequence. (3) The coordinate-as-tokens approach necessitates   \n07 a considerably larger-sized LLM to comprehend the numerical tokens effectively.   \n308 In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as   \n309 shown in Tab. 3. In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating   \n310 our capability to leverage OCR-based results effectively. However, we fall short on document  \n311 oriented VQA tasks since answering some questions that are strongly related to vision information   \n312 may challenge our approach. Two main reasons may well explain this performance discrepancy:   \n313 (1) The visual encoder in LayoutLLM provides additional visual information. (2) LayoutLLM   \n314 incorporates the Chain-of-Thought (CoT) mechanism to model contextual information while it is   \n315 not used in our approach. However, when fine-tuned with tailored data, LayTextLLM significantly   \n316 outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data. More qualitative   \n317 example demonstrates can be found in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 4.6 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/600b08b83bbd79a449c9feb84843e22b303d0c2eed7c2e18324e82b2bd7ab9bb.jpg", "table_caption": [], "table_footnote": ["Table 4: Ablations on pre-training and SFT component of LayTextLLM (Accuracy). "], "page_idx": 8}, {"type": "text", "text": "319 Ablations To better assess the utility of Layout-aware Next Token Prediction and Shuffled-OCR   \n320 Supervised Fine-tuning in LayTextLLM, an ablation study was performed (see Tab. 4). Details on   \n321 the training setup for all variants are provided in Appendix B. It is evident that both LNTP and   \n322 SSFT significantly enhance the utility of LayTextLLM. Specifically, disabling LNTP results in an $8\\%$   \n323 decrease in performance on VQA tasks and a $5.5\\%$ decrease on KIE tasks. Disabling SSFT leads to a   \n324 decrease in average accuracy by $6.7\\%$ and $2.6\\%$ for VQA and KIE tasks, respectively.   \n325 Sequence Length Tab. 5 presents statistics on the average input sequence length across different   \n326 datasets. Intriguingly, despite interleaving bounding box tokens, LayTextLLM consistently exhibits   \n327 the shortest sequence length in three out of four datasets, even surpassing DocLLM, which is coun  \n328 terintuitive. We attribute this to the tokenizer mechanism. For example, using tokenizer.encode(), a   \n329 single word from the OCR engine, like \u201cInternational\u201d is encoded into a single ID [4623]. Conversely,   \n330 when the entire OCR output is processed as one sequence, such as \u201c... CPC,International,Inc...\u201d, the   \n331 word \u201cInternational\u201d is split into two IDs [17579, 1288], corresponding to \u201cIntern\u201d and \u201cational\u201d   \n332 respectively. This type of case occurs frequently, more discussion in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/cafaf4696d540bf60157420f6377a24c4ff82159bbd4729efec649568d93e2bb.jpg", "table_caption": [], "table_footnote": ["Table 5: Average sequence length of each data for different methods using Llama2 tokenizer. "], "page_idx": 8}, {"type": "text", "text": "333 5 Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone   \n335 does not suffice for all real-world applications. There are some instances, particularly in chart analysis,   \n336 where reasoning must be based solely on visual cues (e.g. size, color)\u2014a challenge that remains   \n337 unmet. Questions such as \u201cWhat is the difference between the highest and the lowest green bar?\u201d   \n338 illustrate this gap. The ChartQA results, detailed in Appendix E, also underscore these limitations.   \n339 Addressing these challenges highlights the urgent need for future enhancements that integrate visual   \n340 cue within the capabilities of LayTextLLM. ", "page_idx": 8}, {"type": "text", "text": "341 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "342 We propose LayTextLLM for various VRDU tasks, in which spatial layouts and textual data are   \n343 seamlessly interleaved to make more accurate prediction by introducing a innovative Spatial Layout   \n344 Projector. Two tailored training tasks \u2014 Layout-aware Next Token Prediction and Shuffled-OCR   \n345 Supervised Fine-tuning \u2014 are designed to improve the comprehension of document layouts. Extensive   \n346 experiments confirm the effectiveness of LayTextLLM. ", "page_idx": 8}, {"type": "text", "text": "347 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "348 [1] OpenAI:Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren  \n349 ciaLeoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red   \n350 Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavar  \n351 ian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner,   \n352 Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim   \n353 Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany   \n354 Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek   \n355 Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,   \n356 HyungWon Chung, Dave Cummings, and Jeremiah Currier. Gpt-4 technical report. arXiv   \n357 preprint arXiv:2303.08774, Dec 2023.   \n358 [2] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and   \n359 Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). arXiv preprint   \n360 arXiv:2309.17421, Sep 2023.   \n361 [3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,   \n362 Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly   \n363 capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n364 [4] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.   \n365 [5] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean  \n366 baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.   \n367 Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv   \n368 preprint arXiv:2403.05530, 2024.   \n369 [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,   \n370 Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n371 [7] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng   \n372 Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language   \n373 understanding. arXiv preprint arXiv:2403.05525, 2024.   \n374 [8] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,   \n375 Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv   \n376 preprint arXiv:2403.04652, 2024.   \n377 [9] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang. Docpedia: Un  \n378 leashing the power of large multimodal model in the frequency domain for versatile document   \n379 understanding. arXiv preprint arXiv:2311.11810, 2023.   \n380 [10] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.   \n381 Unidoc: A universal large multimodal model for simultaneous text detection, recognition,   \n382 spotting and understanding. arXiv preprint arXiv:2308.11592, 2023.   \n383 [11] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin   \n384 Jin, Fei Huang, et al. mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document   \n385 understanding. arXiv preprint arXiv:2403.12895, 2024.   \n386 [12] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.   \n387 Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint   \n388 arXiv:2403.04473, 2024.   \n389 [13] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li,   \n390 Siqi Wang, Lei Liao, et al. Textsquare: Scaling up text-centric visual instruction tuning. arXiv   \n391 preprint arXiv:2404.12803, 2024.   \n392 [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong,   \n393 Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to   \n394 commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821,   \n395 2024.   \n396 [15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei,   \n397 Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering   \n398 free-form text-image composition and comprehension in vision-language large model. arXiv   \n399 preprint arXiv:2401.16420, 2024.   \n400 [16] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,   \n401 Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision   \n402 language models. arXiv preprint arXiv:2403.18814, 2024.   \n403 [17] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae   \n404 Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https:   \n405 //llava-vl.github.io/blog/2024-01-30-llava-next/.   \n406 [18] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai   \n407 Xu, Chenliang Li, Junfeng Tian, et al. mPLUG-DocOwl: Modularized multimodal large   \n408 language model for document understanding. arXiv:2307.02499, 2023.   \n409 [19] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur,   \n410 Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. Docllm: A layout-aware generative   \n411 language model for multimodal document understanding. arXiv preprint arXiv:2401.00908,   \n412 2023.   \n413 [20] Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu Sun, Ramya Sree Boppana,   \n414 Zilong Wang, Jiaqi Mu, Hao Zhang, and Nan Hua. Lmdx: Language model-based document   \n415 information extraction and localization. arXiv preprint arXiv:2309.10952, 2023.   \n416 [21] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout   \n417 instruction tuning with large language models for document understanding. CVPR 2024, 2024.   \n418 [22] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:   \n419 Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195,   \n420 2023.   \n421 [23] Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and Heng Tao Shen. Icl-d3ie:   \n422 In-context learning with diverse demonstrations updating for document information extraction.   \n423 In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19485\u2013   \n424 19494, 2023.   \n425 [24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao   \n426 Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning   \n427 perception with language models. arXiv:2302.14045, 2023.   \n428 [25] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu   \n429 Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824,   \n430 2023.   \n431 [26] Wonseok Hwang, Jinyeong Yim, Seung-Hyun Park, Sohee Yang, and Minjoon Seo. Spatial   \n432 dependency parsing for semi-structured document information extraction. Cornell University -   \n433 arXiv,Cornell University - arXiv, May 2020.   \n434 [27] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm:   \n435 Pre-training of text and layout for document image understanding. In Proceedings of the 26th   \n436 ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Aug 2020.   \n437 doi: 10.1145/3394486.3403172. URL http://dx.doi.org/10.1145/3394486.3403172.   \n438 [28] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei   \n439 Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi  \n440 modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual   \n441 Meeting of the Association for Computational Linguistics and the 11th International Joint   \n442 Conference on Natural Language Processing (Volume 1: Long Papers), Jan 2021. doi: 10.18653/   \n443 v1/2021.acl-long.201. URL http://dx.doi.org/10.18653/v1/2021.acl-long.201.   \n444 [29] Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park.   \n445 Bros: A pre-trained language model focusing on text and layout for better key information   \n446 extraction from documents. Proceedings of the AAAI Conference on Artificial Intelligence,   \n447 page 10767\u201310775, Jul 2022. doi: 10.1609/aaai.v36i10.21322. URL http://dx.doi.org/   \n448 10.1609/aaai.v36i10.21322.   \n449 [30] Zineng Tang, Zhenfeng Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Zhu C, Michael Zeng,   \n450 Zhang Cha, and Mohit Bansal. Unifying vision, text, and layout for universal document   \n451 processing. Cornell University - arXiv,Cornell University - arXiv, Dec 2022.   \n452 [31] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for   \n453 document ai with unified text and image masking. In Proceedings of the 30th ACM International   \n454 Conference on Multimedia, pages 4083\u20134091, 2022.   \n455 [32] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,   \n456 Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document   \n457 understanding transformer. In European Conference on Computer Vision, pages 498\u2013517, 2022.   \n458 [33] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisensch  \n459 los, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct:   \n460 Screenshot parsing as pretraining for visual language understanding. In International Confer  \n461 ence on Machine Learning, pages 18893\u201318912. PMLR, 2023.   \n462 [34] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.   \n463 Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint   \n464 arXiv:2306.17107, 2023.   \n465 [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances   \n466 in neural information processing systems, 36, 2024.   \n467 [36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n468 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \n469 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n470 [37] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan   \n471 Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction   \n472 model. arXiv:2304.15010, 2023.   \n473 [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:   \n474 Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n475 [39] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. Building a test   \n476 collection for complex document information processing. In Proceedings of the 29th annual   \n477 international ACM SIGIR conference on Research and development in information retrieval,   \n478 Aug 2006. doi: 10.1145/1148170.1148307. URL http://dx.doi.org/10.1145/1148170.   \n479 1148307.   \n480 [40] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou.   \n448812 Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th   \nInternational Conference on Computational Linguistics, Jan 2020. doi: 10.18653/v1/2020.   \n483 coling-main.82. URL http://dx.doi.org/10.18653/v1/2020.coling-main.82.   \n484 [41] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on   \n485 document images. In Proceedings of the IEEE/CVF winter conference on applications of   \n486 computer vision, pages 2200\u20132209, 2021.   \n487 [42] Minesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawa  \n488 har. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of   \n489 Computer Vision, pages 1697\u20131706, 2022.   \n490 [43] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA:   \n491 A benchmark for question answering about charts with visual and logical reasoning. In   \n492 Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association   \n493 for Computational Linguistics: ACL 2022, pages 2263\u20132279, Dublin, Ireland, May 2022.   \n494 Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL   \n495 https://aclanthology.org/2022.findings-acl.177.   \n496 [44] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehen  \n497 sion on document images. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n498 volume 35, pages 13878\u201313888, 2021.   \n499 [45] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and   \n500 CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019   \n501 International Conference on Document Analysis and Recognition (ICDAR), pages 1516\u20131520.   \n502 IEEE, 2019.   \n503 [46] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and   \n504 Hwalsuk Lee. Cord: a consolidated receipt dataset for post-ocr parsing. In Workshop on   \n505 Document Intelligence at NeurIPS 2019, 2019.   \n506 [47] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form   \n507 understanding in noisy scanned documents. In 2019 International Conference on Document   \n508 Analysis and Recognition Workshops (ICDARW), volume 2, pages 1\u20136. IEEE, 2019.   \n509 [48] Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang   \n510 Bai. Visual information extraction in the wild: practical dataset and end-to-end solution. In   \n511 International Conference on Document Analysis and Recognition, pages 36\u201353. Springer, 2023.   \n512 [49] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang   \n513 Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large   \n514 multi-modal models. arXiv preprint arXiv:2311.06607, 2023.   \n515 [50] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong   \n516 Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning   \n517 for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n518 [51] Liangcai Gao, Yilun Huang, Herve Dejean, Jean-Luc Meunier, Qinqin Yan, Yu Fang, Florian   \n519 Kleber, and Eva Lang. Icdar 2019 competition on table detection and recognition (ctdar). In   \n520 International Conference on Document Analysis and Recognition, 2019.   \n521 [52] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image   \n522 description evaluation. In Proceedings of the IEEE conference on computer vision and pattern   \n523 recognition, pages 4566\u20134575, 2015.   \n524 [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,   \n525 Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified   \n526 text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n527 [54] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words   \n528 with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual   \n529 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n530 1715\u20131725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:   \n531 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "532 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "533 A Qualitative Examples ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "534 Qualitative examples of document-oriented VQA (upper row) and KIE (bottom row) are shown in   \n535 Fig. 6. The results indicate that LayTextLLM is highly effective in utilizing spatial layout information   \n536 to make more accurate predictions for these challenging examples. For example, in the upper   \n537 right figure, many numeric texts in the receipt act as noise for the baseline method. In contrast,   \n538 LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by   \n539 the other examples, underscoring the utility of LayTextLLM. ", "page_idx": 12}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/6cf91568f7d4cb3f7580b80d71ad089d2a68190c7c15d7c258a8c0d6e81fa28e.jpg", "img_caption": ["Figure 6: Qualitative comparison with the baseline method. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "540 B Implementation Detail ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "541 All training and inference procedures are conducted on eight NVIDIA A100 GPUs. ", "page_idx": 13}, {"type": "text", "text": "542 Training LayTextLLM is initialized with Llama2-7B-Base model, the pre-training, SFT, and other   \n543 model hyper-parameters can be seen in Tab. 6. Please note that all variants of LayTextLLM, including   \n544 those utilized in ablation studies, are trained in accordance with the SFT settings. All baseline results   \n545 are sourced from their respective original papers, with the exception of the Llama2-7B series and the   \n546 Llama2- ${\\mathcal{I}}{\\mathbf{B}}_{\\mathrm{coor}}$ series. These were re-implemented and can be referenced in [21, 23].   \n547 Inference For the document-oriented VQA test set, we use the original question-answer pairs as   \n548 the prompt and ground truth, respectively. For Key Information Extraction (KIE) tasks, we reformat   \n549 the key-value pairs into a question-answer format, as described in [12, 19, 21]. Additionally, for the   \n550 FUNSD dataset, we focus our testing on the entity linking annotations as described in [21].   \n551 To eliminate the impact of randomness on evaluation, no sampling methods are employed during   \n552 testing for any of the models. Instead, beam search with a beam size of 1 is used for generation across   \n553 all models. Additionally, the maximum number of new tokens is set to 512, while the maximum   \n554 number of input tokens is set to 4096. ", "page_idx": 13}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/90ed5c8e0f725546360667b266b6a22186c56f638230ae96e7ac3fd34beaa3ae.jpg", "table_caption": ["Table 6: LayTextLLM trainng Hyper-parameters. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "555 C Discussion of Input Sequence Length ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "556 As mentioned in Section 4.6, it is intriguing that LayTextLLM has fewer input sequences than Do  \n557 cLLM, which is counterintuitive given that LayTextLLM interleaves bounding box tokens, typically   \n558 resulting in longer sequence lengths. We attribute this to the Byte Pair Encoding (BPE) tokenizers [54]   \n559 prevalently used in modern LLMs such as Llama2.   \n560 BPE operates by building a vocabulary of commonly occurring subwords (or token pieces) derived   \n561 from the training data. Initially, it tokenizes the text at the character level and then progressively   \n562 merges the most frequent adjacent pairs of characters or sequences. The objective is to strike a   \n563 balance between minimizing vocabulary size and maximizing encoding efficiency.   \n564 Thus, when tokenizing a single word like \u201cInternational\u201d on its own, the tokenizer might identify it   \n565 as a common sequence in the training data and encode it as a single token. This is especially likely if   \n566 \u201cInternational\u201d frequently appears as a standalone word in the training contexts. However, when the   \n567 word \u201cInternational\u201d is part of a larger sequence of words such as including in a long sequence of   \n568 OCR-derived texts like \u201c...335 CPC,International,Inc...\u201d, the context changes. The tokenizer might   \n569 split \u201cInternational\u201d into sub-tokens like \u201cIntern\u201d and \u201cational\u201d because, in various contexts within   \n570 the training data, these subwords might appear more frequently in different combinations or are more   \n571 useful for the model to understand variations in meaning or syntax.   \n572 When using LayTextLLM, we input word-level OCR results into the tokenizer, typically resulting in   \n573 the former situation, where words are encoded as single tokens. Conversely, with DocLLM, the entire   \n574 OCR output is processed as one large sequence, leading to the latter situation and a longer sequence   \n575 length than in LayTextLLM. This difference underscores the utility of LayTextLLM in achieving   \n576 both accuracy and inference efficiency due to its shorter sequence length. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "577 D Shuffle Ratio Exploration ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "578 Tab. 7 presents the results of exploring training and testing shuffling ratios on the FUNSD dataset   \n579 using two different models: Llama2-7B-base and LayTextLLM. The table shows the performance of   \n580 these models at various shuffling ratios ( $100\\%$ , $50\\%$ , $20\\%$ , and $0\\%$ ).   \n581 LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffilng, which further   \n582 underscores the significance of interleaving spatial layouts with text. Particularly at the $100\\%$ shuffle   \n583 level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a   \n584 relatively higher performance. It is also interesting to note that Llama2-7B-base generally improves as   \n585 the shuffilng percentage decreases, whereas LayTextLLM performs best when $20\\%$ of the examples   \n586 with OCR-derived text are shuffled. This observation suggests that LayTextLLM effectively utilizes   \n587 spatial layouts and is less dependent on the sequence of input tokens. Therefore, a certain proportion   \n588 of shuffled examples can serve as adversarial examples to enhance the model\u2019s robustness, addressing   \n589 situations such as errors in the text order from the OCR engine, which are caused by subtle differences   \n590 in horizontal or vertical coordinates. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/37df39d44c02fec9b306d923064ae8cd73e3dae7ec2f92b6671b1d16424b74d1.jpg", "table_caption": [], "table_footnote": ["Table 7: Shuffling ratio exploration in FUNSD dataset. "], "page_idx": 15}, {"type": "text", "text": "591 E Results of ChartQA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "592 As shown in Fig. 7, the question-answer pairs in ChartQA [43] tend to involve the visual cues for   \n593 reasoning. However, with only text and layout information as input, the proposed LayTextLLM   \n594 inevitably have difficulties in reasoning visual-related information. Thus, on the ChartQA dataset,   \n595 LayTextLLM can hardly achieve better performance than previous methods that include visual inputs.   \n596 Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability   \n597 than UniDoc [10]. After incorporating the training set of ChartQA, the performance of LayTextLLM   \n598 can be boosted to $35.7\\%$ . Considering the importance of visual cues in ChartQA-like tasks, we will   \n599 try to involve the visual information into LayTextLLM in future work. ", "page_idx": 15}, {"type": "table", "img_path": "rSSpEmrN0C/tmp/971bbe6a0341e94f2ee40f6f237d2dc692a2a14f5198d21a2602d8b1e9edf0ba.jpg", "table_caption": ["Figure 7: A failure case of LayTextLLM on CharQA. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "rSSpEmrN0C/tmp/94df2cb961cb2880e894c3530aaf57b7a49560091d8ea22c719dc5fc2cd645a1.jpg", "img_caption": ["Table 8: Comparison with SOTA OCR-free MLLMs on ChartQA. \u2217indicates the training set used. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "600 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "602 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n603 paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Justification: We have detailed the contributions accurately in the abstract and introduction. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "616 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As discussed in Section. 5, we have listed some limitations of our work and shown corresponding failure cases in the supplementary material. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "48 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "649 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n650 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This work does not include theoretical results. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "664 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In Section 4.1, 4.2 and Appendix B, we have described the details of implementing and training the proposed model to ensure the reproducibility of our work. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "707 Answer: [No]   \n708 Justification: We would release our code after this paper is accepted.   \n709 Guidelines:   \n710 \u2022 The answer NA means that paper does not include experiments requiring code.   \n711 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n712 public/guides/CodeSubmissionPolicy) for more details.   \n713 \u2022 While we encourage the release of code and data, we understand that this might not be   \n714 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n715 including code, unless this is central to the contribution (e.g., for a new open-source   \n716 benchmark).   \n717 \u2022 The instructions should contain the exact command and environment needed to run to   \n718 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n719 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n720 \u2022 The authors should provide instructions on data access and preparation, including how   \n721 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n722 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n723 proposed method and baselines. If only a subset of experiments are reproducible, they   \n724 should state which ones are omitted from the script and why.   \n725 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n726 versions (if applicable).   \n727 \u2022 Providing as much information as possible in supplemental material (appended to the   \n728 paper) is recommended, but including URLs to data and code is permitted.   \n729 6. Experimental Setting/Details   \n730 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n731 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n732 results?   \n733 Answer: [Yes]   \n734 Justification: We have detailed the experimental setting and implementation details in   \n735 Section 4.1, 4.2 and Appendix B of the supplementary material.   \n736 Guidelines:   \n737 \u2022 The answer NA means that the paper does not include experiments.   \n738 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n739 that is necessary to appreciate the results and make sense of them.   \n740 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n741 material.   \n742 7. Experiment Statistical Significance   \n743 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n744 information about the statistical significance of the experiments?   \n745 Answer: [No]   \n746 Justification: Our deep learning model is designed for a complex task (requiring huge   \n747 computing resources) where traditional error bars are less informative due to the high   \n748 variability in model training and initialization. We ensured the robustness of our model   \n749 by fix the random seed during inference. In addition, comparative analysis with baseline   \n750 models demonstrated improvements in key performance areas, underscoring the practical   \n751 effectiveness of our approach. We acknowledge the limitation of not using traditional   \n752 statistical tests and suggest that future work could explore statistical significance in more   \n753 controlled settings.   \n754 Guidelines:   \n755 \u2022 The answer NA means that the paper does not include experiments.   \n756 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n757 dence intervals, or statistical significance tests, at least for the experiments that support   \n758 the main claims of the paper.   \n759 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n760 example, train/test split, initialization, random drawing of some parameter, or overall   \n761 run with given experimental conditions).   \n762 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n763 call to a library function, bootstrap, etc.)   \n764 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n765 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n766 of the mean.   \n767 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n768 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n769 of Normality of errors is not verified.   \n770 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n771 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n772 error rates).   \n773 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n774 they were calculated and reference the corresponding figures or tables in the text.   \n775 8. Experiments Compute Resources   \n776 Question: For each experiment, does the paper provide sufficient information on the com  \n777 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n778 the experiments?   \n779 Answer: [Yes]   \n780 Justification: We have reported the needed computer resources in Section B of the supple  \n781 mentary material.   \n782 Guidelines:   \n783 \u2022 The answer NA means that the paper does not include experiments.   \n784 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n785 or cloud provider, including relevant memory and storage.   \n786 \u2022 The paper should provide the amount of compute required for each of the individual   \n787 experimental runs as well as estimate the total compute.   \n788 \u2022 The paper should disclose whether the full research project required more compute   \n789 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n790 didn\u2019t make it into the paper).   \n791 9. Code Of Ethics   \n792 Question: Does the research conducted in the paper conform, in every respect, with the   \n793 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n794 Answer: [Yes]   \n795 Justification: The research in this paper conforms with the NeurIPS Code of Ethics in every   \n796 respect.   \n797 Guidelines:   \n798 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n799 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n800 deviation from the Code of Ethics.   \n801 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n802 eration due to laws or regulations in their jurisdiction).   \n803 10. Broader Impacts   \n804 Question: Does the paper discuss both potential positive societal impacts and negative   \n805 societal impacts of the work performed?   \n806 Answer: [NA]   \n807 Justification: There is no societal impact of the work performed.   \n808 Guidelines:   \n809 \u2022 The answer NA means that there is no societal impact of the work performed.   \n810 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n811 impact or why the paper does not address societal impact.   \n812 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n813 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n814 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n815 groups), privacy considerations, and security considerations.   \n816 \u2022 The conference expects that many papers will be foundational research and not tied   \n817 to particular applications, let alone deployments. However, if there is a direct path to   \n818 any negative applications, the authors should point it out. For example, it is legitimate   \n819 to point out that an improvement in the quality of generative models could be used to   \n820 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n821 that a generic algorithm for optimizing neural networks could enable people to train   \n822 models that generate Deepfakes faster.   \n823 \u2022 The authors should consider possible harms that could arise when the technology is   \n824 being used as intended and functioning correctly, harms that could arise when the   \n825 technology is being used as intended but gives incorrect results, and harms following   \n826 from (intentional or unintentional) misuse of the technology.   \n827 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n828 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n829 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n830 feedback over time, improving the efficiency and accessibility of ML).   \n831 11. Safeguards   \n832 Question: Does the paper describe safeguards that have been put in place for responsible   \n833 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n834 image generators, or scraped datasets)?   \n835 Answer: [NA]   \n836 Justification: All the datasets used in this paper are publicly available and they contain no   \n837 unsafe images.   \n838 Guidelines:   \n839 \u2022 The answer NA means that the paper poses no such risks.   \n840 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n841 necessary safeguards to allow for controlled use of the model, for example by requiring   \n842 that users adhere to usage guidelines or restrictions to access the model or implementing   \n843 safety filters.   \n844 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n845 should describe how they avoided releasing unsafe images.   \n846 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n847 not require this, but we encourage authors to take this into account and make a best   \n848 faith effort.   \n849 12. Licenses for existing assets   \n850 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n851 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n852 properly respected?   \n853 Answer: [Yes]   \n854 Justification: For the used datasets and pre-trained models, we have cited their corresponding   \n855 works.   \n856 Guidelines:   \n857 \u2022 The answer NA means that the paper does not use existing assets.   \n858 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n859 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n860 URL.   \n861 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n862 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n863 service of that source should be provided.   \n864 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n865 package should be provided. For popular datasets, paperswithcode.com/datasets   \n866 has curated licenses for some datasets. Their licensing guide can help determine the   \n867 license of a dataset.   \n868 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n869 the derived asset (if it has changed) should be provided.   \n870 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n871 the asset\u2019s creators.   \n872 13. New Assets   \n873 Question: Are new assets introduced in the paper well documented and is the documentation   \n874 provided alongside the assets?   \n875 Answer: [NA]   \n876 Justification: This paper does not release new assets   \n877 Guidelines:   \n878 \u2022 The answer NA means that the paper does not release new assets.   \n879 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n880 submissions via structured templates. This includes details about training, license,   \n881 limitations, etc.   \n882 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n883 asset is used.   \n884 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n885 create an anonymized URL or include an anonymized zip file.   \n886 14. Crowdsourcing and Research with Human Subjects   \n887 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n888 include the full text of instructions given to participants and screenshots, if applicable, as   \n889 well as details about compensation (if any)?   \n890 Answer: [NA]   \n891 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n892 Guidelines:   \n893 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n894 human subjects.   \n895 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n896 tion of the paper involves human subjects, then as much detail as possible should be   \n897 included in the main paper.   \n898 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n899 or other labor should be paid at least the minimum wage in the country of the data   \n900 collector.   \n901 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n902 Subjects   \n903 Question: Does the paper describe potential risks incurred by study participants, whether   \n904 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n905 approvals (or an equivalent approval/review based on the requirements of your country or   \n906 institution) were obtained?   \n907 Answer: [NA]   \n908 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n909 Guidelines:   \n910 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n911 human subjects.   \n912   \n913   \n914   \n915   \n916   \n917   \n918   \n919 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]