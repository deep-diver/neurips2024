{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-12-00", "reason": "This paper is foundational, describing a large language model that is a key component of many other models discussed in this paper."}, {"fullname_first_author": "Rohan Anil", "paper_title": "Gemini: A Family of Highly Capable Multimodal Models", "publication_date": "2023-12-00", "reason": "This paper introduces a family of multimodal models that are highly relevant to the document understanding task addressed in the main paper."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "publication_date": "2023-09-00", "reason": "This paper presents Qwen, a large language model that is used as a baseline for comparison in the main paper."}, {"fullname_first_author": "Hao Feng", "paper_title": "DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding", "publication_date": "2023-11-00", "reason": "This paper provides a strong OCR-based baseline for document understanding, which is compared against the proposed method in the main paper."}, {"fullname_first_author": "Yuliang Liu", "paper_title": "TextMonkey: An OCR-free Large Multimodal Model for Understanding Document", "publication_date": "2024-03-00", "reason": "This paper is a recent state-of-the-art approach for document understanding using LLMs without OCR, offering a key comparison point for the proposed method."}]}