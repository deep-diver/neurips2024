[{"figure_path": "1jG5ngXVs3/tables/tables_6_1.jpg", "caption": "Table 1: Main results. We apply our FlowTurbo on SiT-XL [24] and the 2-RF of InstaFlow [20] to perform class-conditional image generation and text-to-image generation, respectively. The image quality is measured by the FID 50K on ImageNet (256x256) and the FID 5K\u2193 on MS COCO 2017 (512x512). We use the suffix to represent the number of Heun's method block (H), pseudo corrector block (P), and the velocity refiner block (R). Our results demonstrate that FlowTurbo can significantly accelerate the inference of flow-based models while achieving better sampling quality.", "description": "This table presents the main results of the FlowTurbo method, comparing its performance against the baseline Heun's method for both class-conditional and text-to-image generation tasks.  It shows the FID (Fr\u00e9chet Inception Distance) scores, latency (in milliseconds per image), and FLOPs (floating point operations) for different configurations of FlowTurbo, indicating improvements in speed and image quality compared to the baseline.", "section": "4.1 Setups"}, {"figure_path": "1jG5ngXVs3/tables/tables_6_2.jpg", "caption": "Table 2: Comparisons with the state-of-the-arts. We compare the sampling quality and speed of different methods on ImageNet 256 \u00d7 256 class-conditional sampling. We demonstrate that FlowTurbo can significantly improve over the baseline SiT-XL [24] and achieves the fastest sampling (38 ms / img) and the best quality (2.12 FID) with different configurations.", "description": "This table compares the performance of FlowTurbo against other state-of-the-art models on ImageNet class-conditional image generation. The metrics used for comparison are sampling speed (latency in ms/img), FID score (Fr\u00e9chet Inception Distance, lower is better), Inception Score (IS, higher is better), Precision, and Recall.  The results show that FlowTurbo achieves superior performance in both speed and image quality compared to existing methods.", "section": "4.3 Comparisons to State-of-the-Arts"}, {"figure_path": "1jG5ngXVs3/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies. We evaluate the effectiveness of each component in FlowTurbo as well as the selection of some hyper-parameters. (a) We gradually add the components of FlowTurbo to the baseline and show that FlowTurbo can achieve over 50% acceleration with better sampling quality. (b) we experiment with different ranges of \u2206t and find \u2206t \u2208 (0.0, 0.12] yields relatively good results in all the situations. (c) (d) we show how the sampling quality/speed changes with the number of velocity refiner and pseudo corrector blocks.", "description": "This table presents the ablation study of the FlowTurbo framework. It shows the impact of adding each component (velocity refiner, pseudo corrector, and sample-aware compilation) on the FID score and latency.  It also analyzes the effect of different ranges for the hyperparameter \u2206t and the impact of changing the number of velocity refiners and pseudo correctors.", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_8_2.jpg", "caption": "Table 3: Ablation studies. We evaluate the effectiveness of each component in FlowTurbo as well as the selection of some hyper-parameters. (a) We gradually add the components of FlowTurbo to the baseline and show that FlowTurbo can achieve over 50% acceleration with better sampling quality. (b) we experiment with different ranges of \u0394t and find \u0394t \u2208 (0.0, 0.12] yields relatively good results in all the situations. (c) (d) we show how the sampling quality/speed changes with the number of velocity refiner and pseudo corrector blocks.", "description": "This table presents the ablation study of the FlowTurbo framework. It shows the impact of adding each component of FlowTurbo (lightweight velocity refiner, pseudo corrector, and sample-aware compilation) to a baseline Heun's method.  The study also explores the effects of different hyperparameters such as the time step size (\u0394t) and number of velocity refiners and pseudo correctors on FID and inference latency. The results demonstrate the effectiveness of each component and the optimal hyperparameter settings for achieving real-time image generation with high-quality.", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_8_3.jpg", "caption": "Table 4: Comparisons of different orders of the blocks. We compare the results of changing the orders of Heun's block (H), pseudo corrector block (P), and the velocity refiner block (R). Our results show applying the blocks in HNH PNP RNR order yields the best trade-off between generation quality and speed.", "description": "This table presents an ablation study on the order of applying different sampling blocks (Heun's method, pseudo corrector, and velocity refiner) within the FlowTurbo framework for class-conditional image generation.  By testing different sequences of these blocks, the authors demonstrate the optimal ordering (H, P, R) for achieving the best balance between sampling speed and image quality.", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_8_4.jpg", "caption": "Table 4: Comparisons of different orders of the blocks. We compare the results of changing the orders of Heun's block (H), pseudo corrector block (P), and the velocity refiner block (R). Our results show applying the blocks in HNH PNP RNR order yields the best trade-off between generation quality and speed.", "description": "This table presents an ablation study on the order of applying different sampling blocks within the FlowTurbo framework.  It compares four configurations: two with two repeated blocks ( [H1P1R1]x2 and H2P2R2 ), and two with three repeated blocks ( [H1P1R1]x3 and H3P3R3 ). The results show that applying the blocks in the order H, P, and then R provides the best trade-off between FID (sampling quality) and latency (sampling speed).", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_8_5.jpg", "caption": "Table 5: Ablation of refiner architectures.", "description": "This table shows the ablation study of different architectures for the velocity refiner. Two different architectures are compared: SiT-S (a smaller version of SiT-XL) and a single block of SiT-XL.  The results demonstrate that using a block of SiT-XL as the refiner yields slightly better FID scores compared to SiT-S.", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_17_1.jpg", "caption": "Table 6: Ablation of the number of the velocity refiners. We change the number of velocity refiners and compare the sampling quality of each configuration. We find there exists a optimal number of velocity refiners to achieve the lowest FID.", "description": "This table presents an ablation study on the number of velocity refiners used in the FlowTurbo framework.  By varying the number of refiners while keeping other hyperparameters constant, the authors observe that there's an optimal number of refiners that minimizes the Fr\u00e9chet Inception Distance (FID) score, a metric indicating image quality.  The table shows the FID and inference latency for different configurations, highlighting the trade-off between speed and quality.", "section": "4.4 Analysis"}, {"figure_path": "1jG5ngXVs3/tables/tables_18_1.jpg", "caption": "Table 7: Comparisons with state-of-the-art methods on text-to-image generation. We compare our FlowTurbo with state-of-the-art diffusion models (15 steps DPM-Solver++ [23]) and show our FlowTurbo enjoys favorable trade-offs between sampling quality and speed.", "description": "This table compares the performance of FlowTurbo against other state-of-the-art methods for text-to-image generation.  It focuses on the trade-off between sampling speed (latency) and image quality (FID).  The results show that FlowTurbo achieves a favorable balance, providing competitive FID scores with significantly faster generation times than other diffusion models that use 15 steps of DPM-Solver++.", "section": "4.3 Comparisons to State-of-the-Arts"}]