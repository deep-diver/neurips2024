[{"type": "text", "text": "Reconstructing the Image Stitching Pipeline: Integrating Fusion and Rectangling into a Unified Inpainting Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziqi Xie1, Weidong Zhao1,2\u2217, Xianhui Liu1,2, Jian Zhao1, Ning Jia1,2 ", "page_idx": 0}, {"type": "text", "text": "1 College of Computer Science and Technology, Tongji University 2 College of Electronics and Information Engineering, Tongji University {xieziqi, wd, lxh, zjtju1919, jianing7072}@tongji.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning-based image stitching pipelines are typically divided into three cascading stages: registration, fusion, and rectangling. Each stage requires its own network training and is tightly coupled to the others, leading to error propagation and posing significant challenges to parameter tuning and system stability. This paper proposes the Simple and Robust Stitcher (SRStitcher), which revolutionizes the image stitching pipeline by simplifying the fusion and rectangling stages into a unified inpainting model, requiring no model training or fine-tuning. We reformulate the problem definitions of the fusion and rectangling stages and demonstrate that they can be effectively integrated into an inpainting task. Furthermore, we design the weighted masks to guide the reverse process in a pre-trained largescale diffusion model, implementing this integrated inpainting task in a single inference. Through extensive experimentation, we verify the interpretability and generalization capabilities of this unified model, demonstrating that SRStitcher outperforms state-of-the-art methods in both performance and stability. Code: https://github.com/yayoyo66/SRStitcher ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image stitching is a fundamental problem in computer vision, which aims to obtain a larger field of view by merging multiple overlapping images [5]. As illustrated in Figure 1(a), the current deep learning-based image stitching pipeline is typically structured into three sequential stages: (1) Registration Stage. The first stage takes the original image pairs to estimate warping matrices, which are then used to align images. Current learning-based methods focus on designing the homography estimation networks to address the registration problem [22, 21, 7]. (2) Fusion Stage. The second stage merges the aligned images into a single fusion image. Present research in this domain is generally classified into reconstruction-based (recon-based) and seam-based methods. Recon-based methods [32, 36, 33] typically use the encoder-decoder networks to perform pixel-wise reconstruction of the fusion image. While seam-based methods [35, 11] focus on identifying the optimal seams to eliminate the fusion ghosting. (3) Rectangling Stage. The final stage transforms the irregularly shaped fusion image into a standard rectangular format. There are only a few deep learning-based studies for this stage [34, 56], and they are all supervised methods with requirements for labeling data. ", "page_idx": 0}, {"type": "text", "text": "Annoyingly, the cascaded structure of current image stitching pipelines poses significant challenges for training optimization and parameter tuning. Furthermore, errors from early stages tend to propagate to later stages, significantly degrading the performance of later processes. The representative image stitching methods UDIS [33]and $\\mathrm{UDIS++}$ [35] both struggle to effectively fuse images with registration errors, as shown in Figure $1\\,\\textsuperscript{\\textregistered}$ and $\\circledcirc$ . In the rectangling stage (Figure $1\\textcircled{3}$ ), the prominent rectangling method DeepRectangling (DR) [34] also fails to adequately flil gaps, leaving visible black spaces at image boundaries. ", "page_idx": 0}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/1a30fe3d9d237562456c26c85b4b10a8ff39bef268c44d91c2b21ec153c899fa.jpg", "img_caption": ["Figure 1: Comparison between existing pipeline and SRStitcher. Process $\\textcircled{1}$ is implemented by UDIS [33], process $\\circledcirc$ by $\\mathrm{UDIS}++$ [35], process $\\circled{3}$ by DeepRectangling (DR) [34], process $\\circledast$ by Eq. 10 and Eq. 11. The corresponding partial images, $I$ and $I V$ , illustrate how SRStitcher effectively corrects the apparent misalignment of a pillar. Similarly, the partial images $I I$ and $I I I$ demonstrate how SRStitcher repairs the blurry coarse rectangling areas. SRStitcher can be applied to both UDIS and $\\mathrm{UDIS}++$ aligned images and get similar stitched results. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As shown in Figure 1(a), the errors originating in the registration stage persist through to the final stitched image, and the existing methods lack effective mechanisms to address these errors (detailed in Appendix A.6). The recon-based [32, 36, 33] methods are proven to inevitably introduce artifacts in the stitched image in the presence of registration errors. Furthermore, the seam-based [35, 11] methods rely on the assumption that there is a perfect seam between the aligned images. When this assumption is not met, the $\\mathrm{UDIS}++$ [35] forcibly distorts the images to create a perfect seam, resulting in distortion in the stitched image. Therefore, current fusion methods are unable to effectively handle the registration errors shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "To address the error propagation problem, we identify image fusion as the key point for improvement. We reconsider the problem definition of the fusion challenge and hypothesize that By determining the appropriate modification region and introducing an inpainting model with strong generalization ability, the abnormal image content caused by registration error can be effectively corrected. We propose to reformulate the fusion problem by overlaying the less distorted aligned image over the more distorted one, and inpainting the seam area between the images to correct the inappropriate image content. ", "page_idx": 1}, {"type": "text", "text": "Building on reframing the fusion problem, we also revisit the rectangling challenge. The core of the rectangling problem is to fill in the missing rectangling area, which is also essentially an image inpainting problem. Therefore, we question whether fusion and rectangling are truly distinct challenges or if they could be addressed as a unified inpainting task. We recognize that handling fusion and rectangling tasks simultaneously is not a simple matter of determining the inpainting area. More importantly, it requires precise control of the inpainting process. Specifically, the fusion task involves the preservation of original image semantics to the greatest extent possible, while the rectangle task requires more heavy inpainting to fill the missing regions. To effectively manage these varying demands, we introduce weighted masks to guide the reverse process in a pre-trained large-scale diffusion model. This method allows for the adjustment of inpainting intensity across different regions during the reverse process, enabling both tasks to be completed within a single inference. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The main contributions of this paper are: (1) We propose SRStitcher, which reformulates the problem definitions of the fusion and rectangling stages to construct a more streamlined and robust image stitching pipeline. (2) SRStitcher is the first to introduce the concept of inpainting to address the image fusion problem. It incorporates prior knowledge from large-scale pre-trained models into the image stitching pipeline, enhancing the robustness of image fusion against registration errors. (3) Without additional fine-tuning or supervision, SRStitcher improves the generalization of the rectangling method in the zero-shot scenario, opening up new possibilities for unsupervised image rectangling research. (4) We conduct extensive experiments to verify the interpretability and generalization of the proposed unified model. The results show that SRStitcher significantly outperforms the state-of-the-art methods in both quantitative and qualitative evaluations. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Registration parameterization. The goal of the registration stage is to obtain the aligned images based on a transformation matrix. Given inputs $I_{l}(x,\\stackrel{\\bullet}{y}),I_{r}(x,y)\\stackrel{<}{\\in}\\mathbb{R}^{H\\times W}$ , where $x$ and $y$ represent the pixel coordinates, $H,W$ are the height and width, respectively. And $\\mathcal{H}$ donates a $3\\!\\times\\!3$ homography matrix between $I_{l}(x,y)$ and $I_{r}(x,y)$ , which maps the input images to an uniform plane. To clarify the process of image registration, take the example of the four vertex coordinates $(x_{k},\\bar{y}_{k}),k\\in\\{1,2,\\dot{3},4\\}$ of the input image. The new image stitching-domain $\\mathbb{R}^{H^{*}\\times W^{*}}$ can be obtained by Eq. 1. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W^{*}=\\underset{k\\in(1,2,3,4)}{\\operatorname*{max}}\\{x_{k}^{w},x_{k}^{l}\\}-\\underset{k\\in(1,2,3,4)}{\\operatorname*{min}}\\{x_{k}^{w},x_{k}^{l}\\},}\\\\ &{H^{*}=\\underset{k\\in(1,2,3,4)}{\\operatorname*{max}}\\{y_{k}^{w},y_{k}^{l}\\}-\\underset{k\\in(1,2,3,4)}{\\operatorname*{min}}\\{y_{k}^{w},y_{k}^{l}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where, $(x_{k}^{w},y_{k}^{w})=\\mathcal{H}\\times[x_{k}^{r},y_{k}^{r},1]^{T}$ . Then, the input images are mapped into this new image stitchingdomain by warping operation $\\varphi(\\cdot)$ to get the aligned images $I_{w l}(x,y),I_{w r}(x,y)\\,\\in\\,\\mathbb{R}^{H^{*}\\times W^{*}}$ , as shown in Eq. 2. ", "page_idx": 2}, {"type": "equation", "text": "$$\nI_{w l}(x,y),I_{w r}(x,y)=\\varphi(I_{l}(x,y),\\mathtt{I}),\\varphi(I_{r}(x,y),\\mathcal{H}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where, $\\intercal$ is an identity matrix. The masks ${\\cal M}_{w l}(x,y),{\\cal M}_{w r}(x,y)$ corresponding to the aligned images can be obtained in a similar way by Eq. 2, except that the inputs $I_{l}(x,\\stackrel{.}{y}),I_{r}(\\stackrel{.}{x},y)$ are replaced with two all-one matrixes. The specific design of $\\varphi(\\cdot)$ vary slightly among different stitching methods [33, 35], but the aligned image generation of these methods all follows the architecture of Eq. 2. ", "page_idx": 2}, {"type": "text", "text": "Diffusion model. The proposed work is based on the Diffusion Model [16]. Since our method does not include the forward process, we only briefly introduce the reverse process. Suppose the $\\mathbf{x}_{1},...,\\mathbf{x}_{T}$ are latents of the same dimensionality as the $\\mathbf{x}_{0}\\sim q(\\mathbf{x}_{0})$ , where $q(\\cdot)$ is the a Gaussian Markov chain forward process with $T$ steps. And, $\\mathbf{x}_{0}=\\mathcal{E}(I_{0}(x,y))$ , where $\\mathcal E(\\cdot)$ is the image encoder and $I_{0}(x,y)$ is the input image. The joint distribution of $t$ -th inversion step is defined as Eq. 3. ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_{\\theta}(\\mathbf{x}_{t},t)),t\\in(1,T),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where, $\\mu_{\\theta}(\\mathbf{x}_{t},t)$ and $\\Sigma_{\\theta}(\\mathbf{x}_{t},t)$ are parameters of the Gaussian Markov chain in $t$ -th inversion step. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Unified inpainting model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fusion parameterization. Unlike previous methods, our method reconceptualizes the image fusion problem to enhance its robustness against registration errors. Precisely, as shown in Eq. 2, the distortion degree of $I_{w l}(x,y)$ is relatively low because it involves only minor warping based on I. ", "page_idx": 2}, {"type": "text", "text": "This means that even in the presence of registration errors, $I_{w l}(x,y)$ does not introduce large-scale distortions. Therefore, we propose to construct a coarse fusion image $I_{C F}(x,y)$ via Eq. 4. ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{C F}(x,y)=I_{w l}(x,y)+I_{w r}(x,y)\\odot(1-(M_{w l}(x,y)\\&M_{w r}(x,y))),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $\\&$ and $\\odot$ denote the bitwise AND operators and element-wise multiplication operator. The coarse fusion image has noticeable seams, as shown in Figure 1(b). Also, with registration errors, incoherent image content appears around the seams. To solve this problem, we propose to focus on inpainting the image content around the seam, ensuring cohesion and coherence. Therefore, we use the seam mask $\\bar{M_{s e a m}}(x,y)$ to define the area in the fusion image that needs to be inpainted, as detailed in Eq. 5. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{s e a m}(x,y)=\\mathrm{{Di}}\\mathrm{{1ation}}(M_{w l}(x,y),K_{s})\\oplus M_{w l}(x,y)\\vee}\\\\ &{\\qquad\\qquad\\qquad\\mathsf{E r o s i o n}((M_{w l}(x,y),K_{s})\\oplus M_{w l}(x,y)\\&M_{w r}(x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, Dilation $(\\cdot)$ and Erosion $(\\cdot)$ denote the dilation and erosion operations [38], $K_{s}$ is the kernel sizes, $\\vee,\\~\\oplus$ denote bitwise OR and XOR operators. Then, we inpaint $I_{C F}(x,y)$ based on seam mask $M_{s e a m}(x,y)$ and the inpainting function $f_{\\theta}(\\cdot)$ to obtain inpainted fusion image $\\hat{I}_{C F}(x,y)$ , as detailed in Eq. 6. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{I}_{C F}(x,y)=I_{C F}(x,y)\\odot(1-M_{s e a m}(x,y))+f_{\\theta}(I_{C F}(x,y))\\odot M_{s e a m}(x,y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Rectangling parameterization. Our method also defines the rectangling challenge as an inpainting problem based on the content mask $M_{c o n t e n t}(x,y)$ . We use Eq. 7 to obtain the inpainted rectangling image $\\hat{I}_{C R}(x,y)$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{I}_{C R}(x,y)=I_{C F}(x,y)\\odot(1-M_{c o n t e n t}(x,y))+f_{\\theta}(I_{C F}(x,y))\\odot M_{c o n t e n t}(x,y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $M_{c o n t e n t}(x,y)=M_{w l}(x,y)\\lor M_{w r}(x,y).$ ", "page_idx": 3}, {"type": "text", "text": "Unified model. Integrating Eq. 6 and Eq. 7, we obtain a unified inpainting model for fusion and rectangling, as shown in Eq. 8. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{I}_{C F R}(x,y)=I_{C F}(x,y)\\odot(1-M_{i n p a i n t}(x,y))+f_{\\theta}(I_{C F}(x,y))\\odot M_{i n p a i n t}(x,y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $M_{i n p a i n t}(x,y)=M_{s e a m}(x,y)\\lor M_{c o n t e n t}(x,y)$ . By combining equations Eq. 3 and Eq. 8, this inpainting problem can be solved by a diffusion model, as detailed in Eq. 9. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t-1}=\\mathbf{x}_{0}\\odot(1-M_{i n p a i n t}(x,y))+\\mathbf{x}_{t-1}\\odot M_{i n p a i n t}(x,y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $\\mathbf{x}_{0}=\\mathcal{E}(I_{C F}(x,y))$ , and $\\mathbf{x}_{t-1}\\sim\\mathcal{N}(\\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_{\\theta}(\\mathbf{x}_{t},t))$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Weighted mask guided reverse process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After defining the unified inpainting model for the fusion and rectangling tasks in the previous subsection, we discuss the method to control the inpainting strength in different regions during the reverse process, ensuring that both tasks can be accomplished in a single inference. Specifically, regions under the mask $M_{s e a m}(x,y)$ that contain the semantics of the original image are preserved as much as possible. In contrast, regions under $M_{c o n t e n t}(x,y)$ may require more powerful inpainting. We propose weighted masks to guide the reverse process to achieve this varying inpainting strength. ", "page_idx": 3}, {"type": "text", "text": "Weighted masks. Weighted masks are constructed from the weighted initial mask $\\widetilde{M}_{i n i t}(x,y)$ and inpainting mask $\\widetilde{M}_{i n p a i n t}(x,y)$ . ", "page_idx": 3}, {"type": "text", "text": "1: Input: Coarse Fusion image $I_{C F}(x,y)$ ; Inference steps $N$ ; Radius $R$ ;   \n2: Weighted initial mask $\\widetilde{M}_{i n i t}(x,y)$ ; Weighted inpainting mask $\\widetilde{M}_{i n p a i n t}(x,y)$   \n3: prompt $p\\leftarrow$ \"\" $\\triangleright$ Our method does not require prompt guidance   \n4: $\\begin{array}{r l}&{I_{C F R}(x,y)\\gets\\mathtt{T e l e a}(I_{C F}(x,y),M_{c o n t e n t}(x,y),R)}\\\\ &{\\mathbf{x}_{N}\\gets\\mathcal{E}(I_{C F R}(x,y))}\\end{array}$ \u25b7Coarse rectangling   \n5: \u25b7Encode image   \n6: // Based on the inpainting model, so there is a little difference here with the Eq. 9   \n7: x0 \u2190E(ICF R(x, y) \u2299Minit(x, y))   \n8: M  isnmpaalilnt(x, y), M  isnmitall(x, y) \u2190DownSample(M inpaint(x, y), M init(x, y))   \n9: $\\mathbf{x}_{N}^{\\prime}\\gets\\tt A d d N o i s e(\\mathbf{x}_{N},N)$   \n10: $\\hat{\\mathbf{x}}_{N}\\gets\\mathsf{C o n c a t}(\\mathbf{x}_{N}^{\\prime},\\widetilde{M}_{i n i t}^{s m a l l}(x,y),\\mathbf{x}_{0})$   \n11: for $t=N-1,\\cdot\\cdot\\cdot,0$ do \u25b7Reverse process   \n12: $\\mathbf{x}_{t}^{\\prime}\\gets\\tt D e N o i s e(\\hat{\\mathbf{x}}_{t+1},p,t)$   \n13: $\\begin{array}{r}{\\widetilde{M}_{t}^{s m a l l}(x,y)\\gets1-(\\widetilde{M}_{i n p a i n t}^{s m a l l}(x,y)\\preceq\\frac{N-t}{N})}\\end{array}$ \u25b7\u2aafmeans element-wise less-than   \n14: \u02c6xt \u2190Concat(x\u2032t,M  tsmall(x, y), x0)   \n15: end for   \n16: $\\hat{I}_{C F R}(x,y)\\gets\\mathtt{I m a g e D e c o d e r}(\\hat{\\mathbf{x}}_{0})$ \u25b7Decode image   \n17: Output: $\\hat{I}_{C F R}(x,y)$ ", "page_idx": 4}, {"type": "text", "text": "The weighted initial mask $\\widetilde{M}_{i n i t}(x,y)$ assigns different fidelity levels to each pixel of the fusion image, determining how much to modify each pixel based on its fidelity during the reverse process. The formula of $\\overleftarrow{M}_{i n i t}(x,y)$ is given by Eq.10, which is composed of two parts. The left part determines the fidelity levels of pixels in $M_{s e a m}(x,y)$ region, and the right part determines the fidelity levels of pixels in $M_{c o n t e n t}(x,y)$ region. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{M}_{i n i t}(x,y)=\\frac{\\mathrm{{DT}}\\big(M_{s e a m}(x,y),K_{g})\\times\\epsilon_{1}}{\\operatorname*{max}\\mathrm{{DT}}\\big(M_{s e a m}(x,y),K_{g}\\big)}\\oplus\\frac{\\mathrm{{DT}}\\big(M_{c o n t e n t}(x,y),K_{g})\\times\\epsilon_{2}}{\\operatorname*{max}\\mathrm{{DT}}\\big(M_{c o n t e n t}(x,y),K_{g}\\big)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $\\mathrm{{DT}}(\\cdot)$ is the distance transform operation [38] with kernel size $K_{g}$ , $\\epsilon_{1}$ and $\\epsilon_{2}$ are hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "The weighted inpainting mask $\\widetilde{M}_{i n p a i n t}(x,y)$ , as described in Eq.11, is inspired by the suffix principle [24]. During the reverse process, $\\widetilde{M}_{i n p a i n t}(x,y)$ is mapped into multiple sub-masks to define the modified regions at each step $t$ . The region corresponding to $M_{c o n t e n t}(x,y)$ contains no image content, so its size remains constant across all sub-masks, ensuring it is repainted throughout the entire process. Conversely, the region corresponding to $M_{s e a m}(x,y)$ contains a substantial quantity of original image information, and its size gradually increases with each step $t$ , indicating that this region requires progressive modification. This gradual modification method facilitates a more seamless blending of the inpainting content with the original image content. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{M}_{i n p a i n t}(x,y)=M_{c o n t e n t}(x,y)\\vee(1-{\\tt D T}(M_{s e a m}(x,y),K_{g})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Guided reverse process. We observe that when the missing region of the fusion image is large, the diffusion model very easily generates abnormal content, such as abnormal textures and words. To mitigate this issue, we introduce coarse rectangling. To be specific, we employ the Alexandru Telea Algorithm $\\mathtt{T e l e a}(\\cdot)$ [46] to generate the coarse rectangling image: $I_{C F R}(x,y)\\;=\\;{\\tt T e l e a}(I_{C F}(x,y),M_{c o n t e n t}(x,y),R)$ , where $R$ is the radius of a circular neighborhood of each point inpainted. The $\\mathtt{T e l e a}(\\cdot)$ algorithm introduces a weak prior without any specific semantic information to the image $I_{C F}(x,y)$ . As shown in the partial image $I$ of Figure 1, the image of the coarse rectangling region is completely blurred. The experimental result shows that generating images with weak priors significantly reduces the likelihood of producing anomalous content compared to leaving blank areas entirely black. More details regarding the advantages of coarse rectangling can be found in the Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The specific steps of the reverse process are detailed in Algorithm 1. Although this algorithm is based on the Stable Diffusion Inpainting model [41, 3], which differs slightly from the original Stable Diffusion model [2], the underlying principles remain consistent. In addition, our method works without the need for prompt, effectively reducing dataset requirements. ", "page_idx": 5}, {"type": "text", "text": "Appendix A.2 provide more detailed explanation and visualization of the weighted masks and WMGRP algorithm. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset. To validate the performance of our method, we conducted experiments on the large public dataset UDIS-D [33]. To the best of our knowledge, UDIS-D is the only publicly available large-scale dataset in this field. Appendix D.4 provides more results of our method on other traditional small datasets. ", "page_idx": 5}, {"type": "text", "text": "Baselines. To our knowledge, no open-source solutions simultaneously address the fusion and rectangle stages of the image-stitching pipeline as comprehensively as our method. Table 1(a) gives brief statistics of related works, and more related work details provided in Appendix B. Therefore, we have to establish the comparison baselines by combining several existing methods. For the registration and fusion stage, we employ pre-trained models from UDIS [33] and $\\mathrm{UDIS++}$ [35]. For the rectangling stage, we utilize pre-trained models from DeepRectangling (DR) [34], Lama [44], Stable-Diffusion-v1-5-inpainting (SD1.5) [41], and Stable-Diffusion-v2-inpainting (SD2) [3]. Table 1(b) presents the detailed configurations of baselines. ", "page_idx": 5}, {"type": "table", "img_path": "ZViYPzh9Wq/tmp/cccbf6d33ce7a464366da43ca93b83a974ff15099d169e012f24dcd42b8f1c81.jpg", "table_caption": ["Table 1: Statistics of related works and details of comparison baselines. (a) Statistics of related works. (b) Details of comparison baselines. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Variants. In this paper, we mainly present SRStitcher based on Stable Diffusion Inpainting model [3]. However, our method is versatile and can be readily adapted to other diffusion-based models with only minor modifications. In the experiments, we also compare the SRStitcher variants, including: SRStitcher-S based on the Stable Diffusion 2 model [2], SRStitcher-U based on Stable Diffusion 2 Unclip model [4], SRStitcher-C based on Controlnet Inpainting model [54]. For further information on SRStitcher variants, please refer to Appendix D.3. ", "page_idx": 5}, {"type": "text", "text": "Metrics. (1) Stitched image quality. Since UDIS-D is an unsupervised dataset, we use the NoReference Image Quality Assessment (NR-IQA) metrics to evaluate the image quality. Specifically, we use the HIQA [43] and CLIPIQA [49]. (2) Content consistency. We develop a new metric to evaluate the content consistency between the input images and the stitched image. Specifically, we introduce the $\\mathtt{C o C a}(\\cdot)$ [50] model and Bert(\u00b7) [39] model to extract text from the images and generate text embeddings. The similarity between these embeddings is measured by the cosine similarity cosine $(\\cdot)$ . We design the Content Consistency Score (CCS) metric: ", "page_idx": 5}, {"type": "text", "text": "$C C S_{n}$ measures the local consistency, which compares the stitched image $I_{S t i t c h e d}(x,y)$ and the fusion image $I_{F u s i o n}(x,y)$ . Both images are split into $n$ equal parts for detailed comparison: $C C S_{n}\\ =\\ {\\mathsf{c o s i n e}}(\\Upsilon(\\mathrm{\\bfSplit}(I_{S t i t c h e d}(x,y),n))$ , $\\Upsilon(\\mathtt{S p l i t}(I_{F u s i o n}(x,y),n))$ ), where $\\Upsilon(\\cdot)~~=~\\mathrm{Bert}(\\sf C o C a(\\cdot))$ , and for this test, $\\textit{n}=\\textit{4}$ . In addition, $C C S_{g}$ assesses the overall content consistency between the $I_{S t i t c h e d}(x,y)$ and original input images: $\\begin{array}{r l}{C C S_{g}}&{{}=}\\end{array}$ cosine $:(\\Upsilon(I_{S t i t c h e d}(x,y),\\Upsilon(I_{l}(x,y),I_{r}(x,y)))$ . Please refer to Appendix C for further information on the metrics. ", "page_idx": 6}, {"type": "text", "text": "Implement details. All experiments are performed on a single NVIDIA 4090 GPU. In addition, all experiments of SRStitcher described in this paper are based on these pre-aligned images made by $\\mathrm{UDIS}++$ [35]. For hyper-parameters, the guidance scale and inference steps $N$ are set to 7.5 and 50; The $K_{s}$ in Eq. 5 is set to $\\vert\\bar{W}^{*}/\\lambda\\vert\\times\\delta$ , where $\\lambda=200$ and $\\delta=10$ ; The $K_{g}$ in Eq. 10 and Eq. 11 are set to 3; The $R$ in $\\mathtt{T e l e a}(\\cdot)$ is set to 20. The $\\epsilon_{1}$ and $\\epsilon_{2}$ in Eq. 10 are set to 128 and 128. ", "page_idx": 6}, {"type": "table", "img_path": "ZViYPzh9Wq/tmp/34e0b52068b7cdc78cdf64c1b51983bf3a9af2f3ecb8dbc2b678b1fe84cd3c14.jpg", "table_caption": ["Table 2: Quantitative results. The best and second-best results are highlighted by red and blue. $\\star$ refers to the inference results of this method are not affected by seed. $\\dagger$ means the inference results of this method are affected by the seed. We tested the results five times by varying the seed, taking the average and standard deviation. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Quantitative evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We perform a comprehensive quantitative analysis by comparing the results of 10,440 sample pairs from the UDIS-D training set $U D I S-D_{t r a i n}$ and 1,106 sample pairs from the testing set $U D I S\\,-\\,D_{t e s t}$ . Notably, our method does not require training, so to provide a broader base of comparison, the training set of UDIS-D is also included in the comparison experiments. The comparative results are presented in Table 2, which illustrates the significant advantages of SRStitcher in terms of the stitched image quality and content consistency. ", "page_idx": 6}, {"type": "text", "text": "4.3 Qualitative evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform a quantitative evaluation of SRStitcher against other baseline methods, depicted in Figure 2. The first row of Figure 2 presents a challenging registration scenario involving soft and deformable objects, such as wires, which may have deformed unpredictably between two images. Current registration methods cannot accurately align such objects. Instead of attempting to register or fuse these deformed wires, our method opts to inpaint incorrect wires, effectively overcoming registration errors. A more detailed discussion on this is available in Appendix A.4. The second row of Figure 2 illustrates challenges associated with structured and extensive missing areas, where methods like DR and Lama struggle to accurately fill in the image content. The third row addresses the repeated pattern challenge, where a large number of bricks significantly complicates registration accuracy. The fourth row highlights the classic multi-depth layer problem, illustrating how objects like pillars and their backgrounds, being on different depth layers, result in registration inaccuracies. To enhance the clarity of presentation, the results of UDISplus $^+.$ DR, UDIS $^+$ Lama, UDIS $+,$ SD1.5, and UDISplus $;+\\mathrm{SD}1.5$ are omitted from this figure. Detailed qualitative evaluations for all comparison methods are provided in Appendix D. ", "page_idx": 7}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/6c3efd9d5ab3d96cd731328425392f9b5b744053bd13b1f99edc81de2b129a8f.jpg", "img_caption": ["Figure 2: Qualitative evaluation results. All visual results are obtained with seed 0. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 User study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We introduce a user study metric from UDIS [33]. This method allows for a more subjective but insightful visual quality assessment through direct user feedback. For the user study, we display four images simultaneously on a single screen: the two input images, our stitched result, and the stitched result from one of the baseline methods. Participants are asked to determine which result is superior, SRStitcher or Another (comparison baseline). If a clear preference is not apparent, participants can choose Both Good or Both Bad. The study involves 20 participants: 10 researchers (computer vision background) and 10 volunteers (non-computer major). This diverse group ensures a balanced perspective, combining expert technical evaluation with general user impressions. The results are shown in Figure 3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Hyper-parameter. Figure 4 illustrates the ablation results in hyper-parameter of SRStitcher, demonstrating that these parameters are highly interpretable and easy to adjust. (1) $\\lambda;$ : Controls the width of the region in $M_{s e a m}$ . A smaller $\\lambda$ increases the modification range and decreases the CCS. For stitched pictures with color differences, a lower $\\lambda$ value can better fuse the images. We set $\\lambda$ to 200, considering both image smoothness and CCS. (2) $R$ : Controls the granularity of the coarse rectangling image. A higher $R$ value provides a higher quality weak prior for inpainting, reducing the likelihood of generating abnormal content. Ideally, a larger $R$ is preferable, but due to the limitations in GPU acceleration with the $\\mathtt{T e l e a}(\\cdot)$ , a very high $R$ value can slow down the pipeline. Thus, we balance performance and speed by setting $R$ to 20. (3) $\\epsilon_{1}$ : Controls the inpainting strength of the seam area. At $\\epsilon_{1}=128$ , the shape of the pillars appears more reasonable compared to $\\epsilon_{1}=64$ . However, increasing $\\epsilon_{1}$ to 192 significantly alters the image content, so we set it to 128. (4) $\\epsilon_{2}$ : Controls the inpainting strength of the rectangling area. When $\\epsilon_{2}$ is relatively low, the image structure remains largely intact, but increasing it to 192 leads to noticeable structural deficits. Therefore, we set $\\epsilon_{2}$ to 128. Please see Appendix D.2 for more comprehensive hyper-parameters studies. ", "page_idx": 8}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/0f41141f4d4b5dcee413328bca199412f8a6de8475dcf0f82874ad18be0423c4.jpg", "img_caption": ["Figure 3: User study on visual quality.Figure 4: Ablation study results. CCS on each image is the The results are averaged across 20 average score of $U D I S-D_{t e s t}$ with this hyper-parameter participants, with the percentage and seed 0, not a single image. on the ordinate axis. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Weighted Masks. Figure 5 illustrates the ablation results in different masks guided manners. Specifically, Figure 5(a) represents the different effects of using fixed mask and weighted mask in the fusion region. As shown in the red box, the weighted mask better smoothes the image content while preserving the original information of the image by gradually modifying the image content, while the fixed mask significantly modifies the original image content. In addition, Figure 5(b) illustrates the stitched images obtained by the gradually changing weighted masks in the rectanguling region. Since the rectanguling region contains no image content, the generator guided by the gradually changing masks repeatedly smoothes the empty region, resulting in blurred noise. Therefore, we use a fixed size mask in the rectanguling region. ", "page_idx": 8}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/f1ea109369f6a2642d9e015462b8d8a6b2d70014b89c367d4f6d89a386a6e615.jpg", "img_caption": ["Figure 5: Ablation of the masks guided manners. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces SRStitcher, which reconceptualizes the fusion and rectangling stages as a unified inpainting model. Through weighted masks, SRStitcher leverages the robust generalization capabilities of a pre-trained large-scale generation model to accomplish this complex inpainting task without additional fine-tuning or task-specific data annotations. Extensive experiments demonstrate that SRStitcher significantly outperforms existing state-of-the-art methods regarding the quality of the stitched images and its robustness to registration errors and abnormal content. Furthermore, the specific effects and adjustments of each hyper-parameter in SRStitcher are detailed in the ablation studies, illustrating its high interpretability and controllability. ", "page_idx": 9}, {"type": "text", "text": "However, there are still some limitations and open issues in future research: (1)Visible seam. When input images exhibit significant color differences, visible seams may appear with the parameter settings described in the paper. Adjustments to $\\epsilon_{1}$ and $\\lambda$ can partially mitigate this issue, but such modifications can compromise the preservation of original image information. We speculate that a more flexible and appropriately designed hyper-parameter selection scheme could solve this problem. (2) Local blurring. We use coarse rectangling and $\\widetilde{M}_{i n i t}(x,y)$ to control the content generation. However, this approach introduces a side effect where some challenging scenes appear locally blurred (See Appendix D.5). This issue presents a dilemma: accept local blurring or risk producing anomalous images. We temporarily choose to tolerate local blurring. Future improvements will include a refined coarse rectangling approach or fine-tuning the model. (3) Integrating registration. Is it possible to integrate the registration stage into the unified model? According to Diffusion Features (DIFT) [45], it is possible. DIFT proves that the geometric correspondence between images can be effectively established by extracting feature maps from their intermediate layers at a specific timestep during the inverse process. Replacing the registration method used by SRStitcher with DIFT is straightforward. However, our ambitions extend beyond simple replacement. We believe there is potential for a more elegant and concise method to integrate concepts proposed by DIFT into our existing method. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper was supported by the National Key Research and Development Project of China (Grant No.2023YFB3408600) and Science and Technology Innovation Program of Shanghai (Grant No.18DZ2295100). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Aseem Agarwala, Mira Dontcheva, Maneesh Agrawala, Steven Drucker, Alex Colburn, Brian Curless, David Salesin, and Michael Cohen. Interactive digital photomontage. In ACM SIGGRAPH 2004 Papers, pages 294\u2013302. 2004. ", "page_idx": 9}, {"type": "text", "text": "[2] Stability AI. Stable diffusion 2. https://huggingface.co/stabilityai/stable-diffusion-2, 2023.   \n[3] Stability AI. Stable diffusion 2 inpainting. https://huggingface.co/stabilityai/ stable-diffusion-2-inpainting, 2023.   \n[4] Stability AI. Stable diffusion 2.1 unclip. https://huggingface.co/stabilityai/ stable-diffusion-2-1-unclip, 2023.   \n[5] Alex M Andrew. Multiple view geometry in computer vision. Kybernetes, 30(9/10):1333\u20131341, 2001.   \n[6] Matthew Brown and David G Lowe. Automatic panoramic image stitching using invariant features. International journal of computer vision, 74:59\u201373, 2007.   \n[7] Si-Yuan Cao, Runmin Zhang, Lun Luo, Beinan Yu, Zehua Sheng, Junwei Li, and Hui-Liang Shen. Recurrent homography estimation using homography-guided image warping and focus transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9833\u20139842, June 2023.   \n[8] David Capel and Andrew Zisserman. Automated mosaicing with super-resolution zoom. In Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231), pages 885\u2013891. IEEE, 1998.   \n[9] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch toolbox for image quality assessment. [Online]. Available: https://github.com/chaofengc/IQA-PyTorch, 2022.   \n[10] Jun Chen, Yongxi Luo, Jie Wang, Honghua Tang, Yixian Tang, and Jianhui Li. Elimination of irregular boundaries and seams for uav image stitching with a diffusion model. Remote Sensing, 16(9):1483, 2024.   \n[11] Senmao Cheng, Fan Yang, Zhi Chen, Nanjun Yuan, and Wenbing Tao. Deep seam prediction for image stitching based on selection consistency loss. arXiv preprint arXiv:2302.05027, 2023.   \n[12] Junhong Gao, Seon Joo Kim, and Michael S Brown. Constructing image panoramas using dual-homography warping. In CVPR 2011, pages 49\u201356. IEEE, 2011.   \n[13] Junhong Gao, Yu Li, Tat-Jun Chin, and Michael S Brown. Seam-driven image stitching. In Eurographics (Short Papers), pages 45\u201348, 2013.   \n[14] Deepti Ghadiyaram and Alan C. Bovik. Massive online crowdsourced study of subjective and objective picture quality. IEEE Transactions on Image Processing, 25(1):372\u2013387, 2016.   \n[15] Kaiming He, Huiwen Chang, and Jian Sun. Rectangling panoramic images via warping. ACM Transactions on Graphics (TOG), 32(4):1\u201310, 2013.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[17] Van-Dung Hoang, Diem-Phuc Tran, Nguyen Gia Nhu, The-Anh Pham, and Van-Huy Pham. Deep feature extraction for panoramic image stitching. In Intelligent Information and Database Systems: 12th Asian Conference, ACIIDS 2020, Phuket, Thailand, March 23\u201326, 2020, Proceedings, Part II 12, pages 141\u2013151. Springer, 2020.   \n[18] Mingbo Hong, Yuhang Lu, Nianjin Ye, Chunyu Lin, Qijun Zhao, and Shuaicheng Liu. Unsupervised homography estimation with coplanarity-aware gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 17663\u201317672, June 2022.   \n[19] V. Hosu, H. Lin, T. Sziranyi, and D. Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:4041\u20134056, 2020.   \n[20] Jitesh Jain, Yuqian Zhou, Ning Yu, and Humphrey Shi. Keys to better image inpainting: Structure and texture go hand in hand. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 208\u2013217, 2023.   \n[21] Hai Jiang, Haipeng Li, Yuhang Lu, Songchen Han, and Shuaicheng Liu. Semi-supervised deep largebaseline homography estimation with progressive equivalence constraint. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1024\u20131032, 2023.   \n[22] Zhiying Jiang, Zengxi Zhang, Jinyuan Liu, Xin Fan, and Risheng Liu. Multi-spectral image stitching via spatial graph reasoning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 472\u2013480, 2023.   \n[23] Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, Jian Ren, Sergey Tulyakov, and Igor Gilitschenski. invs: Repurposing diffusion inpainters for novel view synthesis. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201312, 2023.   \n[24] Eran Levin and Ohad Fried. Differential diffusion: Giving each pixel its strength, 2023.   \n[25] Dongping Li, Kaiming He, Jian Sun, and Kun Zhou. A geodesic-preserving method for image warping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 213\u2013221, 2015.   \n[26] Jing Li, Zhengming Wang, Shiming Lai, Yongping Zhai, and Maojun Zhang. Parallax-tolerant image stitching based on robust elastic warping. IEEE Transactions on multimedia, 20(7):1672\u20131687, 2017.   \n[27] Kang Liao, Chunyu Lin, Yunchao Wei, Feng Li, Shangrong Yang, and Yao Zhao. Towards complete scene and regular shape for distortion rectification by curve-aware extrapolation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14569\u201314578, 2021.   \n[28] Tianli Liao and Nan Li. Natural image stitching using depth maps, 2023.   \n[29] Kaimo Lin, Nianjuan Jiang, Loong-Fah Cheong, Minh Do, and Jiangbo Lu. Seagull: Seam-guided local alignment for parallax-tolerant image stitching. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 370\u2013385. Springer, 2016.   \n[30] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models, 2022.   \n[31] Philip F McLauchlan and Allan Jaenicke. Image mosaicing using sequential bundle adjustment. Image and Vision computing, 20(9-10):751\u2013759, 2002.   \n[32] Lang Nie, Chunyu Lin, Kang Liao, Meiqin Liu, and Yao Zhao. A view-free image stitching network based on global homography. Journal of Visual Communication and Image Representation, 73:102950, 2020.   \n[33] Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, and Yao Zhao. Unsupervised deep image stitching: Reconstructing stitched features to images. IEEE Transactions on Image Processing, 30:6184\u20136197, 2021.   \n[34] Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, and Yao Zhao. Deep rectangling for image stitching: a learning baseline. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5740\u20135748, 2022.   \n[35] Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, and Yao Zhao. Parallax-tolerant unsupervised deep image stitching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7399\u20137408, 2023.   \n[36] Lang Nie, Chunyu Lin, Kang Liao, and Yao Zhao. Learning edge-preserved image stitching from multiscale deep homography. Neurocomputing, 491:533\u2013543, 2022.   \n[37] nvidia. Tensorrt. https://github.com/NVIDIA/TensorRT, 2023.   \n[38] OpenCV. Open source computer vision library, 2015.   \n[39] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019.   \n[40] Gernot Riegler and Vladlen Koltun. Free view synthesis. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIX 16, pages 623\u2013640. Springer, 2020.   \n[41] Runway. Stable diffusion 1.5 inpainting. https://huggingface.co/runwayml/ stable-diffusion-inpainting/blob/main/sd-v1-5-inpainting.ckpt, 2022.   \n[42] Zaifeng Shi, Hui Li, Qingjie Cao, Huizheng Ren, and Boyu Fan. An image mosaic method based on convolutional neural network semantic features extraction. Journal of Signal Processing Systems, 92:435\u2013444, 2020.   \n[43] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[44] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021.   \n[45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[46] Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics tools, 9(1):23\u201334, 2004.   \n[47] Prune Truong, Martin Danelljan, and Radu Timofte. Glu-net: Global-local universal network for dense flow and correspondences. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6258\u20136268, 2020.   \n[48] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.   \n[49] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023.   \n[50] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.   \n[51] Jian Yu, Yi Yu, and Feipeng Da. Parallax-tolerant image stitching with epipolar displacement field, 2023.   \n[52] Julio Zaragoza, Tat-Jun Chin, Michael S Brown, and David Suter. As-projective-as-possible image stitching with moving dlt. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2339\u20132346, 2013.   \n[53] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Aggregated contextual transformations for high-resolution image inpainting. IEEE Transactions on Visualization and Computer Graphics, 2022.   \n[54] Lvmin Zhang. Control v11p sd15 inpaint. https://huggingface.co/lllyasviel/control_v11p_ sd15_inpaint, 2023.   \n[55] Hongfei Zhou, Yuhe Zhu, Xiaoqian Lv, Qinglin Liu, and Shengping Zhang. Rectangular-output image stitching. In 2023 IEEE International Conference on Image Processing (ICIP), pages 2800\u20132804. IEEE, 2023.   \n[56] Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, and Shuaicheng Liu. Recdiffusion: Rectangling for image stitching with diffusion models. arXiv preprint arXiv:2403.19164, 2024.   \n[57] Imad Zoghlami, Olivier Faugeras, and Rachid Deriche. Using geometric corners to build a 2d mosaic from a set of images. In Proceedings of IEEE computer society conference on computer vision and pattern recognition, pages 420\u2013425. IEEE, 1997. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A More details of SRStitcher ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 More analysis of the designs described in the main paper ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To elucidate the specific role of each design element in SRStitcher, Figure 6 illustrates the results after sequentially removing our designs: ", "page_idx": 13}, {"type": "text", "text": "(a) SRStitcher result. The result is obtained when coarse rectangling, $\\widetilde{M}_{i n i t}(x,y)$ , and $\\widetilde{M}_{i n p a i n t}(x,y)$ are all used. ", "page_idx": 13}, {"type": "text", "text": "(b) Remove coarse rectangling, retain $\\widetilde{M}_{i n i t}(x,y)$ and $\\widetilde{M}_{i n p a i n t}(x,y)$ . Removing coarse rectangling while maintaining $\\widetilde{M}_{i n i t}(x,y)$ results in the incomplete filling. As mentioned above, this is because $\\widetilde{M}_{i n i t}(x,y)$ is still working and retains the original image information. But, the area previously filled by the coarse rectangling returns pure black, which affects the final stitched result. ", "page_idx": 13}, {"type": "text", "text": "(c) Remove coarse rectangling andM init(x, y), retain $\\widetilde{M}_{i n p a i n t}(x,y)$ . Replacing $\\widetilde{M}_{i n i t}(x,y)$ with $M_{c o n t e n t}(x,y)$ eliminates its effect,  allowing the rectan gling area to be completely  filled. However, compared to (a), the content changes significantly, and abnormal content emerges. This result indicates the importance of coarse rectangling in providing weak priors for guiding the generation. ", "page_idx": 13}, {"type": "text", "text": "(d) Remove coarse rectangling, $\\widetilde{M}_{i n i t}(x,y)$ , and $\\widetilde{M}_{i n p a i n t}(x,y)$ . By removing all design elements, SRStitcher becomes a simple in painting model b ased on $M_{i n p a i n t}(x,y)$ . This results in a higher probability of generating abnormal content, with substantial alterations near the seam (indicated by the red box). ", "page_idx": 13}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/b9f1c9252f03e069aa56ad506f238d01e19457e8b7cb2061cbe7f8936d205004.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 6: The effects of each design on SRStitcher results. We demonstrate how each design element influences the results of SRStitcher by removing them one at a time and observing the changes. ", "page_idx": 13}, {"type": "text", "text": "A.2 Physical implications of weighted masks $\\widetilde{M}_{i n i t}(x,y)$ and $\\widetilde{M}_{i n p a i n t}(x,y)$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We design $\\widetilde{M}_{i n i t}(x,y)$ and $\\widetilde{M}_{i n p a i n t}(x,y)$ is illuminated by the following observations: ", "page_idx": 13}, {"type": "text", "text": "(1) The input structure of the inpainting model. Unlike the general Stable Diffusion model[2], the input of the Stable Diffusion Inpainting model comprises the original image, mask and masked image. Through experimental tests, we found that variations in any part of this composite input significantly influence the final output results. ", "page_idx": 13}, {"type": "text", "text": "(2) Impact of masked image. The masked image retains and continuously provides the unmasked area information of the original image throughout the reverse process, ensuring that the unmasked area of the final generated image remains consistent with the original image. ", "page_idx": 13}, {"type": "text", "text": "(3) Impact of mask. The mask is employed to identify the modified regions during the reverse process. Adjusting the scope of the mask during the process allows for the controlled modification of the inpainting intensity across different regions. One point to note is that in the input mask of the ", "page_idx": 13}, {"type": "text", "text": "Stable Diffusion Inpainting model: the black is the area that needs to be retained, and the white is the area that needs to be inpainted. ", "page_idx": 14}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/da174e29f1d200c770eabb9419428e8ff1267c727283b3547d845bde41ae7ac8.jpg", "img_caption": ["Figure 7: Visual production of masks. We provide the correlation between equations and masks to facilitate comprehension. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Based on the above findings, we propose the construction of weighed masks. ", "page_idx": 14}, {"type": "text", "text": "(1) Control of masked image. In our design, $\\widetilde{M}_{i n i t}(x,y)$ is used to create the masked image, determining the extent to which information fro m the original image is retained. We aim to fully preserve the image content in $M_{c o n t e n t}(x,y)$ . While, for areas outside $M_{c o n t e n t}(x,y)$ (that is the coarse rectangling area), we implement a distance transform to change the $M_{c o n t e n t}(x,y)$ . Due to the coarse rectangling regions are blurry (but we must use the weak prior, as explained in A.1), it is undesirable to retain substantial information from these blurry regions. By employing a distance transform, we can gradually reduce the information from the coarse rectangling image starting from the edges of $M_{c o n t e n t}(x,y)$ . This approach ensures that only the most relevant information from the edges of the optimal coarse rectangling is retained, avoiding the use of excessive coarse rectangling data. ", "page_idx": 14}, {"type": "text", "text": "(2) Control of mask. In our design, the inpainting mask is dynamically adjusted throughout the reverse process based on $\\widetilde{M}_{i n p a i n t}(x,y)$ . Although $\\widetilde{M}_{i n p a i n t}(x,y)$ serves as just one mask, its mask area is modified at each step $t$ by calculating the threshold $\\textstyle{\\frac{N-t}{N}}$ and remapping $\\widetilde{M}_{i n p a i n t}(x,y)$ to $\\widetilde{M}_{t}^{s m a l l}(x,y)$ accordingly (as detailed in Algorithm 1). ", "page_idx": 14}, {"type": "text", "text": "Figure 7 provides the detailed mask productions. The final $\\widetilde{M}_{i n i t}(x,y)$ and $\\widetilde{M}_{i n p a i n t}(x,y)$ contain gradient areas, and we realize the gradual control of the rev erse process base d on the gradient areas. The $\\widetilde{M}_{i n i t}(x,y)$ is used to store information about the fusion image. Intuitively, darker places hold mor e information about the original image. ", "page_idx": 14}, {"type": "text", "text": "A.3 Visualization of WMGRP ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As a supplement to Algorithm 1, we provide Figure 8 to visually illustrate the process of WMGRP using the masks generated from Figure 7. The $\\widetilde{M}_{i n p a i n t}(x,y)$ is employed to regulate the intensity of inpainting in distinct regions during the reverse process. Intuitively, the white part is not modified at all, and the black part is modifiable. It can be observed that the black area of inpainting mask is gradually increased during the reverse process, which serves as the guiding process for the gradual modification. ", "page_idx": 14}, {"type": "text", "text": "Although the above content is mainly based on the observations and experimental results of the Stable Diffusion Inpainting model, our experiments have proved that it also applies to the general ", "page_idx": 14}, {"type": "text", "text": "Stable Diffusion model [2] with only minor modifications. We provide the detailed implementation of SRStitcher variants in the Supplementary Material. ", "page_idx": 15}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/84e46d23889119855123b458bf496ad59a5b937504d0ef9e427940f3996bbf73.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: Visualization of WMGRP. For simplicity, we omit the masked image, which is invariant after initialization throughout the process. ", "page_idx": 15}, {"type": "text", "text": "A.4 Why is SRStitcher so effective at overcoming registration errors ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Unlike the previous fusion and rectangling methods SRStitcher does not rigidly adhere to the registration results. Figure 9 provides a clear example of how SRStitcher addresses wire registration errors. In the coarse fusion image $I_{C F R}(x,y)$ , the misregistration problem is still serious, which is reflected in the significantly misaligned wires. ", "page_idx": 15}, {"type": "text", "text": "After the inpainting, these misaligned wires are effectively corrected, while the content in other masked areas remains largely unchanged. We attribute this remarkable correction capability to the strong generalization ability of large-scale generative models. This ability to correct incorrect image content underpins our motivation for employing the large-scale pre-trained model. ", "page_idx": 15}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/4ecb106d99b838724a42f22fce4d74d7c827f761009a9ea9628d9080caa76127.jpg", "img_caption": ["Figure 9: How SRStitcher addresses the issue of registration errors. Due to the disparate parameter settings, the figure differs from the main paper. The display effect and parameters of the main paper shall prevail. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "For visual illustration, Figure 10 compares the fusion effects of SRStitcher and other methods in a large-parallax scene. The result shows that, SRStitcher\u2019s inpainting-based fusion method achieves the best continuity of image content in this challenging scenario. ", "page_idx": 15}, {"type": "text", "text": "A.5 The necessity of using a large-scale diffusion model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Although the diffusion model has been shown to outperform Generative Adversarial Network (GAN) methods on the inpainting problem [30], its high hardware requirements have prevented us from considering it as a first option. Initially, we try to address the inpainting problem using Generative Adversarial Network (GAN)-based methods, such as AOTGAN [53], Lama [44], and FCFGAN [20]. ", "page_idx": 15}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/ab6e89283e8c170594923f20ac5031a7adf211d091f1e2d5c69083c93c3cce19.jpg", "img_caption": ["Figure 10: Comparison of fusion effects of SRStitcher and other methods in the large-parallax scene. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "However, our experiments revealed several shortcomings. GAN-based models struggled with poor generalization ability, displayed unstable training outcomes, and demanded high-quality, well-labeled datasets. These factors complicated the pipeline and increased the workload, particularly in data labeling. ", "page_idx": 16}, {"type": "text", "text": "During the research bottleneck period, we found the concept of Soft-Inpainting proposed by Differential Diffusion [24], which involves blending parts of the image with the original by making slight modifications. This concept inspired us to adapt and extend it to our needs. We applied Soft-Inpainting near the seams and employed more intense Hard-Inpainting in the rectangling areas. With this idea, we successfully implemented SRStitcher proposed in this paper. ", "page_idx": 16}, {"type": "text", "text": "Moreover, as mentioned in the previous section, the strong generalization ability of large-scale diffusion models is also vital to implementing our method, which is the key to the ease of implementation of SRStitcher without training or task-specific data annotations. ", "page_idx": 16}, {"type": "text", "text": "Thus, adopting the diffusion model proved essential for addressing the challenges of this paper. It provided a more fitting solution than any other model available. ", "page_idx": 16}, {"type": "text", "text": "A.6 Why not focus on the registration stage ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The registration stage is not the primary focus of this paper. We employ a simplified homography estimation network from $\\mathrm{UDIS}++$ [35] to address the registration challenges. It is essential to clarify that this does not imply a devaluation of the registration stage. Registration has been the most extensively researched of the three stitching stages, with significant work devoted to improving homography accuracy. However, perfect homography matrices that precisely align images do not exist for scenes that are non-planar or involve cameras with different projection centers. ", "page_idx": 16}, {"type": "text", "text": "There are two mainstream methods to overcome these inherent limitations: the multi-homography warp method [52] and the dense match method [47]. However, the multi-homography method faces challenges in parallelization and integration within deep learning frameworks [35], while dense matching is generally slower and less robust. ", "page_idx": 16}, {"type": "text", "text": "These limitations inform our decision to leverage the existing homography network and concentrate our efforts on enhancing the robustness of the subsequent stages. Our experimental results show that this decision has been beneficial: SRStitcher exhibits greater robustness to registration errors, thereby reducing the precision requirements of the registration stage. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B A brief survey of the image stitching pipeline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The image stitching pipeline can be divided into three stages, and the following subsections are described based on each stage. ", "page_idx": 17}, {"type": "text", "text": "B.1 Image registration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Early image registration works [57, 8, 31] are limited by the feature extraction method, which often falters under conditions of rotation, scaling, and illumination changes. To solve the scale changes problem, AutoStitch [6] marks a significant advancement by incorporating the Scale-invariant Feature Transform (SIFT) to extract scale-invariant features. However, this method is challenging to apply to situations with multiple depth layers. To address multi-depth layers condition, DHW [12] proposes a model that assumes the presence of two distinct planes within the image, applying different homography adjustments to each. However, the performance of this method can be severely impacted by the dynamics of camera movement. More recently, NIS [28] introduces the depth map integration to enhance registration accuracy. However, this method relies on accurately estimating depth maps, presenting its own implementation challenges. Yu et al. [51] develop a technique using the epipolar displacement field to improve registration in scenes with significant parallax. ", "page_idx": 17}, {"type": "text", "text": "Feature-based methods have traditionally been the cornerstone of image registration techniques. However, these methods often need more geometric structure and in low-texture scenarios where traditional feature detection techniques are prone to failure. ", "page_idx": 17}, {"type": "text", "text": "In recent years, the advent of deep learning has revolutionized the field of image registration by enabling the extraction of rich semantic features through deep neural networks. Hoang et al [17] and Shi et al. [42] both propose the use of Convolutional Neural Networks (CNNs) to enhance feature representations in image stitching registration. Despite their progress, these approaches primarily use deep learning for feature enhancement rather than creating a holistic learning-based framework. VFISNet [32] is the first complete learning-based framework for image stitching, but it is limited by its inability to handle images of arbitrary resolutions. EPISNet [36] is improved on VFISNet by introducing a flexible mechanism that supports the input of any image size through scalable image and homography adjustments. HomoGAN [18] introduces a method based on the Generative Adversarial Network(GAN) to enhance the quality of homography estimations, representing a novel application of GANs in this field. Jiang et al. [22] integrates graph convolutional networks into the image stitching framework to boost the precision of multi-spectral image registration. LBHomo [21] introduces a semi-supervised approach to estimate homography more accurately in large-baseline scenes by sequentially multiplying multiple intermediate homography. RHWF [7] introduces homographyguided image warping and Focus transformer into the recursive homography estimation framework to further refine homography estimation accuracy. ", "page_idx": 17}, {"type": "text", "text": "B.2 Image fusion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The earliest fusion method is weighted fusion [5], which requires high registration accuracy. If registration is imperfect or there is a color mismatch between the images, visible seams may appear, which can degrade image quality. APAP [52] introduces a smoothly varying projection field to enhance fusion accuracy. However, APAP tends to introduce severe perspective distortions in nonoverlapping areas, limiting its applicability. Inspired by interactive digital photomontage [1], Gao et al. [13] propose the seam-based fusion method, which involves a seam prediction stage to identify optimal seam lines between overlapping images. Although effective, it is notably time-consuming. Therefore, SEAGULL [29] proposes to improve the previous seam-based methods by using estimated seams to guide local alignment optimization, enhancing seam quality and reducing processing time. However, it struggles with repetitive textures, where it still shows poor performance. ", "page_idx": 17}, {"type": "text", "text": "The methods above are all traditional fusion methods characterized by limited versatility and difficulty adapting to complex scenarios. To solve the defects of traditional solutions, UDIS [33] proposes a reconstruction-based model to improve the quality of the fused image. This method sometimes produces artifacts and strange blurs in overlapping areas despite its advances. Inspiration from traditional seam-based approaches, $\\mathrm{UDS}{+}{+}$ [35] and Dseam [11] both use deep learning to refine the seam finding process. Though these fusion methods offer more robust and flexible solutions to improve the ability to handle complex scenarios, they cannot still correct the registration error effectively. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.3 Image rectangling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Image rectangling is a relatively new area of computer vision with limited research to date. Prior to the advent of deep learning in this domain, traditional solutions such as those proposed by He et al.[15] and Li et al. [25] used mesh-based warping techniques to address missing areas in images. DeepRectangling [34] represents the first deep learning-based approach in image rectangling, accompanied by a baseline and a public dataset tailored for this specific task. The method continues to rely on mesh-based warping but incorporates learning algorithms to enhance the flil quality and handle complex scenarios more effectively. While these methods are groundbreaking, they often change the global relative pixel positions, which could lead to suboptimal results, especially in cases with large missing areas, resulting in incomplete fills. A more recent method RecDiffusion [56] that employs a diffusion model to better solve the rectangling. Although this method provides a sophisticated solution for achieving rectangularity, it is complex in design, requires long inference times, and requires significant computational resources for training, which limits its practical applicability. ", "page_idx": 18}, {"type": "text", "text": "Moreover, the current deep learning-based image rectangling methods are all based on the DIR-D dataset [34]. DIR-D dataset is a strong assumption dataset, which assumes that some challenging scenes are excluded and that the image registration and fusion are flawless. Therefore, current method do not optimize for the robustness of registration and fusion errors, leading to the error propagation problem illustrated in Figure $1\\textcircled{3}$ . ", "page_idx": 18}, {"type": "text", "text": "In addition, there are some works on fliling irregular edges in other fields [40, 27, 23]. However, due to significant differences between these fields and the varying shapes of missing regions, these studies are not easily applicable to the rectangling task in the current image stitching pipeline. ", "page_idx": 18}, {"type": "text", "text": "B.4 Other similar work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Several prior studies have attempted to integrate fusion and rectangling stages. For instance, RDISNet [55] claims to create an end-to-end image stitching framework that combines all three stages. However, our analysis of its network structure and experimental results suggests that RDISNet may struggle with large parallax scenes, and noticeable noise in its stitching outputs adversely impacts image quality. Chen et al. [10] propose a diffusion-based method to address both fusion and rectangling tasks. However, this method necessitates meticulously prepared datasets and retraining of the diffusion model, leading to substantial cost demands. In summary, while these existing methods are innovative, they present significant limitations with low reproducibility. As a result, we exclude them from our baseline comparisons. ", "page_idx": 18}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/1922bf901b52f87e9d155a11ace1a4f5176dbec363b2c040cf15b566a39d583f.jpg", "img_caption": ["Figure 11: Examples of unreasonable NR-IQA scores. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Detail of metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 NR-IQA metric", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "(1) NR-IQA metric settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "HIQA. HIQA [43] is designed for the \u2018wild\u2019 image. HIQA is particularly suitable for evaluating the predominantly outdoor images in the UDIS-D dataset, making it an ideal choice for our analysis. We implement this metric based on the public source code with default parameters. ", "page_idx": 19}, {"type": "text", "text": "CLIPIQA. CLIPIQA [49] based on the Contrastive Language-Image Pre-training (CLIP) models, which allows for adaptable evaluations across different datasets. We use IQA-PyTorch Tool [9] to implement this metric with prompts [\u2018nature image\u2019, \u2018stitched image\u2019] to evaluate whether the stitched images appear more natural. ", "page_idx": 19}, {"type": "text", "text": "(2) Limitation of NR-IQA metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Through our analysis of the results obtained by the NR-IQA metrics HIQA and CLIPIQA, we find a discrepancy between these metrics and human sensory preferences for image quality. Sometimes, our method produces visually higher quality and authenticity images, as shown in Figure 11, but the scores assigned by these NR-IQA metrics are counterintuitively low. ", "page_idx": 19}, {"type": "text", "text": "We believe that this problem arises from a mismatch between the training datasets and the unique challenges of image stitching. Metrics such as HIQA and CLIPIQA, are trained on IQA-specific datasets such as KonIQ-10k [19] and Live-iWT [14], which focus primarily on image distortions such as white noise, low-light noise, and JPEG compression artifacts. These types of distortions are very different from those encountered in image stitching, such as artifacts and incongruous inpainting content. As a result, models trained on such data may struggle to perfectly reflect the true perceptual quality of stitched images. ", "page_idx": 19}, {"type": "text", "text": "While the NR-IQA metric may exhibit inaccuracies in certain scenarios, it typically indicates superior stitching images in most cases, which is why we have chosen to use it. However, we acknowledge the inherent limitations of current NR-IQA metrics, especially their inability to effectively capture the nuanced aspects of image quality improvements in stitching. Therefore, we do not employ these metrics in the ablation experiments. And, we also conduct user study as a supplementary proof. ", "page_idx": 19}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/58085effd4b4c76f7a034a87220dfb7b65abee08d9d1b8297f8229133543ece2.jpg", "img_caption": ["Figure 12: Visual presentation of the CCS metric. Inside the blue box is the text extracted using $\\mathtt{C o C a}(\\cdot)$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 CCS metric", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We design the CCS metric to measure the image content consistency before and after stitching, which is based on the idea of an image-to-text model. We first extract text information based on the image through the $\\mathtt{C o C a}(\\cdot)$ model, then map the text into the embedding space through the Bert(\u00b7) model, and finally measure the cosine similarity of the embedding before and after stitching to calculate the CCS. We give an intuitive evaluation of this metric in Figure 12. ", "page_idx": 20}, {"type": "text", "text": "In defining $C C S_{n}$ , we set $n{=}4$ , a value we believe is most effective. Using $\\scriptstyle n=1$ loses local meaning. For $n\\geq9$ , in scenes with large areas missing (as illustrated in the second scene of Figure 2), parts of the local image may lack semantic content. This absence hampers the extraction of text information, thus undermining the credibility of the CCS metric. Additionally, excessively small input images can negatively impact the performance of the $\\mathtt{C o C a}(\\cdot)$ model. ", "page_idx": 20}, {"type": "text", "text": "We also give the source code for the CCS metric implementation in the Supplementary Material, as shown in metrics/ccs.py. ", "page_idx": 20}, {"type": "text", "text": "D Additional experiments and results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Evaluation results of the example in Figure 2 on all baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To ensure the presentation effect, all baseline results are not provided in the main paper Figure 2.   \nHere, we offer the complete results, as shown in Figure 13. ", "page_idx": 20}, {"type": "text", "text": "Our results in the figure can also be reproduced using the source code and provided data in the Supplementary Material. ", "page_idx": 20}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/b4711afd7ccb1457e32b4b694ec93e3915e6ae8714105483601e56382038443b.jpg", "img_caption": ["Figure 13: Evaluation results of the example in Figure 2 on all baselines. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "(1) The impact of $K_{g}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The $K_{g}$ in in Eq. 10 and Eq. 11 determines the intensity of the distance transform applied within the weighted masks. Despite the potential variations available in the type of distance transform (e.g., L1 vs. L2) and the kernel size, our empirical observations show that these modifications do not significantly impact the stitching results. Therefore, we set a commonly used value using an L2 distance and a kernel size of 3. ", "page_idx": 22}, {"type": "text", "text": "(2) The impact of seed ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We show the impacts of different seeds in Figure 14. Our method produces more stable results with high quality. With different random seeds, Stable-Diffusion-v1-5-inpainting (SD1.5) [41] and Stable-Diffusion-v2-inpainting (SD2) [3] produce completely different abnormal contents. In contrast, our proposed method consistently demonstrates remarkable stability. ", "page_idx": 22}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/b526bd753955d6d617844c706be3e2aa1660630e2a31348b6bebfb12ec45f9fd.jpg", "img_caption": ["Figure 14: Ablation study of the seed. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "(3) The guidance scale and inference step ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The guidance scale and inference step are two classical parameters in diffusion models. The impact of adjusting these parameters has been extensively validated by previous research [48]. Consequently, this paper does not delve into selecting their values but instead adopts two commonly used settings: 7.5 and 50. ", "page_idx": 22}, {"type": "text", "text": "D.3 Qualitative evaluation results of SRStitcher variants ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section presents the qualitative evaluation comparing SRStitcher variants based on various diffusion models. The version based on the Stable Diffusion 2 model [2] is designated as SRStitcherS. Additionally, the implementation utilizing the Stable Diffusion 2 Unclip model [4] is termed SRStitcher-U. Finally, the implementation with Controlnet Inpainting model [54] is defined as SRStitcher-C. ", "page_idx": 22}, {"type": "text", "text": "The test results are shown in the Figure 15. The Stable-Diffusion-v2-inpainting model exhibits the best performance, which is the primary reason for selecting it as our base model. The StableDiffusion-2-1-Unclip, a fine-tuned model based on Stable-Diffusion V2.1, is chosen in an attempt to leverage its CLIP image embedding functionality. However, the structural integrity of the results generated by this model is significantly inferior to those of the other two models, likely due to compromises introduced during fine-tuning. ", "page_idx": 22}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/e6e1d56488f26a5f969a25e9a7464178961c84f0645d5957e4f93f6b952ee00d.jpg", "img_caption": ["Figure 15: Qualitative evaluation results of SRStitcher variants. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Notably, the performance of the SRStitcher-C based on ControlNet has exceeded our expectations. While the model does exhibit a more pronounced issue with local blurring, it demonstrates exceptional capability in preserving the original image information. In future work, should model fine-tuning be employed to further optimize the stitching effect, we may consider beginning our enhancements with the ControlNet model. ", "page_idx": 23}, {"type": "text", "text": "We provide the source code for all SRStitcher variants in the Supplementary Material. Therefore, we do not describe implementation of them here. ", "page_idx": 23}, {"type": "text", "text": "D.4 Generalization on other datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In addition to the UIDS-D dataset, there are traditional datasets in the field of image stitching, such as APAPdataset [52] and REWdataset [26]. However, these datasets are very small, containing only dozens of images, which reduces their usefulness for meaningful comparative experiments. To demonstrate the effectiveness of our method on a broader spectrum of data, we present some experimental results on APAPdataset and REWdataset, as illustrated in Figure 16. ", "page_idx": 23}, {"type": "text", "text": "In addition, Figure 17 shows the experimental results comparing SRStither and other methods on the traditional datasets. ", "page_idx": 23}, {"type": "text", "text": "D.5 Examples of local blurring ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we present examples of local blur and compare them with other baselines, as illustrated in Figure 18. We contend that occasional local blur is a tolerable side effect of our scheme, especially when weighed against the generation of significant anomalous content seen in other models. Future research could potentially address this issue by fine-tuning the model. ", "page_idx": 23}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/eb88d9034adc97da594dfcf9e27a13575d5cb0e688956175f0ebb637953262a6.jpg", "img_caption": ["Figure 16: More results on traditional datasets APAPdataset [52] and REWdataset [26] by SRStitcher. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/1a45f642ae08665c07e6275ef8d2313c4e8f96a19cfd4d46481f1fa01a1f6e7a.jpg", "img_caption": ["Figure 17: Comparison results on traditional datasets APAPdataset [52] and REWdataset [26]. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ZViYPzh9Wq/tmp/80c8738267cb665211e3750d2ee4aaf5ffa6c01efc82a23a48504ca9bd7d6eef.jpg", "img_caption": ["Figure 18: Examples of local blurring. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.6 Speed ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our solution requires only a single inference step, making it significantly faster than more complex models that require two inference steps, such as $\\mathrm{UDIS+SD1.}5$ to UDISplus $+S\\mathrm{D}2$ . Although our method is slightly slower compared to $\\scriptstyle\\mathrm{UDIS+DR}$ to UDISplus+Lama, our experimental results demonstrate that it substantially outperforms these methods regarding stitched image quality, robustness, and generalization. Given these advantages, the minor sacrifice in speed is justifiable. In particular, even without acceleration optimizations like TensorRT [37], our method achieves an average processing speed of 27 it/s on an NVIDIA 4090 GPU, which is sufficient for real-time performance. ", "page_idx": 25}, {"type": "text", "text": "E Broader impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This paper aims to introduce a novel image stitching pipeline design that integrates large-scale models into the image stitching process. Unlike previous diffusion model-based image stitching methods, our method does not require training or task-specific supervised datasets. This significantly lowers the implementation threshold, facilitating broader adoption of the method and encouraging more researchers to contribute and enhance this work. One potential negative impact of our method is its ability to make minor modifications to stitched images. This capability could be misused in visual fraud tasks, particularly in the context of surveillance videos. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "F Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We have made significant efforts to ensure the reproducibility of our method. The code and details of the SRStitcher, Stitcher variants, and CCS metric are all uploaded in the Supplementary Material. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitations of the work are illustrated in section 5 Discussion and conclusion. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide proofs of the proposed model in the main paper and Appendix A. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We give a detailed explanation of all the experimental results in the section 4 Experiments and provide supplementary notes in the Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The source code for this paper has been open sourced and can be accessed through the link provided in the Abstract. And, we use the publicly available datasets. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We give all experimental details in the section 4 Experiments and provide supplementary notes in the Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We report the standard deviation under different seeds in Table 2. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We give all implement details including computer resources in the Experimental setup of section 4 Experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research respects the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We discuss the potential positive societal impacts and negative societal impacts of the work in the Appendix E. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our method does not involve model training and uses publicly available datasets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have explicitly stated the data and model sources used in the main text. All the models and data used are open source and shared. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The source code for this paper has been open sourced and can be accessed through the link provided in the Abstract, with detailed instructions for use. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our user study included only 20 volunteer participants and they only rated the image quality. This test should not fall within the scope of crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing or research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]