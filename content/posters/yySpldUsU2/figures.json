[{"figure_path": "yySpldUsU2/figures/figures_5_1.jpg", "caption": "Figure 1: Examples of slow-learnable (top) and fast-learnable (bottom) in CIFAR-10 found by our method. Examples in the top row (slow-learnable) are harder to identify visually and look more ambiguous (part of the object is in the image or the object is smaller and the area associated with the background is larger). In contrast, examples in the bottom row (fast-learnable) are not ambiguous and are clear representatives of their corresponding class, hence are very easy to visually classify (the entire object is in the image and the area associated with the background is small).", "description": "This figure shows example images from the CIFAR-10 dataset that are classified as either slow-learnable or fast-learnable by the USEFUL method.  Slow-learnable examples are visually ambiguous, often partially obscured or with a cluttered background. Fast-learnable examples are easily identifiable and clearly represent their class.", "section": "4 Method: UpSample Early For Uniform Learning (USEFUL)"}, {"figure_path": "yySpldUsU2/figures/figures_6_1.jpg", "caption": "Figure 2: TSNE visualization of output vectors. (left) ResNet18/CIFAR-10 at epoch 8. (right) CNN/toy data generated based on Definition 3.1 with Ba = 0.2, Be = 1, a = 0.9, iteration 200.", "description": "This figure visualizes the output vectors of a ResNet18 model trained on CIFAR-10 and a CNN model trained on synthetic data using t-distributed stochastic neighbor embedding (t-SNE).  The left panel shows the results for ResNet18 after 8 epochs of training on CIFAR-10, illustrating the separation of slow-learnable and fast-learnable features in the feature space.  The right panel shows the results for a CNN trained on synthetic data generated according to a specific distribution (Definition 3.1 in the paper, with parameters Ba=0.2, Be=1, a=0.9) after 200 iterations. This panel also shows a clear separation between slow and fast learnable features. The visualization helps demonstrate how the model output can be used to identify examples with slow and fast learnable features early in training, which forms the basis for the proposed USEFUL method.", "section": "3 Theoretical Analysis: SAM Learns Different Features More Evenly"}, {"figure_path": "yySpldUsU2/figures/figures_6_2.jpg", "caption": "Figure 3: GD (blue) vs. SAM (orange) on toy datasets. Data is generated based on Definition 3.1 with different Ba and fixed \u03b2e = 1, \u03b1 = 0.9.  \u2026 and  lines denote the alignment (i.e., inner product) of fast-learnable (ve) and slow-learnable (va) features with the model weight (w)). (a), (b) GD and SAM first learn the fast-learnable feature. Notably, GD learns the fast-learnable feature very early. (c) Test accuracy of GD & SAM improves by increasing the strength of the slow-learnable feature.", "description": "This figure compares the training dynamics of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM) on synthetic datasets.  It shows the alignment of learned model weights with fast and slow-learnable features over training iterations. GD learns the fast features much earlier than SAM.  Increasing the strength of slow-learnable features improves accuracy for both GD and SAM, but more so for SAM, highlighting that SAM's improved generalization is linked to more even learning of these features.", "section": "Theoretical Analysis: SAM Learns Different Features More Evenly"}, {"figure_path": "yySpldUsU2/figures/figures_7_1.jpg", "caption": "Figure 4: Test classification error of ResNet18 on CIFAR10, STL10, TinyImageNet and ResNet34 on CIFAR100. The numbers below bars indicate the approximate training cost and the tick on top shows the std over three runs. USEFUL enhances the performance of SGD and SAM on all 5 datasets. TrivialAugment (TA) further boosts SAM's performance (except for CINIC10). Remarkably, USEFUL consistently boosts the performance across all scenarios and achieves (to our knowledge) SOTA performance for ResNet18 and ResNet34 on the selected datasets when combined with SAM and TA.", "description": "This figure compares the test classification error of different models trained using various optimization methods on five image classification datasets.  The models are ResNet18 on CIFAR10, STL10, and TinyImageNet and ResNet34 on CIFAR100.  The optimization methods include SGD, SGD with USEFUL, SAM, SAM with USEFUL, SAM with TrivialAugment, and SAM with USEFUL and TrivialAugment.  The bars show the test error, and the numbers below indicate the relative training time compared to SGD.  The figure demonstrates that USEFUL consistently improves the generalization performance of both SGD and SAM across all datasets, often achieving state-of-the-art results when combined with SAM and Trivial Augmentation.", "section": "5 Experiments"}, {"figure_path": "yySpldUsU2/figures/figures_8_1.jpg", "caption": "Figure 5: Test classification errors of different architectures on CIFAR10. USEFUL improves the performance of SGD and SAM when training different architectures. TrivialAugment (TA) further boosts SAM's capabilities. The results for 3-layer MLP can be found in Figure 9.", "description": "This figure shows the test classification errors for various network architectures (VGG19, DenseNet121, ViT-S) trained on CIFAR-10 using different optimization methods: SGD, SGD with USEFUL, SAM, SAM with USEFUL, SAM with TrivialAugment (TA), and SAM with USEFUL and TA.  The results demonstrate that USEFUL consistently improves the performance of both SGD and SAM across all architectures tested.  Additionally, the combination of SAM with TA further enhances the performance, and adding USEFUL to this combination yields the best results in most cases. The 'Time to Train w.r.t. SGD' indicates the relative training time of each method compared to SGD.", "section": "5.3 USEFUL is Effective across Architectures & Settings"}, {"figure_path": "yySpldUsU2/figures/figures_9_1.jpg", "caption": "Figure 6: USEFUL vs. Random Upsampling, when training ResNet18 on CIFAR10 and CIFAR100.", "description": "This figure compares the performance of USEFUL against random upsampling when training ResNet18 on CIFAR10 and CIFAR100 datasets.  It shows that USEFUL significantly improves the test classification error compared to both standard training (Orig) and random upsampling (Rand) for both SGD and SAM optimizers.  The results highlight the effectiveness of USEFUL in improving generalization by carefully modifying the data distribution, rather than simply increasing the amount of data through random sampling.", "section": "5. Experiments"}, {"figure_path": "yySpldUsU2/figures/figures_30_1.jpg", "caption": "Figure 7: USEFUL first trains the model for a few epochs t, which in practice is around 5\u201310% of the total training epochs. It then clusters examples in every class into 2 groups and upsamples the cluster with higher average loss. Finally, the base model is retrained from scratch on the modified data distribution.", "description": "This figure illustrates the workflow of the USEFUL algorithm.  First, a model is trained on the original dataset for a small number of epochs. Then, USEFUL performs k-means clustering on the model's output for each class to separate examples into two clusters: one with a higher average loss (representing examples containing slow-learnable features), and one with a lower average loss (examples with fast-learnable features).  The algorithm then upsamples the cluster with the higher average loss and trains the model again from scratch on this modified dataset. The result is a model that learns features more uniformly.", "section": "4 Method: UpSample Early For Uniform Learning (USEFUL)"}, {"figure_path": "yySpldUsU2/figures/figures_31_1.jpg", "caption": "Figure 8: The gap between contribution of fast-learnable and slow-learnable features towards the model output in SAM and GD. The toy datasets is generated from the distribution in Definition 3.1 with Ba = Be = a = 1.", "description": "This figure compares the learning speed of fast-learnable and slow-learnable features for both GD and SAM. The y-axis represents the difference between the alignment of model weights with fast-learnable and slow-learnable features, while the x-axis represents the training iteration. The plot shows that SAM learns both types of features at a more uniform speed compared to GD.  The significant gap between the two curves in the early stages reflects the simplicity bias of GD, as it learns fast-learnable features much more quickly than slow-learnable ones. In contrast, SAM exhibits a more balanced learning rate for both feature types.", "section": "3 Theoretical Analysis: SAM Learns Different Features More Evenly"}, {"figure_path": "yySpldUsU2/figures/figures_31_2.jpg", "caption": "Figure 9: Test classification errors of 3-layer MLP on CIFAR10. The number below each bar indicates the estimated cost to train the model and the tick on top shows the standard deviation over three runs. USEFUL improves the performance of SGD and SAM when training with 3-layer MLP.", "description": "This figure shows the test classification errors of a 3-layer Multilayer Perceptron (MLP) model trained on the CIFAR-10 dataset using different optimization methods: SGD, SGD with USEFUL, SAM, SAM with USEFUL, SAM with TrivialAugment (TA), and SAM with USEFUL and TA.  The x-axis represents the training time relative to the time taken by SGD. The y-axis shows the test error rate (%). The bars show the mean test error for each method and the ticks on top represent the standard deviation across multiple runs. The figure demonstrates that incorporating USEFUL consistently improves the performance of both SGD and SAM, achieving lower test error rates compared to the baselines.  The combination of SAM and TA also leads to improved performance.  The best performance is obtained by combining SAM, USEFUL and TA.", "section": "5 Experiments"}, {"figure_path": "yySpldUsU2/figures/figures_31_3.jpg", "caption": "Figure 10: L1 norm of ResNet18 trained on CIFAR10 and ResNet34 trained on CIFAR100. Lower L1 norm indicates a sparser solution and stronger implicit regularization properties [3, Section 4.2]. SAM has a lower L1 norm than SGD, and USEFUL further reduces the L1 norm of SGD and SAM.", "description": "This figure compares the L1 norm of models trained using three different methods: SGD, SAM, and SGD+USEFUL.  Lower L1 norms generally indicate sparser solutions and better implicit regularization, leading to improved generalization.  The results show that SAM already achieves a lower L1 norm than SGD, and that the proposed USEFUL method further reduces the L1 norm when used in conjunction with SGD.", "section": "5.4 USEFUL's Solution has Similar Properties to SAM"}, {"figure_path": "yySpldUsU2/figures/figures_32_1.jpg", "caption": "Figure 11: Forgetting scores for training ResNet18 on CIFAR10. Forgetting scores measure the learning speed of examples in training data. USEFUL approaches the training dynamics of SAM, with more examples being forgotten infrequently and fewer examples being forgotten frequently.", "description": "This figure shows the forgetting scores for training ResNet18 on CIFAR10.  The forgetting score is a metric indicating how quickly examples are learned during training. A lower forgetting score implies that the example is learned quickly and retained effectively by the model. The figure compares the forgetting scores of three different training methods: standard SGD, SGD with USEFUL, and SAM.  It shows that both SGD+USEFUL and SAM have fewer examples with high forgetting scores (meaning that the model struggles to learn and retain them effectively) than standard SGD.  This demonstrates that USEFUL, by modifying the training data distribution, leads to similar training dynamics as SAM which improves model generalization, specifically in learning examples more uniformly in training.", "section": "5. Experiments"}, {"figure_path": "yySpldUsU2/figures/figures_33_1.jpg", "caption": "Figure 12: Comparing test classification errors on Waterbirds. The number below each bar indicates the approximated cost to train the model and the tick on top shows the standard deviation over three runs. USEFUL boosts the performance of SGD and SAM on the balanced test set, showing its generalization to the OOD setting. In addition, the success of USEFUL in fine-tuning reveals its new application to the transfer learning setting.", "description": "This figure compares the performance of different optimization methods (SGD, SGD with USEFUL, SAM, SAM with USEFUL, SAM with TrivialAugment, and SAM with USEFUL and TrivialAugment) on the Waterbirds dataset. The results show that USEFUL consistently improves the performance of both SGD and SAM, highlighting its ability to generalize to out-of-distribution (OOD) settings. The figure also demonstrates the effectiveness of USEFUL in fine-tuning pre-trained models, suggesting its applicability to transfer learning.", "section": "5.2 USEFUL is Effective across Datasets"}, {"figure_path": "yySpldUsU2/figures/figures_34_1.jpg", "caption": "Figure 13: Class count before and after upsampling by USEFUL on long-tail CIFAR10 dataset.", "description": "This figure shows the distribution of classes in the long-tailed CIFAR10 dataset before and after applying the USEFUL method. The original dataset has a highly imbalanced class distribution.  USEFUL method aims to alleviate the simplicity bias by upsampling the under-represented classes, thus improving generalization performance. The figure visually demonstrates the effect of USEFUL on the class distribution, showing how it rebalances the dataset to a more even distribution of examples across the classes.", "section": "D.6 USEFUL is also effective for noisy label data"}, {"figure_path": "yySpldUsU2/figures/figures_35_1.jpg", "caption": "Figure 14: Ablation studies of training ResNet18 on CIFAR10. In each experiment, we used the standard training settings while (left) varying training batch size or (middle) varying learning rate, or (right) varying upsampling factor.", "description": "This figure presents the ablation study results of training ResNet18 on CIFAR10 dataset. It shows how the model's performance changes depending on three factors: batch size, learning rate, and upsampling factor.  The results reveal the impact of these hyperparameters on the effectiveness of the USEFUL method and helps determine optimal values for each.", "section": "D.8 Ablation studies"}, {"figure_path": "yySpldUsU2/figures/figures_35_2.jpg", "caption": "Figure 15: Separating epoch analysis. (left) Red lines indicate our optimal choice of t to separate examples at and restart training. The early epoch that can best separate the examples is when the change in training error starts to shrink. (right) Red points indicate our optimal choice of t.", "description": "This figure shows the training error trajectories for CIFAR10 and CIFAR100.  The left panel shows the training error over epochs for CIFAR10, highlighting the point where the decrease in training error starts to slow (marked with a red dotted line).  This point is selected as the optimal epoch to apply USEFUL. The right panel visually represents the same information but for CIFAR100.  The red dotted line again indicates the suggested epoch for applying the USEFUL technique.  The figure visually supports the claim that choosing an appropriate separating epoch improves model accuracy.", "section": "D.8 Ablation studies"}, {"figure_path": "yySpldUsU2/figures/figures_35_3.jpg", "caption": "Figure 15: Separating epoch analysis. (left) Red lines indicate our optimal choice of t to separate examples at and restart training. The early epoch that can best separate the examples is when the change in training error starts to shrink. (right) Red points indicate our optimal choice of t.", "description": "This figure shows the ablation study on choosing the optimal separating epoch in USEFUL.  The left panel shows training error trajectories for CIFAR10 and CIFAR100, indicating the optimal epoch to separate fast-learnable and slow-learnable examples by the change in training error. The right panel presents test error results, illustrating that selecting the epoch according to the training error produces the best generalization performance.", "section": "D.8 Ablation studies"}]