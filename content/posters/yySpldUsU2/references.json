{"references": [{"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2020-10-01", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), a core concept in the current work, which is rigorously analyzed and extended."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "publication_date": "2019-08-01", "reason": "This paper provides theoretical grounding for the simplicity bias of gradient descent, a central theme addressed and mitigated in the current work."}, {"fullname_first_author": "Zixiang Chen", "paper_title": "Towards understanding the mixture-of-experts layer in deep learning", "publication_date": "2022-12-01", "reason": "This paper provides a theoretical framework for understanding feature learning in deep networks, which is used to analyze and prove the main theoretical results."}, {"fullname_first_author": "Suriya Gunasekar", "paper_title": "Implicit regularization in matrix factorization", "publication_date": "2017-12-01", "reason": "This paper discusses implicit regularization, a phenomenon linked to simplicity bias that is central to the discussion of generalization performance."}, {"fullname_first_author": "Preetum Nakkiran", "paper_title": "Deep double descent: Where bigger models and more data hurt", "publication_date": "2021-12-01", "reason": "This paper addresses the phenomenon of double descent, providing context for understanding generalization in overparameterized models."}]}