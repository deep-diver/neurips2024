[{"figure_path": "gvtCR7dHJ3/figures/figures_2_1.jpg", "caption": "Figure 1: Training curves for the total loss L (:= Lr + Lb), PDE residual loss Lr, and boundary loss Lb for viscous Burgers' equation.", "description": "This figure shows the training curves of total loss, PDE residual loss, and boundary loss for solving viscous Burgers' equation.  It illustrates how the total loss consistently decreases, while PDE loss increases at a certain point during training. This highlights the issue of conflicting gradients and imbalance between losses in training PINNs.", "section": "3 Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_3_1.jpg", "caption": "Figure 2: Conflicting and dominating gradients in PINNs. Here, \u03c6 is defined as the angle between \u2207Lr and \u2207Lb, R = ||\u2207Lr||/||\u2207Lb|| is the magnitude ratio between gradients.", "description": "This figure shows the distribution of the cosine of the angle (\u03c6) between the gradients of the PDE residual loss (\u2207Lr) and the boundary loss (\u2207Lb) during the training of PINNs, as well as the distribution of the ratio (R) of their magnitudes.  The left histogram illustrates the prevalence of conflicting gradients (cos(\u03c6) < 0). The right histogram shows that the magnitude of the PDE residual gradient frequently dominates the boundary loss gradient, indicating a significant imbalance that can hinder effective training.", "section": "3 Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of dual cone region K*t and its subspace Gt", "description": "This figure visualizes the dual cone region K*t and its subspace Gt.  The dual cone K*t is the set of vectors that have non-negative inner products with all vectors in the cone Kt, which is generated by the gradients of the PDE residual loss and the boundary loss. The subspace Gt is a subset of K*t, defined as the set of conic combinations of the projections of the total gradient onto the orthogonal complements of the PDE residual loss gradient and boundary loss gradient.  The figure illustrates how the updated gradient (gdual) in the DCGD algorithm is chosen to ensure it lies within Gt, guaranteeing simultaneous decrease in both PDE residual loss and boundary loss.", "section": "4.1 Dual Cone Region"}, {"figure_path": "gvtCR7dHJ3/figures/figures_5_1.jpg", "caption": "Figure 4: The updated gradient gdual of three DCGD algorithms.", "description": "This figure visualizes how the updated gradient (gdual) is determined in three different variants of the Dual Cone Gradient Descent (DCGD) algorithm: Projection, Average, and Center.  Each subfigure shows a different strategy for selecting gdual within the dual cone region (Gt) based on the gradients of the PDE residual loss and boundary loss.  (a) DCGD (Projection) projects the total gradient onto the subspace Gt. (b) DCGD (Average) averages the projected gradients. (c) DCGD (Center) uses the angle bisector of the two gradients as the updated gradient. The visualization helps to understand the different approaches to ensure that the updated gradient remains within the dual cone, which guarantees that both losses can decrease simultaneously, avoiding the conflicting gradient issues during PINN training.", "section": "4.3 Dual Cone Gradient Descent: Projection, Average, and Center"}, {"figure_path": "gvtCR7dHJ3/figures/figures_6_1.jpg", "caption": "Figure 2: Conflicting and dominating gradients in PINNs. Here, \u03c6 is defined as the angle between \u2207Lr and \u2207Lb, R = ||\u2207Lr||/||\u2207Lb|| is the magnitude ratio between gradients.", "description": "This figure shows the distribution of the cosine of the angle between the gradients of the PDE residual loss and the boundary loss (cos(\u03c6)) and the ratio of their magnitudes (R) during the training process of PINNs for the Helmholtz equation.  The histograms reveal that: (a) In approximately half of the iterations, the gradients are conflicting (cos(\u03c6) < 0), indicating that reducing one loss increases the other. (b) The magnitude of the PDE residual gradient is often significantly larger than that of the boundary loss gradient (R >> 1), implying that the optimization process is dominated by the PDE residual loss, potentially neglecting the boundary loss.", "section": "3 Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_6_2.jpg", "caption": "Figure 2: Conflicting and dominating gradients in PINNs. Here, \u03c6 is defined as the angle between \u2207Lr and \u2207Lb, R = ||\u2207Lr||/||\u2207Lb|| is the magnitude ratio between gradients.", "description": "This figure shows histograms visualizing the distribution of the cosine of the angle (cos(\u03c6)) between the gradients of the PDE residual loss (\u2207Lr) and the boundary loss (\u2207Lb) during the training process of Physics-Informed Neural Networks (PINNs).  It also displays a histogram of the magnitude ratio (R) between the two gradients (||\u2207Lr||/||\u2207Lb||). The histograms illustrate that conflicting gradients (cos(\u03c6) < 0) and gradients with a significant imbalance in magnitude (R being much greater than 1) are frequent occurrences during PINN training, suggesting a potential cause for training instability.", "section": "Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_8_1.jpg", "caption": "Figure 7: Double pendulum problem: prediction of each method. SGD and ADAM find shifted solutions, but DCGD successfully approximates the reference solution.", "description": "This figure compares the performance of three different optimization algorithms (SGD, ADAM, and DCGD) on the double pendulum problem.  The plots show the predicted angles (\u03b81 and \u03b82) over time for each algorithm. The reference solution is shown in blue.  SGD and ADAM fail to accurately predict the reference solution, exhibiting a significant shift in the predicted angles.  In contrast, the DCGD algorithm closely matches the reference solution, indicating its superior ability to solve this challenging problem.", "section": "5.2 Failure Mode of PINNs and Complex PDEs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_18_1.jpg", "caption": "Figure 8: The loss landscape and contour map of the toy example.", "description": "This figure visualizes the loss landscape and contour map of a toy example used in the paper to illustrate the challenges in training Physics-Informed Neural Networks (PINNs) and the benefits of the proposed Dual Cone Gradient Descent (DCGD) method.  The loss landscape is a 3D surface showing how the total loss function varies with two parameters (\u03b8\u2081 and \u03b8\u2082). The contour map provides a 2D projection of the same data, showing the level curves of the loss function.  The Pareto set (optimal solutions) is highlighted in gray in the contour map. This figure demonstrates how conflicting gradients can lead to failure in training PINNs and how DCGD can effectively resolve these issues.", "section": "4.4 Benefits of the DCGD framework"}, {"figure_path": "gvtCR7dHJ3/figures/figures_19_1.jpg", "caption": "Figure 9: Helmholtz equation: approximated solution versus the reference solution.", "description": "This figure visualizes the results of solving the Helmholtz equation using Physics-Informed Neural Networks (PINNs). It compares the exact solution (a), the PINN's prediction (b), and the absolute error between the two (c). The color map represents the magnitude of the solution, allowing for a visual comparison of the accuracy of the PINN's approximation. This figure showcases the effectiveness of the proposed method in approximating solutions to partial differential equations. ", "section": "5.1 Comparison on benchmark equations"}, {"figure_path": "gvtCR7dHJ3/figures/figures_20_1.jpg", "caption": "Figure 10: Burgers' equation: approximated solution versus the reference solution.", "description": "This figure compares the exact solution, the PINN prediction, and the absolute error for the viscous Burgers' equation.  It visually demonstrates the accuracy of the Physics-Informed Neural Network (PINN) in approximating the solution of this benchmark partial differential equation. The plots show the solution across the spatial dimension (x) and the temporal dimension (t). The closeness of the prediction to the exact solution indicates the success of the PINN model in solving the given PDE.", "section": "5.1 Comparison on benchmark equations"}, {"figure_path": "gvtCR7dHJ3/figures/figures_20_2.jpg", "caption": "Figure 10: Burgers' equation: approximated solution versus the reference solution.", "description": "This figure visualizes the results of approximating the solution to the viscous Burgers' equation using a physics-informed neural network (PINN). It includes three subplots: (a) shows the exact solution of the equation; (b) displays the solution predicted by the PINN; and (c) presents the absolute error between the exact and predicted solutions.  The plots illustrate the PINN's ability to approximate the solution, with subplot (c) quantifying the accuracy of the approximation.", "section": "5.1 Comparison on benchmark equations"}, {"figure_path": "gvtCR7dHJ3/figures/figures_21_1.jpg", "caption": "Figure 12: Simple double pendulum example", "description": "This figure shows a schematic of a simple double pendulum.  It consists of two point masses (m1 and m2) connected by two massless rods of lengths l1 and l2. The angles \u03b81 and \u03b82 represent the angles that each rod makes with respect to the vertical. This system is used as an example in the paper to illustrate the challenges in training physics-informed neural networks.", "section": "5.2 Failure Mode of PINNs and Complex PDEs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_21_2.jpg", "caption": "Figure 1: Training curves for the total loss L (:= Lr + Lb), PDE residual loss Lr, and boundary loss Lb for viscous Burgers' equation.", "description": "This figure shows the training curves of a physics-informed neural network (PINN) for solving the viscous Burgers' equation.  It plots the total loss, the PDE residual loss, and the boundary loss against the number of training epochs. The key observation is that while the total loss decreases consistently throughout training, the PDE loss increases at a certain point. This highlights one of the main challenges addressed in the paper:  the imbalance and potential conflict between different loss terms during PINN training.", "section": "3 Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_22_1.jpg", "caption": "Figure 10: Burgers' equation: approximated solution versus the reference solution.", "description": "This figure compares the exact solution, the solution predicted by the PINN model, and the absolute error between the two for the viscous Burgers' equation.  The plots show the solutions across the spatial dimension (x) and the temporal dimension (t).  This visual comparison helps to assess the accuracy and effectiveness of the PINN model in solving the equation.", "section": "5.1 Comparison on benchmark equations"}, {"figure_path": "gvtCR7dHJ3/figures/figures_22_2.jpg", "caption": "Figure 2: Conflicting and dominating gradients in PINNs. Here, \u03c6 is defined as the angle between \u2207Lr and \u2207Lb, R = ||\u2207Lr||/||\u2207Lb|| is the magnitude ratio between gradients.", "description": "This figure illustrates the issues of conflicting and dominating gradients in training Physics-Informed Neural Networks (PINNs). The left histogram shows the distribution of the cosine of the angle (\u03c6) between the gradients of the PDE residual loss (\u2207Lr) and the boundary loss (\u2207Lb).  A negative cosine indicates conflicting gradients, while values close to 1 indicate aligned gradients.  The right histogram shows the distribution of the magnitude ratio (R) between the two gradients. A large R indicates that one gradient is significantly larger than the other, which can hinder effective training. The observation shows that conflicting gradients are prevalent in PINN training, and one gradient often dominates the other, leading to training instability and suboptimal solutions.", "section": "3 Empirical Observations and Issues in Training PINNs"}, {"figure_path": "gvtCR7dHJ3/figures/figures_23_1.jpg", "caption": "Figure 16: 3D-Helmholtz equation: approximated solution versus the reference solution.", "description": "This figure compares the exact solution, the predicted solution by the SPINN model, and the absolute error between them for the 3D Helmholtz equation.  The visualization is a 3D representation showing the solution across the x, y, and z dimensions.  It provides a visual representation of the model's accuracy in approximating the solution of this complex PDE.", "section": "5.2 Failure mode of PINNs and Complex PDEs"}]