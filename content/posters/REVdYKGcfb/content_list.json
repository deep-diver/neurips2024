[{"type": "text", "text": "What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Libo ${\\bf{Q}}{\\bf{i n}}^{\\ddag}{}^{*}$ Qiguang Chen\u2020\u2217 Hao Fei\u2662 Zhi Chen\u2663 Min Li\u2021 Wanxiang Che\u2020 \u2021 School of Computer Science and Engineering, Central South University \u2020 Research Center for Social Computing and Information Retrieval \u2020 Harbin Institute of Technology \u2662Tsinghua University \u2663ByteDance lbqin@csu.edu.cn, qgchen@ir.hit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: \u201cWhat factors affect the performance of MM-ICL?\u201d To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Large Language Models (LLMs) have demonstrated remarkable advancements, showcasing proficiency in a wide range of tasks [Zhao et al., 2023a, Qin et al., 2023, 2024, Hu et al., 2023, Pan et al., 2023]. Notably, advanced LLMs exhibit the emergence of novel capabilities such as In-Context Learning (ICL) [Wei et al., 2022a, Dong et al., 2022, Zhuang et al., 2023], which optimize task performance by incorporating demonstrations into input prompts [Giannou et al., 2023, Li et al., 2023d, Wies et al., 2023, Zhou et al., 2022]. In particular, multi-modal in-context-learning (MM-ICL) is capable of utilizing multi-modal demonstrations to quickly adapt to the downstream task without parameter tuning [Yin et al., 2023, He et al., 2023, Zhang et al., 2024, Li and Lu, 2024]. ", "page_idx": 0}, {"type": "text", "text": "In the literature, a series of works emerge to enhance MM-ICL. Specifically, Gong et al. [2023] manually create a general template with multiple images and corresponding responses during instructiontuning (IT) stage to improve MM-ICL. Tsimpoukelli et al. [2021], Li et al. [2023b], Doveh et al. [2024] and Zhao et al. [2024] develop task-specific MM-ICL templates during the IT stage, further extending its capabilities across more domains. Li et al. [2023a] introduce OtterHD, adapting MMICL for high-definition image tasks. Furthermore, Sun et al. [2023] and Tian et al. [2024] explore the potential of MM-ICL in the image generation tasks. Jin et al. [2024] provide compelling evidence for the effectiveness of MM-ICL in comprehending game instructions. Zong et al. [2024] and Shukor et al. [2024] develop fine-grained benchmarks and evaluate the MM-ICL in classification tasks. ", "page_idx": 0}, {"type": "text", "text": "While significant progress has been witnessed in MM-ICL, the existing work still mainly focuses on how to optimize MM-ICL, ignoring the underlying factors that influence its effectiveness and performance. Such gap impedes a comprehensive understanding of the mechanisms and performance determinants of MM-ICL, thereby limiting further exploration and research in this field. Motivated by this, this paper aims to systematically investigate the research question: What factors affect the performance of MM-ICL?, hoping to offer a unified view and guideline for researchers to build better MM-ICL. Specifically, as illustrated in Figure 1, the MM-ICL process comprises three steps: demonstration retrieval, demonstration ordering, and prompt construction. Therefore, We systematically investigate the following sub-questions: (a) how to select multi-modal demonstrations (Sec. 3.1); (b) how to order multi-modal demonstrations (Sec. 3.2); and (c) how to construct MM-ICL prompts (Sec. 3.3) to this end. To achieve this, we conduct detailed experiments on MM-ICL using 20 strategies across 4 tasks with 6 representative vision large language models (VLLMs). ", "page_idx": 0}, {"type": "image", "img_path": "REVdYKGcfb/tmp/4bad86ba6c207393a000d26dcdebbc2da402e4c172caa3c421ebb734170b7e4f.jpg", "img_caption": ["Figure 1: The whole process of prompting creation for multi-modal in-context-learning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Through extensive investigations, the main findings are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Multi-modal alignment is the bottleneck for MM-ICL. Our analysis confirms that, on average, multi-modal retrieval methods outperform single-modal ones. Furthermore, multimodal alignment in VLLMs has a greater impact on MM-ICL effectiveness than parameter size, identifying alignment as the key limitation in both backbone structure and demonstration quality. \u2022 Intra-demonstration ordering holds greater importance than inter-demonstration ordering. Our investigation first indicates that the intra-demonstration ordering, particularly the ordering of modalities, greatly influences model performance more than demonstration arrangement. \u2022 Introductory instruction guides better task understanding for MM-ICL. To construct a comprehensive MM-ICL prompt, it is essential to include introductory instructions preceding the demonstrations. This approach consistently enhances the performance of MM-ICL campared with summative instruction placed after demonstrations, and intra-demonstration instruction. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we formally present the prompt building process for MM-ICL. As depicted in Figure 1, the process of prompt building for MM-ICL involves three sequential stages: ", "page_idx": 1}, {"type": "text", "text": "(1) Demonstration Retrieval: The core MM-ICL requires retrieval to obtain demonstrations that can help MM-ICL. Formally, given a validation dataset $\\mathcal{V}_{n}=\\{x_{1},x_{2},\\ldots,x_{n}\\}$ , each multi-modal sample $x_{i}$ includes textual input $I_{i}^{t x t}$ , visual input $I_{i}^{v i s}$ , and output $O_{i}$ . For a specific test query $q$ , this step aims to identify a subset of relevant demonstrations $\\mathcal{C}_{k}=\\{x_{\\pi_{j}}\\}_{j=1}^{k}$ , where $x_{\\pi_{j}}\\in\\mathcal{V}_{n}$ . ", "page_idx": 1}, {"type": "text", "text": "(2) Demonstration Ordering: Researches [Lu et al., 2022b, Wu et al., 2023, Xiang et al., 2024] show that LLMs are highly sensitive to the order of demonstrations. Thus, arranging these demonstrations effectively is crucial for MM-ICL. After retrieving relevant demonstrations, we must rearrange the sequence $\\boldsymbol{\\mathcal{L}}_{k}=[x_{\\sigma_{j}}]_{j=1}^{k}$ , which will be used to construct the prompt. ", "page_idx": 1}, {"type": "text", "text": "(3) Prompt Construction: Previous research indicates that using delimiters and instructions can significantly enhance textual ICL capabilities [Min et al., 2022, Qin et al., 2023]. Therefore, the final core step is to transform the ordered demonstrations into a structured prompt $\\mathcal{P}$ , incorporating delimiters and instructions to optimize MM-ICL. ", "page_idx": 1}, {"type": "text", "text": "3 What Factors Affect Multi-modal In-Context Learning? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3.1 Exploration of MM-ICL Demonstration Retrieval ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The efficacy of ICL heavily depends on the quality of the retrieved demonstrations $\\mathcal{C}$ , which provide essential prior knowledge for MM-ICL. As illustrated in Figure 2, the retrieval process encompasses ", "page_idx": 1}, {"type": "image", "img_path": "REVdYKGcfb/tmp/d2fb03beea35a9d612d74ac68729d5bb53f80153337a5a088535a8049b21ee88.jpg", "img_caption": ["Figure 2: The demonstration retrieval process for MM-ICL. ", "(c) Sample Selection "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "three key steps: (1) Sample Representation, (2) Sample Comparison, and (3) Sample Selection. In this section, we conduct a systematic analysis of how various strategies for sample representation, comparison, and selection affect MM-ICL task performance. ", "page_idx": 2}, {"type": "text", "text": "Sample Representation. It involves defining an encoder (Encoder $(\\cdot)$ ) to map each input sample $x_{j}\\in\\mathcal{V}$ and user query $q$ into a shared representation space: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh_{j}=\\operatorname{Encoder}(x_{j}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Specifically, we evaluate various encoder architectures across modalities, focusing on the impact of visual encoder $(\\mathtt{E n c o d e r}_{v i s})$ , text encoder (Encoder ${\\bf\\nabla}_{t x t}$ ), and multi-modal encoder (Encoder $\\mathbf{\\phi}_{m u l t i})$ ) on model performance. ", "page_idx": 2}, {"type": "text", "text": "Sample Comparison. After deriving the representations, we employ a metric $\\mathcal{M}$ to evaluate the quality $\\mathcal{Q}_{j}$ of the sample $h_{j}$ in comparison to the query representation $h_{q}$ and the dataset samples $h_{j}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{j}=\\mathcal{M}(h_{q},h_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Specifically, we explore various comparison metrics, including cosine similarity $\\mathcal{M}_{c o s}$ [Liu et al., 2022a], L2 similarity $\\mathcal{M}_{L2}$ [Liu et al., 2022a], and semantic diversity $\\mathcal{M}_{d i v}$ [Li and Qiu, 2023a], to assess sample quality and understand the correlation with model performance. ", "page_idx": 2}, {"type": "text", "text": "Sample Selection. After quality assessments, we apply a selection criterion $\\boldsymbol{S}$ to identify the $k$ most advantageous samples $x_{\\pi_{j}}$ for inclusion in the demonstration set $\\mathcal{C}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\{x_{\\pi_{j}}|x_{\\pi_{j}}\\in S(q,\\mathcal{Q}_{j}),j\\leq k\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Sample selection is guided by factors such as domain information [He et al., 2023], demonstration style [Agrawal et al., 2023], and token distance [Liu et al., 2022a]. Specifically, we systematically examine samples from both in-domain and out-of-domain collections. And we also assess the impact of image style on the selected demonstrations. Further, we investigate the token distance between modalities to understand its effects on sample selection for MM-ICL. ", "page_idx": 2}, {"type": "text", "text": "3.2 Exploration of MM-ICL Demonstration Ordering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following Lu et al. [2022b] and Wu et al. [2023], the order of the demonstration set $\\mathcal{C}$ significantly impacts MM-ICL performance. As shown in Figure 3, this section explores two key aspects: ", "page_idx": 2}, {"type": "text", "text": "Intra-demonstration Ordering. The sequence within a demonstration, especially modalities (e.g., text and image), is an important component that might affect the MM-ICL capabilities. Therefore, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{\\left(\\underbrace{\\overbrace{\\overbrace{\\Theta}^{4}}}_{\\begin{{\\scriptsize{\\overrightarrow{l o p}}}},\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "", "table_caption": ["Figure 3: The demonstration ordering process for MM-ICL. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "we introduce a intra-demonstration ordering permutation (IOP) to define this sequence: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=[\\mathtt{I O P}(x_{\\pi_{1}}),\\mathtt{I O P}(x_{\\pi_{2}}),\\ldots,\\mathtt{I O P}(x_{\\pi_{k}})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We conduct a systematic exploration of various IOP configurations, including text-image-text $(\\mathtt{I O P}^{\\mathtt{t v t}})$ , text-text-image $(\\mathtt{I O P}^{\\mathtt{t t v}})$ , and image-text-text $(\\mathtt{I O P}^{\\mathtt{v t t}})$ . These order analyses aim to evaluate the impact of different modal sequences on the model\u2019s performance. ", "page_idx": 3}, {"type": "text", "text": "Inter-demonstration Ordering. The sequence in which demonstrations are organized within $\\mathcal{C}$ also is the key component that might impact the performance of MM-ICL. Formally, we define a sample ordering permutation $\\sigma_{j}$ to specify the arrangement: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=[x_{\\sigma_{1}},x_{\\sigma_{2}},\\ldots,x_{\\sigma_{k}}|x_{\\sigma_{j}}\\in\\mathcal{C}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{\\sigma_{j}}$ represents the $j$ -th demonstration in the ordered demonstration list. ", "page_idx": 3}, {"type": "text", "text": "3.3 Exploration of MM-ICL Prompt Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "VLLMs are highly sensitive to input instructions [Kojima et al., 2022, Qin et al., 2023]. Inspired by this, to enhance task comprehension, we incorporate different instructions to explore the performance influence for MM-ICL. Formally, we construct instruction methods $\\mathbb{Z}(\\cdot)$ that describe the task and position them within the prompt. The prompt construction process is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}=\\mathcal{Z}(\\delta(x_{\\sigma_{1}}),\\delta(x_{\\sigma_{2}}),\\dots,\\delta(x_{\\sigma_{k}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Specifically, as shown in Figure 4, we explore three instruction categories to bolster MM-ICL process: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Introductory Instruction $\\tau_{i n t r o})$ refers to the initial guidance that offers an overview of the task prior to any demonstrations. As shown in Figure 4 (a), this instruction, denoted as $\\mathcal{T}_{i n t r o}$ , is positioned at the start of the ordered demonstration sequence, $\\mathcal{L}$ . ", "page_idx": 3}, {"type": "image", "img_path": "REVdYKGcfb/tmp/38d1ff11f98a9c5f32c6de905e43a3f18f788ff47f6c16f8770671371b51177d.jpg", "img_caption": ["Figure 4: The process of instruction injection for MM-ICL prompt construction involves three key elements. The Introductory Instruction provides an overview instruction of the task before demonstrations. The Summative Instruction summarizes after the examples, guiding the model to apply the learned concepts to practical problems. The Intra-demonstration Instruction embeds task-specific guidance within each demonstration, enabling VLLMs to grasp task requirements during learning. Further details and additional prompts are provided in Appendix C.3. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "\u2022 Summative Instruction $\\left(\\mathcal{T}_{s u m}\\right)$ offers a summary after the examples, guiding the model to apply the learned concepts to real-world problems. As shown in Figure 4 (b), this instruction $\\mathcal{T}$ is added at the end of the demonstration list $\\mathcal{L}$ . \u2022 Intra-demonstration Instruction $\\left(\\mathcal{T}_{i n t r a}\\right)$ embeds task instructions within each demonstration, helping VLLMs understand the task requirements during the learning process. As shown in Figure 4 (c), this instruction $\\mathcal{T}$ is included within each demonstration $x_{i}$ in the list $\\mathcal{L}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the setting of Li et al. [2023c], we systematically explore 4 tasks, including image-caption, visual question answering (VQA), image classification, and chain-of-thought reasoning, which come from $\\bar{\\bf M}^{3}\\mathrm{IT}$ [Li et al., 2023c] and $\\mathbf{M}^{3}\\bar{\\mathbf{C}}\\mathbf{o}\\mathbf{T}$ [Chen et al., 2024b] (as shown in Tables 2), providing a universal paradigm can help researchers conduct unified and fairer comparisons and studies within a unified framework. In order to evaluate the MM-ICL performance accurately, we use two indicators for each task. Following Zhang et al. [2019], Li et al. [2023b], and Zong et al. [2024], we use CIDER [Vedantam et al., 2015] and BertScore [Zhang et al., 2019] as image-caption metrics. Since $\\mathbf{M}^{3}\\mathbf{IT}$ includes various VQA tasks with free-form answers, inspired by the success of free-form and precise answer hybrid evaluation in machine reading comprehension, following Rajpurkar et al. [2016], Zhang et al. [2019], we adapt Token-F1 [Rajpurkar et al., 2016] and BertScore as visual question answering (VQA) metrics (The correlation analysis of the indicators and accuracy as shown in Table 3). Following Li et al. [2023c,b], we use accuracy and F1 score as indicators of image classification. Following Lu et al. [2022a], Golovneva et al. [2022] and Qin et al. [2023], we use accuracy and reasoning alignment score [Golovneva et al., 2022] (RAS) as indicators of reasoning. ", "page_idx": 4}, {"type": "text", "text": "To ensure rigorous experimental control, we established a baseline using a multi-modal encoder for data representation and cosine similarity for sample comparison, limiting retrieval to within the same task. This baseline ranks samples based on similarity, with a delimiter and a 3-shot setting (see Appendix A for details). In addition, all open source models complete inference on $2\\,\\mathrm{Al00\\,80G}$ . For all experiments, we select top-p from $\\{0.95,1\\}$ and adjust the temperature parameter within [0, 1]. Among them, temperature is the main error variable in this work. ", "page_idx": 4}, {"type": "text", "text": "5 Empirical Analysis of Factors Affecting MM-ICL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 Empirical Analysis of MM-ICL Demonstration Retrieval ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1.1 Sample Representation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Multi-modal alignment is the bottleneck for MM-ICL in both backbones and demonstrations. To evaluate the impact of semantic representation in different modalities for MM-ICL, we assessed three distinct encoders: RoBERTa [Liu et al., 2019] as a textual encoder for Textual Retriever, CLIP-Vision Encoder [Radford et al., 2021] for Visual Retriever, and BridgeTower [Xu et al., 2023] as multi-modal encoder for Multi-Modal Retriever. As illustrated in Table 1, multi-modal retrieval consistently outperforms zero-shot, randomly selected, and single-modality methods, highlighting the advantages of multi-modal semantic learning for MM-ICL. What\u2019s more, as shown in Table 1, our results reveal that increasing model parameters from 8 billion to over 100 billion does not significantly enhance performance, suggesting that beyond parameter size, multi-modal context understanding and alignment are more crucial for MM-ICL than model scaling. Our analysis demonstrates that multi-modal alignment is the critical factor in both the backbone and demonstrations. ", "page_idx": 4}, {"type": "text", "text": "Current multi-modal encoders still lack modeling of multi-modal logic. Actually, multi-modal retrieval attains better performance in many scenarios like Image Caption and VQA. However, our experiments show that textual retrieval works well for classification and reasoning tasks. Based on the qualitative analysis, we observe that due to the semantic richness of the labels and rationales, textual retrieval can obtain more similar samples. However, the current multi-modal retrieval struggles with complex text semantics, often favoring image similarity. This aligns with recent work [Tong et al., 2023, 2024, Fei et al., 2024c], which is valuable for future exploration. ", "page_idx": 4}, {"type": "text", "text": "Multi-modal context diminishes the necessity of careful demonstration selection. As shown in Table 1, adding relevant demonstrations slightly improves performance, but the gains are less significant compared to text-only ICL scenarios. Specifically, retrieved demonstrations yield an average performance boost of $3.84\\%$ , compared to random demonstrations. In contrast, text-only scenarios show performance increases of over $10\\%$ with carefully selected samples [Shi et al., 2023]. Furthermore, the model remains unaffected by irrelevant samples, and the performance of almost all models is higher than zero-shot. This indicates that multi-modal context significantly reduces the need for careful demonstration selection, unlike in text-only scenarios. ", "page_idx": 4}, {"type": "table", "img_path": "REVdYKGcfb/tmp/41a91218104877a9157756493c9e536af529618e0485004404e577a7fb833efe.jpg", "table_caption": [], "table_footnote": ["Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set. "], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "VLLMs learn semantic representations instead of token pattern representations for MM-ICL. As depicted by Agrawal et al. [2023], textual ICL primarily learns token patterns (e.g., similar output formats, reasoning paths) among demonstration outputs. To investigate whether VLLMs rely on repetitive token patterns, we utilize the average BLEU score across demonstration outputs as a representation of token repetition. Figure 5 shows that only the image captioning task exhibits a positive correlation. In contrast, other tasks show a decline as BLEU scores exceed $30\\%$ . This underscores that MM-ICL primarily learns semantic rather than token pattern representations for effective performance. ", "page_idx": 5}, {"type": "text", "text": "5.1.2 Sample Comparison ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To further analyze the influencing factors of MM-ICL in sample retrieval, this study employs similarity and diversity metrics, which help assess how MM-ICL processes sample similarities and differences, enhancing our understanding of its mechanisms. See Appendix B for more details and results. ", "page_idx": 5}, {"type": "image", "img_path": "REVdYKGcfb/tmp/3359d5f6c49e726369a90f08309cb6b96428cd24a157b48708ac1fc351cf824c.jpg", "img_caption": ["Figure 5: The impact of token pattern representation in Gemini-Pro. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "REVdYKGcfb/tmp/f62df38b2ba38c0042af5d67e784d4bdd64b947c408db2342ea596edc2cf1541.jpg", "img_caption": ["Figure 6: The impact of different sample comparison methodologies in Gemini-Pro. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Cosine similarity matters for sample comparison. Following Liu et al. [2022b], we compare two representative similarity metrics, cosine similarity and L2 similarity. As shown in Figure 6 (a), cosine similarity, which measures the directional semantic alignment, emerges as the superior metric in MM-ICL than L2 similarity. Supported by Deza et al. [2009] and Steck et al. [2024], it indicates that MM-ICL prioritizes semantic directional consistency over complete semantic alignment. ", "page_idx": 6}, {"type": "text", "text": "Diversity does not show significant influence for sample comparison. He et al. [2023], Li and Qiu [2023b] have shown that demonstrations with better diversity can effectively improve textual ICL. To explore whether it exists in MM-ICL, following Li and Qiu [2023b], we ultilize the \u201cdiversity retriever\u201d, which selects the top-10 samples and further chooses the best 3 samples based on semantic diversity to obtain a more diverse MM-ICL. As demonstrated in Figure 6 (b), although diversity significantly enhances performance in text-based ICL, our experiments show limited improvement in MM-ICL tasks. This suggests that diversity may not directly correlate with better MM-ICL. ", "page_idx": 6}, {"type": "text", "text": "5.1.3 Sample Selection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Domain interval matters for sample selection. Prior research highlights the critical role of domain relevance in enhancing ICL performance. Inspired by this, we employ the multi-modal retriever to select samples from both in-domain and out-of-domain pools. Figure 7 (a) shows a nearly $4\\%$ performance drop when out-of-domain demonstrations are included, underscoring the necessity of in-domain demonstrations for optimal MM-ICL. ", "page_idx": 6}, {"type": "text", "text": "Visual style is not a crucial factor in sample selection. Although stylistic similarity in text samples is known to bolster ICL, its effect on the visual modality remains ambiguous. Utilizing CLIP for image classification, we investigate the impact of stylistic coherence in multi-modal samples on MM-ICL performance. As depicted in Figure 7 (b), significant enhancements are observed solely in the VQA task, while captioning and classification show minimal effects and reasoning tasks decline. This indicates that diverse visual styles are not crucial in general MM-ICL. ", "page_idx": 6}, {"type": "text", "text": "Token distances between modalities need to be considered for different tasks to improve sample selection. For textual ICL, excessive token distance between samples can impede performance [Liu et al., 2022a]. We extend this inquiry to MM-ICL, analyzing how token distance across modalities influences results. Specifically, during the sample selection process, we considered the impact of the average token distance between two images on the model within the entire prompt of MM-ICL. As illustrated in Figure 7 (c), the effect of token distance varies by task, typically showing an initial performance increase followed by a decline as distance grows, particularly in non-captioning tasks. This highlights the task-dependent nature of optimal token distance in MM-ICL. ", "page_idx": 6}, {"type": "image", "img_path": "REVdYKGcfb/tmp/d36101307bcefde08a1453aa3535f6f0c12286542b6f3dcc0e9a1670bce47f37.jpg", "img_caption": ["Figure 7: The impact of sample selection on average score performance in Gemini-Pro. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Empirical Analysis of MM-ICL Demonstration Ordering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Intra-demonstration ordering significantly impacts performance. Within the demonstration, organizing the ordering, especially the relationship between modalities is a crucial topic. We investigate this by arranging inputs and outputs across modalities using three methods: text input $\\rightarrow$ text output $\\rightarrow$ image input (Text-Image), text input $\\rightarrow$ image input $\\rightarrow$ text output (Text-Image-Text), and image input $\\rightarrow$ text input $\\rightarrow$ text output (Image-Text). As shown in Figure 8 (a), positioning the image at the start significantly enhances model performance. This suggests that presenting visual information first improves multi-modal comprehension, thereby boosting its learning abilities. ", "page_idx": 7}, {"type": "text", "text": "Inter-demonstration ordering demonstrates minimal impacts. Following Lu et al. [2022c], we investigate how the order of demonstration presentation influences model efficacy. We explore various strategies: random rearrangement, a \"similar-last\" approach where samples similar to the query are shown last, and a \"similar-first\" approach where similar samples are presented first. Figure 8 (b) illustrates that inter-demonstration ordering has a negligible impact on MM-ICL performance. This suggests the order-robustness, with the presentation sequence having minimal effect. ", "page_idx": 7}, {"type": "image", "img_path": "REVdYKGcfb/tmp/5a279b27df0de07b97bae165175e9da023eb403788da8fbd48bf7031e31329aa.jpg", "img_caption": ["Figure 8: The impact of demonstration ordering on performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Empirical Analysis of MM-ICL Prompt Construction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Introductory Instruction is consistently effective for better MM-ICL. To investigate the impact of inserting task-related instructions within prompts, we conduct the following experiment on three categories of instruction: Introductory Instruction, Summative Instruction, and Intra-demonstration Instruction. As depicted in Figure 9, our analysis indicates that introductory instructions stably enhance model performance. In contrast, other instructions generally decrease performance. This finding suggests that introductory instructions facilitate targeted contextual learning and more effective semantic comprehension in demonstrations. We show more prompts and details in Appendix C.3. ", "page_idx": 7}, {"type": "image", "img_path": "REVdYKGcfb/tmp/e081ac83c85126536ed1146a8ca383ac447629dc8ed120a13234d87737938fe6.jpg", "img_caption": ["Figure 9: The impact of injecting instruction into demonstrations on model average score performance. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "REVdYKGcfb/tmp/1b6138170d20a36d1ed3f664b212473f05c320497459c81c4823e08d5d79f3ea.jpg", "img_caption": ["(a) The impact of the number of demon- (b) The impact of the number of demonstrations on average score performance. strations on different task performance. ", "Figure 10: The impact of the number of demonstrations on performance. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "REVdYKGcfb/tmp/553e6a4746e99514549dea8ebb5d8b03ada2b9abcf980607d5ede3f964808219.jpg", "img_caption": ["Figure 11: The impact of inserting delimiter into the input and output of demonstration on model performance. See Figure 16 for more results. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "MM-ICL is affected by the number of demonstrations depending on the task. Contrary to traditional text-based ICL, where performance improves with more samples, our findings in Figure 10 (a) suggest that MM-ICL does not experience significant gains from more demonstrations. To further understand the reason behind, we analysis the performance on different tasks. As shown in Figure 10 (b), increasing the number of demonstrations enhances performance in caption and VQA tasks, a trend also reported in prior studies [Alayrac et al., 2022, Lauren\u00e7on et al., 2024a, Shukor et al., 2024]. However, performance declines when demonstrations exceed three across all tested VLLMs. In more complex reasoning tasks, such as multi-step multi-modal chain-of-thought reasoning, additional demonstrations do not yield effective improvements, aligning with the findings of Chen et al. [2024b], and Fei et al. [2024a]. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we attribute it to the following reasons for this limitation: (1) Cognitive Overload: For complex tasks, understanding numerous demonstrations can overwhelm the model, impeding its ability to process and integrate information effectively [Chen et al., 2024a]. (1) Complexity of Reasoning Tasks: In reasoning tasks, the performance improvement from more demonstrations is often less pronounced than when using diverse retrievers. This suggests that reasoning tasks require sophisticated integration of information, where quality outweighs quantity. See Appendix C.1 for more detailed description. ", "page_idx": 8}, {"type": "text", "text": "The importance of delimiter lessens by text-image interleaved demonstrations. Previous research suggests that specific delimiters for input and output data can demonstrably influence textual ICL capabilities [Min et al., 2022]. Therefore, we utilize ablation experiments to omit these delimiters to examine their necessity (see Appendix C.2 for details). As shown in Figure 11, the resulting minor performance decline suggests that while these delimiters are less critical in MM-ICL, the modality switch inherent to MM-ICL may serve as an implicit delimiter, compensating for the absence of explicit delimiters. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Recent advancements in vision large language models (VLLMs) have achieved great success in various vision-language tasks [Yin et al., 2023, Wu et al., 2024a,b, Wang et al., 2024, Fei et al., 2024b]. Initially, VLLMs lack Multi-modal In-context Learning (MM-ICL) capabilities. To address this, researchers explore incorporating MM-ICL directly into the training phase. This involves constructing training samples with multi-modal interleaved data by manual and general templates, which unlock the MM-ICL capability [Alayrac et al., 2022, Awadalla et al., 2023]. Building on this, Li et al. [2023b], Doveh et al. [2024] and Zhao et al. [2024] extend the MM-ICL to construct a series of task-specific templates, which improves generalization for MM-ICL. Further, Li et al. [2023a] introduce OtterHD and adapt the former process for high-definition images. The potential of MM-ICL is further explored in scene text recognition, image generation, and game instructions [Zhao et al., 2023b, Sun et al., 2023, Jin et al., 2024]. ", "page_idx": 8}, {"type": "text", "text": "Recognizing the effectiveness of MM-ICL, researches shift towards prompt optimization. These methods focus on directly optimizing multi-modal prompts to understand the task and generate the expected output, without parameter adjustments [Gong et al., 2023, Tsimpoukelli et al., 2021, Li et al., 2023b]. This approach has significantly improved performance in visual reasoning tasks [Yang et al., 2022, Zheng et al., 2023]. Another approach involves textualizing visual information to enable VLLMs to leverage their background knowledge through in-context learning, further enhancing visual reasoning [Yang et al., 2023, Lu et al., 2024, Gupta and Kembhavi, 2023, Shen et al., 2024]. In addition, in order to better explore the MM-ICL, Zong et al. [2024] and Shukor et al. [2024] also provide a dataset to test the MM-ICL capabilities of the multi-modal classification. Furthermore, Shukor et al. [2024] take the first step to conduct an instruction modification exploration for MM-ICL. ", "page_idx": 9}, {"type": "text", "text": "Meanwhile, Baldassini et al. [2024], Chen et al. [2024c] pioneer the first naive multi-modal retrieval exploration to enhance MM-ICL. Different from the existing work, our study mainly focuses on a systematic exploration of the effectiveness of key factors influencing the effectiveness of MM-ICL in a unified perspective. To this end, we conduct a detailed analysis and exploration on 6 VLLMs and 20 factors across 4 tasks, aiming to provide systematic and practical guidance for future research. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Broader Impacts. Our work is the first to systematically explore the factors influencing MM-ICL. We aim to enhance the understanding of MM-ICL mechanisms and guide future developments in this field. Additionally, our findings could foster a more comprehensive comprehension of MM-ICL within the community. For social impact, this research may influence the creation of more effective multi-modal large language models and relevant applications. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Work. Due to time and cost constraints, this work is limited to the exploration of image and text modalities. In future research, we can extend our exploration to video modal ICL and multi-lingual MM-ICL scenarios. Another limitation of this work involves the insufficient consideration of certain image instructions, such as grounding or the inclusion of additional arrows. These aspects often require more complex human input and are not adequately supported by most current models. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study is the first to systematically explore MM-ICL by identifying key performance determinants. Our experiments with 6 models and 20 factors across 4 tasks show that multi-modal retrieval significantly outperforms single-modal approaches and the intra-demonstration ordering critically influences learning efficacy. Additionally, incorporating task-specific instructions into prompts enhances model performance. We hope these findings will refine our understanding of MM-ICL mechanisms and guide more effective developments and future research in this evolving field. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (NSFC) via grant 62306342, 62236004, 62441603 and 62476073. This work was also sponsored by the Excellent Young Scientists Fund in Hunan Province (2024JJ4070) and the Science and Technology Innovation Program of Hunan Province under Grant 2024RC3024. We are grateful for resources from the High Performance Computing Center of Central South University, and the CCF-Zhipu.AI Large Model Innovation Fund. Libo Qin is the corresponding author. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8857\u20138873, 2023. ", "page_idx": 9}, {"type": "text", "text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022. ", "page_idx": 9}, {"type": "text", "text": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.   \nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.   \nFolco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, and Benjamin Piwowarski. What makes multimodal in-context learning work? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1539\u20131550, 2024.   \nAli Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4291\u20134301, 2019.   \nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the boundaries of thought: A reasoning granularity framework to quantify and optimize chain-of-thought. arXiv preprint arXiv:2410.05695, 2024a.   \nQiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. $\\mathbf{M}^{3}\\mathbf{CoT}$ : A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8199\u20138221, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.446. URL https://aclanthology.org/2024.acl-long.446.   \nShuo Chen, Zhen Han, Bailan He, Mark Buckley, Philip Torr, Volker Tresp, and Jindong Gu. Understanding and improving in-context learning on vision-language models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024c. URL https://openreview.net/forum?id $\\equiv$ SB2sWF3oCw.   \nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   \nElena Deza, Michel Marie Deza, Michel Marie Deza, and Elena Deza. Encyclopedia of distances. Springer, 2009.   \nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \nSivan Doveh, Shaked Perek, M Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky. Towards multimodal in-context learning for vision & language models. arXiv preprint arXiv:2403.12736, 2024.   \nZhengfang Duanmu, Wentao Liu, Zhongling Wang, and Zhou Wang. Quantifying visual image quality: A bayesian view. Annual Review of Vision Science, 7(1):437\u2013464, 2021.   \nHao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceedings of the International Conference on Machine Learning, 2024a.   \nHao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing. 2024b.   \nHao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhancing video-language representations with structural spatio-temporal alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024c.   \nAngeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In Proc. of ICML, 2023.   \nOlga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.   \nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.   \nGoogle. Gemini: A family of highly capable multimodal models, 2023. URL https://storage. googleapis.com/deepmind-media/gemini/gemini_1_report.pdf.   \nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.   \nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.   \nJiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and Heng Tao Shen. Icl-d3ie: In-context learning with diverse demonstrations updating for document information extraction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19485\u201319494, 2023.   \nMengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. In The Twelfth International Conference on Learning Representations, 2023.   \nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \nYonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W Huang, Wenhu Chen, Zhaofeng He, et al. Read to play (r2-play): Decision transformer with multimodal game instruction. arXiv preprint arXiv:2402.04154, 2024.   \nMaxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1244\u20131254, 2021.   \nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199\u201322213, 2022.   \nJonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for generating descriptive image paragraphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 317\u2013325, 2017.   \nHugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024a.   \nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024b.   \nBo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-modality model. arXiv preprint arXiv:2311.04219, 2023a.   \nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023b.   \nJian Li and Weiheng Lu. A survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024.   \nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M 3 it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023c.   \nXiaonan Li and Xipeng Qiu. MoT: Memory-of-thought enables ChatGPT to self-improve. In Proc. of EMNLP, 2023a.   \nXiaonan Li and Xipeng Qiu. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6219\u20136235, 2023b.   \nYingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023d.   \nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Eneko Agirre, Marianna Apidianaki, and Ivan Vuli\u00b4c, editors, Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10.   \nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, 2022b.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.   \nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.   \nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, 2022c.   \nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.   \nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200\u20132209, 2021.   \nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proc. of EMNLP, 2022.   \nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.   \nOpenAI:, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report, 2023.   \nWenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, and Libo Qin. A preliminary evaluation of chatgpt for zero-shot dialogue understanding. arXiv preprint arXiv:2304.04256, 2023.   \nLibo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2695\u20132709, 2023.   \nLibo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip S Yu. Large language models meet nlp: A survey. arXiv preprint arXiv:2405.12819, 2024.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: $100{,}000{+}$ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, 2016.   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146\u2013162. Springer, 2022.   \nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024.   \nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210\u201331227. PMLR, 2023.   \nMustafa Shukor, Alexandre Rame, Corentin Dancette, and Matthieu Cord. Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-contextlearning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ mMaQvkMzDi.   \nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.   \nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019.   \nHarald Steck, Chaitanya Ekanadham, and Nathan Kallus. Is cosine-similarity of embeddings really about similarity? arXiv preprint arXiv:2403.05440, 2024.   \nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023.   \nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024.   \nShengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\equiv$ T6iiOqsGOh.   \nShengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9568\u20139578, 2024.   \nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200\u2013212, 2021.   \nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.   \nAndreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016.   \nPeng Wang, Yongheng Zhang, Hao Fei, Qiguang Chen, Yukai Wang, Jiasheng Si, Wenpeng Lu, Min Li, and Libo Qin. S3 agent: Unlocking the power of vllm for zero-shot multi-modal sarcasm detection. ACM Transactions on Multimedia Computing, Communications and Applications, 2024.   \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Proc. of NeurIPS, 2022b.   \nNoam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning, 2023.   \nShengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards semantic equivalence of tokenization in multimodal llm. arXiv preprint arXiv:2406.05127, 2024a.   \nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning, 2024b.   \nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, 2023.   \nYanzheng Xiang, Hanqi Yan, Lin Gui, and Yulan He. Addressing order sensitivity of in-context demonstration examples in causal language models. arXiv preprint arXiv:2402.15637, 2024.   \nXiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, and Nan Duan. Bridgetower: Building bridges between encoders in vision-language representation learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10637\u201310647, 2023.   \nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081\u20133089, 2022.   \nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.   \nBarry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2733\u20132743, 2023.   \nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.   \nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.   \nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, 36, 2024.   \nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. MMICL: Empowering vision-language model with multimodal in-context learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ 5KojubHBr8.   \nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2023a.   \nZhen Zhao, Can Huang, Binghong Wu, Chunhui Lin, Hao Liu, Zhizhong Zhang, Xin Tan, Jingqun Tang, and Yuan Xie. Multi-modal in-context learning makes an ego-evolving scene text recognizer. arXiv preprint arXiv:2311.13120, 2023b.   \nGe Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-ofthought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:5168\u20135191, 2023.   \nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022.   \nZiyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Weinan Zhang, and Ting Liu. Through the lens of core competency: Survey on evaluation of large language models. In The 22nd Chinese National Conference on Computational Linguistics: Frontier Forum, page 88, 2023.   \nYongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning. arXiv preprint arXiv:2403.13164, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A The Implement Details for Standard Baseline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To ensure rigorous control of experimental variables, we establish a standard baseline for our study. This baseline utilizes a multi-modal encoder for data representation and cosine similarity for sample comparison, with retrieval restricted to the same task. The following sections provide detailed insights into the implementation of this baseline. ", "page_idx": 16}, {"type": "text", "text": "A.1 Demonstration Retrieval Implementation for Baseline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Multi-modal Encoder for Sample Representation We employ BridgeTower [Xu et al., 2023] as a multi-modal encoder to represent the data in a unified embedding space. This encoder integrates both visual and textual information, prioritizing single modality to capture the rich semantic content present in images. ", "page_idx": 16}, {"type": "text", "text": "Cosine Similarity for Sample Comparison To compare samples effectively, we use cosine similarity, a metric that measures the cosine of the angle between two non-zero vectors in a multidimensional space. This choice is motivated by its effectiveness in capturing the similarity between high-dimensional vectors, which are typical outputs of our multi-modal encoder. Specifically, we compute the cosine similarity between the query and each candidate sample, which is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{cosine}(h_{q},h_{i})=\\frac{h_{q}\\cdot h_{i}}{\\|h_{q}\\|\\|h_{i}\\|}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $h_{q}$ and $h_{i}$ are the embedding vectors of the query $q$ and candidate sample $x_{i}$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "In-domain and Top- $\\mathbf{\\deltaA}$ Retrieval for Sample Selection To ensure the relevance and accuracy of the retrieval process, for sample selection, we first confine retrieval to the same task and domain. This means that comparisons and rankings are conducted exclusively among samples within the same task and domain category, ensuring the contextual appropriateness of the retrieved results. ", "page_idx": 16}, {"type": "text", "text": "In addition, for sample selection, samples are ranked according to their cosine similarity scores. Higher similarity scores indicate a closer alignment with the query sample, enabling the efficient identification of the most relevant samples. This ranking process involves two main steps: (1) Sorting: Candidate samples are sorted in descending order based on their cosine similarity scores relative to ", "page_idx": 16}, {"type": "table", "img_path": "REVdYKGcfb/tmp/010d5d7b52ffa86593dc370e3bbf0af843fd9bfcf443c5208cc39b69a38f5215.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 2: Dataset in $\\mathbf{M}^{3}\\mathbf{IT}$ and $\\mathbf{M}^{3}\\mathbf{CoT},$ , where IC: Image Captioning, CLS: Classification, VQA: Visual Question Answering, R: Chain-of-Thought Reasoning (with NL rationale). Due to the cost, for each task, we evenly sampled 500 items according to the sub-dataset. ", "page_idx": 16}, {"type": "text", "text": "the query. (2) Selection: Subsequently, the top-k ranked samples are selected based on their relevance as determined by the similarity scores. ", "page_idx": 17}, {"type": "text", "text": "A.2 Demonstration Ordering Implementation for Baseline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By default, we utilize the methodology for ordering demonstrations within our baseline model. By default, we adopt a text-after-image (Text-Image) approach for intra-demonstration sorting. This means that, within a single demonstration, textual information is positioned after the corresponding image. This ordering is chosen based on preliminary findings suggesting that such a sequence aids in better contextual understanding and retention of the demonstrated information. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, for the ordering of inter-demonstration sequences, we employ a similarity-based method. This method ranks demonstrations according to their similarity to the query, with more similar demonstrations placed higher in the order. The similarity is determined using a metric that assesses the alignment of key features between the query and the demonstrations. This approach ensures that the most relevant and contextually aligned demonstrations are prioritized, potentially enhancing the model\u2019s performance and the user\u2019s comprehension. ", "page_idx": 17}, {"type": "text", "text": "A.3 Prompt Construction Implementation for Baseline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To ensure consistency and comparability in our baseline, we introduce both a delimiter and a 3-shot setting (following Wei et al. [2022b], Qin et al. [2023]). The delimiter serves to clearly demarcate different segments of the input data, preventing any potential confusion or overlap between distinct portions of the input. This clear separation is crucial for the model to accurately process and understand the structure of the data it receives. ", "page_idx": 17}, {"type": "text", "text": "The 3-shot setting, on the other hand, involves providing three examples for each task within the prompt. This approach is designed to stabilize the learning process by presenting the model with sufficient contextual information. By offering three examples, we strike a balance between providing enough context to guide the model\u2019s understanding and avoiding the cognitive overload that might occur with too many examples. This setting not only enhances the model\u2019s performance but also ensures a more robust and reliable learning process. ", "page_idx": 17}, {"type": "text", "text": "A.4 Baseline Prompt ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the context of using Vision-and-Language Large Models (VLLMs), it is essential to carefully structure the input prompts to ensure accurate processing. The prompt format typically used is illustrated below: ", "page_idx": 17}, {"type": "image", "img_path": "REVdYKGcfb/tmp/34775d22bb5b2b6a500a7f8be3ecd1a6ca95f555ae17cc6980831aaf7c9cd04a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "where any gray text following the percent sign $(\\%)$ is treated as a comment. These comments are not processed as part of the primary input but serve to provide additional context or instructions within the coding environment. This convention helps in maintaining the clarity and functionality of the given prompting. ", "page_idx": 18}, {"type": "text", "text": "In conclusion, the standard baseline established here integrates a multi-modal encoder, cosine similarity, and task-specific retrieval with a focus on visual modalities. It ranks samples based on similarity and employs a delimiter with a 3-shot setting to ensure robust and consistent performance across different tasks. ", "page_idx": 18}, {"type": "text", "text": "B The Implement Details for Sample Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Metric Calculation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Cosine Similarity $(\\mathcal{M}_{c o s})$ Compute the cosine similarity between $h_{q}$ and $h_{j}$ using the formula: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}_{c o s}(h_{q},h_{j})=\\frac{h_{q}\\cdot h_{j}}{\\|h_{q}\\|\\|h_{j}\\|}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "L2 Similarity $\\left(\\mathcal{M}_{L2}\\right)$ Calculate the L2 similarity by computing the negative Euclidean distance between $h_{q}$ and $h_{j}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathscr{M}_{L2}(h_{q},h_{j})=-\\|h_{q}-h_{j}\\|_{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since Euclidean distance measures dissimilarity, we use the negative value to represent similarity, where a higher value indicates greater similarity. ", "page_idx": 18}, {"type": "text", "text": "Semantic Diversity $(\\mathcal{M}_{d i v})$ Semantic diversity is assessed by evaluating the differences in the distributional properties of $h_{q}$ and $h_{j}$ . This assessment involves analyzing the variance in how these properties are distributed across different samples. To determine the presence of semantic diversity within Multi-Modal In-Context Learning (MM-ICL), we adopt the methodology proposed by Li and Qiu [2023b]. Specifically, we employ the \"diversity retriever,\" designed to enhance the diversity of the selected samples. The diversity retriever operates by first selecting the top 10 samples based on a preliminary measure of relevance. From these top 10 samples, it then identifies the 3 samples that exhibit the highest semantic diversity. This two-step process ensures that the final selection of samples for MM-ICL is not only relevant but also diverse in terms of their semantic content. ", "page_idx": 18}, {"type": "text", "text": "B.2 Comparison and Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Comparing the results obtained using different metrics $(\\mathcal{M}_{c o s},\\,\\mathcal{M}_{L2},\\,\\mathcal{M}_{d i v})$ provides a comprehensive understanding of their effectiveness and suitability for specific applications. It is essential to analyze the trade-offs associated with each metric and interpret the results to draw meaningful conclusions about sample quality and relevance. ", "page_idx": 18}, {"type": "text", "text": "As shown in Figure 12, cosine similarity, which measures directional semantic alignment, emerges as the superior metric in MM-ICL compared to L2 similarity. This observation is supported by the findings of Deza et al. [2009] and Steck et al. [2024], who highlight that MM-ICL prioritizes semantic directional consistency over complete semantic alignment. Cosine similarity\u2019s ability to capture the nuances of directional alignment allows for more precise interpretations of semantic relationships within the data, making it particularly effective for MM-ICL tasks. ", "page_idx": 18}, {"type": "text", "text": "In contrast, Figure 13 illustrates that while diversity, as measured by $\\mathcal{M}_{d i v}$ , enhances performance in text-based in-context learning, our experiments reveal limited improvement in MM-ICL tasks. This finding suggests that diversity may not directly correlate with better performance in MM-ICL. The limited impact of diversity on MM-ICL performance could be attributed to the specific nature of multi-modal data, where the interplay between different modalities requires a more nuanced approach than simply maximizing diversity. ", "page_idx": 18}, {"type": "text", "text": "Further analysis of these metrics reveals the inherent trade-offs between them. For instance, while cosine similarity offers advantages in maintaining semantic directional consistency, it may not capture the full extent of semantic similarity that L2 similarity can provide. On the other hand, L2 similarity, though comprehensive in measuring complete alignment, might lack the precision needed for tasks that rely heavily on directional semantic cues. Similarly, while diversity is beneficial in certain contexts, its role in MM-ICL needs to be reconsidered, potentially focusing on optimizing other aspects of sample quality. ", "page_idx": 18}, {"type": "image", "img_path": "REVdYKGcfb/tmp/1f6bc49726cac489d43c10e40989e6358b93dd98de393d3ad2410cbb8416751c.jpg", "img_caption": ["Figure 12: The impact of the different similarity metrics. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "REVdYKGcfb/tmp/437edba1f60406b712193fca7138c8977f3b7a2eb664c7dfb5a16116accaf788.jpg", "img_caption": ["Figure 13: The impact of the utilization on diversity metrics. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "In summary, the evaluation of $\\mathcal{M}_{c o s}$ , $\\mathcal{M}_{L2}$ , and $\\mathcal{M}_{d i v}$ underscores the importance of selecting appropriate metrics based on the specific requirements of the task. Understanding the trade-offs and context-specific effectiveness of these metrics is crucial for optimizing performance in multi-modal in-context learning applications. ", "page_idx": 19}, {"type": "text", "text": "C Exploration of MM-ICL Prompt Construction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 The Implement Details for Demonstration Sampling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To examine the effect of demonstration sample quantity on model performance, as shown in Figure 14, we select a subset of $k^{\\prime}$ demonstrations from the demonstration list $\\mathcal{L}_{k^{\\prime}}$ to the prompt, where $k^{\\prime}$ is the number of retrieved demonstrations. Formally, the prompt construction process is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{P}=\\mathcal{Z}(\\delta(x_{\\sigma_{1}^{j}}),\\delta(x_{\\sigma_{2}^{j}}),\\hdots,\\delta(x_{\\sigma_{k^{\\prime}}^{j}}))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We systematically evaluate the influence of varying $k^{\\prime}$ on MM-ICL performance. ", "page_idx": 20}, {"type": "image", "img_path": "REVdYKGcfb/tmp/78f66a11736c32e04c56abc720becf942cb14a99a45ccbf986c64dda65cf1c62.jpg", "img_caption": ["Figure 14: The demonstration sampling process for MM-ICL prompt construction. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 The Implement Details for Delimiter Injection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To distinctly separate inputs and outputs within demonstrations $x_{i}$ , as shown in Figure 15, we leverage special delimiter markers. Delimiters like [Request] and [Response] are strategically placed before the inputs and outputs, respectively. Formally, delimiter injection function $\\delta$ maps inputs and outputs to the prompting sequences: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta(x_{\\sigma_{i}})=[\\mathrm{Request}]\\ \\oplus I_{i}\\oplus[\\mathrm{Response}]\\ \\oplus O_{i},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $I_{i}$ and $O_{i}$ denotes the input and output for the sample $x_{i}$ , respectively. In addition, $\\oplus$ represents string concatenation operation. ", "page_idx": 20}, {"type": "image", "img_path": "REVdYKGcfb/tmp/2591b91f8995e9beaba7cce4b27daa6e05fb72596f51e773fec840b980c2ec87.jpg", "img_caption": ["Figure 15: The delimiter injection process for MM-ICL prompt construction. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.3 The Implement Details for Instruction Injection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Visual Language Models (VLLMs) are known to be highly sensitive to input instructions, as demonstrated by Kojima et al. [2022] and Qin et al. [2023]. Inspired by this observation, we aim to enhance task comprehension in Multi-Modal In-Context Learning (MM-ICL) by incorporating various instructions to explore their influence on performance. Formally, we develop instruction methods, denoted as $\\mathbb{Z}(\\cdot)$ , which describe the task and are integrated into the prompt construction process. The prompt $\\mathcal{P}$ is constructed as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}=\\mathcal{Z}(\\delta(x_{\\sigma_{1}}),\\delta(x_{\\sigma_{2}}),\\dots,\\delta(x_{\\sigma_{k}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\delta(x_{\\sigma_{i}})$ represents the transformation of the $i$ -th demonstration example. ", "page_idx": 20}, {"type": "image", "img_path": "REVdYKGcfb/tmp/8214bf269eb3ddf442d16e06dd4bb425132d29c996785224462fae7b42346d7a.jpg", "img_caption": ["Figure 16: The impact of inserting delimiter into the input and output of demonstration on model performance. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Specifically, we have designed distinct instructions tailored to different types of tasks, ensuring clarity and appropriateness for each unique context. For image captioning tasks, the prompt is: ", "page_idx": 21}, {"type": "text", "text": "Please provide a caption for the image following the structure of the provided example. ", "page_idx": 21}, {"type": "text", "text": "In this context, the objective is to generate descriptive captions that accurately reflect the content and context of the image. For Visual Question Answering (VQA) tasks, our prompt is: ", "page_idx": 21}, {"type": "text", "text": "Examine the image and answer the question by closely following the structure shown in the example provided. ", "page_idx": 21}, {"type": "text", "text": "The VQA tasks require the model to analyze visual content and respond to specific queries. By following the example, users can produce answers that are precise and directly related to the visual stimuli. For image classification tasks, the prompt is: ", "page_idx": 21}, {"type": "text", "text": "Carefully review the image and categorize it based on the options provided in [REQUEST], following the classification format illustrated in the example. ", "page_idx": 21}, {"type": "text", "text": "Image classification involves categorizing images into predefined classes based on visual content. The provided example demonstrates the expected classification format. For chain-of-thought reasoning tasks, the prompt is: ", "page_idx": 21}, {"type": "text", "text": "Carefully review the given image and the associated text. Utilize the reasoning format illustrated in the provided examples, breaking down your thought process. Ensure that each reasoning step is explicitly connected to observable details in the image or text, and articulate ", "page_idx": 21}, {"type": "text", "text": "your conclusion in a clear and logical manner. ", "page_idx": 21}, {"type": "text", "text": "Chain-of-thought reasoning tasks require a more complex interaction between visual and textual information. The prompt encourages users to break down their reasoning process into clear, logical steps, each supported by specific details from the image or text. ", "page_idx": 22}, {"type": "text", "text": "Furthermore, we explore three categories of instructions to enhance the MM-ICL process: ", "page_idx": 22}, {"type": "text", "text": "Introductory Instruction $\\tau_{i n t r o})$ This instruction provides an overview of the task before presenting any demonstrations. As depicted in Figure 4 (a), the introductory instruction $\\mathcal{T}_{i n t r o}$ is positioned at the beginning of the ordered demonstration list $\\mathcal{L}$ . This setup aims to set the context for the subsequent examples. Specifically, the overall prompt template is as follows: ", "page_idx": 22}, {"type": "image", "img_path": "REVdYKGcfb/tmp/d29951758e1028aa44b511cc52504c73e79c4707f5f53ea500bd0a715b9b080e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Summative Instruction $\\mathcal{(T}_{s u m})$ This instruction offers a summary after the examples, guiding the model to apply the learned concepts to real-world problems. As shown in Figure 4 (b), the summative instruction $\\mathcal{T}$ is added at the end of the demonstration list $\\mathcal{L}$ . This helps in reinforcing the learning objectives and expected outcomes. Specifically, the overall prompt template is as follows: ", "page_idx": 22}, {"type": "image", "img_path": "REVdYKGcfb/tmp/54934a4aa282550a856d7d70774a1a75b8fbc9b3552c0277a22b6aac73581319.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Intra-demonstration Instruction $\\left(\\mathcal{T}_{i n t r a}\\right)$ This instruction embeds task instructions within each example, assisting the model in understanding the task requirements during the learning process. As illustrated in Figure 4 (c), the intra-demonstration instruction $\\mathcal{T}$ is included within each demonstration $x_{i}$ in the list $\\mathcal{L}$ . This method ensures that the task instructions are continuously reinforced throughout the learning process. Specifically, the overall prompt template is as follows: ", "page_idx": 22}, {"type": "table", "img_path": "REVdYKGcfb/tmp/5e2be82a5ee6835293afbce7bf3ce9d4f39391583d08f818da7bba85726bc816.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 3: The correlation analysis of the indicators and reproducted accuracy. The results are obtained by testing on a subset of the test set. ", "page_idx": 23}, {"type": "image", "img_path": "REVdYKGcfb/tmp/87e79803b16768c7a777dbf433f51cef0b5341041cf8c50848868168422f5ae5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "By systematically incorporating these instruction categories into the MM-ICL framework, we aim to investigate their impact on model performance and task comprehension. ", "page_idx": 23}, {"type": "text", "text": "D Prompt Robust ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our preliminary experiments, we observed that variations in prompts do not significantly alter the overall conclusions. Specifically, we employed multiple prompts\u2014differing in instructions and delimiters\u2014while maintaining equivalent semantic content but varying linguistic expression. As demonstrated in Table 4, the influence of these different prompts on the results is minimal. This suggests that our findings are robust to changes in prompt formulation, thereby supporting the reliability of the experimental outcomes. ", "page_idx": 23}, {"type": "table", "img_path": "REVdYKGcfb/tmp/531496c2a514b03a66cdd1c544a7b32a5822951376dbe78c94ae7de93bda8468.jpg", "table_caption": ["Table 4: Performance across different prompts (i.e., P1, P2, P3 and P4). "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have present our main claims and outline the paper\u2019s contributions and scope in lines 6-13 of the Abstract and 44-54 of the Introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Section 7 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not involve any proofs or assumptions. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As shown in Section 3, Section 4 and Appendix, we have provided detailed descriptions and analyses of the experimental setups for all our investigations. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: The code for exploratory prompt work generally does not need to be released, and readers can easily use the prompts we report to directly reproduce the results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: As detailed in Section 4, we have thoroughly described the experimental setups for all our explorations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Error bars are shown in Figure 5 and Figure 7, with an explanation of the error variables provided in Section 4. However, we do not report error bars for all tasks due to the high costs associated with human annotation and computational resource consumption. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: As detailed in Section 4, we outline the specific model compute resources provided. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We are convinced that we comply with NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have addressed the broader impacts of our work in Section 7. Additionally, as our research is primarily an empirical exploration and poses no additional social risks, we have not included a discussion on potential harmfulness. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]