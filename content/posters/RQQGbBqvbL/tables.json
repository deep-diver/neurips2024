[{"figure_path": "RQQGbBqvbL/tables/tables_6_1.jpg", "caption": "Table 1: Main results with AUC as the evaluation metric. The best results are in bold.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by various methods on thirteen benchmark datasets, categorized into NLP, image, and tabular data.  The methods are compared under two noise conditions: class-dependent and instance-dependent.  The \"Single\" method serves as a baseline, representing training on a single, noisy label set. Other methods leverage multiple noisy label sets, utilizing techniques like majority voting, ensemble methods, and specialized noisy-label learning approaches. The table highlights the superiority of the proposed Collaborative Refining framework (CRL) across numerous datasets and noise types, demonstrating its effectiveness in improving model performance by refining data through a collaborative refinement approach.", "section": "4.3 Main Results"}, {"figure_path": "RQQGbBqvbL/tables/tables_7_1.jpg", "caption": "Table 2: Real-world results with AUC as the evaluation metric. The best results are in bold.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by different methods on two real-world datasets: Sentiment and CIFAR-10N.  The AUC is a common metric for evaluating the performance of binary classification models, where a higher AUC indicates better performance. The table compares the proposed Collaborative Refining for Learning from Inaccurate Labels (CRL) framework against several existing methods. The \"Single\" model serves as a baseline, representing a model trained on a single set of labels.  Other methods incorporate different strategies for handling multiple, inaccurate labels.  The best performing method for each dataset is highlighted in bold.", "section": "4.3 Main Results"}, {"figure_path": "RQQGbBqvbL/tables/tables_8_1.jpg", "caption": "Table 3: Ablation results with AUC as the evaluation metric.", "description": "This ablation study evaluates the impact of the two core modules (LRD and RUS) on the overall performance.  It compares the performance of the complete model against baselines using only one of the modules or using a naive version of RUS.  The results are presented as AUC scores across six different datasets, highlighting the contribution of each module in improving the model's accuracy.", "section": "4.4 Discussion"}, {"figure_path": "RQQGbBqvbL/tables/tables_9_1.jpg", "caption": "Table 4: Label qualities on D<sub>d</sub> and the training results. AUC is the evaluation metric.", "description": "This table presents the AUC scores resulting from using different label sources to train a model on the D<sub>d</sub> subset of the data.  It compares the AUC achieved using the labels from the best annotator, a simple majority voting approach, and the proposed LRD method at two different training stages (100 and 500 steps).  Higher AUC scores indicate better model performance. The results demonstrate the improved label quality and resulting performance gains from using the LRD approach compared to the baseline methods.", "section": "4.4 Discussion"}, {"figure_path": "RQQGbBqvbL/tables/tables_9_2.jpg", "caption": "Table 1: Main results with AUC as the evaluation metric. The best results are in bold.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by different methods on thirteen benchmark datasets.  Two types of label noise are considered: class-dependent noise and instance-dependent noise.  The table compares the performance of the proposed Collaborative Refining framework (CRL) against several other state-of-the-art methods for learning from inaccurate labels.  The best performing method for each dataset is highlighted in bold.", "section": "4.3 Main Results"}, {"figure_path": "RQQGbBqvbL/tables/tables_15_1.jpg", "caption": "Table 1: Main results with AUC as the evaluation metric. The best results are in bold.", "description": "This table presents the AUC (Area Under the Curve) scores achieved by various methods on 13 benchmark datasets, categorized by NLP, image, and tabular data.  The results are broken down by class-dependent and instance-dependent noise, illustrating the performance of different approaches in handling various types of noisy labels.  The \"Ours\" column shows the results of the proposed CRL framework, highlighting its superior performance compared to other methods on most datasets.  The best result for each dataset is bolded.", "section": "4.3 Main Results"}, {"figure_path": "RQQGbBqvbL/tables/tables_15_2.jpg", "caption": "Table 1: Main results with AUC as the evaluation metric. The best results are in bold.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by various methods on thirteen benchmark datasets, categorized into NLP, image, and tabular data.  Two types of noise are considered: class-dependent and instance-dependent. The table compares the performance of the proposed Collaborative Refining framework (CRL) against several state-of-the-art methods.  The best AUC score for each dataset and noise type is highlighted in bold, showcasing the superior performance of the CRL framework.", "section": "4.3 Main Results"}]