{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field, introducing the concept of LLMs as few-shot learners, a core idea behind the current work."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper establishes a key benchmark (MATH) and approach for evaluating LLMs on mathematical problem-solving, which is directly relevant to the current research."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-12-01", "reason": "This paper introduces the MATH dataset, a crucial benchmark for evaluating mathematical reasoning capabilities of LLMs, which is used in this paper's experiments."}, {"fullname_first_author": "Longhui Yu", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "publication_date": "2024-05-01", "reason": "This paper presents a state-of-the-art approach to instruction tuning for mathematical problem-solving, serving as a strong baseline and comparison for the proposed method."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "DeepSeekMath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-03", "reason": "This paper introduces a powerful open-weight model (DeepSeekMath) that is used as the core model for the dataset synthesis in this paper's proposed approach."}]}