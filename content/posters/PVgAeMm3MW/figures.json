[{"figure_path": "PVgAeMm3MW/figures/figures_0_1.jpg", "caption": "Figure 1: Example generation results from our single-step image-to-video model. Our model can generate high-quality and motion consistent videos by only performing the sampling once during inference. Please refer to our webpage for whole video sequences.", "description": "This figure displays four example video generation results produced by the single-step image-to-video model presented in the paper.  Each column shows a sequence of frames from a generated video, demonstrating the model's ability to create high-quality, consistent motion across different scenes (an astronaut in space, a woman holding a torch, a robotic horse, and a monorail). The caption highlights that the model achieves this with only one sampling step during inference, showcasing its efficiency compared to existing multi-step approaches. View the full videos on the project's webpage for a more comprehensive understanding.", "section": "Abstract"}, {"figure_path": "PVgAeMm3MW/figures/figures_3_1.jpg", "caption": "Figure 2: Training Pipeline. We initialize our generator and discriminator using the weights of a pre-trained image-to-video diffusion model. The discriminator utilizes the encoder part of the UNet as its backbone, which remains frozen during training. We add a spatial discriminator head and a temporal discriminator head after each downsampling block of the discriminator backbone and only update the parameters of these heads during training. Given a video latent x0, we first add noise \u03c3t through a forward diffusion process to obtain xt. The generator then predicts x\u03020 given xt. We calculate the reconstruction loss Lrecon between x\u03020 and x0. Additionally, we add noise level \u03c3t\u2032 to both x\u03020 and x0 to obtain real and fake samples, x\u0302t\u2032 and xt\u2032. The adversarial loss Ladv is then calculated using these real and fake sample pairs.", "description": "This figure illustrates the training pipeline of the proposed single-step video generation model.  The generator and discriminator are initialized with pre-trained weights from an image-to-video diffusion model.  The discriminator's backbone is frozen, while spatial and temporal discriminator heads are added and trained.  The training process involves adding noise to video latents, generating denoised latents using the generator, and calculating reconstruction and adversarial losses to refine the model.  This process is designed to enable the generation of high-quality videos with a single forward pass.", "section": "3 Method"}, {"figure_path": "PVgAeMm3MW/figures/figures_5_1.jpg", "caption": "Figure 3: Spatial & Temporal Discriminator Heads. Our discriminator heads take in intermediate features of the UNet encoder. Following existing arts [54, 53], we use image conditioning and frame index as the projected condition c. Left: For spatial discriminator heads, the input features are reshaped to merge the temporal axis and the batch axis, such that each frame is considered as an independent sample. Right: For temporal discriminator heads, we merge spatial dimensions to batch axis.", "description": "This figure illustrates the architecture of the spatial and temporal discriminator heads used in the model. The spatial head processes each frame independently by reshaping the input features to merge the temporal and batch axes.  The temporal head, conversely, merges spatial dimensions to the batch axis, enabling it to capture temporal correlations between frames. Both heads receive intermediate features from the UNet encoder and use image conditioning (c) and frame index as input.", "section": "3.3 Spatial Temporal Heads"}, {"figure_path": "PVgAeMm3MW/figures/figures_6_1.jpg", "caption": "Figure 4: Video Generation on Single Conditioning Images from Various Domains. We employ our method on various images generated by SDXL [60] to synthesized videos. The videos contain 14-frame at a resolution of 1024 x 576 with 7 FPS. The results demonstrate that our model can generate high-quality motion-consistent videos of various objects across different domains. Please refer to our webpage for whole video sequences.", "description": "This figure showcases the model's ability to generate high-quality, motion-consistent videos from a single conditioning image.  It presents several example video sequences generated from different images depicting various scenes and objects. Each video consists of 14 frames at 1024 x 576 resolution and a frame rate of 7 FPS. The diversity of scenes highlights the model's adaptability across different domains.", "section": "4.1 Qualitative Visualization"}, {"figure_path": "PVgAeMm3MW/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison between SVD [13], AnimateLCM [21], LADD [28], UFOGen [25], and Our Approach. We provide the synthesized videos (sampled frames) under various settings for different approaches. We use SVD to generate videos under 25, 16, and 8 sampling steps, AnimateLCM to synthesize videos under 4 sampling steps, LADD and UFOGen to generate videos under 1 sampling step. AnimateLCM, LADD and UFOGen generates blurry frames with few-steps and single-step sampling. Our approach can accelerate the sampling speed by 22.9\u00d7 compared with SVD while maintaining similar frame quality and motion consistency.", "description": "This figure compares the video generation results of different methods, including SVD, AnimateLCM, LADD, UFOGen, and the proposed method.  It shows that the proposed method achieves comparable quality to SVD with 25 steps, significantly outperforming other single-step methods and showing a significant speed increase.", "section": "4.2 Comparisons Results"}, {"figure_path": "PVgAeMm3MW/figures/figures_9_1.jpg", "caption": "Figure 6: PDF of \u03c3'.", "description": "This figure shows probability density functions (PDFs) of \u03c3\u2032 for different values of Pmean and Pstd.  The parameter \u03c3\u2032 represents the noise level added to the samples before being passed to the discriminator during training. The different curves illustrate how the distribution of \u03c3\u2032 changes depending on the chosen values of Pmean and Pstd, influencing the training stability and overall model performance.", "section": "3.2 Latent Adversarial Training for Video Diffusion Model"}, {"figure_path": "PVgAeMm3MW/figures/figures_9_2.jpg", "caption": "Figure 7: Analysis of \u03c3' Distributions. We investigate the impact of changing the distribution of \u03c3' by adjusting Pmean and Pstd. The results are shown with the same image conditioning. The first row and the second row display the first and last frames generated, respectively.", "description": "This figure shows the impact of different noise level distributions on the video generation quality.  Four different noise distributions are tested, each defined by parameters Pmean and Pstd which control the mean and standard deviation of the lognormal distribution of noise levels. The results, shown as the first and last frames of generated videos, demonstrate that the quality of the generated video is highly sensitive to the noise distribution used in training. A balanced noise distribution generally yields superior video quality.", "section": "4.3 Ablation Analysis"}, {"figure_path": "PVgAeMm3MW/figures/figures_9_3.jpg", "caption": "Figure 5: Comparison between SVD [13], AnimateLCM [21], LADD [28], UFOGen [25], and Our Approach. We provide the synthesized videos (sampled frames) under various settings for different approaches. We use SVD to generate videos under 25, 16, and 8 sampling steps, AnimateLCM to synthesize videos under 4 sampling steps, LADD and UFOGen to generate videos under 1 sampling step. AnimateLCM, LADD and UFOGen generates blurry frames with few-steps and single-step sampling. Our approach can accelerate the sampling speed by 22.9\u00d7 compared with SVD while maintaining similar frame quality and motion consistency.", "description": "This figure compares the video generation results of several different models, including the proposed single-step method, highlighting the trade-off between the number of sampling steps and the quality of the generated video. The proposed method shows comparable video quality to models using significantly more steps, demonstrating its speed advantage.", "section": "4.2 Comparisons Results"}]