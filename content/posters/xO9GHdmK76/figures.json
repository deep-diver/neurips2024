[{"figure_path": "xO9GHdmK76/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Traditional feature representation without interaction [14, 41]. (b) Recent work with finite feature interaction [35, 45]. (c) Our method: Kernel-enabled infinite feature interaction.", "description": "This figure illustrates three different approaches to feature interaction in neural networks. (a) shows the traditional approach, where features are simply linearly combined.  (b) shows a more modern approach, where element-wise multiplication is used to create higher-order feature interactions.  (c) shows the proposed approach, which uses a kernel function to implicitly map features to an infinite-dimensional space, allowing for much richer interactions.", "section": "Introduction"}, {"figure_path": "xO9GHdmK76/figures/figures_5_1.jpg", "caption": "Figure 2: Comparison of simple representation, finite interaction, and infinite-dimensional interaction. The ? circle in DemoBlock is chosen from element-wise Add, element-wise Mul, or RBF kernel.", "description": "This figure compares three different approaches to feature interaction in a neural network: simple representation (element-wise addition), finite interaction (element-wise multiplication), and infinite-dimensional interaction (RBF kernel).  The left panel shows the architecture of a \"DemoBlock,\" which is a building block within a larger network.  The block comprises several convolutional layers and batch normalization, culminating in a choice of one of the three interaction methods represented by the question mark. The right panel shows the accuracy curves obtained when training these different architectures on the CIFAR dataset.  The graph shows a steady increase in performance when moving from simple representation to finite and then to infinite-dimensional interactions. The bottom panel shows example visualizations of the class activation maps generated by each method, highlighting the differences in how the models interpret the features.  The RBF kernel is shown to produce the most comprehensive class activation map.", "section": "Demo Case Performance of Models on Different Feature Space"}, {"figure_path": "xO9GHdmK76/figures/figures_6_1.jpg", "caption": "Figure 3: Overview of InfiNet. (a) Four-stage hierarchical InfiNet design. (b) InfiBlock Design", "description": "This figure provides a detailed overview of the InfiNet architecture. (a) shows the overall hierarchical structure of InfiNet, consisting of four stages with increasing feature map resolutions and channel counts. Each stage comprises multiple InfiBlocks, which are the building blocks of the network. (b) illustrates the design of a single InfiBlock, showcasing the key components involved in its infinite-dimensional feature interaction mechanism, such as LayerNorm, depth-wise convolutions (dw-Conv), ReLU activations, RBF kernel for feature interaction, and MLP layers. This detailed visualization helps clarify how InfiNet leverages infinite-dimensional feature interactions within a hierarchical framework to achieve state-of-the-art performance.", "section": "5 Method"}, {"figure_path": "xO9GHdmK76/figures/figures_13_1.jpg", "caption": "Figure 4: Visualization Comparison of (1) Feature Representation Space model, (2) Finite Feature Interaction Space model, (3) Infinite-Dimensional Feature Interaction model", "description": "This figure presents a visual comparison of three different models' class activation maps (CAMs). CAMs are heatmaps that highlight the regions of an image that are most influential in a model's prediction.  The three models compared are: (1) A traditional model focusing solely on feature representation space, showing limited attention across the image; (2) A model utilizing finite feature interaction, indicating some improvement in focusing on relevant parts of the image; and (3) InfiNet, the proposed infinite-dimensional feature interaction model, demonstrating a much more focused and precise activation map, correctly emphasizing the key object features in each image.  The comparison visually highlights the effectiveness of the InfiNet's approach in capturing more complete and accurate spatial relationships in an image for classification.", "section": "A.2 More Visualization Comparison"}]