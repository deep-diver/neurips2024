{"references": [{"fullname_first_author": "Jonathan Frankle", "paper_title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "publication_date": "2019-03-08", "reason": "This paper introduces the lottery ticket hypothesis, a foundational concept for the current work's exploration of sparse neural networks in reinforcement learning."}, {"fullname_first_author": "Laura Graesser", "paper_title": "The State of Sparse Training in Deep Reinforcement Learning", "publication_date": "2022-06-17", "reason": "This survey paper provides a comprehensive overview of sparse training techniques in deep reinforcement learning, which is directly relevant to the current research."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Off-Policy Deep Reinforcement Learning without Exploration", "publication_date": "2018-12-06", "reason": "This paper presents BCQ, an offline reinforcement learning algorithm that is used as a baseline and compared against the proposed method in the current work."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "publication_date": "2018-01-08", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which is used as the primary online reinforcement learning algorithm in the current work's empirical evaluations."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline Reinforcement Learning with Implicit Q-Learning", "publication_date": "2021-10-09", "reason": "This paper introduces Implicit Q-Learning (IQL), another offline reinforcement learning algorithm that is used as a baseline and compared against the proposed method in the current work."}]}