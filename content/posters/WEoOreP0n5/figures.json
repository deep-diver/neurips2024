[{"figure_path": "WEoOreP0n5/figures/figures_1_1.jpg", "caption": "Figure 1: For any given task, our proposed method activates a specific part of the neural network.", "description": "This figure illustrates the core concept of the proposed method.  For each task, a specific subset of the neural network's weights (a neural pathway) is activated. The rest of the network is essentially masked off, making the process more efficient. The figure shows multiple tasks (Task 1 to Task N), each with its own pathway, all sharing the same underlying network. A 'Mask' is used to select the active pathway for each task, separating the tasks and potentially saving computational resources.", "section": "3 Neural Pathway Discovery for RL"}, {"figure_path": "WEoOreP0n5/figures/figures_5_1.jpg", "caption": "Figure 2: Performance comparison of DAPD with baseline on HalfCheetah-v2. (a) At 95% sparsity, we show the learning curve of different algorithms. (b) Under varying sparsity levels we compare the average episodic return evaluated end of 1 million training steps. (c) Ablation study of DAPD.", "description": "This figure compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method with several baseline methods for a single-task online reinforcement learning experiment on the HalfCheetah-v2 environment.  Subfigure (a) shows the learning curves of the different algorithms, illustrating their progress over 1 million gradient updates at a sparsity level of 95%. Subfigure (b) displays the final episodic return achieved by each algorithm across a range of sparsity levels (70% to 95%), highlighting the impact of sparsity on performance.  Finally, subfigure (c) presents an ablation study of the DAPD method, demonstrating the importance of the warmup and freeze phases in its efficacy.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_6_1.jpg", "caption": "Figure 2: Performance comparison of DAPD with baseline on HalfCheetah-v2. (a) At 95% sparsity, we show the learning curve of different algorithms. (b) Under varying sparsity levels we compare the average episodic return evaluated end of 1 million training steps. (c) Ablation study of DAPD.", "description": "This figure compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method against several baseline methods on the HalfCheetah-v2 continuous control task.  Subfigure (a) shows learning curves illustrating the episodic return over training steps. (b) shows how performance varies with different sparsity levels and (c) presents ablation studies showing impact of different components of the DAPD method.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_7_1.jpg", "caption": "Figure 5: Energy consumption profile of algorithms on MetaWorld benchmark. We normalize the performance and energy consumption to highlight the trade-off for performance gain.", "description": "This figure compares the performance and energy consumption of different multi-task reinforcement learning algorithms on the MetaWorld benchmark.  The performance is normalized to highlight the trade-off between performance gain and energy consumption.  SAC-DAPD shows a significant improvement in energy efficiency compared to other methods while maintaining competitive performance.", "section": "4.2 Online Multi-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_8_1.jpg", "caption": "Figure 6: (a) We compare the sample complexity analysis of BCQ+PD with BCQ-MT (Multitask) and BCQ-MHMT (Multi-head Multitask) baselines on Half Cheetah multitask. (b) Performance (Normalized Score) plot of BCQ on HalfCheetah multitask trained with PD with baselines on mixed data distribution.", "description": "This figure compares the sample complexity and performance robustness of BCQ with pathway discovery (BCQ+PD) against multi-task (BCQ-MT) and multi-head multi-task (BCQ-MHMT) baselines under different conditions.  Panel (a) shows a boxplot comparing performance across varying training sample sizes. Panel (b) presents a boxplot showing robustness to different data distributions (Medium-Expert Mix, Medium, Expert-Replay). The results illustrate that BCQ+PD exhibits superior sample efficiency and better resilience to distributional shift than the other baselines.", "section": "Performance Under Mixed Data Distribution"}, {"figure_path": "WEoOreP0n5/figures/figures_24_1.jpg", "caption": "Figure 7: Snapshot of trained policy for Halfcheetah multitasks.", "description": "The figure shows a series of snapshots of a trained HalfCheetah agent performing five different tasks: running forward, running backward, performing a forward jump, performing a backward jump, and simply jumping. Each sequence of images displays a series of poses from the agent's movements during each task. This figure serves to visually illustrate the diverse capabilities of the trained model for various locomotion tasks.", "section": "Simulated environments"}, {"figure_path": "WEoOreP0n5/figures/figures_24_2.jpg", "caption": "Figure 8: Snapshot of trained policy for Halfcheetah under four different constrained velocity.", "description": "This figure shows the performance of a trained policy for the HalfCheetah environment under four different constrained velocities (0.5, 1.0, 2.0, and 3.0). Each subfigure displays a sequence of images showing the HalfCheetah's movement at a specific velocity. The images illustrate how the trained policy adapts its movements to achieve the desired velocity while maintaining stability and control.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_24_3.jpg", "caption": "Figure 9: Snapshot of trained policy for Quadrupod multitasks.", "description": "This figure shows a series of snapshots from a trained policy for a quadrupedal robot performing four different tasks: (a) pace forward, (b) pace backward, (c) hopturn, and (d) sidestep. Each row represents one of the four tasks, and each column shows a frame from a short video sequence of the robot executing that task. This visualization helps to illustrate the robot's learned behaviors for each task.", "section": "C.4 Simulated environments"}, {"figure_path": "WEoOreP0n5/figures/figures_25_1.jpg", "caption": "Figure 10: Snapshot of MetaWorld MT10 tasks.", "description": "This figure shows snapshots of the ten different tasks included in the MetaWorld MT10 benchmark.  The tasks involve a robotic arm manipulating objects in a variety of ways, such as reaching for an object, pushing an object, picking and placing objects, opening doors and drawers, pressing buttons, inserting pegs, and opening and closing windows.  These tasks represent a diverse set of challenges in robotic manipulation and are used to evaluate the performance of multi-task reinforcement learning algorithms.", "section": "C.5 Performance evaluation"}, {"figure_path": "WEoOreP0n5/figures/figures_26_1.jpg", "caption": "Figure 11: After the warm-up phase, we sample three sub-networks (highlighted in yellow, pink, and green). These sub-networks exhibit a certain percentage of change in the parameter space from the prior sub-network (blue). We illustrate the overall percentage change using the Venn diagram. The similarity in the learning curves supports the existence of multiple equally effective lottery sub-networks.", "description": "This figure shows the results of an experiment designed to investigate the existence of multiple equally effective lottery sub-networks in online reinforcement learning.  After an initial warm-up phase where the mask is periodically adjusted, three different sub-networks are sampled.  A Venn diagram illustrates the overlap between these sub-networks and the previous sub-network from the warm-up phase.  The close similarity of learning curves in the three sampled sub-networks supports the idea that multiple equally effective solutions exist within the overall network, supporting the lottery ticket hypothesis.", "section": "3.3 Pathways for Offline RL"}, {"figure_path": "WEoOreP0n5/figures/figures_26_2.jpg", "caption": "Figure 12: Learning curve of RiGL (left) and Rlx2 (right) under different sparsity levels. We compare the performance of optimal sparsity level (actor 90% and critic 75% in green) presented in [82] with the performance at 95% sparse network ((for both actor and critic in orange).", "description": "The figure shows a comparison of the learning curves for RiGL and Rlx2 at different sparsity levels (90%/75% and 95%/95%).  The results highlight that RiGL and Rlx2 perform poorly at 95% sparsity, unlike the proposed DAPD method.", "section": "D.1 Online RL Expeirments"}, {"figure_path": "WEoOreP0n5/figures/figures_27_1.jpg", "caption": "Figure 13: At 95% sparsity, we compare the performance of DAPD with baselines on MuJoCo control tasks.", "description": "This figure compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method with three other baseline methods (SAC-Dense, RiGL, and Rlx2) on four different MuJoCo continuous control tasks.  The sparsity level is fixed at 95%, meaning only 5% of the network parameters are used.  The y-axis represents the episodic return (a measure of performance), and the x-axis represents the number of gradient updates (a measure of training progress). The figure shows the learning curves for each method across the four tasks, illustrating DAPD's superior performance and stability compared to the baselines.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_27_2.jpg", "caption": "Figure 13: At 95% sparsity, we compare the performance of DAPD with baselines on MuJoCo control tasks.", "description": "This figure compares the performance of the proposed Data Adaptive Pathway Discovery (DAPD) method with other baseline methods (SAC-Dense, RiGL, and Rlx2) on four different MuJoCo continuous control tasks (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2).  The results are shown as learning curves for each task at a sparsity level of 95%. The x-axis represents the number of gradient updates, and the y-axis represents the average episodic return over 10 evaluations across different seeds.  The figure demonstrates that DAPD consistently outperforms the baseline methods in terms of final episodic return.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_27_3.jpg", "caption": "Figure 15: Sample efficiency comparison of SAC-sparse (95%) network using DAPD with the SAC-dense counterpart. We average episodic return over the 10 evaluations over 5 seeds at different training steps.", "description": "This figure compares the sample efficiency of a 95% sparse network trained using the Data Adaptive Pathway Discovery (DAPD) method against a dense SAC network.  The results show that the DAPD method achieves comparable or better performance than the dense network, even with significantly fewer parameters, across multiple continuous control tasks (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2). This suggests that the DAPD method is more sample efficient and potentially requires less training data to achieve good results.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_28_1.jpg", "caption": "Figure 11: After the warm-up phase, we sample three sub-networks (highlighted in yellow, pink, and green). These sub-networks exhibit a certain percentage of change in the parameter space from the prior sub-network (blue). We illustrate the overall percentage change using the Venn diagram. The similarity in the learning curves supports the existence of multiple equally effective lottery sub-networks.", "description": "The figure demonstrates the concept of lottery tickets in online reinforcement learning. After an initial warm-up phase, the algorithm identifies multiple sub-networks (pathways) within a larger network that achieve similar performance. The Venn diagram illustrates the overlap in parameter space between these sub-networks, supporting the hypothesis that many equally effective sub-networks exist.", "section": "3.2 Data Adaptive Pathway Discovery (DAPD)"}, {"figure_path": "WEoOreP0n5/figures/figures_29_1.jpg", "caption": "Figure 17: When we try to configure pathways without warm-up phase (blue), (a) parameters of the harder tasks do not change from the values at initialization due to small changes in gradients. This results in unsuccessful task learning and can be seen in (b). We find an improvement in performance including warm-up phase (pink).", "description": "This figure shows the batch mean gradient norm and success rate for two MetaWorld tasks (Peg-insert-side and Pick-place) with and without a warm-up phase in the DAPD algorithm. The results indicate that the warm-up phase significantly improves the learning process and ultimately enhances performance.", "section": "D.2 Online Multitask Training and Addressing Gradient Interference"}, {"figure_path": "WEoOreP0n5/figures/figures_30_1.jpg", "caption": "Figure 15: Sample efficiency comparison of SAC-sparse (95%) network using DAPD with the SAC-dense counterpart. We average episodic return over the 10 evaluations over 5 seeds at different training steps.", "description": "This figure compares the sample efficiency of a 95% sparse neural network trained using the Data Adaptive Pathway Discovery (DAPD) method to a dense network, both using Soft Actor-Critic (SAC).  The x-axis represents the number of training steps, and the y-axis represents the average episodic return.  The plot shows that even with significantly fewer parameters, the DAPD method achieves comparable or better performance than the dense network, demonstrating superior sample efficiency.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_31_1.jpg", "caption": "Figure 15: Sample efficiency comparison of SAC-sparse (95%) network using DAPD with the SAC-dense counterpart. We average episodic return over the 10 evaluations over 5 seeds at different training steps.", "description": "The figure compares the sample efficiency of a 95% sparse network trained using the proposed Data Adaptive Pathway Discovery (DAPD) method against a dense SAC network across four MuJoCo continuous control tasks (HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2).  The plots show the average episodic return over 10 evaluations and 5 random seeds at various training steps (250k, 500k, 750k, and 1M).  It demonstrates that DAPD achieves comparable or better performance with significantly fewer parameters, showcasing its improved sample efficiency.", "section": "4.1 Scenario 1: Online Single-Task RL"}, {"figure_path": "WEoOreP0n5/figures/figures_33_1.jpg", "caption": "Figure 1: For any given task, our proposed method activates a specific part of the neural network.", "description": "This figure illustrates the core concept of the proposed method, which involves activating specific sub-networks (neural pathways) within a larger neural network for different tasks.  Each task utilizes a unique pathway, represented by the different colored nodes and connections.  The mask indicates which parts of the network are activated for a specific task.", "section": "3 Neural Pathway Discovery for RL"}]