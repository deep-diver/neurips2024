[{"figure_path": "QWsLks8LCO/tables/tables_7_1.jpg", "caption": "Table 1: Test win rates (%) and standard deviations compared with imitation learning methods.", "description": "This table presents the performance comparison between the proposed LBI method and several baseline imitation learning methods on various StarCraft II maps. The win rate is used as the evaluation metric, and the standard deviation shows the performance variability.  It highlights the superior performance of LBI compared to baseline methods.", "section": "5.1 Performance Comparison"}, {"figure_path": "QWsLks8LCO/tables/tables_7_2.jpg", "caption": "Table 2: Test return and standard deviations compared with offline reinforcement learning methods.", "description": "This table presents a comparison of the test return and standard deviations achieved by the proposed Learning Before Interaction (LBI) method and several existing offline reinforcement learning methods, including BCQ-MA, CQL-MA, ICQ, OMAR, and OMIGA, across four different StarCraft II maps: 5m_vs_6m, 2c_vs_64zg, 6h_vs_8z, and corridor.  The results showcase LBI's superior performance in terms of average return compared to other methods on the selected maps.", "section": "5.1 Performance Comparison"}, {"figure_path": "QWsLks8LCO/tables/tables_8_1.jpg", "caption": "Table 3: Test win rates (%) and standard deviations on unseen tasks.", "description": "This table presents the results of applying the LBI model and two baseline models (MADT and MA-TREX) on unseen tasks from the StarCraft Multi-Agent Challenge benchmark.  It demonstrates the zero-shot generalization capabilities of the LBI model, showing its ability to perform well on tasks not seen during training, unlike the baseline models.  The table displays the win rates (percentage of games won) and their standard deviations across different unseen scenarios, highlighting the superior performance of LBI.", "section": "5 Experiments"}, {"figure_path": "QWsLks8LCO/tables/tables_8_2.jpg", "caption": "Table 4: The ablation results for the dynamics model without residual term (wo-RT), image reference (wo-IR), and using ground-truth image (GTI) as the reference for state prediction.", "description": "This table presents the ablation study of the dynamics model. It shows the prediction error and return (average return across all training maps) for different configurations of the dynamics model:  \n- LBI: The complete dynamics model.\n- LBI-GTI: Using ground truth images for state prediction.\n- LBI-wo-RT: Removing the residual term in the dynamics model.\n- LBI-wo-IR: Removing the image reference in the dynamics model.\n- LBI-wo-RT&IR: Removing both the residual term and image reference in the dynamics model. \nThe results show the impact of each component on the model's performance.", "section": "5.3 Ablation Studies"}, {"figure_path": "QWsLks8LCO/tables/tables_8_3.jpg", "caption": "Table 5: The ablation results for the reward model without reward constraint (wo-RC), behavior regularization (wo-BR), and using ground-truth rewards (w-GTR) provided by the SMAC benchmark.", "description": "This table presents the ablation study results on the reward model. It shows the impact of removing the reward constraint (wo-RC), the behavior regularization (wo-BR), and using ground-truth rewards (w-GTR) on the model's performance.  The results are broken down by return on training and unseen tasks, allowing for an analysis of how each component contributes to the overall performance. The table helps to determine the importance of each component in the model's design and effectiveness.", "section": "5.3 Ablation Studies"}, {"figure_path": "QWsLks8LCO/tables/tables_17_1.jpg", "caption": "Table 6: Return distribution on training maps.", "description": "This table shows the distribution of returns obtained during the training phase across ten different maps in the StarCraft Multi-Agent Challenge (SMAC) benchmark.  Each map presents a unique set of challenges for the multi-agent system, resulting in varying levels of success. The average return across all ten maps was 19.64 \u00b1 1.63. This information provides insight into the difficulty and variability of the training environment.", "section": "A Broader Impacts and Limitations"}, {"figure_path": "QWsLks8LCO/tables/tables_20_1.jpg", "caption": "Table 1: Test win rates (%) and standard deviations compared with imitation learning methods.", "description": "This table compares the test win rates and standard deviations of the proposed LBI method against several other imitation learning methods across various StarCraft II maps.  It shows LBI's performance in comparison to baselines, highlighting the method's effectiveness in multi-agent decision-making tasks.", "section": "5.1 Performance Comparison"}, {"figure_path": "QWsLks8LCO/tables/tables_20_2.jpg", "caption": "Table 1: Test win rates (%) and standard deviations compared with imitation learning methods.", "description": "This table presents the test win rates and standard deviations achieved by the proposed Learning Before Interaction (LBI) method and several imitation learning baselines across various StarCraft Multi-Agent Challenge (SMAC) maps.  The results demonstrate LBI's superior performance compared to the baseline methods.  The maps represent different complexities and scenarios in the SMAC benchmark.", "section": "5.1 Performance Comparison"}, {"figure_path": "QWsLks8LCO/tables/tables_21_1.jpg", "caption": "Table 1: Test win rates (%) and standard deviations compared with imitation learning methods.", "description": "This table presents the test win rates and standard deviations achieved by the proposed Learning Before Interaction (LBI) method and several imitation learning baselines across multiple StarCraft II maps.  It compares the performance of LBI against methods such as Behavior Cloning (BC), Multi-Agent Adversarial Inverse Reinforcement Learning (MA-AIRL), Multi-Agent Decision Transformer (MADT), Multi-Agent TREX (MA-TREX), and Multi-Agent Preference-based Return Transformer (MAPT). The results show LBI's significantly higher win rates and better performance in most scenarios.", "section": "5.1 Performance Comparison"}]