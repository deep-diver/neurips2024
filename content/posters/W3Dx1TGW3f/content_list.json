[{"type": "text", "text": "Contextual Bilevel Reinforcement Learning for Incentive Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vinzenz Thoma \\* Barna Pasztor \\* Andreas Krause ETH AI Center ETH AI Center ETH Zurich vinzenz.thoma@ai.ethz.ch barna.pasztor@ai.ethz.ch krausea@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Giorgia Ramponi University of Zurich giorgia.ramponi@uzh.ch ", "page_idx": 0}, {"type": "text", "text": "Yifan Hu EPFL & ETH Zurich yifan.hu@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The optimal policy in various real-world strategic decision-making problems depends both on the environmental configuration and exogenous events. For these settings, we introduce Contextual Bilevel Reinforcement Learning (CB-RL), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP). CB-RL can be viewed as a Stackelberg Game where the leader and a random context beyond the leader's control together decide the setup of many MDPs that potentially multiple followers best respond to. This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as RLHF, tax design, reward shaping, contract theory and mechanism design. We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve CB-RL, and demonstrate its convergence. Notably, HPGD uses stochastic hypergradient estimates, based on observations of the followers\u2019 trajectories. Therefore, it allows followers to use any training procedure and the leader to be agnostic of the specific algorithm, which aligns with various real-world scenarios. We further consider the setting when the leader can influence the training of followers and propose an accelerated algorithm. We empirically demonstrate the performance of our algorithm for reward shaping and tax design. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Reinforcement Learning (RL), Markov Decision Processes (MDPs) [53] provide a versatile framework for capturing sequential decision-making problems across various domains such as health care [76], energy systems [52], economics [14], and finance [33]. A considerable amount of work has been devoted to solving standard MDPs [2, 62, 67]. However, in many applications, MDPs can be configured on purpose or affected by exogenous events, both of which can significantly impact the corresponding optimal policies. For example, in a simplified economic framework, the optimal decision of an individual household depends both on public policies and economic uncertainties [19, 34]. The policy maker in turn has to make decisions, anticipating the best response of differentlyminded individual agents to its policies and exogenous events outside the policy maker's control. ", "page_idx": 0}, {"type": "text", "text": "To study such problems, we introduce Contextual Bilevel Reinforcement Learning (CB-RL), a hierarchical decision-making framework where followers solve a contextual Markov decision processes (CMDP) [32], configured by the leader. CB-RL is formalized as: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{x}{\\mathrm{min}}}&{F(x):=\\mathbb{E}_{\\xi}[f(x,\\pi_{x,\\xi}^{*},\\xi)]}&{(\\mathrm{leader},\\mathrm{upper-level})}\\\\ {\\mathrm{where}}&{\\pi_{x,\\xi}^{*}=\\underset{\\pi}{\\mathrm{argmax}}\\,J_{\\lambda,x,\\xi}(\\pi)}&{(\\mathrm{follower},\\mathrm{lower-level}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $x$ represents the model configuration of the CMDP chosen by the leader, $\\xi$ represents the context that followers encounter, and the function $J_{\\lambda,x,\\xi}$ denotes the (entropy-regularized) reward of the CMDP for a policy $\\pi$ , a given model parameters $x$ , a context $\\xi$ , and a regularization parameter $\\lambda$ ", "page_idx": 1}, {"type": "text", "text": "In our framework, the leader chooses $x$ to configure various aspects of the CMDP, such as state transitions, the initial state distribution, and the followers\u2032 reward functions. Modeling the problem as a CMDP instead of a standard MDP is essential when modeling situations where the environment is influenced by side information or personal perferences. For example, the context $\\xi$ can capture a wide range of real-world scenarios such as: (1) there is one follower, who responds optimally not only to the leader's chosen $x$ but also to a side information $\\xi$ , such as weather or season, (2) there are multiple followers, each aiming to maximize their own utility, in which case $\\xi$ represents different possible follower preferences, and (3) there are multiple followers, each facing an uncertain contextual variable, i.e., $\\xi=(i,\\eta)$ represents the $i$ -th follower encountering a specific context $\\eta\\sim\\mathbb{P}_{\\eta}$ ", "page_idx": 1}, {"type": "text", "text": "The proposed framework extends the concept of contextual bilevel optimization [37], where the follower engages in static contextual stochastic optimization rather than sequential decision-making. It also expands upon traditional MDP model design [78, 15] and configurable MDPs [50, 55], where typically only one follower attempts to solve an MDP, as opposed to CMDPs. We defer a full discussion of the related works to Appendix A. Beyond these fields, our framework finds various applications in Principal-Agent problems [8], RLHF [13, 58], dynamic Stackelberg games [27, 65], Security Games [60, 46], dynamic mechanism design [18], contract theory [41, 69], and tax design [19, 82, 34]. See Appendix B for the concrete CB-RL formulations of these applications. ", "page_idx": 1}, {"type": "text", "text": "Despite its wide applicability, to the best of our knowledge, there are no algorithms specifically designed for CB-RL. The closest is an algorithm from model design for MDPs [15], which can be adapted to our setting after some modifications. However, [15] requires the follower to solve the MDP deterministically using soft value iteration. Moreover, the full hypergradient is computed in each iteration, as part of which the leader requires access to the lower-level computations. Both of these aspects significantly restrict how well the method can scale to larger settings. ", "page_idx": 1}, {"type": "text", "text": "Instead, in this work, we propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm for the leader that solely relies on trajectory data from followers. The followers can use a variety of possibly stochastic learning algorithms to find an approximately optimal policy for the lower-level CMDPs. The leader in turn is agnostic of the exact algorithm used, as the hypergradient is estimated using only trajectory samples generated from the follower policy. The fact that both the lower-level and the hypergradient computation are stochastic makes HPGD salable to large problem settings. ", "page_idx": 1}, {"type": "text", "text": "We show non-asymptotic convergence of HPGD to a stationary point and validate these findings through experimental evidence. In scenarios where followers grant the leader full control over their training procedure, as posited in prior work [15], we present an accelerated HPGD algorithm, designed to minimize the number of lower-level iterations. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u00b7 We introduce Contextual Bilevel Reinforcement Learning (CB-RL) that captures a wide range of important applications (Sec. 2). It is the first bilevel reinforcement learning framework that throughthecontext $\\xi$ allows multiple followers and side information. We summarize the key differences to the previous literature in Table 1. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm that performs stochastic gradient descent on the upper-level objective (Sec. 3.2). Importantly, we are the first to estimate the hypergradient from lower-level trajectory samples instead of computing it exactly, while further providing convergence guarantees. Furthermore, our approach is agnostic of the learning dynamics of the agent, enabling followers to utilize a wide range of algorithms to solve the lower-level CMDPs. We only assume the leader can sample lower-level trajectories from an inexact oracle. For several widely-used RL algorithms, we explicitly show how to use them to build the inexact oracle needed by HPGD. Noteably, we are the first to consider stochastic lower-level learning algorithms, such as soft Q-learning. ", "page_idx": 1}, {"type": "table", "img_path": "W3Dx1TGW3f/tmp/db84d7dbe8eb4221df33df7a831f9ae61c03b6e2463125c69a6d686a5988ee19.jpg", "table_caption": ["Table 1: Summary of Related Works in Bilevel Reinforcement Learning "], "table_footnote": ["Multi: Multiple followers. Side Info: Side information. Context: CMDP instead of MDP. $r_{x}$ $P_{x}$ , and $\\mu_{x}$ denote the dependence of rewards, transitions, and initial state distribution on $x$ . Agnostic vs. Control: Whether leader can influence the training of the follower(s). Deter vs. Stoch: Requiring full knowledge of hypergradient or estimating it from samples. Complexity is based on $\\|\\nabla F(x)\\|^{2}\\leq\\delta^{\\hat{2}}$ instead of $\\|\\nabla F(x)\\|^{2}\\leq\\delta$ $^{*}[15]$ assumes convexityof $F$ in $x$ and considers $F(x)-\\operatorname*{min}F(x)\\leq\\delta$ . VI: Value Iteration. PMG: Policy Mirror Gradient. PI: Policy Iteration. NPG: Natural Policy Gradient. Q: Q-learning. RT-Q: Randomly Truncated Soft Q-learning. "], "page_idx": 2}, {"type": "text", "text": "\u00b7 We establish the non-asymptotic convergence rate of HPGD to a stationary point of the overall objective\u2014-despite the nonconvex lower-level problem (Sec. 3.2). When assuming full hypergradient information, i.e., deterministic updates, the outer iteration complexity of HPGD reduces to $\\mathcal{O}(\\delta^{-2})$ , recovering previous results. Moreover, we discuss how to estimate the hypergradient if the upper-level loss function admits a specific form of cumulative costs (Sec. 3.3). \u00b7 When the leader is allowed to control the follwers? learning dynamics (Sec 4), we propose a stochastic accelerated algorithm denoted as HPGD RT-Q (Alg 8). It greatly reduces the number of lower-level soft Q-learning iterations from $\\tilde{\\mathcal{O}}(\\delta^{-2})$ to ${\\mathcal{O}}(\\log(\\delta^{-1}))$ , such that we recover the rate for deterministic lower-level updates. For this result we leverage several techniques, such as mini-batches, multilevel Monte Carlo [29, 37], and importance sampling. \u00b7 We demonstrate the performance of HPGD for principal agent reward shaping and tax design (Sec. 5). In certain cases, the stochastic updates of HPGD are beneficial as they avoid local minima. Moreover, HPGD needs fewer outer iterations compared to benchmark algorithms. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a bilevel optimization problem, where the follower solves a Contextual Markov Decision Process (CMPD) and the leader controls the configuration of the CMDP. In particular, the leader chooses a parameter $x\\in X\\subseteq\\mathbb{R}^{d}$ and nature chooses a random context $\\xi$ according to a distribution $\\mathbb{P}_{\\xi}$ . Together $(x,\\xi)$ parameterizes an MDP $\\mathcal{M}_{x,\\xi}$ , which the follower aims to solve. $\\mathcal{M}_{x,\\xi}$ is defined by a tuple $(S,A,r_{x,\\xi},P_{x,\\xi},\\mu_{x,\\xi},\\gamma)$ , where $\\boldsymbol{S}$ denotes the state space, $\\boldsymbol{\\mathcal{A}}$ denotes the action space, $r_{x,\\xi}(\\cdot,\\cdot):S\\times A\\rightarrow\\mathbb{R}$ is the reward function, $P_{x,\\xi}(\\cdot;\\cdot,\\cdot):S\\times S\\times A\\to[0,1]$ denotes the transition kernel, $\\mu_{x,\\xi}$ indicates the initial state distribution, and $\\gamma$ is the discount factor. The subscript $x,\\xi$ implies that rewards, transitions, and initial state distribution depend on the leader's decision $x$ and the context $\\xi$ Connecting to previous works, for a fixed $x$ $\\mathcal{M}_{x,\\xi}$ is a contextual MDP [32] with respect to $\\xi$ For a fixed $\\xi$ $\\mathcal{M}_{x,\\xi}$ generalizes a configurable $M D P$ [50]. Given $\\mathcal{M}_{x,\\xi}$ , the follower maximizes an entropy-regularized objective by choosing a policy $\\pi_{x,\\xi}$ , where $\\pi_{x,\\xi}(a;s)$ denotes the probability of choosing action $a$ in state $s$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}J_{\\lambda,x,\\xi}(\\pi)=\\mathbb{E}_{s_{0}}\\left[V_{\\lambda,x,\\xi}^{\\pi}(s_{0})\\right]=\\mathbb{E}_{s_{0}}\\left[\\mathbb{E}_{P_{x,\\xi}}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(r_{x,\\xi}(s_{t},a_{t})+\\lambda H(\\pi;s_{t})\\right)\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s_{0}\\sim\\mu_{x,\\xi}$ $\\sim\\mu_{x,\\xi},a_{t}\\sim\\pi(\\cdot;s_{t}),s_{t+1}\\sim P_{x,\\xi}(\\cdot;s_{t},a_{t})$ and $\\begin{array}{r}{H(\\pi;s)=-\\sum_{a}\\pi(a;s)\\log\\pi(a;s)}\\end{array}$ We call $\\lambda>0$ the regularization parameter and $V_{\\lambda,x,\\xi}^{\\pi}$ the value function. As standard in RL literature, ", "page_idx": 2}, {"type": "text", "text": "we define the related Q and advantage functions as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\lambda,x,\\xi}^{\\pi}(s,a)=r_{x,\\xi}(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{x,\\xi}(\\cdot;s,a)}\\left[V_{\\lambda,x,\\xi}^{\\pi}(s^{\\prime})\\right]}\\\\ &{A_{\\lambda,x,\\xi}^{\\pi}(s,a)=Q_{\\lambda,x,\\xi}^{\\pi}(s,a)-V_{\\lambda,x,\\xi}^{\\pi}(s)=Q_{\\lambda,x,\\xi}^{\\pi}(s,a)-\\displaystyle\\sum_{a^{\\prime}}\\pi(a^{\\prime};s)Q_{\\lambda,x,\\xi}^{\\pi}(s,a^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The unique optimal policy for (2) is given by $\\pi_{x,\\xi}^{*}(s;a)\\propto\\exp(Q_{\\lambda,x,\\xi}^{*}(s,a)/\\lambda)$ , i.e., the softmax of the optimal Q-function [51].2 Given $x,\\pi_{x,\\xi}^{*},\\xi$ , the leader in turn incurs a loss $f(x,\\pi_{x,\\xi}^{*},\\xi)\\in\\mathbb{R}$ which it wants to minimize in expectation over $\\mathbb{P}_{\\xi}$ . To do so, it needs to choose $x$ to align the follower's plicy $\\pi_{x,\\xi}^{*}$ with the leaders objctive. C-RL can thus be formulated as the following stochastic bilevel optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{x}{\\mathrm{min}}}&{F(x):=\\mathbb{E}_{\\xi}[f(x,\\pi_{x,\\xi}^{*},\\xi)]}&{(\\mathrm{leader},\\mathrm{upper-level})}\\\\ {\\mathrm{where}}&{\\pi_{x,\\xi}^{*}=\\underset{\\pi}{\\mathrm{argmax}}\\,J_{\\lambda,x,\\xi}(\\pi).}&{(\\mathrm{follower},\\mathrm{lower-level})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (4) is well-defined due to entropy regularization ensuring the uniqueness of $\\pi_{x,\\xi}^{*}$ . It further ensures $\\pi_{x,\\xi}^{*}$ is differentiable, stabilizes learning and appears in previous works [15]. Moreover, the difference between the regularized and unregularized problem generally vanishes as $\\lambda\\to0$ [15, 26]. ", "page_idx": 3}, {"type": "text", "text": "CB-RL captures many important real-world problems, including RLHF, dynamic mechanism design, tax design, and general principal-agent problems. We discuss these in detail in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3 Hyper Policy Gradient Descent Algorithm for CB-RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we derive a simple expression for the hypergradient of CB-RL. We present HPGD and prove non-asymptotic convergence. HPGD can be combined with a large class of lower-level MDP solvers satisfying a mild inexact oracle assumption. We show this is the case for several popular RL algorithms. Furthermore, we present results for two important special cases of our problem: (1) when the upper-level objective decomposes as a discounted sum of rewards over the lower-level trajectories, and (2) when the leader can direct the lower-level algorithm. We defer all proofs to Appendix E. and make the following standard assumptions on how $x$ and $\\xi$ influence the setup of the CMDP. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. We assume the following conditions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|f(x_{1},\\pi_{1},\\xi)-f(x_{2},\\pi_{2},\\xi)\\|_{\\infty}\\leq L_{f}\\left(\\|x_{1}-x_{2}\\|_{\\infty}+\\|\\pi_{1}-\\pi_{2}\\|_{\\infty}\\right)}\\\\ &{}&{\\|\\partial_{x}f(x_{1},\\pi_{1},\\xi)-\\partial_{x}f(x_{2},\\pi_{2},\\xi)\\|_{\\infty}\\leq S_{f}\\left(\\|x_{1}-x_{2}\\|_{\\infty}+\\|\\pi_{1}-\\pi_{2}\\|_{\\infty}\\right)}\\\\ &{}&{\\|\\partial_{\\pi}f(x_{1},\\pi_{1},\\xi)-\\partial_{\\pi}f(x_{2},\\pi_{2},\\xi)\\|_{\\infty}\\leq S_{f}\\left(\\|x_{1}-x_{2}\\|_{\\infty}+\\|\\pi_{1}-\\pi_{2}\\|_{\\infty}\\right)}\\\\ &{}&{\\quad\\quad\\forall x,\\xi:|r_{x,\\xi}(s,a)|<\\overline{{R}},\\,\\|\\partial_{x}\\log P_{x,\\xi}(s^{\\prime};s,a)\\|_{\\infty}<K_{1},\\,\\|\\partial_{x}r_{x,\\xi}(s,a)\\|_{\\infty}<K_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.1  Hypergradient derivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The leader's loss $f$ depends on $x$ directly and indirectly through $\\pi_{x,\\xi}^{*}$ . Therefore, the derivative of $f$ with respect to $x$ is commonly referred to as the hypergradient to highlight this nested dependency. It is possible to obtain a closed-form expression of the hypergradient, using the implicit function theorem [28]. However, this involves computing and inverting the Hessian of the follower's value function, which can be computationally expensive and unstable [24, 47]. Instead, we leverage the fact that $\\pi_{x,\\xi}^{*}$ is a softmax function to explicitly compute its derivative with respect to $x$ , which is given by $\\begin{array}{r}{\\frac{d\\pi_{x,\\xi}^{*}(s,a)}{d x}=\\frac{1}{\\lambda}\\pi_{x,\\xi}^{*}(a;s)\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)}\\end{array}$ [15], where $\\partial_{x}A=(\\partial_{x_{1}}A,\\ldots,\\partial_{x_{d}}A)$ Aplying the Dominated Convergence Theorem to switch derivative and expectation, we arrive at Theorem 1. Theorem 1. Under Assumption 3.1, $F$ is differentiable and the hypergradient isgivenby ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d F(x)}{d x}=\\mathbb{E}_{\\xi}\\left[\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}+\\mathbb{E}_{s\\sim\\nu,a\\sim\\pi_{x,\\xi}^{*}}\\left[\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right]\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nu$ is any sampling distribution with full support on the state space $\\boldsymbol{S}$ ", "page_idx": 3}, {"type": "text", "text": "Input: Iterations $T$ , Learning rate $\\alpha$ , Regularization $\\lambda$ , Trajectory oracle $o$ Initial point $x_{0}$   \nfor $t=0$ to $T-1$ do $\\xi\\sim\\mathbb{P}_{\\xi}$ \uff0c $s\\sim\\nu$ and $a\\sim\\pi_{x,\\xi}^{o}(\\cdot;s)$ $\\begin{array}{r l}&{\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s,a)\\gets\\mathtt{G r a d i e n t E s t i m a t o r}(\\xi,x_{t},s,a,o)\\,(\\mathrm{Algor})}\\\\ &{\\overbrace{\\frac{d F}{d x}}^{\\widehat{d F}}\\gets\\frac{\\partial_{1}f(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial x}+\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi))}{\\partial\\pi(s,a)}\\partial_{x}\\overline{{A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}}}(s,a)}\\\\ &{x_{t+1}\\gets x_{t}-\\alpha\\frac{\\widehat{d F}}{d x}}\\end{array}$ thm2)   \nend for   \nOntnut: ${\\hat{x}}_{T}\\sim\\operatorname{Uniform}(\\left\\{x_{0},\\dots,x_{T-1}\\right\\})$   \nThe first term captures the direct infuence of $x$ on $f$ , and the second the indirect influence through   \n$\\pi_{x,\\xi}^{*}$ . For now we assume the leader knows $\\partial_{1}f(\\cdot,\\pi,\\xi)$ and $\\partial_{2}f(x,\\cdot,\\xi)$ . It remains to compute   \n$\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)$ ie. the parial derivative withrest t $x$ evaluated for a given poliy For ths,f. $\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)$ $P_{x,\\xi},\\mu_{x,\\xi}$ $r_{x,\\xi}$ $x$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 2. For given $\\pi,x,\\xi,$ it holds that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi}(s_{0},a_{0})=\\mathbb{E}_{s,a}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\frac{d r_{x,\\xi}(s_{t},a_{t})}{d x}+\\gamma^{t+1}\\frac{d\\log P_{x,\\xi}(s_{t+1};s_{t},a_{t})}{d x}V_{\\lambda,x,\\xi}^{\\pi}(s_{t+1})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note, Theorems 1 and 2 generalize existing results in model design for MDPs to CMDPs [15, 78]. ", "page_idx": 4}, {"type": "text", "text": "3.2  HPGD Algorithm and Convergence Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Computing the exact hypergradient is computationally expensive and thus infeasible in larger settings. Instead, to minimize $\\bar{F}(x)$ , one would ideally sample unbiased estimates of the hypergradient in Equation (5) and run stochastic gradient descent (SGD). However, the leader does not have access to $\\pi_{x,\\xi}^{*}$ and generally no control over the training procedure of the lower level. Instead, we assume the follower adapts any preferred algorithm to solve the MDP up to a certain precision $\\delta$ and the leader can observe trajectories from the follower's policy. Such a setting is well-motivated by economic applications. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. For any $\\mathcal{M}_{x,\\xi}$ , the leader has access to an oracle o, which returns trajectories sampled from a policy $\\pi_{x,\\xi}^{o}$ suchthat $\\forall x,\\forall\\xi:\\mathbb{E}_{o}\\left[\\left\\|\\pi_{x,\\xi}^{*}-\\pi_{x,\\xi}^{o}\\right\\|_{\\infty}^{2}\\right]\\leq\\delta^{2}.$ ", "page_idx": 4}, {"type": "text", "text": "We will show that Assumption 3.2 is relatively mild and holds for a variety of RL algorithms. Given $\\pi_{x,\\xi}^{o}$ $\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s,a)$ rolling out $\\pi_{x,\\xi}^{o}$ for $T$ steps, where $T\\stackrel{\\cdot\\,^{\\cdot}}{\\sim}{\\mathrm{Geo}}(1-\\gamma)$ . We defer the construction (Algorithm 2) and proof of unbiasedness (Proposition 3) to the Appendix. Using this estimator, we introduce HPGD in Algorithm 1. As $F$ is generally nonconvex due to the bilevel structure [28], we demonstrate non-asymptotic convergence to a stationary point of $F$ , which matches the lower bound for solving stochastic smooth nonconvex optimization [1]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Using HPGD, under Assumptions 3.1 and 3.2, for $\\alpha\\leq1/(2S_{f})$ we have the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\frac{d F(\\hat{x}_{T})}{d x}\\right\\|^{2}=\\mathcal{O}\\Big(\\frac{1}{\\alpha T}+\\delta+\\alpha\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For $\\alpha=\\mathcal{O}(1/\\sqrt{T})$ and $\\delta=\\mathcal{O}(1/\\sqrt{T})$ , HPGD converges to a stationary point at rate $\\mathcal{O}(1/\\sqrt{T})$ ", "page_idx": 4}, {"type": "text", "text": "Proof sketch.Using the smoothness of $F$ and the fact that $\\hat{x}_{T}$ is uniformly sampled from all iterates, we upper bound the left side of (6) by the sum of three terms. The first is $\\vert\\bar{F}(x_{0})\\bar{-}\\operatorname*{min}_{x}F(x)\\vert/(\\alpha T)$ The second depends on the bias of our gradient estimate, which we show is linear in $\\delta$ . The last term depends on $\\alpha$ times the variance of our estimator, which is bounded. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "To the best of our knowledge, Theorem 3 is the first result that shows convergence when using stochastic estimates for the hypergradient. When the hypergradient can be computed exactly, the last ${\\mathcal{O}}(\\alpha)$ term vanishes and we recover the deterministic convergence rates of previous works [15, 13, 58]. ", "page_idx": 5}, {"type": "text", "text": "Another major advantage of HPGD is that the follower can use any (possibly stochastic) algorithm satisfying Assumption 3.2 to solve the lower-level MDP, while the leader only needs access to generated trajectories. While Assumption 3.2 certainly holds if the follower solves the MDP exactly, for example with an LP-solver, we are interested in verifying it for common RL algorithms, which can scale to larger state and action spaces. In Appendix E.8, we prove non-asymptotic convergence to $\\pi_{x,\\xi}^{*}$ ${\\mathcal{O}}(\\log1/\\delta)$ (Proposition5); -eaming, which converges at rate of ${\\mathcal{O}}(\\log(1/\\delta)/\\delta^{2})$ (Proposition 6) and Natural Policy Gradient, which converges at rate of ${\\mathcal{O}}(\\log1/\\delta)$ (Proposition 8). Additionaly, we show Vanilla Policy Gradient converges asymptotically in Proposition 7. All these Algorithms thus satisfy Assumption 3.2, which makes HPGD scalable and widely applicable to setings where followers might use a variety of model-free or model-based algorithms. ", "page_idx": 5}, {"type": "text", "text": "3.3  Upper-Level Discounted Reward Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "So far we assumed the leader knows $\\partial_{1}f(\\cdot,\\pi,\\xi)$ and $\\partial_{2}f(x,\\cdot,\\xi)$ . In this subsection, instead, we assume $f$ can be written as the negative expected sum of discounted rewards over the lower-level trajectories and show how to estimate the hypergradient from trajectory samples without explicit knowledge of $\\partial_{1}f(\\cdot,\\pi,\\xi)$ and $\\partial_{2}f(x,\\cdot,\\xi)$ . In many practical applications, such as reward shaping, or dynamic mechanism design (cf. Appendix B), the loss $f$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x,\\pi_{x;\\xi}^{*},\\xi)=-\\mathbb{E}_{s_{0}\\sim\\mu}^{\\pi_{x,\\xi}^{*}}\\Big[\\sum_{t}\\gamma^{t}\\overline{{r}}_{x,\\xi}(s_{t},a_{t})\\Big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\overline{{r}}_{x,\\xi}$ represents the reward of the leader, which is generally distinct from the follower's reward. The expectation is taken over rajectories induced by the lower-level $\\pi_{x,\\xi}^{*}$ In this case, the leader does not know the partial derivatives of $f$ but can still estimate the hypergradient from trajectory samples. The following proposition follows from a similar anlysis as the policy gradient theorem. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. If $f$ decomposes as in Equation (7), then $\\frac{d F(x)}{d x}$ can be expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d F(x)}{d x}=\\mathbb{E}_{\\xi}\\Bigg[\\mathbb{E}_{s_{0}\\sim\\mu_{x,\\xi}}^{\\pi_{x,\\xi}^{*}}\\Bigg[\\sum_{t=0}^{\\infty}\\gamma^{t}\\Bigg(\\frac{1}{\\lambda}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s_{t},a_{t})\\overline{{Q}}_{x,\\xi}(s_{t},a_{t})}\\\\ {+\\left.\\frac{d\\overline{{r}}_{x,\\xi}(s_{t},a_{t})}{d x}+\\partial_{x}\\log P_{x,\\xi}(s_{t};s_{t-1},a_{t-1})\\overline{{V}}_{x,\\xi}(s_{t})\\right)\\Bigg]\\Bigg],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where forcompactness, we slightly abuse notation to express $\\mu_{x,\\xi}(s_{0})$ as $P_{x,\\xi}{\\left(s_{0},a_{-1},s_{-1}\\right)}$ ", "page_idx": 5}, {"type": "text", "text": "Here $\\overline{{V}}_{x,\\xi},\\overline{{Q}}_{x,\\xi}$ are the (unregularized) value and state action value functions with respect to $\\overline{{r}}_{x,\\xi}$ . Comparing to Theorem 1, note that the expectation is over trajectories with starting states distributed according to the actual initial distribution $\\mu_{x,\\xi}$ instead of some $\\nu$ .We discuss how to construct estimators for (8) in Algorithm 5 (Appendix D) and prove unbiasedness in Proposition 4 (Appendix E.7). A special case of Equation (8) appeared in [15], where they consider model design for MDPs, that does not take into account contextual uncertainty or the possibility of multiple followers, i.e when the support of $\\xi$ is a singleton. ", "page_idx": 5}, {"type": "text", "text": "4  Accelerated HPGD with Full Lower-Level Access ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previously, we assumed that the leader does not know the solver used in the lower level and queries trajectories from an oracle. However, in certain settings, such as model design [15], and dynamic mechanism design [18], the leader can additionally influence how the followers solve the CMDP. In this section, we focus on the case when the followers use a stochastic tranining procedure, which usually require a polynomial number of steps in terms of $\\delta^{-1}$ , to learn the optimal lower-level policy. We argue that if the leader has influence on the followers\u2032 training procedure, we can greatly reduce the number oflower-level iterations. ", "page_idx": 5}, {"type": "text", "text": "Let us assume that the lower level is solved using vanilla soft Q-learning (Algorithm 4 in Appendix D). According to Proposition 6 (Appendix E.7), the follower needs to run $\\check{T}\\,=\\,\\mathcal{O}(K2^{K})$ iterations ", "page_idx": 5}, {"type": "text", "text": "Table 2: Bias, variance and lower-level iteration complexity of hypergradient estimators when using vanilla soft Q-learning and RT-Q. ", "page_idx": 6}, {"type": "table", "img_path": "W3Dx1TGW3f/tmp/1b7d29256dddb3bbbc044baedebbb1fdb151cb94a8af8d437912d110dff2170c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "to ensure that E\u2014IT,  T,tlo \u2264 2-K and thus I [dF[- ] $\\begin{array}{r}{\\left\\|\\mathbb{E}\\left[\\frac{d F(x)}{d x}-\\widehat{\\frac{d F_{T}}{d x}}\\right]\\right\\|_{\\infty}=\\mathcal{O}(2^{-K/2})}\\end{array}$ ,where $\\pi^{T}$ denotes the learned policy after running $T$ -th Q-learning iterations and 1 dEr denotes the corresponding hypergradient estimator. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To reduce the lower-level iteration complexity, we propose a randomized early stopping scheme over the lower-level soft Q-learning iterations, denoted as randomly-truncated soft Q-learning (RT-Q). The pseudocode is given in Algorithm 8 (Appendix E.5). We illustrate the high-level idea below. ", "page_idx": 6}, {"type": "text", "text": "Without loss of generality, consider a subsequence $t_{k}:=\\mathcal{O}(k2^{k})$ such that $t_{K}:=T$ . Let ${\\scriptstyle{\\frac{d}{d x}}}F_{T}$ denote the hypergradient estimator, based on the $T$ -th policy iterate $\\pi^{T}$ . It holds that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\frac{d}{l x}}F_{T}={\\frac{d}{d x}}F_{t_{K}}={\\frac{d}{d x}}F_{t_{1}}+\\sum_{k=1}^{K-1}\\left({\\frac{d}{d x}}F_{t_{k+1}}-{\\frac{d}{d x}}F_{t_{k}}\\right)={\\frac{d}{d x}}F_{t_{1}}+\\mathbb{E}_{k\\sim p_{k}}\\left[{\\frac{{\\frac{d}{d x}}F_{t_{k+1}}-{\\frac{d}{d x}}F_{t_{k}}}{p_{k}}}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p_{k}$ denotes a truncated geometric distribution, such that $p_{k}\\,\\propto\\,2^{-k}$ . The above shows that $\\begin{array}{r}{\\frac{d}{d x}F_{t_{1}}\\,+\\,p_{k}^{-1}\\Big[\\frac{d}{d x}F_{t_{k+1}}\\,-\\,\\frac{d}{d x}F_{t_{k}}\\Big]}\\end{array}$ withn $k\\ \\sim\\ p_{k}$ isanunbiasedesiator of $\\scriptstyle{\\frac{d}{d x}}F_{T}$ Using this estimator, the follower does not need to run ${\\mathcal{O}}(K2^{K})$ soft Q-learning iterations but in expectation $\\begin{array}{r}{\\sum_{k=1}^{K-1}p_{k}t_{k}=\\mathcal{O}(K^{2})}\\end{array}$   \nestmatorwiththeamebiaa $\\bar{\\underline{{d}}}_{x}F_{T}$ but a much smaller lower-level iteration complexity. We formalize our results in the following Theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Improved iteration complexity using RT-Q). Using Randomly Truncated soft $Q$ -learning (RT-Q) instead ofvanilla soft $Q$ -learning to estimate the hypergradient, we achieve the bias, variance, and lower-level iteration complexity results summarized in Table 2. ", "page_idx": 6}, {"type": "text", "text": "The idea has been previously studied for contextual bilevel optimization under the name randomly truncated multilevel Monte-Carlo [29, 36, 37]. The reduction in the iteration complexity generally comes at the expense of an increased variance of the hypergradient estimator. In [37], this increase is logarithmic as the lower-level problem is a static optimization problem and samples generated to estimate the hypergradient are independent from the lower-level decision variable. This structure is crucial for controlling the increased variance of the hypergradient estimator. However, for CB-RL, rolloutsgeneraed from $\\pi_{x,\\xi}^{t}$ areused toestimatetehypergradientThese trajetorysamples thus depend on the lower-level decision and it is not possible to control the variance as in [37]. To address this issue, we notice that the major source of randomness in our hypergradient estimators stems from the estimator $\\widehat{\\partial_{x}A^{\\pi^{t_{k}}}(s,a)}$ computed by GradientEstimator (Algorithm 2). We control this randomness by sampling multiple trajectories with a random length, which is in expectation $\\mathcal{O}(1)$ We further sample an action $a$ once from $\\pi^{t_{k+1}}$ and then use it to compute both $\\widehat{\\partial_{x}A^{\\pi^{t_{k+1}}}(s,a)}$ and $\\widehat{\\partial_{x}A^{\\pi^{t_{k}}}(s,a)}$ , using importance sampling. Combining al these tricks with multi-level Monte Carlo, RT-Q achieves a variance of $\\mathcal{O}(K)$ , where $K=\\mathcal{O}(\\overset{\\bullet}{\\log}(\\delta^{-1}))$ ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We illustrate the performance of HPGD in the Four-Rooms environment and on the Tax Design for Macroeconomic Model problem [34, 15] that we extend to multiple households with diverse preferences. We use Adaptive Model Design (AMD) [15] and a zeroth-order gradient approximation algorithm for comparison. Details on the zeroth-order algorithm are deferred to Appendix F.1.2. We note that AMD is not directly applicable to CB-RL as it was designed for solving an MDP without ", "page_idx": 6}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/e4751dd1e6b7d2903ea8769e39e10b33313f01dfa48979b245f41a624c34a314.jpg", "img_caption": ["(a) Four Room State Space (b) Upper-level objective values, $F$ , over the number of outer iterations "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: Four-Rooms State Space and Performance. Left: $S$ denotes the start state while $G^{1}$ and $G^{\\breve{2}}$ denote goal states that are considered separate tasks. $+1$ denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms. ", "page_idx": 7}, {"type": "text", "text": "context. We apply it with modifications described in Appendix F.1.1. To our knowledge, such a zeroth-order gradient method is also the first of its kind for CB-RL. The main distinction between the algorithms is the zeroth-order algorithm requires two oracle queries for each gradient calculation while HPGD and AMD require only one. However, the zeroth-order method only needs to observe the function value of the upper level while the latter two require first-order information about the lower-level contextual MDP. In particular, AMD assumes complete access to the MDP to calculate the exact hypergradient while HPGD relies only on trajectory samples. Technical details about the implementation 3 are deferred to Appendix (F.2). ", "page_idx": 7}, {"type": "text", "text": "5.1  Four-Rooms Environment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure (1a) depicts the lower-level CMDP for the Four-Rooms environment. $S$ denotes the initial position while $G^{1}$ and $G^{2}$ are goal states. We consider the two goal states as separate tasks and define $\\xi$ in Equation (4) to be the uniform distribution over the set of tasks, i.e., $\\xi\\sim\\bar{\\mathrm{Uniform}}(\\{1,2\\})$ We denote the goal state in each task by $G^{\\xi}$ . The state space $\\boldsymbol{S}$ is defined by the cells of the grid world while the actions are the movements in the four directions. In each step $t$ , with probability $2/3$ , the agent moves to $s_{t+1}$ following the chosen direction $a_{t}$ while it takes a random movement with probability $1/3$ . The reward is always zero except when $s_{t}=G^{\\xi}$ where $r(s_{t},a_{t})=1$ , and the episode resets. To incentivize taking the shortest path, we set the discount factor as $\\gamma=0.99$ ", "page_idx": 7}, {"type": "text", "text": "For the upper level, we let $x$ parameterize an additive penalty function $\\tilde{r}_{x}:S\\times A\\to[-0.2,0.0]\\,^{4}$ such that the follower receives a reward of $r+\\tilde{r}_{x}$ , as in the Principal-Agent problem [8]. The goal of the leader is to steer the followers through the cell marked with $+1$ in Figure 1a, denoted by $s^{+1}$ \uff0c while keeping the penalties allocated to states to their minimum. We define $\\overline{r}$ in Equation (7) as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\overline{{r}}_{x,\\xi}(s_{t},a_{t})=\\mathbb{1}_{\\{s_{t}=s^{+1}\\}}-\\beta\\mathbb{1}_{\\{s_{t}=G^{\\xi}\\}}\\sum_{s,a}\\tilde{r}_{x}(s,a),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbb{1}$ is the indicator function and the second term defines the cost associated with implementing the penalties for the lower level. Note that there is a trade-off between the terms in $\\overline{r}$ depending on the context variable $\\xi$ .If $\\xi=2$ , the desired change in the follower's policy can be achieved with small interventions since the shortest path from $S$ to $G^{2}$ is already going through the bottom-left room. When $\\xi=1$ , the leader must completely block the shortest path from $S$ to $G^{1}$ to divert the follower through the desired state. An efficient algorithm for this CB-RL problem therefore must avoid the local optimum of setting $\\tilde{r}=0$ and find the balance between the follower visiting state $s^{+1}$ and implementing too large penalties in the CMDP. ", "page_idx": 7}, {"type": "text", "text": "Figure (1b) depicts the upper-level's objective function over the learning iterations $t$ withhyperparameters $\\lambda=0.001$ and $\\beta=1.0$ . HPGD outperforms both AMD and the Zero-Order algorithms ", "page_idx": 7}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/ef3867c560c9bf64b36bfc5efb44d4f0fa0cc8c2f8f9f3bf66f81f43b476b5bf.jpg", "img_caption": ["Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively. HPGD efficiently steers the lower-level MDP when the task is to reach $G^{1}$ while others are only successful in the case of $G^{2}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Performance over hyperparameters $\\beta$ and $\\lambda$ for the Four Rooms Problem averaged over 10 random seeds with standard errors. Algorithms perform on-par for most hyperparameters while HPGD outperforms others in few. AMD enjoys low variance due to the non-stochastic gradient updates while Zero-Order suffers from the most variation. ", "page_idx": 8}, {"type": "table", "img_path": "W3Dx1TGW3f/tmp/2a332f1000aac4b207e031142ff722c974022d136083669c48f663da3779dfe8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "in this instance in terms of overall performance. The major difference in their performances is that HPGD successfully escapes the local optimum of $\\tilde{r}=0$ after about 5000 steps and assigns all the additive penalty budget to states in the gridworld. On the contrary, AMD and Zero-Order converge to the local optimum of minimizing the implementation penalty term in $\\overline{r}$ . They only utilize $38\\%$ and $26\\%$ of the available budget of $-0.2$ to divert the follower when $\\xi=2$ but neglect the goal state $G^{1}$ ", "page_idx": 8}, {"type": "text", "text": "Figure 2 shows the value of additive penalties $\\tilde{r}$ in the state space with the highest probability paths for the goal states. HPGD successfully blocks the follower when $\\xi=1$ and diverts its shortest path from $S$ to $G^{1}$ along the other rooms, while AMD and Zero-Order fail to assign sufficient penalty to the upper corridor to cause the same effect. All algorithms are successful in ensuring that the shortest path through the bottom-left room is going through the marked state. ", "page_idx": 8}, {"type": "text", "text": "The parameters $\\lambda$ and $\\beta$ were chosen for demonstration purposes to highlight the capability of HPGD to escape local minima, as has been observed for SGD [72]. However, we emphasize that in the majority of the cases, the three algorithms perform equally as shown in Table 3. We provide the figures for the remaining hyperparameters in Appendix F.2.3. The slightly higher performance of AMD and low standard error among initializations is expected since this algorithm calculates the gradient of $f$ deterministically while HPGD and Zero-Order rely on stochastic estimates yielding more variations, especially for the Zero-Order approach. ", "page_idx": 8}, {"type": "text", "text": "5.2  Tax Design for Macroeconomic Models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We test HPGD on a bilevel macroeconomic model evaluating taxation schemes based on [34, 15]. The economic model consists of a finite set of consumption goods $i\\;\\in\\;\\{1,\\ldots,M\\}$ each with price $p_{i}$ .We choose $M=3$ with unit prices. On the lower-level, at each time step $t$ $s_{t}$ denotes the accumulated assets of a household which in turn chooses the number of hours worked $n_{t}$ and consumption $c_{i,t}$ . The environment updates the accumulated assets of the household as $s_{t+1}=$ $s_{t}{+}(1{-}x)w n_{t}{-}\\sum_{i=1}^{M}c_{i,t}{+}\\epsilon$ \u2265=1 Ci,t+e where e is sampled from a normal distribution with mean O and standard deviation $\\varsigma$ . We clip the accumulated assets to the range $[-100,100]$ for numerical stability. The utility of a household is given by $\\begin{array}{r}{u_{t}=\\sigma(s_{t})-\\theta n_{t}^{2}+\\prod_{i=1}^{M}(c_{i,t}/(p(1+y_{i})))^{\\alpha_{i}}}\\end{array}$ where the product-ofconsumption term corresponds to the Cobb-Douglas function [57] and $\\sigma(\\cdot)$ is the value of accumulated assets. We define $\\mathbb{P}_{\\xi}$ as the distribution of households representing different socio-economic groups in the economy. In particular, we define two equal-sized groups with $\\alpha\\,=\\,(0.6,0.3,0.1)$ and $(0.1,0.7,0.2)$ that model their different preferences over the goods in the economy and optimize their consumption behavior using regularized deep Q-learning [26] with $\\lambda=0.2$ . On the upper-level, a tax designer is optimizing the discounted sum of social welfare by setting the income tax $x\\in[0,3]$ and value-added tax $y_{i}\\in[0,3]$ for $i\\in\\{1,\\cdot\\cdot\\cdot,M\\}$ . In each time step $t$ , the social welfare is defined as $\\begin{array}{r}{v_{t}=\\omega(s_{t})+\\sum_{i=1}^{M}c_{i,t}/(1+y_{i})+\\phi\\log(\\sum_{i=1}^{M}c_{i,t}y_{i}/(1+y_{i})+w x n_{t})}\\end{array}$ where $\\omega(\\cdot)$ is the utility of accumulated assets and $\\phi$ is a positive hyperparameter. ", "page_idx": 8}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/e1c4dcb1fe54e0dc4beb263f331b44cb44553d18f6afd63941b9dae78b05c07d.jpg", "img_caption": ["(a) Performance on the Tax Design problem. (b) Tax rates over the outer iterations. HPGD increases income tax HPGD_ quickly learn optimal tax policies quickly and distinguishes VAT rates according to preferences. while Zero-Order takes more iterations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Figure 3a demonstrates that HPGD can successfully optimize the hyperparameters even in continuous complex problems. Benefiting from first-order information, HPGD converges faster than the ZeroOrder algorithm and increases the social-welfare by about $25\\%$ . Additionally, HPGD shows better qualitative results for the optimised tax rates in Figure 3b. HPGD swiftly increases the income tax to improve its objective by increasing tax revenues while sets value-added tax rates according to the preferences of the household groups. Households on average prefer the second good the most, therefore, $y_{2}$ is set low to increase consumption and therefore maximize the second term in $v_{t}$ while the third good is the least preferred for which the tax rate is increased5 since consumption is already low. While Zero-Order shows equivalent performance on Figure 3a to HPGD it fails to distinguish goods when setting value-added tax rates. We defer further details on the implementation, hyperparameter choices, and additional results to Appendix F.3.1. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce CB-RL, a class of stochastic bilevel optimization problems with lower-level contextual MDPs that capture a wide range of important applications, where a leader aims to design environments and incentive structures that align the followers\u2019 policies with the leader's upper-level objective. We propose HPGD, an oracle-based algorithmic framework, and analyze its convergence. Importantly, HPGD works with any existing algorithm that solves the lower-level CMDP to near-optimality, making it suitable in various regimes when the leader can only observe trajectories of followers. Moreover, HPGD is the first provably convergent algorithm in this area, which uses stochastic estimates of the hypergradient. We further propose RT-Q, a more efficient algorithm and study its bias, variance, and cost when the leader can fully control the followers? training. Numerical results further validate the expressiveness of the proposed model and the performance of our algorithm. Future directions include 1) applying HPGD in various real-world applications, 2) studying the setting when the lower-level problem is a game, and 3) studying single-loop algorithms for bilevel reinforcement learning with when the lower-level is just an MDP. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors acknowledge constructive suggestions from Niao He and Sven Seuken. V. Thoma and B. Pasztor are supported by an ETH AI Center Doctoral Fellowship. V. Thoma acknowledges funding from the Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343. This work was supported as a part of NCCR Automation, a National Centre of Competence (or Excellence) in Research, funded by the SNSF (grant number 51NF40_225155). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1):165-214, 2023. (Cited on page 5.)   \n[2] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26-38, 2017. (Cited on page 1.)   \n[3]  Kavosh Asadi and Michael L. Littman. An alternative softmax operator for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 243-252, Sydney, NSW, Australia, 2017. JMLR.org. (Cited on page 49.)   \n[4]  Jan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti. The good shepherd: An oracle agent for mechanism design. In ICLR 2022 Workshop on Gamification and Multiagent Solutions, 2022. (Cited on page 19.)   \n[5]  Jonathan F Bard. Practical bilevel optimization: algorithms and applications, volume 30. Springer Science & Business Media, 2013. (Cited on page 19.)   \n[6]  Tobias Baumann, Thore Graepel, and John Shawe-Taylor. Adaptive mechanism design: Learning to promote cooperation. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-7. IEEE, 2020. (Cited on page 19.)   \n[7] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023. (Cited on page 21.)   \n[8]  Omer Ben-Porat, Yishay Mansour, Michal Moshkovitz, and Boaz Taitler. Principal-agent reward shaping in mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 9502-9510, 2024. (Cited on pages 2, 8, 20, and 21.)   \n[9]  Mathieu Blondel and Vincent Roulet. The elements of differentiable programming, 2024. (Cited on page 46.)   \n[10] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. (Cited on page 8.)   \n[11] Seth Brown, Saumya Sinha, and Andrew J. Schaefer. Markov decision process design: A framework for integrating strategic and operational decisions. Operations Research Letters, 54:107090, May 2024. (Cited on page 19.)   \n[12] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. Operations Research, 70(4):2563- 2578, July 2022. (Cited on page 52.)   \n[13] Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha, Mengdi Wang, and Furong Huang. PARL: A unified framework for policy alignment in reinforcement learning from human feedback. In The Twelth International Conference on Learning Representations, 2024. (Cited on pages 2, 3, 6, 19, and 20.)   \n[14]  Arthur Charpentier, Romuald Elie, and Carl Remlinger. Reinforcement learning in economics and finance. Computational Economics, pages 1-38, 2021. (Cited on page 1.)   \n[15] Siyu Chen, Donglin Yang, Jiayang Li, Senmiao Wang, Zhuoran Yang, and Zhaoran Wang. Adaptive model design for markov decision process. In International Conference on Machine Learning, pages 3679-3700. PMLR, 2022. (Cited on pages 2, 3, 4, 5, 6, 7, 9, 17, 19, 20, 21, 46, and 52.)   \n[16] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods forbilevel problems.Advances in Neural Information Processing Systems, 34:25294-25307, 2021. (Cited on page 19.)   \n[17] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. (Cited on page 20.)   \n[18] Michael Curry, Vinzenz Thoma, Darshan Chakrabarti, Stephen McAleer, Christian Kroer, Tuomas Sandholm, Niao He, and Sven Seuken. Automated design of affine maximizer mechanisms in dynamic settings. Proceedings of the AAAl Conference on Artificial Intelligence, 38(9):9626-9635, March 2024. (Cited on pages 2, 6, 19, and 21.)   \n[19] Michael Curry, Alexander Trott, Soham Phade, Yu Bai, and Stephan Zheng. Learning solutions in large economic networks using deep multi-agent reinforcement learning. In Proceedings of the 2023International Conference onAutonomous Agents and Multiagent Systems,AAMAS '23, page 2760-2762, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems. (Cited on pages 1, 2, and 19.)   \n[20] Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. SBEED: Convergent reinforcement learning with nonlinear function approximation. In Jennifer Dy and Andreas Krause,editors,Proceedingsof the 35th International Conference onMachine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1125-1134. PMLR, 10-15 Jul 2018. (Cited on pages 19 and 48.)   \n[21]  Stephan Dempe. Foundations of bilevel programming. Springer Science & Business Media, 2002. (Cited on page 19.)   \n[22] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design.In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. (Cited on page 19.)   \n[23] Manfred Diaz, Charlie Gauthier, Glen Berseth, and Liam Paull. Generalization games for reinforcement learning. In ICLR Workshop on Agent Learning in Open-Endedness, 2022. (Cited on page 19.)   \n[24]  Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study In Hal Daume IIl and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 1i9 of Proceedings of Machine Learning Research, pages 3133-3144. PMLR, 13-18 Jul 2020. (Cited on pages and 19.)   \n[25] Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAs '18, page 122-130, Richland, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems. (Cited on page 19.)   \n[26] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In International Conference on Machine Learning, pages 2160-2169. PMLR, 2019. (Cited on pages 4, 10, and 19.)   \n[27] Matthias Gerstgrasser and David C. Parkes. Oracles & followers: Stackelberg equilibria in deep multi-agent reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "40th International Conference on Machine Learning, volume 202 of Proceedings of Machine ", "page_idx": 12}, {"type": "text", "text": "Learning Research, pages 11213-11236. PMLR, 23-29 Jul 2023. (Cited on pages 2 and 19.)   \n[28]  Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv: 1802.02246, 2018. (Cited on pages 4, 5, 19, and 34.)   \n[29] Michael B. Giles. Multilevel monte carlo methods. Acta Numerica, 24:259-328, 2015. (Cited on pages 3 and 7.)   \n[30] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 1352-1361, Sydney, NSW, Australia, 2017. JMLR.org. (Cited on page 49.)   \n[31]  Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca D. Dragan. Inverse reward design.In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6768-6777, Red Hook, NY, USA, 2017. Curran Associates Inc. (Cited on page 19.)   \n[32] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015. (Cited on pages and 3.)   \n[33] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in finance. Mathematical Finance, 33(3):437-503, 2023. (Cited on page 1.)   \n[34]  Edward Hill, Marco Bardoscia, and Arthur Turrell. Solving heterogeneous general equilibrium economic models with deep reinforcement learning. arXiv preprint arXiv:2103.16977, 2021. (Cited on pages 1, 2, 7, 9, and 20.)   \n[35] Yifan Hu, Xin Chen, and Niao He. Sample complexity of sample average approximation for conditional stochastic optimization. SIAM Journal on Optimization, 30(3):2103-2133, 2020. (Cited on page 34.)   \n[36]  Yifan Hu, Xin Chen, and Niao He. On the bias-variance-cost tradeoff of stochastic optimization. In Advances in Neural Information Processing Systems, volume 34, pages 22119-22131, 2021. (Cited on page 7.)   \n[37] Yifan Hu, Jie Wang, Yao Xie, Andreas Krause, and Daniel Kuhn. Contextual stochastic bilevel optimization. Advances in Neural Information Processing Systems, 36, 2024. (Cited on pages 2, 3,7, 19, and 27.)   \n[38] Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic first-order methods for conditional stochastic optimization and applications in meta learning. Advances in Neural Information Processing Systems, 33:2759-2770, 2020. (Cited on page 34.)   \n[39] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan. Learning to uilize shaping rewards: A new approach of reward shaping. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15931-15941. Curran Associates, Inc., 2020. (Cited on page 19.)   \n[40] Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, and Niao He. Learning to steer markovian agents under model uncertainty, 2024. (Cited on page 19.)   \n[41] Dima Ivanov, Paul Duitting, Inbal Talgam-Cohen, Tonghan Wang, and David C. Parkes. Principalagent reinforcement learning, 2024. (Cited on pages 2, 20, and 21.)   \n[42]  Sham Kakade. A natural policy gradient. In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS'01, page 1531-1538, Cambridge, MA, USA, 2001. MIT Press. (Cited on page 52.)   \n[43] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in neural information processing systems, 34:30271-30283, 2021. (Cited on page 19.)   \n[4]  Jeongyeol Kwon, Dohyun Kwon, and Hanbaek Lyu. On the complexity of first-order methods in stochastic bilevel optimization. arXiv preprint arXiv:2402.07i01, 2024. (Cited on page 19.)   \n[45] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method for stochastic bilevel optimization. In International Conference on Machine Learning, pages 18083-18113. PMLR, 2023. (Cited on page 19.)   \n[46]  Joshua Letchford and Yevgeniy Vorobeychik. Optimal interdiction of attack plans. In AAMAS, pages 199-206. Citeseer, 2013. (Cited on page 2.)   \n[47] Boyi Liu, Jiayang Li, Zhuoran Yang, Hoi-To Wai, Mingyi Hong, Yu Nie, and Zhaoran Wang. Inducing equilibria via incentives: Simultaneous design-and-play ensures global convergence. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 29001-29013. Curran Associates, Inc., 2022. (Cited on pages 4 and 19.)   \n[48]  Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In Marina Meila and Tong Zhang. editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6882-6892. PMLR, 18-24 Jul 2021. (Cied on page 19.)   \n[49] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In Hal Daume III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6820-6829. PMLR, 13-18 Jul 2020. (Cited on pages 19, 43, 48, 51, and 52.)   \n[50]  Alberto Maria Metelli, Mirco Mutti, and Marcello Restelli. Configurable markov decision processes. In International Conference on Machine Learning, pages 3491-3500. PMLR, 2018. (Cited on pages 2, 3, and 19.)   \n[51] Ofr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 2772-2782, Red Hook, NY, USA, 2017. Curran Associates Inc. (Cited on pages 4, 48, and 50.)   \n[52] ATD Perera and Parameswaran Kamalaruban. Applications of reinforcement learning in energy systems. Renewable and Sustainable Energy Reviews, 137:110618, 2021. (Cited on page 1.)   \n[53] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. (Cited on page 1.)   \n[54] Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation and $q$ -learning. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference onLearning Theory,volume 125 of Proceedings of MachineLearning Research, pages 3185-3205. PMLR, 09-12 Jul 2020. (Cited on pages 49 and 50.)   \n[55]  Giorgia Ramoni, AeroMaria Mtelli, AlesandrConcetti andMarcell Restelli. Lain in non-cooperative configurable markov decision processes. Advances in Neural Information Processing Systems, 34:22808-22821, 2021. (Cited on pages 2 and 19.)   \n[56] R.Tyrell Rockafellar and Roger J. B. Wets. Variational Analysis. Springer Berlin Heidelberg, 1998. (Cited on page 46.)   \n[57]  Aaron Roth, Jonathan Ullman, and Zhiwei Steven Wu. Watch and learn: Optimizing from revealed preferences feedback. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 949-962, 2016. (Cited on page 10.)   \n[58] Han Shen, Zhuoran Yang, and Tianyi Chen. Principled penalty-based methods for bilevel reinforcement learning and rlhf. arXiv preprint arXiv:2402.06886, 2024. (Cited on pages 2, 3, 6, 19, and 20.)   \n[59] Laixi Shi, Eric Mazumdar, Yuejie Chi, and Adam Wierman. Sample-efficient robust multiagent reinforcement learning in the face of environmental uncertainty. arXiv preprint arXiv:2404.18909, 2024. (Cited on page 19.)   \n[60] Arunesh Sinha, Fei Fang, Bo An, Christopher Kiekintveld, and Milind Tambe. Stackelberg security games: looking beyond a decade of success. In Proceedings of the 27th International Joint Conference on Artifcial Intelligence, pages 5494-5501, 2018. (Cited on page 2.)   \n[61] Heinrich von Stackelberg. Marktform und Gleichgewicht. Klassiker der Nationalokonomie. Verlag Wirtschaftund Finanzen, Disseldorf, 1934. (Cited on page 19.)   \n[62] Richard S Suton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. (Cited on page 1.)   \n[63] Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe reinforcement learning via curriculum induction. Advances in Neural Information Processing Systems, 33:12151-12162, 2020. (Cited on page 19.)   \n[64] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Remi Munos, and Matthieu Geist. Leverage the average: an analysis of kl regularization in reinforcement learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc. (Cited on page 21.)   \n[65] Jing Wang, Meichen Song, Feng Gao, Boyi Liu, Zhaoran Wang, and Yi Wu. Differentiable arbitrating in zero-sum markov games. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS \\*23, page 1034-1043, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems. (Cited on pages 2 and 19.)   \n[66] Kai Wang, Lily Xu, Andrew Perrault, Michael K. Reiter, and Milind Tambe. Coordinating followers to reach better equilbria: End-to-nd gradient descent for stackelberg games. roceedings of the AAAI Conference on Artificial Intelligence, 36(5):5219-5227, Jun. 2022. (Cited on page 19.)   \n[67] Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao.Deepreinforcement learning: A survey.IEEE Transactions onNeural Networks and Learning Systems, 2022. (Cited on page 1.)   \n[68]  Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992. (Cited on page 51.)   \n[69] Jibang Wu, Siyu Chen, Mengdi Wang, Huazheng Wang, and Haifeng Xu. Contractual reinforcement learning: Pulling arms with invisible hands, 2024. (Cited on pages 2, 20, and 21.)   \n[70] Shuo Wu, Haoxiang Ma, Jie Fu, and Shuo Han. Robust reward design for markov decision processes, 2024. (Cited on page 19.)   \n[71] Li Xia, Luyao Zhang, and Peter W. Glynn. Risk-sensitive markov decision processes with long-run cvar criterion. Production and Operations Management, 32(12):4049-4067, December 2023. (Cited on page 19.)   \n[72]  Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. In International Conference on Learning Representations, 2021. (Cited on page 9.)   \n[73] Chang Yang, Yuiyu Wang, Xinrun Wang, and Zhen Wang. A game-theoretic perspective of generalization in reinforcement learning. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022. (Cited on page 19.)   \n[74] Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and Hongyuan Zha. Learning to incentivize other learning agents. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15208-15219. Curran Associates, Inc., 2020. (Cited on page 19.)   \n[75] Yan Yang, Bin Gao, and Ya xiang Yuan. Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity, 2024. (Cited on pages 3 and 19.)   \n[76]  Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1-36, 2021. (Cited on page 1.)   \n[77] Guanghui Yu and Chien-Ju Ho. Environment design for biased decision makers. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 592-598. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track. (Cited on page 20.)   \n[78] Haifeng Zhang, Jun Wang, Zhiming Zhou, Weinan Zhang, Ying Wen, Yong Yu, and Wenxin Li. Learning to design games: strategic environments in reinforcement learning. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, ICAI'18, page 3068-3074. AAAI Press, 2018. (Cited on pages 2, 5, and 19.)   \n[79]  Haoqi Zhang and David Parkes.  Value-based policy teaching with active indirect elicitation. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 1, AAAI'08, page 208-214. AAAI Press, 2008. (Cited on page 20.)   \n[80]  Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586-3612, January 2020. (Cited on page 46.)   \n[81] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321-384, 2021. (Cited on page 19.)   \n[82] Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. Science Advances, 8(18):eabk2607, 2022. (Cited on pages 2, 19, and 20.) ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1 Introduction ", "page_idx": 16}, {"type": "text", "text": "2 Problem Formulation 3 ", "page_idx": 16}, {"type": "text", "text": "3 Hyper Policy Gradient Descent Algorithm for CB-RL ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "3.1 Hypergradient derivation   \n3.2 HPGD Algorithm and Convergence Analysis 5   \n3.3 Upper-Level Discounted Reward Objective 6 ", "page_idx": 16}, {"type": "text", "text": "4 Accelerated HPGD with Full Lower-Level Access ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "5  Numerical Experiments ", "page_idx": 16}, {"type": "text", "text": "5.1Four-Rooms Environment 8   \n5.2Tax Design for Macroeconomic Models ", "page_idx": 16}, {"type": "text", "text": "6 Conclusion 10 ", "page_idx": 16}, {"type": "text", "text": "A Related Work 19 ", "page_idx": 16}, {"type": "text", "text": "B Applications: RLHF, Tax Design, Reward Shaping, Contract Design, and Dynamic Mechanism Design 19 ", "page_idx": 16}, {"type": "text", "text": "C  Frequently-Used Notation 22 ", "page_idx": 16}, {"type": "text", "text": "D Algorithms 22 ", "page_idx": 16}, {"type": "text", "text": "E Proofs 24 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Overview 24   \nE.2 Proof of Theorem 1 24   \nE.3 Proof of Theorem 2 25   \nE.4 Proof of Theorem 3 27   \nE.5 Proof of Theorem 4 33   \nE.6 Proof of Proposition 1 : 44   \nE.7 Auxiliary Results 46   \nE.8 Convergence Results for Popular RL Algorithms 48 ", "page_idx": 16}, {"type": "text", "text": "F   Implementation Details 52 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "F.1 Baseline Algorithms 52   \nF.1.1 Adaptive Model Design [15] 52   \nF.1.2 Zero-Order Algorithm 52   \nF.2 Four Rooms . . 53   \nF.2.1 Implementation Details 53   \nF.2.2 Hyperparameters 53   \nF.2.3 Additional Figures 54 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "F.3 Tax Design for Macroeconomic Models 59 ", "page_idx": 17}, {"type": "text", "text": "F.3.1 Implementation Details . 59   \nF.3.2 Effects of lower-level regularization 59 ", "page_idx": 17}, {"type": "text", "text": "F.4  Computational Costs 59 ", "page_idx": 17}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Stochastic bilevel optimization has been extensively explored in the literature [21, 5]. In recent years, there is a pivotal shift to non-asymptotic analysis of stochastic gradient methods [28, 16, 43, 45, 44, 48]. [37] propose contextual stochastic bilevel optimization where the lower level solves a static contextual optimization. Our work generalizes to the lower level solving a contextual MDP. This poses unique challenges in terms of hypergradient estimation and sample generation. Comparing to bilevel optimization, leveraging the special structure of CB-RL, we avoid Hessian and Jacobian estimation of the lower-level MDP when computing the hyper policy gradient, which is crucial for scalability. ", "page_idx": 18}, {"type": "text", "text": "Configurable MDP [5O] is an extension of a traditional MDP allowing external parameters or settings to be adjusted by the decision-maker, often referred to as the configurator. Only recently some works studied the case where the configurator has a different objective than the agent [55]. However, that work assumes access to a finite number of parameters that the configurator can control, while our model goes beyond this assumption. In addition, our model captures the variability and uncertainty that the agent could face in the same configuration environment. ", "page_idx": 18}, {"type": "text", "text": "Stackelberg games are a game theoretic framework, where a leader takes actions to which one or multiple followers choose the best response [61]. Several existing lines of work have studied solving variants of Stackelberg games. Examples include Stackelberg equilibrium solvers [24, 27], opponent shaping [25, 74], mathematical programs with equilibrium constraints [47, 65, 66], inducing cooperation [6, 4], steering economic simulations [19, 82]. These works are either too general with limited implications for our problem or consider entirely distinct settings. ", "page_idx": 18}, {"type": "text", "text": "Multi-agent RL (MARL) studies multiple agents interacting in a joint environment, i.e., their actions together determine the next state [81, 59]. In CB-RL the lower level CMDPs can be seen as a special instance of MARL where the interactions of the followers are restricted to jointly influencing the decision of the leader. ", "page_idx": 18}, {"type": "text", "text": "Bilevel RL studies how to design additional rewards or change the underlying MDP to achieve desirable learning outcomes. Many applications are formulated as bilevel RL, such as environment design for generalization [22, 23, 73], reward shaping [31, 39, 40, 70], safe reinforcement learning [63], optmizing conditional value at risk[71], and model design [15, 78, 11]. Previous work on bilevel RL considers a special case of our setting when there is only one lower-level MDP [15, 13, 58, 75]. In particular, [15] focus on the case when the leader has control on the follower's training procedure. [58] further extend from one single lower-level MDP to a lower-level min-max game. [13] and the concurrent work of [75] focus on the case when the leader can only influence the reward of the MDP. ", "page_idx": 18}, {"type": "text", "text": "The introduction of the context makes CB-RL harder to solve as there can be many followers, each with its own preferences, and their best response policies change even for the same leader decision $x$ when facing different contextual uncertainties. Multiple followers and additional side information are very common, which highlights the practical relevance of our work. In addition, the algorithms in the aforementioned works focus on deterministic updates on the upper and lower level decisions, i.e., assuming access to the full hypergradient and performing exact policy gradient/value iteration, which is both computationally hard and not feasible for large-scale practical applications. To the best of our knowledge, we are the first to provide a convergence analysis for the stochastic case, when the hypergradient is estimated from samples and the lower level uses a stochastic update rule. ", "page_idx": 18}, {"type": "text", "text": "B Applications: RLHF, Tax Design, Reward Shaping, Contract Design, and Dynamic Mechanism Design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We list several applications of CB-RL below. For a clearer exposition, we omit the entropy regularization term at the lower level. However, we stress that some of the referenced works explicit use entropy regularization [18, 15] or make overlapping assumptions such as unique optimal policies [13]. Additionally, for problems without explicit entropy regularization we refer to [15, 49, 20, 26] who have shown that entropy-regularized RL approximates the unregularized problem as $\\lambda\\to0$ both in the upper and lower level. ", "page_idx": 18}, {"type": "text", "text": "Reinforcement Learning from human feedback (RLHF) considers the setting where an agent tries to learn a task from human feedback. The difficulty stems from the fact that the human feedback is given as preferences over two possible trajectories and not directly as a reward [17]. The feedback is of the form $\\{\\tau_{0},\\tau_{1},l\\}$ where $\\tau_{0},\\tau_{1}$ are two trajectories and $l\\in0,1$ indicates whether $\\tau_{0}$ is preferred over $\\tau_{1}$ or vice versa. It has been shown that this problem can be framed as CB-RL, where the upperlevel tries to learn rewards that minimize the cross-entropy loss, between the actual and predicted labels, using the Bradley-Terry Model and the lower level finds the optimal policy with respect to that reward function [13, 58], ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x}{\\mathrm{max}}\\,\\mathbb{E}_{\\tau_{0},\\tau_{1}\\sim\\mathcal{D}(\\pi_{x}^{*}),l}\\left[(1-l)\\log\\mathbb{P}\\left(\\tau_{0}\\succ\\tau_{1}|r_{x}\\right)+l\\log\\mathbb{P}\\left(\\tau_{1}\\succ\\tau_{0}|r_{x}\\right)\\right]}\\\\ &{\\qquad\\mathrm{s.t.}\\;\\pi_{x}^{*}(\\cdot)=\\underset{\\pi}{\\mathrm{argmax}}\\,\\mathbb{E}\\left[\\displaystyle\\sum_{t=0}^{H}\\gamma^{t}r_{x}(s_{t},a_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here H is the time horizon. D is the sampling distribution of trajectories using T\\*.g and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\tau_{0}\\succ\\tau_{1}|r_{x}\\right)=\\frac{\\exp\\sum_{t=0}^{H}\\gamma^{t}r_{x}(s_{t}^{0},a_{t}^{0})}{\\exp\\sum_{t=0}^{H}\\gamma^{t}r_{x}(s_{t}^{0},a_{t}^{0})+\\exp\\sum_{t=0}^{H}\\gamma^{t}r_{x}(s_{t}^{1},a_{t}^{1})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note in the case of standard RLHF the context becomes trivial. ", "page_idx": 19}, {"type": "text", "text": "Tax Design for Macroeconomic Modeling considers a public entity setting tax rates and representative households responding optimally by balancing their short-term utility of consumption and long-term wealth accumulation [34, 15, 82]. A potential formulation of this problem as CB-RL is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{x},\\boldsymbol{y}}\\mathbb{E}_{\\xi}\\left[\\phi(\\boldsymbol{x},\\boldsymbol{y},\\pi_{x,y,\\xi}^{*},\\xi)\\right]\\mathrm{~s.t.~}\\pi_{x,y,\\xi}^{*}(\\cdot)=\\operatorname*{argmax}_{\\pi}\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(r_{\\xi}^{W}(s_{t})+r_{y,\\xi}^{C}(\\pi(s_{t}))\\Big)\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $x$ and $y$ denote the income and value-added tax rates, respectively, and $\\phi$ defines the social welfare objective of the leader. The state $s_{t}$ defines the wealth of a household while their actions decide their working hours and consumption in each time step. The reward function $r_{\\xi}^{W}$ and $r_{y,\\xi}^{C}$ define the households\u2019 utility functions for wealth and consumption, respectively. The value-added tax rate $y$ $r_{y,\\xi}^{C}$ While th intometax $x$ changesthetranstion kernel modeling wealth accumulation, i., $s_{t+1}\\sim P_{x,\\xi}(\\cdot;s_{t},a_{t})$ represents the preferences of the households over several consumption goods and their productivity in this problem formulation. ", "page_idx": 19}, {"type": "text", "text": "Population Principal-Agent Reward Shaping considers a principal aiming to craft a non-negative bonus reward function $\\bar{r_{x}^{B}}$ , parameterized by $x$ , to motivate an agent [8, 7, 79]. Commonly, a principal faces multiple agents that form a distribution. Each agent has its own individual reward function $r_{\\xi}$ . This scenario, termed population principal-agent reward shaping is captured by our CB-RL framework. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x}\\mathbb{E}_{\\xi}^{\\pi_{x,\\xi}^{*}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\bar{r}(s_{t},\\pi_{x,\\xi}^{*}(s_{t}))\\right]\\mathrm{~s.t.~}\\pi_{x,\\xi}^{*}(\\cdot)=\\operatornamewithlimits{a r g m a x}_{\\pi}\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(r_{\\xi}(s_{t},\\pi(s_{t}))+r_{x}^{B}(s_{t},\\pi(s_{t}))\\Big)\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $\\mathbb{E}_{\\xi}$ denotes the xpectation over the distribution of agents and the trajectories. The policy $\\pi_{x,\\xi}^{*}(\\cdot)$ is the optimal response of the $\\xi$ -th agent to the composite reward function $r_{\\xi}+r_{x}^{B}$ . The principal's reward is $\\overline{{r}}(s_{t},a_{t})$ when the agent visits the state action pair $\\left({{s}_{t}},{{a}_{t}}\\right)$ ", "page_idx": 19}, {"type": "text", "text": "Dynamic Contract Design studied by [41, 69] is similar to the above reward shaping. Generalizing it to a contextual setting, the problem consists of an agent of type $\\xi$ that incurs a cost $c_{\\xi}(s,a)$ for taking action $a$ in state $s$ . The principal in turn gets a reward $\\boldsymbol{r}(\\boldsymbol{s},\\boldsymbol{s}^{\\prime})$ for transitioning from state $s$ to $s^{\\prime}$ , but cannot observe the agent's action. It can however offer contracts $x(s,s^{\\prime})$ that get paid if the MDP transitions from state $s$ to $s^{\\prime}$ . These contracts are positive payments by the principal and are thus added to the lower-level objective and substracted from the upper-level objective. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{x}\\mathbb E_{\\xi,\\pi_{x,\\xi}^{*}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(r\\big(s_{t},s_{t+1}\\big)-x\\big(s_{t},s_{t+1}\\big)\\right)\\right]}\\\\ &{\\qquad\\mathrm{~s.t.~}\\pi_{x,\\xi}^{*}(\\cdot)=\\displaystyle\\operatornamewithlimits{a r g m a x}_{\\pi}\\mathbb E_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\Big(x\\big(s_{t},s_{t+1}\\big)-c_{\\xi}\\big(s_{t},a_{t}\\big)\\Big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Dynamic Mechanism Design considers the problem of a mechanism designer controlling an MDP for a group of $n$ bidders, who get a reward based on the observed trajectories [18]. The context $\\xi$ parameterizes the bidders\u2019 reward functions $r_{i,\\xi}$ , which they report to the mechanism designer. The latter wants to learn a policy for the MDP and charge payments to the bidders, to ensure eliciting truthful reward reports and also maximizize an objective $\\mathcal{L}$ , e.g. the total sum of payments. In this setting, [18] propose to search for such a mechanism within the class of affine maximizers, as they guarantee truthful reports by all bidders. In these mechanisms, a set of agent-dependent weights $x_{w,i}$ and state-action dependent boosts $x_{b}$ is chosen by the mechanism designer, then a policy $\\pi$ is learned to maximize the corresponding affinely transformed social welfare $\\begin{array}{r}{\\mathbb{E}_{s_{t},a_{t}\\sim\\pi}\\left[\\sum_{t=0}^{T}\\left(\\sum_{i=1}^{n}x_{w,i}r_{i,\\xi}(s_{t},a_{t})\\right)+x_{b}(s_{t},a_{t})\\right]}\\end{array}$ and bidders are charged for the learned policy depending on their reported reward functions. Searching for the optimal mechanism parameters $x_{w,i}$ and $x_{b}$ to maximize $\\mathcal{L}$ in expectation over $\\xi$ , subject to the constraint that the mechanism's policy maximizes affine social welfare can be formulated as CB-RL. In this case $x_{w,i}$ and $x_{b}$ are the decision variable, the context parameterizes the bidders\u2019 preferences and the affinely transformed social welfare at each time step is the reward function of the lower-level MDP, as shown below: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{x_{w},x_{b}}\\mathbb{E}_{\\xi}\\big[\\mathcal{L}\\left(\\pi_{\\xi,x_{w},x_{b}}^{*},x_{w},x_{b}\\right)\\big]}\\\\ &{\\qquad\\mathrm{~s.t.~}\\pi_{\\xi,x_{w},x_{b}}^{*}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{s_{t},a_{t}\\sim\\pi}\\left[\\displaystyle\\sum_{t=0}^{T}\\left(\\sum_{i=1}^{n}x_{w,i}r_{i,\\xi}\\big(s_{t},a_{t}\\big)\\right)+x_{b}\\big(s_{t},a_{t}\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note, that all previous works in these application areas have either focused on the setting with a single representative follower [8, 15, 41, 69] or presented a problem-specific algorithm that cannot capture our CB-RL framework in its full generality [8, 18]. ", "page_idx": 20}, {"type": "text", "text": "The CB-RL framework is also related to Meta reinforcement learning (Meta RL), that aims to leverage the similarity of several RL tasks to learn common knowledge and use it on new unseen tasks [7]. One way to formulate Meta RL problems is to find a common regularization policy $\\tilde{\\pi}$ for multipletasks. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{nax}_{\\bar{\\pi}}\\mathbb{E}_{\\xi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{\\xi}(s_{t},\\pi_{\\bar{\\pi},\\xi}^{*}(s_{t}))\\right]\\mathrm{s.t.}\\,\\pi_{\\bar{\\pi},\\xi}^{*}(\\cdot)=\\operatorname*{argmax}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{\\xi}(s_{t},\\pi(s_{t}))-\\frac{\\lambda}{2}K L(\\pi(s_{t})||\\tilde{\\pi}(s_{t}))\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\xi$ represents the distribution of multiple RL tasks and $r_{\\xi}$ is the reward for the task indexed by $\\xi$ . Although the upper-level regularizer is different from entropy-regularization it still results in a unique softmax policy of the form $\\begin{array}{r}{\\pi_{s_{\\xi}}^{*}(s,a)\\propto\\exp\\left(\\frac{Q_{s_{\\xi},\\pi}(s,a)}{\\lambda}+\\log(\\tilde{\\pi}(s,a))\\right)}\\end{array}$ [64]. Moreover, in Meta RL the leader does not change the transitions or rewards, but the target policy $\\tilde{\\pi}$ , that goes into the KL regularization term of the followers. ", "page_idx": 20}, {"type": "table", "img_path": "W3Dx1TGW3f/tmp/e96d20c8fb47fa371f0094e7df7fac8938812803f5137b1e0b8a68055ffa1c35.jpg", "table_caption": ["Table 4: Table of notation used in the paper. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We give the pseudocode to certain algorithms/routines/procedures mentioned in the main text. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{Q},T_{V}\\sim\\mathrm{Geo}(1-\\gamma),T_{Q}^{\\prime},T_{V}^{\\prime}\\sim\\mathrm{Geo}(1-\\gamma^{0.5})}\\\\ &{\\tau_{Q}\\leftarrow\\mathrm{SampleTrajectory}(o,\\mathrm{start}=(s,a),\\mathbf{length}=T_{Q}+T_{Q}^{\\prime}+1)}\\\\ &{\\tau_{V}\\leftarrow\\mathrm{SampleTrajectory}(o,\\mathrm{start}=s,\\mathbf{length}=T_{V}+T_{V}^{\\prime}+1)}\\\\ &{\\frac{d}{d x}\\bar{Q}(s,a)\\leftarrow\\sum_{t=0}^{T_{Q}}\\frac{d}{d x}r(s_{t}^{\\tau_{Q}},a_{t}^{\\tau_{Q}})+}\\\\ &{\\frac{\\gamma}{1-\\gamma}\\frac{d}{d x}\\log P\\left(s_{T_{Q}+1}^{\\tau_{Q}};s_{T_{Q}}^{\\tau_{Q}},a_{T_{Q}}^{\\tau_{Q}}\\right)\\sum_{t=T_{Q}+1}^{T_{Q}+T_{Q}^{\\prime}+1}\\gamma^{(t-T_{Q}-1)/2}\\left(r(s_{t}^{\\tau_{Q}},a_{t}^{\\tau_{Q}})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)}\\\\ &{\\partial_{\\alpha}V(s)\\leftarrow\\sum_{t=0}^{T_{V}}\\partial_{x}r(s_{t}^{\\tau_{V}},a_{T_{V}}^{\\tau_{V}})+}\\\\ &{\\frac{\\gamma}{1-\\gamma}\\partial_{x}\\log P\\left(s_{T_{V}+1}^{\\tau_{V}};s_{T_{V}}^{\\tau_{V}},a_{T_{V}}^{\\tau_{V}}\\right)\\sum_{t=T_{V}+1}^{T_{V}+T_{V}^{\\prime}+1}\\gamma^{(t-T_{V}-1)/2}\\left(r(s_{t}^{\\tau_{V}},a_{t}^{\\tau_{V}})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)}\\\\ &{\\mathrm{Output:}\\ \\partial_{x}\\bar{A}(s,a)\\leftarrow\\partial_{x}\\bar{Q}(s,a)-\\partial_{x}\\bar{V}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 Soft Value Iteration ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: Input: Number of iterations $T$   \n2: Result: Approximation $V_{\\lambda}\\approx V_{\\lambda}^{*}$ , policy $\\pi_{\\lambda}\\approx\\pi_{\\lambda}^{*}$   \n3: Initialize $V_{\\lambda}=0$   \n4: for $t=0$ to $T$ do   \n5: for $s\\in S$ do   \n6: for $a\\in A$ do   \n7: $Q_{\\lambda}(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}|s,a}\\left[V_{\\lambda}(s^{\\prime})\\right]$   \n8: end for   \n9: $\\begin{array}{r l}{\\lefteqn{V_{\\mathrm{new},\\lambda}(s)=\\lambda\\log\\left(\\sum_{a\\in\\mathcal{A}}\\exp\\left(\\frac{Q_{\\lambda}(s,a)}{\\lambda}\\right)\\right)}\\quad}&{{}}\\end{array}$   \n10: end for   \n11: set $V_{\\lambda}:=V_{\\mathrm{new},\\lambda}$   \n12: end for   \n13: \u03c0\u2190 $\\begin{array}{r}{\\pi_{\\lambda}^{o}\\leftarrow\\frac{\\exp(Q_{\\lambda}(s,a)/\\lambda)}{\\sum_{a}\\exp(Q_{\\lambda}(s,a)/\\lambda)}}\\end{array}$   \n14: return $V_{\\lambda}$ and $\\pi_{\\lambda}^{o}$ ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{Algorithm\\4\\SoftQ1earning}(T,\\pi_{B},\\{\\alpha_{t}\\}_{t\\geq0})}\\end{array}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: Input: Number of iterations $T$ Behavioural Policy $\\pi_{B}$ , Stepsizes $\\{\\alpha_{t}\\}_{t\\ge0}$   \n2: Result: Approximation $Q_{\\lambda}\\approx Q_{\\lambda}^{*}$ , policy $\\pi_{\\lambda}\\approx\\pi_{\\lambda}^{*}$   \n3: Initialize $Q_{\\lambda}=0$   \n4: Initialise $s_{0}$   \n5: for $t=0$ to $T$ do   \n6: Sample $a\\sim\\pi_{B}(\\cdot;s_{t})$   \n7: Observe next reward $r(s_{t},a)$ and state $s_{t+1}\\sim P(\\cdot|s_{t},a)$   \n8: $\\begin{array}{r}{Q_{\\lambda}(s_{t},a)=Q_{\\lambda}(s_{t},a)+\\alpha_{t}\\left(r(s_{t},a)+\\gamma\\lambda\\log\\left(\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q_{\\lambda}(s_{t+1},a^{\\prime})}{\\lambda}\\right)\\right)\\right)}\\end{array}$   \n9: end for   \n10: $\\begin{array}{r}{\\pi_{\\lambda}^{o}(a;s)\\leftarrow\\frac{\\exp(Q_{\\lambda}(a|s)/\\lambda)}{\\sum_{a^{\\prime}}\\exp(Q_{\\lambda}(s,a^{\\prime})/\\lambda)}}\\end{array}$   \n11: return Q> and \u03c0? ", "page_idx": 22}, {"type": "text", "text": "Input: $\\xi,x$ initial distribution $\\mu_{x,\\xi}$ , oracle $o$   \n$T,\\sim\\operatorname{Geo}(1-\\gamma)$ \uff0c $T^{\\prime}\\sim\\mathrm{Geo}(1-\\gamma^{0.5})$   \n$\\begin{array}{r l}&{\\underset{\\left(S_{0},a_{0},\\ldots,s_{T}+T^{\\prime},a_{T+T^{\\prime}}\\right)\\leftarrow}{\\sim}\\underset{s_{T+T}}{\\sim}\\underset{s_{T+\\tau}}{\\sim}\\underset{s_{T}}{\\sim}\\underset{s_{T}}{\\sim}\\int\\underset{s_{T}}{\\sim}\\mathrm{sampl}\\mathrm{erajectory}(o,\\mathrm{start}=\\mu_{x,\\xi},1\\mathrm{ength}=T+T^{\\prime})}\\\\ &{\\underset{A_{\\lambda,x,\\xi}}{\\sim}(s_{T},a_{T})\\leftarrow\\mathrm{GradientEstimattor}(\\xi,x,s_{T},a_{T},o)}\\\\ &{\\underset{\\left(d x=0\\right)}{\\overset{r}{\\sum}}\\left(\\sum_{t=0}^{T}\\frac{d}{d x}\\overline{r}\\!\\left(s_{t},a_{t}\\right)\\right)+\\frac{1}{\\lambda(1-\\gamma)}\\partial_{x}\\overline{A}_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s_{T},a_{T})\\sum_{t^{\\prime}=T}^{T+T^{\\prime}}\\gamma^{(t-T)/2}\\overline{r}\\!\\left(s_{t^{\\prime}},a_{t^{\\prime}}\\right)}\\\\ &{\\quad+\\frac{1}{1-\\gamma}\\partial_{x}\\log P\\!\\left(s_{T},a_{T-1},s_{T-1}\\right)\\sum_{t^{\\prime}=T}^{T+T^{\\prime}}\\gamma^{(t^{\\prime}-T)/2}\\overline{r}(s_{t^{\\prime}},a_{t^{\\prime}})}\\\\ &{\\underset{\\mathbf{(nutmut)}}{\\overset{\\r}{\\longrightarrow}}\\frac{\\widehat{d F}}{\\mathfrak{(}}}\\end{array}$ T')   \nOutput: E ", "page_idx": 23}, {"type": "text", "text": "Algorithm 6 Vanilla Policy Gradient Algorithm ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Data: Initial parameter $\\theta_{0}$ , initial state $s$   \nResult: Approximate policy $\\pi_{\\theta_{L}}$   \nfor $l=0$ to $L$ do Sample $T\\sim\\mathrm{Geo}(1-\\gamma)$ Sample trajectory $(s_{0},a_{0},s_{1},\\ldots,a_{T-1},s_{T},r_{T},a_{T})$ using policy $\\pi_{\\theta_{l}}$ Sample $T^{\\prime}\\sim\\mathrm{Geo}(1-\\gamma^{2})$ Set $\\tilde{s}_{0}=s_{T^{\\prime}}$ and $\\tilde{a}_{0}=a_{T}$ Sample trajectory $\\left({{\\tilde{s}}_{0}},{{\\tilde{a}}_{0}},{{\\tilde{s}}_{1}},\\ldots,{{\\tilde{a}}_{T^{\\prime}-1}},{{\\tilde{s}}_{T^{\\prime}}},{{\\tilde{r}}_{T^{\\prime}}},{{\\tilde{a}}_{T^{\\prime}}}\\right)$ using policy $\\pi_{\\theta_{l}}$ Determine step-size $\\alpha$ $\\begin{array}{r l}&{\\widehat{\\nabla J}_{s}(\\theta_{l})=\\frac{\\mathbf{\\phi}_{1}^{\\ast}}{1-\\gamma}\\nabla\\log\\pi_{\\theta_{l}}(a_{T}|s_{T})\\sum_{t^{\\prime}=0}^{T^{\\prime}-1}\\gamma^{t^{\\prime}/2}\\tilde{r}_{t^{\\prime}+1}}\\\\ &{\\theta_{l+1}=\\theta_{l}-\\alpha\\widehat{\\nabla J}_{s}(\\theta_{l})}\\end{array}$   \nend for ", "page_idx": 23}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1  Overview ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide proofs for the presented theorems and propositions. We provide the proof of Theorem 1, deriving the hypergradient of function $F(x)$ ; the proof of Theorem 2, deriving the derivative of the action-value function with respect to $x$ ; the proof of our main result, Theorem 3, which shows convergence of HPGD to a stationary point of $F(x)$ ", "page_idx": 23}, {"type": "text", "text": "For the propositions, we show how to estimate the upper-level gradient if $f$ is decomposable in the proof of Proposition 1; In Proposition 2 we show how to compute the gradient of the optimal policy withrespectto $x$ ; the proof of Proposition 3, which shows we can achieve unbiased estimates of the advantage hypergradient; and the proof of Proposition 4, which shows the same for the special case when $f$ decomposes. ", "page_idx": 23}, {"type": "text", "text": "We state and proof Propositions 5 to 8 which show convergence in $L_{2}$ to the optimal policy of soft value iteration, soft Q-learning, Vanilla Policy Gradient and Natural Policy Gradient respectively. ", "page_idx": 23}, {"type": "text", "text": "Lastly, we prove Theorem 4, regarding the reduced iteration complexity of RT-Q claimed in Section 4. ", "page_idx": 23}, {"type": "text", "text": "E.2Proof of Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. The proof relies on on three main ideas. ", "page_idx": 23}, {"type": "text", "text": "1. Show that Dominated Convergence applies, i.e. all derivatives are uniformly bounded by an integrable function. Thus we can exchange derivative and expectation and compute the derivativeof $f$ instead of $F$   \n2. Use Proposition 2 to get an expression for the derivative of the optimal policy with respect to $x$ ", "page_idx": 23}, {"type": "text", "text": "3. Use importance sampling with some distribution $\\nu$ to get an expression of the hypergradient that we can cheaply sample, instead of having to multiply two matrices with size $|\\bar{\\cal S}|\\times|{\\cal A}|$ ", "page_idx": 24}, {"type": "text", "text": "By Proposition 2, it follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\frac{\\partial\\pi_{x,\\xi}^{*}}{\\partial x}\\right\\rVert_{\\infty}\\leq\\frac{2}{\\lambda}\\left\\lVert\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\frac{2}{\\lambda}\\frac{K_{2}}{1-\\gamma}\\frac{K_{1}\\overline{{R}}K_{1}}{(1-\\gamma)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As the partial derivatives of $f$ are bounded by $L_{f}$ (cf. Assumption 3.1), we can apply the Dominated Convergence Theorem to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{J}_{x}\\mathbb{E}\\left[f(x,\\pi_{x,\\xi}^{*},\\xi)\\right]=\\mathbb{E}\\left[\\partial_{x}f(x,\\pi_{x,\\xi}^{*},\\xi)\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}+\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}}\\frac{\\partial\\pi_{x,\\xi}^{*}}{\\partial x}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}+\\sum_{s,a}\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a,x)}\\frac{\\partial\\pi_{x,\\xi}^{*}(a;s)}{\\partial x}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}+\\sum_{s,a}\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\frac{1}{\\lambda}\\pi_{x,\\xi}^{*}(a;s)\\partial_{x}A_{x,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right]\\quad\\mathrm{~(9)~}}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}+\\mathbb{E}_{s\\sim\\nu,a\\sim\\pi_{x,\\xi}^{*}}\\left[\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\partial_{x}A_{x,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use Proposition 2 for eq. (9) and importance sampling with any distribution $\\nu$ in the last equalty, as long as $\\nu$ has ful uport on $\\boldsymbol{S}$ Furthr, we note that $\\begin{array}{r}{\\frac{\\partial_{2}f(x,\\stackrel{*}{\\pi}_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}}\\in\\mathbf{Mat}_{1,|S|\\times|A|}(\\mathbb{R})}\\end{array}$ $\\begin{array}{r}{\\frac{\\partial\\pi_{x,\\xi}^{*}}{\\partial x}\\in\\mathbf{Mat}_{|S|\\times|A|,d}(\\mathbb{R})}\\end{array}$ second equality. The second equality follows from the multivariate chain rule and the first equality from the Dominated Convergence Theorem. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "E.3Proof of Theorem 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Theorem 2 is important as $\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi}(s,a)$ and thus $\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi}(s,a)$ allow us to compute $\\frac{d\\pi_{x,\\xi}^{*}}{d x}$ (cf. Proposition 2). We will prove the theorem using an induction proof, which follows the analysis of the policy gradient theorem. However, instead of at each timestep $\\pi\\big(a_{t};s_{t}\\big)$ depending on a policy parameter $\\theta$ it ill be the transition $P_{x,\\xi}{\\left(s_{t+1};s_{t},a_{t}\\right)}$ and reward $r_{x,\\xi}(s_{t},a_{t})$ depending on $x$ Also note that we only consider the partial derivative with respect to $x$ , evaluated at some policy $\\pi$ , which in the case of \u03c0, ", "page_idx": 24}, {"type": "text", "text": "In the following, we will show by induction that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{i Q_{\\lambda,x,\\xi}^{\\pi}(s,a)}{d x}=\\sum_{t=0}^{\\infty}\\sum_{s^{\\prime},a^{\\prime}}\\gamma^{t}p_{x,\\xi}(s,a\\to s^{\\prime},a^{\\prime};t,\\pi)\\left(\\frac{d r_{x,\\xi}(s^{\\prime},a^{\\prime})}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}\\frac{d P_{x,\\xi}(s^{\\prime\\prime};s^{\\prime},a^{\\prime})}{d x}V_{\\lambda,x,\\xi}^{\\pi}(s^{\\prime\\prime},a^{\\prime};t,\\pi)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $p_{x,\\xi}(s,a\\rightarrow s^{\\prime},a^{\\prime};t,\\pi)$ is the probability that the Markov Chain induced by $\\pi$ , starting from $s,a$ reaches $s^{\\prime},a^{\\prime}$ after $t$ steps. Note, the formulation is equivalent to the one stated in Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "The proof follows the analysis of the standard policy gradient theorem. We drop here the dependence on $x$ and $\\xi$ to simplify the notation. Assuming that $Q_{\\lambda}^{\\pi}(s,a)$ is differentiable for all $s,a$ , we show by induction that for all $n\\in\\mathbb N$ it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d Q_{\\lambda}^{\\pi}(s,a)}{d x}=\\sum_{t=0}^{n}\\sum_{s^{\\prime},a^{\\prime}}\\gamma^{t}p(s,a\\to s^{\\prime},a^{\\prime};t,\\pi)\\left(\\frac{d r(s^{\\prime},a^{\\prime})}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}\\frac{d P(s^{\\prime\\prime};s^{\\prime},a^{\\prime})}{d x}V_{\\lambda}^{\\pi}(s^{\\prime\\prime})\\right)}\\\\ &{\\qquad\\qquad+\\displaystyle\\gamma^{n+1}\\sum_{\\widetilde{s},\\widetilde{a}}p(s,a\\to\\widetilde{s},\\widetilde{a};n+1,\\pi)\\frac{d Q_{\\lambda}^{\\pi}(\\widetilde{s},\\widetilde{a})}{d x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Base case $\\mathrm{\\Delta}n=0$ \uff09It is easy to check that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{d Q_{X}^{\\top}(s,a)}{d x}=\\displaystyle\\frac{d}{d x}\\left(r(s,a)+\\gamma\\sum_{s^{\\prime}}P(s^{\\prime};s,a)V_{X}(s^{\\prime})\\right)}\\\\ &{\\displaystyle=\\displaystyle\\frac{d}{d x}r(s,a)+\\gamma\\sum_{s^{\\prime}}\\left(\\frac{d}{d x}P(s^{\\prime};s,a)V_{X}^{\\top}(s^{\\prime})+P(s^{\\prime};s,a)\\displaystyle\\frac{d}{d x}V_{X}^{\\top}(s^{\\prime})\\right)}\\\\ &{\\displaystyle=\\displaystyle\\frac{d}{d x}r(s,a)+\\gamma\\sum_{s^{\\prime}}\\left(\\frac{d}{d x}P(s^{\\prime};s,a)V_{X}^{\\top}(s^{\\prime})+P(s^{\\prime};s,a)\\displaystyle\\sum_{s^{\\prime}=0}^{d}\\pi(a^{\\prime};s^{\\prime})\\displaystyle\\frac{d}{d x}Q_{X}^{\\top}(s^{\\prime},a^{\\prime})\\right)}\\\\ &{\\displaystyle=\\displaystyle\\sum_{t=0}^{0}\\sum_{s^{\\prime}=\\sigma^{\\prime}}\\gamma^{t}p(s,a\\to s^{\\prime},a^{\\prime};t,\\pi)\\left(\\displaystyle\\frac{d r(s^{\\prime},a^{\\prime})}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}\\displaystyle\\frac{d P(s^{\\prime\\prime};s^{\\prime},a^{\\prime})V_{X}^{\\top}(s^{\\prime\\prime})}{d x}\\right)}\\\\ &{\\displaystyle\\quad+\\,\\gamma^{\\top}\\sum_{s^{\\prime}=\\sigma}p(s,a\\to s,\\tilde{s};1,\\pi)\\displaystyle\\frac{d Q_{X}^{\\top}(\\tilde{s},\\tilde{a})}{d x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We use the definition of $Q_{\\lambda}^{\\pi}(s,a)$ in the first equality. The second follows by the product rule. The third equality follows by the definition of the value function. The last equality comes from rearranging terms. ", "page_idx": 25}, {"type": "text", "text": "Induction step $(n\\implies n+1$ \uff09Assuming eq. (10) holds for $n$ we show it holds for $n+1$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\Delta}}&{=-\\frac{1}{r}\\displaystyle\\sum_{k=1}^{r+1}\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k},\\dot{x}_{k}^{\\prime},\\dot{x}_{k}^{\\prime})\\Big(\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime})+\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime})\\frac{\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime},\\dot{x}_{k}^{\\prime})}{\\dot{\\mathcal{L}}_{k}^{(k)}}V(x_{k}^{\\prime})\\Big)}\\\\ &{\\quad+\\nu^{+1}\\sum_{k=1}^{r+1}\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime},\\dot{x}_{k}^{\\prime},\\dot{x}_{k}^{\\prime})\\,,}\\\\ &{\\quad-\\displaystyle\\sum_{k=1}^{r}\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k},\\dot{x}_{k}^{\\prime})\\Big(\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime})+\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime})\\frac{\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime},\\dot{x}_{k}^{\\prime})}{\\dot{\\mathcal{L}}_{k}^{(k)}}V(x_{k}^{\\prime})\\Big)}\\\\ &{\\quad+\\nu^{+1}\\sum_{k=1}^{r+1}\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime},\\dot{x}_{k}^{\\prime},\\dot{x}_{k}^{\\prime})\\,,}\\\\ &{\\quad-\\displaystyle\\sum_{k=1}^{r}\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k},\\dot{x}_{k}^{\\prime})\\,\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime})+\\gamma\\frac{\\dot{\\mathcal{L}}_{k}^{(k)}(x_{k}^{\\prime},\\dot{x}_{k}^{\\prime})}{\\dot{\\mathcal{L}}_{k}^{(k)}}V(x_{k}^{\\prime})}\\\\ &{\\quad+\\nu^{+1}\\displaystyle\\sum_{k=1}^{r+1}\\dot{\\mathcal{L}}_{k}^{(\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first equality is simply the base case. The second equality follows from the definition of the Q-function. The third equality follows from the multivariate chain rule and the definition of the value function and the last equality is again rearranging terms. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. By the smoothness of $f$ , we use the following bound from [37][Lemma 1] for $\\alpha\\leq1/(2S_{f})$ ", "page_idx": 26}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/58f54127ab6207b428d2cb73cc92fa825cbe677b20b85160e1d746d3da56347d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "The error term naturally decomposes into an initial error divided by $T$ (1), a bias term (2), and a variance term (3), which decreases with the stepsize $\\alpha$ ", "page_idx": 26}, {"type": "text", "text": "For (1) we do not need to simplify any further. ", "page_idx": 26}, {"type": "text", "text": "To prove our claim, we need to show that (3) is ${\\mathcal O}(1)$ , and that (2) is $\\mathcal{O}(\\delta)$ . The latter is the main challenge of this proof. ", "page_idx": 26}, {"type": "text", "text": "Let us begin by bounding the bias term (2). The goal is to show that it can be upper bounded by a sum of terms, which are all linear in $\\left\\|\\pi_{x,\\xi}^{o}-\\pi_{x,\\xi}^{*}\\right\\|_{\\infty}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}\\left[\\frac{d F\\left(x_{t}\\right)}{d x}-\\frac{d\\widehat{F(x_{t})}}{d x}\\right]\\right\\|_{\\infty}}\\\\ &{=\\left\\|\\mathbb{E}_{x_{t}}\\left[\\frac{d F\\left(x_{t}\\right)}{d x}-\\mathbb{E}_{\\xi,\\varepsilon}\\left[\\frac{\\partial_{1}f\\left(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi\\right)}{\\partial x}+\\mathbb{E}_{\\omega\\sim\\pi_{x,\\xi}^{o},\\xi}^{\\nu}\\left[\\frac{1}{\\lambda\\nu\\left(s\\right)}\\frac{\\partial_{2}f\\left(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi\\right)\\right)}{\\partial\\pi\\left(s,a\\right)}\\mathbb{E}\\left[\\frac{d\\widehat{\\Pi_{x^{*},x_{t},\\xi}^{o}}}{d x}(s,a)\\right]\\right.\\right.}\\\\ &{\\leq\\left.\\left\\|\\underbrace{\\mathbb{E}_{x_{t},o,\\xi}\\left[\\frac{\\partial_{1}f\\left(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi\\right)}{\\partial x}-\\frac{\\partial_{1}f\\left(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi\\right)}{\\partial x}\\right]}_{(\\Lambda)}\\right\\|_{\\infty}}\\\\ &{\\quad+\\left\\|\\mathbb{E}_{x_{t},o}^{\\xi,\\nu}\\left[\\frac{1}{\\lambda\\nu\\left(s\\right)}\\sum_{a}\\left(\\pi_{x,\\xi}^{*}(a;s)\\frac{\\partial_{2}f\\left(x,\\pi_{x,\\xi}^{o},\\xi\\right)}{\\partial\\pi_{x,\\xi}^{o}\\left(a;s\\right)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s,a)-\\pi_{x,\\xi}^{o}(a;s)\\frac{\\partial_{2}f\\left(x,\\pi_{x,\\xi}^{o},\\xi\\right)}{\\partial\\pi_{x,\\xi}^{o}\\left(a;s\\right)}\\partial_{x}A_{\\lambda,x}^{\\pi_{x,\\xi}^{o}}(s,a)\\right]\\right\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first equality is by definition and the first inequality follows from the trinalge inequality and we further use the fact that $\\widehat{\\frac{d}{d x}A_{\\lambda,x,\\xi}^{\\pi_{x}^{o}}}$ is an unbiased estimator of  A $\\textstyle{\\frac{d}{d x}}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}$ as shown in Proposition 3. (A) is relatively easy to bound. Indeed by the smoothness of $f$ ( Assumption 3.1), it immediately follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\eta}(\\mathbf{A})\\leq\\mathbb{E}_{x_{t},o,\\xi}\\left[S_{f}\\left\\|\\pi_{x_{t},\\xi}^{*}-\\pi_{x_{t},\\xi}^{o}\\right\\|_{\\infty}\\right]\\leq S_{f}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To bound (B) we again use the triangle inequality to decompose: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{B})=\\Bigg\\|\\mathbb{E}_{x_{t},o}^{\\xi,\\nu}\\Bigg[\\frac{1}{\\lambda\\nu(s)}\\sum_{a}\\Bigg(\\pi_{x,\\xi}^{*}(a;s)\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)-\\pi_{x,\\xi}^{o}(a;s)\\frac{\\partial_{2}f(x,\\pi_{x,t}^{o},\\xi)}{\\partial\\pi_{x,\\xi}^{o}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)}\\\\ &{\\quad\\le\\mathbb{E}_{x_{t},o}^{\\xi,\\nu}\\Bigg[\\frac{1}{\\lambda\\nu(s)}\\sum_{a}\\|\\pi_{x,\\xi}^{*}(a;s)-\\pi_{x,\\xi}^{o}(a;s)\\|_{\\infty}\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\|_{\\infty}\\Bigg]_{\\!\\!\\infty}\\Bigg]_{\\!\\!\\infty}\\Bigg)_{\\!\\!\\!\\infty}\\Bigg)_{\\!\\!\\xi},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n+\\mathbb{E}_{x_{t},o}^{\\xi,\\nu}\\Bigg[\\frac{1}{\\lambda\\nu(s)}\\sum_{a}\\big\\|\\pi_{x,\\xi}^{o}(a;s)\\big\\|_{\\infty}\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{*},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{o}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\big\\|_{\\infty}\\Bigg]\\right\\}\\Bigg|_{\\mathbb{R}^{3}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(a) is relatively easy to bound. We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\mathbb{E}_{x_{t}}^{\\xi,\\nu}\\left[\\frac{1}{\\lambda\\nu(s)}|A|\\delta\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial\\pi_{x,\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ {\\displaystyle\\leq\\mathbb{E}_{x_{t}}^{\\xi,\\nu}\\left[\\frac{1}{\\lambda\\nu(s)}|A|\\delta L_{f}\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ {\\displaystyle\\leq\\mathbb{E}_{x_{t}}^{\\xi,\\nu}\\left[\\frac{2}{\\lambda\\nu(s)}|A|\\delta L_{f}\\left\\|\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\|_{\\infty}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use the assumption on the oracle in the first inequality, that $f$ is Lipschitz continuous in the second inequality, and that $\\left\\lVert\\partial_{x}V_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s)\\right\\rVert_{\\infty}\\leq\\left\\lVert\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\rVert_{\\infty}$ in the third inequality. Note that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|V_{\\lambda,x,\\xi}^{\\pi}(s)\\right\\|_{\\infty}\\leq\\frac{(\\overline{{R}}+\\lambda\\log|\\mathcal{A}|)}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "since the entropy of any policy is bound by $\\log\\left|A\\right|$ (which follows from Jensen's inequality). Using the definition that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)=\\mathbb{E}_{s,a}^{\\pi_{x,\\xi}^{*}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\frac{d r_{x,\\xi}(s_{t},a_{t})}{d x}+\\gamma^{t+1}\\frac{d\\log P_{x,\\xi}(s_{t+1};s_{t},a_{t})}{d x}V_{\\lambda,x,\\xi}^{\\pi}(s_{t+1})\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "it thus holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\lVert\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a)\\right\\rVert_{\\infty}\\leq\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|\\mathcal{A}|)}{(1-\\gamma)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Letting $m:=\\operatorname*{min}_{s}\\nu(s)$ , we thus have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\leq\\frac{2}{\\lambda m}|A|\\delta L_{f}\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{{R}}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For (b) we further simplify using the triangle inequallity: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(b)}\\leq\\displaystyle\\frac{1}{\\lambda m}\\mathbb{E}_{x_{t},\\varepsilon}^{\\xi}\\left[\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{*},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{*}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{o}(a;s)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ &{\\quad\\leq\\displaystyle\\frac{1}{\\lambda m}\\mathbb{E}_{x_{t},o}^{\\xi}\\left[\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{*},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{*}(a;s)}-\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{o}(a;s)}\\right\\|_{\\infty}\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)\\right\\|_{\\infty}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similar to (a), we can bound (i) using the smoothness of $f$ (Assumption 3.1): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{(i)}\\leq2{\\frac{S_{f}}{\\lambda m}}\\delta\\left({\\frac{K_{2}}{1-\\gamma}}+{\\frac{K_{1}(\\overline{{{R}}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Boundingil an i partieular $\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}$ is thetricky parto tisproh We first need to show two intermediate results. First, we bound the difference in entropy between two policies and the difference in the regularized value functions of two policies. Once we have ", "page_idx": 27}, {"type": "text", "text": "$\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}$ $l_{1}~:=$ $\\begin{array}{r}{\\operatorname*{min}_{s,a,x,\\xi}\\pi_{x,\\xi}^{*}(a;s)}\\end{array}$ the minimum probability of playing an action in any state under the optimal policy. Recall that $\\forall x,\\xi,s,a\\::\\:|r_{x,\\xi}(s,a)|\\:<\\:\\overline{{R}}$ and that $0\\,\\leq\\,H(\\pi;s)\\,\\leq\\,\\log\\left|{\\cal A}\\right|$ (by Jensen's inequality). Thus we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{-\\overline{{R}}}{1-\\gamma}\\leq Q_{\\lambda,x,\\xi}^{*}(s,a)\\leq\\frac{\\overline{{R}}+\\lambda\\log|\\cal A|}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Because $\\pi_{x,\\xi}^{*}(s;a)\\propto\\exp(Q_{\\lambda,x,\\xi}^{*}(s,a)/\\lambda)$ , it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\nl_{1}\\geq\\frac{\\exp(\\frac{-\\overline{{R}}}{\\lambda(1-\\gamma)})}{|\\mathcal{A}|\\exp(\\frac{\\overline{{R}}+\\lambda\\log|\\mathcal{A}|}{\\lambda(1-\\gamma)})}>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We assume now that $\\delta$ is suficiently small ie. $\\delta\\le l_{1}/2$ , such that $l_{1}/2\\leq\\operatorname*{min}_{s,a,x,\\xi}\\pi_{x,\\xi}^{o}(a;s)$ We need to have such a lower bound on the policies, touse the fact that the log function is Lipschitz continuous with parameter $\\frac{1}{a}$ on an interval $[a,\\infty)$ for any $a>0$ . Hence we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\|H(\\pi_{x,\\xi}^{*}|s)-H(\\pi_{x,\\xi}^{o}|s)\\right\\|_{\\infty}=\\left\\|\\sum_{a}\\pi_{x,\\xi}^{*}(a;s)\\log\\pi_{x,\\xi}^{*}(a;s)-\\sum_{a}\\pi_{x,\\xi}^{o}(a;s)\\log\\pi_{x,\\xi}^{o}(a;s)\\right\\|_{\\infty}}&{}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\left(\\sum_{a}\\left\\|\\pi_{x,\\xi}^{*}-\\pi_{x,\\xi}^{o}\\right\\|_{\\infty}\\left\\|\\log\\pi_{x,\\xi}^{*}\\right\\|_{\\infty}\\right)+\\left\\|\\log\\pi_{x,\\xi}^{*}-\\log\\pi_{x,\\xi}^{o}\\right\\|_{\\infty}}&{}\\\\ {\\displaystyle\\qquad\\qquad\\leq|A||\\log l_{1}|\\delta+\\frac{2}{l_{1}}\\delta.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We use this to bound the difference in the value functions of $\\pi_{x,\\xi}^{*}$ and $\\pi_{x,\\xi}^{o}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|V_{x}^{\\star,\\varepsilon}(s)-V_{x}^{\\star,\\varepsilon}(s)\\right\\|_{\\infty}}\\\\ &{\\leq\\left\\|\\displaystyle\\sum_{x\\in\\mathcal{X}_{x}}\\left((u,s)Q_{x}^{\\star,\\varepsilon}(s,a)-\\pi_{x,\\varepsilon}^{\\rho}(a,s)Q_{x}^{\\star,\\varepsilon}(s,a)\\right)\\right\\|_{\\infty}+\\lambda\\left\\|H(\\pi_{x,\\varepsilon}^{\\star}(s)-H(\\pi_{x,\\varepsilon}^{\\rho}(s)\\right)\\right\\|_{\\infty}}\\\\ &{\\leq\\left\\|\\displaystyle\\sum_{x\\in\\mathcal{X}_{x}}\\left((u,s)Q_{x}^{\\star,\\varepsilon}(s,a)-\\pi_{x,\\varepsilon}^{\\rho}(a,s)Q_{x}^{\\star,\\varepsilon}(s,a)\\right)\\right\\|_{\\infty}+\\lambda\\left\\langle\\left\\|\\mathcal{H}\\right\\|\\log L_{1}\\right\\|+\\displaystyle\\frac{2}{l}\\right\\rangle}\\\\ &{\\leq\\left\\|\\displaystyle\\sum_{x\\in\\mathcal{X}_{x}}\\alpha_{x,\\varepsilon}^{\\star}(a,s)\\right\\|_{\\infty}\\left\\|Q_{x}^{\\star,\\varepsilon}(s,a)-Q_{x}^{\\star,\\varepsilon}(s,a)\\right\\|_{\\infty}}\\\\ &{\\quad+\\displaystyle\\sum_{y\\in\\mathcal{Y}_{x}}\\left\\|\\pi_{x,\\varepsilon}^{\\star}(a;s)-\\pi_{x,\\varepsilon}^{\\star}(a,s)\\right\\|_{\\infty}\\left\\|Q_{x}^{\\star,\\varepsilon}(s,a)\\right\\|_{\\infty}+\\lambda\\left\\langle\\left\\|\\mathcal{H}\\right\\|\\log L_{1}\\right\\|+\\displaystyle\\frac{2}{l}\\right\\rangle}\\\\ &{\\leq\\lambda\\left\\langle\\left\\|(\\log L_{1}\\right\\|+\\displaystyle\\frac{2}{l}\\right)+\\delta|\\mathcal{A}|\\displaystyle\\frac{\\overline{{\\mathcal{H}}}}{l}+\\gamma^{\\star}\\left\\|\\left\\|\\nabla_{x}^{\\varepsilon,\\varepsilon}(s)-V_{x}^{\\star,\\varepsilon}(s)\\right\\|_{\\infty}}\\\\ &{\\leq\\lambda\\delta\\left(\\frac{\\lambda(1)\\log L_{1}(1+\\frac{L_{1}^{\\star}}{l})}{1-\\gamma}+\\frac{\\delta|\\mathcal{A}|\\mathcal{B}|}{(1-\\gamma)^{2}}+\\frac{\\gamma}{2}\\right)}\\\\ &{\\qquad+\\lambda\\left\\langle\\|\\Pi\\Theta\\|^{2}\\|\\mathcal{H}^{\\star}\\|+\\frac{2}{l}\\right\\rangle+\\frac{\\delta|\\mathcal{A}|\\mathcal{B}|}{l}.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The first inequality follows from the definition of the regularized value function and the triangle inequality. The second inequality follows from our derived bound on the difference of entropies. The third inequality is agian using the triangle inequality and the fourth inequality is bounding the Q-function using that the rewards are upper bounded by $\\overline{{R}}$ and that $Q_{\\lambda,x,\\xi}^{\\pi}(s,a)\\,=\\,r_{x,\\xi}(s,a)\\,+\\,$ $\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{x,\\xi}\\left(\\cdot;s,a\\right)}\\left[V_{\\lambda,x,\\xi}^{\\pi}(s^{\\prime})\\right]$ . The last inequality then follows by iteratively plugging in the inequality for the term $\\left|\\left|V_{\\lambda}^{\\bar{\\pi_{x,\\xi}^{*}}}(s)-\\bar{V_{\\lambda}^{\\pi_{x,\\xi}^{o}}}(s)\\right|\\right|_{\\infty}$ , which gives a geometric sum. We employ a similar technique to bound (ii) using the above results: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\lambda m}\\mathbb{E}_{x_{t},o}^{\\xi}\\left[\\left\\|\\frac{\\partial_{2}f(x,\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial\\pi_{x_{t},\\xi}^{o}(a;s)}\\right\\|_{\\infty}\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)-\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ &{\\displaystyle\\leq\\!\\frac{L_{f}}{\\lambda m}\\mathbb{E}_{x_{t},o}^{\\xi}\\left[\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)-\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ &{\\displaystyle\\leq\\!\\frac{L_{f}}{\\lambda m}2\\mathbb{E}_{x_{t},o}^{\\xi}\\left[\\left\\|\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)-\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{o}}(s,a)\\right\\|_{\\infty}+|A|\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)\\|\\pi_{x,\\xi}^{o}-\\pi_{x_{t},\\xi}^{o}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here the first inequality follows from the Lipschitz continuity of $f$ . For second inequality we use the definition of the advantage function, the triangle inequality and the following inequality: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\lVert\\partial_{x}V_{\\lambda,x,\\xi}^{\\pi_{x}^{*},\\xi}(s)-\\partial_{x}V_{\\lambda,x,\\xi}^{\\pi_{x}^{*},\\xi}(s)\\right\\rVert_{\\infty}}\\\\ &{=\\left\\lVert\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{\\sigma}(a;s)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{*}(a;s)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)\\right\\rVert_{\\infty}}\\\\ &{=\\displaystyle\\left\\lVert\\displaystyle\\sum_{a}\\left(\\pi_{x_{t},\\xi}^{\\sigma}(a;s)-\\pi_{x_{t},\\xi}^{*}(a;s)\\right)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{*}(a;s)\\left(\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)\\right)\\right\\rVert_{\\infty}}\\\\ &{\\le\\!\\!\\lvert A\\rvert\\displaystyle\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\lvert A\\rvert)}{\\left(1-\\gamma\\right)^{2}}\\left\\lVert\\pi_{x_{t},\\xi}^{\\sigma}-\\pi_{x_{t},\\xi}^{*}\\right\\rVert_{\\infty}+\\left\\lVert\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)-\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{x_{t},\\xi}^{*}}(s,a)\\right\\rVert_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from the trianlge inequality and Equation (12) ", "page_idx": 29}, {"type": "text", "text": "We bound the difference in Q-function derivatives as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\partial_{t}\\partial_{x}\\bar{u}_{\\bar{x}}^{(k)}(\\theta_{1})-\\partial_{x}\\bar{u}_{\\bar{x}}^{(k)}(\\theta_{1})\\right\\|_{\\infty}}\\\\ &{=\\left\\|\\displaystyle\\sum_{k=0}^{\\infty}\\displaystyle\\sum_{i=1}^{\\infty}\\gamma_{k}(\\mu_{k},\\alpha^{k})\\,i\\!\\right\\rangle_{\\scriptstyle\\infty}\\left(\\frac{\\displaystyle\\partial_{t}\\bar{u}_{\\bar{x}}(\\theta_{1}^{(k)})}{\\displaystyle\\partial_{x}\\alpha^{k}}+\\gamma_{\\bar{x}}\\displaystyle\\sum_{i=1}^{\\infty}i\\!\\!\\frac{\\partial_{x}\\bar{u}_{\\bar{x}}(\\theta_{1}^{(k)})u_{\\bar{x}}^{*}(\\theta_{1}^{(k)})}{\\displaystyle\\partial_{x}\\alpha^{k}}\\right)}\\\\ &{~~-\\displaystyle\\sum_{k=0}^{\\infty}\\displaystyle\\sum_{i=1}^{\\infty}\\gamma_{k}(\\mu_{k},\\alpha^{k})\\,i\\!\\right\\rangle_{\\scriptstyle\\infty}\\left(\\frac{\\displaystyle\\partial_{t}\\bar{u}_{\\bar{x}}(\\theta_{2}^{(k)})}{\\displaystyle\\partial_{x}\\alpha^{k}}+\\gamma_{\\bar{x}}\\displaystyle\\sum_{i=1}^{\\infty}i\\!\\!\\frac{\\partial_{x}\\bar{u}_{\\bar{x}}(\\theta_{2}^{(k)})u_{\\bar{x}}^{*}(\\theta_{2}^{(k)})}{\\displaystyle\\partial_{x}\\alpha^{k}}\\right)\\right\\|}\\\\ &{=\\left\\|\\displaystyle\\frac{\\bar{u}_{\\bar{x}}(\\theta_{1}^{(k)})}{\\displaystyle\\gamma_{k}\\varepsilon_{\\varepsilon}}+\\gamma_{\\overline{{x}}}\\displaystyle\\frac{\\bar{u}_{\\bar{x}}(\\theta_{1}^{(k)})u_{\\bar{x}}^{*}(\\theta_{2}^{(k)})}{\\displaystyle\\gamma_{k}\\varepsilon_{\\varepsilon}}\\right\\|\\varepsilon_{\\varepsilon}^{2}(\\theta_{1}^{(k)})}\\\\ &{~~+\\displaystyle\\sum_{k=0}^{\\infty}\\displaystyle\\sum_{i=1}^{\\infty}\\gamma_{k}(\\mu_{k},\\alpha^{k})\\,i\\!\\right\\rangle_{\\scriptstyle\\infty}\\left(\\frac{\\displaystyle\\partial_{t}\\bar{u}_{\\bar{x}}(\\theta_{2}^{(k)})}{\\displaystyle\\partial_{\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\overline{{\\phi}}_{x,\\varepsilon}\\left(a_{x}^{\\nu,\\varepsilon},a_{y,\\varepsilon}^{\\nu,\\varepsilon}\\right)+\\gamma\\sum_{\\alpha=1}^{{d_{\\nu}}}\\frac{L^{\\alpha}P_{x,\\varepsilon}\\left(a_{x}^{\\nu,\\varepsilon},a_{y}^{\\nu,\\varepsilon}\\right)}{d\\alpha}V_{x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu}\\right)\\right)\\Bigg|_{\\infty}}\\\\ &{\\leq\\gamma\\sum_{\\nu_{x}}\\left\\|\\frac{L^{\\alpha}P_{x,\\varepsilon}\\left(a_{x}^{\\nu,\\varepsilon},a_{y}^{\\nu,\\varepsilon}\\right)}{d\\alpha}\\right\\|_{\\infty}\\left\\|{\\nabla}_{x,x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu}-V_{x,x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu}\\right)\\right)\\right\\|_{\\infty}}\\\\ &{\\quad+\\gamma\\left\\|\\sum_{\\alpha=1}^{{d_{\\nu}}}P_{x}(a_{x}^{\\nu,\\varepsilon},a_{y}^{\\nu,\\varepsilon},a_{x}^{\\nu,\\varepsilon}(a_{x}^{\\nu},a_{y}^{\\nu,\\varepsilon}))\\beta_{x,\\varepsilon}\\frac{L^{\\alpha}\\left(a_{x}^{\\nu},a_{y,\\varepsilon}^{\\nu,\\varepsilon}\\right)}{d\\alpha}\\alpha}\\\\ &{\\quad-\\sum_{\\nu_{x}}P(a_{x}^{\\nu,\\varepsilon},a_{y}^{\\nu,\\varepsilon},a_{x}^{\\nu},a_{y}^{\\nu,\\varepsilon})\\alpha,\\beta_{x,\\varepsilon}\\frac{\\gamma_{x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu},a_{y}^{\\nu}\\right)}{d\\alpha}\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\sum_{\\nu_{x}}^{\\nu}\\left\\|\\frac{L^{\\alpha}P_{x,\\varepsilon}\\left(a_{x}^{\\nu,\\varepsilon},a_{y,\\varepsilon}^{\\nu}\\right)}{d\\alpha}\\right\\|_{\\infty}\\left\\|{\\nabla}_{x,x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu}\\right)-V_{x,x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu}\\right)\\right\\|_{\\infty}}\\\\ &{\\quad+\\gamma\\left\\|\\partial_{x}Q_{x,\\varepsilon}^{\\nu,\\varepsilon}\\left(a_{x}^{\\nu,\\varepsilon},a_{y}^{\\nu}\\right)\\right\\|_{\\infty}\\Bigg \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the dots indicate multiplication over the linebreak. The first equality follows from plugging in the result from Theorem 2. The second equality follows by taking out all terms with $t=0$ .The first inequality uses the triangle inequality. The second inequality plugs back in the definition from Theorem 2. The last inequality follows from the triangle inequality again. ", "page_idx": 30}, {"type": "text", "text": "Taking the expectation, we thus get: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{x_{t},\\sigma}^{\\xi}\\left[\\left\\|\\partial_{x}Q_{x,x,\\xi}^{\\pi_{t}^{*},\\varepsilon}(s,a)-\\partial_{x}Q_{x,x,\\xi}^{\\pi_{t}^{*},\\varepsilon}(s,a)\\right\\|_{\\infty}\\right]}\\\\ &{\\le\\gamma\\left(|S|K_{1}\\left(\\frac{\\lambda\\delta}{1-\\gamma}\\left(\\frac{|\\lambda|(\\log l_{1})|+\\frac{l_{1}}{l}\\right)}{1-\\gamma}+\\frac{\\delta|A|\\widetilde{R}}{(1-\\gamma)^{2}}\\right)+\\delta\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)\\right)}\\\\ &{\\quad+\\gamma\\mathbb{E}_{x_{t},\\sigma}^{\\xi}\\left[\\left\\|\\partial_{x}Q_{x,x,\\xi}^{\\pi_{t}^{*},\\varepsilon}(s^{\\prime},a)-\\partial_{x}Q_{x,x,\\xi}^{\\pi_{t}^{*},\\varepsilon}(s^{\\prime},a^{\\prime})\\right\\|_{\\infty}\\right]}\\\\ &{\\le\\frac{\\gamma}{1-\\gamma}\\left(|S|K_{1}\\left(\\frac{\\lambda\\delta}{1-\\gamma}\\left(\\frac{|\\lambda||\\log l_{1}|+\\frac{l_{1}}{l}\\right)}{1-\\gamma}+\\frac{\\delta|A|\\widetilde{R}}{(1-\\gamma)^{2}}\\right)+\\delta\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)\\right)}\\\\ &{=\\!\\frac{\\delta\\gamma}{1-\\gamma}\\left(|S|K_{1}\\left(\\frac{\\lambda\\left(|\\lambda|\\log l_{1}|+\\frac{l_{1}}{l}\\right)}{1-\\gamma}+\\frac{|A|\\widetilde{R}}{(1-\\gamma)^{2}}\\right)+\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we use the intermediate result from before to bound the difference between the value functions, the upper bound on the Q-function derivative shown in Equation (12) and the assumption on the oracle to get the first inequality. The second inequality follows from the the resulting geometric sum and the last equality is just rearranging terms to show the linearity in $\\delta$ ", "page_idx": 30}, {"type": "text", "text": "Using this result, we can now bound (ii): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ii})\\leq\\displaystyle\\frac{2L_{f}\\delta\\gamma}{\\lambda m(1-\\gamma)}\\Bigg(|\\mathcal{S}|K_{1}\\left(\\frac{\\lambda\\left(|A||\\log l_{1}|+\\frac{2}{l_{1}}\\right)}{1-\\gamma}+\\frac{|A|\\overline{{R}}}{(1-\\gamma)^{2}}\\right)+\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)}\\\\ &{\\qquad+\\left|A\\right|\\left(\\frac{K_{2}}{\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)\\gamma}\\right)\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With that we are done decomposing (2). Combining everything, we have the following bound: ", "page_idx": 30}, {"type": "equation", "text": "$$\n(2)=\\left\\|\\mathbb{E}\\left[\\frac{d F(x_{t})}{d x}-\\widehat{\\frac{d F(x_{t})}{d x}}\\right]\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma}(\\mathsf{A})+(\\mathsf{B})}\\\\ &{\\mathsf{\\Gamma}(\\mathsf{A})+(\\mathsf{a})+(\\mathsf{b})}\\\\ &{\\mathsf{\\Gamma}(\\mathsf{A})+(\\mathsf{a})+(\\mathsf{i})+(\\mathsf{i})}\\\\ &{\\le{\\cal S}_{f}\\delta+\\frac{2}{\\lambda m}\\vert A\\vert\\delta L_{f}\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\vert A\\vert)}{\\left(1-\\gamma\\right)^{2}}\\right)+\\frac{2{\\cal S}_{f}}{\\lambda m}\\delta\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\vert A\\vert)}{\\left(1-\\gamma\\right)^{2}}\\right)}\\\\ &{\\phantom{=\\ }+\\frac{2L_{f}\\delta\\gamma}{\\lambda m(1-\\gamma)}\\Bigg(\\vert\\mathcal{S}\\vert K_{1}\\left(\\frac{\\lambda\\left(\\vert A\\vert\\log\\vert_{1}\\vert+\\frac{2}{l_{1}}\\right)}{1-\\gamma}+\\frac{\\vert A\\vert\\overline{{R}}}{\\left(1-\\gamma\\right)^{2}}\\right)+\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\vert A\\vert)}{\\left(1-\\gamma\\right)^{2}}\\right)}\\\\ &{\\phantom{=\\ }+\\vert A\\vert\\left(\\frac{K_{2}}{\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\vert A\\vert)}{\\left(1-\\gamma\\right)\\gamma}\\right)\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With that we have tackled terms (1) and (2) in Equation (11). It remains to bound the variance, i.e. term (3). If we can show that $(3)=\\mathcal{O}(1)$ then the last term in Equation (11) is ${\\mathcal{O}}(\\alpha)$ asclaimed and we are done. Indeed, bounding (3) is relatively easy, as all important terms are bounded by Assumption 3.1. We have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\Im\\left[\\left\\|\\frac{d F(x_{t})}{d x}-\\frac{d\\widehat{F(x_{t})}}{d x}\\right\\|_{\\infty}^{2}\\right]\\leq2\\left\\|\\frac{d F(x_{t})}{d x}-\\mathbb{E}\\left[\\frac{d\\widehat{F(x_{t})}}{d x}\\right]\\right\\|_{\\infty}^{2}+2\\mathbb{E}\\left[\\left\\|\\frac{d\\widehat{F(x_{t})}}{d x}-\\mathbb{E}\\left[\\frac{d\\widehat{F(x_{t})}}{d x}\\right]\\right\\|_{\\infty}^{2}\\right]}\\\\ {\\displaystyle\\leq2\\mathcal{O}(\\delta^{2})+2\\mathbb{E}\\left[\\left\\|\\frac{d\\widehat{F(x_{t})}}{d x}-\\mathbb{E}\\left[\\frac{d\\widehat{F(x_{t})}}{d x}\\right]\\right\\|_{\\infty}^{2}\\right]}\\\\ {\\displaystyle\\leq\\mathcal{O}(\\delta^{2})+2\\left(\\mathbb{E}\\left[\\left\\|\\frac{d\\widehat{F(x_{t})}}{d x}\\right\\|_{\\infty}^{2}\\right]-\\left\\|\\mathbb{E}\\left[\\frac{d\\widehat{F(x_{t})}}{d x}\\right]\\right\\|_{\\infty}^{2}\\right)}\\\\ {\\displaystyle\\leq\\mathcal{O}(\\delta^{2})+2\\mathbb{E}\\left[\\left\\|\\frac{d\\widehat{F(x_{t})}}{d x}\\right\\|_{\\infty}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the third inequality follows directly, the second inequality uses the definition of the variance and the first inequality uses that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|a+b\\right\\|^{2}=\\left\\|a\\right\\|^{2}+2a^{\\top}b+\\left\\|b\\right\\|^{2}\\leq\\left\\|a\\right\\|^{2}+2\\left\\|a\\right\\|\\left\\|b\\right\\|+\\left\\|b\\right\\|^{2}\\leq2\\left\\|a\\right\\|+2\\left\\|b\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By the above, it suffices to bound the second moment: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\frac{\\widehat{d F(x_{t})}}{d x}\\right\\|_{\\infty}^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\|\\frac{\\partial_{1}f(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi)}{\\partial x}+\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x_{t},\\pi_{x_{t},\\xi}^{o},\\xi))}{\\partial\\pi(s,a)}\\widehat{\\partial_{x}\\overset{\\pi_{x_{t},\\xi}^{o}}{\\lambda_{\\lambda,x,\\xi}^{o}}}(s,a)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\left\\|L_{f}+\\frac{1}{\\lambda m}L_{f}\\widehat{\\partial_{x}\\overset{\\pi_{x_{t},\\xi}^{o}}{\\lambda_{\\lambda,x,\\xi}^{o}}}(s,a)\\right\\|_{\\infty}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{\\sigma,\\varepsilon}^{\\pi}}(s,a)\\right\\|_{\\infty}\\right]}\\\\ &{\\le4\\mathbb{E}\\Bigg[\\Bigg\\|\\Bigg(\\displaystyle{\\sum_{t=0}^{T_{Q}}\\frac{d}{d x}r(s_{t}^{\\tau_{Q}},a_{t}^{\\tau_{Q}})}}\\\\ &{\\quad+\\frac{\\gamma}{1-\\gamma}\\frac{d}{d x}\\log P(s_{T Q+1}^{\\tau_{Q}};s_{T Q}^{\\tau_{Q}},a_{T Q}^{\\tau_{Q}})\\displaystyle{\\sum_{t=T_{Q}+1}^{T_{Q}+T_{Q}}\\gamma^{(t-T_{Q}-1)/2}\\left(r(s_{t}^{\\tau_{Q}},a_{t}^{\\tau_{Q}})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)}\\Bigg)\\Bigg\\|_{\\infty}^{2}}\\\\ &{\\le4\\mathbb{E}\\left[\\left\\|\\left(T_{Q}K_{2}+\\frac{\\gamma}{1-\\gamma}K_{1}\\frac{\\overline{{R}}+\\lambda\\log|\\mathcal{A}|}{1-\\gamma^{0.5}}\\right)\\right\\|_{\\infty}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $T_{Q}$ is the random variable defined in Algorithm 2. The first inequality uses the definition of the advantage estimate (cf. Algorithm 2), the i.i.d. property of $T_{Q}$ and $T_{V}$ , as well as of $T_{Q}^{\\prime}$ and $T_{V}^{\\prime}$ and Equation (16). The second inequality follows from Assumption 3.1. ", "page_idx": 32}, {"type": "text", "text": "Plugging in the above, we thus get: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon\\left[\\left\\|\\frac{\\widehat{d F(x_{t})}}{d x}\\right\\|_{\\infty}^{2}\\right]=\\mathbb{E}\\left[\\left\\|L_{f}+\\frac{1}{\\lambda m}L_{f}\\partial_{x}\\widehat{A_{x,x_{\\varepsilon},\\xi}^{\\sigma_{\\varepsilon}}}(s,a)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[2\\left\\|L_{f}\\right\\|_{\\infty}^{2}+2\\left(\\frac{L_{f}}{\\lambda m}\\right)^{2}\\left\\|\\partial_{x}\\widehat{A_{x,x_{\\varepsilon},\\xi}^{\\sigma_{\\varepsilon}}}(s,a)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\|L_{f}\\right\\|_{\\infty}^{2}\\right]+8\\left(\\frac{L_{f}}{\\lambda m}\\right)^{2}\\mathbb{E}\\left[\\left\\|\\left(T_{Q}K_{2}+\\frac{\\gamma}{1-\\gamma}K_{1}\\frac{\\overline{{R}}+\\lambda\\log\\left|A\\right|}{1-\\gamma^{0.5}}\\right)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\|L_{f}\\right\\|_{\\infty}^{2}\\right]+16\\left(\\frac{L_{f}}{\\lambda m}\\right)^{2}\\mathbb{E}\\left[\\left\\|T_{Q}K_{2}\\right\\|_{\\infty}^{2}\\right]+16\\mathbb{E}\\left[\\left\\|\\frac{\\gamma}{1-\\gamma}K_{1}\\frac{\\overline{{R}}+\\lambda\\log\\left|A\\right|}{1-\\gamma^{0.5}}\\right]}\\\\ &{=\\mathcal{O}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we repeatedly apply Equation (16) and the fact that the second moment of a geometric random variable is finite. ", "page_idx": 32}, {"type": "text", "text": "Now we can plug all our bounds back into Equation (11) to get the result of Theorem 3. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left\\|\\frac{d{\\cal F}(\\hat{x}_{T})}{d x}\\right\\|_{\\infty}^{2}\\right]\\leq(1)+(2)+(3)}\\\\ {\\displaystyle\\leq\\mathcal{O}(\\frac{1}{\\alpha T})+\\mathcal{O}(\\delta)+2S_{f}\\alpha\\left(\\mathcal{O}(\\delta^{2}+1)\\right)}\\\\ {\\displaystyle\\leq\\mathcal{O}(\\frac{1}{\\alpha T})+\\mathcal{O}(\\delta)+\\mathcal{O}(\\alpha).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "E.5 Proof of Theorem 4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Vanilla soft $Q$ learning ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We give a brief overview of how HPGD is combined with vanilla soft Q-learning (Algorithm 4) to get a bias of $O(2^{-K/2})$ in Algorithm 7. In the algorithm we refer to $t_{K}\\,=\\,\\mathcal{O}(K2^{K})$ as the number of iterations soft Q-learning needs to achieve $\\lambda\\mathbb{E}\\big\\|\\pi^{t_{K}}-\\pi^{*}\\big\\|_{\\infty}^{2}\\leq\\mathbb{E}\\big\\|Q^{t_{K}}-Q_{\\lambda}^{*}\\big\\|_{\\infty}^{2}\\leq2^{-K}$ (cf. Proposition 6). Note we slightly abuse notation, when we pass a policy instead of an oracle to Algorithm 2. However, the policy can be equivalently used to sample trajectories. Note that the idea of HPGD with soft Q-learning is a double-loop methods that aims to find an $\\delta$ -oracle.Similar a $\\delta$ -oracle idea also appears in conditional stochastic optimization [38, 35], in bilevel optimization [28]. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Aiguiiuuu / il UL wiu vaa suit Q-ivaimmg Input: Iterations $T$ , Precision param. $K$ , Learning rate $\\alpha$ , Regularization $\\lambda$ , Initial point $x_{0}$ behavioural policy $\\pi_{B}$ , Q-learning rates $\\{\\alpha_{t}\\}_{t\\ge0}$ for $t=0$ to $T-1$ do $\\xi\\sim\\mathbb{P}_{\\xi}$ $\\begin{array}{r}{\\pi^{t_{K}}\\gets\\mathsf{S o f t Q1e a r n i n g}_{x_{t},\\xi}(t_{K},\\pi_{B},\\{\\alpha_{t}\\}_{t\\geq0})\\,(\\mathrm{Algorithm}\\,4)}\\end{array}$ $s\\sim\\nu$ and $a\\sim\\pi^{t_{K}}(\\cdot;s)$ $\\begin{array}{r l}&{\\bar{\\partial_{x}}A^{\\pi^{t_{K}}}(s,a)\\gets\\mathtt{G r a d i e n t E s t i m a t o r}(\\xi,x_{t},s,a,\\pi^{t_{K}})\\,(\\mathrm{Algorit}\\,}\\\\ &{\\overbrace{\\frac{d F}{d x}}^{\\widehat{d F}}\\gets\\frac{\\partial_{1}f(x_{t},\\pi^{t_{K}},\\xi)}{\\partial x}+\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x_{t},\\pi^{t_{K}},\\xi)}{\\partial\\pi(s,a)}\\partial_{x}\\bar{A}^{\\pi^{t_{K}}}(s,a)}\\\\ &{x_{t+1}\\gets x_{t}-\\alpha\\overbrace{\\frac{d F}{d x}}^{\\widehat{d F}}}\\end{array}$ hm 2) end for Output: ${\\hat{x}}_{T}\\sim\\operatorname{Uniform}(\\left\\{x_{0},\\dots,x_{T-1}\\right\\})$ ", "page_idx": 33}, {"type": "text", "text": "Randomly-Truncated soft Q-learning (RT-Q) ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Let us now turn to RT-Q, for which we provide the pseudocode in Algorithm 8. As above, we denote by $t_{k}=O(k2^{k})$ the number of iterations soft Q-learning needs to achieve $\\lambda\\mathbb{E}\\big|\\big|\\pi^{t_{k}}-\\pi^{*}\\big|\\big|_{\\infty}^{2}\\leq$ $\\mathbb{E}\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\|_{\\infty}^{2}\\leq2^{-k}$ . We slightly abuse notation in the Pseudocode, such that we do not just return the last soft Q-learning iteration but also the second last and the first. Moreover we denote by $\\begin{array}{r}{p_{k}=\\frac{2^{-k}}{1-2^{-K}}}\\end{array}$ and we generally use $\\widehat{x}$ todenote stmate frma single samle and $\\widetilde{x}$ to denote averaged estimates from multiple samples. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 8 HPGD with RT-Q ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Input: Iterations $T$ , Precision param. $K$ , Learning rate $\\alpha$ , Regularization $\\lambda$ , Initial point $x_{0}$   \nbehavioural policy $\\pi_{B}$ , Q-learning rates $\\{\\alpha_{t}\\}_{t\\ge0}$   \nfor $t=0$ to $T-1$ do $\\xi\\sim\\mathbb{P}_{\\xi}$ k\\~pk \u03c0t,\u03c0,\u03c0 SoftQlearni $\\mathsf{n g}_{x_{t},\\xi}(t_{k+1},\\pi_{B},\\{\\alpha_{t}\\}_{t\\geq0})$ (Algorithm 4) s\\~v,a \\~\u03c0tk+1(;s) and $a^{\\prime}\\sim\\pi^{t_{1}}(\\cdot;s)$ $\\begin{array}{r l}&{\\partial_{x}\\overline{{A^{\\tau+\\tau}}}(s,a)\\gets\\frac{1}{2^{s}}\\sum_{l=1}^{2^{s}}\\mathrm{Gradientfostimator}(\\xi,x_{t},s,a,\\pi^{t+})(\\mathrm{Algoitihm}\\,2)}\\\\ &{\\partial_{x}\\overline{{A^{\\tau+1}}}(s,a)\\gets\\frac{1}{2^{s}}\\sum_{l=1}^{2^{s}}\\mathrm{Gradientfostiontfostimator}(\\xi,x_{t},s,a,\\pi^{t+})}\\\\ &{\\partial_{y}\\overline{{A^{\\tau+1}}}(s,a^{\\prime})\\gets\\mathrm{Gradientfostimator}(\\xi,x_{t},s,a^{\\prime},\\pi^{t+})}\\\\ &{\\frac{d F_{t_{k+1}}}{d s}\\gets\\frac{\\partial_{t}f(x,x^{\\tau+1},\\xi)}{\\partial x}+\\frac{1}{\\lambda^{\\nu}(s)}\\frac{\\partial_{t}f(x,t^{\\ast}+\\xi,s,\\xi)}{\\partial\\tau(s,a)}\\partial_{x}\\overline{{A^{\\tau+1}}}(s,a)}\\\\ &{\\frac{d F_{t_{k+1}}^{\\{\\{+\\}}}}{d s}=\\frac{\\partial_{t}f(x,\\pi^{t}+\\xi)}{\\partial x}+\\frac{\\pi^{t}(a,s)}{\\pi^{t+}(s,a)}\\frac{1}{\\lambda^{\\nu}(s)}\\frac{\\partial_{t}f(x,\\pi^{t}+\\xi)}{\\partial\\tau(s,a)}\\partial_{x}\\overline{{A^{\\tau+\\tau}}}(s,a)}\\\\ &{\\frac{d F_{t_{k}}}{d s}=\\frac{\\partial_{t}f(x,\\pi^{t}+\\xi)}{\\partial x}+\\frac{1}{\\lambda^{\\nu}(s)}\\frac{\\partial_{t}f(x,\\pi^{t}+\\xi)}{\\partial\\tau(s,a^{\\prime})}\\partial_{x}\\overline{{A^{\\tau+1}}}(s,a^{\\prime})}\\\\ &{\\frac{d F_{t_{k+1}}^{\\{\\{+\\}}}}{d s}=\\frac{\\overline{{\\hat{d}_{t_{k+1}}}}}{d s}+\\frac{\\overline{{\\hat{d}_{t_{k+1}}}}}{d s}\\frac{\\partial_{t}f(x,\\pi^{t}+\\xi)}{\\partial\\tau^{t}}}\\\\ &{\\frac{d F_{t_{k+1}}^ $   \nend for   \nOutput: ${\\hat{x}}_{T}\\sim\\operatorname{Uniform}(\\left\\{x_{0},\\dots,x_{T-1}\\right\\})$ ", "page_idx": 33}, {"type": "text", "text": "Proof. First let us specify that \u201cbias\"\u2019 refers to the bias of the hypergradient estimator, i.e. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\frac{d F(x_{t})}{d x}-\\widehat{\\frac{d F(x_{t})}{d x}}\\right]\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Equivalently for \"variance\u201d we mean the variance of the estimator i.e. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\widehat{\\frac{d F(x_{t})}{d x}}-\\mathbb{E}\\left[\\widehat{\\frac{d F(x_{t})}{d x}}\\right]\\right\\|_{\\infty}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Moreover, the \u201citeration complexity\u201d is the number of soft Q-learning iterations needed to solve the lowerlevel. ", "page_idx": 34}, {"type": "text", "text": "For Vanilla soft Q-learning, we can just combine previous results to compute the iteration complexity and variance to achieve a bias of $2^{\\bar{-}K/2}$ . For RT-Q, we formalize the intuition of Section 4 to show we can achieve the same bias with only $\\mathcal{O}(K^{2})$ iterations because we rarely perform many soft Q-learning iterations but assign a relatively higher magnitude to these few accurate hypergradient estimates. This necessarily increases variance and most of the proof will be spent on how to bound it. Here we \u201cdivide and conquer\u201d the variance until we have easy terms that depend linearly on ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\pi_{x,\\xi}^{t_{k}}-\\pi_{x,\\xi}^{*}\\right\\rVert_{\\infty}^{2}\\right]=\\mathcal{O}(2^{-k})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "or related terms we can easily bound. To decompose the variance, our main tool will be the following identity ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|a+b\\right\\|^{2}=\\left\\|a\\right\\|^{2}+2\\left\\|a\\right\\|\\left\\|b\\right\\|+\\left\\|b\\right\\|^{2}\\leq2\\left\\|a\\right\\|+2\\left\\|b\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which we have already derived and used in the proof of Theorem 3 and also follows from the Parallelogram Law. ", "page_idx": 34}, {"type": "text", "text": "Vanilla soft $Q$ learning ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We start with the analysis of using HPGD with vanilla soft Q-learning to estimate dF(\u00b1) In Theorem 3, weshowed that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\frac{d F(x_{t})}{d x}-\\widehat{\\frac{d F(x_{t})}{d x}}\\right]\\right\\|_{\\infty}=\\mathcal{O}(\\delta),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{o}\\left[\\left\\|\\pi_{x,\\xi}^{*}-\\pi_{x,\\xi}^{o}\\right\\|_{\\infty}^{2}\\right]\\leq\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In Proposition 6, we show that after $t_{K}=\\mathcal{O}(K2^{K})$ soft Q-learning iterations, it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\pi_{x,\\xi}^{t_{K}}-\\pi_{x,\\xi}^{*}\\right\\|_{\\infty}^{2}\\right]=\\mathcal{O}(2^{-K}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Where $\\pi_{x,\\xi}^{t_{K}}$ denotesther $t_{K}$ -th iterate ofthe soft Q-learmin algorithm. Theresut forcomplexity and bias follow directly. It remains to bound the variance. However, we have already shown in Theorem 3 that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\widehat{\\frac{d F_{t_{K}}}{d x}}-\\mathbb{E}\\left[\\widehat{\\frac{d F_{t_{K}}}{d x}}\\right]\\right\\rVert_{\\infty}^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\lVert\\widehat{\\frac{d F(x_{t})}{d x}}\\right\\rVert_{\\infty}^{2}\\right]=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Randomly-Truncated soft $Q$ learning $(R T{\\mathcal{Q}})$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We first show that $\\frac{d F_{t_{K}}^{R T}}{d x}$ dRi,k . For this observe that the following estimators have the same mean: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall k:\\mathbb{E}\\left[\\frac{\\widetilde{d F_{t_{k}}^{t_{k+1}}}}{d x}\\right]=\\mathbb{E}\\left[\\frac{\\widetilde{d F_{t_{k}}}}{d x}\\right]=\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}}}{d x}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The first equality holds because of importance sampling and the second inequality holds by the linearity of expectation. Plugging in these identities, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}^{t r}}}{d x}\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}}}{d x}\\right]+\\underset{k=1}{\\overset{K}{\\sum}}p_{k}\\frac{1}{p_{k}}\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k+1}}}}{d x}-\\frac{\\widehat{d F_{t_{k}}^{t_{k+1}}}}{d x}\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}}}{d x}\\right]+\\underset{k=1}{\\overset{K}{\\sum}}p_{k}\\frac{1}{p_{k}}\\left(\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k+1}}}}{d x}\\right]-\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}}}{d x}\\right]\\right)}\\\\ &{=\\mathbb{E}\\left[\\frac{\\widehat{d F_{t_{k}}}}{d x}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It follows directly that the hypergradient estimators obtained by RT-Q and vanilla soft Q-learning must have thesamebias. ", "page_idx": 35}, {"type": "text", "text": "For a sampled $k\\in\\{1,\\ldots,K\\}$ : soft Q-learning has an iteration complexity $c_{k}$ of $O(k2^{k})$ to build the following hypergradient estimator: ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{d F_{t_{K}}^{R T}}{d x}}={\\frac{\\widetilde{d F_{t_{1}}}}{d x}}+{\\frac{\\widetilde{\\frac{d F_{t_{k+1}}}{d x}}-{\\frac{\\widetilde{d F_{t_{k}}^{t_{k+1}}}}{d x}}}{p_{k}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As we sample any $k$ with probability $\\begin{array}{r}{p_{k}=\\frac{2^{-k}}{1-2^{-K}}}\\end{array}$ , the expected iteration complexity is then given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{k=1}^{K}c_{k}p_{k}}}\\\\ {{\\displaystyle=\\sum_{k=1}^{K}c_{k}\\frac{2^{-k}}{1-2^{-K}}}}\\\\ {{\\displaystyle=\\sum_{k=1}^{K}\\mathcal{O}\\left(\\frac{k}{2^{-k}}\\right)\\frac{2^{-k}}{1-2^{-K}}}}\\\\ {{\\displaystyle\\le\\mathcal{O}\\left(K\\right)\\sum_{k=1}^{K}\\mathcal{O}(\\frac{1}{1-2^{-K}})}}\\\\ {{\\displaystyle=\\mathcal{O}(K^{2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which proves our claim. The attentive reader will note that RT-Q runs GradientEstimator $2^{k}$ times instead of once and thus samples $2^{k}$ trajectories to estimate the advantage derivative instead of one like vanilla soft Q-learning. Nonetheless, RT-Q has the better sample complexity as both methods need to sample $\\bar{\\mathcal{O}}(k2^{k})$ state action pairs for a given $k$ to run soft Q-learning and then estimate the advantage derivative. The same analysis as for the iteration complexity thus shows that the sample complexity of vanilla soft Q-learning is ${\\mathcal{O}}(K2^{K})$ and $\\mathcal{O}(K^{2})$ for RT-Q. ", "page_idx": 35}, {"type": "text", "text": "It remains to show that the variance is of order $\\mathcal{O}(K)$ . This is the most challenging part of the proof. As outlined previously, we will iteratively decompose the variance until we can bound all terms by $\\mathcal{O}(K)$ ", "page_idx": 35}, {"type": "text", "text": "For better readability we introduce the following notation for a given pair $s,a$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nH_{k}(1)={\\frac{\\partial_{1}f(x,\\pi^{t_{k}},\\xi)}{\\partial x}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{H_{k}(2)=\\left\\{\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x,\\pi^{t}\\nu,\\xi)}{\\partial\\pi(s,a)}\\widetilde{\\partial_{x}A^{\\pi^{t}k}}(s,a)\\right.\\ \\mathrm{~if~}k=1}\\\\ {\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x,\\pi^{t}\\nu,\\xi)}{\\partial\\pi(s,a)}\\widetilde{\\partial_{x}A^{\\pi^{t}k}}(s,a)\\ \\ \\mathrm{~if~}k>1}\\\\ {H_{k}^{k+1}(2)=\\frac{1}{\\lambda\\nu(s)}\\frac{\\pi^{t}k(a;s)}{\\pi^{t}\\kappa+1}\\frac{\\partial_{2}f(x,\\pi^{t}\\nu,\\xi))}{\\partial\\pi(s,a)}\\widehat{\\partial_{x}A^{\\pi^{t}k}}(s,a)}\\\\ {H^{*}(1)=\\frac{\\partial_{1}f(x,\\pi_{x,\\xi}^{*},\\xi)}{\\partial x}}\\\\ {H^{*}(2)=\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{2}f(x,\\pi_{x,\\xi}^{*},\\xi))}{\\partial\\pi(s,a)}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi^{*},\\xi}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then for a given $\\xi,s,a$ (which are sampled at the beginning of RT-Q) we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{d}{d x}F_{t\\kappa}^{R T}=H_{1}(1)+H_{1}(2)+\\frac{H_{k+1}(1)+H_{k+1}(2)-H_{k}(1)-H_{k}^{k+1}(2)}{p_{k}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now let us decompose the variance of the RT-Q hypergradient estimator using the newly introduced notation: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{d}{d x}F_{t\\kappa}^{R T}-\\mathbb{E}\\left[\\frac{d}{d x}F_{t\\kappa}^{R T}\\right]\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{d}{d x}F_{t\\kappa}^{R T}-\\displaystyle\\frac{d}{d x}F(x)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{d}{d x}F_{t\\kappa}^{R T}-H_{1}(1)-H_{1}(2)\\right\\|_{\\infty}^{2}\\right]+2\\mathbb{E}\\left[\\left\\|H_{1}(1)+H_{1}(2)-\\displaystyle\\frac{d}{d x}F(x)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq4\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{H_{k+1}(1)-H_{k}(1)}{p_{k}}\\right\\|_{\\infty}^{2}\\right]+4\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{H_{k+1}(2)-H_{k}^{k+1}(2)}{p_{k}}\\right\\|_{\\infty}^{2}\\right]+2\\mathbb{E}\\left[\\left\\|H_{1}(1)+H_{1}(2)-\\displaystyle\\frac{d}{d x}\\right\\|_{\\infty}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We proceed to bound the individual terms. ", "page_idx": 36}, {"type": "text", "text": "Note that (3) is indepedent of $k$ . Using Equation (15) in the analysis of Theorem 3, we can bound it as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(3)\\leq4\\left(\\left(\\displaystyle\\frac{K_{2}}{1-\\gamma}+\\displaystyle\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|\\mathcal{A}|)}{(1-\\gamma)^{2}}\\right)+\\displaystyle\\frac{1}{1-\\gamma}K_{2}+\\displaystyle\\frac{\\gamma}{1-\\gamma}K_{1}\\displaystyle\\frac{\\overline{{R}}+\\lambda\\log|\\mathcal{A}|}{1-\\gamma^{0.5}}\\right)^{2}}\\\\ &{\\quad=\\mathcal{O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(1) is also relatively easy to bound, as shown below: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(1)=\\sum_{k=1}^{K}\\frac{1}{p_{k}}\\mathbb{E}\\left[\\left\\|H_{k+1}(1)-H_{k}(1)\\right\\|_{\\infty}^{2}\\right]}\\quad}&{}\\\\ &{=\\sum_{k=1}^{K}\\frac{1}{p_{k}}\\mathbb{E}\\left[\\left\\|\\frac{\\partial_{1}f(x,\\pi^{t_{k+1}},\\xi)}{\\partial x}-\\frac{\\partial_{1}f(x,\\pi^{t_{k}},\\xi)}{\\partial x}\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\le\\sum_{k=1}^{K}\\frac{1}{p_{k}}S_{j}^{2}\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}-\\pi^{t_{k}}\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\le\\sum_{k=1}^{K}\\frac{1}{p_{k}}S_{j}^{2}\\left(\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}-\\pi_{x,\\xi}^{*}\\right\\|_{\\infty}^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\pi^{t_{k}}-\\pi_{x,\\xi}^{*}\\right\\|_{\\infty}^{2}\\right]\\right)}\\\\ &{\\le\\sum_{k=1}^{K}\\frac{1}{p_{k}}S_{j}^{2}\\left(\\frac{1}{2^{k+1}}+\\frac{1}{2^{k}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\sum_{k=1}^{K}2^{k}S_{f}^{2}\\frac{3}{2^{k}}}\\\\ {\\displaystyle=3S_{f}^{2}K}\\\\ {\\displaystyle=\\mathcal{O}(K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the frs equality we simply usethat $k$ is sampled with probabity $\\begin{array}{r}{p_{k}=\\frac{2^{-k}}{1-2^{-K}}}\\end{array}$ Inthe second equality we plug in the definitions of $H_{k+1}(1)$ and $H_{k}(1)$ . In the first inequality we use the smoothness of $f$ . The second inequality uses Equation (16). The third inequality follows from the fact that $t_{k+1}$ is chosen to guarantee an expected distance of at most $\\frac{1}{2^{k+1}}$ 0 $\\pi_{x,\\xi}^{*}$ The remaining equalities follow from plugging in $p_{k}$ and rearranging terms. ", "page_idx": 37}, {"type": "text", "text": "Now we want to repeat the same analysis again for (2). As for (1) we get a sum over $k$ with the factor $\\displaystyle\\frac{1}{p_{k}}$ which w eed to compensatebybounding $\\mathbb{E}\\left[\\left\\lVert H_{k+1}(2)-H_{k}^{k+1}(2)\\right\\rVert_{\\infty}^{2}\\right]$ $O(2^{-k})$ However, bounding the later is more invovled than our analysis for (1). In the following pages, we williteratively apply Equation (16) until the terms get easy enough, such that we can use one of the following two facts about RT-Q: ", "page_idx": 37}, {"type": "text", "text": "1. $t_{k}=\\mathcal{O}(k2^{k})$ is chosen sucht that $\\lambda\\mathbb{E}\\|\\pi^{t_{k}}-\\pi^{*}\\|_{\\infty}^{2}\\leq\\mathbb{E}\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\|_{\\infty}^{2}\\leq2^{-k}$ 2. $\\partial_{x}A^{\\pi^{t_{k+1}}}(s,a)$ andrelatedtmeanaeraestmave $2^{k}$ trajectory samples, such that their variance (with respect to these random rollouts) is $\\mathcal{O}\\big(\\frac{1}{2^{k}}\\big)$ ", "page_idx": 37}, {"type": "text", "text": "We briefly note that in the analysis below we will sometimes write $Q(s,a)$ or $A(s,a)$ inside the infinity norm for ease of exposition. When we do so, the infinity norm is still interpreted as the maximum over all possible $s,a$ and not as the absolute value of the $\\mathrm{^Q}$ -function or advantage for a specific $s,a$ ", "page_idx": 37}, {"type": "text", "text": "Let us start decomposing the numerator of (2): ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|H_{++1}(2)-H_{+}^{k+1}(2)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\frac{1}{\\mathcal{N}(s)}\\frac{\\partial_{j}\\int(x,\\tau^{k},\\xi)\\,_{\\tau}^{*}}{\\partial\\pi(s,0)}\\frac{1}{\\partial x^{k+1}}(s,a)-\\frac{1}{\\lambda\\nu(s)}\\frac{\\pi^{k}(x,\\xi)}{\\pi^{k+1}(x,\\xi)}\\frac{\\partial_{j}\\int(x,\\tau^{k},\\xi))}{\\partial\\pi(s,a)}\\frac{1}{\\partial x^{k+1}}(s,a)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\|\\frac{1}{\\mathcal{N}(s)}\\frac{\\partial_{j}\\int(x,\\tau^{k},\\xi)}{\\partial\\pi(s,a)}\\frac{1}{\\partial x^{k+1}}(s,a)\\right\\|_{\\infty}^{2}\\left\\|\\frac{\\pi^{k+1}(x,\\xi)}{\\pi^{k+1}(x,\\xi)}-\\frac{\\pi^{k}(x,\\xi)}{\\pi^{k+1}(x,\\xi)}\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\quad+2\\mathbb{E}\\left[\\left\\|\\frac{\\pi^{k}(x,\\tau^{k},\\xi)}{\\pi^{k+1}(x,\\xi)}\\right\\|_{\\infty}^{2}\\left\\|\\frac{1}{\\mathcal{N}(s)}\\frac{\\partial_{j}\\int(x,\\tau^{k+1},\\xi)}{\\partial\\pi(s,a)}\\partial_{j}\\widehat{x}^{\\pi^{k+1}(s,\\xi)}(s,a)-\\frac{1}{\\lambda\\nu(s)}\\frac{\\partial_{j}\\int(x,\\tau^{k},\\xi)}{\\partial\\pi(s,a)}\\widehat{\\partial_{x}A^{\\pi}}\\right\\|_{\\infty}^{2}}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\|\\frac{1}{\\mathcal{N}(s)}\\frac{\\partial_{j}\\int(x,\\tau^{k},\\xi)}{\\partial\\pi(s,a)}\\widehat{\\partial_{x}A^{\\pi^{k+1}}}(s,a)\\right\\|_{\\infty}^{2}\\left\\|\\frac{\\pi^{k+1}(x,\\xi)}{\\partial\\pi(s,a)}-\\pi^{k}(\\alpha;s)\\right\\|_{\\infty}^{2}\\left\\|\\frac{1}{\\pi^{k+1}(x,\\xi)}\\\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The first equality is just plugging in definitions. The first inequality follows from Equation (16) and the second inequality comes from the Cauchy-Schwarz inequality. ", "page_idx": 37}, {"type": "text", "text": "Let us begin with bounding $(\\mathrm{i})$ . First we want to tackle the fractions of the policies. For this, recall that $\\forall x,\\xi,s,a:|r_{x,\\xi}(s,a)|<\\overline{{R}}$ and that $0\\leq H(\\pi;s)\\leq\\log|A|$ (by Jensen's inequality). Thus we have that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{-\\overline{{{R}}}}{1-\\gamma}\\leq Q_{\\lambda}(s,a)\\leq\\frac{\\overline{{{R}}}+\\lambda\\log|\\cal A|}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The above bounds extend to any soft Q-learning estimate, which can be obtained by Algorithm 4, since by Assumption 3.1 the algorithm cannot observe rewards with greater magnitude than $\\overline{{R}}$ .As Algorithm 4 returns a softmax policy, i.e. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pi^{t_{k}}(a;s)=\\frac{\\exp(Q^{t_{k}}(a;s)/\\lambda)}{\\sum_{a^{\\prime}}\\exp(Q^{t_{k}}(a^{\\prime};s)/\\lambda)},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "it holds that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall t_{k},\\forall s,\\forall a:\\frac{\\exp(\\frac{\\overline{{R}}+\\lambda\\log|A|}{\\lambda(1-\\gamma)})}{|A|\\exp(\\frac{-\\overline{{R}}}{\\lambda(1-\\gamma)})}\\geq\\pi^{t_{k}}(a;s)\\geq\\frac{\\exp(\\frac{-\\overline{{R}}}{\\lambda(1-\\gamma)})}{|A|\\exp(\\frac{\\overline{{R}}+\\lambda\\log|A|}{\\lambda(1-\\gamma)})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exists M_{1}<\\infty,\\forall t_{k}:\\left\\|\\frac{1}{\\pi^{t_{k}}(a;s)}\\right\\|_{\\infty}^{2}\\leq M_{1}-}\\\\ &{\\exists M_{2}<\\infty,\\forall t_{k}:\\left\\|\\frac{\\pi^{t_{k}}(a;s)}{\\pi^{t_{k+1}}(a;s)}\\right\\|_{\\infty}^{2}\\leq M_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using this we can bound (i). Let $m=\\operatorname*{min}_{s}\\nu(s)$ , then: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\frac{M_{1}}{\\lambda m}2\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{\\gamma}{1-\\gamma}K_{1}\\frac{\\overline{{R}}+\\lambda\\log|\\mathcal{A}|}{1-\\gamma^{0.5}}\\right)\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}(a;s)-\\pi^{t_{k}}(a;s)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\leq\\mathcal{O}\\left(2\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}(a;s)-\\pi_{x,\\xi}^{*}(a;s)\\right\\|_{\\infty}^{2}\\right]+2\\mathbb{E}\\left[\\left\\|\\pi^{t_{k}}(a;s)-\\pi_{x,\\xi}^{*}(a;s)\\right\\|_{\\infty}^{2}\\right]\\right)}\\\\ &{\\leq\\mathcal{O}\\left(\\frac{1}{2^{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The first inequality uses Equation (15). The second inequality uses Equation (16) and the last equality uses the convergence of soft Q-learning. ", "page_idx": 38}, {"type": "text", "text": "Now we turn to bound (ii): ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta_{1}\\leq\\frac{\\lambda\\alpha_{2}^{2}}{\\lambda}\\Bigg[\\Bigg|\\frac{\\partial\\alpha_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)-\\frac{\\partial\\beta_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)\\Bigg|_{s,\\alpha}^{2}}\\\\ {\\leq\\frac{\\lambda\\alpha_{2}^{2}}{\\lambda}\\Bigg[\\Bigg|\\frac{\\partial\\alpha_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x}(s,a)-\\frac{\\partial\\beta_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)\\Bigg|_{s,\\alpha}^{2}}\\\\ {+\\frac{\\varepsilon}{\\lambda}\\Bigg]\\Bigg|\\frac{\\partial\\alpha_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)-\\frac{\\partial\\beta_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)\\Bigg|_{s,\\alpha}^{2}}\\\\ {\\leq\\frac{\\lambda\\alpha_{2}^{2}}{\\lambda}\\Bigg[\\Bigg|\\frac{\\partial\\alpha_{2}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}\\frac{\\partial\\alpha_{1}^{2}\\lambda^{2}}{\\partial x^{2}}\\Bigg|_{s,\\alpha}^{2}\\Bigg|_{s,\\alpha}^{2}\\Bigg|_{s,\\alpha}^{2}\\Bigg|_{s,\\alpha}^{2}\\Bigg|_{s,\\alpha}^{2}}\\\\ {+\\Bigg|\\frac{\\partial\\alpha_{2}^{2}\\lambda^{2}}{\\partial x^{2}}(s,a)\\Bigg|_{s,\\alpha}^{2}\\Bigg|_{s,\\alpha}^{2}\\frac{\\partial\\alpha_{1}^{2}(x,t+\\varepsilon)}{\\partial x}_{(k)}-\\frac{\\partial\\beta_{2}^{2}(x,t+\\varepsilon)}{\\partial x(k)}\\Bigg| \n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The first inequality uses $M_{2}$ to bound the policy fraction the definition of $m\\,=\\,\\operatorname*{min}_{s}\\nu(s)$ and Cauchy-Schwarz. The second inequality uses Equation (16) and Cauchy-Schwarz. For the final inequality, we rearrange and use the Lipschitz continuity of $f$ (cf. Assumption 3.1). ", "page_idx": 38}, {"type": "text", "text": "(A) is relatively easy to bound as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(A)}\\leq\\left(2\\left(\\displaystyle\\frac{K_{2}}{1-\\gamma}+\\displaystyle\\frac{\\gamma}{1-\\gamma}K_{1}\\displaystyle\\frac{\\overline{{R}}+\\lambda\\log|\\mathcal{A}|}{1-\\gamma^{0.5}}\\right)\\right)^{2}S_{f}\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}(a;s)-\\pi^{t_{k}}(a;s)\\right\\|_{\\infty}^{2}\\right]}\\\\ &{\\quad\\leq\\mathcal{O}\\left(2\\mathbb{E}\\left[\\left\\|\\pi^{t_{k+1}}(a;s)-\\pi_{x,\\xi}^{*}(a;s)\\right\\|_{\\infty}^{2}\\right]+2\\mathbb{E}\\left[\\left\\|\\pi^{t_{k}}(a;s)-\\pi_{x,\\xi}^{*}(a;s)\\right\\|_{\\infty}^{2}\\right]\\right)}\\\\ &{\\quad\\leq\\mathcal{O}\\left(\\displaystyle\\frac{1}{2^{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In the first inequality we use the smoothness of $f$ and Equation (15). In the second inequality we use Equation (16) and in the final inequality the convergence of soft Q-learning. ", "page_idx": 39}, {"type": "text", "text": "Now, we turn our attention towards bounding (B). First, notice that both advantage derivatives are evaluated for the same state-action pair. This is because in Algorithm 8 we sample $a$ once according to $\\pi^{t_{k+1}}$ and reuse the same $a$ for $A^{\\pi^{t_{k}}}(s,a)$ by doing importance sampling. Without this trick, it would be hopeless to bound (B). ", "page_idx": 39}, {"type": "text", "text": "When bounding (B), we need to consider two sources of randomness. First, there is the randomness in the soft Q-learning iterations. Second, we have to account for the randomness over the trajectory rollouts of GradientEstimator to estimate $\\widetilde{\\partial_{x}A^{\\pi}}$ . GradienEstimator first separately estimates $\\widehat{\\partial_{x}V^{\\pi^{t}\\boldsymbol{k}}}$ and $\\widehat{\\partial_{x}V^{\\pi^{t}\\boldsymbol{k}}}$ from a trajectory and then returns $\\widehat{\\partial_{x}A^{\\pi^{t_{k}}}}=\\widehat{\\partial_{x}Q^{\\pi^{t_{k}}}}-\\widehat{\\partial_{x}V^{\\pi^{t_{k}}}}$ . We therefore denote by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\widetilde{\\partial_{x}V^{\\pi^{t_{k}}}}:=\\frac{1}{2^{k}}\\sum_{l=1}^{2^{k}}\\widehat{\\partial_{x}V^{\\pi^{t_{k}}}}(\\tau_{l}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "the average over the $2^{k}$ value function derivatives estimated as part of the $2^{k}$ GradientEstimator procedures performed in RT-Q. Note because of the unbiasedness of GradienEstimator (cf. Proposition 3), it holds that $\\partial_{x}V^{\\pi^{t_{k}}}=\\mathbb{E}_{\\tau}\\left[\\widetilde{\\partial_{x}V^{\\pi^{t_{k}}}}\\right]$ ", "page_idx": 39}, {"type": "text", "text": "Using the above notation, we get the following bound by repeatedly applying Equation (16): ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbb{B})\\leq2\\mathbb{E}\\bigg\\|\\widehat{\\partial_{x}\\mathcal{G}^{\\pi^{*}}}(s,\\alpha)-\\partial_{x}\\widehat{Q}^{\\pi^{*}+1}(s,\\alpha)\\bigg\\|_{\\infty}^{2}+2\\mathbb{E}\\bigg\\|\\widehat{\\partial_{x}\\mathcal{V}^{\\pi^{*}}}(s)-\\partial_{x}\\widehat{V}^{\\pi^{*}+1}(s)\\bigg\\|_{\\infty}^{2}}\\\\ &{\\qquad\\leq4\\mathbb{E}\\bigg\\|\\widehat{\\partial_{x}\\mathcal{G}^{\\pi^{*}}}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)\\bigg\\|_{\\infty}^{2}+4\\mathbb{E}\\bigg\\|\\partial_{x}\\widehat{Q}^{\\pi^{*}+1}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)\\bigg\\|_{\\infty}^{2}}\\\\ &{\\qquad\\quad+4\\mathbb{E}\\bigg\\|\\partial_{x}\\widehat{V}^{\\pi^{*}}(s)-\\partial_{x}\\mathcal{V}^{\\pi^{*}}(s)\\bigg\\|_{\\infty}^{2}+4\\mathbb{E}\\bigg\\|\\partial_{x}\\widehat{V}^{\\pi^{*}+1}(s)-\\partial_{x}\\mathcal{V}^{\\pi^{*}}(s)\\bigg\\|_{\\infty}^{2}}\\\\ &{\\qquad\\leq8\\mathbb{E}\\bigg\\|\\widehat{\\partial_{x}\\mathcal{G}^{\\pi^{*}}}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)\\bigg\\|_{\\infty}^{2}+8\\mathbb{E}\\bigg\\|\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)\\bigg\\|_{\\infty}^{2}}\\\\ &{\\qquad+8\\mathbb{E}\\bigg\\|\\partial_{x}\\widehat{Q}^{\\pi^{*}+1}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}+1}(s,\\alpha)\\bigg\\|_{\\infty}^{2}+8\\mathbb{E}\\bigg\\|\\partial_{x}Q^{\\pi^{*}+1}(s,\\alpha)-\\partial_{x}Q^{\\pi^{*}}(s,\\alpha)\\bigg\\|_{\\infty}^{2}}\\\\ &{\\qquad+8\\mathbb{E}\\bigg\\|\\widehat{\\partial_{x}\\mathcal{V}^{\\pi^{*}}}(s)-\\partial_{x}\\mathcal{V}^{\\pi^{*}}(s)\\bigg\\|_{\\infty}^{2}+8\\mathbb{E}\\bigg \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Here $Q_{\\lambda}^{*}$ denotes the optimal Q-function, i.e. for the policy $\\pi_{x,\\xi}^{*}$ . In the equations above we have two flavours of terms. The first are the differences between the derivative estimator and its expectation for a given policy and the second are the differences between the expected derivative under the learned and optimal policy. We start by bounding the first kind of terms. Recall that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\widetilde{\\partial_{x}Q^{\\pi^{t_{k}}}}=\\frac{1}{2^{k}}\\sum_{l=1}^{2^{k}}\\widehat{\\partial_{x}Q^{\\pi^{t_{k}}}}(\\tau_{l}),\\quad\\widetilde{\\partial_{x}V^{\\pi^{t_{k}}}}=\\frac{1}{2^{k}}\\sum_{l=1}^{2^{k}}\\widehat{\\partial_{x}V^{\\pi^{t_{k}}}}(\\tau_{l})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "As the second moment of $\\widehat{\\partial_{x}Q^{\\pi^{t_{k}}}(\\tau_{l})}$ and $\\widehat{\\partial_{x}V^{\\pi^{t}k}}\\left(\\tau_{l}\\right)$ is bounded, we have that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tau}\\left[\\left\\|\\widetilde{\\partial_{x}Q^{\\pi^{t_{k}}}}-\\partial_{x}Q^{\\pi^{t_{k}}}\\right\\|_{\\infty}^{2}\\right]=\\mathcal{O}(2^{-k}),\\quad\\mathbb{E}_{\\tau}\\left[\\left\\|\\widetilde{\\partial_{x}V^{\\pi^{t_{k}}}}-\\partial_{x}V^{\\pi^{t_{k}}}\\right\\|_{\\infty}^{2}\\right]=\\mathcal{O}(2^{-k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "since we use $2^{k}$ sampled trajectories. The same analysis of course also holds for $t_{k+1}$ and thus plugging this in, we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{({\\bf B})\\leq\\mathcal{O}(\\frac{1}{2^{k}})+8\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{k}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}+8\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{\\pi^{k}+1}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}}\\\\ &{\\quad+8\\mathbb{E}\\left\\|\\partial_{x}V^{\\pi^{\\pi^{k}}}(s)-\\partial_{x}V^{\\pi^{*}}(s)\\right\\|_{\\infty}^{2}+8\\mathbb{E}\\left\\|\\partial_{x}V^{\\pi^{\\pi^{k+1}}}(s)-\\partial_{x}V^{\\pi^{*}}(s)\\right\\|_{\\infty}^{2}}\\\\ &{\\quad\\leq\\mathcal{O}(\\frac{1}{2^{k}})+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k+1}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}}\\\\ &{\\quad+16|A|^{2}\\left(\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)^{2}\\left(\\mathbb{E}\\left\\|\\pi^{t_{k}}-\\pi^{*}\\right\\|_{\\infty}^{2}+\\mathbb{E}\\left\\|\\pi^{t_{k+1}}-\\pi^{*}\\right\\|_{\\infty}^{2}\\right)}\\\\ &{\\quad\\leq\\mathcal{O}(\\frac{1}{2^{k}})+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k+1}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In the first inequality, we simply plug in Equation (18). In the third inequality we use the convergence of soft Q-learning and in the second inequality, we use the following identity (cf. Equation (14)): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\partial_{x}V_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{*}}(s)-\\partial_{x}V_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{*}}(s)\\right\\rVert_{\\infty}^{2}}\\\\ &{=\\left\\lVert\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{o}(a;s)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{*}}(s,a)-\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{*}(a;s)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t}^{*}}(s,a)\\right\\rVert_{\\infty}^{2}}\\\\ &{=\\left\\lVert\\displaystyle\\sum_{a}\\left(\\pi_{x_{t},\\xi}^{o}(a;s)-\\pi_{x_{t},\\xi}^{*}(a;s)\\right)\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{o}}(s,a)-\\displaystyle\\sum_{a}\\pi_{x_{t},\\xi}^{*}(a;s)\\left(\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{*}}(s,a)-\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{o}}(s)\\right)\\right\\rVert_{\\infty}^{2}}\\\\ &{\\le2|A|^{2}\\left(\\displaystyle\\frac{K_{1}(\\overline{{R}}+\\lambda\\log|A|)}{(1-\\gamma)^{2}}\\right)^{2}\\left\\lVert\\pi_{x_{t},\\xi}^{o}-\\pi_{x_{t},\\xi}^{*}\\right\\rVert_{\\infty}+2\\left\\lVert\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{o}}(s,a)-\\partial_{x}Q_{\\lambda,x,\\xi}^{\\pi_{e,t,\\xi}^{o}}(s,a)\\right\\rVert_{\\infty}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we use Equation (16) for the last inequality. ", "page_idx": 40}, {"type": "text", "text": "To bound (B), it only remains to upper bound $\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}\\ \\mathrm{~and}$ $\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k+1}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}$ . Below we will derive an upper bound for the former term. The same analysis yields an equivalent bound for the latter term. We denote by $V_{\\lambda}^{*}$ the optimal regularized value function. Note, the analysis is similar to the one performed in the proof of Theorem 3, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\partial_{x}\\bar{Q}^{\\pi^{k}}(s,a)-\\partial_{x}\\bar{Q}_{\\lambda}^{\\pi}(s,a)\\right\\|_{\\infty}}\\\\ &{=\\displaystyle\\left\\|\\sum_{t=0}^{\\infty}\\sum_{s^{\\prime}=\\nu^{\\prime}}^{\\gamma}\\bar{r}^{p_{\\pi_{\\xi}}}(s,a)-s^{\\prime},a^{\\prime};t^{\\prime},\\pi^{\\pi^{k}}\\right|\\left(\\frac{d r_{\\pi_{\\xi}}\\left(s^{\\prime},a^{\\prime}\\right)}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}^{}\\frac{d P_{\\pi_{\\xi}}\\left(s^{\\prime\\prime},s^{\\prime},a^{\\prime}\\right)}{d x}V^{\\pi^{k}}(s^{\\prime\\prime})\\right)\\right.}\\\\ &{\\quad\\left.-\\displaystyle\\sum_{t=0}^{\\infty}\\sum_{s^{\\prime\\prime}=\\nu^{\\prime}}^{\\gamma}\\bar{r}^{p_{\\pi_{\\xi}}}(s,a)-s^{\\prime},a^{\\prime};t^{\\prime},\\pi_{\\pi_{\\xi}}^{*},\\xi\\right)\\left(\\frac{d r_{\\pi_{\\xi}}\\left(s^{\\prime},a^{\\prime}\\right)}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}^{}\\frac{d P_{\\pi_{\\xi}}\\left(s^{\\prime\\prime},s^{\\prime},a^{\\prime}\\right)}{d x}V_{\\pi}^{\\pi_{\\xi}}(s^{\\prime\\prime})\\right)\\right\\|_{\\infty}}\\\\ &{=\\displaystyle\\left\\|\\frac{d r_{\\pi_{\\xi}}\\left(s,a\\right)}{d x}+\\gamma\\sum_{s^{\\prime}=\\nu^{\\prime}}^{d P_{\\pi_{\\xi}}\\left(s^{\\prime},s,a\\right)}V^{\\pi^{k}}(s^{\\prime\\prime})}\\\\ &{\\quad+\\displaystyle\\sum_{t=1}^{\\infty}\\sum_{s^{\\prime\\prime}=\\nu^{\\prime}}^{\\gamma}\\bar{r}^{p_{\\pi_{\\xi}}}(s,a)-s^{\\prime},a^{\\prime};t^{\\prime},\\pi^{\\pi^{k}}\\right)\\left(\\frac{d r_{\\pi_{\\xi}}\\left(s^{\\prime},a^{\\prime}\\right)}{d x}+\\gamma\\sum_{s^{\\prime\\prime}}^{}\\frac{d P_{\\pi_{\\xi}}\\left(s^{\\prime\\prime},s^{\\prime},a^{\\prime}\\right)}{d x}V^{\\pi^{\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\sum_{k=1}^{\\infty}\\sum_{l=1}^{\\infty}\\gamma_{k,l}(\\theta_{l,k})\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}+\\gamma_{\\theta_{l_{1}}}\\alpha_{l}(\\theta_{l,k})\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l})+\\displaystyle\\sum_{l=1}^{\\infty}\\frac{d(\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k})\\alpha_{l}\\alpha_{l}\\alpha_{l})}{d\\ensuremath{\\mathcal{N}}_{l_{1}}}}\\\\ &{\\quad\\le\\gamma_{\\theta}|\\alpha_{l}\\alpha_{l}^{\\prime}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}\\alpha_{l}|_{\\theta_{1}}\\left\\|\\alpha^{\\prime\\prime}\\alpha^{\\prime}\\gamma^{-1}(\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k})\\alpha^{\\prime}\\gamma^{-1}(\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k})\\alpha-\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k})\\alpha_{l})\\right.}\\\\ &{\\quad+\\displaystyle9\\left|\\sum_{l=1}^{\\infty}p(\\theta_{l,k})\\alpha_{l}\\alpha_{l}^{\\prime}\\alpha^{\\prime}\\gamma^{-1}\\right|_{\\theta_{l,k}}\\alpha_{l}^{\\prime}\\alpha_{l}\\left(\\theta_{l,k}^{\\prime}\\alpha_{l}-\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k})\\alpha_{l}^{\\prime}\\alpha_{l}\\right)}\\\\ &{\\quad\\displaystyle\\left(\\frac{d\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k}^{\\prime})\\alpha_{l}}{d\\ensuremath{\\mathcal{N}}_{l_{1}}}+\\gamma_{\\phi}\\frac{d\\ensuremath{\\mathcal{N}}_{l_{2}}(\\theta_{l,k}^{\\prime})\\alpha_{l}^{\\prime}\\alpha_{l}}{d\\ensuremath{\\mathcal{N}}_{l_{2}}}\\right)\\!\\right\\}\\left\\langle\\alpha^{\\prime}\\theta_{l,k}^{\\prime}\\left(\\theta_{l,k}^{\\prime}\\right)\\alpha^{\\prime}\\gamma^{-1}(\\ensuremath{\\mathcal{N}}_{l_{1}}(\\theta_{l,k}^{\\prime})\\alpha_ \n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial\\theta_{i}}(\\nabla^{2}\\theta_{j}\\nabla^{2}\\theta_{i}\\nabla^{2}\\theta_{j}\\nabla^{2}\\theta_{i}\\nabla^{2}\\theta_{j}\\nabla^{2}\\theta^{2})}\\\\ &{\\quad+\\frac{\\theta_{i}}{2}(\\partial_{t}\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i})}}^{(0)}+\\gamma\\frac{\\theta_{i}}{2}\\overline{{h_{j}(\\theta_{j})}}^{(0)})\\nabla^{2}\\theta_{i}\\nabla^{2}\\theta_{j}\\nabla^{2}\\theta_{i}}\\\\ &{\\quad+\\gamma\\frac{\\theta_{i}}{2}(\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i})}}^{(0)}+\\gamma\\frac{\\theta_{i}}{2}\\overline{{h_{j}(\\theta_{j})}}^{(0)})\\Bigg]\\_,}\\\\ &{\\quad+\\gamma\\Bigg|\\frac{\\theta_{i}}{2}(\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i})}}^{(0)}+\\gamma\\frac{\\theta_{i}}{2}\\overline{{h_{j}(\\theta_{j})}}^{(0)})\\nabla^{2}\\theta_{i}}\\\\ &{\\quad-\\frac{\\theta_{i}}{2}\\gamma\\rho^{(1)}(\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i})}}^{(0)})\\nabla^{2}\\theta_{j}\\nabla^{2}\\theta_{i}\\nabla^{2}\\theta_{j}}\\\\ &{\\quad+\\gamma\\frac{\\theta_{i}}{2}(\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i})}}^{(0)}+\\gamma\\frac{\\theta_{i}}{2}\\overline{{h_{j}(\\theta_{j})}}^{(0)})\\Bigg|\\_,}\\\\ &{\\quad+\\gamma\\frac{\\theta_{i}}{2}(\\overline{{\\theta_{i}}}^{(0)}+\\gamma\\frac{\\theta_{j}}{2}\\overline{{h_{i}(\\theta_{i} \n$$$$\n\\begin{array}{r l}&{\\langle\\gamma^{-}\\rangle\\delta\\left|\\frac{\\mathcal{H}_{\\delta,\\varepsilon}(Z^{[\\varepsilon]};u)}{\\mathcal{H}_{\\delta,\\varepsilon}(Z^{[\\varepsilon]};u)}\\right|_{\\infty}\\left\\|\\gamma^{-}(\\nu^{*})-\\gamma^{*}(\\nu^{*})\\right\\|_{\\infty}}\\\\ &{\\quad+\\gamma\\left|\\alpha\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}[\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}]\\right|_{\\infty}\\Bigg|\\sum_{\\{u,v\\}^{\\varepsilon}\\in\\mathcal{V}_{r}}\\gamma\\Bigg|_{\\infty}}\\\\ &{\\quad+\\gamma\\sum_{\\rho}^{\\varepsilon}\\gamma(\\nu_{\\infty,u}^{\\varepsilon})\\nu^{\\varepsilon}(\\nu^{*})\\mu_{\\varepsilon}(Z^{[\\varepsilon]};u)\\Bigg|_{\\infty}\\Bigg|\\nu^{\\varepsilon}\\kappa_{u}^{-}-\\nu^{\\varepsilon}\\Bigg|_{\\infty}}\\\\ &{\\quad+\\gamma\\Bigg|\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}[\\alpha_{\\infty,u}^{\\varepsilon}]\\nu^{\\varepsilon}(\\nu^{*})\\mu_{\\varepsilon}(Z^{[\\varepsilon]};u)-\\delta\\eta^{\\varepsilon}\\nu^{\\varepsilon}(\\nu^{*})\\Bigg|_{\\infty}}\\\\ &{\\quad\\times\\langle\\gamma^{-}\\rangle\\delta\\left|\\frac{\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}(Z^{[\\varepsilon]};u)}{\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}(Z^{[\\varepsilon]};u)}\\right|_{\\infty}\\Bigg|\\nu^{\\varepsilon}\\nu^{\\varepsilon}(\\nu^{*})-\\nu^{\\varepsilon}\\chi_{u}^{\\varepsilon}(\\nu^{*})\\Bigg|_{\\infty}}\\\\ &{\\quad+\\gamma\\left|\\alpha\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}[\\alpha_{\\infty,u}^{\\varepsilon}]\\nu^{\\varepsilon}\\alpha_{\\infty,v}^{\\varepsilon}\\nu^{\\varepsilon}\\right|_{\\infty}}\\\\ &{\\quad+\\gamma\\left|\\alpha\\mathcal{H}_{\\delta,\\varepsilon}^{[\\varepsilon]}[\\alpha_{\\infty,u}^{\\varepsilon}]-\\delta\\alpha_{\\infty,0}^{\\varepsilon}\\nu^{\\varepsilon}(\\nu^{*},\\varepsilon^{*})\\right|_{\\infty}}\\\\ &{\\quad\\times|\\gamma^{\\varepsilon}\\nu_{\\infty,v}^{\\varepsilon}|_{\\infty}\\Bigg|\\nu^{\\varepsilon}\\nu^{\\varepsilon}(\\nu^{*})-\\nu_{\\infty,v}^{\\varepsilon\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The first equality follows from plugging in the result from Theorem 2. The second equality follows by taking out all terms with $t=0$ . The first inequality used the triangle inequality. The second inequality plugs back in the definition from Theorem 2. The third inequality uses Cauchy-Schwarz. ", "page_idx": 41}, {"type": "text", "text": "The fourth inequality follows from simplifying. The fifth inequality uses Equation (12). The sixth inequality uses the geometric sum. ", "page_idx": 42}, {"type": "text", "text": "To simplify the bound above we want to expres $\\left\\|V^{\\pi^{t_{k}}}(s^{\\prime})-V_{\\lambda}^{*}(s^{\\prime})\\right\\|_{\\infty}\\left{\\mathrm{using}}\\,\\left\\|Q^{\\pi^{t_{k}}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}$ and $\\|\\pi^{t_{k}}-\\pi^{*}\\|_{\\infty}$ (as we know the latter two converge for soft Q-learning). We have previously seen in Equation (13) that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|V^{\\pi^{t_{k}}}(s^{\\prime})-V_{\\lambda}^{*}(s^{\\prime})\\right\\|_{\\infty}\\leq\\left\\|Q^{\\pi^{t_{k}}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}+\\left\\|\\pi^{t_{k}}-\\pi^{*}\\right\\|_{\\infty}\\left(\\frac{\\overline{{R}}}{1-\\gamma}+\\lambda|A||\\log l_{2}|+\\frac{2}{l_{2}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here we use $l_{2}$ to denote the minimum possible value that any policy output by soft Q-learning can achieve.Note $l_{2}>0$ , which follows from Equation (17). ", "page_idx": 42}, {"type": "text", "text": "Plugging this result back in, we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q_{\\lambda}^{*}(s,a)\\right\\|_{\\infty}}\\\\ &{\\leq\\!\\displaystyle\\frac{\\gamma}{1-\\gamma}|S|K_{1}\\left(\\left\\|Q^{\\pi^{t_{k}}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}+\\left\\|\\pi^{t_{k}}-\\pi^{*}\\right\\|_{\\infty}\\left(\\displaystyle\\frac{\\overline{{R}}}{1-\\gamma}+\\lambda|A||\\log l_{2}|+\\displaystyle\\frac{2}{l_{2}}\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\frac{\\gamma}{1-\\gamma}\\left(\\displaystyle\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log\\mathcal{A})}{(1-\\gamma)^{2}}\\right)\\left\\|\\pi_{x_{t},\\xi}^{*}-\\pi^{t_{k}}\\right\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From [49][Lemma 24] we know ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\pi^{t_{k}}-\\pi^{*}\\right\\|_{\\infty}\\leq\\frac{1}{\\lambda}\\left\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "With that result we can simplify to ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q_{\\lambda}^{*}(s,a)\\right\\|_{\\infty}}\\\\ &{\\le\\displaystyle\\frac{\\gamma}{1-\\gamma}|\\mathcal{S}|K_{1}\\left(\\left\\|Q^{\\pi^{t_{k}}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}+\\frac{1}{\\lambda}\\left\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}\\left(\\frac{\\overline{{R}}}{1-\\gamma}+\\lambda|A||\\log l_{2}|+\\frac{2}{l_{2}}\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\frac{\\gamma}{1-\\gamma}\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log A)}{(1-\\gamma)^{2}}\\right)\\frac{1}{\\lambda}\\left\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}}\\\\ &{\\le\\left\\|Q^{t_{k}}-Q_{\\lambda}^{*}\\right\\|_{\\infty}\\left(\\frac{\\gamma}{1-\\gamma}|\\mathcal{S}|K_{1}\\left(1+\\frac{1}{\\lambda}\\frac{\\overline{{R}}}{1-\\gamma}+|A||\\log l_{2}|+\\frac{2}{\\lambda l_{2}}\\right)+\\frac{\\gamma}{1-\\gamma}\\left(\\frac{K_{2}}{1-\\gamma}+\\frac{K_{1}(\\overline{{R}}+\\lambda\\log A)}{(1-\\gamma)^{2}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore it follows that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q_{\\lambda}^{*}(s,a)\\right\\rVert_{\\infty}^{2}\\right]=\\mathcal{O}\\left(\\frac{1}{2^{k}}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where we use the convergence of soft Q-learning. ", "page_idx": 42}, {"type": "text", "text": "Using the above analysis, we can now bound: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{B})\\leq\\mathcal{O}(\\frac{1}{2^{k}})+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}+24\\mathbb{E}\\left\\|\\partial_{x}Q^{\\pi^{t_{k+1}}}(s,a)-\\partial_{x}Q^{\\pi^{*}}(s,a)\\right\\|_{\\infty}^{2}}\\\\ &{\\quad=\\mathcal{O}\\left(\\frac{1}{2^{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This allows us to finish bounding (ii) as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{({\\bf\\ddot{u}})\\leq4\\frac{M_{2}}{\\lambda m}({\\bf A})+L_{f}^{2}({\\bf B})}}\\ ~}\\\\ {{\\displaystyle~~~~=\\mathcal{O}(\\frac{1}{2^{k}}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Plugging this result into (2), we thus have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(2)\\leq\\sum_{k=1}^{K}\\displaystyle\\frac{2}{p_{k}}\\left((\\mathrm{i})+(\\mathbf{i}\\mathbf{i})\\right)}\\\\ {\\displaystyle\\qquad=\\sum_{k=1}^{K}2^{k}\\left(\\mathcal{O}(\\displaystyle\\frac{1}{2^{k}})+\\mathcal{O}(\\displaystyle\\frac{1}{2^{k}})\\right)}\\\\ {\\displaystyle\\qquad=\\mathcal{O}(K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Coming back to the start, we get for the variance our desired result as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left\\|\\frac{d}{d x}F_{t\\kappa}^{R T}-\\mathbb{E}\\left[\\frac{d}{d x}F_{t\\kappa}^{R T}\\right]\\right\\|_{\\infty}^{2}\\right]}\\\\ {\\displaystyle~~\\leq4(1)+4(2)+2(3)}\\\\ {\\displaystyle~~=\\mathcal{O}(K)+\\mathcal{O}(K)+\\mathcal{O}(1)}\\\\ {\\displaystyle~~=\\mathcal{O}(K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "E.6Proof of Proposition 1 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Proof. In this proof we will derive an expression for ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{d f(x,\\pi_{\\lambda,x}^{*},\\xi)}{d x}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Applying the Dominated Convergence Theorem then directly gives the expression for the derivative Oof $F(x)$ .Aswefocuson $f$ we can drop any dependence on $\\xi$ below to make the proof more readable and concise. ", "page_idx": 43}, {"type": "text", "text": "Let ", "page_idx": 43}, {"type": "text", "text": "$p(\\mu\\to s,t,\\pi,x)$ denote the probability given the leader's choise $x$ of reaching state $s$ after $t$ steps starting at $\\mu$ and following policy $\\pi$ $p(\\mu\\to s,a,t,\\pi,x)$ denote the probability under choice $x$ of reaching state $s$ after $t$ steps and then taking action $a$ starting at $\\mu$ and following policy $\\pi$ \u00b7 $p(\\mu\\to s,a,s^{\\prime},t,\\pi,x)$ denote the probability under choice $x$ of reaching state $s^{\\prime}$ after $t$ steps having previously been in state $s$ and having taken action $a$ , starting at $\\mu$ and following policy $\\pi$ ", "page_idx": 43}, {"type": "text", "text": "Assuming $\\overline{{V}}(s)$ is differentiable for all $s$ , we show the following statement by induction. ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{i f(x,\\pi_{\\lambda,x}^{*},\\xi)}{d x}=\\sum_{s}\\mu_{x}(s)\\frac{d\\log\\mu_{x}}{d x}\\overline{{V}}(s)+\\sum_{t=1}^{n+1}\\sum_{s}\\sum_{a}\\sum_{s}^{\\prime}\\gamma^{t}p(\\mu_{x}\\to s,a,s^{\\prime},t,\\pi_{\\lambda,x}^{*},x)\\frac{d\\log P_{x}(s^{\\prime};s,x)}{d x}}\\\\ &{\\displaystyle\\qquad\\qquad+\\sum_{t=0}^{n}\\gamma^{t}\\sum_{s}\\sum_{a}p(\\mu_{x}\\to s,a,t,\\pi_{\\lambda,x}^{*},x)\\left(\\frac{1}{\\lambda}\\partial_{x}A_{\\lambda,x}^{\\pi_{\\lambda,x}^{*}}\\overline{{Q}}(s,a)+\\frac{d\\overline{{r}}_{x}(s,a)}{d x}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad+\\gamma^{n+1}\\sum_{s}p(\\mu_{x}\\to s,t,\\pi_{\\lambda,x}^{*},x)\\partial_{x}\\overline{{V}}(s^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that taking $n\\to\\infty$ then directly proves our claim. ", "page_idx": 43}, {"type": "text", "text": "Base case $\\mathbf{\\nabla}n={\\bf0}$ We prove the statement for $n=0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial f\\big(x,\\eta;\\xi\\big)}{\\partial x}=\\frac{d}{d x}\\sum_{i=1}^{d}{\\eta_{i}\\big(s\\big)\\sum_{s}\\pi_{s}^{*}(a_{i},s)\\widetilde{Q}\\big(s,a\\big)}}\\\\ {\\displaystyle}&{{}=\\sum_{s}\\frac{d\\mu_{s}(s)}{d s}\\nabla\\big(s\\big)+\\sum_{s}\\mu_{s}(s)\\sum_{u^{\\prime}=1,u^{\\prime}(s),s}^{\\eta_{i}}\\bigg(\\frac{1}{\\lambda^{3}}\\partial_{x}A_{x,u^{\\prime}}^{\\eta_{i}^{*}}+\\partial_{x}\\overline{{Q}}_{0}\\big(s,a\\big)\\bigg)}}\\\\ {\\displaystyle}&{{}=\\sum_{s}\\frac{d\\mu_{s}(s)}{d s}V\\big(s\\big)+\\sum_{s}\\mu_{s}(s)\\sum_{u^{\\prime}=1}^{\\eta_{i}}\\alpha_{s}^{*}\\big(a_{i},s)\\bigg(\\frac{1}{\\lambda}\\partial_{x}A_{x,u^{\\prime}}^{\\eta_{i}^{*}}+\\frac{d\\overline{{Q}}_{x}(s,a)}{d s}}\\\\ {\\displaystyle}&{{}\\quad+\\sum_{s}\\sum_{u^{\\prime}=1}^{d}\\left(P_{s}(s^{\\prime},s,a)\\frac{d\\log P_{s}(s^{\\prime},s,a)}{d s}\\nabla\\big(s\\big)+P_{s}\\big(s^{\\prime},s,a)\\partial_{x}\\nabla\\big(s^{\\prime}\\big)\\right)}}\\\\ {\\displaystyle}&{{}=\\sum_{s}\\mu_{s}(s)\\frac{d\\log P_{s}(s)}{d s}V\\big(s)+\\sum_{u^{\\prime}=1}^{d}\\sum_{s}\\gamma^{\\prime}\\eta_{\\left(\\mu_{s}-\\widehat{s}\\right),a^{\\prime}}\\partial_{x}\\mathcal{E}_{\\frac{t}{s},a^{\\prime}}^{\\eta_{i}}}\\\\ {\\displaystyle}&{{}\\quad+\\sum_{s=1}^{\\eta_{i}}\\gamma^{\\prime}\\sum_{s}\\varphi\\big(\\mu_{s}-s,a,t,s,a^{\\prime}\\big)\\bigg(\\frac{1}{\\lambda^{3}}\\partial_{x}A_{x,x^{\\prime}}^{\\eta_{i}}\\mathcal{Q}_{s}\\big(s,a)+\\frac{d\\eta_{s}P_{s}(s^{\\prime},s)}{d s}}\\\\ {\\\n$$dlog Pe(s;s,a)V(s) ", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we use the definition of $f$ in the first equality. The second equality follows fromProposition 2 and the product rule. Rearranging terms gives the third equality. ", "page_idx": 44}, {"type": "text", "text": "Induction step $n\\implies n+1$ Assuming Equation (19) holds for $n$ , we prove it for $n+1$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{2}\\approx_{2}(a+b)\\frac{\\mathcal{X}_{2}(a+b)}{d t}(1-\\sum_{s=0}^{T}\\sum_{w=0}^{t-1}\\sum_{v=t}^{p}(\\mu_{1}-a+b,a^{\\prime})\\frac{\\mathcal{X}_{2}(a+b)}{d t})^{2}}\\\\ &{\\quad+\\displaystyle\\sum_{s=0}^{T}\\sum_{w=1}^{t}\\mu_{1}(a+b,a^{\\prime})\\frac{\\mathcal{X}_{1}(a+b^{\\prime})}{d t}\\left(\\frac{1}{a}(b+c^{2})(\\mu_{1}+a)\\frac{\\mathcal{X}_{2}(a+b^{\\prime})}{d t}\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{s=0}^{T+1}\\sum_{w=1}^{T}\\hat{\\mu}_{1}(a+b,a^{\\prime})\\frac{\\mathcal{X}_{1}(a+b^{\\prime})}{d t}(1-a^{\\prime})}\\\\ &{\\quad-\\displaystyle\\sum_{s=0}^{T}\\mu_{1}(a)\\frac{\\mathcal{X}_{2}(a+b^{\\prime})}{d t}(1-\\sum_{s=0}^{T}\\sum_{w=1}^{t-1}\\hat{\\gamma}_{1}(\\mu_{1}-a+b,a^{\\prime})\\frac{\\mathcal{X}_{2}(a+b^{\\prime})}{d t})\\frac{\\hat{\\mathcal{X}}_{2}(a+b^{\\prime})}{d t}\\nabla_{1}(a)}\\\\ &{\\quad+\\displaystyle\\sum_{s=0}^{T}\\sum_{w=1}^{T}\\hat{\\gamma}_{2}\\succcurlyeq_{2}(a+b,a^{\\prime})\\frac{\\mathcal{X}_{1}(a+b^{\\prime})}{d t}\\left(\\frac{1}{a}(b,a^{\\prime})\\frac{\\mathcal{X}_{2}(a+b^{\\prime})}{d t}+\\frac{\\mathcal{X}_{3}(a+b^{\\prime})}{d t}\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{s=0}^{T+1}\\sum_{w=1}^{T}\\hat{\\mu}_{1}(a+b,a^{\\prime})\\frac{\\mathcal{X}_{2}(a+b^{\\prime})}{d t}\\nabla_{1}(a+b^{\\prime})\\frac{\\mathcal{X}_{3}(a+b^{\\prime})}{d t}+\\frac{\\mathcal{X}_{4}(a+b^{\\prime})}{d t}}\\\\ &{\\quad+\\displaystyle\\sum_ \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which proves our claim. In the first equality we use the definition of $\\overrightharpoon{V}$ and the product rule. The second inequality follows from collecting terms. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "E.7 Auxiliary Results ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Proposition 2 (Gradient of Best response Policy). It holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{d\\pi_{x,\\xi}^{*}(s,a)}{d x}=\\frac{1}{\\lambda}\\pi_{x,\\xi}^{*}(a;s)\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{*}}(s,a).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d\\hat{\\mathbf{r}}_{x,\\xi}^{\\varepsilon}(s,t)}{d s}=\\frac{d}{d s}\\frac{\\exp(Q_{\\lambda,\\xi}(s,\\varepsilon)/\\lambda)}{\\sum_{k=0}^{K}\\exp(Q_{\\lambda,\\xi}(s,\\varepsilon)/\\lambda)}}\\\\ &{=\\frac{\\exp(Q_{\\lambda,\\xi}(s,\\varepsilon)/\\lambda)}{\\sum_{k=0}^{K}\\exp(Q_{\\lambda,\\xi}(s,\\varepsilon)/\\lambda)}\\sum_{\\substack{\\tau\\leq\\tau\\leq\\tau}}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)}\\\\ &{=\\frac{\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\cdot\\sum_{\\substack{\\tau\\leq\\tau}}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)^{2}}{\\left(\\sum_{k=0}^{K}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\right)^{2}}}\\\\ &{\\quad-\\frac{\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\cdot\\sum_{\\substack{\\tau\\leq\\tau}}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)^{2}}{\\left(\\sum_{k=0}^{K}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\right)^{3}}}\\\\ &{=\\tau^{\\varepsilon}(s,\\frac{\\partial Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)}{\\partial s},\\frac{\\partial Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)}{\\partial s},\\frac{\\partial\\tau^{\\varepsilon}(s,\\varepsilon)}{\\partial s})\\sum_{\\substack{\\tau\\leq\\tau}}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\frac{\\partial Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)}{\\partial s}}\\\\ &{=\\frac{1}{\\lambda}\\pi^{\\varepsilon}(s,\\mu_{\\tau})\\partial_{\\lambda,\\xi}Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)-\\frac{1}{\\lambda}\\pi^{\\varepsilon}(s,\\varepsilon)\\sum_{\\substack{\\tau\\leq\\tau}}\\exp(Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)/\\lambda)\\frac{\\partial Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon)}{\\partial s}}\\\\ &{=\\frac{1}{\\lambda}\\pi^{\\varepsilon}(\\mu_{\\tau})\\left\\{Q_{\\lambda,\\xi}^{\\varepsilon}(s,\\varepsilon),(s,\\varepsilon)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The second equality follows from the quotient rule. The third and fourth equality follows from the definition of $\\pi_{x,\\xi}^{*}(s,a)$ . The remaining equalities leverage the definition of the advantage function. ", "page_idx": 45}, {"type": "text", "text": "Note, we can replace the total with the partial derivative of $Q^{*}(x)=\\operatorname*{max}_{\\pi}Q(\\pi,x)$ , due to Rockafellar's thoerem because $Q$ is continuously differentiable in $\\mathbf{X}$ for all $\\pi$ and $\\Pi$ is compact and convex [9, 56]. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Prpsiava $\\stackrel{\\widehat{\\partial_{x}A_{\\lambda,x,\\xi}^{\\overline{{\\sigma}}}}}{\\partial_{x}A_{\\lambda,x,\\xi}^{\\overline{{\\sigma}}_{x,\\xi}^{\\overline{{\\varepsilon}}}}(s,a)}$ of Algorihm 2 is an unbiased estimate of $\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s,a)$ ie ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\widehat{\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}}(s,a)\\right]=\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{o}}(s,a).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. We drop any dependence on $x,\\xi,\\pi_{x,\\xi}^{o}$ for notational clarity. We further emphasize that the trick of truncating a rollout after a geomtrically sampled time to obtain unbiased gradients is commonly used in the RL literature for obtaining unbiased estimates of the standard policy gradient [80]. ", "page_idx": 45}, {"type": "text", "text": "We show that the estimator $\\widehat{\\partial_{x}Q_{\\lambda}}(s,a)$ given by ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{=0}^{T_{Q}}\\frac{d}{d x}r(s_{t},a_{t})+\\frac{\\gamma}{1-\\gamma}\\frac{d}{d x}\\log P(s_{T Q+1};s_{T Q},a_{T Q})\\sum_{t=T_{Q}+1}^{T_{Q}+T_{Q}^{\\prime}+1}\\gamma^{(t-T_{Q}-1)/2}\\left(r(s_{t},a_{t})+\\lambda H(\\pi(\\cdot;s_{t}))\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "is unbiased. The same argument then holds for $\\widehat{\\frac{d}{d x}V_{\\lambda}}(s)$ and implies that $\\widehat{\\frac{d}{d x}A_{\\lambda}}(s,a)$ is unbiased. First of all, we have: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=0}^{T_{Q}^{\\prime}}\\gamma^{(t)/2}\\left(r(s_{t},a_{t})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)\\right]\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!=\\!\\mathbb{E}_{T_{Q}}\\mathbb{E}_{s}^{\\pi}\\Bigg[\\!\\sum_{t=0}^{T_{Q}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{\\!\\!\\!=\\!\\!\\!\\mathbb{E}_{s}^{\\pi}\\Bigg[\\!\\Bigg[\\!\\frac{T_{Q}}{t\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\sum_{s=0}^{T_{Q}}}\\gamma^{(t)/2}\\left(r(s_{t},a_{t})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)\\Bigg]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we use Fubini's theorem for eq. (20) and the Dominated Convergence Theorem and the fact that $T_{Q}^{\\prime}\\sim\\mathrm{Geo}(1-\\gamma^{0.5})$ in eq. (21). Because $T_{Q}$ and $T_{Q}^{\\prime}$ are sampled independently,it immediately followsthat ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T_{Q},T_{Q}^{\\prime}}\\mathbb{E}_{s,a}^{\\pi}\\Bigg[\\displaystyle\\sum_{t=0}^{T_{Q}}\\displaystyle\\frac{d}{d x}r^{(s_{t},a_{t})}}\\\\ &{\\phantom{\\sum_{t=0}^{T_{Q}}\\displaystyle\\sum_{\\tau=1}^{d}\\log P\\big(s_{T_{Q}+1};s_{T_{Q}},a_{T_{Q}}\\big)}\\displaystyle\\sum_{t=T_{Q}+1}^{T_{Q}+T_{Q}^{\\prime}+1}\\gamma^{(t-T_{Q}-1)/2}\\left(r(s_{t},a_{t})+\\lambda H(\\pi(\\cdot;s_{t}))\\right)\\Bigg]}\\\\ &{=\\!\\!\\mathbb{E}_{T_{Q}}\\mathbb{E}_{s,a}^{\\pi}\\!\\left[\\displaystyle\\sum_{\\underline{{t}}=0}^{T_{Q}}\\displaystyle\\frac{d}{d x}r^{(s_{t},a_{t})}+\\underbrace{\\displaystyle\\frac{\\gamma}{1-\\gamma}\\displaystyle\\frac{d\\log P\\big(s_{T_{Q}+1};s_{T_{Q}},a_{T_{Q}}\\big)}{d x}V_{\\lambda}^{\\pi}(s_{T_{Q}})}_{(2)}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We seperately show that the two summands are unbaised estimates. Then by linearity of expectation the result follows. For (1) using Fubini's theorem and Dominated Convergence Theorem it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T_{Q}}\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T_{Q}}\\frac{d}{d x}r(s_{t},a_{t})\\right]=\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{k=0}^{\\infty}(1-\\gamma)\\gamma^{k}\\displaystyle\\sum_{t=0}^{k}\\frac{d}{d x}r(s_{t},a_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(1-\\gamma)\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\sum_{k=t}^{\\infty}\\gamma^{k}\\frac{d}{d x}r(s_{t},a_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=(1-\\gamma)\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\frac{\\gamma^{t}}{1-\\gamma}\\displaystyle\\frac{d}{d x}r(s_{t},a_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\displaystyle\\frac{d}{d x}r(s_{t},a_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Similarly for (2), we have using Fubini and Dominated Convergence Theorem that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\mathbb{E}_{T_{Q}}\\mathbb{E}_{s,a}^{\\pi}\\left[\\frac{\\gamma}{1-\\gamma}\\displaystyle\\frac{d\\log P(s_{T_{Q}+1};s_{T_{Q}},a_{T_{Q}})}{d x}V_{\\lambda}^{\\pi}(s_{T_{Q}})\\right]}\\\\ &{=\\!\\frac{\\gamma}{1-\\gamma}\\mathbb{E}_{s,a}^{\\pi}\\mathbb{E}_{T_{Q}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\mathbb{1}_{t=T_{Q}}\\displaystyle\\frac{d\\log P(s_{t+1};s_{t},a_{t})}{d x}V_{\\lambda}^{\\pi}(s_{t})\\right]}\\\\ &{=\\!\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t+1}\\displaystyle\\frac{d\\log P(s_{t+1};s_{t},a_{t})}{d x}V_{\\lambda}^{\\pi}(s_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Plugging these results back in, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T_{Q}}\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T_{Q}}\\frac{d}{d x}r(s_{t},a_{t})+\\frac{\\gamma}{1-\\gamma}\\frac{d\\log P(s_{T_{Q}+1};s_{T_{Q}},a_{T_{Q}})}{d x}V_{\\lambda}^{\\pi}(s_{T_{Q}})\\right]}\\\\ &{=\\!\\mathbb{E}_{s,a}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\frac{d}{d x}r(s_{t},a_{t})+\\gamma^{t+1}\\frac{d\\log P(s_{t+1};s_{t},a_{t})}{d x}V_{\\lambda}^{\\pi}(s_{t})\\right]}\\\\ &{=\\!\\partial_{x}Q_{\\lambda}^{\\pi}(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which proves the proposition. ", "page_idx": 47}, {"type": "text", "text": "Proposition 4 (Unbiased gradient estimator for $F$ ).The gradient estimator described in Algorithm 5 is unbiased for the given policy T.E", "page_idx": 47}, {"type": "text", "text": "Proof. We need to show that: ", "page_idx": 47}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{\\xi}\\left[\\mathbb{E}_{s_{0},\\xi_{\\mu}}^{\\pi_{x,\\xi_{\\xi}}^{\\sigma}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\frac{1}{\\lambda}\\partial_{x}A_{\\lambda,x,\\xi}^{\\pi_{x,\\xi}^{\\sigma}}(s_{t},a_{t})\\overline{{Q}}(s_{t},a_{t})+\\frac{d}{d x}\\overline{{r}}(s_{t},a_{t})+\\partial_{x}\\log P_{x,\\xi}(s_{t},a_{t-1},s_{t-1})\\overline{{V}}(s_{t})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We can show the claim seperately for (1), (2) and (3). Note for (1) and (3) the claim directly follows from the proof of Proposition 3. And the proof for (2) works almost identical to the one for (3), relying on the fact that a truncation via a geometric distribution is identical to an infinite trajectory with a discount factor. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "E.8 Convergence Results for Popular RL Algorithms ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "For the next Proposition, consider the following soft Bellmann optimality operator, which has been shown to be a contraction [20, 51]. ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left(\\mathcal{T}_{\\lambda}^{*}V_{\\lambda}\\right)(s):=\\lambda\\log\\left(\\sum_{a\\in\\mathcal{A}}\\exp\\left(\\frac{r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}|s,a}\\left[V_{\\lambda}\\left(s^{\\prime}\\right)\\right]}{\\lambda}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using Equation (22), one can define a standard soft value iteration algorithm (see Algorithm 3 in Appendix D). We show soft value iteration satisfies Assumption 3.2. ", "page_idx": 47}, {"type": "text", "text": "Proposition 5. Algorithm 3 converges, such that $\\left\\|\\pi_{x,\\xi}^{*}-\\pi_{x,\\xi}^{o}\\right\\|_{\\infty}^{2}\\leq\\delta^{2}$ after $T$ iterations, where $T=\\mathcal{O}(\\log{1/\\delta})$ ", "page_idx": 47}, {"type": "text", "text": "Proof. From [49][Lemma 24] we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\|\\pi^{o}-\\pi^{*}\\|_{\\infty}\\le\\|\\pi^{o}-\\pi^{*}\\|_{1}\\le\\frac1\\lambda\\left\\|Q_{\\lambda}^{T}-Q_{\\lambda}^{*}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Moreover ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda}\\left\\|Q_{\\lambda}^{T}-Q_{\\lambda}^{*}\\right\\|_{\\infty}\\leq\\frac{1}{\\lambda}\\left\\|V_{\\lambda}^{T}-V_{\\lambda}^{*}\\right\\|_{\\infty}\\leq\\frac{\\gamma^{T}}{\\lambda}\\left\\|V_{\\lambda}^{*}\\right\\|\\leq\\frac{\\gamma^{T}}{\\lambda(1-\\gamma)}\\left(\\overline{{R}}+\\lambda\\log\\left|A\\right|\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we use the contraction property shown in [20, 51] and the fact that we instantiate $V_{\\lambda}$ with O, such that no value iterate can ever be larger than higher than $\\left(\\overline{{R}}+\\lambda\\log\\vert A\\vert\\right)$ . The claim follows from $\\delta\\leq\\mathcal{O}(\\gamma^{T})$ \u53e3 ", "page_idx": 47}, {"type": "text", "text": "As soft value iteration assumes knowledge of the transition function and scales badly when the state and action space are large, in practice stochastic methods such as soft Q-learning are used instead. For this method, consider the soft Bellman state-action optimality operator [3, 30]: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left(T_{\\lambda}^{*}Q_{\\lambda}\\right)(s,a):=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P\\left(\\cdot\\vert s,a\\right)}\\left[\\lambda\\log\\left(\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q_{\\lambda}(s,a^{\\prime})}{\\lambda}\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We can use Equation (23) to run soft Q-learning, as described in Algorithm 4 in Appendix D.   \nEquivalently to soft value iteration, we can show soft Q-learning satisfies Assumption 3.2. ", "page_idx": 48}, {"type": "text", "text": "Proposition 6. Let $\\pi_{B}$ be suffciently exploratory, such that the induced Markov chain is ergodic. Thensoft $Q$ -learning converges,such that $\\mathbb{E}_{o}\\left[\\left\\lVert\\pi_{x,\\xi}^{*}-\\pi_{x,\\xi}^{o}\\right\\rVert_{\\infty}^{2}\\right]\\,\\le\\,\\delta^{2}$ after $T$ iterations,where $\\begin{array}{r}{T=\\mathcal{O}(\\frac{\\log(1/\\delta)}{\\delta^{2}}).}\\end{array}$ ", "page_idx": 48}, {"type": "text", "text": "We use the following Theorem from [54] to prove our claim: ", "page_idx": 48}, {"type": "text", "text": "Theorem [54] Let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ be an operator We use $F_{i}$ to denote the i 'th entry of $F$ We consider the following stochastic approximation scheme that keeps updating $\\boldsymbol{x}(t)\\in\\mathbb{R}^{d}$ starting from $x(0)$ being the all zero vector, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{x_{i}(t+1)=x_{i}(t)+\\alpha_{t}\\left(F_{i}(x(t))-x_{i}(t)+w(t)\\right)}&{f o r\\;i=i_{t},}\\\\ &{x_{i}(t+1)=x_{i}(t)}&{f o r\\;i\\neq i_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $i_{t}\\,\\in\\,\\{1,\\ldots,d\\}$ is a stochastic process adapted to a filtration $\\mathcal{F}_{t}$ and $w(t)$ is some noise. Assume the following: ", "page_idx": 48}, {"type": "text", "text": "Assumption $^{\\,l}$ (Contraction) (a) Operator $F$ is $\\gamma$ contraction in $\\|\\cdot\\|_{\\infty}$ , i.e. for any $x,y\\;\\in\\;\\mathbb{R}^{d}$ \uff0c $\\left\\|F(x)-F(y)\\right\\|_{\\infty}\\le\\gamma\\left\\|x-y\\right\\|_{\\infty}$ $(b)$ There exists some constant $C>0$ s.t. $\\left\\|F(x)\\right\\|_{\\infty}\\leq\\gamma\\left\\|x\\right\\|_{\\infty}+$ $C,\\forall x\\in\\mathbb{R}^{d}$ ", "page_idx": 48}, {"type": "text", "text": "Assumption 2 (Martingale Difference Sequence) $w(t)$ is $\\mathcal{F}_{t+1}$ measurable and satisfies $\\mathbb{E}w(t)\\mid\\mathcal{F}_{t}=$ $\\boldsymbol{O}$ .Further, $|w(t)|\\leq\\bar{w}$ almost surely for some constant $\\bar{w}$ ", "page_idx": 48}, {"type": "text", "text": "Assumption $^3$ (Sufcient Exploration) There exists a $\\sigma\\in(0,1)$ and positive integer, $\\tau$ , such that, for any $i\\in\\mathcal N$ and $t\\geq\\tau,\\mathbb{P}\\left(i_{t}=i\\mid\\mathcal{F}_{t-\\tau}\\right)\\geq\\sigma$ ", "page_idx": 48}, {"type": "text", "text": "Suppose Assumptions 1,2 and $^3$ hold. Further, assume there exists constant $\\bar{x}~\\geq~\\|x^{*}\\|_{\\infty}~s.t.$ $\\forall t,\\|x(t)\\|_{\\infty}\\;\\leq\\;\\bar{x}$ almost surely. Let the step size be $\\begin{array}{r}{\\alpha_{t}\\ =\\ \\frac{h}{t+t_{0}}}\\end{array}$ with $t_{0}\\,\\geq\\,\\operatorname*{max}(4h,\\tau)$ and $\\begin{array}{r}{h\\geq\\frac{2}{\\sigma(1-\\gamma)}}\\end{array}$ Then, with probabiliy at least $1-\\delta$ ", "page_idx": 48}, {"type": "equation", "text": "$$\nx(T)-x^{*}\\Vert_{\\infty}\\leq\\frac{12\\bar{\\epsilon}}{1-\\gamma}\\sqrt{\\frac{(\\tau+1)h}{\\sigma}}\\sqrt{\\frac{\\log\\left(\\frac{2(\\tau+1)T^{2}n}{\\delta}\\right)}{T+t_{0}}}+\\frac{4}{1-\\gamma}\\operatorname*{max}\\left(\\frac{16\\bar{\\epsilon}h\\tau}{\\sigma},2\\bar{x}\\left(\\tau+t_{0}\\right)\\right)\\frac{1}{T+t_{0}}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\bar{\\epsilon}=2\\bar{x}+C+\\bar{w}$ ", "page_idx": 48}, {"type": "text", "text": "Proof. Our algorithm can be seen as a stochastic approximation scheme where we update $Q$ asynchronously just like $x$ above in the following way ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{Q_{s_{t},a_{t}}(t+1)=Q_{s_{t},a_{t}}(t)+\\alpha_{t}\\left(F_{s_{t},a_{t}}(Q(t))-Q_{s_{t},a_{t}}(t)+w_{t}\\right)}\\\\ &{Q_{s,a}(t+1)=Q_{s,a}(t)}&{\\mathrm{~for~}s,a\\ne s_{t},a_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\nF_{s_{t},a_{t}}(Q)=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda\\log\\left(\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q(s,a^{\\prime})}{\\lambda}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and the errors: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{t}=r\\big(s_{t},a_{t}\\big)+\\gamma\\lambda\\log\\left(\\displaystyle\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q_{\\lambda}\\big(s_{t+1},a^{\\prime}\\big)}{\\lambda}\\right)\\right)}\\\\ &{\\qquad-\\,r\\big(s_{t},a_{t}\\big)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot;s_{t},a_{t})}\\left[\\lambda\\log\\left(\\displaystyle\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q_{\\lambda}\\big(s_{t+1},a^{\\prime}\\big)}{\\lambda}\\right)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Wenowshowthat $F$ satisfies the assumptions of the Theorem from [54] and use the result to prove our own claim. ", "page_idx": 49}, {"type": "text", "text": "In the following we let $\\mathcal{F}_{t}$ be the $\\sigma$ -algebra generated by the random variables $\\left(s_{0},a_{0},\\cdot\\cdot\\cdot,s_{t},a_{t}\\right)$ First we restate the following identity from [51] ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\lambda}^{*}(Q)(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda\\log\\left(\\displaystyle\\sum_{a^{\\prime}\\in A}\\exp\\left(\\frac{Q(s,a^{\\prime})}{\\lambda}\\right)\\right)\\right]}\\\\ {=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\operatorname*{max}_{\\pi}\\langle Q(\\cdot,s^{\\prime}),\\pi\\rangle+\\lambda H(\\pi;s^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We use it to show that $T_{\\lambda}^{*}$ is a contraction. Indeed we have: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|T_{\\lambda}^{\\ast}(Q_{1})-T_{\\lambda}^{\\ast}(Q_{2})\\right\\|_{\\infty}}\\\\ &{=\\Bigg|\\Bigg|r(s,a)+\\gamma\\underset{\\pi}{\\operatorname*{max}}\\sum_{s^{\\prime}}P(s^{\\prime};s,a)\\left(\\langle Q_{1}(\\cdot,s^{\\prime}),\\pi\\rangle+\\lambda H(\\pi;s^{\\prime})\\right)}\\\\ &{\\quad-\\left.r(s,a)-\\gamma\\underset{\\pi}{\\operatorname*{max}}\\sum_{s^{\\prime}}P(s^{\\prime};s,a)\\left(\\langle Q_{2}(\\cdot,s^{\\prime}),\\pi\\rangle+\\lambda H(\\pi;s^{\\prime})\\right)\\ \\Bigg|\\Bigg|_{\\infty}}\\\\ &{\\leq\\gamma\\left\\|\\underset{\\pi}{\\operatorname*{max}}\\sum_{s^{\\prime}}P(s^{\\prime};s,a)\\left(\\langle Q_{1}(\\cdot,s^{\\prime}),\\pi\\rangle-\\langle Q_{2}(\\cdot,s^{\\prime}),\\pi\\rangle\\right)\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\left\\|Q_{1}-Q_{2}\\right\\|_{\\infty}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Moreover, it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\nF(Q)\\leq\\overline{{R}}+\\gamma\\left\\|Q\\right\\|_{\\infty}+\\lambda\\log|A|.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "So we can set $C=\\overline{{R}}+\\lambda\\log|A|$ ", "page_idx": 49}, {"type": "text", "text": "Next we note that $w_{t}$ is $F_{t+1}$ -measurable (it depends on $s_{t+1}$ ) and that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}[w_{t}|\\mathcal{F}_{t}]=0.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Moreover w(t) is bounded by w = 2?(R+\u5165log IAD. ", "page_idx": 49}, {"type": "text", "text": "Further we have assumed that the behavioural policy. $\\pi_{B}$ is sufficiently exploratory. Let $\\tilde{\\mu}$ be the corresponding stationary distribution, $\\mu_{\\mathrm{min}}=\\operatorname*{inf}_{s,a}\\tilde{\\mu}(s,a)$ and $t_{m i x}$ the mixing time. Then [54] show that for $\\begin{array}{r}{\\sigma=\\frac{1}{2}\\mu_{\\mathrm{min}}}\\end{array}$ and $\\begin{array}{r}{\\tau=\\lceil\\log_{2}(\\frac{2}{\\mu_{\\mathrm{min}}})\\rceil t_{m i x}}\\end{array}$ it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\forall s\\in{\\mathcal{S}},a\\in{\\mathcal{A}},\\forall t\\geq\\tau:\\mathbb{P}(s_{t},a_{t}=s,a|{\\mathcal{F}}_{t-\\tau})\\geq\\sigma.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Moreover we note that $Q(t)$ and $Q_{\\lambda}^{*}$ are bound by $\\begin{array}{r}{\\overline{{x}}=\\frac{\\overline{{R}}+\\lambda\\log|A|}{1-\\gamma}}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "Using the Theorem from [54] we thus have the following result: ", "page_idx": 49}, {"type": "text", "text": "Let Qt = t+to wih $\\begin{array}{r}{t_{0}\\geq\\operatorname*{max}\\left(4h,\\left\\lceil\\log_{2}\\frac{2}{\\mu_{\\mathrm{min}}}\\right\\rceil t_{\\mathrm{mix}}\\right)}\\end{array}$ \u03bc(1) Then, with probabilty at least $1-p$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|Q(T)-Q_{\\lambda}^{*}\\right\\|_{\\infty}\\leq}\\\\ &{\\leq\\frac{60(\\bar{R}+\\lambda\\log|\\mathcal{A}|)}{(1-\\gamma)^{2}}\\sqrt{\\frac{2\\left(\\left\\lceil\\log_{2}\\frac{2}{\\mu_{\\operatorname*{min}}}\\right\\rceil t_{\\operatorname*{min}}+1\\right)h}{\\mu_{\\operatorname*{min}}}}\\sqrt{\\frac{\\log\\left(\\frac{2\\left(\\left\\lceil\\log_{2}\\frac{2}{\\mu_{\\operatorname*{min}}}\\right\\rceil t_{\\operatorname*{mix}}+1\\right)T^{2}|\\mathcal{S}||\\mathcal{A}|\\right)}{P}}{T+t_{0}}}}\\\\ &{+\\frac{4(\\bar{R}+\\lambda\\log|\\mathcal{A}|)}{(1-\\gamma)^{2}}\\operatorname*{max}\\left(\\frac{160h\\left\\lceil\\log_{2}\\frac{2}{\\mu_{\\operatorname*{min}}}\\right\\rceil t_{\\operatorname*{min}}}{\\mu_{\\operatorname*{min}}},2\\left(\\left\\lceil\\log_{2}\\frac{2}{\\mu_{\\operatorname*{min}}}\\right\\rceil t_{\\operatorname*{min}}+t_{0}\\right)\\right)\\frac{1}{T+t_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We denote the bound above by (A). Let us choose $p=\\delta^{2}$ 6 With probability $p$ $\\|Q(T)-Q_{\\lambda}^{*}\\|_{\\infty}$ is not bounded by (A). However, tis always upper bounded by 2(R+\\log(IAD) ", "page_idx": 50}, {"type": "text", "text": "Seting $\\begin{array}{r}{T=\\mathcal{O}(\\frac{\\log(1/\\delta)}{\\delta^{2}})}\\end{array}$ , we get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Lambda:=\\mathcal{O}\\left(\\sqrt{\\frac{\\log(\\frac{r}{T})}{T}+\\frac{1}{T}}\\right)}&{{}}\\\\ {\\quad-\\mathcal{O}\\left(\\sqrt{\\frac{\\log(\\frac{r}{T})}{T}}\\right)}&{{}}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\frac{\\log(\\frac{\\log(\\frac{r}{T})}{T})}{\\log(\\frac{r}{T})}}\\right)}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\frac{\\log(\\frac{\\log(\\frac{r}{T})}{T})}{\\log(1/T)}}\\right)}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\frac{\\log(\\frac{\\log(\\frac{r}{T})}{T})}{\\log(1/T)}}\\right)}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\int_{-\\infty}^{1/3}\\log(\\frac{\\log(1/T)}{T})}\\right)}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\int_{-\\infty}^{1/3}\\log(\\frac{\\log(1/T)}{T})}\\right)}\\\\ {\\quad}&{{}=\\mathcal{O}\\left(\\sqrt{\\int_{-\\infty}^{1/3}\\log(\\frac{\\log(1/T)}{T})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Since $\\delta<1$ \uff0c $\\log(1/\\delta)>0$ and for $x>0$ it holds that ${\\frac{\\log(x)}{x}}<1/e$ such that we get: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{(\\mathbf{A}){\\leq}\\mathcal{O}\\left(\\delta\\sqrt{3+\\frac{1}{e}}\\right)}}\\\\ {\\displaystyle{=\\mathcal{O}\\left(\\delta\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using [49][Lemma 24], we arrive at: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{o}\\left[\\left\\Vert\\pi^{o}-\\pi^{*}\\right\\Vert_{\\infty}^{2}\\right]}\\\\ &{\\le\\!4\\mathbb{E}_{o}\\left[\\left\\Vert Q_{\\lambda}^{T}-Q_{\\lambda}^{*}\\right\\Vert_{\\infty}^{2}\\right]}\\\\ &{\\le\\!4\\left((1-p)(\\mathbf{A})^{2}+p\\left(\\frac{2(\\bar{R}+\\lambda\\log(|A|))}{1-\\gamma}\\right)^{2}\\right)}\\\\ &{=\\!\\mathcal{O}(\\delta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "A popular class of RL algorithms are policy gradient methods such as REINFORCE [68]. For the entropy-regularised problem, it generally makes sense to choose a softmax parametrization for the policy, asweknow $\\bar{\\pi}_{x,\\xi}^{*}(s;a)\\propto\\bar{\\exp}(Q_{\\lambda,x,\\xi}^{*}(s,a)/\\lambda)$ [49]. We defer the details to Algorithm 6 in Appendix D and present the following convergence result, which shows using vanilla policy gradient for the lower level also fulfills Assumption 3.2\u2014at least asymptotically. ", "page_idx": 50}, {"type": "text", "text": "Proposition 7. Vanilla policy. gradient with softmax parameterization converges, such that $\\forall\\delta,\\bar{\\exists}T,\\forall t\\geq T:\\left\\lVert\\pi_{x,\\xi}^{*}-\\bar{\\pi}_{t,x,\\xi}^{o}\\right\\rVert\\leq\\delta^{2}$ Wwhere $\\pi_{t,x,\\xi}^{o}$ is the computed policy afert iterations. 6Note that $\\delta<1$ , as it represents the distance between two softmax policies under the supremum norm. ", "page_idx": 50}, {"type": "text", "text": "Proof. As in most proofs we drop the subscripts for $x,\\xi$ . The proof is an adaptation of the one presented in [49][Lemma 16]. We denote by $\\pi_{t}$ the iterates of the policies of the algorithm and by $\\bar{V}_{\\lambda}^{\\pi_{t}}(\\mu)$ the corresponding value function with starting distribution $\\mu$ . It can be shown that $V_{\\lambda}^{\\pi}$ is $s$ smooth for some $s$ [49]. Choosing a stepsize of $1/s$ , we have that the value functions increase monotonically, i.e. ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall t:V_{\\lambda}^{\\pi_{t+1}}(\\mu)\\geq V_{\\lambda}^{\\pi_{t}}(\\mu).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "At the same time, it holds that: ", "page_idx": 51}, {"type": "equation", "text": "$$\nV_{\\lambda}^{\\pi_{t}}(\\mu)\\leq\\frac{\\overline{{R}}+\\lambda\\log\\mathcal{A}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By monotone convergence it thus follows that $V_{\\lambda}^{\\pi_{t}}(\\mu)\\to V_{\\lambda}^{*}(\\mu)$ \uff0cwhere $V_{\\lambda}^{*}(\\mu)$ is the maximum possible value. ", "page_idx": 51}, {"type": "text", "text": "Since $\\pi_{t}\\in\\Delta(A)^{|S|}$ and $\\Delta(\\mathcal{A})^{|S|}$ is compact it follows that $\\{\\pi_{t}\\}_{t}$ has a convergent subsequence $\\{\\pi_{t_{k}}\\}_{k}$ . Denote by $\\pi^{*}$ the limit of this subsequence. It has to hold that $V_{\\lambda}^{\\pi^{*}}(\\mu)=V_{\\lambda}^{*}(\\mu)$ and thus $\\pi^{*}$ is the optimal policy. ", "page_idx": 51}, {"type": "text", "text": "Now assume that $\\{\\pi_{t}\\}_{t}$ does not converge to $\\pi^{*}$ . In that case ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\exists\\epsilon,\\forall t,\\exists t^{\\prime}\\geq t:\\|\\pi^{*}-\\pi_{t^{\\prime}}\\|_{\\infty}>\\epsilon.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Note that due to entropy regularization $V_{\\lambda}^{*}(\\mu)$ is the unique maximum. This means that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\exists\\kappa:\\operatorname*{max}\\{V_{\\lambda}^{\\pi}|\\left\\|\\pi-\\pi^{*}\\right\\|_{\\infty}\\geq\\epsilon\\}+\\kappa<V_{\\lambda}^{*}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "It follows then that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\forall t,\\exists t^{\\prime}\\geq t:\\left\\|V_{\\lambda}^{\\pi^{*}}-V_{\\lambda}^{\\pi_{t^{\\prime}}}\\right\\|_{\\infty}>\\kappa,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Which implies $V_{\\lambda}^{\\pi_{t}}(\\mu)$ does not converge to $V^{*}$ a contradiction to our conclusion above. It therefore has to hold that $\\pi_{t}\\to\\pi^{*}$ \u53e3 ", "page_idx": 51}, {"type": "text", "text": "The asymptotic guarantee of Vanilla Policy Gradient can be improved to non-asymptotic by using Natural Policy Gradient, as introduced by [42]. We restate the following result from [12]. ", "page_idx": 51}, {"type": "text", "text": "Proposition 8 (Linear convergence of exact entropy-regularized NPG, [12]). For any learning rate $0<\\eta\\leq(1-\\gamma)/\\tau$ , the entropy-regularized NPG updates (18) satisfy ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert Q_{\\lambda}^{\\star}-Q_{\\lambda}^{(t+1)}\\right\\rVert_{\\infty}\\leq C_{1}\\gamma(1-\\eta\\lambda)^{t}\\qquad}\\\\ {\\left\\lVert\\log\\pi_{\\lambda}^{\\star}-\\log\\pi^{(t+1)}\\right\\rVert_{\\infty}\\leq2C_{1}\\lambda^{-1}(1-\\eta\\lambda)^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for all $t\\geq0$ where ", "page_idx": 51}, {"type": "equation", "text": "$$\nC_{1}:=\\left\\|Q_{\\lambda}^{\\star}-Q_{\\lambda}^{(0)}\\right\\|_{\\infty}+2\\lambda\\left(1-\\frac{\\eta\\lambda}{1-\\gamma}\\right)\\left\\|\\log\\pi_{\\lambda}^{\\star}-\\log\\pi^{(0)}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "F  Implementation Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "F.1 Baseline Algorithms ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "F.1.1 Adaptive Model Design [15] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "As noted in Section 5, the Adaptive Model Design (AMD) algorithm [15] was proposed for the Regularized Markov Design (RMD) problem which is a special case of Contextual Bilevel Reinforcement Learning. In particular, when $\\mathbb{P}_{\\xi}$ is a Dirichlet distribution CB-RL reduces to the RMD problem. To account for this difference, we modify the AMD algorithm (Algorithm 2 in [15]) as described in Algorithm 9. We denote the upper-level reward and value functions with the superscript $u$ inthe algorithm. ", "page_idx": 51}, {"type": "text", "text": "F.1.2 Zero-Order Algorithm ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Algorithm 10 defines the zero-order gradient estimation algorithm described in Section 5. We parametrize the perturbation constant to decrease with the number of iterations such as $\\begin{array}{r}{u_{t}=\\frac{C}{t}}\\end{array}$ Cwhere $C$ is a positive constant. ", "page_idx": 51}, {"type": "text", "text": "Input: Iterations $T$ , Inner iterations: $K$ , Learning rate $\\alpha$ , Regularization $\\lambda$ , gradient of the   \npre-learned model $\\nabla_{x}\\log{P}$ , gradient of the reward function $\\nabla_{x}r$   \nInitialize $x_{0}$ $Q_{0}$ \uff0c $\\nabla_{x}Q_{0}$ , and $\\tilde{Q}_{0}$   \nfor $t=0$ to $T-1$ do $\\xi\\sim\\mathbb{P}_{\\xi}$ for $k=0$ to $K-1$ do $\\begin{array}{r l}&{\\mathbf{\\Psi}_{\\tau_{k}}^{\\textrm{n}\\;\\textrm{-}\\;\\textrm{o u p}_{L}\\;\\textrm{-}\\;\\textrm{a u}}}\\\\ &{\\;\\;\\pi_{\\alpha_{t},\\xi}\\longleftarrow\\exp(\\lambda Q_{k}(s,\\cdot))}\\\\ &{\\;\\;\\mathrm{Calculate}\\;V_{k},\\nabla_{x_{t}}V_{k},V_{k}^{U},\\nabla_{x_{t}}A_{k},A_{k}^{u},\\tilde{V}_{k}}\\\\ &{\\;Q_{k+1}\\longleftarrow T_{r,\\gamma}(V_{k})}\\\\ &{\\;\\;\\nabla_{x_{t}}Q_{k+1}=\\mathcal{T}_{\\nabla_{x_{t}}r,\\gamma}(\\nabla_{x_{t}}V_{k}+V_{k}\\nabla_{x_{t}}\\log P)}\\\\ &{\\;Q_{k+1}^{u}=\\mathcal{T}_{r_{u},\\gamma_{u}}(V_{k}^{u})}\\\\ &{\\;\\;\\tilde{Q}_{k+1}\\leftarrow\\mathcal{T}_{\\nabla_{x_{t}}r^{u}+\\lambda A_{k}^{u}\\nabla_{x_{t}}A_{k}}(\\tilde{V}_{k}+V_{k}^{u}\\nabla_{x_{t}}\\log P)}\\end{array}$ end for Set $\\begin{array}{r}{\\widehat{\\frac{d F}{d x}}=\\tilde{V}_{K}}\\end{array}$ Xt+1 \u2190xt+\u03b1 Reinitialize $Q_{0}\\leftarrow Q_{K}$ \uff0c $\\nabla_{x}Q_{0}\\leftarrow\\nabla_{x}Q_{K}$ , and $\\tilde{Q}_{0}\\leftarrow\\tilde{Q}_{K}$   \nend for   \nOutput: Optimised parameter $x_{T}$ ", "page_idx": 52}, {"type": "text", "text": "Algorithm 10 Zero-Order Algorithm ", "text_level": 1, "page_idx": 52}, {"type": "table", "img_path": "W3Dx1TGW3f/tmp/fca702f2769bd180d37510821ad77b9a31f25879153a988925a78ed27053f87d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "F.2  Four Rooms ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "F.2.1 Implementation Details ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We parametrize the penalty function $\\tilde{r}$ as the softmax transformation of $\\boldsymbol{x}\\in\\mathbb{R}^{d_{s}+1}$ where the $i$ -th entry of $x$ corresponds to the $i$ -th cell in the state space $\\boldsymbol{S}$ and the additional dimension $d_{s}+1$ is used to allocate the penalties not effective and also excluded from the penalty term received by the leader at the end of each episode. In particular, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\tilde{r}(s,a)=-0.2*\\mathrm{softmax}(s;x),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where softmax $(s;x)$ denotes the value of the softmax transformation of $x$ at the entry corresponding to the state $s$ . Note that this parametrization explicitly restricts the maximum available budget for penalties to $-0.2$ ", "page_idx": 52}, {"type": "text", "text": "F.2.2 Hyperparameters ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "For the upper-level optimization problem, we use gradient norm clipping of 1.0. The learning rate for each algorithm has been chosen as the best performing one from $[\\bar{1}.0,0.5,0.1,0.05,0.0\\bar{1}]$ individually. Additionally, we tune the parameter $C$ for the Zero-order algorithm on the values [0.1, 0.5, 1.0, 2.0, 5.0]. For Hyper Policy Gradient Descent, we sample 10, 000 environment steps for each gradient calculation. ", "page_idx": 52}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/043a5a981c61b11025c1c2a29c9fb0a80dd172bdf1761de130063f2c1bbd4862.jpg", "img_caption": ["Figure 4: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.001$ and $\\beta=3.0$ "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/9bcde79784fba7b66fe68ce0187142796509fc25d62aeaac2e92744077b25478.jpg", "img_caption": ["Figure 5: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.001$ and $\\beta=5.0$ "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/994032746e2b3f6dc54b0c5b61f3e5668d1a1cbaf1c09855c1a12f05ffe00366.jpg", "img_caption": ["Figure 6: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.003$ and $\\beta=1.0$ "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/134dfb5d6ee317ed3484ad4679d78a1bc111dcd15f45208f34df1065db615aca.jpg", "img_caption": ["Figure 7: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.003$ and $\\beta=3.0$ "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/5615cd8b967729e02aeec244a7a2407c4930ce7ae699bb47bb6c541cf906b164.jpg", "img_caption": ["Figure 8: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.003$ and $\\beta=5.0$ "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/51dd231992d7dd132c79044fc5adf9584a9b1bcec11ed6839f0064d5a4251277.jpg", "img_caption": ["Figure 9: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.005$ and $\\beta=1.0$ "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/191dbdc301f73a1a07695d9c9e2f68acb9b45149757c9d547456b429ca501073.jpg", "img_caption": ["Figure 10: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.005$ and $\\beta=3.0$ "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/e5348d22679591025d0c6d033857108736d116e4df613cae2f3621e50e642e6c.jpg", "img_caption": ["Figure 11: Upper-level objective values, $F$ , over the number of outer iterations for hyperparameters $\\lambda=0.005$ and $\\beta=5.0$ "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/f83534d376c8072e38fd4e1643e19ea8d9d88f2096c0da1e111186244c6ee4ae.jpg", "img_caption": ["Figure 12: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.001$ and $\\bar{\\beta}=3.0$ "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/7dfd7b9110b6faf729932b6ff7c63c3a6fd631b01ef29497bc8cf6c4031997d9.jpg", "img_caption": ["Figure 13: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.001$ and $\\bar{\\beta}=5.0$ "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/7bb5ff33a424652b3ce506709e75d1a314a7cc61a8f880e2a90af60b7b6c5156.jpg", "img_caption": ["Figure 14: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.003$ and $\\bar{\\beta}=1.0$ "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/11461666e465f383422d948fd537b38a72ab9b572a30b6620ac87fe55e7f6699.jpg", "img_caption": ["Figure 15: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.003$ and $\\bar{\\beta}=3.0$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/c4181a24b8997721260322cf99c55ffb6e4cc1d49a03ba79bd6df2b8f0e533d9.jpg", "img_caption": ["Figure 16: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.003$ and $\\bar{\\beta}=5.0$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/d7b9d254f971be0b7e436dfcd275ad34495e9c46e8bce9b5bdd1f4ef4cbb80c1.jpg", "img_caption": ["Figure 17: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.005$ and $\\bar{\\beta}=1.0$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/0698de517334ef2a681718ecb56eb7ddd6bcff0a0250823dbf9b890f2c1a4aaa.jpg", "img_caption": ["Figure 18: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.005$ and $\\bar{\\beta}=3.0$ "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/4ecad57197c2f88b9862a0f085dc3ec8a8293e216036251074df240515b031ef.jpg", "img_caption": ["Figure 19: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters $\\lambda=0.005$ and $\\bar{\\beta}=5.0$ "], "img_footnote": [], "page_idx": 58}, {"type": "text", "text": "F.3  Tax Design for Macroeconomic Models ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "F.3.1  Implementation Details ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "In our experiments, we use $\\sigma(s)=\\operatorname*{max}(0,\\log(s/20+1))$ and $\\omega(s)=\\operatorname*{min}(s,0)$ and select the hyperparameters as $\\theta\\,=\\,0.1,\\phi\\,=\\,5.0,\\varsigma\\,=\\,5.0,w\\,=\\,1.0$ and $\\lambda\\:=\\:0.3$ .We use 3 products in the simulated economy with unit prices and assume two equal-sized socio-economic groups with preferences $\\alpha=(0.6,0.3,0.1)$ and (0.1, 0.7, 0.2). Assets are initialized as $s_{0}\\sim N(0,\\sqrt{2})$ and each episode is truncated after 200 steps. Both chosen consumption levels and hours worked are discretized with ranges $[0,5]$ and [0, 8] and cardinality 5 and 10, respectively. All tax rates are initialized as 0.3 and optimized over the continuous domain [0.o, 2.0]. ", "page_idx": 58}, {"type": "text", "text": "F.3.2  Effects of lower-level regularization ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Figure 20 and Figure 21 shows the results for $\\lambda\\:=\\:0.3$ and $\\lambda\\:=\\:0.1$ ,respectively.Changing the regularization has small effects on the final results and only marginally change the speed of convergence, especially for the Zero-Order algorithm. ", "page_idx": 58}, {"type": "text", "text": "F.4  Computational Costs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "We ran our experiments on a shared cluster equipped with various NVIDIA GPUs and AMD EPYC CPUs. Our default configuration for all experiments was a single GPU with $24\\:\\mathrm{GB}$ ofmemory,16 CPU cores, and 4 GB of RAM per CPU core. For all parameter configurations reported in Table 3, the total runtime of the experiments for HPGD, AMD, and Zero-Order were 17, 40, and 2 hours, respectively, totaling 59 hours. Our total computational costs including the intermediate experiments are estimated to be 2-3 times more. ", "page_idx": 58}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/afe25432c36849cf91ac7194ab6006aa4b77d2490b84662660c5d4afc8da66d3.jpg", "img_caption": ["(a) Performance on the Tax Design problem ", "Figure 20: Results with $\\lambda=0.3$ ", "(b) Tax rates over the outer iterations. "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "W3Dx1TGW3f/tmp/0b0f7f43ca94cd329c09dd319379b385cc1abf38133e8e2c065e9b0dfed7f20d.jpg", "img_caption": ["(a) Performance on the Tax Design problem ", "Figure 21: Results with $\\lambda=0.1$ ", "(b) Tax rates over the outer iterations. "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Our Abstract gives a short overview of our main contributions. Moreover in the Introduction (Section 1), we state the contributions clearly with bullet points and point to the relevant sections. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 60}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: For all proofs we clearly state the assumptions (see Assumption 3.1 and Assumption 3.2) we need to make and write the main limitations multiple times in the text. For the experiments we clearly point to the sensitivity of the regularization parameters. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 61}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: Our Appendix E contains all proofs to all our statements. For our main theorem (Theorem 3) we further give a proof sketch in the main paper to help with intuition. Our Assumptions are clearly stated. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 61}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We describe the main experimental setting in Section 5 while provide details explanation in Appendix F. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 61}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 62}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with suffcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We provide all code related to the research in this paper at https : //github .   \ncom/lasgroup/HPGD. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 62}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: All experimental details including hyperparameter selection is described in Appendix F. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We report the average over 10 random seeds and the corresponding standard errors for the experimental results reported in Table 3 and on Figure 1b. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 63}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We report our used resources in Appendix F.4. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPs Code of Ethics and conducted the research following the guidelines. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 63}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: Our work has a theoretical focus without an immediate impact on society. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 64}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: To our best knowledge there do not exist such risks for our work. Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We cite all codes and models used. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 65}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: We neither have crowdsourcing experiments nor research with human subjects. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 65}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: We neither have crowdsourcing experiments nor research with human subjects. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 65}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 66}]