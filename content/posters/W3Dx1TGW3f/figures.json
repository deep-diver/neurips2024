[{"figure_path": "W3Dx1TGW3f/figures/figures_7_1.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "This figure contains two subfigures. The left subfigure shows a diagram of the Four-Rooms environment used in the experiment.  The right subfigure shows a plot of the upper-level objective function F (y-axis) versus the number of outer iterations T (x-axis).  The plot compares the performance of three algorithms: HPGD, AMD, and Zero-Order. The results indicate that HPGD outperforms the other two algorithms, particularly by avoiding local optima.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_8_1.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively. HPGD efficiently steers the lower-level MDP when the task is to reach G\u00b9 while others are only successful in the case of G\u00b2.", "description": "This figure shows the reward penalties assigned to each state by three different algorithms: HPGD, AMD, and Zero-Order, in the Four-Rooms environment. The goal is to guide the lower-level agent to a specific target state (+1).  HPGD effectively achieves this goal for G\u00b9, whereas AMD and Zero-Order only succeed for G\u00b2. The visualization uses a color map to represent penalty magnitude, showing the strategy used by each algorithm.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_9_1.jpg", "caption": "Figure 3: Performance on the Tax Design problem. (b) Tax rates over the outer iterations. HPGD increases income tax quickly and distinguishes VAT rates according to preferences while Zero-Order takes more iterations.", "description": "This figure shows the performance of the HPGD and Zero-Order algorithms on the Tax Design problem. The left panel displays the upper-level objective values over the number of outer iterations. The right panel shows the tax rates (income tax and VAT rates for three goods) over the outer iterations.  The results demonstrate that HPGD quickly learns optimal tax policies that distinguish VAT rates according to consumer preferences.  Zero-Order, in contrast, takes significantly more iterations to achieve a similar level of performance.", "section": "5.2 Tax Design for Macroeconomic Models"}, {"figure_path": "W3Dx1TGW3f/figures/figures_26_1.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "The figure consists of two subfigures. The left subfigure shows the state space of the Four-Rooms environment, which is a grid-world with four rooms and a goal in each room. The start state (S) is in the top-left corner, and there are two goal states (G\u00b9 and G\u00b2) located in different rooms. The +1 symbol indicates a target cell. The right subfigure shows the performance of three different algorithms (HPGD, AMD, and Zero-Order) in terms of the upper-level objective value (F) as a function of the number of outer iterations (T). The HPGD algorithm consistently performs better than the other two, demonstrating its ability to escape local optima and achieve higher performance.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_53_1.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "The figure shows the results of experiments conducted using the Four-Rooms environment. The left panel depicts the state space of the environment, highlighting the start state (S), two goal states (G\u00b9 and G\u00b2), and a target cell (+1) that the upper-level algorithm aims to influence the lower-level agent to reach. The right panel presents a comparison of the performance of three different algorithms: HPGD, AMD, and a Zero-Order method, demonstrating HPGD's superior ability to reach higher objective values by escaping local optima.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_53_2.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "This figure shows the results of applying different algorithms (HPGD, AMD, Zero-Order) on a Four-Rooms environment. The left panel displays the state space of the environment, highlighting the start state (S), goal states (G1, G2), and target state (+1). The right panel shows the performance of each algorithm.  HPGD outperforms AMD and Zero-Order, demonstrating its ability to navigate the state space effectively and avoid becoming stuck in local optima.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_54_1.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "This figure consists of two subfigures. The left subfigure shows a grid-world environment called the Four-Rooms environment with a starting state (S), two goal states (G\u00b9 and G\u00b2), and a target cell (+1).  The right subfigure presents a performance comparison of three algorithms (HPGD, AMD, and Zero-Order) on the Four-Rooms environment, plotting the upper-level objective values over the number of outer iterations. It shows that HPGD surpasses AMD and Zero-Order, escaping local optima and achieving a better objective value.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_54_2.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "The figure consists of two subfigures. The left subfigure shows the state space of the Four-Rooms environment, which is a grid world with four rooms and a goal in two different locations (G1 and G2). The agent starts in a designated start state (S) and the objective for the upper level is to guide the agent to a specific target state (+1).  The right subfigure is a performance plot comparing HPGD against two other algorithms (AMD and Zero-Order) across outer iterations in terms of optimizing the upper-level objective function. This plot demonstrates that HPGD outperforms the other algorithms, especially by escaping local optima and achieving a higher value for the objective function.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_54_3.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "This figure shows the results of an experiment comparing the performance of three different algorithms (HPGD, AMD, and Zero-Order) on a Four-Rooms environment. The left panel displays the state space of the environment, showing the starting position (S), two goal states (G1 and G2), and the target cell (+1) that the upper-level algorithm aims to lead the lower-level agent to. The right panel presents a graph illustrating the performance of each algorithm over a certain number of iterations.  It shows that HPGD is able to reach a higher objective value compared to the other algorithms, likely by escaping local optima.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_55_1.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "The figure contains two subfigures. The left subfigure shows the state space of the Four-Rooms environment. The right subfigure presents a comparison of the performance of three different algorithms (HPGD, AMD, and Zero-Order) in terms of the upper-level objective values over the number of outer iterations. The plot demonstrates that HPGD outperforms the other algorithms by escaping local optima and achieving a higher objective value.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_55_2.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "This figure shows the results of the Four-Rooms experiment. The left panel shows a grid-world environment where the agent starts at S and must reach either goal state (G1 or G2).  The +1 indicates the state the leader wants the follower agent to reach. The right panel displays the performance comparison of three algorithms in the Four-Rooms environment. HPGD outperforms the other two (AMD and Zero-Order) because of its ability to avoid getting stuck in local optima.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_55_3.jpg", "caption": "Figure 1: Four-Rooms State Space and Performance. Left: S denotes the start state while G\u00b9 and G\u00b2 denote goal states that are considered separate tasks. +1 denotes the target cell to which the upper-level aims to steer the lower-level MDP. Right: HPGD escapes local optima achieving higher performance than comparison algorithms.", "description": "The figure shows two subfigures. The left subfigure displays the state space of the Four-Rooms environment used in the experiments. The right subfigure presents a performance comparison of three algorithms: HPGD, AMD, and Zero-Order. The algorithms aim to optimize the upper-level objective in the Four-Rooms environment by influencing the lower-level MDP. The plot shows the upper-level objective values over the number of outer iterations. HPGD outperforms the other two algorithms, escaping local optima and achieving better results.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_56_1.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure shows the reward penalties assigned to each state in the Four-Rooms environment by three different algorithms: HPGD, AMD, and Zero-Order. The penalties are represented by color intensity, with darker shades indicating stronger penalties. The algorithms aim to guide the lower-level agent towards a specific target state (+1) by strategically placing penalties.  The figure visually compares the approaches of each algorithm in achieving this goal.", "section": "Numerical Experiments"}, {"figure_path": "W3Dx1TGW3f/figures/figures_56_2.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure shows the reward penalties assigned to states by three different algorithms (HPGD, AMD, and Zero-Order) in the Four-Rooms environment.  The color intensity represents the magnitude of the penalty, with darker shades indicating stronger penalties.  The goal is to guide the lower-level agent through a specific state (+1). The figure helps visualize how each algorithm modifies the environment to achieve its objective, highlighting differences in their approaches to reward shaping.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_56_3.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure shows the reward penalties assigned to each state in the Four-Rooms environment by three different algorithms: HPGD, AMD, and Zero-Order.  The color intensity represents the magnitude of the penalty, with darker shades indicating stronger penalties. The figure visually compares how each algorithm uses penalties to influence the lower-level agent's behavior, showcasing differences in their strategies for guiding the agent to a specific target location (+1).", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_57_1.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure shows the reward penalties assigned by three different algorithms (HPGD, AMD, and Zero-Order) to influence the lower-level agent's behavior in the Four-Rooms environment.  The penalties, represented by color intensity, are designed to guide the agent towards a specific target cell (+1).  The visualization helps to compare how each algorithm modifies the environment to achieve its objective, highlighting potential differences in their strategies and effectiveness.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_57_2.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure shows the reward penalties assigned by three different algorithms (HPGD, AMD, and Zero-Order) to the lower-level agent in each state of the Four-Rooms environment. The color intensity represents the magnitude of the penalty, with darker shades indicating stronger penalties.  The algorithms aim to steer the lower-level agent through a specific target cell (+1).  The figure illustrates how the different optimization strategies lead to varying penalty distributions across the states, reflecting the diverse ways in which the algorithms achieve their objective.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_57_3.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure visualizes the reward penalties assigned to each state by three different algorithms: HPGD, AMD, and Zero-Order.  The goal is to guide the lower-level agent through a specific cell in the Four-Rooms environment. The color intensity represents the magnitude of the penalty, with darker shades indicating stronger penalties.  The figure shows how each algorithm strategically applies penalties to influence the agent's path.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_58_1.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure compares reward penalties assigned by three different algorithms (HPGD, AMD, and Zero-Order) in the Four-Rooms environment. Each algorithm aims to guide a lower-level agent toward a specific target cell (+1) by adjusting the reward penalties for each state. The color intensity represents the magnitude of the penalty. Darker shades indicate stronger negative penalties, while lighter shades show weaker penalties. The figure visualizes how each algorithm strategically uses penalties to influence the agent's path, highlighting their differences in achieving the objective.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_58_2.jpg", "caption": "Figure 2: Reward penalties given to the lower-level agent in each state of the Four-Rooms problem optimized by the HPGD, AMD, and Zero-Order, respectively, for hyperparameters \u03bb = 0.001 and \u03b2 = 3.0.", "description": "This figure compares the reward penalties assigned to different states in the Four-Rooms environment by three different algorithms: HPGD, AMD, and Zero-Order. The color intensity represents the magnitude of the penalty, with darker shades indicating stronger penalties. The goal is to observe how each algorithm shapes the reward function to guide the lower-level agent's behavior.  Specifically, the leader aims to influence the agent to pass through a particular cell (+1) en route to one of the goal states (G1 or G2). The figure helps to illustrate the differences in the approaches taken by each algorithm to solve this bilevel optimization problem.", "section": "5.1 Four-Rooms Environment"}, {"figure_path": "W3Dx1TGW3f/figures/figures_59_1.jpg", "caption": "Figure 3: Performance on the Tax Design problem. (a) Performance on the Tax Design problem. (b) Tax rates over the outer iterations. HPGD increases income tax quickly and distinguishes VAT rates according to preferences, while Zero-Order takes more iterations.", "description": "This figure shows the results of applying the HPGD and Zero-Order algorithms to a tax design problem.  The left panel displays the upper-level objective function (social welfare) over the number of outer iterations. HPGD converges to a higher social welfare value much faster than the Zero-Order method. The right panel displays the tax rates (income tax and VAT rates for three goods) over the outer iterations.  HPGD efficiently learns to adjust tax rates to maximize social welfare, while the Zero-Order method takes significantly longer to find optimal rates and fails to distinguish between goods in setting optimal VAT rates.", "section": "5.2 Tax Design for Macroeconomic Models"}, {"figure_path": "W3Dx1TGW3f/figures/figures_59_2.jpg", "caption": "Figure 3: Performance on the Tax Design problem. (b) Tax rates over the outer iterations. HPGD increases income tax quickly learn optimal tax policies quickly and distinguishes VAT rates according to preferences. while Zero-Order takes more iterations.", "description": "This figure shows the performance of HPGD and Zero-Order algorithms on a tax design problem for macroeconomic models. The left panel displays the upper-level objective (social welfare) over the outer iterations, demonstrating that HPGD converges faster than Zero-Order.  The right panel shows the evolution of tax rates (income tax and VAT rates for three goods) over iterations. HPGD efficiently adapts tax rates to maximize social welfare while Zero-Order struggles to learn optimal policies.", "section": "5 Numerical Experiments"}]