[{"figure_path": "0m19blQT6y/tables/tables_4_1.jpg", "caption": "Table 1: Pearson correlation (absolute value) of quantization error between different metrics (e.g., MSE vs. PSNR denotes the correlation between two metrics) when quantizing individual layers to 1, 2, and 3 bits. CS denotes CLIP Score.", "description": "This table presents the Pearson correlation coefficients between different image quality metrics (MSE, PSNR, LPIPS, and CLIP Score) when individual layers of a Stable Diffusion model are quantized to 1, 2, or 3 bits.  It shows the strength of the linear relationship between pairs of metrics across different quantization levels.  A high correlation suggests that these metrics capture similar aspects of image quality degradation caused by quantization.", "section": "3.2 Per-Layer Quantization Error Analysis"}, {"figure_path": "0m19blQT6y/tables/tables_4_2.jpg", "caption": "Table 2: Pearson correlation (absolute value) of quantization error between different bit pairs (e.g., 1 vs. 2 denotes the correlation between the two bit widths) for a single metric when quantizing individual layers to 1, 2, and 3 bits.", "description": "This table presents the Pearson correlation coefficients between quantization errors obtained when quantizing individual layers to different bit-widths (1, 2, and 3 bits).  Each cell shows the correlation for a single metric (MSE, PSNR, LPIPS, or CLIP score) between two different bit-widths.  For instance, the value 0.929 in the first row and first column indicates a strong positive correlation (0.929) between the MSE obtained using 1-bit quantization and the MSE obtained using 2-bit quantization. This suggests that layers with high MSE in 1-bit quantization are likely to also have high MSE in 2-bit quantization. The table helps to understand the relationship between quantization errors across different bit-widths for each metric.", "section": "3.2 Per-Layer Quantization Error Analysis"}, {"figure_path": "0m19blQT6y/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with existing quantization methods, including LSQ [11], Q-Diffusion [38], EfficientDM [17], and Apple-MBP [62]. The CLIP score is measured on 1K PartiPrompts.", "description": "This table compares the performance of BitsFusion (the proposed method) against other existing weight quantization methods for diffusion models.  The methods are compared based on their bit-width and the CLIP score achieved on a set of 1000 PartiPrompts.  The CLIP score measures the alignment between generated images and their text descriptions, indicating the overall quality of the generated images.", "section": "5 Experiments"}, {"figure_path": "0m19blQT6y/tables/tables_7_2.jpg", "caption": "Table 4: Analysis of our proposed methods measured under various CFG scales, i.e., 3.5, 5.5, 7.5, and 9.5. We use LSQ [11] as the basic QAT method, which involves the training of weights and scaling factors of a uniformly 2-bit quantized UNet. Then, we gradually introduce each proposed technique to evaluate their effectiveness. CLIP scores are measured on 1K PartiPrompts.", "description": "This table presents an ablation study, analyzing the impact of different techniques incorporated into the BitsFusion model on its performance.  The baseline is a simple 2-bit quantization (LSQ).  Subsequent rows add components of the BitsFusion method one at a time, showing how each contributes to the overall CLIP score. The results are evaluated across various CFG (Classifier-Free Guidance) scales to demonstrate robustness. The final row shows the overall improvement achieved by BitsFusion.", "section": "5.2 Ablation Analysis"}, {"figure_path": "0m19blQT6y/tables/tables_8_1.jpg", "caption": "Table 5: Analysis of \u03b7 in the mixed-precision strategy.", "description": "This table presents the results of an ablation study on the parameter size factor (\u03b7) used in the mixed-precision quantization strategy.  Different values of \u03b7 were tested (0, 0.1, 0.2, 0.3, 0.4, 0.5), and the corresponding CLIP scores are shown.  The experiment aimed to determine the optimal value of \u03b7 that balances model size reduction with maintaining good image generation quality.", "section": "5.2 Ablation Analysis"}, {"figure_path": "0m19blQT6y/tables/tables_31_1.jpg", "caption": "Table 4: Analysis of our proposed methods measured under various CFG scales, i.e., 3.5, 5.5, 7.5, and 9.5. We use LSQ [11] as the basic QAT method, which involves the training of weights and scaling factors of a uniformly 2-bit quantized UNet. Then, we gradually introduce each proposed technique to evaluate their effectiveness. CLIP scores are measured on 1K PartiPrompts.", "description": "This table presents an ablation study evaluating the impact of different techniques incorporated into the BitsFusion model.  It starts with a baseline method (LSQ) and sequentially adds components like balanced integer initialization, alternating optimization, mixed precision with caching, feature distillation, and time-step aware sampling.  The results, measured by CLIP score on 1k PartiPrompts, show the performance improvement at each step, demonstrating the effectiveness of the proposed techniques in improving the quantization performance of the UNet in Stable Diffusion. The CFG (classifier-free guidance) scale is varied to show the model's robustness across different guidance levels.", "section": "5.2 Ablation Analysis"}]