[{"type": "text", "text": "BitsFusion: 1.99 bits Weight Quantization of Diffusion Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Sui1,2,\u2020 Yanyu Li1 Anil Kag1 Yerlan Idelbayev1 Junli Cao1 Ju Hu1 Dhritiman Sagar1 Bo Yuan2 Sergey Tulyakov1 Jian Ren1,\u2217 1Snap Inc. 2Rutgers University Project Page: https://snap-research.github.io/BitsFusion ", "page_idx": 0}, {"type": "image", "img_path": "0m19blQT6y/tmp/e9e168bc598d6194c8ad317c3b6af879dcf056a8e67fc80e8ab61a0183bab965.jpg", "img_caption": ["Figure 1: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion, where the weights of UNet are quantized into 1.99 bits, achieving $7.9\\times$ smaller storage than the one from Stable Diffusion v1.5. All the images are synthesized under the setting of using PNDM sampler [49] with 50 sampling steps and random seed as 1024. Prompts and more generations are provided in App. M. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content. However, these models contain a huge number of parameters, resulting in a significantly large model size. Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices. In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with $7.9\\times$ smaller size while exhibiting even better generation quality than the original one. Our approach includes several novel techniques, such as assigning optimal bits to each layer, initializing the quantized model for better performance, and improving the training strategy to dramatically reduce quantization error. Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent efforts in developing diffusion-based image generation models [77, 31, 79, 21, 80] have demonstrated remarkable results in synthesizing high-fidelity and photo-realistic images, leading to various applications such as content creation and editing [68, 67, 61, 71, 90, 88, 50, 40], video generation [20, 75, 3, 1, 16, 57, 15], and 3D asset synthesis [87, 44, 64, 74, 65], among others. ", "page_idx": 0}, {"type": "text", "text": "However, Diffusion Models (DMs) come with the drawback of a large number of parameters, e.g., millions or even billions, causing significant burdens for transferring and storing due to the bulky model size, especially on resource-constrained hardware such as mobile and wearable devices. ", "page_idx": 1}, {"type": "text", "text": "Existing studies have explored reducing the model size of large-scale text-to-image diffusion models by designing efficient architectures and network pruning [41, 92, 32]. These approaches usually require significant amounts of training due to the changes made to the pre-trained networks. Another promising direction for model storage reduction is quantization [12, 30], where floating-point weights are converted to low-bit fixed-point representations, thereby saving computation memory and storage. ", "page_idx": 1}, {"type": "text", "text": "There have been emerging efforts on compressing the DMs through quantization [73, 38, 17, 39]. However, these approaches still face several major challenges, especially when quantizing large-scale text-to-image diffusion models like Stable Diffusion v1.5 (SD-v1.5) [70]. First, many of these methods are developed on relatively small-scale DMs trained on constrained datasets. For example, models trained on CIFAR-10 require modest storage of around 100 MB [21, 39]. In contrast, SD-v1.5 necessitates 3.44 GB of storage in a full-precision format. Adapting these methods to SD-v1.5 remains to be a challenging problem. Second, current arts mainly focus on quantizing weights to 4 bits. How to quantize the model to extremely low bit is not well studied. Third, there is a lack of fair and extensive evaluation of how quantization methods perform on large-scale DMs, i.e., SD-v1.5. ", "page_idx": 1}, {"type": "text", "text": "To tackle the above challenges, this work proposes BitsFusion, a quantization-aware training framework that employs a series of novel techniques to compress the weights of pre-trained large-scale DMs into extremely low bits (i.e., 1.99 bits), achieving even better performance (i.e., higher image quality and better text-image alignment). Consequently, we compress the 1.72 GB UNet (FP16)1 of SD-v1.5 into a ${219}\\,\\mathrm{MB}$ model, achieving a $7.9\\times$ compression ratio. Specifically, our contributions can be summarized into the following four dimensions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Mixed-Precision Quantization for DMs. We propose an effective approach for quantizing DMs in a mixed-precision manner. First, we thoroughly analyze the appropriate metrics to understand the quantization error in the quantized DMs (Sec. 3.2). Second, based on the analysis, we quantize different layers into different bits according to their quantization error (Sec. 3.3). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Initialization for Quantized DMs. We introduce several techniques to initialize the quantized model to improve performance, including time embedding pre-computing and caching, adding balance integer, and alternating optimization for scaling factor initialization (Sec. 4.1). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Improved Training Pipeline for Quantized DMs. We improve the training pipeline for the quantized model with the proposed two-stage training approach (Sec. 4.2). In the first stage, we use the full-precision model as a teacher to train the quantized model through distillation. Our distillation loss forces the quantized model to learn both the predicted noise and the intermediate features from the teacher network. Furthermore, we adjust the distribution of time step sampling during training, such that the time steps causing larger quantization errors are sampled more frequently. In the second stage, we fine-tune the model using vanilla noise prediction [21]. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive Quantitative Evaluation. For the first time in the literature, we conduct extensive quantitative analysis to compare the performance of the quantized model against the original SD-v1.5. We include results on various benchmark datasets, i.e., TIFA [25], GenEval [13], CLIP score [66] and FID [19] on MS-COCO 2014 validation set [46]. Additionally, we perform human evaluation on PartiPrompts [86]. Our 1.99-bit weights quantized model consistently outperforms the full-precision model across various evaluations, demonstrating the effectiveness of our approach. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To enhance model efficiency in terms of storage and computational costs, quantization [11, 59, 58, 43, 36, 60, 48, 84, 45, 53] is adopted for diffusion models [73, 38, 18, 76, 81, 83, 51, 85, 4, 82, 7, 93, 27, 17, 39, 91] with primarily two types: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ does not require a full training loop; instead, it utilizes a limited calibration dataset to adjust the quantization parameters. For example, PTQ4DM [73] calibrates the quantization parameters to minimize the quantization error of DMs. Q-Diffusion [38] minimizes the quantization error via the block-wise reconstruction [42]. PTQD [18] integrates quantization noise into the stochastic noise inherent in the sampling steps of DMs. TDQ [76] optimizes scaling factors for activations across different time steps, applicable to both PTQ and QAT strategies. TFMQ [27] focuses on reconstructing time embedding and projection layers to prevent over-ftiting. However, PTQ often results in performance degradation compared to QAT, particularly when aiming for extremely low-bit DMs. In contrast, QAT involves training the full weights to minimize the quantization error, thereby achieving higher performance compared to PTQ. For instance, EfficientDM [17], inspired by LoRA [24], introduces a quantization-aware low-rank adapter to update the LoRA weights, avoiding training entire weights. Q-DM [39] employs normalization and smoothing operation on attention features through proposed Q-attention blocks, enhancing quantization performance. Nevertheless, existing works primarily study 4 bits and above quantization on small-scale DMs trained on constrained datasets. In this paper, we focus on quantizing large-scale Stable Diffusion to extremely low bits and extensively evaluating the performance across different benchmark datasets. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Mixed Precision Quantization for Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first go through the formulations of weight quantization and generative diffusion models. We then determine the mixed-precision strategy, assigning optimized bit widths to different layers to reduce the overall quantization error. Specifically, we first analyze the quantization error of each layer in the diffusion model and conclude sensitivity properties. Then, based on the analysis, we assign appropriate bits to each layer by jointly considering parameter efficiency (i.e., size savings). ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quantization is a popular and commonly used technique to reduce model size. While many quantization forms exist, we focus on uniform quantization, where full-precision values are mapped into discrete integer values as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{\\mathrm{int}}=\\mathtt{C l i p}(\\lfloor\\frac{\\pmb{\\theta}_{\\mathtt{f p}}}{\\mathtt{s}}\\rceil+I_{z},0,2^{b}-1),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{\\tt f p}$ denotes the floating-point weights, $\\theta_{\\mathrm{int}}$ is the quantized integer weights, s is the scaling factor, $I_{z}$ is the zero point, and $b$ is the quantization bit-width. \u230a\u00b7\u2309denotes the nearest rounding operation and $\\mathtt{C l i p}(\\cdot)$ denotes the clipping operation that constrains $\\theta_{\\mathrm{int}}$ within the target range. Following the common settings [38, 17], we apply the channel-wise quantization and set 8 bits for the first and last convolutional layer of the UNet. ", "page_idx": 2}, {"type": "text", "text": "Stable Diffusion. Denoising diffusion probabilistic models [77, 21] learn to predict real data distribution $\\mathbf{x}\\,\\sim\\,p_{\\mathrm{data}}$ by reversing the ODE flow. Specifically, given a noisy data sample $\\mathbf{z}_{t}=$ $\\alpha_{t}\\mathbf{x}+\\sigma_{t}\\epsilon$ ( $\\ \\alpha_{t}$ and $\\sigma_{t}$ are SNR schedules and $\\epsilon$ is the added ground-truth noise), and a quantized denoising model $\\hat{\\epsilon}_{\\theta_{\\mathrm{int},s}}$ parameterized by $\\theta_{\\mathrm{int}}$ and s, the learning objective can be formulated as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathbf{s}}=\\mathbb{E}_{t,\\mathbf{x}}\\left[\\|\\epsilon-\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},\\mathbf{s}}(t,\\mathbf{z}_{t},\\mathbf{c})\\|\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t$ is the sampled time step and $\\mathbf{c}$ is the input condition (e.g., text embedding). Note that during the training of quantized model, we optimize $\\theta_{\\tt f p}$ and s by backpropagating $\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathbf{s}}$ via StraightThrough Estimator (STE) [2] and quantize the weights to the integers for deployment. Here, for the notation simplicity, we directly use $\\theta_{\\mathrm{int}}$ to represent the optimized weights in the quantized models. ", "page_idx": 2}, {"type": "text", "text": "The latent diffusion model [70] such as Stable Diffusion conducts the denoising process in the latent space encoded by variational autoencoder (VAE) [34, 69], where the diffusion model is the UNet [9]. This work mainly studies the quantization for the UNet model, given it is the major bottleneck for the storage and runtime of the Stable Diffusion [41]. During the inference time, classifier-free guidance (CFG) [22] is usually applied to improve the generation, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\epsilon}_{\\theta_{\\mathrm{int}},{\\bf s}}(t,{\\bf z}_{t},{\\bf c})=w\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},{\\bf s}}(t,{\\bf z}_{t},{\\bf c})-(w-1)\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},{\\bf s}}(t,{\\bf z}_{t},\\emptyset),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $w\\ge1$ and $\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},\\mathbf{s}}(t,\\mathbf{z}_{t},\\mathcal{D})$ denotes the generation conditioned on the null text prompt $\\mathcal{Q}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Per-Layer Quantization Error Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Obtaining Quantized Models. We first perform a per-layer sensitivity analysis for the diffusion model. Specifically, given a pre-trained full-precision diffusion model, we quantize each layer to 1, 2, and 3 bits while freezing others at full-precision, and performing quantization-aware training (QAT) ", "page_idx": 2}, {"type": "image", "img_path": "0m19blQT6y/tmp/80dd62a7f89cde263796b1c24d221d796865a2f834e379f826b173a0a4c38c6b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "(a) Left most column shows the images synthesized by SD-v1.5 FP32 and other columns show images generated by the quantized models, where only one layer is quantized (e.g., CA toq denotes the cross-attention layer for Query projection is quantized and RB conv shortcut denotes the Convolution Shotcut layer in Residual Block is quantized. The quantized layers follow the same order of highlighted layers in (b) and (c), from left to right. Quantizing the layers impact both the image quality (as in RB conv shortcut) and text-image alignment (e.g., the teddy bear disappears after quantizing some CA tok layers). ", "page_idx": 3}, {"type": "image", "img_path": "0m19blQT6y/tmp/dd6ce7222aa87f31be9ee3a12a2d7fb9af9f26cb1b9f3077ad450203c8354dec.jpg", "img_caption": ["Figure 2: 1-bit quantization error analysis for all the layers from the UNet of SD-v1.5. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "respectively. For instance, for the SD-v1.5 UNet with 256 layers (excluding time embedding, the first and last layers), we get a total of 768 quantized candidates. We perform QAT over each candidate on a pre-defined training sub dataset, and validate the incurred quantization error of each candidate by comparing it against the full-precision model (more details in App. B). ", "page_idx": 3}, {"type": "text", "text": "Measuring Quantization Errors. To find the appropriate way to interpret the quantization error, we analyze four metrics: Mean-Squared-Error (MSE) that quantifies the pixel-level discrepancies between images (generations from floating and the quantized model in our case), LPIPS [89] that assesses human-like perceptual similarity judgments, PSNR [23] that measures image quality by comparing the maximum possible power of a signal with the power of a corrupted noise, and CLIP score [66] that evaluates the correlation between an image and its language description. After collecting the scores (examples in Fig. 2b and Fig. 2c, full metrics are listed in App. F), we further measure the consistency of them by calculating the Pearson correlation [8] for different metrics under the same bit widths (in Tab. 1), and different bit widths under the same metric (in Tab. 2). With these empirical results, we draw the following two main observations. ", "page_idx": 3}, {"type": "text", "text": "Observation 1: MSE, PSNR, and LPIPS show strong correlation and they correlate well with the visual perception of image quality. ", "page_idx": 3}, {"type": "text", "text": "Tab. 1 shows that MSE is highly correlated with PSNR and LPIPS under the same bit width. Additionally, we observe a similar trend of per-layer quantization error under different bit widths, as in Tab. 2. As for visual qualities in Fig. 2a and 2b, we can see that higher MSE errors lead to severe image quality degradation, e.g., the highlighted RB conv shortcut. Therefore, the MSE metric effectively reflects quality degradations incurred by quantization, and it is unnecessary to incorporate PSNR and LPIPS further. ", "page_idx": 3}, {"type": "text", "text": "Observation 2: After low-bit quantization, changes in CLIP score are not consistently correlated with MSE across different layers. Although some layers show smaller MSE, they may experience larger semantic degradation, reflected in larger CLIP score changes. ", "page_idx": 3}, {"type": "text", "text": "We notice that, after quantization, the CLIP score changes for all layers only have a weak correlation with MSE, illustrated in Tab. 1. Some layers display smaller MSE but larger changes in CLIP score. ", "page_idx": 3}, {"type": "text", "text": "Table 1: Pearson correlation (absolute value) of quantization error between different metrics (e.g., MSE vs. PSNR denotes the correlation between two metrics) when quantizing individual layers to 1, 2, and 3 bits. CS denotes CLIP Score. ", "page_idx": 4}, {"type": "table", "img_path": "0m19blQT6y/tmp/ff541bca29943d816b1317d77e55f0ab361db76fd60846d5d61171d03e7006f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 2: Pearson correlation (absolute value) of quantization error between different bit pairs (e.g., 1 vs. 2 denotes the correlation between the two bit widths) for a single metric when quantizing individual layers to 1, 2, and 3 bits. ", "page_idx": 4}, {"type": "table", "img_path": "0m19blQT6y/tmp/e3a56ccf4f1a5dac14057ed2a392f95d562e7ff524ef07a57a58a0faa9706160.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "For example, in Fig. 2b, the MSE of CA tok layer $5_{t h}$ highlighted layer (green) from left to right) is less than that of RB conv layer $\\ensuremath{\\left\\vert6_{t h}\\right\\vert}$ highlighted layer (orange) from left to right), yet the changes in CLIP score are the opposite. As observed in the first row of Fig. 2a, compared to RB conv layer, quantizing this CA tok layer changes the image content from \"a teddy bear\" to \"a person\", which diverges from the text prompt A teddy bear on a skateboard in Times Square, doing tricks on a cardboard box ramp. This occurs because MSE measures only the difference between two images, which does not capture the semantic degradation. In contrast, the CLIP score reflects the quantization error in terms of semantic information between the text and image. Thus, we employ the CLIP score as a complementary metric to represent the quantization error. ", "page_idx": 4}, {"type": "text", "text": "3.3 Deciding the Optimal Precision ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the above observations, we then develop the strategy for bit-width assignments. We select MSE and CLIP as our quantitative metrics, along with the number of parameters of each layer as the indicator of size savings. ", "page_idx": 4}, {"type": "text", "text": "Assigning bits based on MSE. Intuitively, layers with more parameters and lower quantization error are better candidates for extremely low-bit quantization, as the overall bit widths of the model can be significantly reduced. According to this, we propose a layer size-aware sensitivity score $\\boldsymbol{S}$ . For the $i_{t h}$ layer, its sensitivity score for the $b^{\\phantom{\\dagger}}$ -bits $(b\\in\\{1,2,3\\})$ ) is defined as $S_{i,b}\\,=\\,M_{i,b}N_{i}^{-\\eta}$ , where $M$ denotes the MSE error, $N$ is the total number of parameters of the layer, and $\\eta\\in[0,1]$ denotes the parameter size factor. To determine the bit width $(i.e.,\\,b^{*})$ for each layer, we define a sensitivity threshold as $\\scriptstyle{S_{o}}$ , and the $i_{t h}$ layer is assigned to $b_{i}^{*}$ -bits, where $b_{i}^{*}=\\operatorname*{min}\\{b|S_{i,b}<S_{o}\\}$ . The remaining layers are 4 bits. ", "page_idx": 4}, {"type": "text", "text": "Assigning bits based on CLIP score. For the layers with a high CLIP score dropping after quantization, instead of assigning bits based on sensitivity score as discussed above, we directly assign higher bits to those layers. Therefore, the quantized model can produce content that aligns with the semantic information of the prompt. We provide the detailed mixed-precision algorithm in Alg. 1 of App. B. ", "page_idx": 4}, {"type": "text", "text": "4 Training Extreme Low-bit Diffusion Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the bits of each layer decided, we then train the quantized model with a series of techniques to improve performance. The overview of our approach is illustrated in Fig. 3. ", "page_idx": 4}, {"type": "text", "text": "4.1 Initializing the Low-bit Diffusion Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Time Embedding Pre-computing and Caching. During the inference time of a diffusion model, a time step $t$ is transformed into an embedding through projection layers to be incorporated into the diffusion model. As mentioned by existing works [27], the quantization of the projection layers can lead to large quantization errors. However, the embedding from each time step $t$ is always the same, suggesting that we can actually pre-compute the embedding offline and load cached values during inference, instead of computing the embedding every time. Furthermore, the storage size of the time embedding is $25.6\\times$ smaller than the projection layers. Therefore, we pre-compute the time embedding and save the model without the project layers. More details are provided in App. C. ", "page_idx": 4}, {"type": "text", "text": "Adding Balance Integer. In general, weight distributions in deep neural networks are observed as symmetric around zero [94]. To validate the assumption on SD-v1.5, we analyze its weight distribution for the layers under full precision by calculating the skewness of weights. Notably, the skewness of more than $97\\%$ of the layers ranges between $[-0.5,0.5]$ , indicating that the weight distributions are symmetric in almost all layers. Further details are provided in App. D. ", "page_idx": 4}, {"type": "image", "img_path": "0m19blQT6y/tmp/97f529aed7725b8030529e619b0e9790d72a7878e87f50d7b957be0541da1414.jpg", "img_caption": ["Figure 3: Overview of the training and inference pipeline for the proposed BitsFusion. Left: We analyze the quantization error for each layer in SD-v1.5 (Sec. 3.2) and derive the mixed-precision recipe (Sec. 3.3) to assign different bit widths to different layers. We then initialize the quantized UNet by adding a balance integer, pre-computing and caching the time embedding, and alternately optimizing the scaling factor (Sec. 4.1). Middle: During the Stage-I training, we freeze the teacher model (i.e., SD-v1.5) and optimize the quantized UNet through CFG-aware quantization distillation and feature distillation losses, along with sampling time steps by considering quantization errors (Sec. 4.2). During the Stage-II training, we fine-tune the previous model with the noise prediction. Right: For the inference stage, using the pre-cached time features, our model processes text prompts and generates high-quality images. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "However, existing works on diffusion model quantization overlook the symmetric property [38, 73, 39], as they perform relatively higher bits quantization, e.g., 4 or 8 bits. This will hurt the model performance at extremely low bit levels. For example, in 1-bit quantization, the possible most symmetric integer outcomes can only be $\\{0,1\\}$ or $\\{-1,0\\}$ . Similarly, for 2-bit quantization, the most balanced mapping integers can be either $\\{-2,-1,0,1\\}$ or $\\{-1,0,1,2\\}$ , significantly disrupting the symmetric property. The absence of a single value among 2 or 4 numbers under low-bit quantization can have a significant impact. To tackle this, we leverage the bit balance strategy [37, 56] to initialize the model. Specifically, we introduce an additional value to balance the original quantization values. Namely, in a 1-bit model, we adjust the candidate integer set from $\\{0,1\\}$ to $\\{-1,0,1\\}$ , achieving a more balanced distribution. By doing so, we treat the balanced $n$ -bits weights as $1\\circ\\operatorname{g}(2^{n}+1)$ -bits. ", "page_idx": 5}, {"type": "text", "text": "Scaling Factor Initialization via Alternating Optimization. Initializing scaling factors is an important step in quantization. Existing QAT works typically employ the Min-Max initialization strategy [17, 52] to ensure the outliers are adequately represented and preserved. However, such a method faces challenges in extremely low-bit quantization settings like 1-bit, since the distribution of the full-precision weights is overlooked, leading to a large quantization error and the increased difficulty to converge. Therefore, we aim to minimize the $\\ell_{2}$ error between the quantized weights and full-precision weights with the optimization objective as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}}\\|\\mathbf{s}\\cdot(\\theta_{\\mathrm{int}}-I_{z})-\\theta_{\\mathrm{fp}}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Nevertheless, considering the rounding operation, calculating an exact closed-form solution is not straightforward [29]. Inspired by the Lloyd-Max algorithm [28, 54], we use an optimization method on scaling factor s to minimize the initialization error of our quantized diffusion model as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{\\mathrm{int}}^{j}=Q_{\\mathrm{int}}(\\pmb{\\theta}_{\\mathrm{fp}},\\mathbf{s}^{j-1});\\ \\mathbf{s}^{j}=\\frac{\\theta_{\\mathrm{fp}}^{j}(\\pmb{\\theta}_{\\mathrm{int}}^{j}-I_{z})^{\\top}}{(\\theta_{\\mathrm{int}}^{j}-I_{z})(\\pmb{\\theta}_{\\mathrm{int}}^{j}-I_{z})^{\\top}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q_{\\mathrm{int}}(\\cdot)$ denotes the integer mapping quantization operation that converts the full-precision weights to integer as Eq. (1), and $j$ represents the iterative step. The optimization is done for 10 steps. ", "page_idx": 5}, {"type": "text", "text": "4.2 Two-Stage Training Pipeline ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With the mixed-precision model initialized, we introduce the two-stage training pipeline. In Stage-I, we train the quantized model using the full-precision model as the teacher through distillation loss. In Stage-II, we fine-tune the model from the previous stage using noise prediction [21, 80]. ", "page_idx": 6}, {"type": "text", "text": "CFG-aware Quantization Distillation. Similar to existing works [11], we fine-tune the quantized diffusion model to improve the performance. Here both the weights and scaling factors are optimized. Additionally, we notice that training the quantized model in a distillation fashion using the full-precision model yields better performance than training directly with vanilla noise prediction. Furthermore, during distillation, it is crucial for the quantized model to be aware of CFG, i.e., text dropping is applied during distillation. Specifically, our training objective is as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathrm{s}}^{\\mathrm{noise}}=\\mathbb{E}_{t,\\mathrm{x}}\\left[\\left\\|\\hat{\\epsilon}_{\\theta_{t}_{\\mathrm{p}}}(t,\\mathbf{z}_{t},\\mathbf{c})-\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},\\mathrm{s}}(t,\\mathbf{z}_{t},\\mathbf{c})\\right\\|\\right],\\mathbf{c}=\\mathcal{O}\\;\\mathrm{if}\\;P\\sim U[0,1]<p\\;\\mathrm{else}\\;\\mathbf{c},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $P$ controls the text dropping probability during training and $p$ is set as 0.1. ", "page_idx": 6}, {"type": "text", "text": "Feature Distillation. To further improve the generation quality of the quantized model, we distill the full-precision model at a more fine-grained level through feature distillation [32] as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathrm{s}}^{\\mathtt{f e a t}}=\\mathbb{E}_{t,\\mathbf{x}}\\left[\\|\\mathcal{F}_{\\theta_{\\mathrm{fp}}}(t,\\mathbf{z}_{t},\\mathbf{c})-\\mathcal{F}_{\\theta_{\\mathrm{int}},\\mathrm{s}}(t,\\mathbf{z}_{t},\\mathbf{c})\\|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{F}_{\\theta}(\\cdot)$ denotes the operation for getting features from the Down and Up blocks in UNet. We then have the overall distillation loss $\\mathcal{L}^{\\tt\\bar{d}\\tt i s t}$ in Stage-I as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{dist}}=\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathrm{s}}^{\\mathrm{noise}}+\\lambda\\mathcal{L}_{\\theta_{\\mathrm{int}},\\mathrm{s}}^{\\tt f e a t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is empirically set as 0.01 to balance the magnitude of the two loss functions. ", "page_idx": 6}, {"type": "text", "text": "Quantization Error-aware Time Step Sampling. The training of diffusion models requires sampling different time steps in each optimization iteration. We explore how to adjust the strategy for time step sampling such that the quantization error in each time step can be effectively reduced during training. We first train a 1.99-bit quantized model with Eq. (8). Then, we calculate the difference of the predicted latent features be$\\begin{array}{r}{\\mathbb{E}_{t,\\mathbf{x}}[\\frac{1-\\dot{\\overline{{\\alpha}}}_{t}}{\\bar{\\alpha}_{t}}\\|\\hat{\\epsilon}_{\\theta_{t\\mathrm{p}}}(t,\\mathbf{z}_{t},\\mathbf{c})\\,-\\,\\hat{\\epsilon}_{\\theta_{\\mathrm{int}},\\mathbf{s}}(t,\\bar{\\mathbf{z}}_{t},\\mathbf{c})\\|^{2}]}\\end{array}$ , where $t\\in[0,1,\\cdot\\cdot\\cdot\\,,999]$ $\\bar{\\alpha}_{t}$   \nderivation in App. E). The evaluation is conducted on a dataset with 128 image-text pairs. Fig. 4 shows the quantization error does not distribute equally across all time steps. Notably, the quantization error keeps increasing as the time steps approach $t=999$ . ", "page_idx": 6}, {"type": "image", "img_path": "0m19blQT6y/tmp/1916a348ff7d101296d80946503ac4a5bb5f5566570089ab3e7f7fe9c2de227c.jpg", "img_caption": ["Figure 4: More time steps are sampled towards where larger quantization error occurs. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "To mitigate the quantization error prevalent near the time steps $t=999$ , we propose a sampling strategy by utilizing a distribution specifically tailored to sample more time steps exhibiting the largest quantization errors, thereby enhancing performance. To achieve this goal, we leverage the Beta distribution. Specifically, time steps are sampled according to $t\\sim B e t a(\\alpha,\\beta)$ , as shown in Fig. 4. We empirically set $\\alpha=3.0$ and $\\beta=1.0$ for the best performance. Combining the strategy of time steps sampling with Eq. (8), we conduct the Stage-I training. ", "page_idx": 6}, {"type": "text", "text": "Fine-tuning with Noise Prediction. After getting the model trained with the distillation loss in Stage-I, we then fine-tune it with noise prediction, as in Eq. (2), in Stage-II. We apply a text dropping with probability as $10\\%$ and modify the distribution of time step sampling based on the quantization error, as introduced above. The reason we leverage two-stage fine-tuning, instead of combining Stage-I and Stage-II, is that we observe more stabilized training results. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Implementation Details. We develop our code using diffusers library2 and train the models with AdamW optimizer [33] and a constant learning rate as 1e\u221205 on an internal dataset. For Stage-I, ", "page_idx": 6}, {"type": "image", "img_path": "0m19blQT6y/tmp/f9ac6a02ffe9ebcc9d8616e8c4b162219846b06ff9088ba7ef7fd9a580310bcf.jpg", "img_caption": ["Figure 5: Comparison between our 1.99-bits model vs. SD-v1.5 on various evaluation metrics with CFG scales ranging from 2.5 to 9.5. Ours-I denotes the model with Stage-I training and Ours-II denotes the model with Stage-II training. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Comparison with existing quantization methods, including LSQ [11], QDiffusion [38], EfficientDM [17], and Apple-MBP [62]. The CLIP score is measured on 1K PartiPrompts ", "page_idx": 7}, {"type": "text", "text": "Table 4: Analysis of our proposed methods measured under various CFG scales, i.e., 3.5, 5.5, 7.5, and 9.5. We use LSQ [11] as the basic QAT method, which involves the training of weights and scaling factors of a uniformly 2-bit quantized UNet. Then, we gradually introduce each proposed technique to evaluate their effectiveness. CLIP scores are measured on 1K PartiPrompts. ", "page_idx": 7}, {"type": "table", "img_path": "0m19blQT6y/tmp/99b3181026198f9a6ce39d5c3b632615fe8f885e9c50149989bd6aa1e78f40e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "0m19blQT6y/tmp/8dabb58d43833d842e7ce42ed1285ca1e301c06200d95e598dee5c98b04c1941.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "we use 8 NVIDIA A100 GPUs with a total batch size of 256 to train the quantized model for 20K iterations. For Stage-II, we use 32 NVIDIA A100 GPUs with a total batch size of 1024 to train the quantized model for 50K iterations. During inference, we adopt the PNDM scheduler [49] with 50 sampling steps to generate images for comparison. Other sampling approaches (e.g., DDIM [78] and DPMSolver [55]) lead to the same conclusion (App. K). ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. We conduct evaluation on CLIP Score and FID on MS-COCO [47], TIFA [26], GenEval [14], and human evaluation on PartiPrompts [86]. We adopt ViT-B/32 model [10] in CLIP score and the Mask2Former(Swin- $.\\mathsf{S}\\mathrm{-}8\\!\\times\\!2.$ ) [5] in GenEval. App. I provides details for the metrics. ", "page_idx": 7}, {"type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison with SD-v1.5. Our quantized 1.99-bits UNet consistently outperforms the full-precision model across all metrics. ", "page_idx": 7}, {"type": "text", "text": "\u2022 30K MS-COCO 2014 Validation Set. For the CLIP score, as demonstrated in Fig. 5a, attributed to the proposed mixed-precision recipe with the introduced initialization techniques and advanced training schemes in Stage-I, our 1.99-bits UNet, with a storage size of 219MB, achieves performance comparable to the original SD-v1.5. Following Stage-II training, our model surpasses the performance of the original SD-v1.5. With CFG scales ranging from 2.5 to 9.5, our model yields $0.002\\sim0.003$ higher CLIP scores.   \n\u2022 TIFA. As shown in Fig. 5b, our 1.99-bits model with Stage-I training performs comparably to the SD-v1.5. With the Stage-II training, our model achieves better metrics over the SD-v1.5.   \n\u2022 GenEval. We show the comparison results for GenEval in Fig. 5c (detailed comparisons of GenEval score are presented in Appn. L). Our model outperforms SD-v1.5 for all CFG scales.   \n\u2022 Human Evaluation. With the question: Given a prompt, which image has better aesthetics and image-text alignment? More users prefer the images generated by our quantized model over SDv1.5, with the ratio as $54.4\\%$ . The results are shown in Fig. 6. We provide a detailed comparison in App. J. ", "page_idx": 7}, {"type": "text", "text": "Comparison with Other Quantization Approaches. Additionally, we conduct the experiments by comparing our approach with other works including LSQ [11], Q-Diffusion [38], EfficientDM [17], and Apple-MBP [62], as shown in Tab. 3. Our model achieves a higher CLIP score compared with all other works and better performance than SD-v1.5. ", "page_idx": 7}, {"type": "table", "img_path": "0m19blQT6y/tmp/7c85ed32fa16120952601ee99dffd2df8f6d7b7101625aab3bdb66282e21ae07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here we perform extensive analysis for our proposed method. We mainly evaluate different experimental settings using the CLIP score measured on 1K PartiPrompts [86]. ", "page_idx": 8}, {"type": "text", "text": "Analysis of the Proposed Techniques. We adopt the LSQ [11] as the basic QAT method to update the weights and scaling factors of a uniform 2-bit UNet with Min-Max initialization. Results are presented in Tab. 4 with the following details: ", "page_idx": 8}, {"type": "text", "text": "\u2022 +Balance. By adding a balance integer, a 2-bit model that typically represents 4 integer values can now represent 5 integers, becoming a 2.32-bit model by ${\\tt l o g}(4+1)$ bits. The average CLIP score has significantly increased from 0.2797 to 0.3054.   \n\u2022 +Alternating Opt. By further utilizing the scaling factor initialization via alternating optimization, the average CLIP score of the 2.32-bit model increases to 0.3100.   \n\u2022 +Mixed/Caching. By leveraging time embedding pre-computing and caching, we minimize the storage requirements for time embedding and projection layers by only retaining the calculated features. This significantly reduces the averaged bits. Combined with our mixed-precision strategy, this approach reduces the average bits from 2.32 to 1.99 bits and can even improve the performance, i.e., CLIP score improved from 0.3100 to 0.3118.   \n\u2022 +Feat Dist. By incorporating the feature distillation loss, i.e., Eq. (7), the model can learn more fine-grained information from the teacher model, improving CLIP score from 0.3118 to 0.3142.   \n\u2022 $^{+}$ Time Sampling. By employing a quantization error-aware sampling strategy at various time steps, the model focuses more on the time step near $t=999$ . With this sampling strategy, our 1.99-bits model performs very closely to, or even outperforms, the original SD-v1.5.   \n\u2022 +Fine-tuning. By continuing with Stage-II training that incorporates noise prediction, our 1.99- bits model consistently outperforms the SD-v1.5 across various guidance scales, improving the CLIP score to 0.3183. ", "page_idx": 8}, {"type": "text", "text": "Effect of $\\eta$ in Mixed-Precision Strategy. Tab. 5 illustrates the impact of the parameter size factor $\\eta$ (as discussed in Sec. 3.3) in determining the optimal mixed precision strategy. We generate six different mixed precision recipes with different $\\eta$ with 20K training iterations for comparisons. Initially, we explore the mixed precision strategy determined with and without the parameter size factor. Setting $\\eta=0$ results in $\\bar{N}^{-\\eta}=1$ , indicating that the mixed precision is determined without considering the impact of parameter size. The results show that neglecting the parameter size significantly degrades performance. Further, we empirically choose $\\eta=0.3$ in our experiments after comparing different values of $\\eta$ . ", "page_idx": 8}, {"type": "text", "text": "Effect of $\\lambda$ of Distillation Loss. Tab. 6 illustrates the impact of the balance factor $\\lambda$ for loss functions in Eq. (8). We empirically choose $\\lambda=0.01$ in our experiments after comparing the performance. ", "page_idx": 8}, {"type": "text", "text": "Effect of $\\alpha$ in Time Step-aware Sampling Strategy. Tab. 7 illustrates the impact of the $\\alpha$ for different Beta sampling distribution. As analyzed in Sec. 4.2, the quantization error increases near $t=999$ . To increase sampling probability near this time step, Beta distribution requires $\\alpha>1$ with $\\beta\\,=\\,1$ . A larger $\\alpha$ enhances the sampling probability near $t\\,=\\,999$ . Compared to $\\alpha=1.5$ and $\\alpha=2.0$ , $\\alpha=3.0$ concentrates more on later time steps and achieves the best performance. We choose $\\alpha=3.0$ in our experiments. ", "page_idx": 8}, {"type": "text", "text": "Analysis for Different Schedulers. One advantage of our training-based quantization approach is that our quantized model consistently outperforms SD-v1.5 for various sampling approaches. We conduct extensive evaluations on TIFA to show we achieve better performance than SD-v1.5 for using both DDIM [78] and DPMSolver [55] to perform the sampling. More details are shown in App. K. ", "page_idx": 8}, {"type": "text", "text": "FID Results. As stated in SDXL [63] and PickScore [35], FID may not honestly reflect the actual performance of the model in practice. FID measures the average distance between generated images and reference real images, which is largely influenced by the training datasets. Also, FID does not capture the human preference which is the crucial metric for evaluating text-to-image synthesis. We present FID results evaluated on the 30K MS-COCO 2014 validation set in Fig. 7. Our Stage-I model has a similar FID as SD-v1.5. However, as training progresses, although our Stage-II model is preferred by users, its FID score is higher than both Stage-I and SD-v1.5. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "0m19blQT6y/tmp/0226ecda88556a93c934c62cefb415789d0d741abcb145cff12cfdb4bd285eaf.jpg", "img_caption": ["Figure 6: Overall human evaluation comparisons between SD-v1.5 and BitsFusion. Notably, BitsFusion, is favored $54.41\\%$ of the time over SD-v1.5. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "0m19blQT6y/tmp/da90ab8774a6322736d11aa6522df58f059c9a9ab14bcfd2aa5908ea733d21c1.jpg", "img_caption": ["Figure 7: FID results evaluated on 30K MS-COCO 2014 validation set. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To enhance the storage efficiency of the large-scale diffusion models, we introduce an advanced weight quantization framework, BitsFusion, which effectively compresses the weights of UNet from SD-v1.5 to 1.99 bits, achieving a $7.9\\times$ smaller model size. BitsFusion even outperforms SD-v1.5 in terms of generation quality. Specifically, we first conduct a comprehensive analysis to understand the impact of each layer during quantization and establish a mixed-precision strategy. Second, we propose a series of effective techniques to initialize the quantized model. Third, during the training stage, we enforce the quantized model to learn the full-precision SD-v1.5 by using distillation losses with the adjusted distribution of time step sampling. Finally, we fine-tune the previous quantized model through vanilla noise prediction. Our extensive evaluations on TIFA, GenEval, CLIP score, and human evaluation consistently demonstrate the advantage of BitsFusion over full-precision SD-v1.5. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 1 ", "page_idx": 9}, {"type": "text", "text": "[2] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n3 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563\u201322575, 2023. 1 [4] Hanwen Chang, Haihao Shen, Yiyang Cai, Xinyu Ye, Zhenzhong Xu, Wenhua Cheng, Kaokao Lv, Weiwei Zhang, Yintong Lu, and Heng Guo. Effective quantization for diffusion models on cpus. arXiv preprint arXiv:2311.16133, 2023. 2 [5] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290\u20131299, 2022. 8,   \n30   \n[6] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043\u20133054, 2023. 30   \n[7] Huanpeng Chu, Wei Wu, Chengjie Zang, and Kun Yuan. Qncd: Quantization noise correction for diffusion models. arXiv preprint arXiv:2403.19140, 2024. 2   \n[8] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 1\u20134, 2009. 4   \n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021. 3   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 8   \n[11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. In International Conference on Learning Representations, 2019. 2, 7, 8, 9   \n[12] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In Low-Power Computer Vision, pages 291\u2013326. Chapman and Hall/CRC, 2022. 2   \n[13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 2   \n[14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 8, 30   \n[15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing textto-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 1   \n[16] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 1   \n[17] Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 6, 8   \n[18] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2   \n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 2   \n[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1   \n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020. 1, 2, 3, 7, 20   \n[22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3   \n[23] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 2366\u20132369. IEEE, 2010. 4   \n[24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 3   \n[25] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023. 2   \n[26] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406\u201320417, 2023. 8, 30   \n[27] Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal feature maintenance quantization for diffusion models. arXiv preprint arXiv:2311.16503, 2023. 2, 3, 5, 18   \n[28] Kyuyeon Hwang and Wonyong Sung. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In 2014 IEEE Workshop on Signal Processing Systems (SiPS), pages 1\u20136. IEEE, 2014. 6   \n[29] Yerlan Idelbayev, Pavlo Molchanov, Maying Shen, Hongxu Yin, Miguel A Carreira-Perpin\u00e1n, and Jose M Alvarez. Optimal quantization using scaled codebook. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12095\u201312104, 2021. 6   \n[30] Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, and Sergey Tulyakov. F8net: Fixed-point 8-bit only multiplication for network quantization. arXiv preprint arXiv:2202.05239, 2022. 2   \n[31] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022. 1   \n[32] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. 2, 7   \n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7   \n[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3   \n[35] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 9   \n[36] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network quantization with element-wise gradient scaling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6448\u20136457, 2021. 2   \n[37] Fengfu Li, Bin Liu, Xiaoxing Wang, Bo Zhang, and Junchi Yan. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016. 6   \n[38] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17535\u201317545, 2023. 2, 3, 6, 8   \n[39] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6   \n[40] Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Textcraftor: Your text encoder can be image quality controller. arXiv preprint arXiv:2403.18978, 2024. 1   \n[41] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. 2, 3   \n[42] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. 2   \n[43] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. 2   \n[44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv preprint arXiv:2211.10440, 2022. 1   \n[45] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87\u2013100, 2024. 2   \n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014. 2   \n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014. 8, 30   \n[48] Jing Liu, Bohan Zhuang, Peng Chen, Chunhua Shen, Jianfei Cai, and Mingkui Tan. Single-path bit sharing for automatic loss-aware model compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):12459\u201312473, 2023. 2   \n[49] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 1, 8   \n[50] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generation with latent structural diffusion. arXiv preprint arXiv:2310.08579, 2023. 1   \n[51] Xuewen Liu, Zhikai Li, Junrui Xiao, and Qingyi Gu. Enhanced distribution alignment for post-training quantization of diffusion models. arXiv preprint arXiv:2401.04585, 2024. 2   \n[52] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. 6   \n[53] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant\u2013llm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. 2   \n[54] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137, 1982. 6 [55] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022. 8, 9, 31, 32 [56] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 2024. 6 [57] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024.   \n1 [58] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197\u20137206. PMLR, 2020. 2 [59] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325\u20131334, 2019. 2 [60] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. 2 [61] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1 [62] Atila Orhon, Michael Siracusa, and Aseem Wadhwa. Stable diffusion with core ml on apple silicon, 2022. 8, 9 [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2023. 9 [64] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using   \n2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 1 [65] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, HsinYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 1 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 2, 4, 18 [67] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [68] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021. 1 [69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014. 3 [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. 2, 3 [71] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH   \n2022 Conference Proceedings, pages 1\u201310, 2022. 1 [72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022. 30 [73] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1972\u20131981, 2023. 2, 6 [74] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2023. 1 [75] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1 [76] Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models. Advances in Neural Information Processing Systems,   \n36, 2024. 2, 3 [77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015. 1, 3 [78] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 8, 9, 31, 32 [79] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1 [80] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 7 [81] Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu. Post-training quantization with progressive calibration and activation relaxing for text-to-image diffusion models. arXiv preprint arXiv:2311.06322, 2023. 2 [82] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate data-free quantization for diffusion models. arXiv preprint arXiv:2305.18723, 2023. 2 [83] Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan. Quest: Low-bit diffusion model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666,   \n2024. 2 [84] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023. 2 [85] Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, and Hongbo Zhang. Efficient quantization strategies for latent diffusion models. arXiv preprint arXiv:2312.05431, 2023. 2 [86] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research,   \n2022. 2, 8, 9, 30 [87] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978, 2022. 1   \n[88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023. 1   \n[89] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 4   \n[90] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N Metaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6027\u20136037, 2023. 1   \n[91] Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. arXiv preprint arXiv:2405.17873, 2024. 2   \n[92] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-toimage generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. 2   \n[93] Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, and Xianglong Liu. Binarydm: Towards accurate binarization of diffusion model. arXiv preprint arXiv:2404.05662, 2024. 2   \n[94] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. In International Conference on Learning Representations, 2016. 5 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Limitations 18   \nB More details for Mixed-Precision Algorithm 18   \nC More Details for Time Embedding Pre-computing and Caching 18   \nD Analysis of Symmetric Weight Distribution 20   \nE More Details for Quantization Error Across Different Time Steps 20   \nF Detailed Metrics for Quantization Error by Quantizing Different Layers 21   \nG More Visualization for Quantization Error by Quantizing Different Layers 25   \nH 1.99 Bits Mixed Precision Recipe 26   \nI Details for Evaluation Metrics 30   \nJ Human Evaluation 30   \nJ.1 Analysis on Categories 30   \nJ.2 Analysis on Challenges 30   \nK Evaluation on Different Schedulers 31   \nL Detailed GenEval Results 32   \nM More Comparisons 33   \nM.1 Prompts . . 33   \nM.2 Additional Image Comparisons 33 ", "page_idx": 16}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this work, we study the storage size reduction of the UNet in Stable Diffusion v1.5 through weight quantization. The compression of VAE and CLIP text encoder [66] is also an interesting direction, which is not explored in this work. Additionally, our weight quantization techniques could be extended to the activations quantization, as a future exploration. ", "page_idx": 17}, {"type": "text", "text": "B More details for Mixed-Precision Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Sec. 3, we analyze the per-layer quantization error and develop the mixed-precision strategy. Here, we provide the detailed algorithm as outlined in Alg. 1. The inputs include: a pre-defined candidate set of bit-width $b\\in\\{1,2,\\mathit{\\bar{3}}\\}$ , the full-precision $\\mathrm{SD-v}1.5\\;D$ , the total number of layers $L$ (except for the time embedding, time projection, the first and last convolutional layers), the training dataset $X$ , the number of training iterations $T$ , the number of evaluation images for calculating metrics $K$ , the bit threshold $\\scriptstyle{S_{o}}$ , the parameter size factor $\\eta$ , and the number of parameters of the $i_{t h}$ layer $N_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "In the first stage, we aim to obtain quantized models by quantizing each individual layer. Given the full-precision SD-v1.5 UNet $D$ , we consecutively perform the quantization on every single layer to 1, 2, or 3 bits individually, while maintaining the remaining layers at FP32 format. Notice, to align with our experiments, we add the balance integer and initialize the scaling factor with our alternating optimization. For each quantized model, the weights and scaling factors are fine-tuned using quantization-aware training to minimize the quantization error by learning the predicted noise of the SD-v1.5. We obtain quantized models $D_{i,b},i=1,2,\\cdots,L,b=1,2,3$ . ", "page_idx": 17}, {"type": "text", "text": "In the second stage, we measure the quantization error of each layer by calculating various metrics from comparing images generated by the quantized model $D_{i,b}$ with those from the unquantized SD-v1.5 $D$ . Specifically, we generate $K=100$ baseline images $I_{d}$ from the full-precision SD-v1.5 model with PartiPrompts. Then, for each quantized model $D_{i,b}$ , we use identical prompts and seed to generate corresponding images $I_{i,b}$ . We calculate the quantization error by measuring the metrics including MSE, CLIP score, PSNR, and LPIPS using these images and prompts. ", "page_idx": 17}, {"type": "text", "text": "In the third stage, we collect the mixed-precision recipe. We first compute a sensitivity score for each layer, factoring in both the MSE and the parameter size adjusted by $\\eta$ . For the $i_{t h}$ layer, its sensitivity score for the $b$ -bits $(b\\in\\{1,2,3\\}$ ) is defined as $S_{i,b}=M_{i,b}N_{i}^{-\\eta}$ , where $M$ denotes the MSE error, $N$ is the total number of parameters of the layer, and $\\eta\\in[0,1]$ denotes the parameter size factor. To determine the bit width $(i.e.,\\,b^{*})$ for each layer, we define a sensitivity threshold as $\\scriptstyle{S_{o}}$ , and the $i_{t h}$ layer is assigned to $b_{i}^{*}$ -bits, where $b_{i}^{*}=\\operatorname*{min}\\{b|S_{i,b}<S_{o}\\}$ . The remaining layers are set as 4 bits if they fail to meet the threshold. After determining the initial bits based on the MSE error, we refine this recipe by considering the degradation in the CLIP score associated with each bit-width. We simply consider the CLIP score change at 3 bits. We assign layers with the highest $10\\%$ , $5\\%$ , $2\\%$ CLIP score drop with 1, 2, 3 more bits, respectively. ", "page_idx": 17}, {"type": "text", "text": "The final output is a mixed-precision recipe $\\{b_{i}^{*}\\},i=1,2,\\cdots\\,,L,$ , specifying the bit-width for each layer. Then, we set the first and last convolutional layers as 8 bits and pre-computing and caching the time embedding and projection layers. ", "page_idx": 17}, {"type": "text", "text": "C More Details for Time Embedding Pre-computing and Caching ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Sec. 4.1, we introduce \"Time Embedding Pre-computing and Caching\". Here, we provide more details for the algorithm. In the Stable Diffusion model, the time step $t\\in[0,1,\\cdot\\cdot\\cdot\\,,999]$ is transformed into a time embedding $\\mathtt{e m b}_{t}$ through the equation $\\mathsf{e m b}_{t}=e(t)$ , where $e(t)$ denotes the time embedding layer and $\\mathbf{emb}_{t}\\in\\mathbb{R}^{d_{t e}}$ . In SD-v1.5, $d_{t e}=1280$ . Then, for each ResBlock, denoted as $R_{i}$ for $i=1,2,\\cdots\\,,N_{r}$ , where $N_{r}$ is total number of ResBlocks with time projection layers, the $\\mathtt{e m b}_{t}$ is encoded by time projection layers $r_{i}(\\cdot)$ by $\\mathbf{F}_{i,t}=r_{i}\\big(\\mathbf{em}\\mathbf{b}_{t}\\big)$ . Notice that $r_{i}(\\cdot)$ and $e(\\cdot)$ are both linear layers. Finally, $\\mathsf{F}_{i,t}$ is applied to the intermediate activations of each $R_{i}$ via addition operation, effectively incorporating temporal information into the Stable Diffusion model. ", "page_idx": 17}, {"type": "text", "text": "As observed before [27], time embedding and projection layers exhibit considerable sensitivity to quantization during PTQ on DM. To address this problem, existing work specifically pays attention to reconstructing layers related to time embedding [27]. In this study, we propose a more effective ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Mixed-Precision Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Input: Candidate bits set $b\\in\\{1,2,3\\}$ , SD-v1.5 model $D$ , number of total layers $L$ (except for the time   \nembedding, time projection, the first and last convolutional layers), dataset $X$ , training iterations $T$ , number of   \nevaluation images $K$ , threshold $S_{o}$ , parameter size factor $\\eta$ , number of parameters of the $i_{t h}$ layer $N_{i}$ .   \nOutput: Mixed precision recipe $\\{b_{i}^{\\ast}\\},i=1,2,\\cdots,L$ .   \n1: 1: Obtaining the quantized models.   \n2: for $b=1$ to 3 do   \n3: for $i=1$ to $L$ do   \n4: Quantize the $i$ -th layer to $b$ bits via Eq. (1) and proposed initialization methods in Sec. 4.1 to get model   \n$D_{i,b}$ ;   \n5: for $t=1$ to $T$ do   \n6: Updating weights and scaling factors by minimizing the quantization error using quantization-aware   \ntraining on $D_{i,b}$ with data $X$ ;   \n7: end for   \n8: end for   \n9: end for   \n10: 2: Calculating quantization error metrics.   \n11: Generating $K$ images $I_{d}$ via SD-v1.5;   \n12: for $b=1$ to 3 do   \n13: for $i=1$ to $L$ do   \n14: Generating $K$ images $I_{i,b}$ via quantized model $D_{i,b}$ ;   \n15: Calculating MSE, $M_{i,b}$ via images $I_{i,b}$ and $I_{d}$ ;   \n16: Calculating PSNR, $P_{i,b}$ via images $I_{i,b}$ and $I_{d}$ ;   \n17: Calculating LPIPS, $L_{i,b}$ via images $I_{i,b}$ and $I_{d}$ ;   \n18: Calculating CLIP score drop, $C_{i,b}$ via images $I_{i,b}$ and prompts;   \n19: end for   \n20: end for   \n21: 2: Deciding the optimal precision.   \n22: Calculating sensitivity score $S_{i,b}=M_{i,b}N_{i}^{-\\eta}$ ;   \n23: for $i=1$ to $L$ do   \n24: $b_{i}^{*}\\gets4$ ;   \n25: for $b=3$ to 1 do   \n26: if $S_{i,b}<S_{o}$ then   \n27: Assign the $i$ -th layer with $^b$ bits with $b_{i}^{*}\\gets b$ ;   \n28: end if   \n29: end for   \n30: end for   \n31: Calculating CLIP score drop, $C_{i,3}$ and its $p_{t h}$ percentile $C_{p}$ ;   \n32: for $i=1$ to $L$ do   \n33: if $C_{i,3}>C_{90}$ then   \n34: $b_{i}^{*}\\gets b_{i}^{*}+1$ ;   \n35: end if   \n36: if $C_{i,3}>C_{95}$ then   \n37: $b_{i}^{*}\\gets b_{i}^{*}+1$ ;   \n38: end if   \n39: if $C_{i,3}>C_{98}$ then   \n40: $b_{i}^{*}\\gets b_{i}^{*}+1$ ;   \n41: end if   \n42: end for ", "page_idx": 18}, {"type": "text", "text": "method. We observe that 1) during the inference stage, for each time step $t$ , the $\\mathtt{e m b}_{t}$ and consequently $\\mathsf{F}_{i,t}$ remain constant. 2) In the Stable Diffusion model, the shape of $\\mathtt{F}_{i,t}$ are considerably smaller compared to time embedding and projection layers. Specifically, in SD-v1.5, $\\mathsf{F}_{i,t}$ is with the dimension in $\\{320,640,1280\\}$ which is largely smaller than time projection layers $\\dot{W_{r}}\\,\\in\\,\\mathbb{R}^{D\\times1280}$ , where $D\\,\\in\\,\\{320,640,1280\\}$ . Therefore, we introduce an efficient and lossless method named Time Embedding Pre-computing and Caching. Specifically, for total $T_{\\mathrm{inf}}$ inference time steps, we opt to store only $T_{\\mathrm{inf}}$ time features, rather than retaining the original time embedding layers $e(\\cdot)$ and the time projection layers in the $i$ -th ResBlock $r_{i}(\\cdot)$ . ", "page_idx": 18}, {"type": "text", "text": "The inference time steps are set as 50 or less in most Stable Diffusion models. This method significantly reduces more than $1280/50=25.6\\times$ storage requirements and entire computational costs in terms of time-related layers. Given that the storage size of the pre-computed $\\mathtt{F}_{i,t}$ is substantially smaller than that of the original linear layers, this approach effectively diminishes the average bit of our quantized model without any performance degradation. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D Analysis of Symmetric Weight Distribution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Sec. 4.1, we introduce \"Adding Balance Integer\" by assuming the weight distribution in Stable Diffusion is symmetric. Here, we provide more analysis for the assumption. To verify the weight distribution is symmetric around zero in SD-v1.5, we measure the skewness of the weight distribution of each layer. Lower skewness indicates a more symmetric weight distribution. As illustrated in Fig. 8, $97\\%$ of layers exhibiting skewness between [-0.5, 0.5], this suggests that most layers in SD-v1.5 have symmetric weight distributions. ", "page_idx": 19}, {"type": "image", "img_path": "0m19blQT6y/tmp/0c2a902cfe03f6c5dd63c44095570d8e29875563c3c706e5a4b5f20a550725f4.jpg", "img_caption": ["Figure 8: Skewness of weight distribution of each layer in SD-v1.5. Lower skewness represents the weight distribution is more symmetric. $97\\%$ layers are with skewness between [-0.5, 0.5], indicating that most layers have symmetric weight distribution in SD-v1.5. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E More Details for Quantization Error Across Different Time Steps ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Sec. 4.2, we introduce the \"Quantization Error-aware Time Step Sampling\" method. Here, we provide more details for measuring the quantization error from the predicted latent instead of the predicted noise. During the inference stage, the actual denoising step requires the scaling operation on the predicted noise in diffusion models. Therefore, directly calculating the quantization error via noise prediction is not accurate. Instead, we calculate the quantization error in the latent feature space. We derive the relationship of quantization error calculated from the predicted latent and noise as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E=\\mathbb{E}_{t,\\mathbf{x}}\\left[\\Vert\\hat{\\mathbf{z}}_{{\\theta}_{t}}(t,\\mathbf{z}_{t},\\mathbf{c})-\\hat{\\mathbf{z}}_{{\\theta}_{\\mathrm{int},\\mathbf{s}}}(t,\\mathbf{z}_{t},\\mathbf{c})\\Vert^{2}\\right],}\\\\ &{\\quad=\\mathbb{E}_{t,\\mathbf{x}}\\left[\\left\\Vert\\left(\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\mathbf{z}_{t}-\\frac{\\sqrt{1-\\bar{\\alpha}_{t}}}{\\sqrt{\\bar{\\alpha}_{t}}}\\hat{\\epsilon}_{{\\theta}_{t}_{\\mathrm{p}}}(t,\\mathbf{z}_{t},\\mathbf{c})\\right)-\\left(\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\mathbf{z}_{t}-\\frac{\\sqrt{1-\\bar{\\alpha}_{t}}}{\\sqrt{\\bar{\\alpha}_{t}}}\\hat{\\epsilon}_{{\\theta}_{\\mathrm{int},\\mathbf{s}}}(t,\\mathbf{z}_{t},\\mathbf{c})\\right)\\right\\Vert^{2}\\right],}\\\\ &{\\quad=\\mathbb{E}_{t,\\mathbf{x}}\\left[\\frac{1-\\bar{\\alpha}_{t}}{\\bar{\\alpha}_{t}}\\left\\Vert\\hat{\\epsilon}_{{\\theta}_{t}_{\\mathrm{p}}}(t,\\mathbf{z}_{t},\\mathbf{c})-\\hat{\\epsilon}_{{\\theta}_{\\mathrm{int},\\mathbf{s}}}(t,\\mathbf{z}_{t},\\mathbf{c})\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "F Detailed Metrics for Quantization Error by Quantizing Different Layers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Sec. 3.2, we calculate the various metrics for representing the quantization error when quantizing different layers. Here, we provide detailed metrics when quantizing each layer of SD-v1.5 to 1, 2, and 3 bits. ", "page_idx": 20}, {"type": "image", "img_path": "0m19blQT6y/tmp/ea5a464877316ea39eeae79dd9d1cdd90aa2fd134c661713d019663a793e6f0d.jpg", "img_caption": ["(c) MSE value caused by the 3-bit quantized layers in SD-v1.5. Figure 9: MSE value caused by the quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "0m19blQT6y/tmp/92e014a93bb2b35c7e52bf76b14148839ee5a0689436000e0ec50464dda1c46c.jpg", "img_caption": ["(a) CLIP score degradation caused by the 1-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "0m19blQT6y/tmp/cc7af0f3eada71354bd21920971c3c9102e8d600ec7c1da281c74c483917b1e0.jpg", "img_caption": ["(b) CLIP score degradation caused by the 2-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "0m19blQT6y/tmp/56d982cdc74c4926c770e2e3956c1ff6c16b8a62fb0827e3b0aee05680f995bb.jpg", "img_caption": ["(c) CLIP score degradation caused by the 3-bit quantized layers in SD-v1.5. Figure 10: CLIP score degradation caused by quantized layers in SD-v1.5 "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "0m19blQT6y/tmp/99d53b800581f99a50db34ec90b9adae06d934dffc9416fcbe9b5c8a30008060.jpg", "img_caption": ["(a) LPIPS value of the 1-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "0m19blQT6y/tmp/84dea6c1f37d82699c33f2c7ed6691244f019f99ca50cd371a0b5fb624eb708d.jpg", "img_caption": ["(b) LPIPS value of the 2-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "0m19blQT6y/tmp/18417142a4e1a82752230f248c2caff1150f73639d9626c985869cbf157ef0e0.jpg", "img_caption": ["(c) LPIPS value of the 3-bit quantized layers in SD-v1.5. Figure 11: LPIPS value of quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "0m19blQT6y/tmp/72fbd70ec1ade1e906d0fcd13e3a40dc623583239dc394a03e1159f30dedf09e.jpg", "img_caption": ["(a) PSNR value of the 1-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "0m19blQT6y/tmp/95d50adf7c6c59873280f99e545a1da1ed831e92a69dac2872f07fa5cb039123.jpg", "img_caption": ["(b) PSNR value of the 2-bit quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "0m19blQT6y/tmp/aecc43dee6c341a3914434e8b26f25e46438fa0b5aaa210d31df87bc3b4d7cb6.jpg", "img_caption": ["(c) PSNR value of the 3-bit quantized layers in SD-v1.5. Figure 12: PSNR value of quantized layers in SD-v1.5. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "G More Visualization for Quantization Error by Quantizing Different Layers ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Sec. 3.2, we show the images for representing the quantization error when quantizing different layers. Here, we provide more visualization for demonstrating the different quantization errors caused by quantizing different layers to 1 bit. The quantized layers from left to right correspond to the annotated layers at the bottom: SD-v1.5 w/o quantization, Down.0.0.attn2.toq, Down.0.0.attn2.tok, Down.0.0.attn2.tov, Down.1.0.attn2.tok, Down.1.1.attn2.tok, Down.2.res.0.conv1, Up.2.res.2.convshortcut, Up.3.2.attn2.tok, Up.3.res.2.convshortcut. ", "page_idx": 24}, {"type": "image", "img_path": "0m19blQT6y/tmp/4018cae39f724faa4dc17a40440eb8a469fbcb63ecd5392cb9d4d3c1f07c2cfe.jpg", "img_caption": ["Figure 13: Quantization errors demonstrated in generated images (via PartiPrompts) after performing 1-bit quantization on different individual layers. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H 1.99 Bits Mixed Precision Recipe ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide our 1.99 bits recipe in our experiments. During the training and inference stage, we add a balancing integer to the $n$ -bits values, resulting in $\\log(2^{n}+1)$ bits. We calculate the average bits by i log(2bi +N1)w\u00d7Ni+16\u2217Ntf, where bi\u2217 is the calculated bit-width in the ith layer, Ni is the number of weights of the $i_{t h}$ layer, $N_{t f}$ is the number of parameters for pre-cached time features, and $N_{w}$ is the total number of weights in linear and convolutional layers. We calculate the model size by integrating all other parameters as 32 bits. The index and name of each layer are listed: ", "page_idx": 25}, {"type": "text", "text": "1 down_blocks.0.attentions.0.proj_in: 6   \n2 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q: 5   \n3 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k: 5   \n4 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v: 4   \n5 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0: 6   \n6 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q: 5   \n7 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k: 7   \n8 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v: 3   \n9 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0: 3   \n10 down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj: 3   \n11 down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2: 3   \n12 down_blocks.0.attentions.0.proj_out: 5   \n13 down_blocks.0.attentions.1.proj_in: 4   \n14 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q: 3   \n15 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k: 4   \n16 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v: 6   \n17 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0: 5   \n18 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q: 5   \n19 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k: 7   \n20 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v: 2   \n21 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0: 3   \n22 down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj: 3   \n23 down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2: 3   \n24 down_blocks.0.attentions.1.proj_out: 6   \n25 down_blocks.0.resnets.0.conv1: 3   \n26 down_blocks.0.resnets.0.conv2: 3   \n27 down_blocks.0.resnets.1.conv1: 3   \n28 down_blocks.0.resnets.1.conv2: 4   \n29 down_blocks.0.downsamplers.0.conv: 4   \n30 down_blocks.1.attentions.0.proj_in: 4   \n31 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: 3   \n32 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: 3   \n33 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: 4   \n34 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: 4   \n35 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: 3   \n36 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: 5   \n37 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: 4   \n38 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: 3   \n39 down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: 2   \n40 down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: 2   \n41 down_blocks.1.attentions.0.proj_out: 4   \n42 down_blocks.1.attentions.1.proj_in: 4   \n43 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: 2   \n44 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: 2   \n45 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: 4   \n46 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: 4   \n47 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: 3   \n48 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: 6   \n49 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: 4   \n50 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: 3   \n51 down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: 2   \n52 down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: 2   \n53 down_blocks.1.attentions.1.proj_out: 4   \n54 down_blocks.1.resnets.0.conv1: 3   \n55 down_blocks.1.resnets.0.conv2: 3   \n56 down_blocks.1.resnets.0.conv_shortcut: 7   \n57 down_blocks.1.resnets.1.conv1: 3   \n58 down_blocks.1.resnets.1.conv2: 2   \n59 down_blocks.1.downsamplers.0.conv: 4   \n60 down_blocks.2.attentions.0.proj_in: 3   \n61 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: 3   \n62 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: 2   \n63 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: 3   \n64 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: 3   \n65 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: 3   \n66 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: 4   \n67 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: 4   \n68 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: 3   \n69 down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: 2   \n70 down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: 1   \n71 down_blocks.2.attentions.0.proj_out: 3   \n72 down_blocks.2.attentions.1.proj_in: 4   \n73 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: 4   \n74 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: 2   \n75 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: 3   \n76 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: 3   \n77 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: 3   \n78 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: 4   \n79 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: 4   \n80 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: 3   \n81 down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: 2   \n82 down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: 2   \n83 down_blocks.2.attentions.1.proj_out: 4   \n84 down_blocks.2.resnets.0.conv1: 3   \n85 down_blocks.2.resnets.0.conv2: 2   \n86 down_blocks.2.resnets.0.conv_shortcut: 4   \n87 down_blocks.2.resnets.1.conv1: 2   \n88 down_blocks.2.resnets.1.conv2: 1   \n89 down_blocks.2.downsamplers.0.conv: 1   \n90 down_blocks.3.resnets.0.conv1: 1   \n91 down_blocks.3.resnets.0.conv2: 1   \n92 down_blocks.3.resnets.1.conv1: 1   \n93 down_blocks.3.resnets.1.conv2: 1   \n94 up_blocks.0.resnets.0.conv1: 1   \n95 up_blocks.0.resnets.0.conv2: 2   \n96 up_blocks.0.resnets.0.conv_shortcut: 1   \n97 up_blocks.0.resnets.1.conv1: 1   \n98 up_blocks.0.resnets.1.conv2: 1   \n99 up_blocks.0.resnets.1.conv_shortcut: 1   \n100 up_blocks.0.resnets.2.conv1: 2   \n101 up_blocks.0.resnets.2.conv2: 1   \n102 up_blocks.0.resnets.2.conv_shortcut: 1   \n103 up_blocks.0.upsamplers.0.conv: 1   \n104 up_blocks.1.attentions.0.proj_in: 3   \n105 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: 2   \n106 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: 1   \n107 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: 2   \n108 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: 3   \n109 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: 3   \n110 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: 4   \n111 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: 4   \n112 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: 2   \n113 up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: 2   \n114 up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: 2   \n115 up_blocks.1.attentions.0.proj_out: 3   \n116 up_blocks.1.attentions.1.proj_in: 3   \n117 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: 2   \n118 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: 2   \n119 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: 2   \n120 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: 2   \n121 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: 2   \n122 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: 4   \n123 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: 3   \n124 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: 1   \n125 up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: 1   \n126 up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: 1   \n127 up_blocks.1.attentions.1.proj_out: 3   \n128 up_blocks.1.attentions.2.proj_in: 3   \n129 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q: 1   \n130 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k: 1   \n131 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v: 2   \n132 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0: 2   \n133 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q: 1   \n134 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k: 3   \n135 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v: 2   \n136 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0: 1   \n137 up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj: 1   \n138 up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2: 1   \n139 up_blocks.1.attentions.2.proj_out: 2   \n140 up_blocks.1.resnets.0.conv1: 1   \n141 up_blocks.1.resnets.0.conv2: 1   \n142 up_blocks.1.resnets.0.conv_shortcut: 3   \n143 up_blocks.1.resnets.1.conv1: 1   \n144 up_blocks.1.resnets.1.conv2: 1   \n145 up_blocks.1.resnets.1.conv_shortcut: 3   \n146 up_blocks.1.resnets.2.conv1: 1   \n147 up_blocks.1.resnets.2.conv2: 1   \n148 up_blocks.1.resnets.2.conv_shortcut: 3   \n149 up_blocks.1.upsamplers.0.conv: 2   \n150 up_blocks.2.attentions.0.proj_in: 4   \n151 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: 2   \n152 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: 2   \n153 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: 3   \n154 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: 3   \n155 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: 1   \n156 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: 2   \n157 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: 1   \n158 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: 1   \n159 up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: 1   \n160 up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: 1   \n161 up_blocks.2.attentions.0.proj_out: 3   \n162 up_blocks.2.attentions.1.proj_in: 4   \n163 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: 2   \n164 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: 3   \n165 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: 3   \n166 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: 3   \n167 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: 1   \n168 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: 3   \n169 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: 1   \n170 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: 1   \n171 up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: 1   \n172 up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: 1   \n173 up_blocks.2.attentions.1.proj_out: 3   \n174 up_blocks.2.attentions.2.proj_in: 4   \n175 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q: 2   \n176 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k: 2   \n177 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v: 2   \n178 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0: 3   \n179 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q: 2   \n180 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k: 3   \n181 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v: 1   \n182 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0: 1   \n183 up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj: 1   \n184 up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2: 1   \n185 up_blocks.2.attentions.2.proj_out: 3   \n186 up_blocks.2.resnets.0.conv1: 1   \n187 up_blocks.2.resnets.0.conv2: 2   \n188 up_blocks.2.resnets.0.conv_shortcut: 4   \n189 up_blocks.2.resnets.1.conv1: 1   \n190 up_blocks.2.resnets.1.conv2: 2   \n191 up_blocks.2.resnets.1.conv_shortcut: 4   \n192 up_blocks.2.resnets.2.conv1: 1   \n193 up_blocks.2.resnets.2.conv2: 1   \n194 up_blocks.2.resnets.2.conv_shortcut: 4   \n195 up_blocks.2.upsamplers.0.conv: 3   \n196 up_blocks.3.attentions.0.proj_in: 4   \n197 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q: 2   \n198 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k: 2   \n199 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v: 6   \n200 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0: 3   \n201 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q: 2   \n202 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k: 3   \n203 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v: 1   \n204 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0: 1   \n205 up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj: 1   \n206 up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2: 1   \n207 up_blocks.3.attentions.0.proj_out: 4   \n208 up_blocks.3.attentions.1.proj_in: 4   \n209 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q: 2   \n210 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k: 3   \n211 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v: 5   \n212 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0: 3   \n213 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q: 2   \n214 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k: 3   \n215 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v: 1   \n216 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0: 1   \n217 up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj: 2   \n218 up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2: 1   \n219 up_blocks.3.attentions.1.proj_out: 4   \n220 up_blocks.3.attentions.2.proj_in: 6   \n221 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q: 2   \n222 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k: 3   \n223 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v: 4   \n224 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0: 3   \n225 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q: 4   \n226 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k: 5   \n227 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v: 1   \n228 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0: 1   \n229 up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj: 3   \n230 up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2: 2   \n231 up_blocks.3.attentions.2.proj_out: 4   \n232 up_blocks.3.resnets.0.conv1: 1   \n233 up_blocks.3.resnets.0.conv2: 2   \n234 up_blocks.3.resnets.0.conv_shortcut: 4   \n235 up_blocks.3.resnets.1.conv1: 2   \n236 up_blocks.3.resnets.1.conv2: 2   \n237 up_blocks.3.resnets.1.conv_shortcut: 4   \n238 up_blocks.3.resnets.2.conv1: 2   \n239 up_blocks.3.resnets.2.conv2: 2   \n240 up_blocks.3.resnets.2.conv_shortcut: 4   \n241 mid_block.attentions.0.proj_in: 2   \n242 mid_block.attentions.0.transformer_blocks.0.attn1.to_q: 3   \n243 mid_block.attentions.0.transformer_blocks.0.attn1.to_k: 1   \n244 mid_block.attentions.0.transformer_blocks.0.attn1.to_v: 2   \n245 mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0: 2   \n246 mid_block.attentions.0.transformer_blocks.0.attn2.to_q: 1   \n247 mid_block.attentions.0.transformer_blocks.0.attn2.to_k: 4   \n248 mid_block.attentions.0.transformer_blocks.0.attn2.to_v: 4   \n249 mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0: 3   \n250 mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj: 2   \n251 mid_block.attentions.0.transformer_blocks.0.ff.net.2: 1   \n252 mid_block.attentions.0.proj_out: 3   \n253 mid_block.resnets.0.conv1: 1   \n254 mid_block.resnets.0.conv2: 1   \n255 mid_block.resnets.1.conv1: 1   \n256 mid_block.resnets.1.conv2: 1   \nconv_in: 8   \nconv_out: 8 ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "I Details for Evaluation Metrics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Sec. 5, we measure the performance on various metrics such as TIFA, GenEval, CLIP score and FID. Here, we provide more details for these metrics. ", "page_idx": 29}, {"type": "text", "text": "TIFA Score. TIFA v1.0 [26] aims to measure the faithfulness of generated images. It includes various 4K text prompts sampled from the MS-COCO captions [47], DrawBench [72], PartiPrompts [86], and PaintSkill [6], associated with a pre-generated set of question-answer pairs, resulting in 25K questions covering 4.5K diverse elements. Image faithfulness is measured by determining if the VQA model can accurately answer the questions from the generated images. ", "page_idx": 29}, {"type": "text", "text": "GenEval Score. GenEval [14] measures the consistency between the generated images and the description, including 6 different tasks: single object, two object, counting, colors, position, color attribution. All text prompts are generated from task-specific templates filled in with: randomly sampled object names from MS-COCO [47], colors from Berlin-Kay basic color theory, numbers with 2, 3, 4, and relative positions from \"above\", \"below\", \"to the left of\", or \"to the right of\". We adopt the pre-trained object detection model Mask2Former $(\\mathtt{S w i n-S-8\\times2})$ [5] for evaluation. ", "page_idx": 29}, {"type": "text", "text": "CLIP score and FID. CLIP score measures measure the similarity between text prompts and corresponding generated images. FID is used to evaluate the quality of generated images by measuring the distance between the distributions of features extracted from generated images and target images. In the main experiments, evaluation are measured based on MS-COCO 2014 validation set with 30K image-caption pairs [47]. We adopt ViT-B/32 model to evaluate the CLIP score in our experiments. ", "page_idx": 29}, {"type": "text", "text": "J Human Evaluation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Sec. 5, we provide the human evaluation results. Here, we provide more detailed human evaluation with category and challenge comparisons on PartiPrompts (P2), comparing Stable Diffusion v1.5 and BitsFusion, with the question: Given a prompt, which image has better aesthetics and image-text alignment? Our model is selected 888 times out of 1632 comparisons, indicating a general preference over SD-v1.5, which is chosen 744 times, demonstrating more appealing and accurate generated images. ", "page_idx": 29}, {"type": "text", "text": "J.1 Analysis on Categories ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Illustrations, People, and Arts. Our model significantly outperforms SD-v1.5 in generating illustrations (77 wins out of 124), images of people (101 out of 174), and arts (45 out of 65). ", "page_idx": 29}, {"type": "text", "text": "Outdoor and Indoor Scenes. Our model also shows strength in generating both outdoor (73 out of 131) and indoor scenes (23 out of 40), suggesting better environmental rendering capabilities. ", "page_idx": 29}, {"type": "text", "text": "J.2 Analysis on Challenges ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Complex and Fine-grained Detail: Our model excels in generating images with complex details (73 out of 113) and fine-grained details (173 out of 312), suggesting advanced capabilities in maintaining detail at varying complexity levels. ", "page_idx": 29}, {"type": "text", "text": "Imagination and Style & Format: Our model also shows a strong performance in tasks requiring imaginative (92 out of 149) and stylistic diversity (118 out of 204), highlighting its flexibility and creative handling of artistic elements. ", "page_idx": 29}, {"type": "image", "img_path": "0m19blQT6y/tmp/0e7996a7f38d622071fd7091de4e83df446463c6c1ffeded8cbaa05e16aaf074.jpg", "img_caption": ["Figure 14: Human evaluation across particular categories. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "0m19blQT6y/tmp/e239dbbe24159dfca25fed4815bad77463f7f38c98862ccd4ad3067c05990a74.jpg", "img_caption": ["Figure 15: Human evaluation across particular challenges. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "The strong performance in imaginative and artistic categories presents an opportunity to target applications in creative industries, such as digital art and entertainment, where these capabilities can be particularly valuable. ", "page_idx": 30}, {"type": "text", "text": "K Evaluation on Different Schedulers ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the main experiments in Sec. 5, we leverage the PNDM scheduler to generate images. Here, we measured the performance of different schedulers, such as DDIM [78] and DPMSolver [55], to demonstrate the generality and effectiveness of BitsFusion. We set 50 inference steps and fix ", "page_idx": 30}, {"type": "text", "text": "the random seed as 1024. As shown in Fig. 16, BitsFusionconsistently outperforms SD-v1.5 with different schedulers. ", "page_idx": 31}, {"type": "image", "img_path": "0m19blQT6y/tmp/d68e5b8a4265bc73422f0d5753b8d49f846904b7b08e7a1d3fdd047e4acefc02.jpg", "img_caption": ["Figure 16: TIFA scores comparisons between SD-v1.5 and BitsFusion, with different schedulers. Left: TIFA scores measured with DDIM [78] scheduler. Right: TIFA score measured with DPMSolver [55] scheduler. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "L Detailed GenEval Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Sec. 5, we provide the overall GenEval results. Here, we provide detailed GenEval results for further comparisons as illustrated in Tab. 8. ", "page_idx": 31}, {"type": "table", "img_path": "0m19blQT6y/tmp/17699bf3bf119b62e96588e0773ad258bc7f1f3f7df8a7c3158225c1efff157b.jpg", "table_caption": ["Table 8: Detailed GenEval with different CFG scales. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "M More Comparisons ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We provide the prompts for the images featured in the Fig. 1. Additionally, we provide more generated images for the comparison. ", "page_idx": 32}, {"type": "text", "text": "M.1 Prompts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Prompts of Fig. 1 from left to right are: ", "page_idx": 32}, {"type": "text", "text": "1. a portrait of an anthropomorphic cyberpunk raccoon smoking a cigar, cyberpunk!, fantasy, elegant, digital painting, artstation, concept art, matte, sharp focus, illustration, art by josan Gonzalez ", "page_idx": 32}, {"type": "text", "text": "2. Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail. ", "page_idx": 32}, {"type": "text", "text": "3. tropical island, $^{\\texttt{8k}}$ , high resolution, detailed charcoal drawing, beautiful hd, art nouveau, concept art, colourful, in the style of vadym meller ", "page_idx": 32}, {"type": "text", "text": "4. anthropomorphic art of a fox wearing a white suit, white cowboy hat, and sunglasses, smoking a cigar, texas inspired clothing by artgerm, victo ngai, ryohei hase, artstation. highly detailed digital painting, smooth, global illumination, fantasy art by greg rutkowsky, karl spitzweg ", "page_idx": 32}, {"type": "text", "text": "5. a painting of a lantina elder woman by Leonardo da Vinci . details, smooth, sharp focus, illustration, realistic, cinematic, artstation, award winning, rgb , unreal engine, octane render, cinematic light, macro, depth of field, blur, red light and clouds from the back, highly detailed epic cinematic concept art CG render made in Maya, Blender and Photoshop, octane render, excellent composition, dynamic dramatic cinematic lighting, aesthetic, very inspirational, arthouse. ", "page_idx": 32}, {"type": "text", "text": "6. panda mad scientist mixing sparkling chemicals, high-contrast painting ", "page_idx": 32}, {"type": "text", "text": "7. An astronaut riding a horse on the moon, oil painting by Van Gogh. ", "page_idx": 32}, {"type": "text", "text": "8. A red dragon dressed in a tuxedo and playing chess. The chess pieces are fashioned after robots. ", "page_idx": 32}, {"type": "text", "text": "M.2 Additional Image Comparisons ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We provide more images for further comparisons. For each set of two rows, the top row displays images generated using the full-precision Stable Diffusion v1.5, while the bottom row features images generated from BitsFusion, where the weights of UNet are quantized into 1.99 bits and the model size is $7.9\\times$ smaller than the one from SD-v1.5. All the images are synthesized under the setting of using PNDM sampler with 50 sampling steps and random seed as 1024. ", "page_idx": 32}, {"type": "image", "img_path": "0m19blQT6y/tmp/65a6f19ebb46630ce23cd4798029c3f151db890865599d8ae061a12bb49969ba.jpg", "img_caption": ["Figure 17: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: A person standing on the desert, desert waves, gossip illustration, half red, half blue, abstract image of sand, clear style, trendy illustration, outdoor, top view, clear style, precision art, ultra high definition image; b: A detailed oil painting of an old sea captain, steering his ship through a storm. Saltwater is splashing against his weathered face, determination in his eyes. Twirling malevolent clouds are seen above and stern waves threaten to submerge the ship while seagulls dive and twirl through the chaotic landscape. Thunder and lights embark in the distance, illuminating the scene with an eerie green glow.; c: A solitary figure shrouded in mists peers up from the cobble stone street at the imposing and dark gothic buildings surrounding it. an old-fashioned lamp shines nearby. oil painting.; d: A deep forest clearing with a mirrored pond reflecting a galaxy-fliled night sky; e: a handsome 24 years old boy in the middle with sky color background wearing eye glasses, it\u2019s super detailed with anime style, it\u2019s a portrait with delicated eyes and nice looking face; f: A dog that has been meditating all the time. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "0m19blQT6y/tmp/b7f0b2b7b985329645cdb45165cde2c17116d54aa708e71e6410716233fb7239.jpg", "img_caption": ["Figure 18: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: A small cactus with a happy face in the Sahara desert.; b: A middle-aged woman of Asian descent, her dark hair streaked with silver, appears fractured and splintered, intricately embedded within a sea of broken porcelain. The porcelain glistens with splatter paint patterns in a harmonious blend of glossy and matte blues, greens, oranges, and reds, capturing her dance in a surreal juxtaposition of movement and stillness. Her skin tone, a light hue like the porcelain, adds an almost mystical quality to her form.; c: A high contrast portrait photo of a fluffy hamster wearing an orange beanie and sunglasses holding a sign that says \"Let\u2019s PAINT!\u201d; d: An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt , he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm flim.; e: poster of a mechanical cat, techical Schematics viewed from front and side view on light white blueprint paper, illustartion drafting style, illustation, typography, conceptual art, dark fantasy steampunk, cinematic, dark fantasy; f: I want to supplement vitamin c, please help me paint related food. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "0m19blQT6y/tmp/2d0fecdd2cdc362c5cf03682ae87c7e0dbe9ea144ead4fa3b46c1376b3dd2cf9.jpg", "img_caption": ["Figure 19: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: new cyborg with cybertronic gadgets and vr helmet, hard surface, beautiful colours, sharp textures, shiny shapes, acid screen, biotechnology, tim hildebrandt, bruce pennington, donato giancola, larry elmore, masterpiece, trending on artstation, featured on pixiv, cinematic composition, dramatic pose, beautiful lighting, sharp, details, hyper - detailed, hd, hdr, $4\\;k,$ , $8\\,k$ ; b: portrait of teenage aphrodite, light freckles, curly copper colored hair, smiling kindly, wearing an embroidered white linen dress with lace neckline, intricate, elegant, mother of pearl jewelry, glowing lights, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by wlop, mucha, artgerm, and greg Rutkowski; c: portrait of a dystopian cute dog wearing an outfit inspired by the handmaid $\\ddot{\\imath}\\zeta^{l}\\!_{2}$ s tale $^\\textit{\\textregistered}O\\textit{17}$ ), intricate, headshot, highly detailed, digital painting, artstation, concept art, sharp focus, cinematic lighting, digital painting, art by artgerm and greg rutkowski, alphonse mucha, cgsociety; d: Portrait of a man by Greg Rutkowski, symmetrical face, a marine with a helmet, using a VR Headset, Kubric Stare, crooked smile, he\u2019s wearing a tacitcal gear, highly detailed portrait, scif,i digital painting, artstation, book cover, cyberpunk, concept art, smooth, sharp foccus ilustration, Artstation HQ; e: Film still of female Saul Goodman wearing a catmaid outfit, from Red Dead Redemption 2 (2018 video game), trending on artstation, artstationHD, artstationHQ; f: oil paining of robotic humanoid, intricate mechanisms, highly detailed, professional digital painting, Unreal Engine 5, Photorealism, HD quality, $8k$ resolution, cinema 4d, 3D, cinematic, professional photography, art by artgerm and greg rutkowski and alphonse mucha and loish and WLOP "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "0m19blQT6y/tmp/8ad3aaffb8290582d3170b0a0cd95803852ea6394627bac4640205661136e357.jpg", "img_caption": ["Figure 20: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: anthropomorphic tetracontagon head in opal edgy darknimite mudskipper, intricate, elegant, highly detailed animal monster, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm, bob eggleton, michael whelan, stephen hickman, richard corben, wayne barlowe, trending on artstation and greg rutkowski and alphonse mucha, $\\boldsymbol{\\vartheta}\\,\\boldsymbol{k}$ ; b: background shows moon, many light effects, particle, lights, gems, symmetrical!!! centered portrait dark witch, large cloak, fantasy forest landscape, dragon scales, fantasy magic, undercut hairstyle, short purple black fade hair, dark light night, intricate, elegant, sharp focus, digital painting, concept art, matte, art by wlop and artgerm and greg rutkowski and alphonse mucha, masterpiece; c: cat seahorse fursona, autistic bisexual graphic designer and musician, long haired attractive androgynous fluffy humanoid character design, sharp focus, weirdcore voidpunk digital art by artgerm, akihiko yoshida, louis wain, simon stalenhag, wlop, noah bradley, furaffinity, artstation hd, trending on deviantart; d: concept art of ruins of a victorian city burning down by j. c. leyendecker, wlop, ruins, dramatic, octane render, epic painting, extremely detailed, $\\delta\\,k$ ; e: hyperrealistic Gerald Gallego as a killer clown from outer space, trending on artstation, portrait, sharp focus, illustration, art by artgerm and greg rutkowski and magali Villeneuve; f: low angle photo of a squirrel dj wearing on - ear headphones and colored sunglasses, stadning at a dj table playing techno music at a dance club, hyperrealistic, highly detailed, intricate, smoke, colored lights, concept art, digital art, oil painting, character design by charlie bowater, ross tran, artgerm, makoto shinkai, wlop "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "0m19blQT6y/tmp/089bd79fef79dfc0119c1e7f0aac26104342ffa2e4a2e552d22cb684907f3511.jpg", "img_caption": ["Figure 21: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: a photograph of an ostrich wearing a fedora and singing soulfully into a microphone; b: a pirate ship landing on the moon; c: a pumpkin with a candle in it; d: a rabbit wearing a black tophat and monocle; e: a red sports car on the road; f: a robot cooking in the kitchen. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "0m19blQT6y/tmp/eb388b4763a784f386a3f2186f407b3c0baa724f3173e4eeac2e42a29fbd86ef.jpg", "img_caption": ["Figure 22: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: a baby daikon radish in a tutu; b: a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants; c: a woman with long black hair and dark skin; d: an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants; e: a blue sports car on the road; f: a butterfly. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "0m19blQT6y/tmp/a3f2000cd34a594c4d449e4348a690a425aea60f3e9f61c30cef48897e23102c.jpg", "img_caption": ["Figure 23: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: Helmet of a forgotten Deity, clowing corals, extremly detailed digital painting, in the style of Fenghua Zhong and Ruan Jia and jeremy lipking and Peter Mohrbacher, mystical colors, rim light, beautiful lighting, 8k, stunning scene, raytracing, octane, trending on artstation; b: Jeff Bezos as a female amazon warrior, closeup, D&D, fantasy, intricate, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, illustration, hearthstone, art by Artgerm and Greg Rutkowski and Alphonse Mucha; c: Portrait of a draconic humanoid, HD, illustration, epic, D&D, fantasy, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha, monster hunter illustrations art book; d: [St.Georges slaying a car adorned with checkered flag. Soviet Propaganda!!! poster!!!, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, illustration, octane render, unreal engine, photography]; e: a fire - breathing dragon at a medieval hobbit home, ornate, beautiful, atmosphere, vibe, mist, smoke, chimney, rain, wet, pristine, puddles, waterfall, clear stream, bridge, forest, flowers, concept art illustration, color page, 4 k, tone mapping, doll, akihiko yoshida, james jean, andrei riabovitchev, marc simonetti, yoshitaka amano, digital illustration, greg rutowski, volumetric lighting, sunbeams, particles; f: portrait of a well-dressed raccoon, oil painting in the style of Rembrandt "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We propose BitsFusion that quantizes the UNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with $7.9\\times$ smaller size while even better generation quality than the original one. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We list the limitations in Section \"Limitations\" in Appendix ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: We do not include theory assumptions and proofs. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide the details of the quantization strategy, quantization error metrics, mixed precision recipe and implementation setting in the Method, Experiments, and Appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We plan to release our code and trained models to facilitate the research efforts towards extreme low-bits quantization. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We provide detailed experimental settings in the Experiment section. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: Similar to other Stable Diffusion and quantization works, we report the results on large-scale datasets without the error bar. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide the experimental resources in the Experiment section. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We list the broader impacts in Section \"Broader Impacts\" in Appendix. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper does not have such risks. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We claim and cite the asset in the paper. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 42}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]