[{"figure_path": "PWkjxjgGLP/tables/tables_5_1.jpg", "caption": "Table 1: Examples for Relative Text-Position Prediction Task. Refer to Appendix I for the full list.", "description": "This table shows example instruction templates used for the Relative Text-Position Prediction (RTPP) task.  The RTPP task is designed to help the model learn to read text using layout information by having it predict the positions of given text segments within an image.  The table showcases examples for three variations of a reading partial text task (RPT), where the model is prompted to read specific portions of the text (first, middle, or last sections) and a predicting text position task (PTP), where the model is asked to specify the location (percentage range) of a given query text within the image. Appendix I contains the complete list of instruction templates.", "section": "3.4 Relative Text-Position Prediction Task"}, {"figure_path": "PWkjxjgGLP/tables/tables_6_1.jpg", "caption": "Table 2: Configurations of OCR-free document understanding baselines.", "description": "This table compares different baselines used for OCR-free document understanding. It lists the number of model parameters, trainable parameters, pretraining data, and fine-tuning data for each baseline.  The baselines are categorized into two groups: 'Document-specific Pretraining' and 'MLLM-based Instruction Tuning'.  The table helps to illustrate the differences in model size, training data, and approach used among various OCR-free methods.", "section": "4 Experiments"}, {"figure_path": "PWkjxjgGLP/tables/tables_6_2.jpg", "caption": "Table 3: Performance comparison with other OCR-free document understanding baselines. The bold-faced numbers indicate the best performance in each column.", "description": "This table presents a quantitative comparison of the proposed model's performance against other state-of-the-art OCR-free document understanding methods across various benchmark datasets.  The datasets cover diverse document understanding tasks, including visual question answering, infographics question answering, form information extraction, table fact verification, chart question answering, visual machine reading comprehension, and text-based visual question answering.  The table highlights the superior performance of the proposed approach, indicated by boldfaced numbers showing the best results in each task. The results demonstrate the effectiveness of the proposed method in handling various document types and complexity levels.", "section": "4 Experiments"}, {"figure_path": "PWkjxjgGLP/tables/tables_7_1.jpg", "caption": "Table 4: Results of main ablation studies with the BLIP-2-based model. Note that the reconstruction loss (Recon) only comes with MS + HVFA.", "description": "This table presents the results of ablation studies conducted using the BLIP-2-based model.  The impact of different components on model performance is evaluated across various document understanding tasks (DocVQA, InfoVQA, DeepForm, etc.). The results show the effect of including multi-scale features (MS), the hierarchical visual feature aggregation (HVFA) module, a reconstruction layer, and the relative text position prediction (RTPP) task on overall performance. Notably, only when both multi-scale features and the HVFA module are used, reconstruction loss is considered. ", "section": "4.4 Ablation Study"}, {"figure_path": "PWkjxjgGLP/tables/tables_7_2.jpg", "caption": "Table 5: Ablation study on the variations of hierarchical visual feature aggregation techniques. We employ a BLIP-2-based model for experiments.", "description": "This table presents the results of an ablation study investigating different variations of the hierarchical visual feature aggregation (HVFA) techniques.  The study uses a BLIP-2-based model and evaluates performance across multiple document understanding benchmarks (DocVQA, InfoVQA, DeepForm, KLC, WTQ, TabFact, ChartQA, VisualMRC, TextVQA, and TextCaps). Variations include: spatial dimension reduction methods (max pooling, linear projectors, cross-attentive pooling, and cross-local-attentive pooling), query token initialization strategies (random vectors and max-pooled features), stop-gradient techniques (with and without stop-gradient), and reconstruction loss weight adjustments (\u03bb = 1.0, \u03bb = 0.1, \u03bb = 0.01, and \u03bb = 0). The table shows the impact of each variation on the performance metrics for each benchmark.", "section": "4.4 Ablation Study"}, {"figure_path": "PWkjxjgGLP/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparison with other OCR-free document understanding baselines. The bold-faced numbers indicate the best performance in each column.", "description": "This table compares the performance of the proposed OCR-free document understanding framework with other existing OCR-free baselines across ten different document understanding benchmarks.  It shows the performance (measured using different metrics appropriate to each benchmark) for each model on several tasks. The bold numbers highlight the best performing model for each task.", "section": "4 Experiments"}, {"figure_path": "PWkjxjgGLP/tables/tables_16_1.jpg", "caption": "Table 3: Performance comparison with other OCR-free document understanding baselines. The bold-faced numbers indicate the best performance in each column.", "description": "This table presents a quantitative comparison of the proposed model's performance against other state-of-the-art OCR-free document understanding baselines across multiple benchmark datasets.  The datasets evaluate various aspects of document understanding, including visual question answering, information extraction, and table fact verification.  The bold numbers highlight the best performance achieved by any model for each specific benchmark.", "section": "4 Experiments"}, {"figure_path": "PWkjxjgGLP/tables/tables_16_2.jpg", "caption": "Table 8: Performance of the BLIP-2-based model with varying scales.", "description": "This table shows the performance of the BLIP-2 based model on various document understanding benchmarks with different numbers of visual scales used as input.  The number of scales refers to the levels of detail incorporated through the shape-adaptive cropping (SAC) method (global, nh x nw, and 2nh x 2nw).  The table presents the trade-off between model performance and computational efficiency, as adding scales improves accuracy but reduces the throughput (images processed per second).", "section": "4.4 Ablation Study"}, {"figure_path": "PWkjxjgGLP/tables/tables_17_1.jpg", "caption": "Table 9: Comparison with task-specific state-of-the-arts methods.", "description": "This table compares the performance of the proposed OCR-free document understanding framework with the state-of-the-art (SOTA) methods for each of the ten document understanding benchmark datasets.  It highlights the relative performance of the proposed approach against task-specific models, showcasing its ability to perform well across diverse and challenging tasks compared to specialized methods.", "section": "4. Comparison with Task-specific SoTA Methods"}, {"figure_path": "PWkjxjgGLP/tables/tables_18_1.jpg", "caption": "Table 2: Configurations of OCR-free document understanding baselines.", "description": "This table presents the configurations of several OCR-free document understanding baselines.  It compares different methods across several key metrics, including the number of model parameters, the number of trainable parameters, the size of the pretraining data used, and the size of the fine-tuning data used.  This allows for a comparison of the computational resources and data requirements of different approaches to OCR-free document understanding.", "section": "4.1 Datasets and Evaluation Metrics"}]