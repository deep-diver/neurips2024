[{"type": "text", "text": "A teacher-teacher framework for clinical language representation learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Feiqing Huang\u2217 Harvard T.H. Chan School of Public Health fqhuang@hsph.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Shenghan Zhang\u2217 Harvard Medical School shenghan_zhang@hms.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Sara Morini Sweet Harvard Medical School sara_morini@hms.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Tianxi Cai\u2020 Harvard T.H. Chan School of Public Health Harvard Medical School tcai@hsph.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, there has been a proliferation of ready-to-use large language models (LLMs) designed for various applications, both general-purpose and domainspecific. Instead of advocating for the development of a new model or continuous pretraining of an existing one, this paper introduces a pragmatic teacher-teacher framework to facilitate mutual learning between two pre-existing models. By leveraging two teacher models possessing complementary knowledge, we introduce a LIghtweight kNowledge alignmEnt (LINE) module aimed at harmonizing their knowledge within a unified representation space. This framework is particularly valuable in clinical settings, where stringent regulations and privacy considerations dictate the handling of detailed clinical notes. Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data. Validation and downstream tasks further demonstrate the effectiveness of the proposed framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clinical data frequently appears in various forms, with each format capturing overlapping but complementary aspects of patient information. For instance, narrative details in a clinical note may be processed into a structured list of clinical concepts for research purposes. Similarly, a CT scan report provides descriptive findings that are directly linked to the visual data in the corresponding scan. Ideally, both forms of data would be jointly accessible; however, practical challenges, such as privacy restrictions, often limit us to only one form. This limitation raises an important question: ", "page_idx": 0}, {"type": "text", "text": "Could we use the accessible form of data to approximate or represent the unobserved one? ", "page_idx": 0}, {"type": "text", "text": "To explore this question, we can leverage the capabilities of large pretrained models. Recent years have seen the emergence of large language models (LLMs), which have demonstrated impressive performance across a range of tasks, including prediction, generation, and representation learning [1, 28, 35]. In the clinical domain, many pretrained LLMs\u2014such as CODER [39], UmlsBERT [16], Clinical BioBERT [3], SapBERT [14], PubmedBERT [9], and BioBERT [12]\u2014are based on BERT architectures with approximately 300 million parameters. These models are typically initialized with BERT weights and then undergo further pretraining on biomedical texts, a process often referred to as continual pretraining. While this approach allows the model to integrate both general and domainspecific knowledge, it is also time-consuming and resource-intensive. The recent proliferation of off-the-shelf clinical LLMs now prompts a pragmatic idea: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can the pretrained models be made to directly exchange knowledge with one another? ", "page_idx": 1}, {"type": "text", "text": "Inspired by the two questions posed above, we propose a teacher-teacher framework for clinical representation learning. The core idea is as follows: given paired training data in two distinct forms and two pretrained models with complementary knowledge bases, the framework processes each form of data separately through one of the models. It then processes the resulting representations through a newly proposed module and further aligns them within a unified representation space. This alignment entails a mutual teaching process, where both models act as teachers, exchanging their knowledge. The complementary knowledge arises from two sources: first, the complementary information inherent to each form of data, and second, the distinct knowledge embedded in each pretrained model due to differences in their original training corpora. We summarize our key innovations as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 LINE module for teacher-teacher learning: We propose a lightweight knowledge alignment (LINE) module to facilitate effective teacher-teacher learning. This module takes representations from two pretrained teacher models and projects them into a unified representation space, guided by an alignment loss and a relational loss. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Efficient training with residual information recovery: We adopt a two-stage, few-epoch training process focused solely on the LINE module, keeping the pretrained model weights frozen. In the first stage, we train the LINE module for a few epochs to achieve initial alignment and to capture residuals (complementary information across two forms of data). In the second stage, these residuals are used to refine the training data for further few-epoch training, offering insights into the complementary information between data forms. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Generating cross-form representation: Once trained, the LINE module enables cross-form representation for downstream tasks. When only one data form is available, the corresponding teacher model and LINE projection can generate a proxy for the missing data, enabling its use in the downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "We evaluate our teacher-teacher framework on two fronts: (1) whether knowledge alignment can improve the performance of one or both teacher models across different tasks, and (2) whether the available data form can be used to generate better proxies for the other, missing data form. ", "page_idx": 1}, {"type": "text", "text": "1.1 Clinical use case ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "While using one form data to represent another is a relevant question across various fields, we demonstrate the utility of our proposed method through a real-world clinical use case. This example is particularly relevant to clinical data privacy concerns, highlighting its practical importance. Specirically, we focus on two data forms: clinical notes and their corresponding structured lists of clinical concepts. Clinical notes play a crucial role in numerous applications, such as disease diagnosis, disease progression prediction, patient classification, and risk assessment [26, 30, 15].However, working with real clinical notes is subject to strict regulations due to privacy concerns. Sharing clinical notes between institutions necessitates approval from an institutional review board (IRB), adding layers of regulatory complexity and substantially extending project timelines. ", "page_idx": 1}, {"type": "text", "text": "These challenges are a major barrier to scaling up clinical datasets for pre-training. As an alternative, efficient software [e.g. 37, 24] can directly extract structured clinical concepts from clinical notes. This approach offers two advantages: First, because clinical concepts are drawn from a predefined dictionary \u2013 specifically, the Unified Medical Language System (UMLS) [6] \u2013 it reduces the risk of including sensitive information, making data sharing across research groups more feasible. Second, known relationships among many clinical concepts can be further incorporated to form a structured graph, capturing both the textual content and interrelationships among concepts. ", "page_idx": 1}, {"type": "text", "text": "In this use case, we treat the clinical note as the missing data form and reframe our question accordingly: Can we leverage the list of clinical concepts to effectively summarize and provide a proxy for the content in the clinical note? ", "page_idx": 1}, {"type": "text", "text": "1.2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Different modes of learning Existing knowledge within pretrained LLMs varies based on their learning modes and domains of their pretrained data. The first mode, learning by generalization, involves training models on extensive datasets to generalize knowledge through example-based learning [1, 28]. These models encode knowledge directly into their embeddings, which capture syntactic and semantic relationships in texts. Numerous studies have demonstrated the efficacy of LLMs in encoding syntactic heuristics and retaining factual knowledge to varying extents [8, 20, 29, 19, 22]. Recent advancements, exemplified by models such as Sentence-T5 [18], E5 [32], BGE [35], and GPT-4 [2], have focused on generating high-quality textual representation for downstream tasks. However, in specialized clinical domains, vocabulary diverges significantly from general corpora, requiring that these general-purpose models undergo additional training on biomedical data to ensure relevance and accuracy. Models like BioBERT [12] and PubmedBERT [9], pretrained on biomedical texts, have demonstrated superior performance compared to BERT in biomedical-specific tasks. Clinical BioBERT [3] further augment their training data with MIMIC datasets to incorporate medical knowledge derived from real-world medical practices. Nevertheless, scaling up training data in the clinical domain poses challenges due to privacy regulations, as previously noted. ", "page_idx": 2}, {"type": "text", "text": "The second mode, learning by integrating pre-existing knowledge, focuses on encoding established factual associations between concepts into the model\u2019s representation space. In the clinical domain, knowledge repositories such as UMLS [6] offer comprehensive biomedical ontologies and relatedness information between concepts, which can be treated as existing knowledge graph and harnessed during the pre-training phase. For example, SapBERT [14] leverages synonymous relationships extracted from UMLS during self-supervised pretraining, strengthening the model\u2019s understanding of semantic similarity. UmlsBERT [16] incorporates semantic type information from UMLS, enabling a deeper semantic understanding of clinical texts. Furthermore, CODER [39] encodes more granular relationships, such as \"may prevent or treat\" and \"may cause\", into their knowledge networks, enhancing its ability to capture medical associations. By integrating such pre-existing domain-specific knowledge graph during training, these models show improved ability to navigate the intricacies of specialized domains, leading to better performance in domain-specific tasks [14, 16, 39]. ", "page_idx": 2}, {"type": "text", "text": "Distinct from these two modes, the proposed teacher-teacher framework adopts the third mode, learning by alignment. This mode of learning involves creating a joint representation space between two models using paired data, such as an image with its textual description, audio/video with its transcript, or multilingual translated texts. This approach focuses on teaching models to distinguish between paired and unpaired data, making it especially effective for multimodal applications. For example, the seminal CLIP framework [21] uses different encoders for text and images to embed each mode of information separately. It then employs a contrastive loss to align these multimodal embeddings by end-to-end training on both encoders, ensuring that related text and image pairs are closely aligned in the representation space. Similarly, Li et al. [13] utilize a text encoder and a text-referred audio encoder with inter-modality and intra-modality training, enhancing the model\u2019s ability to integrate complementary features from different views. Wang et al. [34] propose an efficient global-local alignment framework for co-training language with videos, and its improved performance further highlights the advantages of learning the homogeneity as well as heterogeneity across language and visual modalities. For multilingual translation tasks, alignment learning has also demonstrated remarkable efficacy. For example, Reimers and Gurevych [23] introduce a teacher-student network model in which a fixed teacher network, well-versed in a target language, guides a student network, initially proficient in a different language, to emulate the teacher\u2019s knowledge. This method not only enhances translation accuracy but also creates a joint representation space that offers potentially interesting insights into cross-linguistic differences. ", "page_idx": 2}, {"type": "text", "text": "Other related works In a broader context, if we view the accessible form of data as the source domain and the inaccessible one as the target domain, our approach is weakly related to generative domain adaptation; however, unlike generative domain adaptation, our method requires paired data during training. Traditional methods in this field often utilize generative models to synthesize samples across domains, helping the model generalize effectively in new contexts. Some example frameworks include generative adversarial networks [41], variational autoencoders [33], diffusion models [5], or generative pretrained transformers [36]. However, our framework diverges from these in that we specifically deploy two pretrained teacher models, each with distinct expertise and capabilities. This design allows the models not only to align but also to exchange complementary knowledge during the learning process, potentially enhancing the performance of one or both teacher models. ", "page_idx": 2}, {"type": "image", "img_path": "zDaD8zv8tG/tmp/3389a2c033ab3dd760878053df34575e591481aaefbdf5f820bcb82bb4de3a15.jpg", "img_caption": ["Figure 1: The proposed teacher-teacher framework is illustrated for a specific use case. The two forms of training data are unstructured clinical notes and structured clinical concepts represented in a relational graph. Two pretrained teacher LLMs independently embed these two forms of data, with the LINE module facilitating mutual learning and alignment. Through this process, the embeddings of the two data forms converge toward a shared representation space, enabling knowledge exchange and cross-form representation between the two teacher models. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2 The teacher-teacher framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Training data and alignment objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section provides a detailed illustration of our LINE framework for a specific clinical use case, aligning clinical concepts with corresponding clinical texts. The algorithm can be extended to support a broad range of other alignment tasks, as discussed in Section 4. Specifically, our paired training data take the form (CLINICAL TEXT, LIST OF CLINICAL CONCEPTS), where each list of clinical concepts is derived from the clinical texts. We further augment the clinical concepts with known relationships from UMLS to form a structured graph. The primary objective is to create a \u201csummarized\u201d embedding for the list of concepts and align it with the projected text embedding, as illustrated in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "2.2 Teacher models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A key design principle in our framework is selecting two teacher models with complementary knowledge bases. To this end, we choose (1) a general-purpose LLM pretrained on large-scale, general-domain text, and (2) a domain-specific LLM with established knowledge of biomedical concept relationships. In this setup, the general-purpose model (Teacher 1) learns factual relationships between clinical concepts from the domain-specific model (Teacher 2), while Teacher 2 enhances its grasp of the contextual connections linking these concepts. In the paper, we explore two combinations of teacher models: BGE+CODER and GPT- $^{4+}$ CODER. ", "page_idx": 3}, {"type": "text", "text": "Teacher 1: General purpose LLM BGE is a general-purpose embedding model trained on large, diverse language datasets. Initially trained on 100 million text pairs from open web corpora, BGE further underwent multitask learning on various retrieval and reranking tasks, including the CMedQAv2 medical question and answer dataset [40]. It ranks highly on the Massive Text Embedding Benchmark (MTEB) dataset [17], which covers tasks like classification, reranking, retrieval, STS, and paired classification tasks. GPT-4 is a large-scale, multimodal model considered as one of the state-of-the-art tools for language modeling and generation. Although its model architecture and trained weights are not publicly available, GPT-4 can generate embeddings for textual data, accessible through the OpenAI API. For our experiments, we used the \"text-embedding-3-small\" model, which produces embeddings with a dimension of 1536. ", "page_idx": 3}, {"type": "text", "text": "Teacher 2: LLM with domain knowledge CODER is initialized by PubMedBERT [9] and trained on the UMLS 2020AA release relation triplets, encompassing 4.27M concepts, 15.48M terms, and 87.89M relations across 127 semantic types, 14 relation types, and 923 relationship attributes. It employs contrastive learning with relation triplets, i.e. (head entity, relation, tail entity). CODER demonstrates superior performance in term normalization and disease classification tasks compared to SapBERT, Clinical BERT, PubMedBERT, and BioBERT. ", "page_idx": 4}, {"type": "text", "text": "In our two combinations of teacher models, the general purpose LLMs (BGE and GPT-4) with their extensive and diverse training, generally outperform CODER. This imbalance might suggest that they cannot learn from CODER. However, our experiments suggest that not only can CODER benefti from the knowledge of the general-purpose LLM, but the performance of BGE or GPT-4 also improves on certain tasks with alignment training. We hypothesize that the alignment process regularizes the embedding space of the general-purpose LLM by incorporating CODER\u2019s association graph, resulting in a more diversified and robust representation space. ", "page_idx": 4}, {"type": "text", "text": "2.3 LIghtweight kNowledge alignmEnt module (LINE) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our framework eliminates the need for any further training of the original teacher models. Instead, we introduce a trainable LIghtweight kNowledge alignmEnt module (LINE), which explicitly models projection operations to align the embeddings from the general-purpose LLM and the domain-specific LLM into a unified representation space. ", "page_idx": 4}, {"type": "text", "text": "First, we derive clinical text embeddings and concept embeddings from the general-purpose LLM and domain-specific LLM using pretrained checkpoints available through Huggingface or the OpenAI API, denoted as $\\mathbf{t}^{(0)}\\in\\mathbb{R}^{d_{t}}$ and $\\mathbf{c}^{(0)}\\in\\mathbb{R}^{d_{c}}$ , respectively. These initial embeddings retain the original knowledge encoded within each teacher model. We then incorporate trainable projection layers specific to each LLM to refine and align these embeddings. ", "page_idx": 4}, {"type": "text", "text": "Projection layer for general purpose LLM: Fully-connected layer The projection layer for the output embeddings $\\mathbf{t}^{(0)}$ from BGE or GPT-4 is implemented as a simple fully-connected linear layer: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{t}:=\\mathcal{F}(\\mathbf{t}^{(0)})=\\mathbf{FC}(\\mathbf{t}^{(0)}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This approach aligns with prior work [4, 7, 21], where linear projections are used without non-linear activation functions. ", "page_idx": 4}, {"type": "text", "text": "Projection layer for domain-specific LLM: Multihead graph attention This layer is designed to reinforce the domain-specific knowledge embedded in CODER from the UMLS ontology. We focus on modeling seven broad categories of UMLS relations: parent-child hierarchy, synonyms, related and possibly synonymous, broad relationship, narrow relationship, non-synonymous/narrow/broad relationship, and quantifiable relationships. Each relationship type is represented by a distinct adjacency matrix, and a multi-head graph attention (MHGA) layer assigns each type to a separate attention head. This design enables the layer to explicitly account for the unique structure of each relation. The outputs from all attention heads are aggregated and projected into the joint representation space. Formally, for any concept with an initial CODER embedding $\\mathbf{c}^{(0)}$ , we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{c}:=\\mathscr{G}(\\mathbf{c}^{(0)})=\\mathbf{average}_{k\\in[\\tau]}\\left[\\mathbf{GraphAttn}_{k}(\\mathbf{W}_{c}\\mathbf{c}^{(0)},\\mathbf{adjacency\\;matrix}_{k})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{c}\\in\\mathbb{R}^{d_{t}\\times d_{c}}$ is the projection matrix that maps the initial CODER embeddings to the same dimension as the BGE or GPT-4 embeddings. Additional details on the handling of negatively mentioned concepts are provided in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "2.4 Training strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To enable the two teacher models to enhance each other\u2019s knowledge in a time-efficient and resourceefficient way, we adopt a two-stage, few-epoch training strategy exclusively for the LINE module. This approach effectively aligns the embeddings from the general-purpose and domain-specific LLMs without any additional training of the original pretrained models. It also allows for the use of proprietary models, such as GPT-4, where access to the underlying pretrained model is restricted. ", "page_idx": 4}, {"type": "text", "text": "Unified representation via alignment The first stage involves an initial round of few-epoch training aimed at defining residuals, i.e. the differences between the learned representations of the two data forms. This stage is critical for capturing complementary information between the two models. ", "page_idx": 4}, {"type": "text", "text": "Specifically, we aim to align the embedding for clinical texts with its list of associated concept embeddings in the joint representation space. This alignment is quantified using a monotonic score function $s$ , which measures the degree of alignment between the elements of each data pair. Formally, let $\\mathbf{p}_{i}=\\left(\\{\\mathbf{c}_{i,j}\\}_{j\\in[m_{i}]},\\mathbf{t}_{i}\\right)$ denote a positive pair, where $\\mathbf{t}_{i}$ is the clinical text embedding and $\\{\\mathbf{c}_{i,j}\\}_{j\\in[m_{i}]}$ are the corresponding concept embeddings with a total of $m_{i}$ concepts, for all $i\\,\\in\\,[N]$ . The negative pair is defined as $\\mathbf{n}_{i,i^{\\prime}}\\,=\\,(\\{\\mathbf{c}_{i,j}\\}_{j\\in[m_{i}]},\\mathbf{t}_{i^{\\prime}})$ for $i^{\\prime}\\neq i\\in[N]$ . The score function $s$ is defined as $s(\\cdot)=s(\\{\\mathbf{c}_{j}\\}_{j\\in[m]},\\mathbf{t})=-\\|\\mathbf{t}-\\mathrm{mean}(\\{\\mathbf{c}_{j}\\}_{j\\in[m]})\\|_{2}$ . Our goal is to ensure $s(\\mathbf{p}_{i})\\geq s(\\mathbf{n}_{i,i^{\\prime}})$ , indicating better alignment of positive pairs over negative pairs. ", "page_idx": 5}, {"type": "text", "text": "To account for complementary information in the positive pairs, we introduce a data-dependent alignment weight $\\rho_{i}\\,\\in\\,(0,1]$ to allow varying degrees of information overlap. For example, in extracting clinical concepts from clinical texts, misspellings can result in failure to detect key concepts; see Table A1 in the Appendix for concrete examples. A smaller $\\rho_{i}$ value indicates more tolerance for misalignment, reflecting cases with less overlap between the text and concept embeddings. The alignment loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{i}\\sum_{i^{\\prime}\\neq i}\\rho_{i}f(s(\\mathbf{p}_{i}),s(\\mathbf{n}_{i,i^{\\prime}})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f$ is a contrastive loss function, and we used the triplet loss [25] for our experiments. In our specific context, the weight $\\rho_{i}$ is computed as the percentage of words in the text captured in the concept list, i.e. $\\rho_{i}=$ numbert ootf alw owrodrsd  accocuonutn itne dt efxotr  iin text i ", "page_idx": 5}, {"type": "text", "text": "In addition, to preserve the integrity of existing concept relations, we incorporate a relational contrastive loss during training. Here, positive pairs represent known relationships, while negative pairs indicate absent or unknown relationships. This additional loss term ensures that information from the known association graph is retained throughout the alignment process. ", "page_idx": 5}, {"type": "text", "text": "Refinement through residual recovery Once the first training stage stabilizes, we move to the second stage, focusing on recovering residual information to further refine the alignment. The residual ${\\bf{e}}_{i}$ for each data pair is computed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{e}_{i}=\\mathbf{t}_{i}-\\operatorname{mean}(\\{\\mathbf{c}_{i,j}\\}_{j\\in[m_{i}]}),\\quad i\\in[N].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To further consolidate information from residuals, we make use of a projected concept dictionary $\\mathbb{C}_{\\mathcal{G}}$ , consisting of all clinical concepts from UMLS, projected using the trained $\\mathcal{G}$ defined in (1). For each residual $\\mathbf{e}_{i}$ , we select concepts from $\\mathbb{C}_{\\mathcal{G}}$ that have a cosine similarity of 0.9 or higher with $\\mathbf{e}_{i}$ . The most similar concept from the selected subset, denoted as $\\mathbf{c}_{i,m_{i}+1}$ , is then added to the concept embeddings set, refining the training pair to $(\\{\\mathbf{c}_{i,j}\\}_{j\\in[m_{i}+1]},\\mathbf{t}_{i})$ for further few-epoch training. ", "page_idx": 5}, {"type": "text", "text": "3 Training and validation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Data preprocessing and training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our model, referred to as LINE, is trained with two configurations of teacher models: BGE $^+$ CODER and GPT- $^{4+}$ CODER. For our training dataset, we utilized 332K discharge notes and 2.2M radiology reports from 146K patients available in the MIMIC-IV database [11]. Given the structured nature of both types of clinical notes, we began by segmenting the notes into their respective sections. Radiology reports were divided into seven sections: examination, indication, technique, comparison, finding, procedures, and impression. Similarly, discharge notes were segmented into fourteen sections: chief complaint, major surgical or invasive procedure, history of present illness, past medical history, social history, family history, physical exam, pertinent results, brief hospital course, medications on admission, discharge medications, discharge disposition, discharge diagnosis, discharge condition, discharge instructions, and follow-up instructions. ", "page_idx": 5}, {"type": "text", "text": "To extract clinical concepts from these notes, we employed the Narrative Information Linear Extraction (NILE) software [37]. NILE outputs a list of clinical concepts present in each segmented text, indicating whether each concept is mentioned positively or negatively. Figure 1 provides an illustrative example. Next, we combined the original text with its extracted clinical concepts to create paired training data. This preprocessing resulted in a dataset of 7.2M text-concept pairs. During this process, we located the extracted concepts within the text and counted the number of words accounted for by the list of concepts. The alignment weight for each text was then calculated as \u03c1 = numbert ootf alw owrodrsd  accocuonutnted for. To construct the relational adjacency matrices for the CODER projection ", "page_idx": 5}, {"type": "text", "text": "Table 1: For each positive concept-text pair, we randomly selected 100 other texts to replace the original text, forming negative pairs. We then calculated the cosine similarity between the mean of the clinical concept list and the text, with embeddings generated by various models. By ranking these cosine similarities from highest to lowest, we identified the rank of each positive pair among its negative pairs and computed the mean rank, mean reverse rank, and Top-10 accuracy $\\mathrm{Top}10@\\mathrm{Acc})$ . Improvements over the corresponding teacher models are indicated by $\"\\downarrow\"$ for mean rank and $\"\\uparrow\"$ for mean reverse rank and Top $10\\textcircled{\\omega}$ Acc. ", "page_idx": 6}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/38d1271fb76616f425b08d3534f02dc30f9b0fb99812af1e5752fe3f21bf2266.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "layer, we leveraged the 2022AB release of the UMLS knowledge graph, which encompasses 9M concepts, defined by 4M unique identifiers (CUIs), and includes 25M relationships. ", "page_idx": 6}, {"type": "text", "text": "We adopted the Adam optimizer with a learning rate of $10^{-3}$ and a batch size of 128. The training process was divided into two stages as detailed in Section 2.4: We first trained the model for three epochs initially, during which we identified residual concepts not adequately captured. Then, using the model checkpoint from the first phase, we recovered the residual concepts and refined the training pairs. The model was then trained for an additional two epochs. The alignment loss and the relational loss both converged rapidly. All experiments were conducted using an NVIDIA RTX 8000 GPU with 48GB of VRAM. The entire training process required less than 10 hours on a single GPU. ", "page_idx": 6}, {"type": "text", "text": "3.2 Validation on alignment objective ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section validates the effectiveness of our trained LINE model in aligning the representation spaces of the two teachers. To achieve this, we utilized a holdout subset of patients whose clinical notes were excluded from the training set. This subset comprised 100K radiology reports from 92K patients. We applied the same data preprocessing steps, resulting in 275K text-concept pairs. Our evaluation method involved the following steps. For each positive pair, we randomly sampled 100 negative pairs by substituting the clinical text with a different one. We then computed the cosine similarity between the mean embedding of the concept list and the text embedding for all pairs, in line with our alignment objective. Finally, we ranked these pairs based on their cosine similarity scores, from highest to lowest. ", "page_idx": 6}, {"type": "text", "text": "To assess performance, we calculated the mean rank, mean reverse rank, and Top-10 accuracy for all positive pairs. These metrics are presented in Table 1. We compared our LINE model against several models: GPT-4, BGE, CODER, SapBERT, Clinical BioBERT, PubMedBERT, and BioBERT. As shown in Table 1, LINE with BGE $^{+}$ CODER achieves the highest alignment between concept and text representations, outperforming the other models. Both LINE models show improved alignment compared to their respective teacher models, validating the consistency of our training objectives. ", "page_idx": 6}, {"type": "text", "text": "3.3 Validation on downstream tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, the performance of our trained LINE models on various downstream tasks is evaluated based on two main aspects: (1) to determine whether the teacher models can benefit from mutual learning, and (2) to assess whether the LINE module can generate more effective proxies for clinical text using the list of clinical concepts. Specifically, we use the LINE-projected CODER embeddings for tasks involving clinical concepts and the LINE-projected BGE or GPT-4 embeddings for tasks involving clinical text. Given that two model configurations are used, we refer to each configuration as \"LINE\" in our tables, with \"Teacher $^{1+}$ Teacher $2\"$ specified for clarity. Three types of downstream tasks are considered: (1) a zero-shot clinical concept similarity evaluation task; (2) two standard i2b2 clinical NLP tasks; and (3) a renal cancer recurrence detection task. ", "page_idx": 6}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/f8cef6246ef7e89bf01a1d7a06a680fb53e826011c64ced3f78d535611f245c8.jpg", "table_caption": ["Table 2: AUCs for detecting related pairs versus randomly selected pairs under various models. The classes of clinical concepts include parent-child hierarchy, siblings hierarchy, may treat or may prevent, classifies as, differential diagnosis, method of, and causative of. The last row lists the detailed number of positive pairs in each class. Improvements over the corresponding teacher models are indicated by \"\u2191\". "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/fc150543f2aa70b97bc897dfaf739889a9bf7f0d823e230c2df40d97a9d028fe.jpg", "table_caption": ["Table 3: The mean of the F1 scores for different models over five random initializations on two i2b2 datasets. The results for BioBERT, Clinical BioBERT and UmlsBERT were directly copied from [16] for comparison. Here, the LINE projection is applied to the token-level BGE embeddings. The best result under each metric is highlighted in bold font. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.3.1 Clinical concept similarity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated the quality of the LINE-projected embeddings based on their ability to detect known relationships among 222K pairs of 159K concepts. These pairs were curated from several major UMLS relation classes, including \"may treat or may prevent\u201d, \"classifies\u201d, \"differential diagnosis\u201d, \"method of\", and \"causative\u201d. Detailed counts of positive pairs for each class are provided in the last row of Table 2. To ensure a fair comparison with the other models, we removed the edges corresponding to these evaluation pairs from the relational adjacency matrices used in the LINE model. We then assessed the model\u2019s ability to capture different types of relationships by calculating cosine similarities between the embedding vectors of related pairs and comparing these to randomly selected pairs with similar semantic characteristics. For example, when evaluating the \"may treat or may prevent\" relationship, we used related disease-drug pairs as positive examples and compared them against negative pairs also composed of disease-drug pairs. ", "page_idx": 7}, {"type": "text", "text": "We calculated the Area Under the Curve (AUC) of the cosine similarities to assess the models\u2019 ability to distinguish known pairs from random ones. As shown in Table 2, GPT-4 and BGE outperform CODER and all other baseline clinical LLMs, serving as strong benchmarks. Notably, the LINEprojected embeddings achieve AUCs that not only exceed those of the original CODER model but also outperform the BGE benchmark in six of the seven relation classes and the GPT-4 benchmark in four of the seven. In the remaining classes, LINE\u2019s AUCs are comparable to their respective BGE and GPT-4 benchmarks, with a deviation less than 0.008. ", "page_idx": 7}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/5a52c41fe6490c5c34eb44af06d9f9506f44fc7270783a8f3fc281230d461d44.jpg", "table_caption": ["Table 4: Performance metrics of sentence embeddings and their proxy embeddings generated from concept lists for various models, averaged over five-fold cross-validation. Improvements over the corresponding teacher models are indicated by \"\u2191\". "], "table_footnote": ["Table 5: Difference in performance metrics between sentence embeddings and their proxy embeddings generated from concept lists, calculated using results in Table 4. Reductions in difference, which indicate improved alignment, are marked by $\"\\downarrow\"$ . "], "page_idx": 8}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/6917df74cc3f4ab747ef2ddf3be65047713ea2d16282767bd7f56dddfcd2fc17.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "3.3.2 Clinical NLP benchmarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluated our model on two standard biomedical named entity recognition (NER) benchmark tasks: the i2b2 2006 de-identification challenge [31] and the i2b2 2014 de-identification challenge [27]. We followed the train/validation/test splits specified in the original challenges, as detailed in Table 1 of [3]. The datasets from 2006 and 2014 contain 317 and 43 label classes, respectively. The NER tasks involve tokenizing sentences and then classifying each token within the sentence. For this task, we used token-level embeddings from each model and the LINE projection was applied to token-level BGE embeddings. GPT-4 was not included due to the limited access to its token-level embedding. Our fine-tuning process followed the setting in [16]. Specifically, a single linear layer was added on top of each model and trained for 20 epochs. We adopted Adam optimizer and the learning rates for CODER, BGE and LINE were set to $\\bar{2}\\times10^{-5}$ , $2\\times\\bar{1}0^{-5}$ and $5\\times5\\times10^{-4}$ , respectively. The mean of the F1 scores calculated over five random initializations are reported in Table 3, and it can be observed that LINE achieves the best overall performance. ", "page_idx": 8}, {"type": "text", "text": "3.3.3 Renal cancer recurrence detection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The dataset used for this downstream task comes from Mass General Brigham and was previously curated, fully de-identified, and annotated for another ongoing project. Here, we briefly describe the curation and annotation process. First, a phenotyping algorithm [38] was applied to identify a pool of renal cell carcinoma (RCC) patients. From this pool, $300\\;\\mathrm{RCC}$ patients were randomly selected, and a clinician reviewed all notes to extract one diagnostic-relevant sentence per patient. Each sentence was annotated into one of three classes: uninformative of RCC recurrence, informative of past or current recurrence, and informative of no recurrence. We then used NILE [37] to extract clinical concepts from each sentence, excluding those where NILE failed to extract any concept. This resulted in a final set of 288 labeled sentences, each paired with a list of corresponding clinical concepts. The dataset comprises 136 uninformative sentences, 64 informative of recurrence, and 88 informative of no recurrence. ", "page_idx": 8}, {"type": "text", "text": "This classification task is designed to test how effectively proxy sentences, generated from concept lists, can detect recurrence information. Specifically, we aim to understand how well the proxy sentences perform in classifying uninformative, recurrence-informative, and non-recurrence-informative cases, simulating scenarios where only concept lists, not full sentences, are available. To this end, we additionally include classification results using raw sentences as a benchmark for expected optimal performance. We used LINE-projected CODER to embed the concept lists and LINE-projected GPT-4 or BGE to embed the raw sentences. A single linear layer was added on top of both models, and we fine-tuned it for 2000 epochs, with early stopping when the change in loss was less than $10^{-5}$ . We used Adam optimizer with a learning rate of $\\bar{10}^{-3}$ . The same setup was applied to all models for fair comparison. On average, the fine-tuning process took approximately one or two minutes per model. The average performance metrics from five-fold cross-validation are reported in Table 4. As shown, when raw sentences are available, GPT-4 and BGE achieve significantly better classification results than the other benchmarks. LINE-projected embeddings for GPT-4 and BGE manage to maintain a comparable level of classification performance. However, using only concept lists for GPT-4 and BGE results in a substantial drop in classification performance. Notably, adding the LINE projection reduces this performance gap, as shown in Table 5, supporting the effectiveness of the LINE module in better approximating sentence embeddings using concept lists. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces a teacher-teacher paradigm in which two pretrained LLMs align different forms of data to enable knowledge exchange and cross-form representation through a two-stage, few-epoch training of the LINE module, guided by a well-defined alignment objective. Our downstream analysis demonstrates that (1) the proposed LINE module effectively generates cross-form representations, and (2) alignment learning enhances performance for both models, even when one teacher model is more advanced than the other. ", "page_idx": 9}, {"type": "text", "text": "Although our primary focus is on a specific clinical use case, this teacher-teacher framework has broad potential applications. One promising application is enhancing the indexing and searchability of non-textual data. For example, medical images (e.g., CT or MRI scans) are often accompanied by clinical notes from which clinical concepts can be extracted. By segmenting images into regions and associating them with clinical concepts, we create a concept-based index of image content. The teacher-teacher framework aligns each image segment with its concept list, allowing the concept list to serve as a proxy representation for image content. This enables concept- or keyword-based searches for specific image segments via embeddings, making non-textual data more searchable. The framework also supports alignment between modalities without direct pairing. For example, CT and MRI scans are seldom captured together for the same patient, but clinical notes serve as a potential intermediary. In this scenario, clinical concepts extracted from notes are embedded by a pretrained teacher model. Separate teacher models then embed CT and MRI images. The alignment objective splits into two tasks: aligning clinical concepts with CT images and aligning clinical concepts with MRI images. This dual alignment establishes an indirect connection between CT and MRI images, creating a unified representation without direct pairing. These examples illustrate the framework\u2019s broad applicability in aligning disparate modalities, supporting cross-form searches and enhancing the utility of diverse data sources. ", "page_idx": 9}, {"type": "text", "text": "Our clinical use case further demonstrates the practical benefits of this framework in improving patient care and data security. By reducing the need for direct access to clinical notes, the framework minimizes privacy concerns and strengthens data protection. Additionally, it facilitates the integration of up-to-date LLMs, improving performance in tasks essential to patient care and potentially leading to better outcomes. The framework\u2019s use of external, factual knowledge also reduces misinformation risks, fostering trust in AI-assisted clinical tools. ", "page_idx": 9}, {"type": "text", "text": "Despite its strengths, the framework has two notable limitations. First, its reliance on paired data may limit its applicability in situations where aligned datasets are scarce. Although intermediaries can help bridge certain gaps, the framework\u2019s effectiveness remains constrained in cases where no suitable intermediaries are available. Second, the framework requires access to a comprehensive knowledge graph for effective training. In cases where such resources are unavailable or incomplete, model performance and generalizability may be impacted. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are deeply grateful to the area chair and the anonymous reviewers for their valuable comments, which greatly improved the quality of this paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019. [4] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019. [5] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and St\u00e9phane Lathuili\u00e8re. One-shot unsupervised domain adaptation with personalized diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 698\u2013708, 2023. [6] Olivier Bodenreider. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl_1):D267\u2013D270, 2004. [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020. [8] Yoav Goldberg. Assessing bert\u2019s syntactic abilities. arXiv preprint arXiv:1901.05287, 2019. [9] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021.   \n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[11] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023.   \n[12] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240, 2020.   \n[13] Hang Li, Yu Kang, Tianqiao Liu, Wenbiao Ding, and Zitao Liu. Ctal: Pre-training cross-modal transformer for audio-and-language representations. arXiv preprint arXiv:2109.00181, 2021.   \n[14] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. Self-alignment pretraining for biomedical entity representations. arXiv preprint arXiv:2010.11784, 2020.   \n[15] Ben J Marafino, Miran Park, Jason M Davies, Robert Thombley, Harold S Luft, David C Sing, Dhruv S Kazi, Colette DeJong, W John Boscardin, Mitzi L Dean, et al. Validation of prediction models for critical care outcomes using natural language processing of electronic health record data. JAMA network open, 1(8):e185097\u2013e185097, 2018.   \n[16] George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen, and Alexander Wong. Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus. arXiv preprint arXiv:2010.10391, 2020.   \n[17] Niklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022.   \n[18] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.   \n[19] Matthew E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. arXiv preprint arXiv:1808.08949, 2018.   \n[20] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.   \n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[22] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[23] Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. arXiv preprint arXiv:2004.09813, 2020.   \n[24] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-Schuler, and Christopher G Chute. Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation and applications. Journal of the American Medical Informatics Association, 17(5):507\u2013513, 2010.   \n[25] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823, 2015.   \n[26] Seyedmostafa Sheikhalishahi, Riccardo Miotto, Joel T Dudley, Alberto Lavelli, Fabio Rinaldi, Venet Osmani, et al. Natural language processing of clinical notes on chronic diseases: systematic review. JMIR medical informatics, 7(2):e12239, 2019.   \n[27] Amber Stubbs, Christopher Kotfila, and \u00d6zlem Uzuner. Automated systems for the deidentification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1. Journal of biomedical informatics, 58:S11\u2013S19, 2015.   \n[28] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[29] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316, 2019.   \n[30] Fuchiang R Tsui, Lingyun Shi, Victor Ruiz, Neal D Ryan, Candice Biernesser, Satish Iyengar, Colin G Walsh, and David A Brent. Natural language processing and machine learning of electronic health records for prediction of first-time suicide attempts. JAMIA open, 4(1):ooab011, 2021.   \n[31] \u00d6zlem Uzuner, Yuan Luo, and Peter Szolovits. Evaluating the state-of-the-art in automatic de-identification. Journal of the American Medical Informatics Association, 14(5):550\u2013563, 2007.   \n[32] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.   \n[33] Qian Wang, Fanlin Meng, and Toby P Breckon. Data augmentation with norm-vae for unsupervised domain adaptation. arXiv preprint arXiv:2012.00848, 2020.   \n[34] Xiaohan Wang, Linchao Zhu, and Yi Yang. T2vlad: global-local sequence alignment for textvideo retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5079\u20135088, 2021.   \n[35] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597, 2023.   \n[36] Gokul Yenduri, M Ramalingam, G Chemmalar Selvi, Y Supriya, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, G Deepti Raj, Rutvij H Jhaveri, B Prabadevi, Weizheng Wang, et al. Gpt (generative pre-trained transformer)\u2013a comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions. IEEE Access, 2024.   \n[37] Sheng Yu, Tianrun Cai, and Tianxi Cai. Nile: fast natural language processing for electronic health records. arXiv preprint arXiv:1311.6063, 2013.   \n[38] Sheng Yu, Yumeng Ma, Jessica Gronsbell, Tianrun Cai, Ashwin N Ananthakrishnan, Vivian S Gainer, Susanne E Churchill, Peter Szolovits, Shawn N Murphy, Isaac S Kohane, et al. Enabling phenotypic big data with phenorm. Journal of the American Medical Informatics Association, 25(1):54\u201360, 2018.   \n[39] Zheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao Li, Fei Wang, and Sheng Yu. Coder: Knowledgeinfused cross-lingual medical term embedding for term normalization. Journal of biomedical informatics, 126:103983, 2022.   \n[40] Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061\u2013 74071, 2018.   \n[41] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Handling negation in clinical concepts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we provide additional details on how we handle concepts that are negatively mentioned. For example, consider the sentence, \"There is no focal consolidation, pleural effusion, or pneumothorax.\" The terms \"focal consolidation\", \"pleural effusion\", and \"pneumothorax\" are stated as negative, indicating their absence. This negation significantly alters the sentence\u2019s meaning compared to a positive assertion, such as \"There is focal consolidation, pleural effusion, or pneumothorax.\" Therefore, these negative concepts must be treated differently. To address this, we propose creating a dictionary of negative concepts derived from the original positive concepts, treating them as distinct entities. For example, the positive concept \"pneumothorax\" would have a corresponding negative \"concept pneumothorax unobserved\". Since the Unified Medical Language System (UMLS) only includes relations for positive concepts, we update the negative concepts by first using the multihead graph attention module, as described in Section 2.3, to update their corresponding positive concepts. The negative concept embeddings then pass through a projection layer. To ensure differentiation, we introduce a loss function to maintain a cosine similarity between the updated negative concept and its corresponding positive concept below a predefined threshold, $\\delta$ . For each positive-negative concept pair, denoted as $\\left(\\mathbf{c}_{p},\\mathbf{c}_{n}\\right)$ , the corresponding loss is calculated as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\log{\\frac{e^{\\delta-\\cos(\\mathbf{c}_{p},\\mathbf{c}_{n})}}{1+e^{\\delta-\\cos(\\mathbf{c}_{p},\\mathbf{c}_{n})}}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $0<\\delta\\leq0.5$ . This loss function is implemented throughout our two-stage training process. ", "page_idx": 13}, {"type": "text", "text": "A.2 Information recovered from residuals ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Table A1, we present some examples of crucial clinical concepts that remain undetected due to misspellings but can be subsequently recovered in the first stage of our training process. ", "page_idx": 13}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/d16c411b9e21194827b8077f8d5e6076b5d603e6494cd479d43606bacc8d9fc5.jpg", "table_caption": ["Table A1: Examples of crucial clinical concepts that remain undetected due to misspellings but can be subsequently recovered through our two-stage training process. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/1953a343def1dcba238d96d6417593e633e79cb137aa9c4dfc90b5db9eacbe57.jpg", "table_caption": ["Table A2: Further results for Table 1. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/11f316f604cb3894005810aa974859f74ce40ace64612b252b8d40732cf476c5.jpg", "table_caption": ["Table A3: Further results for Table 4. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Comparison with related literature ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this subsection, we compare the proposed teacher-teacher framework with Retrieval-Augmented Generation (RAG), a method that combines large language models with knowledge databases to improve generation quality. A key distinguishing feature of our framework is that, rather than directly incorporating a knowledge database into Teacher 1 to enhance generation, we introduce a second generative model, Teacher 2, which is inherently endowed with relational knowledge. Teacher 2 then transfers this knowledge to Teacher 1 through alignment learning. This structural difference also supports an additional objective of our framework: enabling Teacher 2 to generate a purely concept-based embedding that can serve as a proxy for the clinical text, i.e. cross-form generation. ", "page_idx": 14}, {"type": "text", "text": "A.4 Further experiment results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We include two additional benchmarks to our validation task on the alignment objective (Section 3.2) and the renal cancer recurrence detection task (Section 3.3.3), as both tasks utilize two teacher models. Specifically, we projected the CODER embeddings onto the BGE embedding space (denoted as \"CODER $\\rightarrow\\!\\mathrm{BGE^{\\prime\\prime}}$ ) and also performed the reverse projection from BGE to CODER (denoted as \"BGE $\\rightarrow$ CODER\") using a projection matrix. Compared to the two benchmarks, the proposed LINE model achieves better alignment and reduces the performance gap between using sentences and only concepts to detect recurrence information. ", "page_idx": 14}, {"type": "text", "text": "Additionally, we report the standard deviation of the F1 scores over five random initializations on two i2b2 datasets in Table A4 ", "page_idx": 14}, {"type": "text", "text": "A.5 Computational efficiency ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To demonstrate the computational efficiency of the proposed framework, using our training data, we compare the increase in training time over several training epochs for the following three approaches: (1) directly fine-tuning BGE or CODER, (2) fine-tuning with low-rank adaptation (LoRA [10]), and (3) training the LINE module while keeping the pretrained LLMs frozen. The computation time was estimated using the tqdm function on a single NVIDIA RTX 8000 GPU with 48GB of VRAM. The rough estimates, shown in Figure A1, indicate that even with LoRA, training large models like BGE takes several days per epoch on a single GPU, with increasing computational overhead as training progresses. In contrast, our proposed LINE module requires only 2 hours per epoch on the same hardware. ", "page_idx": 14}, {"type": "table", "img_path": "zDaD8zv8tG/tmp/7e7409cf8a3048acaac897df12259cc21582b40631a6b60b4a027b2198e185c4.jpg", "table_caption": ["Table A4: The mean of the F1 scores for different models over five random initializations on two i2b2 datasets. The results for BioBERT, Clinical BioBERT and UmlsBERT were directly copied from [16] for comparison. Here, the LINE projection is applied to the token-level BGE embeddings. The best result under each metric is highlighted in bold font. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "zDaD8zv8tG/tmp/9e0adcfcd35b91e7f3fd01ca92b94b2bb301dac1fc9f1f79fb45437311a70de9.jpg", "img_caption": ["Figure A1: Computational Time Comparison between LINE and Baselines "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We summarized our main contributions and scope in both the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We have discussed our limitations in the discussion section. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not contain any theoretical result. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We include the information needed to reproduce the main results. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: The code is not current available, but we will try to work on that in the near future. For the downstream tasks, we use both publicly available data as well as a dataset which is not publicly available. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We include all the experiment details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Standard errors are reported in some tables. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard error or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We include information about the computing resources. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We conform with the code of ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss our societal impact in the clinical domain in the introduction and discussion sections. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have not yet release either our model or the non-public dataset. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have made appropriate citations in places needed. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper has not released any new assets at the moment. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The only non-public dataset used was previously curated, fully de-identified and annotated for another ongoing project with an existing IRB approval. The dataset used in the paper has already been fully de-identified and annotated. Hence, no human subjects are involved. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The only non-public dataset used was previously curated, fully de-identified and annotated for another ongoing project with an existing IRB approval. The dataset used in the paper has already been fully de-identified and annotated. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]