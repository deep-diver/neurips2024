[{"type": "text", "text": "Noise Balance and Stationary Distribution of Stochastic Gradient Descent ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 How the stochastic gradient descent (SGD) navigates the loss landscape of a neu  \n2 ral network remains poorly understood. This work shows that the minibatch noise   \n3 of SGD regularizes the solution towards a noise-balanced solution whenever the   \n4 loss function contains a rescaling symmetry. We prove that when the rescaling   \n5 symmetry exists, the SGD dynamics is limited to only a low-dimensional sub  \n6 space and prefers a special set of solutions in an infinitely large degenerate man  \n7 ifold, which offers a partial explanation of the effectiveness of SGD in training   \n8 neural networks. We then apply this result to derive the stationary distribution   \n9 of stochastic gradient flow for a diagonal linear network with arbitrary depth and   \n10 width, which is the first analytical expression of the stationary distribution of SGD   \n11 in a high-dimensional non-quadratic potential. The stationary distribution exhibits   \n12 complicated nonlinear phenomena such as phase transitions, loss of ergodicity,   \n13 memory effects, and fluctuation inversion. These phenomena are shown to exist   \n14 uniquely in deep networks, highlighting a fundamental difference between deep   \n15 and shallow models. Lastly, we discuss the implication of the proposed theory for   \n16 the practical problem of variational Bayesian inference. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 In natural and social sciences, one of the most important objects of study of a stochastic system is   \n19 its stationary distribution, which is often found to offer fundamental insights into understanding a   \n20 given stochastic process [36, 29]. Arguably, a great deal of insights into SGD can be obtained if we   \n21 have an analytical understanding of its stationary distribution, which remains unknown until today.   \n22 The stochastic gradient descent (SGD) algorithm is defined as $\\begin{array}{r}{\\Delta\\theta_{t}=-\\frac{\\eta}{S}\\sum_{x\\in B}\\nabla_{\\theta}\\ell\\big(\\theta,x\\big)}\\end{array}$ , where $\\theta$   \n23 is the model parameter and $\\ell(\\theta,x)$ is a per-sample loss whose expe ctatio n \u2211over $x$ gives the training   \n24 loss: $L(\\theta)\\,=\\,\\mathbb{E}_{x}[\\ell(\\theta,x)]$ . $B$ is a randomly sampled minibatch of data points, each independently   \n25 sampled from the training set, and $S$ is the minibatch size. Two aspects of the algorithm make it   \n26 difficult to understand this algorithm: (1) its dynamics is discrete in time, and (2) the randomness is   \n27 highly nonlinear and parameter-dependent. This work relies on the continuous-time approximation   \n28 and deals with the second aspect. ", "page_idx": 0}, {"type": "text", "text": "29 The main contributions are ", "page_idx": 0}, {"type": "text", "text": "30 1. the derivation of the \u201claw of balance,\u201d which shows that SGD converges to a special subset of   \n31 noised-balanced solutions when the rescaling symmetry is present;   \n32 2. the first-of-its-kind solution of the stationary distribution of an analytical model trained by SGD;   \n33 3. discovery of novel phenomena such as phase transitions, loss of ergodicity, memory effects, and   \n34 fluctuation inversion, all implied by our theory.   \n35 Organization. The next section discusses the closely related works. In Section 3, we prove the   \n36 law of balance, the first main result of this work, and discuss its implications for common neural   \n37 networks. In Section 4, we apply the law of balance to derive the stationary distribution of SGD for   \n38 a highly nontrivial loss landscape. The last section concludes this work. All proofs and derivations   \n39 are given in Appendix A. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "40 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "41 Solution of the Fokker Planck (FP) Equation. The FP equation is a high-dimensional partial   \n42 differential equation whose solution (and its existence) is an open problem in mathematics and many   \n43 fields of sciences and only known for a few celebrated special cases [28]. Our solution is the first of   \n44 its kind in a deep-learning setting. Stationary distribution of SGD. One of the earliest works that   \n45 computes the stationary distribution of SGD is the Lemma 20 of Ref. [3], which assumes that the   \n46 noise has a constant covariance and shows that if the loss function is quadratic, then the stationary   \n47 distribution is Gaussian. Similarly, using a saddle point expansion and assuming that the noise is   \n48 parameter-independent, a series of recent works showed that the stationary distribution of SGD is   \n49 exponential in the model parameters close to a local minimum: $p(\\theta)\\,\\propto\\,\\dot{\\exp}[-a\\theta^{T}H\\theta]$ , for some   \n50 constant $a$ and matrix $H$ [21, 41, 19]. Assuming that the noise cov(ari)a n\u221dce onl[y \u2212depends ]on the loss   \n51 function value $L(\\theta)$ , Refs. [24] and [39] showed that the stationary distribution is power-law-like   \n52 and proportional t(o) $L(\\theta)^{-c_{0}}$ for some constant $c_{0}$ . A primary feature of these previous results is that   \n53 stationary distribution does not exhibit any memory effect and also preserves ergodicity. Until now,   \n54 no analytical solution to the stationary distribution of SGD is known, making it impossible to judge   \n55 how good the previous approximate results are. Our result is the first to derive an exact solution to   \n56 the stationary distribution of SGD without any approximation. We will see that in contrast to the   \n57 approximate solutions in the previous results, the actual distribution of SGD has both a memory   \n58 effect and features the loss of ergodicity.   \n59 Symmetry and SGD dynamics. Also related to our work is the study of how symmetry affects the   \n60 learning dynamics of SGD. A major prior work is [17], which studies the dynamics of SGD when   \n61 there is scale invariance, conjecturing that SGD reaches a fast equilibrium state at the early stage of   \n62 training. Our result is different as we study a different type of symmetry, the rescaling symmetry. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "63 3 Noise Balance ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 We consider the continuous-time limit of SGD [15, 16, 18, 32, 8, 11]: ", "page_idx": 1}, {"type": "equation", "text": "$$\nd\\theta=-\\nabla_{\\theta}L d t+\\sqrt{T C(\\theta)}d W_{t},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "65 where $C(\\theta)\\,=\\,\\mathbb{E}\\bigl[\\nabla\\ell(\\theta)\\nabla^{T}\\ell(\\theta)\\bigr]$ is the gradient covariance, $d W_{t}$ is a stochastic process satisfying   \n66 $d W_{t}\\sim N(0,I d t)$ and $\\mathbb{E}[d W_{t}d W_{t^{\\prime}}^{T}]=\\delta(t{-}t^{\\prime})I$ , and $T=\\eta/S$ . Apparently, $T$ gives the average noise   \n67 level  i\u223cn th(e dyna)mics. P[revious w o]r k=s h(av\u2212e s)uggested  t=hat/ the ratio $T$ is a main factor determining   \n68 the behavior of SGD, and using different $T$ often leads to different generalization performance   \n69 [31, 19, 44]. ", "page_idx": 1}, {"type": "text", "text": "70 3.1 Rescaling Symmetry and Law of Balance ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 Due to standard architecture designs, a type of invariance \u2013 the rescaling symmetry \u2013 often appears   \n72 in the loss function and it is preserved for all sampling of minibatches. The per-sample loss $\\ell$ is said   \n73 to have the rescaling symmetry for all $x$ if $\\ell(u,w,x)\\,=\\,\\ell\\,(\\lambda u,w/\\lambda,x)$ for a scalar $\\lambda\\in\\mathbb{R}_{+}$ . This   \n74 type of symmetry appears in many scenarios in deep learning. For example, it appears in any neural   \n75 network with the ReLU activation. It also appears in the self-attention of transformers, often in the   \n76 form of key and query matrices [37]. When this symmetry exists between $u$ and $w$ , one can prove   \n77 the following result, which we refer to as the law of balance.   \n78 Theorem 3.1. Let u, $w$ , and $v$ be parameters of arbitrary dimensions. Let $\\ell(u,w,v,x)$ satisfy   \n79 $\\ell(u,w,v,x)=\\ell(\\lambda u,w/\\lambda,v,x)$ for arbitrary $x$ and any $\\lambda\\in\\mathbb{R}_{+}$ . Then, ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(\\|u\\|^{2}-\\|w\\|^{2})=-T\\big(u^{T}C_{1}u-w^{T}C_{2}w\\big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "80 where $C_{1}\\,=\\,\\mathbb{E}[A^{T}A]-\\mathbb{E}[A^{T}]\\mathbb{E}[A]$ , $C_{2}\\,=\\,\\mathbb{E}[A A^{T}]-\\mathbb{E}[A]\\mathbb{E}[A^{T}]$ and $A_{k i}\\,=\\,\\partial\\tilde{\\ell}/\\partial\\bigl(u_{i}w_{k}\\bigr)$ with   \n81 $\\Tilde{\\ell}(u_{i}w_{k},v,x)\\equiv\\ell(u_{i},w_{k},v,x)$ .1   \n82 Here, $v$ stands for the parameters that are irrelevant to the symmetry, and $C_{1}$ and $C_{2}$ are positive   \n83 semi-definite by definition. The theorem still applies if the model has parameters other than $u$ and   \n84 $w$ . The theorem can be applied recursively when multiple rescaling symmetries exist. See Figure 1   \n85 for an illustration the the dynamics and how it differs from other types of GD.   \n86 While the matrices $C_{1}$ and $C_{2}$ may not always be full-rank, we   \n87 emphasize that in common deep-learning settings with rescal  \n88 ing symmetry, the law of balance is almost always well-defined   \n89 and applicable. In Appendix A.4, we prove that under very   \n90 general settings, for all active hidden neurons of a two-layer   \n91 ReLU net, $C_{1}$ and $C_{2}$ are always full-rank. Equation (2) is the   \n92 law of balance, and it implies two different types of balance.   \n93 The first type of balance is the balance of gradient noise. The   \n94 proof of the theorem shows that the stationary point of the law   \n95 in (2) is equivalent to ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Tr}_{\\boldsymbol{w}}[C(\\boldsymbol{w})]=\\mathrm{Tr}_{\\boldsymbol{u}}[C(\\boldsymbol{u})],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "96 where $C(w)$ and $C(u)$ are the gradient covariance of $w$ and   \n97 $u$ , respectively. Therefore, SGD prefers a solution where the   \n98 gradient noise between the two layers is balanced. Also, this   \n99 implies that the balance conditions of the law is only dependent   \n100 on the diagonal terms of the Fisher information (if we regard   \n101 the loss as a log probability), which is often well-behaved. As   \n102 a last caveat, we emphasize that the fact that the noise will   \n103 balance does not imply that either trace will converge or stay   \n104 close to a fixed value \u2013 it is also possible for both terms to   \n105 oscillate while their difference is close to zero.   \n106 The second type is the norm ratio balance between layers,   \n107 though the norm ratio may not necessarily be finite. Equation (2) implies that in the degenerate   \n108 direction of the rescaling symmetry, a single and unique point is favored by SGD. Let $u\\,=\\,\\lambda u^{*}$   \n109 and $w\\ =\\ \\lambda^{-1}w^{*}$ for arbitrary $u^{*}$ and $w^{*}$ , then, the stationary point of the law is reac h=ed at   \n110 $\\lambda^{4}\\,=\\,\\frac{(w^{*})^{T}C_{2}w^{*}}{(u^{*})^{T}C_{1}u^{*}}$ . The quantity $\\lambda$ can be called the \u201cbalancedness\u201d of the norm, and the law states   \n111 that when a rescaling symmetry exists, a special balancedness is preferred by the SGD algorithm.   \n112 When $C_{1}$ or $C_{2}$ vanishes, $\\lambda$ or $\\lambda^{-1}$ diverges, and so does SGD. Therefore, having a nonvanishing   \n113 noise actually implies that SGD training will be more stable. For common problems, $C_{1}$ and $C_{2}$   \n114 are positive definite and, thus, if we know the spectrum of $C_{1}$ and $C_{2}$ at the end of training, we can   \n115 estimate a rough norm ratio at convergence: ", "page_idx": 2}, {"type": "image", "img_path": "AM1znjeo9l/tmp/eb5448d697a761fbc2df86d8a7313b0e52831a21679036b80f2674c4da4831f0.jpg", "img_caption": ["Figure 1: Dynamics of GD and SGD and GD with injected Gaussian noise for the simple problem $\\begin{array}{r l}{\\ell(u,w)}&{{}=}\\end{array}$ $(u w x\\!-y)^{2}$ . Due to the rescaling symmetry between $u$ and $w$ , GD follows a conservation law: $u^{2}(t)\\mathrm{~-~}w^{2}(t)\\mathrm{~=~}$ $u^{2}(0)-w^{2}(0)$ , SGD converges to the balanced solution $u^{2}\\;\\;=\\;\\;w^{2}$ , while GD with injected noise diverges due to simple diffusion in the degenerate directions. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n-T(\\lambda_{1M}||u||^{2}-\\lambda_{2m}||w||^{2})\\leq\\frac{d}{d t}(||u||^{2}-||w||^{2})\\leq-T(\\lambda_{1m}||u||^{2}-\\lambda_{2M}||w||^{2}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "116 where $\\lambda_{1m(2m)}$ and $\\lambda_{1M(2M)}$ represent the minimal and maximal eigenvalue of the matrix $C_{1(2)}$ ,   \n117 respectively. Thefore, the value of $||u||^{2}/||w||^{2}$ is restricted by (See Section A.5) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{2m}}{\\lambda_{1M}}\\leq\\frac{\\|u\\|^{2}}{\\|w\\|^{2}}\\leq\\frac{\\lambda_{2M}}{\\lambda_{1m}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "118 Thus, a remaining question is whether the quantities $u^{T}C_{1}u$ and $w^{T}C_{2}w$ are generally well-defined   \n119 and nonvanishing or not. The following proposition shows that for a generic two-layer ReLU net,   \n120 $u^{T}C_{1}u$ and $w^{T}\\bar{C}_{2}w$ are almost everywhere strictly positive. We define a two-layer ReLU net as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x)=\\sum_{i}^{d}u_{i}\\mathrm{ReLU}(w_{i}^{T}x+b_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $u_{i}\\in\\mathbb{R}^{d_{u}},w_{i}\\in\\mathbb{R}^{d_{w}}$ and $b_{i}$ is a scalar with $i$ being the index of the hidden neuron. For each   \n122 $i$ , the mo d\u2208el has the  \u2208rescaling symmetry: $u_{i}\\to\\lambda u_{i}$ , $(\\bar{w_{i}},b_{i})\\to(\\lambda^{-1}w_{i},\\lambda^{-1}b_{i})$ . We thus apply the   \n123 law of balance to each neuron separately. Th e \u2192per-sa m(ple lo)s s \u2192fu(nction is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(\\theta,x)=\\|f(x)-y(x,\\epsilon)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 Here, $x$ has a full-rank covariance $\\Sigma_{x}$ , and $y\\,=\\,g(x)+\\epsilon$ for some function $g$ and $\\epsilon$ is a zero-mean   \n125 random vector independent of $x$ and have the full-rank covariance $\\Sigma_{\\epsilon}$ . The following theorem shows   \n126 that for this network, $C_{1}$ and $C_{2}$ are full rank unless the neuron is \u201cdead\u201d.   \n127 Theorem 3.2. Let the loss function be given in Eq. (6). Let $C_{1}^{(i)}$ and $C_{2}^{(i)}$ denote the corresponding   \n128 noise matrices of the $i$ -th neuron, and $p_{i}:=\\mathbb{P}(w_{i}^{T}x+b_{i}>0)$ . Then, $C_{1}^{(i)}$ and C2 $C_{2}^{(i)}$ are full-rank for   \n129 all i such that $p_{i}>0$ .   \n130 See Figure 2. We train a two-layer ReLU network with the number of neurons: $20\\to200\\to20$ .   \n131 The dataset is a synthetic data set, where $x$ is drawn from a normal distribution, and the labels:   \n132 $y=x+\\epsilon$ , for an independent Gaussian noise $\\epsilon$ with unit variance. While every neuron has a rescaling   \n133 s y=mm+etry, we focus on the overall rescaling symmetry between the two weight matrices. The norm   \n134 between the two layers reach a state of approximate balance \u2013 but not a precise balance. At the same   \n135 time, the model evolves during training towards a state where $u^{T}C_{1}u$ and $w^{T}C_{2}w$ are balanced.   \n136 Standard analysis shows that the difference between SGD and GD is of order $T^{2}$ per unit time step,   \n137 and it is thus often believed that SGD can be understood perturbatively through GD [11]. However,   \n138 the law of balance implies that the difference between GD and SGD is not perturbative. As long   \n139 as there is any level of noise, the difference between GD and SGD at stationarity is $O(1)$ . This   \n140 theorem also implies the loss of ergodicity, an important phenomenon in nonequilibrium physics   \n141 [26, 34, 22, 35], because not all solutions with the same training loss will be accessed by SGD with   \n142 equal probability. ", "page_idx": 2}, {"type": "image", "img_path": "AM1znjeo9l/tmp/1192922a701451f968cf824c1af1ccde10e5d1616e3711a244599c1f5a43d663.jpg", "img_caption": ["Figure 2: A two-layer ReLU network trained on a full-rank dataset. Left: because of the rescaling symmetry, the norms of the two layers are balanced approximately (but not exactly). Right: the first and second terms in Eq. (2). We see that both terms evolve towards a point where they exactly balance. In agreement with our theory, SGD training leads to an approximate norm balance and exact gradient noise balance. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "143 3.2 1d Rescaling Symmetry ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "144 The theorem greatly simplifies when both $u$ and $w$ are one-dimensional. ", "page_idx": 3}, {"type": "text", "text": "145 Corollary 3.3. If $u,w\\in\\mathbb{R},$ , then, $\\begin{array}{r}{\\frac{d}{d t}|u^{2}-w^{2}|=-T C_{0}|u^{2}-w^{2}|,}\\end{array}$ , where $\\begin{array}{r}{C_{0}=\\mathrm{Var}[\\frac{\\partial\\ell}{\\partial(u w)}]}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "146 Before we apply the theorem to study the stationary distributions, we stress the importance of this   \n147 balance condition. This relation is closely related to Noether\u2019s theorem [23, 1, 20]. If there is no   \n148 weight decay or stochasticity in training, the quantity $||u||^{2}\\!-\\!||w||^{2}$ will be a conserved quantity under   \n149 gradient flow [6, 14, 33], as is evident by taking the infinite $S$ limit. The fact that it monotonically   \n150 decays to zero at a finite $T$ may be a manifestation of some underlying fundamental mechanism. A   \n151 more recent result in Ref. [38] showed that for a two-layer linear network, the norms of two layers   \n152 are within a distance of order $\\bar{O}(\\eta^{-1})$ , suggesting that the norm of the two layers are balanced. Our   \n153 result agrees with Ref. [38] in this case, but our result is stronger because our result is nonperturba  \n154 tive, only relies on the rescaling symmetry, and is independent of the loss function or architecture   \n155 of the model. It is useful to note that when $L_{2}$ regularization with strength $\\gamma$ is present, the rate   \n156 of decay changes from $T C_{0}$ to $T C_{0}+\\gamma$ . This points to a nice interpretation that when rescaling   \n157 symmetry is present, the implicit bia s of SGD is equivalent to weight decay. See Figure 1 for an   \n158 illustration of this point.   \n159 Example: two-layer linear network. It is instructive to illustrate the application of the law to   \n160 a two-layer linear network, the simplest model that obeys the law. Let $\\theta\\;=\\;(w,u)$ denote the set   \n161 of trainable parameters; the per-sample loss is $\\begin{array}{r}{\\ell(\\theta,x)\\,=\\,\\bigl(\\sum_{i}^{d}u_{i}w_{i}x-y\\bigr)^{2}+\\gamma\\|\\theta\\|^{2}}\\end{array}$ . Here, $d$ is the   \n162 width of the model, $\\gamma||\\theta||^{2}$ is the $L_{2}$ regularizati(on te)r =m  (w\u2211ith streng t\u2212h $\\gamma\\ge0$ , \u2223\u2223an\u2223\u2223d $\\mathbb{E}_{x}$ denotes the   \n163 averaging over the trai\u2223n\u2223i\u2223n\u2223g set, which could be a continuous distribution  o\u2265r a discrete sum of delta   \n164 distributions. It will be convenient for us also to define the shorthand: $\\begin{array}{r}{\\boldsymbol{v}:=\\sum_{i}^{d}u_{i}\\boldsymbol{w}_{i}}\\end{array}$ . The distribution   \n165 of $v$ is said to be the distribution of the \u201cmodel.\u201d Applying the law of bala n\u2211ce, we obtain that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\big(u_{i}^{2}-w_{i}^{2}\\big)=-4\\big[T\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big)+\\gamma\\big]\\big(u_{i}^{2}-w_{i}^{2}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "166 where we have introduced the parameters ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{1}:=\\mathrm{Var}[x^{2}],\\quad\\alpha_{2}:=\\mathbb{E}[x^{3}y]-\\mathbb{E}[x^{2}]\\mathbb{E}[x y],\\quad\\alpha_{3}:=\\mathrm{Var}[x y].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 When $\\alpha_{1}\\alpha_{3}-\\alpha_{2}^{2}$ or $\\gamma>0$ , the time evolution of $|u^{2}-w^{2}|$ can be upper-bounded by an exponentially   \n168 decreasing f unction in time: $|u_{i}^{2}-w_{i}^{2}|(t)\\,<|u_{i}^{2}-w_{i}^{2}|(0)\\exp\\left(-4T(\\alpha_{1}\\alpha_{3}-\\alpha_{2}^{2})t/\\alpha_{1}-4\\gamma t\\right)\\,\\to\\,0$ .   \n169 Namely, the quantity $(u_{i}^{2}\\mathrm{~-~}w_{i}^{2})$ deca ys to 0 w ith pr obability 1. We thus have $u_{i}^{2}\\ =\\ w_{i}^{2}$ for all   \n170 $i\\in\\{1,\\dot{\\cdots},d\\}$ at station a(rity , \u2212in a g)reement with the Corollary. ", "page_idx": 4}, {"type": "text", "text": "171 4 Stationary Distribution of SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "172 As an important application of the law of balance, we solve the stationary distribution of SGD   \n173 for a deep diagonal linear network. While linear networks are limited in expressivity, their loss   \n174 landscape and dynamics are highly nonlinear and exhibits many shared phenomenon with nonlinear   \n175 neural networks [13, 30]. Let $\\theta$ follow the high-dimensional Wiener process given by Eq.(1). The   \n176 probability density evolves according to its Kolmogorov forward (Fokker-Planck) equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}p(\\theta,t)=-\\sum_{i}\\frac{\\partial}{\\partial_{\\theta_{i}}}\\left(p(\\theta,t)\\frac{\\partial}{\\partial_{\\theta_{i}}}L(\\theta)\\right)+\\frac{1}{2}\\sum_{i,j}\\frac{\\partial^{2}}{\\partial_{\\theta_{i}}\\partial_{\\theta_{j}}}C_{i j}(\\theta)p(\\theta,t).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 The solution of this partial differential equation is an open problem for almost all high-dimensional   \n178 problems. This section solves it for a high-dimensional non-quadratic potential of a machine learn  \n179 ing relevance. ", "page_idx": 4}, {"type": "text", "text": "180 4.1 Depth-0 Case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 Let us first derive the stationary distribution of a one-dimensional linear regressor, which will be a   \n182 basis for comparison to help us understand what is unique about having a \u201cdepth\u201d in deep learning.   \n183 The per-sample loss is $\\ell(x,^{\\cdot}v)=(v x-y)^{2}+\\gamma v^{2}$ . Defining ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta_{1}:=\\mathbb{E}[x^{2}],\\quad\\beta_{2}:=\\mathbb{E}[x y],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 the global minimizer of the loss can be written as: $v^{*}\\,=\\,\\beta_{2}/\\beta_{1}$ . The gradient variance is also not   \n185 trivial: $C(v)\\;:=\\;\\mathrm{Var}[\\nabla_{v}\\ell(v,x)]\\;=\\;4\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big)$ ./ Note that the loss landscape $L$ only   \n186 depends on $\\beta_{1}$ and $\\beta_{2}$ , and the gradient noise only depends on $\\alpha_{1},\\alpha_{2}$ and, $\\alpha_{3}$ . It is thus reasonable   \n187 to call $\\beta$ the landscape parameters and $\\alpha$ the noise parameters. Both $\\beta$ and $\\alpha$ appear in all stationary   \n188 distributions, implying that the stationary distributions of SGD are strongly data-dependent. Another   \n189 relevant quantity is $\\Delta:=\\operatorname*{min}_{v}C(v)\\geq0$ , which is the minimal level of noise on the landscape. It   \n190 turns out that the statio\u2236n=ary distrib(ut)i o\u2265n is qualitatively different for $\\Delta=0$ and for $\\Delta>0$ . For all the   \n191 examples in this work, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta=\\mathrm{Var}[x^{2}]\\mathrm{Var}[x y]-\\mathrm{cov}(x^{2},x y)=\\alpha_{1}\\alpha_{3}-\\alpha_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 When is $\\Delta$ zero? It happens when, for all samples of $(x,y)$ , $x y+c=k x^{2}$ for some constant $k$ and   \n193 $c$ . We focus on the case $\\Delta>0$ in the main text, which  i(s mo)st lik e+ly  t=he case for practical situations.   \n194 The other cases are dealt with in Section A. ", "page_idx": 4}, {"type": "text", "text": "195 For $\\Delta>0$ , the stationary distribution for linear regression is (Section A) ", "page_idx": 4}, {"type": "equation", "text": "$$\np(v)\\propto\\left(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\right)^{-1-\\frac{\\beta_{1}^{\\prime}}{2T\\alpha_{1}}}\\exp\\left[-\\frac{1}{T}\\frac{\\alpha_{2}\\beta_{1}^{\\prime}-\\alpha_{1}\\beta_{2}}{\\alpha_{1}\\sqrt{\\Delta}}\\arctan\\left(\\frac{\\alpha_{1}v-\\alpha_{2}}{\\sqrt{\\Delta}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "196 in agreement with the previous result [24]. Two notable features exist for this distribution: (1)   \n197 the power exponent for the tail of the distribution depends on the learning rate and batch size, and   \n198 (2) the integral of $p(v)$ converges for an arbitrary learning rate. On the one hand, this implies that   \n199 increasing the learning rate alone cannot introduce new phases of learning to a linear regression; on   \n200 the other hand, it implies that the expected error is divergent as one increases the learning rate (or   \n201 the feature variation), which happens at $T=\\beta_{1}^{\\prime}/\\alpha_{1}$ . We will see that deeper models differ from the   \n202 single-layer model in these two crucial aspects. ", "page_idx": 4}, {"type": "text", "text": "203 4.2 An Analytical Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "204 Now, we consider the following model with a notion of depth and width; its loss function is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell=\\left[\\sum_{i}^{d_{0}}\\left(\\prod_{k=0}^{D}u_{i}^{(k)}\\right)x-y\\right]^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "AM1znjeo9l/tmp/4f81611f91a8856066004ccd0e5df382fa8f43de8a2dd93be64fab0cc9732dc1.jpg", "img_caption": ["Figure 3: Stationary distributions of SGD for simple linear regression $(D\\ =\\ 0)$ ), and a two-layer network $\\(D=1)$ ) across different $T=\\eta/S$ : $T=0.05$ (left) and $T=0.5$ (Mid). We see that for $D=1$ , the stationary distribution is strongly affected by the choice of the learning rate. In contrast, for $\\textit{D}=\\mathrm{~0~}$ , the stationary distribution is also centered at the global minimizer of the loss function, and the choice of the learning rate only affects the thickness of the tail. Right: the stationary distribution of a one-layer tanh-model, $f(x)=\\operatorname{tanh}(v x)$ $\\left[D=0\\right]$ ) and a two-layer tanh-model $f(x)=w\\operatorname{tanh}(u x)$ $(D=1)$ ). For $D=1$ , we define $v:=w u$ . The vertical line shows the ground truth. The deeper model never learns the wrong sign of $w u$ , whereas the shallow model can learn the wrong one. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "205 where $D$ can be regarded as the depth and $d_{0}$ the width. When the width $d_{0}=1$ , the law of balance is   \n206 sufficient to solve the model. When $d_{0}>1$ , we need to eliminate additiona l degrees of freedom. We   \n207 note that this model conceptually rese mbles (but not identical to) a diagonal linear network, which   \n208 has been found to well approximate the dynamics of real networks [27, 25, 2, 7].   \n209 We introduce $\\begin{array}{r}{v_{i}:=\\prod_{k=0}^{D}u_{i}^{(k)}}\\end{array}$ , and so $v=\\Sigma_{i}\\,v_{i}$ , where we call $v_{i}$ a \u201csubnetwork\u201d and $v$ the \u201cmodel.\u201d   \n210 The following p r\u2236o= p\u220fosition shows that i n=d e\u2211pendent of $d_{0}$ and $D$ , the dynamics of this model can be   \n211 reduced to a one-dimensional form by invoking the law of balance.   \n212 Theorem 4.1. For all $i\\neq j$ , one (or more) of the following conditions holds for all trajectories at   \n213 stationarity: $(I)$ $v_{i}=0$ ,  o\u2260r $v_{j}\\,=\\,0$ , or $L(\\theta)=0$ ; (2) $\\operatorname{sgn}(v_{i})=\\operatorname{sgn}(v_{j})$ . In addition, $(2a)$ if $D=1$ ,   \n214 for a constant $c_{\\mathrm{0}}$ , $\\log|v_{i}|-\\log|v_{j}|=c_{0}$ ; $(2b)$ if $D>1$ , $|v_{i}|^{2}-|v_{j}|^{2}=0$ .   \n215 This theorem contains many interesting aspects. First of all, the three situations in item 1 directly   \n216 tell us the distribution of $v$ if the initial state of of $v$ is given by these conditions.2 This implies a   \n217 memory effect, namely, that the stationary distribution of SGD can depend on its initial state. The   \n218 second aspect is the case of item 2, which we will solve below. Item 2 of the theorem implies that all   \n219 the $v_{i}$ of the model must be of the same sign for any network with $D\\geq1$ . Namely, no subnetwork   \n220 of the original network can learn an incorrect sign. This is dramatically different from the case of   \n221 $D=0$ . We will discuss this point in more detail below. The third interesting aspect of the theorem is   \n222 that it implies that the dynamics of SGD is qualitatively different for different depths of the model.   \n223 In particular, $D\\,=\\,1$ and $D\\,>\\,1$ have entirely different dynamics. For $D\\,=\\,1$ , the ratio between   \n224 every pair of $v_{i}$ and $v_{j}$ is a conserved quantity. In sharp contrast, for $D>1$ , the distance between   \n225 different $v_{i}$ is no longer conserved but decays to zero. Therefore, a new ba l>ancing condition emerges   \n226 as we increase the depth. Conceptually, this qualitative distinction also corroborates the discovery   \n227 in Ref. [43], where $D=1$ models are found to be qualitatively different from models with $D>1$ .   \n228 With this theorem, we are ready to solve the stationary distribution. It suffices to condition on the   \n229 event that $v_{i}$ does not converge to zero. Let us suppose that there are $d$ nonzero $v_{i}$ that obey item   \n230 2 of Theorem 4.1 and $d$ can be seen as an effective width of the model. We stress that the effective   \n231 width $d\\leq d_{0}$ depends on the initialization and can be arbitrary.3 Therefore, we condition on a fixed   \n232 value of $d$ to solve for the stationary distribution of $v$ (Appendix A):   \n233 Theorem 4.2. Let $\\delta(x)$ denote the Dirac delta fun\u2217ction. For an arbitrary factor $z\\ i n[0,1]$ , an   \n234 invariant solution of t(he) Fokker-Planck Equation is $p^{*}(v)=(1-z)\\delta(v)+z{\\overset{*}{p_{\\pm}}}(v),$ , where ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\pm}(|v|)\\propto\\frac{1}{|v|^{3(1-1/(D+1))}g_{\\mp}(v)}\\exp\\left(-\\frac{1}{T}\\int_{0}^{|v|}d|v|\\frac{d^{1-2/(D+1)}(\\beta_{1}|v|\\mp\\beta_{2})}{(D+1)|v|^{2D/(D+1)}g_{\\mp}(v)}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "235 where $p_{-}$ is the distribution on $(-\\infty,0)$ and $p_{+}$ is that on $(0,\\infty)$ , and $g_{\\mp}(v)=\\alpha_{1}|v|^{2}\\mp2\\alpha_{2}|v|+\\alpha_{3}$ . ", "page_idx": 5}, {"type": "text", "text": "236 The arbitrariness of the scalar $z$ is due to the memory effect of SGD \u2013 if all parameters are initialized   \n237 at zero, they will remain there with probability 1. This means that the stationary distribution is not   \n238 unique. Since the result is symmetric in the sign of $\\beta_{2}\\,=\\,\\mathbb{E}[x y]$ , we assume that $\\mathbb{E}[x y]>0$ from   \n239 now on. ", "page_idx": 6}, {"type": "text", "text": "240 Also, we focus on the case $\\gamma=0$ in the main text.4The distribution of $v$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\pm}(|v|)\\propto\\frac{|v|^{\\pm\\beta_{2}/2\\alpha_{3}T-3/2}}{(\\alpha_{1}|v|^{2}\\mp2\\alpha_{2}|v|+\\alpha_{3})^{1\\pm\\beta_{2}/4T\\alpha_{3}}}\\exp\\left(-\\frac{1}{2T}\\frac{\\alpha_{3}\\beta_{1}-\\alpha_{2}\\beta_{2}}{\\alpha_{3}\\sqrt{\\Delta}}\\arctan\\frac{\\alpha_{1}|v|\\mp\\alpha_{2}}{\\sqrt{\\Delta}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "241 This measure is worth a close examination. First, the exponential term is upper and lower bounded   \n242 and well-behaved in all situations. In contrast, the polynomial term becomes dominant both at   \n243 infinity and close to zero. When $v<0$ , the distribution is a delta function at zero: $p(v)=\\delta(v)$ . To   \n244 see this, note that the term $v^{-\\beta_{2}/2\\alpha_{3}T-3/2}$ integrates to give $v^{-\\beta_{2}/2\\alpha_{3}T-1/2}$ close to th(e o)r i=gin(, w)hich   \n245 is infinite. Away from the origin, the integral is finite. This signals that the only possible stationary   \n246 distribution has a zero measure for $\\upsilon\\,\\neq\\,0$ . The stationary distribution is thus a delta distribution,   \n247 meaning that if $x$ and $y$ are positivel y\u2260 correlated, the learned subnets $v_{i}$ can never be negative,   \n248 independent of the initial configuration.   \n249 For $v>0$ , the distribution is nontrivial. Close to $v=0$ , the distribution is dominated by $v^{\\beta_{2}/2\\alpha_{3}T-3/2}$ ,   \n250 which > integrates to $v^{\\beta_{2}/2\\alpha_{3}T-1/2}$ . It is only fini t=e below a critical $T_{c}\\;=\\;\\beta_{2}/\\alpha_{3}$ . This is a phase  \n251 transition-like behavior. As $T\\rightarrow(\\beta_{2}/\\alpha_{3})_{-}$ , the integral diverges and t e=nds t/o a delta distribution.   \n252 Namely, if $T>T_{c}$ , we have $u_{i}\\,=\\,w_{i}\\,=\\,0$ )for all $i$ with probability 1, and no learning can happen.   \n253 If $T<T_{c}$ , the stationary distrib ution  has a finite variance, and learning may happen. In the more   \n254 gene r<al setting, where weight decay is present, this critical $T$ shifts to $\\begin{array}{r}{T_{c}\\ =\\ \\frac{\\beta_{2}-\\gamma}{\\alpha_{3}}}\\end{array}$ . When $T\\,=\\,0$ ,   \n255 the phase transition occurs at $\\beta_{2}\\,=\\,\\gamma$ , in agreement with the threshold weight decay identified in   \n256 Ref. [45]. See Figure 3 for illust r=ations of the distribution across different values of $T$ . We also   \n257 compare with the stationary distribution of a depth-0 model. Two characteristics of the two-layer   \n258 model appear rather striking: (1) the solution becomes a delta distribution at the sparse solution   \n259 $u=w=0$ at a large learning rate; (2) the two-layer model never learns the incorrect sign ( $\\stackrel{.}{v}$ is always   \n260 non-negative). Another exotic phenomenon implied by the result is what we call the \u201cfluctuation   \n261 inversion.\u201d Naively, the variance of model parameters should increase as we increase $T$ , which is the   \n262 noise level in SGD. However, for the distribution we derived, the variance of $v$ and $u$ both decrease   \n263 to zero as we increase $T$ : injecting noise makes the model fluctuation vanish. We discuss more about   \n264 this \u201cfluctuation inversion\u201d in the next section.   \n265 Also, while there is no other phase-transition behavior below $T_{c}$ , there is still an interesting and   \n266 practically relevant crossover behavior in the distribution of the parameters as we change the learn  \n267 ing rate. When training a model, The most likely parameter we obtain is given by the maximum   \n268 likelihood estimator of the distribution, $\\hat{v}:=\\arg\\operatorname*{max}p(v)$ . Understanding how $\\hat{v}(T)$ changes as a   \n269 function of $T$ is crucial. This quantity als o\u2236 =exhibits non(tri)vial crossover behaviors( at )critical values   \n270 of $T$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "271 When $T<T_{c}$ , a nonzero maximizer for $p(v)$ must satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\nv^{*}=-\\frac{\\beta_{1}-10\\alpha_{2}T-\\sqrt{(\\beta_{1}-10\\alpha_{2}T)^{2}+28\\alpha_{1}T(\\beta_{2}-3\\alpha_{3}T)}}{14\\alpha_{1}T}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "272 The existence of this solution is nontrivial, which we analyze in Appendix A.8. When $T\\,\\rightarrow\\,0$ , a   \n273 solution always exists and is given by $v\\,=\\,\\beta_{2}/\\beta_{1}$ , which does not depend on the learning rate or   \n274 noise $C$ . Note that $\\beta_{2}/\\beta_{1}$ is also the min i=mum/ point of $L(u_{i},w_{i})$ . This means that SGD is only a   \n275 consistent estimator of/ the local minima in deep learning i(n the v)anishing learning rate limit. How   \n276 biased is SGD at a finite learning rate? Two limits can be computed. For a small learning rate, the   \n277 leading order correction to the solution is $\\begin{array}{r}{v=\\frac{\\beta_{2}}{\\beta_{1}}+\\left(\\frac{10\\alpha_{2}\\beta_{2}}{\\beta_{1}^{2}}-\\frac{7\\alpha_{1}\\beta_{2}^{2}}{\\beta_{1}^{3}}-\\frac{3\\alpha_{3}}{\\beta_{1}}\\right)T}\\end{array}$ . This implies that the   \n278 common Bayesian analysis that relies on a Lapl ace expansion of the loss  fluctuation around a local   \n279 minimum is improper. The fact that the stationary distribution of SGD is very far away from the   \n280 Bayesian posterior also implies that SGD is only a good Bayesian sampler at a small learning rate.   \n281 Example. It is instructive to consider an example of a structured dataset: $y\\,=\\,k x+\\epsilon\\,$ , where $x\\sim$   \n$\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})$ ", "page_idx": 6}, {"type": "text", "text": "282 ${\\mathcal{N}}(0,1)$ and the noise $\\epsilon$ obeys . We let $\\gamma=0$ for simplicity. If $\\begin{array}{r}{\\sigma^{2}>\\frac{8}{21}k^{2}}\\end{array}$ , there alwa y\u223cs ", "page_idx": 6}, {"type": "text", "text": "283 exists a transitional learning rate: $\\begin{array}{r}{T^{*}=\\frac{4k+\\sqrt{42}\\sigma}{4(21\\sigma^{2}-8k^{2})}}\\end{array}$ . Obviously, $T_{c}/3<T^{*}$ . One can characterize the   \n284 learning of SGD by comparing $T$ with $T_{c}$ and $T^{*}$ . For this simple example, SGD can be classified   \n285 into roughly 5 different regimes. See Figure 4. ", "page_idx": 7}, {"type": "text", "text": "286 4.3 Power-Law Tail of Deeper Models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "287 An interesting aspect of the depth-1 model is that its distri  \n288 bution is independent of the width $d$ of the model. This is   \n289 not true for a deep model, as seen from Eq. (14). The $d\\!\\cdot$ -   \n290 dependent term vanishes only if $D=1$ . Another intriguing   \n291 aspect of the depth-1 distribution is that its tail is indepen  \n292 dent of any hyperparameter of the problem, dramatically   \n293 different from the linear regression case. This is true for   \n294 deeper models as well.   \n295 Since $d$ only affects the non-polynomial part of the dis  \n296 tribution, the stationary distribution scales as $p(v)\\ \\propto$   \n297 v3(1\u22121/(D+1))(\u03b111v2\u22122\u03b12v+\u03b13). Hence, when v \u2192\u221e, the scal  \n298 ing behaviour is $v^{-5+3/(D+1)}$ . The tail gets monotonically   \n299 thinner as one increases the depth. For $D\\,=\\,1$ , the expo  \n300 nent is $7/2$ ; an infinite-depth network has an exponent of 5.   \n301 Therefor/e, the tail of the model distribution only depends   \n302 on the depth and is independent of the data or details of   \n303 training, unlike the depth-0 model. In addition, due to the   \n304 scaling $\\boldsymbol{v}^{5-3/(D+1)}$ for $v\\,\\rightarrow\\,\\infty$ , we can see that $\\mathbb{E}[v^{2}]$ will   \n305 never diverge no matter how large the $T$ is.   \n306 An intriguing feature of this model is that the model with at   \n307 least one hidden layer will never have a divergent training   \n308 loss. This directly explains the puzzling observation of the   \n309 edge-of-stability phenomenon in deep learning: SGD train  \n310 ing often gives a neural network a solution where a slight   \n311 increment of the learning rate will cause discrete-time in  \n312 stability and divergence [40, 4]. These solutions, quite sur  \n313 prisingly, exhibit low training and testing loss values even   \n314 when the learning rate is right at the critical learning rate of   \n315 instability. This observation contradicts naive theoretical expectations. Let $\\eta_{\\mathrm{sta}}$ denote the largest   \n316 stable learning rate. Close to a local minimum, one can expand the loss function up to the second or  \n317 der to show that the value of the loss function $L$ is proportional to $\\mathrm{Tr}[\\Sigma]$ . However, $\\Sigma\\propto1/(\\eta_{\\mathrm{sta}}\\!-\\!\\eta)$   \n318 should be a very large value [42, 19], and therefore $L$ should dive[rge]. Thus, the ed g\u221de o/f( stabi\u2212lity)   \n319 phenomenon is incompatible with the naive expectation up to the second order, as pointed out by   \n320 Ref. [5]. Our theory offers a direct explanation of why the divergence of loss does not happen: for   \n321 deeper models, the fluctuation of model parameters decreases as the gradient noise level increases,   \n322 reaching a minimal value before losing stability. Thus, SGD always has a finite loss because of the   \n323 power-law tail and fluctuation inversion. See Figure 5\u2013mid. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "AM1znjeo9l/tmp/5485c24070299c5ee9e5f053d6dd135ac2412dd331040e7e73bb2d6434fdea27.jpg", "img_caption": ["Figure 4: Regimes of learning for SGD as a function of $T$ and the noise in the dataset $\\sigma$ . According to (1) whether the sparse transition has happened, (2) whether a nontrivial maximum probability estimator exists, and (3) whether the sparse solution is a maximum probability estimator, the learning of SGD can be characterized into 5 regimes. Regime I is where SGD converges to a sparse solution with zero variance. In regime $\\mathbf{II}$ , the stationary distribution has a finite spread, but the probability of being close to the sparse solution is very high. In regime $\\mathbf{III}$ , the probability density of the sparse solution is zero, and therefore the model will learn without much problem. In regime b, a local nontrivial probability maximum exists. The only maximum probability estimator in regime a is the sparse solution. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "324 Infinite- $D$ limit. As $D$ tends to infinity, the distribution becomes ", "text_level": 1, "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{\\Psi}_{v}(v)\\propto\\frac{1}{v^{3+k_{1}}(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})^{1-k_{1}/2}}\\exp\\Bigg(-\\frac{d}{D T}\\left(\\frac{\\beta_{2}}{\\alpha_{3}v}+\\frac{\\alpha_{2}\\alpha_{3}\\beta_{1}-2\\alpha_{2}^{2}\\beta_{2}+\\alpha_{1}\\alpha_{3}\\beta_{2}}{\\alpha_{3}^{2}\\sqrt{\\Delta}}\\arctan(\\frac{\\alpha_{1}v-\\alpha_{2}}{\\sqrt{\\Delta}})\\right)\\Bigg),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "325 where $k_{1}\\,=\\,d\\big(\\alpha_{3}\\beta_{1}\\,-\\,2\\alpha_{2}\\beta_{2}\\big)/(T D\\alpha_{3}^{2})$ . An interesting feature is that the architecture ratio $d/D$   \n326 always ap p=ear(s simu l\u2212taneousl)y/ (with $1/T$ . This implies that for a sufficiently deep neural netwo/rk,   \n327 the ratio $D/d$ also becomes proportion/al to the strength of the noise. Since we know that $T=\\eta/S$   \n328 determines the performance of SGD, our result thus shows an extended scaling law of training:   \n329 $\\begin{array}{r}{\\frac{d}{D}\\frac{S}{\\eta}\\;=\\;c o n s t}\\end{array}$ . The architecture aspect of the scaling law also agrees with an alternative analysis   \n330 [9, 10], where the optimal architecture is found to have a constant ratio of $d/D$ . See Figure 5.   \n331 Now, if we $T$ , there are three situations: (1) $d=o(D)$ , (2) $d=c_{0}D$ for a constant $c_{\\mathrm{0}}$ , (3) $d=\\Omega(D)$ .   \n332 If $d=o(D)$ , $k_{1}\\rightarrow0$ and the distribution conv e=rge(s to) $p(v)\\propto v^{-3}(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})^{-1}$ , w =hich( is )a   \n333 delt a= di(stri)butio n\u2192 at 0. Namely, if the width is far small(er )t h\u221dan the( depth,\u2212 the m o+del w)ill collapse to   \n334 zero. Therefore, we should increase the model width as we increase the depth. In the second case,   \n335 $d/D$ is a constant and can thus be absorbed into the definition of $T$ and is the only limit where we   \n336 obtain a nontrivial distribution with a finite spread. If $d=\\Omega(D)$ , the distribution becomes a delta   \n337 distribution at the global minimum of the loss landscap e=, $p(\\dot{\\boldsymbol{v}})\\dot{\\bar{\\alpha}}=\\,\\delta(\\boldsymbol{v}-\\beta_{2}/\\beta_{1})$ and achieves the   \n338 global minimum. ", "page_idx": 7}, {"type": "image", "img_path": "AM1znjeo9l/tmp/991cca558b99eccab9ac0317be87e978362ba83d66feffdccb9c46d6c8a5458b.jpg", "img_caption": ["Figure 5: SGD on deep networks leads to a well-controlled distribution and training loss. Left: Power law of the tail of the parameter distribution of deep linear nets. The dashed lines show the upper $_{(-7/2)}$ and lower $(-5)$ bound of the exponent of the tail. The predicted power-law scaling agrees with the experiment, and the exponent decreases as the theory predicts. Mid: training loss of a tanh network. $D=0$ is the case where only the input weight is trained, and $D=1$ is the case where both input and output layers are trained. For $D=0$ , the model norm increases as the model loses stability. For $D=1$ , a \u201cfluctuation inversion\u201d effect appears. The fluctuation of the model vanishes before it loses stability. Right: performance of fully connected tanh nets on MNIST. Scaling the learning rate as $1/D$ keeps the model performance relatively unchanged. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "339 4.4 Implication for Variational Bayesian Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "340 One of the major implications of the analytical solution we found for machine learning practice   \n341 is the inappropriateness of using SGD to approximate a Bayesian posterior. Because every SGD   \n342 iteration can be regarded as a sampling of the model parameters. A series of recent works have   \n343 argued that the stationary distribution can be used as an approximation of the Bayesian posterior   \n344 for fast variational inference [21, 3], $p_{\\mathrm{Bayes}}(\\theta)\\approx p_{\\mathrm{SGD}}(\\theta)$ , a method that has been used for a wide   \n345 variety of applications [12]. However, our result implies that such an approximation is likely to   \n346 fail. Common in Bayesian deep learning, we interpret the per-sample loss as the log probability   \n347 and the weight decay as a Gaussian prior over the parameters, the true model parameters have a log   \n348 probability of ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\log p_{\\mathrm{Bayes}}(\\theta|x)\\propto\\ell(\\theta,x)+\\gamma\\|\\theta\\|^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "349 This distribution has a nonzero measure everywhere for any differentiable loss. However, the distri  \n350 bution for SGD in Eq.(14) has a zero probability density almost everywhere because a 1d subspace   \n351 has a zero Lebesgue measure in a high-dimensional space. This implies that the KL divergence be  \n352 tween the two distributions (either $\\mathrm{KL}(p_{\\mathrm{Bayes}}||p_{\\mathrm{SGD}})$ or $\\mathrm{KL}(p_{\\mathrm{SGD}}||p_{\\mathrm{Bayes}}))$ is infinite. Therefore,   \n353 we can infer that in the information-the(oretic s\u2223\u2223ense, $p_{\\mathrm{{SGD}}}$ can(not be\u2223 \u2223used to) approximate $p_{\\mathrm{Bayes}}$ . ", "page_idx": 8}, {"type": "text", "text": "354 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "355 In this work, we first showed that SGD systematically moves towards a balanced solution when   \n356 rescaling symmetry exists, a result we termed the law of balance. Applying the law of balance, we   \n357 have characterized the stationary distribution of SGD analytically, which is an unanswered funda  \n358 mental problem in the study of SGD. This is the first analytical expression for a globally nonconvex   \n359 and beyond quadratic loss without the need for any approximation. With this solution, we have   \n360 discovered many phenomena that could be relevant to deep learning that were previously unknown.   \n361 We found that SGD only has probability of exploring a one-dimensional submanifold even for a   \n362 very-dimensional problem, ignoring all irrelevant directions. We applied our theory to the important   \n363 problem of variational inference and showed that it is, in general, not appropriate to approximate   \n364 the posterior with SGD, at least when any symmetry is present in the model. If one really wants   \n365 to use SGD for variational inference, special care is required to at least remove symmetries from   \n366 the loss function, which could be an interesting future problem. Our theory is limited, as the model   \n367 we solved is only a minimal model of reality, and it would be interesting to consider more realistic   \n368 models in the future. Also, it would be interesting to extend the law of balance to a broader class of   \n369 symmetries. ", "page_idx": 8}, {"type": "text", "text": "370 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "371 [1] John C Baez and Brendan Fong. A noether theorem for markov processes. Journal of Mathe  \n372 matical Physics, 54(1):013301, 2013.   \n373 [2] Raphae\u00a8l Berthier. Incremental learning in diagonal linear networks. Journal of Machine Learn  \n374 ing Research, 24(171):1\u201326, 2023.   \n375 [3] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer  \n376 ence, converges to limit cycles for deep networks. In 2018 Information Theory and Applica  \n377 tions Workshop (ITA), pages 1\u201310. IEEE, 2018.   \n378 [4] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gra  \n379 dient descent on neural networks typically occurs at the edge of stability. arXiv preprint   \n380 arXiv:2103.00065, 2021.   \n381 [5] Alex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gra  \n382 dient descent at the edge of stability. arXiv preprint arXiv:2209.15594, 2022.   \n383 [6] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge  \n384 neous models: Layers are automatically balanced. Advances in neural information processing   \n385 systems, 31, 2018.   \n386 [7] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal   \n387 linear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint   \n388 arXiv:2302.08982, 2023.   \n389 [8] Xavier Fontaine, Valentin De Bortoli, and Alain Durmus. Convergence rates and approxi  \n390 mation results for sgd and its continuous-time counterpart. In Mikhail Belkin and Samory   \n391 Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134   \n392 of Proceedings of Machine Learning Research, pages 1965\u20132058. PMLR, 15\u201319 Aug 2021.   \n393 [9] Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients?   \n394 Advances in neural information processing systems, 31, 2018.   \n395 [10] Boris Hanin and David Rolnick. How to start training: The effect of initialization and archi  \n396 tecture. Advances in Neural Information Processing Systems, 31, 2018.   \n397 [11] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of   \n398 nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.   \n399 [12] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Ben  \n400 namoun. Hands-on bayesian neural networks\u2014a tutorial for deep learning users. IEEE Com  \n401 putational Intelligence Magazine, 17(2):29\u201348, 2022.   \n402 [13] Kenji Kawaguchi. Deep learning without poor local minima. Advances in Neural Information   \n403 Processing Systems, 29:586\u2013594, 2016.   \n404 [14] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori   \n405 Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dy  \n406 namics. arXiv preprint arXiv:2012.04728, 2020.   \n407 [15] Jonas Latz. Analysis of stochastic gradient descent in continuous time. Statistics and Comput  \n408 ing, 31(4):39, 2021.   \n409 [16] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of   \n410 stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning   \n411 Research, 20(40):1\u201347, 2019.   \n412 [17] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with tra  \n413 ditional optimization analyses: The intrinsic learning rate. Advances in Neural Information   \n414 Processing Systems, 33:14544\u201314555, 2020.   \n415 [18] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochas  \n416 tic differential equations (sdes), 2021.   \n417 [19] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate   \n418 stochastic gradient descent, 2021.   \n419 [20] Agnieszka B Malinowska and Moulay Rchid Sidi Ammi. Noether\u2019s theorem for control prob  \n420 lems on time scales. arXiv preprint arXiv:1406.0705, 2014.   \n421 [21] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as ap  \n422 proximate bayesian inference. Journal of Machine Learning Research, 18:1\u201335, 2017.   \n423 [22] John C Mauro, Prabhat K Gupta, and Roger J Loucks. Continuously broken ergodicity. The   \n424 Journal of chemical physics, 126(18), 2007.   \n425 [23] Tetsuya Misawa. Noether\u2019s theorem in symmetric stochastic calculus of variations. Journal of   \n426 mathematical physics, 29(10):2178\u20132180, 1988.   \n427 [24] Takashi Mori, Liu Ziyin, Kangqiao Liu, and Masahito Ueda. Power-law escape rate of sgd. In   \n428 International Conference on Machine Learning, pages 15959\u201315975. PMLR, 2022.   \n429 [25] Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias   \n430 of the step size in linear diagonal neural networks. In International Conference on Machine   \n431 Learning, pages 16270\u201316295. PMLR, 2022.   \n432 [26] Richard G Palmer. Broken ergodicity. Advances in Physics, 31(6):669\u2013735, 1982.   \n433 [27] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for di  \n434 agonal linear networks: a provable benefit of stochasticity. Advances in Neural Information   \n435 Processing Systems, 34:29218\u201329230, 2021.   \n436 [28] Hannes Risken and Hannes Risken. Fokker-planck equation. Springer, 1996.   \n437 [29] Tomasz Rolski, Hanspeter Schmidli, Volker Schmidt, and Jozef L Teugels. Stochastic pro  \n438 cesses for insurance and finance. John Wiley & Sons, 2009.   \n439 [30] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear   \n440 dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.   \n441 [31] N. Shirish Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large  \n442 Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ArXiv e-prints,   \n443 September 2016.   \n444 [32] Justin Sirignano and Konstantinos Spiliopoulos. Stochastic gradient descent in continuous   \n445 time: A central limit theorem. Stochastic Systems, 10(2):124\u2013151, 2020.   \n446 [33] Hidenori Tanaka and Daniel Kunin. Noether\u2019s learning dynamics: Role of symmetry breaking   \n447 in neural networks, 2021.   \n448 [34] D Thirumalai and Raymond D Mountain. Activated dynamics, loss of ergodicity, and transport   \n449 in supercooled liquids. Physical Review E, 47(1):479, 1993.   \n450 [35] Christopher J Turner, Alexios A Michailidis, Dmitry A Abanin, Maksym Serbyn, and Zlatko   \n451 Papic\u00b4. Weak ergodicity breaking from quantum many-body scars. Nature Physics, 14(7):745\u2013   \n452 749, 2018.   \n453 [36] Nicolaas Godfried Van Kampen. Stochastic processes in physics and chemistry, volume 1.   \n454 Elsevier, 1992.   \n455 [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n456 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n457 processing systems, 30, 2017.   \n458 [38] Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homo  \n459 geneity: Convergence and balancing effect, 2022.   \n460 [39] Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type part ii:   \n461 Continuous time analysis. Journal of Nonlinear Science, 34(1):1\u201345, 2024.   \n462 [40] Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning:   \n463 A dynamical stability perspective. Advances in Neural Information Processing Systems, 31,   \n464 2018.   \n465 [41] Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dy  \n466 namics: Stochastic gradient descent exponentially favors flat minima. arXiv preprint   \n467 arXiv:2002.03495, 2020.   \n468 [42] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint   \n469 arXiv:1810.00004, 2018.   \n470 [43] Liu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. In   \n471 Advances in Neural Information Processing Systems, 2022.   \n472 [44] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in   \n473 SGD. In International Conference on Learning Representations, 2022.   \n474 [45] Liu Ziyin and Masahito Ueda. Exact phase transitions in deep learning. arXiv preprint   \n475 arXiv:2205.12510, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "476 A Theoretical Considerations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "477 A.1 Background ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "478 A.1.1 Ito\u2019s Lemma ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "479 Let us consider the following stochastic differential equation (SDE) for a Wiener process $W(t)$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nd X_{t}=\\mu_{t}d t+\\sigma_{t}d W(t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 We are interested in the dynamics of a generic function of $X_{t}$ . Let $Y_{t}=f(t,X_{t})$ ; Ito\u2019s lemma states   \n481 that the SDE for the new variable is ", "page_idx": 12}, {"type": "equation", "text": "$$\nd f(t,X_{t})=\\left(\\frac{\\partial f}{\\partial t}+\\mu_{t}\\frac{\\partial f}{\\partial X_{t}}+\\frac{\\sigma_{t}^{2}}{2}\\frac{\\partial^{2}f}{\\partial X_{t}^{2}}\\right)d t+\\sigma_{t}\\frac{\\partial f}{\\partial x}d W(t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "482 Let us take the variable $Y_{t}=X_{t}^{2}$ as an example. Then the SDE is ", "page_idx": 12}, {"type": "equation", "text": "$$\nd Y_{t}=\\left(2\\mu_{t}X_{t}+\\sigma_{t}^{2}\\right)d t+2\\sigma_{t}X_{t}d W(t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "483 Let us consider another example. Let two variables $X_{t}$ and $Y_{t}$ follow ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d X_{t}=\\mu_{t}d t+\\sigma_{t}d W(t),}\\\\ {d Y_{t}=\\lambda_{t}d t+\\phi_{t}d W(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "484 The SDE of $X_{t}Y_{t}$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nd\\bigl(X_{t}Y_{t}\\bigr)=\\bigl(\\mu_{t}Y_{t}+\\lambda_{t}X_{t}+\\sigma_{t}\\phi_{t}\\bigr)d t+\\bigl(\\sigma_{t}Y_{t}+\\phi_{t}X_{t}\\bigr)d W(t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "485 A.1.2 Fokker Planck Equation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "486 The general SDE of a 1d variable $X$ is given by: ", "page_idx": 12}, {"type": "equation", "text": "$$\nd X=-\\mu(X)d t+B(X)d W(t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 The time evolution of the probability density $P(x,t)$ is given by the Fokker-Planck equation: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial P(X,t)}{\\partial t}=-\\frac{\\partial}{\\partial X}J(X,t),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "488 where $\\begin{array}{r}{J(X,t)~=~\\mu(X)P(X,t)\\,+\\,\\frac{1}{2}\\frac{\\partial}{\\partial X}[B^{2}(X)P(X,t)]}\\end{array}$ . The stationary distribution satisfying   \n489 $\\partial P(X,t)/\\partial t=0$ is ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(X)\\propto\\frac{1}{B^{2}(X)}\\exp\\left[-\\int d X\\frac{2\\mu(X)}{B^{2}(X)}\\right]:=\\tilde{P}(X),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 which gives a solution as a Boltzmann-type distribution if $B$ is a constant. We will apply Eq. (25)   \n491 to determine the stationary distributions in the following sections. ", "page_idx": 12}, {"type": "text", "text": "492 A.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "493 Proof. We omit writing $v$ in the argument unless necessary. By definition of the symmetry   \n494 $\\ell(\\mathbf{u},\\mathbf{w},x)=\\ell(\\lambda\\mathbf{u},\\mathbf{w}/\\lambda,x)$ , we obtain its infinitesimal transformation $\\ell(\\mathbf{u},\\mathbf{w},x)=\\ell((1+\\epsilon)\\mathbf{u},(1-$   \n495 $\\epsilon){\\bf w}/\\lambda,x)$ .  =Ex(panding/ this) to first order in $\\epsilon$ , we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i}u_{i}{\\frac{\\partial\\ell}{\\partial u_{i}}}=\\sum_{j}w_{j}{\\frac{\\partial\\ell}{\\partial w_{j}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "496 The equations of motion are ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle{\\frac{d u_{i}}{d t}=-\\frac{\\partial\\ell}{\\partial u_{i}},}}\\\\ {\\displaystyle{\\frac{d w_{j}}{d t}=-\\frac{\\partial\\ell}{\\partial w_{j}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "497 Using Ito\u2019s lemma, we can find the equations governing the evolutions of $u_{i}^{2}$ and $w_{j}^{2}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d u_{i}^{2}}{d t}=2u_{i}\\frac{d u_{i}}{d t}+\\frac{(d u_{i})^{2}}{d t}=-2u_{i}\\frac{\\partial\\ell}{\\partial u_{i}}+T C_{i}^{u},}\\\\ {\\displaystyle\\frac{d w_{j}^{2}}{d t}=2w_{j}\\frac{d w_{j}}{d t}+\\frac{(d w_{j})^{2}}{d t}=-2w_{j}\\frac{\\partial\\ell}{\\partial w_{j}}+T C_{j}^{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 where $\\begin{array}{r}{C_{i}^{u}=\\mathrm{Var}[\\frac{\\partial\\ell}{\\partial u_{i}}]}\\end{array}$ and $\\begin{array}{r}{C_{j}^{w}=\\mathrm{Var}[\\frac{\\partial\\ell}{\\partial w_{j}}]}\\end{array}$ . With Eq. (26), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(\\|u\\|^{2}-\\|w\\|^{2})=-T(\\sum_{j}C_{j}^{w}-\\sum_{i}C_{i}^{u})=-T\\left(\\sum_{j}\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial w_{j}}\\right]-\\sum_{i}\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial u_{i}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "499 Due to the rescaling symmetry, the loss function can be considered as a function of the matrix $u w^{T}$   \n500 Here we define a new loss function as $\\tilde{\\ell}(u_{i}w_{j})=\\ell(u_{i},w_{j})$ . Hence, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial w_{j}}=\\sum_{i}u_{i}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{j})},\\frac{\\partial\\ell}{\\partial u_{i}}=\\sum_{j}w_{j}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{j})}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "501 We can rewrite Eq. (30) into ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(\\|u\\|^{2}-\\|w\\|^{2})=-T(u^{T}C_{1}u-w^{T}C_{2}w),,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "502 where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(C_{1})_{i j}=\\mathbb{E}\\left[\\sum_{k}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{k})}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{j}w_{k})}\\right]-\\sum_{k}\\mathbb{E}\\left[\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{k})}\\right]\\mathbb{E}\\left[\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{j}w_{k})}\\right],}\\\\ &{\\quad\\quad\\quad\\quad\\equiv\\mathbb{E}[A^{T}A]-\\mathbb{E}[A^{T}]\\mathbb{E}[A]}\\\\ &{(C_{2})_{k l}=\\mathbb{E}\\left[\\sum_{i}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{k})}\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{l})}\\right]-\\sum_{i}\\mathbb{E}\\left[\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{k})}\\right]\\mathbb{E}\\left[\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{l})}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\equiv\\mathbb{E}[A A^{T}]-\\mathbb{E}[A]\\mathbb{E}[A^{T}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503 where ", "page_idx": 13}, {"type": "equation", "text": "$$\n(A)_{i k}\\equiv{\\frac{\\partial\\tilde{\\ell}}{\\partial(u_{i}w_{k})}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "504 The proof is thus complete. ", "page_idx": 13}, {"type": "text", "text": "505 A.3 Second-order Law of Balance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "506 Considering the modified loss function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{tot}}=\\ell+\\frac{1}{4}T\\|\\nabla L\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 In this case, the Langevin equations become ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{d w_{j}=-\\frac{\\partial\\ell}{\\partial w_{j}}d t-\\frac{1}{4}T\\frac{\\partial||\\nabla L||^{2}}{\\partial w_{j}},}}\\\\ {\\displaystyle{d u_{i}=--\\frac{\\partial\\ell}{\\partial u_{i}}d t-\\frac{1}{4}T\\frac{\\partial||\\nabla L||^{2}}{\\partial u_{i}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "508 Hence, the modified SDEs of $u_{i}^{2}$ and $w_{j}^{2}$ can be rewritten as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d u_{i}^{2}}{d t}=2u_{i}\\frac{d u_{i}}{d t}+\\frac{(d u_{i})^{2}}{d t}=-2u_{i}\\frac{\\partial\\ell}{\\partial u_{i}}++T C_{i}^{u}-\\frac{1}{2}T u_{i}\\nabla_{u_{i}}|\\nabla L|^{2},}\\\\ {\\displaystyle\\frac{d w_{j}^{2}}{d t}=2w_{j}\\frac{d w_{j}}{d t}+\\frac{(d w_{j})^{2}}{d t}=-2w_{j}\\frac{\\partial\\ell}{\\partial w_{j}}+T C_{j}^{w}-\\frac{1}{2}T w_{j}\\nabla_{w_{j}}|\\nabla L|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "509 In this section, we consider the effects brought by the last term in Eqs. (39) and (40). From the   \n510 infinitesimal transformation of the rescaling symmetry: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{j}w_{j}{\\frac{\\partial\\ell}{\\partial w_{j}}}=\\sum_{i}u_{i}{\\frac{\\partial\\ell}{\\partial u_{i}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 we take the derivative of both sides of the equation and obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{\\partial{\\cal L}}{\\partial u_{i}}+\\sum_{j}u_{j}\\frac{\\partial^{2}{\\cal L}}{\\partial u_{i}\\partial u_{j}}=\\sum_{j}w_{j}\\frac{\\partial^{2}{\\cal L}}{\\partial u_{i}\\partial w_{j}}},}\\\\ {\\displaystyle{\\sum_{j}u_{j}\\frac{\\partial^{2}{\\cal L}}{\\partial w_{i}\\partial u_{j}}=\\frac{\\partial{\\cal L}}{\\partial w_{i}}+\\sum_{j}w_{j}\\frac{\\partial^{2}{\\cal L}}{\\partial w_{i}\\partial w_{j}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 where we take the expectation to $\\ell$ at the same time. By substituting these equations into Eqs. (39)   \n513 and (40), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d||u||^{2}}{d t}-\\frac{d||w|||^{2}}{d t}=T\\sum_{i}(C_{i}^{u}+\\left(\\nabla_{u_{i}}L\\right)^{2})-T\\sum_{j}(C_{j}^{w}+\\left(\\nabla_{w_{j}}L\\right)^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "514 Then following the procedure in Appendix. A.2, we can rewrite Eq. (44) as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d||{\\boldsymbol u}||^{2}}{d t}-\\frac{d||{\\boldsymbol w}||^{2}}{d t}=-T\\big({\\boldsymbol u}^{T}C_{1}{\\boldsymbol u}+{\\boldsymbol u}^{T}D_{1}{\\boldsymbol u}-{\\boldsymbol w}^{T}C_{2}{\\boldsymbol w}-{\\boldsymbol w}^{T}D_{2}{\\boldsymbol w}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-T\\big({\\boldsymbol u}^{T}E_{1}{\\boldsymbol u}-{\\boldsymbol w}^{T}E_{2}{\\boldsymbol w}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "515 where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(D_{1})_{i j}=\\displaystyle\\sum_{k}\\mathbb{E}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}w_{k})}\\right]\\mathbb{E}\\left[\\frac{\\partial\\ell}{\\partial(u_{j}w_{k})}\\right],}\\\\ &{(D_{2})_{k l}=\\displaystyle\\sum_{i}\\mathbb{E}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}w_{k})}\\right]\\mathbb{E}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}w_{l})}\\right],}\\\\ &{(E_{1})_{i j}=\\mathbb{E}\\left[\\displaystyle\\sum_{k}\\frac{\\partial\\ell}{\\partial(u_{i}w_{k})}\\frac{\\partial\\ell}{\\partial(u_{j}w_{k})}\\right],}\\\\ &{(E_{2})_{k l}=\\mathbb{E}\\left[\\displaystyle\\sum_{i}\\frac{\\partial\\ell}{\\partial(u_{i}w_{k})}\\frac{\\partial\\ell}{\\partial(u_{i}w_{l})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "516 For one-dimensional parameters $u,w$ , Eq. (45) is reduced to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\big(u^{2}-w^{2}\\big)=-\\mathbb{E}\\left[\\left(\\frac{\\partial\\ell}{\\partial(u w)}\\right)^{2}\\right]\\big(u^{2}-w^{2}\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "517 Therefore, we can see this loss modification increases the speed of convergence. Now, we move   \n518 to the stationary distribution of the parameter $v$ . At the stationarity, if $u_{i}\\,=\\,-w_{i}$ , we also have the   \n519 distribution $P(\\dot{v})=\\delta(v)$ like before. However, when $u_{i}=w_{i}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d v}{\\mathcal{H}}=-4v(\\beta_{1}v-\\beta_{2})+4T v(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})-4\\beta_{1}^{2}T v(\\beta_{1}v-\\beta_{2})(3\\beta_{1}v-\\beta_{2})+4v\\sqrt{T(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "520 Hence, the stationary distribution becomes ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(v)\\propto\\frac{v^{\\beta_{2}/2\\alpha_{3}T-3/2-\\beta_{2}^{2}/2\\alpha_{3}}}{(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})^{1+\\beta_{2}/4T\\alpha_{3}+K_{1}}}\\exp\\left(-\\left(\\frac{1}{2T}\\frac{\\alpha_{3}\\beta_{1}-\\alpha_{2}\\beta_{2}}{\\alpha_{3}\\sqrt{\\Delta}}+K_{2}\\right)\\arctan\\frac{\\alpha_{1}v-\\alpha_{2}}{\\sqrt{\\Delta}}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "521 where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle K_{1}=\\frac{3\\alpha_{3}\\beta_{1}^{2}-\\alpha_{1}\\beta_{2}^{2}}{4\\alpha_{1}\\alpha_{3}},}}\\\\ {{\\displaystyle K_{2}=\\frac{3\\alpha_{2}\\alpha_{3}\\beta_{1}^{2}-4\\alpha_{1}\\alpha_{3}\\beta_{1}\\beta_{2}+\\alpha_{1}\\alpha_{2}\\beta_{2}^{2}}{2\\alpha_{1}\\alpha_{3}\\sqrt{\\Delta}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "522 From the expression above we can see $K_{1}\\ll1+\\beta_{2}/4T\\alpha_{3}$ and $K_{2}\\ll(\\alpha_{3}\\beta_{1}-\\alpha_{2}\\beta_{2})/2T\\alpha_{3}\\sqrt{\\Delta}$ .   \n523 Hence, the effect of modification can only b e \u226asee n+ in th/e term proport io\u226ana(l to $v$ .  \u2212The pha)s/e transition   \n524 point is modified as ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{c}=\\frac{\\beta_{2}}{\\alpha_{3}+\\beta_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "525 Compared with the previous result $\\begin{array}{r}{T_{c}\\,=\\,\\frac{\\beta_{2}}{\\alpha_{3}}}\\end{array}$ , we can see the effect of the loss modification is $\\alpha_{3}\\rightarrow$   \n526 $\\alpha_{3}+\\beta_{2}^{2}$ , or equivalently, $\\mathrm{Var}[x y]\\to\\mathbb{E}[x^{2}y^{2}]$ . This effect can be seen from $E_{1}$ and $E_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "527 A.4 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "528 Proof. For any $i$ , one can obtain the expressions of $C_{1}^{(i)}$ and $C_{2}^{(i)}$ from Theorem 3.1 as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C_{1}^{(i)})_{\\alpha_{1},\\alpha_{2}}=4p_{i}\\mathbb{E}_{i}\\left[\\|\\tilde{x}\\|^{2}(\\sum_{j=1}^{d}u_{j}^{\\alpha_{1}}v_{j}^{T}\\tilde{x}-y^{\\alpha_{1}})(\\sum_{j=1}^{d}u_{j}^{\\alpha_{2}}v_{j}^{T}\\tilde{x}-y^{\\alpha_{2}})\\right]-4p_{i}^{2}\\sum_{\\beta}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}(\\sum_{j=1}^{d}u_{j}^{\\alpha_{1}}v_{j}^{T}\\tilde{x}-y^{\\alpha_{1}})\\right]}\\\\ &{}&{=4p_{i}\\mathbb{E}_{i}\\left[\\|\\tilde{x}\\|^{2}r^{\\alpha_{1}}r^{\\alpha_{2}}\\right]-4p_{i}^{2}\\sum_{\\beta}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}r^{\\alpha_{1}}\\right]\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}r^{\\alpha_{2}}\\right],\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\left(55\\right)}\\\\ {\\langle C_{2}^{(i)}\\right)_{\\beta_{1},\\beta_{2}}=4\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta_{1}}\\tilde{x}^{\\beta_{2}}\\|\\displaystyle\\sum_{j=1}^{d}u_{j}v_{j}^{T}\\tilde{x}-y\\|^{2}\\right]-4\\sum_{\\alpha}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta_{1}}(\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}-y^{\\alpha})\\right]\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta_{2}}(\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}-y^{\\alpha})\\right]}\\\\ &{}&{=4p_{i}\\mathbb{E}_{i}\\left[\\|r\\|^{2}\\tilde{x}^{\\beta_{1}}\\tilde{x}^{\\beta_{2}}\\right]-4p_{i}^{2}\\sum_{\\alpha}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta_{1}}r^{\\alpha}\\right]\\mathbb{E}_{i}\\left[\\tilde{\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "529 where we use the notation $\\begin{array}{r}{r^{\\alpha}:=\\;\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}-y^{\\alpha},\\tilde{x}:=\\;(x^{T},1)^{T},v_{i}\\;=\\;(w_{i}^{T},b_{i})^{T}}\\end{array}$ and $\\mathbb{E}_{i}[O]:=$   \n530 $\\mathbb{E}[O|w_{i}^{T}x+b_{i}>0]$ . ", "page_idx": 15}, {"type": "text", "text": "531 We start with showing that ${C}_{1}^{(1)}$ is full-rank. Let $m$ be an arbitrary unit vector in $\\mathbb{R}^{d_{u}}$ . We have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{m^{T}C_{1}^{(i)}m=4p_{i}\\mathbb{E}_{i}\\left[\\|\\tilde{x}\\|^{2}(m^{T}r)^{2}\\right]-4p_{i}^{2}\\sum_{\\beta}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}(m^{T}r)\\right]\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}(m^{T}r)\\right]}}\\\\ &{}&{\\ge4p_{i}^{2}\\mathbb{E}_{i}\\left[\\|\\tilde{x}\\|^{2}(m^{T}r)^{2}\\right]-4p_{i}^{2}\\sum_{\\beta}\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}(m^{T}r)\\right]\\mathbb{E}_{i}\\left[\\tilde{x}^{\\beta}(m^{T}r)\\right]}\\\\ &{}&{=4p_{i}^{2}\\sum_{\\beta}\\mathrm{Var}_{i}\\tilde{x}^{\\beta}m^{T}r]}\\\\ &{}&{=4p_{i}^{2}\\sum_{\\beta}[\\mathrm{Var}_{i}[\\tilde{x}^{\\beta}m^{T}(g(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}v_{j}^{T}\\tilde{x})]+\\mathrm{Var}_{i}[\\tilde{x}^{\\beta}m^{T}\\epsilon]-2\\mathrm{Cov}_{i}[\\tilde{x}^{\\beta}m^{T}(g(x)-\\displaystyle\\sum_{j=1}^{d}]]}\\\\ &{}&{\\ge4p_{i}^{2}\\sum_{\\beta}\\mathrm{Var}_{i}[\\tilde{x}^{\\beta}m^{T}\\epsilon]>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "532 where the last inequality follows from ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Cov}[\\tilde{x}^{\\beta}m^{T}(g(x)-\\sum_{j=1}^{d}u_{j}v_{j}^{T}\\tilde{x}),\\tilde{x}^{\\beta}m^{T}\\epsilon]}}\\\\ &{}&\\\\ &{=\\mathbb{E}_{i}\\big[(\\tilde{x}^{\\beta})^{2}m^{T}(g(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}v_{j}^{T}\\tilde{x})m^{T}\\epsilon\\big]-\\mathbb{E}_{i}\\big[\\tilde{x}^{\\beta}m^{T}(g(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}v_{j}^{T}\\tilde{x})\\big]\\mathbb{E}_{i}\\big[\\tilde{x}^{\\beta}m^{T}\\epsilon\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "533 Here we denote that $\\mathrm{Var}_{i}[O]:=\\mathbb{E}_{i}[O^{2}]-\\mathbb{E}_{i}[O]^{2}$ and $\\operatorname{Cov}_{i}[O_{1},O_{2}]:=\\mathbb{E}_{i}[O_{1}O_{2}]-\\mathbb{E}_{i}[O_{1}]\\mathbb{E}_{i}[O_{2}]$ . ", "page_idx": 15}, {"type": "text", "text": "534 For $C_{2}^{(i)}$ , we let the vector $\\tilde{n}:=(n^{T},n_{f})^{T}$ be a unit vector in $\\mathbb{R}^{d_{w}+1}$ , yielding ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{n}^{T}C_{2}^{(i)}\\tilde{n}=4p_{i}\\mathbb{E}_{i}\\left[||r||^{2}(\\tilde{n}^{T}\\tilde{x})^{2}\\right]-4p_{i}^{2}\\sum_{\\alpha}\\mathbb{E}_{i}\\left[r^{\\alpha}(\\tilde{n}^{T}\\tilde{x})\\right]\\mathbb{E}_{i}\\left[r^{\\alpha}(\\tilde{n}^{T}\\tilde{x})\\right]}\\\\ &{\\qquad\\qquad\\geq4p_{i}^{2}\\mathbb{E}_{i}\\left[||r||^{2}(\\tilde{n}^{T}\\tilde{x})^{2}\\right]-4p_{i}^{2}\\sum_{\\alpha}\\mathbb{E}_{i}\\left[r^{\\alpha}(\\tilde{n}^{T}\\tilde{x})\\right]\\mathbb{E}_{i}\\left[r^{\\alpha}(\\tilde{n}^{T}\\tilde{x})\\right]}\\\\ &{\\qquad\\qquad=4p_{i}^{2}\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}[r^{\\alpha}\\tilde{n}^{T}\\tilde{x}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "535 Note that this quantity can be decomposed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[r^{\\alpha}\\tilde{n}^{T}\\tilde{x}\\big]=\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\big(g^{\\alpha}(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}+\\epsilon^{\\alpha}\\big)\\big(\\tilde{n}^{T}\\tilde{x})\\big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\big(g^{\\alpha}(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}\\big)\\big(n^{T}x+n_{f}\\big)\\big]+\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\epsilon^{\\alpha}\\big(n^{T}x+n_{f}\\big)\\big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad-2\\displaystyle\\sum_{\\alpha}\\mathrm{Cov}_{i}\\big[\\big(g^{\\alpha}(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x}\\big)\\big(n^{T}x+n_{f}\\big),\\epsilon^{\\alpha}\\big(n^{T}x+n_{f}\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "536 The covariance term vanishes because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Cov}[(g^{\\alpha}(x)-\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x})(n^{T}x+n_{f}),\\epsilon^{\\alpha}(n^{T}x+n_{f})]}}\\\\ &{}&\\\\ &{\\cdot\\mathbb{E}_{i}\\big[(g^{\\alpha}(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x})\\epsilon^{\\alpha}(n^{T}x+n_{f})^{2}\\big]-\\mathbb{E}_{i}\\big[(g^{\\alpha}(x)-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x})(n^{T}x+n_{f})\\big]\\mathbb{E}_{i}\\big[\\epsilon^{\\alpha}(n^{T}x+n_{f})\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "0. ", "page_idx": 16}, {"type": "text", "text": "537 Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ i^{T}C_{2}^{(i)}\\tilde{n}\\geq\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[(g^{\\alpha}(\\boldsymbol{x})-\\displaystyle\\sum_{j=1}^{d}u_{j}^{\\alpha}v_{j}^{T}\\tilde{x})(n^{T}\\boldsymbol{x}+n_{f})\\big]+\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\epsilon^{\\alpha}(n^{T}\\boldsymbol{x}+n_{f})\\big]}\\\\ &{\\qquad\\quad\\geq\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\epsilon^{\\alpha}(n^{T}\\boldsymbol{x}+n_{f})\\big]}\\\\ &{\\qquad=\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\epsilon^{\\alpha}\\big]\\mathrm{Var}_{i}\\big[(n^{T}\\boldsymbol{x}+n_{f})\\big]+\\displaystyle\\sum_{\\alpha}(\\mathrm{Var}_{i}[\\epsilon^{\\alpha}]\\mathbb{E}_{i}\\big[(n^{T}\\boldsymbol{x}+n_{f})^{2}\\big]+\\mathrm{Var}_{i}[n^{T}\\boldsymbol{x}+n_{f}]\\mathbb{E}_{i}\\big[(\\epsilon^{\\alpha})\\big]}\\\\ &{\\qquad\\geq\\displaystyle\\sum_{\\alpha}\\mathrm{Var}_{i}\\big[\\epsilon^{\\alpha}\\big]\\mathbb{E}_{i}\\big[(n^{T}\\boldsymbol{x}+n_{f})^{2}\\big]>0,}&{\\mathrm{(62)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "538 where the penultimate inequality follows from the fact that $\\epsilon$ is independent of $x$ . Hence, both the   \n539 matrices $C_{1}^{(i)}$ and C2i are full-rank. The proof is completed. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "540 A.5 Derivation of Eq. (4) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "541 We here prove inequality (4). At stationarity, $d(\\|u\\|^{2}-\\|w\\|^{2})/d t=0$ , indicating ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{1M}\\|u\\|^{2}-\\lambda_{2m}\\|w\\|^{2}\\geq0,\\;\\lambda_{1m}\\|u\\|^{2}-\\lambda_{2M}\\|w\\|^{2}\\leq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "542 The first inequality in Eq. (63) gives the solution ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\|u\\|^{2}}{\\|w\\|^{2}}}\\geq{\\frac{\\lambda_{2m}}{\\lambda_{1M}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "543 The second inequality in Eq. (63) gives the solution ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\|u\\|^{2}}{\\|w\\|^{2}}\\leq\\frac{\\lambda_{2M}}{\\lambda_{1m}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "544 Combining these two results, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{2m}}{\\lambda_{1M}}\\leq\\frac{\\|u\\|^{2}}{\\|w\\|^{2}}\\leq\\frac{\\lambda_{2M}}{\\lambda_{1m}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "545 which is Eq. (4). ", "page_idx": 16}, {"type": "text", "text": "546 A.6 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "547 Proof. This proof is based on the fact that if a certain condition is satisfied for all trajectories with   \n548 probability 1, this condition is satisfied by the stationary distribution of the dynamics with probabil  \n549 ity 1.   \n550 Let us first consider the case of $D>1$ . We first show that any trajectory satisfies at least one of   \n551 the following five conditions: for any $i$ , (i) $v_{i}\\to0$ , (ii) $L(\\theta)\\to0$ , or (iii) for any $k\\ne l$ , $(u_{i}^{(k)})^{2}\\mathrm{~-~}$   \n552 $(u_{i}^{(l)})^{2}\\to0$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "553 The SDE for $u_{i}^{(k)}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d u_{i}^{(k)}}{d t}=-2\\frac{v_{i}}{u_{i}^{(k)}}\\big(\\beta_{1}v-\\beta_{2}\\big)+2\\frac{v_{i}}{u_{i}^{(k)}}\\sqrt{\\eta\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big)}\\frac{d W}{d t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "554 where $\\begin{array}{r}{v_{i}:=\\prod_{k=1}^{D}u_{i}^{(k)}}\\end{array}$ , and so $v=\\Sigma_{i}\\,v_{i}$ . There exists rescaling symmetry between $u_{i}^{(k)}$ and $u_{i}^{(l)}$ for   \n555 . By  \u2236t=h e\u220f law of balance, we  =h a\u2211ve ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d}{d t}[(u_{i}^{(k)})^{2}-(u_{i}^{(l)})^{2}]=-T[(u_{i}^{(k)})^{2}-(u_{i}^{(l)})^{2}]\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}^{(k)}u_{i}^{(l)})}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "556 where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}^{(k)}u_{i}^{(l)})}\\right]=\\big(\\frac{v_{i}}{u_{i}^{(k)}u_{i}^{(l)}}\\big)^{2}\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "557 with $\\begin{array}{r}{v_{i}/(u_{i}^{(k)}u_{i}^{(l)})\\ =\\ \\prod_{s\\neq k,l}u_{i}^{(s)}}\\end{array}$ . In t\u23a6he long-time limit, $(u_{i}^{(k)})^{2}$ converges to $(u_{i}^{(l)})^{2}$ unless   \n558 $\\begin{array}{r}{\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}^{(k)}u_{i}^{(l)})}\\right]\\,=\\,0}\\end{array}$ , which is equivalent to $v_{i}/\\big(u_{i}^{(k)}u_{i}^{(l)}\\big)\\,=\\,0$ or $\\alpha_{1}v^{2}\\mathrm{~-~}2\\alpha_{2}v\\mathrm{~+~}\\alpha_{3}\\mathrm{~=~}0$ . These   \n559 two conditions correspond to conditions (i) and (ii). The latter is because $\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}=0$ takes   \n560 place if and only if $v=\\alpha_{2}/\\alpha_{1}$ and $\\alpha_{2}^{2}-\\alpha_{1}\\alpha_{3}=0$ together with $L(\\theta)=0$ . Ther\u2212efore, a+t sta t=ionarity,   \n561 we must have conditi o=ns (i/), (ii), or (i ii\u2212).   \n562 Now, we prove that when (iii) holds, the condition 2-(b) in the theorem statement must hold: for   \n563 $D=1$ , $(\\log|v_{i}|-\\log|v_{j}|)=c_{0}$ with $\\operatorname{sgn}(v_{i})=\\operatorname{sgn}(v_{j})$ . When (iii) holds, there are two situations.   \n564 First, if $v_{i}\\,=\\,0$ , we have $u_{i}^{(}k)\\,=\\,0$ for all $k$ , and $v_{i}$ will stay 0 for the rest of the trajectory, which   \n565 correspond s= to condition (i). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "566 If $v_{i}\\neq0$ , we have ui(k) 0 for all k. Therefore, the dynamics of vi is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{i v_{i}}{d t}=-2\\sum_{k}\\left(\\frac{v_{i}}{u_{i}^{(k)}}\\right)^{2}(\\beta_{1}v-\\beta_{2})+2\\sum_{k}\\left(\\frac{v_{i}}{u_{i}^{(k)}}\\right)^{2}\\sqrt{\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})}\\frac{d W}{d t}+4\\sum_{k,l}\\left(\\frac{v_{i}^{3}}{(u_{i}^{(k)}u_{i}^{(l)})^{2}}\\right)\\eta(\\alpha_{1}v-\\alpha_{2}v+\\alpha_{3}v_{l})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "567 Comparing the dynamics of $v_{i}$ and $v_{j}$ for $i\\neq j$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{d w_{i}/d t}{\\sum_{k}(v_{i}/u_{i}^{(k)})^{2}}-\\frac{d v_{j}/d t}{\\sum_{k}(v_{j}/u_{j}^{(k)})^{2}}=4\\left(\\frac{\\sum_{m,l}v_{i}^{3}/(u_{i}^{(m)}u_{i}^{(l)})^{2}}{\\sum_{k}(v_{i}/u_{i}^{(k)})^{2}}-\\frac{\\sum_{m,l}v_{j}^{3}/(u_{j}^{(m)}u_{j}^{(l)})^{2}}{\\sum_{k}(v_{j}/u_{j}^{(k)})^{2}}\\right)\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v_{k})}\\\\ &{}&{=4\\left(v_{i}\\frac{\\sum_{m,l}v_{i}^{2}/(u_{i}^{(m)}u_{i}^{(l)})^{2}}{\\sum_{k}(v_{i}/u_{i}^{(k)})^{2}}-v_{j}\\frac{\\sum_{m,l}v_{j}^{2}/(u_{j}^{(m)}u_{j}^{(l)})^{2}}{\\sum_{k}(v_{j}/u_{j}^{(k)})^{2}}\\right)\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v_{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "568 By condition (iii), we have $|u_{i}^{(0)}|=\\cdots=|u_{i}^{(D)}|$ , i.e., $(v_{i}/u_{i}^{(k)})^{2}=(v_{i}^{2})^{D/(D+1)}$ and $(v_{i}/u_{i}^{(m)}u_{i}^{(l)})^{2}=$   \n569 $(v_{i}^{2})^{(D-1)/(D+1)}$ .5 Therefo \u2223re, w\u2223e =o b\u22efta=i n\u2223 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d v_{i}/d t}{\\langle D+1\\rangle(v_{i}^{2})^{D/(D+1)}}-\\frac{d v_{j}/d t}{(D+1)(v_{j}^{2})^{D/(D+1)}}=\\left(v_{i}\\frac{D(v_{i}^{2})^{(D-1)/(D+1)}}{2(v_{i}^{2})^{D/(D+1)}}-v_{j}\\frac{D(v_{j}^{2})^{(D-1)/(D+1)}}{2(v_{j}^{2})^{D/(D+1)}}\\right)\\eta(\\alpha_{1}v_{i})^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "570 We first consider the case where $v_{i}$ and $v_{j}$ initially share the same sign (both positive or both nega  \n571 tive). When $D>1$ , the left-hand side of Eq. (72) can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{1-D}\\frac{d v_{i}^{2/(D+1)-1}}{d t}+4D v_{i}^{1-2/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})-\\frac{1}{1-D}\\frac{d v_{j}^{2/(D+1)-1}}{d t}-4D v_{j}^{1-2/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}v^{2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "572 which follows from Ito\u2019s lemma: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{I v_{i}^{2/(D+1)-1}}{d t}=\\bigg(\\frac{2}{D+1}-1\\bigg)v_{i}^{2/(D+1)-2}\\frac{d v_{i}}{d t}+2(\\frac{2}{D+1}-1)(\\frac{2}{D+1}-2)v_{i}^{2/(D+1)-3}\\left(\\sum_{k}(\\frac{v_{i}}{u_{i}^{(k)}})^{2}\\sqrt{\\eta(\\alpha_{k}+1)}v_{i}^{2/(D+1)-3}\\right)}}&{{}=\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}\\int_{\\mathbb{D}}d\\hat{\\xi}\\hat{N}\\,\\mathrm{d}\\hat{\\xi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "573 Substitute in Eq. (72), we obtain Eq. (73). ", "page_idx": 18}, {"type": "text", "text": "574 Now, we consider the right-hand side of Eq. (72), which is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n2D v_{i}^{1-2/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})-2D v_{j}^{1-2/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "575 Combining Eq. (73) and Eq. (75), we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{1-D}\\frac{d v_{i}^{2/(D+1)-1}}{d t}-\\frac{1}{1-D}\\frac{d v_{j}^{2/(D+1)-1}}{d t}=-2D\\big(v_{i}^{1-2/(D+1)}-v_{j}^{1-2/(D+1)}\\big)\\eta\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "576 By defining $z_{i}=v_{i}^{2/(D+1)-1}$ , we can further simplify the dynamics: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{d\\bigl(z_{i}-z_{j}\\bigr)}{d t}=2D\\bigl(D-1\\bigr)\\left(\\frac{1}{z_{i}}-\\frac{1}{z_{j}}\\right)\\eta\\bigl(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\bigr)}}\\\\ &{}&{=-2D\\bigl(D-1\\bigr)\\frac{z_{i}-z_{j}}{z_{i}z_{j}}\\eta\\bigl(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "577 Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{i}(t)-z_{j}(t)=\\exp\\left[-\\int d t\\frac{2D(D-1)}{z_{i}z_{j}}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "578 Therefore, if $v_{i}$ and $v_{j}$ initially have the same sign, they will decay to the same value in the long  \n579 time limit $t\\to\\infty$ , which gives condition 2-(b). When $v_{i}$ and $v_{j}$ initially have different signs, we can   \n580 write Eq. (72) as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overbrace{\\mathscr{D}+1)(|v_{i}|^{2})^{D/(D+1)}}^{d|v_{i}|/d t}+\\frac{d|v_{j}|/d t}{(D+1)(|v_{j}|^{2})^{D/(D+1)}}=\\left(|v_{i}|\\frac{D(|v_{i}|^{2})^{(D-1)/(D+1)}}{2(|v_{i}|^{2})^{D/(D+1)}}+|v_{j}|\\frac{D(|v_{j}|^{2})^{(D-1)/(D+1)}}{2(|v_{j}|^{2})^{D/(D+1)}}\\right.}\\\\ {\\times\\left.\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\\right.\\qquad\\qquad\\qquad\\qquad\\left.(79)\\quad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "581 Hence, when $D>1$ , we simplify the equation with a similar procedure as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{1-D}\\frac{d|v_{i}|^{2/(D+1)-1}}{d t}+\\frac{1}{1-D}\\frac{d|v_{j}|^{2/(D+1)-1}}{d t}=-2D(|v_{i}|^{1-2/(D+1)}+|v_{j}|^{1-2/(D+1)})\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "582 Defining $z_{i}=|v_{i}|^{2/(D+1)-1}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\left(z_{i}+z_{j}\\right)}{d t}=2D(D-1)\\left(\\displaystyle\\frac{1}{z_{i}}+\\frac{1}{z_{j}}\\right)\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})}\\\\ &{\\qquad\\qquad=2D(D-1)\\frac{z_{i}+z_{j}}{z_{i}z_{j}}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "583 which implies ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{i}(t)+z_{j}(t)=\\exp\\left[\\int d t\\frac{2D(D-1)}{z_{i}z_{j}}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "584 From this equation, we reach the conclusion that if $v_{i}$ and $v_{j}$ have different signs initially, one of   \n585 them converges to 0 in the long-time limit $t\\,\\rightarrow\\,\\infty$ , corresponding to condition 1 in the theorem   \n586 statement. Hence, for $D>1$ , at least one of th e \u2192con\u221editions is always satisfied at $t\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "587 Now, we prove the theorem for $D=1$ , which is similar to the proof above. The law of balance gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}[(u_{i}^{(1)})^{2}-(u_{i}^{(2)})^{2}]=-T[(u_{i}^{(1)})^{2}-(u_{i}^{(2)})^{2}]\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}^{(1)}u_{i}^{(2)})}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "588 We can see that $|u_{i}^{(1)}|\\,\\rightarrow\\,|u_{i}^{(2)}|$ takes place unless $\\begin{array}{r}{\\mathrm{Var}\\left[\\frac{\\partial\\ell}{\\partial(u_{i}^{(1)}u_{i}^{(2)})}\\right]\\ =\\ 0}\\end{array}$ , which is equivalent to   \n589 $L(\\theta)=0$ . This corresponds to condition (ii). Hence, if condition (ii) is violated, we need to prove   \n590 co(nd)i ti=on (iii). In this sense, $|u_{i}^{(1)}|\\to|u_{i}^{(2)}|$ occurs and Eq. (72) can be rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d v_{i}/d t}{\\left|v_{i}\\right|}-\\frac{d v_{j}/d t}{\\left|v_{j}\\right|}=\\big(\\mathtt{s i g n}(v_{i})-\\mathtt{s i g n}(v_{j})\\big)\\eta\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "591 When $v_{i}$ and $v_{j}$ are both positive, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d v_{i}/d t}{v_{i}}-\\frac{d v_{j}/d t}{v_{j}}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "592 With Ito\u2019s lemma, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\log(v_{i})}{d t}=\\frac{d v_{i}}{v_{i}d t}-2\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "593 Therefore, Eq. (85) can be simplified to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d(\\log(v_{i})-\\log(v_{j}))}{d t}=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "594 which indicates that all $v_{i}$ with the same sign will decay at the same rate. This differs from the case   \n595 of $D>2$ where all $v_{i}$ decay to the same value. Similarly, we can prove the case where $v_{i}$ and $v_{j}$ are   \n596 both  n>egative. ", "page_idx": 19}, {"type": "text", "text": "597 Now, we consider the case where $v_{i}$ is positive while $v_{j}$ is negative and rewrite Eq. (84) as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d v_{i}/d t}{v_{i}}+\\frac{d(|v_{j}|)/d t}{|v_{j}|}=2\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "598 Furthermore, we can derive the dynamics of $v_{j}$ with Ito\u2019s lemma: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\log(|v_{j}|)}{d t}=\\frac{d v_{i}}{v_{i}d t}-2\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "599 Therefore, Eq. (88) takes the form of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d(\\log(v_{i})+\\log(|v_{j}|))}{d t}=-2\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "600 In the long-time limit, we can see $\\log(v_{i}|v_{j}|)$ decays to $-\\infty$ , indicating that either $v_{i}$ or $v_{j}$ will decay   \n601 to 0. This corresponds to condition 1( in\u2223 th\u2223e) theorem s t\u2212ate\u221ement. Combining Eq. (87) and Eq. (90),   \n602 we conclude that all $v_{i}$ have the same sign as $t\\to\\infty$ , which indicates condition 2-(a) if conditions   \n603 in item 1 are all violated. The proof is thus complete. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "604 A.7 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "605 Proof. Following Eq. (70), we substitute $u_{i}^{(k)}$ with $v_{i}^{1/D}$ for arbitrary $k$ and obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d v_{i}}{d t}=-2(D+1)|v_{i}|^{2D/(D+1)}\\big(\\beta_{1}v-\\beta_{2}\\big)+2(D+1)|v_{i}|^{2D/(D+1)}\\sqrt{\\eta\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big)}\\frac{d W}{d t}}\\\\ {\\displaystyle\\quad\\quad+\\,2(D+1)D v_{i}^{3}|v_{i}|^{-4/(D+1)}\\eta\\big(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "606 With Eq. (78), we can see that for arbitrary $i$ and $j,v_{i}$ will converge to $v_{j}$ in the long-time limit. In   \n607 this case, we have $v=d v_{i}$ for each $i$ . Then, the SDE for $v$ can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{i v}{I t}=-\\,2(D+1)d^{2/(D+1)-1}|v|^{2D/(D+1)}\\big(\\beta_{1}v-\\beta_{2}\\big)+2(D+1)d^{2/(D+1)-1}|v|^{2D/(D+1)}\\sqrt{\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\beta_{3})}}\\\\ &{\\qquad+\\,2(D+1)D d^{4/(D+1)-2}v^{3}|v|^{-4/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "608 If $v>0$ , Eq. (92) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{i v}{H}=-2(D+1)d^{2/(D+1)-1}v^{2D/(D+1)}(\\beta_{1}v-\\beta_{2})+2(D+1)d^{2/(D+1)-1}v^{2D/(D+1)}\\sqrt{\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\beta_{3})}}}\\\\ &{}&{+\\,2(D+1)D d^{4/(D+1)-2}v^{3-4/(D+1)}\\eta(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\\eqno(93)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "609 Therefore, the stationary distribution of a general deep diagonal network is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma}^{\\left(v\\right)}\\propto\\frac{1}{v^{3\\left(1-1/(D+1)\\right)}\\left(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\right)}\\exp\\left(-\\frac{1}{T}\\int d v\\frac{d^{1-2/(D+1)}\\bigl(\\beta_{1}v-\\beta_{2}\\bigr)}{(D+1)v^{2D/(D+1)}\\bigl(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}\\bigr)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "610 If $v<0$ , Eq. (92) becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{I|v|}{d t}=-2(D+1)d^{2/(D+1)-1}|v|^{2D/(D+1)}(\\beta_{1}|v|+\\beta_{2})-2(D+1)d^{2/(D+1)-1}|v|^{2D/(D+1)}\\sqrt{\\eta(\\alpha_{1}|v|^{2}+2\\beta_{1})}}}\\\\ &{}&{+\\,2(D+1)D d^{4/(D+1)-2}|v|^{3-4/(D+1)}\\eta(\\alpha_{1}|v|^{2}+2\\alpha_{2}|v|+\\alpha_{3}).\\eqno{(95)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "611 The stationary distribution of $|v|$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left.\\left(\\left|v\\right|\\right)\\propto\\frac{1}{\\left|v\\right|^{3(1-1/(D+1))}(\\alpha_{1}|v|^{2}+2\\alpha_{2}|v|+\\alpha_{3})}\\exp\\left(-\\frac{1}{T}\\int d|v|\\frac{d^{1-2/(D+1)}(\\beta_{1}|v|+\\beta_{2})}{(D+1)|v|^{2D/(D+1)}(\\alpha_{1}|v|^{2}+2\\alpha_{2}|v|+\\alpha_{3})}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "612 Thus, we have obtained ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ast_{\\pm}(\\vert v\\vert)\\propto\\frac{1}{\\vert v\\vert^{3(1-1/(D+1))}(\\alpha_{1}\\vert v\\vert^{2}\\mp2\\alpha_{2}\\vert v\\vert+\\alpha_{3})}\\exp\\left(-\\frac{1}{T}\\int d\\vert v\\vert\\frac{d^{1-2/(D+1)}(\\beta_{1}\\vert v\\vert\\mp\\beta_{2})}{(D+1)\\vert v\\vert^{2D/(D+1)}(\\alpha_{1}\\vert v\\vert^{2}\\mp2\\alpha_{2}\\vert v\\vert}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "613 Especially when $D=1$ , the distribution function can be simplified as ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{\\pm}(|v|)\\propto\\frac{|v|^{\\pm\\beta_{2}/2\\alpha_{3}T-3/2}}{(\\alpha_{1}|v|^{2}\\mp2\\alpha_{2}|v|+\\alpha_{3})^{1\\pm\\beta_{2}/4T\\alpha_{3}}}\\exp\\left(-\\frac{1}{2T}\\frac{\\alpha_{3}\\beta_{1}-\\alpha_{2}\\beta_{2}}{\\alpha_{3}\\sqrt{\\Delta}}\\arctan\\frac{\\alpha_{1}|v|\\mp\\alpha_{2}}{\\sqrt{\\Delta}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "614 where we have used the integral ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int d v\\frac{\\beta_{1}v\\mp\\beta_{2}}{\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}}=\\frac{\\alpha_{3}\\beta_{1}-\\alpha_{2}\\beta_{2}}{\\alpha_{3}\\sqrt\\Delta}\\arctan\\frac{\\alpha_{1}|v|\\mp\\alpha_{2}}{\\sqrt\\Delta}\\pm\\frac{\\beta_{2}}{\\alpha_{3}}\\log(v)\\pm\\frac{\\beta_{2}}{2\\alpha_{3}}\\log(\\alpha_{1}v^{2}-2\\alpha_{2}v+\\alpha_{3}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "615 Furthermore, we can also see that $p(v)=\\delta(v)$ is also the stationary distribution of the Fokker-Planck   \n616 equation of Eq. (93). Hence, the ge(ne)r a=l st(ati)onary distribution of $v$ can be expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\np^{*}(v)=(1-z)\\delta(v)+z p_{\\pm}(v).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "617 The proof is complete. ", "page_idx": 20}, {"type": "text", "text": "618 A.8 Analysis of the maximum probability point ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "619 To investigate the existence of the maximum point given in Eq. (16), we treat $T$ as a variable and   \n620 study whether $(\\beta_{1}-10\\alpha_{2}T)^{2}+28\\alpha_{1}T(\\beta_{2}-3\\stackrel{.}{\\alpha}_{3}T):=A$ in the square root is always positive or not.   \n621 When $\\begin{array}{r}{T<\\frac{\\beta_{2}}{3\\alpha_{3}}=T_{c}/3}\\end{array}$ , $A$ is )p o+sitive for( arb i\u2212trary d)a t\u2236a=. When $\\begin{array}{r}{T>\\frac{\\beta_{2}}{3\\alpha_{3}}}\\end{array}$ , we divide the discussion into   \n622 several cases.  First, when $\\alpha_{1}\\alpha_{3}>\\frac{25}{21}\\alpha_{2}^{2}$ , there always exists a root for the expression $A$ . Hence, we   \n623 find that ", "page_idx": 20}, {"type": "equation", "text": "$$\nT=\\frac{-5\\alpha_{2}\\beta_{1}+7\\alpha_{1}\\beta_{2}+\\sqrt{7}\\sqrt{3\\alpha_{1}\\alpha_{3}\\beta_{1}^{2}-10\\alpha_{1}\\alpha_{2}\\beta_{1}\\beta_{2}+7\\alpha_{1}^{2}\\beta_{2}^{2}}}{2\\left(21\\alpha_{1}\\alpha_{3}-25\\alpha_{2}^{2}\\right)}:=T^{*}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "624 is a critical point. When $T_{c}/3<T<T^{*}$ , there exists a solution to the maximum condition. When   \n625 $T>T^{*}$ , there is no solution /to  t<he  m<aximum condition. ", "page_idx": 20}, {"type": "text", "text": "662276 caonndd s. $\\alpha_{2}^{2}<\\alpha_{1}\\alpha_{3}<\\textstyle{\\frac{25}{21}}\\alpha_{2}^{2}$ ,  Inw et hhisa vcea ,n ewehdi tcoh  fiunrdtihceart ecso tmhpaat rteh teh em vaaxliume ubmet pwoeiennt $5\\alpha_{2}\\beta_{1}$ $7\\alpha_{1}\\beta_{2}$ $5\\alpha_{2}\\beta_{1}\\,<\\,\\overline{{7}}\\dot{\\alpha}_{1}\\beta_{2}$ $A>0$   \n628 exists. If $5\\alpha_{2}\\beta_{1}>7\\alpha_{1}\\beta_{2}$ , w e <need to further chec k >the value of minimum of $A$ , which takes the   \n629 form of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{T}A(T)=\\frac{(25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3})\\beta_{1}^{2}-(7\\alpha_{1}\\beta_{2}-5\\alpha_{2}\\beta_{1})^{2}}{25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "630 If $\\begin{array}{r l r}{\\frac{7\\alpha_{1}}{5\\alpha_{2}}}&{<}&{\\frac{\\beta_{1}}{\\beta_{2}}\\;\\;<\\;\\;\\frac{5\\alpha_{2}+\\sqrt{25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}}}{3\\alpha_{3}}}\\end{array}$ , the minimum of $A$ is always positive and the maximum   \n631 exists. However, if $\\begin{array}{r l r}{\\frac{\\beta_{1}}{\\beta_{2}}}&{\\ge}&{\\frac{5\\alpha_{2}+\\sqrt{25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}}}{3\\alpha_{3}}}\\end{array}$ , there is always a critical learning rate $T^{*}$ . If   \n632 $\\begin{array}{r}{\\frac{\\beta_{1}}{\\beta_{2}}~=~\\frac{5\\alpha_{2}+\\sqrt{25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}}}{3\\alpha_{3}}}\\end{array}$ , there is only one critical learning rate as $\\begin{array}{r}{T_{c}~=~\\frac{5\\alpha_{2}\\beta_{1}-7\\alpha_{1}\\beta_{2}}{2\\left(25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}\\right)}}\\end{array}$ . When   \n633 $T_{c}/3\\,<\\,T\\,<\\,T^{*}$ , ther\u221ae is a solution to the maximum condition, while there is no solution when   \n634 T  T \u2217. If \u03b21 $\\begin{array}{r}{\\frac{\\beta_{1}}{\\beta_{2}}>\\frac{5\\alpha_{2}+\\sqrt{25\\alpha_{2}^{2}-21\\alpha_{1}\\alpha_{3}}}{3\\alpha_{3}}}\\end{array}$ , there are two critical points: ", "page_idx": 20}, {"type": "table", "img_path": "AM1znjeo9l/tmp/47ca97a97bb5c2968aaea2ccf90825c6f0eb54151c4658f7d3977ebacfb1adf4.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of distributions $\\overline{{p(v)}}$ in a depth-1 neural network. Here, we show the distribution in the nontrivial subspace when the( d)ata $x$ and $y$ are positively correlated. The $\\Theta(1)$ factors are neglected for concision. "], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{1,2}=\\frac{-5\\alpha_{2}\\beta_{1}+7\\alpha_{1}\\beta_{2}\\mp\\sqrt{7}\\sqrt{3\\alpha_{1}\\alpha_{3}\\beta_{1}^{2}-10\\alpha_{1}\\alpha_{2}\\beta_{1}\\beta_{2}+7\\alpha_{1}^{2}\\beta_{2}^{2}}}{2\\left(21\\alpha_{1}\\alpha_{3}-25\\alpha_{2}^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "663356 iFso rn $T<T_{1}$ oann tdo $T>T_{2}$ a, xtihmerue me xciostnsd iat isoonl.u tiTohne  tloa stth ec amsea xiis $\\begin{array}{r}{\\alpha_{2}^{2}\\,=\\,\\alpha_{1}\\alpha_{3}\\,<\\,\\frac{25}{21}\\alpha_{2}^{2}}\\end{array}$ $T_{1}<T<T_{2}$ s, et,h tehree   \n663387 cerxiptirceasls iloeanr noifn $A$ riast es iamndp ltihfeie dm aasx $\\beta_{1}^{2}\\,+\\,28\\alpha_{1}\\beta_{2}T\\,-\\,20\\alpha_{2}\\beta_{1}T$ .h e lHeessn,c ew, hew nh $\\frac{\\beta_{1}}{\\beta_{2}}~<~\\frac{7\\alpha_{1}}{5\\alpha_{2}}$ ,r teh iesr ea liws anyos   \n$\\begin{array}{r}{\\frac{\\beta_{1}}{\\beta_{2}}>\\frac{7\\alpha_{1}}{5\\alpha_{2}}}\\end{array}$   \n639 a critical learning rate as $\\begin{array}{r}{T^{*}\\,=\\,\\frac{\\beta_{1}^{2}}{20\\alpha_{2}\\beta_{1}-28\\alpha_{1}\\beta_{2}}}\\end{array}$ . W\u2217hen $T<T^{*}$ , there is a solution to the maximum   \n640 condition, while there is no solution when $T>T^{*}$ . ", "page_idx": 21}, {"type": "text", "text": "641 A.9 Other Cases for $D=1$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "642 The other cases are worth studying. For the interpolation case where the data is linear $(y=k x$ for   \n643 some $k$ ), the stationary distribution is different and simpler. There exists a nontrivial fixed  p=oint for   \n644 $\\begin{array}{r}{\\sum_{i}(u_{i}^{2}-w_{i}^{2})\\colon\\sum_{j}u_{j}\\dot{w_{j}}=\\frac{\\alpha_{2}}{\\alpha_{1}}}\\end{array}$ , which is the global minimizer of $L$ and also has a vanishing noise. It   \n645 is help ful to  note the following relationships for the data distribution when it is linear: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\alpha_{1}=\\mathrm{Var}[x^{2}],}\\\\ {\\alpha_{2}=k\\mathrm{Var}[x^{2}]=k\\alpha_{1},}\\\\ {\\alpha_{3}=k^{2}\\alpha_{1},}\\\\ {\\beta_{1}=\\mathbb{E}[x^{2}],}\\\\ {\\beta_{2}=k\\mathbb{E}[x^{2}]=k\\beta_{1}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "646 Since the analysis of the Fokker-Planck equation is the same, we directly begin with the distribution   \n647 function in Eq. (15) for $u_{i}\\,=\\,-w_{i}$ which is given by $P(|v|)\\,\\propto\\,\\delta(|v|)$ . Namely, the only possible   \n648 weights are $u_{i}=w_{i}=0$ , the  =s a\u2212me as the non-interpolatio(\u2223n \u2223c)a se\u221d. T(h\u2223is\u2223 )is because the corresponding   \n649 stationary distr ibutio n is ", "page_idx": 21}, {"type": "equation", "text": "$$\nP(|v|)\\propto\\frac{1}{|v|^{2}(|v|+k)^{2}}\\exp\\left(-\\frac{1}{2T}\\int d|v|\\frac{\\beta_{1}(|v|+k)+\\alpha_{1}\\frac{1}{T}(|v|+k)^{2}}{\\alpha_{1}|v|(|v|+k)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "650 The integral of Eq. (105) with respect to v diverges at the origin due to the factor v23 +2T \u03b111k ", "page_idx": 21}, {"type": "text", "text": "651 For the case $u_{i}=w_{i}$ , the stationary distribution is given from Eq. (15) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{P(v)\\propto\\displaystyle\\frac{1}{v^{2}(v-k)^{2}}\\exp\\left(-\\frac{1}{2T}\\int d v\\frac{\\beta_{1}(v-k)+\\alpha_{1}T(v-k)^{2}}{\\alpha_{1}v(v-k)^{2}}\\right)}}\\\\ {{\\propto v^{-\\frac{3}{2}+\\frac{\\beta_{1}}{2T\\alpha_{1}k}}(v-k)^{-2-\\frac{\\beta_{1}}{2T\\alpha_{1}k}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "652 Now, we consider the case of $\\gamma\\neq0$ . In the non-interpolation regime, when $u_{i}=-w_{i}$ , the stationary   \n653 distribution is still $p(v)\\,=\\,\\delta(v)$ .\u2260 For the case of $u_{i}\\,=\\,w_{i}$ , the stationary distri b=u t\u2212ion is the same as   \n654 in Eq. (15) after rep(la)c in=g $\\beta$ )with $\\beta_{2}^{\\prime}\\;=\\;\\beta_{2}\\,-\\,\\gamma$ . It  =still has a phase transition. The weight decay   \n655 has the effect of shifting $\\beta_{2}$ by $-\\gamma$ . I n= the  i\u2212nterpolation regime, the stationary distribution is still   \n656 $p(v)\\;=\\;\\delta(v)$ when $u_{i}~=~-w_{i}$ .  H\u2212owever, when $u_{i}~=~w_{i}$ , the phase transition still exists since the   \n657 st(ati)o n=ary( di)stribution  is= ", "page_idx": 22}, {"type": "equation", "text": "$$\np(v)\\propto\\frac{v^{-\\frac{3}{2}+\\theta_{2}}}{(v-k)^{2+\\theta_{2}}}\\exp\\left(-\\frac{\\beta_{1}\\gamma}{2T\\alpha_{1}}\\frac{1}{k(k-v)}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "658 where $\\begin{array}{r}{\\theta_{2}\\,=\\,\\frac{1}{2T\\alpha_{1}k}\\big(\\beta_{1}-\\frac{\\gamma}{k}\\big)}\\end{array}$ . The phase transition point is $\\theta_{2}=1/2$ , which is the same as the non  \n659 interpolat ion one.   \n660 The last situation is rather special, which happens when $\\Delta=0$ but $y\\ne k x$ : $y=k x-c/x$ for some   \n661 $c\\neq0$ . In this case, the parameters $\\alpha$ and $\\beta$ are the same as th=ose given  \u2260in Eq. ( 1=04) e x\u2212ce/pt for $\\beta_{2}$ : ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta_{2}=k\\mathbb{E}\\big[x^{2}\\big]-k c=k\\beta_{1}-k c.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "662 The corresponding stationary distribution is ", "page_idx": 22}, {"type": "equation", "text": "$$\nP(|v|)\\propto\\frac{|v|^{-\\frac{3}{2}-\\phi_{2}}}{(|v|+k)^{2-\\phi_{2}}}\\exp\\left(\\frac{c}{2T\\alpha_{1}}\\frac{1}{k(k+|v|)}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "663 where $\\phi_{2}~=~{\\frac{1}{2T\\alpha_{1}k}}\\bigl(\\beta_{1}\\,-\\,c\\bigr)$ . Here, we see that the behavior of stationary distribution $P(|v|)$ is   \n664 influenced by the sign of $c$ . When $c<0$ , the integral of $P(|v|)$ diverges due to the factor $\\left|v\\right|^{-{\\frac{3}{2}}-\\phi_{2}}<$   \n665 $|\\boldsymbol{v}|^{-3/2}$ and Eq. (109) becomes $\\delta(|v|)$ <again. However, whe(n\u2223 $c>0$ , the integral of $|v|$ may n \u2223ot\u2223 diverge<.   \n666 \u2223T\u2223he critical point is $\\textstyle{\\frac{3}{2}}+\\phi_{2}=1$ o(r\u2223 e\u2223q)uivalently: $c=\\beta_{1}+T\\alpha_{1}k$ . This is because  \u2223wh\u2223en $c<0$ , the data   \n667 points are all distribu te+d a b=ove the line $y\\,=\\,k x$ .  =Hen c+e, $u_{i}\\,=\\,-w_{i}$ can only give a tri v<ial solution.   \n668 However, if $c>0$ , there is the possibility  t=o learn the negat iv=e  \u2212slope $k$ . When $0<c<\\beta_{1}+T\\alpha_{1}k$ ,   \n669 the integral of $P(|v|)$ still diverges and the distribution is equivalent to $\\delta(|v|)$ . No w<, w e< con s+ider the   \n670 case of $u_{i}=w_{i}$ . The stationary distribution is ", "page_idx": 22}, {"type": "equation", "text": "$$\nP(|v|)\\propto\\frac{|v|^{-\\frac{3}{2}+\\phi_{2}}}{(|v|-k)^{2+\\phi_{2}}}\\exp\\left(-\\frac{c}{2T\\alpha_{1}}\\frac{1}{k-|v|}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "671 It also contains a critical point: $\\textstyle-{\\frac{3}{2}}+\\phi_{2}=-1$ , or equivalently, $c=\\beta_{1}-\\alpha_{1}k T$ . There are two cases.   \n672 When $c<0$ , the probability densit y only has support for $|v|>k$ since the gradient always pulls the   \n673 paramet e<r $|v|$ to the region $|v|>k$ . Hence, the divergen c\u2223e\u2223 a>t $|v|\\,=\\,0$ is of no effect. When $c>0$ ,   \n674 the probability density has support on $0<|v|<k$ for the same reason. Therefore, if $\\beta_{1}\\,>\\,\\alpha_{1}k T$ ,   \n675 there exists a critical point $c=\\beta_{1}-\\alpha_{1}k T$ <.  \u2223W\u2223h<en $c>\\beta_{1}-\\alpha_{1}k T$ , the distribution functi o>n $P(|v|)$   \n676 becomes $\\delta(|v|)$ . When $c<\\beta_{1}\\!-\\!\\alpha_{1}k T$ , the integral of  t>he di s\u2212tribution function is finite for $0<|\\boldsymbol{v}|<k$ ,   \n677 indicating the learning of the neural network. If $\\beta_{1}\\leq\\alpha_{1}k T$ , there will be no criticality and $P(|v|)$   \n678 is always equivalent to $\\delta(|v|)$ . The effect of havin g weight decay can be similarly analyzed, and   \n679 the result can be systema(ti\u2223c\u2223a)lly obtained if we replace $\\beta_{1}$ with $\\beta_{1}+\\gamma/k$ for the case $u_{i}\\,=\\,-w_{i}$ or   \n680 replacing $\\beta_{1}$ with $\\beta_{1}-\\gamma/k$ for the case $u_{i}=w_{i}$ . ", "page_idx": 22}, {"type": "text", "text": "681 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "88 Guidelines:   \n89 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n90 made in the paper.   \n91 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n92 contributions made in the paper and important assumptions and limitations. A No or   \n93 NA answer to this question will not be perceived well by the reviewers.   \n94 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n95 much the results can be expected to generalize to other settings.   \n96 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these   \n97 goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "698 2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in the Discussion session at lines 369-371. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "730 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "731 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n732 a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We believe that the assumptions are clarified and complete proofs are provided for the theoretical parts. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n8 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n9 referenced.   \n0 \u2022 All assumptions should be clearly stated or referenced in the statement of any theo  \n1 rems.   \n2 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n3 they appear in the supplemental material, the authors are encouraged to provide a   \n4 short proof sketch to provide intuition.   \n45 \u2022 Inversely, any informal proof provided in the core of the paper should be comple  \n6 mented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "748 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "49 Question: Does the paper fully disclose all the information needed to reproduce the main   \n50 experimental results of the paper to the extent that it affects the main claims and/or conclu  \n51 sions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We believe that all of the experimental results are reproducable in our work. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "786 5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "787 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n788 tions to faithfully reproduce the main experimental results, as described in supplemental   \n789 material?   \n790 Answer: [No]   \n791 Justification: The code or data of the experiments are simple and easy to reproduce follow  \n792 ing the description in the main text.   \n793 Guidelines:   \n794 \u2022 The answer NA means that paper does not include experiments requiring code.   \n795 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n796 public/guides/CodeSubmissionPolicy) for more details.   \n797 \u2022 While we encourage the release of code and data, we understand that this might not   \n798 be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n799 including code, unless this is central to the contribution (e.g., for a new open-source   \n800 benchmark).   \n801 \u2022 The instructions should contain the exact command and environment needed to run to   \n802 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n803 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n804 \u2022 The authors should provide instructions on data access and preparation, including how   \n805 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n806 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n807 proposed method and baselines. If only a subset of experiments are reproducible, they   \n808 should state which ones are omitted from the script and why.   \n809 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n810 versions (if applicable).   \n811 \u2022 Providing as much information as possible in supplemental material (appended to the   \n812 paper) is recommended, but including URLs to data and code is permitted.   \n813 6. Experimental Setting/Details   \n814 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n815 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n816 results?   \n817 Answer: [Yes]   \n818 Justification: We have specified the training and test details in the captions of the experi  \n819 ments in Figs. 2,4, and 5.   \n820 Guidelines:   \n821 \u2022 The answer NA means that the paper does not include experiments.   \n822 \u2022 The experimental setting should be presented in the core of the paper to a level of   \n823 detail that is necessary to appreciate the results and make sense of them.   \n824 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n825 material.   \n826 7. Experiment Statistical Significance   \n827 Question: Does the paper report error bars suitably and correctly defined or other appropri  \n828 ate information about the statistical significance of the experiments?   \n829 Answer: [No]   \n830 Justification: Here the dynamics is deterministic and there is no need to consider the error   \n831 bars here.   \n832 Guidelines:   \n833 \u2022 The answer NA means that the paper does not include experiments.   \n834 \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confi  \n835 dence intervals, or statistical significance tests, at least for the experiments that support   \n836 the main claims of the paper.   \n837 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n838 example, train/test split, initialization, random drawing of some parameter, or overall   \n839 run with given experimental conditions).   \n840 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n841 call to a library function, bootstrap, etc.)   \n842 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n843 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n844 of the mean.   \n845 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should prefer  \n846 ably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of   \n847 Normality of errors is not verified.   \n848 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n849 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n850 error rates).   \n851 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n852 they were calculated and reference the corresponding figures or tables in the text.   \n853 8. Experiments Compute Resources   \n854 Question: For each experiment, does the paper provide sufficient information on the com  \n855 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n856 the experiments?   \n857 Answer: [No]   \n858 Justification: The experiments can be simply conducted on personal computers.   \n859 Guidelines:   \n860 \u2022 The answer NA means that the paper does not include experiments.   \n861 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n862 or cloud provider, including relevant memory and storage.   \n863 \u2022 The paper should provide the amount of compute required for each of the individual   \n864 experimental runs as well as estimate the total compute.   \n865 \u2022 The paper should disclose whether the full research project required more compute   \n866 than the experiments reported in the paper (e.g., preliminary or failed experiments   \n867 that didn\u2019t make it into the paper).   \n868 9. Code Of Ethics   \n869 Question: Does the research conducted in the paper conform, in every respect, with the   \n870 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n871 Answer: [Yes]   \n872 Justification: We have confirmed that the research is conducted with the NeurIPS Code of   \n873 Ethics.   \n874 Guidelines:   \n875 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n876 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n877 deviation from the Code of Ethics.   \n878 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n879 eration due to laws or regulations in their jurisdiction).   \n880 10. Broader Impacts   \n881 Question: Does the paper discuss both potential positive societal impacts and negative   \n882 societal impacts of the work performed?   \n883 Answer: [NA]   \n884 Justification: Our work is a fundamental research on the dynamics of SGD and hence it   \n885 does not have direct positive or negative societal impacts.   \n886 Guidelines:   \n887 \u2022 The answer NA means that there is no societal impact of the work performed.   \n888 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n889 impact or why the paper does not address societal impact.   \n890 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n891 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n892 (e.g., deployment of technologies that could make decisions that unfairly impact spe  \n893 cific groups), privacy considerations, and security considerations.   \n894 \u2022 The conference expects that many papers will be foundational research and not tied   \n895 to particular applications, let alone deployments. However, if there is a direct path to   \n896 any negative applications, the authors should point it out. For example, it is legitimate   \n897 to point out that an improvement in the quality of generative models could be used to   \n898 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n899 that a generic algorithm for optimizing neural networks could enable people to train   \n900 models that generate Deepfakes faster.   \n901 \u2022 The authors should consider possible harms that could arise when the technology is   \n902 being used as intended and functioning correctly, harms that could arise when the   \n903 technology is being used as intended but gives incorrect results, and harms following   \n904 from (intentional or unintentional) misuse of the technology.   \n905 \u2022 If there are negative societal impacts, the authors could also discuss possible mitiga  \n906 tion strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n907 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n908 feedback over time, improving the efficiency and accessibility of ML).   \n909 11. Safeguards   \n910 Question: Does the paper describe safeguards that have been put in place for responsible   \n911 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n912 image generators, or scraped datasets)?   \n913 Answer: [No]   \n914 Justification: We believe there is no risks for misuse for the data and models.   \n915 Guidelines:   \n916 \u2022 The answer NA means that the paper poses no such risks.   \n917 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n918 necessary safeguards to allow for controlled use of the model, for example by re  \n919 quiring that users adhere to usage guidelines or restrictions to access the model or   \n920 implementing safety filters.   \n921 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n922 should describe how they avoided releasing unsafe images.   \n923 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n924 not require this, but we encourage authors to take this into account and make a best   \n925 faith effort.   \n926 12. Licenses for existing assets   \n927 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n928 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n929 properly respected?   \n930 Answer:[NA]   \n931 Justification: [NA]   \n932 Guidelines:   \n933 \u2022 The answer NA means that the paper does not use existing assets.   \n934 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n935 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n936 URL.   \n937 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n938 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n939 service of that source should be provided.   \n940 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n941 package should be provided. For popular datasets, paperswithcode.com/   \n942 datasets has curated licenses for some datasets. Their licensing guide can help   \n943 determine the license of a dataset.   \n944 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n945 the derived asset (if it has changed) should be provided.   \n946 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n947 the asset\u2019s creators.   \n948 13. New Assets   \n949 Question: Are new assets introduced in the paper well documented and is the documenta  \n950 tion provided alongside the assets?   \n951 Answer: [No]   \n952 Justification: Nothing introduced.   \n953 Guidelines:   \n954 \u2022 The answer NA means that the paper does not release new assets.   \n955 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n956 submissions via structured templates. This includes details about training, license,   \n957 limitations, etc.   \n958 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n959 asset is used.   \n960 \u2022 At submission time, remember to anonymize your assets (if applicable). You can   \n961 either create an anonymized URL or include an anonymized zip file.   \n962 14. Crowdsourcing and Research with Human Subjects   \n963 Question: For crowdsourcing experiments and research with human subjects, does the pa  \n964 per include the full text of instructions given to participants and screenshots, if applicable,   \n965 as well as details about compensation (if any)?   \n966 Answer: [NA]   \n967 Justification: We believe that neither the crowdsourcing nor the research with human sub  \n968 jects is included in our work.   \n969 Guidelines:   \n970 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n971 with human subjects.   \n972 \u2022 Including this information in the supplemental material is fine, but if the main contri  \n973 bution of the paper involves human subjects, then as much detail as possible should   \n974 be included in the main paper.   \n975 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, cura  \n976 tion, or other labor should be paid at least the minimum wage in the country of the   \n977 data collector.   \n978 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n979 Subjects   \n980 Question: Does the paper describe potential risks incurred by study participants, whether   \n981 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n982 approvals (or an equivalent approval/review based on the requirements of your country or   \n983 institution) were obtained?   \n984 Answer: [NA]   \n985 Justification: Our work does not contain crowdsourcing or research with human subjects.   \n986 Guidelines:   \n987 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n988 with human subjects.   \n989 \u2022 Depending on the country in which research is conducted, IRB approval (or equiva  \n990 lent) may be required for any human subjects research. If you obtained IRB approval,   \n991 you should clearly state this in the paper. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]