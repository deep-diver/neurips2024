[{"figure_path": "FuTfZK7PK3/tables/tables_3_1.jpg", "caption": "Table 1: General comparison of FedExP, RPM and FedExProx in terms of conditions and convergence. Each entry indicates whether the method has the corresponding feature (\u2714) or not (X). We use the sign \"-\" where a feature is not applicable to the corresponding method.", "description": "This table compares three federated learning algorithms: FedExP, RPM, and FedExProx.  It highlights key differences across several features: the requirement for an interpolation regime or convexity assumptions, whether they provide acceleration in strongly convex settings, the necessity of smoothness assumptions, the ability to handle partial client participation, and whether they use a constant extrapolation parameter or adaptively determine it based on smoothness or partial participation. Finally, it notes if the algorithm exhibits 'semi-adaptivity', a property related to convergence.", "section": "1.1 Contributions"}, {"figure_path": "FuTfZK7PK3/tables/tables_3_2.jpg", "caption": "Table 2: Comparison of convergence of FedExP, FedProx, FedExProx, FedExProx-GraDS and FedExProx-StoPS. The local step size of FedExP is set to be the largest possible value 1/6tL in the full batch case, where t is the number of local iterations of GD performed. We assume the assumptions of Theorem 1 also hold here. The notations are introduced in Theorem 1 and Theorem 2. The convergence for our methods are described for arbitrary \u03b3 > 0. We use K to denote the total number of iterations. For FedExProx, optimal constant extrapolation is used. The O (\u00b7) notation is hidden for all complexities in this table.", "description": "This table compares the convergence rates of several federated learning optimization methods.  It shows the iteration complexity (general, best-case, and worst-case scenarios) for FedExP, FedProx, and the proposed FedExProx variants (FedExProx, FedExProx-GraDS, and FedExProx-StoPS).  The table highlights the impact of various factors, including whether the model is strongly convex, the number of participating devices, and the use of adaptive extrapolation strategies.  The results demonstrate improved convergence rates for the proposed methods.", "section": "1.1 Contributions"}, {"figure_path": "FuTfZK7PK3/tables/tables_15_1.jpg", "caption": "Table 1: General comparison of FedExP, RPM and FedExProx in terms of conditions and convergence. Each entry indicates whether the method has the corresponding feature (\u2714) or not (X). We use the sign \"-\" where a feature is not applicable to the corresponding method.", "description": "This table compares three different federated learning methods: FedExP, RPM, and FedExProx.  It highlights key features and requirements of each method, such as whether they require the interpolation regime, convexity, smoothness, and constant extrapolation parameters.  It also indicates whether each method supports partial client participation and adaptive extrapolation strategies.  The table helps readers understand the differences and trade-offs among the three methods.", "section": "1.1 Contributions"}, {"figure_path": "FuTfZK7PK3/tables/tables_20_1.jpg", "caption": "Table 1: General comparison of FedExP, RPM<sup>a</sup> and FedExProx in terms of conditions and convergence. Each entry indicates whether the method has the corresponding feature (\u2713) or not (\u2717). We use the sign \u201c\u2014\u201d where a feature is not applicable to the corresponding method.", "description": "This table compares three methods: FedExP, RPM, and FedExProx, highlighting their requirements and convergence properties.  It shows whether each method needs an interpolation regime, convexity, smoothness,  allows partial client participation, uses a constant extrapolation parameter, and has semi-adaptivity.  It provides a concise summary of the key differences between the algorithms.", "section": "1.1 Contributions"}, {"figure_path": "FuTfZK7PK3/tables/tables_25_1.jpg", "caption": "Table 5: Summary of convergence of new algorithms appeared in our paper in the convex setting. The O(\u00b7) notation is hidden for all complexities in this table. For convergence in the full client participation case, results of Theorem 1 and Theorem 2 are used where the relevant notations are defined. For convergence in the partial participation, the results of Theorem 5 are used.", "description": "This table summarizes the iteration complexities of FedExProx, FedExProx-GraDS, and FedExProx-StoPS in different participation settings (full participation, partial participation, and single client).  The complexities are given in Big O notation and depend on parameters such as the smoothness constant (Lmax or Ly), the step size (\u03b3), and the extrapolation parameter (\u03b1k). The table provides a concise comparison of the convergence rates of the proposed algorithms and highlights the impact of different participation strategies on the overall iteration complexity.", "section": "1.1 Contributions"}]