[{"type": "text", "text": "Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bahri Batuhan Bilecen\u2217 Ahmet Berke Gokmen\u2217 Aysegul Dundar Bilkent University, Department of Computer Engineering, Ankara, T\u00fcrkiye {batuhan.bilecen@, berke.gokmen@ug, adundar@cs}.bilkent.edu.tr ", "page_idx": 0}, {"type": "image", "img_path": "SlDx451MjC/tmp/a7202ffa6d6b8c07b29563348c249536c949cf4f3a6e4e6f5c00b77ae287cd4c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: From a single input image (first column), our framework reconstructs 3D representation by inverting images into PanoHead\u2019s latent space, which can be viewed in a 360-degree perspective. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D GAN inversion aims to project a single image into the latent space of a 3D Generative Adversarial Network (GAN), thereby achieving 3D geometry reconstruction. While there exist encoders that achieve good results in 3D GAN inversion, they are predominantly built on EG3D, which specializes in synthesizing near-frontal views and is limiting in synthesizing comprehensive 3D scenes from diverse viewpoints. In contrast to existing approaches, we propose a novel framework built on PanoHead, which excels in synthesizing images from a 360-degree perspective. To achieve realistic 3D modeling of the input image, we introduce a dual encoder system tailored for high-fidelity reconstruction and realistic generation from different viewpoints. Accompanying this, we propose a stitching framework on the triplane domain to get the best predictions from both. To achieve seamless stitching, both encoders must output consistent results despite being specialized for different tasks. For this reason, we carefully train these encoders using specialized losses, including an adversarial loss based on our novel occlusion-aware triplane discriminator. Experiments reveal that our approach surpasses the existing encoder training methods qualitatively and quantitatively. Please visit the project page. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In the realm of generative models, 2D GANs have gained renown for their remarkable ability to achieve striking realism through adversarial training, effectively capturing intricate details and textures to produce visually convincing images, especially on face images [18, 19]. However, their inherent limitation lies in their lack of depth perception, which restricts their applicability in three-dimensional contexts. In contrast, 3D GANs mark a groundbreaking advancement by seamlessly integrating Neural Radiance Fields (NeRF) into their architecture [13, 9, 14, 5]. This integration not only allows them to match the realism of their 2D counterparts but also ensures consistency in three-dimensional geometry. While extensive studies have focused on 2D GAN inversion [42, 29, 31, 4, 33, 28], recent efforts have seen the proposal of inversion methods tailored for 3D GANs [39, 7, 21]. ", "page_idx": 1}, {"type": "text", "text": "2D GAN inversion techniques focus on projecting the images into the GAN\u2019s natural latent space to enhance editability and achieve high fidelity to the input image; however, inverting 3D GAN models presents additional challenges. This process requires accurate 3D reconstruction, which means realistic fliling of invisible regions, ensuring coherence and completeness in the resulting three-dimensional scenes. Recently, inversion methods for 3D GANs have been developed, firstly, optimization-based and then encoder-based methods. Optimization-based methods [20, 35, 38] employ reconstruction losses to invert images into a latent code specific to the given view. Furthermore, network parameters are optimized by generating pseudo-multi-view images from the optimized latent codes to enhance detail preservation. Such optimization is required for each inference image; it is time-consuming and requires GPUs with large memory. Therefore, researchers focus on encoder-based methods [39, 7, 21]. While successful inversions are achieved with these methods, they rely on EG3D [9] framework [39, 7, 21], which is constrained to synthesizing near-frontal views. However, our work utilizes PanoHead [5], a method capable of rendering full-head image synthesis, enabling a comprehensive 360-degree perspective. This advancement introduces additional challenges, particularly concerning the invisibility of many parts in the input image. Despite this, the inversion model is expected to reasonably predict and reconstruct these occluded regions to ensure high-quality 3D reconstruction. Our experiments demonstrate that extending the methods proposed for EG3D is ineffective. ", "page_idx": 1}, {"type": "text", "text": "When projecting images onto PanoHead\u2019s latent space, we observe a trade-off between achieving high-fidelity reconstruction of the input image and generating realistic representations of the invisible parts of the head. Some models can perfectly reconstruct the image from a given view but produce unrealistic outputs when the camera parameters change. Conversely, other models generate realistic representations under varying camera parameters but fail to achieve high-fidelity reconstruction of the input image. To achieve high-fidelity reconstruction of the input image and realistic representations of the invisible parts of the head simultaneously, we train a dual encoder. One encoder specializes in reconstructing the given view, while the other focuses on generating high-quality invisible views. We propose stitching the triplane domain generations to produce the final result. This approach combines the outputs from both encoders to achieve both high-fidelity reconstructions of the given view and high-quality representations of the invisible parts of the head. To achieve seamless stitching, both encoders must output consistent results despite being specialized for different tasks. For this reason, we carefully train these encoders using specialized losses, including an adversarial loss based on our novel occlusion-aware triplane discriminator. This ensures that both encoders learn to produce consistent and complementary outputs, enabling seamless stitching of generations for the final result. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To achieve high fidelity to the input and realistic generations for different camera views, we train dual encoders and introduce a stitching pipeline that combines the best predictions from both encoders for visible and invisible regions.   \n\u2022 We propose a novel occlusion-aware discriminator that enhances both fidelity and realism.   \n\u2022 We conduct extensive experiments to show the effectiveness of our framework. Quantitative and qualitative results show the superiority of our method compared to the state-of-the-art. Some visual results can be seen in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3D Generative Models. Generative Adversarial Networks (GANs), coupled with differentiable renderers, have achieved significant strides in generating 3D-aware multi-view consistent images. ", "page_idx": 1}, {"type": "text", "text": "While early efforts, such as HoloGAN [24], operating on voxel representations, subsequent works shifted towards mesh representations [27, 15], and the latest advancements are built around implicit representations [10, 14, 26, 25]. Among implicit representations, triplane representations have emerged as a popular choice due to their computational efficiency and the high-quality outputs [9, 13, 5]. The architectures of works like EG3D [9] and PanoHead [5] bear resemblance to the structure of StyleGAN2 [19]. They consist of mapping and synthesis networks, generating triplanes which are subsequently projected to a 2D image through volumetric rendering operations akin to those used in NeRF [23]. While EG3D is trained on the FFHQ [18] dataset with limited angle diversity, PanoHead achieves a 360-degree perspective in face generation thanks to their dataset selection and model improvements. In our work, we delve into PanoHead\u2019s latent space and construct our inversion encoder based on PanoHead. ", "page_idx": 2}, {"type": "text", "text": "GAN Inversion. In recent years, GAN inversion, particularly in the context of StyleGAN, has garnered significant attention due to its extensive editing capabilities. The primary objective of these studies is to embed an image into StyleGAN\u2019s latent space, enabling subsequent modifications. Initially, this was approached through latent optimization, where latent codes were iteratively adjusted using back-propagation to minimize the reconstruction loss between the generated and target images [11, 1, 2, 19, 30]. For 3D-aware GAN model inversions, supplementary heuristics have been introduced into the optimization process. These include considerations like facial symmetry [38] and multi-view optimization strategies [35]. However, such methods are computationally intensive as they require optimizing each image\u2019s latent codes. Moreover, while minimizing the reconstruction loss can yield visually similar results, it does not guarantee that the image resides within the natural latent space of GANs. This distinction is crucial for effective image editing. Without aligning with StyleGAN\u2019s inherent latent space, reconstructed images may not respond correctly to editing techniques, thus limiting their practical utility. This consideration also extends to 3D-aware GAN inversion methods, where encoding geometric information is paramount. Even if an input image can be faithfully reconstructed, its realism may falter when observed from alternative viewpoints, emphasizing the importance of aligning with the GAN\u2019s native latent space. ", "page_idx": 2}, {"type": "text", "text": "To enhance efficiency, image encoders have been specifically trained for the inversion task, initially targeting StyleGAN [42, 29, 31, 4, 28, 36, 37], and more recently for EG3D [39, 7, 21]. These specialized encoders capitalize on insights gained from training datasets to swiftly project images into latent spaces. Moreover, they can be trained with diverse objectives beyond mere image reconstruction. For instance, some employ discriminators to compare generated and real images and latent space discriminators to ensure inversion aligns with the GAN\u2019s natural latent space. As a result, these methods generally offer faster inversion processes. This study focuses on 3D-GAN inversion, specifically targeting PanoHead [5]. The task poses significant challenges due to PanoHead\u2019s ability to capture a comprehensive 360-degree perspective, necessitating the prediction of a substantial portion of invisible elements by inversion encoders. Our experiments demonstrate that models trained for EG3D are ineffective in this context. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview of PanoHead ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "SlDx451MjC/tmp/f068a3148acbeb77889314a4d5314969d1417d4b177cd34c0973cfe5d056fe18.jpg", "img_caption": ["Figure 2: Overall architecture of PanoHead. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The overall architecture of PanoHead is given in Fig. 2. Resemblant to EG3D, PanoHead utilizes a mapping network that takes a random vector $z$ and the camera conditioning \u03c0mapping. After $z$ is mapped to a $w$ , StyleGANbased backbone G generates a tri-grid triplane. Unlike EG3D, PanoHead\u2019s triplanes have 3 times the number of channels in comparison, hence the name tri-grid. This approach is stated ", "page_idx": 2}, {"type": "text", "text": "to ease 360-degree synthesis. The resultant triplane is then rendered via a volumetric neural renderer $\\mathcal{R}$ with pose $\\pi_{\\mathrm{render}}$ and super-resolved to yield a synthesized image. ", "page_idx": 2}, {"type": "text", "text": "3.2 Training an encoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces the general pipeline of the encoders employed in our dual-encoder framework. Each encoder takes an input image I and predicts the latent code $w^{+}$ , which is then passed to the generator to produce triplane features. These synthesized features are then fed into the renderer to generate a 2D image with a specified camera parameter $\\pi_{c a m}$ , as described by Eq. (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\epsilon}_{w^{+}=\\mathbf{\\epsilon}\\mathbf{E_{1}}(\\mathbf{I})}}\\\\ {\\mathbf{I_{out}^{s v}}_{=\\mathcal{R}(\\mathbf{G}(w^{+}),\\pi_{c a m})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{I_{out}^{s v}}$ denotes the output rendering for the same view as the input, and $\\mathbf{E_{1}}$ represents the encoder. While the $\\mathcal{W}^{+}$ space allows for leveraging priors embedded in the generator, its limited expressive power in reconstructing image details has been noted due to the information bottleneck of its $14\\times512$ dimensions. To address this limitation, both 2D inversion techniques [33, 28] and 3D GAN inversion methods [7] permit higher-rate features to pass to the generators, facilitating the capture of fine details. In 3D GAN inversion methods, these higher-rate features are encoded through a smaller second network and transmitted to the triplane features. We adopt a similar approach in our encoders. We refer to the final output as $\\mathbf{I}_{\\mathbf{final}}^{\\mathbf{sv}}$ . Further details of the architecture are provided in the Appendix. ", "page_idx": 3}, {"type": "text", "text": "The primary challenge in this setting arises from establishing appropriate training objective losses, as our training dataset consists solely of single images, providing ground truth only for the rendered image from the same view as the input. For these output and ground-truth pairs, we set the usual reconstruction losses, namely, LPIPS perceptual loss [41], $\\mathcal{L}_{2}$ reconstruction loss (MSE), and ArcFace [12] based identity loss as given in Eq. (2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathbf{E}_{1}}\\mathcal{L}_{\\mathrm{LPIPS}}(\\mathbf{I}_{\\mathbf{final}}^{\\mathbf{sv}},\\mathbf{I})+\\mathcal{L}_{2}(\\mathbf{I}_{\\mathbf{final}}^{\\mathbf{sv}},\\mathbf{I})+\\mathcal{L}_{\\mathrm{identity}}(\\mathbf{I}_{\\mathbf{final}}^{\\mathbf{sv}},\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While models trained with the objective given in Eq. (2) learn to reconstruct a given view, they often struggle to generalize and produce realistic features from other camera views. Consequently, while our first encoder is trained with the objective in Eq. (2), we design an adversarial-based loss objective for our second encoder. This second encoder generates realistic predictions for invisible views, as explained in Section 3.3. ", "page_idx": 3}, {"type": "image", "img_path": "SlDx451MjC/tmp/f34e00efe844ceb9ef05249b23828fefcada2a3f4f123643bdbd8875ce3475b7.jpg", "img_caption": ["3.3 Occlusion-aware triplane discriminator "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Our training methodology for the triplane discriminator involves generating real samples by sampling latent vectors $\\mathcal{Z}^{+}$ and producing in-domain triplanes using PanoHead. Fake samples are generated from encoded images. Despite the effectiveness of adversarial loss in enhancing reconstructions, challenges may persist in achieving high fidelity to the input due to the origin of real samples from the generator G. To address this, we propose an occlusion-aware discriminator $\\mathcal{D}$ , trained exclusively with features from occluded pixels. This ensures that visible regions, such as frontal views $\\pi_{R}$ , have reduced influence during the training of $\\mathcal{D}$ . ", "page_idx": 3}, {"type": "text", "text": "To achieve a realistic reconstruction of the 3D model, represented in a triplane structure, it is essential to guide the encoder for visible views and overall coherence. Since we lack one-to-one ground truth to guide the triplane structure, we experiment with various setups incorporating adversarial losses. A naive approach to utilize adversarial loss would be to render estimated triplanes from other views and assess the realism of these 2D images using a discriminator. However, our experiments observe that this setup hinders the model\u2019s ability to learn high fidelity to the input image, as will be further detailed in Section 5.2. Moreover, randomly rendering different views can only guide limited parts of the triplane structure rather than the overall. ", "page_idx": 3}, {"type": "image", "img_path": "SlDx451MjC/tmp/cd949e6471c3150577486775cd58e2b70511256e69fc1c0af3099fc146ac6cfb.jpg", "img_caption": ["Figure 4: The inference pipeline with dual encoders for full 3D head reconstruction. Given a face portrait with pose $\\pi_{R}$ , we can perform 360-degree rendering from any given pose $\\pi_{\\mathrm{novel}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To overcome this limitation and avoid the computational burden of rendering unnecessary views, we explore the possibility of training a discriminator in the triplane domain. In our training process for the triplane discriminator, we follow a procedure where we sample latent vectors $\\mathcal{Z}^{+}$ and generate in-domain triplanes using PanoHead, serving as our real samples. Meanwhile, the fake samples are triplanes generated from encoded images, as depicted in Fig. 3. Despite the observed improvement in reconstructions facilitated by this adversarial loss, we note a persistent challenge hindering the network\u2019s ability to achieve high fidelity to the input. This discrepancy may stem from the real samples originating from the generator, lacking the detailed feature characteristic of real-world images. Therefore, this may lead the encoder to omit to encode realistic facial details if they are absent in the synthesized samples. We propose our occlusion-aware discriminator to overcome this limitation. This discriminator is exclusively trained with features corresponding to occluded pixels. This approach ensures that triplane features associated with visible regions, such as a frontal face, are not utilized for discriminator training. Additionally, we introduce a masking mechanism for synthesized triplanes to mitigate any distribution mismatch arising between encoded and synthesized triplanes. This masking process contributes to aligning the distributions of real and fake samples, further enhancing the coherence of the training dynamics. ", "page_idx": 4}, {"type": "text", "text": "We find the set of visible points based on the depth map of the given view via inverse rendering. Specifically, the occlusion mask $O_{\\pi_{R}}$ is estimated by Eq. (3): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{O}_{\\pi_{R}}=\\mathbb{R}^{3}\\setminus\\{\\mathbf{p}[x,y,z]\\ :\\ \\pi_{R}\\mathcal{D}\\boldsymbol{K}^{-1}\\mathbf{I}[u,v,1]^{T}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{p}[x,y,z]$ is the triplane coordinates, $\\pi_{R}$ is the extrinsic camera parameters of the input view, $\\mathcal{D}$ is the depth map from the input view, $\\kappa$ is the intrinsic camera parameters, $\\mathbf{I}[u,v,1]$ are the homogeneous coordinates of the input image I rendered from the input view. More clearly, from the current camera pose, we map back to the depth values to obtain the mask of visible regions $(1{-}\\mathcal{O}_{\\pi_{R}})$ . Then, we invert the visible region mask to obtain $O_{\\pi_{R}}$ . The utilization of occlusion masks has been previously investigated in 3D methodologies, albeit in different contexts. For instance, they have been used in generating pseudo-ground truth images to facilitate optimization-based 3D reconstruction [38] and integrated into passing high-rate residual features to the triplane [39]. However, it is the first time used in the discriminator. This allows for a selective focus on regions where the encoder may encounter challenges in faithfully replicating realism. ", "page_idx": 4}, {"type": "text", "text": "Compliant with recent advancements in adversarial training, we follow WGAN loss [6] for ${\\mathcal{L}}_{\\mathrm{adv}}$ in Eq. (4), where $\\mathbf{T_{final}^{s v}}$ and $\\mathbf{T_{synth}}$ are encoded and $\\mathcal{Z}^{+}$ synthesized triplanes, respectively. Details are given in the Appendix. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathbf{E_{2}}}\\operatorname*{max}_{\\mathcal{D}}\\mathcal{L}_{\\mathrm{adv}}\\big(\\mathcal{O}_{\\pi_{R}}\\mathbf{T}_{\\mathbf{final}}^{\\mathbf{sv}},\\mathcal{O}_{\\pi_{R}}\\mathbf{T}_{\\mathbf{synth}}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "SlDx451MjC/tmp/45edd346f7677e145a2252f943a556bce613aa48bb5c4f72f14f265f21f594bd.jpg", "img_caption": ["Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In our approach, we train two encoders: the first, as outlined in Section 3.2, and the second, augmented with an additional adversarial loss detailed in Section 3.3. While the initial encoder excels at reconstructing high-fidelity facial images from the input, it often produces unrealistic results for other viewpoints, as depicted in Fig. 5. Conversely, the second encoder yields better overall outcomes, albeit with slightly diminished fidelity to the input face. ", "page_idx": 5}, {"type": "text", "text": "Our aim is to devise a dual-encoder pipeline that harnesses the strengths of both encoded features. To achieve this, we leverage the occlusion masks derived in Section 3.3, as illustrated in Fig. 4. By combining the visible portions from Encoder 1 and the occluded segments from Encoder 2, we generate our final output, as demonstrated in the last row of Fig. 5. ", "page_idx": 5}, {"type": "text", "text": "While each encoder contributes partially to the ultimate feature, achieving seamless integration necessitates consistency in the output of both encoders despite their distinct specializations. For instance, if Encoder 1 flawlessly renders a given view of the face but fails to capture the correct geometry, artifacts may arise in the combined result. Thus, it remains imperative to train both encoders comprehensively to ensure an overall high-quality outcome. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We combined images of FFHQ [18] and LPFF [34] and split it for training $(\\sim\\!140\\mathrm{k})$ and validation $(\\sim\\!14\\mathrm{k})$ . CelebA-HQ [17] and multi-view MEAD [32] are employed for additional evaluation. We removed face portrait backgrounds for training and evaluation datasets, applied camera and image mirroring during training, and performed pose rebalancing proposed in [34] as data augmentation. We utilized the same dataset to train competitive methods for fair evaluation. The models are trained for $500\\mathrm{k}$ iterations with a batch size of 3 on a single RTX 4090 GPU. The learning rate is $1e^{-4}$ for both encoders and the occlusion-aware discriminator. Ranger is utilized as the optimizer, which is a combination of Rectified Adam [22] with Lookahead [40]. ", "page_idx": 5}, {"type": "text", "text": "4.2 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The baseline models are provided in Table 1. We note that no encoder pipelines are aimed for full 360-degree head reconstruction. We train the models with the author\u2019s released code to invert images into PanoHead\u2019s latent space. ", "page_idx": 5}, {"type": "text", "text": "4.3 Evaluation metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We report $\\mathcal{L}_{2}$ , LPIPS [41] and ID [12] scores for original-view reconstruction, which measure the fidelity to the input image. For the novel-view quality, we measure Fr\u00e9chet inception distance (FID) [16]. Since our validation datasets have limited angle variance, we measure the distance between 1k randomly synthesized and 1k encoded real-life image distributions. The images are rendered from varying yaw angles, covering the 360-degree range to include occluded regions. We also utilize the multi-view image dataset MEAD dataset. Specifically, we fed front MEAD images $\\mathrm{\\nabla0^{\\circ}}$ yaw) to all methods, rendered them from novel views of MEAD (from $60^{\\circ}$ to $0^{\\circ}$ yaw), and compare them with their corresponding ground truths. This allows us to report LPIPS and ID metrics alongside the FID metric for the novel views. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Comparisons with state-of-the-art ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "SlDx451MjC/tmp/4457bedc457f2878e7cdf53843c60dc04c0cce5658b042de29910ba9fc2b829f.jpg", "table_caption": ["Table 1: Quantitative scores on various test sets. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "SlDx451MjC/tmp/493d8e4a35b6965276ef36f581dc755aa34143e900a84f1fc725ff6e9afd93b5.jpg", "table_caption": ["Table 2: Quantitative scores on multi-view MEAD dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1 provides quantitative comparisons against state-of-the-art optimization and encoder-based methods. GOAE achieves significantly better same-view reconstruction scores (L2, LPIPS, and ID), however, shows much worse FID scores, indicating their inability to produce realistic views. While TriplaneNetv2 achieves similar same-view reconstruction scores as our method, its FID score is also significantly worse. Over", "page_idx": 6}, {"type": "text", "text": "all, the pSp and e4e methods perform worse than ours in all metrics. PTI achieves similar results to our method but takes $\\times250$ longer and requires a GPU with large memory. ", "page_idx": 6}, {"type": "text", "text": "We extend the quantitative analyses to multi-view with the MEAD dataset. Specifically, we feed front MEAD images ${\\mathrm{~\\,~}}^{0}$ yaw) to all methods, rendered them from novel views of MEAD (from $60^{\\circ}$ to $0^{\\circ}$ yaw), and compare them with their corresponding ground truths. Table 2 reveals that our method significantly improves over compared methods especially in LPIPS and ID metrics. ", "page_idx": 6}, {"type": "text", "text": "Qualitative results are shown in Fig. 8 and Fig. 6. The competing methods produce unrealistic outputs when viewed from angles other than the input view. Among these, PTI achieves good front and side views but fails to generate realistic hair from the back. Our method achieves the best results overall. ", "page_idx": 6}, {"type": "text", "text": "We also include mesh comparisons in Fig. 9. Ours is better than the most recent encoder-based method [7] and generally performs well compared to PTI. Note that PTI mostly generates smoother meshes (row 3) but can sometimes struggle depending on the input sample (row 6). ", "page_idx": 6}, {"type": "text", "text": "5.2 Ablation study ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "SlDx451MjC/tmp/e05afdac8f036c07346d2fdf60bf9a9bafc68a1bcabf44ac59d7cc0ff1dbbcaa.jpg", "table_caption": ["Table 3: Ablation on occlusion-aware discriminator $\\mathcal{D}$ . "], "table_footnote": ["results. "], "page_idx": 6}, {"type": "text", "text": "In Table 3 and Fig. 7, we present an ablation study demonstrating the effectiveness of our occlusion-aware triplane discriminator quantitatively and qualitatively. The first row of results shows that not using any discriminator achieves good reconstruction of the given view, as indicated by LPIPS and ID scores. This is also visible in the first-row in Fig. 7. However, this approach fails to generalize to novel views, as evidenced by the FID score and visual ", "page_idx": 6}, {"type": "image", "img_path": "SlDx451MjC/tmp/684d54e1c2964bb2f4a280e12a474732f3925244de962ace7e92f2f4c06136f5.jpg", "img_caption": ["Figure 6: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "SlDx451MjC/tmp/9235704c2000f4818b241d3a049918c927ed213dd91982935f4d4e9e23bcf76b.jpg", "img_caption": ["Figure 7: Qualitative results of ablation on occlusion-aware discriminator $\\mathcal{D}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "On the other hand, training the model with an additional adversarial objective that operates on novel images generated using randomly sampled camera parameters improves the FID score but significantly harms the fidelity of the input image. Training a discriminator in the triplane domain and applying adversarial losses from this domain improves overall scores compared to training the discriminator in the 2D image domain. However, as seen in Fig. 7 (third row), the face still lacks high fidelity to the input, and other views are unrealistic. Lastly, using the occlusion-aware triplane discriminator improves identity fidelity and FID scores. The hair looks more natural, similar to the ones generated by the model when sampled from $z$ . ", "page_idx": 7}, {"type": "table", "img_path": "SlDx451MjC/tmp/9164bf6f4a47a7ef5d9686eb89158463bbe5e8b609de6703b986d31e51a3d9fc.jpg", "table_caption": ["Table 4: Ablation on training data and latent space. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "In our framework, we chose to embed images into the $\\mathcal{W}^{+}$ space. Table 4 presents an ablation study that explores utilizing different projection spaces and various combinations of training data. Training an encoder to project images to the $\\mathcal{Z}$ or $\\mathcal{Z}^{+}$ space, where $\\mathcal{Z}$ is sampled 14 times, results in better FID scores. However, this comes at the cost of high-fidelity reconstruction. Similarly, transitioning to a less constrained $\\mathcal{W}^{+}$ space enhances fidelity to the input but worsens the ", "page_idx": 7}, {"type": "text", "text": "FID score. Addressing this challenge necessitates additional measures, such as the proposed dual encoder setup with the occlusion-aware discriminator objective. It is important to note that the distinction between the $\\mathcal{Z}^{+}$ and $\\mathcal{W}^{+}$ space arises from the camera parameters incorporated into the mapping network. While $\\mathcal{Z}^{+}$ employs various samples of $\\mathcal{Z}$ , it adheres to the same set of camera parameters assigned to the mapping network. In contrast, the $\\mathcal{W}^{+}$ space does not impose such constraints during encoding. Given that the real image dataset we use primarily consists of limited camera poses, typically front-view faces, we investigate training the encoder with synthetically generated images from PanoHead. However, solely utilizing synthetic images generated from samples of $\\mathcal{Z}^{+}$ to introduce more diversity compared to $\\mathcal{Z}$ leads to poor performance on real image validation sets regarding reconstruction quality. When combining synthetic and real images, we observe an improvement compared to using them individually. ", "page_idx": 7}, {"type": "image", "img_path": "SlDx451MjC/tmp/dcb3efafd7980ed3e557e7a3516617d352bf832a8e2bf4da8f5f06dedf5f9ee7.jpg", "img_caption": ["Figure 8: Left to right: input $(0^{\\circ})$ , reconstruction $(0^{\\circ})$ , GT target $(\\pm60^{\\circ})$ , and render on $\\pm60^{\\circ}$ using the reconstruction triplanes of $0^{\\circ}$ on MEAD dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "SlDx451MjC/tmp/ee46bc630f6fdbc604baeab58ea298fe4de142e3b0d23276508f9a8913e7c34d.jpg", "img_caption": ["Figure 9: Inputs (first), reconstructions (second), and $360^{\\circ}$ mesh renders (rest) of our method. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "SlDx451MjC/tmp/b74aa0d880f0ab98033a7221f40fd9cfe7c2ae8af3c1f1eb6f9374e854888327.jpg", "img_caption": ["Figure 10: Hair edits from source image (first) to destination image (second) and $360^{\\circ}$ renders (rest). Table 5: Ablation on dual-encoder. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Lastly, we show the results of using the dual encoder in Table 5. The visual results were previously presented in Fig. 5. The dual mechanism leverages the strengths of both encoders, achieving the same LPIPS and ID scores as Encoder 1 while also producing FID scores very similar to those of Encoder 2. ", "page_idx": 9}, {"type": "table", "img_path": "SlDx451MjC/tmp/a994ac1afbbb5f6a911fc0b84156be8d8661775523f3d499488e2a07103fd244.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Editing application ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We follow the reference-based editing in [8] in our pipeline. This method encodes input images, and edits are performed in the triplane space. This approach utilizes the fact that triplanes have a canonical space, allowing for the transfer of local parts from one triplane to another. Fig. 10 demonstrates a successful transfer of hairstyle from a reference image to the target human in 3D. Another advantage of encoder-based models over the optimization ones is the feasibility of such applications. For example, this would not be possible with PTI since the generator is fine-tuned for each sample, preventing the copying of features from one image to another in the encoded feature space. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "image", "img_path": "SlDx451MjC/tmp/87e1219dd2c01f39ed7c6d43a58e38c0dd8904831b354089037b60952b12e8cd.jpg", "img_caption": ["Figure 11: Example failure cases. Inputs (first), reconstructions (second), and novel views. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In summary, this study introduces a 3D GAN inversion framework that projects single images into the latent space of a 3D GAN for accurate 3D geometry reconstruction. While prior encoders excel at synthesizing near-frontal views, they struggle with diverse 3D scenes, motivating our exploration of alternatives. Using PanoHead\u2019s 360-degree synthesis, we developed a dual encoder system for high-fidelity reconstruction and realistic multi-view generation. A stitching mechanism in the triplane domain ensures optimal predictions from both encoders. With specialized losses, including an occlusion-aware triplane discriminator, our framework achieves superior qualitative and quantitative performance over existing methods. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. Our framework has the potential to revolutionize the movie industry, AR, and VR, enabling applications like animating portraits and creating realistic game environments. However, it raises ethical concerns, particularly the risk of \"deep fakes\". We stress the need for safeguards to ensure the ethical use of this technology. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We acknowledge that there is room for improvements in the fidelity of images, the realism and 3D-consistency of generations (see Fig. 11, row 2), and the smoothness of the meshes (see Fig. 9). Since the projection is made onto the latent space of PanoHead, our method may not handle out-of-domain or tail samples well (such as images with high-frequency details or accessories). For instance, our method struggles with hats, as demonstrated in the first row of Fig. 11. We recognize that, in certain cases, the artifacts are visible in the back middle of the head and are more noticeable in the mesh rendering as shown in Fig. 9. Additional research is required. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. This work was supported by the BAGEP Award of the Science Academy. We acknowledge EuroHPC Joint Undertaking for awarding the project ID EHPC-AI-2024A02-031 access to Leonardo at CINECA, Italy. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abdal, R., Qin, Y., Wonka, P.: Image2stylegan: How to embed images into the stylegan latent space? In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4432\u20134441 (2019) [2] Abdal, R., Qin, Y., Wonka, P.: Image2stylegan $^{++}$ : How to edit the embedded images? In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8296\u20138305 (2020)   \n[3] Alaluf, Y., Patashnik, O., Cohen-Or, D.: Restyle: A residual-based stylegan encoder via iterative refinement. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6711\u20136720 (2021) [4] Alaluf, Y., Tov, O., Mokady, R., Gal, R., Bermano, A.: Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18511\u201318521 (2022)   \n[5] An, S., Xu, H., Shi, Y., Song, G., Ogras, U.Y., Luo, L.: Panohead: Geometry-aware 3d full-head synthesis in 360deg. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 20950\u201320959 (2023) [6] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan (2017) [7] Bhattarai, A.R., Nie\u00dfner, M., Sevastopolsky, A.: Triplanenet: An encoder for eg3d inversion. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3055\u20133065 (2024) [8] Bilecen, B.B., Yalin, Y., Yu, N., Dundar, A.: Reference-based 3d-aware image editing with triplanes. arXiv preprint arXiv:2404.03632 (2024) [9] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16123\u201316133 (2022)   \n[10] Chan, E.R., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5799\u20135809 (2021)   \n[11] Creswell, A., Bharath, A.A.: Inverting the generator of a generative adversarial network. IEEE transactions on neural networks and learning systems 30(7), 1967\u20131974 (2018)   \n[12] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4690\u20134699 (2019)   \n[13] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems 35, 31841\u201331854 (2022)   \n[14] Gu, J., Liu, L., Wang, P., Theobalt, C.: Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985 (2021)   \n[15] Henderson, P., Tsiminaki, V., Lampert, C.H.: Leveraging 2d data to learn textured 3d mesh generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7498\u20137507 (2020)   \n[16] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017)   \n[17] Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017)   \n[18] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4401\u20134410 (2019)   \n[19] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8110\u20138119 (2020)   \n[20] Ko, J., Cho, K., Choi, D., Ryoo, K., Kim, S.: 3d gan inversion with pose optimization. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2967\u20132976 (2023)   \n[21] Li, X., De Mello, S., Liu, S., Nagano, K., Iqbal, U., Kautz, J.: Generalizable one-shot 3d neural head avatar. Advances in Neural Information Processing Systems 36 (2024)   \n[22] Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., Han, J.: On the variance of the adaptive learning rate and beyond. In: Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020) (April 2020)   \n[23] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM 65(1), 99\u2013106 (2021)   \n[24] Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.L.: Hologan: Unsupervised learning of 3d representations from natural images. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7588\u20137597 (2019)   \n[25] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11453\u201311464 (2021)   \n[26] Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J.J., Kemelmacher-Shlizerman, I.: Stylesdf: High-resolution 3d-consistent image and geometry generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13503\u201313513 (2022)   \n[27] Pavllo, D., Spinks, G., Hofmann, T., Moens, M.F., Lucchi, A.: Convolutional generation of textured 3d meshes. Advances in Neural Information Processing Systems 33, 870\u2013882 (2020)   \n[28] Pehlivan, H., Dalva, Y., Dundar, A.: Styleres: Transforming the residuals for real image editing with stylegan. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1828\u20131837 (2023)   \n[29] Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., Cohen-Or, D.: Encoding in style: a stylegan encoder for image-to-image translation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2287\u20132296 (2021)   \n[30] Roich, D., Mokady, R., Bermano, A.H., Cohen-Or, D.: Pivotal tuning for latent-based editing of real images. ACM Transactions on Graphics (TOG) 42(1), 1\u201313 (2022)   \n[31] Tov, O., Alaluf, Y., Nitzan, Y., Patashnik, O., Cohen-Or, D.: Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG) 40(4), 1\u201314 (2021)   \n[32] Wang, K., Wu, Q., Song, L., Yang, Z., Wu, W., Qian, C., He, R., Qiao, Y., Loy, C.C.: Mead: A large-scale audio-visual dataset for emotional talking-face generation. In: ECCV (August 2020)   \n[33] Wang, T., Zhang, Y., Fan, Y., Wang, J., Chen, Q.: High-fidelity gan inversion for image attribute editing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11379\u201311388 (2022)   \n[34] Wu, Y., Zhang, J., Fu, H., Jin, X.: Lpff: A portrait dataset for face generators across large poses. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20327\u201320337 (2023)   \n[35] Xie, J., Ouyang, H., Piao, J., Lei, C., Chen, Q.: High-fidelity 3d gan inversion by pseudo-multiview optimization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 321\u2013331 (2023)   \n[36] Yildirim, A.B., Pehlivan, H., Bilecen, B.B., Dundar, A.: Diverse inpainting and editing with gan inversion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 23120\u201323130 (2023)   \n[37] Yildirim, A.B., Pehlivan, H., Dundar, A.: Warping the residuals for image editing with stylegan. arXiv preprint arXiv:2312.11422 (2023)   \n[38] Yin, F., Zhang, Y., Wang, X., Wang, T., Li, X., Gong, Y., Fan, Y., Cun, X., Shan, Y., Oztireli, C., et al.: 3d gan inversion with facial symmetry prior. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 342\u2013351 (2023)   \n[39] Yuan, Z., Zhu, Y., Li, Y., Liu, H., Yuan, C.: Make encoder great again in 3d gan inversion through geometry and occlusion-aware encoding. In: Proceedings of the IEEE/CVF international conference on computer vision (2023)   \n[40] Zhang, M., Lucas, J., Ba, J., Hinton, G.E.: Lookahead optimizer: k steps forward, 1 step back. In: Advances in Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019)   \n[41] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586\u2013595 (2018)   \n[42] Zhu, J., Shen, Y., Zhao, D., Zhou, B.: In-domain gan inversion for real image editing. In: European conference on computer vision. pp. 592\u2013608. Springer (2020) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "SlDx451MjC/tmp/bdf2989e4ef177a1d3967db8bc0d837ebcaa27cbcd04be6748827004025eb81d.jpg", "table_caption": ["A.1 Architecture details "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 6: Architecture for discriminator. Conv2D parameters are: (input channels, output channels, kernel size, stride, padding), respectively. Bias terms are disabled. ", "page_idx": 13}, {"type": "text", "text": "Encoder 1 and 2. For our Encoder 1 and 2, we employ 2-stage encoding for $\\mathcal{W}^{+}$ and high-rate $\\mathcal{F}$ features, also seen in common with other style-based encoder methods [29, 3, 4, 39, 7]. We opted for the architecture in [29] for $\\mathcal{W}^{+}$ stage (GradualStyleEncoder) and in [7] for $\\mathcal{F}$ stage (TriplanenetEncoder), both for Encoder 1 and 2. ", "page_idx": 13}, {"type": "text", "text": "Ablation encoders. GradualStyleEncoder is used for the $\\mathcal{Z}^{+}$ encoder, where resulting 14 latent vectors are later passed through the mapping network with truncation $\\psi\\,=\\,0.85$ and canonical front camera pose. However, for the $\\mathcal{Z}$ encoder, a smaller variation of [29] (BackboneEncoderUsingLastLayerIntoW) is implemented. This choice is due to $\\mathcal{Z}$ being less expressive compared to $\\mathcal{Z}^{+}$ (1 dim vs. $14\\:\\mathrm{dim};$ ) and hence a smaller encoder being sufficient. The same mapping network parameters for the $\\mathcal{Z}^{+}$ case are also utilized for $\\mathcal{Z}$ encoder outputs. ", "page_idx": 13}, {"type": "text", "text": "Occlusion-aware triplane discriminator. We follow a feedforward network approach with a channel bottleneck for our triplane discriminator $\\mathcal{D}$ (Table 6). Noting that the occluded triplane dimensions are [batch_size,3,96,256,256], we first add each depth slice to the channel dimension to get the input shape as [batch_size,288,256,256]. Output dimensions are [batch_size,1]. We do not utilize saturating functions such as sigmoid at the end since WGANbased loss [6] is utilized. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Ablation image discriminator. For the back-view image discriminator used in ablations, we change the input channel number of the model in Table 6 from 288 to 3. ", "page_idx": 13}, {"type": "text", "text": "A.2 Training objectives and hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The training objective for Encoder 1 with parameters $\\theta_{\\mathbf{E}_{1}}$ is given in Eq. (5). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{I^{sv}}=\\mathcal{R}(\\mathbf{G}(\\mathbf{E_{1}}(\\mathbf{I})),\\pi)}\\\\ {\\arg\\underset{\\theta\\mathbf{E_{1}}}{\\operatorname*{min}}~\\lambda_{1}\\mathcal{L}_{\\mathrm{LPIPS}}(\\mathbf{I^{sv}},\\mathbf{I})+\\lambda_{2}\\mathcal{L}_{2}(\\mathbf{I^{sv}},\\mathbf{I})+\\lambda_{3}\\mathcal{L}_{\\mathrm{identity}}(\\mathbf{I^{sv}},\\mathbf{I})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{I^{sv}}$ is the same-view reconstruction of $\\mathbf{I}$ with pose $\\pi$ . Coefficients are set as $\\lambda_{1}~=~0.8$ , $\\lambda_{2}=1.0$ , $\\lambda_{3}=0.5$ . ", "page_idx": 13}, {"type": "text", "text": "The training objective for Encoder 2 with parameters $\\theta_{\\mathbf{E}_{2}}$ is given in Eq. (6). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}_{\\mathrm{enc}}=\\mathbf{G}(\\mathbf{E}_{2}(\\mathbf{I}))}\\\\ {\\mathbf{I}^{\\mathrm{sv}}=\\mathcal{R}(\\mathbf{T}_{\\mathrm{enc}},\\pi)}\\\\ {\\arg\\underset{\\theta_{\\mathbf{E}_{2}}}{\\operatorname*{min}}~\\lambda_{1}\\mathcal{L}_{\\mathrm{LPIPS}}(\\mathbf{I}^{\\mathrm{sv}},\\mathbf{I})+\\lambda_{2}\\mathcal{L}_{2}(\\mathbf{I}^{\\mathrm{sv}},\\mathbf{I})+\\lambda_{3}\\mathcal{L}_{\\mathrm{identity}}(\\mathbf{I}^{\\mathrm{sv}},\\mathbf{I})+\\lambda_{4}\\mathcal{L}_{\\mathrm{adv}}(\\mathcal{D}(\\mathcal{O}_{\\pi}\\mathbf{T}_{\\mathrm{enc}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{T}_{\\mathrm{enc}}$ is the encoded triplane of image I, $\\mathcal{D}$ is the discriminator, $O_{\\pi}$ is the occlusion mask from the same view $\\pi$ . $\\lambda_{1,2,3}$ are the same as in Eq. (5), and $\\lambda_{4}=0.001$ . ${\\mathcal{L}}_{\\mathrm{adv}}$ in Eq. (6) is given in Eq. (7): ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{adv}}(x)={\\mathsf{s o f t p l u s}}(-x)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where softplus is a smooth and differentiable approximation to ReLU. ", "page_idx": 13}, {"type": "text", "text": "The training objective for discriminator $\\mathcal{D}$ with parameters $\\theta_{\\mathcal{D}}$ is given in Eq. (8). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}_{\\mathrm{enc}}=\\mathbf{G}(\\mathbf{E}_{2}(\\mathbf{I}))}\\\\ {\\mathbf{T}_{\\mathrm{synth}}=\\mathbf{G}(\\mathbf{M}(z^{+}\\sim\\mathcal{N}(0,\\operatorname{I}_{14}),\\pi_{\\mathrm{front}}))}\\\\ {\\arg\\underset{\\theta_{D}}{\\operatorname*{min}}\\ \\lambda\\mathcal{L}_{\\mathrm{adv}}(\\mathcal{D}(\\mathcal{O}_{\\pi}\\mathbf{T}_{\\mathrm{enc}}),\\ {\\mathcal{D}}(\\mathcal{O}_{\\pi}\\mathbf{T}_{\\mathrm{synth}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{T}_{\\mathrm{synth}}$ is the synthesised triplane from randomly sampled $z^{+}$ , $\\mathbf{M}$ is the mapping network, $\\pi_{\\mathrm{front}}$ is the canonical front pose. $\\mathcal{L}_{\\mathrm{adv}}$ in Eq. (8) is given in Eq. (9): ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{adv}}(x,y)={\\mathsf{s o f}}\\,{\\mathsf{t p1u s}}(-x)+{\\mathsf{s o f}}\\,{\\mathsf{t p1u s}}(y)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\lambda$ is set as 0.5. ", "page_idx": 14}, {"type": "text", "text": "We jointly train Encoder 2 and discriminator $\\mathcal{D}$ in a traditional adversarial fashion. We further employ R1-regularization to encourage L1-lipschitzness [6] to justify using softplus, where its weighting coefficient is 10 and is applied every 16 iterations. ", "page_idx": 14}, {"type": "text", "text": "A.3 Additional qualitative results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our method can handle diverse ethnicities and challenging input views, demonstrated in Fig. 12. We also showcase additional visual results for competing methods in Figs. 13 to 20, ablation on discriminator in Fig. 21, ablation on dual-encoder structure in Figs. 22 to 24. The first columns are input images, the second columns are reconstructions from the input views, and the rest are renderings of models from novel views. ", "page_idx": 14}, {"type": "image", "img_path": "SlDx451MjC/tmp/f8ffa870bb5c2e7475254864dc4b7b3cb5e5baef39c5f20961ae7b836873616c.jpg", "img_caption": ["Figure 12: Inputs with diverse ethnicities and challenging views (first), reconstructions (second), and $360^{\\circ}$ renders (rest). "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "SlDx451MjC/tmp/d12bc42026a40a4d6fd34b5b1cb34e679b9d18fd3d2b142541b27fa9ce599e4b.jpg", "img_caption": ["Figure 13: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "SlDx451MjC/tmp/b799a691ed6e3c7794c82ecc16bd19b3f19b2d5a3d70b91e0e65aea85f3acf39.jpg", "img_caption": ["Figure 14: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "SlDx451MjC/tmp/6773be36cc205da33b2d69164dc8efe6aac76046577d40dd23ee4a4448dfea38.jpg", "img_caption": ["Figure 15: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "SlDx451MjC/tmp/f8e4d855e1f4b19da793f678eb5748407b68832e1235b07c79ab55959a998429.jpg", "img_caption": ["Figure 16: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "SlDx451MjC/tmp/dcf6f988d1adea034a5434aeaee700f3a26c71ef0772f5851dbdfe1e6e3140a8.jpg", "img_caption": ["Figure 17: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "SlDx451MjC/tmp/1355d616cf3db314e5da2907e5ec0ebe15f90a71e6548c2971df6415d466fd61.jpg", "img_caption": ["Figure 18: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "SlDx451MjC/tmp/9db0a8f555be49c62d9cc9d12f47a036fe03092102476d2cd25b318d1e37afaf.jpg", "img_caption": ["Figure 19: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "SlDx451MjC/tmp/5d3791cb66dc0b670e262fce92f9272d295a811d23ee04792699045134a59b41.jpg", "img_caption": ["Figure 20: Comparisons of ours and competing methods. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "SlDx451MjC/tmp/5af3e4fac435c07621020b858afad6107ea259b3e23b9570bdd819fa7071d4f1.jpg", "img_caption": ["Figure 21: Qualitative results of ablation on occlusion-aware discriminator $\\mathcal{D}$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "SlDx451MjC/tmp/095baa7d071d07e3783369261a77486b0213ad3ce87e7d75d439253c76a2a025.jpg", "img_caption": ["Figure 22: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first column. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "SlDx451MjC/tmp/816e66dfc0b82458af3285df18a43018b6089affe47432b88728302f511fc992.jpg", "img_caption": ["Figure 23: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first column. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "SlDx451MjC/tmp/0dfbb6d8e6f504fab48f1bfa926bc966d0de80b74eca00f0399f12f1b2f5fe4c.jpg", "img_caption": ["Figure 24: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first column. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our claims include utilizing dual encoders trained to specialize in visible and occluded regions, along with a stitching pipeline that seamlessly combines the most accurate predictions from both encoders. To further enhance the reconstruction of occluded regions, we propose an occlusion-aware discriminator, enabling the image encoder to generate realistic features for these challenging areas. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across multiple benchmarks and scenarios. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Limitations are given in Sec. 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The training and evaluation schemes are detailed in Sections 4.1 and 4.3 and Appendix A, which provides reproducability. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not provide open access to the code as of now; however, the datasets are public and can be obtained from the pointed references. The code is planned to be released through the project page provided in Abstract. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Training objectives are given in Eqs. (2) and (4), the training recipe in Section 4.1, and the evaluation metrics in Section 4.3. Further details about the experimental setup are provided in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not provide error bars due to the computational budget it would require. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Time of execution of methods are provided in Table 1, as well as the computing resources in Section 4.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Broader impacts are provided in Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We describe the potential risks in Section 6. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We properly mentioned the original owners of previous work via references, and complied with the licenses. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not share new assets as of now. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper neither involves crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]