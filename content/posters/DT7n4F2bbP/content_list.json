[{"type": "text", "text": "Tensor-Based Synchronization and the Low-Rankness of the Block Trifocal Tensor ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniel Miao \u2217 Gilad Lerman\u2217 Joe Kileel\u2020", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The block tensor of trifocal tensors provides crucial geometric information on the three-view geometry of a scene. The underlying synchronization problem seeks to recover camera poses (locations and orientations up to a global transformation) from the block trifocal tensor. We establish an explicit Tucker factorization of this tensor, revealing a low multilinear rank of (6,4,4) independent of the number of cameras under appropriate scaling conditions. We prove that this rank constraint provides sufficient information for camera recovery in the noiseless case. The constraint motivates a synchronization algorithm based on the higher-order singular value decomposition of the block trifocal tensor. Experimental comparisons with stateof-the-art global synchronization methods on real datasets demonstrate the potential of this algorithm for significantly improving location estimation accuracy. Overall this work suggests that higher-order interactions in synchronization problems can be exploited to improve performance, beyond the usual pairwise-based approaches. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Synchronization is crucial for the success of many data-intensive applications, including structure from motion, simultaneous localization and mapping (SLAM), and community detection. This problem involves estimating global states from relative measurements between states. While many studies have explored synchronization in different contexts using pairwise measurements, few have considered measurements between three or more states. In real-world scenarios, relying solely on pairwise measurements often fails to capture the full complexity of the system. For instance, in networked systems, interactions frequently occur among groups of nodes, necessitating approaches that can handle higherorder relationships. Extending synchronization to consider measurements between three or more states, however, increases computational complexity and requires sophisticated mathematical models. Addressing these challenges is vital for advancing various technological fields. For example, higher-order synchronization can improve the accuracy of 3D reconstructions in structure from motion by leveraging more complex geometric relationships. In SLAM, it enhances mapping and localization precision in dynamic environments by considering multi-robot interactions. Similarly, in social networks, it could lead to more accurate identification of tightly-knit groups. Developing efficient algorithms to handle higherorder measurements will open new research avenues and make systems more resilient and accurate. ", "page_idx": 0}, {"type": "text", "text": "In this work, we focus on a specific instance of the synchronization problem within the context of structure from motion in 3D computer vision, where each state represents the orientation and location of a camera. Traditional approaches rely on relative measurements encoded by fundamental matrices, which describe the relative projective geometry between pairs of images. Instead, we consider higher-order relative measurements encoded in trifocal tensors, which capture the projective information between triplets of images. Trifocal tensors uniquely determine the geometry of three views, even in the collinear case [1], making them more favorable than triplets of fundamental matrices for synchronization. To understand the structure and properties of trifocal tensors in multi-view geometry, we carefully study the mathematical properties of the block tensor of trifocal tensors. We then use these theoretical insights to develop effective synchronization algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Directly relevant previous works. In the structure from motion problem, synchronization has traditionally been done using incremental methods, such as Bundler [2] and COLMAP [3]. These methods process images sequentially, gradually recovering camera poses. However, the order of image processing can impact reconstruction quality, as error may significantly accumulate. Bundle adjustment [4], which jointly optimizes camera parameters and 3D points, has been used to limit drifting but is computationally expensive. ", "page_idx": 1}, {"type": "text", "text": "Alternatively, global synchronization methods have been proposed. These methods process multiple images simultaneously, avoiding iterative procedures and offering more rigorous and robust solutions. Global methods generally optimize noisy and corrupted measurements by exploiting the structure of relative measurements and imposing constraints. Many global methods solve for orientation and location separately, using structures on $\\bar{S}O(3)$ and the set of locations. Solutions for retrieving camera poses from pairwise measurements have been developed for camera orientations [5, 6, 7, 8, 9, 10], camera locations [11, 12, 13], and both simultaneously [14, 15, 16, 17]. Some methods explore the structure on fundamental or essential matrices [18, 19, 20]. ", "page_idx": 1}, {"type": "text", "text": "Several attempts to extract information from trifocal tensors include works by: Leonardos et al. [21], which parameterizes calibrated trifocal tensors with non-collinear pinhole cameras as a quotient Riemannian manifold and uses the manifold structure to estimate individual trifocal tensors robustly; Larsson et al. [22], which proposes minimal solvers to determine calibrated radial trifocal tensors for use in an incremental pipeline, handling distorted images with constraints invariant to radial displacement; and Moulon et al. [23], which introduces a structure from motion pipeline, retrieving global rotations via cleaning the estimation graph and solving a least squares problem, and solving for translations by estimating trifocal tensors individually by linear programs. To our knowledge, no prior works develop a global pipeline where the synchronization operates directly on trifocal tensors. ", "page_idx": 1}, {"type": "text", "text": "Contribution of this work. The main contributions of this work are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We establish an explicit Tucker factorization of the block trifocal tensor when its blocks are suitably scaled, demonstrating a low multilinear rank of (6,4,4). Moreover, we prove that this rank constraint is sufficient to determine the scales and fully characterizes camera poses in the noiseless case.   \n\u2022 We develop a method for synchronizing trifocal tensors by enforcing this low rank constraint on the block tensor. We validate the effectiveness of our method through tests on several real datasets in structure from motion. ", "page_idx": 1}, {"type": "text", "text": "2 Low-rankness of the block trifocal tensor ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first briefly review relevant background material in Section 2.1. Then we present the main new construction and theoretical results in Section 2.2. ", "page_idx": 1}, {"type": "text", "text": "2.1 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1.1 Cameras and 3D geometry ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given a collection of $n$ images $I_{1},...,I_{n}$ of a 3D scene, let $t_{i}\\!\\in\\!\\mathbb{R}^{3}$ and $R_{i}\\in S O(3)$ denote the location and orientation of the camera associated with the image $I_{i}$ in the global coordinate system. Moreover, each camera is associated with a calibration matrix $K_{i}$ that encodes the intrinsic parameters of a camera, including the focal length, the principal points, and the skew parameter. Then, the $3\\!\\times\\!4$ camera matrix has the following form, $P_{i}\\!=\\!K_{i}R_{i}[I_{3\\times3},-t_{i}]$ and is defined up to nonzero scale. Three-dimensional world points $X$ are represented as $\\mathbb{R}^{4}$ vectors in homogeneous coordinates, and the projection of $X$ into the image corresponding to $P$ is $x\\!=\\!P X$ . 3D world lines $L$ can be represented via Pl\u00fccker coordinates as an $\\mathbb{R}^{\\breve{6}}$ vector. Then the projection of $L$ onto the image corresponding to $P$ is $l=\\mathcal{P}L$ , where $\\mathcal{P}$ is the $3\\!\\times\\!6$ line projection matrix. It can be written as $\\scriptstyle{\\mathcal{P}}\\,=\\,\\left[P^{2}\\land P^{\\dot{3}};P^{3}\\land\\check{P}^{1};P^{1}\\land P^{2}\\right]$ where $P^{i}$ is the $i$ -th row of the camera matrix $P$ and wedge denotes exterior product. Explicitly the $(i,j)$ element of the line projection matrix can be calculated as the determinant of the submatrix, where the $i$ -th row is omitted and the column are selected as the $j$ -th pair from [(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)]. The elements on the second row are multiplied by $-1$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To retrieve global poses, relative measurement of pairs or triplets of images is needed. Let $x_{i}$ and $x_{j}$ be any pair of corresponding keypoints in images $I_{i}$ and $I_{j}$ respectively, meaning that they are images of a common world point. The fundamental matrix $F_{i j}$ is a $3\\!\\times\\!3$ matrix such that $x_{i}^{T}F_{i j}x_{j}\\!=\\!0$ . It is known that $F_{i j}$ encodes the relative orientation $R_{i j}\\!=\\!R_{i}R_{j}^{T}$ and translation $t_{i j}\\!=\\!R_{i}(t_{i}\\!-\\!t_{j})$ through $F_{i j}=$ $K_{i}^{-T}[t_{i j}]\\!\\times\\!R_{i j}K_{j}^{-1}$ . The essential matrix corresponds to the calibrated case, where $K_{i}\\,{=}\\,I_{3\\times3}$ for all $i$ . ", "page_idx": 2}, {"type": "text", "text": "2.1.2 Trifocal tensors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Analogous to the fundamental matrix, the trifocal tensor $T_{i j k}$ is a $3\\times3\\times3$ tensor that relates the features across images and characterizes the relative pose between a triplet of cameras $P_{i},P_{j},P_{k}$ . The trifocal tensor $T_{i j k}$ corresponding to cameras $P_{i},P_{j},P_{k}$ can be calculated by ", "page_idx": 2}, {"type": "equation", "text": "$$\n(T_{i j k})_{w q r}\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\begin{array}{c}{{\\!\\!\\sim\\!P_{i}^{w}\\!\\!}}\\\\ {{\\!\\!P_{j}^{q}}}\\\\ {{\\!\\!P_{k}^{r}}}\\end{array}\\right]\\!,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $P_{i}^{w}$ is the $w$ -th row of $P_{i}$ , and $\\sim P_{i}^{w}$ is the $2\\times4$ submatrix of $P_{i}$ omitting the $w$ -th row. The trifocal tensor determines the geometry of three cameras up to a global projective ambiguity, or up to a scaled rigid transformation in the calibrated case. In addition to point correspondences, trifocal tensors satisfy constraints for corresponding lines, and mixtures thereof. For example, let $l_{i},l_{j},l_{k}$ be corresponding image lines in the views of cameras $P_{i},P_{j},P_{k}$ respectively, then the lines are related through the trifocal tensor $T_{i j k}$ by $\\left(l_{j}^{T}[(T_{i j k})_{1::},(T_{i j k})_{2::},(\\mathbf{\\bar{{T}}}_{i j k})_{3::}]l_{k}\\right)[l]_{\\times}\\!=\\!0^{T}$ , where $[l]_{\\times}$ denotes the $\\times3$ skew-symmetric matrix corresponding to cross product by $l$ . We refer to [1] for more details of the properties of a trifocal tensor. We include the standard derivation of the trifocal tensor in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "Since corresponding lines put constraints on the trifocal tensor, one advantage of incorporating trifocal tensors into structure from motion pipelines is that trifocal tensors can be estimated purely from line correspondences or a mixture of points and lines. Fundamental matrices can not be estimated directly from line correspondences, so the effectiveness of pairwise methods for datasets where feature points are scarce is limited. Furthermore, trifocal tensors have the potential to improve location estimation. From pairwise measurements, one can only get the relative direction but not the scale and the location estimation in the pairwise setting is a \u201cnotoriously difficult problem\" (quoting from pages 316-317 of [24]). However, trifocal tensors encode the relative scales of the direction and can greatly simplify the location estimation procedure. We refer to several works on characterizing the complexity of minimal problems for individual trifocal tensors [25, 26], and on developing methods for solving certain minimal problems [27],[28], [29], [30], [31], [32], [33]. We also refer to [34] for a survey paper on structure from motion, which discusses minimal problem solvers from the perspective of computational algebraic geometry. ", "page_idx": 2}, {"type": "text", "text": "2.1.3 Tucker decomposition and the multilinear rank of tensors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We review basic material on the Tucker decomposition and the multilinear rank of a tensor. We refer to [35] for more details while adopting its notation. Let $T\\!\\in\\!\\mathbb{R}^{I_{1}\\times I_{2}\\times\\cdots\\times I_{N}}$ be an order $N$ tensor. The mode- $^{\\,.\\,i}$ flattening (or matricization) $\\bar{T}_{(i)}\\in\\mathbb{R}^{I_{i}\\times(I_{1}\\dots I_{i-1}I_{i+1}\\dots I_{N})}$ is the rearrangement of $T$ into a matrix by taking mode- $^{\\cdot i}$ fibers to be columns of the flattened matrix. By convention, the ordering of the columns in the flattening follows lexicographic order of the modes excluding $i$ . Symbols $\\otimes$ and $\\odot$ denote the Kronecker product and the Hadamard product respectively. The norm on tensors is defined as $\\|T\\|=\\|T_{(1)}\\|_{F}$ . The $i$ -rank of $T$ is the column rank of $T_{(i)}$ and is denoted as $\\mathrm{rank}_{i}(T)$ . Let $R_{i}{=}{\\mathrm{rank}}_{i}(T)$ . Then the multilinear rank of $T$ is defined as mlran $\\dot{\\mathfrak{s}}(T)=(R_{1},R_{2},\\dots,R_{N})$ . The $i$ -mode product of $T$ with a matrix $U\\!\\in\\!\\mathbb{R}^{m\\times I_{i}}$ is a tensor in $\\mathbb{R}^{I_{1}\\times\\dots\\times I_{i-1}\\times m\\times I_{i+1}\\times\\dots\\times I_{N}}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n(T\\times_{i}U)_{j_{1}\\cdots j_{i-1}k j_{i+1}\\cdots j_{N}}=\\sum_{j_{i}=1}^{I_{i}}\\!T_{j_{1}j_{2}\\cdots j_{N}}U_{k j_{i}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the Tucker decomposition of $T\\!\\in\\!\\mathbb{R}^{I_{1}\\times I_{2}\\times\\cdots\\times I_{N}}$ is a decomposition of the following form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nT\\!=\\!\\mathcal{G}\\!\\times\\!_{1}A_{1}\\times_{2}A_{2}\\times_{3}\\cdots\\times_{N}A_{N}\\!=\\![\\mathcal{G};A_{1},A_{2},...,A_{N}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{G}\\in\\mathbb{R}^{Q_{1}\\times\\cdots\\times Q_{N}}$ is the core tensor, and $A_{n}\\in\\mathbb{R}^{I_{n}\\times Q_{n}}$ are the factor matrices. Without loss of generality, the factor matrices can be assumed to have orthonormal columns. Given the multilinear rank of the core tensor $(R_{1},...,R_{N})$ , the Tucker decomposition approximation problem can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathcal{G}\\in\\mathbb{R}^{R_{1}\\times\\dots\\times R_{N}},A_{i}\\in\\mathbb{R}^{I_{i}\\times R_{i}}}{\\mathrm{argmin}}\\|T\\!-\\![\\mathcal{G};A_{1},A_{2},...,A_{N}]\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A standard way of solving (2) is the higher-order singular value decomposition (HOSVD). The HOSVD is computed with the following steps. First, for each $i$ calculate the factor matrix $A_{i}$ as the $R_{i}$ leading left singular vectors of $T_{(i)}$ . Second, set the core tensor $\\mathcal{G}$ as ${\\mathcal{G}}=T\\times_{1}A_{1}^{T}\\times_{2}\\cdots\\times_{N}A_{N}^{T}$ . Though the solution from HOSVD will not be the optimal solution to (2), it satisfies a quasi-optimality property: if $T^{*}$ is the optimal solution, and $T^{\\prime}$ the solution from HOSVD, then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|T\\!-\\!T^{\\prime}\\|\\!\\leq\\!\\sqrt{N}\\|T\\!-\\!T^{*}\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.2 Low Tucker rank of the block trifocal tensor and one shot camera retrieval ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose we are given a set of camera matrices $\\{P_{i}\\}_{i=1}^{n}$ with $n\\!\\geq\\!3$ and scales fixed on each camera matrix. Define the block trifocal tensor $T^{n}$ to be the $3n\\times3n\\times3n$ tensor, where the $3\\!\\times\\!3\\!\\times\\!3$ sized $i j k$ block is the trifocal tensor corresponding to the triplet of cameras $P_{i},P_{j},P_{k}$ . We assume for all blocks that have overlapping indices, the corresponding $3\\!\\times\\!3\\!\\times\\!3$ tensor is also calculated using the formula (1). We summarize key properties of $T^{n}$ in Proposition 1 and Theorem 1. The proof of Proposition 1 is by direct computation and can be found in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. We have the following observations for the block trifocal tensor $T^{n}$ . For all distinct $i,j\\in[n]$ , we have the following properties: ", "page_idx": 3}, {"type": "text", "text": "(i) $T_{i i i}^{n}\\!=\\!0_{3\\times3\\times3}$ (ii) The $T_{j i i}^{n}$ blocks are rearrangements of elements in the fundamental matrix $F_{i j}$ up to signs.   \n(iii) The $T_{i j i}^{n}$ and $T_{i i j}^{n}$ blocks encode the epipoles.   \n(iv) The horizontal slices $T^{n}(i,:,:)$ of $T^{n}$ are skew symmetric.   \n(v) When all cameras are calibrated, three singular values of $T_{(1)}^{n}$ are equal. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Tucker factorization and low multilinear rank of block trifocal tensor). The block trifocal tensor $T^{n}$ admits a Tucker factorization, $T^{n}\\!=\\!\\mathcal{G}\\!\\times\\!{_1}\\mathcal{P}\\!\\times\\!{_2}\\mathcal{C}\\!\\times\\!{_3}\\mathcal{C},$ , where $\\mathcal{G}\\!\\in\\!\\mathbb{R}^{6\\times4\\times4}$ , $\\mathcal{P}\\!\\in\\!\\mathbb{R}^{3n\\times6}$ , and $\\mathcal{C}\\in\\mathbb{R}^{3n\\times4}$ . If the $n$ cameras that produce $T^{n}$ are not all collinear, then mlran $\\stackrel{\\cdot}{\\mathrm{c}}(T^{n})=(6,4,4)$ . If the n cameras that produce $T^{n}$ are collinear, then mlrank $\\mathfrak{z}(T^{n})\\preceq(6,4,4)$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. We can explicitly calculate that $T^{n}\\,{=}\\,{\\mathcal{G}}\\times_{1}{\\mathcal{P}}\\times_{2}{\\mathcal{C}}\\times_{3}{\\mathcal{C}}$ . The details of the calculation are in Appendix A.2. The specific forms for $\\mathcal{G S P}$ are the following. The horizontal slices of the core are ", "page_idx": 3}, {"type": "image", "img_path": "DT7n4F2bbP/tmp/b9aa4b7316d19a3cded49565d5657971d3daab5439bfb1fdc702a51847810a4a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The factor matrices are $\\mathcal{C}\\!=\\!\\left[P_{1},P_{2},...,P_{n}\\right]^{T}\\!\\in\\!\\mathbb{R}^{3n\\times4}$ and $\\mathcal{P}\\!=\\!\\left[S_{1},S_{2},...,S_{n}\\right]^{T}\\!\\in\\!\\mathbb{R}^{3n\\times6}$ , where $P_{i}$ are the camera matrices and $S_{i}$ are the corresponding line projection matrices. ", "page_idx": 3}, {"type": "text", "text": "Now, we suppose that the $n$ cameras are not collinear. We first show that $\\mathcal{C}$ and $\\mathcal{P}$ both have full rank. From [1], the null space of a camera matrix $P_{i}$ is generated by the camera center. For the sake of contradiction, suppose that rank $(\\mathcal{C})<4$ . Then there exists $x\\in\\mathbb{R}^{4}$ such that $x\\!\\neq\\!0$ and $\\mathcal{C}x=0$ . This means that $P_{i}x\\!=\\!0$ for all $i\\!=\\!1,...,n$ . Then, $x$ is the camera centre for all cameras, which means that the cameras are centered at one point and are collinear. Similarly, every vector in the null space of the line projection matrix $S_{i}$ is a line that passes through the camera centre [1]. For the sake of contradiction, suppose that rank $\\left(\\mathcal{P}\\right)<6$ . Then there exists $\\bar{x}\\in\\mathbb{R}^{6}$ such that $x\\neq0$ and $\\mathcal{P}x=0$ . This implies that $S_{i}x\\!=\\!0$ for all $i\\!=\\!1,...,n$ , which means that $x$ is a line that passes through all of the camera centers. Again the cameras are collinear, which is a contradiction. Next we write the flattening of the block trifocal tensor as $T_{(1)}^{n}=\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}\\otimes\\mathcal{C})^{T}$ . Then $\\mathcal{P}\\in\\mathbb{R}^{3n\\times6}$ has rank 6, and $(\\mathcal{C}\\otimes\\mathcal{C})^{T}\\in\\mathbb{R}^{16\\times9n^{2}}$ has rank 16. Given the specific form of $\\mathcal{G}$ , where $\\mathcal{G}_{(1)}\\in\\mathbb{R}^{6\\times16}$ it is easy to check rank $(\\mathcal{G}_{(1)})=6$ . Thus, ra $\\upkappa(T_{(1)}^{n})\\,{=}\\,6$ . Similarly, we can show that rank $(T_{(2)}^{n})\\,{=}\\,4$ , and rank $\\,(T_{(3)}^{n})\\,{=}\\,4$ . This implies that the multilinear rank of the block trifocal tensor is (6,4,4) when the $n$ cameras are not collinear. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "When the $n$ cameras are collinear, the individual factors in each flattening may be rank deficient, so that ran $\\mathfrak{c}(T_{(1)}^{n})\\leq6,\\mathrm{rank}(T_{(2)}^{n})\\leq4$ , and rank $(T_{(3)}^{n})\\,{\\leq}\\,^{4}$ . This implies mlrank $(T^{n})\\preceq(6,4,4)$ . \u53e3 ", "page_idx": 4}, {"type": "text", "text": "The theorem inspires a straightforward way of retrieving global poses from the block trifocal tensor, which we summarize in the following claim. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 (One shot camera pose retrieval). Given the block trifocal tensor $T^{n}$ produced by cameras $P_{1}{,}P_{2}{,}...,P_{n}$ , the cameras can be retrieved from $T^{n}$ up to a global projective ambiguity using the higher-order SVD. The cameras will be the leading 4 singular vectors of T (n2) or T (n3). ", "page_idx": 4}, {"type": "text", "text": "Using the higher-order SVD on $T^{n}$ , we can get a Tucker decomposition of the block trifocal tensor $T^{n}=\\bar{\\mathcal{G}}\\times_{1}\\bar{\\mathcal{\\hat{P}}}\\times_{2}\\hat{\\mathcal{C}}\\times_{3}\\hat{\\mathcal{C}^{\\prime}}$ . Though the Tucker factorization is not unique [35], as we can apply an invertible linear transformation to one of the factor matrices and apply the inverse onto the core tensor, this invertible linear transformation can be interpreted as the global projective ambiguity for projective 3D reconstruction algorithms. Thus, the cameras can be retrieved by taking the leading four singular vectors of the mode-2 and mode-3 flattenings of the block tensor. ", "page_idx": 4}, {"type": "text", "text": "Very importantly however, in practice each trifocal tensor block in $T^{n}$ can be estimated from image data only up to an unknown multiplicative scale [1]. The following theorem establishes the fact that the multilinear rank constraints provide sufficient information for determining the correct scales. In the statement $\\odot_{b}$ denotes blockwise scalar multiplication, thus the $(i,j,k)$ -block of $\\lambda\\odot_{b}T^{n}$ is $\\lambda_{i j k}T_{i j k}^{n}\\!\\in\\!\\mathbb{R}^{3\\times3\\times3}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $T^{n}\\in\\mathbb{R}^{3n\\times3n\\times3n}$ be a block trifocal tensor corresponding to $n\\geq4$ calibrated or uncalibrated cameras in generic position. Let $\\boldsymbol{\\dot{\\lambda}}\\in\\mathbb{R}^{n\\times n\\times n}$ be a block scaling with $\\lambda_{i j k}$ nonzero iff $i,j,k$ are not all equal. Assume that $\\lambda\\odot_{b}T^{n}\\in\\mathbb{R}^{3n\\times3n\\times3n}$ has multilinear rank (6,4,4) where $\\odot_{b}$ denotes blockwise scalar multiplication. Then there exist $\\alpha,\\beta,\\gamma\\in\\mathbb{R}^{n}$ such that $\\lambda_{i j k}=\\alpha_{i}\\beta_{j}\\gamma_{k}$ whenever $i,j,k$ are not all the same. ", "page_idx": 4}, {"type": "text", "text": "Sketch. The idea is to identify certain submatrices in the flattenings of $\\lambda\\odot_{b}T^{n}$ which must have determinant 0, and use these to solve for $\\lambda$ . A proof is in Appendix A.4. We remark that the proof technique extends that of [36, Theorem 5.1], which showed a similar result for a matrix problem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 is the basic guarantee for our algorithm development below. We stress that the ambiguities brought by $\\alpha,\\beta,\\gamma$ are not problematic for purposes of recovering the camera matrices by Proposition 2. Indeed, $(\\alpha\\otimes\\beta\\otimes\\gamma)\\odot_{b}T^{n}=\\mathcal{G}\\times_{1}(D_{\\alpha}\\mathcal{P})\\times_{2}(D_{\\beta}\\mathcal{C})\\times_{3}(D_{\\gamma}\\mathcal{C})$ where $D_{\\alpha}\\in\\mathbb{R}^{3n\\times3n}$ is the diagonal matrix with each entry of $\\alpha$ triplicated, etc. Hence the camera matrices can still be recovered up to individual scales (as expected) and a global projective transformation, from the higher-order SVD. ", "page_idx": 4}, {"type": "text", "text": "3 Synchronization of the block trifocal tensor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we develop a heuristic method for synchronizing the block trifocal tensor $T^{n}$ by exploiting the multilinear rank of $T^{n}$ from Theorem 1. Let ${\\hat{T}}^{n}$ denote the estimated block trifocal tensor, and $T^{n}$ the ground truth. Assume that there are $n$ images and a set of trifocal tensor estimates $\\hat{T}_{i j k}$ where $(i,j,k)\\in\\Omega$ and $\\Omega$ is the set of indices whose corresponding trifocal tensor is estimated. Note that each estimated trifocal tensor $\\hat{T}_{i j k}$ will have an unknown scale $\\lambda_{i j k}\\in\\mathbb{R}^{*}$ associated with it. We always assume that we observe the iii blocks, as they will be 0. We formulate the block trifocal tensor ${\\hat{T}}^{n}$ by plugging in the estimates $\\hat{T}_{i j k}$ and setting the unobserved positions $((i,j,k)\\notin\\Omega)$ to $3\\times3\\times3$ tensors of all zeros. Let $W_{\\Omega}\\in\\dot{\\{0,1\\}}^{3n\\times3n\\times3n}$ denote the block tensor where the $(i,j,k)$ blocks are ones for $(i,j,k)\\!\\in\\!\\Omega$ and zeros otherwise. Let $W_{\\Omega^{C}}$ denote the opposite. In our experiments, we observe that the HOSVD is quite robust against noise for retrieving camera poses, which arises e.g., from numerical sensitivities when first estimating relative poses [37]. Therefore we develop an algorithm that projects ${\\hat{T}}^{n}$ onto the set of tensors that have multilinear rank of (6,4,4) while completing the tensor and retrieving an appropriate set of scales. Specifically, we can write our problem as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda}\\|\\Lambda\\odot\\hat{T}^{n}-\\mathcal{P}_{\\tau}(\\Lambda\\odot\\hat{T}^{n})\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Lambda\\in\\mathbb{R}^{3n\\times3n\\times3n}$ , each $3\\times3\\times3$ block is uniform, $\\Lambda_{i j k}$ blocks are zero for $(i,j,k)\\not\\in\\Omega$ , and $\\Lambda$ satisfies a normalization condition like $\\|\\Lambda\\|^{2}\\!=\\!1$ to avoid its vanishing. However, we drop this normalization constant in our implementation as we never observe $\\Lambda$ vanishing in practice. (For convenience, we formulate this section with the notation of $\\Lambda\\!\\in\\!\\mathbb{R}^{3n\\times3n\\times3n}$ and Hadamard multiplication, rather than $\\lambda\\in\\mathbb{R}^{n\\times n\\times n}$ and blockwise scalar multiplication from Theorem 2.) Furthermore in problem (4), $\\mathcal{P}_{\\tau}$ denotes the exact projection onto the set $\\Gamma\\!=\\!\\{T\\!\\in\\!\\mathbb{R}^{3n\\times3n\\times3n}\\colon\\!\\mathrm{mlrank}(T)\\!=\\!(6,4,4\\hat{)}\\}$ . Note that though HOSVD provides an efficient way to project onto $\\Gamma$ , it is quasi-optimal and not the exact projection. The exact projection is much harder to calculate, and in general NP-hard. The algorithm below adopts an alternating projection strategy to estimate the best set of scales. ", "page_idx": 5}, {"type": "text", "text": "3.1 Higher-order SVD with a hard threshold (HOSVD-HT) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The key idea for our algorithm is to use the relative scales on the rank truncated tensor as a heuristic to retrieve scales for the estimated block tensor. There are two main challenges for calculating the rank truncated tensor. First, the exact projection $\\mathcal{P}_{\\tau}$ onto $\\Gamma$ is expensive and difficult to calculate. Second, many blocks in the block tensor will be unknown if the corresponding images of the block lacks corresponding point and directly projecting the uncompleted tensor will be inaccurate. We apply an HOSVD framework with imputations to tackle the challenges. Regarding the first challenge, HOSVD is a simple, efficient, and quasi-optimal (3) projection onto $\\Gamma$ . Though inexact, it is a reliable approximation. For the second challenge, the tensor T\u02c6 n must be completed. We adopt the matrix completion idea of HARD-IMPUTE [38], where the matrix is filled-in iteratively with the rank truncated matrix obtained using the hard-thresholded SVD. In other words, we complete the missing blocks with the corresponding blocks in the rank truncated tensor. We define three hyperparameters $l_{1},l_{2},l_{3}$ that correspond to the thresholding parameters of the hard-thresholded SVD on modes 1,2,3 of the block tensor respectively. Specifically, for each mode- $i$ flattening $T_{(i)}^{n}$ , we calculate the full SVD $T_{(i)}^{n}\\,{=}\\,U S V^{T}$ . Since our tensor will scale cubically with the number of cameras, we suggest using a randomized SVD. We refer to [39] for different randomized strategies. Assume the singular values $\\sigma_{i}$ on the diagonal of $S$ are sorted in descending order, as usual. We return the factor matrix $A_{i}$ as the top $a$ left singular vectors in $U$ , where $a\\!=\\!\\operatorname*{max}\\{\\bar{i}:S_{i i}\\!>\\!l_{i}\\}$ . Our adapted truncation method is summarized by Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 HOSVD-HT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: $\\hat{T}^{n}\\in\\mathbb{R}^{3n\\times3n\\times3n}$ : the estimated block tensor; $l_{1},l_{2},l_{3}\\!\\in\\!\\mathbb{R}$ : the thresholds for modes 1,2,   \nrespectively   \nOutput: $\\bar{\\hat{T}}_{r}\\!\\in\\!\\mathbb{R}^{3n\\times3n\\times3n}$ : the rank truncated tensor.   \nfor $\\dot{\\mathbf{1}}=1$ to 3 do Perform the randomized SVD on the mode- $^{\\,\\cdot\\,i}$ flattening such that $\\hat{T}_{(i)}^{n}\\gets U S V^{T}$ $a_{i}\\!\\gets\\!\\operatorname*{max}\\{i:S_{i i}>l_{i}\\}$ $A_{i}\\leftarrow$ first $a_{i}$ columns of $U$   \nend for   \n$\\begin{array}{l}{{\\overline{{\\mathcal{G}}}\\!=\\!\\bar{T}^{\\bar{n}}\\!\\times\\!1A_{1}^{T}\\!\\times\\!2A_{2}^{T}\\!\\times\\!3A_{3}^{T}}}\\\\ {{\\hat{T}^{r}\\!\\gets\\![\\![\\mathcal{G};A_{1},A_{2},A_{3}]\\!]}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "From now on, we refer to hard-thresholded HOSVD as HOSVD-HT and denote the operation as $\\mathcal{P}_{h t}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Scale recovery ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "HOSVD-HT provides an efficient way for projecting ${\\hat{T}}^{n}$ onto the set of tensors with with truncated rank. To recover scales, we use the rank truncated tensor\u2019s relative scale as a heuristic to adjust the scale on our estimated block trifocal tensor ${\\hat{T}}^{(n)}$ . For each step, we solve ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Lambda^{(t+1)}\\!=\\!\\operatorname{argmin}_{\\Lambda}\\lVert\\Lambda\\odot\\hat{T}^{n}-\\mathcal{P}_{h t}(\\Lambda^{(t)}\\odot\\hat{T}^{n})\\rVert^{2}\\quad\\mathrm{s.t.~}\\Lambda_{i j k}\\!=\\!0_{3\\times3\\times3}\\operatorname{for}\\left(i,j,k\\right)\\!\\in\\!\\Omega^{C},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we drop the normalization condition on $\\Lambda$ because in practice it is not needed. We solve (5) for each observed block separately. Denoting ${\\mathcal{P}}_{h t}(\\Lambda^{(t)}{\\odot}{\\hat{T}}^{n})$ as $(\\hat{T}_{r}^{n})^{(t)}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Lambda_{i j k}^{(t+1)}\\!=\\!\\operatorname*{argmin}_{\\mu}\\lVert\\mu\\cdot\\hat{T}_{i j k}^{n}-(\\hat{T}_{r}^{n})_{i j k}^{(t)}\\rVert^{2}\\!=\\!\\frac{t r a c e((\\hat{T}_{i j k}^{n})_{(1)}^{T}((\\hat{T}_{r}^{n})_{i j k}^{(t)})_{(1)})}{\\lVert((\\hat{T}_{r}^{n})_{i j k}^{(t)})_{(1)}\\rVert_{F}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall that our strategy for completing the tensor is to impute the tensor with the entries from the rank truncated tensor using HOSVD-HT. Specifically, given the current imputed tensor $(\\hat{T}^{n})^{(t)}$ , we calculate $\\mathcal{P}_{h t}\\big((\\hat{T}^{n})^{(t)}\\big)$ and the new scales $\\Lambda^{(t+1)}$ . Then update with ", "page_idx": 6}, {"type": "equation", "text": "$$\n(\\hat{T}^{n})^{(t+1)}\\!=\\!(\\Lambda^{(t+1)}\\odot(\\hat{T}^{n})^{(t)}\\odot W_{\\Omega})\\!+\\!\\mathcal{P}_{h t}((\\hat{T}^{n})^{(t)})\\odot W_{\\Omega^{c}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.3 Synchronization algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we summarize our synchronization framework in Algorithm 2. We have observed that the ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Synchronization of the block trifocal tensor Input: $\\hat{T}^{n}\\in\\mathbb{R}_{-}^{3n\\times3n\\times3n};W_{\\Omega},W_{\\Omega}c\\in\\{0,1\\}^{3n\\times3n\\times3n};l_{1},l_{2},l_{3}\\in\\mathbb{R}$ Output: $\\mathcal{C}\\!\\in\\!\\mathbb{R}^{3n\\times4}$ : camera matrices up to a $4\\!\\times\\!4$ projective ambiguity and camera-wise scales Initialize ${\\hat{T}}^{n}$ by imputing unobserved blocks randomly to get $({\\hat{T}}^{n})^{\\bar{(0)}}$ while not converged do Calculate $\\mathcal{P}_{h t}\\big((\\hat{T}^{n})^{(t)}\\big)$ using HOSVD-HT Calculate $\\Lambda^{(t+1)}$ (5) using (6) $(\\hat{T}^{n})^{(t+1)}\\!\\gets\\!(\\Lambda^{(t+1)}\\odot(\\bar{\\hat{T}}^{n})^{(t)}\\odot W_{\\Omega})\\!+\\!\\mathcal{P}_{h t}((\\hat{T}^{n})^{(t)})\\odot W_{\\Omega}c$ t\u2190t+1 end while $(\\mathcal{G},A_{1},A_{2},A_{3})\\!\\gets\\!H O S V D((\\hat{T}^{n})^{(t)})$ $\\mathcal{C}\\!\\gets\\!\\mathrm{First}\\,4$ columns of $A_{2}$ ", "page_idx": 6}, {"type": "text", "text": "algorithm can overfit, as the recovered scales will experience sudden and huge leaps. Our stopping criteria for the algorithm is when we observe sudden jumps in the variance of the new scales or when we exceed a maximum number of iterations. Another challenge in structure from motion datasets is that estimations may be highly corrupted. The HOSVD framework mainly consists of retrieving a dominant subspace from each flattening. Thus, it is natural to replace the SVD on each flattening with a more robust subspace recovery method, such as the Tyler\u2019s M estimator (TME) [40] or a recent extension of TME that incorporates the information of the dimension of the subspace in the algorithm [41]. We refer to Appendix A.5.2 for more details and provide an implementation there. ", "page_idx": 6}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments of Algorithm 2 on two benchmark real datasets, the EPFL datasets [42] and the Photo Tourism datasets [11]. We observe that the algorithm performs better in the calibrated setting, and since the calibration matrix is usually known in practice, we restrict our scope of experiments to calibrated trifocal tensors. We compare against three state-of-the-art synchronization based on two view measurements, NRFM [18] and LUD [12]. NRFM relies on nonconvex optimization and requires a good initialization. We test NRFM with an initialization obtained from LUD and with a random initialization. We also test BATA [43] initialized with MPLS [9]. We refer to A.6 in the appendix for a comprehensive summary of numerical results including rotation and translation estimation errors. We include our code in the following github repository: TrifocalSync. ", "page_idx": 6}, {"type": "text", "text": "4.1 EPFL dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For EPFL, we follow the experimental setup and adopt code from [44] and test an entire structure from motion pipeline. We first describe the structure from motion pipeline for EPFL experiements. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Step 1 (feature detection and feature matching). We obtain matched features across pairs of images using a modern deep learning based feature detection and matching algorithm, GlueStick [45]. Though we do not implement this in our experiments, there have been methods developed to further screen corrupted keypoint matches or obtain matches robustly, such as [46, 47, 48]. Key points across a triplet of cameras is matched from pairs and is included only if it appears in all the pair combinations of the three images. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Step 2 (estimation and refinement of trifocal tensors). With the triplet matches, we calculate the trifocal tensors with more than 11 correspondences. To have an even sparser graph, one can skip the estimation of trifocal tensors and rely on the imputation for images that have less than a number bigger than 11 point correspondences. This can further speed up the trifocal tensor estimation process. We apply STE from [41] to find $40\\%$ of the correspondences as inliers, then use at most 30 inlier point correspondences to linearly estimate the trifocal tensor. To refine the estimates, we apply bundle adjustment on the inliers and delete triplets with reprojection error larger than 1 pixel. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 Step 3 (synchronization). We synchronize the estimated block trifocal tensor with a robust variant of SVD using the framework described in Algorithm 2. The robustness comes from replacing SVD with a robust subspace recovery method [41]. More details can be found in Appendix A.5.2. Recall that the cameras we retrieve are up to a global projective ambiguity. When comparing with ground truth poses, we first align our estimated cameras with the ground truth cameras by finding a $4\\!\\times\\!4$ projective transformation. Then we round the cameras to calibrated cameras and compare. ", "page_idx": 7}, {"type": "text", "text": "We test our full pipeline on two EPFL datasets on a personal machine with 2 GHz Intel Core i5 with 4 cores and 16GB of memory. To test NRFM [18], LUD [12] and BATA [43] initialized with MPLS [9], we estimate the corresponding essential matrices using the GC-ransac [49]. We did not include blocks corresponding to two views in our trifocal tensor pipeline. The mean and median translation errors are summarized in Figure 1 here and more comprehensive results can be found in Table 1 and Table 2 in the appendix. ", "page_idx": 7}, {"type": "image", "img_path": "DT7n4F2bbP/tmp/41aa8ea7fe90b0d519945bfe60878d41809dd0816e25c7aac1ede237648ca958.jpg", "img_caption": ["Figure 1: EPFL translation error comparison between our method, NRFM initialized by LUD, LUD, and NRFM initialized randomly. BATA(MPLS) stands for BATA initialized by MPLS. HZ8 stands for HerzP8, FP11 for FountainP11, HZ25 for Herz P25, EN10 for EntryP10, CS19 for CastleP19, CS30 for CastleP30. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The EPFL datasets generally have a plethora of point correspondences, so that the trifocal tensors are estimated accurately. When the dataset focuses on a single scene, our algorithm retrieves locations competitively. Our algorithm achieves the best location estimation for 4 out of 6 datasets. The translation error bars are not visible for FP11, HZP8, EN10 due to the accuracy that we achieve. However, our pipeline is incapable of accurately processing CastleP19 and CastleP30. The main reason is that our algorithm relies on having a very dense observation graph to ensure high completion rate. CastleP19 and CastleP30 are datasets where the camera scans portions of the general area sequentially, so that not many triplets have overlapping features. Our method is not suitable for this type of dataset. However, it is possible to apply our algorithm in parallel on groups of neighboring frames, so that the completion rate is high in each group. Then the results can be merged to obtain a larger reconstruction. Rotations for the two view methods are estimated via rejecting outliers from iteratively applying [10]. We also compare against [43] for location estimation, where we initialize with a state-of-the-art global rotation estimation method [9]. Our algorithm achieves superior rotation estimation for only 2 out of the 6 datasets. See Table 1 and 2 in the appendix for comprehensive errors. ", "page_idx": 7}, {"type": "text", "text": "4.2 Photo Tourism ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on the Photo Tourism datasets. The Photo Tourism datasets consist of internet images of real world scenes. Each scene has hundreds to thousands of images. The datasets [11] provide essential matrix estimates, and we estimate the trifocal tensors from the given essential matrices. To limit the computational cost for tensors, we downsample the datasets by choosing cameras with observations more than a certain percentage in the corresponding block frontal slice while maintaining a decent number of cameras. Note that this may not be the optimal way of extracting a dense subset in general. The maximum number of cameras we select for each dataset is 225 cameras. The largest dataset Piccadilly has 2031 cameras initially. We randomly sample 1000 cameras and then run our procedure. For Roman Forum and Piccadilly, the two view methods further deleted cameras from the robust rotation estimation process or parallel rigidity test. We rerun and report the trifocal tensor synchronization algorithm with the further downsampled data. We initialize the hard thresholding parameters for HOSVD-HT by first imputing the trifocal tensor with small random entries and then calculating the singular values for each of the flattenings. We take $l_{i}$ to be the tertile singular value for each mode- $^{-i}$ flattening. We then keep this parameter fixed for the synchronization process. Recall that the $j i i$ blocks in the block trifocal tensor correspond to elements in the essential matrix $E_{i j}$ . We also include these essential matrix estimations in the block trifocal tensor. The Photo Tourism experiments were run on an HPC center with 32 cores, but the only procedure that can benefit from parallel computing in a single experiment is the scale retrieval. Mean and median translation errors are summarized in Figure 2. Fully comprehensive results can be found in Tables 3 and 4 in Appendix A.6. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "DT7n4F2bbP/tmp/a8101a2ed855cb153e239af23c05eb66a50ea40baf9c58bb5a35a2f794b9dd46.jpg", "img_caption": ["Figure 2: Photo Tourism translation error comparison between our method, NRFM initialized by LUD, LUD, NRFM initialized randomly, and BATA initialized with MPLS. Note that we have not been able to acquire results for Piccadilly for B $\\mathrm{ATA+MPLS}$ . ", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our method is able to achieve competitive translation errors on 8 of the 14 datasets tested. Similar to the observation in the EPFL experiments, our algorithm performs well when the viewing graph is dense, or in other words, when the estimation percentage is high. We achieve better locations in 6 out of 8 datasets where the estimation percentage exceeds $60\\%$ , and better locations in only 2 out of 6 datasets where the estimation percentage falls below $60\\%$ . We achieve reasonable rotation estimations for 10 out of 14 datasets, but not as good as LUD. See Table 4 for a comprehensive result. Since the block trifocal tensor scales cubically with respect to the number of cameras, our algorithm runtime is longer than most two view global methods. This could be alleviated by synchronizing dense subsets in parallel and merging the results to construct a larger reconstruction. ", "page_idx": 8}, {"type": "text", "text": "Additional remark: Trifocal tensors can be estimated from line correspondences or a mix of point and line correspondences, while fundamental matrices are estimated from only point correspondences. There are many situations where accurate point correspondences are in short supply but there is a plethora of clear and distinct lines. For example, see datasets in a recent SfM method using lines [50]. We demonstrate the potential of our method to be adapted to process datasets with only lines or very few points. Due to the limited availability of well annotated line datasets, we provide a small synthetic experiment that simulates a case where only lines correspondences are present. We first generate 20 random camera matrices, then we generate 25 lines that are projected on and shared across all images. We add about 0.02 percent of noise in terms of the relative frobenius norms between the line equation parameters and the noise. We estimate the trifocal tensor of three different views from line correspondences linearly. One remark is that our synchronization method works well only when the signs of the initial unknown scales are mostly uniform. We manually use ground truth trifocal tensors to correct the sign of the scale. This has not been an issue in the previous experiments due to bundle adjustment for EPFL and the overall good estimations in Photo Tourism. In practice, the sign of the scale on a trifocal tensor can be corrected via triangulation of points or reconstruction of lines, and correcting the sign using the depths of the reconstructed points or intersecting line segments. We synchronize the trifocal tensors with Algorithm 2 and were able to achieve a mean rotation error of 0.61 degrees, median rotation error of 0.49 degrees, mean location error of 0.76, and median location error of 0.74. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced the block tensor of trifocal tensors characterizing the three-view geometry of a scene. We established an explicit Tucker factorization of the block trifocal tensor and proved it has a low multilinear rank of (6,4,4) under appropriate scaling. We developed a synchronization algorithm based on tensor decomposition that retrieves an appropriate set of scales, and synchronizes rotations and translations simultaneously. On several real data benchmarks we demonstrated state-of-the-art performance in terms of camera location estimation, and saw particular advantages on smaller and denser sets of images. Overall, this work suggests that higher-order interactions in synchronization problems have the potential to improve performance over pairwise-based methods. ", "page_idx": 9}, {"type": "text", "text": "There are several limitations to our tensor-based synchronization method. First, our rotation estimations are not as strong as our location estimations. Second, our algorithm performance is affected by the estimation percentage of trifocal tensors within the block trifocal tensor. One could incorporate more robust completion methods and explore new approaches for processing sparse triplet graphs. Further, our block trifocal tensor scales cubically in terms of the number of cameras and becomes computationally expensive for large datasets. We can develop methods for extracting dense subgraphs, synchronizing in parallel, then merging results to obtain a larger reconstruction, similarly to the distributed algorithms of [51] and [52]. Moreover, our synchronization method\u2019s success depends on accurate trifocal tensor estimations, and it motivates further work on robust estimation of multi-view tensors. Algorithm 2 could also be made more robust by adding outlier rejection techniques. Finally we plan to extend our theory by proving convergence of our algorithm and exploring structures for even higher-order tensors, such as quadrifocal tensors. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "D.M. and G.L. were supported in part by NSF award DMS 2152766. J.K. was supported in part by NSF awards DMS 2309782 and CISE-IIS 2312746, the DOE award SC0025312, and start-up grants from the College of Natural Science and Oden Institute at the University of Texas at Austin. ", "page_idx": 9}, {"type": "text", "text": "We thank Shaohan Li and Feng Yu for helpful discussions on processing EPFL and Photo Tourism.   \nWe also thank Hongyi Fan for helpful advice and references on estimating trifocal tensors. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2003.   \n[2] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo Tourism: Exploring photo collections in 3D. In Proceedings of the ACM Special Interest Group on Computer Graphics and Interactive Techniques Conference, SIGGRAPH 2006, pages 835\u2013846, 2006.   \n[3] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 4104\u20134113, 2016.   \n[4] Bill Triggs, Philip McLauchlan, Richard Hartley, and Andrew Fitzgibbon. Bundle adjustment \u2014 A modern synthesis. In Bill Triggs, Andrew Zisserman, and Richard Szeliski, editors, Vision Algorithms: Theory and Practice, pages 298\u2013372, Berlin, Heidelberg, 2000. Springer Berlin Heidelberg.   \n[5] Venu Madhav Govindu. Lie-algebraic averaging for globally consistent motion estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2004, volume 1, pages 1\u20138, 2004.   \n[6] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong Li. Rotation averaging. International Journal of Computer Vision, 103:267\u2013305, 2013.   \n[7] Avishek Chatterjee and Venu Madhav Govindu. Robust relative rotation averaging. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):958\u2013972, 2018.   \n[8] Avishek Chatterjee and Venu Madhav Govindu. Efficient and robust large-scale rotation averaging. In Proceedings of the IEEE International Conference on Computer Vision, ICCV 2013, pages 521\u2013528, 2013.   \n[9] Yunpeng Shi and Gilad Lerman. Message passing least squares framework and its application to rotation synchronization. In Proceedings of the International Conference on Machine Learning, ICML 2020, pages 8796\u20138806, 2020.   \n[10] Mica Arie-Nachimson, Shahar Kovalsky, Ira Kemelmacher-Shlizerman, Amit Singer, and Ronen Basri. Global motion estimation from point matches. In Proceedings of the International Conference on 3D Imaging, Modeling, Processing, Visualization & Transmission, 3DIMPVT 2012, pages 81\u201388, 2012.   \n[11] Kyle Wilson and Noah Snavely. Robust global translations with 1DSfM. In Proceedings of the European Conference on Computer Vision, EECV 2014, pages 61\u201375, 2014.   \n[12] Onur Ozyesil and Amit Singer. Robust camera location estimation by convex programming. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, pages 2674\u20132683, 2015.   \n[13] Thomas Goldstein, Paul Hand, Choongbum Lee, Vladislav Voroninski, and Stefano Soatto. ShapeFit and ShapeKick for robust, scalable structure from motion. In Proceedings of the European Conference on Computer Vision, EECV 2016, pages 289\u2013304, 2016.   \n[14] David Rosen, Luca Carlone, Afonso Bandeira, and John Leonard. SE-Sync: A certifiably correct algorithm for synchronization over the special Euclidean group. International Journal of Robotics Research, 38(2-3):95\u2013125, 2019.   \n[15] Federica Arrigoni, Beatrice Rossi, and Andrea Fusiello. Spectral synchronization of multiple views in SE(3). SIAM Journal on Imaging Sciences, 9(4):1963\u20131990, 2016.   \n[16] Mihai Cucuringu, Yaron Lipman, and Amit Singer. Sensor network localization by eigenvector synchronization over the Euclidean group. ACM Transactions on Sensor Networks, 8(3), 2012.   \n[17] Jesus Briales and Javier Gonzalez-Jimenez. Cartan-Sync: Fast and global SE(d)-synchronization. IEEE Robotics and Automation Letters, 2(4):2127\u20132134, 2017.   \n[18] Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David Jacobs, Amit Singer, and Ronen Basri. A new rank constraint on multi-view fundamental matrices, and its application to camera location recovery. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, pages 4798\u20134806, 2017.   \n[19] Yoni Kasten, Amnon Geifman, Meirav Galun, and Ronen Basri. Algebraic characterization of essential matrices and their averaging in multiview settings. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 5895\u20135903, 2019.   \n[20] Yoni Kasten, Amnon Geifman, Meirav Galun, and Ronen Basri. GPSfM: Global projective SFM using algebraic constraints on multi-view fundamental matrices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 3264\u20133272, 2019.   \n[21] Spyridon Leonardos, Roberto Tron, and Kostas Daniilidis. A metric parametrization for trifocal tensors with non-colinear pinholes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, pages 259\u2013267, 2015.   \n[22] Viktor Larsson, Nicolas Zobernig, Kasim Taskin, and Marc Pollefeys. Calibration-free structure-from-motion with calibrated radial trifocal tensors. In Proceedings of the European Conference on Computer Vision, EECV 2020, pages 382\u2013399, 2020.   \n[23] Pierre Moulon, Pascal Monasse, and Renaud Marlet. Global fusion of relative motions for robust, accurate and scalable structure-from-motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2013, pages 3248\u20133255, 2013.   \n[24] Onur Ozyesil, Vladislav Voroninski, Ronen Basri, and Amit Singer. A survey of structure from motion. Acta Numerica, 26:305\u2013364, 2017.   \n[25] Joe Kileel. Minimal problems for the calibrated trifocal variety. SIAM Journal on Applied Algebra and Geometry, 1(1):575\u2013598, 2017.   \n[26] Timothy Duff, Kathlen Kohn, Anton Leykin, and Tomas Pajdla. PLMP-point-line minimal problems in complete multi-view visibility. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 1675\u20131684, 2019.   \n[27] Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret Regan, David da Costa de Pinho, Elias Tsigaridas, Charles Wampler, Jonathan Hauenstein, Peter Giblin, Benjamin Kimia, Anton Leykin, and Tomas Pajdla. Trifocal relative pose from lines at points. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(6):7870\u20137884, 2023.   \n[28] David Nister and Henrik Stewenius. A minimal solution to the generalised 3-point pose problem. Journal of Mathematical Imaging and Vision, 27(1):67\u201379, 2007.   \n[29] Ali Elqursh and Ahmed Elgammal. Line-based relative pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011, pages 3049\u20133056, 2011.   \n[30] Yubin Kuang and Kalle Astrom. Pose estimation with unknown focal length using points, directions and lines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2013, pages 529\u2013536, 2013.   \n[31] Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, and Tomas Pajdla. A clever elimination strategy for efficient minimal solvers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, pages 4912\u20134921, 2017.   \n[32] Pedro Miraldo, Tiago Dias, and Srikumar Ramalingam. A minimal closed-form solution for multi-perspective pose estimation using points and lines. In Proceedings of the European Conference on Computer Vision, ECCV 2018, pages 474\u2013490, 2018.   \n[33] Joe Kileel, Zuzana Kukelova, Tomas Pajdla, and Bernd Sturmfels. Distortion varieties. Foundations of Computational Mathematics, 18:1043\u20131071, 2018.   \n[34] Joe Kileel and Kathl\u00e9n Kohn. Snapshot of algebraic vision. arXiv preprint arXiv:2210.11443, 2022.   \n[35] Tamara Kolda and Brett Bader. Tensor decompositions and applications. SIAM Review, 51(3):455\u2013500, 2009.   \n[36] Tommi Muller, Adriana Duncan, Eric Verbeke, and Joe Kileel. Algebraic constraints and algorithms for common lines in cryo-EM. Biological Imaging, pages 1\u201330, Published online 2024.   \n[37] Hongyi Fan, Joe Kileel, and Benjamin Kimia. On the instability of relative pose estimation and RANSAC\u2019s role. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2022, pages 8935\u20138943, 2022.   \n[38] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287\u20132322, 2010.   \n[39] Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217\u2013288, 2011.   \n[40] David Tyler. A distribution-free M-estimator of multivariate scatter. Annals of Statistics, pages 234\u2013251, 1987.   \n[41] Feng Yu, Teng Zhang, and Gilad Lerman. A subspace-constrained Tyler\u2019s estimator and its applications to structure from motion. arXiv preprint arXiv:2404.11590, 2024.   \n[42] Christoph Strecha, Wolfgang Von Hansen, Luc Van Gool, Pascal Fua, and Ulrich Thoennessen. On benchmarking camera calibration and multi-view stereo for high resolution imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2008, pages 1\u20138, 2008.   \n[43] Bingbing Zhuang, Loong-Fah Cheong, and Gim Hee Lee. Baseline desensitizing in translation averaging. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018), June 2018.   \n[44] Laura Julia and Pascal Monasse. A critical review of the trifocal tensor estimation. In Proceedings of the Pacific Rim Symposium on Image and Video Technology, PSIVT 2017, Revised Selected Papers 8, pages 337\u2013349. Springer, 2018.   \n[45] R\u00e9mi Pautrat, Iago Su\u00e1rez, Yifan Yu, Marc Pollefeys, and Viktor Larsson. Gluestick: Robust image matching by sticking points and lines together. In Proceedings of the IEEE/CVF International Conference on Computer Vision, CVPR 2023, pages 9706\u20139716, 2023.   \n[46] Yunpeng Shi, Shaohan Li, Tyler Maunu, and Gilad Lerman. Scalable cluster-consistency statistics for robust multi-object matching. In Proceedings of the International Conference on 3D Vision, 3DV 2021, pages 352\u2013360, 2021.   \n[47] Yunpeng Shi, Shaohan Li, and Gilad Lerman. Robust multi-object matching via iterative reweighting of the graph connection laplacian. Advances in Neural Information Processing Systems, 33:15243\u201315253, 2020.   \n[48] Shaohan Li, Yunpeng Shi, and Gilad Lerman. Fast, accurate and memory-efficient partial permutation synchronization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2022, pages 15735\u201315743, 2022.   \n[49] Daniel Barath and Ji\u02c7r\u00ed Matas. Graph-cut ransac. In Proceedings of the IEEE conference on computer vision and pattern recognition, CVPR 2018, pages 6733\u20136741, 2018.   \n[50] Shaohui Liu, Yifan Yu, R\u00e9mi Pautrat, Marc Pollefeys, and Viktor Larsson. 3d line mapping revisited. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, pages 21445\u201321455, 2023.   \n[51] Shaohan Li, Yunpeng Shi, and Gilad Lerman. Efficient detection of long consistent cycles and its application to distributed synchronization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, pages 5260\u20135269, 2024.   \n[52] Andrea Porfiri Dal Cin, Luca Magri, Federica Arrigoni, Andrea Fusiello, and Giacomo Boracchi. Synchronization of group-labelled multi-graphs. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 6433\u20136443. IEEE, 2021.   \n[53] Joe Harris. Algebraic Geometry: A First Course, volume 133. Springer Science & Business Media, 1992.   \n[54] Ying Sun, Prabhu Babu, and Daniel Palomar. Regularized Tyler\u2019s scatter estimator: Existence, uniqueness, and algorithms. IEEE Transactions on Signal Processing, 62(19):5143\u20135156, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Derivation of the trifocal tensor ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To provide a better intuition for the trifocal tensor, we briefly summarize the derivation of the trifocal tensor from [1] and [21] under the general setup of uncalibrated cameras. ", "page_idx": 13}, {"type": "text", "text": "Let $P_{i}\\!=\\!K_{i}R_{i}[I,\\!-\\!t_{i}]$ be the form of the camera matrix for $P_{1},P_{2},P_{3}$ . Let $L$ be a line in the 3D world scene, and $l_{1},l_{2},l_{3}$ the corresponding projections in the images $I_{1},I_{2},I_{3}$ respectively. Each $l_{i}$ back projects to a plane $\\pi_{i}=P_{i}^{T}\\bar{l_{i}}$ in $\\mathbb{R}^{3}$ , and since $l_{i}$ correspond to the same $L$ in the 3D world scene, $\\pi\\!=\\!\\left[\\pi_{1},\\!\\pi_{2},\\!\\pi_{3}\\right]$ must be rank deficient and its kernel will generically be spanned by $L$ . Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi^{\\prime}\\!=\\!\\left[\\!\\!\\begin{array}{c c}{{K_{1}^{-T}R_{1}}}&{{0}}\\\\ {{t_{1}^{T}}}&{{1}}\\end{array}\\!\\!\\right]\\pi\\!=\\!\\left[\\!\\!\\begin{array}{c c c}{{l_{1}}}&{{K_{1}^{-T}R_{12}K_{2}^{T}l_{2}}}&{{K_{1}^{-T}R_{13}K_{3}^{T}l_{3}}}\\\\ {{0}}&{{(t_{1}\\!-\\!t_{2})^{T}R_{2}^{T}K_{2}^{T}l_{2}}}&{{(t_{1}\\!-\\!t_{3})^{T}R_{3}^{T}K_{3}^{T}l_{3}}}\\end{array}\\!\\!\\right]\\!=\\![\\!\\pi_{1}^{\\prime},\\pi_{2}^{\\prime},\\pi_{3}^{\\prime}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "will also be rank-deficient, implying that the columns of $\\pi^{\\prime}$ are linearly dependent. This means that there exist $\\alpha,\\beta$ such that $\\pi_{1}^{\\prime}\\!=\\!\\alpha\\pi_{2}^{\\prime}\\!+\\!\\beta\\pi_{3}^{\\prime}$ . We can choose $\\alpha\\!=\\!-(t_{1}\\!-\\!t_{3})^{T}\\mathring{R}_{3}^{T}\\mathring{K}_{3}^{T}l_{3},\\beta\\!=\\!(t_{1}\\!-\\!t_{2})^{T}R_{2}^{T}K_{2}^{T}l_{2}$ , so that ", "page_idx": 13}, {"type": "equation", "text": "$$\nl_{1}\\!=\\!l_{2}^{T}[K_{2}R_{2}(t_{1}\\!-\\!t_{2})K_{1}^{-T}R_{13}K_{3}^{T}]l_{3}\\!-\\!l_{2}^{T}[K_{2}R_{12}^{T}K_{1}^{-1}(t_{1}\\!-\\!t_{3})^{T}R_{3}^{T}K_{3}^{T}]l_{3}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, the canonical trifocal tensor centered at camera 1 is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\nT_{i}\\!=\\!K_{2}R_{2}(t_{1}\\!-\\!t_{2})e_{i}^{T}K_{1}^{-T}R_{13}K_{3}^{T}\\!-\\!K_{2}R_{12}^{T}K_{1}^{-1}e_{i}(t_{1}\\!-\\!t_{3})^{T}R_{3}^{T}K_{3}^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $e_{i}\\!\\in\\!\\mathbb{R}^{3}$ is the $i$ -th standard basis vector. The trifocal tensor will be the tensor $\\{T_{1},T_{2},T_{3}\\}$ , where the $T_{i}$ \u2019s are stacked along the first mode. The line incidence relation is then $(l_{1})_{i}=l_{2}^{T}T_{i}l_{3}$ . Other combinations of point and line incidence relations are also encoded by the trifocal tensor; see [1] for details. The construction for calibrated cameras is the same, just with $P_{i}$ in calibrated form. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof details for Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We include a detailed calculation for the Tucker factorization of the block trifocal tensor. Recall that each individual trifocal tensor corresponding to the cameras $^{a,b,c}$ can be calculated as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{i\\varphi r}=(-1)^{i+1}\\mathrm{det}\\left[\\stackrel{\\sim}{b^{\\mu}}_{c}\\right]=(-1)^{i+1}\\mathrm{det}\\left[\\stackrel{\\sim}{b^{\\mu}}_{c}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ =(-1)^{i+1}\\mathrm{det}\\left[\\stackrel{\\sim}{a^{\\nu}}_{0}3\\ \\ \\ m_{\\#}\\Bigg]\\left(b_{i\\theta_{1}}c_{\\gamma}-b_{i\\theta_{2}}c_{\\nu_{1}}\\right)+\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{2}}\\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(-b_{i\\theta_{1}}c_{\\nu_{1}}+b_{i\\theta_{2}}c_{\\nu_{1}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{1}}\\ \\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(b_{i\\theta_{2}}c_{\\nu_{2}}-b_{i\\theta_{2}}c_{\\nu_{2}}\\right)+\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{2}}\\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(b_{i\\theta_{1}}c_{\\nu_{1}}-b_{i\\theta_{2}}c_{\\nu_{1}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{1}}\\ \\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(b_{i\\theta_{2}}c_{\\nu_{2}}-b_{i\\theta_{2}}c_{\\nu_{2}}\\right)+\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{2}}\\ \\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(b_{i\\theta_{1}}c_{\\nu_{1}}-b_{i\\theta_{2}}c_{\\nu_{1}}+b_{i\\theta_{2}}c_{\\nu_{1}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathrm{det}\\left[\\stackrel{\\mathrm{a}}{a^{\\mu}}_{n\\theta_{1}}\\ \\ \\ \\ \\ a_{m\\mathbf{i}}\\right]\\left(b_{i\\theta_{2}}c_{\\nu_{1}}+b_{i\\theta_{2}}c \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last equality can be easily checked since $\\mathcal{G}$ is sparse. For example, when $k=1$ , $\\mathcal{P}(a)_{i1}$ is the determinant of the submatrix dropping the $i$ -th row and keeping columns 1 and 2, which is $\\operatorname*{det}[a_{m1}a_{m2};a_{n1}a_{n2}]$ . The only nonzero elements in the first horizontal slice are $\\mathcal{G}(1,4,3)=-1$ and ${\\mathcal{G}}(1,\\,3,\\,4)\\ =\\ 1$ . Then, the nonzero elements in the sum when $k\\ =\\ 1$ will be exactly $\\begin{array}{r}{\\mathcal{P}(a)_{i1}{\\sum_{w=1}^{4}}b_{q w}{\\sum_{j=1}^{4}}c_{r j}\\mathcal{G}_{1w j}\\!=\\!\\mathcal{P}(a)_{i1}\\big(b_{q3}c_{r4}\\!-\\!b_{q4}c_{r3}\\big).}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Then, since $\\mathcal{P}$ will be the stackings of $\\mathcal{P}(P_{i})$ , $\\mathcal{C}$ is the stacking of camera matrices in Theorem 1, each $i j k$ block in $T^{n}$ will be calculated by exactly the corresponding $i,j,k$ blocks in $\\mathcal{P}\\mathcal{C}\\mathcal{C}$ respectively using the calculations above. ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof details for Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof for $(i)$ . We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(T_{i i i}^{n})_{w q r}\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\stackrel{\\sim}{P_{i}^{u}}\\right]\\!=\\!0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since $P_{i}$ is a $3\\!\\times\\!4$ matrix and the submatrix above will always have two identical rows. ", "page_idx": 14}, {"type": "text", "text": "Proof for $(i i)$ : Consider the wqr element of the $j i i$ block trifocal tensor, $T_{j i i}^{n}$ . It can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n(T_{j i i}^{n})_{w q r}\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\stackrel{\\sim\\,P_{j}^{w}}{P_{i}^{q}}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, when $q=r$ , clearly $(T_{j i i}^{n})_{w q r}\\,{=}\\,0$ as we will have identical rows again. When $q\\not=r$ , we first observe that $(T_{j i i}^{n})_{w q r}\\!=\\!-(T_{j i i}^{n})_{w r q}$ since we just swap two rows. Second, ", "page_idx": 14}, {"type": "equation", "text": "$$\n(T_{j i i}^{n})_{w q r}\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\!\\!\\begin{array}{c}{{\\!\\!\\sim\\!P_{j}^{w}\\!}}\\\\ {{P_{i}^{d}\\!}}\\\\ {{P_{i}^{r}\\!}}\\end{array}\\!\\!\\right]\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\!\\!\\begin{array}{c}{{\\!\\!\\sim\\!P_{j}^{w}\\!}}\\\\ {{\\!\\!\\sim P_{i}^{m}\\!}}\\end{array}\\!\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $m\\in\\{1,2,3\\}\\setminus\\{q,r\\}$ . This is exactly the bilinear relationship in [1] defining the fundamental matrix $(F_{j i})_{m w}$ element up to a possible negative sign. ", "page_idx": 14}, {"type": "text", "text": "Proof for $(i i i)$ : We can only show this for $T_{i i j}^{n}$ blocks from symmetry. The elements in $T_{i i j}^{n}$ blocks can be calculated as ", "page_idx": 14}, {"type": "equation", "text": "$$\n(T_{i j i}^{n})_{w q r}\\!=\\!(-1)^{w+1}\\mathrm{det}\\left[\\stackrel{\\displaystyle\\sim P_{i}^{w}}{P_{i}^{q}}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Elements are nonzero only when $w\\!=\\!q$ , and they correspond to determinants of matrices with three rows from one $P_{i}$ and one row from $P_{j}$ . By [1], these are exactly the elements of the epipoles. When $w\\!=\\!1$ , the order of the rows in the determinant corresponding to camera $i$ is (2,3,1), when $w\\!=\\!2$ , the order is (1,3,2) and there is a negative sign in front of the determinant, and when $w\\!=\\!3$ , the order is (1,2,3). Since the first and last case are even permutations of the rows of $P_{i}$ , and the second case is corrected by a negative sign, $(T_{i j i}^{n})_{w w}$ : is exactly the epipole. ", "page_idx": 14}, {"type": "text", "text": "Proof for $(i\\nu)$ : On a horizontal slice, the camera along the 1st mode is fixed, and blocks symmetric across the diagonal is calculated by cameras, which the 2nd and 3rd mode cameras are swapped. Then, we will simply be swapping rows in (1), which means that we will simply be changing signs for elements symmetric across the diagonal, implying skew symmetry. ", "page_idx": 14}, {"type": "text", "text": "Proof for (v): Now assume that we have a block trifocal tensor whose corresponding cameras are all calibrated. Let $\\mathcal{P}$ be the line projection matrix, $\\boldsymbol{\\mathcal{C}}\\,=\\,[P_{1},P_{2},\\dots,P_{n}]^{T}$ is the stacked camera matrix, and $\\mathcal{G}$ is the core tensor. The flattening in the 1st mode can be written as $T_{(1)}^{n}=\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}\\otimes\\mathcal{C})^{T}$ , where $T_{(1)}^{n}$ is a $3n\\times9n^{2}$ matrix. For the proof, we calculate the eigenvalue of $\\'\\!\\!\\!\\!\\!\\!\\!\\!\\!T_{(1)}^{n}(T_{(1)}^{n})^{T}\\!=\\!\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}\\otimes\\mathcal{C})^{T}(\\mathcal{C}\\otimes\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{(1)}^{n}(T_{(1)}^{n})^{T}\\!=\\!\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}\\otimes\\mathcal{C})^{T}(\\mathcal{C}\\otimes\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\otimes\\mathcal{C}^{T})(\\mathcal{C}\\otimes\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second and third line uses two Kronecker product properties: $(A\\otimes B)^{T}=A^{T}\\otimes B^{T}$ and $(A\\otimes B)(C\\otimes D)\\!=\\!A C\\otimes\\!B D$ as long as $A C$ and $B D$ are defined. ", "page_idx": 14}, {"type": "text", "text": "We first calculate $(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})$ . ", "page_idx": 14}, {"type": "text", "text": "We assume that the cameras are centered at the origin, i.e. $\\textstyle\\sum_{i=1}^{n}t_{i}\\,{=}\\,0$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{C}^{T}\\mathcal{C}\\!=\\!\\left[\\!\\!-\\!\\sum_{i=1}^{n{I}_{3\\times3}}\\!\\!\\!\\!t_{i}^{T}\\!\\!\\!\\right]\\!,\\;\\;\\;\\sum_{i=1}^{n}\\!\\!\\!t_{i}\\Vert^{2}\\!\\right]=\\!\\!\\left[\\!\\!\\begin{array}{c c}{n{I}_{3\\times3}}&{\\!\\!\\!0_{3\\times1}}\\\\ {0_{1\\times3}}&{\\!\\!\\!\\sum_{i=1}^{n}\\!\\!\\!\\Vert{t}_{i}\\Vert^{2}\\!\\!}\\end{array}\\!\\!\\right]\\!,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "so that ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\!=\\!\\!\\left[\\!\\!\\begin{array}{c c}{n I_{3\\times3}\\mathcal{C}^{T}\\mathcal{C}}&{0_{3\\times1}\\otimes\\mathcal{C}^{T}\\mathcal{C}}\\\\ {0_{1\\times3}\\mathcal{C}^{T}\\mathcal{C}}&{\\sum_{i=1}^{n}\\lVert t_{i}\\rVert^{2}\\otimes\\mathcal{C}^{T}\\mathcal{C}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have an explicit form for $\\mathcal{G}_{(1)}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{G}_{(1)}=\\left[\\begin{array}{c c c c c c c c c c c c c c c c c}{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{-1}&{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}&{1}&{0}&{0}&{0}&{0}&{0}&{-1}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{-1}&{0}&{0}&{1}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{-1}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}&{0}&{0}&{0}&{-1}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{-1}&{0}&{0}&{1}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $X\\!=\\!\\mathcal{C}^{T}\\mathcal{C}$ and let $X_{i j}$ denote the $i j$ entry in ${\\mathcal{C}}^{T}{\\mathcal{C}}$ . Let $\\textstyle a\\!=\\!\\sum_{i=1}^{n}\\lVert t_{i}\\rVert^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "We first show that $\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}$ is diagonal by direct computation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{11}(Y^{T}\\boldsymbol{\\mathcal{C}}_{6}\\boldsymbol{\\cup}Y^{T}\\boldsymbol{\\mathcal{C}}_{1}^{T})}\\\\ &{\\quad\\quad\\quad\\quad\\times\\boldsymbol{\\mathcal{A}}_{31}\\quad\\boldsymbol{\\mathcal{A}}_{32}\\quad\\quad\\quad\\quad\\quad\\pi\\boldsymbol{\\mathcal{X}}_{43}\\quad\\quad\\quad\\alpha\\boldsymbol{\\mathcal{X}}_{31}\\quad\\quad\\quad\\quad\\boldsymbol{\\mathcal{X}}_{44}\\quad\\quad\\quad\\quad\\quad\\;\\;\\;}\\\\ &{\\quad\\quad-\\frac{\\mathcal{X}_{1}(Y^{T})}{\\mathcal{X}_{32}}\\quad\\quad\\quad\\quad\\;\\boldsymbol{\\mathcal{X}}_{44}\\quad\\quad\\quad\\boldsymbol{\\mathcal{A}}_{22}\\quad\\quad\\quad\\quad\\;\\;\\boldsymbol{\\mathcal{X}}_{53}\\quad\\quad\\quad\\;\\;\\;\\;\\;\\boldsymbol{a}\\quad\\quad\\quad\\;\\;\\;\\;\\;\\;\\boldsymbol{\\mathcal{D}}_{14}\\quad\\quad\\quad\\quad\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\quad\\quad\\quad-\\frac{\\mathcal{X}_{33}}{\\mathcal{N}_{33}}\\quad\\quad\\quad-\\frac{\\mathcal{N}_{1}\\boldsymbol{\\mathcal{X}}_{34}}{\\mathcal{N}_{12}}\\quad\\quad\\quad\\;\\;\\boldsymbol{\\mathcal{X}}_{33}+\\boldsymbol{\\mathcal{N}}_{22}\\quad\\quad\\quad\\quad\\;\\;\\boldsymbol{\\mathcal{X}}_{44}+\\boldsymbol{\\mathcal{N}}_{11}\\quad\\quad\\quad\\quad\\;\\;-\\frac{\\mathcal{N}_{12}}{\\mathcal{N}_{33}}\\quad\\quad\\quad\\;\\;-\\boldsymbol{\\mathcal{N}}_{41}\\quad\\quad\\;\\;\\;\\;\\;-\\boldsymbol{n}\\boldsymbol{\\mathcal{X}}_{33}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\boldsymbol{\\mathcal{X}}_{14}\\quad\\quad\\quad\\quad-\\boldsymbol{\\mathcal{N}}_{12}\\quad\\quad\\quad\\quad\\quad-\\boldsymbol{\\mathcal{N}}_{12}\\quad\\quad\\quad\\quad\\;\\;\\;\\boldsymbol{\\mathcal{X}}_{14}\\quad\\quad\\quad\\;\\;\\;\\boldsymbol{\\mathcal{X}}_{31}\\quad\\quad\\quad\\;\\;\\;\\;\\boldsymbol{\\mathcal{N}}_{23}\\quad\\quad\\quad\\quad\\;\\;\\;\\boldsymbol{\\mathcal{N}}_{34}\\quad\\quad\\quad\\;\\;\\;\\;\\boldsymbol{\\mathcal{D}}_{35}\\quad\\quad\\quad\\;\\;\\;\\;\\;\\boldsymbol{\\mathcal{X}}_{72}\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then calculate the spectral decomposition of $T_{(1)}^{n}$ . With a slight abuse of notation, let $P_{i}^{k}$ denote the $k$ -th column of $P_{i}$ . The $3n\\!\\times\\!6$ rank-6 stacked line projection matrix would have columns ordered according to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}{\\big[e_{1}\\wedge e_{2}}&{\\;e_{1}\\wedge e_{3}}&{\\;e_{1}\\wedge e_{4}}&{\\;e_{2}\\wedge e_{3}}&{\\;e_{2}\\wedge e_{4}}&{\\;e_{3}\\wedge e_{4}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and since the second row in $\\mathcal{P}$ for each camera is $P_{i}^{3}{\\wedge}P_{i}^{1}$ it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{P}\\!=\\left[\\!\\!\\begin{array}{c c c c c c c}{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {P_{i}^{1}\\times P_{i}^{2}}&{P_{i}^{1}\\times P_{i}^{3}}&{P_{i}^{1}\\times P_{i}^{4}}&{P_{i}^{2}\\times P_{i}^{3}}&{P_{i}^{2}\\times P_{i}^{4}}&{P_{i}^{3}\\times P_{i}^{4}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Or equivalently, the stacked wedge products between columns. Let $\\mathcal{P}\\!=\\!U S V^{T}$ be the thin singular value decomposition of $\\mathcal{P}$ , so that $U$ is a $3n\\!\\times\\!6$ orthonormal matrix, $S$ is a $6\\!\\times\\!6$ diagonal matrix where all diagonal entries are nonzero, and $V$ is a $6\\!\\times\\!6$ orthonormal matrix. ", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{(1)}^{n}(T_{(1)}^{n})^{T}\\!=\\!\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}}\\\\ &{\\qquad\\qquad\\quad=U(S V^{T}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}V S)U^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $V$ is orthonormal, $S V^{T}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}V S$ is still a diagonal matrix. We just need to establish the fact that three of the diagonal entries are the same. ", "page_idx": 16}, {"type": "text", "text": "For one camera, $\\mathcal{P}^{T}\\mathcal{P}$ equals ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{c c c c c c}{1}&{0}&{P_{i}^{2}\\cdot P_{i}^{4}}&{0}&{-P_{i}^{1}\\cdot P_{i}^{4}}&{0}\\\\ {1}&{1}&{P_{i}^{3}\\cdot P_{i}^{4}}&{0}&{0}&{-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{4})}&{-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{3})}\\\\ &{P_{i}^{4}\\cdot P_{i}^{4}-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{1}\\cdot P_{i}^{4})}&{1}&{0}&{-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{4})}&{-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{3})}\\\\ &&&{1}&{P_{i}^{4}\\cdot P_{i}^{4}}&{-(P_{i}^{2}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{4})}&{-(P_{i}^{2}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{4})}\\\\ &&&&{(P_{i}^{4}\\cdot P_{i}^{4})-(P_{i}^{2}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{4})}&{(P_{i}^{4}\\cdot P_{i}^{4})-(P_{i}^{3}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{3})}\\\\ {1}&{0}&{P_{i}^{2}\\cdot P_{i}^{4}}&{0}&{-P_{i}^{4}\\cdot P_{i}^{4}}&{0}\\\\ {1}&{1}&{P_{i}^{3}\\cdot P_{i}^{4}}&{0}&{0}&{-(P_{i}^{1}\\cdot P_{i}^{4})}&{0}\\\\ &&&{1}&{0}&{0}&{-(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the matrix is symmetric and we reduce redundancy by omitting the entries below the diagonal. For $n$ cameras, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{P}^{T}\\mathcal{P}=\\left[\\begin{array}{c c c c c}{n}&{0}&{\\sum_{i}P_{i}^{2}\\cdot P_{i}^{4}}&{0}&{-\\sum_{i}P_{i}^{1}\\cdot P_{i}^{4}}&{0}\\\\ &{n}&{\\sum_{i}P_{i}^{3}\\cdot P_{i}^{4}}&{0}&{0}&{-\\sum_{i}P_{i}^{1}\\cdot P_{i}^{4}}\\\\ &&{\\sum_{i}\\|P_{i}^{4}\\|^{2}-\\|P_{i}^{1}\\cdot P_{i}^{4}\\|^{2}}&{0}&{-\\sum_{i}(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{2})}&{-\\sum_{i}(P_{i}^{1}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{3})}\\\\ &&{n}&{\\sum_{i}P_{i}^{3}\\cdot P_{i}^{4}}&{-\\sum_{i}P_{i}^{2}\\cdot P_{i}^{4}}\\\\ &&&{\\sum_{i}\\|P_{i}^{4}\\|^{2}-\\|P_{i}^{2}\\cdot P_{i}^{4}\\|^{2}}&{-\\sum_{i}(P_{i}^{2}\\cdot P_{i}^{4})(P_{i}^{4}\\cdot P_{i}^{3})}\\\\ &&&{\\sum_{i}\\|P_{i}^{4}\\|^{2}-\\|P_{i}^{2}\\|^{2}-\\|P_{i}^{4}\\|^{2}-\\|P_{i}^{3}.P_{i}^{4}\\|^{2}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P_{i}^{a}\\cdot P_{i}^{b}$ means the dot product between the ath and bth column ", "page_idx": 16}, {"type": "text", "text": "The SVD of $\\mathcal{P}^{T}\\mathcal{P}$ is $\\mathcal{P}^{T}\\mathcal{P}\\!=\\!H D H^{T}$ , where $H$ is $6\\!\\times\\!6$ orthonormal matrix, $D$ is $6\\!\\times\\!6$ diagonal matrix. However, since we have an $n I_{3\\times3}$ submatrix in $\\mathcal{P}^{T}\\mathcal{P}$ , we deduce that $\\mathbf{n}$ appears as an eigenvalue 3 times for $\\mathcal{P}^{T}\\mathcal{P}$ , where we can use the determinant identity for block matrices. We check that this indeed holds by a computer calculation, generating random instances of $P_{i}$ \u2019s and calculating the eigenvalues for $\\gamma^{\\boldsymbol{T}}\\mathcal{P}$ . ", "page_idx": 16}, {"type": "text", "text": "As a result, in the thin SVD of $\\mathcal{P}$ , we have $\\mathcal{P}\\!=\\!U S V^{T}$ where $S\\!=\\!\\sqrt{D},V\\!=\\!H.$ Then in ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{(1)}^{n}(T_{(1)}^{n})^{T}\\!=\\!\\mathcal{P}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}\\mathcal{P}^{T}}\\\\ &{\\qquad\\qquad\\quad=U(S V^{T}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}V S)U^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we see that $S V^{T}\\mathcal{G}_{(1)}(\\mathcal{C}^{T}\\mathcal{C}\\otimes\\mathcal{C}^{T}\\mathcal{C})\\mathcal{G}_{(1)}^{T}V S$ is a diagonal matrix where three of the entries are the same. By the uniqueness of the eigenvalues, we see that we have a spectral decomposition of $T_{(1)}^{n}(T_{(1)}^{n})^{T}$ , so that three of the singular values of $T_{(1)}^{n}$ are equal. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof details for Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Note blockwise multiplication by a rank-1 tensor with nonzero entries preserves multilinear rank, since it is a Tucker product by invertible diagonal matrices. Therefore, without loss of generality we may assume $\\lambda_{i11}=\\lambda_{1j1}=\\lambda_{11k}=1$ for all $i,j,k\\in\\{2,3,...,n\\}$ . Below we will prove it follows $\\lambda_{i j k}=c$ if exactly one of $i,j,k$ equals 1, and $\\lambda_{i j k}=c^{2}$ if none of $i,j,k$ equal 1 and the indices are not all the same, for some constant $c\\!\\in\\!\\mathbb{R}^{*}$ . This will immediately imply the theorem, because taking $\\alpha\\!=\\!\\beta\\!=\\!(1c c...c)$ and $\\begin{array}{r}{\\gamma\\!=\\!\\left({\\frac{1}{c}}11...1\\right)}\\end{array}$ achieves $\\lambda_{i j k}\\!=\\!\\alpha_{i}\\beta_{j}\\gamma_{k}$ whenever $i,j,k$ are not all the same. ", "page_idx": 16}, {"type": "text", "text": "We consider the matrix flattenings $T_{(2)}^{n}$ and $(\\lambda\\odot_{b}T^{n})_{(2)}$ in $\\mathbb{R}^{3n\\times9n^{2}}$ of the block trifocal tensor and its scaled counterpart, with rows corresponding to the second mode of the tensors. By Theorem 1 and assumptions, the flattenings have matrix rank 4, thus all of their $5\\times5$ minors vanish. The argument consists of considering several carefully chosen $5\\times5$ submatrices of $(\\lambda\\odot_{b}T^{n})_{(2)}$ to prove the existence of a constant $c$ as above. Index the rows and columns of the flattenings by $(j,\\!r)$ and $(i q,k s)$ respectively, for $i,j,k\\in[n]$ and $q,r,s\\in$ [3], so that e.g., $((\\lambda\\odot_{b}T^{n})_{(2)})_{(j,r),(i q,k s)}\\!=\\!\\lambda_{i j k}(\\dot{T}_{i j k}^{n})_{q r s}$ . ", "page_idx": 16}, {"type": "text", "text": "Step 1: The first submatrix of $\\big(\\lambda\\odot_{b}T^{n}\\big)_{(2)}$ we consider has column labels (i1,11), (i1,21), (i1,31), (i1,12), (i2,11) and row labels (1,1), (1,2),(1,3), $\\overrightarrow{\\it{\\Omega}}$ ,1),(i,2), where $i\\in\\{2,...,n\\}$ . Explicitly, it is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c}{(T_{i1}^{n})_{111}}&{(T_{i11}^{n})_{211}}&{(T_{i11}^{n})_{311}}&{(T_{i11}^{n})_{112}}&{(T_{11i}^{n})_{111}}\\\\ {(T_{i11}^{n})_{121}}&{(T_{i11}^{n})_{221}}&{(T_{i11}^{n})_{321}}&{(T_{i11}^{n})_{122}}&{(T_{11i}^{n})_{121}}\\\\ {(T_{i11}^{n})_{131}}&{(T_{i11}^{n})_{231}}&{(T_{i11}^{n})_{331}}&{(T_{i11}^{n})_{132}}&{(T_{11i}^{n})_{131}}\\\\ {\\lambda_{i i1}(T_{i11}^{n})_{111}}&{\\lambda_{i i1}(T_{i11}^{n})_{211}}&{\\lambda_{i i1}(T_{i11}^{n})_{311}}&{\\lambda_{i i1}(T_{i i1}^{n})_{112}}&{\\lambda_{1i i}(T_{1i i}^{n})_{111}}\\\\ {\\lambda_{i i1}(T_{i i1}^{n})_{121}}&{\\lambda_{i i1}(T_{i i1}^{n})_{221}}&{\\lambda_{i i1}(T_{i i1}^{n})_{321}}&{\\lambda_{i i1}(T_{i i1}^{n})_{122}}&{\\lambda_{1i i}(T_{1i i}^{n})_{121}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which we abbreviate as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\begin{array}{c c c c c}{\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}\\\\ {\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}\\\\ {\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}&{\\;\\;*}\\\\ {\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{1i i}{*}}\\\\ {\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{i i1}{*}}&{\\lambda_{1i i}{*}}\\end{array}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with asterisk denoting the corresponding entry in $T_{(2)}^{n}$ . As a function of $\\lambda_{i i1},\\lambda_{1i i}$ , the determinant of (12) is a degree $\\leq2$ polynomial, which must be divisible by $\\lambda_{i i1}$ and $\\lambda_{i i1}\\!-\\!\\lambda_{1i i}$ (because if $\\lambda_{i i1}\\!=\\!0$ then clearly the bottom two rows of (12) are linearly independent, and if $\\lambda_{i i1}\\!-\\!\\lambda_{1i i}\\!=\\!0$ we have a submatrix of $T_{(2)}^{n}$ with the bottom two rows scaled uniformly). So the determinant of (12) is a scalar multiple of $\\lambda_{i i1}(\\dot{\\lambda_{i i1}}\\!-\\!\\lambda_{1i i})$ . Note that the multiple is a polynomial function of the cameras $P_{1}$ and $P_{i}$ . We claim that generically the multiple is nonzero; and to see this, it suffices to exhibit a single instance of (calibrated)cameraswherethedeterminantof(12)doesnotvanishidenticallyforall $\\lambda_{i i1},\\lambda_{1i i}$ duetothepolynomiality (e.g., see [53]). We check that this indeed holds by a computer calculation, generating numerical instances of $P_{1}$ and $P_{i}$ randomly. Thus the vanishing of the minor in (12) implies $\\lambda_{i i1}(\\lambda_{i i1}\\!-\\!\\lambda_{1i i})\\!=$ 0, whence $\\lambda_{i i1}\\!=\\!\\lambda_{1i i}$ since $\\lambda_{i i1}\\!\\neq\\!0$ . An analogous calculation with $\\left(\\lambda\\mathcal{\\omega}_{b}T^{n}\\right)_{(3)}$ gives $\\lambda_{i1i}\\!=\\!\\lambda_{1i i}$ . ", "page_idx": 17}, {"type": "text", "text": "Step 2: Next consider the submatrix of $(\\lambda\\odot_{b}T^{n})_{(2)}$ with column labels $(j1,11)$ , $(j1,21)$ , $(j1,31)$ , $(j1,12)$ , (1j,11) and row labels (1,1), (1,2), (1,3), (i,1), (i,2), where $i,j\\in\\{2,...,n\\}$ are distinct. It looks like ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c}{\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{1i j}\\ast}\\\\ {\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{1i j}\\ast}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with asterisks denoting entries of $T_{(2)}^{n}$ . Similarly to the previous case, the determinant of (13) must be a scalar multiple of $\\lambda_{j i1}(\\lambda_{j i1}-\\dot{\\lambda}_{1i j})$ where the scale depends polynomially on $P_{1},P_{i},P_{j}$ . By a computer computation, we find that the scale is nonzero for random instances of cameras (alternatively, note the polynomial system in step 1 is a special case of the present one). It the scale is generically nonzero, hence $\\lambda_{j i1}\\!=\\!\\lambda_{1i j}$ . An analogous calculation with $\\left(\\lambda\\odot_{b}T^{n}\\right)_{\\left(3\\right)}$ gives $\\lambda_{i1j}\\!=\\!\\lambda_{1i j}$ . ", "page_idx": 17}, {"type": "text", "text": "Step 3: Consider the submatrix of $(\\lambda\\odot_{b}T^{n})_{(2)}$ with column labels $(j1,\\!11)$ , $(j1,\\!21)$ , $(j1,31)$ , $(j1,12)$ , $(1k,11)$ and row labels (1,1), (1,2), (1,3), (i,1), (i,2), for $i,j,k\\in\\{2,...,n\\}$ distinct. It looks like ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[{\\begin{array}{c c c c c}{*}&{*}&{*}&{*}&{*}\\\\ {*}&{*}&{*}&{*}&{*}\\\\ {*}&{*}&{*}&{*}&{*}\\\\ {\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{1i k}*}\\\\ {\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{j i1}*}&{\\lambda_{1i k}*}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The determinant of (14) is a scalar multiple of $\\lambda_{j i1}(\\lambda_{j i1}\\!-\\!\\lambda_{1i k})$ . By a direct computer computation as before, it is a nonzero multiple generically (alternatively, note the polynomial system in step 1 is a special of the present one). We deduce $\\lambda_{j i1}\\!=\\!\\lambda_{1i k}$ . An analogous calculation with $\\left(\\lambda\\mathcal{\\omega}_{b}T^{n}\\right)_{(3)}$ gives $\\lambda_{i1j}\\!=\\!\\lambda_{1k j}$ . ", "page_idx": 17}, {"type": "text", "text": "In particular, combining with step 2 it follows $\\lambda_{1i j}\\!=\\!\\lambda_{1j i}$ , because $\\lambda_{1i j}\\!=\\!\\lambda_{k1j}\\!=\\!\\lambda_{1k j}\\!=\\!\\lambda_{i k1}\\!=\\!\\lambda_{1k i}\\!=$ $\\lambda_{j1i}=\\lambda_{1j i}$ . From this, step 1 and step 2, we have that the $\\lambda$ -scale does not depend on the ordering of its indices, provided there is a 1 among the indices. ", "page_idx": 17}, {"type": "text", "text": "Step 4: Consider the submatrix of $(\\lambda\\odot_{b}T^{n})_{(2)}$ with column labels $(j1,\\!11)$ , $(j1,\\!21)$ , $(j1,31)$ , $(j1,12)$ , (1i,11) and row labels (1,1), (1,2), (1,3), (i,1), $(i,2)$ , for $i,j\\in\\{2,...,n\\}$ distinct. It looks like ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c}{\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\ast}&{\\ast}&{\\ast}&{\\ast}&{\\ast}\\\\ {\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{1i i}\\ast}\\\\ {\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{j i1}\\ast}&{\\lambda_{1i i}\\ast}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The determinant of (15) is a scalar multiple of $\\lambda_{j i1}(\\lambda_{j i1}-\\lambda_{1i i})$ . By a direct computer computation, it is a nonzero multiple generically (alternatively, note the polynomial system in step 1 is a special case of the present one). We deduce $\\lambda_{j i1}\\!=\\!\\lambda_{1i i}$ . ", "page_idx": 18}, {"type": "text", "text": "Putting together what we know so far, all $\\lambda$ -scales with a single 1-index agree. Indeed, this follows from $\\lambda_{1i i}=\\lambda_{j i1}=\\lambda_{i j1}=\\lambda_{1j j}$ so all $\\lambda$ -scales with a single 1-index and two repeated indices agree, combined with $\\lambda_{j i1}\\!=\\!\\lambda_{1i i}$ and the last sentence of step 3. Let $c\\!\\in\\!\\mathbb{R}^{*}$ denote this common scale. ", "page_idx": 18}, {"type": "text", "text": "Step 5: Consider the submatrix of $\\big(\\lambda\\odot_{b}T^{n}\\big)_{(2)}$ with column labels (1i,11), (1i,21), (1i,31), (1i,12), $(i j,11)$ and row labels (1,1), (1,2), (1,3), (i,1), $(i,2)$ , for $i,j\\in\\{2,...,n\\}$ distinct. It looks like ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\begin{array}{c c c c c}{{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{i i j}*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{i i j}*}}\\end{array}\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a function of $c$ and $\\lambda_{i i j}$ , the determinant of (16) is a scalar multiple of $c(c^{2}-\\lambda_{i i j})$ (the second factor is present because it corresponds to scaling the bottom two rows and rightmost column of a $5\\times5$ submatrix of $T_{(2)}$ each by $c$ , which preserves rank deficiency). By a direct computer computation, we find that the scale is nonzero for a random instance of $P_{1},P_{i},P_{j}$ , therefore it is nonzero generically. It follows $c^{2}\\,{=}\\,\\lambda_{i i j}$ . An analogous calculation with $\\left(\\lambda\\odot_{b}T^{n}\\right)_{\\left(3\\right)}$ gives $c^{2}\\!=\\!\\lambda_{i j i}$ . ", "page_idx": 18}, {"type": "text", "text": "Step 6: Consider the submatrix of $\\big(\\lambda\\odot_{b}T^{n}\\big)_{(2)}$ with column labels (1i,11), (1i,21), (1i,31), (1i,12), $(j i,11)$ and row labels (1,1), (1,2), (1,3), (i,1), (i,2), for $i,j\\in\\{2,...,n\\}$ distinct. It looks like ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\begin{array}{c c c c c}{{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{j i i}*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{j i i}*}}\\end{array}\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly to the previous step, the determinant of (17) must be a scalar multiple of $c(c^{2}-\\lambda_{j i i})$ . By a direct computer computation, it is a nonzero multiple generically. We deduce $c^{2}\\!=\\!\\lambda_{j i i}$ . ", "page_idx": 18}, {"type": "text", "text": "Step 7: Consider the submatrix of $\\big(\\lambda\\odot_{b}T^{n}\\big)_{(2)}$ with column labels $(1i,11)$ , $(1i,21)$ , (1i,31), (1i,12), $(i k,11)$ and row labels (1,1), (1,2), (1,3), $(j,1)$ , $(j,2)$ , for $i,j,k\\in\\{2,...,n\\}$ distinct. It looks like ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\begin{array}{c c c c c}{{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{*}}&{{*}}&{{*}}&{{*}}&{{c*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{i j k}*}}\\\\ {{c*}}&{{c*}}&{{c*}}&{{c*}}&{{\\lambda_{i j k}*}}\\end{array}\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The determinant of (18) is a scalar multiple of $c(c^{2}-\\lambda_{i j k})$ . By a direct computer computation, it is a nonzero multiple generically (alternatively, note the polynomial system in step 5 is a special case of the present case). We deduce $c^{2}\\!=\\!\\lambda_{i j k}$ . ", "page_idx": 18}, {"type": "text", "text": "At this point, by steps 5,6,7 we have that all $\\lambda$ -scales with no 1-indices and not all indices the same must equal $c^{2}$ . Combined with the second paragraph of step 4, this shows $c$ satisfies the property announced at the start of the proof. Therefore the proof is complete. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.5 Implementation details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.5.1 Estimating trifocal tensors from three fundamental matrices ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given three cameras $P_{1},P_{2},P_{3}$ and the corresponding fundamental matrices $F_{21},F_{31},F_{32}$ , we can calculate the trifocal tensor $T_{i j k}$ using the following procedure detailed in [1]. Specifically, from ", "page_idx": 18}, {"type": "text", "text": "$F_{21}$ calculate an initial estimate of the cameras $P_{1}^{\\prime},P_{2}^{\\prime}$ . Then, $P_{3}^{T}F_{32}P_{2}^{\\prime}$ and $P_{3}^{T}F_{31}P_{1}^{\\prime}$ should be skew-symmetric matrices. This gives 20 linear equations in terms of the entries in $P_{3}$ , which can be used to solve for the trifocal tensor. Note that there are no geometrical constraints when calculating $P_{3}$ , and there will be no guarantee of the quality of the estimation. ", "page_idx": 19}, {"type": "text", "text": "A.5.2 Higher-order regularized subspace-constrained Tyler\u2019s estimator (HOrSTE) for EPFL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We describe the robust variant of SVD that we used for the EPFL experiments in Section 4. Numerically, it performs more stably and accurately than HOSVD-HT, yet it is an iterative procedure and each iteration requires an SVD of the $3n\\!\\times\\!9n^{2}$ flattening. This becomes computationally expensive when $n$ becomes large and the number of iterations are also large. However, since the number of cameras for the EPFL dataset are below 20 cameras, the computational overhead is not too great. ", "page_idx": 19}, {"type": "text", "text": "In HOSVD, a low dimensional subspace is estimated using the singular value decomposition and taking the $R_{n}$ leading left singular vectors for each mode- ${\\cdot n}$ flattening. The Tyler\u2019s M Estimator (TME) [40] is a robust covariance estimator of a $D$ dimensional dataset $\\overline{{\\{x_{i}\\}_{i=1}^{N}}}\\overline{{\\subset}}\\mathbb{R}^{D}$ . It minimizes the objective function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Sigma\\in\\mathbb{R}^{D\\times D}}\\frac{D}{N}{\\sum_{i=1}^{N}}\\mathrm{log}(x_{i}^{T}\\Sigma^{-1}x_{i})\\!+\\!\\mathrm{logdet}(\\Sigma)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "such that $\\Sigma$ is positive definite and has trace 1. The TME estimator can be applied to robustly find an $R_{n}$ dimensional subspace by taking the $R_{n}$ leading eigenvectors of the covariance matrix of TME. To compute the TME, [40] proposes an iterative algorithm, where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Sigma^{(k)}\\!=\\!\\sum_{i=1}^{N}\\!\\frac{x_{i}x_{i}^{T}}{x_{i}^{T}(\\Sigma^{(k-1)})^{-1})x_{i}}/t r(\\sum_{i=1}^{N}\\!\\frac{x_{i}x_{i}^{T}}{x_{i}^{T}(\\Sigma^{(k-1)})^{-1})x_{i}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "TME doesn\u2019t exist when $D>N$ , but a regularized TME has been proposed by [54]. The iterations become ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Sigma^{(k)}\\!=\\!\\frac{1}{1\\!+\\!\\alpha}\\frac{D}{N}\\!\\sum_{i=1}^{N}\\!\\frac{x_{i}x_{i}^{T}}{x_{i}^{T}(\\Sigma^{(k-1)})^{-1})x_{i}}\\!+\\!\\frac{\\alpha}{1\\!+\\!\\alpha}I\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\alpha$ is a regularization parameter, and $I$ is the $D\\times D$ identity matrix. TME does not assume the dimension of the subspace $d$ is predetermined. In the case when $d$ is prespecified, [41] improves the TME estimator by incorporating the information into the algorithm and develops the subspace-constrained Tyler\u2019s Estimator (STE). For each iteration, STE equalizes the trailing $D\\!-\\!d$ eigenvalues of the estimated covariance matrix and uses a parameter $0\\!<\\!\\gamma\\!<\\!1$ to shrink the eigenvalues. The iterative procedure for STE is summarized into 3 steps: ", "page_idx": 19}, {"type": "text", "text": "1. Calculate the unnormalized TME, $\\begin{array}{r}{Z^{(k)}\\!=\\!\\sum_{i=1}^{N}\\!\\big(x_{i}x_{i}^{T}/x_{i}^{T}(\\Sigma^{(k-1)})^{-1}\\big)x_{i}\\big).}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "2. Perform the eigendecomposition of $Z^{(k)}=U^{(k)}S^{(k)}(U^{(k)})^{T}$ , and set the trailing $D-d$ eigenvalues as $\\bar{\\gamma}\\sum_{i=d+1}^{D}\\bar{\\sigma_{i}}/(D\\!-\\!d)$ .   \n3. Calculate $\\Sigma^{(k)}~=~U^{(k)}S^{(k)}(U^{(k)})^{T}/t r(U^{(k)}S^{(k)}(U^{(k)})^{T})$ , which is the normalized covariance matrix. Repeat steps 1-3 until convergence. ", "page_idx": 19}, {"type": "text", "text": "Similar to the regularized TME, STE can also be regularized to succeed in situations where there are fewer inliers, and can improve the robustness of the algorithm. The regularized STE differs from STE in only the first step, which is replaced by ", "page_idx": 19}, {"type": "text", "text": "We apply the regularized STE to the HOSVD framework, and call the resulting projection the higher-order regularized STE (HOrSTE). It is performed via the following steps: ", "page_idx": 19}, {"type": "text", "text": "1. For each $n$ , calculate the factor matrices $A_{n}$ as the $R_{n}$ leading left singular vectors from regularized STE applied to $T_{(n)}$ .   \n2. Set the core tensor $\\mathcal{G}$ as ${\\mathcal{G}}\\!=\\!T\\!\\times\\!_{1}A_{1}^{T}\\!\\times\\!_{2}A_{2}^{T}\\!\\cdots\\!\\times\\!_{N}A_{N}^{T}$ . ", "page_idx": 19}, {"type": "text", "text": "A.6 Additional numerical results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we include comprehensive results for the rotation and translation errors for the EPFL and Photo Tourism experiments. Table 1 and 2 contains all results for EPFL datasets. Table 3 contains the location estimation errors for Photo Tourism. Table 4 contains the rotation estimation errors for Photo Tourism. In Table 4, we only report the rotation errors for LUD for all the methods that we compared against, as they are mostly the same since they used the same rotation averaging method. ", "page_idx": 20}, {"type": "text", "text": "Table 1: EPFL synchronization errors. $\\bar{e}_{r}$ is the mean rotation error in degrees, $\\hat{e}_{r}$ is the median rotation error in degrees. $\\bar{e}_{t}$ is the mean location error, $\\hat{e}_{t}$ is the median location error. NRFM(LUD) is NRFM initialized with LUD and NRFM is randomly initialized. BATA(MLPS) is BATA initialized with MPLS. ", "page_idx": 20}, {"type": "table", "img_path": "DT7n4F2bbP/tmp/aafbaa9e14c4cd49dd37588b0d8438dd837069ad9d7cbb87616e012f94e6a67e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "DT7n4F2bbP/tmp/06da990fe18af17086edf693ead0502c16c655116accad46ed1af665979d7232.jpg", "table_caption": ["Table 2: EPFL synchronization errors. $\\bar{e}_{r}$ is the mean rotation error in degrees, $\\hat{e}_{r}$ is the median rotation error in degrees. BATA(MLPS) is BATA initialized with MPLS. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 3: Translation errors for Photo Tourism. $n$ is the size after downsampling. Est. $\\%$ is the ratio of observed blocks over total number of blocks. $\\bar{e}_{t}$ is the mean location error, $\\hat{e}_{t}$ is the median location error. NRFM(L)isNRFMinitializedwithLUDandNRFM(R)israndomlyinitialized. ThenotationPRmeans that the dataset was further downsampled to match the two view methods. BATA is BATA initialized with MPLS. We were not able to get results for our subsampled dataset for Piccadilly with MPLS. ", "page_idx": 20}, {"type": "table", "img_path": "DT7n4F2bbP/tmp/78926467b7f22cd8cc3cdaab6bc2d58d46d1956817224ad619026db65b4de365.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 4: Rotation errors for Photo Tourism. $N$ is the total number of cameras. $n$ is the size after down sampling. Est. $\\%$ is the ratio of observed blocks over total number of blocks. $\\bar{e}_{r}$ is the mean rotation error, $\\hat{e}_{r}$ is the median rotation error. The notation PR means that the dataset was further down sampled to match the two view methods. We were not able to get results for our subsampled dataset for Piccadilly with MPLS. ", "page_idx": 21}, {"type": "table", "img_path": "DT7n4F2bbP/tmp/ec6da2e29f177909ee98788885560ccc6ed354e08902cbddeeee30c313c2b681.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": ". Claims ", "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please check Section 3 and 4 for the main claims of our paper and the experimental results. An accurate summary of the contributions is found at the end of the introduction section, and also conveyed in the abstract. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please check our conclusion in Section 5. The last paragraph describes the limitations of the work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please see the proofs in Appendix A.3, A.2, A.4 and statements in Section 2.   \nThe statements are precise, and accompanied by complete proofs. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we mention the numerical details and processing of our algorithm in section Section 4 for each experiment. We also include code in the supplementary material with instructions on how to run them. Our code depends on other publicly available code. Instructions can be found in supplementary material. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please feel free to check our code in the supplementary material and the README.MD file for instructions. A public version will be available in the future. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see our experiment details in Section 4. In particular, we included the way we chose hyperparameter and specific numbers when applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Though we didn\u2019t set a random seed for some experiments, there were insignificant differences between different runs of the method. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see our experiment details in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We only used open source code and have complied to the code of ethics. Please check code in supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our method is focused on a general framework for tensor based structure from motion and we run experiments on open source datasets. No negative social impacts are related to our work. Having a tensor based synchronization method opens up more research directions and could help apply structure from motion to develop applications where point correspondences are scarce. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper doesn\u2019t pose such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We cited the authors whose code in our code, and respected all relevant licenses. We include links to code in our submitted code as well. Please refer to the code and the experiment details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide code for our methods and instructions for running the methods in the supplementary material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 AccordingtotheNeurIPSCodeofEthics, workersinvolvedindatacollection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper doesn\u2019t involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]