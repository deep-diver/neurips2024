{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the LoRA method, a core technique that the current paper builds upon and improves, making it a highly important reference."}, {"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "publication_date": "2019-06-09", "reason": "This paper introduces adapter modules, another key concept in parameter-efficient fine-tuning which the current paper uses and improves, making it another critical reference."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-00", "reason": "This paper introduces BERT, a foundational large language model used extensively in the experiments and comparisons within the current paper."}, {"fullname_first_author": "Alex Wang", "paper_title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "publication_date": "2018-11-00", "reason": "The GLUE benchmark is a central evaluation dataset used in the current paper's experiments, establishing this paper's importance as a foundational resource."}, {"fullname_first_author": "Alex Krizhevsky", "paper_title": "CIFAR-10 (Canadian Institute for Advanced Research)", "publication_date": "2010-00-00", "reason": "CIFAR-10 is a primary image dataset used for experimental evaluation in the current work, highlighting this paper's significance as a key dataset resource."}]}