{"references": [{"fullname_first_author": "Twan van Laarhoven", "paper_title": "L2 regularization versus batch and weight normalization", "publication_date": "2017", "reason": "This paper provides a foundational comparison of different regularization techniques, which is directly relevant to the core topic of the main paper."}, {"fullname_first_author": "Chiyuan Zhang", "paper_title": "Understanding deep learning (still) requires rethinking generalization", "publication_date": "2021-02", "reason": "This paper offers insights into generalization in deep learning, a central theme which this paper builds upon and further investigates."}, {"fullname_first_author": "Guodong Zhang", "paper_title": "Three mechanisms of weight decay regularization", "publication_date": "2019", "reason": "This paper explores different interpretations of weight decay, an important concept analyzed and expanded upon in the main paper."}, {"fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "publication_date": "2019", "reason": "This paper introduces decoupled weight decay, a technique that is empirically investigated and discussed in the main paper in the context of attention layers."}, {"fullname_first_author": "David J C Mackay", "paper_title": "Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks", "publication_date": "1995", "reason": "This paper provides the original Bayesian perspective on weight decay, which forms a basis for the theoretical framework of the main paper."}]}