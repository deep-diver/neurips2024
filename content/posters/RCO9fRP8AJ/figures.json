[{"figure_path": "RCO9fRP8AJ/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Traditional methods require paired RGB-D data for training and use single-modality point clouds as input during inference. Right: ImOV3D involves using a vast amount of 2D images to generate pseudo point clouds during the training phase, which are then rendered back into images. In the inference phase, with only point clouds as input, we still construct a pseudo-multimodal representation to enhance detection performance.", "description": "This figure compares traditional open-vocabulary 3D object detection methods with the proposed ImOV3D method. Traditional methods rely on paired RGB-D data for training and use single-modality point clouds during inference.  In contrast, ImOV3D uses a large number of 2D images to generate pseudo point clouds during training.  These pseudo point clouds are then rendered into images to help bridge the gap between 2D image data and 3D point cloud data. During inference, even with only point clouds as input, ImOV3D utilizes pseudo-multimodal representations to enhance detection performance.", "section": "1 Introduction"}, {"figure_path": "RCO9fRP8AJ/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of ImOV3D: Our model takes 2D images as input and puts them into the Pseudo 3D Annotation Generator to produce pseudo annotations. These 2D images are also fed into the Point Cloud Lifting Module to generate pseudo point clouds. Subsequently, using the Point Cloud Renderer, these pseudo point clouds are rendered into pseudo images, which then get processed by a 2D open vocabulary detector to detect 2D proposals and transfer the 2D semantic information to 3D space. Armed with pseudo point clouds, annotations, and pseudo images data, we proceed to train a multimodal 3D detector.", "description": "This figure illustrates the overall architecture of the ImOV3D model.  It starts with 2D images as input, which are used in two parallel paths. The first path uses a Point Cloud Lifting Module and a Pseudo 3D Annotation Generator to create pseudo 3D point clouds and annotations. The second path uses a Point Cloud Renderer to generate pseudo images from these pseudo point clouds. Both pseudo 3D data and pseudo images are then fed into a Pseudo-Multimodal Detector, which uses a 3D backbone and a 2D open vocabulary detector, along with CLIP for text prompts. The inference phase uses point clouds as input and a pseudo-multimodal representation to generate detections.", "section": "3 Method"}, {"figure_path": "RCO9fRP8AJ/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of 3D Data Revision Module: (a) The rotation correction module involves processing an RGB image through a Normal Estimator to generate a normal map. This map then helps extract a horizontal surface mask for identifying horizontal point clouds, from which normal vectors Npred are obtained. These vectors are aligned with the Z-axis to compute the rotation matrix R. (b) In the 3D box filtering module, prompts related to object dimensions are first provided to GPT-4 to determine the mean size for each category. This mean size is then used to filter out boxes that do not meet the threshold criteria.", "description": "This figure illustrates the 3D data revision module, which consists of two parts: rotation correction and 3D box filtering. The rotation correction part uses a normal estimator to generate a normal map from an RGB image to identify horizontal point clouds, aligns the normal vector with the Z-axis to calculate the rotation matrix R. The 3D box filtering part uses GPT-4 to determine the average size of each object category and filters out boxes that do not meet the threshold criteria.", "section": "3.2 Point Cloud Lifting Module"}, {"figure_path": "RCO9fRP8AJ/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results include (a) 2D RGB images, (b) 2D depth maps with 2D OVDetector annotations, and (c) pseudo images with annotations from a fine-tuned 2D detector.", "description": "This figure shows qualitative results of the proposed method. It presents three types of images for two different scenes: 2D RGB images, depth maps obtained from point clouds, and pseudo images generated from point clouds.  The pseudo images are processed by a 2D open vocabulary detector to generate bounding boxes around the detected objects. This visualization helps demonstrate how the method leverages 2D information (images) to improve 3D object detection in the case of limited 3D data.", "section": "5.2 Ablation Study of Depth and Pseudo Images"}, {"figure_path": "RCO9fRP8AJ/figures/figures_8_2.jpg", "caption": "Figure 5: (a) and (b) show data volume ablation results. (c) illustrates transferability ablation results.", "description": "This figure shows the ablation study on the data volume and transferability of the proposed ImOV3D model.  Subfigures (a) and (b) demonstrate the impact of reducing the amount of real 3D training data (from 100% to 10%) on the model's performance (measured by mAP@0.25) on SUNRGBD and ScanNet datasets, respectively. Subfigure (c) shows the transferability of the model, testing its performance when training on one dataset and testing on the other. The results highlight the robustness of ImOV3D even with limited 3D data and its strong transferability across datasets.", "section": "5 Ablation Study"}, {"figure_path": "RCO9fRP8AJ/figures/figures_9_1.jpg", "caption": "Figure 6: (a) KDE plots of volume ratios (Ratioy) for top 10 classes in SUNRGBD validation set. (b) Visualization comparison of OV-3DET with ours in SUNRGBD.", "description": "Figure 6(a) shows kernel density estimation (KDE) plots illustrating the distribution of volume ratios for the top 10 classes in the SUNRGBD validation set.  The volume ratio quantifies the precision of 3D bounding box size predictions by comparing predicted volumes to ground truth volumes. Figure 6(b) provides a visual comparison of the object detection results of OV-3DET and ImOV3D on the SUNRGBD dataset.  It highlights the improved accuracy of ImOV3D's 3D bounding box predictions.", "section": "5 Ablation Study"}, {"figure_path": "RCO9fRP8AJ/figures/figures_15_1.jpg", "caption": "Figure 7: Illustration of (A) Depth Images, (B) Rendered Point Cloud Images after Partial-View Removal, and (C) Ground Truth 2D Images", "description": "This figure shows the results of the partial-view removal process. (A) displays the depth images generated using a monocular depth estimation model. (B) presents the rendered point cloud images obtained after removing overlapping points visible from multiple viewpoints. This step enhances the robustness of the model by simulating occlusions, improving performance across various scenarios. (C) shows the ground truth 2D images corresponding to the point cloud data, enriching the 3D data by providing additional texture information.", "section": "3.2 Point Cloud Lifting Module"}, {"figure_path": "RCO9fRP8AJ/figures/figures_17_1.jpg", "caption": "Figure 6: (a) KDE plots of volume ratios (Ratioy) for top 10 classes in SUNRGBD validation set. (b) Visualization comparison of OV-3DET with ours in SUNRGBD.", "description": "Figure 6(a) shows the Kernel Density Estimation (KDE) plots of volume ratios for the top 10 classes in the SUNRGBD validation set.  The volume ratio, Ratioy, indicates how well the predicted 3D bounding box volume matches the ground truth volume. A value close to 1 suggests high accuracy.  Figure 6(b) provides a visual comparison of the 3D object detection results between OV-3DET and the proposed ImOV3D method on the SUNRGBD dataset.  This visual comparison highlights the improved detection performance achieved by ImOV3D.", "section": "Ablation Study of 3D Data Revision"}, {"figure_path": "RCO9fRP8AJ/figures/figures_17_2.jpg", "caption": "Figure 9: Results of Using the Meansize Database in the 3D Box Filtering Module: This figure shows the outcome of applying the meansize database constructed by GPT-4 [1] in the 3D box filtering module to filter B3Dpseudo with a threshold T = 0.1. The figure presents the number of 3D box of the top 50 classes with the most instances. The results highlight the efficiency and accuracy of the GPT-constructed meansize in improving the performance of the filtering process.", "description": "This figure displays the results of using a mean size database (generated by GPT-4) within a 3D box filtering module.  It shows the number of 3D boxes (before and after filtering) for the top 50 most frequent object classes in a dataset. The comparison highlights the effectiveness of using the GPT-4 generated mean sizes in improving the accuracy of the 3D bounding box filtering process.", "section": "5.1 Ablation Study of 3D Data Revision"}, {"figure_path": "RCO9fRP8AJ/figures/figures_21_1.jpg", "caption": "Figure 10: During the pretraining stage, the visual comparison tested on the SUNRGBD[43] dataset includes four columns: the first column shows the ground truth images, the second column displays the ground truth point clouds, the third column presents the detection results by OV-3DET [30], and the fourth column reveals the detection outcomes by ImOV3D.", "description": "This figure compares the results of object detection between ImOV3D and OV-3DET on the SUNRGBD dataset.  It shows four columns for each scene: ground truth images, ground truth point clouds, OV-3DET detections, and ImOV3D detections. The visualization highlights the differences in the quality and accuracy of object detection between the two methods during the pre-training phase, where only pseudo-3D data is used to train ImOV3D.", "section": "4.2.1 Pretraining \u2192 3D Training Data Free OV-3Det"}, {"figure_path": "RCO9fRP8AJ/figures/figures_22_1.jpg", "caption": "Figure 10: During the pretraining stage, the visual comparison tested on the SUNRGBD[43] dataset includes four columns: the first column shows the ground truth images, the second column displays the ground truth point clouds, the third column presents the detection results by OV-3DET [30], and the fourth column reveals the detection outcomes by ImOV3D.", "description": "This figure compares the performance of ImOV3D and OV-3DET during the pretraining stage on the SUNRGBD dataset.  It shows ground truth images and point clouds alongside the 3D bounding boxes generated by each method. This allows for a visual comparison of the accuracy of object detection.", "section": "4.2.1 Pretraining \u2192 3D Training Data Free OV-3Det"}]