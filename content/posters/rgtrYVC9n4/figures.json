[{"figure_path": "rgtrYVC9n4/figures/figures_1_1.jpg", "caption": "Figure 1: Sparse ratios by our method (left) and OWL (middle), WikiText-2 perplexity results (right).", "description": "This figure compares the layer-wise sparsity ratios achieved by the proposed DSA method and the OWL method, along with a uniform sparsity baseline. The left and middle panels show the sparsity ratios assigned to each layer of a LLaMA-V1-7B model by DSA and OWL, respectively. The right panel shows the perplexity on WikiText-2 dataset for LLaMA-V1-7B model under the different sparsity methods. It visually demonstrates how the proposed method dynamically allocates sparsity based on layer importance, leading to better performance compared to the uniform sparsity and OWL approaches.", "section": "1 Introduction"}, {"figure_path": "rgtrYVC9n4/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our DSA framework. We search for allocation functions to map element-wise scores to sparse ratios. We build pre-process, reduction, transform, and post-process operations as the search space for the allocation function, and then we perform evolutionary search.", "description": "This figure illustrates the DSA (Discovering Sparsity Allocation) framework for finding optimal sparsity allocation functions in LLMs. It shows a workflow that begins with element-wise scores, proceeds through four stages of operations (pre-processing, reduction, transformation, and post-processing), and ultimately arrives at sparsity ratios. The evolutionary search process, depicted as a graph of verification performance against search iterations, is also shown.  The operations within each stage are depicted with boxes indicating their role and contribution to the overall process.", "section": "4 Allocation Function Search Space"}, {"figure_path": "rgtrYVC9n4/figures/figures_9_1.jpg", "caption": "Figure 3: Comparison of search curves of evolution search and random search in our sparse allocation function discovery for LLaMA-1 7B on WikiText-2. Evolutionary search converges faster than random search and can achieve potential results with better perplexity (\u2193) performance.", "description": "This figure compares the performance of two search algorithms, evolutionary search and random search, in finding optimal sparsity allocation functions for the LLaMA-1 7B language model on the WikiText-2 dataset.  The x-axis represents the number of generations in the search, and the y-axis shows the perplexity, a measure of the model's performance (lower is better). The plot shows that evolutionary search converges to a lower perplexity significantly faster than random search, indicating its greater efficiency in finding high-performing allocation functions.", "section": "7.4 Analysis"}]