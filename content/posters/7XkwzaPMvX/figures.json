[{"figure_path": "7XkwzaPMvX/figures/figures_6_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure shows a comparison of fairness metrics (FPRD and FNRD) for human decisions made under three types of AI explanations: adversarially manipulated explanations, SHAP explanations, and LIME explanations.  The results across four different decision-making tasks are displayed. Error bars show the 95% confidence intervals, and significance levels are indicated by asterisks. Lower values of FPRD and FNRD represent greater fairness in human decisions.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_6_2.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure shows a comparison of fairness metrics (False Positive Rate Difference - FPRD and False Negative Rate Difference - FNRD) for human decisions made using adversarially manipulated explanations versus those made using SHAP or LIME explanations across four different tasks. The results indicate that adversarially manipulated explanations led to significantly less fair decisions than SHAP or LIME explanations in most tasks.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_7_1.jpg", "caption": "Figure 3: Comparing the average accuracy, overreliance, and the underreliance of human decision outcomes under the benignly manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.01, 0.05, and 0.1 respectively.", "description": "This figure displays the results of a study comparing human decision-making performance under three types of AI explanations: benignly manipulated explanations, SHAP explanations, and LIME explanations.  The figure shows the accuracy, overreliance (trusting AI too much), and underreliance (not trusting AI enough) for each explanation method across four different tasks. Error bars indicate the 95% confidence intervals, and significance levels (***, **, *) show the statistical significance of differences between the methods.", "section": "Evaluation II: Manipulating AI Explanations for Benign Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_8_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure displays the results of a study comparing the fairness of human decisions made when using different types of AI explanations.  The x-axis represents the four decision-making tasks. The y-axis shows the False Positive Rate Difference (FPRD) and False Negative Rate Difference (FNRD).  The bars show the average FPRD and FNRD for decisions made using adversarially manipulated explanations, SHAP explanations, and LIME explanations. Error bars indicate the 95% confidence intervals. Asterisks indicate the statistical significance of the differences.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_12_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure displays the fairness of human decisions (measured by FPRD and FNRD) when using adversarially manipulated explanations, SHAP explanations, and LIME explanations.  It shows that adversarially manipulated explanations lead to significantly less fair decisions compared to SHAP or LIME explanations across multiple tasks (Census, Recidivism, Bias, Toxicity). Error bars indicate 95% confidence intervals and asterisks represent statistical significance levels.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_13_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure displays the False Positive Rate Difference (FPRD) and False Negative Rate Difference (FNRD) for human decisions made under three different explanation types: adversarially manipulated explanations, SHAP explanations, and LIME explanations.  Error bars show the 95% confidence interval for each mean value.  Asterisks indicate the statistical significance of differences between the manipulation methods; more asterisks mean higher significance.  Values closer to zero indicate fairer decisions.", "section": "5 Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_14_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure compares the fairness of human decisions when using adversarially manipulated explanations against those using SHAP and LIME explanations across four tasks.  Fairness is measured using False Positive Rate Difference (FPRD) and False Negative Rate Difference (FNRD).  Lower values indicate greater fairness. The figure shows that adversarially manipulated explanations lead to significantly less fair decisions compared to both SHAP and LIME.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_14_2.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure displays the results of a comparison of fairness metrics (False Positive Rate Difference and False Negative Rate Difference) across three types of AI explanations: adversarially manipulated, SHAP, and LIME.  The results show that adversarially manipulated explanations lead to less fair decisions compared to SHAP and LIME explanations, indicating the potential for malicious manipulation of AI explanations to create biased outcomes. Error bars indicate 95% confidence intervals, and asterisks denote statistical significance levels.", "section": "5 Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_15_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure compares the fairness of human decisions using three types of AI explanations: adversarially manipulated explanations, SHAP explanations, and LIME explanations.  It shows that adversarially manipulated explanations led to significantly less fair decisions (higher FPRD and FNRD) than SHAP or LIME explanations. The error bars represent the 95% confidence intervals, and asterisks indicate statistical significance levels.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_15_2.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure compares the fairness of human decisions (measured by False Positive Rate Difference (FPRD) and False Negative Rate Difference (FNRD)) when using adversarially manipulated explanations, SHAP explanations, and LIME explanations.  Statistical significance is shown using asterisks. The closer the FPRD and FNRD values are to zero, the fairer the decisions.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_16_1.jpg", "caption": "Figure B.5: The visual comparisons of adversarially manipulated, LIME, and SHAP explanations for the Toxicity Detection task.", "description": "This figure shows examples of adversarially manipulated explanations alongside LIME and SHAP explanations for the Toxicity Detection task.  It visually demonstrates how the adversarially manipulated explanations differ from the original methods by highlighting certain words to potentially influence the human's decision.  The top panel shows an example where the AI model correctly predicted a non-toxic sentence (label 0), and the manipulated explanation subtly nudges a human toward that same conclusion. The bottom panel shows a case where the AI model correctly predicts a toxic sentence (label 1). However, the manipulated explanation tries to influence a human to judge the sentence as non-toxic.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_16_2.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure compares the fairness of human decisions using three types of explanations: adversarially manipulated explanations, SHAP explanations, and LIME explanations.  Fairness is measured by FPRD (False Positive Rate Difference) and FNRD (False Negative Rate Difference).  The results show that adversarially manipulated explanations lead to significantly less fair decisions than SHAP and LIME explanations across most tasks. Error bars represent 95% confidence intervals, and asterisks denote statistical significance levels.", "section": "5 Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_17_1.jpg", "caption": "Figure 3: Comparing the average accuracy, overreliance, and the underreliance of human decision outcomes under the benignly manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01 respectively.", "description": "This figure compares the performance of human decision-making under three different explanation types: benignly manipulated explanations, SHAP explanations, and LIME explanations.  The results are displayed in three subfigures: accuracy, overreliance, and under-reliance.  Statistically significant differences are indicated by asterisks (*, **, ***), showing the impact of explanation manipulation on human decision accuracy and reliance on AI model predictions.", "section": "Evaluation II: Manipulating AI Explanations for Benign Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_18_1.jpg", "caption": "Figure 1: Comparing average FPRD and FNRD of the human decision outcomes under the adversarially manipulated explanation, SHAP explanation, or LIME explanation. Error bars represent the 95% confidence intervals of the mean values. ***, **, and * denote significance levels of 0.1, 0.05, and 0.01, respectively. For both FPRD and FNRD, a value closer to zero indicates that the human decisions are more fair.", "description": "This figure displays the fairness levels of human decisions using two metrics (False Positive Rate Difference and False Negative Rate Difference) across four different tasks.  The fairness is compared when using adversarially manipulated explanations versus standard SHAP and LIME explanations.  The results show that adversarially manipulated explanations lead to less fair human decisions.", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_18_2.jpg", "caption": "Figure B.4: The visual comparisons of adversarially manipulated, LIME, and SHAP explanations for the Bias Detection task.", "description": "This figure displays three visualizations of explanations generated by LIME, SHAP, and an adversarially manipulated method for a bias detection task.  Each visualization shows how different words in the sentence are weighted in terms of their contribution to the AI model's decision to classify the sentence as biased or not biased.  The manipulated explanation is designed to be misleading by altering weights to potentially influence human judgments of bias.", "section": "B Evaluation I: Manipulating AI Explanations for Adversarial Purposes (Additional Results)"}, {"figure_path": "7XkwzaPMvX/figures/figures_18_3.jpg", "caption": "Figure B.4: The visual comparisons of adversarially manipulated, LIME, and SHAP explanations for the Bias Detection task.", "description": "This figure shows three different explanations generated by LIME, SHAP, and an adversarially manipulated method for two examples of text snippets in the Bias Detection task.  Each explanation highlights different words in the text, indicating varying degrees of importance in determining the bias of the text.  The adversarially manipulated explanation is designed to mislead the human evaluator regarding the text's bias, showcasing how these methods can be manipulated for malicious purposes. ", "section": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}, {"figure_path": "7XkwzaPMvX/figures/figures_19_1.jpg", "caption": "Figure B.5: The visual comparisons of adversarially manipulated, LIME, and SHAP explanations for the Toxicity Detection task.", "description": "This figure shows a comparison of three different types of explanations for the Toxicity Detection task. The explanations are shown for two examples where the AI model correctly predicts whether the text is toxic or not.  The figure shows how adversarially manipulated explanations highlight the toxic words more strongly, while LIME and SHAP explanations provide a more nuanced analysis of the text. The aim is to visualize how adversarially manipulated explanations are very different from the LIME and SHAP explanations and might affect the user's perception of toxicity.", "section": "B Evaluation I: Manipulating AI Explanations for Adversarial Purposes"}]