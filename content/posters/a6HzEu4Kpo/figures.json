[{"figure_path": "a6HzEu4Kpo/figures/figures_4_1.jpg", "caption": "Figure 1: The depiction of sample-level, group-level, and combined uncertainties.", "description": "This figure illustrates three scenarios of uncertainties in data distributions. (1) sample-level: shows individual data points scattered around the decision boundary, representing sample-level uncertainty. (2) group-level: shows clusters of data points, each cluster representing a group. The decision boundary separates the groups, indicating group-level uncertainty. (3) both: combines both sample-level and group-level uncertainties, showing a more complex and realistic distribution.", "section": "3 Problem Formulation and Algorithm"}, {"figure_path": "a6HzEu4Kpo/figures/figures_8_1.jpg", "caption": "Figure 2: Ablation study of TTSO*", "description": "This ablation study compares four variants of the TTSO framework to understand the impact of pre-trained LLMs and TTSO fine-tuning on model performance.  The four variants are: (1) TTSO++, a pretrained GPT2 model fine-tuned with TTSO; (2) TTSO+-, a pretrained GPT2 model without TTSO fine-tuning; (3) TTSO-+, a randomly initialized GPT2 model fine-tuned with TTSO; and (4) TTSO--, a randomly initialized GPT2 model without TTSO fine-tuning.  The results show the average accuracy for each variant across three datasets (HHAR, PAMAP, WESAD).", "section": "5.2 Ablation Study"}, {"figure_path": "a6HzEu4Kpo/figures/figures_20_1.jpg", "caption": "Figure 3: Structure of LLM Fine-tuning with TTSO, illustrating the two-phase approach starting with alignment fine-tuning followed by downstream fine-tuning, adapted specifically for time series out-of-distribution generalization tasks.", "description": "This figure shows the architecture used for fine-tuning LLMs with the TTSO framework.  The process is broken into two stages. In the first stage (alignment fine-tuning), the input time series data is passed through an input embedding layer into a language model.  A contrastive loss is used during training. The second stage (downstream fine-tuning) uses the output of the language model as input to a classifier which is trained using a supervised loss. This two-stage approach helps adapt the LLM to the specific time series classification task while retaining the knowledge learned during pre-training.", "section": "3.4 TTSO for Fine-tuning LLMs"}, {"figure_path": "a6HzEu4Kpo/figures/figures_21_1.jpg", "caption": "Figure 4: Sample-Level Uncertainty: Each line represents a window of time series data with the same label.", "description": "This figure illustrates the concept of sample-level uncertainty in time series data. Each line represents a short segment of time series data points with the same label, highlighting the variability within a single class.  The variations between the lines, even though they all belong to the same class, show the inherent noise and fluctuations present at the sample level.  This is in contrast to group-level uncertainty, which is not shown in this figure but refers to differences between groups or classes.", "section": "E.2 Illustration of Sample-level and Group-level Uncertainties"}, {"figure_path": "a6HzEu4Kpo/figures/figures_22_1.jpg", "caption": "Figure 5: Group-Level Uncertainty: Histogram of \u2018x\u2019 axis values, with each color representing a different group.", "description": "This figure demonstrates the group-level uncertainty by displaying the distribution of x-axis values from the accelerometer across different groups (users). Each color represents a distinct group, and each group\u2019s unique characteristics contribute to the overall group-level uncertainty.", "section": "E.2 Illustration of Sample-level and Group-level Uncertainties"}, {"figure_path": "a6HzEu4Kpo/figures/figures_22_2.jpg", "caption": "Figure 6: The effect of varying the number of Transformer layers (k) on average accuracy for OOD generalization across HHAR, PAMAP, and WESAD datasets.", "description": "This figure shows the impact of the number of Transformer layers in a GPT-2 model on the average accuracy of out-of-distribution (OOD) generalization across three different datasets: HHAR, PAMAP, and WESAD.  The x-axis represents the number of Transformer layers (k), and the y-axis represents the average accuracy.  The plot allows for a comparison of the performance across the three datasets as the number of layers changes.  It helps to determine the optimal number of layers to achieve the best OOD generalization performance.", "section": "E.3 Ablation Study on LLM Architectures and Parameter Configurations"}]