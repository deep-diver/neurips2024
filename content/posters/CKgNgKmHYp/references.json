{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language Models are Few-Shot Learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models and demonstrates the capabilities of LLMs in few-shot learning, which is central to the current paper's focus on LLM personalization."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language Models are Unsupervised Multitask Learners", "publication_date": "2019-02-14", "reason": "This paper introduces the concept of LLMs as unsupervised multitask learners, which is a core concept used in the current paper's approach to LLM personalization."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All You Need", "publication_date": "2017-12-05", "reason": "This paper introduces the Transformer architecture, which is the basis for most modern LLMs, including those used in the current paper's experiments."}, {"fullname_first_author": "Alec Radford", "paper_title": "Improving Language Understanding by Generative Pre-Training", "publication_date": "2018-06-11", "reason": "This paper introduces the GPT model, a significant advancement in LLMs that is fundamental to the current paper's work on black-box LLM personalization."}, {"fullname_first_author": "A. Salemi", "paper_title": "LaMP: When Large Language Models Meet Personalization", "publication_date": "2023-04-04", "reason": "This paper introduces the LaMP benchmark, which the current paper utilizes for its experiments, making it a crucial resource for evaluating LLM personalization techniques."}]}