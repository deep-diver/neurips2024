[{"figure_path": "CKgNgKmHYp/tables/tables_6_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model (gpt-3.5-turbo), random in-context learning (ICL-Random), retrieval-augmented generation (RAG), and profile-augmented generation (PAG).  The results show accuracy (Acc), F1-score (F1), mean absolute error (MAE), root mean squared error (RMSE), ROUGE-1 (R-1), ROUGE-L (R-L), and BLEU scores for each task and baseline, highlighting HYDRA's superior performance.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_6_2.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model and methods using random sampling, retrieval, and profile augmentation. The metrics used vary depending on the task and include accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU scores. The table highlights the best and second-best performing methods for each task to showcase HYDRA's improved performance. ", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_8_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model, random in-context learning, retrieval-augmented generation (RAG), and profile-augmented generation (PAG).  The table shows accuracy, F1-score (for classification tasks), mean absolute error (MAE), root mean squared error (RMSE), and ROUGE scores (for generation tasks) for each method and task.  Higher values are better for accuracy, F1-score, ROUGE-1, ROUGE-L, and BLEU; lower values are better for MAE and RMSE.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_15_1.jpg", "caption": "Table 4: Summary of LLM personalization baselines and HYDRA on the inclusion of different components. We present an overview of the existing LLM adaptation methods, focusing on six key aspects: (1) personalization for specific users, (2) global knowledge across different users, (3) retrieval from user history, (4) retrieval for relevance, (5) retrieval for usefulness, (6) learning-based method and (7) personalization of black-box LLMs.", "description": "This table summarizes existing LLM personalization methods, including prompting-based, learning-based, and the proposed HYDRA framework. It compares these methods based on six key aspects: personalization for specific users; presence of global knowledge; whether the method uses retrieval from user history; whether the retrieval prioritizes relevance or usefulness; whether the method is learning-based; and whether the method supports black-box LLMs.  This allows for a clear comparison across methods and highlights HYDRA's unique capabilities.", "section": "B.1 LLM Personalization"}, {"figure_path": "CKgNgKmHYp/tables/tables_16_1.jpg", "caption": "Table 5: Summary of LLM adaptation baselines and HYDRA-Adapter on the inclusion of different components. We present an overview of the existing LLM adaptation methods, focusing on five key aspects: (1) accessibility of model parameters, (2) availability of high-dimensional representations for input sequences or output generations, (3) availability of token probabilities, (4) necessity of retrieval corpus, and (5) utilization of a smaller adapter model. Adapted from Table 1 in Sun et al. [48].", "description": "This table compares different LLM adaptation methods based on five key features: access to model parameters, availability of high-dimensional representations, access to token probabilities, need for a retrieval corpus, and the use of a smaller adapter model.  It highlights the differences between white-box, grey-box, and black-box LLM adaptation techniques, showing HYDRA-Adapter's unique position as a method that requires neither full model parameters nor a large retrieval corpus, while still using a smaller adapter model.", "section": "B.3 Black-Box LLM Adaptation"}, {"figure_path": "CKgNgKmHYp/tables/tables_18_1.jpg", "caption": "Table 6: Dataset statistics of five different personalization tasks in the LaMP benchmark [41].", "description": "This table presents a summary of the five datasets used in the LaMP benchmark experiments. For each dataset, the table provides the type of task (classification or generation), the number of training, validation, and test instances, and the average length of the input and output sequences.  Additionally, it shows the number of profiles (unique users) and classes (categories or tags) present in each dataset.", "section": "4.1 Experimental Setup"}, {"figure_path": "CKgNgKmHYp/tables/tables_20_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model and methods that utilize random sampling, retrieval, and profile augmentation.  The metrics used vary depending on the specific task and include accuracy, F1 score, mean absolute error (MAE), root mean squared error (RMSE), ROUGE-1, ROUGE-L, and BLEU.  Higher values are better for most metrics, while lower values are better for MAE and RMSE.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_20_2.jpg", "caption": "Table 8: Effect of different black-box LLM adapters (k=4).", "description": "This table compares the performance of HYDRA-Adapter with the Personalized BBox-Adapter [48] across three tasks from the LaMP benchmark: LaMP-3 (MAE and RMSE), LaMP-4 (R-1, R-L, and BLEU), and LaMP-5 (R-1, R-L, and BLEU).  The results show that HYDRA-Adapter outperforms the baseline across all metrics. This highlights the effectiveness of HYDRA's adapter design in capturing global and user-specific knowledge to better adapt to personalized model outputs.", "section": "G.5 Effect of Black-Box LLM Adapters"}, {"figure_path": "CKgNgKmHYp/tables/tables_21_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model (gpt-3.5-turbo), random in-context learning (ICL-Random), retrieval-augmented generation (RAG), and profile-augmented generation (PAG).  The table shows accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU scores for each task, indicating HYDRA's superior performance in most cases.  The 'k' value represents the number of retrieved items used in some methods.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_21_2.jpg", "caption": "Table 10: Time complexity analysis with running time summary on the LaMP benchmark.", "description": "This table shows the time complexity and actual running times for different stages of the HYDRA model across the five LaMP tasks.  The time complexity is expressed in Big O notation and reflects the dependence on various factors such as the number of training/test users, number of retrieved history records, sequence length and the hidden dimension (d). The running times are provided for each of the HYDRA components in different modes (training, fitting new users, inference).  These results offer insights into the efficiency and scalability of the proposed approach.", "section": "4.4 Scale-up Analysis"}, {"figure_path": "CKgNgKmHYp/tables/tables_22_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model (gpt-3.5-turbo), in-context learning with randomly selected items from user history (ICL-Random), retrieval-augmented prompting (RAG), and profile-augmented prompting (PAG).  The table shows accuracy (Acc.), F1-score (F1), mean absolute error (MAE), root mean squared error (RMSE), ROUGE-1 (R-1), ROUGE-L (R-L), and BLEU scores for each task and baseline method.  Higher values are generally better for accuracy, F1, ROUGE, and BLEU, while lower values are better for MAE and RMSE.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_22_2.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines (gpt-3.5-turbo, ICL-Random, RAG, and PAG) across multiple evaluation metrics relevant to each task type (accuracy, F1 score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU). The results show HYDRA's superior performance in most cases, highlighting the effectiveness of the proposed model factorization approach.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_22_3.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines (gpt-3.5-turbo, ICL-Random, RAG, and PAG) across various evaluation metrics relevant to each task type (accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU).  The table highlights HYDRA's superior performance compared to the baselines.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_23_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model (gpt-3.5-turbo), random in-context learning, retrieval-augmented generation (RAG), and profile-augmented generation (PAG).  The metrics used vary depending on the task and include accuracy, F1-score, mean absolute error (MAE), root mean squared error (RMSE), ROUGE-1, ROUGE-L, and BLEU scores.  The table highlights HYDRA's superior performance across all five tasks.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_23_2.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on the LaMP benchmark across five different tasks.  It compares HYDRA's performance against several baselines, including a zero-shot model and various prompt-based approaches.  The metrics used vary depending on the task (accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, BLEU) and reflect the effectiveness of the different methods in achieving personalization.  The table highlights the best-performing methods for each task and indicates whether higher or lower values are better for each metric.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_24_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance (accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, BLEU) against several baselines (gpt-3.5-turbo, ICL-Random, RAG, PAG).  Higher values are better for accuracy, F1-score, ROUGE scores, and BLEU; lower values are better for MAE and RMSE. The best and second-best results for each task are highlighted.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_24_2.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on the LaMP benchmark across five different tasks.  It compares HYDRA's performance against several baselines, including a zero-shot model, random in-context learning, retrieval-augmented prompting, and profile-augmented prompting.  The results show the accuracy, F1-score (for classification tasks), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ROUGE scores (for generation tasks), highlighting HYDRA's superiority.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_24_3.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines, including a zero-shot model, random in-context learning, retrieval-augmented prompting, and profile-augmented prompting.  The results are presented using various metrics appropriate to each task type (accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU), showing HYDRA's superior performance across all tasks.", "section": "4.2 Main Results"}, {"figure_path": "CKgNgKmHYp/tables/tables_25_1.jpg", "caption": "Table 1: Main experiment results on the LaMP benchmark. R-1 and R-L represent ROUGE-1 and ROUGE-L, respectively. k indicates the number of retrieved items. \u2191 denotes that higher values are better, while \u2193 implies that lower values are preferred. The best score and 2nd best score for each task are highlighted in bold and underlined, respectively. Notations are consistent across tables.", "description": "This table presents the main experimental results of HYDRA on five different personalization tasks from the LaMP benchmark.  It compares HYDRA's performance against several baselines (gpt-3.5-turbo, ICL-Random, RAG, and PAG), using metrics appropriate to each task (accuracy, F1-score, MAE, RMSE, ROUGE-1, ROUGE-L, and BLEU).  The table shows that HYDRA significantly outperforms all baselines across all five tasks, demonstrating its effectiveness in black-box LLM personalization.", "section": "4.2 Main Results"}]