[{"type": "text", "text": "UniGAD: Unifying Multi-level Graph Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiqing Lin1\u2217, Jianheng Tang2,3, Chenyi $\\mathbf{Z}\\mathbf{i}^{3}$ , H.Vicky Zhao1, Yuan Yao2, Jia $\\mathbf{Li^{2,3\\dag}}$ ", "page_idx": 0}, {"type": "text", "text": "Tsinghua University 2Hong Kong University of Science and Technology 3Hong Kong University of Science and Technology (Guangzhou) linyq20@mails.tsinghua.edu.cn, jtangbf@connect.ust.hk, barristanzi666@gmail.com, vzhao@tsinghua.edu.cn, {yuany,jialee}@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Anomaly Detection (GAD) aims to identify uncommon, deviated, or suspicious objects within graph-structured data. Existing methods generally focus on a single graph object type (node, edge, graph, etc.) and often overlook the inherent connections among different object types of graph anomalies. For instance, a money laundering transaction might involve an abnormal account and the broader community it interacts with. To address this, we present UniGAD, the first unified framework for detecting anomalies at node, edge, and graph levels jointly. Specifically, we develop the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler) that unifies multi-level formats by transferring objects at each level into graph-level tasks on subgraphs. We theoretically prove that MRQSampler maximizes the accumulated spectral energy of subgraphs (i.e., the Rayleigh quotient) to preserve the most significant anomaly information. To further unify multi-level training, we introduce a novel GraphStitch Network to integrate information across different levels, adjust the amount of sharing required at each level, and harmonize conflicting training goals. Comprehensive experiments show that UniGAD outperforms both existing GAD methods specialized for a single task and graph prompt-based approaches for multiple tasks, while also providing robust zero-shot task transferability. All codes can be found at https://github.com/lllyyq1121/UniGAD. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Anomaly Detection (GAD) involves identifying a minority of uncommon graph objects that significantly deviate from the majority within graph-structured data [17, 2]. These anomalies can manifest as abnormal nodes, unusual relationships, irregular substructures within the graph, or entire graphs that deviate significantly from others. GAD has many practical applications in various contexts, including the identification of bots and fake news on social media [3, 1, 4, 30], detection of sensor faults and internet invasions in IoT networks [8, 14], and prevention of fraudsters and money laundering activities in transaction networks [19, 55]. The mainstream GAD models originate from the Graph Neural Networks (GNNs), which have recently gained popularity for mining graph data [52, 24, 57, 16]. To address the specific challenges of graph anomalies such as label imbalance [34, 31], relation camouflage [12, 38], and feature heterophily [50, 15], numerous adaptations of standard GNNs have been proposed [9, 67, 13, 40, 39, 10, 53, 41, 62]. ", "page_idx": 0}, {"type": "text", "text": "However, existing GAD approaches typically focus on a single type of graph object, such as node-level or graph-level anomaly detection, often overlooking the inherent correlations between different types of objects in graph-structured data. For example, a money laundering transaction might involve both an abnormal account and the broader community it interacts with, while the specific cancer of a cell is determined by particular proteins or protein complexes within the cell. Although some unsupervised methods fuse information from nodes, edges, and subgraphs through reconstruction [27, 10, 47] or contrastive pre-training [58, 13, 36], they are still limited to single-level label supervision or prediction. There is a need for a unified approach that considers these correlations information across different levels and performs multi-level anomaly detection. ", "page_idx": 0}, {"type": "image", "img_path": "sRILMnkkQd/tmp/5bbfe8de1d950760dc6db5668a3ac1dff8209a63ca66a67e23a537d5850e43ee.jpg", "img_caption": ["(I) Unify multi-level formats. (II) Unify multi-level training. Figure 1: The overall framework of UniGAD. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To design a unified model for addressing multi-level GAD, we identify two key challenges: ", "page_idx": 1}, {"type": "text", "text": "1. How to unify multi-level formats? Addressing node-level, edge-level, and graph-level tasks uniformly is challenging due to their inherent differences. Some recent works provide insights into unifying these tasks through the use of large language models (LLMs) or prompt tuning. While some methods leverage the generalization capability of LLMs [32, 54, 29] on text-attributed graphs, such semantic information is often unavailable in anomaly detection scenarios due to privacy concerns. On the other hand, graph prompt learning methods [48, 37, 61] design induced $k$ -hop graphs to transform node or edge levels into graph-level tasks. Nevertheless, their sampling strategies are not specifically tailored to anomaly data, resulting in inappropriate node selections that \u2018erase\u2019 critical anomaly information. This oversight can severely impact the effectiveness of anomaly detection. 2. How to unify multi-level training? Training a single model for multi-level tasks involves various influencing factors, such as transferring information between different levels and achieving a balanced training of these level tasks. There is limited research on multi-task learning in the graph learning domain. Efforts like ParetoGNN [23] employ multiple self-supervised learning objectives (e.g., similarity, mutual information) as independent tasks, but these are insufficient for managing multilevel supervision. A comprehensive approach is needed to effectively integrate and balance the training of different level tasks in multi-level GAD. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose UniGAD, a unified GAD model that leverages the transferability of information across node-level, edge-level, and graph-level tasks. To address the first challenge, we develop a novel subgraph sampler, MRQSampler, that maximizes accumulated spectral energy (i.e., the Rayleigh quotient) in the sampled subgraph with theoretical guarantee, ensuring that the sampled subgraphs contain the most critical anomaly information from nodes and edges. For the second challenge, we introduce the GraphStitch Network, which unifies multi-level training by integrating separate but identical networks for nodes, edges, and graphs into a unified multi-level model. This is achieved using a novel GraphStitch Unit that facilitates information sharing across different levels while maintaining the effectiveness of individual tasks. We perform comprehensive experiments on 14 GAD datasets and compare 17 state-of-the-art methods covering both node-level and graph-level GAD techniques, as well as prompt-based general multi-task graph learning methods. Results show that UniGAD achieves superior performance and offers robust zero-shot transferability across different tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Graph Anomaly Detection. Leveraging deep learning techniques in GAD has led to significant advancements and a wide range of applications [3, 14, 1, 4, 19, 65], thoroughly reviewed in a comprehensive survey [43]. Node-level anomaly detection, the most prevalent scenario in GAD, has witnessed numerous adaptations and improvements in graph neural networks (GNNs) aimed at enhancing performance from either a spatial [34, 38, 25] or spectral [28, 50, 15] perspective. Despite these advancements, recent benchmarks such as BOND [33] for unsupervised settings and GADBench [49] for supervised settings reveal that no single model excels across all datasets, highlighting the need for model selection tailored to specific datasets and task characteristics. For graph-level anomaly detection, various methodologies have been proposed, including transformation learning [66], knowledge distillation [42], and evolutionary mapping [44]. SIGNET [35] employs information bottleneck to generate informative subgraphs for explaining graph-level anomalies, while Rayleigh Quotient GNN [11] explores the spectral properties of anomalous graphs. Although both node-level and graph-level anomaly detection are rapidly evolving fields, to the best of our knowledge, there is no existing model that supports the joint detection of both node-level and graph-level anomalies. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Multi-task Learning on Graphs. Multi-task learning involves training a model to handle multiple tasks simultaneously, utilizing shared representations and relationships within the graph to enhance performance across all tasks. Recently, techniques such as graph prompt-based approaches and large language model (LLM)-based approaches have shown promise in this area. Prompt frameworks [69] like GraphPrompt [37], All-in-One [48], PRODIGY [22], MultiGPrompt [61], and SGL-PT [68] are designed to address a wide array of graph tasks. These approaches transform tasks at other levels into graph-level tasks by leveraging induced graphs. The All-in-One framework enhances connectivity by adding links between the prompt graph and the original graph, whereas GraphPrompt inserts the prompt token into graph nodes through element-wise multiplication. On the other hand, LLM-based frameworks [32, 54, 29, 6, 51] utilize the power of LLMs to learn from different levels, but they require graphs with text attributes or descriptions, which are not applicable in most anomaly detection scenarios. Additionally, some multi-task GNN efforts [23] focus on multiple self-supervised specific objectives (such as similarity and mutual information) as independent tasks, which are not suitable for unifying GAD with multi-level label supervision and prediction. ", "page_idx": 2}, {"type": "text", "text": "Notation. Let $\\mathcal{G}=\\{\\nu,\\varepsilon,X\\}$ denote a connected undirected graph, where $\\mathcal{V}=\\{v_{1},v_{2},...,v_{N}\\}$ is the set of $N$ nodes, $\\mathcal{E}=\\{e_{i j}\\}$ is the set of edges, and $\\b X\\in\\bar{\\mathbb{R}^{n\\times F}}$ is node features. Let $\\pmb{A}$ be the corresponding adjacency matrix, $_{D}$ be the degree matrix with $\\begin{array}{r}{D_{i i}=\\sum_{j}A_{i j}}\\end{array}$ . Laplacian matrix $\\textbf{\\emph{L}}$ is then defined as $D-A$ (regular) or as $I-D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}$ (normalized), where $\\boldsymbol{\\mathit{I}}$ is an identity matrix. The Laplacian matrix is a symmetric matrix and can be eigen-decomposed as ${\\cal L}=U{\\bf A}U^{\\check{T}}$ , where the diagonal matrix $\\Lambda$ consists of real eigenvalues (graph spectrum). Besides, we define the subgraph as $\\mathcal{G}_{i}$ centered on node $v_{i}$ and our sampled subgraph for node $v_{i}$ as $\\mathcal{S}_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Problem Formulation. The multi-level graph anomaly detection problem introduces a more universal challenge compared to traditional single-level approaches, described as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Multi-level GAD). Given a training set $\\tau_{r}(\\mathcal{N},\\mathcal{E},\\mathcal{G})$ containing nodes, edges, and graphs with arbitrary labels at any of these levels, the goal is to train a unified model to predict anomalies in a test set $\\tau_{e}(\\mathcal{N},\\mathcal{E},\\mathcal{G})$ , which also contains arbitrary labels at any of these levels. ", "page_idx": 2}, {"type": "text", "text": "Note that our approach does not require the presence of labels at all three levels simultaneously. It is feasible to have labels at one or more levels. Our proposed model aims to leverage the transferability of information across different levels to enhance its predictive capability. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section details the proposed model UniGAD for multi-level GAD, comprising a GNN encoder, MRQSampler, and GraphStitch Network, as shown in Fig. 1. Firstly, a shared pre-trained unsupervised GNN encoder is utilized to learn a more generalized node representation. To unify multi-level formats, the MRQSampler employs spectral sampling to extract subgraphs that contain the highest amount of anomalous information from nodes and edges, thus converting tasks at all three levels into graph-level tasks (Sec. 3.1). To unify multi-level training, the GraphStitch Network integrates information from different levels, adjusts the amount of sharing required at each level, and harmonizes conflicting training goals. (Sec. 3.2). ", "page_idx": 2}, {"type": "text", "text": "3.1 Spectral Subgraph Sampler for Unifying Multi-level Formats ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this subsection, we present the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler), the core module of our unified framework. By sampling subgraphs of nodes or edges, we transform node-level and edge-level tasks into graph-level tasks. Our sampler optimizes these subgraphs to maximize the Rayleigh quotient, ensuring that the sampled subgraphs retain a higher concentration of anomaly information. ", "page_idx": 2}, {"type": "image", "img_path": "sRILMnkkQd/tmp/4f824e5d673589a508b710a9a1a2a81230de96d330c3b742830c8cba7ae7b1ba.jpg", "img_caption": ["Figure 2: Message passing in GNNs and rooted subtree sampling. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1.1 Analysis of the Subgraph Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "What is a suitable subgraph for GAD? Existing methods on selecting subgraphs for target nodes or edges often use straightforward approaches like $r$ -ego or $k$ -hop subgraphs [48]. However, the size of the subgraph is critical for classification outcomes. If the subgraph is too large, it includes too many irrelevant nodes, while if it is too small, it may not align effectively with graph-level tasks. ", "page_idx": 3}, {"type": "text", "text": "To measure anomaly information in a subgraph, recent studies [50, 11] have identified a \u2018rightshift\u2019 phenomenon in the spectral energy distribution, moving from low to higher frequencies. This accumulated spectral energy can be quantified by the Rayleigh quotient [20]: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR Q(\\pmb{x},\\pmb{L})=\\frac{\\pmb{x}^{T}\\pmb{L}\\pmb{x}}{\\pmb{x}^{T}\\pmb{x}}=\\frac{\\sum_{(i,j)\\in\\mathcal{E}}{\\cal A}_{i j}(x_{j}-x_{i})^{2}}{\\sum_{i\\in\\mathcal{V}}x_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The following lemma [50] illustrates the relationship between the Rayleigh quotient $R Q(x,L)$ and anomaly information: ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Tang, 2022). Rayleigh quotient $R Q(x,L)$ , i.e. the accumulated spectral energy of the graph signal, is monotonically increasing with the anomaly degree. ", "page_idx": 3}, {"type": "text", "text": "Thus, for any node $v_{i}$ , our sampling objective is to identify the induced subgraph with the highest Rayleigh quotient containing the most anomaly information. ", "page_idx": 3}, {"type": "text", "text": "Where to Sample Subgraph From? To preserve the properties of target nodes, it is essential to sample subgraphs centered around these nodes, capturing key surrounding nodes. The most intuitive methods are $r$ -ego graphs or $k$ -hop graphs. However, considering the message-passing mechanisms of most GNNs [24, 56, 16], a classical work [57] provides valuable insight: ", "page_idx": 3}, {"type": "text", "text": "Lemma 2 (Xu, 2018). A GNN recursively updates each node\u2019s feature vector through its rooted subtree structures to capture the network structure and features of surrounding nodes. ", "page_idx": 3}, {"type": "text", "text": "As shown in Fig. 2, the message-passing process of GNNs suggests that a rooted subtree centered on the target node is more consistent with the GNN\u2019s architecture. Therefore, we sample subgraphs from these rooted subtree structures. The remaining question is: How to implement subgraph sampling based on the above? To address this, we introduce a novel MRQSampler in the next subsection. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Building on the motivation in Section 3.1.1, our approach involves sampling subgraphs for each node starting from the rooted subtree with the node as its root. The target node is always included. We then select the subtree with the maximum Rayleigh quotient from all possible subtrees as the representative subgraph for that node to ensure it contain the maximum anomaly information. We formulate this as the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{S^{\\star}=\\underset{S\\subseteq\\mathcal{G}}{\\arg\\operatorname*{max}}}&{\\frac{\\sum_{(p,q)\\in\\mathcal{E}_{S}}(x_{p}-x_{q})^{2}}{\\sum_{p\\in S}x_{p}^{2}},}\\\\ {\\mathrm{s.t.}\\quad}&{v\\in S,}\\\\ &{\\forall v_{p}\\in S,\\;(v,v_{p})\\;\\mathrm{is\\accessible}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{G}$ represents $k$ -depth rooted subtree from $v$ , and $\\boldsymbol{S}$ is a possible subgraph from $\\mathcal{G}$ . The first constraint ensures the target node is included, and the second constraint ensures message passability. Generally, similar selecting subgraphs in this manner is considered an NP-Hard problem [59]. However, leveraging the properties of trees, we propose an algorithm to solve the optimal solution. ", "page_idx": 3}, {"type": "image", "img_path": "sRILMnkkQd/tmp/37a9abc2b1609d8a4782eb1144ef0358859f4f4cdab77db99b1d7d30112ff0d4.jpg", "img_caption": ["Figure 3: MRQSampler: (i) Derive the condition (Theorem 2) satisfied with the optimal subtree. (ii) Decompose the problem into simpler sub-problems by recursing through the tree depth to solve the optimal subtree with the dynamic programming (DP) algorithm. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We first determine the conditions that increase a subgraph\u2019s Rayleigh quotient when adding a node, presented in the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. For a graph $\\mathcal{G}$ , let one of its subgraphs be $\\boldsymbol{S}$ , and let its Rayleigh quotient be $R Q(S)$ . If a new node $v_{n e w}\\in\\mathcal G-\\mathcal S$ is added to $\\boldsymbol{S}$ , the Rayleigh quotient $R Q(S)$ will increase if and only $i f$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta(v_{n e w})=\\frac{\\sum_{v_{r}\\in S}(x_{n e w}-x_{r})^{2}}{x_{n e w}^{2}}>R Q(S).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 can be found in Appendix A.1. We can extend this theorem from a single new node $v_{n e w}$ to a new node set $\\nu_{n e w}$ , leading to the following corollary: ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. For a graph $\\mathcal{G}$ , let one of its subgraphs be $\\boldsymbol{S}$ , and let its Rayleigh quotient be $R Q(S)$ . If a new nodeset $\\mathcal{V}n e w\\subset\\mathcal{G}-\\mathcal{S}$ is added to $\\boldsymbol{S}$ , the Rayleigh quotient $R Q(S)$ will increase if and only $i f$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta(\\mathcal{V}_{n e w})=\\frac{\\sum_{(i,r)\\in\\mathcal{E}_{S+\\mathcal{V}_{n e w}}}(x_{n e w_{i}}-x_{r})^{2}+\\sum_{(i,j)\\in\\mathcal{E}_{\\mathcal{V}_{n e w}}}(x_{n e w_{i}}-x_{n e w_{j}})^{2}}{\\sum_{v_{n e w}\\in\\mathcal{V}_{n e w}}x_{n e w}^{2}}>R Q(S).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof details are also in Appendix A.2. While the above analysis can indeed increase the Rayleigh quotient of the sampled subgraph, the sampling order may cause the results to fall into a local optimum, which may not guarantee a globally optimal solution. To identify the nodes that must be sampled in the optimal subgraph, we present the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. For a graph $\\mathcal{G}$ , let one of its subgraph be $\\boldsymbol{S}$ , the $S^{*}$ be its final optimal subgraph, and $S\\subset S^{*}$ . For a new connected nodeset $\\tilde{\\mathcal{V}}_{n e w}\\cap S=\\emptyset,$ , it is contained in $S^{*}$ when it satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{m a x}(\\tilde{\\mathcal{V}}_{n e w})=\\operatorname*{max}_{\\tilde{\\mathcal{V}}_{n e w}\\subseteq\\mathcal{G}-S}\\Delta(\\tilde{\\mathcal{V}}_{n e w}),\\;a n d\\,\\Delta_{m a x}(\\tilde{\\mathcal{V}}_{n e w})>R Q(S).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We refer readers to Appendix A.3 for the rigorous proof. Through the above analysis, we derive the conditions of the nodeset contained in the optimal subtree (Theorem 2). When $\\tilde{\\mathcal{V}}_{n e w}$ satisfies Eq. (5), it always increases the Rayleigh quotient based on the current subgraph, ensuring that $\\nu_{n e w}$ is contained in the optimal solution. Thus, we decouple the problem of finding the subgraph with the maximum Rayleigh quotient into a process of finding the maximum $\\Delta_{m a x}(\\mathcal{V}_{n e w})$ each time, until adding any node/node set fails to increase the $R Q(S)$ . Following this, we design a dynamic programming (DP) algorithm to ensure the optimal subset satisfies these conditions. ", "page_idx": 4}, {"type": "text", "text": "MRQSampler Algorithm. We introduce the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler), which uses dynamic programming (DP) to find the optimal solution. We break down the computation for the central node into sub-problems, storing the results of sub-problems to avoid redundant computations in future calculations. For a rooted subtree with the target node (edge) as the root, its children are unconnected to each other. In Fig. 3, we consider a 2-depth subtree and summarize our algorithm as follows: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Stage 1: We recursively compute and store the maximum $\\Delta(\\tilde{\\mathcal{V}}_{n e w})$ for each subtree, which can be down into simpler sub-problems similar to the previous one and calculates each layer in the tree recursively from the bottom up. ", "page_idx": 4}, {"type": "image", "img_path": "sRILMnkkQd/tmp/f5c383e0fbc7b21838e7c38be37916c3d443e628af66f4d4595edb4d1a1ce9bd.jpg", "img_caption": ["Figure 4: GraphStitch network structure in UniGAD. Node level is highlighted. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "\u2022 Stage 2: Based on Theorem 2, we iteratively select the descendant with the maximum $\\bar{\\Delta_{m a x}(\\tilde{\\gamma}_{n e w})}$ (within its own rooted subtree) of the target node and the currently selected nodeset, until the conditions of Theorem 2 are no longer satisfied, i.e., when the Rayleigh quotient of the sampled subgraph no longer increases. ", "page_idx": 5}, {"type": "text", "text": "For efficiency, this approach can obtain the subgraph with the maximum Rayleigh quotient of the target node/edge\u2019s rooted subtree while reducing the algorithmic complexity to $O(N\\log N)$ . It can be further accelerated in parallel since the computation for different nodes is independent. Additionally, the sampling process only needs to be computed once in training and inference processes, minimally impacting model efficiency. For the detailed pseudocode of the algorithm, please refer to Appendix B. Note that we use mean pooling for entire graphs, but for subgraphs, we use weighted pooling to highlight central nodes/edges, with an exponential decay based on the number of hops to the central nodes/edges. This method transforms node-level and edge-level tasks into graph-level tasks, ensuring that the most anomaly information is retained in the sampled subgraphs. ", "page_idx": 5}, {"type": "text", "text": "3.2 GraphStitch Network for Unifying Multi-level Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After obtaining graph representations, training them together through a fully connected layer can negatively impact individual levels due to the inherent differences across different-level anomalies. This can result in mediocre performance at all levels. A key challenge, therefore, is to facilitate information transfer between multi-levels without compromising single-level effectiveness. Inspired by work in the computer vision field [45], we introduce the novel GraphStitch Network to jointly consider multi-level representations. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we train separate but identical networks for each level and use the GraphStitch unit to combine these networks into a multi-level network, managing the degree of sharing required at different levels. This approach aims to maintain single-level effectiveness while enhancing multi-level information transfer. The network structure is illustrated in Fig. 4. ", "page_idx": 5}, {"type": "text", "text": "To elaborate, we denote $\\mathbf{e}_{N},\\,\\mathbf{e}_{E}$ , and $\\mathbf{e}_{G}$ as the embeddings for nodes, edges, and graphs, respectively. The node embedding $\\mathbf{e}_{N}\\,=\\,(\\mathbf{e}_{n n},\\mathbf{e}_{n e},\\mathbf{e}_{n g})^{\\top}$ consists of outputs from three separate but identically structured networks specialized for nodes, edges, and graphs. Similarly, the edge and graph embeddings are represented as $\\mathbf{e}_{E}=(\\mathbf{e}_{e n},\\mathbf{e}_{e e},\\mathbf{e}_{e g})^{\\top}$ and $\\mathbf{e}_{G}=(\\mathbf{e}_{g n},\\mathbf{e}_{g e},\\mathbf{e}_{g g})^{\\top}$ . ", "page_idx": 5}, {"type": "text", "text": "We define a GraphStitch operation as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\tilde{\\mathbf{e}}_{N},\\tilde{\\mathbf{e}}_{E},\\tilde{\\mathbf{e}}_{G})=d i a g\\left[\\left(\\begin{array}{l l l}{\\alpha_{n n}}&{\\alpha_{n e}}&{\\alpha_{n g}}\\\\ {\\alpha_{e n}}&{\\alpha_{e e}}&{\\alpha_{e g}}\\\\ {\\alpha_{g n}}&{\\alpha_{g e}}&{\\alpha_{g g}}\\end{array}\\right)(\\mathbf{e}_{N},\\mathbf{e}_{E},\\mathbf{e}_{G})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The sharing of representations is achieved by learning a linear combination of the outputs from the three networks. This linear combination is parameterized using learnable $\\alpha$ . In particular, when training data lacks a certain level, the influence of that level on other levels is defined as zero during training but still retains the influence of other levels on this level. In this way, it allows the labels for training and testing to be arbitrary. Besides, if all the cross terms $(\\alpha_{n e},\\alpha_{n g},\\alpha_{e n},\\alpha_{e g},\\alpha_{g n},\\alpha_{g e})$ are equal to 0 means that training the three networks jointly is equivalent to training them independently. Finally, the embeddings for nodes, edges, and graphs are fed into three independent multi-layer perceptrons (MLPs) to compute the abnormal probabilities $p_{i}^{N}$ , $p_{i}^{\\mathcal{E}}$ , and $p_{i}^{\\mathcal{G}}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "In addition to the GraphStitch structure, UniGAD optimizes the loss functions for multi-level tasks. Specifically, the gradients of each level task\u2019s loss may confilct in direction or magnitude, potentially causing negative effects and resulting in worse performance compared to learning single-level tasks individually. Therefore, UniGAD uses a multi-level weighted cross-entropy loss for training: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\sum_{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\beta^{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\left[\\gamma y_{i}^{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\log\\left(p_{i}^{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\right)+\\left(1-y_{i}^{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\right)\\log\\left(1-p_{i}^{\\{N,\\mathcal{E},\\mathcal{G}\\}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma$ is the ratio of anomaly labels $y_{i}=1,$ ) to normal labels $(y_{i}=0)$ ), and $\\beta^{\\{\\mathcal{N},\\mathcal{E},\\mathcal{G}\\}}$ are adaptive weights for different tasks. We adopt a \u2018Gradient Surgery\u2019 approach [60] to adjust the $\\beta^{\\{\\mathcal{N},\\mathcal{E},\\mathcal{G}\\}}$ , altering the gradients by projecting each onto the normal plane of the others. This prevents the interfering components of the gradients from affecting the network and minimizes interference among different-level GAD tasks. In this way, UniGAD ensures that each level remains relatively independent while facilitating cross-passing of relevant information between multi-level tasks. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct experiments to evaluate our UniGAD with node-level, edge-level, and graph-level tasks by answering the following questions: Q1: How effective is UniGAD in unifying in multi-level anomaly detection? Q2: Can UniGAD transfer information across different levels in zero-shot learning? Q3: What are the contributions of the modular design in the UniGAD model? Q4: How do the time and space efficiencies of UniGAD compare to those of other methods? ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We consider a total of 14 datasets, including both single-graph datasets and multi-graph datasets. 7 single-graph datasets are used to evaluate the performance of unifying node-level and edgelevel tasks: Reddit, Weibo, Amazon, Yelp, Tolokers, and Questions, T-finance from the work [49], which contain node-level anomaly labels. For edge anomaly labels, we generated them according to a specific anomaly probability following the formula $P_{\\mathrm{anom}}^{(i,j)}=a v g(P_{\\mathrm{anom}}^{i},P_{\\mathrm{anom}}^{j})$ . And 7 multi-graph datasets are used to validate the performance of unifying node-level and graph-level tasks, including BM-MN, BM-MS, BM-MT, MUTAG, MNIST0, MNIST1, and T-Group. The first six datasets are from [35], containing both node anomaly labels and graph anomaly labels. Moreover, we release a real-world large-scale social group dataset T-Group, combining the data (graph anomaly labels) in [26]. For its node anomaly labels, we assume that if a node appears in 3 malicious social groups, we consider it a malicious node. Statistical data for these datasets can be found in Table 1, including the percentage of training data, the number of graphs, edges, nodes, feature dimensions, and the proportions of abnormal nodes, edges, and graphs $(\\mathrm{Nodes}_{a b}$ , $\\mathrm{Edges}_{a b}$ , and Graphs $^{\\prime}a b$ ). ", "page_idx": 6}, {"type": "table", "img_path": "sRILMnkkQd/tmp/3bfb5b6e5f836fde93f30725f75586483912b8c05fecaf43151dfb8bff8c4d75.jpg", "table_caption": ["Table 1: Detailed statistics of the datasets used in our experiments. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baselines. To comprehensively compare with traditional single-level tasks, we consider nine representative node-level methods: GCN [24], GIN [57], GraphSAGE [16], SGC [56], GAT [52], BernNet [18], PNA [7], AMNet [5], and BWGNN [50]. Given the limited work on edge anomalies, we adapt a concatenated strategy [64] from link prediction, resulting in nine corresponding edge-level methods: GCNE, GINE, GSAGEE, SGCE, GATE, BernE, PNAE, AME, and BWE. For graph-level anomaly detection, we consider six state-of-the-art methods: OCGIN [66], OCGTL [46], GLocalKD [42], iGAD [63], GmapAD [44], and RQGNN [11]. Additionally, to compare multi-task models, we include two recent multi-task graph prompt methods: GraphPrompt [37] and All-in-One [48]. While these methods were not originally proposed for joint multi-task training, we adapt their ideas and develop multi-task versions for our comparison, GraphPrompt-U and All-in-One-U, whose modifications were limited to the data preprocessing component to accommodate the simultaneous handling of multiple object types (node/edge or node/graph) within induced graphs. ", "page_idx": 6}, {"type": "table", "img_path": "sRILMnkkQd/tmp/212c2e0aa1798b1927434cd75efdc2ee328b1e64db0f25ebfe5c2b57d6accbe5.jpg", "table_caption": ["Table 2: Comparison of unified performance (AUROC) at both node and edge levels with different single-level methods, multi-task methods, and our proposed method. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Implementations. We evaluate three metrics: AUROC (Sec. 4), Macro F1-score and AUPRC (Appendix E). For each result, we conduct 5 runs and report the mean results. In UniGAD, we choose two backbone GNN encoders: GCN [24] and BWGNN [50]. We use a shared graph pre-training method, GraphMAE [21], to obtain a more generalized node representation. For multi-dimensional feature vectors, we normalize all feature dimensions and then take the norm (1-norm in our case) to obtain a composite feature for each node, allowing us to identify the most anomalous nodes in MRQSampler based on this comprehensive feature. To avoid data leakage, for single-graph datasets, edges between the training set and the testing set are not considered; for multi-graph datasets, the training set and the testing set consist of different graphs and their nodes. More details on the implementation can be found in the Appendix C. ", "page_idx": 7}, {"type": "text", "text": "4.2 Multi-Level Performance Comparison (RQ1) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To compare the performance of multi-level anomaly detection, we conduct experiments under two settings. For the single-graph datasets, we compare the performance of unified training on node-level and edge-level data. For the multi-graph datasets, we compare the performance of unified training on node-level and graph-level data. ", "page_idx": 7}, {"type": "text", "text": "Node-level and edge-level jointly. We first evaluate the performance of unified training on node-level and edge-level data. We compare UniGAD against three groups of GNN models mentioned above: node-level models, edge-level models, and multi-task graph learning methods. Table 2 reports the AUROC of each model on six datasets, with the best result on each dataset highlighted in boldface. Overall, we find that UniGAD achieves state-of-the-art performance in nearly all scenarios. UniGAD outperforms single-level specialized models, indicating that unified training with UniGAD leverages information from other levels to enhance the performance of individual tasks. Multi-task approaches (GraphPrompt-U and All-in-One-U) tend to negatively impact multi-task performance, potentially because they are unable to effectively handle different types of anomaly label supervision. Meanwhile, UniGAD is designed for a multi-task setting, the performance on a single level might be slightly compromised to ensure the model performs well across all tasks in some datasets. ", "page_idx": 7}, {"type": "text", "text": "Node-level and graph-level jointly. We then evaluate the unified training of node-level and graphlevel tasks under similar settings. Table 3 shows the results, and UniGAD achieves state-of-the-art performance in nearly all scenarios. Our observations are as follows. First, there is a multi-level synergy in UniGAD, where strong performance in one task benefits the performance of other tasks. ", "page_idx": 7}, {"type": "table", "img_path": "sRILMnkkQd/tmp/3f85fd23bf96915a5ef8281a297966909df396b935d1e145e94039e07298fd90.jpg", "table_caption": ["Table 3: Comparison of unified performance (AUROC) at both node and graph levels with different single-level methods, multi-task methods, and our proposed method. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "sRILMnkkQd/tmp/537d01698e4ad46af7ae560369e1ff1a93044e0ba33b49aa0df9b475bb48be91.jpg", "table_caption": ["Table 4: Zero-shot transferability (AUROC) at node and edge levels. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "For example, in MNIST-0 and MNIST-1, compared to other graph-level GAD methods, UniGAD significantly boosts graph-level performance by leveraging strong node-level results. Second, UniGAD performs better on large graphs, likely because graph structure plays a more significant role in smaller datasets. However, the backbones of UniGAD (GCN, BWGNN) are primarily node-level models, which may not effectively encode graph-level structural information. This limitation\u2019s impact diminishes in large-scale graph datasets. Besides, methods like All-in-One-U often run out of time (OOT) with large datasets because they redundantly learn the same node representations across different subgraphs, making processing impractically slow, especially for large graph-level datasets like T-Group. UniGAD addresses this issue by using a shared GNN encoder across tasks, avoiding redundant learning and enhancing efficiency. ", "page_idx": 8}, {"type": "text", "text": "4.3 The Transferability in Zero-Shot Learning (RQ2) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To assess the transfer capability of UniGAD, we explore zero-shot learning scenarios where labels for a given level have never been exposed during training, as shown in Tables 4 and 5. In these experiments, UniGAD is trained solely with labels from alternative levels. The notation $\\mathbf{N}\\rightarrow\\mathbf{E}$ indicates using node labels to infer edge labels, with analogous notations for other label transfers. Our findings indicate that in zero-shot scenarios, UniGAD outperforms existing multi-task prompt learning methods. Moreover, the classification performance of UniGAD under zero-shot transfer learning even surpasses some of the leading baselines in supervised settings on Yelp and BM-MS. It highlights the superior transfer capability of UniGAD across various GAD tasks. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study (RQ3) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the contribution of each module in UniGAD, we present the ablation study results in Table 6. For the sampler module, we compare the results without subgraph sampling (w/o GS.), using a simple sampler with all 2- hop neighbors (w 2hop.), and using random sampling (w RS.). For the GraphStitch module, we replace it with a unified MLP (w/o ST.). The results indicate that both the subgraph sampler ", "page_idx": 8}, {"type": "table", "img_path": "sRILMnkkQd/tmp/49b8e596f727cac7bfdc6b193bd89e439e21db83edd0c0ad7b4c46b90a8be837.jpg", "table_caption": ["Table 6: Performance of UniGAD and its variants. "], "table_footnote": ["(SG.) and the GraphStitch (ST.) modules enhance the overall performance of UniGAD. Additionally, "], "page_idx": 8}, {"type": "table", "img_path": "sRILMnkkQd/tmp/5d1304de11445f1ce1f1c2fd7997f1ccd635ce23c93b641f223a75154e281068.jpg", "table_caption": ["Table 5: Zero-shot transferability (AUROC) at node and graph levels. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "sRILMnkkQd/tmp/a5cc8259129087728ab585895dde420576e695a389e2a54a2ff970f94c2352cb.jpg", "img_caption": ["Figure 5: The evaluation of time and space efficiency metrics. We highlight the percentage of total execution time spent by MRQSampler. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "inappropriate subgraph sampling may perform worse than no subgraph sampling, likely due to the loss of anomalous information during the sampling process. ", "page_idx": 9}, {"type": "text", "text": "4.5 Efficiency Analysis (RQ4) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "we conduct a comprehensive evaluation of both time and space efficiency on the large-scale, real-world T-Group dataset. To provide a more straightforward comparison between single-task and multi-task baselines, we calculate the average, minimum, and maximum for combinations of single-task nodelevel and graph-level models, and compare these with multi-task models. The results, as shown in Fig. 5 (a), indicate that in terms of execution time, our method is slower than the combination of the fastest single-level models but faster than the average of the combination. Regarding peak memory usage, Fig. 5 (b) demonstrates that graph-level models consume significantly more memory than node-level models. Our method maintains memory consumption comparable to node-level models and substantially lower than both graph-level GAD models and prompt-based methods. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents UniGAD, a unified graph anomaly detection framework that jointly addresses anomalies at the node, edge, and graph levels. The model integrates two novel components: the MRQSampler and the GraphStitch network. MRQSampler maximizes spectral energy to ensure subgraphs capture critical anomaly information, addressing the challenge of unifying different graph object formats. The GraphStitch Network unifies multi-level training by using identical networks for nodes, edges, and graphs, facilitated by the GraphStitch Unit for effective information sharing. Our thorough evaluations across 14 GAD datasets, including two real-world large-scale datasets (T-Finance and T-Group), and comparisons with 17 graph learning methods show that UniGAD not only surpasses existing models in various tasks but also exhibits strong zero-shot transferability capabilities. A limitation of our paper is that the GNN encoder primarily focuses on node-level embeddings, which may result in lost information about the graph structure. We leave the exploration of multi-level tasks pre-training in the future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Y. Lin and H. Zhao were supported by the Beijing Natural Science Foundation under Grant IS24036. J. Li was supported by NSFC Grant No. 62206067 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673. Y.Yao was in part supported by the HKRGC GRF-16308321 and the NSFC/RGC Joint Research Scheme Grant N_HKUST635/20. In addition, Y. Lin was also awarded a Tsinghua Scholarship for Overseas Graduate Studies at the Hong Kong University of Science and Technology. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Esma A\u00efmeur, Sabrine Amri, and Gilles Brassard. Fake news, disinformation and misinformation in social media: a review. Social Network Analysis and Mining, 13(1):30, 2023. [2] Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: a survey. Data mining and knowledge discovery, 29(3):626\u2013688, 2015. [3] Ketan Anand, Jay Kumar, and Kunal Anand. Anomaly detection in online social network: A survey. In 2017 International Conference on Inventive Communication and Computational Technologies (ICICCT), pages 456\u2013459. IEEE, 2017. [4] Alessandro Bondielli and Francesco Marcelloni. A survey on fake news and rumour detection techniques. Information Sciences, 497:38\u201355, 2019. [5] Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and Weihao Jiang. Can abnormality be detected by graph neural networks? In IJCAI, 2022. [6] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model for graph computational problems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 353\u2013364, 2024. [7] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Velic\u02c7kovic\u00b4. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260\u201313271, 2020. [8] Kelton AP Da Costa, Jo\u00e3o P Papa, Celso O Lisboa, Roberto Munoz, and Victor Hugo C de Albuquerque. Internet of things: A survey on machine learning-based intrusion detection approaches. Computer Networks, 151:147\u2013157, 2019. [9] Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time series. In Proceedings of the AAAI conference on artificial intelligence, pages 4027\u20134035, 2021.   \n[10] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks. In Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM, 2019.   \n[11] Xiangyu Dong, Xingyi Zhang, and Sibo Wang. Rayleigh quotient graph neural networks for graph-level anomaly detection. arXiv preprint arXiv:2310.02861, 2023.   \n[12] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph neural network-based fraud detectors against camouflaged fraudsters. In CIKM, pages 315\u2013324, 2020.   \n[13] Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, and Zhibin Dong. Graph anomaly detection via multi-scale contrastive learning networks with augmented view. arXiv preprint arXiv:2212.00535, 2022.   \n[14] Anuroop Gaddam, Tim Wilkin, Maia Angelova, and Jyotheesh Gaddam. Detecting sensor faults, anomalies and outliers in the internet of things: A survey on the challenges and solutions. Electronics, 9(3):511, 2020.   \n[15] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Addressing heterophily in graph anomaly detection: A perspective of graph spectrum. In Proceedings of the ACM Web Conference, 2023.   \n[16] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017.   \n[17] Jiawei Han, Micheline Kamber, and Jian Pei. Data Mining: Concepts and Techniques, 3rd edition. Morgan Kaufmann, 2011.   \n[18] Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021.   \n[19] Waleed Hilal, S Andrew Gadsden, and John Yawney. Financial fraud: a review of anomaly detection techniques and recent advances. Expert systems With applications, 193:116429, 2022.   \n[20] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.   \n[21] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 594\u2013604, 2022.   \n[22] Qian Huang, Hongyu Ren, Peng Chen, Gregor Kr\u017emanc, Daniel Zeng, Percy S Liang, and Jure Leskovec. Prodigy: Enabling in-context learning over graphs. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye, and Chuxu Zhang. Multi-task self-supervised graph neural networks enable stronger task generalization. arXiv preprint arXiv:2210.02016, 2022.   \n[24] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.   \n[25] Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong Li. Spam review detection with graph convolutional networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2703\u20132711, 2019.   \n[26] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang. Semisupervised graph classification: A hierarchical graph perspective. In The World Wide Web Conference, pages 972\u2013982, 2019.   \n[27] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection in attributed networks. In IJCAI, pages 2152\u20132158, 2017.   \n[28] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In CIKM, pages 2233\u20132236, 2019.   \n[29] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. A survey of graph meets large language model: Progress and future directions. arXiv preprint arXiv:2311.12399, 2023.   \n[30] Yiqing Lin and H Vicky Zhao. Maximum entropy attack on decision fusion with herding behaviors. IEEE Signal Processing Letters, 2024.   \n[31] Fanzhen Liu, Xiaoxiao Ma, Jia Wu, Jian Yang, Shan Xue, Amin Beheshti, Chuan Zhou, Hao Peng, Quan Z Sheng, and Charu C Aggarwal. Dagad: Data augmentation for graph anomaly detection. arXiv preprint arXiv:2210.09766, 2022.   \n[32] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One for all: Towards training one graph model for all classification tasks. arXiv preprint arXiv:2310.00149, 2023.   \n[33] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H Chen, Zhihao Jia, and Philip S Yu. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. In Advances in Neural Information Processing Systems, volume 35, 2022.   \n[34] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: A gnn-based imbalanced learning approach for fraud detection. In Proceedings of the Web Conference 2021, 2021.   \n[35] Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, and Shirui Pan. Towards selfinterpretable graph-level anomaly detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[36] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on attributed networks via contrastive self-supervised learning. IEEE transactions on neural networks and learning systems, 2021.   \n[37] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. In The Web Conference, pages 417\u2013428, 2023.   \n[38] Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. Alleviating the inconsistency problem of applying graph neural network to fraud detection. In SIGIR, pages 1569\u20131572, 2020.   \n[39] Zhiyuan Liu, Chunjie Cao, and Jingzhang Sun. Mul-gad: a semi-supervised graph anomaly detection framework via aggregating multi-view information. arXiv preprint arXiv:2212.05478, 2022.   \n[40] Zhiyuan Liu, Chunjie Cao, Fangjian Tao, and Jingzhang Sun. Revisiting graph contrastive learning for anomaly detection. arXiv preprint arXiv:2305.02496, 2023.   \n[41] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 4424\u20134431, 2019.   \n[42] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly detection by glocal knowledge distillation. In Proceedings of the fifteenth ACM international conference on web search and data mining, pages 704\u2013714, 2022.   \n[43] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge and Data Engineering, 2021.   \n[44] Xiaoxiao Ma, Jia Wu, Jian Yang, and Quan Z Sheng. Towards graph-level anomaly detection via deep evolutionary mapping. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1631\u20131642, 2023.   \n[45] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3994\u20134003, 2016.   \n[46] Chen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. Raising the bar in graph-level anomaly detection. arXiv preprint arXiv:2205.13845, 2022.   \n[47] Amit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, and Pan Li. Gad-nr: Graph anomaly detection via neighborhood reconstruction. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 576\u2013585, 2024.   \n[48] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2120\u20132131, 2023.   \n[49] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and benchmarking supervised graph anomaly detection. Advances in Neural Information Processing Systems, 36, 2024.   \n[50] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In International Conference on Machine Learning, 2022.   \n[51] Jianheng Tang, Qifan Zhang, Yuhan Li, and Jia Li. Grapharena: Benchmarking large language models on graph computational problems. arXiv preprint arXiv:2407.00379, 2024.   \n[52] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2017.   \n[53] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. A semi-supervised graph attentive network for financial fraud detection. In ICDM, pages 598\u2013607. IEEE, 2019.   \n[54] Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, and Julian McAuley. Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment. arXiv preprint arXiv:2402.08785, 2024.   \n[55] Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio Bellei, Tom Robinson, and Charles E Leiserson. Anti-money laundering in bitcoin: Experimenting with graph convolutional networks for financial forensics. arXiv preprint arXiv:1908.02591, 2019.   \n[56] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In ICML, pages 6861\u20136871, 2019.   \n[57] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ICLR, 2019.   \n[58] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed network anomaly detection with data augmentation. In Proceedings of the PAKDD, 2022.   \n[59] Kuo Yang, Zhengyang Zhou, Xu Wang, Pengkun Wang, Limin Li, and Yang Wang. Raye-sub: Countering subgraph degradation via perfect reconstruction.   \n[60] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824\u20135836, 2020.   \n[61] Xingtong Yu, Chang Zhou, Yuan Fang, and Xinming Zhang. Multigprompt for multi-task pre-training and prompting on graphs. arXiv preprint arXiv:2312.03731, 2023.   \n[62] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng. Fraudre: Fraud detection dual-resistant to graph inconsistency and imbalance. In 2021 IEEE International Conference on Data Mining (ICDM), pages 867\u2013876. IEEE, 2021.   \n[63] Ge Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su, Chuan Zhou, Quan Z Sheng, Leman Akoglu, et al. Dual-discriminative graph neural network for imbalanced graph-level anomaly detection. Advances in Neural Information Processing Systems, 35:24144\u2013 24157, 2022.   \n[64] Muhan Zhang. Graph neural networks: link prediction. Graph Neural Networks: Foundations, Frontiers, and Applications, pages 195\u2013223, 2022.   \n[65] Haihong Zhao, Bo Yang, Jiaxu Cui, Qianli Xing, Jiaxing Shen, Fujin Zhu, and Jiannong Cao. Effective fault scenario identification for communication networks via knowledge-enhanced graph neural networks. IEEE Transactions on Mobile Computing, 23(4):3243\u20133258, 2023.   \n[66] Lingxiao Zhao and Leman Akoglu. On using classification datasets to evaluate graph outlier detection: Peculiar observations and new insights. Big Data, 11(3):151\u2013180, 2023.   \n[67] Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao. Addgraph: Anomaly detection in dynamic graph using attention-based temporal gcn. In IJCAI, pages 4419\u20134425, 2019.   \n[68] Yun Zhu, Jianhao Guo, and Siliang Tang. Sgl-pt: A strong graph learner with graph prompt tuning. arXiv preprint arXiv:2302.12449, 2023.   \n[69] Chenyi Zi, Haihong Zhao, Xiangguo Sun, Yiqing Lin, Hong Cheng, and Jia Li. Prog: A graph prompt learning benchmark. arXiv preprint arXiv:2406.05346, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 The proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. For a new node $v_{n e w}$ , let $S^{\\prime}$ be the subgraph after the addition of $v_{n e w}$ . Based on the fact that the definition of Rayleigh quotient $\\begin{array}{r}{\\boldsymbol{R Q}(\\mathcal{S})=\\frac{\\dot{\\sum}_{(p,q)\\in\\mathcal{E}_{\\mathcal{S}}}(x_{p}-x_{q})^{2}}{\\sum_{p\\in\\mathcal{S}}x_{p}^{2}}}\\end{array}$ , it need to satisfy the following condition in order to increase the Rayleigh quotient $R Q(S^{\\prime})>R Q(S)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}+\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w}-x_{r})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}+x_{n e w}^{2}}>\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w}-x_{r})^{2}}\\end{array}$ represents the sum of the feature difference between the new node $v_{n e w}$ and the connecting edge of the node $v$ in the subgraph $\\boldsymbol{S}$ . It is worth noting that these edges are present in the original graph $\\mathcal{G}$ . Since both the numerator and denominator are positive numbers, the Eq. (A.1) can be transformed into: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}+\\sum_{p\\in\\mathcal{S}}(x_{n e w}-x_{p})^{2}\\right]\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}>\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}(\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}+x_{n e w}^{2}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which can be obviously simplified to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{v_{r}\\in S}(x_{n e w}-x_{r})^{2}\\sum_{v_{r}\\in S}x_{r}^{2}>x_{n e w}^{2}\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We move the term with the new node to the same side of the equation and rearrange the Eq. (A.3), and we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w}-x_{r})^{2}}{x_{n e w}^{2}}>\\frac{\\sum_{(p,q)\\in\\mathcal{E}_{\\mathcal{S}}}(x_{p}-x_{q})^{2}}{\\sum_{p\\in\\mathcal{S}}x_{p}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\begin{array}{r}{R Q(S)\\,=\\,\\frac{\\sum_{(p,q)\\in\\mathcal{E}_{S}}{(x_{p}-x_{q})^{2}}}{\\sum_{p\\in\\mathcal{S}}{x_{p}^{2}}}}\\end{array}$ , we denote $\\begin{array}{r}{\\Delta(v_{n e w})\\:=\\:\\frac{\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w}-x_{r})^{2}}{x_{n e w}^{2}}}\\end{array}$ and we then obtain the Theorem 1 in Section 3.1.2. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.2 The proof of Corollary 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 1, for a new nodeset $v_{n e w_{i}}\\in\\mathcal{V}_{n e w}$ , let $S^{\\prime}$ be the subgraph after the addition of $\\nu_{n e w}$ and it also needs to satisfy $R Q(S^{\\prime})>\\dot{R Q}(S)$ , which is expanded as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}+\\sum_{v_{n e w_{i}}\\in\\mathcal{V}_{n e w_{i}}}\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w_{i}}-x_{r})^{2}+\\sum_{(i,j)\\in\\mathcal{E}_{n e w}}(x_{n e w_{i}}-x_{n e w_{j}})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}+\\sum_{v_{n e w_{i}}\\in\\mathcal{V}_{n e w}}x_{n e w_{i}}^{2}}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,>\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\sum_{v_{n e w_{i}}\\in\\mathcal{V}_{n e w_{i}}}\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w_{i}}-x_{r})^{2}}\\end{array}$ represents the sum of the feature difference between the newly added nodeset Vnew and the connecting edge of the subgraph S.  vnewi\u2208Vnew newi represents the internal sum of the newly added nodeset $\\nu_{n e w}$ . Similar to the proof of Theorem 1, this formula can be transformed into: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{n\\in w_{i}\\in\\mathcal{V}_{n e w_{i}}}\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w_{i}}-x_{r})^{2}+\\sum_{(i,j)\\in\\mathcal{E}_{n e w}}(x_{n e w_{i}}-x_{n e w_{j}})^{2}\\right]\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}>\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}\\sum_{v_{n e w_{i}}\\in\\mathcal{V}_{n e w_{i}}}x_{n e w_{j}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging the Eq. (A.6), we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\sum_{v_{n e w}\\in\\mathcal{V}_{n e w}}\\sum_{v_{r}\\in\\mathcal{S}}(x_{n e w}-x_{r})^{2}+\\sum_{(i,j)\\in\\mathcal{E}_{\\mathcal{V}_{n e w}}}(x_{n e w_{i}}-x_{n e w_{j}})^{2}}{\\sum_{v_{n e w}\\in\\mathcal{V}_{n e w}}x_{n e w}^{2}}>\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is the same as Corollary 1 in Section 3.1.2. ", "page_idx": 14}, {"type": "text", "text": "A.3 The proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We define the nodeset $\\mathcal{V}_{n e w}^{*}$ has the highest $\\Delta_{m a x}(\\mathcal{V}_{n e w})$ and $\\Delta_{m a x}(\\mathcal{V}_{n e w})>R Q(S)$ . To prove that the $\\nu_{n e w}^{*}$ is contained in the optimal subgraph $S^{*}$ , we give the proof by contradiction. Assume that the negation of the statement is true, so there does not exist $\\mathcal{V}_{n e w}^{*}$ in $S^{*}$ . We will discuss the issues based on two scenarios. ", "page_idx": 15}, {"type": "text", "text": "In the first scenario, we assume that the current subgraph $\\boldsymbol{S}$ is already the optimal solution. According to Corollary 1, we find that adding $\\mathcal{V}_{n e w}^{*}$ can increase $R Q(S)$ since it satisfies the condition $\\bar{\\Delta_{m a x}}(\\mathcal{V}_{n e w})>R Q(S)$ . Therefore, it is obvious that the current set $\\boldsymbol{S}$ is not the optimal solution. In the other scenario, we assume that there is another nodeset $\\mathcal{V}_{n e w}^{\\prime}\\left(\\,\\mathcal{V}_{n e w}^{\\prime}\\cap S^{*}=\\emptyset\\right)$ , which together with the current subgraph $\\mathcal{S}+\\mathcal{V}_{n e w}^{\\prime}$ forms the optimal solution. According to the corollary 1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{m a x}(\\mathcal{V}_{n e w}^{*})=\\frac{\\sum_{\\gamma_{n e w}^{*}}\\sum_{S}(x_{n e w}^{*}-x_{r})^{2}+\\sum_{\\mathcal{E}_{\\gamma_{n e w}^{*}}}(x_{n e w_{i}}^{*}-x_{n e w_{j}}^{*})^{2}}{\\sum_{\\gamma_{n e w}^{*}}x_{n e w}^{*}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and it satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\Delta_{m a x}(\\mathcal{V}_{n e w}^{*})>\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}},\\right.}\\\\ {\\left.\\Delta_{m a x}(\\mathcal{V}_{n e w}^{*})>\\frac{\\sum_{v_{n e w}^{\\prime}}\\sum_{S}(x_{n e w}^{\\prime}-x_{r})^{2}+\\sum_{\\mathcal{E}_{\\mathcal{V}_{n e w}^{\\prime}}}(x_{n e w_{i}^{\\prime}}-x_{n e w_{j}^{\\prime}})^{2}}{\\sum_{v_{n e w}^{\\prime}}x_{n e w}^{\\prime}^{\\prime}},\\forall\\gamma_{n e w}^{\\prime}\\subseteq\\mathcal{E}-S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To continue with the proof, we present a useful inequality first. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Dan\u2019s Favorite Inequality). Let $a_{1},...,a_{n}$ and $b_{1},...,b_{n}$ be positive numbers. Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i}\\frac{a_{i}}{b_{i}}\\leq\\frac{\\sum_{i}a_{i}}{\\sum_{i}b_{i}}\\leq\\operatorname*{max}_{i}\\frac{a_{i}}{b_{i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Here we give a classical proof, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i}a_{i}=\\sum_{i}b_{i}\\left(\\frac{a_{i}}{b_{i}}\\right)\\leq\\sum_{i}b_{i}\\left(\\operatorname*{max}_{j}\\frac{a_{j}}{b_{j}}\\right)=\\left(\\operatorname*{max}_{j}\\frac{a_{j}}{b_{j}}\\right)\\sum_{i}b_{i},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sum a_{i}}{\\sum b_{i}}\\leq\\operatorname*{max}_{j}\\frac{a_{j}}{b_{j}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "One can similarly prove ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\sum a_{i}}{\\sum b_{i}}\\geq\\operatorname*{min}_{j}\\frac{a_{j}}{b_{j}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Lemma 3 and Eq. (A.9), we obtain the following inequality. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{m a x}(\\mathcal{V}_{n e w}^{*})>}\\\\ &{\\frac{\\sum_{(p,q)\\in\\mathcal{E}}(x_{p}-x_{q})^{2}+\\sum_{\\mathcal{V}_{n e w}^{\\prime}}\\sum_{\\mathcal{S}}(x_{n e w}^{\\prime}-x_{r})^{2}+\\sum_{\\mathcal{E}_{\\mathcal{V}_{n e w}^{\\prime}}}(x_{n e w_{i}}^{\\prime}-x_{n e w_{j}}^{\\prime})^{2}}{\\sum_{v_{r}\\in\\mathcal{S}}x_{r}^{2}+\\sum_{\\mathcal{V}_{n e w}^{\\prime}}x_{n e w}^{\\prime}^{2}},\\forall\\gamma_{n^{\\prime}\\!\\:e w}^{\\prime}\\!\\subseteq\\!\\mathcal{G}\\!-\\!\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Analyzing the above equation reveals that the right side of the formula is $R Q(\\mathcal{V}_{n e w}^{\\prime}+\\mathcal{S})$ . That is, for any $\\mathcal{V}_{n e w}^{\\prime}$ , adding $\\mathcal{V}_{n e w}^{*}$ still makes $R Q(\\mathcal{V}_{n e w}^{\\prime}\\,\\bar{+}\\,\\mathcal{S})$ increasing according to the Corollary 1, which contradicts the assumption that $R Q(\\mathcal{V}_{n e w}^{\\prime}+\\mathcal{S})$ is the optimal solution. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{m a x}(\\mathscr{V}_{n e w})=\\operatorname*{max}_{\\mathscr{V}_{n e w}\\subseteq\\mathscr{G}-\\mathscr{S}}\\Delta(\\mathscr{V}_{n e w}),\\;\\mathrm{and}\\;\\Delta_{m a x}(\\mathscr{V}_{n e w})>R Q(\\mathscr{S}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "However, identifying the maximum $\\Delta_{m a x}(\\gamma_{n e w})$ from the $\\mathcal{V}_{n e w}\\subseteq\\mathcal{G}-\\mathcal{S}$ is still a NP-hard problem. We consider relaxing any nodesets $\\mathcal{V}_{n e w}^{\\prime}$ to any connected nodesets $\\mathcal{V}_{n e w}^{c}$ . Any nodesets can be decomposed into several disconnected smaller nodesets, that is, $\\mathcal{V}_{n e w}^{\\prime}=\\overset{\\cdot}{\\mathcal{V}}_{n e w}^{c1}\\cup\\mathcal{V}_{n e w}^{c2}\\cup....$ Since there are no edges connecting these nodesets, the following decomposition formula can be derived. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\sum_{\\gamma_{n e w}^{\\prime}}\\sum_{S}(x_{n e w}^{\\prime}-x_{r})^{2}=\\sum_{\\gamma_{n e w}^{\\prime1}}\\sum_{S}(x_{n e w}^{c1}-x_{r})^{2}+\\sum_{\\gamma_{n e w}^{\\prime2}}\\sum_{S}(x_{n e w}^{c2}-x_{r})^{2}+\\dots,\\right.}\\\\ {\\left\\{\\sum_{\\xi_{\\gamma_{n e w}^{\\prime}}}(x_{n e w_{i}}^{\\prime}-x_{n e w_{j}}^{\\prime})^{2}=\\sum_{\\xi_{\\gamma_{n e w}^{\\prime1}}}(x_{n e w_{i}}^{c1}-x_{n e w_{j}}^{c1})^{2}+\\sum_{\\xi_{\\gamma_{n e w}^{\\prime2}}}(x_{n e w_{i}}^{c2}-x_{n e w_{j}}^{c2})^{2}+\\dots,\\right.}\\\\ {\\left.\\sum_{\\gamma_{n e w}^{\\prime}}x_{n e w}^{\\prime}\\right.^{2}=\\sum_{\\gamma_{n e w}^{\\prime1}}x_{n e w}^{c1}+\\sum_{\\gamma_{n e w}^{\\prime2}}x_{n e w}^{c2}\\overset{2}{=}+\\dots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Considering the condition that maximizes the Rayleigh quotient of any connected $\\mathcal{V}_{n e w}^{c i}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{m a x}(\\gamma_{n e w}^{*})>\\Delta_{m a x}(\\gamma_{n e w}^{c i})=\\frac{\\sum_{\\gamma_{n e w}^{c i}}\\sum_{S}(x_{n e w}^{c i}-x_{r})^{2}+\\sum_{\\xi_{\\gamma_{n e w}^{c i}}}(x_{n e w_{i}}^{c i}-x_{n e w_{j}}^{c i})^{2}}{\\sum_{\\gamma_{n e w}^{c i}}x_{n e w}^{c i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "According to Lemma 3, Eq. (A.14) is still satisfied. Therefore, we derive that $\\nu_{n e w}^{*}$ is contained in the optimal solution. ", "page_idx": 16}, {"type": "text", "text": "B The Pseudocode of MRQSampler Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We give the pseudocode of MRQSampler in Algorithm 1, which illustrates the algorithm for finding the subgraph with the target node that maximizes the Rayleigh Quotient. In section 3.1.2, we give a diagram of the sampling range of 2-hop and 1-hop cases. For the completeness of the theory, we give the complete algorithm for arbitrary $k$ -hop cases in the pseudocode form. ", "page_idx": 16}, {"type": "text", "text": "For node $r$ , we focus on the $k$ -hop spanning tree $T$ with $r$ as the root node. And for any node $v$ in $T$ except for the root $r$ , $\\Delta_{m a x}[v]$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{m a x}[v]:=\\operatorname*{max}_{S\\subseteq\\mathcal{T}_{v}}\\frac{\\left(x_{i}-x_{p}\\right)^{2}+\\sum_{(i,j)\\subseteq\\varepsilon_{S}}\\left(x_{i}-x_{j}\\right)^{2}}{\\sum_{i\\subseteq S}x_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ${\\mathcal{S}}\\subseteq{\\mathcal{T}}_{v}$ are the connected subgraphs of the subtree $T_{v}$ with $v$ as the root node, and $p$ is the parent node of the node $v$ . As described in the Section 3.1.2, we break the computation into two steps: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Stage 1: Compute and store the maximum $\\Delta_{m a x}[v]$ for subtrees rooted with each node $v$ except for the root $r$ , which is performed by recursively calling the function GetMaxDeltas in Algorithm 1. \u2022 Stage 2: Use these memorized results to compute the optimal Rayleigh Quotient $R Q$ and its corresponding subgraph, which is performed by the function MRQSampler. ", "page_idx": 16}, {"type": "text", "text": "In Stage 1, the first thing we need to know is how we get $\\Delta_{m a x}[v]$ . Similar to the analysis of the Theorem 2, we can also obtain the condition that the nodeset is in the final optimal subgraph with largest $\\Delta_{m a x}[v]$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{m a x}[v_{n e w}]=\\operatorname*{max}_{\\{\\tilde{v}_{n e w}\\}}\\Delta(\\tilde{\\mathcal{V}}_{n e w}),\\;\\mathrm{and}\\;\\Delta_{m a x}[v_{n e w}]>\\Delta[v].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This process is similar to finding the maximum $R Q$ . In other words, we keep retrieving the unselected descendants with maximum $\\Delta_{m a x}[v_{n e w}]$ , and then check whether its $\\Delta_{m a x}[v_{n e w}]$ exceeds the current $\\Delta[v]$ . If it does, the inclusion of the optimal subgraph with it can increase the current $\\Delta[v]$ , otherwise, it can no longer be increased by adding any descendants and the maximum $\\Delta_{m a x}[v]$ is reached. ", "page_idx": 16}, {"type": "text", "text": "Globals: $r:$ the original root of the tree; $i:$ an arbitrary node; $x[i]$ : the node $i$ \u2019s feature; $T_{r}[i]$ : an array that stores the child nodes of node $i$ in the tree; $\\delta[i]\\leftarrow\\{\\bar{\\Delta_{m a x}}[i],a_{i},b_{i},N,I\\}$ : an array of structures that stores the maximum $\\Delta_{m a x}[i]$ achievable by any connected subgraph $\\mathcal{V}$ containing the node $i$ within the subtree rooted at $i$ and $a_{i},b_{i}$ stores the numerator and denominator of $\\Delta_{m a x}[i]$ . $N$ is the optimal selected nodes, $I$ is the inferior candidates, ", "page_idx": 17}, {"type": "text", "text": "1: # The function for computing $\\Delta_{m a x}[i]$ in STAGE I   \n2: Input: $v\\leftarrow$ root of the current subtree; $p\\leftarrow$ parent of $v$   \n3: Output: $\\delta[i]\\leftarrow$ structure array with $\\Delta_{m a x}[i]$ and correlated variables   \n4: function GetMaxDeltas $(v,p)$   \n5: $N,I,U\\gets\\{\\}$ $\\triangleright$ Optimal selected nodes, Inferior candidates, Un-selected children of $v$   \n6: Q \u2190SortedSet() $\\triangleright$ Candidates queue   \n7: av \u2190(x[v] \u2212x[p])2 \u25b7Initialize the numerator of the maximum $\\bar{\\Delta_{m a x}[v]}$   \n8: bv \u2190x[v]2 $\\triangleright$ Initialize the denominator of the maximum $\\Delta_{m a x}[v]$   \n9: $\\Delta_{m a x}[v]\\gets a_{v}/b_{v}$ $\\triangleright$ Initialize the maximum $\\Delta_{m a x}[v]$ for current sub-tree   \n10: for $c$ in $T[v]$ do   \n11: $\\delta[c]\\gets\\mathtt{G e t M a x D e l t a s}(c,v)$ \u25b7Result of the subtree rooted with child $c$   \n12: Q.insert([c, \u03b4[c]])   \n13: U.insert $(c)$   \n14: while $Q.\\mathtt{s i z e}()\\neq0$ do   \n15: c $\\delta\\lceil c\\rceil\\leftarrow Q.\\mathtt{p o p\\_l a r g e s t}()\\triangleright\\mathrm{Ret}$ rieve the candidate $c$ with $\\Delta_{m a x}[c]$ and structure $\\delta[c]$   \n16: if $\\dot{\\Delta_{m a x}}[v]\\stackrel{*}{>}\\bar{(}a_{v}+\\bar{\\delta}[c].a_{c})/(b_{v}+\\delta[c].b_{c})$ then $\\triangleright$ Optimization criterion of $\\Delta_{m a x}[v]$   \n17: Break   \n18: $U$ .remove_if_exist(c)   \n19: av \u2190av + \u03b4[c].ac \u25b7Update the maximum $\\Delta_{m a x}[v]$   \n20: bv \u2190bv + \u03b4[c].bc   \n21: \u2206max[v] \u2190av/bv   \n22: Q.insert(\u03b4[c].I) $\\triangleright$ Activate the inferior candidates   \n23: N.insert(\u03b4[c].N)   \n24: I \u2190Q $\\triangleright$ The remaining candidates are the inferior ones   \n25: I.insert(U) $\\triangleright$ Add the un-selected children to the inferior set   \n26: $\\begin{array}{r l}&{\\delta[v]\\gets\\{\\Delta_{m a x}[v],a_{v},b_{v},N,I\\}}\\\\ &{\\mathbf{return}\\;\\delta[v]}\\end{array}$ $\\triangleright$ Memorise the results   \n27:   \n28:   \n29: # The main function of MRQSampler in STAGE II   \n30: Input: $r\\gets$ the original root of the tree (target sampling node)   \n31: Output: $R Q_{m a x}\\gets$ maximum $R Q$ ; $N\\gets$ optimal sampling nodeset   \n32: function MRQSampler $(r)$   \n33: aRQ \u21900 $\\triangleright$ Initialize the numerator of the $R Q_{m a x}$   \n34: bRQ \u2190x[r]2 $\\triangleright$ Initialize the denominator of the $R Q_{m a x}$   \n35: $R Q_{m a x}\\leftarrow a_{R Q}/b_{R Q}$   \n36: Q \u2190SortedSet()   \n37: for $c$ in $T[r]$ do   \n38: \u03b4[c] \u2190GetMaxDeltas(c, r) \u25b7Recursively calculate the $\\Delta_{m a x}$ in Stage 1   \n39: Q.insert([c, \u03b4[c]])   \n40: while $Q.\\mathtt{s i z e}()\\neq0$ do   \n41: c $,\\delta[c]\\gets Q$ .pop_largest()   \n42: if $\\bar{R Q}_{m a x}>(a_{R Q}+\\delta[c].a_{c})/(b_{R Q}+\\delta[c].b_{c})$ then $\\triangleright$ Optimization criterion of the RQ   \n43: Break   \n44: aRQ \u2190aRQ + \u03b4[c].ac $\\triangleright$ Update the result   \n45: bRQ \u2190bRQ + \u03b4[c].bc   \n46: RQmax \u2190aRQ/bRQ   \n47: Q.insert(\u03b4[c].I) $\\triangleright$ Activate the inferior candidates   \n48: N.insert(\u03b4[c].N) \u25b7Update the selected nodeset   \n49: return {RQmax, N} ", "page_idx": 17}, {"type": "text", "text": "For ease of computation, we store the optimal $\\Delta_{m a x}[v]$ by its numerator $a_{v}$ and denominator $b_{v}$ (line 7-9). Next, we recursively calculate the result for each subtree rooted by every child $c$ of the current root $v$ (line 11). To simplify complexity in the following steps, we store these results in a sorted container (e.g. binary search tree) $Q$ (line 12). Next, we keep retrieving the subgraph with the highest $\\Delta_{m a x}[c]$ from $Q$ and compute whether the $\\Delta_{m a x}[c]$ increases after adding it to the current solution (line 15-17). According to Eq. (A.19), we obtain the optimal $\\Delta_{m a x}[v]$ for the current subtree with root $v$ when the candidate cannot make $\\Delta_{m a x}[v]$ larger. Moreover, sets from subtrees that are not optimal for $v$ may still be selected at higher levels. Therefore, we also need to keep track of those inferior subtrees and re-consider them when other subtrees that connect to them are merged into the solution (line 24-25). Note that subtrees in $I$ are only considered as candidates when the optimal subgraph with root $v$ is selected at a higher layer (the \u201cactivation\u201d in line 22). ", "page_idx": 18}, {"type": "text", "text": "In Stage 2, the overall routine for obtaining $R Q_{m a x}$ is very similar to the one in GetMaxDeltas, except that the initial value is set to $\\begin{array}{r}{R Q_{m a x}=\\frac{0}{x[r]^{2}}}\\end{array}$ since the root $r$ has no parent node. In other words, the algorithmic logic of the two functions in Stage 1 and Stage 2 is similar. Stage 2 can be regarded as a special case of Stage 1 without a parent node, utilizing the implementation of memoization from Stage 1. ", "page_idx": 18}, {"type": "text", "text": "Assuming that the $\\mathbf{k}$ -hop spanning tree $T$ has $K$ nodes, the time complexity of Algorithm $O(K l o g K)$ , since we will at worst examine each edge and sort them. Notice that the computation is irrelevant between different nodes, it can be further accelerated by simultaneously processing multiple nodes. In practice, we observe that the optimal choice of $\\boldsymbol{\\mathrm{k}}$ -hop is typically $<=2$ , and thus the recursive computation can be unrolled thus further improving the efficiency. ", "page_idx": 18}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Node-level Baselines. GCN (Graph Convolutional Network [24]) leverages convolution operations to propagate information from nodes to their neighbors. SGC (Simple Graph Convolution [56]) further simplifies GCN by removing non-linearities and collapsing weight matrices between consecutive layers to improve efficiency. GIN (Graph Isomorphism Network [57]) captures graph structures by generating identical embeddings for structurally identical graphs, ensuring invariance to node label permutations. GraphSAGE (Graph Sample and AggregatE [16]) generates node embeddings through sampling and aggregating features from local neighborhoods, supporting inductive learning. GAT (Graph Attention Networks [52]) incorporates an attention mechanism to assign varying importance levels to different nodes during neighborhood aggregation, focusing on the most informative parts. PNA (Principle Neighbor Aggregation [7]) combines multiple aggregators with degree-scalers for effective neighborhood aggregation. AMNet (Adaptive Multi-frequency Graph Neural Network [5]) captures both low and high-frequency signals by stacking multiple BernNets [18], adaptively combining signals of different frequencies. BWGNN (Beta Wavelet Graph Neural Network [50]) employs the Beta kernel to tackle higher frequency anomalies with flexible band-pass filters. ", "page_idx": 18}, {"type": "text", "text": "Graph-level Baselines. OCGIN [66] is a one-class graph-level anomaly detector based on a graph isomorphism network that addresses performance fluctuations in general graph classification methods. OCGTL [46] combines deep one-class classification with graph transformation learning. GlocalKD learns rich global and local normal pattern information by joint distillation of graph and node representations. iGAD [63] employs an attribute-aware GNN and a substructure-aware deep random walk kernel to achieve dual-discriminative capability for anomalous attributes and substructures. GmapAD [44] proposes an explainable graph mapping approach that projects graphs into a latent space for effective anomaly detection. RQGNN [11] identifies differences in the spectral energy distributions between anomalous and normal graphs. ", "page_idx": 18}, {"type": "text", "text": "Multi-task Baselines. GraphPrompt [37] learns different task-specific prompt vectors for each task, which are added to the graph-level representations by element-wise multiplication. All-in-One [48] treats an extra subgraph as a prompt and merges it with the original graph by cross links. ", "page_idx": 18}, {"type": "text", "text": "Hardware Specifications. Our experiments were mainly carried out on a Linux server equipped with dual AMD EPYC 7763 64-core CPU processor, 256GB RAM, and an NVIDIA RTX 4090 GPU with 24GB memory. Some of the extremely large datasets, such as T-Finance, and certain memory-intensive baselines were implemented on the NVIDIA $8^{*}\\mathrm{A}800$ GPUs. We mark the results as OOT (Out of Time) if the model training exceeds 2 days. For some large datasets, methods with GPU memory requirements exceeding 80GB were marked as OOM (Out of Memory), such as iGAD and GmapAD, and were attempted to be run on the CPU. iGAD was successfully completed, but GmapAD still encountered a timeout issue. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Metrics. We utilize three widely used metrics to evaluate the performance of all methods: F1- macro, AUROC and AUPRC. F1-macro is the unweighted mean of the F1-scores for the two classes, disregarding the imbalance ratio between normal and anomaly labels. AUROC represents the area under the Receiver Operating Characteristic Curve. AUPRC represents the area under the Precision-Recall Curve, emphasizing model performance on imbalanced datasets by focusing on the trade-off between precision and recall. ", "page_idx": 19}, {"type": "text", "text": "Hyperparameter Tuning. As described in Section 3.2, we first use an unsupervised model based on GraphMAE [21] to learn the general representation of the input features. The hyperparameters for this step are set to the default values from the official GraphMAE implementation, with 50 training epochs. Table 7 lists all the hyperparameters used in our model along with their corresponding search spaces. During training, we conduct a grid search to identify the model that achieves the highest AUROC score on the validation set. Finally, we evaluate the selected model on the test sets and report the performance metrics. ", "page_idx": 19}, {"type": "table", "img_path": "sRILMnkkQd/tmp/a76d9537e246647fdc438f6674f68695e3f94b7f36a2ac36fcc98d03c1c8ba36.jpg", "table_caption": ["Table 7: Hyperparameters search space for UniGAD. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Limitations and Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since the GNN encoder we employ mainly focuses on node-level features, the learned representations may not be perfectly suited for edge and graph level tasks. Therefore, we leave the exploration of how to integrate multiple tasks in the pre-training phase to future work. As a generalized graph anomaly detection model, our work will be helpful in detecting classical graph anomaly applications, such as financial fraud, cybercrime, etc. On the other hand, an error in the recognition result may put normal groups or behaviors into anomalies, causing disturbance for the normal users in the graph network. ", "page_idx": 19}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We also provide the results of all experiments under the F1-macro and AUPRC evaluation metrics. Similar to the arrangement in the main text, for F1-macro, we show the results of multi-level performance comparison under F1-macro metric in Table 8 and Table 9. The results of Zero-Shot Comparison under F1-macro metric are in Table 10 and Table 11. For AUPRC, we show the results of multi-level performance comparison under AUPRC metric in Table 12 and Table 13. The results of Zero-Shot Comparison under AUPRC metric are in Table 14 and Table 15. It can be observed from these tables that similar conclusions can be drawn as with the AUROC results in Section 4. UniGAD demonstrates superior performance across most datasets, regardless of whether unified or zero-shot performance is evaluated. ", "page_idx": 19}, {"type": "table", "img_path": "sRILMnkkQd/tmp/74292984332c497e2d70d5065bc9f9d2a5742bc73e0c2799b54013538b19a8f8.jpg", "table_caption": ["Table 8: Comparison of unified performance (F1-macro) at both node and edge levels with different single-level methods, multi-task methods, and our proposed method. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 9: Comparison of unified performance (F1-macro) at both node and graph levels with different single-level methods, multi-task methods, and our proposed method. ", "page_idx": 20}, {"type": "table", "img_path": "sRILMnkkQd/tmp/2819d125e0f113cce525512edc932cde438fdc87397c5539afbc6975720a8728.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "sRILMnkkQd/tmp/53479b24fad38776186aab4db91b488a2f76f7e5c4ab6b1202bc7526535edf66.jpg", "table_caption": ["Table 10: Zero-shot transferability (F1-macro) at node and edge levels. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "sRILMnkkQd/tmp/175ccda8e080eec729b2b6fff665b188eb4d4a45154b7505d140b4fc1357eff6.jpg", "table_caption": ["Table 11: Zero-shot transferability (F1-macro) at node and graph levels. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "sRILMnkkQd/tmp/cac7f156e6a7ace503814f970c85a7eca6b0d3e72628a364b8d620f207d3527e.jpg", "table_caption": ["Table 12: Comparison of unified performance (AUPRC) at both node and edge levels with different single-level methods, multi-task methods, and our proposed method. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 13: Comparison of unified performance (AUPRC) at both node and graph levels with different single-level methods, multi-task methods, and our proposed method. ", "page_idx": 21}, {"type": "table", "img_path": "sRILMnkkQd/tmp/3af8424d8f0c43dfaab2a2a5e0d069b7de3e0594230e82325e4cc94a749314c7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "sRILMnkkQd/tmp/5f5306bb338f581ec9da4bce745291711f11b24e33643aae06bb1d7ccfcfaa18.jpg", "table_caption": ["Table 14: Zero-shot transferability (AUPRC) at node and edge levels. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "sRILMnkkQd/tmp/ead83534f776590815903c56629e9957dddd10e5768875f7f15297f652057cf9.jpg", "table_caption": ["Table 15: Zero-shot transferability (AUPRC) at node and graph levels "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We confirm that the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix D. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We provide a complete proof of our theoretical results (Theorem 1, Corollary 1, Theorem 2) in Appendix A.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a link to the code in the abstract and include detailed implementation information in Appendix C to enhance reproducibility. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We make the code of our model and newly released T-Group dataset opensourced at https://anonymous.4open.science/r/UniGAD-A087/. Other datasets are used only publicly available datasets as stated in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Appendix C for the experiment implementation details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All of our experimental results come from the mean of 5 randomized trials. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Appendix C for the computation resources. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We confirmed the research conducted in the paper conform with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please refer to Appendix D for the relative discussion. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We properly credit all referenced baseline works and datasets in Section 4 and Appendix C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the code via the anonymous link in the abstract, which will be open-sourced under the MIT license. Comprehensive documentation is included with the assets to ensure ease of use and understanding. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]