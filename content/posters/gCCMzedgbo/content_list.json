[{"type": "text", "text": "TrAct: Making First-layer Pre-Activations Trainable ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felix Petersen Christian Borgelt Stefano Ermon Stanford University University of Salzburg Stanford University mail@felix-petersen.de christian@borgelt.net ermon@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the training of the first layer of vision models and notice the clear relationship between pixel values and gradient update magnitudes: the gradients arriving at the weights of a first layer are by definition directly proportional to (normalized) input pixel values. Thus, an image with low contrast has a smaller impact on learning than an image with higher contrast, and a very bright or very dark image has a stronger impact on the weights than an image with moderate brightness. In this work, we propose performing gradient descent on the embeddings produced by the first layer of the model. However, switching to discrete inputs with an embedding layer is not a reasonable option for vision models. Thus, we propose the conceptual procedure of (i) a gradient descent step on first layer activations to construct an activation proposal, and (ii) finding the optimal weights of the first layer, i.e., those weights which minimize the squared distance to the activation proposal. We provide a closed form solution of the procedure and adjust it for robust stochastic training while computing everything efficiently. Empirically, we find that TrAct (Training Activations) speeds up training by factors between $1.25\\times$ and $4\\times$ while requiring only a small computational overhead. We demonstrate the utility of TrAct with different optimizers for a range of different vision models including convolutional and transformer architectures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the learning of first-layer embeddings / pre-activations in vision models, and in particular learning the weights with which the input images are transformed in order to obtain these embeddings. In gradient descent, the updates to first-layer weights are directly proportional to the (normalized) pixel values of the input images. As a consequence (assuming that input images are standardized), high contrast, very dark, or very bright images have a greater impact on the trained first-layer weights, while low contrast images with medium brightness have only smaller impact on training. ", "page_idx": 0}, {"type": "text", "text": "While, in the past, mainly transformations of the input images, especially various forms of normalization have been considered, either as a preprocessing step or as part of the neural network architecture, our approach targets the training process directly without modifying the model architecture or any preprocessing. The goal of our approach is to achieve a training behavior that is equivalent to training the pre-activations or embedding values themselves. For example, in language models [1], the first layer is an \u201cEmbedding\u201d layer that maps a token id to an embedding vector (via a lookup). When training language models, this embedding vector is trained directly, i.e., the update to the embedding directly corresponds to the gradient of the pre-activation of the first layer. As discussed above, this is not the case in vision models as, here, the updates to the first-layer weight matrix correspond to the outer product between the input pixel values and the gradient of the pre-activation of the first layer. Bridging this gap between the \u201cEmbedding\u201d layer in language models, and \u201cConv2D\u201d / \u201cLinear\u201d / \u201cDense\u201d layers in vision models, we propose a novel technique for training the pre-activations of the latter, effectively mimicking training behavior of the \u201cEmbedding\u201d layer in language models. As vision models rely on pixel values rather than tokens, and any discretization of image patches, e.g., via clustering is not a reasonable option, we approach the problem via a modification of the gradient (and therefore a modification of the training behavior) without modifying the original model architectures. We illustratively compare the updates in language and vision models and demonstrate the modification that TrAct introduces in Figure 1. ", "page_idx": 0}, {"type": "image", "img_path": "gCCMzedgbo/tmp/eefdae87bc505f503afaffd582ffa0bd97349e54689885502188abb91ade89b1.jpg", "img_caption": ["Figure 1: TrAct learns the first layer of a vision model but with the training dynamics of an embedding layer. We illustrate this in an example with two 4-dimensional inputs $_x$ , a weight matrix $W$ of size $4\\times3$ , and resulting pre-activations $z$ of size $2\\times3$ . For language models (left), the input $_x$ is two tokens from a dictionary of size 4. For vision models (center $^+$ right), the input $_x$ is two patches of the image, each totaling 4 pixels. During backpropagation, we obtain the gradient wrt. our pre-activations $\\nabla z$ , from which the gradient and update to the weights $W$ is computed $(\\Delta W)$ . The resulting update to the pre-activations $\\Delta z$ equals $x^{\\top}\\cdot\\Delta\\dot{W}$ . For language models (left), $\\Delta z=\\nabla z$ , i.e., the training dynamics of the embeddings layer corresponds to updating the embeddings directly wrt. the gradient. Specifically, the update in a language model, for a token identifier $i$ , is $W_{i}\\leftarrow W_{i}-\\bar{\\eta}\\cdot\\nabla_{z}\\mathcal{L}(z)$ where $z=W_{i}$ is the activation of the first layer and at the same time the ith row of the embedding (weight) matrix $W$ . Equivalently, we can write $\\boldsymbol{z}\\gets\\boldsymbol{z}-\\boldsymbol{\\dot{\\eta}}\\cdot\\nabla_{\\boldsymbol{z}}\\mathcal{L}(\\boldsymbol{z})$ . However, in vision models (center), the update $\\Delta z$ strongly deviates from the respective gradients $\\nabla z$ . TrAct corrects for this by adjusting $\\Delta W$ via a corrective term $(x\\cdot x^{\\top}+\\lambda\\cdot I)^{-1}$ (orange box), such that the update to $z$ closely approximates $\\nabla z$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The proposed method is general and applicable to a variety of vision model architecture types, from convolutional to vision transformer models. In a wide range of experiments, we demonstrate the utility of the proposed approach, effectively speeding up training by factors ranging from $1.25\\times$ to $4\\times$ , or, within a given training budget, improving model performance consistently. The approach requires only one hyperparameter $\\lambda$ , which is easy to select, and our default value works consistently well across all 50 considered model architecture $^+$ data set $^+$ optimizer settings. ", "page_idx": 1}, {"type": "text", "text": "The remainder of this paper is organized as follows: in Section 2, we introduce related work, in Section 3, we introduce and derive TrAct from a theoretical perspective, and in Section 3.1 we discuss implementation considerations of TrAct. In Section 4, we empirically evaluate our method in a variety of experiments, spanning a range of models, data sets, and training strategies, including an analysis of the mild behavior of the hyperparameter, an ablation study, and a runtime analysis. We conclude the paper with a discussion in Section 5. The code is publicly available at github.com/Felix-Petersen/tract. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "It is not surprising that the performance of image classification and object recognition models depends heavily on the quality of the input images, especially on their brightness range and contrast. For example, image augmentation techniques generate modified versions of the original images as additional training examples. Some of these techniques work by geometric transformations (rotation, mirroring, cropping), others by adding noise, changing contrast or modifying the image in the color space [2]. In the area of vision transformers [3], [4] so-called 3-augmentation (Gaussian blur, reduction to grayscale, and solarization) has been shown to be essential to performance [5]. Augmentation approaches are similar to image enhancement as a preprocessing step, because they generate possibly enhanced versions of the images as additional training examples, even though they leave the original images unchanged, which are also still used as training examples. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Another direction related to the problem we deal with in this paper are various normalizations and standardizations, starting with the most common one of standardizing the data to mean 0 and standard deviation 1 (over the training set), and continuing through batch normalization [6], weight normalization [7], layer normalization [8], which are usually applied not just for the first layer, but throughout the network, and in particular patch-wise normalization of the input images [9], which we will draw on for comparisons. We note that, e.g., Dual PatchNorm [9], in contrast to our approach, modifies the actual model architecture, but not the gradient backpropagation procedure. ", "page_idx": 2}, {"type": "text", "text": "However, none of these approaches directly addresses the actual concern that weight changes in the first layer are proportional to the inputs, but instead only modify the inputs and architectures to make training easier or faster. In contrast to these approaches, we address the training problem itself and propose a different way of optimizing first-layer weights for unchanged inputs. Of course, this does not mean that input enhancement techniques are superfluous with our method, but only that additional performance gains can be obtained by including TrAct during training. ", "page_idx": 2}, {"type": "text", "text": "In the context of deviating from standard gradient descent-based optimization [10], there are different lines of work in the space of second-order optimization [11], e.g., K-FAC [12], ViViT [13], ISAAC [14], Backpack [15], and Newton Losses [16], which have inspired our methodology for modifying the gradient computation. In particular, the proposed approach integrates second-order ideas for solving a (later introduced) sub\u2013optimization-problem in closed-form [17], and has similarities to a special case of ISAAC [14]. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, let us consider a regular gradient descent of a vision model. Let $z\\,=\\,f(x;W)$ be the first layer embeddings excluding an activation function and $W$ be the weights of this first layer, i.e., for a fully-connected layer $f(\\bar{x};W)=W\\cdot x$ . Here, we have $\\boldsymbol{x}\\in\\mathbb{R}^{n\\times b}$ , $z\\in\\mathbb{R}^{m\\times b}$ , and $\\dot{W}\\in\\mathbb{R}^{m\\times n}$ for a batch size of $b$ . We remark that our input $x$ may be unfolded, supporting convolutional and vision transformer networks. Further, let $\\hat{y}=\\^{\\!}g(\\hat{z};\\theta_{\\setminus W})=g(f(x;W);\\bar{\\theta_{\\setminus W}})$ be the prediction of the entire model. Moreover, let $\\mathcal{L}(\\hat{y},y)$ be the loss function for a label $y$ and wlog. let us assume it is an averaging loss (i.e., reduction over batch dimension via mean). During backpropagation, the gradient of the loss wrt. $z$ , i.e., $\\nabla_{z}\\,\\mathcal{L}(g(z;\\theta_{\\setminus W}),y)$ or $\\nabla_{z}\\mathcal{L}(z)$ for short, will be computed. Conventionally, the gradient wrt. $W$ , i.e., $\\nabla_{W}\\mathcal{L}(g(f(x;W);\\theta_{\\setminus W}),y)$ or $\\nabla_{W}\\mathcal{L}(W)$ for short, is computed during backpropagation as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{W}\\mathcal{L}(W)=\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "leading to the gradient descent update step of ", "page_idx": 2}, {"type": "equation", "text": "$$\nW\\leftarrow W-\\eta\\cdot\\nabla_{W}\\mathcal{L}(W)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Equation 1 clearly shows the direct proportionality between the gradient wrt. the first layer weights and the input (magnitudes), showing that larger input magnitudes produce proportionally larger changes in first layer network weights. We remark that a corresponding relationships also holds in later layers of the neural network, but emphasize that, in later layers, the relationship shows a proportionality to activation magnitude, which is desirable. ", "page_idx": 2}, {"type": "text", "text": "To resolve this dependency on the inputs and make training more efficient, we propose to conceptually optimize in the space of first layer embeddings $z$ . In particular, we could perform a gradient descent step on $z$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nz^{\\star}\\leftarrow z-\\eta\\cdot b\\cdot\\nabla_{z}\\mathcal{L}(z)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $b$ is a multiplier because $\\mathcal{L}(z)$ is (per convention) the empirical expectation over the batch dim. ", "page_idx": 2}, {"type": "text", "text": "However, now, $z^{\\star}$ depends on the inputs and is not part of the actual model parameters. We can resolve this problem by determining how to update $W$ such that $f(x;W)$ is as close to $z^{\\star}$ as possible. Conceptually, we compute the optimal update $\\Delta W^{\\star}$ by solving the optimization problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Delta W}~\\|z^{\\star}-(W+\\Delta W)\\cdot x\\|_{2}^{2}\\qquad\\mathrm{subject~to}\\quad\\|\\Delta W\\|_{2}\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we (1) want to minimize the distance between $z^{\\star}$ the embeddings implied by the change of $W$ by $\\Delta W$ , and (2) want to keep the change $\\Delta W$ small. ", "page_idx": 3}, {"type": "text", "text": "We enforce that weight matrix changes $\\Delta W$ are small $(\\|\\Delta W\\|_{2\\leq\\epsilon})$ by taking the Lagrangian of the problem, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Delta W}\\;\\|z^{\\star}-(W+\\Delta W)\\cdot x\\|_{2}^{2}\\,+\\,\\lambda b\\cdot\\|\\Delta W\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with a heuristically selected Lagrangian multiplier $\\lambda\\cdot b$ (parameterized with $b$ because the first part is also proportional to $b$ ). We simplify Equation 5 to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Delta W}\\;\\|-\\eta b\\cdot\\nabla_{z}\\mathcal{L}(z)-\\Delta W\\cdot x\\|_{2}^{2}\\,+\\,\\lambda b\\cdot\\|\\Delta W\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and ease the presentation by considering it from a row-wise perspective, i.e., for $\\Delta W_{i}\\in\\mathbb{R}^{1\\times n}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Delta W_{i}}\\;\\|\\;{-\\eta\\boldsymbol{b}\\cdot\\nabla_{\\!z_{i}}\\mathcal{L}(z)-\\Delta W_{i}\\cdot\\boldsymbol{x}}\\|_{2}^{2}\\,+\\,\\lambda\\boldsymbol{b}\\cdot\\|\\Delta W_{i}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The problem is separable into a row-wise perspective because the norm $(\\parallel\\cdot\\parallel_{2}^{2})$ is the squared Frobenius norm and the rows have independent solutions. ", "page_idx": 3}, {"type": "text", "text": "In the following, we provide a closed-form solution for optimization problem (7), which is related to [17], [18]. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. The solution $\\Delta W_{i}^{\\star}$ of Equation 7 is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta W_{i}^{\\star}=-\\,\\eta\\cdot\\nabla_{z_{i}}\\mathcal{L}(z)\\cdot\\boldsymbol{x}^{\\top}\\cdot\\biggl(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\biggr)^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof deferred to Supplementary Material A. ", "page_idx": 3}, {"type": "text", "text": "Extending the solution to $\\Delta W$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta W^{\\star}=-\\,\\eta\\cdot\\nabla_{z}\\mathcal{L}(z)\\cdot{x^{\\top}}\\cdot\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and can accordingly use it for an update step for $W$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nW\\leftarrow W+\\Delta W^{\\star}\\qquad\\mathrm{or}\\qquad W\\leftarrow W-\\eta\\cdot\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}\\cdot\\bigg(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\bigg)^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The update in Equation 10 directly inserts the solution of the problem formulated in Equation 4. This computation is efficient as it only requires inversion of an $n\\times n$ matrix, where $n$ in the case of convolutions correspond to 3 (RGB) times the squared first layer\u2019s kernel size, and for vision transformers corresponds to the number of pixels per patch. The values of $n$ typically range from $n=27$ (CIFAR ResNet) to $n=768$ (ImageNet large-scale vision transformer). ", "page_idx": 3}, {"type": "text", "text": "Lemma 2. Using TrAct does not change the set of possible convergence points compared to vanilla (full batch) gradient descent. Herein, we use the standard definition of convergence points as those points where no update is performed because the gradient is zero. ", "page_idx": 3}, {"type": "text", "text": "Proof sketch: First, we remark that only the training of the first layer is affected by TrAct. To show the statement, we show that (i) a zero gradient for GD implies that TrAct also performs no update and that (ii) TrAct performing no update implies zero gradients for GD. Proof deferred to SM A. ", "page_idx": 3}, {"type": "text", "text": "The statement formalizes that TrAct does not change the set of attainable models, but instead only affects the behavior of the optimization itself. ", "page_idx": 3}, {"type": "text", "text": "For an illustration of how TrAct affects updates to $W$ and $z$ , with a comparison to language models and conventional vision models, see Figure 1. ", "page_idx": 3}, {"type": "text", "text": "3.1 Implementation Considerations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To implement the proposed update in Equation 10 in modern automatic differentiation frameworks [19], [20], we can make use of a custom backward or backpropagation for the first layer. ", "page_idx": 3}, {"type": "text", "text": "Standard gradient for the weights: ", "page_idx": 4}, {"type": "text", "text": "For TrAct, we perform an in-place replacement by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{W}\\leftarrow\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{W}\\leftarrow\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}\\cdot\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "implemented via a backward function: ", "page_idx": 4}, {"type": "text", "text": "def backward(grad_z, x, W): grad_W $=$ grad_z.T @ x return grad_W ", "page_idx": 4}, {"type": "text", "text": "i.e., we replace the backward of the first layer by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{b}\\,,\\,\\texttt{n=x\\_s h a p e}}\\\\ &{\\mathtt{g r a d\\_w}=\\mathtt{g r a d\\_z}\\,.\\,\\texttt{T}\\otimes\\textbf{x}\\otimes\\,\\texttt{i n v e r s e}(}\\\\ &{\\qquad\\qquad\\mathtt{x\\_T}\\odot\\textbf{x}/\\textbf{b}+\\textbf{1}*\\,\\mathtt{e y e}(\\mathfrak{n})\\,)}\\\\ &{\\mathtt{r e t u r n~\\_g r a d\\_W~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Figure 2: Implementation of TrAct, where l corresponds to the hyperparameter $\\lambda$ . ", "page_idx": 4}, {"type": "text", "text": "Details are shown in Figure 2. This applies the TrAct update from Equation 10 when using the SGD optimizer. Moreover, extensions of the update corresponding to optimizers like ADAM [21] (including, e.g., momentum, learning rate scheduler, regularizations, etc.) can be attained by using a respective optimizer and pretending (towards the optimizer) that the TrAct update corresponds to the gradient. As it only requires a modification of the gradient computation of the first layer, the proposed method allows for easy adoption in existing code. All other layers / weights $(\\theta_{\\setminus W})$ are trained conventionally without modification. Convolutions can be easily expressed as a matrix multiplication via an unfolding of the input; accordingly, we unfold the inputs respectively in the case of convolution. ", "page_idx": 4}, {"type": "text", "text": "Moreover, we would like to show a second method of applying TrAct that is exactly equivalent. Typically, we have some batch of data ${\\tt x}$ and a first embedding layer embed, as well as a remaining network net, a loss, and targets $\\mathtt{g t}$ . In the following, we show how the forward and backward is usually written (left) and how it can be modified to incorporate TrAct (right): ", "page_idx": 4}, {"type": "text", "text": "$_z=$ embed(x) # first layer pre-act $\\texttt{y}=$ net $(z)$ # remainder of the net loss(y, gt).backward() # backprop ", "page_idx": 4}, {"type": "text", "text": "$_z=$ embed(x @ inverse(x.T @ x/b+l\\*eye(n))) z.data $=$ embed(x) # overwrites the values # in z but leaves the gradient as before y = net(z)   \nloss(y, gt).backward() ", "page_idx": 4}, {"type": "text", "text": "This modifies the input of embed for the gradient computation, but replaces the actual values propagated through the remaining network z.data by the original values, therefore not affecting downstream layers. which modifies the input of embed for the gradient computation, but replaces the actual values propagated through the remaining network z.data by the original values, therefore not affecting downstream layers. This illustrates interesting relationships: TrAct is minimally invasive, can be removed or included at any time without breaking the network, and does not have learnable parameters. TrAct can be seen as in some sense related to normalizing / whitening / inverting the input for the purpose of gradient computation, but then switching the embeddings back to the original first layer embeddings / activations for propagation through the remainder of the network. ", "page_idx": 4}, {"type": "text", "text": "We provide an easy-to-use wrapper module that can be applied to the first layer, and automatically provides the TrAct gradient computation replacement procedure. For example, for PyTorch [19], the TrAct module can be applied to nn.Linear and nn.Conv2d layers by wrapping them as ", "page_idx": 4}, {"type": "text", "text": "and for existing implementations, we can apply TrAct, e.g., for vision transformers via: ", "page_idx": 4}, {"type": "text", "text": "4 Experimental Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 CIFAR-10 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Setup For the evaluation on the CIFAR-10 data set [22], we consider the ResNet-18 [23] as well as a small ViT model. We consider training from scratch as the method is particularly designed for this case. We perform training for 100, 200, 400, and 800 epochs. For the ResNet models, we use the Adam and SGD with momentum (0.9) optimizers, both with cosine learning rate schedules; learning rates, due to their significance, will be discussed alongside respective experiments. Further, we use the standard softmax cross-entropy loss. For the ViT, we use Adam with a cosine learning rate scheduler as well as a softmax cross-entropy loss with label smoothing (0.1). The selected $\\mathrm{ViT}^{1}$ is particularly designed for effective training on CIFAR scales and has 7 layers, 12 heads, and hidden sizes of 384. Each model is trained with a batch size of 128 on an Nvidia RTX 4090 GPU with PyTorch [19]. ", "page_idx": 4}, {"type": "image", "img_path": "gCCMzedgbo/tmp/e3ca9962f120cdfa299125e0a7460b75da8d9308cef4d8557b310914f3e56328.jpg", "img_caption": ["Figure 3: Training a ResNet-18 on CIFAR-10. We train for $\\{100,200,400,800\\}$ epochs using a cosine learning rate schedule and with SGD (left) and Adam (right). Learning rates have been selected as optimal for each baseline. Averaged over 5 seeds. TrAct (solid lines) consistently outperforms the baselines (dashed)\u2014in many cases already with a quarter of the number of the epochs of the baseline. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "As mentioned above, the learning rate is a significant factor in the evaluation. Therefore, throughout this paper, to remove any bias towards the proposed method (and even give an advantage to the baseline), we utilize the optimal learning rate of the baseline also for the proposed method. For the Adam optimizer, we consider a learning rate grid of $\\{10^{-2},10^{-2.5},10^{-\\dot{3}},\\dot{1}0^{-3.5}\\}$ ; for SGD with momentum, a learning rate grid of $\\left\\{0.1,0.09,0.08,0.07\\right\\}$ . The optimal learning rate is determined for each number of epochs using regular training; in particular, for Adam, we have $\\{100{\\rightarrow}10^{-2}$ , $200{\\rightarrow}10^{-2}$ , $400{\\rightarrow}10^{\\circ}3$ , $800{\\rightarrow}1\\bar{0}^{-3}\\bar{\\}$ , and, for SGD with momentum, we find that a learning rate of 0.08 is optimal in each case. For the ViT, we considered a learning rate grid of $\\{10^{-3},10^{-3.1}$ , $10^{-3.2}$ , $10^{-3.3}$ , $10^{-3.4}$ , $10^{-3.5}$ , $10^{-3.6}$ , $10^{-3.7}$ $\\cdot^{7},10^{-3.8},10^{-3.9},10^{-4}\\}$ . Here, the optimal learning rates (based on the baseline) are $\\{100{\\rightarrow}10^{-3}$ , $200{\\rightarrow}10^{-3.2}$ , $400{\\rightarrow}1\\bar{0}^{-3.5}$ , $800{\\rightarrow}10^{\\circ}{}^{-3.5}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Results In Figure 3 we show the results for ResNet-18 trained on CIFAR-10. We can observe that TrAct improves the test accuracy in every setting, in particular, for both optimizers, for all four numbers of epochs, and for all three choices of the hyperparameter $\\lambda\\in\\{0.05,0.1,0.2\\}$ . Moreover, we can observe that, for SGD, the accuracy after 100 epochs is already better than for the baseline after 800 epochs. For Adam, we can see that TrAct after 100 epochs performs similar to the baseline after 400 epochs, and TrAct after 200 epochs performs similar to the baseline after 800 epochs. Comparing the different choices of $\\lambda$ , $\\lambda=0.05$ performs best in most cases. ", "page_idx": 5}, {"type": "text", "text": "The results for the ViT model are displayed in Figure 4. Again, we can observe that TrAct consistently outperforms the baselines for all $\\lambda$ . Further, we can observe that TrAct with 200 epochs performs comparable to the baseline with 400 epochs. We emphasize that, again, the optimal learning rate has been selected based on the baseline. Overall, here, $\\lambda=0.1$ performed best. ", "page_idx": 5}, {"type": "text", "text": "4.2 CIFAR-100 ", "text_level": 1, "page_idx": 5}, {"type": "image", "img_path": "gCCMzedgbo/tmp/21b650437c1dad5303dbabe98eba71166106b58ba8b37d562e0c1f63e1d2a28a.jpg", "img_caption": ["Figure 4: Training a ViT on CIFAR-10. We train for $\\{100,200,400,800\\}$ epochs using a cosine learning rate schedule and with Adam. Learning rates have been selected as optimal for each baseline. Avg. over 5 seeds. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Setup For CIFAR-100, we consider two experimental settings. First, we consider the training of 36 different convolutional model architectures based on a strong and popular repository2 for CIFAR-100. We use the same hyperparameters as the reference, i.e., SGD with momentum (0.9), weight decay (0.0005), and learning rate schedule with 60 epochs at 0.1, 60 epochs at 0.02, 40 epochs at 0.004, 40 epochs at 0.0008, and a warmup schedule during the first epoch, for a total of 200 epochs. We reproduced each baseline on a set of 5 separate seeds, and discarded the models that produced NaNs on any of the 5 seeds of the baseline. To make the evaluation feasible, we limit the hyperparameter for TrAct to $\\lambda=0.1$ . Second, we also reproduce the ResNet-18 CIFAR-10 experiment but with CIFAR-100. The results for this are displayed in Figure 10 in the Supplementary Material and demonstrate similar relations as the corresponding Figure 3. Again, all models are trained with a batch size of 128 on a single NVIDIA RTX 4090 GPU. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Results We display the results for the 36 CIFAR-100 models in Table 1. We can observe that TrAct outperforms the baseline wrt. top-1 and top-5 accuracy for 33 and 34 out of 36 models, respectively. Further, except for those 5 models, for which TrAct and the baseline perform comparably (each better on one metric), TrAct is better than vanilla training. Specifically, for 31 models, TrAct outperforms the baseline on both metrics, and the overall best result is also achieved by TrAct. Further, TrAct improves the accuracy on average by $0.49\\%$ on top-1 accuracy and by $0.23\\%$ on top-5 accuracy, a statistically very significant improvement over the baseline. The average standard deviations $0.25\\%$ $0.15\\%$ ", "page_idx": 6}, {"type": "table", "img_path": "gCCMzedgbo/tmp/0cfb27c1804bcac1676755c73c1921acc59a9995cceb47c961977c340fe917aa.jpg", "table_caption": [], "table_footnote": ["are and for top-1 and top-5 accuracy, respectively. ", "Table 1: Results on CIFAR-100 trained for 200 epochs, averaged over 5 seeds. The standard deviations and results for TrAct with only 133 epochs are depicted in Tables 6 and 7 in the SM. "], "page_idx": 6}, {"type": "text", "text": "In addition, we also considered training the models with TrAct for only 133 epochs, i.e., $2/3$ of the training time. Here, we found that, on average, regular training for 200 epochs is comparable with TrAct for 133 epochs with a small advantage for TrAct. In particular, the average accuracy of TrAct with 133 epochs is $75.94\\%$ (top-1) and $93.{\\bar{3}}4\\%$ (top-5), which is a small improvement over regular training for 200 epochs. The individual results are reported in Table 7 in the Supplementary Material. ", "page_idx": 6}, {"type": "text", "text": "4.3 ImageNet ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we consider training on the ImageNet data set [40]. We train ResNet- $\\{18,34,50\\}$ , ViT-S and ViT-B models. ", "page_idx": 6}, {"type": "text", "text": "ResNet Setup For the ResNet- $\\{18,34,50\\}$ models, we train for $\\{30,60,90\\}$ epochs and consider base learning rates in the grid $\\left\\{0.2,0.141,0.1,0.071,0.05\\right\\}$ and determine the choice for each model / training length combination with standard baseline training. We find that for each model, when training for 30 epochs, 0.141 performs best, and, when training for $\\{60,90\\}$ epochs, 0.1 performs best as the base learning rate. We use SGD with momentum (0.9), weight decay (0.0001), and the typical learning rate schedule, which decays the learning rate after $1/3$ and $2/3$ of training by 0.1 each. For TrAct, we (again) use the same learning rate as optimal for the baseline, and consider $\\lambda\\in\\{0.05,0.1,0.2\\}$ . Each ResNet model is trained with a batch size of 256 on a single NVIDIA RTX 4090 GPU. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "ResNet Results We start by discussing the ResNet results and then proceed with the vision transformers. We present training plots forResNet-50 in Figure 5. Here, we can observe an effective speedup of a factor of 1.5 during training, which we also demonstrate in Table 2. In particular, the difference in accuracy for TrAct $\\lambda=0.1)$ with 60 compared to the baseline with full 90 epoch training is $-0.02\\%$ and $+0.04\\%$ for top-1 and top-5. ", "page_idx": 7}, {"type": "table", "img_path": "gCCMzedgbo/tmp/d0aac259811daf881e73090e03d8b696d84a773ba6729f68ea61dfdf569248be.jpg", "table_caption": [], "table_footnote": ["Table 2: Final test accuracies (ImageNet valid set) for training ResNet-50 [23] on ImageNet. TrAct with only 60 epochs performs comparable to the baseline with 90 epochs. "], "page_idx": 7}, {"type": "image", "img_path": "gCCMzedgbo/tmp/ade0dcd072bb4fd905b54b518347c35642490978f07bd7a1d487e322302559f1.jpg", "img_caption": ["Figure 5: Test accuracy of ResNet-50 trained on ImageNet for $\\{30,60,90\\}$ epochs. When training for 60 epochs with TrAct, we achieve comparable accuracy to standard training for 90 epochs, showing a $1.5\\times$ speedup. Plots for ResNet-18/34 are in the SM. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "ViT Setup For training the ViTs, we reproduce the \u201cDeiT III\u201d [5], which provides the strongest baseline that is reproducible on a single 8-GPU node. We train each model with the same hyperparameters as in the official source code3. We note that the ViT-S and ViT-B are both trained at a batch size of 2 048 and are pre-trained on resolutions of 224 and 192, respectively, and both models are finetuned on a resolution of 224. We consider pre-training for 400 and 800 epochs. Finetuning for each model is performed for 50 epochs. For the 400 epoch pre-training with TrAct, we use the stronger $\\lambda=0.1$ , while for the longer 800 epoch pre-training we use the weaker $\\lambda=0.2$ . We train the ViT-S models on 4 NVIDIA A40 GPUs and the ViT-B models on 8 NVIDIA V100 (32GB) GPUs. ", "page_idx": 7}, {"type": "text", "text": "ViT Results In Table 3 we present the results for training vision transformers. First, we observe that our reproductions following the official code and hyperparameters improved over the originally reported baselines, potentially due to contemporary improvements in the underlying libraries (our hardware only supported more recent versions). Notably, TrAct consistently improves upon our improved baselines. We note that we did not change any hyperparameters for training with TrAct. For ViT-S, using TrAct leads to $36\\%$ of the improvement that can be achieved by training the baseline twice as long. These improve", "page_idx": 7}, {"type": "table", "img_path": "gCCMzedgbo/tmp/a364553284db195d0c86999ffd70d4527115c0798e8810d1da99f7b13ffd243c.jpg", "table_caption": [], "table_footnote": ["Table 3: Results for training ViTs (DeiT-III) on ImageNet-1k. $^\\dagger$ denotes our reproduction. "], "page_idx": 7}, {"type": "text", "text": "ments can be considered quite substantial considering that these are very large models and we modified only the training of the first layer. Notably, here, the runtime overheads were particularly small, ranging from $0.08\\bar{\\%}$ to $0.25\\%$ . Finally, we consider the quality of the pre-trained model outside ", "page_idx": 7}, {"type": "text", "text": "of ImageNet. We fine-tune the ViT-S (800 epoch pre-training) model on the data sets CIFAR-10 and CIFAR-100 [22] (200 epochs), Flowers-102 [41] (5000 epochs), and Stanford Cars [42] (1000 epochs). For the baseline, both pre-training and finetuning were performed with the vanilla ", "page_idx": 8}, {"type": "table", "img_path": "gCCMzedgbo/tmp/9026d698ced93f31e4dbe9c3cc9d5962af6063db4cb81aef8986a6ac933e6a6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Transfer learning results for ViT-S on CIFAR-10 and CIFAR-100 [22], Flowers-102 [41], and Stanford Cars [42]. ", "page_idx": 8}, {"type": "text", "text": "method, and, for TrAct, both pre-training and fine-tuning were performed with TrAct. In Table 4, we can observe consistent improvements for training with TrAct. ", "page_idx": 8}, {"type": "text", "text": "4.4 Effect of $\\lambda$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "$\\lambda$ is the only hyperparameter introduced by TrAct. Often, with an additional hyperparameter, the hyperparameter space becomes more difficult to manage. However, for TrAct, the selection of $\\lambda$ is simple and compatible with existing hyperparameters. Therefore, throughout all experiments in this paper, we kept all other hyperparameters equal to the optimal choice for the respective baselines, and only considered $\\lambda\\,\\in\\,\\{0.05,0.1,0.2\\}$ . A general trend is that $\\lambda s$ ", "page_idx": 8}, {"type": "image", "img_path": "gCCMzedgbo/tmp/02a95737b3d0b483ad0e8258dbcb5ee7c5050773163b838a11fed4c4c49e97d4.jpg", "img_caption": ["Figure 6: Effect of $\\lambda$ for training a ViT on CIFAR-10. Training for $200\\;\\mathrm{ep}$ ., setup as Fig. 4, avg. over 5 seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "with smaller , TrAct becomes more aggressive, which tends to be more favorable in shorter training, and for larger $\\lambda s$ , TrAct is more moderate, which is ideal for longer trainings. However, in many cases, the particular choice of $\\lambda\\in\\{0.05,0.1,0.2\\}$ has only a subtle impact on accuracy as can be seen throughout the figures in this work. Further, going beyond this range of $\\lambda\\mathbf{s}$ , in Fig. 6, we can observe that TrAct is robust against changes in this parameter. In all experiments, the data was as-per-convention standardized to mean 0 and standard deviation 1; deviating from this convention could change the space of $\\lambda s$ . For significantly different tasks and drastically different kernel sizes or number of input channels, we expect that the space of $\\lambda s$ could change. Overall, we recommend $\\lambda=0.1$ as a starting point and, for long training, we recommend $\\lambda=0.2$ . ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As an ablation study, we first compare TrAct to patch-wise layer normalization for ViTs. For this, we normalize the pixel values of each input patch to mean 0 and standard deviation 1. This is an alternate solution to the conceptual problem of low contrast image regions having a lesser effect on the first layer optimization compared to higher contrast image regions. However, here, we also note that, in contrast to TrAct, the actual neural network inputs are changed through the normalization. Further, we consider DualPatchNorm [9] as a comparison, which additionally includes a second patch normalization layer after the first linear layer, and introduces additional trainable weight parameters for affine transformations into both patch normalization layers. ", "page_idx": 8}, {"type": "text", "text": "We use the same setup as for the CIFAR-10 ViT and run each setting for 5 seeds. The results are displayed in Figure 7. Here, we observe that patch normalization improves training for up to 400 epochs compared to the baseline; however, not as much as TrAct does. Further, we find that DualPatchNorm performs equivalently compared to input patch normalization and worse than TrAct, except for the case of 200 epochs where it performs insignificantly better than TrAct. For training for 800 epochs, patch normalization and DualPatchNorm do not improve the baseline and perform insignificantly worse, whereas TrAct still shows accuracy improvements. This effect may be explained by ", "page_idx": 8}, {"type": "image", "img_path": "gCCMzedgbo/tmp/034b4e83a59085f0c3e792cae5c788f7c94fae3ae82dd8aff1fa80a1f2f9a862.jpg", "img_caption": ["Figure 7: Ablation Study: training a ViT on CIFAR10, including patch normalization (black, dashed) and DualPatchNorm (cyan, dashed). Setups as in Figure 4, averaged over 5 seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "the fact that patch normalization is a scalar form of whitening, and whitening can hurt generalization capabilities due to a loss of information [43]. In particular, what may be problematic is that patch normalization also affects the model behavior during inference, which contrasts TrAct. ", "page_idx": 8}, {"type": "text", "text": "As a second ablation study, we examine what happens if we (against convention) do not perform standardization of the data set. We train the same ViTs as above on CIFAR-10 for 200 epochs, averaged over 5 seeds. We consider two cases: first, an input value range of $[0,1]$ and a quite extreme input value range of $[0,255]$ . ", "page_idx": 9}, {"type": "text", "text": "We display the results in Figure 8. Here, we observe that TrAct is more robust against a lack of standardization. Interestingly, we observe that TrAct performs better for the range of $[0,255]$ than $[0,1]$ . The reason for this is that TrAct suffers from obtaining only positive inputs, which affects the $x x^{\\top}$ matrix in Equation 10; however, we note that regular training suffers even more from the lack of standardization. When considering the range of [0, 255], we observe that TrAct is virtually agnostic to $\\lambda$ , which is caused by the $x x^{\\top}$ matrix becoming very large. The reason why TrAct performs so well here (compared to ", "page_idx": 9}, {"type": "image", "img_path": "gCCMzedgbo/tmp/ad1f7366b8deff519dcc4ffefb27c9887a440c8e0b991fefe7713e5fae39e0b0.jpg", "img_caption": ["Figure 8: Ablation Study: training a ViT on CIFAR-10 without data standardization and with input value ranges of [0, 1] vs. [0, 255]. Setups as in Figure 4, 200 epochs, and avg. over 5 seeds. All other experiments in this work are trained with data standardization. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "the baseline) is that, due to the large $x x^{\\dagger}$ , the updates $\\Delta W$ become very small. This is more desirable compared to the standard gradient, which explodes due to its proportionality to the input values, and therefore drastically degrades training. ", "page_idx": 9}, {"type": "text", "text": "In each experiment, we used only a single optimizer for the entire model; however, our theory assumes that TrAct is used with SGD. This motivates the question of whether it is advantageous to train the first layer with SGD, while training the remainder of the model, e.g., with Adam. ", "page_idx": 9}, {"type": "text", "text": "Thus, as a final ablation study, we extend the experiment from Figure 3 (right) by training the first layer with SGD while training the remaining model with Adam. We display the results in Figure 9 where we can observe small improvements when using SGD for the TrAct layer. ", "page_idx": 9}, {"type": "image", "img_path": "gCCMzedgbo/tmp/78e7f66047fdc8bdb7928f7b0a9f397efdcbe624f82253e0cce179ebbc93a1b5.jpg", "img_caption": ["Figure 9: Ablation Study: extending Figure 3 (right) by training the first layer with TrAct and SGD (pink) and the remainder of the model still with Adam. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.6 Runtime Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we provide a training runtime analysis. Overall, the trend is that, for large models, TrAct adds on a tiny runtime overhead, while it can become more expensive for smaller models. In particular, for the CIFAR- $10\\;\\mathrm{ViT}$ , the average training time per 100 epochs increased by $9.7\\%$ from 1091s to 1197s. Much of this can be attributed to the required additional CUDA calls and non-fused operations, which can be expensive for cheaper tasks. However, when considering larger models, this overhead almost entirely amortizes. In particular the ViT-S (800 epochs) pre-training cost increased by only $0.08\\%$ from 133:52 hours to 133:58 hours. The pre-training cost of the ViT-B (400 epochs) increased by $0.25\\%$ from 98:28 hours to 98:43 hours. We can see that, in each case, the training cost overhead is clearly more than worth the reduced requirement of epochs already. Further, fused kernels could drastically reduce the computational overhead; in particular, our current implementation replaces an existing fused operation by multiple calls from the Python space. As TrAct only affects training, and the modification isn\u2019t present during forwarding, TrAct has no effect on inference time. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion & Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced TrAct, a novel training strategy that modifies the optimization behavior of the first layer, leading to significant performance improvements across a range of 50 experimental setups. The approach is efficient and effectively speeds up training by factors between $1.25\\times$ and $4\\times$ depending on the model size. We hope that the simplicity of integration into existing training schemes as well as the robust performance improvements motivate the community to adopt TrAct. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was in part supported by the Federal Agency for Disruptive Innovation SPRIN-D, the Land Salzburg within the WISS 2025 project IDA-Lab (20102-F1901166-KZP and 20204-WISS/225/197- 2019), the ARO (W911NF-21-1-0125), the ONR (N00014-23-1-2159), and the CZ Biohub. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2017.   \n[2] C. Shorten and T. M. Khoshgoftaar, \u201cA survey on image data augmentation for deep learning,\u201d Journal of Big Data, vol. 6, 2019.   \n[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, and S. G. et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[4] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in Proc. International Conference on Machine Learning (ICML), 2021. [5] H. Touvron, M. Cord, and H. J\u00e9gou, \u201cDeiT III: Revenge of the ViT,\u201d in Proc. European Conference on Computer Vision (ECCV), 2022.   \n[6] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d in Proc. International Conference on Machine Learning (ICML), 2015.   \n[7] T. Salimans and D. P. Kingma, \u201cWeight normalization: A simple reparameterization to accelerate training of deep neural networks,\u201d in Proc. Conf. Neural Information Processing Systems (NeurIPS 2016, Barcelona, Spain), Neural Information Processing Systems Foundation, 2016.   \n[8] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d Computing Research Repository (CoRR) in arXiv, 2016.   \n[9] M. Kumar, M. Dehghani, and N. Houlsby, \u201cDual patchnorm,\u201d Trans. on Machine Learning Research, 2023.   \n[10] F. Dangel, \u201cBackpropagation Beyond the Gradient,\u201d Ph.D. dissertation, University of T\u00fcbingen, 2023.   \n[11] S. J. Wright, Numerical optimization. 2006.   \n[12] J. Martens and R. Grosse, \u201cOptimizing neural networks with kronecker-factored approximate curvature,\u201d in Proc. International Conference on Machine Learning (ICML), 2015.   \n[13] F. Dangel, L. Tatzel, and P. Hennig, \u201cViViT: Curvature access through the generalized Gauss-Newton\u2019s low-rank structure,\u201d Transactions on Machine Learning Research, 2022.   \n[14] F. Petersen, T. Sutter, C. Borgelt, D. Huh, H. Kuehne, Y. Sun, and O. Deussen, \u201cISAAC Newton: Input-based approximate curvature for Newton\u2019s method,\u201d in Proc. International Conference on Learning Representations (ICLR), 2023.   \n[15] F. Dangel, F. Kunstner, and P. Hennig, \u201cBackpack: Packing more into backprop,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[16] F. Petersen, C. Borgelt, T. Sutter, H. Kuehne, O. Deussen, and S. Ermon, \u201cNewton losses: Using curvature information for learning with differentiable algorithms,\u201d in Conference on Neural Information Processing Systems (NeurIPS), 2024.   \n[17] A. E. Hoerl and R. W. Kennard, \u201cRidge regression: Biased estimation for nonorthogonal problems,\u201d Technometrics, vol. 12, no. 1, pp. 55\u201367, 1970.   \n[18] D. Calvetti and L. Reichel, \u201cTikhonov regularization with a solution constraint,\u201d SIAM Journal on Scientific Computing, vol. 26, no. 1, pp. 224\u2013239, 2004.   \n[19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[20] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, JAX: Composable transformations of Python $^+$ NumPy programs, 2018. [Online]. Available: http://github.com/google/jax.   \n[21] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. International Conference on Learning Representations (ICLR), 2015.   \n[22] A. Krizhevsky, V. Nair, and G. Hinton, \u201cCifar-10 (canadian institute for advanced research),\u201d 2009. [Online]. Available: http://www.cs.toronto.edu/\\~kriz/cifar.html.   \n[23] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[24] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, \u201cSqueezenet: Alexnet-level accuracy with 50x fewer parameters and $<0.5\\;\\mathrm{mb}$ model size,\u201d Computing Research Repository (CoRR) in arXiv, 2016.   \n[25] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, \u201cMobilenets: Efficient convolutional neural networks for mobile vision applications,\u201d Computing Research Repository (CoRR) in arXiv, 2017.   \n[26] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[27] X. Zhang, X. Zhou, M. Lin, and J. Sun, \u201cShufflenet: An extremely efficient convolutional neural network for mobile devices,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[28] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, \u201cShufflenet v2: Practical guidelines for efficient cnn architecture design,\u201d in Proc. European Conference on Computer Vision (ECCV), 2018.   \n[29] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d Computing Research Repository (CoRR) in arXiv, 2014.   \n[30] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \u201cDensely connected convolutional networks,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions (2014),\u201d Computing Research Repository (CoRR) in arXiv, 2014.   \n[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[33] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, \u201cInception-v4, inception-resnet and the impact of residual connections on learning,\u201d in AAAI Conference on Artificial Intelligence, 2017.   \n[34] F. Chollet, \u201cXception: Deep learning with depthwise separable convolutions,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[35] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He, \u201cAggregated residual transformations for deep neural networks,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[36] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[37] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, \u201cLearning transferable architectures for scalable image recognition,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[38] S. Zagoruyko and N. Komodakis, \u201cWide residual networks,\u201d Computing Research Repository (CoRR) in arXiv, 2016.   \n[39] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep networks with stochastic depth,\u201d in Proc. European Conference on Computer Vision (ECCV), 2016.   \n[40] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2009.   \n[41] M.-E. Nilsback and A. Zisserman, \u201cAutomated flower classification over a large number of classes,\u201d in 2008 Sixth Indian conference on computer vision, graphics & image processing, IEEE, 2008.   \n[42] J. Krause, J. Deng, M. Stark, and L. Fei-Fei, \u201cCollecting a large-scale dataset of fine-grained cars,\u201d 2013.   \n[43] N. Wadia, D. Duckworth, S. S. Schoenholz, E. Dyer, and J. Sohl-Dickstein, \u201cWhitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization,\u201d in Proc. International Conference on Machine Learning (ICML), 2021.   \n[44] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards real-time object detection with region proposal networks,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2015.   \n[45] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. [Online]. Available: http://www.pascal-network.org/challenges/ VOC/voc2007/workshop/index.html. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Theory ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. The solution $\\Delta W_{i}^{\\star}$ of Equation 7 is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Delta W_{i}^{\\star}=-\\,\\eta\\cdot\\nabla_{z_{i}}\\mathcal{L}(z)\\cdot\\boldsymbol{x}^{\\top}\\cdot\\biggl(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\biggr)^{-1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. We would like to solve the optimization problem ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\Delta W_{i}}\\;\\|_{\\negmedspace}\\eta b\\,\\nabla_{\\!z_{i}}\\mathcal{L}(z)-\\Delta W_{i}x\\|_{2}^{2}\\;+\\;\\lambda b\\,\\|\\Delta W_{i}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A necessary condition for a minimum of the functional ", "page_idx": 12}, {"type": "equation", "text": "$$\nF(\\Delta W_{i})=(-\\eta b\\,\\nabla_{z_{i}}{\\mathcal{L}}(z)-\\Delta W_{i}x)^{2}\\,+\\,\\lambda b\\,(\\Delta W_{i})(\\Delta W_{i})^{\\top}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "is that $\\nabla_{\\Delta W_{i}}F(\\Delta W_{i})$ vanishes: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\nabla_{\\Delta W_{i}}F(\\Delta W_{i})}&{=}&{\\nabla_{\\Delta W_{i}}(-\\eta b\\,\\nabla_{z_{i}}{\\mathcal L}(z)-\\Delta W_{i}x)^{2}+\\lambda b\\nabla_{\\Delta W_{i}}((\\Delta W_{i})(\\Delta W_{i})^{\\top})}\\\\ &{=}&{2(-\\eta b\\,\\nabla_{z_{i}}{\\mathcal L}(z)-\\Delta W_{i}x)(\\nabla_{\\Delta W_{i}}(-\\eta b\\,\\nabla_{z_{i}}{\\mathcal L}(z)-\\Delta W_{i}x))+2\\lambda b\\,\\Delta W_{i}}\\\\ &{=}&{2(-\\eta b\\,\\nabla_{z_{i}}{\\mathcal L}(z)-\\Delta W_{i}x)(-x)^{\\top}+2\\lambda b\\,\\Delta W_{i}}\\\\ &{=}&{2(\\eta b\\,\\nabla_{z_{i}}{\\mathcal L}(z)+\\Delta W_{i}x)\\,x^{\\top}+2\\lambda b\\,\\Delta W_{i}\\,\\,\\,\\,\\,\\,\\frac{1}{\\,\\,\\,}\\,\\,\\,0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It follows for the optimal $\\Delta W_{i}^{\\star}$ that minimizes $F(\\Delta W_{i})$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\!\\!\\!b\\left(\\nabla_{z_{i}}\\mathcal{L}(z)\\right)x^{\\top}+\\Delta{\\cal W}_{i}^{\\star}x x^{\\top}+\\lambda b\\,\\Delta{\\cal W}_{i}^{\\star}=0}\\\\ &{\\iff}&{\\!\\!-\\eta\\!\\!\\ b\\left(\\nabla_{z_{i}}\\mathcal{L}(z)\\right)x^{\\top}=\\Delta{\\cal W}_{i}^{\\star}(x x^{\\top}+\\lambda b I_{n})}\\\\ &{\\iff}&{\\Delta{\\cal W}_{i}^{\\star}=-\\eta\\!\\!\\!b\\left(\\nabla_{z_{i}}\\mathcal{L}(z)\\right)x^{\\top}(x x^{\\top}+\\lambda b I_{n})^{-1}}\\\\ &{\\Leftrightarrow}&{\\Delta{\\cal W}_{i}^{\\star}=-\\eta\\left(\\nabla_{z_{i}}\\mathcal{L}(z)\\right)x^{\\top}\\Big(\\displaystyle\\frac{x x^{\\top}}{b}+\\lambda I_{n}\\Big)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 2. Using TrAct does not change the set of possible convergence points compared to vanilla (full batch) gradient descent. Herein, we use the standard definition of convergence points as those points where no update is performed because the gradient is zero. ", "page_idx": 12}, {"type": "text", "text": "Proof. First, we remark that only the training of the first layer is affected by TrAct. To show the statement, we show that $(i)$ a zero gradient for GD implies that TrAct also performs no update and that $(i i)$ TrAct performing no update implies zero gradients for GD. ", "page_idx": 12}, {"type": "text", "text": "$(i)$ In the first case, we assume that gradient descent has converged, i.e., the gradient wrt. first layer weights is zero $\\nabla_{W}\\mathcal{L}(W)=\\mathbf{0}$ . We want to show that, in this case, our proposed update is also zero, i.e., $\\Delta W^{\\star}=\\mathbf{0}$ . Using the definition of $\\Delta W^{\\star}$ from Equation 9, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta W^{\\star}=-\\,\\eta\\cdot\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}\\cdot\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)^{-1}}\\\\ &{\\qquad=-\\,\\eta\\cdot\\nabla_{W}\\mathcal{L}(W)\\cdot\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)^{-1}}\\\\ &{\\qquad=-\\,\\eta\\cdot\\bf0\\cdot\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)^{-1}=\\bf0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which shows this direction. ", "page_idx": 12}, {"type": "text", "text": "$(i i)$ In the second case, we have $\\Delta W^{\\star}\\,=\\,\\mathbf{0}$ and need to show that this implies $\\nabla_{W}\\mathcal{L}(W)\\,=$ 0. For this, we can observe that $\\left(x x^{\\top}/b+\\lambda\\cdot I_{n}\\right)^{-1}$ is PD (positive definite) by definition and ", "page_idx": 12}, {"type": "text", "text": "$\\left(x x^{\\top}/b+\\lambda\\cdot I_{n}\\right)$ also exists. If $\\Delta W^{\\star}=\\mathbf{0}$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{0}=\\Delta W^{\\star}=\\Delta W^{\\star}\\,\\Bigl(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\Bigr)}\\\\ &{\\quad=-\\eta\\!\\cdot\\!\\nabla_{z}\\mathcal{L}(z)\\!\\cdot\\!x^{\\top}\\!\\cdot\\!\\Bigl(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\Bigr)^{-1}\\!\\!\\left(\\frac{x x^{\\top}}{b}+\\lambda\\cdot I_{n}\\right)}\\\\ &{\\quad=-\\,\\eta\\cdot\\nabla_{z}\\mathcal{L}(z)\\cdot x^{\\top}=-\\,\\eta\\cdot\\nabla_{W}\\mathcal{L}(W)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which also shows this direction. ", "page_idx": 13}, {"type": "text", "text": "Overall, we showed that if gradient descent has converged according to the standard notion of a zero gradient, then our update has also converged and vice versa. ", "page_idx": 13}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We display additional results in Figures 10, 11, and 12 as well as in Tables 6 and 7. ", "page_idx": 13}, {"type": "text", "text": "As an additional experiment, in order to verify the applicability of TrAct beyond training / pre-training, we train Faster R-CNN models [44] on PASCAL VOC2007 [45] using a VGG-16 backbone [29]. However, Faster R-CNN uses a pretrained vision encoder where the first 4 layers are frozen. In order to enable TrAct, as TrAct only affects the training of the first layer, we unfreeze these first layers when training the object detection head. The mean average precision (mAP) on test data for the vanilla model versus TrAct training are shown in Table 5. ", "page_idx": 13}, {"type": "table", "img_path": "gCCMzedgbo/tmp/9a54a99d9aac775884d072fdfad621aa2858a86deefc9bbf741682dbe6af3163.jpg", "table_caption": [], "table_footnote": ["Table 5: Mean average precision (mAP) on test data for Faster R-CNN [44] with a VGG-16 backbone on PASCAL VOC2007 [45], averaged over 2 seeds. "], "page_idx": 13}, {"type": "text", "text": "We can observe that TrAct performs better than the vanilla method by about $1.1\\%$ . We would like to point out that, while is TrAct especially designed for speeding up pretraining or training from scratch, i.e., when actually learning the first layer, we find that it also helps in finetuning pretrained models. Here, a limitation is of course that TrAct requires actually training the first layer. ", "page_idx": 13}, {"type": "image", "img_path": "gCCMzedgbo/tmp/5b55d094f33ee7ea7f8ee092ba5f7dd18a8767ded19eaed713285faf8fb21304.jpg", "img_caption": ["Figure 10: Training a ResNet-18 on CIFAR-100 with the CIFAR-10 setup from Section 4.1. Displayed is top-1 accuracy. We train for $\\{100,200,400,800\\}$ epochs using a cosine learning rate schedule and with SGD (left) and Adam (right). Learning rates have been selected as optimal for each baseline. Averaged over 5 seeds. TrAct (solid lines) consistently outperforms the baselines (dashed lines). "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "gCCMzedgbo/tmp/d84f84d742639c97b33bde4b7301214e1a69ef7f63994afdce69c2ee008fc0cb.jpg", "img_caption": ["Figure 11: Test accuracy of ResNet-18 trained on ImageNet for $\\{30,60,90\\}$ epochs. Displayed is the top-1 (left) and top-5 (right) accuracy. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "gCCMzedgbo/tmp/a11804271b33e49a28014b8d745d27ee1b2b8ef44ab19e23c56bba9e41fdef4f.jpg", "img_caption": ["Figure 12: Test accuracy of ResNet-34 trained on ImageNet for $\\{30,60,90\\}$ epochs. Displayed is the top-1 (left) and top-5 (right) accuracy. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "gCCMzedgbo/tmp/0b922c680f1b7e697aaa89419c95eea47d04aff105c06411ddeb0341b5e22454.jpg", "table_caption": [], "table_footnote": ["Table 6: Results on CIFAR-100, trained for 200 epochs, averaged over 5 seeds including standard deviations. "], "page_idx": 15}, {"type": "table", "img_path": "gCCMzedgbo/tmp/3a3cb10fe3fe0331e1aa2fecce54195ae1e3ffe16e96c53a83ba06154525cb22.jpg", "table_caption": ["Table 7: Results on CIFAR-100, trained for 133 epochs, averaged over 5 seeds including standard deviations. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": ". Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] Justification: We address all claims made in the abstract and introduction in the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All assumptions are pointed out in the work. Limitations are discussed. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All assumptions are provided. Proofs are provided in the SM. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We discuss all experimental parameters necessary for reproduction. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The code will be made publicly available at github.com/Felix-Petersen/tract. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All training details are either explicitly discussed in the main paper or inherited from the references. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Standard deviations are reported and correctly defined, described as such, and utilize Bessel\u2019s correction. In some experiments, we display each seed\u2019s accuracy. In the ImageNet ViT experiments, running multiple seeds was not feasible due to compute cost. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We describe the hardware for each experiment and a provide runtime analysis section. ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research conducted in this paper is conforming with the NeurIPS Code of Ethics.   \nNo animal were harmed in the execution of this research. ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The proposed method reduces training cost of vision models or improves vision models under the same computational budget. ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All utilized assets (data sets, models, and code bases) are cited. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 18}]