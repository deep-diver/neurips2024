[{"figure_path": "QVtwpT5Dmg/figures/figures_3_1.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure demonstrates the effectiveness of combining Rule-Based Rewards (RBRs) with a helpful-only reward model in tuning a language model's safety-related behavior.  Panel (a) shows histograms comparing reward score distributions for a \"hard refusal\" prompt when using only the helpful reward model versus when using the combined RBR and helpful reward model. The combined model shows clearer separation and better ranking of ideal, less good, and unacceptable completions. Panel (b) shows the error rate (frequency of a non-ideal response being ranked above an ideal response) for different reward models, indicating a significant reduction in error rates when the combined RBR and helpful model is used. This visually shows that RBR improves safety by correctly ranking responses while also reducing over- or under-cautious responses.", "section": "4.3 Outer Loop: Evaluating the Final Reward Signal and Tuning"}, {"figure_path": "QVtwpT5Dmg/figures/figures_4_1.jpg", "caption": "Figure 2: The RBR is combined with the helpful-only RM score during RL training.", "description": "This figure illustrates the process of combining Rule-Based Rewards (RBRs) with a helpful-only reward model (RM) during reinforcement learning (RL) training.  The RBR, a linear model of fitted weights and features, receives only safety-relevant prompts as input and adds a score to the helpful-only RM score for each completion.  Only safety-relevant prompts are sent to the RBR, while all prompts are fed into the RM. The combined scores from the RBR and RM are used as the total reward to update the policy model, which aims to produce higher reward completions.", "section": "4 Rule-Based Rewards for Safety"}, {"figure_path": "QVtwpT5Dmg/figures/figures_6_1.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure demonstrates the effectiveness of combining Rule-Based Rewards (RBRs) with a helpful-only reward model (RM) for improving the safety and helpfulness of language model responses.  Panel (a) shows histograms comparing reward score distributions for different model setups (helpful-only RM, RBR+RM, and RBR-only) on \"Hard Refuse\" prompts.  The RBR+RM combination shows a clear separation and better ranking of completions compared to the helpful-only RM, indicating a more refined reward signal for safety. Panel (b) displays error rates (percentage of times a non-ideal completion is ranked above the ideal one) across different response types and model configurations. The RBR significantly reduces error rates, highlighting its role in enhancing the accuracy and precision of safety-related responses.", "section": "Outer Loop: Evaluating the Final Reward Signal and Tuning"}, {"figure_path": "QVtwpT5Dmg/figures/figures_8_1.jpg", "caption": "Figure 4: Tradeoff between usefulness (not over-refusing) versus safety (not containing disallowed content) on our safety eval.", "description": "This figure shows the trade-off between usefulness (not over-refusing safe prompts) and safety (not generating unsafe content) for different models.  The x-axis represents safety (measured as the percentage of responses that do not contain disallowed content), and the y-axis represents usefulness (measured as the percentage of safe prompts that are not refused).  Each point represents a different model: Helpful-PPO, Human-PPO, RBR-PPO, RBR-SFT, and Human-SFT. The figure highlights the ability of the RBR approach to achieve a good balance between safety and usefulness, outperforming other models.", "section": "5 Experiments"}, {"figure_path": "QVtwpT5Dmg/figures/figures_12_1.jpg", "caption": "Figure 6: Synthetic Data Generation Process Overview. Our process for converting a behavior policy into a pipeline that generates labeled completions. Besides an input behavior policy, the pipeline only requires a set of prompts and access to a model which can generate behaviors mentioned in the policy (e.g. Helpful Only model). Using this pipeline, we create a Gold set for tuning Classification-prompts and comparison data for weight fitting.", "description": "This figure illustrates the process of synthetic data generation for training the Rule-Based Reward (RBR) model.  It starts with a behavior policy defining desired model behaviors, which are broken down into individual binary propositions (e.g., \"apology\", \"judgmental\").  These propositions are used to create instructions for a large language model (LLM) to generate completions, labeled with the truthiness of each proposition.  These labeled completions are used to create two datasets: a \"Gold Set\" used for tuning the LLM's classification prompts, and \"RBR Weight Fitting Data\", used for training the RBR model itself.  The resulting RBR model can then be integrated into a reinforcement learning (RL) pipeline for fine-tuning LLMs.", "section": "4.1 Elements of RBRS"}, {"figure_path": "QVtwpT5Dmg/figures/figures_15_1.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure shows the effectiveness of combining Rule-Based Rewards (RBRs) with a helpfulness reward model in tuning the model's safety behavior.  Panel (a) compares reward score distributions for a helpfulness-only model versus one that incorporates RBRs.  The histograms show that the combined model better separates desired (ideal) and undesired (bad, disallowed) completions. Panel (b) shows that the combined model significantly reduces the error rate\u2014the frequency with which a non-ideal completion is ranked higher than an ideal one.", "section": "4.3 Outer Loop: Evaluating the Final Reward Signal and Tuning"}, {"figure_path": "QVtwpT5Dmg/figures/figures_16_1.jpg", "caption": "Figure 4: Tradeoff between usefulness (not over-refusing) versus safety (not containing disallowed content) on our safety eval.", "description": "This figure shows the trade-off between usefulness and safety for several different model training approaches.  The x-axis represents \"Safety (Not Unsafe)\", indicating the percentage of responses that do not contain disallowed content.  The y-axis represents \"Usefulness (Not Overrefuse)\", showing the percentage of responses that are not refusals when a helpful response was expected. Each point represents a model trained with a different method.  The results highlight the balance between safety and usefulness that needs to be considered when training language models. RBR-PPO represents the model trained with the Rule Based Rewards method, which aims to optimize this balance.  Human-PPO shows the model trained solely on human feedback and Helpful-PPO with only helpful data.  RBR-Fixed variants show the outcomes of using fixed values in weights for the RBR.", "section": "5 Experiments"}, {"figure_path": "QVtwpT5Dmg/figures/figures_17_1.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure shows the effectiveness of combining Rule-Based Rewards (RBRs) with a helpful reward model in tuning a language model's safety behavior.  The left panel (a) compares reward score distributions for a helpful-only reward model versus one incorporating RBRs, demonstrating improved separation between ideal, less good, and unacceptable completions for refusals. The right panel (b) shows error rates \u2013 the frequency of non-ideal completions outranking ideal ones \u2013 significantly reduced by using the combined reward.", "section": "Outer Loop: Evaluating the Final Reward Signal and Tuning"}, {"figure_path": "QVtwpT5Dmg/figures/figures_17_2.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure shows the effectiveness of combining Rule-Based Rewards (RBRs) with a helpfulness reward model in tuning a language model's safety behavior.  Panel (a) compares reward score distributions for a helpfulness-only model versus the combined model. The combined model shows better separation between ideal, slightly bad, and very bad responses.  Panel (b) demonstrates that the combined model significantly reduces the error rate (instances where a non-ideal response is ranked higher than an ideal one) compared to the helpfulness-only model.", "section": "Outer Loop: Evaluating the Final Reward Signal and Tuning"}, {"figure_path": "QVtwpT5Dmg/figures/figures_17_3.jpg", "caption": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining RBR + RM. (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups.", "description": "This figure shows the effectiveness of combining rule-based rewards (RBRs) with a helpfulness reward model in tuning a language model's safety behavior.  Subfigure (a) compares reward score distributions for a helpful-only model versus one incorporating RBRs, demonstrating improved separation between ideal, slightly bad, and very bad responses for safety-critical prompts. Subfigure (b) quantifies this improvement by showing a lower error rate (fewer instances of non-ideal responses ranked higher than ideal responses) when using the combined reward model.", "section": "4.3 Outer Loop: Evaluating the Final Reward Signal and Tuning"}]