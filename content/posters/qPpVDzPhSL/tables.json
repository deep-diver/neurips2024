[{"figure_path": "qPpVDzPhSL/tables/tables_6_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and \u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the main results of the experiments conducted on two core tasks: binary summarization and binary function name recovery.  It compares the performance of three different approaches: using only the decompiled code, using retrieval-augmented code, and using the ProRec framework.  The results are reported using various metrics, including CHRF, GPT4-based metrics for functionality and context relevance, and token and character-level precision, recall, and F1 scores.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_7_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and \u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the main results of the experiments conducted on two tasks: binary summarization and binary function name recovery.  It compares three different approaches: using only the decompiled code, using retrieval-augmented methods, and using the proposed ProRec framework.  The results are evaluated using various metrics: CHRF, GPT-4 based evaluations for context relevance (G4-C) and functionality (G4-F), token-level precision/recall/F1 (PsymLM, RSymLM, FSymLM), and character-level BLEU, ROUGE-L, and METEOR. The table highlights the improvements achieved by ProRec across all metrics.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_8_1.jpg", "caption": "Table 3: Statistics of prober with different base SCFM sizes.", "description": "This table presents the statistics of the cross-modal knowledge prober using different sizes of base source code foundation models (SCFMs).  It shows the number of trainable parameters in the prober, the ratio of trainable parameters to the total parameters, the evaluation loss, the n-gram recall at different n-gram lengths (1-4), and the CHRF score.  These statistics illustrate the impact of the base SCFM size on the prober's performance. ", "section": "Analysis"}, {"figure_path": "qPpVDzPhSL/tables/tables_17_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \u201cG4-F\u201d and \u201cG4-C\u201d denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \u201cCMETEOR\u201d stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the quantitative results of the proposed ProRec framework on two main tasks: binary summarization and binary function name recovery.  It compares the performance of three different model setups: using only the decompiled code, retrieval augmented baseline, and ProRec.  Evaluation metrics include various automatically computed scores (CHRF, GPT-4 based scores for functionality and context relevance, token-level precision/recall/F1 score, character-level BLEU/ROUGE-L/METEOR) to assess the quality of the generated summaries and recovered function names.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_17_2.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and \u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the quantitative results of the proposed ProRec framework on two core tasks: binary summarization and binary function name recovery.  It shows the performance of different LLMs (GPT-3.5-turbo, Gemini-Pro, Claude-3) with three setups: using only the decompiled code, using the code with retrieval-augmented method, and using the code with ProRec. Multiple metrics are used to assess the quality of the generated summaries and the accuracy of recovered function names, including CHRF, GPT4-based metrics for context relevance and functionality, and token-level/character-level precision, recall, and F1 scores.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_18_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and\n\u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM,\nFSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and\n\"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the quantitative results of the proposed ProRec framework on two binary reverse engineering tasks: binary summarization and binary function name recovery.  It compares the performance of three different approaches (using only decompiled code, retrieval-augmented baseline, and ProRec) across multiple large language models (LLMs). The metrics used to evaluate the performance of the summarization task include CHRF, GPT4-based metrics for functionality (G4-F) and context relevance (G4-C), character-level BLEU, ROUGE-L, and METEOR. For function name recovery, token-level precision, recall, F1-score, and character-level BLEU, ROUGE-L, and METEOR are reported.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_19_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and \u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the main results of the experiments conducted in the paper on two tasks: binary summarization and binary function name recovery. For each task, results are shown for three different settings: using only the decompiled code, using the decompiled code along with source code snippets retrieved from a datastore (retrieval-augmented baseline), and using the decompiled code along with source code snippets generated by ProRec (the proposed method). The metrics used to evaluate the results include CHRF, GPT4-based metrics (G4-F and G4-C) for summarization, and token-level and character-level metrics (PsymLM, RSymLM, FSymLM, cBLEU, cRoL, CMETEOR) for function name recovery.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_19_2.jpg", "caption": "Table 6: Binary function name recovery results with self-probing on 100 examples.", "description": "This table presents the results of binary function name recovery experiments using self-probing with 100 examples.  It compares the performance of three different approaches: using only the decompiled code, using the decompiled code along with source code snippets retrieved from a datastore (retrieval), and using the decompiled code along with source code snippets generated by self-probing. The performance is evaluated using token-level precision (PSymLM), recall (RSymLM), and F1-score (FSymLM) metrics.", "section": "4 Results"}, {"figure_path": "qPpVDzPhSL/tables/tables_21_1.jpg", "caption": "Table 1: Main results of binary summarization and binary function name recovery. \"G4-F\" and \u201cG4-C\" denote GPT4Evaluator for functionality and context relevance, respectively. PsymLM, RSymLM, FSymLM denote token-level precision, recall, F-1 score as in SymLM [30]. \u201ccBLEU\u201d, \u201ccRoL\u201d, and \"CMETEOR\" stands for character-level BLEU, ROUGE-L, and METEOR scores.", "description": "This table presents the main results of the experiments conducted on two tasks: binary summarization and binary function name recovery.  For binary summarization, it shows the performance of different models (GPT-3.5-turbo, Gemini-Pro, and Claude-3) with three different setups: (1) using only the decompiled code, (2) using decompiled code plus retrieved source code, and (3) using decompiled code plus source code generated by ProRec.  The metrics used are CHRF, GPT4-based scores for functionality and context relevance, and character-level BLEU, ROUGE-L, and METEOR scores. For binary function name recovery, the table presents token-level precision, recall, and F1-score, along with the same three setups as above.  This provides a quantitative comparison of ProRec's effectiveness against baseline methods for both tasks.", "section": "4 Results"}]