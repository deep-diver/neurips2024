{"references": [{"fullname_first_author": "Olga Kovaleva", "paper_title": "Bert busters: outlier dimensions that disrupt transformers", "publication_date": "2021", "reason": "This paper is among the first to identify and define outlier features in transformer models, which is the central focus of the current paper."}, {"fullname_first_author": "Yelysei Bondarenko", "paper_title": "Understanding and overcoming the challenges of efficient transformer quantization", "publication_date": "2021", "reason": "This paper directly addresses the practical problem of quantization in transformer models, a key motivation for the current research on outlier features."}, {"fullname_first_author": "Xiuying Wei", "paper_title": "Outlier suppression: pushing the limit of low-bit transformer language models", "publication_date": "2022", "reason": "This work directly tackles the problem of outlier features hindering low-bit transformer language models, proposing a method to mitigate their impact, which is closely related to the current paper's goal."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Stable and low-precision training for large-scale vision-language models", "publication_date": "2023", "reason": "This paper investigates methods to improve the stability and precision of training for large vision-language models which is highly relevant given that the current paper works towards minimising outlier features."}, {"fullname_first_author": "Yelysei Bondarenko", "paper_title": "Quantizable transformers: removing outliers by helping attention heads do nothing", "publication_date": "2023", "reason": "This paper directly addresses the problem of outlier features hindering the quantization of transformer models and offers a novel solution to this problem."}]}