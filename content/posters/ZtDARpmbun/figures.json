[{"figure_path": "ZtDARpmbun/figures/figures_1_1.jpg", "caption": "Figure 1: The traditional training paradigm in NECIL considers conflicts between old and new classes only when new classes arrive and is prone to overlap. We suggest prospective learning to reduce conflicts: (1) reserve space for unknown classes; (2) make the newly coming class embedded in the reserved space.", "description": "This figure illustrates the difference between traditional training and prospective learning approaches in Non-exemplar Class-Incremental Learning (NECIL).  Traditional training methods result in overlapping embedding spaces for old and new classes as new classes are introduced, leading to confusion and poor performance. In contrast, prospective learning aims to reserve space in the embedding space for future classes during the base phase, allowing for better integration of new classes during the incremental phase and minimizing overlap and confusion. This proactive space management helps maintain the performance of the model on previously learned classes while adapting to new classes.", "section": "1 Introduction"}, {"figure_path": "ZtDARpmbun/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.", "description": "This figure illustrates the two main components of the Prospective Representation Learning (PRL) method for Non-Exemplar Class-Incremental Learning (NECIL).  Panel A shows the 'Preemptive Embedding Squeezing' (PES) step in the base phase, where the model proactively reserves space for future classes by compacting the embeddings of the current classes.  Panel B depicts the 'Prototype-Guided Representation Update' (PGRU) strategy during the incremental phase.  Here, new class features are projected into a latent space, pushed away from the prototypes of old classes, and then used to guide an update of the main model. This ensures the new classes don't interfere with the previously learned ones.", "section": "3.3 Prospective Representation Learning"}, {"figure_path": "ZtDARpmbun/figures/figures_5_1.jpg", "caption": "Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.", "description": "This figure illustrates the two main components of the proposed Prospective Representation Learning (PRL) method for Non-exemplar Class-Incremental Learning (NECIL).  Panel (A) shows the base phase, where a preemptive embedding squeezing (PES) constraint is used to create space for future classes by compacting the embeddings of the current classes.  Panel (B) depicts the incremental phase, where a prototype-guided representation update (PGRU) strategy is employed.  PGRU projects new class features and saved prototypes into a latent space, ensuring that new features are clustered away from old prototypes, thereby reducing confusion between old and new classes when updating the model.", "section": "3.3 Prospective Representation Learning"}, {"figure_path": "ZtDARpmbun/figures/figures_7_1.jpg", "caption": "Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.", "description": "This figure illustrates the two main components of the Prospective Representation Learning (PRL) method for Non-exemplar Class-Incremental Learning (NECIL).  Part (A) shows the \"Preemptive Embedding Squeezing\" which occurs during the base phase of training.  This step aims to proactively reserve space in the embedding space for future classes by compacting the representations of the currently known classes. Part (B) details the \"Prototype-Guided Representation Update\", which happens in the incremental phase. This part utilizes stored prototypes of old classes to guide the embedding of new classes into the reserved space, minimizing interference between old and new class representations.  The latent space is highlighted as a key component in aligning current and previous representations during updates.", "section": "3.3 Prospective Representation Learning"}, {"figure_path": "ZtDARpmbun/figures/figures_8_1.jpg", "caption": "Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.", "description": "This figure illustrates the two main phases of the Prospective Representation Learning (PRL) approach for Non-Exemplar Class-Incremental Learning (NECIL).  The base phase (A) focuses on preemptive embedding squeezing (PES), which compresses the embeddings of existing classes to create space for future classes.  The incremental phase (B) uses a prototype-guided representation update (PGRU) mechanism.  This ensures that new classes are embedded in the reserved space created in the base phase and are kept separate from existing classes, thus preventing catastrophic forgetting and improving performance on both old and new classes.", "section": "3.3 Prospective Representation Learning"}, {"figure_path": "ZtDARpmbun/figures/figures_8_2.jpg", "caption": "Figure 3: Detailed accuracy curves showing the top-1 accuracy of each incremental phase on CIFAR-100, TinyImageNet and ImageNet-Subset.", "description": "This figure displays detailed accuracy curves for three different datasets (CIFAR-100, TinyImageNet, and ImageNet-Subset) across varying numbers of incremental phases (5, 10, and 20).  The curves show the top-1 accuracy at each phase for multiple methods (Fine-tuning, PASS, SSRE, SOPE, PRAKA, NAPA-VQ, and PRL). This allows for a comparison of the performance of these methods over the entire incremental learning process, rather than simply looking at average accuracy.", "section": "4.2 Comparison with SOTA"}, {"figure_path": "ZtDARpmbun/figures/figures_9_1.jpg", "caption": "Figure 6: The traditional training paradigm in NECIL considers conflicts between old and new classes only when new classes arrive and is prone to overlap. We propose prospective learning to reduce conflicts: (1) reserve space for unknown classes; (2) make the newly coming class embedded in the reserved space.", "description": "This figure compares the performance of three different methods (PRAKA, NAPA-VQ, and PRL) on both old and new tasks in a class incremental learning setting.  The left plot shows the average accuracy on previously seen classes (old tasks) as more tasks are added. The right plot shows the accuracy on the newly introduced classes (current tasks) for each incremental phase.  It demonstrates that PRL maintains better performance on old tasks and shows better plasticity (ability to learn new tasks) compared to the other methods.", "section": "Plasticity and stability analysis"}, {"figure_path": "ZtDARpmbun/figures/figures_15_1.jpg", "caption": "Figure 3: Detailed accuracy curves showing the top-1 accuracy of each incremental phase on CIFAR-100, TinyImageNet and ImageNet-Subset.", "description": "This figure shows the detailed accuracy curves of different incremental class learning methods on three datasets: CIFAR-100, TinyImageNet, and ImageNet-Subset. Each curve represents a different number of incremental phases (5, 10, or 20). The x-axis shows the number of classes, while the y-axis represents the top-1 accuracy.  The figure illustrates the performance of different algorithms over the course of incremental learning, showing how accuracy changes as new classes are added.", "section": "4.2 Comparison with SOTA"}, {"figure_path": "ZtDARpmbun/figures/figures_15_2.jpg", "caption": "Figure 3: Detailed accuracy curves showing the top-1 accuracy of each incremental phase on CIFAR-100, TinyImageNet and ImageNet-Subset.", "description": "This figure presents detailed accuracy curves across different datasets (CIFAR-100, TinyImageNet, and ImageNet-Subset) and incremental learning phase configurations (5, 10, and 20 phases). Each line represents a specific dataset, demonstrating the accuracy of the model's top-1 predictions for each phase of incremental training.  It allows for a visual comparison of model performance across datasets and different numbers of incremental phases.", "section": "4.2 Comparison with SOTA"}, {"figure_path": "ZtDARpmbun/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.", "description": "This figure shows the overview of the proposed Prospective Representation Learning (PRL) method for Non-exemplar Class-Incremental Learning (NECIL).  Panel (A) illustrates the base phase where a preemptive embedding squeezing (PES) technique is used to proactively reserve space for future classes by compacting the embedding space of current classes. Panel (B) shows the incremental phase where a prototype-guided representation update (PGRU) method aligns the new class features with the reserved space while avoiding conflicts with existing classes by pushing them away from old class prototypes in a latent space. This combination of techniques aims to prevent catastrophic forgetting and improve the model's ability to learn new classes without access to past data.", "section": "3.3 Prospective Representation Learning"}]