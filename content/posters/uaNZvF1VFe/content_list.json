[{"type": "text", "text": "Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Jiang', Sifan Yang12, Wenhao Yang12, Lijun Zhang1,3,2,\\* 'National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 2School of Artificial Intelligence, Nanjing University, Nanjing, China 3Pazhou Laboratory (Huangpu), Guangzhou, China jiangw, yangsf, yangwh, zhanglj}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sign stochastic gradient descent (signSGD) is a communication-efficient method that transmits only the sign of stochastic gradients for parameter updating. Existing literature has demonstrated that signSGD can achieve a convergence rate of $\\bar{O(d^{1/2}T^{-1/4})}$ , where $d$ represents the dimension and $T$ is the iteration number. In this paper, we improve this convergence rate to $\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the Sign-based Stochastic Variance Reduction (SSVR) method, which employs variance reduction estimators to track gradients and leverages their signs to update. For finite-sum problems, our method can be further enhanced to achieve a convergence rate of $\\mathcal{O}(\\bar{m}^{1/4}d^{1/2}T^{-1/2})$ , where $m$ denotes the number of component functions. Furthermore, we investigate the heterogeneous majority vote in distributed settings and introduce two novel algorithms that attain improved convergence rates of $\\bar{O}(d^{1/2}T^{-1/2}+d n^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$ respectively, outperforming the previous results of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$ , where $n$ represents the number of nodes. Numerical experiments across different tasks validate the effectiveness of our proposed methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper investigates the stochastic optimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is a smooth and non-convex function. We assume that only noisy estimations of the gradient $\\nabla f(\\mathbf{x})$ can be accessed, represented as $\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})$ , where $\\xi$ is a random sample drawn from a stochastic oracle such that $\\mathbb{E}[\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})]=\\nabla f(\\mathbf{x})$ ", "page_idx": 0}, {"type": "text", "text": "The most well-known method for problem (1) is stochastic gradient descent (SGD), which performs $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\nabla f(\\mathbf{x}_{t};\\xi_{t})$ for each iteration, where $\\xi_{t}$ is the sample used in the $t$ -th iteration, and $\\eta$ is the learning rate. It has been proved that the SGD method can obtain a convergence rate of ${\\mathcal{O}}(T^{-1/4})$ [Ghadimi and Lan, 2013], where $T$ is the iteration number. Recently, sign stochastic gradient descent (signSGD) method [Seide et al., 2014, Bernstein et al., 2018] has become popular in the machine learning community, which uses the sign of the stochastic gradient to update, i.e., ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\operatorname{Sign}(\\nabla f(\\mathbf{x}_{t};\\xi_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "This method can largely reduce the communication overhead in distributed environments, and prior research [Bernstein et al., 2018, 2019] has established that signSGD can achieve a convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/4})$ measured in terms of the $l_{1}$ -norm. Since the ${\\mathcal{O}}(T^{-1/4})$ rate is already optimal for SGD methods when measured in the $l_{2}$ -norm [Arjevani et al., 2023] , we can not further improve the dependence on $T$ for signSGD method, considering that $\\left\\|\\mathbf{x}\\right\\|_{2}\\leq\\left\\|\\mathbf{x}\\right\\|_{1}$ for any $\\mathbf{x}$ However, it is also known that variance reduction techniques can further enhance the convergence rate to ${\\mathcal{O}}(T^{-1/3})$ \uff0c under a slightly stronger assumption of average smoothness [Fang et al., 2018, Wang et al., 2019, Cutkosky and Orabona, 2019]. This leads to a natural question: Can the convergence of sign-based methods be further improved by employing variance reduction techniques along with the average smoothness assumption? We respond affirmatively by introducing the Sign-based Stochastic Variance Reduction (SSVR) method. By integrating variance reduction technique [Cutkosky and Orabona, 2019] with sign operations, we achieve an improved convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/3})$ measured in the $l_{1}$ -norm, matching the optimal rates in terms of $T$ for stochastic variance reduction methods [Fang et al., 2018, Li et al., 2021, Arjevani et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Furthermore, we investigate a special case of problem (1), in which the objective function exhibits a finite-sum structure: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^{m}f_{i}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where each $f_{i}(\\cdot)$ is smooth and non-convex. This problem has been extensively studied in stochastic optimization [Zhang et al., 2013, Defazio et al., 2014, Fang et al., 2018], but is less explored with sign-based methods. Previous literature proposes signSVRG [Chzhen and Schechtman, 2023] method to deal with the finite-sum problem, which achieves a convergence rate of $\\mathcal{O}(m^{1/2}d^{1/2}T^{-1/2})$ However, its dependence on $m$ is sub-optimal, failing to match the $\\mathcal{O}(m^{1/4}T^{-1/2})$ lower bound [Fang et al., 2018, Li et al., 2021] for problem (2). To address this gap, we propose the SSVR-FS algorithm, which periodically computes the exact gradient [Zhang et al., 2013, Johnson and Zhang, 2013] and incorporates it into the variance reduction estimator. In this way, we can achieve an improved convergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$ for finite-sum problems. ", "page_idx": 1}, {"type": "text", "text": "Finally, sign-based methods are especially favorable in distributed settings, where the parameter server aggregates gradient signs from each worker through majority vote [Bernstein et al., 2018], allowing 1-bit compression of communication in both directions. Existing literature [Bernstein et al., 2018, 2019] has proved that signSGD can obtain a convergence rate of $\\bar{\\mathcal{O}}(d^{1/2}T^{-1/4})$ for majority vote in homogeneous settings, where the data across nodes is uniformly distributed or identical. For the more challenging heterogeneous setting, in which data distribution can vary significantly across nodes, existing methods can only achieve convergence rates of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ [Sun et al., 2023] and $\\mathcal{O}(d^{3/8}T^{-1/8})^{3}$ [Jin et al., 2023], where $n$ denotes the number of nodes. Note that the first rate indicates that the gradient does not converge to zero as $T$ approaches infinity, and the second one suffers from a high sample complexity. To address these limitations, we first introduce our basic SSVR-MV method, which employs variance reduction estimators to track gradients and replaces the sign operation in each worker as a stochastic unbiased sign operation. This practice ensures 1-bit compression and unbiased estimation at the same time, and the newly proposed method can obtain an improved convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/2}+d n^{-1/2})$ . By further substituting the sign operation in the parameter server with another stochastic unbiased sign operation, our method can further achieve a convergence rate of $\\mathcal{O}(d^{1/4}T^{-1/4})$ , which converges to zero as $T$ increases. ", "page_idx": 1}, {"type": "text", "text": "In summary, compared with existing methods, this paper makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 For stochastic non-convex functions, we develop a sign-based variance reduction algorithm to achieve an improved convergence rate of $\\mathcal{O}\\bar{(d^{1/2}\\bar{T}^{-1/3})}$ , surpassing the $\\mathcal{O}(d^{1/\\bar{2}}T^{-1/4})$ rate for signSGD methods.   \n\u00b7 For non-convex finite-sum optimization, we further improve the our proposed method to obtain an enhanced convergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}\\bar{T}^{-1/2})$ , which is better than the $\\mathcal{O}(m^{1/2}d^{1/2}T^{-1/2})$ convergence rate for SignSVRG method. ", "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of results for sign-based algorithms. Here, stochastic indicates problem (1), finite-sum represents problem (2), $N$ is the number of stochastic gradient calls, and $m$ is the number of component functions. Note that some rates are measured under squared $l_{1}$ -or $l_{2}$ -norm, and we convert them to $l_{1}.$ or $l_{2}$ -norm for a fair comparison. ", "page_idx": 2}, {"type": "table", "img_path": "uaNZvF1VFe/tmp/072a48ebee56854b1b82e226f396e71437d2e372dedb7fc110e690ad1f9356a8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "uaNZvF1VFe/tmp/8dbc9a9802c800665642731aab16fcc9f1ad7e9946414f89369677acb747ccdb.jpg", "table_caption": ["Table 2: Summary of results for sign-based algorithms under the majority vote setting, where $n$ is the number of workers. Some rates are measured under squared $l_{1}$ or $l_{2}$ -norm, and we convert them to $l_{1}$ -or $l_{2}$ -norm for a fair comparison. "], "table_footnote": ["\u00b7 We also investigate sign-based variance reduction methods with heterogeneous majority vote in distributed settings. The proposed algorithms can obtain the convergence rates of $\\mathcal{O}(d^{1/2}T^{-1/2}+d n^{-1/2}\\overline{{{)}}}$ and $\\bar{\\mathcal{O}}(d^{\\bar{1/4}}T^{-1/\\bar{4}})$ , which outperform the previous results of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$ ,respectively. "], "page_idx": 2}, {"type": "text", "text": "We compare our results with existing methods in Table 1 and Table 2, and validate the effectiveness of our method via numerical experiments in Section 5. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section provides an overview of the existing literature on signSGD methods and stochastic variance reduction techniques. ", "page_idx": 2}, {"type": "text", "text": "2.1  SignSGD and its variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The idea of only transmitting the sign information of the stochastic gradient traces back to the 1-bit SGD algorithm, introduced by Seide et al. [2014]. Despite the biased nature of the sign operation, Bernstein et al. [2018] demonstrated that signSGD achieves a convergence rate of $\\mathcal{O}(d^{\\bar{1/2}}T^{\\bar{-}1/4})$ by using large batch sizes in each iteration. Despite the theoretical assurance, Karimireddy et al. [2019] highlighted that signSGD may not converge to the optimal solutions for convex functions and could suffer from poor generalization without large batches. To address these issues, they proposed the ", "page_idx": 2}, {"type": "text", "text": "EF-signSGD method, which integrates error feedback into signSGD to correct errors introduced by the sign operation. Instead of requiring unbiased stochastic gradients in previous literature, Safaryan and Richtarik [2021] assumed that the signs of the stochastic gradient are the same as those of true gradient with a probability greater than $1/2$ . Under this assumption, they demonstrated that signSGD can obtain a similar convergence rate but does not require large batches anymore. Recently, Sun et al. [2023] proposed the signSGD-SIM method, which incorporates the momentum into the signSGD, achieving a convergence rate of $\\mathcal{O}(d T^{-1/4})$ with constant batch sizes and an improved convergence of $\\mathcal{O}(d^{3/2}T^{-2/7})$ with second-order smoothness. ", "page_idx": 3}, {"type": "text", "text": "To deal with the finite-sum problems, Chzhen and Schechtman [2023] developed SignSVRG algorithm, which combines SVRG [Johnson and Zhang, 2013] method with signSGD and achieves a convergence rate of $\\mathcal{O}(d^{1/2}m^{1/2}T^{-1/2})$ ,where $m$ is the number of component functions.More recently, Qin et al. [2023] further investigate signSGD with random reshuffling, achieving a convergence rate of $\\mathcal{O}\\left(m^{-1/2}T^{-1/2}\\log(m T)+\\|\\sigma\\|_{1}\\right)$ where $\\sigma$ is the variance bound of stochastic gradients. By leveraging variance-reduced gradients and momentum updates, they further propose the SignRVR and SignRVM methods, both achieving the convergence rate of $\\mathcal{O}\\left(\\dot{d}^{1/2}m^{1/2}\\bar{T}^{-\\bar{1}/2}\\right)$ ", "page_idx": 3}, {"type": "text", "text": "In distributed settings, sign-based methods with majority vote are also widely investigated. Bernstein et al. [2018, 2019] first indicated that signSGD and its momentum variant Signum can enable 1-bit compression of worker-server communication, obtaining the $\\mathcal{O}(d^{1/2}T^{-1/4})$ convergence rates in the homogeneous environment. For the more challenging heterogeneous settings, SSDM method [Safaryan and Richtarik, 2021] attains the same $\\mathcal{O}(d^{\\bar{1}/2}\\bar{T}^{-1/4})$ convergence rate, but the information sent back to the server is not a sign information anymore. To remedy this issue, Sto-signSGD algorithm [Jin et al., 2023] is proposed, equipped with a convergence rate of $\\mathcal{O}(d^{3/4}T^{-1/4})$ measured in squared $l_{2}$ -norm. More recently, Sun et al. [2023] introduced the MV-signSGD-SIM algorithm and demonstrated a convergence rate of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ , which could be further enhanced to $\\mathcal{O}(d^{3/2}T^{-2/7}+d n^{-1/2})$ under second-order smoothness conditions, where $n$ denotes the number of nodes in the distributed system. ", "page_idx": 3}, {"type": "text", "text": "2.2  Stochastic variance reduction methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Stochastic variance reduction methods have gained significant attention in the optimization community in recent years. Among the pioneering approaches, the stochastic average gradient (SAG) method [Roux et al., 2012] and Stochastic Dual Coordinate Ascent (SDCA) algorithm [ShalevShwartz and Zhang, 2013] utilize a memory of previous gradients to ensure variance reduction, achieving linear convergence for strongly convex functions. To circumvent the need for storing gradients, the stochastic variance reduced gradient (SVRG) [Zhang et al., 2013, Johnson and Zhang, 2013] recalculates the full gradient periodically to enhance the accuracy of gradient estimators, maintaining linear convergence for strongly convex functions. Inspired by SAG and SVRG, Defazio et al. [2014] introduced the SAGA algorithm, which not only provides superior convergence rates but also supports proximal regularization. Subsequently, the stochastic recursive gradient algorithm (SARAH) [Nguyen et al., 2017] employs a simple recursive approach to update gradient estimators, ensuring better convergence for smooth convex functions. ", "page_idx": 3}, {"type": "text", "text": "For non-convex optimization, inspired by the SVRG algorithm, many methods [Reddi et al., 2016, Lei et al., 2017, Zhou et al., 2020] employ variance reduction to design their algorithms and provide the corresponding convergence guarantes. More recent well-known advancements include the SPIDER [Fang et al., 2018] and SpiderBoost [Wang et al., 2019] methods, which improved the ${\\mathcal{O}}(T^{-1/4})$ convergence rate of traditional SGD to ${\\mathcal{O}}(T^{-1/3})$ under the average smoothness assumption. The convergence rate can be further improved to ${\\mathcal{O}}(m^{\\dot{1}/4}T^{-1/2})$ for problems with a finite-sum structure, where $m$ represents the number of component functions. However, these methods typically require a huge batch size to ensure convergence. To avoid this limitation, the stochastic recursive momentum (STORM) method [Cutkosky and Orabona, 2019] introduces a momentum-based updating and an adaptive learning rate based on the stochastic gradients, achieving a convergence rate of $\\tilde{O}(T^{-1/3})$ without necessitating large batches. More recently, variance reduction techniques are widely employed in more complex problems to improve the existing convergence rates, such as compositional optimization [Wang et al., 2017a,b, Yuan et al., 2019, Jiang et al., 2022a, 2023], multi-level optimization [Chen et al., 2021, Zhang and Xiao, 2021, Jiang et al., 2022b, 2024b], adaptive algorithms [Kavis et al., 2022, Jiang et al., 2024a], and distributionally robust optimization [Yu et al., 2024]. ", "page_idx": 3}, {"type": "text", "text": "1: Input: time step $T$ , initial point $\\mathbf{x}_{1}$   \n2: for time step $t=1$ to $T$ do   \n3:  Draw a batch of samples $\\{\\xi_{t}^{1},\\cdots,\\xi_{t}^{B_{1}}\\}$   \n4: Compute $\\begin{array}{r}{\\mathbf{v}_{t}=\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})+\\left(1-\\beta\\right)\\left(\\mathbf{v}_{t-1}-\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})\\right)}\\end{array}$   \n5: Update the decision variable: $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\operatorname{Sign}\\left(\\mathbf{v}_{t}\\right)$   \n6: end for   \n7: Select $\\tau$ uniformly at random from $\\{1,\\ldots,T\\}$   \n8: Return $\\mathbf{x}_{\\tau}$ ", "page_idx": 4}, {"type": "text", "text": "3 The proposed methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the proposed methods for the expectation case, i.e., problem (1), and the finite-sum structure, i.e., problem (2), respectively, along with corresponding theoretical guarantees. ", "page_idx": 4}, {"type": "text", "text": "3.1 Sign-based stochastic variance reduction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we introduce our Sign-based Stochastic Variance Reduction (SSVR) method for problem (1). One crucial step in stochastic optimization is to track the gradient of the objective function. Here, we use a variance reduction gradient estimator $\\mathbf{v}_{t}$ to evaluate the overall gradient $\\nabla f(\\mathbf{x}_{t})$ $\\left(t=1\\right)$ .thestimatoris dfined as $\\begin{array}{r}{\\mathbf{v}_{1}=\\frac{1}{B_{0}}\\sum_{k=1}^{B_{0}}\\nabla f(\\mathbf{x}_{1};\\xi_{1}^{k})}\\end{array}$ where $B_{0}$ is the batch size used in the first iteration. For subsequent iterations $t\\geq2$ $\\mathbf{v}_{t}$ is updated in the style of STORM [Cutkosky and Orabona, 2019], i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})+\\left(1-\\beta\\right)\\left(\\mathbf{v}_{t-1}-\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta$ represents the momentum parameter and $B_{1}$ is the batch size. This method ensures that the expectation of the estimation error $\\bar{\\mathbb{E}}[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}]$ would be reduced gradually. After obtaining the gradient estimator $\\mathbf{v}_{t}$ , we update the decision variable using the sign of $\\mathbf{v}_{t}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\,\\mathrm{Sign}\\left(\\mathbf{v}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The whole algorithm is outlined in Algorithm 1. Next, we introduce the following assumptions for our SsVR method, which are standard and commonly adopted in the analysis of variance reduction methods and stochastic non-convex optimization [Fang et al., 2018, Wang et al., 2019, Cutkosky and Orabona, 2019, Li et al., 2021]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Average smoothness) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol\\xi}\\left[\\left\\|\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})-\\nabla f(\\mathbf{y};\\boldsymbol{\\xi})\\right\\|^{2}\\right]\\leq L^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Bounded variance) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\left[\\left\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{x})\\right\\|^{2}\\right]\\le\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With the above assumptions, we can obtain the theoretical guarantee for our method as stated below. ", "page_idx": 4}, {"type": "text", "text": "Thrnb $\\begin{array}{r}{\\beta=\\mathcal{O}(\\frac{1}{T^{2/3}}),\\eta=\\mathcal{O}(\\frac{1}{d^{1/2}T^{2/3}}),B_{0}=\\mathcal{O}(T^{1/3}),}\\end{array}$ and $B_{1}=\\mathcal{O}(1)$ , our SSVR method ensures: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla f(\\mathbf{x}_{\\tau})\\Vert_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/2}}{T^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark: This convergence rate surpasses the $\\mathcal{O}(d^{1/2}T^{-1/4})$ rate achieved by previous sign-based methods [Bernstein et al., 2018, 2019], and it also outperforms the $\\mathcal{O}(d^{3/2}T^{-2/7})$ convergence rate under the second-order smoothness condition [Sun et al., 2023]. Specifically, to ensure that $\\mathbb{E}[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|_{1}]\\leq\\epsilon.$ our method requires a sample complexity of $\\mathcal{O}(d^{3/2}\\epsilon^{-3})$ , which is much better than the $O(d^{2}\\epsilon^{-4})$ and $\\mathcal{O}(d^{21/4}\\epsilon^{-7/2})$ complexities of previous approaches. ", "page_idx": 4}, {"type": "text", "text": "1: Input: time step $T$ , initial point $\\mathbf{x}_{1}$   \n2: for time step $t=1$ to $T$ do   \n3:if $t$ mod $I==0$ then   \n4: Set $\\tau=t$ and compute $\\begin{array}{r}{\\nabla f(\\mathbf{x}_{\\tau})=\\frac{1}{m}\\sum_{i=1}^{m}\\nabla f_{i}(\\mathbf{x}_{\\tau})}\\end{array}$   \n5: end if   \n6: Sample $i_{t}$ randomly from $\\{1,2,\\cdot\\cdot\\cdot,m\\}$   \n7: Compute gradient estimator $\\mathbf{v}_{t}$ according to equation (3)   \n8:Update the decision variable: $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathrm{Sign}\\left(\\mathbf{v}_{t}\\right)$   \n9: end for   \n10: Select $\\varphi$ uniformly at random from $\\{1,\\ldots,T\\}$   \n11: Return xq ", "page_idx": 5}, {"type": "text", "text": "3.2   Sign-based stochastic variance reduction for finite-sum structure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now extend our SSVR method to deal with the finite-sum structure in problem (2). In this context, we introduce the following assumption for each component function, which is standard and widely adopted in existing literature [Fang et al., 2018, Wang et al., 2019, Li et al., 2021]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Smoothness\uff09 For each $i\\in\\{1,2,\\cdots,m\\}$ , the gradient functions satisfy:", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{i}(\\mathbf{x})-\\nabla f_{i}(\\mathbf{y})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To handle the finite-sum problems, we retain the core structure of our SSVR method while incorporating elements from the SVRG [Zhang et al., 2013, Johnson and Zhang, 2013] approach. Specifically, we compute a full batch gradient at the first step and every $I$ iteration, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x}_{\\tau})=\\frac{1}{m}\\sum_{i=1}^{m}\\nabla f_{i}(\\mathbf{x}_{\\tau}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For other iterations, we randomly select an index $i_{t}$ from the set $\\{1,2,\\cdots,m\\}$ and construct a variance reduction gradient estimator $\\mathbf{v}_{t}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\underbrace{\\nabla f_{i_{t}}(\\mathbf{x}_{t})+(1-\\beta)(\\mathbf{v}_{t-1}-\\nabla f_{i_{t}}(\\mathbf{x}_{t-1}))}_{\\mathrm{STORM}\\;\\mathrm{esitinator}}-\\underbrace{\\beta\\left(\\nabla f_{i_{t}}(\\mathbf{x}_{\\tau})-\\nabla f(\\mathbf{x}_{\\tau})\\right)}_{\\mathrm{error\\;correction}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first two terms of $\\mathbf{v}_{t}$ align with the STORM estimator, and the last term measures the difference of past gradients between the selected component function $\\nabla f_{i_{t}}(\\mathbf{x}_{\\tau})$ and the overall objective $\\nabla f(\\mathbf{x}_{\\tau})$ Note that the STORM estimator employs the component gradient $\\nabla f_{i_{t}}(\\mathbf{x}_{t})$ to track the overall gradient $\\nabla f(\\mathbf{x}_{t})$ , which leads to an estimation error due to the gap between the component function and the overall objective. This gap can be effectively mitigated by the error correction term we introduced in equation (3). With such a design, we can obtain a better gradient estimation of the overall gradient, and ensure that the estimation error $\\mathbb{E}[\\left\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\right\\|^{2}]$ can be reduced gradually. After computing $\\mathbf{v}_{t}$ , we utilize its sign information to update the decision variable. The detailed procedure is outlined in Algorithm 2. Next, we present the theoretical convergence for this method. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 Under Asumption $^3$ by seting $\\begin{array}{r}{\\beta=\\mathcal{O}(\\frac{1}{m}).}\\end{array}$ $I=m_{\\mathrm{~\\rightmoon~}}$ and $\\begin{array}{r}{\\eta=\\mathcal{O}(\\frac{1}{m^{1/4}d^{1/2}T^{1/2}})}\\end{array}$ our algorithm ensures: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\nabla F(\\mathbf{x}_{\\varphi})\\|_{1}]\\leq\\mathcal{O}\\left(\\frac{m^{1/4}d^{1/2}}{T^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark: To ensure $\\mathbb{E}[\\|\\nabla F(\\mathbf{x}_{\\varphi})\\|_{1}]\\le\\epsilon$ , the sample complexity is $\\mathcal{O}(m+\\frac{d\\sqrt{m}}{\\epsilon^{2}})$ , which improves over the $\\mathcal{O}\\big(\\frac{d m}{\\epsilon^{2}}\\big)$ complexity of the previous SignSVRG method [Chzhen and Schechtman, 2023]. ", "page_idx": 5}, {"type": "text", "text": "4  Sign-based stochastic variance reduction with majority vote ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Sign-based methods are advantageous in distributed settings for their low communication overhead, as they can only transmit sign information between nodes via majority vote. This section explores signbased stochastic methods with majority vote, a typical example of distributed learning extensively ", "page_idx": 5}, {"type": "text", "text": "1: Input: time step $T$ , initial point $\\mathbf{x}_{1}$   \n2: for time step $t=1$ to $T$ do   \n3: On node $j\\in\\{1,2,\\cdots\\,,n\\}$   \n4: Draw sample $\\xi_{t}^{j}$ and compute $\\mathbf{v}_{t}^{j}=\\nabla f_{j}(\\mathbf{x}_{t};\\xi_{t}^{j})+\\left(1-\\beta\\right)\\Big(\\mathbf{v}_{t-1}-\\nabla f_{j}(\\mathbf{x}_{t-1};\\xi_{t}^{j})\\Big)$   \n5: Option 1: Send $\\mathrm{S}_{\\mathrm{R}}\\big(\\mathbf{v}_{t}^{j}\\big)$ to the parameter server, where $R=4G$   \n6: Option 2: Send $\\mathrm{S_{G}}(\\hat{\\mathbf{v}}_{t}^{j})$ to the parameter server, where $\\begin{array}{r}{\\hat{\\mathbf{v}}_{t}^{j}=\\Pi_{G}[\\mathbf{v}_{t}^{j}]}\\end{array}$   \n7: On parameter server:   \n8: Option 1: Send $\\begin{array}{r}{\\mathbf{v}_{t}=\\mathrm{Sign}\\left(\\frac{1}{n}\\sum_{j=1}^{n}\\mathrm{S_{R}}\\left(\\mathbf{v}_{t}^{j}\\right)\\right)}\\end{array}$ to all nodes   \n9: Option 2: Send $\\begin{array}{r}{\\mathbf{v}_{t}=\\mathrm{S}_{1}\\left(\\frac{1}{n}\\sum_{j=1}^{n}\\mathrm{S}_{\\mathrm{G}}\\left(\\hat{\\mathbf{v}}_{t}^{j}\\right)\\right)}\\end{array}$ to all nodes   \n10: On node $j\\in\\{1,2,\\cdots\\,,n\\}$   \n11: Update the decision variable $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{v}_{t}$   \n12: end for   \n13: Select $\\tau$ uniformly at random from $\\{1,\\ldots,T\\}$   \n14: Return X ", "page_idx": 6}, {"type": "text", "text": "studied in previous sign-based algorithms [Bernstein et al., 2018, 2019, Safaryan and Richtarik, 2021, Sun et al., 2023]. To begin with, we investigate the following distributed learning task: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\bf x}\\in\\mathbb{R}^{d}}f({\\bf x}):=\\frac{1}{n}\\sum_{j=1}^{n}f_{j}({\\bf x}),\\quad f_{j}({\\bf x})=\\mathbb{E}_{\\xi^{j}\\sim{\\mathcal{D}}_{j}}\\left[f_{j}({\\bf x};\\xi^{j})\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{D}_{j}$ represents the data distribution for node $j$ and $f_{j}(\\mathbf x)$ is the corresponding loss function. Some previous studies [Bernstein et al., 2018, 2019] investigate the homogeneous setting, which assumes the data across each node is uniformly distributed or identical, ensuring that $\\mathbb{E}[f_{i}(\\mathbf{x})]^{\\mathsf{\\bar{\\mu}}}=f(\\mathbf{x})$ In contrast, this paper considers the more challenging heterogeneous setting [Jin et al., 2023, Sun et al., 2023], where data distributions can vary significantly across nodes. ", "page_idx": 6}, {"type": "text", "text": "For sign-based methods in distributed settings, each node $j$ computes a gradient estimator $\\mathbf{v}_{t}^{j}$ and transmits its sign, i.e., $\\mathrm{Sign}(\\mathbf{v}_{t}^{j})$ , to the parameter server. Note that the server can not directly send the aggregate information $\\begin{array}{r}{\\sum_{j=1}^{n}\\mathrm{Sign}(\\mathbf{v}_{t}^{j})}\\end{array}$ back to each node, since i loses binary characteristic ater summation. A natural solution is to apply another sign operation to update the decision variable as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\,\\mathrm{Sign}\\left(\\frac{1}{n}\\sum_{j=1}^{n}\\mathrm{Sign}(\\mathbf{v}_{t}^{j})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This process is called majority vote [Bernstein et al., 2018], as each worker votes on the sign of the gradient, with the server tallying these votes and broadcasting the decision back to the nodes. However, the sign operation introduces bias in the estimation, and employing it twice can significantly amplify this bias, particularly in a heterogeneous environment. Previous analysis [Chen et al., 2020] indicates that signSGD fails to converge in the heterogeneous setting. To deal with this problem, we introduce an unbiased sign operation $\\bar{\\mathrm{S}}_{\\mathrm{R}}(\\cdot)$ , which is defined below. ", "page_idx": 6}, {"type": "text", "text": "Definition 1 For any vector v with $\\|\\mathbf{v}\\|_{\\infty}\\leq R_{:}$ define the function mapping $\\mathrm{S}_{\\mathrm{R}}(\\mathbf{v})$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n[\\mathrm{S}_{\\mathrm{R}}(\\mathbf{v})]_{k}=\\left\\{\\begin{array}{l l}{+1,}&{w i t h\\,p r o b a b i l i t y\\,\\frac{1}{2}+\\frac{[\\mathbf{v}]_{k}}{2R},}\\\\ {}&{}\\\\ {-1,}&{w i t h\\,p r o b a b i l i t y\\,\\frac{1}{2}-\\frac{[\\mathbf{v}]_{k}}{2R}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark: This operation provides an unbiased estimation of $\\mathbf{v}/R$ ,suchthat $\\mathbb{E}[\\mathrm{S}_{\\mathrm{R}}(\\mathbf{v})]=\\mathbf{v}/R$ .It is worth noting that the function mapping is valid when $\\|\\mathbf{v}\\|_{\\infty}\\leq R$ since the probability should always fall within $[0,1]$ . For this purpose, we need to further assume that the gradient is bounded. ", "page_idx": 6}, {"type": "text", "text": "Utilizing this unbiased sign operation, we can update the decision variable as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\operatorname{Sign}\\left({\\frac{1}{n}}\\sum_{j=1}^{n}\\operatorname{S}_{\\mathrm{{R}}}(\\mathbf{v}_{t}^{j})\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "After applying $\\mathrm{S}_{\\mathrm{R}}(\\cdot)$ , the output is a sign information, which can be transported between nodes efficiently. The complete algorithm, named SSVR with majority vote (SSVR-MV), is described in Algorithm 3 (with Option $^{\\,l}$ ). Note that in Step 4, we set $\\mathbf{v}_{1}^{j}=\\nabla f(\\mathbf{x}_{1};\\xi_{1}^{j})$ When $t=1$ .Next,we present the convergence guarantee for the proposed algorithm with the following assumption. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4 For each node $j$ the stochastic gradient is bounded by $G$ in the infinity norm, such that $\\|\\bar{\\nabla}f_{j}(\\mathbf{x};\\boldsymbol{\\xi})\\|_{\\infty}\\leq G$ ", "page_idx": 7}, {"type": "text", "text": "Theorem3 UnderAsumptions 1, 2 and 4, by setting $\\beta=\\textstyle{\\frac{1}{2}}$ and $\\begin{array}{r}{\\eta=\\mathcal{O}\\big(\\frac{1}{T^{1/2}d^{1/2}}\\big)}\\end{array}$ our SSVR-MV method (with Option $^{\\,I}$ )ensures: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/2}}{T^{1/2}}+\\frac{d}{n^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark: Our rate is better than the previous result of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ , and also outperforms the rate of $\\mathcal{O}(d^{3/2}T^{-2/7}+d n^{-1/2})$ under the second-order smoothness [Sun et al., 2023]. ", "page_idx": 7}, {"type": "text", "text": "Although the above convergence rate is superior to previous results, we have to note that the gradient does not converge to zero even as $T\\to\\infty$ . To address this issue, we propose replacing another sign operation with the $\\mathrm{S_{1}}(\\cdot)$ mapping, as defined in equation (5) with $R=1$ . Additionally, in our prior analysis, we ensured that each $\\mathbf{v}_{t}^{j}$ is bounded by assuming the stochastic gradient is bounded and using a constant $\\beta$ . Here, we instead suppose that the true gradient is bounded, as detailed below. ", "page_idx": 7}, {"type": "text", "text": "Assumption $\\mathbf{4}^{\\prime}$ For each node $j$ the gradient is bounded such that $\\|\\nabla f_{j}(\\mathbf{x})\\|\\leq G$ ", "page_idx": 7}, {"type": "text", "text": "Remark: This assumption is weaker than the one used by Sun et al. [2023], which assumes all stochastic gradients are bounded, i.e., $\\|\\nabla f_{j}(\\mathbf{x};\\boldsymbol{\\xi})\\|\\leq G$ ", "page_idx": 7}, {"type": "text", "text": "To ensure each gradient estimator is bounded, we employ a projection operation $\\begin{array}{r}{\\hat{\\mathbf{v}}_{t}^{j}\\,=\\,\\Pi_{G}[\\mathbf{v}_{t}^{j}]}\\end{array}$ where $\\Pi_{G}$ denotes the projection onto a ball of radius $G$ . This allows us to utilize an unbiased sign mapping $\\mathrm{S_{G}}(\\hat{\\mathbf{v}}_{t}^{j})$ before transmission to the parameter server. The revised algorithm is presented in Algorithm 3 (with Option 2), and the modifications lie in Steps 6 and 9. We now present the convergence guarantee for this modified approach below. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 Under Assumptions 1, 2 and $\\mathcal{I^{\\prime}}$ by setting $\\beta=\\mathcal{O}(\\frac{1}{T^{1/2}})$ and $\\begin{array}{r}{\\eta=\\mathcal{O}\\big(\\frac{1}{d^{1/2}T^{1/2}}\\big)}\\end{array}$ our SSVR-MV method (with Option 2) ensures: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/4}}{T^{1/4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark: This rate converges to zero as $T\\rightarrow\\infty$ , and offers a significant improvement over the previous results of $\\mathcal{O}(d^{3/8}\\bar{T}^{-1/8})$ [Jin et al., 2023]. Our result is also better than the $\\mathcal{O}(d^{1/2}T^{-1/4})$ convergence rate obtained by Safaryan and Richtarik [2021], whose algorithm requires transmitting $\\begin{array}{r}{\\sum_{j=1}^{n}\\bar{\\mathrm{sign}}(\\mathbf{v}_{t}^{j})}\\end{array}$ back to all nodes, which is actually not sign information anymore. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we assess the performance of the proposed methods through numerical experiments. We first evaluate the SSVR and SSVR-FS algorithms within the centralized setting, and then assess the performance of SSVR-MV method in the distributed learning environment. All experiments are conducted onNVIDIA3090GPUs. ", "page_idx": 7}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/a9bc0107743fc63608dc166838c39eaf1882df1b3a64e95c6dbdd244aac2b9d2.jpg", "img_caption": ["Figure 1: Results for CIFAR-10 dataset in the centralized environment. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/65dae1a5266c39b0327bb4ee27decc90ab5d9c3ad1e4d14a6d293efa8ce0ceff.jpg", "img_caption": ["Figure 2: Results for CIFAR-100 dataset in the distributed environment. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1 Evaluation of SSVR and SSVR-FS methods in the centralized environment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To begin with, we conduct numerical experiments on multi-class image classification tasks to validate the effectiveness of our proposed methods. Concretely, we train a ResNet18 model [He et al., 2016] on the CIFAR-10 dataset [Krizhevsky, 2009]. We compare the performance of our SSVR and SSVR-FS methods against signSGD [Bernstein et al., 2018], signSGD-SIM [Sun et al., 2023], and SignSVRG [Chzhen and Schechtman, 2023]. For hyper-parameter tuning, we either follow the recommendations from the original papers or employ a grid search to determine the best settings. Specifically, the momentum parameter $\\beta$ is searched from the set $\\left\\lbrace0.1,0.5,0.9,0.99\\right\\rbrace$ and the learning rate is fine-tuned within the range of $\\{1e{-5,1e{-4,1e{-3,1e{-2,1e{-1}}}}\\}$ ", "page_idx": 8}, {"type": "text", "text": "Results. The training loss, gradient norm, and testing accuracy are presented in Figure 1, with curves averaged over five runs. We observe that all methods exhibit a rapid decrease in training losses, with our methods showing a more pronounced reduction in the gradient norm. In terms of testing accuracy, our SSVR algorithm outperforms other sign-based methods, and our SSVR-FS method achieves superior accuracy in the final epochs. ", "page_idx": 8}, {"type": "text", "text": "5.2  Evaluation of SSVR-MV method in the distributed learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Subsequently, we conduct experiments to evaluate the effectiveness of the SSVR-MV method in the distributed environment. Specifically, we train a ResNet50 model [He et al., 2016] on the CIFAR100 dataset [Krizhevsky, 2009] with 4 and 8 nodes respectively. We compare the performance of our method against signSGD (with majority vote) [Bernstein et al., 2018], Signum (with majority vote) [Bernstein et al., 2019], SSDM [Safaryan and Richtarik, 2021], Sto-signSGD [Jin et al., 2023], and MV-signSGD-SIM [Sun et al., 2023]. The hyper-parameter tuning follows the same methodology as in the centralized environment experiment. ", "page_idx": 8}, {"type": "text", "text": "Results. We plot the training loss and testing accuracy in Figure 2, with all curves averaged over five runs. The results indicate that the training loss of our SSVR-MV algorithm decreases rapidly, and our method obtains higher testing accuracy compared to other methods, both in experiments with 4 nodes and8nodes. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore sign-based stochastic variance reduction (SsVR) methods, which use only the sign information of variance-reduced estimators to update decision variables. The proposed method achieves an improved convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/3})$ , surpassing the $\\mathcal{O}(d^{1/2}\\bar{T}^{-1/4})$ convergence rate of signSGD methods. When applied to finite-sum problems, this rate can be further enhanced to $\\mathcal{O}(m^{1/4}\\bar{d}^{1/2}T^{-1/2})$ , which is also better than the $\\bar{\\mathcal{O}}(m^{1/2}d^{1/2}T^{-1/2})$ convergence rate of SignSVRG. Finally, we investigate the SSVR method in distributed settings and devise novel algorithms to attain convergence rates of $\\mathcal{O}(d^{1/2}T^{-1/2}+d n^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$ , which improve upon the previous results of $\\mathcal{O}(d T^{-1/4}+d n^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$ respectively. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSFC (62122037), the Collaborative Innovation Center of Novel Software Technology and Industrialization, and the Postgraduate Research & Practice Innovation Program of Jiangsu Province (No. KYCX24_0231). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. E. Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165-214, 2023.   \nJ. Bernstein, Y-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimisation for non-convex problems. In Proceedings of the 35th International Conference on Machine Learning, pages 560-569, 2018.   \nJ. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar. signSGD with majority vote is communication efficient and fault tolerant. In International Conference on Learning Representations, 2019.   \nT. Chen, Y. Sun, and W. Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. IEEE Transactions on Signal Processing, 69:4937-4948, 2021.   \nX. Chen, T. Chen, H. Sun, S. Z. Wu, and M. Hong. Distributed training with heterogeneous data: Bridging median- and mean-based algorithms. In Advances in Neural Information Processing Systems 33, pages 21616-21626, 2020.   \nZ. Chen, Y. Zhou, Y. Liang, and Z. Lu. Generalized-smooth nonconvex optimization is as effcient as smooth nonconvex optimization. In Proceedings of the 40th International Conference on Machine Learning, pages 5396-5427, 2023.   \nE. Chzhen and S. Schechtman. SignSVRG: fixing SignSGD via variance reduction. ArXiv e-prints, arXiv:2305.13187, 2023.   \nA. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex SGD. In Advances in Neural Information Processing Systems 32, pages 15210-15219, 2019.   \nA. Defazio, F. R. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pages 1646-1654, 2014.   \nC. Fang, C. J. Li, Z. Lin, and T. Zhang. SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems 31, pages 689-699, 2018.   \nM. Faw, I. Tziotis, C. Caramanis, A. Mokhtari, S. Shakkottai, and R. Ward. The power of adaptivity in SGD: Self-tuning step sizes with unbounded gradients and affine variance. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178, pages 313-355, 2022.   \nS. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.   \nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.   \nW. Jiang, G. Li, Y Wang, L. Zhang, and T. Yang. Multi-block-single-probe variance reduced estimator for coupled compositional optimization. In Advances in Neural Information Processing Systems 35, pages 32499-32511, 2022a.   \nW. Jiang, B. Wang, Y. Wang, L. Zhang, and T. Yang. Optimal algorithms for stochastic multi-level compositional optimization. In Proceedings of the 39th International Conference on Machine Learning, pages 10195-10216, 2022b.   \nW. Jiang, J. Qin,L. Wu, C. Chen, T. Yang,and L.Zhang. Learnin unormalized statistical models via compositional optimization. In Proceedings of the 4Oth International Conference on Machine Learning, pages 15105-15124, 2023.   \nW. Jiang, S. Yang, Y. Wang, and L. Zhang. Adaptive variance reduction for stochastic optimization under weaker assumptions. In Advances in Neural Information Processing Systems 37, 2024a.   \nW. Jiang, S. Yang, W. Yang, Y. Wang, Y. Wan, and L. Zhang. Projection-free variance reduction methods for stochastic constrained multi-level compositional optimization. In Proceedings of the 41st International Conference on Machine Learning, pages 21962-21987, 2024b.   \nR. Jin, Y. Huang, X. He, H. Dai, and T. Wu. Stochastic-Sign SGD for federated learning with theoretical guarantees. ArXiv e-prints, arXiv:2002.10940, 2023.   \nR. Johnson and T. Zhang. Acclerating stochastic gradient descent using predictive ariance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.   \nS. P. Karimiredy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback fixes SignSGD and other gradient compression schemes. In Proceedings of the 36th International Conference on Machine Learning, pages 3252-3261, 2019.   \nA. Kavis, S. Skoulakis, K. Antonakopoulos, L. T. Dadi, and V. Cevher. Adaptive stochastic variance reduction for non-convex finite-sum minimization. In Advances in Neural Information Processing Systems 35, pages 23524-23538, 2022.   \nA. Krizhevsky. Learning multiple layers of features from tiny images. Masters Thesis, Deptartment of Computer Science, University of Toronto, 2009.   \nL. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, volume 30, pages 2345-2355, 2017.   \nZ. Li, H. Bao, X. Zhang, and P Richtarik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In Proceedings of the 38th International Conference on Machine Learning, pages 6286-6295, 2021.   \nL. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning, pages 2613-2621, 2017.   \nZ. Qin, Z. Liu, and P. Xu. Convergence of sign-based random reshuffling algorithms for nonconvex optimization. ArXiv e-prints, arXiv:2310.15976, 2023.   \nS. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of The 33rd International Conference on Machine Learning, pages 314-323, 2016.   \nN. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25, pages 2672-2680, 2012.   \nM. Safaryan and P. Richtarik. Stochastic sign descent methods: New algorithms and better theory. In Proceedings of the 38th International Conference on Machine Learning, pages 9224-9234, 2021.   \nF. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Conference of the International Speech Communication Association, pages 1058-1062, 2014.   \nS. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(16):567-599,2013.   \nT. Sun, Q. Wang, D. Li, and B. Wang. Momentum ensures convergence of SIGNSGD under weaker assumptions. In Proceedings of the 4Oth International Conference on Machine Learning, pages 33077-33099, 2023.   \nM. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2): 419-449, 2017a.   \nM. Wang, J. Liu, and E. X. Fang. Accelerating stochastic composition optimization. Journal of Machine Learning Research, 18:105:1-105:23, 2017b.   \nZ. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh. SpiderBoost and momentum: Faster variance reduction algorithms. In Advances in Neural Information Processing Systems 32, pages 2406-2416, 2019.   \nD. Yu, Y. Cai, W. Jiang, and L. Zhang. Effcient algorithms for empirical group distributionally robust optimization and beyond. In Proceedings of the 41st International Conference on Machine Learning, pages 57384-57414, 2024.   \nH. Yuan, X. Lian, C. J. Li, J. Liu, and W. Hu. Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent. In Advances in Neural Information Processing Systems 32, pages 14905-14916, 2019.   \nJ. Zhang and L. Xiao. Multilevel composite stochastic optimization via nested variance reduction. SIAM Journal on Optimization, 31(2):1131-1157, 2021.   \nL. Zhang, M. Mahdavi, and R. Jin. Linear convergence with condition number independent access of full gradients. In Advances in Neural Information Processing Systems 26, pages 980-988, 2013.   \nD. Zhou, P. Xu, and Q. Gu. Stochastic nested variance reduction for nonconvex optimization. Journal of Machine Learning Research, 21(103):1-63, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Firstly, note that Assumption 1 indicates that the objective function $f(\\mathbf{x})$ is also $L$ -smooth [Li et al., 2021]. Given this property, we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{t+1})}\\\\ &{\\le f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{L}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ &{\\le f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),-\\eta\\mathrm{Sign}(\\mathbf{v}_{t})\\rangle+\\frac{\\eta^{2}L}{2}\\|\\mathbf{sign}(\\mathbf{v}_{t})\\|^{2}}\\\\ &{\\le f(\\mathbf{x}_{t})+\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathrm{Sign}(\\mathbf{v}_{t})\\rangle-\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))\\rangle+\\frac{\\eta^{2}L d}{2}}\\\\ &{=f(\\mathbf{x}_{t})+\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathrm{Sign}(\\mathbf{v}_{t})\\rangle-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le f(\\mathbf{x}_{t})+2\\eta\\sqrt{d}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}+\\frac{\\eta^{2}L d}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last inequality is due to the fact that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathrm{Sign}(\\mathbf{v}_{t})\\rangle}\\\\ &{=\\displaystyle\\sum_{i=1}^{d}\\langle[\\nabla f(\\mathbf{x}_{t})]_{i},\\mathrm{Sign}([\\nabla f(\\mathbf{x}_{t})]_{i})-\\mathrm{Sign}([\\mathbf{v}_{t}]_{i})\\rangle}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}2[|\\nabla f(\\mathbf{x}_{t})|]\\cdot\\mathrm{If}(\\mathrm{Sign}([\\nabla f(\\mathbf{x}_{t})]_{i})\\ne\\mathrm{Sign}([\\mathbf{v}_{t}]_{i}))}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}2[|\\nabla f(\\mathbf{x}_{t})|]-[\\mathbf{v}_{t}]_{i}\\cdot\\nabla(\\mathrm{Sign}([\\nabla f(\\mathbf{x}_{t})]_{i})\\ne\\mathrm{Sign}([\\mathbf{v}_{t}]_{i}))}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}2[|\\nabla f(\\mathbf{x}_{t})|]-[\\mathbf{v}_{t}]_{i}\\cdot\\nabla(\\mathrm{Sign}([\\nabla f(\\mathbf{x}_{t})]_{i})\\ne\\mathrm{Sign}([\\mathbf{v}_{t}]_{i}))}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}2[|\\nabla f(\\mathbf{x}_{t})|]_{i}-[\\mathbf{v}_{t}]_{i}}\\\\ &{=2\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|_{1}}\\\\ &{\\le2\\nabla\\tilde{d}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Summing up and rearranging the equation (6), we derive: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]}\\\\ {\\displaystyle\\leq\\frac{f(\\mathbf{x}_{1})-f(\\mathbf{x}_{T+1})}{\\eta T}+2\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+\\frac{\\eta L d}{2}}\\\\ {\\displaystyle\\leq\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\sqrt{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}+\\frac{\\eta L d}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we define $\\Delta_{f}=f\\left(\\mathbf{x}_{1}\\right)-f_{*}$ , and the second inequality is due to Jensen's Inequality. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{t}:=\\mathbf{x}:=\\cdots}\\\\ &{=\\mathbf{z}\\left[\\left[1-\\beta\\gamma_{1}\\cdots\\gamma_{i}\\right]\\;\\nabla_{j}[\\alpha_{1+1}\\xi_{1+1}^{k}-(1-\\beta)\\frac{1}{B_{1}}\\frac{\\mathbf{P}}{\\nabla L_{1}}\\nabla f(x_{i}\\xi_{1+1}^{k})-\\nabla f(x_{i+1})}\\\\ &{=\\mathbf{z}\\left[\\left[1-\\beta\\right](\\nabla_{i}-\\nabla f(x_{i})+\\beta)\\frac{1}{B_{1}}\\frac{\\nabla_{\\alpha}}{\\nabla L_{1}}(\\nabla f(x_{i}\\xi_{1}^{k})-\\nabla f(x_{i}))\\right.}\\\\ &{\\qquad+\\left.\\left(\\nabla f(x_{i})-\\nabla f(x_{i+1})+\\frac{1}{B_{1}}\\frac{\\nabla_{\\alpha}}{\\nabla L_{1}}\\nabla f(x_{i+1}\\xi_{1}^{k})-\\frac{1}{B_{1}}\\frac{\\nabla_{\\alpha}}{\\nabla L_{1}}\\nabla f(x_{i}\\xi_{1+1}^{k})\\right)\\right]\\right]^{2}\\right]}\\\\ &{\\leq(1-\\beta)^{2}\\mathbb{E}\\left[\\mathbf{1}_{N}-\\nabla f(x_{i})\\right]^{2}+2\\beta^{2}\\mathbb{E}\\left[\\left[\\frac{1}{B_{1}}\\frac{P}{\\nabla L_{1}}\\nabla f(x_{i+1}\\xi_{1}^{k})-\\nabla f(x_{i})\\right]^{2}\\right]}\\\\ &{\\qquad+2\\mathbb{E}\\left[\\left[\\frac{1}{B_{1}}\\frac{P}{\\nabla L_{1}}(\\nabla f(x_{i+1}\\xi_{1+1}^{k})-\\nabla f(x_{i+1}\\xi_{1+1}^{k}))\\right]\\right]^{2}}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\left[\\mathbf{1}_{N}-\\nabla f(x_{i})\\right]^{2}+2\\frac{2\\beta^{2}}{B_{1}}\\frac{\\nabla_{\\alpha}}{\\nabla L_{1}}(\\nabla f(x_{i+1}\\xi_{1}^{k})-\\nabla f(x_{i})\\xi_{1}^{k})}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\mathbf{1}_{N}-\\nabla f(x_{i})\\right]^{2}+\\frac{2\\beta^{2}}{B_{1}}\\frac{P}{\\nabla L_\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the frst inequality is due to the fact E(\u03b2 \u2265B=1 (Vf(xt; \\$t+1) - Vf(xt)\uff09 + Vf(xt) $\\begin{array}{r}{-\\nabla f(\\mathbf{x}_{t+1})+\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\left(\\nabla f(\\mathbf{x}_{t+1};\\xi_{t+1}^{k})-\\nabla f(\\mathbf{x}_{t};\\xi_{t+1}^{k})\\right)\\right]=0,}\\end{array}$ and $(a+b)^{2}\\leq2a^{2}+2b^{2}$ Summing up and noticing that we use a batch size of $B_{0}$ in the first iteration, we can ensure ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]\\leq\\frac{\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right\\|^{2}\\right]}{\\beta T}+\\frac{2\\sigma^{2}\\beta}{B_{1}}+\\frac{2L^{2}\\eta^{2}d}{\\beta B_{1}}}\\\\ {\\displaystyle\\leq\\frac{\\sigma^{2}}{B_{0}\\beta T}+\\frac{2\\sigma^{2}\\beta}{B_{1}}+\\frac{2L^{2}\\eta^{2}d}{\\beta B_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Incorporating the above into equation (8) and setting that $\\beta=\\mathcal{O}\\left(T^{-2/3}\\right),\\eta=\\mathcal{O}\\left(d^{-1/2}T^{-2/3}\\right),$ $B_{0}=\\mathcal{O}\\left(T^{1/3}\\right),B_{1}=\\mathcal{O}\\left(1\\right).$ we observe: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\displaystyle\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}+\\frac{\\eta L d}{2}}&{}\\\\ {\\leq\\displaystyle\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\sqrt{\\frac{\\sigma^{2}}{B_{0}\\beta T}+\\frac{2\\sigma^{2}\\beta}{B_{1}}+\\frac{2L^{2}\\eta^{2}d}{\\beta B_{1}}}+\\frac{\\eta L d}{2}}&{}\\\\ {=\\mathcal{O}\\left(\\frac{(\\Delta_{f}+\\sigma+L)\\,d^{1/2}}{T^{1/3}}\\right)}&{}\\\\ {=\\mathcal{O}\\left(\\frac{d^{1/2}}{T^{1/3}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which finishes the proof of Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To improve the convergence rate for finite-sum structures, we can reuse the results of equation (8), but bound the term $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}\\end{array}$ differently. Since $\\mathbf{v}_{t}=(1-\\beta)\\mathbf{v}_{t-1}+\\beta\\mathbf{h}_{t}+(1-$ ", "page_idx": 13}, {"type": "text", "text": "$\\beta$ $\\mathbf{\\xi}^{3}\\left(\\nabla f_{i_{t}}(\\mathbf{x}_{t})-\\nabla f_{i_{t}}(\\mathbf{x}_{t-1})\\right)$ , where $\\mathbf{h}_{t}=\\nabla f_{i_{t}}(\\mathbf{x}_{t})-\\nabla f_{i_{t}}(\\mathbf{x}_{\\tau})+\\nabla f(\\mathbf{x}_{\\tau})$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{k}[\\vert\\mathbf{v}_{J}(\\mathbf{x}_{I+1})-\\mathbf{v}_{I+1}\\vert]\\,\\,\\,\\big]}\\\\ &{=\\mathbb{E}\\left[\\big\\Vert(1-\\beta)\\mathbf{v}_{t}+\\beta\\mathbf{h}_{t+1}+(1-\\beta)(\\nabla f_{i+1}(\\mathbf{x}_{t+1})-\\nabla f_{i+1}(\\mathbf{x}_{t}))-\\nabla f(\\mathbf{x}_{t+1})\\big\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\|(1-\\beta)(\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t}))+\\beta(\\mathbf{h}_{t+1}-\\nabla f(\\mathbf{x}_{t+1}))\\right.}\\\\ &{\\qquad\\left.+(1-\\beta)\\left(\\nabla f(\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t+1})+\\nabla f_{i+1}(\\mathbf{x}_{t+1})-\\nabla f_{i+1}(\\mathbf{x}_{t})\\right)\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)^{2}\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]+2\\beta^{2}\\mathbb{E}\\left[\\|\\mathbf{h}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]}\\\\ &{\\qquad+2(1-\\beta)^{2}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{t})-\\nabla f(\\mathbf{x}_{t+1})+\\nabla f_{i+1}(\\mathbf{x}_{t+1})-\\nabla f_{i+1}(\\mathbf{x}_{t})\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)^{2}\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]+2\\beta^{2}\\mathbb{E}\\left[\\|\\mathbf{h}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]}\\\\ &{\\qquad+2(1-\\beta)^{2}\\mathbb{E}\\left[\\|\\nabla f_{i+1}(\\mathbf{x}_{t+1})-\\nabla f_{i+1}(\\mathbf{x}_{t})\\|^{2}\\right]}\\\\ &{\\le(1-\\beta)\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]+2\\beta^{2}\\mathbb{E}\\left[\\|\\mathbf{h}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\| \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is due to the fact that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\mathbf{h}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\nabla f_{i_{t+1}}(\\mathbf{x}_{t+1})-\\nabla f_{i_{t+1}}(\\mathbf{x}_{\\tau})+\\nabla f(\\mathbf{x}_{\\tau})-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\left[\\|\\nabla f_{i_{t+1}}(\\mathbf{x}_{t+1})-\\nabla f_{i_{t+1}}(\\mathbf{x}_{\\tau})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq L^{2}\\mathbb{E}\\left[\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{\\tau}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By rearranging and summing up, we establish: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}\\\\ &{\\leq\\frac{\\mathbb{E}\\left[\\|\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\|^{2}\\right]}{\\beta T}+2\\beta L^{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{\\tau}\\|^{2}\\right]+\\frac{2L^{2}}{\\beta}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}\\\\ &{\\leq2\\beta I^{2}L^{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]+\\frac{2L^{2}}{\\beta}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}\\\\ &{\\leq2L^{2}\\left(\\beta I^{2}+\\displaystyle\\frac{1}{\\beta}\\right)\\eta^{2}d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use full batch in the first iteration, and the second inequality is due to the fact that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{\\tau}\\right\\|^{2}=\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|\\sum_{i=\\tau}^{t}\\left(\\mathbf{x}_{i+1}-\\mathbf{x}_{i}\\right)\\right\\|^{2}\\leq\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}I\\sum_{i=\\tau}^{t}\\left\\|\\mathbf{x}_{i+1}-\\mathbf{x}_{i}\\right\\|^{2}}\\\\ {\\displaystyle\\leq\\frac{I^{2}}{T}\\sum_{t=1}^{T}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Incorporate the abveinto quation (8 and setting $\\begin{array}{r}{I=m,\\beta=\\mathcal{O}\\left(\\frac{1}{m}\\right)}\\end{array}$ , and $\\begin{array}{r}{\\eta=\\mathcal{O}\\left(\\frac{1}{m^{1/4}d^{1/2}T^{1/2}}\\right)}\\end{array}$ \uff0c we refine the bound as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\displaystyle\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|^{2}\\right]}+\\frac{\\eta L d}{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\sqrt{2L^{2}\\left(\\beta I^{2}+\\frac{1}{\\beta}\\right)\\eta^{2}d}+\\frac{\\eta L d}{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{O}\\left(\\frac{(\\Delta_{f}+L)\\,m^{1/4}d^{1/2}}{T^{1/2}}\\right)}\\\\ &{\\qquad\\qquad=\\mathcal{O}\\left(\\frac{m^{1/4}d^{1/2}}{T^{1/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "By setting $\\beta=\\frac{1}{2}$ , we can ensure that $\\left\\|\\mathbf{v}_{t}^{j}\\right\\|_{\\infty}\\leq R=4G$ ,since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|{\\mathbf{v}_{t}^{j}}\\right\\|_{\\infty}=\\left\\|{(1-\\beta){\\mathbf{v}_{t-1}^{j}}+\\nabla f({\\mathbf{x}_{t}};{\\boldsymbol{\\xi}_{t}^{j}})-(1-\\beta)f({\\mathbf{x}_{t-1}};{\\boldsymbol{\\xi}_{t}^{j}})}\\right\\|_{\\infty}}\\\\ {\\qquad\\leq(1-\\beta)\\left\\|{\\mathbf{v}_{t-1}^{j}}\\right\\|_{\\infty}+(2-\\beta)G}\\\\ {\\qquad\\leq(1-\\beta)^{t-1}\\left\\|{\\mathbf{v}_{1}^{j}}\\right\\|_{\\infty}+(2-\\beta)G\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}}\\\\ {\\qquad\\leq\\!\\!G+\\frac{(2-\\beta)G}{\\beta}\\leq4G=R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the overall objective function $f(\\mathbf{x})$ is $L$ -smooth, we have the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{t+1})\\leq f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{L}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ &{\\qquad\\leq f(\\mathbf{x}_{t})-\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}\\left(\\frac{1}{n}\\sum_{j=1}^{N}S(\\mathbf{v}_{j}^{t})\\right)\\right\\rangle+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\qquad=f(\\mathbf{x}_{t})+\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathrm{Sign}\\left(\\frac{1}{n}\\sum_{j=1}^{N}S(\\mathbf{v}_{j}^{t})\\right)\\right\\rangle}\\\\ &{\\qquad\\qquad=\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))\\right\\rangle+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\qquad=f(\\mathbf{x}_{t})+\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathrm{Sign}\\left(\\frac{1}{n}\\sum_{j=1}^{N}S(\\mathbf{v}_{j}^{t})\\right)\\right\\rangle-\\eta\\left\\|\\nabla f(\\mathbf{x}_{t})\\right\\|_{1}+\\frac{\\eta^{2}L}{2}}\\\\ &{\\qquad\\leq f(\\mathbf{x}_{t})+2\\eta R\\sqrt{d}\\left\\|\\frac{\\nabla f(\\mathbf{x}_{t})}{R}-\\frac{1}{n}\\sum_{j=1}^{N}S(\\mathbf{v}_{t}^{t})\\right\\|-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}+\\frac{\\eta^{2}L d}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is because of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left<\\nabla f(\\mathbf{x}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}))-\\mathrm{Sign}\\left(\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\mathbf{x}(\\nu)\\right)\\right>}\\\\ &{=\\frac{\\sum}{\\mathrm{id}}\\left<\\nabla f(\\mathbf{x})\\vert_{*}\\mathrm{Sign}(\\nabla f(\\mathbf{x})\\vert_{*})-\\mathrm{Sign}\\left(\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\mathbf{x}(\\nu)\\right]\\right)\\right>}\\\\ &{\\le\\frac{\\sum}{\\mathrm{id}}\\pi|\\nabla f(\\mathbf{x})\\vert_{*}/f_{n}|,\\quad\\left(\\mathrm{Sign}(\\nabla f(\\mathbf{x})\\vert_{*})\\right)\\times\\mathrm{Sign}\\left(\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\mathbf{\\Phi}(\\nu)\\right]\\right)\\right>}\\\\ &{\\le\\frac{\\sum}{\\mathrm{id}}\\pi\\left<\\frac{1}{n}\\frac{\\nabla}{f_{n}}(\\mathbf{x})\\vert_{*}-\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\mathbf{x}(\\nu)\\right]\\right>,\\quad\\cdot\\left(\\mathrm{Sign}(|\\nabla f(\\mathbf{x})|_{*}),\\,\\phi\\le\\mathrm{Im}\\left(\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}-1}\\,\\sigma(\\nu)\\right]\\right)\\right)}\\\\ &{\\le\\frac{\\sum}{\\mathrm{id}}\\frac{\\sum}{\\mathrm{id}}\\frac{\\left[\\nabla f(\\mathbf{x})\\right]_{*}}{f_{n}}-\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\mathbf{x}(\\nu)\\right]\\Bigg|,}\\\\ &{\\le\\frac{\\sum}{\\mathrm{id}}\\frac{\\sum}{\\mathrm{id}}\\frac{\\left[\\nabla f(\\mathbf{x})\\right]_{*}}{f_{n}}-\\left[\\frac{1}{n}\\frac{\\nabla}{f_{n}-1}\\,\\delta(\\nu)\\right],}\\\\ &{=2R\\left|\\frac{\\nabla f(\\mathbf{x})}{f_{n}}-\\frac{1}{n}\\frac{\\nabla}{f_{n}}\\boldsymbol{x}(\\nu)\\right|_{*}^{\\top}\\le2R\\mathcal{C}\\left|\\left[\\frac{\\nabla f(\\mathbf{x})}{R}-\\frac{1}{n}\\frac{\\nabla}{f_{n}-1}\\,\\delta(\\nu)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Rearranging and taking the expectation over equation (10), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb{E}}[(\\mu_{1}\\kappa_{2})\\biggr\\|(\\mu_{1},\\eta_{2})\\biggr\\|(\\eta_{1},\\eta_{2})}\\\\ &{\\leq2\\|{\\mathcal{E}}[(\\mu_{1})\\biggr\\|\\left(\\frac{\\|\\eta_{1}\\|}{\\|\\eta_{2}\\|}-\\frac{1}{n_{0}\\|{\\mathcal{E}}_{p}}\\right)\\biggr\\|\\left(\\frac{1}{n_{0}}\\eta_{1}\\right)\\biggr\\|\\left(\\eta_{2}\\biggr)\\biggr\\|+\\eta_{1}\\kappa_{2}\\biggr\\|\\left(\\eta_{1}\\biggr)\\biggr\\|+\\frac{\\eta_{2}\\|{\\mathcal{E}}[u]}{2}}\\\\ &{\\leq2\\|{\\mathcal{E}}[(\\mu_{1})\\biggr\\|\\left(\\frac{\\|\\eta_{1}\\|}{\\|\\eta_{2}\\|}-\\frac{1}{n_{0}\\|{\\mathcal{E}}_{p}}\\right)^{2}]\\right\\|+2\\|{\\mathcal{E}}[u]\\biggr\\|\\left[\\frac{1}{n_{0}}\\biggl\\|\\sum_{i=1}^{n}\\biggl(\\kappa_{1}\\kappa_{1}\\biggr)-\\frac{u_{1}^{2}}{n_{0}}\\right]\\biggr\\|}\\\\ &{\\qquad-\\|u\\|\\left[\\eta_{1}\\biggr\\|(\\kappa_{1})+\\frac{\\eta_{1}\\|{\\mathcal{E}}[u]}{2}\\right]}\\\\ &{\\leq2\\|u\\|\\left[\\eta_{2}\\biggr\\|(\\kappa_{1})-\\frac{1}{n_{0}\\|{\\mathcal{E}}_{p}}\\right]\\biggr\\|+2\\eta_{1}\\kappa_{2}\\|\\left[\\kappa_{1}\\biggr\\|\\sum_{i=1}^{n}\\biggl(\\kappa_{1}\\kappa_{1}(\\eta_{1})-\\frac{u_{1}^{2}}{n_{0}}\\biggr)\\right]\\biggr\\|}\\\\ &{\\qquad-\\eta_{2}\\|\\left[\\eta_{1}\\biggr\\|(\\kappa_{1})+\\frac{\\eta_{2}\\|{\\mathcal{E}}[u]}{2}\\right]}\\\\ &{\\leq2\\|u\\|\\sum_{i=1}^{n}\\frac{u_{1}^{2}}{n_{0}\\|{\\mathcal{E}}_{p}}\\sqrt{\\eta_{1}}\\left\\|\\left(2\\eta_{2}\\right)\\biggr\\|\\left(\\frac{1}{n_{0}}\\frac{u_{1}^{2}}{n_{0}\\|{\\mathcal{E}}_{p}}\\right)\\right\\|}\\\\ &{ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the third inequality is due to the fact that $\\left(\\mathbb{E}\\left[X\\right]\\right)^{2}\\,\\leq\\,\\mathbb{E}\\left[X^{2}\\right]$ , and the forth inequality is because of $\\begin{array}{r}{\\mathbb{E}\\left[S\\left(\\mathbf{v}_{t}^{j}\\right)\\right]=\\frac{\\mathbf{v}_{t}^{j}}{R}}\\end{array}$ , as well asthe $S$ operatio in each node s independent. ", "page_idx": 16}, {"type": "text", "text": "Rearranging the terms and summing up, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\mathbf{x}_{t})\\right\\Vert_{1}\\right]\\leq\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\mathbb{E}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\left\\Vert\\nabla f(\\mathbf{x}_{t})-\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{t}^{j}\\right\\Vert\\right]+\\frac{2d R}{\\sqrt{n}}+\\frac{\\eta L d}{2}}\\\\ {\\displaystyle\\leq\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\left\\Vert\\mathbb{E}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\left\\Vert\\nabla f(\\mathbf{x}_{t})-\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{t}^{j}\\right\\Vert^{2}\\right]+\\frac{2d R}{\\sqrt{n}}+\\frac{\\eta L d}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is due to Jensen's inequality. ", "page_idx": 16}, {"type": "text", "text": "For each worker $j$ , we have the following according to the definition of $\\mathbf{v}_{t}^{j}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}_{t+1}^{j}-\\nabla f_{j}(\\mathbf{x}_{t+1})=(1-\\beta)\\left(\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right)+\\beta\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right)}\\\\ &{\\qquad\\qquad+\\left(1-\\beta\\right)\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t};\\xi_{t+1}^{j})+\\nabla f_{j}(\\mathbf{x}_{t})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing over $\\{n\\}$ and noting that $\\begin{array}{r}{\\nabla f({\\mathbf x})=\\frac{1}{n}\\sum_{j=1}^{n}\\nabla f_{j}({\\mathbf x})}\\end{array}$ we can obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{t+1}^{j}-\\nabla f(\\mathbf{x}_{t+1})=\\frac{1}{n}\\sum_{j=1}^{n}\\left(\\mathbf{v}_{t+1}^{j}-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right)}\\\\ &{\\displaystyle=\\bigl(1-\\beta\\bigr)\\frac{1}{n}\\sum_{j=1}^{n}\\left(\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right)+\\beta\\frac{1}{n}\\sum_{j=1}^{n}\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right)}\\\\ &{\\displaystyle\\qquad+\\left(1-\\beta\\right)\\frac{1}{n}\\sum_{j=1}^{n}\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t};\\xi_{t+1}^{j})+\\nabla f_{j}(\\mathbf{x}_{t})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{j=1}^{n}\\nabla_{j}\\bar{\\mathbf{r}}_{i+1}-\\nabla f(\\mathbf{x}_{i+1})\\right\\|^{2}\\right]}\\\\ &{\\leq(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{j=1}^{n}\\left(\\mathbf{r}_{j}^{j}-\\nabla f_{j}(\\mathbf{x}_{i})\\right)\\right\\|^{2}\\right]+2\\beta^{2}\\displaystyle\\frac{1}{n^{2}}\\displaystyle\\sum_{j=1}^{n}\\mathbb{E}\\left[\\left\\|\\nabla f_{j}(\\mathbf{x}_{i+1};\\xi_{i+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{i+1})\\right\\|^{2}\\right]}\\\\ &{\\quad+2(1-\\beta)^{2}\\displaystyle\\frac{1}{n^{2}}\\sum_{j=1}^{n}\\mathbb{E}\\left[\\left\\|\\nabla f_{j}(\\mathbf{x}_{i+1};\\xi_{i+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{i};\\xi_{i+1}^{j})\\right\\|^{2}\\right]}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{j=1}^{n}\\left(\\nabla_{j}^{j}-\\nabla f_{j}(\\mathbf{x}_{i})\\right)\\right\\|^{2}\\right]+\\displaystyle\\frac{2\\beta^{2}\\sigma^{2}}{n}+\\displaystyle\\frac{2L^{2}}{n}\\left\\|\\mathbf{x}_{i+1}-\\mathbf{x}_{i}\\right\\|^{2}}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{j=1}^{n}\\nabla_{j}^{j}-\\nabla f(\\mathbf{x}_{j})\\right\\|^{2}\\right]+\\displaystyle\\frac{2\\beta^{2}\\sigma^{2}}{n}+\\displaystyle\\frac{2L^{2}T^{2}q^{2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By summing up and rearranging, we observe ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{t}^{j}-\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}\\right]\\leq\\frac{\\mathbb{E}\\left[\\left\\|\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{1}^{j}-\\nabla f(\\mathbf{x}_{1})\\right\\|^{2}\\right]}{\\beta T}+\\frac{2\\sigma^{2}\\beta}{n}+\\frac{2L^{2}\\eta^{2}d}{n\\beta}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, by setting $\\beta=\\textstyle{\\frac{1}{2}}$ and $\\eta=\\mathcal{O}\\left(T^{-1/2}d^{-1/2}\\right)$ , we ensure that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{i=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\leq\\frac{\\Delta_{f}}{\\eta T}+\\frac{2d R}{\\sqrt{n}}+\\frac{\\eta L d}{2}+2\\sqrt{d}\\left\\lbrace\\mathbb{E}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\left\\|\\nabla f(\\mathbf{x}_{t})-\\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{v}_{t}^{j}\\right\\|^{2}\\right]\\right.}\\\\ {\\displaystyle\\leq\\frac{\\Delta_{f}}{\\eta T}+\\frac{2d R}{\\sqrt{n}}+\\frac{\\eta L d}{2}+2\\sqrt{d}\\sqrt{\\frac{\\sigma^{2}}{n\\beta T}+\\frac{2\\sigma^{2}\\beta}{n}+\\frac{2L^{2}\\eta^{2}d}{n\\beta}}}\\\\ {\\displaystyle=\\mathcal{O}\\left(\\frac{d^{1/2}}{T^{1/2}}+\\frac{d}{n^{1/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Proof of Theorem 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Due to the fact that the overall objective function $f(\\mathbf{x})$ is $L$ -smooth, we have the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(\\mathbf{x}_{t+1})\\leq f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{L}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ {\\displaystyle\\leq f(\\mathbf{x}_{t})-\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{S}_{1}\\left(\\frac{1}{n}\\sum_{j=1}^{n}S_{G}(\\hat{\\mathbf{v}}_{t}^{j})\\right)\\right\\rangle+\\frac{\\eta^{2}L d}{2}}\\\\ {\\displaystyle=f(\\mathbf{x}_{t})+\\eta\\left\\langle\\nabla f(\\mathbf{x}_{t}),\\nabla f(\\mathbf{x}_{t})-\\mathrm{S}_{1}\\left(\\frac{1}{n}\\sum_{j=1}^{n}S_{G}(\\hat{\\mathbf{v}}_{t}^{j})\\right)\\right\\rangle-\\eta\\left\\|\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}+\\frac{\\eta^{2}L d}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking expectations leads to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\mathbf{Y}_{++}\\right)-f\\left(\\mathbf{x}_{+}\\right)\\right]}\\\\ &{\\le\\operatorname*{sup}\\left[\\Biggl\\{\\mathbf{\\check{V}}\\{\\mathbf{Y}(\\mathbf{x}_{1}),\\mathbf{\\check{Y}}(\\mathbf{x}_{1})\\}-\\mathbf{\\mathcal{S}}_{1}\\left(\\frac{1}{n}\\sum_{j=1}^{n}S_{C}(\\hat{\\mathbf{x}}_{j}^{t})\\right)\\Biggr\\}\\right]-\\eta\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}\\right]+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le\\operatorname*{sup}\\left[\\Biggl\\{\\mathbf{\\check{V}}\\{\\mathbf{Y}(\\mathbf{x}_{1}),\\nabla f(\\mathbf{x}_{1})\\}-\\frac{1}{n}\\sum_{j=1}^{n}S_{C}(\\hat{\\mathbf{x}}_{j}^{t})\\Biggr\\}\\right]-\\eta\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}\\right]+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le\\operatorname*{sup}\\left[\\Biggl\\{\\mathbf{\\check{V}}f(\\mathbf{x}_{1}),\\nabla f(\\mathbf{x}_{1})-\\frac{1}{n}\\sum_{j=1}^{n}\\Phi_{j}^{t}\\Biggr\\}\\right]-\\eta\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}\\right]+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le\\operatorname*{sup}\\left[\\Biggl\\{\\frac{1}{2}\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}+\\frac{1}{2}\\left\\}\\right]\\nabla f(\\mathbf{x}_{1})-\\frac{1}{n}\\sum_{j=1}^{n}\\Phi_{j}^{t}\\Biggr\\}\\Biggr|=\\eta\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}\\right]+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le\\frac{\\eta}{2}\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})-\\frac{1}{n}\\sum_{j=1}^{n}\\Phi_{j}^{t}\\right\\vert\\right]^{2}-\\frac{\\eta}{2}\\mathbb{E}\\left[\\left\\vert\\nabla f(\\mathbf{x}_{1})\\right\\vert^{2}\\right]+\\frac{\\eta^{2}L d}{2}}\\\\ &{\\le\\frac{\\eta}{2}\\mathbb{E}\\left[\\left\\vert\\nabla f \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Rearranging the terms and summing up: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\mathbb{E}\\left\\|\\nabla f(\\mathbf{x}_{i})\\right\\|^{2}\\right\\|\\leq\\frac{2\\Delta_{f}}{\\eta T}+\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\left\\|\\nabla f(\\mathbf{x}_{t})-\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\tilde{\\mathbf{v}}_{t}^{j}\\right\\|^{2}\\right]+\\eta L d}\\\\ &{\\phantom{2p c}\\leq\\frac{2\\Delta_{f}}{\\eta T}+\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\left\\|\\nabla f_{j}(\\mathbf{x}_{t})-\\boldsymbol{\\hat{\\mathbf{v}}}_{t}^{j}\\right\\|^{2}\\right]+\\eta L d}\\\\ &{\\phantom{2p c}\\leq\\frac{2\\Delta_{f}}{\\eta T}+\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\left\\|\\nabla f_{j}(\\mathbf{x}_{t})-\\boldsymbol{\\Pi}_{G}\\left[\\boldsymbol{\\mathbf{v}}_{t}^{j}\\right]\\right\\|^{2}\\right]+\\eta L d}\\\\ &{\\phantom{2p c}\\leq\\frac{2\\Delta_{f}}{\\eta T}+\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\left\\|\\nabla f_{j}(\\mathbf{x}_{t})-\\boldsymbol{\\hat{\\mathbf{v}}}_{t}^{j}\\right\\|^{2}\\right]+\\eta L d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is due to the non-expansive property of the projection operation. For each worker $j$ , according to the definition of $\\mathbf{v}_{t}^{j}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}_{t+1}^{j}-\\nabla f_{j}(\\mathbf{x}_{t+1})=(1-\\beta)\\left(\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right)+\\beta\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right)}\\\\ &{\\qquad\\qquad+\\left(1-\\beta\\right)\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t};\\xi_{t+1}^{j})+\\nabla f_{j}(\\mathbf{x}_{t})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t+1}^{j}-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right\\|^{2}\\right]}\\\\ &{\\leq(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right\\|^{2}\\right]+2\\beta^{2}\\mathbb{E}\\left[\\left\\|\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t+1})\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad+2(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|\\left(\\nabla f_{j}(\\mathbf{x}_{t+1};\\xi_{t+1}^{j})-\\nabla f_{j}(\\mathbf{x}_{t};\\xi_{t+1}^{j})\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right\\|^{2}\\right]+2\\beta^{2}\\sigma^{2}+2L^{2}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|^{2}}\\\\ &{\\leq(1-\\beta)\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\right\\|^{2}\\right]+2\\beta^{2}\\sigma^{2}+2L^{2}\\eta^{2}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As a result, we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{n}\\sum_{j=1}^{n}\\frac{1}{T}\\sum_{t=1}^{T}\\Big\\lVert\\mathbf{v}_{t}^{j}-\\nabla f_{j}(\\mathbf{x}_{t})\\Big\\rVert^{2}\\right]\\le\\!\\frac{\\sigma^{2}}{\\beta T}+2\\sigma^{2}\\beta+\\frac{2L^{2}\\eta^{2}d}{\\beta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, combining the abve an setting th $\\begin{array}{r}{\\beta=\\mathcal{O}\\left(\\frac{1}{T^{1/2}}\\right),\\eta=\\mathcal{O}\\left(\\frac{1}{d^{1/2}T^{1/2}}\\right)}\\end{array}$ we obtain the fnal bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]\\leq\\sqrt{\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|^{2}\\right]}}&{}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2\\Delta_{f}}{\\eta T}+\\eta L d+\\frac{\\sigma^{2}}{\\beta T}+2\\sigma^{2}\\beta+\\frac{2L^{2}\\eta^{2}d}{\\beta}}}\\\\ &{\\qquad\\qquad\\leq\\mathcal{O}\\left(\\sqrt{\\frac{d^{1/2}\\left(\\Delta_{f}+L\\right)+\\sigma^{2}+L^{2}}{T^{1/2}}}\\right)}\\\\ &{\\qquad=\\mathcal{O}\\left(\\frac{d^{1/4}}{T^{1/4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E  Results under weaker assumptions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we demonstrate that our proposed methods can maintain similar convergence rates under less stringent assumptions \u2014 expected $\\alpha$ -symmetric generalized-smoothness [Chen et al., 2023] and affine variance [Faw et al., 2022]. We first detail these relaxed assumptions below. ", "page_idx": 19}, {"type": "text", "text": "Assumption $\\mathbf{1^{\\prime}}$ (Expected $\\alpha$ -symmetric generalized-smoothness) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\left[\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{y};\\xi)\\|^{2}\\right]\\le\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\\mathbb{E}_{\\xi}\\left[\\left(L_{0}+L_{1}\\displaystyle\\operatorname*{max}_{\\theta\\in[0,1]}\\|\\nabla f(\\mathbf{x}_{\\theta};\\xi)\\|^{\\alpha}\\right)^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{x}_{\\theta}:=\\theta\\mathbf{x}+(1-\\theta)\\mathbf{y}$ and $0\\leq\\alpha\\leq1$ ", "page_idx": 19}, {"type": "text", "text": "Assumption ${\\mathfrak{Z}}^{\\prime}$ (Affine variance) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\left[\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{x})\\|^{2}\\right]\\le\\Gamma^{2}\\|\\nabla f(\\mathbf{x})\\|^{2}+\\Lambda^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark: Assumption $1^{\\prime}$ can be reduced to standard average smoothness (Assumption 1) when $L_{1}\\,=\\,0$ .Note that $\\alpha$ -symmetric generalized-smooth functions not only include asymmetric and Hessian-based generalized-smooth functions, but also contain high-order polynomials and exponential functions [Chen et al., 2023]. Moreover, affine variance is also weaker than Assumption 2 and can be reduced to it when $\\Gamma=0$ ", "page_idx": 19}, {"type": "text", "text": "We then demonstrate that these relaxed conditions are sufficient for our algorithms to achieve the same convergencerate. ", "page_idx": 19}, {"type": "text", "text": "Theorem 5 Under Assumptions $I^{\\prime}$ and $2^{\\prime}$ by setting that $\\begin{array}{r}{\\beta=\\mathcal{O}(\\frac{d^{1/3}}{T^{2/3}}),\\,\\eta=\\mathcal{O}(\\frac{1}{d^{1/6}T^{2/3}}),\\,B_{0}}\\end{array}$ $B_{0}=$ $\\mathcal{O}(1)$ $B_{1}={\\mathcal{O}}(d)$ and denoting $N$ as the samples used, our SSVR method guarantees: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{\\tau})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/2}}{N^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, we introduce the following relaxed assumption for the finite-sum problem. ", "page_idx": 20}, {"type": "text", "text": "Assumption ${\\mathfrak{z}}^{\\prime}$ (Generalized smoothness) For each $i\\in\\{1,2,\\cdots,m\\}$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(\\mathbf{x})-\\nabla f_{i}(\\mathbf{y})\\|\\leq\\|\\mathbf{x}-\\mathbf{y}\\|\\left(L_{0}+L_{1}\\operatorname*{max}_{\\theta\\in[0,1]}\\|\\nabla f(\\mathbf{x}_{\\theta})\\|^{\\alpha}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{x}_{\\theta}:=\\theta\\mathbf{x}+(1-\\theta)\\mathbf{y}$ and $0\\leq\\alpha\\leq1$ ", "page_idx": 20}, {"type": "text", "text": "This assumption is weaker than the standard Assumption 3. We validate that our SSVR-FS algorithm can still achieve similar convergence under this relaxed condition. ", "page_idx": 20}, {"type": "text", "text": "Theorem 6 UnderAssumption $3^{\\prime}$ by seting $\\begin{array}{r}{\\eta\\,=\\,\\mathcal{O}(\\operatorname*{min}\\{\\frac{1}{m^{1/4}d^{1/2}T^{1/2}},\\frac{1}{m d}\\})}\\end{array}$ \uff0c $\\beta\\,=\\,\\mathcal{O}({\\frac{1}{m}})$ and , our SSVR-FS algorithm ensures: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla F(\\mathbf{x}_{\\varphi})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{m^{1/4}d^{1/2}}{T^{1/2}}+\\frac{m d}{T}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark: When the iteration number $T$ is large, the dominant term becomes $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$ which aligns with the results in Theorem 2. ", "page_idx": 20}, {"type": "text", "text": "E.1 Proof of Theorem 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first present some useful tools for analysis. According to Proposition 4 in Chen et al. [2023], Assumption $1^{\\prime}$ leads to the following lemmas. ", "page_idx": 20}, {"type": "text", "text": "Lemma 1 For $\\alpha\\in(0,1)$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({\\mathbf x}_{t+1})\\le f({\\mathbf x}_{t})+\\langle\\nabla f({\\mathbf x}_{t}),{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\rangle}\\\\ {\\qquad\\qquad+\\,\\displaystyle\\frac{1}{2}\\|{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\|\\nabla f({\\mathbf x}_{t})\\|^{\\alpha}+2K_{2}\\|{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\|^{\\frac{\\alpha}{1-\\alpha}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{K_{0}:=L_{0}\\left(2^{\\frac{\\alpha^{2}}{1-\\alpha}}+1\\right),\\,K_{1}:=L_{1}\\cdot2^{\\frac{\\alpha^{2}}{1-\\alpha}}\\cdot3^{\\alpha},\\,K_{2}:=L_{1}^{\\frac{1}{1-\\alpha}}\\cdot2^{\\frac{\\alpha^{2}}{1-\\alpha}}\\cdot3^{\\alpha}(1-\\alpha)^{\\frac{\\alpha}{1-\\alpha}}.}\\end{array}$ For $\\alpha=1$ ,we also have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{t+1})\\leq f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\cfrac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(L_{0}+L_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)\\exp\\left(L_{1}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, according to the Proposition 4 in Chen et al. [2023], we have the following guarantees. ", "page_idx": 20}, {"type": "text", "text": "Lemma 2 For $\\alpha\\in(0,1)$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{y};\\xi)\\|^{2}\\le\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\\big(\\overline{{K}}_{0}+\\overline{{K}}_{1}\\mathbb{E}_{\\xi}\\|\\nabla f(\\mathbf{y};\\xi)\\|^{\\alpha}+\\overline{{K}}_{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{\\frac{\\alpha}{1-\\alpha}}\\big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\overline{{K}}_{0}=2^{\\frac{2-\\alpha}{1-\\alpha}}L_{0}$ \uff0c $\\overline{{K}}_{1}=2^{\\frac{2-\\alpha}{1-\\alpha}}L_{1}$ \uff0c $\\overline{{K}}_{2}=(5L_{1})^{\\frac{1}{1-\\alpha}}$", "page_idx": 20}, {"type": "text", "text": "For $\\alpha=1$ ,we also have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{y};\\xi)\\|^{2}\\le2\\|\\mathbf{x}-\\mathbf{y}\\|^{2}(L_{0}^{2}+2L_{1}^{2}\\mathbb{E}_{\\xi}\\|\\nabla f(\\mathbf{y};\\xi)\\|^{2})\\exp(12L_{1}^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we can begin our proof. For $\\alpha\\in(0,1)$ , according to Lemma 1, by setting $\\eta\\leq d^{-\\frac{1}{2}}$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|^{\\alpha}+2K_{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{\\alpha-\\alpha}\\right)}\\\\ &{\\le f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),-\\eta\\mathrm{sign}(\\mathbf{x}_{t})\\rangle}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\left(1+\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)+2K_{2}\\right)}\\\\ &{\\le f(\\mathbf{x}_{t})+\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathbf{sign}(\\mathbf{x}_{t})\\rangle-\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))\\rangle}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\left(1+\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)+2K_{2}\\right)}\\\\ &{=f(\\mathbf{x}_{t})+\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathrm{Sign}(\\nabla f(\\mathbf{x}_{t}))-\\mathbf{Sign}(\\mathbf{x}_{t})\\rangle-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left((K_{0}+K_{1}+2K_{2})+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)}\\\\ &{\\le\\int(\\mathbf{x}_{t})+2\\eta\\sqrt{d}|\\nabla f(\\mathbf{x}_{t})-\\mathbf{x}_{t}|\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}+\\frac{\\eta^{2}}{2}\\left((K_{0}+K_{1}+2K_{2})+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right), \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality is due to the fact that $\\alpha<1$ and $\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|^{2}\\leq\\eta^{2}d\\leq1$ ", "page_idx": 21}, {"type": "text", "text": "Rearranging and summing up, we then have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\le\\!\\!\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\frac{\\eta d(K_{0}+K_{1}+2K_{2})}{2}+\\frac{\\eta d K_{1}}{2T}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\eta\\leq\\operatorname*{min}\\lbrace\\frac{1}{\\sqrt{d}},\\frac{1}{d K_{1}}\\rbrace}\\end{array}$ , we can get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\frac{2\\Delta_{f}}{\\eta T}+4\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+\\eta d(K_{0}+K_{1}+2K_{2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\alpha=1$ according toLemma and setting $\\begin{array}{r}{\\eta\\leq\\frac{1}{L_{1}\\sqrt{d}}}\\end{array}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad f(\\mathbf{x}_{t+1})}\\\\ &{\\leq f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(L_{0}+L_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)\\exp\\left(L_{1}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|\\right)}\\\\ &{\\leq f(\\mathbf{x}_{t})+\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{\\exp\\left(1\\right)}{2}\\eta^{2}d\\left(L_{0}+L_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)}\\\\ &{\\leq f(\\mathbf{x}_{t})+2\\eta\\sqrt{d}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}+\\frac{3\\eta^{2}d}{2}\\left(L_{0}+L_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality is due to $L_{1}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|\\leq1$ , and others follow the previous proof. Rearranging and summing up, we then have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]}\\\\ &{\\displaystyle\\le\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+\\frac{3\\eta d L_{0}}{2}+\\frac{3\\eta d L_{1}}{2T}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\eta\\le\\frac{1}{3d L_{1}}}\\end{array}$ , we can get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\frac{2\\Delta_{f}}{\\eta T}+4\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+3\\eta d L_{0}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we begin to bound the term $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]}\\end{array}$ . According to the defnition of $\\mathbf{v}_{t}$ \uff0c we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})=\\!(1-\\beta)\\left(\\mathbf{v}_{t-1}-\\nabla f(\\mathbf{x}_{t-1})\\right)+\\beta\\left(\\cfrac{1}{B_{1}}\\displaystyle\\sum_{k=1}^{B_{1}}\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t})\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left(1-\\beta\\right)\\left(\\cfrac{1}{B_{1}}\\displaystyle\\sum_{k=1}^{B_{1}}\\left(\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})\\right)+\\nabla f(\\mathbf{x}_{t-1})-\\nabla f(\\mathbf{x}_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Denote that $\\begin{array}{r}{G_{t}\\,=\\,\\Big(\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\big(\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})\\big)+\\nabla f(\\mathbf{x}_{t-1})-\\nabla f(\\mathbf{x}_{t})\\Big)}\\end{array}$ and $\\Delta_{t}\\,=$ $\\begin{array}{r}{\\frac{1}{B_{1}}\\sum_{k=1}^{B_{1}}\\left(\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t})\\right)}\\end{array}$ . By summing up, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})}\\\\ &{=(1-\\beta)(\\mathbf{v}_{t-1}-\\nabla f(\\mathbf{x}_{t-1}))+\\beta\\Delta_{t}+(1-\\beta)G_{t}}\\\\ &{=\\cdot\\cdot\\cdot}\\\\ &{=(1-\\beta)^{t-1}\\left(\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right)+\\beta\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}+(1-\\beta)\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we can know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\|\\right]\\le(1-\\beta)^{t-1}\\mathbb{E}\\left[\\|\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;\\beta\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]+(1-\\beta)\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we give the following two important lemmas, and their proofs can be found in Appendix E.1.1 and Appendix E.1.2, respectively. ", "page_idx": 22}, {"type": "text", "text": "Lemma 3 ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\sqrt{\\left\\|\\displaystyle\\sum_{s=1}^{t-r}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|^{2}+\\frac{1}{B_{1}}\\displaystyle\\sum_{s=1}^{r}(1-\\beta)^{2s-2}\\Lambda^{2}}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{s=t+1-r}^{t}\\frac{\\Gamma}{\\sqrt{B_{1}}}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 4 ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\sqrt{\\left\\|\\displaystyle\\sum_{s=1}^{t-r}(1-\\beta)^{t-s}G_{s}\\right\\|^{2}+\\displaystyle\\frac{1}{B_{1}}\\sum_{s=1}^{r}2(1-\\beta)^{2s-2}\\eta^{2}L_{3}^{2}d}\\right]}&{}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{\\sqrt{2d}\\eta L_{4}}{\\sqrt{B_{1}}}\\sum_{s=t+1-r}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]\\leq\\sqrt{\\frac{\\Delta_{2}}{B_{1}}\\sum_{s=1}^{t}(1-\\beta)^{2s-2}+\\displaystyle\\sum_{s=1}^{t}\\frac{\\Gamma}{\\sqrt{B_{1}}}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\Lambda}{\\sqrt{B_{1}\\beta}}+\\displaystyle\\sum_{s=1}^{t}\\frac{\\Gamma}{\\sqrt{B_{1}}}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\\\ &{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]\\leq\\sqrt{\\frac{2\\eta^{2}L_{3}^{2}d}{B_{1}}\\sum_{s=1}^{t}(1-\\beta)^{2s-2}}+\\displaystyle\\frac{\\sqrt{2d}\\eta L_{4}}{\\sqrt{B_{1}}}\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{2d}p L_{3}}{\\sqrt{B_{1}\\beta}}+\\frac{\\sqrt{2d}\\eta L_{4}}{\\sqrt{B_{1}}}\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining above inequalities and setting $\\beta=\\eta\\sqrt{d}$ ,we derive ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathbb{E}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\|\\nabla f(\\mathbf{x}_{i})-\\mathbf{v}_{i}\\|\\right]}\\\\ &{\\overset{,}{\\le}\\frac{1}{T}\\sum_{i=1}^{T}(1-\\beta)^{i-1}\\mathbb{E}\\|\\mathbf{v}_{i}-\\nabla f(\\mathbf{x}_{i})\\|+\\beta\\frac{1}{T}\\frac{T}{\\sum_{i=1}^{T}}\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sum_{i=1}^{T}\\beta}[\\mathbf{1}_{i}-\\beta]^{i-*-\\alpha_{\\Delta_{i}}}\\right\\|\\right]}\\\\ &{~~~+\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sum_{i=1}^{T}\\beta}[\\mathbf{1}_{i}-\\beta]^{-\\alpha_{\\Delta_{i}}}\\mathbb{E}\\right\\|\\right]}\\\\ &{\\overset{,}{\\le}\\frac{\\sigma}{\\sqrt{B_{\\sigma}T}}\\frac{1}{T}\\sum_{i=1}^{T}(1-\\beta)^{i-1}+\\frac{\\Delta_{i}^{2}\\sqrt{B_{\\sigma}}}{\\sqrt{B_{\\sigma}T}}+\\frac{\\sqrt{2A}I_{\\Delta}I_{\\Delta}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}+\\left(\\frac{\\sqrt{B_{\\sigma}T}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}+\\frac{\\sqrt{2A}I_{\\Delta}I_{\\Delta}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}\\right)\\frac{1}{T}\\sum_{i=1}^{T}(1-\\beta)^{i-1}\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sqrt{B_{\\sigma}B_{\\sigma}}}\\right\\|\\right]}\\\\ &{\\overset{,}{\\le}\\frac{\\sigma}{\\sqrt{B_{\\sigma}T}}+\\frac{\\Delta_{i}^{2}\\sqrt{B_{\\sigma}}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}+\\sqrt{2B_{\\sigma}B_{\\sigma}}+\\left(\\frac{\\sqrt{B_{\\sigma}T}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}+\\frac{\\sqrt{2B}I_{\\Delta}I_{\\Delta}}{\\sqrt{B_{\\sigma}B_{\\sigma}}}\\right)\\left(\\frac{T}{T}(1-\\beta)^{i}\\right)\\frac{1}{T}\\frac{T}{\\sum_{i=1}^{T}}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{i})\\|\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\alpha\\in(0,1)$ , by setting that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{0}=1,}\\\\ &{B_{1}\\geq\\operatorname*{max}\\{256\\Gamma^{2},512L_{4}^{2}\\}d,}\\\\ &{\\;\\;\\beta=\\frac{d^{1/3}}{T^{2/3}},}\\\\ &{\\eta=\\frac{1}{d^{1/6}T^{2/3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and suppose that iteration number ", "page_idx": 23}, {"type": "equation", "text": "$$\nT\\geq\\mathcal{O}(d^{2}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then we can guarantee ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]}\\\\ &{\\le\\frac{2\\Delta f}{\\eta T}+\\eta d(K_{0}+K_{1}+2K_{2})+\\frac{4\\sigma}{\\eta T\\sqrt{B_{0}}}+\\frac{4(\\Lambda+\\sqrt{2}L_{3})d^{3/4}\\eta^{1/2}}{\\sqrt{B_{1}}}}\\\\ &{\\quad+\\left(\\frac{4\\sqrt{d}\\Gamma}{\\sqrt{B_{1}}}+\\frac{4\\sqrt{2d}L_{4}}{\\sqrt{B_{1}}}\\right)\\displaystyle\\frac{1}{T}\\mathop{\\mathbb{E}}_{\\Gamma=1}^{T}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]}\\\\ &{\\le\\mathcal{O}\\left(\\frac{d^{1/6}}{T^{1/3}}\\right)+\\frac{1}{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]}\\\\ &{\\le\\mathcal{O}\\left(\\frac{d^{1/6}}{T^{1/3}}\\right)+\\frac{1}{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which indicates that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/6}}{T^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that the batch size for each iteration is $B_{1}={\\mathcal{O}}(d)$ , by assuming that $N=B_{1}*T$ ,weknow that the convergence concerning $N$ is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{d^{1/6}}{T^{1/3}}\\right)=\\mathcal{O}\\left(\\frac{d^{1/2}}{(B_{1}T)^{1/3}}\\right)=\\mathcal{O}\\left(\\frac{d^{1/2}}{N^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similar results can be easily obtained for $\\alpha=1$ , i.e., we can also guarantee the following for $\\alpha=1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{d^{1/6}}{N^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E.1.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We prove this lemma by mathematical induction. ", "page_idx": 24}, {"type": "text", "text": "1) When $r=0$ , we have the following: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert\\right]=\\mathbb{E}\\left[\\sqrt{\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert^{2}}\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which satisfies the above lemma. ", "page_idx": 24}, {"type": "text", "text": "2) Then, suppose the lemma holds for $r=k$ .For $r=k+1$ ,wehave ", "page_idx": 25}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/2223dfcc6212e86815777979bca3768fb9c744765689a8e8e838ac53235745d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "where the second inequality is due to the Jensen Inequality. ", "page_idx": 25}, {"type": "text", "text": "E.1.2 Proof of Lemma 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We prove this lemma by mathematical induction. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{~When~}r=0,\\mathrm{we~can~easily~prove~}\\mathbb{E}\\left[\\left\\|\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]=\\mathbb{E}\\left[\\sqrt{\\left\\|\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/0324bff1b17135b4654f988a56d7fef2dff1fba5e32a5c749c6e556739235cea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "where the third inequality is due to the following, for simplify we denote $\\xi_{t-k}=\\xi_{t-k}^{1},...,\\xi_{t-k}^{B_{1}}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi_{t-k}}\\left[\\left\\|\\mathcal{Q}_{t-k}\\right\\|^{2}\\right]=\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B{\\frac{1}{2}}}\\Big\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j})+\\nabla f(\\mathbf{x}_{t-k-1})-\\nabla f(\\mathbf{x}_{t-k})}\\\\ &{=\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B{\\frac{1}{2}}}\\Big\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j})\\Big\\|^{2}\\right]+\\displaystyle\\frac{1}{B_{1}}\\left[\\big\\|\\nabla f(\\mathbf{x}_{t-k-1})-\\nabla f(\\mathbf{x}_{t-k-1})\\big\\|^{2}\\right]}\\\\ &{\\quad-2\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B{\\frac{1}{2}}}\\Big\\langle\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j}),\\nabla f(\\mathbf{x}_{t-k-1})-\\nabla f(\\mathbf{x}_{t-k-1})\\Big\\rangle\\right]}\\\\ &{=\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B{\\frac{1}{2}}}\\Big\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j})\\Big\\|^{2}\\right]-\\displaystyle\\frac{1}{B_{1}}\\left[\\big\\|\\nabla f(\\mathbf{x}_{t-k-1})-\\nabla f(\\mathbf{x}_{t-k})\\big\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B{\\frac{1}{2}}}\\Big\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^ \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $\\alpha\\in(0,1)$ , denoting $L_{3}^{2}=\\left(\\overline{{K}}_{0}+\\overline{{K}}_{1}+\\overline{{K}}_{2}\\right)^{2}+\\overline{{K}}_{1}^{2}\\Lambda^{2}$ , and $L_{4}^{2}=\\overline{{K}}_{1}^{2}(1+\\Gamma^{2})$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\ell\\rightarrow-\\infty}\\left|\\left|\\frac{1}{B_{1}^{2}}\\sum_{s=1}^{\\beta_{1}}\\right|\\nabla f(\\mathbf{x}_{t-k};\\ell_{s-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j})\\right|^{2}\\right|}\\\\ &{\\leq\\frac{1}{B_{1}^{2}}\\frac{B_{1}}{j_{1}}\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|^{2}\\left(\\overline{{K}}_{0}+\\overline{{K}}_{1}\\mathbb{E}_{\\ell\\rightarrow-\\infty}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})\\right\\|^{\\alpha}\\right]+\\overline{{K}}_{2}\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|^{\\alpha}\\right|\\right.}\\\\ &{\\leq\\frac{\\eta^{2}d_{2}^{\\alpha}}{B_{1}^{2}}\\frac{B_{1}}{j_{2}\\ln^{2}}\\left(\\overline{{K}}_{0}+\\overline{{K}}_{1}+\\overline{{K}}_{2}+\\overline{{K}}_{1}\\mathbb{E}_{\\ell\\rightarrow-\\infty}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})\\right\\|\\right]\\right)^{2}}\\\\ &{\\leq\\frac{2\\eta^{2}d_{1}^{\\alpha}}{B_{1}}\\left(\\overline{{K}}_{0}+\\overline{{K}}_{1}+\\overline{{K}}_{2}\\right)^{2}+\\frac{2\\eta^{2}d_{2}^{\\alpha}}{B_{1}^{2}}\\sum_{s=1}^{\\beta_{1}}\\overline{{K}}_{1}^{2}\\left(\\mathbb{E}_{\\ell\\rightarrow-\\infty}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})\\right\\|\\right]\\right)^{2}}\\\\ &{\\leq\\frac{2\\eta^{2}d_{1}^{\\alpha}}{B_{1}}(\\overline{{K}}_{0}+\\overline{{K}}_{1}+\\overline{{K}}_{2})^{2}+\\frac{2\\eta^{2}d_{1}^{\\alpha}}{B_{1}^{2}}\\overline{{K}}_{1}^{2}\\left((1+\\Gamma^{2})\\left\\|\\nabla f(\\mathbf{x}_{t\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality holds by setting $\\eta\\le d^{-1/2}$ such that $\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|\\leq\\eta\\sqrt{d}\\leq1$ ", "page_idx": 27}, {"type": "text", "text": "For $\\alpha=1$ 1, uenouig $L_{3}^{2}=3(L_{0}^{2}+2L_{1}^{2}\\Lambda^{2})$ , and $L_{4}^{2}=6L_{1}^{2}(1+\\Gamma^{2})$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi_{t-k}}\\left[\\displaystyle\\frac{1}{B_{1}^{2}}\\sum_{j=1}^{B_{1}}\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})-\\nabla f(\\mathbf{x}_{t-k-1};\\xi_{t-k}^{j})\\right\\|^{2}\\right]}\\\\ &{\\le\\displaystyle\\frac{2}{B_{1}^{2}}\\sum_{j=1}^{B_{1}}\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|^{2}\\left(L_{0}^{2}+2L_{1}^{2}\\mathbb{E}_{\\xi_{t-k}}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})\\right\\|^{2}\\right]\\right)\\exp\\left(12L_{1}^{2}\\left\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\|^{2}\\right)}\\\\ &{\\le\\displaystyle\\frac{6\\eta^{2}d}{B_{1}^{2}}\\sum_{j=1}^{B_{1}}\\left(L_{0}^{2}+2L_{1}^{2}\\mathbb{E}_{\\xi_{t-k}}\\left[\\left\\|\\nabla f(\\mathbf{x}_{t-k};\\xi_{t-k}^{j})\\right\\|^{2}\\right]\\right)}\\\\ &{\\le\\displaystyle\\frac{6\\eta^{2}d}{B_{1}}(L_{0}^{2}+2L_{1}^{2}\\big((1+\\Gamma^{2})\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|^{2}+\\Lambda^{2}\\big))}\\\\ &{\\le\\displaystyle\\frac{2\\eta^{2}d L_{1}^{2}}{B_{1}}+\\frac{2\\eta^{2}d L_{1}^{2}}{B_{1}}\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the secdinequaltyhlsbysen $\\begin{array}{r}{\\eta\\leq\\frac{1}{\\sqrt{12L_{1}^{2}d}}}\\end{array}$ such that $\\begin{array}{r}{\\left\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\|^{2}\\leq\\frac{1}{12L_{1}^{2}}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "E.2 Proof of Theorem 6 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Similar to Lemma 1 and 2, we can have the following lemma for generalized individual smoothness. ", "page_idx": 28}, {"type": "text", "text": "Lemma 5 For $\\alpha\\in(0,1)$ ,generalized individual smoothness (Assumption $3^{\\prime}$ )leadsto ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{i}({\\mathbf x}_{t+1})\\le f_{i}({\\mathbf x}_{t})+\\langle\\nabla f_{i}({\\mathbf x}_{t}),{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\rangle}\\\\ {\\qquad\\qquad+\\,\\frac{1}{2}\\|{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\|\\nabla f({\\mathbf x}_{t})\\|^{\\alpha}+2K_{2}\\|{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\|^{\\frac{\\alpha}{1-\\alpha}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as well as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f_{i}(\\mathbf{x}_{t+1})-\\nabla f_{i}(\\mathbf{x}_{t})\\|\\le\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|\\big(K_{0}+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|^{\\alpha}+K_{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{\\frac{\\alpha}{1-\\alpha}}\\big).}\\\\ &{\\,\\,e\\,K_{0}:=L_{0}\\big(2^{\\frac{\\alpha^{2}}{1-\\alpha}}+1\\big),\\,K_{1}:=L_{1}\\cdot2^{\\frac{\\alpha^{2}}{1-\\alpha}}\\cdot3^{\\alpha},\\,K_{2}:=L_{1}^{\\frac{1}{1-\\alpha}}\\cdot2^{\\frac{\\alpha^{2}}{1-\\alpha}}\\cdot3^{\\alpha}(1-\\alpha)^{\\frac{\\alpha}{1-\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 6For $\\alpha=1$ generalized individual smoothness (Assumption $3^{\\prime}$ )leadsto ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{i}({\\bf x}_{t+1})\\le f_{i}({\\bf x}_{t})+\\langle\\nabla f_{i}({\\bf x}_{t}),{\\bf x}_{t+1}-{\\bf x}_{t}\\rangle}\\\\ {\\displaystyle~~~~~~~~~~~~~~~+\\frac{1}{2}\\|{\\bf x}_{t+1}-{\\bf x}_{t}\\|^{2}\\left(L_{0}+L_{1}\\|\\nabla f({\\bf x}_{t})\\|\\right)\\exp\\left(L_{1}\\|{\\bf x}_{t+1}-{\\bf x}_{t}\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as well as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f_{i}(\\mathbf{x}_{t+1})-\\nabla f_{i}(\\mathbf{x}_{t})\\|\\leq\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|\\big(L_{0}+L_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\big)\\exp\\big(L_{1}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, we can begin our proof. For $\\alpha\\in(0,1)$ , according to Lemma 5, by setting $\\eta\\leq d^{-\\frac{1}{2}}$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;f(\\mathbf{x}_{t+1})-f(\\mathbf{x}_{t})}\\\\ &{\\leq\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}f_{i}(\\mathbf{x}_{t+1})-\\frac{1}{m}\\sum_{i=1}^{m}f_{i}(\\mathbf{x}_{t})}\\\\ &{\\leq\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\left\\langle\\nabla f_{i}(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\rangle+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|^{\\alpha}+2K_{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{\\alpha}\\right)}\\\\ &{\\leq\\langle\\nabla f(\\mathbf{x}_{t}),-\\eta\\mathbf{\\delta}\\mathbf{x}_{t}\\mathbf{\\mathrm{pap}}(\\mathbf{v}_{t})\\rangle+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\left(K_{0}+K_{1}\\left(\\mathbf{1}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right)+2K_{2}\\right)}\\\\ &{\\leq\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{0}\\mathbf{p}(\\nabla f(\\mathbf{x}_{t}))-\\mathbf{x}_{0}\\mathbf{p}(\\mathbf{v}_{t})\\rangle-\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{0}\\mathbf{p}(\\nabla f(\\mathbf{x}_{t}))\\rangle}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}(K_{0}+K_{1}\\left(\\mathbf{1}\\|\\nabla f(\\mathbf{x}_{t})\\|+2K_{2}\\right)}\\\\ &{=\\eta\\langle\\nabla f(\\mathbf{x}_{t}),\\mathbf{x}_{0}\\mathbf{p}(\\nabla f(\\mathbf{x}_{t}))-\\mathbf{\\delta}\\mathbf{x}_{0}\\mathbf{p}(\\mathbf{v}_{t})\\rangle-\\eta\\|\\nabla f(\\mathbf{x}_{t})\\|}\\\\ &{\\quad+\\frac{1}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}((K_{0}+K_{1}+2K_{2})+K_{1}\\|\\nabla f(\\mathbf{x}_{t})\\|)}\\\\ & \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second inequality is due to the fact that $\\alpha<1$ and $\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|^{2}\\leq\\eta^{2}d\\leq1$ ", "page_idx": 28}, {"type": "text", "text": "Rearranging and summing up, we then have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\le\\!\\!\\frac{\\Delta_{f}}{\\eta T}+2\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\frac{\\eta d(K_{0}+K_{1}+2K_{2})}{2}+\\frac{\\eta d K_{1}}{2T}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\eta\\leq\\operatorname*{min}\\lbrace\\frac{1}{\\sqrt{d}},\\frac{1}{d K_{1}}\\rbrace}\\end{array}$ , we can get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\frac{2\\Delta_{f}}{\\eta T}+4\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+\\eta d(K_{0}+K_{1}+2K_{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For $\\alpha=1$ , by setting $\\begin{array}{r}{\\eta\\leq\\frac{1}{3d L_{1}}}\\end{array}$ ,we can apply the very similar analysis and obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})\\|_{1}\\right]\\leq\\frac{2\\Delta_{f}}{\\eta T}+4\\sqrt{d}\\cdot\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]+3\\eta d L_{0}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we bound the term $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]}\\end{array}$ According to the definition of $\\mathbf{v}_{t}$ , we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})=\\!(1-\\beta)\\left(\\mathbf{v}_{t-1}-\\nabla f(\\mathbf{x}_{t-1})\\right)+\\beta\\left(\\mathbf{h}_{t}-\\nabla f(\\mathbf{x}_{t})\\right)\\!}\\\\ &{\\qquad\\qquad\\qquad+\\left(1-\\beta\\right)\\left(\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})+\\nabla f(\\mathbf{x}_{t-1})-\\nabla f(\\mathbf{x}_{t})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denote that $G_{t}=\\left(\\nabla f(\\mathbf{x}_{t};\\xi_{t}^{k})-\\nabla f(\\mathbf{x}_{t-1};\\xi_{t}^{k})+\\nabla f(\\mathbf{x}_{t-1})-\\nabla f(\\mathbf{x}_{t})\\right)$ and $\\Delta_{t}=\\mathbf{h}_{t}-\\nabla f(\\mathbf{x}_{t})$ By summing up, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})}\\\\ &{=\\!(1-\\beta)(\\mathbf{v}_{t-1}-\\nabla f(\\mathbf{x}_{t-1}))+\\beta\\Delta_{t}+(1-\\beta)G_{t}}\\\\ &{=\\!(1-\\beta)^{t-1}\\left(\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right)+\\beta\\displaystyle\\sum_{s=1}^{t}\\!(1-\\beta)^{t-s}\\Delta_{s}+(1-\\beta)\\displaystyle\\sum_{s=1}^{t}\\!(1-\\beta)^{t-s}G_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, we can know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\Vert\\right]}\\\\ &{\\leq(1-\\beta)^{t-1}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{1}-\\nabla f(\\mathbf{x}_{1})\\right\\Vert\\right]+\\beta\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\Vert\\right]+(1-\\beta)\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\Vert\\right]}\\\\ &{\\leq\\beta\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\Vert\\right]+(1-\\beta)\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\Vert\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first term vanishes since we use full batch in the first iteration. ", "page_idx": 29}, {"type": "text", "text": "Then we give the following two important lemmas, and their proofs can be found in Appendix E.2.1 and Appendix E.2.2, respectively. ", "page_idx": 29}, {"type": "text", "text": "Lemma 7 ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert\\right]\\leq\\mathbb{E}\\left[\\sqrt{\\left\\lVert\\sum_{s=1}^{t-r}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert^{2}+\\sum_{s=1}^{r}2(1-\\beta)^{2s-2}\\eta^{2}I^{2}L_{5}^{2}d}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 8 ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\sqrt{\\left\\|\\displaystyle\\sum_{s=1}^{t-r}(1-\\beta)^{t-s}G_{s}\\right\\|^{2}+\\displaystyle\\sum_{s=1}^{r}2(1-\\beta)^{2s-2}\\eta^{2}L_{7}^{2}d}\\right]}&{}\\\\ &{\\qquad\\qquad+\\displaystyle\\sqrt{2d}\\eta L_{8}\\displaystyle\\sum_{s=t+1-r}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using these lemmas and setting $r=t$ , we then have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]}\\\\ &{\\leq\\sqrt{2\\eta^{2}I^{2}L_{5}^{2}d\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{2s-2}}+\\sqrt{2d}\\eta I L_{6}\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\\\ &{\\leq\\frac{\\sqrt{2d}\\eta I L_{5}}{\\sqrt{\\beta}}+\\sqrt{2d}\\eta I L_{6}\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "as well as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]}\\\\ &{\\leq\\!\\sqrt{2\\eta^{2}L_{7}^{2}d\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{2s-2}}+\\sqrt{2d}\\eta L_{8}\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\\\ &{\\leq\\!\\frac{\\sqrt{2d}\\eta L_{7}}{\\sqrt{\\beta}}+\\sqrt{2d}\\eta L_{8}\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining above inequalities and setting $\\beta=1/m$ and $I=m$ ,we derive ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf{x}_{t})-\\mathbf{v}_{t}\\|\\right]}\\\\ &{\\le\\beta\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]+\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\|\\right]}\\\\ &{\\le\\sqrt{2d\\beta}\\eta L E_{3}+\\displaystyle\\frac{\\sqrt{2d}I}{\\sqrt{\\beta}}D L_{7}+\\Big(\\sqrt{2d}\\eta\\beta I L G_{6}+\\sqrt{2d}I L_{8}\\Big)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\sum_{s=1}^{T}(1-\\beta)^{t-s}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{s})\\|\\right]}\\\\ &{\\le\\sqrt{2d m}\\eta(L_{5}+L\\tau)+\\sqrt{2d}\\eta(L_{6}+L_{8})\\left(\\displaystyle\\sum_{i=1}^{T}(1-\\beta)^{i}\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{t})\\|\\right]}\\\\ &{\\le\\sqrt{2d m}\\eta(L_{5}+L\\tau)+\\sqrt{2d}\\eta m(L_{6}+L_{8})\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\|\\nabla f(\\mathbf{x}_{t})\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $\\alpha\\in(0,1)$ , by seting $\\begin{array}{r}{\\eta=\\operatorname*{min}\\left\\{\\frac{1}{m^{1/4}d^{1/2}T^{1/2}},\\frac{1}{8\\sqrt{2}m d(L_{6}+L_{8}+1)}\\right\\}}\\end{array}$ we can guarantee ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f({\\mathbf x}_{t})\\|_{1}\\right]}\\\\ {\\displaystyle\\le\\frac{2\\Delta_{f}}{\\eta T}+\\eta d(K_{0}+K_{1}+2K_{2})+4d\\sqrt{2m}\\eta(L_{5}+L_{7})+4d\\sqrt{2}\\eta m(L_{6}+L_{8})\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\left[\\|\\nabla f({\\mathbf x}_{t})\\|_{1}\\right]}\\\\ {\\displaystyle\\le\\mathcal{O}\\left(\\frac{m^{1/4}d^{1/2}}{T^{1/2}}+\\frac{m d}{T}\\right)+\\frac{1}{2}\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f({\\mathbf x}_{t})\\|_{1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which indicates that $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla f(\\mathbf x_{t})\\|_{1}\\right]\\leq\\mathcal{O}\\left(\\frac{m^{1/4}d^{1/2}}{T^{1/2}}+\\frac{m d}{T}\\right)}\\end{array}$ Similar results can be easly obtained for $\\alpha=1$ ", "page_idx": 30}, {"type": "text", "text": "E.2.1 Proof of Lemma 7 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We prove this lemma by mathematical induction. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1) When $r=0$ , we have the following: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert\\right]=\\mathbb{E}\\left[\\sqrt{\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\rVert^{2}}\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which satisfies the above lemma. ", "page_idx": 31}, {"type": "text", "text": "2) Then, suppose the lemma holds for $r=k$ .For $r=k+1$ ,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\underset{s=1}{\\overset{t}{\\prod}}_{1=1}^{t}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\sqrt{\\left\\|\\underset{s=1}{\\overset{t}{\\prod}}(1-\\beta)^{t-s}\\Delta_{s}\\right\\|^{2}+\\underset{s=1}{\\overset{t}{\\sum}}2(1-\\beta)^{2s-2}\\eta^{2}I^{2}L_{s}^{2}}\\right]}\\\\ &{\\qquad+\\sqrt{2a}I L_{a}L_{a}\\underset{s=t+1-k}{\\overset{t}{\\sum}}\\quad(1-\\beta)^{t-s}\\mathbb{E}\\|\\nabla f(\\mathbf{x}_{s})\\|}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}_{i\\rightarrow\\infty}\\left[\\sqrt{\\left\\|\\underset{s=1}{\\overset{t}{\\sum}}(1-\\beta)^{t-s}\\Delta_{s}+(1-\\beta)^{k}\\Delta_{t-s}\\right\\|^{2}+\\underset{s=1}{\\overset{t}{\\sum}}2(1-\\beta)^{2s-2}\\eta^{2}I^{2}L_{s}^{2}}\\right]\\right]}\\\\ &{\\qquad+\\sqrt{2a}I L_{a}\\underset{s=t+1-k}{\\overset{t}{\\sum}}\\quad(1-\\beta)^{t-s}\\mathbb{E}\\|\\nabla f(\\mathbf{x}_{s})\\|}\\\\ &{\\qquad+\\underset{s=t+1-k}{\\overset{t}{\\sum}}\\quad(1-\\beta)^{t-s}\\mathbb{E}\\|\\nabla f(\\mathbf{x}_{s})\\|\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\Bigg|\\frac{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}-\\alpha^{j}\\Delta_{i}+(1-\\beta^{j})^{i}\\Delta_{i}\\Bigg)\\Bigg|_{1}+\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})^{j}\\Delta_{i}\\cdot\\beta^{j}\\cdot\\mathbb{I}^{i}[\\Delta\\xi_{i}]}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})^{j}}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})^{j}}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum_{i=1}^{n}(1-\\beta^{i})}{\\displaystyle\\sum_{i=1}^{n}\\Bigg(1-\\beta^{i}\\Delta_{i}\\cdot\\frac{\\displaystyle\\sum\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the third inequality is due to the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\Delta_{t-k}\\right\\Vert^{2}\\right]=\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k})-\\nabla f_{i_{t-k}}(\\mathbf{x}_{\\tau})+\\nabla f(\\mathbf{x}_{\\tau})-\\nabla f(\\mathbf{x}_{t-k})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k})-\\nabla f_{i_{t-k}}(\\mathbf{x}_{\\tau})\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $\\alpha\\in(0,1)$ , denoting that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{L_{5}^{2}=\\left(K_{0}+K_{1}+K_{2}\\right)^{2},}}\\\\ {{L_{6}^{2}=K_{1}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i_{t-k}}\\left[\\left\\|\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{\\tau}\\big)\\right\\|^{2}\\right]}\\\\ &{\\leq\\left\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{\\tau}\\right\\|^{2}\\left(K_{0}+K_{1}\\left\\|\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\|^{\\alpha}+K_{2}\\big\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{\\tau}\\big\\|^{\\frac{\\alpha}{1-\\alpha}}\\right)^{2}}\\\\ &{\\leq\\eta^{2}I^{2}d\\left(K_{0}+K_{1}+K_{2}+K_{1}\\left\\|\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\|\\right)^{2}}\\\\ &{\\leq2\\eta^{2}I^{2}d\\left(K_{0}+K_{1}+K_{2}\\right)^{2}+2\\eta^{2}I^{2}d K_{1}^{2}\\left\\|\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\|^{2}}\\\\ &{\\leq2\\eta^{2}d I^{2}L_{5}^{2}+2\\eta^{2}I^{2}d L_{6}^{2}\\left\\|\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality holds by setting ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta\\le I^{-1}d^{-1/2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{\\tau}\\|\\leq\\eta I\\sqrt{d}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $\\alpha=1$ , denoting that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{L_{5}^{2}=9L_{0}^{2}}}\\\\ {{L_{6}^{2}=9L_{1}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{\\tau}\\big)\\right\\Vert^{2}\\right]}\\\\ &{\\leq2\\left\\Vert\\mathbf{x}_{t-k}-\\mathbf{x}_{\\tau}\\right\\Vert^{2}\\left(L_{0}^{2}+L_{1}^{2}\\left\\Vert\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\Vert^{2}\\right)\\left(\\exp\\left(L_{1}^{2}\\left\\Vert\\mathbf{x}_{t-k}-\\mathbf{x}_{\\tau}\\right\\Vert^{2}\\right)\\right)^{2}}\\\\ &{\\leq18\\eta^{2}d I^{2}\\left(L_{0}^{2}+L_{1}^{2}\\left\\Vert\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\Vert^{2}\\right)}\\\\ &{\\leq2\\eta^{2}I^{2}d L_{5}^{2}+2\\eta^{2}I^{2}d L_{6}^{2}\\left\\Vert\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality holds by setting ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{\\sqrt{L_{1}^{2}I^{2}d}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "such that we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|^{2}\\leq\\eta^{2}I^{2}d\\leq\\frac{1}{L_{1}^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "E.2.2Proof of Lemma 8 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We prove this lemma by mathematical induction. ", "page_idx": 32}, {"type": "text", "text": "1) When $r=0$ , we can easily prove that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\rVert\\right]=\\mathbb{E}\\left[\\sqrt{\\left\\lVert\\sum_{s=1}^{t}(1-\\beta)^{t-s}G_{s}\\right\\rVert^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/5ec0f9983ce72c58f8db45d63dce92fdde6ce3ed9ed6ff1842ef257d2cf25984.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "where the third inequality is due to the following: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert G_{t-k}\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k-1}\\big)+\\nabla f\\big(\\mathbf{x}_{t-k-1}\\big)-\\nabla f(\\mathbf{x}_{t-k})\\big\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k-1}\\big)\\right\\Vert^{2}\\right]+\\left[\\left\\Vert\\nabla f\\big(\\mathbf{x}_{t-k-1}\\big)-\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\Vert^{2}\\right]}\\\\ &{\\quad-\\ 2\\mathbb{E}_{i_{t-k}}\\left[\\big\\langle\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k-1}\\big),\\nabla f\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f\\big(\\mathbf{x}_{t-k-1}\\big)\\big\\rangle\\right]}\\\\ &{=\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k-1}\\big)\\right\\Vert^{2}\\right]-\\left[\\left\\Vert\\nabla f\\big(\\mathbf{x}_{t-k-1}\\big)-\\nabla f\\big(\\mathbf{x}_{t-k}\\big)\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k}\\big)-\\nabla f_{i_{t-k}}\\big(\\mathbf{x}_{t-k-1}\\big)\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $\\alpha\\in(0,1)$ , denoting $L_{7}^{2}=\\left(K_{0}+K_{1}+K_{2}\\right)^{2}$ , and $L_{8}^{2}=K_{1}^{2}$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{i_{t-k}}\\left[\\left\\|\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k})-\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k-1})\\right\\|^{2}\\right]}\\\\ &{\\leq\\left\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\|^{2}\\left(K_{0}+K_{1}\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|^{\\alpha}+K_{2}\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|^{\\frac{\\alpha}{1-\\alpha}}\\right)^{2}}\\\\ &{\\leq\\eta^{2}d\\left(K_{0}+K_{1}+K_{2}+K_{1}\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|\\right)^{2}}\\\\ &{\\leq2\\eta^{2}d\\left(K_{0}+K_{1}+K_{2}\\right)^{2}+2\\eta^{2}d K_{1}^{2}\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|^{2}}\\\\ &{\\leq2\\eta^{2}d L_{7}^{2}+2\\eta^{2}d L_{8}^{2}\\left\\|\\nabla f(\\mathbf{x}_{t-k})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second inequality holds by setting $\\eta\\le d^{-1/2}$ such that $\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\|\\leq\\eta\\sqrt{d}\\leq1$ For $\\alpha=1$ , denoting $L_{7}^{2}=9L_{0}^{2}$ and $L_{4}^{2}=9L_{1}^{2}$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i_{t-k}}\\left[\\left\\Vert\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k})-\\nabla f_{i_{t-k}}(\\mathbf{x}_{t-k-1})\\right\\Vert^{2}\\right]}\\\\ &{\\leq2\\left\\Vert\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\Vert^{2}\\left(L_{0}^{2}+L_{1}^{2}\\left\\Vert\\nabla f(\\mathbf{x}_{t-k})\\right\\Vert^{2}\\right)\\left(\\exp\\left(L_{1}^{2}\\left\\Vert\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\Vert^{2}\\right)\\right)^{2}}\\\\ &{\\leq18\\eta^{2}d\\left(L_{0}^{2}+2L_{1}^{2}\\left\\Vert\\nabla f(\\mathbf{x}_{t-k})\\right\\Vert^{2}\\right)}\\\\ &{\\leq2\\eta^{2}d L_{7}^{2}+2\\eta^{2}d L_{8}^{2}\\left\\Vert\\nabla f(\\mathbf{x}_{t-k})\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "wherethescond inquality holds by seting $\\begin{array}{r}{\\eta\\leq\\frac{1}{\\sqrt{L_{1}^{2}d}}}\\end{array}$ suchthat wehave $\\left\\|\\mathbf{x}_{t-k}-\\mathbf{x}_{t-k-1}\\right\\|^{2}\\leq$ $\\begin{array}{r}{\\eta^{2}d\\leq\\frac{1}{L_{1}^{2}}.}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "F  Additional experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we present additional experiments on the CIFAR-10 dataset to validate whether the proposed SSVR method is sensitive to hyper-parameters such as learning rate $\\eta$ , momentum parameter $\\beta$ , and batch size. Specifically, we fix the value of $\\beta$ as 0.5 and try different learning rates from the set $\\{5e-3,1e-3,\\bar{5}e-4,1\\bar{e}-4,5e-5\\}$ . Then, we fix the learning rate as $1e-3$ and enumerate $\\beta$ from the set $\\left\\lbrace0.3,0.5,0.7,0.9,0.99\\right\\rbrace$ . The results are reported in Figure 3 and Figure 4, respectively, which indicate that our method is insensitive to the choice of hyper-parameters within a certain range. Finally, we also try different batch sizes from the set $\\{64,\\dot{1}\\bar{2}8,\\bar{2}56,512\\}$ , and the results are shown in Figure 5. It can be seen that our algorithm does not necessitate large batches for convergence and is not sensitive to variations in batch sizes. ", "page_idx": 34}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/a292996ba35f8e5092b24954d4ed54fb2273fd3450f616f32025b9b3974cbf93.jpg", "img_caption": ["Figure 3: Results for CIFAR-10 dataset with different learning rates. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/0219e7b002870343e58bc6175837c524f4a72c2d67777ec90c6717ea0700166a.jpg", "img_caption": ["Figure 4: Results for CIFAR-10 dataset with different $\\beta$ "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "uaNZvF1VFe/tmp/cce560aa59e5e1cd38266dae3d2d6b3a5bef1922f17c7c7138c5c972830b68a7.jpg", "img_caption": ["Figure 5: Results for CIFAR-10 dataset with different batch sizes. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The claims presented in the abstract and introduction accurately represent the contributions and scope of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The theoretical results demonstrated in the paper rely on specific assumptions, which have been clearly stated in the main text. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper provides assumptions and proofs for each theoretical result. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper discloses the information necessary to reproduce the main experimental results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: Due to privacy concerns and ongoing research, we do not include the code. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper describes the training and testing details ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The paper reports error bars. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have provided the relevant information. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This is primarily a theoretical paper with no potential negative social impact. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use explicitly are properly respected. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]