[{"figure_path": "tZtepJBtHg/tables/tables_43_1.jpg", "caption": "Table 1: Hyperparameter summary of NN experiments. (*) we train until convergence on oracle validation accuracy.", "description": "This table summarizes the hyperparameters used in the neural network (NN) experiments described in the paper.  It shows the settings for MNIST and CIFAR-100 datasets, including the learning rate, batch size, number of epochs, and other relevant parameters. The asterisk (*) indicates that training continued until convergence on a validation set was achieved.", "section": "J Additional NN Experiments & Details"}, {"figure_path": "tZtepJBtHg/tables/tables_44_1.jpg", "caption": "Table 1: Hyperparameter summary of NN experiments. (*) we train until convergence on oracle validation accuracy.", "description": "This table summarizes the hyperparameters used in the neural network (NN) experiments described in the paper.  It lists the values for standard deviation of noise (\u03c1), the size of the target set (M), the number of samples in the target space (m), the size of the candidate set (k), batch size (b), number of epochs, and the learning rate for both MNIST and CIFAR-100 datasets.  The (*) indicates that for MNIST, training continued until convergence on the oracle validation accuracy was reached.", "section": "J Additional NN Experiments & Details"}, {"figure_path": "tZtepJBtHg/tables/tables_52_1.jpg", "caption": "Table 2: Ablation study of noise standard deviation p in the CIFAR-100 experiment. We list the accuracy after 100 rounds per decision rule, with its standard error over 10 random seeds. \"(top-b)\" denotes variants where batches are selected by taking the top-b points according to the decision rule rather than using batch selection via conditional embeddings. Shown in bold are the best performing decision rules, and shown in italics are results due to numerical instability.", "description": "This table presents an ablation study on the effect of noise standard deviation (p) on the performance of different active learning algorithms for the CIFAR-100 dataset.  It shows the accuracy achieved after 100 rounds of selection, along with standard errors across 10 random seeds.  The table compares the performance of ITL, VTL, CTL and several baselines, both with and without top-b batch selection.  Bold values indicate top performance, and italicized values highlight results affected by numerical instability.", "section": "J.7 Ablation study of noise standard deviation p"}, {"figure_path": "tZtepJBtHg/tables/tables_63_1.jpg", "caption": "Table 3: Magnitudes of \u03b3n for common kernels. The magnitudes hold under the assumption that X is compact. Here, B\u03bd is the modified Bessel function. We take the magnitudes from Theorem 5 of Srinivas et al. (2009) and Remark 2 of Vakili et al. (2021). The notation O(\u00b7) subsumes log-factors. For \u03bd = 1/2, the Mat\u00e9rn kernel is equivalent to the Laplace kernel. For \u03bd \u2192 \u221e, the Mat\u00e9rn kernel is equivalent to the Gaussian kernel. The functions sampled from a Mat\u00e9rn kernel are [\u03bd] square differentiable. The kernel-agnostic bound follows by simple reduction to a linear kernel in |X| dimensions.", "description": "This table shows the upper bounds on the information gain for different kernels, which is useful for analyzing the sample complexity of active learning algorithms.  The bounds are given in terms of the number of samples (n) and the dimensionality (d) of the input space. The table also notes the relationship between Mat\u00e9rn and Laplace/Gaussian kernels.", "section": "Additional GP Experiments & Details"}]