[{"type": "text", "text": "Transductive Active Learning: Theory and Applications ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jonas H\u00fcbotter\u2217 Department of Computer Science ETH Z\u00fcrich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Bhavya Sukhija Department of Computer Science ETH Z\u00fcrich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Lenart Treven Department of Computer Science ETH Z\u00fcrich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Yarden As Department of Computer Science ETH Z\u00fcrich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Andreas Krause Department of Computer Science ETH Z\u00fcrich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a generalization of classical active learning to real-world settings with concrete prediction targets where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. We analyze a family of decision rules that sample adaptively to minimize uncertainty about prediction targets. We are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate their strong sample efficiency in two key applications: active fine-tuning of large neural networks and safe Bayesian optimization, where they achieve state-of-the-art performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning, at its core, is about designing systems that can extract knowledge or patterns from data. One part of this challenge is determining not just how to learn given observed data but deciding what data to obtain next, given the information already available. More formally, given an unknown and sufficiently regular function $f$ over a domain $\\mathcal{X}$ : How can we learn $f$ sample-efficiently from (noisy) observations? This problem is widely studied in active learning and experimental design (Chaloner & Verdinelli, 1995; Settles, 2009). ", "page_idx": 0}, {"type": "text", "text": "Active learning methods commonly aim to learn $f$ globally, i.e., across the entire domain $\\mathcal{X}$ . However, in many real-world problems, (i) the domain is so large that learning $f$ globally is hopeless or (ii) agents have limited information and cannot access the entire domain (e.g., due to restricted access or to act safely). Thus, global learning is often not desirable or even possible. Instead, intelligent systems are typically required to act in a more directed manner and extrapolate beyond their limited information. This work formalizes the above two aspects of active learning, which have remained largely unaddressed by prior work. We provide a comprehensive overview of related work in Section 6. ", "page_idx": 0}, {"type": "text", "text": "\u201cDirected\u201d transductive active learning We consider the generalized problem of transductive active learning, where given two arbitrary subsets of the domain $\\mathcal{X}$ ; a target space ${\\mathcal{A}}\\subseteq{\\mathcal{X}}$ , and a sample space ${\\mathcal{S}}\\subseteq{\\mathcal{X}}$ , we study the question: ", "page_idx": 0}, {"type": "text", "text": "How can we learn $f$ within $\\boldsymbol{\\mathcal{A}}$ by actively sampling observations within $\\boldsymbol{S}$ ? ", "page_idx": 0}, {"type": "text", "text": "This problem is ubiquitous in real-world applications such as safe Bayesian optimization, where $\\boldsymbol{S}$ is a set of safe parameters and $\\boldsymbol{\\mathcal{A}}$ might represent parameters outside $\\boldsymbol{S}$ whose safety we want to infer. Active fine-tuning of neural networks is another example, where the target space $\\boldsymbol{\\mathcal{A}}$ represents the test set over which we want to minimize risk, and the sample space $\\boldsymbol{S}$ represents the dataset from which we can retrieve data points to fine-tune our model to $\\boldsymbol{\\mathcal{A}}$ . Figure 1 visualizes some instances of transductive active learning. ", "page_idx": 1}, {"type": "text", "text": "Whereas most prior work has focused on the \u201cglobal\u201d inductive instance $\\mathcal{X}=\\mathcal{A}=\\mathcal{S}$ , MacKay (1992) was the first to consider specific target spaces $\\boldsymbol{\\mathcal{A}}$ and proposed the principle of selecting points in $\\boldsymbol{S}$ to minimize the \u201cposterior uncertainty\u201d about points in $\\boldsymbol{\\mathcal{A}}$ . Since then, several works have studied this principle empirically (e.g., Seo et al., 2000; Yu et al., 2006; Bogunovic et al., 2016; Wang et al., 2021; Kothawade et al., 2021; Bickford Smith et al., 2023). In this work, we model $f$ as a Gaussian process or (equivalently) as a function in a reproducing kernel Hilbert space, for which the above principle is analytically and computationally tractable. Our contributions are: ", "page_idx": 1}, {"type": "image", "img_path": "tZtepJBtHg/tmp/158b413dea50e29a42e52d5875f946be802f1210429deec269aa361df7b50083.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Instances of transductive active learning with target space $\\boldsymbol{\\mathcal{A}}$ shown in blue and sample space $\\boldsymbol{S}$ shown in gray. The points denote plausible observations within $\\boldsymbol{S}$ to \u201clearn\u201d $\\boldsymbol{\\mathcal{A}}$ . In $\\mathbf{\\Gamma}(\\mathbf{A})$ , the target space contains \u201ceverything\u201d within $\\boldsymbol{S}$ as well as points outside $\\boldsymbol{S}$ . In $(\\mathbf{B},\\,\\mathbf{C},$ D), one makes observations directed towards learning about a particular target. Prior work on inductive active learning has focused on the instance $A=S$ . ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theory (Section 3): We are the first to give rates for the uniform convergence of uncertainty over the target space $\\boldsymbol{\\mathcal{A}}$ to the smallest attainable value, given samples from the sample space $\\boldsymbol{S}$ (Theorems 3.2 and 3.3), Our results provide a theoretical justification for the principle of minimizing posterior uncertainty in transductive active learning, and indicate that transductive active learning can be more sample efficient than inductive active learning. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Applications: We show that transductive active learning improves upon the state-of-the-art in the batch-wise active fine-tuning of neural networks for image classification (Section 4) and in safe Bayesian optimization (Section 5). ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We assume for now that the target space $\\boldsymbol{\\mathcal{A}}$ and sample space $\\boldsymbol{S}$ are finite, and relax these assumptions in the appendices. We model $f$ as a stochastic process and denote the marginal random variables $f({\\pmb x})$ by $f_{x}$ , and joint random vectors $\\{f_{\\mathbf{x}}\\}_{\\mathbf{x}\\in X}$ for some $X\\subseteq\\mathcal{X},|X|<\\infty^{\\prime}$ by $f_{X}$ . Let $\\pmb{y}_{X}$ denote the noisy observations of $f_{X}$ , $\\{y_{\\pmb{x}}=f_{\\pmb{x}}+\\varepsilon_{\\pmb{x}}\\}_{\\pmb{x}\\in X}$ , where $\\varepsilon_{x}$ is independent noise.2 We study the \u201cadaptive\u201d setting, where in round $n$ the agent selects a point $\\pmb{x}_{n}\\in S$ and observes $y_{n}=y_{{\\mathbf{x}}_{n}}$ . The agent\u2019s choice of ${\\pmb x}_{n}$ may depend on the outcome of prior observations $\\!\\!\\!D_{n-1}\\,{\\stackrel{\\mathrm{det}}{=}}\\{(x_{i},y_{i})\\}_{i<n}^{\\mathrm{~\\tiny~\\top~}}$ . ", "page_idx": 1}, {"type": "text", "text": "Background on information theory We briefly recap several important concepts from information theory of which we provide formal definitions in Appendix B. The (differential) entropy $\\operatorname{H}[f]$ is one possible measure of uncertainty about $\\pmb{f}$ and the conditional entropy $\\operatorname{H}[f\\mid y]$ is the (expected) posterior uncertainty about $\\pmb{f}$ after observing $\\textit{\\textbf{y}}$ . The information gain $\\operatorname{I}({\\dot{f}};{\\dot{y}})=\\operatorname{H}[f]-\\operatorname{H}[f\\mid y]$ measures the (expected) reduction in uncertainty about $\\pmb{f}$ due to $\\textit{\\textbf{y}}$ . We denote the information gain about $\\boldsymbol{\\mathcal{A}}$ from observing $X$ by $\\operatorname{I}(f_{A};y_{X})$ . The maximum information gain about $\\boldsymbol{\\mathcal{A}}$ from $n$ observations within $\\boldsymbol{S}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\gamma}_{A,S}(n)\\stackrel{\\mathrm{def}}{=}\\underset{|X\\stackrel{\\r{\\sum}}{|\\leq}{n}}{\\operatorname*{max}}\\ \\mathbf{I}(f_{A};\\pmb{y}_{X}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This \u201cinformation capacity\u201d measures the information about $f_{A}$ that is accessible from within $\\boldsymbol{S}$ , and 2ha0s2 1b)e ienn  tuhse esde tptirnegv iwouhselrye $\\mathcal{X}=\\mathcal{A}=\\mathcal{S}$ , itvaaksi netg  atlh.e,  f2o0r0m9 ; ofC $\\gamma_{n}\\stackrel{\\mathrm{def}}{=}\\gamma\\dot{x_{}^{\\prime}}(n)\\stackrel{\\mathrm{def}}{=}\\gamma{x_{}^{\\prime}}{x_{}^{}}(n)$ . 17W;e  Vreakmilair ke tt hala.t, $\\gamma_{A,S}(n)\\leq\\gamma_{S}(n)$ holds uniformly for all $A,S$ , and $n$ due to the data processing inequality. Generally, $\\gamma_{\\mathcal{A},\\mathcal{S}}(n)$ can be substantially smaller if the target space is a sparse subset of the sample space. ", "page_idx": 1}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We analyze the following principle for transductive active learning: ", "page_idx": 2}, {"type": "text", "text": "Select samples to minimize the posterior \u201cuncertainty\u201d about $f$ within $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "This principle yields a family of simple and natural decision rules which depend on the chosen measure of \u201cuncertainty\u201d. Two natural measures of uncertainty are (1) the entropy of prediction targets, $\\operatorname{H}[f_{A}]$ , and (2) their total variance, $\\textstyle\\sum_{\\mathbf{x^{\\prime}}\\in\\mathcal{A}}\\operatorname{Var}[f_{\\mathbf{x^{\\prime}}}]$ . The corresponding decision rules are ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb x}_{n}=\\underset{{\\pmb x}\\in S}{\\arg\\operatorname*{min}}\\,\\mathrm{H}[f_{A}~|~{\\mathcal{D}}_{n-1},y_{x}]=\\underset{{\\pmb x}\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{I}(f_{A};y_{x}~|~{\\mathcal{D}}_{n-1}),}\\\\ &{{\\pmb x}_{n}=\\underset{{\\pmb x}\\in S}{\\arg\\operatorname*{min}}\\,\\mathrm{tr}\\,\\,\\mathrm{Var}[f_{A}~|~{\\mathcal{D}}_{n-1},y_{x}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with an implicit expectation over the feedback $y_{x}$ . That is, ITL (short for Information-based Transductive Learning) and VTL (Variance-based $T L$ ) select ${\\pmb x}_{n}$ so as to minimize the uncertainty about the prediction targets $f_{A}$ (in expectation) after having received the feedback $y_{n}$ . Unlike VTL, ITL takes into account the mutual dependence between points in $\\boldsymbol{\\mathcal{A}}$ . These decision rules were suggested previously (MacKay, 1992; Seo et al., 2000; Yu et al., 2006) without deriving theoretical guarantees; and they generalize several widely used algorithms which we discuss in more detail in Section 6. Most prominently, in the inductive setting where ${\\mathcal{S}}\\subseteq A$ , ITL reduces to $\\pmb{x}_{n}=\\arg\\operatorname*{max}_{\\pmb{x}\\in S}\\operatorname{I}(f_{\\pmb{x}};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})$ , i.e., is \u201cundirected\u201d and reduces to standard uncertainty-based active learning strategies (cf. Appendix C.1). The convergence properties for the special instance of ITL with ${\\mathcal{S}}={\\mathcal{A}}$ have been studied extensively. To the best of our knowledge, we are the first to extend these guarantees to the more general setting of transductive active learning. ", "page_idx": 2}, {"type": "text", "text": "In our presented results, we make the following assumption. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. In the case of ITL, the information gain $\\psi_{A}(X)=\\operatorname{I}(f_{A};{\\pmb y}_{X})$ is submodular. In the case of VTL, the variance reduction $\\psi_{A}(X)=\\operatorname{tr}\\operatorname{Var}[f_{A}]-\\operatorname{tr}\\operatorname{Var}[f_{A}^{\\phantom{\\dagger}}|\\,{\\bar{y_{X}}}]$ is submodular. ", "page_idx": 2}, {"type": "text", "text": "Under this assumption, $\\psi_{A}(\\pmb{x}_{1:n})$ is a constant factor approximation of $\\operatorname*{max}_{X\\subseteq S,|X|\\leq n}\\psi_{A}(X)$ due to the seminal result on submodular function maximization by Nemhauser et al. (1978). Similar assumptions have been made, e.g., by Bogunovic et al. (2016) and Kothawade et al. (2021). Assumption 3.1 is satisfied exactly for ITL when ${\\mathcal{S}}\\subseteq A$ and $f$ is a Gaussian process (cf. Lemma C.9), and we provide an extensive discussion of our results in Appendix C.4 for instances where Assumption 3.1 is satisfied approximately, relying on the notion of weak submodularity (Das & Kempe, 2018). ", "page_idx": 2}, {"type": "text", "text": "3.1 Gaussian Process Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When $f\\sim\\mathcal{G P}(\\mu,k)$ is a Gaussian process (GP, Williams & Rasmussen, 2006) with known mean function $\\mu$ and kernel $k$ , and the noise $\\varepsilon_{x}$ is mutually independent and zero-mean Gaussian with known variance, the ITL and VTL objectives have a closed form expression (cf. Appendix F) and can be optimized efficiently. Further, the information capacity $\\gamma_{n}$ is sublinear in $n$ for a rich class of GPs (Srinivas et al., 2009; Vakili et al., 2021), with rates summarized in Table 3 of the appendix. ", "page_idx": 2}, {"type": "text", "text": "Convergence to irreducible uncertainty So far, our discussion was centered around the role of the target space $\\boldsymbol{\\mathcal{A}}$ in facilitating directed learning. An orthogonal contribution of this work is to study extrapolation from the sample space $\\mathcal{S}$ to points $x\\in A\\setminus S$ . To this end, we derive bounds on the marginal posterior variance $\\sigma_{n}^{2}(\\mathbf{\\dot{x}})\\,{\\overset{\\underset{\\mathrm{det}}{}}{=}}\\,\\mathrm{Var}[\\mathbf{\\dot{f}}(\\mathbf{x})\\mid\\mathbf{\\mathcal{D}}_{n}]$ for points in $\\boldsymbol{\\mathcal{A}}$ . These bounds depend on the instance of transductive active learning (i.e., $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{S}$ ) and might be of independent interest for active learning. For ITL and VTL, they imply uniform convergence of the variance for a rich class of GPs. To the best of our knowledge, this work is the first to present such bounds. ", "page_idx": 2}, {"type": "text", "text": "We define the irreducible uncertainty as the variance of $f({\\boldsymbol{x}})$ provided complete knowledge of $f$ in $\\boldsymbol{S}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\eta_{S}^{2}(\\pmb{x})\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathrm{Var}[f_{\\pmb{x}}\\mid f_{S}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As the name suggests, $\\eta_{S}^{2}(\\pmb{x})$ represents the smallest uncertainty one can hope to achieve from observing only within $\\boldsymbol{S}$ . For all $\\pmb{x}\\in S$ , it is easy to see that $\\eta_{S}^{2}(\\dot{\\pmb x})=0$ . However, the irreducible uncertainty of $\\boldsymbol{x}\\not\\in{\\mathcal{S}}$ may be (and typically is!) strictly positive. ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.2 (Bound on marginal variance for ITL and VTL). Let Assumption 3.1 hold and the data be selected by either ITL or VTL.. Assume that $f\\sim\\mathcal{G P}(\\mu,k)$ with known mean function $\\mu$ ", "page_idx": 2}, {"type": "text", "text": "and kernel $k$ , the noise $\\varepsilon_{x}$ is mutually independent and zero-mean Gaussian with known variance, and $\\gamma_{n}$ is sublinear in $n$ . Then there exists a constant $C$ such that for any $n\\geq1$ and $\\pmb{x}\\in A$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}({\\pmb x})\\leq\\underbrace{\\eta_{S}^{2}({\\pmb x})}_{i r r e d u c i b l e}+\\underbrace{C\\frac{\\gamma_{A,S}(n)}{\\sqrt{n}}\\,.}_{r e d u c i b l e}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, if $\\pmb{x}\\in\\mathcal{A}\\cap\\mathcal{S}$ , there exists a constant $C^{\\prime}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}({\\pmb x})\\leq C^{\\prime}\\frac{\\gamma_{A,S}(n)}{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, Equation (1) of Theorem 3.2 can be understood as bounding an epistemic \u201cgeneralization gap\u201d (Wainwright, 2019) of the learner. The reducible uncertainty converges to zero at all prediction targets $\\pmb{x}\\in A$ , e.g., for linear, Gaussian, and smooth Mat\u00e9rn kernels. As to be expected, a smaller target space (i.e., more targeted sampling) leads to faster convergence due to a smaller information capacity $\\gamma_{A,S}(n)\\ll\\gamma_{n}$ . Equation (2) matches prior results for the setting $s=A$ . We provide a formal proof of Theorem 3.2 in Appendix C.6. ", "page_idx": 3}, {"type": "text", "text": "3.2 Agnostic Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The result from the GP setting translates also to the agnostic setting, where the \u201cground truth\u201d $f^{\\star}$ may be any sufficiently regular fixed function on $\\mathcal{X}$ .3 In this case, we use the model $f$ from Section 3.1 as a (misspecified) model of $f^{\\star}$ , with some kernel $k$ and zero mean function $\\mu(\\cdot)=0$ . We denote by $\\mu_{n}(\\pmb{x})=\\mathbb{E}[f(\\pmb{x})\\mid\\mathcal{D}_{n}]$ the posterior mean of $f$ . W.l.o.g. we assume in the following result that the prior variance is bounded, i.e., $\\mathrm{Var}[f(\\pmb{x})]\\leq1$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3 (Bound on approximation error for ITL and VTL, following Abbasi-Yadkori (2013); Chowdhury & Gopalan (2017)). Let Assumption 3.1 hold and the data be selected by either ITL or VTL. Pick any $\\delta\\in(0,1)$ . Assume that $f^{\\star}$ lies in the reproducing kernel Hilbert space $\\mathcal{H}_{k}(\\mathcal{X})$ of the kernel $k$ with norm $\\|f^{\\star}\\|_{k}<\\infty_{\\!_{\\!_{\\!}}}$ , the noise $\\varepsilon_{n}$ is conditionally $\\rho$ -sub-Gaussian, and $\\gamma_{n}$ is sublinear in n. Let $\\beta_{n}(\\delta)=\\|\\vec{f}^{\\star}\\|_{k}^{\\quad\\cdot\\cdot}+\\rho\\sqrt{2(\\gamma_{n}+1+\\log(1/\\delta))}$ . Then for any $n\\geq1$ and $\\pmb{x}\\in\\mathcal{A}$ , jointly with probability at least $1-\\delta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n|f^{\\star}({\\pmb x})-\\mu_{n}({\\pmb x})|\\leq\\beta_{n}(\\delta)\\Big[\\underbrace{\\eta_{S}({\\pmb x})}_{i r r e d u c i b l e}+\\underbrace{\\nu_{A,S}({\\pmb n})}_{r e d u c i b l e}\\Big]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nu_{\\mathcal{A},S}^{2}(n)$ denotes the reducible part of Equation (1). ", "page_idx": 3}, {"type": "text", "text": "We provide a formal proof of Theorem 3.3 in Appendix C.7. Theorem 3.3 generalizes approximation error bounds of prior works to the extrapolation setting, where some prediction targets $\\pmb{x}\\in A$ lie outside the sample space $\\boldsymbol{S}$ . For prediction targets $\\mathbf{\\mathcal{x}}\\in\\mathcal{A}\\cap\\mathcal{S}$ , the irreducible uncertainty vanishes, and we recover previous results from the setting ${\\mathcal{S}}={\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorems 3.2 and 3.3 show that ITL and VTL efficiently learn $f$ at the prediction targets $\\boldsymbol{\\mathcal{A}}$ for large classes of \u201csufficiently regular\u201d functions $f$ . In the following, we validate these results experimentally by showing that ITL and VTL exhibit strong empirical performance in a broad range of applications. ", "page_idx": 3}, {"type": "text", "text": "3.3 Experiments in the Gaussian Process Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before demonstrating ITL and VTL on GPs to develop more intuition, we introduce a natural correlation-based baseline, which will later uncover connections to existing approaches: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in\\mathcal{S}}{\\arg\\operatorname*{max}}\\sum_{\\pmb{x^{\\prime}}\\in\\mathcal{A}}\\mathrm{Cor}[f_{\\pmb{x}},f_{\\pmb{x^{\\prime}}}\\mid\\mathcal{D}_{n-1}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "How does the smoothness of $f$ affect ITL? We contrast two \u201cextreme\u201d kernels: the Gaussian kernel $k(\\pmb{x},\\pmb{x}^{\\prime})=\\exp(-\\left\\|\\pmb{x}-\\pmb{x}^{\\prime}\\right\\|_{2}^{2}/2)$ and the Laplace kernel $k(\\pmb{x},\\pmb{x}^{\\prime})=\\exp(-\\mathrm{\\ensuremath{\\left\\vert\\pmb{x}-\\pmb{x}^{\\prime}\\right\\vert}\\right\\vert_{1}})$ . In the mean-squared sense, the Gaussian kernel yields a smooth process $f$ whereas the Laplace kernel yields a continuous but non-differentiable $f$ (Williams & Rasmussen, 2006). Figure 2 shows how ITL adapts to the smoothness of $f$ : Under the \u201csmooth\u201d Gaussian kernel, points outside $\\boldsymbol{\\mathcal{A}}$ provide higher-order information. In contrast, under the \u201crough\u201d Laplace kernel and if ${\\mathcal{A}}\\subseteq S$ , points outside $\\boldsymbol{\\mathcal{A}}$ do not provide any additional information, and therefore are not sampled by ITL. If, however, ${\\mathcal{A}}\\subseteq{\\mathcal{S}}$ , information \u201cleaks\u201d $\\boldsymbol{\\mathcal{A}}$ even under a Laplace kernel prior. That is, even for non-smooth functions, the point with most information need not be in $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "image", "img_path": "tZtepJBtHg/tmp/065514451a4d29432b2aa23cb254137cd13710a8c6ee730bfab19ae611fc03a1.jpg", "img_caption": ["Figure 2: Initial 25 samples of ITL under a Gaussian kernel with lengthscale 1 (left) and a Laplace kernel with lengthscale 10 (right). Shown in gray is the sample space $\\boldsymbol{S}$ and shown in blue is the target space $\\boldsymbol{\\mathcal{A}}$ . In three of the four examples, points outside the target space provide useful information. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "tZtepJBtHg/tmp/df0201d8d313b2c6d79d4ec62e87ebd60fe710fd98df6612578a6ad5529c3fe3.jpg", "img_caption": ["Figure 3: Entropy of $f_{A}$ ranging from $-3850$ to $-3725$ and the mean marginal standard deviations of $f_{A}$ ranging from 0 to 0.15. Experiment is using the Gaussian kernel of the left instance $({\\mathcal{A}}\\subset{\\mathcal{S}})$ from Figure 2. It can be seen that ITL and VTL outperform UNSA and RANDOM. Uncertainty bands correspond to one standard error over 10 random seeds. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Does ITL outperform uncertainty sampling? Uncertainty sampling (UNSA, Lewis & Catlett, 1994) is one of the most popular active learning methods. UNSA selects points $\\textbf{\\em x}$ with high prior uncertainty: $\\pmb{x}_{n}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\bar{S}}\\sigma_{n-1}^{2}(\\pmb{x})$ . This is in stark contrast to ITL and VTL which select points $\\textbf{\\em x}$ that minimize posterior (epistemic) uncertainty about $\\boldsymbol{\\mathcal{A}}$ . It can be seen that UNSA is the special \u201cundirected\u201d case of ITL when ${\\mathcal{S}}\\subseteq A$ and observation noise is homoscedastic (cf. Appendix C.1). ", "page_idx": 4}, {"type": "text", "text": "We compare UNSA to ITL, VTL, and CTL in Figure 3. We observe that ITL and VTL outperform UNSA which also samples points that are not informative about $\\boldsymbol{\\mathcal{A}}$ . Further, ITL and VTL outperform \u201clocal\u201d UNSA (i.e., UNSA constrained to $A\\cap S)$ which neglects all information provided by points outside $\\boldsymbol{\\mathcal{A}}$ .4 As one would expect, VTL has an advantage with respect to reducing the total variance of $f_{A}$ , whereas ITL reduces the entropy of $f_{A}$ faster. We include ablations in Appendix H where we, in particular, observe that the advantage of ITL and VTL over UNSA increases as the volume of prediction targets shrinks in comparison to the size of domain. ", "page_idx": 4}, {"type": "text", "text": "4 Active Fine-Tuning of Neural Networks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Fine-tuning a large pre-trained model is a cost- and computation-effective approach to improve performance on a given target domain (Lee et al., 2022). While previous work has studied the effectiveness of various training procedures for fine-tuning (e.g., Eustratiadis et al., 2024), we ask: How can we select the right data for fine-tuning to a specific task? This active fine-tuning problem is an instance of the introduced \u201cdirected\u201d transductive learning problem: Concretely, consider a supervised setting, where the function $f$ maps inputs $\\pmb{x}\\in\\mathcal{X}$ to outputs $y\\in\\mathcal{V}$ . We have access to noisy samples from a training set $\\boldsymbol{S}$ on $\\mathcal{X}$ , and we would like to learn $f$ such that our estimate minimizes a given risk measure, such as classification error, with respect to a test distribution $\\mathcal{P}_{\\!A}$ on $\\mathcal{X}$ . The goal is to actively and efficiently sample from $\\boldsymbol{S}$ to minimize risk with respect to $\\mathcal{P}_{\\!A}$ . We show in this section that ITL and VTL can learn $f$ from only few examples from $\\boldsymbol{S}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "How can we leverage the latent structure learned by the pre-trained model? As common in related work, we approximate the (pre-trained) neural network (NN) $f(\\cdot;\\theta)$ as a linear function in a latent embedding space, $f(\\pmb{x};\\pmb{\\theta})\\approx\\stackrel{\\cdot}{\\beta}^{\\top}\\phi_{\\pmb{\\theta}}(\\pmb{x})$ , with weights $\\beta\\in\\mathbb{R}^{p}$ and embeddings $\\phi_{\\theta}:\\mathcal{X}\\rightarrow\\mathbb{R}^{p}$ . Common choices of embeddings include last-layer embeddings (Devlin et al., 2019; Holzm\u00fcller et al., 2023), neural tangent embeddings arising from neural tangent kernels (Jacot et al., 2018) which are motivated by their relationship to the training and fine-tuning of ultra-wide NNs (Arora et al., 2019; Lee et al., 2019; Khan et al., 2019; He et al., 2020; Malladi et al., 2023), and loss gradient embeddings (Ash et al., 2020). We provide a comprehensive overview of embeddings in Appendix J.2. Now, supposing the prior $\\beta\\sim\\mathcal{N}(\\mathbf{0},\\Sigma)$ , often with $\\Sigma=I$ (Khan et al., 2019; He et al., 2020; Antor\u00e1n et al., 2022; Wei et al., 2022), this approximation of $f$ is a Gaussian process with kernel $k(\\pmb{x},\\pmb{x}^{\\prime})=\\phi_{\\pmb{\\theta}}(\\pmb{x})^{\\top}\\pmb{\\Sigma}\\phi_{\\pmb{\\theta}}(\\pmb{x}^{\\prime})$ which quantifies the similarity between points in terms of their alignment in the learned latent space. Note that the correlation $\\check{k(\\pmb{x},\\pmb{x}^{\\prime})}/\\sqrt{k(\\pmb{x},\\pmb{x})k(\\pmb{x}^{\\prime},\\pmb{x}^{\\prime})}$ between two points ${\\boldsymbol{x}},{\\boldsymbol{x}}^{\\prime}$ is equal to the cosine similarity of their embeddings. ", "page_idx": 5}, {"type": "text", "text": "In this context, Theorem 3.2 bounds the epistemic posterior uncertainty about a prediction using the approximation ${\\boldsymbol{\\beta}}^{\\top}\\phi_{\\boldsymbol{\\theta}}({\\boldsymbol{x}})$ , given that the model is trained using data selected by ITL or VTL. Theorem 3.3 bounds the generalization error when using the posterior mean of $\\beta$ for prediction. This extends recent work which has studied estimators of this generalization error (Wei et al., 2022). ", "page_idx": 5}, {"type": "text", "text": "Batch selection: Diversity via conditional embeddings Efficient labeling and training necessitates a batch-wise selection of inputs. The selection of a batch of size $b>1$ can be seen as an individual non-adaptive active learning problem, and significant recent work has shown that batch diversity is crucial in this setting (Ash et al., 2020; Zanette et al., 2021; Holzm\u00fcller et al., 2023; Pacchiano et al., 2024). An information-based batch-wise selection strategy is formalized by the following nonadaptive transductive active learning problem (Chen & Krause, 2013) and the greedy approximation of $B_{n}$ by ITL which selects elements $\\pmb{x}_{n,i}$ of the $n$ -th batch iteratively based on $\\scriptstyle{\\mathbf{\\boldsymbol{x}}}_{n,1:i-1}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nB_{n}=\\underset{B\\subseteq\\mathcal{S},\\|B\\|=b}{\\arg\\operatorname*{max}}\\ \\operatorname{I}(f_{A};y_{B}\\mid\\mathcal{D}_{n-1});\\qquad x_{n,i}=\\underset{x\\in\\mathcal{S}}{\\arg\\operatorname*{max}}\\operatorname{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n-1},y_{x_{n,1:i-1}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The batch $B_{n}$ is diverse and informative by design. We show that under Assumption 3.1, $B_{n}^{\\prime}={\\pmb x}_{n,1:b}$ yields a constant-factor approximation of $B_{n}$ (cf. Appendix C.3). ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiments on Active Fine-Tuning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our empirical evaluation is motivated by the following practical example: We deploy a pre-trained image classifier to user\u2019s phones who use it within their local environment. We would like to locally fine-tune a user\u2019s model to their environment. Since the users\u2019 images $\\boldsymbol{\\mathcal{A}}$ are unlabeled, this requires selecting a small number of relevant and diverse images from the set of labeled images $\\boldsymbol{S}$ . As such, we will focus here on the setting where the points in our test set do not lie in our training set (i.e., $A\\cap S=\\emptyset$ ), and discuss alternative instances such as active domain adaptation in Appendix I. ", "page_idx": 5}, {"type": "text", "text": "Testbeds & architectures We use the MNIST (LeCun et al., 1998) and CIFAR-100 (Krizhevsky et al., 2009) datasets as testbeds. In both cases, we take $\\boldsymbol{S}$ to be the training set, and we consider the task of learning the digits 3, 6, and 9 (MNIST) or the first 10 categories of CIFAR-100.6 For MNIST, we train a simple convolutional neural network with ReLU activations, three convolutional layers with max-pooling, and two fully-connected layers. For CIFAR-100, we fine-tune an EfficientNet-B0 (Tan & Le, 2019) pre-trained on ImageNet (Deng et al., 2009), augmented by a final fully-connected layer. We train the NNs using the cross-entropy loss and the ADAM optimizer (Kingma & Ba, 2014). ", "page_idx": 5}, {"type": "text", "text": "Results In Figure 4, We compare against (i) active learning methods which largely aim for sample diversity but which are not directed towards the target distribution $\\mathcal{P}_{\\!A}$ (e.g., BADGE; Ash et al., 2020), and (ii) search methods that aim to retrieve the most relevant samples from $\\boldsymbol{S}$ with respect to the targets $\\mathcal{P}_{\\!A}$ (e.g., maximizing cosine similarity to target embeddings as is common in vector databases; ", "page_idx": 5}, {"type": "image", "img_path": "tZtepJBtHg/tmp/775fda4d0ce0148a77aa5ce48258669db1688682dfc29b30f1149da81cee4f96.jpg", "img_caption": ["Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). RANDOM selects each observation uniformly at random from $\\boldsymbol{S}$ . The batch size is 1 for MNIST and 10 for CIFAR-100. Uncertainty bands correspond to one standard error over 10 random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of $\\mathcal{P}_{\\!A}$ . See Appendix J for details and ablations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Settles & Craven, 2008; Johnson et al., 2019). INFORMATIONDENSITY (ID, Settles & Craven, 2008) is a heuristic approach aiming to combine (i) diversity and (ii) relevance. In Appendix J.5, we also compare against a wide range of additional baselines (e.g., CORESET (Sener & Savarese, 2017), TYPICLUST (Hacohen et al., 2022), PROBCOVER (Yehuda et al., 2022), etc.) that fall into one of the categories (i) and (ii), and which perform similar to the baselines listed here. ", "page_idx": 6}, {"type": "text", "text": "We observe that ITL, VTL, and CTL consistently and significantly outperform random sampling from $\\boldsymbol{S}$ as well as all baselines. We see that relevance-based methods such as COSINESIMILARITY have an initial advantage over RANDOM but for batch sizes larger than 1 they quickly fall behind due to diminishing informativeness of the selected data. In contrast, diversity-based methods such as BADGE are more competitive with RANDOM but do not explicitly aim to retrieve relevant samples. ", "page_idx": 6}, {"type": "text", "text": "Remarkably, transductive active learning outperforms random data selection even in the MNIST experiment where the model is randomly initialized. This suggests that the learned embeddings can be informative for data selection even in the early stages of training, bootstrapping the learning progress. ", "page_idx": 6}, {"type": "text", "text": "Balancing sample relevance and diversity Our proposed methods unify approaches to coverage (promoting diverse samples) and search (aiming for relevant samples with respect to a given query $\\boldsymbol{\\mathcal{A}}$ ) which leads to the significant improvement upon the state-of-the-art in Figure 4. Notably, for a batch size and query size of 1 and if correlations are non-negative, ITL, VTL, CTL, and the canonical cosine similarity are equivalent. CTL can be seen as a direct generalization of cosine similarity-based retrieval to batch and query sizes larger than one. In contrast to CTL, ITL and VTL may also sample points which exhibit a strong negative correlation (which is also informative). ", "page_idx": 6}, {"type": "text", "text": "We observe empirically that ITL obtains samples from $\\mathcal{P}_{\\!A}$ at more than twice the rate of COSINESIMILARITY, which translates to a significant improvement in accuracy in more difficult learning tasks, while requiring fewer (labeled) samples from $\\boldsymbol{S}$ . This phenomenon manifests for both MNIST and CIFAR-100, as well as imbalanced datasets $\\boldsymbol{S}$ or imbalanced reference samples from $\\mathcal{P}_{\\!A}$ (cf. Appendix J.6). The improvement in accuracy appears to increase in the large-data regime, where the learning tasks become more difficult. Akin to a previously identified scaling trend with size of the pretraining dataset (Tamkin et al., 2022), this suggests a potential scaling trend where the improvement of ITL over random batch selection grows as models are fine-tuned on a larger pool of data. ", "page_idx": 6}, {"type": "text", "text": "Towards task-driven few-shot learning Being able to efficiently and automatically select data may allow dynamic few-shot fine-tuning to individual tasks (Vinyals et al., 2016; Hardt & Sun, 2024), e.g., fine-tuning the model to each test point / query / prompt. Such task-driven few-shot learning can be seen as a form of \u201cmemory recall\u201d akin to associative memory (Hopfield, 1982). Our results are a first indication that task-driven learning can lead to substantial performance gains, and we believe that this is a promising direction for future studies. ", "page_idx": 7}, {"type": "text", "text": "5 Safe Bayesian Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Another practical problem that can be cast as \u201cdirected\u201d learning is safe Bayesian optimization (Safe BO, Sui et al., 2015; Berkenkamp et al., 2021) which has applications in natural science (Cooper & Netoff, 2022) and robotics (Wischnewski et al., 2019; Sukhija et al., 2023; Widmer et al., 2023). Safe BO solves the following optimization problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in S^{\\star}}f^{\\star}(x)\\quad{\\mathrm{where}}\\quad S^{\\star}=\\{x\\in{\\mathcal{X}}\\mid g^{\\star}(x)\\geq0\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which can be generalized to multiple constraints. The functions $f^{\\star}$ and $g^{\\star}$ , and hence also the \u201csafe set\u201d $S^{\\star}$ , are unknown and have to be actively learned from data. However, it is crucial that the data collection does not violate the constraint, i.e., $\\pmb{x}_{n}\\in S^{\\star},\\forall n\\geq1$ . ", "page_idx": 7}, {"type": "text", "text": "Safe Bayesian optimization as Transductive Active Learning In the agnostic setting from Section 3.2, GPs $f$ and $g$ can be used as well-calibrated models of the ground truths $f^{\\star}$ and $g^{\\star}$ , and we denote lower- and upper-confidence bounds by $l_{n}^{f}(\\pmb{x}),l_{n}^{g}(\\pmb{x})$ and $u_{n}^{\\pmb{\\tilde{f}}}({\\pmb x}),u_{n}^{g}({\\pmb x})$ , respectively. These confidence bounds induce a pessimistic safe set $\\underline{{{S}}}_{n}\\,{\\stackrel{\\mathrm{aer}}{=}}\\{{\\pmb x}\\mid l_{n}^{g}({\\pmb x})\\geq0\\}$ and an optimistic safe set $\\widehat{S}_{n}\\,{\\stackrel{\\mathrm{def}}{=}}\\{{\\pmb x}\\mid u_{n}^{g}({\\pmb x})\\geq0\\}$ which satisfy $S_{n}\\subseteq S^{\\star}\\subseteq\\widehat{S}_{n}$ with high probability at all times. Similarly, the  set of potential maximizers ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{A}_{n}\\,\\overset{\\mathrm{def}}{=}\\{{\\pmb x}\\in\\widehat{S}_{n}\\mid u_{n}^{f}({\\pmb x})\\geq\\operatorname*{max}_{{\\pmb x}^{\\prime}\\in S_{n}}l_{n}^{f}({\\pmb x}^{\\prime})\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "contains the solution to Equation (4) at all times with high probability. ", "page_idx": 7}, {"type": "text", "text": "The (simple) regret $r_{n}(S)\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\operatorname*{max}_{\\pmb{x}\\in S}\\,f^{\\star}(\\pmb{x})-f^{\\star}({\\widehat{\\pmb x}}_{n})$ with $\\widehat{\\pmb{x}}_{n}\\overset{\\mathrm{def}}{=}\\arg\\operatorname*{max}_{\\pmb{x}\\in S_{n}}l_{n}^{f}(\\pmb{x})$ measures the worst-case performance of a decision rule. To achieve small regret, one faces an explorationexpansion dilemma wherein one needs to explore points that are known-to-be-safe, i.e., lie in the estimated safe set ${\\mathcal{S}}_{n}$ , and might be optimal, while at the same time discovering new safe points by \u201cexpanding\u201d $\\ensuremath{\\boldsymbol{S}}_{n}$ . Accordingly, a natural choice for the target space of Safe BO is $A_{n}$ since it captures both exploration and expansion simultaneously.7To prevent constraint violation, the sample space is restricted to the pessimistic safe set ${\\mathcal{S}}_{n}$ . In Safe BO, both the target space and sample space change with each round $n$ , and we generalize our theoretical results from Section 3 in Appendix C to this setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Convergence to safe optimum). Pick any $\\epsilon>0$ , $\\delta\\in(0,1)$ . Assume that $f^{\\star}$ , $g^{\\star}$ lie in the reproducing kernel Hilbert space $\\mathcal{H}_{k}(\\mathcal{X})$ of the kernel $k$ , and that the noise $\\varepsilon_{n}$ is conditionally $\\rho$ -sub-Gaussian. Then, we have with probability at least $1-\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nS a f e t y\\colon f o r\\,a l l\\,n\\geq1,\\quad\\mathbf{x}_{n}\\in S^{\\star}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, assume $S_{0}\\neq\\emptyset$ and denote with $\\mathcal{R}$ the largest reachable safe set starting from ${\\mathcal S}_{0}$ . Then, the convergence of reducible uncertainty implies that there exists $n^{\\star}>0$ such that with probability at least $1-\\delta$ , ", "page_idx": 7}, {"type": "text", "text": "We provide a formal proof in Appendix C.8. Central to the proof is the application of Theorem 3.3 to show that the safety of parameters outside the safe set ${\\mathcal{S}}_{n}$ can be inferred efficiently. In Section 3, we outline settings where the reducible uncertainty converges which is the case for a very general class of functions, and for such instances Theorem 5.1 guarantees optimality in the largest reachable safe set $\\mathcal{R}$ . $\\mathcal{R}$ represents the largest set any safe learning algorithm can explore without violating the safety constraints (with high probability) during learning (cf. Definition C.29). Our guarantees are similar to those of other Safe BO algorithms (Berkenkamp et al., 2021) but require fewer assumptions and generalize to continuous domains. We obtain Theorem 5.1 from a more general result (Theorem C.34) which can be specialized to yield \u201cfree\u201d novel convergence guarantees for problems other than Bayesian optimization, such as level set estimation, by choosing an appropriate target space. ", "page_idx": 7}, {"type": "image", "img_path": "tZtepJBtHg/tmp/9931163d2e43617c4e5c285b54c640363a42b53903f40ccbbaa73f042229be4e.jpg", "img_caption": ["Figure 5: We compare ITL and VTL to ORACLE SAFEOPT, which has oracle knowledge of the Lipschitz constants, SAFEOPT, where the Lipschitz constants are estimated from the GP, as well as HEURISTIC SAFEOPT and ISE, and observe that ITL and VTL systematically perform well. We compare against additional baselines in Appendix K.1. The regret is evaluated with respect to the ground truth objective $f^{\\star}$ and constraint $g^{\\star}$ , and averaged over 10 (in synthetic experiments) and 25 (in the quadcopter experiment) random seeds. Additional details can be found in Appendix K.4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1 Experiments on Safe Bayesian Optimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate two synthetic experiments for a 1d and 2d parameter space, respectively (cf. Appendix K.4 for details), which demonstrate the various shortcomings of existing Safe BO baselines. Additionally, as third experiment, we safely tune the controller of a quadcopter. ", "page_idx": 8}, {"type": "text", "text": "Safe controller tuning for a quadcopter We consider a quadcopter with unknown dynamics; $\\pmb{s}_{t+1}=\\pmb{T}(\\pmb{s}_{t},\\pmb{u}_{t})$ where $\\mathbf{u}_{t}\\in\\bar{\\mathbb{R}}^{d_{u}}$ is the control signal and $\\dot{\\boldsymbol{s}}_{t}\\in\\dot{\\mathbb{R}}^{d_{s}}$ is the state at time $t$ . The inputs $\\pmb{u}_{t}$ are calculated through a deterministic function of the state $\\pi:{\\mathcal{S}}\\rightarrow{\\mathcal{U}}$ which we call the policy. The policy is parameterized via parameters $x\\in\\mathcal{X}$ , e.g., PID controller gains, such that ${\\pmb u}_{t}={\\pmb\\pi}_{x}\\big({\\pmb s}_{t}\\big)$ . The goal is to find the optimal parameters with respect to an unknown objective $f^{\\star}$ while satisfying some unknown constraint(s) $\\mathbf{\\bar{\\mu}}_{g}\\star(\\mathbf{x})\\geq0$ , e.g., the quadcopter does not fall on the ground. This is a typical Safe BO problem which is widely applied for safe controller learning in robotics (Berkenkamp et al., 2021; Baumann et al., 2021; Widmer et al., 2023). ", "page_idx": 8}, {"type": "text", "text": "Results We compare ITL and VTL to SAFEOPT (Berkenkamp et al., 2021), which is undirected, i.e., expands in all directions including ones that are known-to-be suboptimal, and ISE (Bottero et al., 2022), which is solely expansionist \u2014 does not trade-off expansion-exploration. We provide a detailed discussion of baselines in Appendix K.2. In all our experiments, summarized in Figure 5, we observe that ITL and VTL systematically perform well, i.e., better or on par with the state-of-the-art. We attribute this to its directed exploration and less conservative expansion over SAFEOPT (cf. 1d task and quadcopter experiment), and natural trade-off between expansion and exploration as opposed to ISE (see 2d task). Generally, VTL has a slight advantage over ITL, which is because VTL minimizes marginal variances (as opposed to entropy), which are decisive for expanding the safe set. While ITL and VTL do not violate constraints, we observe that other methods that do not explicitly enforce safety such as EIC (Gardner et al., 2014) lead to constraint violation (cf. Appendix K.4.2). ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "(Inductive) active learning The special case of transductive active learning where ${\\mathcal{A}}={\\mathcal{S}}={\\mathcal{X}}$ has been widely studied. We refer to this special instance as inductive active learning, since the goal is to extract as much information as possible as opposed to making predictions on a specific target set. ", "page_idx": 8}, {"type": "text", "text": "Several works have previously found entropy-based decision rules to be useful for inductive active learning (Krause & Guestrin, 2007; Guo & Greiner, 2007; Krause et al., 2008) and semi-supervised learning (Grandvalet & Bengio, 2004). The variance-based VTL has previously been proposed by Cohn (1993) in the special case of inductive active learning without proving theoretical guarantees. VTL was then recently re-derived by Shoham & Avron (2023) along other experimental design criteria under the lens of minimizing risk for inductive one-shot learning in overparameterized models. Substantial work on active learning has studied entropy-based criteria in parameter-space, most notably BALD (MacKay, 1992; Houlsby et al., 2011; Gal et al., 2017; Kirsch et al., 2019), which selects $\\begin{array}{r}{{\\pmb x}_{n}=\\arg\\operatorname*{max}_{{\\pmb x}\\in\\mathcal{X}}\\operatorname{I}(\\pmb{\\theta};y_{\\pmb x}\\mid\\mathcal{D}_{n-1})}\\end{array}$ , where $\\pmb{\\theta}$ is the random parameter vector of a parametric model (e.g., obtained via Bayesian deep learning). Such methods are inherently inductive in the sense that they do not facilitate learning on specific prediction targets. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Transductive active learning In contrast, ITL operates in output-space where it is straightforward to specify prediction targets, and which is computationally easier. Special cases of ITL when ${\\mathcal{S}}={\\mathcal{X}}$ and $|\\mathcal{A}|=1$ have been proposed in the foundational work of MacKay (1992) on \u201cdirected\u201d output-space active learning. As generalization to larger target spaces, MacKay (1992) proposed mean-marginal ITL, ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=\\underset{{\\pmb x}\\in{\\cal S}}{\\arg\\operatorname*{max}}\\,\\sum_{{\\pmb x}^{\\prime}\\in{\\cal A}}\\mathrm{I}(f_{{\\pmb x}^{\\prime}};y_{{\\pmb x}}\\mid{\\mathcal D}_{n-1})\\,,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for which we derive analogous versions of Theorems 3.2 and 3.3 in Appendix D.3. We note that similarly to VTL, MM-ITL disregards the mutual dependence of points in the target space $\\boldsymbol{\\mathcal{A}}$ and differs from VTL only in a different weighting of the posterior marginal variances of the prediction targets (cf. Appendix D.3). Recently, Bickford Smith et al. (2023) generalized MM-ITL by treating the prediction target as a random variable, and Kothawade et al. (2021) and Bickford Smith et al. (2024) demonstrated the use of output-space decision rules for image classification tasks in a pre-training context. ", "page_idx": 9}, {"type": "text", "text": "Influence functions measure the change in a model\u2019s prediction when a single data point is removed from the training data (Cook, 1977; Koh & Liang, 2017; Pruthi et al., 2019). Influence functions have been used for data selection in settings closely related to the transductive active fine-tuning of neural networks proposed in this work (Xia et al., 2024). They select data that reduces a first-order Taylor approximation to the test loss after fine-tuning a neural network, which corresponds to maximizing cosine similarity to the prediction targets in a loss-gradient embedding space. We show in our experiments that transductive active learning can substantially outperform COSINESIMILARITY. We attribute this primarily to influence functions implicitly assuming that the influence of selected data adds linearly (i.e., two equally scored data points are expected to doubly improve the model performance, Xu & Kazantsev, 2019, Section 3.2). This assumption does not hold in practice as seen, e.g., by simply duplicating data. The same limitation applies to the related approach of datamodels (Ilyas et al., 2022). ", "page_idx": 9}, {"type": "text", "text": "Other work on directed active learning Directed active learning methods have been proposed for the problem of determining the optimum of an unknown function, also known as best-arm identification (Audibert et al., 2010) or pure exploration bandits (Bubeck et al., 2009). Entropy search methods (Hennig & Schuler, 2012; Hern\u00e1ndez-Lobato et al., 2014) are widely used and select $\\begin{array}{r}{\\pmb{x}_{n}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\operatorname{I}(\\pmb{x}^{*};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})}\\end{array}$ in input-space where $\\pmb{x}^{*}=\\arg\\operatorname*{max}_{\\pmb{x}}f_{\\pmb{x}}$ . Similarly to ITL, output-space entropy search methods (Hoffman & Ghahramani, 2015; Wang & Jegelka, 2017), which select $\\bar{\\pmb{x}_{n}}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\operatorname{I}(f^{*};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})$ with $f^{*}=\\operatorname*{max}_{\\pmb{x}}f_{\\pmb{x}}$ , are more computationally tractable. In fact, output-space entropy search is a special case of ITL with a stochastic target space (cf. Equation (47) in Appendix K.1). Bogunovic et al. (2016) analyze TRUVAR in the context of Bayesian optimization and level set estimation. TRUVAR is akin to VTL with a similar notion of \u201ctarget space\u201d, but their algorithm and analysis rely on a threshold scheme which requires that ${\\mathcal{A}}\\subseteq S$ . Fiez et al. (2019) introduce the transductive linear bandit problem, which is a special case of transductive active learning limited to a linear function class and with the objective of determining the maximum within an initial candidate set.8 We mention additional more loosely related works in Appendix A. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We investigated the generalization of active learning to settings with concrete prediction targets and/or with limited information due to constrained sample spaces. This provides a flexible framework, applicable also to other domains than were discussed (such as recommender systems, molecular design, robotics, etc.) by varying the choice of target space and sample space. Further, we proved novel generalization bounds which may be of independent interest for active learning. Finally, we demonstrated across broad applications that sampling relevant and diverse points (as opposed to only one of the two) leads to a substantial improvement upon the state-of-the-art. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Many thanks to Armin Lederer, Johannes Kirschner, Jonas Rothfuss, Lars Lorch, Manish Prajapat, Nicolas Emmenegger, Parnian Kassraie, and Scott Sussex for their insightful feedback on different versions of this manuscript, as well as Anton Baumann for helpful discussions. We further thank Freddie Bickford Smith for a constructive discussion regarding the relationship between our work and prior work. ", "page_idx": 10}, {"type": "text", "text": "This project was supported in part by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and Innovation Program Grant agreement no. 815943, the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and by a grant of the Hasler foundation (grant no. 21039). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abbasi-Yadkori, Y. Online learning for linearly parametrized control problems. PhD thesis, University of Alberta, 2013.   \nAntor\u00e1n, J., Janz, D., Allingham, J. U., Daxberger, E., Barbano, R. R., Nalisnick, E., and Hern\u00e1ndezLobato, J. M. Adapting the linearised laplace model evidence for modern deep learning. In ICML, 2022.   \nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely wide neural net. NeurIPS, 32, 2019.   \nArthur, D., Vassilvitskii, S., et al. k-means $^{++}$ : The advantages of careful seeding. In SODA, volume 7, 2007.   \nAsh, J., Goel, S., Krishnamurthy, A., and Kakade, S. Gone fishing: Neural active learning with fisher embeddings. NeurIPS, 34, 2021.   \nAsh, J. T., Zhang, C., Krishnamurthy, A., Langford, J., and Agarwal, A. Deep batch active learning by diverse, uncertain gradient lower bounds. ICLR, 2020.   \nAudibert, J.-Y., Bubeck, S., and Munos, R. Best arm identification in multi-armed bandits. In COLT, 2010.   \nBalestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian, Y., et al. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023.   \nBarrett, A. B. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. Physical Review E, 91(5), 2015.   \nBaumann, D., Marco, A., Turchetta, M., and Trimpe, S. Gosafe: Globally optimal safe robot learning. In ICRA, 2021.   \nBengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In ICML, volume 26, 2009.   \nBeraha, M., Metelli, A. M., Papini, M., Tirinzoni, A., and Restelli, M. Feature selection via mutual information: New theoretical insights. In IJCNN, 2019.   \nBerkenkamp, F., Schoellig, A. P., and Krause, A. Safe controller optimization for quadrotors with gaussian processes. In ICRA, 2016.   \nBerkenkamp, F., Krause, A., and Schoellig, A. P. Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. Machine Learning, 2021.   \nBerlind, C. and Urner, R. Active nearest neighbors in changing environments. In ICML, 2015.   \nBickford Smith, F., Kirsch, A., Farquhar, S., Gal, Y., Foster, A., and Rainforth, T. Prediction-oriented bayesian active learning. In AISTATS, 2023.   \nBickford Smith, F., Foster, A., and Rainforth, T. Making better use of unlabelled data in bayesian active learning. In AISTATS, 2024.   \nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. In ICML, 2015.   \nBogunovic, I., Scarlett, J., Krause, A., and Cevher, V. Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation. NeurIPS, 29, 2016.   \nBottero, A., Luis, C., Vinogradska, J., Berkenkamp, F., and Peters, J. R. Information-theoretic safe exploration with gaussian processes. NeurIPS, 35, 2022.   \nBottero, A. G., Luis, C. E., Vinogradska, J., Berkenkamp, F., and Peters, J. Information-theoretic safe bayesian optimization. arXiv preprint arXiv:2402.15347, 2024.   \nBubeck, S., Munos, R., and Stoltz, G. Pure exploration in multi-armed bandits problems. In ALT, volume 20, 2009.   \nChaloner, K. and Verdinelli, I. Bayesian experimental design: A review. Statistical Science, 1995.   \nChandra, B. Quadrotor simulation, 2023. URL https://github.com/Bharath2/ Quadrotor-Simulation.   \nChen, Y. and Krause, A. Near-optimal batch mode active learning and adaptive submodular optimization. In ICML, 2013.   \nChowdhury, S. R. and Gopalan, A. On kernelized multi-armed bandits. In ICML, 2017.   \nCohn, D. Neural network exploration using optimal experiment design. NeurIPS, 6, 1993.   \nColeman, C., Chou, E., Katz-Samuels, J., Culatana, S., Bailis, P., Berg, A. C., Nowak, R., Sumbaly, R., Zaharia, M., and Yalniz, I. Z. Similarity search for efficient active learning and search of rare concepts. In AAAI, volume 36, 2022.   \nCook, R. D. Detection of influential observation in linear regression. Technometrics, 19(1), 1977.   \nCooper, S. E. and Netoff, T. I. Multidimensional bayesian estimation for deep brain stimulation using the safeopt algorithm. medRxiv, 2022.   \nCover, T. M. Elements of information theory. John Wiley & Sons, 1999.   \nDas, A. and Kempe, D. Algorithms for subset selection in linear regression. In STOC, volume 40, 2008.   \nDas, A. and Kempe, D. Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection. JMLR, 19(1), 2018.   \nDaxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux-effortless bayesian deep learning. NeurIPS, 34, 2021.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.   \nEmmenegger, N., Mutny\\`, M., and Krause, A. Likelihood ratio confidence sets for sequential decision making. NeurIPS, 37, 2023.   \nEsfandiari, H., Karbasi, A., and Mirrokni, V. Adaptivity in adaptive submodularity. In COLT, 2021.   \nEustratiadis, P., Dudziak, \u0141., Li, D., and Hospedales, T. Neural fine-tuning search for few-shot learning. ICLR, 2024.   \nFiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. Sequential experimental design for transductive linear bandits. NeurIPS, 32, 2019.   \nFu, B., Cao, Z., Wang, J., and Long, M. Transferable query selection for active domain adaptation. In CVPR, 2021.   \nGal, Y., Islam, R., and Ghahramani, Z. Deep bayesian active learning with image data. In ICML, 2017.   \nGao, M., Zhang, Z., Yu, G., Ar\u0131k, S. \u00d6., Davis, L. S., and Pfister, T. Consistency-based semisupervised active learning: Towards minimizing labeling cost. In ECCV, 2020.   \nGardner, J. R., Kusner, M. J., Xu, Z. E., Weinberger, K. Q., and Cunningham, J. P. Bayesian optimization with inequality constraints. In ICML, volume 2014, 2014.   \nGeifman, Y. and El-Yaniv, R. Deep active learning over the long tail. arXiv preprint arXiv:1711.00941, 2017.   \nGrandvalet, Y. and Bengio, Y. Semi-supervised learning by entropy minimization. NeurIPS, 17, 2004.   \nGraves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. Automated curriculum learning for neural networks. In ICML, 2017.   \nGraybill, F. A. An introduction to linear statistical models. Literary Licensing, LLC, 1961.   \nGuo, Y. and Greiner, R. Optimistic active-learning using mutual information. In IJCAI, volume 7, 2007.   \nHacohen, G., Dekel, A., and Weinshall, D. Active learning on a budget: Opposite strategies suit high and low budgets. ICML, 2022.   \nHardt, M. and Sun, Y. Test-time training on nearest neighbors for large language models. ICLR, 2024.   \nHe, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian deep ensembles via the neural tangent kernel. NeurIPS, 33, 2020.   \nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. ICLR, 2017.   \nHennig, P. and Schuler, C. J. Entropy search for information-efficient global optimization. JMLR, 13 (6), 2012.   \nHern\u00e1ndez-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. Predictive entropy search for efficient global optimization of black-box functions. NeurIPS, 27, 2014.   \nHoffman, M. W. and Ghahramani, Z. Output-space predictive entropy search for flexible global optimization. In NeurIPS workshop on Bayesian Optimization, 2015.   \nHolzm\u00fcller, D., Zaverkin, V., K\u00e4stner, J., and Steinwart, I. A framework and benchmark for deep batch active learning for regression. JMLR, 24(164), 2023.   \nHopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 1982.   \nHoulsby, N., Husz\u00e1r, F., Ghahramani, Z., and Lengyel, M. Bayesian active learning for classification and preference learning. CoRR, 2011.   \nH\u00fcbotter, J., Sukhija, B., Treven, L., As, Y., and Krause, A. Active few-shot fine-tuning. ICLR workshop on Bridging the Gap Between Practice and Theory in Deep Learning, 2024.   \nIlyas, A., Park, S. M., Engstrom, L., Leclerc, G., and Madry, A. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022.   \nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. NeurIPS, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Johnson, J., Douze, M., and J\u00e9gou, H. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3), 2019. ", "page_idx": 13}, {"type": "text", "text": "Kaddour, J., S\u00e6mundsson, S., et al. Probabilistic active meta-learning. NeurIPS, 33, 2020.   \nKassraie, P. and Krause, A. Neural contextual bandits without regret. In AISTATS, 2022.   \nKhan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. NeurIPS, 32, 2019.   \nKhanna, R., Elenberg, E., Dimakis, A., Negahban, S., and Ghosh, J. Scalable greedy feature selection via weak submodularity. In AISTATS, 2017.   \nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR, 2014.   \nKirsch, A. Black-box batch active learning for regression. arXiv preprint arXiv:2302.08981, 2023.   \nKirsch, A., Van Amersfoort, J., and Gal, Y. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. NeurIPS, 32, 2019.   \nKirschner, J., Mutny, M., Hiller, N., Ischebeck, R., and Krause, A. Adaptive and safe bayesian optimization in high dimensions via one-dimensional subspaces. In ICML, 2019.   \nKoh, P. W. and Liang, P. Understanding black-box predictions via influence functions. In ICML, 2017.   \nKothawade, S., Beck, N., Killamsetty, K., and Iyer, R. Similar: Submodular information measures based active learning in realistic scenarios. NeurIPS, 34, 2021.   \nKrause, A. and Golovin, D. Submodular function maximization. Tractability, 3, 2014.   \nKrause, A. and Guestrin, C. Nonmyopic active learning of gaussian processes: an explorationexploitation approach. In ICML, volume 24, 2007.   \nKrause, A., Singh, A., and Guestrin, C. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. JMLR, 9(2), 2008.   \nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.   \nKumari, L., Wang, S., Das, A., Zhou, T., and Bilmes, J. An end-to-end submodular framework for data-efficient in-context learning. In NAACL, 2024.   \nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. NeurIPS, 30, 2017.   \nLeCun, Y., Cortes, C., and Burges, C. J. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.   \nLee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., and Sohl-Dickstein, J. Deep neural networks as gaussian processes. ICLR, 2018.   \nLee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. NeurIPS, 32, 2019.   \nLee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C. Surgical fine-tuning improves adaptation to distribution shifts. NeurIPS workshop on Distribution Shifts, 2022.   \nLewis, D. and Gale, W. A sequential algorithm for training text classifiers. In SIGIR, 1994.   \nLewis, D. D. and Catlett, J. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings. 1994.   \nMacKay, D. J. Information-based objective functions for active data selection. Neural computation, 4(4), 1992.   \nMaddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. NeurIPS, 32, 2019.   \nMalladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. In ICML, 2023.   \nMartens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, 2015.   \nMehta, R., Shui, C., Nichyporuk, B., and Arbel, T. Information gain sampling for active learning in medical image classification. In UNSURE, 2022.   \nMurphy, K. P. Probabilistic machine learning: Advanced topics. MIT Press, 2023.   \nMutny, M. and Krause, A. Experimental design for linear functionals in reproducing kernel hilbert spaces. NeurIPS, 35, 2022.   \nNemhauser, G. L., Wolsey, L. A., and Fisher, M. L. An analysis of approximations for maximizing submodular set functions\u2014i. Mathematical programming, 14, 1978.   \nOstrovsky, R., Rabani, Y., Schulman, L. J., and Swamy, C. The effectiveness of lloyd-type methods for the $\\boldsymbol{\\mathrm{k}}$ -means problem. JACM, 2013.   \nPacchiano, A., Lee, J. N., and Brunskill, E. Experiment planning with function approximation. NeurIPS, 37, 2024.   \nPeng, H., Long, F., and Ding, C. Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8), 2005.   \nPrabhu, V., Chandrasekaran, A., Saenko, K., and Hoffman, J. Active domain adaptation via clustering uncertainty-weighted embeddings. In ICCV, 2021.   \nPruthi, G., Liu, F., Kale, S., and Sundararajan, M. Estimating training data influence by tracing gradient descent. In NeurIPS, 2019.   \nRahimi, A. and Recht, B. Random features for large-scale kernel machines. NeurIPS, 20, 2007.   \nRai, P., Saha, A., Daum\u00e9 III, H., and Venkatasubramanian, S. Domain adaptation meets active learning. In NAACL HLT workshop on Active Learning for Natural Language Processing, 2010.   \nRothfuss, J., Koenig, C., Rupenyan, A., and Krause, A. Meta-learning priors for safe bayesian optimization. In COLT, 2023.   \nRusso, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., et al. A tutorial on thompson sampling. Foundations and Trends\u00ae in Machine Learning, 11(1), 2018.   \nSaha, A., Rai, P., Daum\u00e9, H., Venkatasubramanian, S., and DuVall, S. L. Active supervised domain adaptation. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD, 2011.   \nScheffer, T., Decomain, C., and Wrobel, S. Active hidden markov models for information extraction. In IDA, 2001.   \nSchreiter, J., Nguyen-Tuong, D., Eberts, M., Bischoff, B., Markert, H., and Toussaint, M. Safe exploration for active learning with gaussian processes. In ECML PKDD, 2015.   \nSener, O. and Savarese, S. Active learning for convolutional neural networks: A core-set approach. ICLR, 2017.   \nSeo, S., Wallat, M., Graepel, T., and Obermayer, K. Gaussian process regression: Active data selection and test point rejection. In Mustererkennung 2000. Springer, 2000.   \nSettles, B. Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2009.   \nSettles, B. and Craven, M. An analysis of active learning strategies for sequence labeling tasks. In EMNLP, 2008.   \nShoham, N. and Avron, H. Experimental design for overparameterized learning with application to single shot deep active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \nShwartz-Ziv, R. and LeCun, Y. To compress or not to compress\u2013self-supervised learning and information theory: A review. arXiv preprint arXiv:2304.09355, 2023.   \nSoviany, P., Ionescu, R. T., Rota, P., and Sebe, N. Curriculum learning: A survey. IJCV, 2022.   \nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, volume 27, 2009.   \nStrang, G. Introduction to linear algebra. SIAM, 5 edition, 2016.   \nSu, J.-C., Tsai, Y.-H., Sohn, K., Liu, B., Maji, S., and Chandraker, M. Active adversarial domain adaptation. In WACV, 2020.   \nSui, Y., Gotovos, A., Burdick, J., and Krause, A. Safe exploration for optimization with gaussian processes. In ICML, 2015.   \nSukhija, B., Turchetta, M., Lindner, D., Krause, A., Trimpe, S., and Baumann, D. Gosafeopt: Scalable safe exploration for global optimization of dynamical systems. Artificial Intelligence, 2023.   \nTamkin, A., Nguyen, D., Deshpande, S., Mu, J., and Goodman, N. Active learning helps pretrained models learn the intended task. NeurIPS, 35, 2022.   \nTan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.   \nThompson, W. R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 1933.   \nTu, S., Frostig, R., Singh, S., and Sindhwani, V. JAX: A python library for differentiable optimal control on accelerators, 2023. URL http://github.com/google/trajax.   \nTurchetta, M., Berkenkamp, F., and Krause, A. Safe exploration for interactive machine learning. NeurIPS, 32, 2019.   \nVakili, S., Khezeli, K., and Picheny, V. On information gain and regret bounds in gaussian process bandits. In AISTATS, 2021.   \nVapnik, V. Estimation of dependences based on empirical data. Springer Science & Business Media, 1982.   \nVergara, J. R. and Est\u00e9vez, P. A. A review of feature selection methods based on mutual information. Neural computing and applications, 24, 2014.   \nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. NeurIPS, 29, 2016.   \nWainwright, M. J. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \nWang, C., Sun, S., and Grosse, R. Beyond marginal uncertainty: How accurately can bayesian regression models estimate posterior predictive correlations? In AISTATS, 2021.   \nWang, Z. and Jegelka, S. Max-value entropy search for efficient bayesian optimization. In ICML, 2017.   \nWei, A., Hu, W., and Steinhardt, J. More than a toy: Random matrix models predict how real-world neural representations generalize. In ICML, 2022. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Widmer, D., Kang, D., Sukhija, B., H\u00fcbotter, J., Krause, A., and Coros, S. Tuning legged locomotion controllers via safe bayesian optimization. CORL, 2023. ", "page_idx": 16}, {"type": "text", "text": "Wilks, S. S. Certain generalizations in the analysis of variance. Biometrika, 1932.   \nWilliams, C. K. and Rasmussen, C. E. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \nWischnewski, A., Betz, J., and Lohmann, B. A model-free algorithm to safely approach the handling limit of an autonomous racecar. In ICCVE, 2019.   \nXia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D. Less: Selecting influential data for targeted instruction tuning. In ICML, 2024.   \nXu, M. and Kazantsev, G. Understanding goal-oriented active learning via influence functions. In NeurIPS Workshop on Machine Learning with Guarantees, 2019.   \nYe, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Compositional exemplars for in-context learning. In ICML, 2023.   \nYehuda, O., Dekel, A., Hacohen, G., and Weinshall, D. Active learning through a covering lens. NeurIPS, 35, 2022.   \nYu, H. and Kim, S. Passive sampling for regression. In ICDM, 2010.   \nYu, K., Bi, J., and Tresp, V. Active learning via transductive experimental design. In ICML, volume 23, 2006.   \nZanette, A., Dong, K., Lee, J. N., and Brunskill, E. Design of experiments for stochastic contextual linear bandits. NeurIPS, 34, 2021.   \nZheng, H., Liu, R., Lai, F., and Prakash, A. Coverage-centric coreset selection for high pruning rates. ICLR, 2023. ", "page_idx": 16}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A general principle of \u201ctransductive learning\u201d was already formulated by the famous computer scientist Vladimir Vapnik in the 20th century. Vapnik proposes the following \u201cimperative for a complex world\u201d: ", "page_idx": 17}, {"type": "text", "text": "When solving a problem of interest, do not solve a more general problem as an intermediate step. Try to get the answer that you really need but not a more general one. ", "page_idx": 17}, {"type": "text", "text": "\u2013 Vapnik (1982) ", "page_idx": 17}, {"type": "text", "text": "These appendices provide additional background, proofs, experiment details, and ablation studies. ", "page_idx": 17}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Additional Related Work 20 ", "page_idx": 17}, {"type": "text", "text": "B Background 20   \nB.1 Information Theory 20   \nB.2 Gaussian Processes 20   \nC Proofs 21   \nC.1 Undirected Case of ITL 21   \nC.2 Non-adaptive Data Selection & Submodularity 21   \nC.3 Batch Diversity: Batch Selection as Non-adaptive Data Selection 22   \nC.4 Measures of Synergies & Approximate Submodularity 23   \nC.5 Convergence of Marginal Gain . 25   \nC.6 Proof of Theorem 3.2 26   \nC.7 Proof of Theorem 3.3 31   \nC.8 Proof of Theorem 5.1 32   \nC.9 Useful Facts and Inequalities 36 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Interpretations & Approximations of Principle (\u2020) 37 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Interpretations of ITL 37   \nD.2 Interpretations of VTL 37   \nD.3 Mean Marginal ITL . 38   \nD.4 Correlation-based Transductive Learning . 40   \nD.5 Summary 40 ", "page_idx": 17}, {"type": "text", "text": "E Stochastic Target Spaces 41 ", "page_idx": 17}, {"type": "text", "text": "F Closed-form Decision Rules 41 ", "page_idx": 17}, {"type": "text", "text": "G Computational Complexity 42 ", "page_idx": 17}, {"type": "text", "text": "H Additional GP Experiments & Details 42 ", "page_idx": 17}, {"type": "text", "text": "I Alternative Settings for Active Fine-Tuning 43 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "I.1 Prediction Targets are Contained in Sample Space: ${\\mathcal{A}}\\subseteq S$ 43   \nI.2 Active Domain Adaptation 44 ", "page_idx": 18}, {"type": "text", "text": "J Additional NN Experiments & Details 44 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "J.1 Experiment Details 45   \nJ.2 Embeddings and Kernels 46   \nJ.3 Towards Uncertainty Quantification in Latent Space 47   \nJ.4 Batch Selection via Conditional Embeddings 47   \nJ.5 Baselines 48   \nJ.6 Additional experiments . 52   \nJ.7 Ablation study of noise standard deviation $\\rho$ 52 ", "page_idx": 18}, {"type": "text", "text": "K Additional Safe BO Experiments & Details 54 ", "page_idx": 18}, {"type": "text", "text": "K.1 A More Exploitative Stochastic Target Space 54   \nK.2 Detailed Comparison with Prior Works . 55   \nK.3 Jumping Past Local Barriers 60   \nK.4 Experiment Details 61 ", "page_idx": 18}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The general principle of non-active \u201ctransductive learning\u201d was introduced by Vapnik (1982). The notion of \u201ctarget\u201d from transductive active learning is akin to the notion of \u201ctask\u201d in curriculum learning (Bengio et al., 2009; Graves et al., 2017; Soviany et al., 2022). The study of settings where the irreducible uncertainty is zero is related to the study of estimability in experimental design (Graybill, 1961; Mutny & Krause, 2022). In feature selection, selecting features that maximize information gain with respect to a to-be-predicted label is a standard approach (Peng et al., 2005; Vergara & Est\u00e9vez, 2014; Beraha et al., 2019) which is akin to ITL (cf. Appendix D). The themes of relevance and diversity are also important for efficient in-context learning (e.g., Ye et al., 2023; Kumari et al., 2024) and data pruning (Zheng et al., 2023). Transductive active learning is complimentary to other learning methodologies, such as semi-supervised learning (Gao et al., 2020), self-supervised learning (Shwartz-Ziv & LeCun, 2023; Balestriero et al., 2023), and meta-learning (Kaddour et al., 2020; Rothfuss et al., 2023). ", "page_idx": 19}, {"type": "text", "text": "B Background ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Information Theory ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Throughout this work, log denotes the natural logarithm. Given random vectors $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ , we denote by ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\qquad\\mathrm{H}[\\pmb{x}]\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathbb{E}_{p(\\pmb{x})}[-\\log p(\\pmb{x})],}\\\\ &{\\mathrm{H}[\\pmb{x}\\mid\\pmb{y}]\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathbb{E}_{p(\\pmb{x},\\pmb{y})}[-\\log p(\\pmb{x}\\mid\\pmb{y})],\\quad{\\mathrm{and}}}\\\\ &{\\,\\,\\,\\operatorname{I}(\\pmb{x};\\pmb{y})\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathrm{H}[\\pmb{x}]-\\mathrm{H}[\\pmb{x}\\mid\\pmb{y}]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the (differential) entropy, conditional entropy, and information gain, respectively (Cover, 1999).9 ", "page_idx": 19}, {"type": "text", "text": "The multivariate information gain (Murphy, 2023) between random vectors $\\mathbf{\\Delta}x,y,z$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{I}(\\pmb{x};\\pmb{y};z)\\stackrel{\\mathrm{def}}{=}\\operatorname{I}(\\pmb{x};\\pmb{y})-\\operatorname{I}(\\pmb{x};\\pmb{y}\\mid z)}\\\\ &{\\quad\\quad\\quad=\\operatorname{I}(\\pmb{x};\\pmb{y})+\\operatorname{I}(\\pmb{x};z)-\\operatorname{I}(\\pmb{x};\\pmb{y},z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $\\operatorname{I}(x;y;z)\\neq0$ it is said that $\\textit{\\textbf{y}}$ and $_{z}$ interact regarding their information about $\\textbf{\\em x}$ . If the interaction is positive, it is said that the information of $_{z}$ about $\\textbf{\\em x}$ is redundant given $\\textit{\\textbf{y}}$ . Conversely, if the interaction is negative, it is said that the information of $_{\\textit{z}}$ about $\\textbf{\\em x}$ is synergistic with $\\textit{\\textbf{y}}$ . The notion of synergy is akin to the frequentist notion of \u201csuppressor variables\u201d in linear regression (Das & Kempe, 2008). ", "page_idx": 19}, {"type": "text", "text": "B.2 Gaussian Processes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The stochastic process $f$ is a Gaussian process (GP, Williams & Rasmussen (2006)), denoted $f\\sim\\mathcal{G P}(\\mu,k)$ , with mean function $\\mu$ and kernel $k$ if for any finite subset $X=\\{\\pmb{x}_{1},\\ldots,\\pmb{x}_{n}\\}\\subseteq\\mathcal{X}$ , $f_{X}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}}_{X},K_{X X})$ is jointly Gaussian with mean vector $\\pmb{\\mu}_{X}(i)=\\mu(\\pmb{x}_{i})$ and covariance matrix $\\overrightharpoon{K_{X X}}(i,j)=k(\\pmb{x}_{i},\\pmb{x}_{j})$ . ", "page_idx": 19}, {"type": "text", "text": "In the following, we formalize the assumptions from the GP setting (cf. Section 3.1). ", "page_idx": 19}, {"type": "text", "text": "Assumption B.1 (Gaussian prior). We assume that $f\\sim\\mathcal{G P}(\\mu,k)$ with known mean function $\\mu$ and kernel $k$ . ", "page_idx": 19}, {"type": "text", "text": "Assumption B.2 (Gaussian noise). We assume that the noise $\\varepsilon_{x}$ is mutually independent and zero-mean Gaussian with known variance ${\\rho}^{2}(x)>0$ . We write $P_{X}=\\operatorname{diag}\\rho^{2}(\\mathbf{\\bar{x}}_{1}),\\dotsc,\\rho^{2}(\\mathbf{x}_{n})$ . ", "page_idx": 19}, {"type": "text", "text": "Under Assumptions B.1 and B.2, the posterior distribution of $f$ after observing points $X$ is $\\mathcal{G P}(\\mu_{n},k_{n})$ with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{n}(x)=\\mu(x)+K_{x X}(K_{X X}+P_{X})^{-1}(y_{X}-\\mu_{X}),}\\\\ &{k_{n}(x,x^{\\prime})=k(x,x^{\\prime})-K_{x X}(K_{X X}+P_{X})^{-1}K_{X x^{\\prime}},}\\\\ &{\\quad\\sigma_{n}^{2}(x)=k_{n}(x,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For Gaussian random vectors $\\pmb{f}$ and $\\textit{\\textbf{y}}$ , the entropy is $\\begin{array}{r}{\\mathrm{H}[{\\pmb f}]=\\frac{n}{2}\\log(2\\pi e)+\\frac{1}{2}\\log|\\mathrm{Var}[{\\pmb f}]|}\\end{array}$ , the information gain is $\\begin{array}{r}{\\operatorname{I}(\\pmb{f};\\pmb{y})=\\frac{1}{2}(\\log|\\operatorname{Var}[\\pmb{y}]|-\\log|\\operatorname{Var}[\\pmb{y}\\mid\\pmb{f}]|)}\\end{array}$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma_{n}=\\operatorname*{max}_{X\\subseteq\\mathcal{X}\\atop|X|\\leq n}\\frac{1}{2}\\log\\left|I+P_{X}^{-1}K_{X X}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will write ", "page_idx": 20}, {"type": "text", "text": "$\\sigma^{2}\\,{\\overset{\\underset{\\mathrm{def}}{}}{=}}\\,\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\sigma_{0}^{2}(\\mathbf{x})$ , and $\\tilde{\\sigma}^{2}\\,\\overset{\\mathrm{def}}{=}\\operatorname*{max}_{{\\pmb x}\\in\\mathcal{X}}\\sigma_{0}^{2}({\\pmb x})+\\rho^{2}({\\pmb x}).$ ", "page_idx": 20}, {"type": "text", "text": "The following is a brief overview of the structure of this section: ", "page_idx": 20}, {"type": "text", "text": "1. Appendix C.1 relates ITL in the inductive learning setting $(S\\subseteq A)$ to prior work.   \n2. Appendix C.2 relates the designs selected by ITL and VTL to the optimal designs for corresponding non-adaptive objectives.   \n3. Appendix C.3 shows that batch selection via ITL or VTL leads to informative and diverse batches, utilizing the results from Appendix C.2.   \n4. Appendix C.4 introduces measures of synergies that generalize the submodularity assumption (cf. Assumption 3.1).   \n5. Appendix C.5 proves key results on the convergence of the ITL and VTL objectives.   \n6. Appendix C.6 proves Theorem 3.2 (convergence in GP setting).   \n7. Appendix C.7 proves Theorem 3.3 (convergence in agnostic setting).   \n8. Appendix C.8 proves Theorem 5.1 (convergence in safe BO application).   \n9. Appendix C.9 includes useful facts. ", "page_idx": 20}, {"type": "text", "text": "C.1 Undirected Case of ITL ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We briefly examine the important special case of ITL where ${\\mathcal{S}}\\subseteq A$ . In this setting, for all $\\pmb{x}\\in S$ , the decision rule of ITL simplifies to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n})\\stackrel{(i)}{=}\\mathrm{I}(f_{A\\setminus\\{x\\}};y_{x}\\mid f_{x},\\mathcal{D}_{n})+\\mathrm{I}(f_{x};y_{x}\\mid\\mathcal{D}_{n})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\stackrel{(i i)}{=}\\mathrm{I}(f_{x};y_{x}\\mid\\mathcal{D}_{n})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathrm{H}[y_{x}\\mid\\mathcal{D}_{n}]-\\mathrm{H}[\\varepsilon_{x}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ follows from the chain rule of information gain and $\\pmb{x}\\in\\mathcal{S}\\subseteq\\mathcal{A}$ ; and $(i i)$ follows from the conditional independence $f_{A}\\perp y_{x}\\mid f_{x}$ . ", "page_idx": 20}, {"type": "text", "text": "If additionally $f$ is a GP then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{H}[y_{x}\\mid{\\mathcal{D}}_{n}]-\\mathrm{H}[\\varepsilon_{x}]={\\frac{1}{2}}\\log\\!\\left(1+{\\frac{\\mathrm{Var}[f_{x}\\mid{\\mathcal{D}}_{n}]}{\\mathrm{Var}[\\varepsilon_{x}]}}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This decision rule has also been termed total information gain (MacKay, 1992). When ${\\mathcal{S}}\\subseteq A$ and observation noise is homoscedastic, this decision rule is equivalent to uncertainty sampling. ", "page_idx": 20}, {"type": "text", "text": "C.2 Non-adaptive Data Selection & Submodularity ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall the non-myopic information gain $\\psi_{A}(X)=\\operatorname{I}(f_{A};{\\pmb y}_{X})$ (ITL) and variance reduction $\\psi_{A}(X)=\\operatorname{tr}\\operatorname{Var}[f_{A}]-\\operatorname{tr}\\operatorname{Var}[f_{A}\\mid y_{X}]$ (VTL) objective functions from Assumption 3.1. In this section, we will relate the designs selected by ITL and VTL to the optimal designs for these objectives. To this end, consider the non-adaptive optimization problem ", "page_idx": 20}, {"type": "equation", "text": "$$\nX^{\\star}=\\arg\\operatorname*{max}_{X\\subseteq S}\\psi_{A}(X).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. For both ITL and VTL, $\\psi_{A}$ is non-negative and monotone. ", "page_idx": 21}, {"type": "text", "text": "Proof. For ITL, $\\psi_{A}(X)\\geq0$ follows from the non-negativity of mutual information. To conclude monotonicity, note that for any $X^{\\prime}\\subseteq X\\subseteq S$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{I}(f_{A};y_{X^{\\prime}})=\\operatorname{H}[f_{A}]-\\operatorname{H}[f_{A}\\mid y_{X^{\\prime}}]\\leq\\operatorname{H}[f_{A}]-\\operatorname{H}[f_{A}\\mid y_{X}]=\\operatorname{I}(f_{A};y_{X})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "due to monotonicity of conditional entropy (which is also called the \u201cinformation never hurts\u201d principle). ", "page_idx": 21}, {"type": "text", "text": "For VTL, recall that tr $\\operatorname{Var}[f_{A}\\mid y_{X}]\\leq\\operatorname{tr}\\operatorname{Var}[f_{A}\\mid y_{X^{\\prime}}]$ for any $X^{\\prime}\\subseteq X\\subseteq S$ (with an implicit expectation over ${\\pmb y}_{X},{\\pmb y}_{X^{\\prime}})$ . Non-negativity and monotonicity of $\\psi_{A}$ then follow analogously to ITL. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. The marginal gain $\\Delta_{\\mathcal{A}}({\\pmb x}\\mid X)\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\psi_{\\mathcal{A}}(X\\cup\\{{\\pmb x}\\})-\\psi_{\\mathcal{A}}(X)$ of $\\pmb{x}\\in S$ given $X\\subseteq S$ is the ITL and VTL objective, respectively. ", "page_idx": 21}, {"type": "text", "text": "Proof. For ITL, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\cal A}({\\pmb x}\\mid{\\cal X})=\\operatorname{I}(f_{\\cal A};{\\pmb y}_{\\cal X},y_{\\cal x})-\\operatorname{I}(f_{\\cal A};y_{\\cal X})}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{H}[f_{\\cal A}\\mid{\\pmb y}_{\\cal X}]-\\operatorname{H}[f_{\\cal A}\\mid{\\pmb y}_{\\cal X},y_{\\cal x}]}\\\\ &{\\qquad\\qquad\\quad=\\operatorname{I}(f_{\\cal A};y_{\\cal x}\\mid{\\pmb y}_{\\cal X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is precisely the ITL objective. ", "page_idx": 21}, {"type": "text", "text": "For VTL, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\cal A}(x\\mid X)=\\mathrm{tr}\\;\\mathrm{Var}[f_{\\cal A}\\mid y_{X}]-\\mathrm{tr}\\;\\mathrm{Var}[f_{\\cal A}\\mid y_{X},y_{x}]}\\\\ {=-\\mathrm{tr}\\;\\mathrm{Var}[f_{\\cal A}\\mid y_{X},y_{x}]+\\mathrm{const}\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is precisely the VTL objective. ", "page_idx": 21}, {"type": "text", "text": "Definition C.3 (Submodularity). $\\psi_{A}$ is submodular if and only if for all $x\\in S$ and $X^{\\prime}\\subseteq X\\subseteq S$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{\\cal A}({\\pmb x}\\mid{\\cal X}^{\\prime})\\geq\\Delta_{\\cal A}({\\pmb x}\\mid{\\cal X}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem C.4 (Nemhauser et al. (1978)). Let Assumption 3.1 hold. For any $n\\geq1$ , $i f$ ITL or VTL selected $\\pmb{x}_{1:n}$ , respectively, then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\psi_{\\cal A}({\\pmb x}_{1:n})\\geq(1-1/e)\\operatorname*{max}_{X\\subseteq{\\cal S}\\atop|X|\\leq n}\\psi_{\\cal A}(X).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. This is a special case of a canonical result from non-negative monotone submodular function maximization (Nemhauser et al., 1978; Krause & Golovin, 2014). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.3 Batch Diversity: Batch Selection as Non-adaptive Data Selection ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall the non-adaptive optimization problem ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{n,k}=\\underset{|B|=k}{\\arg\\operatorname*{max}}\\,\\mathrm{I}(f_{A};y_{B}\\mid\\mathcal{D}_{n-1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "from Equation (3) with batch size $k>0$ , and denote by ${\\cal B}_{n,k}^{\\prime}=x_{n,1:k}$ the greedy approximation from Equation (3). The selection of an individual batch can be seen as a single non-adaptive optimization problem with marginal gain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{n}(\\pmb{x}\\mid\\pmb{B})=\\operatorname{I}(\\pmb{f}_{A};\\pmb{y}_{B},\\pmb{y}_{x}\\mid\\mathcal{D}_{n-1})-\\operatorname{I}(\\pmb{f}_{A};\\pmb{y}_{B}\\mid\\mathcal{D}_{n-1})}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{H}[\\pmb{f}_{A}\\mid\\mathcal{D}_{n-1},\\pmb{y}_{B}]-\\operatorname{H}[\\pmb{f}_{A}\\mid\\mathcal{D}_{n-1},\\pmb{y}_{B},\\pmb{y}_{x}]}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{I}(\\pmb{f}_{A};\\pmb{y}_{x}\\mid\\mathcal{D}_{n-1},\\pmb{y}_{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and which is precisely the objective function of ITL from Equation (3). Hence, the approximation guarantees from Theorems C.4 and C.11 apply. The derivation is analogous for VTL. ", "page_idx": 21}, {"type": "text", "text": "Prior work has shown that the greedy solution $B_{n}^{\\prime}$ is also competitive with a fully sequential \u201cbatchless\u201d decision rule (Chen & Krause, 2013; Esfandiari et al., 2021). ", "page_idx": 21}, {"type": "text", "text": "C.4 Measures of Synergies & Approximate Submodularity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We will now show that \u201cdownstream synergies\u201d, if present, can be seen as a source of learning complexity, which is orthogonal to the information capacity $\\gamma_{n}$ . ", "page_idx": 22}, {"type": "text", "text": "Example C.5. Consider the example where $f$ is a stochastic process of three random variables $X,Y,Z$ where $X$ and $Y$ are Bernoulli $\\textstyle(p\\,=\\,{\\frac{1}{2}})$ , and $Z$ is the XOR of $X$ and $Y$ . Suppose that observations are exact (i.e., $\\varepsilon_{n}=0$ ), that the target space $\\boldsymbol{\\mathcal{A}}$ comprises the output variable $Z$ while the sample space $\\boldsymbol{S}$ comprises the input variables $X$ and $Y$ . Observing any single $X$ or $Y$ yields no information about $Z\\colon\\mathrm{\\bar{I}}(Z;X)=\\bar{\\mathrm{I}}(Z;Y)=0$ , however, observing both inputs jointly perfectly determines $Z$ $:\\operatorname{I}(Z;X,Y)=1$ . Thus, $\\gamma_{n}(\\mathcal{A};S)=1$ if $n\\geq2$ and $\\gamma_{n}(\\mathcal{A};S)=0$ else. ", "page_idx": 22}, {"type": "text", "text": "Learning about $Z$ in examples of this kind is difficult for agents that make decisions greedily, since the next action (observing $X$ or $Y$ ) yields no signal about its long-term usefulness. We call a sequence of observations, such as $\\{X,Y\\}$ , synergistic since its combined information value is larger than the individual values. The prevalence of synergies is not captured by the information capacity $\\gamma_{n}(\\mathcal{A};S)$ since it measures only the joint information gain of $n$ samples within $\\boldsymbol{S}$ . Instead, the prevalence of synergies is captured by the sequence $\\Gamma_{n}^{'\\,}\\,{\\stackrel{\\mathrm{det}}{=}}\\,\\mathrm{max}_{{\\pmb x}\\in{\\mathcal S}}\\,{\\bar{\\Delta}}_{{\\cal A}}({\\pmb x}\\mid{\\pmb x}_{1:n})$ , which measures the maximum information gain of $y_{n+1}$ . If $\\Gamma_{n}>\\Gamma_{n-1}$ at any round $n$ , this indicates a synergy. The following key object measures the additional complexity due to synergies. ", "page_idx": 22}, {"type": "text", "text": "Definition C.6 (Task complexity). For $n\\geq1$ , assuming $\\Gamma_{i}>0$ for all $1\\leq i\\leq n$ , we define the task complexity as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha_{A,S}(n)\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}_{i\\in\\{0,...,n-1\\}}\\frac{\\Gamma_{n-1}}{\\Gamma_{i}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\alpha_{\\ensuremath{\\mathcal{A}},\\ensuremath{\\mathcal{S}}}(n)$ is large only if the information gain of $y_{n}$ is larger than that of a previous observation $y_{i}$ . Intuitively, if $\\alpha_{\\ensuremath{\\mathcal{A}},\\ensuremath{\\mathcal{S}}}(n)$ is large, the agent had to discover the implicit intermediate observations $y_{1},\\dotsc,y_{n-1}$ that lead to downstream synergies. We will subsequently formalize the intimate connections of the task complexity to synergies and submodularity. Note that in the GP setting, $\\alpha_{\\ensuremath{\\mathcal{A}},\\ensuremath{\\mathcal{S}}}(n)$ can be computed online by keeping track of the smallest $\\Gamma_{i}$ during previous rounds $i$ . Further, note that $\\alpha_{\\ensuremath{\\mathcal{A}},S}(n)\\le1$ if $\\psi_{A}$ is submodular. ", "page_idx": 22}, {"type": "text", "text": "C.4.1 The Information Ratio ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Another object will prove useful in our analysis of synergies. ", "page_idx": 22}, {"type": "text", "text": "Consider an alternative multiplicative interpretation of the multivariate information gain (cf. Equation (7)), which we call the information ratio of $X\\subseteq S$ given $D\\subseteq S,|X|,|D|<\\infty$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{\\kappa}(X\\mid D)\\stackrel{\\mathrm{def}}{=}\\frac{\\sum_{{\\pmb{x}}\\in X}\\Delta_{\\pmb{A}}({\\pmb{x}}\\mid D)}{\\Delta_{\\pmb{A}}(X\\mid D)}\\in[0,\\infty).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Observe that $\\bar{\\kappa}(X\\mid D)$ measures the synergy properties of $\\pmb{y}_{X}$ with respect to $f_{A}$ given ${\\mathit{\\textbf{y}}}_{D}$ in a multiplicative sense. That is, if $\\bar{\\kappa}(X\\mid D)>1$ then information in $\\pmb{y}_{X}$ is redundant, whereas if $\\bar{\\kappa}(X\\,\\,\\bar{|\\,}\\,D)<1$ then information in $\\pmb{y}_{X}$ is synergistic, and if $\\bar{\\kappa}(X\\mid D)=1$ then $\\pmb{y}_{X}$ do not mutually interact with respect to $f_{A}$ (all given $y_{D}$ ). In the degenerate case where $\\Delta_{\\cal A}(\\bar{X^{\\phantom{\\dagger}}}|\\;D)=0$ (which implies $\\textstyle\\sum_{{\\pmb x}\\in{\\cal X}}\\Delta_{{\\pmb A}}({\\pmb x}\\mid{\\dot{\\cal D}})=0)$ ) we therefore let $\\bar{\\kappa}(X\\mid D)=1$ . ", "page_idx": 22}, {"type": "text", "text": "The information ratio of ITL is strictly positive in the Gaussian case We prove the following straightforward lower bound to the information ratio of ITL. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.7. Let $X,D~\\subseteq~S,|X|,|D|\\ <\\ \\infty$ . If $f_{A}$ and $\\pmb{y}_{X\\cup D}$ are jointly Gaussian then $\\bar{\\kappa}(X\\mid D)>0$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. W.l.o.g. assume $D=\\emptyset$ . We let $X=\\{\\pmb{x}_{1},\\dots,\\pmb{x}_{k}\\}$ and prove lower and upper bound separately. We assume w.l.o.g. that $\\operatorname{I}(f_{A};y_{X})>0$ which implies $\\vert\\mathrm{Var}[f_{A}\\mid y_{X}]\\vert<\\vert\\mathrm{Var}[f_{A}]\\vert$ . Thus, there exists some $i$ such that $f_{A}$ and $y_{x_{i}}$ are dependent, so $\\vert\\mathrm{Var}[\\pmb{f}_{A}\\mid\\boldsymbol{y}_{\\pmb{x}_{i}}]\\vert<\\vert\\mathrm{Var}[\\pmb{f}_{A}]\\vert$ which implies $\\mathrm{I}(f_{A};y_{{\\pmb x}_{i}})>0$ . We therefore conclude that $\\bar{\\kappa}(X)>0$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "The following example shows that this lower bound is tight. ", "page_idx": 22}, {"type": "text", "text": "Example C.8 (Synergies of Gaussian random variables, inspired by Section 3 of Barrett (2015)). Consider the three random variables $X,\\,Y$ , and $Z$ (think ${\\bar{A}}=\\{X\\}$ and $S=\\{Y,Z\\}$ ) which are jointly Gaussian with mean vector 0 and covariance matrix ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma=\\left[\\!\\!\\begin{array}{c c c}{1}&{a}&{a}\\\\ {a}&{1}&{0}\\\\ {a}&{0}&{1}\\end{array}\\!\\!\\right],\\qquad\\mathrm{for\\;}2a^{2}<1}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the constraint on $a$ is to ensure that $\\Sigma$ is positive definite. Computing the mutual information, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{I}(X;Y)=\\operatorname{I}(X;Z)=-{\\frac{1}{2}}\\log(1-a^{2})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and $\\textstyle\\operatorname{I}(X;Y,Z)=-{\\frac{1}{2}}\\log(1-2a^{2})$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname{I}(X;Y)+\\operatorname{I}(X;Z)}{\\operatorname{I}(X;Y,Z)}}={\\frac{\\log(1-2a^{2}+a^{4})}{\\log(1-2a^{2})}}<1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{a\\to{\\frac{1}{\\sqrt{2}}}}{\\frac{\\log(1-2a^{2}+a^{4})}{\\log(1-2a^{2})}}=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and hence \u2014 perhaps unintuitively \u2014 even if $Y$ and $Z$ are uncorrelated, their information about $X$ may be arbitrarily synergistic. ", "page_idx": 23}, {"type": "text", "text": "C.4.2 The Submodularity of the Special \u201cUndirected\u201d Case of ITL ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the inductive active learning problem considered in most prior works, where ${\\mathcal{S}}\\subseteq A$ and $f$ is a Gaussian process, it holds for ITL that $\\alpha_{\\r{A,S}}(n)=1$ since all learning targets appear explicitly in $\\boldsymbol{S}$ : Lemma C.9. Let ${\\mathcal{S}}\\subseteq A.$ Then $\\psi_{A}$ of ITL is submodular. ", "page_idx": 23}, {"type": "text", "text": "Proof. Fix any $\\textbf{\\em x}\\in{\\cal S}$ and $X^{\\prime}\\subseteq\\,X\\subseteq S$ . Let ${\\bar{X}}\\,{\\overset{\\underset{\\mathrm{def}}{}}{=}}\\,X\\setminus X^{\\prime}$ . By the definition of conditional   \ninformation gain, we have $\\Delta_{\\cal A}(x\\mid X)=\\operatorname{I}(y_{x};f_{\\cal A}\\mid y_{X})=\\operatorname{I}(y_{x};f_{\\cal A},y_{X^{\\prime}}\\mid y_{\\bar{X}})-\\operatorname{I}(y_{x};y_{X^{\\prime}}\\mid y_{\\bar{X}}).$   \nSince for any $\\pmb{x}\\in S$ and $X\\subseteq S$ , $y_{x}\\perp y_{X}\\mid f_{A}$ , this simplifies to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{I}(y_{x};f_{A}\\mid y_{X})=\\operatorname{I}(y_{x};f_{A}\\mid y_{{\\bar{X}}})-\\operatorname{I}(y_{x};y_{X^{\\prime}}\\mid y_{{\\bar{X}}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It then follows from $\\operatorname{I}(y_{\\mathbf{-}};y_{X^{\\prime}}\\mid y_{\\bar{X}})\\geq0$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{\\mathcal{A}}(x\\mid X)=\\operatorname{I}(y_{x};f_{\\mathcal{A}}\\mid y_{X})\\le\\operatorname{I}(y_{x};f_{\\mathcal{A}}\\mid y_{\\bar{X}})=\\Delta_{\\mathcal{A}}(x\\mid X^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This implies that $\\alpha_{\\r{A,S}}(n)\\leq1$ for any $n$ and $\\bar{\\kappa}(X\\mid D)\\ge1$ for any $X,D\\subseteq S$ when ${\\mathcal{S}}\\subseteq A$ . ", "page_idx": 23}, {"type": "text", "text": "C.4.3 The Submodularity Ratio ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Building upon the theory of maximizing non-negative monotone submodular functions (Nemhauser et al., 1978; Krause & Golovin, 2014), Das & Kempe (2018) define the following notion of \u201capproximate\u201d submodularity: ", "page_idx": 23}, {"type": "text", "text": "Definition C.10 (Submodularity ratio). The submodularity ratio of $\\psi_{A}$ up to cardinality $n\\geq1$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\kappa_{\\cal A}(n)\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}_{{D\\subseteq{\\bf x}_{1:n}\\atop{D\\cap X=\\emptyset}}}\\bar{\\kappa}(\\stackrel{\\cdot}{X}\\mid D),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where they define $\\begin{array}{r}{\\frac{0}{0}\\equiv1}\\end{array}$ . $\\psi_{A}$ is said to be $\\kappa$ -weakly submodular for some $\\kappa>0$ if $\\operatorname*{inf}_{n\\in\\mathbb{N}}\\kappa_{A}(n)\\geq\\kappa$ . ", "page_idx": 23}, {"type": "text", "text": "As a special case of Theorem 6 from Das & Kempe (2018), applying that $\\psi_{A}$ is non-negative and monotone, we obtain the following result. ", "page_idx": 23}, {"type": "text", "text": "Theorem C.11 (Das & Kempe (2018)). For any $n\\geq1$ , $i f$ ITL or VTL selected $x_{1:n}$ , respectively, then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{A}(\\pmb{x}_{1:n})\\geq(1-e^{-\\kappa_{A}(n)})\\operatorname*{max}_{\\stackrel{X\\leq S}{|X|\\leq n}}\\psi_{A}(X).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\psi_{A}$ is submodular, it is implied that $\\kappa_{A}(n)\\geq1$ for all $n\\geq1$ in which case Theorem C.11 recovers Theorem C.4. ", "page_idx": 23}, {"type": "text", "text": "C.5 Convergence of Marginal Gain ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our following analysis allows for changing target spaces $A_{n}$ and sample spaces $\\textstyle S_{n}$ (cf. Section 5), and to this end, we redefine $\\Gamma_{n}\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\operatorname*{max}_{{\\pmb x}\\in{\\cal S}_{n}}\\breve{\\Delta_{A_{n}}}(\\dot{\\pmb x}\\mid{\\pmb x}_{1:n})$ . The following theorems show that the marginal gains of ITL and VTL converge to zero, and will serve as the main tool for establishing Theorems 3.2 and 3.3. We will abbreviate $\\alpha_{\\ensuremath{\\mathcal{A}},\\ensuremath{\\mathcal{S}}}(n)$ by $\\alpha_{n}$ . ", "page_idx": 24}, {"type": "text", "text": "Theorem C.12 (Convergence of Marginal Gain for ITL). Assume that Assumptions B.1 and B.2 are satisfied. Fix any integers $n_{1}>n_{0}\\ge0_{\\cdot}$ , $\\Delta=n_{1}-n_{0}+1$ such that for all $i\\in\\{n_{0},\\ldots,n_{1}-1\\}$ , $A_{i+1}\\subseteq A_{i}$ and ${\\boldsymbol{S}}\\,{\\stackrel{\\mathrm{det}}{=}}\\,{\\bar{S}}_{i+1}={\\boldsymbol{S}}_{i}$ . Further, assume $|{\\mathcal{A}}_{n_{0}}|<\\infty$ . Then, if the sequence $\\{x_{i+1}\\}_{i=n_{0}}^{n_{1}}$ was generated by ITL, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Gamma_{n_{1}}\\leq\\alpha_{n_{1}}\\frac{\\gamma_{\\Delta}}{\\Delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, if $n_{0}=0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Gamma_{n_{1}}\\leq\\alpha_{n_{1}}\\frac{\\gamma_{A_{0},S}(\\Delta)}{\\Delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{t^{\\star}}=\\frac{1}{3}\\sum_{m=1}^{K}\\mathbb{E}_{t^{\\star}}}&{=\\phantom{\\frac{1}{3}}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}}\\\\ &{\\phantom{\\frac{1}{3}}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}}\\\\ &{\\phantom{\\frac{1}{3}}=\\frac{\\sin\\theta_{0}}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\phantom{\\frac{1}{3}}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\phantom{\\frac{1}{3}}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}}\\\\ &{\\phantom{\\frac{1}{3}}\\mathbb{E}_{t^{\\star}}\\simeq\\frac{1}{3}\\sum_{\\mathbf{x}=1}^{K}\\mathbb{E \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(i)$ follows from the definition of the task complexity $\\alpha_{n_{1}}$ (cf. Definition C.6); $(i i)$ uses the objective of ITL and that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; $(i i i)$ uses $A_{i+1}\\subseteq A_{i}$ and monotonicity of information gain; $(i v)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; $(v)$ uses the chain rule of information gain; $(v i)$ uses $\\pmb{{y}}_{X}^{\\prime}\\perp\\pmb{{f}}_{A_{n_{0}}}\\mid\\pmb{{f}}_{X}$ and the data processing inequality. The conditional independence follows from the assumption that the observation noise is independent. Similarly, $y_{X}\\perp\\mathcal{D}_{n_{0}}\\mid f_{X}$ which implies $(v i i)$ ", "page_idx": 24}, {"type": "text", "text": "If $n_{0}=0$ , then the bound before line $(v i)$ simplifies to $\\alpha_{n_{1}}\\gamma_{A_{0},S}(\\Delta)/\\Delta$ ", "page_idx": 24}, {"type": "text", "text": "The result for VTL is stated, for simplicity, only for the case where the target space and sample space are fixed. ", "page_idx": 25}, {"type": "text", "text": "Theorem C.13 (Convergence of Marginal Gain for VTL). Assume that Assumptions B.1 and B.2 are satisfied. Then for any $n\\geq1$ , if the sequence $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ is generated by VTL, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Gamma_{n-1}\\leq\\frac{2\\sigma^{2}\\alpha_{n}}{n}\\sum_{{\\pmb x}^{\\prime}\\in{\\cal A}}\\gamma_{\\{{\\pmb x}^{\\prime}\\},S}(n).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We remark that $\\begin{array}{r}{\\sum_{\\pmb{x}^{\\prime}\\in\\mathcal{A}}\\gamma_{\\{\\pmb{x}^{\\prime}\\},\\mathcal{S}}(n)\\leq|\\mathcal{A}|\\gamma_{\\mathcal{A},\\mathcal{S}}(n).}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{i}=}&{-\\frac{1}{n}\\sum_{k=1}^{\\infty}r_{k},}\\\\ &{=\\Updownarrow}\\\\ &{\\frac{\\Phi_{i}}{\\Phi_{i}}\\frac{\\Phi_{j}}{\\sin\\theta_{i}}\\sum_{k=1}^{n}r_{k},}\\\\ &{=\\frac{r_{k}}{\\sin\\theta_{i}}\\sum_{l=1}^{n}\\left\\{\\pi\\mathrm{boss~}[\\phi_{i},\\phi_{j}]-\\frac{1}{\\sin\\theta_{i}}\\mathrm{Coss~}[\\phi_{i},\\phi_{j}]_{l}\\right\\},}\\\\ &{\\quad\\sqrt{\\pi}\\mathrm{~and~}r_{i}[\\phi_{i},\\phi_{j}]_{l}=\\frac{r_{k}\\sin\\theta_{i}}{\\sin\\theta_{i}}\\prod_{l=1}^{n}r_{l}\\sin\\theta_{i}\\mathrm{Coss~}[\\phi_{i},\\phi_{l}]_{l}}\\\\ &{\\quad\\sqrt{\\pi}\\mathrm{~and~}r_{i}[\\phi_{i},\\phi_{j}]_{l}=\\frac{r_{l}^{2}-r_{k}^{2}}{\\sin\\theta_{i}}\\mathrm{Coss~}[\\phi_{i},\\phi_{j}]_{l},}\\\\ &{\\quad\\sqrt{\\pi}\\frac{r_{k}^{2}}{\\sin\\theta_{i}}\\sum_{l=1}^{n}\\frac{1}{r_{l}\\sin^{2}\\theta_{l}}\\frac{\\mathrm{Coss~}[\\phi_{i},\\phi_{j}]_{l}}{\\sin^{2}\\theta_{i}\\left(\\frac{\\sqrt{\\pi}\\mathrm{bos~}[\\phi_{i},\\phi_{j}]}{\\sqrt{\\pi}\\mathrm{~and~}\\phi_{j}}-\\frac{1}{2}\\right)}}\\\\ &{=\\frac{r_{k}^{2}m_{k}}{2}\\sum_{l=1}^{n}\\frac{1}{r_{l}\\sin^{2}\\theta_{l}}\\sum_{l=1}^{n}(r_{l}\\cos\\theta_{i})_{l},}\\\\ &{\\quad\\sqrt{\\pi}\\frac{r_{l}^{2}m_{l}^{2}}{\\sin^{2}\\theta_{i}}\\sum_{l=1}^{n}(r_{l}\\cos\\theta_{i})_{l},}\\\\ &{\\quad\\sqrt{\\pi}\\frac{r_{l}^{2}m_{l}^{2}}{\\sin^{2}\\theta_{i}}\\sum_{l=1}^{n}(r_{l}\\cos\\theta_ \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(i)$ follows from the definition of the task complexity $\\alpha_{n_{1}}$ (cf. Definition C.6); $(i i)$ follows from the VTL decision rule and that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; $(i i i)$ follows from Lemma C.38 and monotonicity of variance; $(i v)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; and $(v)$ uses the chain rule of mutual information. The remainder of the proof is analogous to the proof of Theorem C.12 (cf. Appendix C.5). \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Keeping track of the task complexity online In general, the task complexity $\\alpha_{n}$ may be larger than one in the \u201cdirected\u201d setting (i.e., when ${\\mathcal{S}}\\ {\\mathcal{Z}}\\ A$ ). However, note that $\\alpha_{n}$ can easily be evaluated online by keeping track of the smallest $\\Gamma_{i}$ during previous rounds $i$ . ", "page_idx": 25}, {"type": "text", "text": "C.6 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We will now prove Theorem 3.2. We first prove the convergence of marginal variance within $\\boldsymbol{S}$ for ITL, before proving the convergence outside $\\boldsymbol{S}$ in Appendix C.6.1. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.14 (Uniform convergence of marginal variance within $\\boldsymbol{S}$ for ITL). Assume that Assumptions B.1 and $B.2$ are satisfied. For any $n\\geq0$ and $x\\in A\\cap S$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}({\\pmb x})\\leq2\\tilde{\\sigma}^{2}\\cdot\\Gamma_{n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{n}^{2}(x)=\\mathrm{Var}[f_{\\pi}\\;|\\;\\mathcal{D}_{n}]-\\underbrace{\\mathrm{Var}[f_{\\pi}\\;|\\;f_{\\pi},\\mathcal{D}_{n}]}_{\\Theta}}&{}\\\\ {~~~~}&{~~~~}\\\\ {\\frac{\\langle\\Pi\\rangle}{\\Omega}\\;\\mathrm{Var}[f_{\\pi}\\;|\\;\\mathcal{D}_{n}]-\\rho^{2}(x)-\\langle\\mathrm{Var}[g_{\\pi}\\;|\\;f_{\\pi},\\mathcal{D}_{n}]-\\rho^{2}(x)\\rangle}\\\\ {~~~~}&{~~~=\\mathrm{Var}[g_{\\pi}\\;|\\;\\mathcal{D}_{n}]-\\langle\\mathrm{Var}[g_{\\pi}\\;]\\;f_{\\pi},\\mathcal{D}_{n}]}\\\\ {~~~~}&{~~~\\stackrel{{(a)}}{\\le}\\beta^{2}\\mathrm{log}\\Bigg(\\frac{\\mathrm{Var}[g_{\\pi}\\;|\\;\\mathcal{D}_{n}]}{\\mathrm{Var}[g_{\\pi}\\;|\\;f_{\\pi},\\mathcal{D}_{n}]}\\Bigg)}\\\\ {~~~~}&{~~~=2\\beta^{2}\\cdot1\\langle f_{\\pi}\\;\\rangle\\;\\bigcap_{n}}\\\\ {\\frac{\\langle\\Pi\\rangle}{\\Omega}\\;\\mathrm{O}^{2}\\cdot1[f_{\\pi};\\mathfrak{D}_{n}]}&{}\\\\ {\\frac{\\langle\\mathrm{W}\\rangle}{\\Omega}\\;\\mathrm{O}^{2}\\cdot\\underbrace{\\mathrm{max}[(f_{\\pi}\\;\\mathrm{D}_{n})\\;\\mathrm{I}(f_{\\pi})]}_{\\eq\\alpha\\beta}}\\\\ {~~~~}&{~~~\\stackrel{{(a)}}{\\le}2\\beta^{2}\\cdot\\underbrace{\\mathrm{max}[(f_{\\pi}\\;\\mathrm{D}_{n})\\;\\mathrm{O}(f_{\\pi})]}_{\\eq\\alpha\\beta}}\\\\ {~~~~}&{~~~\\stackrel{{(b)}}{\\le}\\frac{1}{2}\\beta^{2}\\cdot\\underbrace{\\mathrm{max}[(f_{\\pi};\\mathrm{D}_{n})\\;\\mathrm{O}(f_{\\pi})]}_{\\eq\\alpha\\beta}}\\\\ {~~~~}&{~~~=2\\beta^{2}\\cdot\\underbrace{\\mathrm{max}[(f_{\\pi};\\mathrm{D}_{n})\\;\\mathrm{I}(f_{\\pi})]}_{\\in\\mathcal{D}_{n}}}\\\\ {~~~~~}&{~~=2\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(i)$ follows from the noise assumption (cf. Assumption B.2); $(i i)$ follows from Lemma C.38 and using monotonicity of variance; $(i i i)$ follows from $\\pmb{x}\\in A$ and monotonicity of information gain; $(i v)$ follows from $x\\in S$ ; and $(v)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "C.6.1 Convergence outside $\\boldsymbol{S}$ for ITL ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We will now show convergence of marginal variance to the irreducible uncertainty for points outside the sample space. ", "page_idx": 26}, {"type": "text", "text": "Our proof roughly proceeds as follows: We construct an \u201capproximate Markov boundary\u201d of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ , and show (1) that the size of this Markov boundary is independent of $n$ , and (2) that a small uncertainty reduction within the Markov boundary implies that the marginal variances at the Markov boundary and(!) $\\textbf{\\em x}$ are small. ", "page_idx": 26}, {"type": "text", "text": "Definition C.15 (Approximate Markov boundary). For any $\\epsilon>0$ , $n\\geq0$ , and $\\pmb{x}\\in\\mathcal{X}$ , we denote by $B_{n,\\epsilon}({\\boldsymbol{x}})$ the smallest (multi-)subset of $\\boldsymbol{S}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n},\\pmb{y}_{B_{n,\\epsilon}(\\pmb{x})}]\\leq\\eta_{S}^{2}(\\pmb{x})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We call $B_{n,\\epsilon}({\\boldsymbol{x}})$ an $\\epsilon$ -approximate Markov boundary of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ . ", "page_idx": 26}, {"type": "text", "text": "Equation (14) is akin to the notion of the smallest Markov blanket in $\\boldsymbol{S}$ of some $\\pmb{x}\\in\\mathcal{X}$ (called a Markov boundary) which is the smallest set $B\\subseteq S$ such that $f_{x}\\perp f_{S}\\mid f_{B}$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma C.16 (Existence of an approximate Markov boundary). For any $\\epsilon>0$ , let $k$ be the smallest integer satisfying ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{k}\\leq\\frac{\\epsilon\\lambda_{\\operatorname*{min}}(K_{\\mathcal{S}\\mathcal{S}})}{2|\\mathcal{S}|\\sigma^{2}\\tilde{\\sigma}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for any $n\\geq0$ and $\\pmb{x}\\in\\mathcal{X}$ , there exists an $\\epsilon$ -approximate Markov boundary $B_{n,\\epsilon}({\\boldsymbol{x}})$ of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ with size at most $k$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma C.16 shows that for any $\\epsilon>0$ there exists a universal constant $b_{\\epsilon}$ (with respect to $n$ and $\\textbf{\\em x}$ ) such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n|B_{n,\\epsilon}({\\pmb x})|\\leq b_{\\epsilon}\\qquad\\forall n\\geq0,{\\pmb x}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We defer the proof of Lemma C.16 to Appendix C.6.3 where we also provide an algorithm to compute $B_{n,\\epsilon}({\\boldsymbol{x}})$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma C.17. For any $\\epsilon>0$ , $n\\geq0$ , and $x\\in\\mathcal{X}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}(\\pmb{x})\\leq2\\sigma^{2}\\cdot\\operatorname{I}(f_{\\pmb{x}};\\pmb{y}_{B_{n,\\epsilon}(\\pmb{x})}\\mid\\pmb{\\mathcal{D}}_{n})+\\eta_{S}^{2}(\\pmb{x})+\\epsilon\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $B_{n,\\epsilon}({\\boldsymbol{x}})$ is an $\\epsilon$ -approximate Markov boundary of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{n}^{2}(\\pmb{x})=\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n}]-\\eta_{S}^{2}(\\pmb{x})+\\eta_{S}^{2}(\\pmb{x})}\\\\ &{\\qquad\\quad\\stackrel{(i)}{\\leq}\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n}]-\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{Y}_{\\mathcal{B}_{n,\\epsilon}(\\pmb{x})},\\mathcal{D}_{n}]+\\eta_{S}^{2}(\\pmb{x})+\\epsilon}\\\\ &{\\qquad\\quad\\stackrel{(i i)}{\\leq}\\sigma^{2}\\log\\left(\\frac{\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n}]}{\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{Y}_{\\mathcal{B}_{n,\\epsilon}(\\pmb{x})},\\mathcal{D}_{n}]}\\right)+\\eta_{S}^{2}(\\pmb{x})+\\epsilon}\\\\ &{\\qquad=2\\sigma^{2}\\cdot\\mathrm{I}(f_{\\pmb{x}};\\pmb{y}_{\\mathcal{B}_{n,\\epsilon}(\\pmb{x})}\\mid\\mathcal{D}_{n})+\\eta_{S}^{2}(\\pmb{x})+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(i)$ follows from the defining property of an $\\epsilon_{}$ -approximate Markov boundary (cf. Equation (14)); and $(i i)$ follows from Lemma C.38 and using monotonicity of variance. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma C.18. For any $\\epsilon>0,n\\geq0,$ , and $x\\in A$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{I}(f_{\\pmb{x}};\\pmb{y}_{B_{n,\\epsilon}(\\pmb{x})}\\mid\\mathcal{D}_{n})\\le\\frac{b_{\\epsilon}}{\\bar{\\kappa}_{n}(B_{n,\\epsilon}(\\pmb{x}))}\\Gamma_{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $_{\\mathit{B}_{n,\\epsilon}(x)}$ is an $\\epsilon$ -approximate Markov boundary of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ , $|B_{n,\\epsilon}({\\pmb x})|\\ \\leq\\ b_{\\epsilon}$ , and where $\\bar{\\kappa}_{n}(\\cdot)\\overset{\\mathrm{qer}}{=}\\bar{\\kappa}(\\cdot\\mid\\pmb{x}_{1:n})$ denotes the information ratio from Equation (8). ", "page_idx": 27}, {"type": "text", "text": "We remark that $\\bar{\\kappa}_{n}(\\cdot)>0$ as is shown in Lemma C.7, and hence, the right-hand side of the inequality is well-defined. ", "page_idx": 27}, {"type": "text", "text": "Proof. We use the abbreviated notation ${\\boldsymbol{B}}={\\boldsymbol{B}}_{n,\\epsilon}({\\boldsymbol{x}})$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{I}(f_{\\pi};y_{B}\\ |\\ \\mathcal{D}_{n})\\stackrel{(i)}{\\leq}\\mathbb{I}(f_{A};y_{B}\\ |\\ \\mathcal{D}_{n})}&{}\\\\ {\\stackrel{(i i)}{\\leq}\\frac{1}{\\tilde{K}_{n,k}}\\displaystyle\\sum_{\\tilde{\\pi}\\in\\mathcal{B}}\\mathbb{I}(f_{A};y_{\\tilde{\\pi}}\\ |\\ \\mathcal{D}_{n})}\\\\ &{\\stackrel{(i i i)}{\\leq}\\frac{b_{\\tilde{\\pi}}}{\\tilde{K}_{n,k}}\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}\\in\\mathcal{B}}\\mathbb{I}(f_{A};y_{B}\\ |\\ \\mathcal{D}_{n})}\\\\ &{\\stackrel{(i v)}{\\leq}\\frac{b_{\\tilde{\\pi}}}{\\tilde{K}_{n,k}}\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}\\in\\mathcal{E}}\\mathbb{I}(f_{A};y_{B}\\ |\\ \\mathcal{D}_{n})}\\\\ &{\\stackrel{(e)}{=}\\frac{b_{\\tilde{\\pi}}}{\\tilde{K}_{n,k}}\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}\\in\\mathcal{E}}\\mathbb{I}(f_{A};y_{\\tilde{\\pi}}\\ |\\ \\mathcal{D}_{n})}\\\\ &{=\\frac{b_{\\tilde{\\pi}}}{\\tilde{K}_{n,k}}\\displaystyle\\sum_{\\tilde{\\pi}\\in\\mathcal{E}}\\mathbb{I}(f_{A};y_{\\tilde{\\pi}}\\ |\\ \\mathcal{D}_{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(i)$ follows from monotonicity of mutual information; $(i i)$ follows from the definition of the information ratio $\\bar{\\kappa}_{n,b_{\\epsilon}}$ (cf. Equation (8)); $(i i i)$ follows from $b\\leq b_{\\epsilon};(i v)$ follows from $B\\subseteq S$ ; and $(v)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 3.2 for ITL . The case where $x\\in A\\cap S$ is shown by Lemma C.14 with $C=2\\tilde{\\sigma}^{2}$ . To prove the more general result, fix any $\\pmb{x}\\in A$ and $\\epsilon>0$ . By Lemma C.16, there exists an $\\epsilon$ -approximate Markov boundary $B_{n,\\epsilon}({\\boldsymbol{x}})$ of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ such that $|B_{n,\\epsilon}({\\pmb x})|\\leq b_{\\epsilon}$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{n}^{2}({\\pmb x})\\overset{(i)}{\\leq}2\\sigma^{2}\\cdot\\operatorname{I}(f_{{\\pmb x}};{\\pmb y}_{B_{n,{\\epsilon}}({\\pmb x})}\\mid\\mathcal{D}_{n})+\\eta_{S}^{2}({\\pmb x})+\\epsilon}\\\\ &{\\quad\\quad\\quad\\overset{(i i)}{\\leq}\\frac{2\\sigma^{2}b_{\\epsilon}}{\\bar{\\kappa}_{n}\\left(B_{n,{\\epsilon}}({\\pmb x})\\right)}\\Gamma_{n}+\\eta_{S}^{2}({\\pmb x})+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(i)$ follows from Lemma C.17; and $(i i)$ follows from Lemma C.18. ", "page_idx": 28}, {"type": "text", "text": "Let $\\epsilon=c{\\frac{\\gamma{\\sqrt{n}}}{\\sqrt{n}}}$ with $c=2|S|\\sigma^{2}\\widetilde{\\sigma}^{2}/\\lambda_{\\mathrm{min}}(K_{S S})$ . Then, by Equation (15), $b_{\\epsilon}$ can be bounded for instance by $\\sqrt{n}$ . Together with Theorem C.12 this implies for ITL that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sigma_{n}^{2}(\\pmb{x})\\le\\eta_{S}^{2}(\\pmb{x})+2\\sigma^{2}\\sqrt{n}\\,\\Gamma_{n}+c\\gamma_{\\sqrt{n}}/\\sqrt{n}}}\\\\ &{}&{\\le\\eta_{S}^{2}(\\pmb{x})+c^{\\prime}\\gamma_{n}/\\sqrt{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for a constant $c^{\\prime}$ , e.g., $c^{\\prime}=2\\sigma^{2}+c$ . ", "page_idx": 28}, {"type": "text", "text": "C.6.2 Convergence outside $\\boldsymbol{S}$ for VTL ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 3.2 for VTL . Analogously to Lemma C.17, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{n}^{2}(\\pmb{x})=\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n}]-\\eta_{S}^{2}(\\pmb{x})+\\eta_{S}^{2}(\\pmb{x})}\\\\ &{\\quad\\quad\\quad\\stackrel{(i)}{\\leq}\\mathrm{Var}[f_{\\pmb{x}}\\mid\\mathcal{D}_{n}]-\\mathrm{Var}[f_{\\pmb{x}}\\mid\\pmb{y}_{\\pmb{B}_{n,\\epsilon}(\\pmb{x})},\\mathcal{D}_{n}]+\\eta_{S}^{2}(\\pmb{x})+e}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(i)$ follows from the defining property of an $\\epsilon$ -approximate Markov boundary (cf. Equation (14)). Further, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mathop{Var}}[f_{x}\\ |\\ \\mathcal{D}_{n}]-\\mathrm{\\mathop{Var}}[f_{x}\\ |\\ y_{B_{n,\\epsilon}(x)},\\mathcal{D}_{n}]}\\\\ &{\\stackrel{(i)}{\\le}\\displaystyle\\sum_{\\tilde{x}\\in B_{n,\\epsilon}(x)}\\left(\\mathrm{\\mathop{Var}}[f_{x}\\ |\\ \\mathcal{D}_{n}]-\\mathrm{\\mathop{Var}}[f_{x}\\ |\\ y_{\\tilde{x}},\\mathcal{D}_{n}]\\right)}\\\\ &{\\stackrel{(i i)}{\\le}\\displaystyle\\sum_{\\tilde{x}\\in B_{n,\\epsilon}(x)}\\left(\\mathrm{\\mathop{tr}}\\mathrm{\\mathop{Var}}[f_{A}\\ |\\ y_{1:n}]-\\mathrm{\\mathop{tr}}\\mathrm{\\mathop{Var}}[f_{A}\\ |\\ y_{\\tilde{x}},y_{1:n}]\\right)}\\\\ &{\\stackrel{(i i i)}{\\le}b_{\\epsilon}\\Gamma_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(i)$ follows from the submodularity of $\\psi_{A};(i i)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; and $(i i i)$ follows from the definition of $\\Gamma_{n}$ and Lemma C.16. ", "page_idx": 28}, {"type": "text", "text": "The remainder of the proof is analogous to the result for ITL, using Theorem C.13 to bound $\\Gamma_{n}$ . ", "page_idx": 28}, {"type": "text", "text": "C.6.3 Existence of an Approximate Markov Boundary ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now derive Lemma C.16 which shows the existence of an approximate Markov boundary of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma C.19. For any $S\\subseteq S$ and $k\\geq0$ , there exists $B\\subseteq S$ with $|B|=k$ such that for all $x^{\\prime}\\in S$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{Var}[f_{x^{\\prime}}\\mid y_{B}]\\leq2\\tilde{\\sigma}^{2}\\frac{\\gamma_{k}}{k}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We choose $B\\subseteq S$ greedily using the acquisition function ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{x}}_{k}\\overset{\\mathrm{def}}{=}\\underset{\\tilde{\\pmb{x}}\\in S}{\\arg\\operatorname*{max}}\\operatorname{I}(\\,f_{S};y_{\\tilde{\\pmb{x}}}\\mid\\pmb{y}_{B_{k-1}})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ${\\cal B}_{k}=\\tilde{{\\pmb x}}_{1:k}$ . Note that this is the \u201cundirected\u201d special case of ITL, and hence, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Var}\\big[f_{\\pmb{x}^{\\prime}}\\mid\\pmb{y}_{B_{k}}\\big]\\stackrel{(i)}{\\leq}2\\tilde{\\sigma}^{2}\\Gamma_{k}}\\\\ &{}&{\\stackrel{(i i)}{\\leq}2\\tilde{\\sigma}^{2}\\frac{\\gamma_{k}}{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(i)$ is due to Lemma C.14; and $(i i)$ is due to Theorem C.12 and $\\alpha_{S,S}(k)\\leq1$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma C.20. Given any $\\epsilon>0$ and $B\\subseteq S\\subseteq S$ with $|S|<\\infty$ , such that for any $x^{\\prime}\\in S$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Var}[f_{\\mathbf{x^{\\prime}}}\\mid\\mathbf{y}_{B}]\\leq\\frac{\\epsilon\\lambda_{\\mathrm{min}}(K_{S S})}{|S|\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then for any $x\\in\\mathcal{X}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Var}[f_{x}\\mid y_{B}]\\le\\mathrm{Var}[f_{x}\\mid f_{S}]+\\epsilon.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We will denote the right-hand side of Equation (20) by $\\epsilon^{\\prime}$ . We have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}[f_{x}\\ |\\ y_{B}]}\\\\ &{\\stackrel{(i)}{=}\\mathbb{E}_{f_{s}}[\\mathrm{Var}_{f_{x}}[f_{x}\\ |\\ f_{B}]\\ |\\ y_{B}]}\\\\ &{\\quad\\quad\\quad+\\,\\mathrm{Var}_{f_{s}}[\\mathbb{E}_{f_{x}}[f_{x}\\ |\\ f_{S},y_{B}]\\ |\\ y_{B}]}\\\\ &{\\stackrel{(i i)}{=}\\mathrm{Var}_{f_{x}}[f_{x}\\ |\\ f_{S},y_{B}]+\\mathrm{Var}_{f_{s}}[\\mathbb{E}_{f_{x}}[f_{x}\\ |\\ f_{S},y_{B}]\\ |\\ y_{B}]}\\\\ &{\\stackrel{(i i i)}{=}\\underbrace{\\mathrm{Var}_{f_{x}}[f_{x}\\ |\\ f_{S}]}_{\\mathrm{ireducble~uncertainty}}+\\underbrace{\\mathrm{Var}_{f_{s}}[\\mathbb{E}_{f_{x}}[f_{x}\\ |\\ f_{S}]\\ |\\ y_{B}]}_{\\mathrm{reducble~cpistemic~uncertainty}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(i)$ follows from the law of total variance; $(i i)$ uses that the conditional variance of a Gaussian depends only on the location of observations and not on their value; and $(i i i)$ follows from $f_{x}\\perp$ $y_{B}\\mid f_{S}$ since $B\\subseteq S$ . It remains to bound the reducible uncertainty. ", "page_idx": 29}, {"type": "text", "text": "Let $h_{\\pmb{x}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},~f_{S}\\mapsto\\mathbb{E}[f_{\\pmb{x}}~|~f_{S}]$ where we write $d{\\stackrel{\\mathrm{def}}{=}}\\left|S\\right|$ . Using the formula for the GP posterior mean, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nh_{x}(f_{S})=\\mathbb{E}[f_{x}]+z^{\\top}(f_{S}-\\mathbb{E}[f_{S}])\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $z\\,{\\stackrel{\\mathrm{def}}{=}}\\,K_{S S}^{-1}K_{S x}$ . Because $h$ is a linear function in $f_{S}$ we have for the reducible uncertainty that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Varf}_{\\mathcal{F}}[h_{x}(f_{S})\\ |\\ y_{B}]=z^{\\top}\\mathrm{Var}[f_{S}\\ |\\ y_{B}]z}&{}\\\\ {\\overset{(i)}{\\leq}d\\cdot z^{\\top}\\mathrm{diag}\\mathrm{Var}[f_{S}\\ |\\ y_{B}]z}&{}\\\\ {\\overset{(i i)}{\\leq}\\epsilon^{\\top}d\\ z^{\\top}z}&{}\\\\ {=\\epsilon^{\\prime}d\\ K_{x S}K_{S S}^{-1}K_{S S}^{-1}K_{S x}}&{}\\\\ {\\leq\\frac{\\epsilon^{\\prime}d}{\\lambda_{\\mathrm{min}}(K_{S S})}K_{x S}K_{S S}^{-1}K_{S x}}&{}\\\\ {\\overset{(i i i)}{\\leq}\\frac{\\epsilon^{\\prime}d\\sigma^{2}}{\\lambda_{\\mathrm{min}}(K_{S S})}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(i)$ follows from Lemma C.37; $(i i)$ follows from the assumption that $\\operatorname{Var}[f_{x^{\\prime}}\\mid{\\pmb y}_{B}]\\le\\epsilon^{\\prime}$ for all $x^{\\prime}\\in S$ ; and $(i i i)$ follows from ", "page_idx": 29}, {"type": "equation", "text": "$$\nK_{x S}K_{S S}^{-1}K_{S x}\\le K_{x x}=\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "since $K_{x x}-K_{x S}K_{S S}^{-1}K_{S x}\\geq0$ . ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma C.16. Let $B\\ \\subseteq\\ S$ be the set of size $k$ generated by Lemma C.19 to satisfy $\\operatorname{Var}[f_{x^{'}}\\mid y_{B}]\\le2\\tilde{\\sigma}^{2}\\gamma_{k}/k$ for all $x^{\\prime}\\in S$ . We have for any $x\\in\\mathcal{X}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Var}[f_{x}\\mid\\mathcal{D}_{n},{\\pmb y}_{B}]\\stackrel{(i)}{\\leq}\\mathrm{Var}[f_{x}\\mid{\\pmb y}_{B}]}\\\\ {\\stackrel{(i i)}{\\leq}\\mathrm{Var}[f_{x}\\mid{\\pmb f}_{S}]+\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(i)$ follows from monotonicity of variance; and $(i i)$ follows from Lemma C.20; using $|{\\cal S}|<\\infty$ and the condition on $k$ . ", "page_idx": 29}, {"type": "text", "text": "We remark that Lemma C.19 provides an algorithm (just \u201cundirected\u201d ITL!) to compute an approximate Markov boundary, and the set $B$ returned by this algorithm is a valid approximate Markov boundary for all $x\\in\\mathcal{X}$ . One can simply swap-in ITL with target space $\\{x\\}$ for \u201cundirected\u201d ITL to obtain tighter (but instance-dependent) bounds on the size of the approximate Markov boundary. ", "page_idx": 29}, {"type": "text", "text": "C.6.4 Generalization to Continuous $\\boldsymbol{S}$ for Finite Dimensional RKHSs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this subsection we generalize Theorem 3.2 to continuous sample spaces $\\boldsymbol{S}$ . We will make the following assumption: ", "page_idx": 29}, {"type": "text", "text": "Assumption C.21. The RKHS of the kernel $k$ is finite dimensional. In other words, the kernel $k$ can be expressed as $k(\\pmb{x},\\pmb{x}^{\\prime})=\\phi(\\pmb{x})^{\\top}\\phi(\\pmb{x}^{\\prime})$ for some feature map $\\phi:\\mathcal{X}\\to\\mathbb{R}^{d}$ with $d<\\infty$ . ", "page_idx": 30}, {"type": "text", "text": "In the following, we will denote the design matrix of the sample space $\\boldsymbol{S}$ by $\\Phi\\ {\\stackrel{\\mathrm{def}}{=}}[\\phi(\\mathbf{x}):\\mathbf{x}\\in{\\overleftarrow{S}}]^{\\top}\\in\\mathbb{R}^{|S|\\times d}$ , and we denote by $\\Pi_{\\Phi}$ its orthogonal projection onto the orthogonal complement of the span of $\\Phi$ . In particular, it holds that ", "page_idx": 30}, {"type": "text", "text": "1. $\\Pi_{\\Phi}\\boldsymbol{v}=\\mathbf{0}$ for all $\\pmb{v}\\in\\operatorname{span}\\Phi$ , and   \n2. $\\Pi_{\\Phi}\\pmb{v}=\\pmb{v}$ for all $\\pmb{v}\\in(\\operatorname{span}\\Phi)^{\\perp}$ . ", "page_idx": 30}, {"type": "text", "text": "Especially, $\\pmb{v}\\in\\ker\\Pi_{\\Phi}$ if and only if $\\pmb{v}\\in\\operatorname{span}\\Phi$ . This projection can be computed as follows: Lemma C.22. It holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Pi_{\\Phi}=I-\\Phi^{\\top}(\\Phi\\Phi^{\\top})^{-1}\\Phi.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. $\\Phi^{\\top}(\\Phi\\Phi^{\\top})^{-1}\\Phi$ is the orthogonal projection onto the span of $\\Phi$ (see, e.g., Strang, 2016, page 211). ", "page_idx": 30}, {"type": "text", "text": "Lemma C.23. Under Assumption C.21, the irreducible uncertainty $\\eta_{S}^{2}(\\pmb{x})$ of $x\\in\\mathcal{X}$ is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\eta_{S}^{2}(\\pmb{x})=\\big\\lVert\\phi(\\pmb{x})\\big\\rVert_{\\Pi_{\\Phi}}^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\|v\\|_{A}={\\sqrt{v^{\\top}A v}}$ denotes the Mahalanobis distance. ", "page_idx": 30}, {"type": "text", "text": "Proof. This is an immediate consequence of the formula for the conditional variance of multivariate Gaussians (cf. Appendix B.2), applied to the linear kernel. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemmas C.22 and C.23 imply that $\\eta_{S}^{2}(\\pmb{x}^{\\parallel})\\,=\\,0$ for all $\\mathbf{\\boldsymbol{x}}^{\\parallel}\\,\\in\\,\\mathcal{X}$ with $\\phi(\\pmb{x}^{\\parallel})\\ \\in\\ \\mathrm{span}\\,\\Phi$ . That is, the irreducible uncertainty is zero for points in the span of the sample space. In contrast, for points $x^{\\perp}$ with $\\phi(x^{\\perp})\\;\\in\\;(\\dot{\\mathrm{span}}\\:\\Phi)^{\\perp}$ , the irreducible uncertainty equals the initial uncertainty: $\\eta_{S}^{2}({\\pmb x}^{\\perp})=\\sigma_{0}^{2}({\\pmb x}^{\\perp})$ . The irreducible uncertainty of any other point $\\textbf{\\em x}$ can be computed by simple decomposition of $\\phi({\\pmb x})$ into parallel and orthogonal components. ", "page_idx": 30}, {"type": "text", "text": "Assuming that Assumption C.21 holds and given any (non-finite) $\\mathcal{S}\\subseteq\\mathcal{X}$ , there exists a basis $\\Omega_{\\cal S}~\\subseteq~{\\cal X}$ in the space of embeddings $\\phi(\\cdot)$ such that span $S=\\operatorname{span}\\Omega_{S}$ and $|\\Omega_{\\mathcal{S}}|\\ \\leq\\ d$ . The generalized existence of an approximate Markov boundary for continuous domains can then be shown analogously to Lemma C.16: ", "page_idx": 30}, {"type": "text", "text": "Lemma C.24 (Existence of an approximate Markov boundary for a continuous domain). Let $\\boldsymbol{S}$ be any (continuous) subset of $\\mathcal{X}$ and let Assumption C.21 hold with $d<\\infty$ . Further, for any $\\epsilon>0$ , let $k$ be the smallest integer satisfying ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{k}\\leq\\frac{\\epsilon\\lambda_{\\operatorname*{min}}(K_{\\Omega_{s}\\Omega_{s}})}{2d\\sigma^{2}\\tilde{\\sigma}^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, for any $n\\geq0$ and $\\pmb{x}\\in\\mathcal{X}$ , there exists an $\\epsilon$ -approximate Markov boundary $B_{n,\\epsilon}({\\pmb x})$ of $\\textbf{\\em x}$ in $\\boldsymbol{S}$ with size at most $k$ . ", "page_idx": 30}, {"type": "text", "text": "Proof sketch. The proof follows analogously to Lemma C.16 by conditioning on the finite set $\\Omega_{S}$ as opposed to $\\boldsymbol{S}$ . \u53e3 ", "page_idx": 30}, {"type": "text", "text": "C.7 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We first formalize the assumptions of Theorem 3.3: ", "page_idx": 30}, {"type": "text", "text": "Assumption C.25 (Regularity of $f^{\\star}$ ). We assume that $f^{\\star}$ is in a reproducing kernel Hilbert space $\\mathcal{H}_{k}(\\mathcal{X})$ associated with a kernel $k$ and has bounded norm, that is, $\\|f^{\\star}\\|_{k}\\leq B$ for some finite $B\\in\\mathbb{R}$ . ", "page_idx": 30}, {"type": "text", "text": "Assumption C.26 (Sub-Gaussian noise). We further assume that each $\\varepsilon_{n}$ from the noise sequence $\\{\\varepsilon_{n}\\}_{n=1}^{\\infty}$ is conditionally zero-mean $\\rho({\\pmb x}_{n})$ -sub-Gaussian with known constants $\\rho({\\pmb x})>0$ for all $\\pmb{x}\\in\\mathcal{X}$ . Concretely, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall n\\geq1,\\lambda\\in\\mathbb{R}:\\quad\\mathbb{E}\\big[e^{\\lambda\\epsilon_{n}}\\mid\\mathcal{D}_{n-1}\\big]\\leq\\exp\\bigg(\\frac{\\lambda^{2}\\rho^{2}({\\pmb x}_{n})}{2}\\bigg)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathcal{D}_{n-1}$ corresponds to the $\\sigma$ -algebra generated by the random variables $\\{\\pmb{x}_{i},\\epsilon_{i}\\}_{i=1}^{n-1}$ and ${\\pmb x}_{n}$ . ", "page_idx": 30}, {"type": "text", "text": "We make use of the following foundational result, showing that under the above two assumptions the (misspecified) Gaussian process model from Section 3.1 is an all-time well-calibrated model of $f^{\\star}$ : ", "page_idx": 31}, {"type": "text", "text": "Lemma C.27 (Well-calibrated confidence intervals; Abbasi-Yadkori (2013); Chowdhury & Gopalan (2017)). Pick $\\delta\\in(0,1)$ and let Assumptions $C.25$ and $C.26$ hold. Let ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\beta_{n}(\\delta)=\\|f^{\\star}\\|_{k}+\\rho\\sqrt{2(\\gamma_{n}+1+\\log(1/\\delta))}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\rho=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\rho(\\mathbf{x})$ .10 Then, for all $\\pmb{x}\\in\\mathcal{X}$ and $n\\geq0$ jointly with probability at least $1-\\delta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n|f^{\\star}(\\pmb{x})-\\mu_{n}(\\pmb{x})|\\leq\\beta_{n}(\\delta)\\cdot\\sigma_{n}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\mu_{n}(x)$ and $\\sigma_{n}^{2}({\\pmb x})$ are mean and variance (as defined in Appendix $B.2$ ) of the GP posterior of $f({\\boldsymbol{x}})$ conditional on the observations $\\mathcal{D}_{n}$ , pretending that $\\varepsilon_{i}$ is Gaussian with variance $\\bar{\\rho}^{2}({\\pmb x}_{i})$ . ", "page_idx": 31}, {"type": "text", "text": "The proof of Theorem 3.3 is a straightforward application of Lemma C.27 and Theorem 3.2: ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 3.3. By Theorem 3.2, we have that for all $\\pmb{x}\\in A$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sigma_{n}(\\pmb{x})\\leq\\sqrt{\\eta_{S}^{2}(\\pmb{x})+\\nu_{A,S}^{2}(n)}\\leq\\eta_{S}(\\pmb{x})+\\nu_{A,S}(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The result then follows by application of Lemma C.27. ", "page_idx": 31}, {"type": "text", "text": "C.8 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we derive our main result on Safe BO. In Appendix C.8.1, we give the definition of the reachable safe set $\\mathcal{R}$ and derive the conditions under which convergence to the reachable safe set is guaranteed. Then, in Appendix C.8.2, we prove Theorem 5.1. ", "page_idx": 31}, {"type": "text", "text": "Notation In the agnostic setting from Section 3.2 (i.e., under Assumptions C.25 and C.26), Lemma C.27 provides us with the following $(1-\\delta)$ -confidence intervals (CIs) ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{n}(\\pmb{x})\\overset{\\mathrm{def}}{=}\\mathcal{C}_{n-1}(\\pmb{x})\\cap[\\mu_{n}(\\pmb{x})\\pmb{\\pmb{\\mathscr{\\beta}}}_{n}(\\pmb{\\delta})\\cdot\\sigma_{n}(\\pmb{x})]}\\\\ &{\\quad\\quad\\mathrm{~We~}\\quad\\mathrm{wite}\\quad\\b{u}_{n}(\\pmb{x})\\overset{\\mathrm{def}}{=}\\operatorname*{max}\\mathcal{C}_{n}(\\pmb{x}),\\quad\\boldsymbol{l}_{n}(\\pmb{x})\\overset{\\mathrm{def}}{=}\\operatorname*{min}\\mathcal{C}_{n}(\\pmb{x})}\\\\ &{\\quad\\quad\\cdots\\cdots\\cdots\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\leq_{-1}({\\pmb x})=\\mathbb{R}$ . , and $w_{n}(\\pmb{x})\\stackrel{\\mathrm{aer}}{=}u_{n}(\\pmb{x})-l_{n}(\\pmb{x})$ for its upper bound, lower bound, and width, respectively. ", "page_idx": 31}, {"type": "text", "text": "We learn separate statistical models $f$ and $\\{g_{1},\\ldots,g_{q}\\}$ for the ground truth objective $f^{\\star}$ and ground truth constraints $\\{g_{1}^{\\star},\\ldots,g_{q}^{\\star}\\}$ . We write $\\mathcal{T}^{\\mathrm{qer}}\\dot{\\{f,1,\\ldots,q\\}}$ and collect the constraints in $\\bar{\\mathcal{Z}}_{s}\\,{\\overset{\\mathrm{def}}{=}}\\{1,\\ldots,q\\}$ . Without loss of generality, we assume that the confidence intervals include the ground truths with probability at least $1-\\delta$ jointly for all $i\\in\\mathcal{T}$ .11 For $i\\in\\mathcal{Z}$ , denote by $u_{n,i},l_{n,i},w_{n,i},\\eta_{i},\\beta_{n,i}$ the respective quantities. In the following, we do not explicitly denote the dependence of $\\beta_{n}$ on $\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "To improve clarity, we will refer to the set of potential maximizers defined in Equation (5) as $\\mathcal{M}_{n}$ and denote by $A_{n}$ an arbitrary target space. ", "page_idx": 31}, {"type": "text", "text": "We point out the following corollary: ", "page_idx": 31}, {"type": "text", "text": "Corollary C.28 (Safety). With high probability, jointly for any $n\\geq0$ and any $i\\in\\mathcal{Z}_{s}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall{\\pmb x}\\in S_{n}:g_{i}^{\\star}({\\pmb x})\\geq0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "C.8.1 Convergence to Reachable Safe Set ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Definition C.29 (Reachable safe set). Given any pessimistic safe set ${\\mathcal{S}}\\subseteq{\\mathcal{X}}$ and any $\\epsilon\\geq0$ and $\\beta\\geq0$ , we define the reachable safe set up to $(\\epsilon,\\beta)$ -slack and its closure as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\epsilon,\\beta}(S)\\overset{\\mathrm{def}}{=}S\\cup\\{x\\in\\mathcal{X}\\setminus\\mathcal{S}\\mid}\\\\ &{\\qquad\\qquad\\qquad g_{i}^{\\star}(x)-\\beta(\\eta_{i}(x;\\mathcal{S})+\\epsilon)\\geq0\\mathrm{~for~all~}i\\in\\mathbb{Z}_{s}\\}}\\\\ &{\\bar{\\mathcal{R}}_{\\epsilon,\\beta}(S)\\overset{\\mathrm{def}}{=}\\underset{n\\to\\infty}{\\operatorname*{lim}}(\\mathcal{R}_{\\epsilon,\\beta})^{n}(S)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(\\mathcal{R}_{\\epsilon,\\beta})^{n}$ denotes the $n$ -th composition of $\\mathcal{R}_{\\epsilon,\\beta}$ with itself. ", "page_idx": 31}, {"type": "text", "text": "Remark C.30. Convergence of the safe set to the closure of the reachability operator can only be guaranteed for finite safe sets $\\left\\langle\\left\\vert S^{\\star}\\right\\vert<\\infty\\right)$ . The following proofs readily generalize to continuous domains by considering convergence within the $k$ -th composition of the reachability operator with itself for some $k<\\infty$ . In this case the sample complexity grows with $k$ rather than $\\vert\\mathcal{S}^{\\star}\\vert$ . The only required modification is to lift the assumption of Theorem C.12 that information is gained only while safe sets remain constant (i.e., $S_{i+1}=S_{i}$ for all $i$ ). This assumption is straightforward to lift since for any $n\\geq0$ and $T\\geq1$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in{\\mathcal S}_{n}}\\Delta_{A}(x\\mid x_{1:n+T})\\leq\\frac{1}{T}\\sum_{t=1}^{T}\\operatorname*{max}_{x\\in{\\mathcal S}_{n}}\\Delta_{A}(x\\mid x_{1:n+t})\\leq\\frac{1}{T}\\sum_{t=1}^{T}\\operatorname*{max}_{x\\in{\\mathcal S}_{n+t}}\\Delta_{A}(x\\mid x_{1:n+t})\\leq\\frac{\\gamma_{T}}{T},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "using submodularity for the first inequality and the monotonicity of the safe set for the second inequality. In particular, this shows that one continues learning about points in the original safe set \u2014 even as the safe set grows. ", "page_idx": 32}, {"type": "text", "text": "We denote by ${\\mathcal{S}}_{\\mathrm{0}}$ the initial pessimistic safe set induced by the (prior) statistical model $g$ (cf. Section 5) and write $\\bar{\\mathcal{R}}_{\\epsilon,\\beta}^{'}\\overset{\\mathrm{aet}}{=}\\bar{\\mathcal{R}}_{\\epsilon,\\beta}(S_{0}^{\\mathrm{~\\!~\\!~}})$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma C.31 (Properties of the reachable safe set). For all $S,S^{\\prime}\\subseteq\\mathcal{X}$ , $\\epsilon\\geq0$ , and $\\beta\\geq0$ : ", "page_idx": 32}, {"type": "equation", "text": "$\\begin{array}{r}{S^{\\prime}\\subseteq\\mathcal{S}\\implies\\mathcal{R}_{\\epsilon,\\beta}(S^{\\prime})\\subseteq\\mathcal{R}_{\\epsilon,\\beta}(S),}\\end{array}$ ", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof (adapted from lemma 7.1 of Berkenkamp et al. (2021)). ", "page_idx": 32}, {"type": "text", "text": "1. Let $\\pmb{x}\\in\\mathcal{R}_{\\epsilon,\\beta}(\\pmb{S}^{\\prime})$ . If $\\textbf{\\textit{x}}\\in\\ S$ then $\\pmb{x}\\in\\mathcal{R}_{\\epsilon,\\beta}(\\pmb{S})$ , so let $\\textbf{\\textit{x}}\\not\\in\\ S$ . Then, by definition, for all $i\\in\\mathcal{Z}_{s}$ , $f_{i}^{\\star}({\\pmb x})-\\beta\\eta_{i}({\\pmb x};S^{\\prime})-\\epsilon\\ge0$ . By the monotonicity of variance, $\\eta_{i}(\\pmb{x};S^{\\prime})\\geq\\eta_{i}(\\pmb{x};S)$ for all $i\\in\\mathcal{Z}$ , and hence $f_{i}^{\\star}({\\pmb x})-\\beta\\dot{\\eta}_{i}({\\pmb x};S)-\\epsilon\\geq0$ for all $i\\in\\mathcal{Z}_{s}$ . It follows that $\\pmb{x}\\in\\mathcal{R}_{\\epsilon,\\beta}(\\pmb{S})$ .   \n2. By the monotonicity of variance, $\\eta_{i}(\\pmb{x};\\mathcal{R}_{\\epsilon,\\beta}(\\pmb{S}))\\geq\\eta_{i}(\\pmb{x};\\mathcal{S})$ for all $x\\in\\mathcal{X}$ and $i\\in\\mathcal{Z}$ . Thus, by definition of the safe region, we have that $\\mathcal{R}_{\\epsilon,\\beta}(\\mathcal{R}_{\\epsilon,\\beta}(\\boldsymbol{S}))\\subseteq\\mathcal{S}$ . The result follows by taking the limit.   \n3. The result follows directly from the definition of the true safe set $S^{\\star}$ (cf. Equation (4)). ", "page_idx": 32}, {"type": "text", "text": "Clearly, we cannot expand the safe set beyond $\\bar{\\mathcal{R}}_{0,0}$ . The following is our main intermediate result, showing that either we expand the safe set at some point or the uncertainty converges to the irreducible uncertainty. ", "page_idx": 32}, {"type": "text", "text": "Lemma C.32. Given any $n_{0}~\\geq~0$ , $\\epsilon\\mathrm{~>~0~}$ , let $n^{\\prime}$ be the smallest integer such that $\\nu_{n^{\\prime},\\tilde{\\epsilon}^{2}}\\;\\leq\\;\\tilde{\\epsilon}$ where $\\tilde{\\epsilon}\\,=\\,\\epsilon/2$ . Let $\\beta_{n_{0}+n^{\\prime}}=\\operatorname*{max}_{i\\in{\\mathbb Z}_{s}}\\beta_{n_{0}+n^{\\prime},i}$ . Assume that the sequence of target spaces is monotonically decreasing, i.e., ${\\mathcal{A}}_{n+1}\\subseteq{\\mathcal{A}}_{n}$ . Then, we have with high probability (at least) one of ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big(\\forall\\pmb{x}\\in\\mathcal{A}_{n_{0}+n^{\\prime}},\\ \\forall i\\in\\mathcal{Z}:}\\\\ &{\\quad w_{n_{0}+n^{\\prime},i}(\\pmb{x})\\leq\\beta_{n_{0}+n^{\\prime}}[\\eta_{i}(\\pmb{x};S_{n_{0}+n^{\\prime}})+\\epsilon]}\\\\ &{\\quad a n d\\quad\\mathcal{A}_{n_{0}+n^{\\prime}}\\cap\\mathcal{R}_{\\epsilon,\\beta_{n_{0}+n^{\\prime}}}(S_{n_{0}+n^{\\prime}})\\subseteq S_{n_{0}+n^{\\prime}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "or $|S_{n_{0}+n^{\\prime}+1}|>|S_{n_{0}}|.$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Suppose that $|S_{n_{0}+n^{\\prime}+1}|=|S_{n_{0}}|$ . Then, by Theorem 3.3 (using that the sequence of target spaces is monotonically decreasing), for any $\\pmb{x}\\in\\mathcal{A}_{n_{0}+n^{\\prime}}$ and $i\\in\\mathcal{Z}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\nw_{n_{0}+n^{\\prime},i}(\\pmb{x})\\leq\\beta_{n_{0}+n^{\\prime}}[\\eta_{i}(\\pmb{x};S_{n_{0}+n^{\\prime}})+\\epsilon].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As $S_{n_{0}+n^{\\prime}+1}=S_{n_{0}+n^{\\prime}}$ we have for all $x\\in{\\mathcal{A}}_{n_{0}+n^{\\prime}}\\setminus S_{n_{0}+n^{\\prime}}$ and $i\\in\\mathcal{Z}_{s}$ , with high probability that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0>l_{n_{0}+n^{\\prime},i}(\\pmb{x})\\geq g_{i}^{\\star}(\\pmb{x})-w_{n_{0}+n^{\\prime},i}(\\pmb{x})}&{}\\\\ {\\geq g_{i}^{\\star}(\\pmb{x})-\\beta_{n_{0}+n^{\\prime}}\\big[\\eta_{i}(\\pmb{x};S_{n_{0}+n^{\\prime}})+\\epsilon\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It follows that $\\begin{array}{r}{A_{n_{0}+n^{\\prime}}\\cap\\mathcal{R}_{\\epsilon,\\beta_{n_{0}+n^{\\prime}}}(S_{n_{0}+n^{\\prime}})\\subseteq S_{n_{0}+n^{\\prime}}}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "To gather more intuition about the above lemma, consider the target space ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\mathcal{E}}_{n}\\,{\\stackrel{\\mathrm{def}}{=}}\\,{\\widehat{S}}_{n}\\setminus S_{n}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We call ${\\mathcal{E}}_{n}$ the potential expanders since it contains all points which might be safe, but are not yet known to be safe. Under this target space, the above lemma simplifies slightly: ", "page_idx": 33}, {"type": "text", "text": "Lemma C.33. For any $n\\geq0$ and $\\epsilon,\\beta\\geq0,$ , if $\\mathcal{E}_{n}\\subseteq\\mathcal{A}_{n}$ then with high probability, ", "page_idx": 33}, {"type": "equation", "text": "$$\nS_{n}\\cup(A_{n}\\cap\\mathcal{R}_{\\epsilon,\\beta}(S_{n}))=\\mathcal{R}_{\\epsilon,\\beta}(S_{n}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. With high probability, $\\mathcal{R}_{\\epsilon,\\beta}(S_{n})\\subseteq\\widehat{S}_{n}=S_{n}\\cup\\mathcal{E}_{n}$ . The lemma is a direct consequence. ", "page_idx": 33}, {"type": "text", "text": "The above lemmas can be combined to yield our main result of this subsection, establishing the convergence of ITL to the reachable safe set. ", "page_idx": 33}, {"type": "text", "text": "Theorem C.34 (Convergence to reachable safe set). For any $\\epsilon>0,$ , let $n^{\\prime}$ be the smallest integer satisfying the condition of Lemma C.32, and define $n^{\\star}\\,{\\stackrel{\\mathrm{def}}{=}}\\,(|S^{\\star}|+1)n^{\\prime}$ . Let $\\bar{\\beta}_{n^{\\star}}\\,\\geq\\,\\beta_{n,i}$ for all $n\\,\\leq\\,n^{\\star},i\\,\\in\\,{\\mathcal{T}}_{s}$ . Assume that the sequence of target spaces is monotonically decreasing, i.e., ${\\mathcal{A}}_{n+1}\\subseteq{\\mathcal{A}}_{n}$ . Then, the following inequalities hold jointly with probability at least $1-\\delta$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n(i)\\ \\forall n\\geq0,\\ \\forall i\\in{\\mathcal{T}}_{s}:g_{i}^{\\star}(\\mathbf{x}_{n})\\geq0,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{n^{\\star}}\\cap\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}\\subseteq\\mathcal{S}_{n^{\\star}}\\subseteq\\bar{\\mathcal{R}}_{0,0}=S^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "convergence to safe region ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\forall x\\in{\\mathcal{A}}_{n^{\\star}},\\,\\forall i\\in{\\mathbb{Z}}:w_{n^{\\star},i}(x)\\leq\\bar{\\beta}_{n^{\\star}}\\eta_{i}(x;\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}})+\\epsilon,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "convergence of width ", "page_idx": 33}, {"type": "equation", "text": "$$\n(i\\nu)\\,\\ \\forall\\pmb{x}\\in\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}},\\ \\forall i\\in\\mathcal{T}:\\eta_{i}(\\pmb{x};\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}})=0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "convergence of width within safe region ", "page_idx": 33}, {"type": "text", "text": "Proof. $(i)$ is a direct consequence of Corollary C.28. $S_{n^{\\star}}\\subseteq S^{\\star}$ follows directly from the pessimistic safe set $S_{n^{\\star}}$ from $(i i)$ being a subset of the true safe set $S^{\\star}$ . $(i v)$ follows directly from the definition of irreducible uncertainty. Thus, it remains to establish $A_{n^{\\star}}\\cap\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}\\subseteq S_{n^{\\star}}$ and $(i i i)$ . ", "page_idx": 33}, {"type": "text", "text": "Recall that with high probability $|S_{n}|\\in[0,|S^{\\star}|]$ for all $n\\geq0$ . Thus, the size of the pessimistic safe set can increase at most $\\left\\vert S^{\\star}\\right\\vert$ many times. By Lemma C.32, using the assumption on $n^{\\prime}$ , the size of the pessimistic safe set increases at least once every $n^{\\prime}$ iterations, or else: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall x\\in\\mathcal{A}_{n_{0}+n^{\\prime}},\\ \\forall i\\in\\mathcal{Z}:\\boldsymbol{w}_{n_{0}+n^{\\prime},i}(\\boldsymbol{x})\\leq\\beta_{n_{0}+n^{\\prime}}\\big[\\eta_{i}\\big(\\boldsymbol{x};\\mathcal{S}_{n_{0}+n^{\\prime}}\\big)+\\epsilon\\big]}\\\\ &{\\mathrm{and}\\quad\\mathcal{A}_{n_{0}+n^{\\prime}}\\cap\\mathcal{R}_{\\epsilon,\\beta_{n_{0}+n^{\\prime}}}(S_{n_{0}+n^{\\prime}})\\subseteq S_{n_{0}+n^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Because the safe set can expand at most $\\left|S^{\\star}\\right|$ many times, Equation (28) occurs eventually for some $n_{0}\\leq|S^{\\star}|n^{\\prime}$ . In this case, since $\\bar{\\beta}_{n^{\\star}}\\;\\geq\\;\\dot{\\beta}_{n_{0}+n^{\\prime}}$ and ${\\mathcal{A}}_{n^{\\star}}\\subseteq{\\mathcal{A}}_{n_{0}+n^{\\prime}}$ (as $n_{0}+n^{\\prime}\\leq n^{\\star})$ we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{n^{\\star}}\\cap\\mathcal{R}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}(S_{n_{0}+n^{\\prime}})\\subseteq\\mathcal{A}_{n_{0}+n^{\\prime}}\\cap\\mathcal{R}_{\\epsilon,\\beta_{n_{0}+n^{\\prime}}}(S_{n_{0}+n^{\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\subseteq S_{n_{0}+n^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Lemma C.31 (ii), this implies ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{n^{\\star}}\\cap\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}\\subseteq S_{n_{0}+n^{\\prime}}\\subseteq S_{n^{\\star}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We emphasize that Theorem C.34 holds for arbitrary target spaces $A_{n}$ . If additionally, $\\mathcal{E}_{n}\\subseteq\\mathcal{A}_{n}$ for all $n\\,\\geq\\,0$ then by Lemma C.33, Theorem C.34 (ii) strengthens to $\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}\\subseteq S_{n^{\\star}}$ . Intuitively, $\\mathcal{E}_{n}\\subseteq\\mathcal{A}_{n}$ ensures that one aims to expand the safe set in all directions. Conversely, if $\\mathcal{E}_{n}\\nsubseteq{\\mathcal{A}}_{n}$ then one aims only to expand the safe set in the direction of $A_{n}$ (or not at all if ${\\mathcal{A}}_{n}\\subseteq S_{n})$ ). ", "page_idx": 33}, {"type": "text", "text": "\u201cFree\u201d convergence guarantees in many applications Theorem C.34 can be specialized to yield convergence guarantees in various settings by choosing an appropriate target space $A_{n}$ . Straightforward application of Theorem C.34 (informally) requires that the sequence of target spaces is monotonically decreasing (i.e., ${\\mathcal{A}}_{n+1}\\subseteq{\\mathcal{A}}_{n})$ , and that each target space $A_{n}$ is an \u201cover-approximation\u201d of the actual set of targeted points (such as the set of optimas in the Bayesian optimization setting). We discuss two such applications in the following. ", "page_idx": 34}, {"type": "text", "text": ". Pure expansion: For example, for the target space ${\\mathcal{E}}_{n}$ , Theorem C.34 bounds the convergence of the safe set to the reachable safe set. In this case, the transductive active learning problem corresponds to the \u201cpure expansion\u201d setting, also addressed by the ISE baseline discussed in Section 5. The ISE baseline, however, does not establish convergence guarantees of the kind of Theorem C.34. Note that ${\\mathcal{E}}_{n}$ satisfies the (informal) requirements laid out previously, since it is monotonically decreasing by definition, and with high probability, any point $x\\in S^{\\star}$ that is not in ${\\mathcal{S}}_{n}$ is contained within ${\\mathcal{E}}_{n}$ . ", "page_idx": 34}, {"type": "text", "text": "2. Level set estimation: Given any $\\tau~\\in~\\mathbb{R}$ , we denote the (safe) $\\tau$ -level set of $f^{\\star}$ by ${\\mathcal{L}}^{\\tau}\\,{\\overset{\\underset{\\mathrm{def}}{}}{=}}\\{{\\pmb x}\\in S^{\\star}\\mid f^{\\star}({\\pmb x})=\\tau\\}$ . We define the potential level set as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{n}^{\\tau}\\stackrel{\\mathrm{def}}{=}\\{\\pmb{x}\\in\\widehat{S}_{n}\\;|\\;l_{n}^{f}(\\pmb{x})\\leq\\tau\\leq u_{n}^{f}(\\pmb{x})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "That is, ${\\mathcal{L}}_{n}^{\\tau}$ is the subset of the optimistic safe set $\\widehat{S}_{n}$ where the $\\tau$ -level set of $f^{\\star}$ may be located. Analogously to the potential expanders,  it is straightforward to show that ${\\mathcal{L}}_{n}^{\\tau}$ over-approximates the true $\\tau$ -level set and is monotonically decreasing. ", "page_idx": 34}, {"type": "text", "text": "We remark that our guarantees from this section also apply to the standard (\u201cunsafe\u201d) setting where $S^{\\star}=S_{0}=\\mathcal{X}$ . ", "page_idx": 34}, {"type": "text", "text": "C.8.2 Convergence to Safe Optimum ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we specialize Theorem C.34 for the case that the target space contains the potential maximizers $\\mathcal{M}_{n}$ (cf. Equation (5)). It is straightforward to see that the sequence $\\mathcal{M}_{n}$ is monotonically decreasing (i.e., ${\\mathcal{M}}_{n+1}\\subseteq{\\mathcal{M}}_{n})$ ). The following lemma shows that the potential maximizers overapproximate the set of safe maxima ${\\mathcal{X}}^{*}\\,{\\overset{\\underset{\\mathrm{def}}{}}{=}}\\,\\arg\\operatorname*{max}_{\\mathbf{x}\\in S^{\\star}}\\,f^{\\star}(\\mathbf{x})$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma C.35 (Potential maximizers over-approximate safe maxima). For all $n~\\ge~0$ and with probability at least $1-\\delta$ , ", "page_idx": 34}, {"type": "text", "text": "Proof. If $\\boldsymbol{x}\\not\\in\\mathcal{M}_{n}$ then ", "page_idx": 34}, {"type": "equation", "text": "$$\nu_{n,f}(\\pmb{x})<\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{S}_{n}}l_{n,f}(\\pmb{x}^{\\prime})\\leq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{S}^{\\star}}l_{n,f}(\\pmb{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we used $S_{n}\\subseteq S^{\\star}$ with high probability, which directly implies with high probability that $x\\notin\\mathcal{X}^{*}$ . ", "page_idx": 34}, {"type": "text", "text": "For the other direction, if $\\pmb{x}\\in\\mathcal{X}^{\\ast}$ then ", "page_idx": 34}, {"type": "equation", "text": "$$\nu_{n,f}(\\pmb{x})\\geq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{S}^{\\star}}l_{n,f}(\\pmb{x}^{\\prime})\\geq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{S}_{n}}l_{n,f}(\\pmb{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with high probability. ", "page_idx": 34}, {"type": "text", "text": "We denote the set of optimal actions which are safe up to $(\\epsilon,\\beta)$ -slack by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\boldsymbol{\\chi}_{\\epsilon,\\beta}^{*}\\stackrel{\\mathrm{def}}{=}\\underset{\\mathbf{\\boldsymbol{x}}\\in\\bar{\\mathcal{R}}_{\\epsilon,\\beta}}{\\arg\\operatorname*{max}}\\,f^{\\star}(\\mathbf{\\boldsymbol{x}}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and by $f_{\\epsilon,\\beta}^{*}$ the maximum value attained by $f^{\\star}$ at any of the points in $\\mathcal{X}_{\\epsilon,\\beta}^{*}$ . The regret can be expressed as ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{n}(\\bar{\\mathcal{R}}_{\\epsilon,\\beta})=f_{\\epsilon,\\beta}^{*}-f^{\\star}(\\widehat{\\pmb{x}}_{n})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The following theorem formalizes Theorem 5.1 and establishes convergence to the safe optimum. ", "page_idx": 34}, {"type": "text", "text": "Theorem C.36 (Convergence to safe optimum). For any $\\epsilon>0$ , let $n^{\\prime}$ be the smallest integer satisfying the condition of Lemma C.32, and define $n^{\\star}\\,{\\stackrel{\\mathrm{def}}{=}}\\,(|S^{\\star}|+1)n^{\\prime}$ . Let $\\bar{\\beta}_{n^{\\star}}\\geq\\beta_{n,i}$ for all $n\\leq n^{\\star},i\\in\\mathcal{T}_{s}$ . Then, the following inequalities hold jointly with probability at least $1-\\delta$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n(i)\\ \\forall n\\geq0,\\ \\forall i\\in{\\mathcal{T}}_{s}:g_{i}^{\\star}(\\mathbf{x}_{n})\\geq0,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n(i i)\\ \\forall n\\geq n^{\\star}:r_{n}(\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}})\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Fix any $\\mathbf{\\boldsymbol{x}}^{*}\\;\\in\\;\\lambda_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}^{*}\\;\\subseteq\\;\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}$ . Assume w.l.o.g. that $x^{*}\\in\\mathcal{M}_{n^{\\star}}$ .12 Then, with high probability, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}^{*}=f^{\\star}(\\pmb{x}^{*})\\leq u_{n^{\\star},f}(\\pmb{x}^{*})}&{}\\\\ {=l_{n^{\\star},f}(\\pmb{x}^{*})+w_{n^{\\star},f}(\\pmb{x}^{*})}&{}\\\\ {\\stackrel{(i)}{\\leq}l_{n^{\\star},f}(\\pmb{\\hat{x}_{n^{\\star}}})+w_{n^{\\star},f}(\\pmb{x}^{*})}&{}\\\\ {\\leq f^{\\star}(\\pmb{\\hat{x}_{n^{\\star}}})+w_{n^{\\star},f}(\\pmb{x}^{*})}&{}\\\\ {\\stackrel{(i i)}{\\leq}f^{\\star}(\\pmb{\\hat{x}_{n^{\\star}}})+\\epsilon}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $(i)$ follows from the definition of ${\\widehat{\\pmb{x}}}_{n}$ ; and $(i i)$ follows from Theorem C.34 and noting that x\u2217\u2208Mn\u22c6\u2229R\u00af\u03f5, \u03b2\u00af\u22c6. ", "page_idx": 35}, {"type": "text", "text": "We have shown that $f^{\\star}(\\widehat{\\pmb{x}}_{n^{\\star}})\\geq f_{\\epsilon,\\bar{\\beta}_{n^{\\star}}}^{*}-\\epsilon.$ , which implies $r_{n^{\\star}}(\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}})\\leq\\epsilon$ . Since the upper- and lower-confidence bounds are monotonically decreasing $/$ increasing, respectively, we have that for all $n\\geq n^{\\star}$ , $r_{n}(\\bar{\\mathcal{R}}_{\\epsilon,\\bar{\\beta}_{n^{\\star}}})\\leq\\epsilon$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "C.9 Useful Facts and Inequalities ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We denote by $\\preceq$ the Loewner partial ordering of symmetric matrices. ", "page_idx": 35}, {"type": "text", "text": "Lemma C.37. Let $A\\in\\mathbb{R}^{n\\times n}$ be a positive definite matrix with diagonal $_{D}$ . Then, $A\\preceq n D$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Equivalently, one can show $n D\\,{\\cal-}\\,{\\cal A}\\,\\succeq\\,{\\bf0}$ . We write $A\\,{\\stackrel{\\mathrm{def}}{=}}\\,D^{1/2}Q D^{1/2}$ , and thus, $Q\\,=$ $D^{-\\bar{1}/2}A\\bar{D}^{-1/2}$ is a positive definite symmetric matrix with all diagonal elements equal to 1. It remains to show that ", "page_idx": 35}, {"type": "equation", "text": "$$\nn D-A=D^{1/2}(n I-Q)D^{1/2}\\succeq\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that $\\textstyle\\sum_{i=1}^{n}\\lambda_{i}(\\mathbf{Q})=\\operatorname{tr}Q=n$ , and hence, all eigenvalues of $Q$ belong to $(0,n)$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma C.38. If $a,b\\in(0,M]$ for some $M>0$ and $b\\geq a$ then ", "page_idx": 35}, {"type": "equation", "text": "$$\nb-a\\leq M\\cdot\\log\\left({\\frac{b}{a}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If additionally, $a\\ge M^{\\prime}$ for some $M^{\\prime}>0$ then ", "page_idx": 35}, {"type": "equation", "text": "$$\nb-a\\geq M^{\\prime}\\cdot\\log\\left({\\frac{b}{a}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $f(x)\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\log x$ . By the mean value theorem, there exists $c\\in(a,b)$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{1}{c}}=f^{\\prime}(c)={\\frac{f(b)-f(a)}{b-a}}={\\frac{\\log b-\\log a}{b-a}}={\\frac{\\log({\\frac{b}{a}})}{b-a}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, ", "page_idx": 35}, {"type": "equation", "text": "$$\nb-a=c\\cdot\\log\\left({\\frac{b}{a}}\\right)<M\\cdot\\log\\left({\\frac{b}{a}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "12Otherwise, with high probability, f \u22c6(xn\u22c6) > f \u03f5\u2217, \u03b2\u00afn\u22c6 ", "page_idx": 35}, {"type": "text", "text": "Under the additional condition that $a\\ge M^{\\prime}$ , we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\nb-a=c\\cdot\\log\\left({\\frac{b}{a}}\\right)>M^{\\prime}\\cdot\\log\\left({\\frac{b}{a}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "D Interpretations $\\pmb{\\&}$ Approximations of Principle (\u2020) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We give a brief overview of interpretations and approximations of ITL, as well as alternative decision rules adhering to the fundamental principle $(\\dagger)$ . ", "page_idx": 36}, {"type": "text", "text": "The discussed interpretations of $(\\dagger)$ differ mainly in how they quantify the \u201cuncertainty\u201d about $\\boldsymbol{\\mathcal{A}}$ . In the GP setting, this \u201cuncertainty\u201d is captured by the covariance matrix $\\Sigma$ of $f_{A}$ , and we consider two main ways of \u201cscalarizing\u201d $\\Sigma$ : ", "page_idx": 36}, {"type": "text", "text": "1. the total (marginal) variance $\\operatorname{tr}\\Sigma$ , and   \n2. the \u201cgeneralized variance\u201d $|\\Sigma|$ . ", "page_idx": 36}, {"type": "text", "text": "The generalized variance \u2014 which was originally suggested by Wilks (1932) as a generalization of variance to multiple dimensions \u2014 takes into account correlations. In contrast, the total variance discards all correlations between points in $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 36}, {"type": "text", "text": "All discussed decision rules following principle $\\left(\\dagger\\right)$ (i.e., ITL, VTL, MM-ITL) differ only in their weighting of the points in $\\boldsymbol{\\mathcal{A}}$ , and they coincide when $|\\mathcal{A}|=1$ . ", "page_idx": 36}, {"type": "text", "text": "D.1 Interpretations of ITL ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We briefly discuss three interpretations of ITL. ", "page_idx": 36}, {"type": "text", "text": "Minimizing generalized variance In the GP setting, ITL can be equivalently characterized as minimizing generalized posterior variance: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{n}=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n})}\\\\ &{\\quad=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\,\\frac{1}{2}\\log\\biggl(\\frac{\\left|\\mathrm{Var}\\right[f_{A}\\mid\\mathcal{D}_{n-1}\\right]\\mid}{\\left|\\mathrm{Var}[f_{A}\\mid\\mathcal{D}_{n-1},y_{x}]\\right|}\\biggr)}\\\\ &{\\quad=\\underset{x\\in S}{\\arg\\operatorname*{min}}\\left|\\mathrm{Var}[f_{A}\\mid\\mathcal{D}_{n-1},y_{x}]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Maximizing relevance and minimizing redundancy An alternative interpretation of ITL is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n})=\\underbrace{\\operatorname{I}(f_{A};y_{x})}_{\\mathrm{relevance}}-\\underbrace{\\operatorname{I}(f_{A};y_{x};\\mathcal{D}_{n})}_{\\mathrm{redundancy}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\operatorname{I}(f_{A};y_{x};{\\mathcal D}_{n})~=~\\operatorname{I}(f_{A};y_{x})\\;-\\;\\operatorname{I}(f_{A};y_{x}\\mid{\\mathcal D}_{n})$ denotes the multivariate information gain (cf. Appendix B). In this way, ITL can be seen as maximizing observation relevance while minimizing observation redundancy. This interpretation is common in the literature on feature selection (Peng et al., 2005; Vergara & Est\u00e9vez, 2014; Beraha et al., 2019). ", "page_idx": 36}, {"type": "text", "text": "Steepest descent in measure spaces ITL can be seen as performing steepest descent in the space of probability measures over $f_{A}$ , with the KL divergence as metric: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n})=\\mathbb{E}_{y_{x}}[\\mathrm{KL}(p(f_{A}\\mid\\mathcal{D}_{n},y_{x})\\|p(f_{A}\\mid\\mathcal{D}_{n}))].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "That is, ITL finds the observation yielding the \u201clargest update\u201d to the current density. ", "page_idx": 36}, {"type": "text", "text": "D.2 Interpretations of VTL ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Quantifying the uncertainty about $f_{A}$ by the marginal variance of points in $\\boldsymbol{\\mathcal{A}}$ rather than entropy (or generalized variance), the principle $\\left(\\dagger\\right)$ leads to VTL. Note that if $|\\mathcal{A}|=1$ , then VTL is equivalent to ITL. Unlike the similar, but more technical, TRUVAR algorithm proposed by Bogunovic et al. (2016), VTL does not require truncated variances, and hence, VTL can be applied to constrained settings (where ${\\mathcal{A}}\\subseteq S)$ as well. ", "page_idx": 36}, {"type": "text", "text": "Relationship to ITL Note that the ITL criterion in the GP setting can be expressed as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{n}=\\underset{\\pmb{x}\\in\\mathcal{S}}{\\arg\\operatorname*{min}}\\operatorname{tr}\\log\\operatorname{Var}[\\pmb{f}_{A}\\mid\\mathcal{D}_{n-1},y_{x}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where for a positive semi-definite matrix $\\pmb{A}$ with spectral decomposition $\\pmb{A}=\\pmb{V}\\pmb{\\Lambda}\\pmb{V}^{\\top}$ we write $\\log A\\,=\\,V\\log\\Lambda V^{\\top}$ for the logarithmic matrix function. To derive Equation (34) we use that $\\begin{array}{r}{\\log|\\pmb{A}|=\\sum_{i}\\bar{\\log\\lambda_{i}}(\\pmb{A})=\\operatorname{tr}\\log\\pmb{A}}\\end{array}$ . Hence, ITL and VTL are identical up to a different weighting of the eige nvalues of the posterior covariance matrix. ", "page_idx": 37}, {"type": "text", "text": "Minimizing a bound to the approximation error Chowdhury & Gopalan (2017) (page 19) bound the approximation error $|f^{\\star}({\\bar{\\mathbf{x}}})^{-}-\\mu_{n}({\\mathbf{x}})|$ by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\underbrace{|k_{t}(x)^{\\top}(K_{t}+P_{t})^{-1}\\varepsilon_{1:t}|}_{\\mathrm{variance}}+\\underbrace{|f^{\\star}(x)-k_{t}(x)^{\\top}(K_{t}+P_{t})^{-1}f_{1:t}|}_{\\mathrm{bias}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $k_{t}(\\mathbf{\\boldsymbol{x}})\\stackrel{\\mathrm{def}}{=}K_{x x_{1:t}}$ , $K_{t}\\stackrel{\\mathrm{def}}{=}K_{x_{1:t}}\\pmb{x}_{1:t}$ , and $P_{t}\\,{\\stackrel{\\mathrm{def}}{=}}\\,P_{x_{1:t}}$ . Similar to a standard bias-variance decomposition, the first term measures variance and the second term measures bias. Following Lemma C.27, VTL can be seen as greedily minimizing this bound to the approximation error (i.e., both bias and variance). ", "page_idx": 37}, {"type": "text", "text": "Maximizing correlation to prediction targets weighted by their variance It can be shown (see the proof below) that the VTL decision rule is equivalent to ", "text_level": 1, "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\arg\\operatorname*{max}_{\\pmb{x}\\in S}\\sum_{\\pmb{x}^{\\prime}\\in A}\\mathrm{Var}[f_{\\pmb{x}^{\\prime}}\\mid\\mathcal{D}_{n-1}]\\cdot\\mathrm{Cor}[f_{\\pmb{x}^{\\prime}},y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1}]^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "That is, VTL maximizes the squared correlation between the next observation and the prediction targets, weighted by their variance. Intuitively, prediction targets are weighted by their variance since more can be learned about a prediction target with higher variance. This is precisely what leads to the \u201cdiverse\u201d sample selection, and is akin to \u201cuncertainty sampling\u201d among the prediction targets and then selecting the observation which is most correlated with the selected prediction target. ", "page_idx": 37}, {"type": "text", "text": "Proof. Starting with the VTL objective, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in S}{\\arg\\operatorname*{min}}\\ \\underset{x^{\\prime}\\in A}{\\sum}\\ \\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n},y_{x}]=\\underset{x\\in S}{\\arg\\operatorname*{min}}\\ \\underset{x^{\\prime}\\in A}{\\sum}\\Bigg(\\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n}]-\\frac{\\mathrm{Cov}[f_{x^{\\prime}},y_{x}\\ |\\ \\mathcal{D}_{n}]^{2}}{\\mathrm{Var}[y_{x}\\ |\\ \\mathcal{D}_{n}]}\\Bigg)}\\\\ {=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\ \\underset{x^{\\prime}\\in A}{\\sum}\\ \\frac{\\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n}]\\cdot\\mathrm{Cov}[f_{x^{\\prime}}\\ ,y_{x}\\ |\\ \\mathcal{D}_{n}]^{2}}{\\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n}]\\cdot\\mathrm{Var}[y_{x}\\ |\\ \\mathcal{D}_{n}]}+\\mathrm{const}}\\\\ {=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\ \\underset{x^{\\prime}\\in A}{\\sum}\\ \\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n}]\\cdot\\mathrm{Cor}[f_{x^{\\prime}}\\ ,y_{x}\\ |\\ \\mathcal{D}_{n}]^{2}+\\mathrm{const}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "D.3 Mean Marginal ITL ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "MacKay (1992) previously proposed \u201cmean-marginal\u201d ITL (MM-ITL) in the setting where ${\\mathcal{S}}={\\mathcal{X}}$ , which selects ", "page_idx": 37}, {"type": "equation", "text": "$$\nx_{n}=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\sum_{x^{\\prime}\\in A}\\mathrm{I}(f_{x^{\\prime}};y_{x}\\mid\\mathcal{D}_{n-1})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and which simplifies in the GP setting to ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{n}=\\underset{x\\in S}{\\arg\\operatorname*{max}}\\,\\frac{1}{2}\\displaystyle\\sum_{x^{\\prime}\\in A}\\log\\Big(\\frac{\\mathrm{Var}[f_{x^{\\prime}}\\mid\\mathcal{D}_{n-1}]}{\\mathrm{Var}[f_{x^{\\prime}}\\mid\\mathcal{D}_{n-1},y_{x}]}\\Big)}\\\\ &{\\quad=\\underset{x\\in S}{\\arg\\operatorname*{min}}\\,\\displaystyle\\sum_{x^{\\prime}\\in A}\\log\\mathrm{Var}[f_{x^{\\prime}}\\mid\\mathcal{D}_{n-1},y_{x}]}\\\\ &{\\quad=\\underset{x\\in S}{\\arg\\operatorname*{min}}\\,\\mathrm{tr}\\,\\log\\mathrm{diag}\\,\\mathrm{Var}[f_{A}\\mid\\mathcal{D}_{n-1},y_{x}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Analogously to the derivation of Equation (34), this can also be expressed as ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=\\underset{{\\pmb x}\\in{\\cal S}}{\\arg\\operatorname*{min}}\\left|\\mathrm{diag}\\,\\mathrm{Var}[{\\pmb f}_{{\\pmb A}}\\mid{\\mathcal D}_{n-1},y_{{\\pmb x}}]\\right|.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Effectively, MM-ITL ignores the mutual interaction between points in $\\boldsymbol{\\mathcal{A}}$ . As can be seen from Equation (37) and as is also mentioned by MacKay (1992), MM-ITL is equivalent to VTL up to a different weighting of the points in $\\boldsymbol{\\mathcal{A}}$ : instead of minimizing the average posterior variance (as in VTL), MM-ITL minimizes the average posterior log-variance. Under the lens of principle $(\\dagger)$ , this can be seen as minimizing the average marginal entropy of predictions within the target space: ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=\\underset{{\\pmb x}\\in{\\cal S}}{\\arg\\operatorname*{min}}\\,\\sum_{{\\pmb x}^{\\prime}\\in{\\cal A}}\\mathrm H[f_{{\\pmb x}^{\\prime}}\\mid{\\mathcal D}_{n-1},y_{\\pmb x}]\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We remark that MM-ITL is a special case of EPIG (Bickford Smith et al., 2023, Appendix E.2). ", "page_idx": 38}, {"type": "text", "text": "Not a generalization of uncertainty sampling Unlike ITL, MM-ITL is not a generalization of uncertainty sampling. The reason is precisely that MM-ITL ignores the mutual interaction between points in $\\boldsymbol{\\mathcal{A}}$ . Consider the example where $\\mathcal{X}\\overset{\\cdot}{=}\\mathcal{S}=\\mathcal{A}=\\{1,\\dotsc,10\\}$ where $f_{1:9}$ are highly correlated while $f_{10}$ is mostly independent of the other points. Visually, imagine a smooth function (i.e., under a Gaussian kernel) with points 1 through 9 close to each other and point 10 far away. Further, suppose that point 10 has a slightly larger marginal variance than the others. Then, MM-ITL would select one of the points $1:9$ since this leads to the largest reduction in the marginal (log-)variances (i.e., to a small posterior \u201cuncertainty\u201d).13 In contrast, ITL selects the point with the largest prior marginal variance (cf. Appendix C.1), point 10, since this leads to the largest reduction in entropy.14 ", "page_idx": 38}, {"type": "text", "text": "Similarity to VTL Observe that the following decision rule is equivalent to VTL: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{tr}\\,\\operatorname{Var}[\\pmb{f}_{A}\\mid\\mathcal{D}_{n-1}]-\\mathrm{tr}\\,\\operatorname{Var}[\\pmb{f}_{A}\\mid\\mathcal{D}_{n-1},y_{x}].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By Lemma C.38, for any $x\\in S$ , this objective value can be tightly lower- and upper-bounded (up to constant-factors) by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x^{\\prime}\\in A}\\log\\biggl(\\frac{\\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n-1}]}{\\mathrm{Var}[f_{x^{\\prime}}\\ |\\ \\mathcal{D}_{n-1},y_{x}]}\\biggr)}\\\\ &{\\displaystyle=2\\sum_{x^{\\prime}\\in A}\\mathrm{I}(f_{x^{\\prime}};y_{x}\\ |\\ \\mathcal{D}_{n-1})}\\\\ &{\\displaystyle\\stackrel{(i)}{=}\\displaystyle\\sum_{x^{\\prime}\\in A}\\log\\biggl(1-\\mathrm{Cor}[f_{x^{\\prime}},y_{x}\\ |\\ \\mathcal{D}_{n-1}]^{2}\\biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $(i)$ is detailed in example 8.5.1 of Cover (1999). Thus, VTL and MM-ITL are closely related. ", "page_idx": 38}, {"type": "text", "text": "Experiments In our experiments with Gaussian processes from Figures 2 and 6, we observe that MM-ITL performs similarly to VTL and CTL. ", "page_idx": 38}, {"type": "text", "text": "Convergence of uncertainty We derive a convergence guarantee for MM-ITL which is analogous to the guarantees for ITL from Theorem C.12 and for VTL from Theorem C.13. We will assume for simplicity that $\\Gamma_{n}$ is monotonically decreasing in $n$ (i.e., $\\alpha_{n}\\leq1$ ). ", "page_idx": 38}, {"type": "text", "text": "Theorem D.1 (Convergence of uncertainty reduction of MM-ITL). Assume that Assumptions B.1 and B.2 are satisfied. Then for any $n\\geq1$ , $i f\\Gamma_{0}\\geq\\cdot\\cdot\\geq\\Gamma_{n-1}$ and the sequence $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ is generated by MM-ITL, then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Gamma_{n-1}\\leq\\frac{1}{n}\\sum_{{\\pmb{x}}^{\\prime}\\in{\\cal A}}\\gamma_{n}({\\{{\\pmb{x}}^{\\prime}\\}};{\\cal S}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Gamma_{n-1}=\\frac{1}{n}\\sum_{i=0}^{n-1}\\Gamma_{n-1}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left|\\bar{\\mathbf{U}}\\right|}{2}\\frac{\\left|\\bar{\\mathbf{U}}\\right|}{n}\\overline{{\\mathbf{U}}}_{n}^{-1}\\mathbf{\\hat{P}}_{n},}\\\\ &{=\\frac{1}{n}\\overline{{\\frac{1}{n}\\frac{1}{\\omega^{2}+\\mathbf{U}}{n^{3}}}}\\sum_{\\ell=1}^{1}(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\left|\\mathcal{D}_{n}\\right|)}\\\\ &{\\overset{,,}{\\overline{{\\eta}}}\\equiv\\frac{1}{n^{3}}\\sum_{i=0}^{1}\\sum_{\\ell=1}^{3}\\left|(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\left|\\mathcal{D}_{n}\\right|)}\\\\ &{\\overset{,,}{\\overline{{\\eta}}}\\equiv\\frac{1}{n^{3}}\\sum_{i=0}^{1}\\sum_{\\ell=1}^{3}\\left|(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell-1}\\left|\\mathcal{D}_{n}\\right|)}\\\\ &{\\overset{,,}{\\overline{{\\eta}}}\\equiv\\frac{1}{n}\\sum_{i=0}^{1}\\sum_{\\ell=1}^{3}\\left|(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell-1}\\left|\\mathcal{R}_{n}\\right|)}\\\\ &{\\overset{,,}{\\overline{{\\eta}}}\\equiv\\frac{1}{n^{3}}\\sum_{i=1}^{3}(f_{i}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell-1})}\\\\ &{\\le\\frac{1}{n}\\sum_{\\ell=1}^{1}\\frac{\\sum_{i=0}^{n}1}{n^{3}\\sum_{i=0}^{1}\\sum_{j=1}^{N}\\left(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\right)}}\\\\ &{=\\frac{1}{n}\\sum_{\\ell=1}^{3}\\gamma_{\\ell}\\left(f_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell}^{\\top}\\mathbf{\\hat{p}}_{\\ell} \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $(i)$ follows by assumption; $(i i)$ follows from the MM-ITL decision rule; $(i i i)$ uses that the posterior variance of Gaussians is independent of the realization and only depends on the location of observations; and $(i v)$ uses the chain rule of mutual information. The remainder of the proof is analogous to the proof of Theorem C.12 (cf. Appendix C.5). \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Noting that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{I}(f_{x^{\\prime}};y_{x}\\mid\\mathcal{D}_{n-1})\\leq\\sum_{x^{\\prime}\\in\\mathcal{A}}\\mathrm{I}(f_{x^{\\prime}};y_{x}\\mid\\mathcal{D}_{n-1})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for any $n\\,\\geq\\,1,\\,x\\,\\in\\,{\\mathcal{X}}$ , and $x^{\\prime}\\in\\mathcal{A}$ , Theorem 3.2 can be readily rederived for MM-ITL (cf. Lemmas C.14 and C.18). Hence, the posterior marginal variances of MM-ITL can be bounded uniformly in terms of $\\Gamma_{n}$ analogously to ITL. ", "page_idx": 39}, {"type": "text", "text": "D.4 Correlation-based Transductive Learning ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We will briefly look at the CTL (Correlation-based $\\mathbf{\\nabla}T\\mathbf{\\Sigma}$ ) decision rule ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in\\cal{S}}{\\arg\\operatorname*{max}}\\sum_{\\pmb{x^{\\prime}}\\in\\mathcal{A}}\\mathrm{Cor}[f_{\\pmb{x}},f_{\\pmb{x^{\\prime}}}\\mid\\mathcal{D}_{n-1}]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which permits no interpretation under principle $\\left(\\dagger\\right)$ . However, if all correlations are non-negative (such as for the standard Gaussian and Mat\u00e9rn kernels), CTL is closely related to ITL, VTL, and MM-ITL (cf. Equations (35) and (39)). In this case, if $|{\\mathcal{A}}|=1$ , then all decision rules coincide. ", "page_idx": 39}, {"type": "text", "text": "If, on the other hand, correlations may be negative then there is a crucial difference between CTL and the decision rules motivated from principle (\u2020). Namely, decision rules following $\\left(\\dagger\\right)$ exhibit a preference for points with high absolute correlation to prediction targets as opposed to CTL which prefers points with high positive correlation. This stems from the intuitive fact that points with a strong negative correlation are equally informative as points with a strong positive correlation. Nevertheless, we observe in our experiments that (even for a linear kernel which does not ensure non-negative correlations) points selected by ITL and VTL are typically positively correlated with prediction targets. ", "page_idx": 39}, {"type": "text", "text": "D.5 Summary ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We have seen that ITL, VTL, and MM-ITL can be seen as different interpretations of the same fundamental principle $\\left(\\dagger\\right)$ , with the approximations CTL. If $|\\mathcal{A}|=1$ and correlations are non-negative, then all four decision rules are equivalent. CTL prefers points with high positive ", "page_idx": 39}, {"type": "text", "text": "correlation whereas the other decision rules prefer points with high absolute correlation. ITL is the only decision rule that takes into account the mutual dependence between points in $\\boldsymbol{\\mathcal{A}}$ , and VTL and MM-ITL differ only in their weighting of the posterior marginal variances of points in $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 40}, {"type": "text", "text": "E Stochastic Target Spaces ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "When the target space $\\boldsymbol{\\mathcal{A}}$ is large, it may be computationally infeasible to compute the exact objective.   \nA natural approach to address this issue is to approximate the target space by a smaller set of size $K$ . ", "page_idx": 40}, {"type": "text", "text": "Discretizing the target space One possibility is to discretize the target space $\\boldsymbol{\\mathcal{A}}$ . Compact target spaces can be addressed, e.g., via discretization arguments which are common in the Bayesian optimization literature (see, e.g., appendix C.1 of Srinivas et al. (2009)). That is, if the target space can be covered approximately using a finite (possibly large) set of points, the guarantees of Theorem 3.2 extend directly. This, however, can be impractical when the required size of discretization for sufficiently small approximation error is large. In the following, we briefly discuss a natural alternative approach based on sampling points from $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 40}, {"type": "text", "text": "Target distributions Let ${\\mathcal{A}}\\subseteq{\\mathcal{X}}$ be a (possibly continuous) target space, and let $\\mathcal{P}_{\\!A}$ be a probability distribution supported on $\\boldsymbol{\\mathcal{A}}$ . In iteration $n$ , a subset $A_{n}$ of $K$ points is sampled independently from $\\boldsymbol{\\mathcal{A}}$ according to the distribution $\\mathcal{P}_{\\!A}$ and the objective is computed on this subset. Formally, this amounts to a single-sample Monte Carlo approximation of ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}\\in\\arg\\operatorname*{max}_{\\pmb{x}\\in S}\\mathbb{E}_{\\pmb{A}}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{P}_{\\pmb{A}}\\big[\\mathrm{I}(\\pmb{f}_{\\pmb{A}};\\pmb{y}_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})\\big].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The convergence guarantees from Appendix $\\mathrm{C}$ can be generalized to the setting of stochastic target spaces by estimating how often points \u201cnear\u201d a specified prediction target $\\pmb{x}\\in A$ are sampled. ", "page_idx": 40}, {"type": "text", "text": "Definition E.1 ( $\\chi$ -ball at $\\textbf{\\em x}$ ). Given $\\pmb{x}\\in A$ and any $\\gamma\\geq0$ , we call the set ", "page_idx": 40}, {"type": "equation", "text": "$$\nB_{\\gamma}({\\pmb x})\\,{\\stackrel{\\mathrm{def}}{=}}\\{{\\pmb x}^{\\prime}\\in{\\mathcal{X}}\\mid\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|\\leq\\gamma\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "the $\\gamma$ -ball at $\\textbf{\\em x}$ . Further, we call $\\mathcal{P}_{A}(B_{\\gamma}({\\pmb x}))$ the weight of that ball. ", "page_idx": 40}, {"type": "text", "text": "Proposition E.2 (sketch). Given any $n\\geq1,K\\geq1,\\gamma>0,$ , and $\\pmb{x}\\in A$ , suppose that $B_{\\gamma}({\\pmb x})$ has weight $p>0$ . Assume that the ITL objective is $L_{I}$ -Lipschitz continuous. Then, with probability at least $1-\\exp(-(1-p)n/(8K))$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}(\\pmb{x})\\lesssim\\eta_{S}^{2}(\\pmb{x})+C L_{I}\\gamma\\frac{\\gamma_{k(n)}}{\\sqrt{k(n)}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $k(n)\\,{\\stackrel{\\mathrm{def}}{=}}\\,K p n/2$ . ", "page_idx": 40}, {"type": "text", "text": "Proof sketch. Let $Y_{i}\\sim\\operatorname{Binom}(K,p)$ denote the random variable counting the number of occurrences of a point from $B_{\\gamma}({\\pmb x})$ in $A_{i}$ . Moreover, we write $X_{i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbb{1}\\{B_{\\gamma}(\\pmb{x})\\cap A_{i}\\ {\\stackrel{\\smile}{=}}\\ \\emptyset\\}$ . Note that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\nu\\stackrel{\\mathrm{def}}{=}\\mathbb{E}X_{i}=\\mathbb{P}(B_{\\gamma}(\\pmb{x})\\cap A_{i}\\neq\\emptyset)=1-\\mathbb{P}(Y_{i}=0)=1-(1-p)^{K}\\approx K p\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the approximation stems from a first-order truncation of the Bernoulli series. Let $X\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\sum_{i=1}^{n}{\\dot{X}}_{i}^{\\phantom{i}}$ with $\\mathbb{E}X=n\\nu\\approx K p n$ . ", "page_idx": 40}, {"type": "text", "text": "Using the assumed Lipschitz-continuity of the objective, we know that $\\mathrm{I}(f_{A^{\\prime}};y_{x}\\mid\\mathcal{D}_{n-1})\\le L_{I}\\gamma\\mathrm{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n-1})$ where $A^{\\prime}\\,{\\overset{\\underset{\\mathrm{def}}{}}{=}}(A\\setminus\\{\\mathbf{x}_{\\gamma}\\})\\cup\\{\\mathbf{x}\\}$ and $x_{\\gamma}$ is the point from the $\\gamma$ -ball at $\\textbf{\\em x}$ . The bound then follows analogously to Theorem 3.2. ", "page_idx": 40}, {"type": "text", "text": "Finally, by Chernoff\u2019s bound, at least $K p n/2$ iterations contain a point from $B_{\\gamma}({\\pmb x})$ with probability at least $1-\\exp(-K p n/8)$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "This strategy can also be used to generalize the VTL, CTL, and MM-ITL objectives to stochastic target spaces. ", "page_idx": 40}, {"type": "text", "text": "F Closed-form Decision Rules ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Below, we list the closed-form expressions for the ITL and VTL objectives. In the following, $k_{n}$ denotes the kernel conditional on $\\mathcal{D}_{n}$ . ", "page_idx": 40}, {"type": "text", "text": "ITL ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{I}(f_{A};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})=\\frac{1}{2}\\log\\biggl(\\frac{\\mathrm{Var}[y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1}]}{\\mathrm{Var}[y_{\\pmb{x}}\\mid f_{A},\\mathcal{D}_{n-1}]}\\biggr)}}\\\\ &{}&{=\\frac{1}{2}\\log\\biggl(\\frac{k_{n-1}(\\pmb{x},\\pmb{x})+\\rho^{2}}{\\hat{k}_{n-1}(\\pmb{x},\\pmb{x})+\\rho^{2}}\\biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\hat{k}_{n}({\\pmb x},{\\pmb x})=k_{n}({\\pmb x},{\\pmb x})-k_{n}({\\pmb x},A){\\pmb K}_{n}({\\pmb A},A)^{-1}k_{n}({\\pmb A},{\\pmb x}).$ ", "page_idx": 41}, {"type": "text", "text": "VTL ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname{tr}\\operatorname{Var}[f_{A}\\mid\\mathcal{D}_{n-1},y_{x}]=\\sum_{x^{\\prime}\\in A}\\biggl(k_{n-1}(x^{\\prime},x^{\\prime})-\\frac{k_{n-1}(x,x^{\\prime})^{2}}{k_{n-1}(x,x)+\\rho^{2}}\\biggr).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "G Computational Complexity ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Evaluating the acquisition function of ITL in round $n$ requires computing for each $\\pmb{x}\\in S$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{I}(f_{A};y_{x}\\mid\\mathcal{D}_{n})}\\\\ &{~=\\cfrac{1}{2}\\,\\log\\bigg(\\cfrac{\\big|\\mathrm{Var}[f_{A}\\mid\\mathcal{D}_{n}]\\big|}{\\big|\\mathrm{Var}[f_{A}\\mid y_{x},\\mathcal{D}_{n}]\\big|}\\bigg)}\\\\ &{~=\\cfrac{1}{2}\\,\\log\\bigg(\\cfrac{\\mathrm{Var}[y_{x}\\mid\\mathcal{D}_{n}]}{\\mathrm{Var}[y_{x}\\mid f_{A},\\mathcal{D}_{n}]}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let $|S|=m$ and $|{\\mathcal{A}}|=k$ . Then, the forward method has complexity $O(m\\cdot k^{3})$ . For the backward method, observe that the variances are scalar and the covariance matrix $\\mathrm{Var}[\\dot{f}_{A}\\mid D_{n}]$ only has to be inverted once for all points $\\textbf{\\em x}$ . Thus, the backward method has complexity $O(k^{3}+m)$ . ", "page_idx": 41}, {"type": "text", "text": "When the size $m$ of $\\boldsymbol{S}$ is relatively small (and hence, all points in $\\boldsymbol{S}$ can be considered during each iteration of the algorithm), GP inference corresponds simply to computing conditional distributions of a multivariate Gaussian. The performance can therefore be improved by keeping track of the full posterior distribution over $f_{S}$ of size $O\\!\\left(m^{2}\\right)$ and conditioning on the latest observation during each iteration of the algorithm. In this case, after each observation the posterior can be updated at a cost of $O\\big(m^{2}\\big)$ which does not grow with the time $n$ , unlike classical GP inference. ", "page_idx": 41}, {"type": "text", "text": "Overall, when $m$ is small, the computational complexity of ITL is $O\\big(k^{3}+m^{2}\\big)$ . When $m$ is large (or possibly infinite) and a subset of $\\tilde{m}$ points is considered in a given iteration, the computational complexity of ITL is $O\\big(k^{3}+\\tilde{m}\\cdot n^{3}\\big)$ , neglecting the complexity of selecting the $\\tilde{m}$ candidate points. In the latter case, the computational cost of ITL is dominated by the cost of GP inference. ", "page_idx": 41}, {"type": "text", "text": "Khanna et al. (2017) discuss distributed and stochastic approximations of greedy algorithms to (weakly) submodular problems that are also applicable to ITL. ", "page_idx": 41}, {"type": "text", "text": "H Additional GP Experiments & Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We use homoscedastic Gaussian noise with standard deviation $\\rho\\:=\\:0.1$ and a discretization of $\\mathcal{X}=[-3,3]^{2}$ of size 2 500. Uncertainty bands correspond to one standard error over 10 random seeds. ", "page_idx": 41}, {"type": "text", "text": "Additional experiments Figure 6 includes the following additional experiments: ", "page_idx": 41}, {"type": "text", "text": "1. Extrapolation Setting ${\\mathcal{A}}\\cap S=\\emptyset,$ ): Right experiment from Figure 2 under the Gaussian kernel. ITL has a similar advantage as in the setting shown in Figure 3. 2. Heteroscedastic Noise: Left experiment from Figure 2 under the Gaussian kernel with heteroscedastic Gaussian noise ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\rho(\\pmb{x})={\\binom{1}{0.1}}\\begin{array}{l}{\\mathrm{if}\\;\\pmb{x}\\in[-\\frac{1}{2},\\frac{1}{2}]^{2}}\\\\ {\\mathrm{otherwise}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If observation noise is heteroscedastic, in considering posterior rather than prior uncertainty, ITL avoids points with high aleatoric uncertainty, which accelerates learning. ", "page_idx": 41}, {"type": "image", "img_path": "tZtepJBtHg/tmp/6b51f41e4d60566c38853181fce98ac2f2f29a105af616165f548d20b350e49c.jpg", "img_caption": ["Figure 6: Additional GP experiments "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "3. Effect of Smoothness: Experiment from Figure 3 under the Laplace kernel. All algorithms except for US and RANDOM perform equally well. This validates our claims from Section 3.3: in the extreme non-smooth case of a Laplace kernel and ${\\mathcal{A}}\\subseteq S$ , points outside $\\boldsymbol{\\mathcal{A}}$ do not provide any additional information, and ITL and \u201clocal\u201d UNSA coincide.   \n4. Sparser Target: Experiment from Figure 3 under the Gaussian kernel, but with domain extended to $\\mathcal{X}=[-10,10]^{2}$ . ", "page_idx": 42}, {"type": "text", "text": "Hyperparameters of TRUVAR As suggested by Bogunovic et al. (2016), we use $\\tilde{\\eta}_{(1)}^{2}=1$ , $r=0.1$ , and $\\delta=0$ (even though the theory only holds for $\\delta>0$ ). The TRUVAR baseline only applies when ${\\mathcal{A}}\\subseteq S$ (cf. Section 6). ", "page_idx": 42}, {"type": "text", "text": "Smoothing to reduce numerical noise Applied running average with window 5 to entropy curves of Figures 2 and 6 to smoothen out numerical noise. ", "page_idx": 42}, {"type": "text", "text": "I Alternative Settings for Active Fine-Tuning ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In our main experiments, we consider the setting $\\mathcal{A}\\cap\\mathcal{S}=\\emptyset$ , i.e., the prediction targets cannot be used for fine-tuning since their labels are not known. This setting is particularly relevant for practical applications where the model is fine-tuned dynamically at test time to each prediction target (or a small set of prediction target). Put differently, in this \u201ctransductive\u201d setting, extrapolation to new prediction targets happens at test-time with knowledge of the prediction target(s). This is in contrast to a more traditional \u201cinductive\u201d setting, where extrapolation happens at train-time without knowledge of the concrete prediction targets, but under the assumption of samples from (or knowledge of) the target distribution. In the following, we briefly survey two settings motivated from an \u201cinductive\u201d perspective. ", "page_idx": 42}, {"type": "text", "text": "I.1 Prediction Targets are Contained in Sample Space: ${\\mathcal{A}}\\subseteq S$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "If labels can be obtained cheaply, one can also fine-tune on the prediction targets directly, i.e., ${\\mathcal{A}}\\subseteq S$ . Note, however, that the set $\\boldsymbol{\\mathcal{A}}$ is still assumed to be small (e.g., $\\vert\\mathcal{A}\\vert=100$ in the CIFAR-100 experiment). We perform an experiment in this setting and report the results in Figure 7. The experiment shows that \u2014 similarly to the GP experiment from Figure 2 \u2014 there can be additional value in fine-tuning the model on relevant data selected from $\\boldsymbol{S}$ beyond simply fine-tuning the model on $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 42}, {"type": "image", "img_path": "tZtepJBtHg/tmp/c4a9b20dfb033c38074bc7e5c4d9814332cc736dcb084e62c05a427d5abbf881.jpg", "img_caption": ["Figure 7: Evaluation of CIFAR-100 experiment in the setting ${\\mathcal{A}}\\subseteq S$ , i.e., one can also sample from the 100 prediction targets $\\boldsymbol{\\mathcal{A}}$ . The solid black line denotes the performance of the model fine-tuned on all of $\\boldsymbol{\\mathcal{A}}$ . This experiment shows that there is additional value in fine-tuning the model on relevant data from $\\boldsymbol{S}$ beyond simply fine-tuning the model on $\\boldsymbol{\\mathcal{A}}$ . The baselines are summarized in Appendix J.5 "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "I.2 Active Domain Adaptation ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Active DA (Rai et al., 2010; Saha et al., 2011; Berlind & Urner, 2015) studies the problem of selecting the most informative samples from a (large) target domain $\\boldsymbol{\\mathcal{A}}$ , given a model trained on a source domain $\\boldsymbol{S}$ . This problem can be cast as an instance of transductive active learning with target space $\\boldsymbol{\\mathcal{A}}$ and sample space $S^{\\prime}=S\\cup A$ where the model is already conditioned on all of $\\boldsymbol{S}$ . This is slightly different from the setting considered in Section 4 where $\\boldsymbol{\\mathcal{A}}$ is small and not necessarily part of the sample space. We hypothesize that ITL behaves similarly to recent work on active DA (Su et al., 2020; Prabhu et al., 2021; $\\mathrm{Fu}$ et al., 2021): querying informative and diverse samples from $\\boldsymbol{\\mathcal{A}}$ that are dissimilar to $\\boldsymbol{S}$ . Evaluating ITL and VTL empirically in this setting is a promising direction for future work. ", "page_idx": 43}, {"type": "text", "text": "J Additional NN Experiments & Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We outline the active fine-tuning of NNs in Algorithm 1. ", "page_idx": 43}, {"type": "table", "img_path": "tZtepJBtHg/tmp/7be2bc8cd631282010504ff7301d143bb7e701f3b07d0f780d1da5288a08b172.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "In Appendix J.1, we detail metrics and hyperparameters. We describe in Appendices J.2 and J.3 how to compute the (initial) conditional kernel matrix $\\kappa$ , and in Appendix J.4 how to update this matrix $\\kappa$ to obtain conditional embeddings for batch selection. ", "page_idx": 43}, {"type": "text", "text": "In Appendix J.5, we show that ITL and CTL significantly outperform a wide selection of commonly used heuristics. In Appendices J.6 and J.7, we conduct additional experiments and ablations. ", "page_idx": 43}, {"type": "text", "text": "Table 1: Hyperparameter summary of NN experiments. $(^{*})$ we train until convergence on oracle validation accuracy. ", "page_idx": 44}, {"type": "table", "img_path": "tZtepJBtHg/tmp/d468a9f24dfdd871c523fc3b3da548e5387c00f7e392ebec66fd79a0669b8f71.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "H\u00fcbotter et al. (2024) discusses additional motivation and related work that has previously studied active fine-tuning, but which has largely focused on the training algorithm rather than data selection. ", "page_idx": 44}, {"type": "text", "text": "J.1 Experiment Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We evaluate the accuracy with respect to $\\mathcal{P}_{\\!A}$ using a Monte Carlo approximation with out-of-sample data: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname{accuracy}(\\widehat{\\pmb{\\theta}})\\approx\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{P}_{\\pmb{A}}}\\mathbb{1}\\{y=\\arg\\operatorname*{max}_{i}f_{i}(\\pmb{x};\\widehat{\\pmb{\\theta}})\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We provide an overview of the hyperparameters used in our NN experiments in Table 1. The effect of noise standard deviation $\\rho$ is small for all tested $\\rho\\,\\in\\,[1,100]$ (cf. ablation study in Table 2).15 $M$ denotes the size of the sample $A\\sim\\mathcal{P}_{\\!\\mathscr{A}}$ . In each iteration, we select the target space $A\\leftarrow A^{\\prime}$ as a random subset of $m$ points from $A$ .16 We provide an ablation over $m$ in Appendix J.6. ", "page_idx": 44}, {"type": "text", "text": "During each iteration, we select the batch $B$ according to the decision rule from a random sample from $\\mathcal{P}_{S}$ of size $k$ .17 ", "page_idx": 44}, {"type": "text", "text": "Since we train the MNIST model from scratch, we train from random initialization until convergence on oracle validation accuracy.18 We do this to stabilize the learning curves, and provide the least biased (due to the training algorithm) results. For CIFAR-100, we train for 5 epochs (starting from the previous iterations\u2019 model) which we found to be sufficient to obtain good performance. ", "page_idx": 44}, {"type": "text", "text": "We use the ADAM optimizer (Kingma & Ba, 2014). In our CIFAR-100 experiments, we use a pre-trained EfficientNet-B0 (Tan & Le, 2019), and fine-tune the final and penultimate layers. We freeze earlier layers to prevent overfitting to the \u201cfew-shot\u201d training data. ", "page_idx": 44}, {"type": "text", "text": "To prevent numerical inaccuracies when computing the ITL objective, we optimize ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname{I}(y_{A};y_{x}\\mid{\\mathcal{D}}_{n-1})={\\frac{1}{2}}\\log\\left({\\frac{\\operatorname{Var}[y_{x}\\mid{\\mathcal{D}}_{n-1}]}{\\operatorname{Var}[y_{x}\\mid y_{A},{\\mathcal{D}}_{n-1}]}}\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "instead of Equation (43), which amounts to adding $\\rho^{2}$ to the diagonal of the covariance matrix before inversion. This appears to improve numerical stability, especially when using gradient embeddings.19 ", "page_idx": 44}, {"type": "text", "text": "In our experiments, we use last-layer neural tangent embeddings20and $\\Sigma=I$ to evaluate ITL and VTL, and select inputs for labeling and training $f$ . Notably, we use this linear Gaussian approximation of $f$ only to guide the active data selection and not for inference. ", "page_idx": 45}, {"type": "text", "text": "J.2 Embeddings and Kernels ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Using a neural network to parameterize $f$ , we evaluate the canonical approximations of $f$ by a stochastic process in the following. ", "page_idx": 45}, {"type": "text", "text": "An embedding $\\phi(x)$ is a latent representation of an input $\\textbf{\\em x}$ . Collecting the embeddings as rows in the design matrix $\\Phi$ of a set of inputs $X$ , one can approximate the network by the linear function $f_{X}=\\Phi\\beta$ with weights $\\beta$ . Approximating the weights by $\\beta\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$ implies that $f_{X}\\sim\\mathcal{N}(\\Phi\\mu,\\Phi\\Sigma\\Phi^{\\top})$ . The covariance matrix $\\mathbf{\\cal{K}}_{X X}=\\boldsymbol{\\Phi}\\boldsymbol{\\Sigma}\\boldsymbol{\\Phi}^{\\top}$ can be succinctly represented in terms of its associated kernel $k(\\pmb{x},\\pmb{x}^{\\prime})=\\phi(\\pmb{x})^{\\top}\\pmb{\\Sigma}\\phi(\\pmb{x}^{\\prime})$ . Here, ", "page_idx": 45}, {"type": "text", "text": "\u2022 $\\phi(x)$ is the latent representation of $\\textbf{\\em x}$ , and \u2022 $\\Sigma$ captures the dependencies in the latent space. ", "page_idx": 45}, {"type": "text", "text": "While any choice of embedding $\\phi$ is possible, the following are common choices: ", "page_idx": 45}, {"type": "text", "text": ". Last-Layer: A common choice for $\\phi(x)$ is the representation of $\\textbf{\\em x}$ from the penultimate layer of the neural network (Holzm\u00fcller et al., 2023). Interpreting the early layers as a feature encoder, this uses the low-dimensional feature map akin to random feature methods (Rahimi & Recht, 2007). ", "page_idx": 45}, {"type": "text", "text": "2. Output Gradients (eNTK): Another common choice is $\\phi(\\boldsymbol{x})\\,=\\,\\nabla_{\\theta}\\,f(\\boldsymbol{x};\\theta)$ where $\\pmb{\\theta}$ are the network parameters (Holzm\u00fcller et al., 2023). Its associated kernel is known as the empirical neural tangent kernel (eNTK) and the posterior mean of this GP approximates ultra-wide NNs trained with gradient descent (Jacot et al., 2018; Arora et al., 2019; Lee et al., 2019; Khan et al., 2019; He et al., 2020; Malladi et al., 2023). Kassraie & Krause (2022) derive bounds of $\\gamma_{n}$ under this kernel. If $\\pmb{\\theta}$ is restricted to the weights of the final linear layer, then this embedding is simply the last-layer embedding. ", "page_idx": 45}, {"type": "text", "text": "3. Loss Gradients: Another possible choice is ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\phi(\\pmb{x})=\\nabla_{\\pmb{\\theta}}\\,\\ell(\\pmb{f}(\\pmb{x};\\pmb{\\theta}),\\hat{y}(\\pmb{x}))|_{\\pmb{\\theta=\\widehat{\\theta}}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\ell$ is a loss function, ${\\hat{y}}(\\pmb{x})$ is the predicted label, and $\\widehat{\\pmb{\\theta}}$ are the current parameter estimates (Ash et al., 2020). ", "page_idx": 45}, {"type": "text", "text": "4. Outputs (eNNGP): Another possible choice is $\\phi({\\pmb x})=f({\\pmb x})$ , i.e., the output of the network. Its associated kernel is known as the empirical neural network Gaussian process (eNNGP) kernel (Lee et al., 2018).   \n5. Predictive (Kirsch, 2023): Given a Bayesian neural network (Blundell et al., 2015) or probabilistic (deep) ensemble (Lakshminarayanan et al., 2017), which induce samples $\\pmb{\\theta}_{1},\\dots,\\pmb{\\theta}_{K}\\sim p(\\pmb{\\theta})$ from the distribution over network parameters, one can approximate the predictive covariance $k(\\pmb{x},\\pmb{x}^{\\prime})=\\mathrm{Cov}_{\\pmb{\\theta}}[f(\\pmb{x};\\pmb{\\theta}),f(\\pmb{x}^{\\prime};\\pmb{\\dot{\\theta}})]$ . This kernel measures proximity in the prediction space rather than parameter space and as such does not require gradient information. Tdheef corresponding feKature map is $\\begin{array}{r}{\\phi(\\pmb{x})=\\frac{1}{\\sqrt{K}}[\\bar{f}(\\pmb{x};\\pmb{\\theta}_{1})~\\cdot\\cdot\\cdot~\\bar{f}(\\pmb{x};\\pmb{\\theta}_{K})]^{\\intercal}}\\end{array}$ where $\\begin{array}{r}{\\bar{f}(\\pmb{x};\\pmb{\\theta}_{k})\\stackrel{\\mathrm{def}}{=}f(\\pmb{x};\\pmb{\\dot{\\theta_{k}}})-\\frac{1}{K}\\sum_{l=1}^{K}f(\\pmb{x};\\pmb{\\dot{\\theta}_{l}})}\\end{array}$ . ", "page_idx": 45}, {"type": "text", "text": "In the additional experiments from this appendix we use last-layer embeddings unless noted otherwise. We compare the performance of last-layer and the loss gradient embedding ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\phi(\\pmb{x})=\\pmb{\\nabla_{\\pmb{\\theta}^{\\prime}}}\\,\\ell_{\\mathrm{CE}}(\\pmb{f}(\\pmb{x};\\pmb{\\theta}),\\hat{y}(\\pmb{x}))|_{\\pmb{\\theta=\\widehat{\\theta}}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\theta^{\\prime}$ are the parameters of the final output layer, $\\widehat{\\pmb{\\theta}}$ are the current parameter estimates, $\\hat{y}(\\pmb{x})=\\arg\\operatorname*{max}_{i}\\hat{f}_{i}(\\pmb{x};\\hat{\\pmb{\\theta}})$ are the associated predicted la bels, and $\\ell_{\\mathrm{CE}}$ denotes the cross-entropy loss. This gradient emb edding captures the potential update direction upon observing a new point (Ash et al., 2020). Moreover, Ash et al. (2020) show that for most neural networks, the norm of these gradient embeddings are a conservative lower bound to the norm assumed by taking any other proxy label ${\\hat{y}}(\\pmb{x})$ . In Figure 8, we observe only negligible differences in performance between this and the last-layer embedding. ", "page_idx": 45}, {"type": "image", "img_path": "tZtepJBtHg/tmp/6f7a193ac3d2b1088fdd9ae1d084c89de583894f2570c1a339651f08278311e6.jpg", "img_caption": ["Figure 8: Comparison of loss gradient (\u201cG-\u201d) and last-layer embeddings (\u201cL-\u201d). "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "tZtepJBtHg/tmp/dc9af2d5a335e24a7f692a9a96ef4c298205cd646dd7e2866461133417250085.jpg", "img_caption": ["Figure 9: Uncertainty quantification (i.e., estimation of $\\Sigma$ ) via a Laplace approximation (LA, Daxberger et al. (2021)) over last-layer weights using a Kronecker factored log-likelihood Hessian approximation (Martens & Grosse, 2015) and the loss gradient embeddings from Equation (45). The results are shown for the MNIST experiment. We do not observe a performance improvement beyond the trivial approximation $\\Sigma=I$ . "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "J.3 Towards Uncertainty Quantification in Latent Space ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "A straightforward and common approximation of the uncertainty about NN weights is given by $\\Sigma=I$ , and we use this approximation throughout our experiments. ", "page_idx": 46}, {"type": "text", "text": "The poor performance of UNSA (cf. Appendix J.5) with this approximation suggests that with more sophisticated approximations, the performance of ITL, VTL, and CTL can be further improved. Further research is needed to study the effect of more sophisticated approximations of \u201cuncertainty\u201d in the latent space. For example, with parameter gradient embeddings, the latent space is the network parameter space where various approximations of $\\Sigma$ based on Laplace approximation (Daxberger et al., 2021; Antor\u00e1n et al., 2022), variational inference (Blundell et al., 2015), or Markov chain Monte Carlo (Maddox et al., 2019) have been studied. We also evaluate Laplace approximation (LA, Daxberger et al. (2021)) for estimating $\\Sigma$ but see no improvement (cf. Figure 9). Nevertheless, we believe that uncertainty quantification is a promising direction for future work, with the potential to improve performance of ITL and its variations substantially. ", "page_idx": 46}, {"type": "text", "text": "J.4 Batch Selection via Conditional Embeddings ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We will refer to the greedy decision rule from Equation (3) as BACE, short for Batch selection via Conditional Embeddings. BACE can be implemented efficiently using the Gaussian approximation of $f_{X}$ from Appendix J.2 by iteratively conditioning on the previously selected points $\\scriptstyle{\\mathbf{\\emx}}_{n,1:i-1}$ , and updating the kernel matrix $K_{X X}$ using the closed-form formula for the variance of conditional ", "page_idx": 46}, {"type": "image", "img_path": "tZtepJBtHg/tmp/98f7420244626c032069d65f8691ca9751c426a8ff5f0ff51ea1573f054d5f60.jpg", "img_caption": ["Figure 10: Advantage of batch selection via conditional embeddings over top- ${\\it b}$ selection in the CIFAR-100 experiment. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Gaussians: ", "page_idx": 47}, {"type": "equation", "text": "$$\nK_{X X}\\leftarrow K_{X X}-\\frac{1}{K_{x_{j}x_{j}}+\\rho^{2}}K_{X x_{j}}K_{x_{j}X}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $j$ denotes the index of the selected $\\pmb{x}_{n,i}$ within $X$ and $\\rho^{2}$ is the noise variance. Note that $K_{\\pmb{x}_{j}\\pmb{x}_{j}}$ is a scalar and $K_{X{\\pmb x}_{j}}$ is a row vector, and hence, this iterative update can be implemented efficiently. ", "page_idx": 47}, {"type": "text", "text": "We remark that Equation (3) is a natural extension of previous non-adaptive active learning methods, which typically maximize some notion of \u201cdistance\u201d between points in the batch, to the \u201cdirected\u201d setting (Ash et al., 2020; Zanette et al., 2021; Holzm\u00fcller et al., 2023; Pacchiano et al., 2024). BACE simultaneously maximizes \u201cdistance\u201d between points in a batch and minimizes \u201cdistance\u201d to points in $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 47}, {"type": "text", "text": "The sample efficiency of BACE $B_{n}$ , and therefore also the greedily constructed $B_{n}^{\\prime}$ (which gives a constant-factor approximation with respect to the objective), yields diverse batches by design. In Figure 10, we compare BACE to selecting the top- $\\cdot b$ points according to the decision rule (which does not yield diverse batches). We observe a significant improvement in accuracy and data retrieval when using BACE. We expect the gap between both approaches to widen further with larger batch sizes. ", "page_idx": 47}, {"type": "text", "text": "Computational complexity of BACE As derived in Appendix G, a single batch selection step of BACE has complexity ${\\dot{O}}{\\left(b(k^{3}+m^{2})\\right)}$ where $b$ is the size of the batch, $k=|{\\mathcal{A}}|$ is the size of the target space, and $m=|S|$ is the size of the candidate set. In the case of large $m$ , an alternative implementation whose runtime does not depend on $m$ is described in Appendix G. ", "page_idx": 47}, {"type": "text", "text": "J.5 Baselines ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In Figure 11, we compare against additional baselines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 Both TYPICLUST (Hacohen et al., 2022) and PROBCOVER (Yehuda et al., 2022) are recent methods to select points that \u201ccover\u201d the data distribution well. To maintain comparability between algorithms, we use the same embeddings as for ITL which are re-computed before every new batch selection. ITL significantly outperforms TYPICLUST & PROBCOVER, which only attempt to cover $\\boldsymbol{S}$ well without taking $\\boldsymbol{\\mathcal{A}}$ into account (i.e., are \u201cundirected\u201d). ", "page_idx": 47}, {"type": "text", "text": "\u2022 Mehta et al. (2022) introduced EIG for training neural classification models, which uses the same decision rule as ITL, but approximates the conditional entropy based on the networks\u2019 softmax output rather than using a GP approximation. We approximate the conditional entropy using a single gradient step of the hallucinated updates on the parameters of the final layer, as mentioned by Mehta et al. (2022). We observe that EIG is not competitive for batch-wise selection (CIFAR-100) since it does not encourage batch diversity. Moreover, we observe that EIG is orders of magnitude slower than ITL (since it has to compute $|S|\\cdot C$ individual gradient steps where $C$ is the number of classes). We note that since our datasets are balanced, the AEIG algorithm from Mehta et al. (2022) coincides with EIG. ", "page_idx": 47}, {"type": "image", "img_path": "tZtepJBtHg/tmp/a1036d27a2361b7560f1e32e2f3a1320a8fad6cd15f0d795925f1f0493440ae5.jpg", "img_caption": ["Figure 11: Comparison to baselines for the experiment of Figure 4. "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "Since, EIG does not have an open-source implementation, we implemented it ourselves following Mehta et al. (2022). For TYPICLUST & PROBCOVER, we use the author\u2019s implementation. In the figure, we show that ITL & VTL substantially outperform all baselines. ", "page_idx": 48}, {"type": "text", "text": "In the following, we briefly describe other commonly used \u201cundirected\u201d decision rules. ", "page_idx": 48}, {"type": "text", "text": "Denote the softmax distribution over labels $i$ at inputs $\\textbf{\\em x}$ by ", "page_idx": 48}, {"type": "equation", "text": "$$\np_{i}(\\pmb{x};\\pmb{\\widehat{\\theta}})\\propto\\exp(f_{i}(\\pmb{x};\\pmb{\\widehat{\\theta}})).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The following heuristics computed based on the softmax distribution aim to quantify the \u201cuncertainty\u201d about a particular input $\\textbf{\\em x}$ : ", "page_idx": 48}, {"type": "text", "text": "\u2022 MAXENTROPY (Settles & Craven, 2008): ", "page_idx": 48}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=\\underset{{\\pmb x}\\in{\\cal S}}{\\arg\\operatorname*{max}}\\,\\mathrm{H}[p({\\pmb x};\\widehat{\\pmb\\theta}_{n-1})].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "\u2022 MAXMARGIN (Scheffer et al., 2001; Settles & Craven, 2008): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{S}}p_{1}(\\pmb{x};\\widehat{\\pmb{\\theta}}_{n-1})-p_{2}(\\pmb{x};\\widehat{\\pmb{\\theta}}_{n-1})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $p_{1}$ and $p_{2}$ are the two largest class probabilities. ", "page_idx": 48}, {"type": "text", "text": "\u2022 LEASTCONFIDENCE (Lewis & Gale, 1994; Settles & Craven, 2008; Hendrycks & Gimpel, 2017; Tamkin et al., 2022): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathcal{S}}p_{1}(\\pmb{x};\\widehat{\\pmb{\\theta}}_{n-1})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $p_{1}$ is the largest class probability. ", "page_idx": 48}, {"type": "text", "text": "An alternative class of decision rules aims to select diverse batches by maximizing the distances between points. Embeddings $\\phi(x)$ induce the (Euclidean) embedding distance ", "page_idx": 48}, {"type": "equation", "text": "$$\nd_{\\phi}(\\pmb{x},\\pmb{x}^{\\prime})\\stackrel{\\mathrm{def}}{=}\\|\\phi(\\pmb{x})-\\phi(\\pmb{x}^{\\prime})\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Similarly, a kernel $k$ induces the kernel distance ", "page_idx": 48}, {"type": "equation", "text": "$$\nd_{k}({\\pmb x},{\\pmb x}^{\\prime})\\,{\\overset{\\mathrm{def}}{=}}\\,\\sqrt{k({\\pmb x},{\\pmb x})+k({\\pmb x}^{\\prime},{\\pmb x}^{\\prime})-2k({\\pmb x},{\\pmb x}^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It is straightforward to see that if $k(\\pmb{x},\\pmb{x}^{\\prime})=\\phi(\\pmb{x})^{\\top}\\phi(\\pmb{x}^{\\prime})$ , then embedding and kernel distances coincide, i.e., $d_{\\phi}(\\mathbf{x},\\mathbf{x}^{\\prime})=d_{k}(\\mathbf{x},\\mathbf{x}^{\\prime})$ . ", "page_idx": 48}, {"type": "text", "text": "\u2022 MAXDIST (Holzm\u00fcller et al., 2023; Yu & Kim, 2010; Sener & Savarese, 2017; Geifman & El-Yaniv, 2017) constructs the batch by choosing the point with the maximum distance to the nearest previously selected point: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in\\cal S}{\\arg\\operatorname*{max}}\\operatorname*{min}_{i<n}d(\\pmb{x},\\pmb{x}_{i})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "\u2022 Similarly, K-MEANS $^{++}$ (Holzm\u00fcller et al., 2023) selects the batch via $\\mathrm{K{-}M E A N S{+}}+$ seeding (Arthur et al., 2007; Ostrovsky et al., 2013). That is, the first centroid $x_{1}$ is chosen uniformly at random and the subsequent centroids are chosen with a probability proportional to the square of the distance to the nearest previously selected centroid: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\pmb{x}_{n}=\\pmb{x})\\propto\\operatorname*{min}_{i<n}d(\\pmb{x},\\pmb{x}_{i})^{2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "When using the loss gradient embeddings from Equation (45), this decision rule is known as BADGE (Ash et al., 2020). ", "page_idx": 49}, {"type": "text", "text": "Finally, we summarize common kernel-based decision rules. ", "page_idx": 49}, {"type": "text", "text": "\u2022 UNDIRECTED ITL chooses ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{n}=\\underset{\\pmb{x}\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{I}(\\pmb{f}_{S};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})}\\\\ {=\\underset{\\pmb{x}\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{I}(\\,f_{\\pmb{x}};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This can be shown to be equivalent to MAXDET (Holzm\u00fcller et al., 2023) which selects ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=\\underset{{\\pmb x}\\in{\\cal S}}{\\arg\\operatorname*{max}}\\left|K_{x}+\\sigma^{2}{\\pmb I}\\right|\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $K_{x}$ denotes the kernel matrix over ${\\pmb x}_{1:n-1}\\cup\\{{\\pmb x}\\}$ , conditioned on the prior observations Dn\u22121. ", "page_idx": 49}, {"type": "text", "text": "\u2022 UNSA (Lewis & Catlett, 1994) which with embeddings $\\phi_{n-1}$ after round $n-1$ corresponds to: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in\\mathcal{S}}{\\arg\\operatorname*{max}}\\,\\sigma_{n-1}^{2}(\\pmb{x})=\\underset{\\pmb{x}\\in\\mathcal{S}}{\\arg\\operatorname*{max}}\\left\\|\\phi_{n-1}(\\pmb{x})\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "With batch size $b=1$ , UNSA coincides with UNDIRECTED ITL. When evaluated with gradient embeddings, this acquisition function is similar to previously used \u201cembedding length\u201d or \u201cgradient length\u201d heuristics (Settles & Craven, 2008). ", "page_idx": 49}, {"type": "text", "text": "\u2022 UNDIRECTED VTL (Cohn, 1993) is the special case of VTL without specified prediction targets (i.e., ${\\mathcal{A}}=S$ ). In the literature, this decision rule is also known as BAIT (Holzm\u00fcller et al., 2023; Ash et al., 2021). ", "page_idx": 49}, {"type": "text", "text": "We compare to the abovementioned decision rules and summarize the results in Figure 12. We observe that most \u201cundirected\u201d decision rules perform worse (and often significantly so) than RANDOM. This is likely due to frequently selecting points from the support of $\\mathcal{P}_{S}$ which are not in the support of $\\mathcal{P}_{\\!A}$ since the points are \u201cadversarial examples\u201d that the model\u03b8 is not trained to perform well on. In the case of MNIST, the poor performance can also partially be attributed to the well-known \u201ccold-start problem\u201d (Gao et al., 2020). ", "page_idx": 49}, {"type": "text", "text": "In Figure 4, we also compare to the following \u201cdirected\u201d decision rules: ", "page_idx": 49}, {"type": "text", "text": "\u2022 COSINESIMILARITY (Settles & Craven, 2008) selects $x_{n}\\,=\\,\\arg\\operatorname*{max}_{\\mathbf{x}\\in S}\\angle_{\\phi_{n-1}}(\\mathbf{x},A)$ where ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\angle_{\\phi}(\\pmb{x},\\mathcal{A})\\overset{\\mathrm{def}}{=}\\frac{1}{|\\mathcal{A}|}\\sum_{\\pmb{x^{\\prime}}\\in\\mathcal{A}}\\frac{\\phi(\\pmb{x})^{\\top}\\phi(\\pmb{x^{\\prime}})}{\\|\\phi(\\pmb{x})\\|_{2}\\,\\|\\phi(\\pmb{x^{\\prime}})\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "\u2022 INFORMATIONDENSITY (Settles & Craven, 2008) is defined as the multiplicative combination of MAXENTROPY and COSINESIMILARITY: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\underset{\\pmb{x}\\in S}{\\arg\\operatorname*{max}}\\,\\mathrm{H}[p(\\pmb{x};\\widehat{\\pmb{\\theta}}_{n-1})]\\cdot\\left(\\mathcal{L}_{\\phi_{n-1}}(\\pmb{x},\\mathcal{A})\\right)^{\\beta}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\beta>0$ controls the relative importance of both terms. We set $\\beta=1$ in our experiments. ", "page_idx": 49}, {"type": "image", "img_path": "tZtepJBtHg/tmp/59b0eae397211650b91a85175eda31ff9d1a14f3bed68f87fff42a90d71971ac.jpg", "img_caption": ["Figure 12: Comparison of \u201cundirected\u201d baselines for the experiment of Figure 4. In the MNIST experiment, UNSA and UNDIRECTED ITL coincide, and we therefore only plot the latter. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "tZtepJBtHg/tmp/01fa16f2d60f1f2efc28d2f543efe10045ace794343bbae2ebb58eb296ecfb39.jpg", "img_caption": ["Figure 13: Imbalanced $\\mathcal{P}_{S}$ experiment. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "tZtepJBtHg/tmp/eb01cffbcedd3afef6aa30f9485b6557048686ec41d9b41833c39bab10600d1b.jpg", "img_caption": ["Figure 14: Imbalanced $A\\sim\\mathcal{P}_{A}$ experiment. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "tZtepJBtHg/tmp/ee551e9d2fc2c4d8b8c57d2ebee4e378beae60299c1390f252344d85c4480b43.jpg", "img_caption": ["Figure 15: Performance of VTL & choice of $k$ in the CIFAR-100 experiment. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "J.6 Additional experiments ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "We conduct the following additional experiments: ", "page_idx": 51}, {"type": "text", "text": "Imbalanced $\\mathcal{P}_{S}$ (Figure 13): We artificially remove $80\\%$ of the support of $\\mathcal{P}_{\\!A}$ from $\\mathcal{P}_{S}$ . For example, in case of MNIST, we remove $80\\%$ of the images with labels 3, 6, and 9 from $\\mathcal{P}_{S}$ . This makes the learning task more difficult, as $\\mathcal{P}_{\\!A}$ is less represented in $\\mathcal{P}_{S}$ , meaning that the \u201ctargets\u201d are more sparse. The trend of ITL outperforming CTL which outperforms RANDOM is even more pronounced in this setting. ", "page_idx": 51}, {"type": "text", "text": "2. Imbalanced $A\\sim\\mathcal{P}_{A}$ (Figure 14): We artificially remove $50\\%$ of part of the support of $\\mathcal{P}_{\\!A}$ while generating $A\\sim\\mathcal{P}_{A}$ to evaluate the robustness of ITL and CTL in presence of an imbalanced target space $\\boldsymbol{\\mathcal{A}}$ . Concretely, in case of MNIST, we remove $50\\%$ of the images with labels 3 and 6 from $A$ . In case of CIFAR-100, we remove $50\\%$ of the images with labels $\\{0,\\ldots,4\\}$ from $A$ . We still observe the same trends as in the other experiments.   \n3. VTL & choice of $k$ (Figure 15): We observe that VTL performs almost as well as ITL. Additionally, we evaluate the effect of the number of points $k$ at which the decision rule is evaluated. Not surprisingly, we observe that the performance of ITL, VTL, and CTL improves with larger $k$ .   \n4. Choice of $m$ (Figure 16): Next, we evaluate the choice of $m$ , i.e., the size of the target space $\\boldsymbol{\\mathcal{A}}$ relative to the number $M$ of candidate points $A\\sim\\mathcal{P}_{\\!A}$ . We write $p=m/M$ . We generally observe that a larger $p$ leads to better performance (with $p=1$ being the best choice). However, it appears that a smaller $p$ can be beneficial with respect to accuracy when a large number of batches are selected. We believe that this may be because a smaller $p$ improves the diversity between selected batches.   \n5. Choice of $M$ (Figure 17): Finally, we evaluate the choice of $M$ , i.e., the size of $A\\sim\\mathcal{P}_{A}$ . Not surprisingly, we observe that the performance of ITL improves with larger $M$ . ", "page_idx": 51}, {"type": "text", "text": "J.7 Ablation study of noise standard deviation $\\rho$ ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "In Table 2, we evaluate the CIFAR-100 experiment with different noise standard deviations $\\rho$ . We observe that the performance of batch selection via conditional embeddings drops (mostly for the less numerically stable gradient embeddings) if $\\rho$ is too small, since this leads to numerical inaccuracies when computing the conditional embeddings. Apart from this, the effect of $\\rho$ is negligible. ", "page_idx": 51}, {"type": "image", "img_path": "tZtepJBtHg/tmp/7626f81ce126b55b76f82f6beea56933ae23b13493c71706fb059a062e5943e9.jpg", "img_caption": ["Figure 16: Evaluation of the choice of $m$ relative to the size $M$ of $A\\sim\\mathcal{P}_{A}$ . Here, $p=m/M$ . "], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "tZtepJBtHg/tmp/2b0741f94bdd5360f6d5367343a17b5865db399cbce6c78bb0d85ddc3d15d762.jpg", "img_caption": ["Figure 17: Evaluation of the choice of $M$ , i.e., the size of $A\\sim\\mathcal{P}_{A}$ , in the CIFAR-100 experiment. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Table 2: Ablation study of noise standard deviation $\\rho$ in the CIFAR-100 experiment. We list the accuracy after 100 rounds per decision rule, with its standard error over 10 random seeds. \u201c(top- $\\cdot b$ )\u201d denotes variants where batches are selected by taking the top- $\\cdot b$ points according to the decision rule rather than using batch selection via conditional embeddings. Shown in bold are the best performing decision rules, and shown in italics are results due to numerical instability. ", "page_idx": 52}, {"type": "table", "img_path": "tZtepJBtHg/tmp/e23bbe6f93f93ba364908d6994483d72f9bddd94f6ab259e6ad9ad0cd719162f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "tZtepJBtHg/tmp/a9ea5215ac27e2ef7480d21f09decea32952b309f207651cac5f892731d19628.jpg", "img_caption": ["Figure 18: We perform the tasks of Figure 5 using Thompson sampling to evaluate the stochastic target space $\\mathcal{P}_{A n}$ . We additionally compare to GOOSE (cf. Appendix K.2.3) and ISE-BO (cf. Appendix K.2.4). "], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "K Additional Safe BO Experiments & Details ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In Appendix K.1, we discuss the use of stochastic target spaces in the safe BO setting. We provide a comprehensive overview of prior works in Appendix K.2 and an additional experiment highlighting that ITL, unlike SAFEOPT, is able to \u201cjump past local barriers\u201d in Appendix K.3. In Appendix K.4, we provide details on the experiments from Figure 5. ", "page_idx": 53}, {"type": "text", "text": "K.1 A More Exploitative Stochastic Target Space ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Alternatively to the target space $A_{n}$ which comprises all potentially optimal points, we evaluate the stochastic target space ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{P}_{A n}(\\cdot)=\\mathbb{P}\\underset{\\substack{\\mathbf{x}\\in\\mathcal{X}:g(\\mathbf{x})\\geq0}}{\\arg\\operatorname*{max}}\\ f(\\mathbf{x})=\\cdot\\mid\\mathcal{D}_{n}\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which effectively weights points in $A_{n}$ according to how likely they are to be the safe optimum, and is therefore more exploitative than the uniformly-weighted target space discussed so far. Samples from $\\mathcal{P}_{A_{n}}$ can be obtained efficiently via Thompson sampling (Thompson, 1933; Russo et al., 2018). Observe that $\\mathcal{P}_{A_{n}}$ is supported precisely on the set of potential maximizers $A_{n}$ . We provide a formal analysis of stochastic target spaces in Appendix E. Whether transductive active learning with $A_{n}$ or $\\mathcal{P}_{A_{n}}$ performs better is task-dependent, as we will see in the following. ", "page_idx": 53}, {"type": "text", "text": "Note that performing ITL with this target space is analogous to output-space entropy search (Wang & Jegelka, 2017). Samples from $\\mathcal{P}_{A_{n}}$ can be obtained via Thompson sampling (Thompson, 1933; Russo et al., 2018). That is, in iteration $n+1$ , we sample $K\\in\\mathbb N$ independent functions $f^{(j)}\\sim f\\mid D_{n}$ from the posterior distribution and select $K$ points $\\pmb{x}^{(1)},\\ldots,\\pmb{x}^{(\\check{K})}$ which are a safe maximum of $f^{(1)},\\ldots,^{\\dagger}\\!f^{(K)}$ , respectively. ", "page_idx": 53}, {"type": "text", "text": "Experiments In Figure 18, we contrast the performance of ITL with $\\mathcal{P}_{A_{n}}$ to the performance of ITL with the exact target space $A_{n}$ . We observe that their relative performance is instance dependent: in tasks that require more difficult expansion, ITL with $A_{n}$ converges faster, whereas in simpler tasks (such as the 2d experiment), ITL with $\\mathcal{P}_{A_{n}}$ converges faster. We compare against the GOOSE algorithm (Turchetta et al., 2019) which is a heuristic extension of SAFEOPT that explores more greedily in directions of (assumed) high reward (cf. Appendix K.2.3). GOOSE suffers from the same limitations as SAFEOPT, which were highlighted in Section 5, and additionally is limited by its heuristic approach to expansion which fails in the 1d task and safe controller tuning task. Analogously to our experiments with SAFEOPT, we also compare against ORACLE GOOSE which has oracle knowledge of the true Lipschitz constants. ", "page_idx": 53}, {"type": "text", "text": "The different behaviors of ITL with $A_{n}$ and $\\mathcal{P}_{A_{n}}$ , respectively, as well as SAFEOPT and GOOSE are illustrated in Figure 19. We observe that ITL with $A_{n}$ and SAFEOPT expand the safe set more \u201cuniformly\u201d since the set of potential maximizers encircles the true safe set.21 Intuitively, this is because the set of potential maximizers conservatively captures migh points might be safe and optimal. In contrast, ITL with $\\mathcal{P}_{A\\,n}$ and GOOSE focus exploration and expansion in those regions where the objective is likely to be high. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "image", "img_path": "tZtepJBtHg/tmp/46b8e2e9789f085d61dfc81b8465b3ec5bc8b7ccf7c6c5db31707e1602baf1e5.jpg", "img_caption": ["Figure 19: The first 100 samples of (A) ITL with $A_{n}$ , (B) SAFEOPT, (C) ORACLE SAFEOPT, (D) ITL with $\\mathcal{P}_{A_{n}}$ , (E) GOOSE, (F) ORACLE GOOSE. The white region denotes the pessimistic safe set ${\\cal S}_{100}$ , the light gray region denotes the true safe set $S^{\\star}$ (i.e., the \u201cisland\u201d), and the darker gray regions denotes unsafe points (i.e., the \u201cocean\u201d). "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "K.2 Detailed Comparison with Prior Works ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "The most widely used method for Safe BO is SAFEOPT (Sui et al., 2015; Berkenkamp et al., 2021) which keeps track of separate candidate sets for expansion and exploration and uses UNSA to pick one of the candidates in each round. Treating expansion and exploration separately, sampling is directed towards expansion in all directions \u2014 even those that are known to be suboptimal. The safe set is expanded based on a Lipschitz constant of $g^{\\star}$ , which is assumed to be known. In most real-world settings, this constant is unknown and has to be estimated using the GP. This estimate is generally conservative and results in suboptimal performance. To this end, Berkenkamp et al. (2016) proposed HEURISTIC SAFEOPT which relies solely on the confidence intervals of $g$ to expand the safe set, but lacks convergence guarantees. More recently, Bottero et al. (2022) proposed ISE which queries parameters from ${\\mathcal{S}}_{n}$ that yield the most \u201cinformation\u201d about the safety of another parameter in $\\mathcal{X}$ . Hence, ISE focuses solely on the expansion of the safe set ${\\mathcal{S}}_{n}$ and does not take into account the objective $f$ . In practice, this can lead to significantly worse performance on the simplest of problems (cf. Figure 5). In contrast, ITL balances expansion of and exploration within the safe set. Furthermore, ISE does not have known convergence guarantees of the kind of Theorem 5.1. In parallel independent work, Bottero et al. (2024) proposed a combination of ISE and max-value entropy search (Wang & Jegelka, 2017) for which they derive a similar guarantee to Theorem 5.1.22 Similar to SAFEOPT, their method aims to expand the safe set in all directions including those that are known to be suboptimal. In contrast, ITL directs expansion only towards potentially optimal regions. ", "page_idx": 54}, {"type": "text", "text": "In the 1d task and quadcopter experiment (cf. Figure 5), we observe that SAFEOPT and even ORACLE SAFEOPT converge significantly slower than ITL to the safe optima. We believe this is due to their conservative Lipschitz-continuity/global smoothness-based expansion, as opposed to ITL\u2019s expansion, which adapts to the local smoothness of the constraints. HEURISTIC SAFEOPT, which does not rely on the Lipschitz constant for expansion, does not efficiently expand the safe set due to its heuristic that only considers single-step expansion. This is especially the case for the 1d task. Furthermore, in the 2d task, we notice the suboptimality of ISE since it does not take into account the objective, and purely aims to expand the safe set. ITL, on the other hand, balances expansion and exploration. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "K.2.1 SAFEOPT ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "SAFEOPT (Sui et al., 2015; Berkenkamp et al., 2021) is a well-known algorithm for Safe BO. ", "page_idx": 55}, {"type": "text", "text": "Lipschitz-based expansion SAFEOPT expands the set of known-to-be safe points by assuming knowledge of an upper bound $L_{i}$ to the Lipschitz constant of the unknown constraints $g_{i}^{\\star}$ .23 In each iteration, the (pessimistic) safe set ${\\mathcal{S}}_{n}$ is updated to include all points which can be reached safely (with respect to the Lipschitz continuity) from a known-to-be-safe point ${\\pmb x}\\in{\\cal S}_{n}$ . Formally, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{OpT~\\underset{\\r{=}}{\\operatorname*{def}}~\\underset{\\r{=}}{\\bigcup}\\bigcup_{s\\in S_{n-1}^{\\operatorname{SAFEOPT}}}\\{x^{\\prime}\\in\\mathcal{X}\\mid}\\\\ &{\\qquad\\qquad\\quad l_{n,i}(\\pmb{x})-L_{i}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}\\geq0\\mathrm{~for~all~}i\\in\\mathcal{Z}_{s}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The expansion of the safe set is illustrated in Figure 20. ", "page_idx": 55}, {"type": "text", "text": "We remark two main limitations of this approach. First, the Lipschitz constant is an additional safety critical hyperparameter of the algorithm, which is typically not known. The RKHS assumption (cf. Assumption C.25) induces an assumption on the Lipschitz continuity, however, the worst-case a-priori Lipschitz constant is typically very large, and prohibitive for expansion. Second, the Lipschitz constant is global property of the unknown function, meaning that it does not adapt to the local smoothness. For example, a constraint may be \u201cflat\u201d in one direction (permitting straightforward expansion) and \u201csteep\u201d in another direction (requiring slow expansion). Furthermore, the Lipschitz constant is constant over time, whereas ITL is able to adapt to the local smoothness and reduce the (induced) Lipschitz constant over time. ", "page_idx": 55}, {"type": "text", "text": "Undirected expansion SAFEOPT addresses the trade-off between expansion and exploration by focusing learning on two different sets. First, the set of maximizers ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{n}^{\\mathrm{SAFEOPT}}\\stackrel{\\mathrm{def}}{=}\\{\\pmb{x}\\in S_{n}^{\\mathrm{SAFEOPT}}\\mid\\mid\\qquad\\qquad\\qquad}\\\\ {u_{n,f}(\\pmb{x})\\geq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in S_{n}^{\\mathrm{SAFEOPT}}}l_{n,f}(\\pmb{x})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "which contains all known-to-be-safe points which are potentially optimal. Note that if $S_{n}^{\\mathrm{SAFEOPT}}=S_{n}$ then $\\mathcal{M}_{n}^{\\mathrm{SAFEOPT}}\\subseteq\\mathcal{A}_{n}$ since $A_{n}$ contains points which are potentially optimal and potentially safe but possibly unsafe. ", "page_idx": 55}, {"type": "text", "text": "To facilitate expansion, for each point $\\pmb{x}\\in S_{n}$ , the algorithm considers a set of expanding points ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{n}^{\\mathrm{SAFEOPT}}(\\pmb{x})\\overset{\\mathrm{def}}{=}\\{\\pmb{x}^{\\prime}\\in\\mathcal{X}\\setminus\\mathcal{S}_{n}^{\\mathrm{SAFEOPT}}\\mid}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad u_{n,i}(\\pmb{x})-L_{i}\\Vert\\pmb{x}-\\pmb{x}^{\\prime}\\Vert_{2}\\geq0\\mathrm{~for~all~}i\\in\\mathbb{Z}_{s}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "A point is expanding if it is unsafe initially and can be (optimistically) deduced as safe by observing $\\textbf{\\em x}$ . The set of expanders corresponds to all known-to-be-safe points which optimistically lead to expansion of the safe set: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathcal{G}_{n}^{\\mathrm{SAFEOPT}}\\stackrel{\\mathrm{def}}{=}\\{\\pmb{x}\\in S_{n}\\mid\\vert\\mathcal{F}_{n}(\\pmb{x})\\vert>0\\}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "That is, an expander is a safe point $\\textbf{\\em x}$ which is \u201cclose\u201d to at least one expanding point $\\pmb{x}^{\\prime}$ . Observe that here, we start with a safe $\\textbf{\\em x}$ and then find a close and potentially safe $\\pmb{x}^{\\prime}$ using the Lipschitz-property of the constraint function. Thus, the set of expanding points is inherently limited by the assumed Lipschitzness (cf. Figure 20), and generally a subset of the potential expanders ${\\mathcal{E}}_{n}$ (cf. Equation (27)): ", "page_idx": 55}, {"type": "text", "text": "Lemma K.1. For any $n\\geq0$ , i $^{f}S_{n}^{\\mathrm{SAFEOPT}}=S_{n}$ then ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\bigcup_{\\pmb{x}\\in S_{n}}\\mathcal{F}_{n}^{\\mathrm{SAFEOPT}}(\\pmb{x})\\subseteq\\mathcal{E}_{n}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "image", "img_path": "tZtepJBtHg/tmp/4cdf5f339574da8f1d39b22c811dedd3ceb61d098cd8752f4122de2f97e89edf.jpg", "img_caption": ["Figure 20: Illustration of the expansion of the safe set \u00e0 la SAFEOPT. Here, the blue region denotes the pessimistic safe set $\\boldsymbol{S}$ , the red region denotes the true safe set $S^{\\star}$ , and the orange region denotes the optimistic safe set $\\hat{s}$ . Whereas ITL learns about the point $\\pmb{x}^{\\prime}$ directly, SAFEOPT expands the safe set using the reduction of uncertainty at $\\textbf{\\em x}$ , and then extrapolating using the Lipschitz constant (cf. Equation (48)). The dashed orange line denotes the expanding points of SAFEOPT which under-approximate the optimistic safe set of ITL (cf. Lemma K.1). Thus, ITL may even learn about points in $\\hat{s}$ which are \u201cout of reach\u201d for SAFEOPT. "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "Proof. Without loss of generality, we consider the case where $\\mathcal{T}_{s}=\\{i\\}$ . We have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathcal E_{n}=\\widehat{S}_{n}\\setminus S_{n}=\\{{\\pmb x}\\in\\mathcal X\\setminus S_{n}\\mid u_{n,i}({\\pmb x})\\geq0\\}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The result follows directly by observing that $L_{i}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}\\ge0$ . ", "page_idx": 56}, {"type": "text", "text": "SAFEOPT then selects $x_{n+1}$ according to uncertainty sampling within the maximizers and expanders: $\\mathcal{M}_{n}^{\\mathrm{SAFEOPT}}\\;\\cup\\;\\mathcal{G}_{n}^{\\mathrm{SAFEOPT}}$ . We remark that due to the separate handling of expansion and exploration, SAFEOPT expands the safe set in all directions \u2014 even those that are known to be suboptimal. In contrast, ITL only expands the safe set in directions that are potentially optimal by balancing expansion and exploration through the single set of potential maximizers $A_{n}$ . ", "page_idx": 56}, {"type": "text", "text": "Based on uncertainty sampling As mentioned in the previous paragraph, SAFEOPT selects as next point the maximizer/expander with the largest prior uncertainty.24 In contrast, ITL selects the aproei nnt otw iitdheinnt ${\\mathcal{S}}_{n}$ l  wash itcyhp imcailnliym $\\mathcal{M}_{n}^{\\mathrm{SAFEOPT}}\\;\\cup\\;\\mathcal{G}_{n}^{\\mathrm{SAFEOPT}}\\subset\\bar{S}_{n}^{\\mathrm{AFEOPT}}$ SSAFEOPTand An \u0338\u2287Sn n $A_{n}$ .t the two approaches ", "page_idx": 56}, {"type": "text", "text": "We show empirically in Section 3.3 that depending on the kernel choice (i.e., the smoothness assumptions), uncertainty sampling within a given target space neglects higher-order information that can be attained by sampling outside the set. This can be seen even more clearly when considering linear functions, in which case points outside the maximizers and expanders can be equally informative as points inside. ", "page_idx": 56}, {"type": "text", "text": "Finally, note that the set of expanders is constructed \u201cgreedily\u201d, i.e., only considering single-step expansion. This is necessitated as the inference of safety is based on single reference points. Instead, ITL directly quantifies the information gained towards the points of interest without considering intermediate reference points. ", "page_idx": 56}, {"type": "text", "text": "Requires homoscedastic noise SAFEOPT imposes a homoscedasticity assumption on the noise which is an artifact of the analysis of uncertainty sampling. It is well known that in the presence of heteroscedastic noise, one has to distinguish epistemic and aleatoric uncertainty. Uncertainty sampling fails because it may continuously sample a high variance point where the variance is dominated by aleatoric uncertainty, potentially missing out on reducing epistemic uncertainty at points with small aleatoric uncertainty. In contrast, maximizing mutual information naturally takes into account the two sources of uncertainty, preferring those points where epistemic uncertainty is large and aleatoric uncertainty is small (cf. Appendix C.1). ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "Suboptimal reachable safe set Sui et al. (2015) and Berkenkamp et al. (2021) show that SAFEOPT converges to the optimum within the closure $\\bar{\\mathcal{R}}_{\\epsilon}^{\\mathrm{SAFEOPT}}(S_{0})$ of ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\epsilon}^{\\mathrm{SaFEOPT}}(S)\\stackrel{\\mathrm{def}}{=}S\\cup\\{x\\in\\mathcal{X}\\mid\\exists x^{\\prime}\\in S\\mathrm{~such~that~}}\\\\ &{\\quad\\quad\\quad\\;\\;f_{i}^{\\star}(\\pmb{x}^{\\prime})-(L_{i}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}+\\epsilon)\\geq0\\mathrm{~for~all~}i\\in\\mathcal{Z}_{s}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Note that analogously to the expansion of the safe set, the \u201cexpansion\u201d of the reachable safe set is based on \u201cinferring safety\u201d through a reference point in $\\boldsymbol{S}$ and using Lipschitz continuity. This is opposed to the reachable safe set of ITL (cf. Definition C.29). ", "page_idx": 57}, {"type": "text", "text": "We remark that under the additional assumption that a Lipschitz constant is known, ITL can easily be extended to expand its safe set based on the kernel and the Lipschitz constant, resulting in a strictly larger reachable safe set than SAFEOPT. We leave the concrete formalization of this extension to future work. Moreover, we do not evaluate this extension in our experiments, as we observe that even without the additional assumption of a Lipschitz constant, ITL outperforms SAFEOPT in practice. ", "page_idx": 57}, {"type": "text", "text": "K.2.2 HEURISTIC SAFEOPT ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Berkenkamp et al. (2016) also implement a heuristic variant of SAFEOPT which does not assume a known Lipschitz constant. This heuristic variant uses the same (pessimistic) safe sets ${\\mathcal{S}}_{n}$ as ITL. The set of maximizers is identical to SAFEOPT. As expanders, the heuristic variant considers all safe points $\\pmb{x}\\in S_{n}$ . $\\textbf{\\em x}$ hwee rnee txot  pboe iontb sise rtvheedn  nseelxet ctweitdh  bvya luunec ${\\pmb u}_{n}({\\pmb x})$ y l esaadm tpol $|S_{n+1}|>|S_{n}|$ o- StAhFiEs OsPeTt. $\\mathcal{G}_{n}^{\\mathrm{H-SAFEOPT}}$ $\\mathcal{M}_{n}^{\\mathrm{SAFEOPT}}\\cup\\mathcal{G}_{n}^{\\mathrm{H}}$ ", "page_idx": 57}, {"type": "text", "text": "The heuristic variant shares some properties with SAFEOPT, such that it is based on uncertainty sampling, not adapting to heteroscedastic noise, and separate notions of maximizers and expanders (leading to an \u201cundirected\u201d expansion of the safe set). Note that there are no known convergence guarantees for heuristic SAFEOPT. Importantly, note that similar to SAFEOPT the set of expanders is constructed \u201cgreedily\u201d, and in particular, does only take into account single-step expansion. In contrast, an objective such as ITL which quantifies the \u201cinformation gained towards expansion\u201d also actively seeks out multi-step expansion. ", "page_idx": 57}, {"type": "text", "text": "K.2.3 GOOSE ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "To address the \u201cundirected\u201d expansion of SAFEOPT discussed in the previous section, Turchetta et al. (2019) proposed goal-oriented safe exploration (GOOSE). GOOSE extends any unsafe BO algorithm (which we subsequently call an oracle) to the safe setting. In our experiments, we evaluate GOOSE-UCB which uses UCB as oracle and which is also the variant studied by Turchetta et al. (2019). In the following, we assume for ease of notation that $\\mathcal{T}_{s}=\\{c\\}$ . ", "page_idx": 57}, {"type": "text", "text": "Given the oracle proposal $x^{\\star}$ , GOOSE first determines whether $x^{\\star}$ is safe. If $x^{\\star}$ is safe, $x^{\\star}$ is queried next. Otherwise, GOOSE first learns about the safety of $x^{\\star}$ by querying \u201cexpansionist\u201d points until the oracle\u2019s proposal is determined to be either safe or unsafe. ", "page_idx": 57}, {"type": "text", "text": "GOOSE expands the safe set identically to SAFEOPT according to Equation (48). In the context of GOOSE, $S_{n}^{\\bar{\\mathrm{SAFEOPT}}}$ is called the pessimistic safe set. To determine that a point cannot be deduced as safe, GOOSE also keeps track of a Lipschitz-based optimistic safe set: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{S}_{n,\\epsilon}^{\\mathrm{GoOSE}}\\stackrel{\\mathrm{def}}{=}\\bigcup_{\\pmb{x}\\in S_{n-1}^{\\mathrm{SAFEOPT}}}\\{\\pmb{x}^{\\prime}\\in\\mathcal{X}\\mid}\\\\ &{\\quad\\quad\\quad\\quad\\quad u_{n,c}(\\pmb{x})-L_{c}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}-\\epsilon\\geq0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "We summarize the algorithm in Algorithm 2 where we denote by $\\mathcal O(\\mathcal X)$ the oracle proposal over the domain $\\mathcal{X}$ . ", "page_idx": 57}, {"type": "text", "text": "It remains to discuss the heuristic used to select the \u201cexpansionist\u201d points. GOOSE considers all points $\\pmb{x}\\in S_{n}^{\\mathrm{SAFEOPT}}$ with confidence bands of size larger than the accuracy $\\epsilon$ , i.e., ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathcal{W}_{n,\\epsilon}^{\\mathrm{GoOSE}}\\stackrel{\\mathrm{def}}{=}\\{x\\in S_{n}^{\\mathrm{SaFEOPT}}\\mid u_{n,c}({\\pmb x})-l_{n,c}({\\pmb x})>\\epsilon\\}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Given: Lipschitz constant $L_{c}$ , prior model $\\{f,g_{c}\\}$ , oracle $\\scriptscriptstyle\\mathcal{O}$ , and precision $\\epsilon$   \nSet initial safe set $S_{0}^{\\mathrm{SAFEOPT}}$ based on prior   \n$\\begin{array}{l}{\\widehat{S}_{n,\\epsilon}^{\\mathrm{GoosE}}\\leftarrow\\chi}\\\\ {n\\leftarrow0}\\end{array}$   \nfor $k$ from 1 to $\\infty$ do $\\pmb{x}_{k}^{\\star}\\leftarrow\\mathcal{O}(\\widehat{S}_{n,\\epsilon}^{\\mathrm{GoOSE}})$ while $\\pmb{x}_{k}^{\\star}\\notin\\mathcal{S}_{n}^{\\mathrm{SAFEOPT}}\\,\\mathbf{do}$ Observe \u201cexpansionist\u201d point $x_{n+1}$ , set $n\\gets n+1$ , and update model and safe sets end while Observe $\\pmb{x}_{k}^{\\star}$ , set $n\\gets n+1$ , and update model and safe sets   \nend for ", "page_idx": 58}, {"type": "text", "text": "Which of the points in this set is evaluated depends on a set of learning targets AnG,O\u03f5OSEd=efS nG,O\u03f5OSE\\ $S_{n}^{\\mathrm{SAFEOPT}}$ akin to the \u201cpotential expanders\u201d ${\\mathcal{E}}_{n}$ (cf. Equation (27)), to each of which w e assign a priority $h(x)$ . When $h(x)$ is large, this indicates that the algorithm is prioritizing to determine whether $\\textbf{\\em x}$ is safe. We use as heuristic the negative $\\ell_{1}$ -distance between $\\textbf{\\em x}$ and $x^{\\star}$ . GOOSE then considers the set of potential immediate expanders ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathcal{G}_{n,\\epsilon}^{\\mathrm{GooSE}}(\\alpha)\\overset{\\mathrm{def}}{=}\\{\\pmb{x}\\in\\mathcal{W}_{n,\\epsilon}^{\\mathrm{GooSE}}\\mid\\exists\\pmb{x}^{\\prime}\\in\\mathcal{A}_{n,\\epsilon}^{\\mathrm{GooSE}}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "The \u201cexpansionist\u201d point selected by GOOSE is then any point in $\\mathcal{G}_{n,\\epsilon}^{\\mathrm{GoOSE}}(\\alpha^{\\star})$ where $\\alpha^{\\star}$ denotes the largest priority such that $|\\mathcal{G}_{n,\\epsilon}^{\\mathrm{GoOSE}}(\\alpha^{\\star})|>0$ ", "page_idx": 58}, {"type": "text", "text": "We observe empirically that the sample complexity of GOOSE is not always better than that of SAFEOPT. Notably, the expansion of the safe set is based on a \u201cgreedy\u201d heuristic. Moreover, determining whether a single oracle proposal $x^{\\star}$ is safe may take significant time. Consider the (realistic) example where the prior is uniform, and UCB proposes a point which is far away from the safe set and suboptimal. GOOSE will typically attempt to derive the safety of the proposed point until the uncertainty at all points within $\\dot{S_{0}^{\\mathrm{SAFEOPT}}}$ is reduced to $\\epsilon$ .25 Thus, GOOSE can \u201cwaste\u201d a significant number of samples, aiming to expand the safe set towards a known-to-be suboptimal point. In larger state spaces, due to the greedy nature of the expansion strategy, this can lead to GOOSE being effectively stuck at a suboptimal point for a significant number of rounds. ", "page_idx": 58}, {"type": "text", "text": "K.2.4 ISE and ISE-BO ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Recently, Bottero et al. (2022) proposed an information-theoretic approach to efficiently expand the safe set which they call information-theoretic safe exploration (ISE). Specifically, they choose the next action ${\\pmb x}_{n}$ by approximating ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\underset{\\pmb{x}\\in S_{n-1}}{\\arg\\operatorname*{max}}\\ \\underset{\\pmb{x}^{\\prime}\\in\\mathcal{X}}{\\operatorname*{max}}\\,\\mathrm{I}(\\mathbb{1}\\{g_{\\pmb{x}^{\\prime}}\\geq0\\};y_{\\pmb{x}}\\mid\\mathcal{D}_{n-1})\\,.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "In a parallel independent work, Bottero et al. (2024) extended ISE to the Safe BO problem where they propose to choose ${\\pmb x}_{n}$ according to ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\underset{{\\pmb{x}}\\in{\\cal{S}}_{n-1}}{\\arg\\operatorname*{max}}\\operatorname*{max}\\{\\alpha^{\\mathrm{ISE}}({\\pmb{x}}),\\alpha^{\\mathrm{MES}}({\\pmb x})\\}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\alpha^{\\mathrm{MES}}$ denotes the acquisition function of max-value entropy search (Wang & Jegelka, 2017). Similarly to SAFEOPT, ISE-BO treats expansion and exploration separately, which leads to \u201cundirected\u201d expansion of the safe set. That is, the safe set is expanded in all directions, even those that are known to be suboptimal. In contrast, ITL balances expansion and exploration through the single set of potential maximizers $A_{n}$ . With a stochastic target space, ITL generalizes max-value entropy search (cf. Appendix K.1). ", "page_idx": 58}, {"type": "text", "text": "We evaluate ISE-BO in Figure 18 and observe that it does not outperform ITL and VTL in any of the tasks, while performing poorly in the 1d task and suboptimally in the 2d task. ", "page_idx": 58}, {"type": "image", "img_path": "tZtepJBtHg/tmp/3a449bfb128933deddf9f7de63daf17367026b9d77c3cb980e6b20d4b11af15e.jpg", "img_caption": ["Figure 21: The ground truth $f^{\\star}$ is shown as the dashed black line. The solid black line denotes the constraint boundary. The GP prior is given by a linear kernel with sin-transform and mean $0.1x$ . The light gray region denotes the initial optimistic safe setS 0 and the dark gray region denotes the initial pessimistic safe set ${\\mathcal S}_{0}$ . "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "tZtepJBtHg/tmp/35437a8a4a8e29b3afb38bc25cdd0fbfd87f189112c656ba6b161456ef0b1cec.jpg", "img_caption": ["Figure 22: First 100 samples of ITL using the potential expanders ${\\mathcal{E}}_{n}$ (cf. Equation (27)) as target space (left) and SAFEOPT sampling only from the set of expanders nSAFEOPT(right). "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "K.3 Jumping Past Local Barriers ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "In this additional experiment we demonstrate that ITL is able to extrapolate safety beyond local unsafe \u201cbarriers\u201d, which is a fundamental limitation of Lipschitz-based methods such as SAFEOPT. We consider the ground truth function and prior statistical model shown in Figure 21. Note that initially, there are three disjoint safe \u201cregions\u201d known to the algorithm corresponding to two of the three safe \u201cbumps\u201d of the ground truth function. In this experiment, the main challenge is to \u201cjump past\u201d the local barrier separating the leftmost and initially unknown safe \u201cbump\u201d. ", "page_idx": 59}, {"type": "text", "text": "Figure 22 shows the sampled points during the first 100 iterations of SAFEOPT and ITL. Clearly, SAFEOPT does not discover the third safe \u201cbump\u201d while ITL does. Indeed, it is a fundamental limitation of Lipschitz-based methods that they can never \u201cjump past local barriers\u201d, even if the oracle Lipschitz constant were to be known and tight (i.e., locally accurate) around the barrier. This is because Lipschitz-based methods expand to the point $\\textbf{\\em x}$ based on a reference point $\\pmb{x}^{\\prime}$ , and by definition, if $\\textbf{\\em x}$ is added to the safe set so are all points on the line segment between $\\textbf{\\em x}$ and $\\mathbf{\\nabla}x^{\\prime}$ . Hence, if there is a single point on this line segment which is unsafe (i.e., a \u201cbarrier\u201d), the algorithm will never expand past it. This limitation does not exist for kernel-based algorithms as expansion occurs in function space. ", "page_idx": 59}, {"type": "text", "text": "Moreover, note that for a non-stationary kernel such as in this example, ITL samples the \u201cclosest points\u201d in function space rather than Euclidean space. We observe that SAFEOPT still samples \u201clocally at the boundary\u201d whereas ITL samples the most informative point which in this case is the local maximum of the sinusoidal function. In other words, ITL adapts to the geometry of the function. This generally leads us to believe that ITL is more capable to exploit (non-stationary) prior knowledge than distance-based methods such as SAFEOPT. ", "page_idx": 59}, {"type": "image", "img_path": "tZtepJBtHg/tmp/132a9b382bd886a3508029cf8fc1a5c27eab17832d8a466fa8647a7c36da547b.jpg", "img_caption": ["Figure 23: Ground truth and prior well-calibrated model in 1d synthetic experiment. The function serves simultaneously as objective and as constraint. The light gray region denotes the initial safe set ${\\mathcal{S}}_{0}$ . "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "tZtepJBtHg/tmp/e4c590411df32fca168fe7aba84ccb92ebe8fc5ed41a28ca90beae44d13b84c1.jpg", "img_caption": ["Figure 24: Size of $\\ensuremath{\\boldsymbol{S}}_{n}$ in 1d synthetic experiment. The dashed black line denotes the size of $S^{\\star}$ . In this task, \u201cdiscovering\u201d the optimum is closely linked to expansion of the safe set, and HEURISTIC SAFEOPT fails since it does not expand the safe set sufficiently. "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "K.4 Experiment Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "K.4.1 Synthetic Experiments ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1d task Figure 23 shows the objective and constraint function, as well as the prior. We discretize using 500 points. The main difficulty in this experiment lies in sufficiently expanding the safe set to discover the global maximum. Figure 24 plots the size of the safe set $\\ensuremath{\\boldsymbol{S}}_{n}$ for the compared algorithms, which in this experiment matches the achieved regret closely. ", "page_idx": 60}, {"type": "text", "text": "2d task We model our constraint in the form of a spherical \u201cisland\u201d where the goal is to get a good view of the coral reef located to the north-east of the island while staying in the interior of the island during exploration (cf. Figure 25). The precise objective and constraint functions are unknown to the agent. Hence, the agent has to gradually and safely update its belief about boundaries of the \u201cisland\u201d and the location of the coral reef. The prior is obtained by a single observation within the center of the island $[-0.5,0.5]^{2}$ . We discretize using 2 500 points. ", "page_idx": 60}, {"type": "image", "img_path": "tZtepJBtHg/tmp/6d52ba408af03e150fe56386e0bc18d9c8aefc13b02d6c4dbab1667226c5692a.jpg", "img_caption": ["Figure 25: Ground truth in 2d synthetic experiment. "], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "K.4.2 Safe Controller Tuning for Quadcopter ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Modeling the real-world dynamics We learn a feedback policy (i.e., \u201ccontrol gains\u201d) to compensate for inaccuracies in the initial controller. In our experiment, we model the real world dynamics and the adjusted model using the PD control feedback (Widmer et al., 2023), ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\delta_{t}({\\pmb x})\\,{\\stackrel{\\mathrm{def}}{=}}({\\pmb x}^{\\star}-{\\pmb x})[({\\pmb s}^{\\star}-{\\pmb s}_{t})\\,\\,(\\dot{\\pmb s}^{\\star}-\\dot{\\pmb s}_{t})],\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $x^{\\star}$ are the unknown ground truth disturbance parameters, and $s^{\\star}$ and ${\\dot{s}}^{\\star}$ are the desired state and state derivative, respectively. This yields the following ground truth dynamics: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{s}_{t+1}(\\pmb{x})=\\pmb{T}(\\pmb{s}_{t},\\pmb{u}_{t}+\\pmb{\\delta}_{t}(\\pmb{x})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "The feedback parameters $\\pmb{x}\\,=\\,[\\pmb{x}_{p}\\,\\pmb{\\ x}_{d}]^{\\top}$ can be split into $\\pmb{x}_{p}$ tuning the state difference which are called proportional parameters and $\\pmb{x}_{d}$ tuning the state derivative difference which are called derivative parameters. We use the \u201ccritical damping\u201d heuristic to relate the proportional and derivative parameters: ${\\pmb x}_{d}=2\\sqrt{{\\pmb x}_{p}}$ . We thus consider the restricted domain $\\mathcal{X}\\doteq[0,20]^{4}$ where each dimension corresponds to the proportional feedback to one of the four rotors. ", "page_idx": 61}, {"type": "text", "text": "Ground truth disturbance parameters are sampled from a chi-squared distribution with one degree of freedom (i.e., the square of a standard normal distribution), $\\pmb{x}_{p}^{\\star}\\sim\\chi_{1}^{2}$ , and $\\pmb{x}_{d}^{\\star}$ is determined according to the critical damping heuristic. ", "page_idx": 61}, {"type": "text", "text": "The learning problem The goal of our learning problem is to move the quadcopter from its initial position $\\mathbf{\\dot{s}}(0)=[1\\mathrm{~}1\\mathrm{~}1]^{\\intercal}$ (in Euclidean space with meter as unit) to position $\\pmb{\\dot{s}}^{\\star}=[0\\mathrm{~}0\\mathrm{~}2]^{\\top}$ . Moreover, we aim to stabilize the quadcopter at the goal position, and therefore regularize the control signal towards an action $\\pmb{u}^{\\star}$ which results in hovering (approximately) without any disturbances. We formalize these goals with the following objective function: ", "page_idx": 61}, {"type": "equation", "text": "$$\nf^{\\star}(\\pmb{x})\\stackrel{\\mathrm{def}}{=}-\\sigma\\left(\\sum_{t=0}^{T}\\|\\pmb{s}^{\\star}-\\pmb{s}_{t}(\\pmb{x})\\|_{Q}^{2}+\\|\\pmb{u}^{\\star}-\\pmb{u}_{t}(\\pmb{x})\\|_{R}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $\\sigma(v)\\stackrel{\\mathrm{def}}{=}\\operatorname{tanh}((v-100)/100)$ is used to smoothen the objective function and ensure that its range is $[-1,1]$ . The non-smoothed control objective in Equation (51) is known as a linear-quadratic regulator (LQR) which we solve exactly for the undisturbed system using ILQR (Tu et al., 2023). Finally, we want to ensure at all times that the quadcopter is at least 0.5 meter above the ground, that is, ", "page_idx": 61}, {"type": "equation", "text": "$$\ng^{\\star}({\\pmb x})\\,{\\overset{\\mathrm{def}}{=}}\\,\\underset{t\\in[T]}{\\operatorname*{min}}\\,{\\pmb s}_{t}^{z}({\\pmb x})-0.5\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where we denote by $\\mathbf{\\mathcal{s}}_{t}^{z}$ the ${\\bf Z}$ -coordinate of state $\\scriptstyle s_{t}$ . ", "page_idx": 61}, {"type": "text", "text": "We use a time horizon of $T\\,=\\,3$ seconds which we discretize using 100 steps. The objective is modeled by a zero-mean GP with a Mat\u00e9rn $\\nu=5/2)$ ) kernel with lengthscale 0.1, and the constraint is modeled by a GP with mean $-0.5$ and a Mat\u00e9rn $\\chi=5/2\\chi$ ) kernel with lengthscale 0.1. The prior is obtained by a single observation of the \u201csafe seed\u201d $\\left[0\\;0\\;0\\;10\\right]^{\\top}$ . ", "page_idx": 61}, {"type": "text", "text": "Adaptive discretization We discretize the domain $\\mathcal{X}$ adaptively using coordinate LINEBO (Kirschner et al., 2019). That is, in each iteration, one of the four control dimensions is selected uniformly at random, and the active learning oracle is executed on the corresponding one-dimensional subspace. ", "page_idx": 62}, {"type": "text", "text": "Safety Using the (unsafe) constrained BO algorithm EIC (Gardner et al., 2014) leads constraint violation,26 while ITL and VTL do not violate the constraints during learning for any of the random seeds. ", "page_idx": 62}, {"type": "text", "text": "Hyperparameters The observation noise is Gaussian with standard deviation $\\rho=0.1$ . We let $\\beta=10$ . The control target is $\\pmb{u}^{\\star}=\\left[1.766\\mathrm{~0~0~0~}\\right]^{\\top}$ . ", "page_idx": 62}, {"type": "text", "text": "The state space is 12-dimensional where the first three states correspond to the velocity of the quadcopter, the next three states correspond to its acceleration, the following three states correspond to its angular velocity, and the last three states correspond to its angular velocity in local frame. The LQR parameters are given by ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{Q=\\mathrm{diag}\\left\\{1,1,1,1,1,1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1\\right\\}}}&{{\\mathrm{and}}}\\\\ {{R=0.01\\cdot\\mathrm{diag}\\left\\{5,0.8,0.8,0.3\\right\\}}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "The quadcopter simulation was adapted from Chandra (2023). ", "page_idx": 62}, {"type": "text", "text": "Each one-dimensional subspace is discretized using 2 000 points. ", "page_idx": 62}, {"type": "text", "text": "Random seeds We repeat the experiment for 25 different seeds where the randomness is over the ground truth disturbance, observation noise, and the randomness in the algorithm. ", "page_idx": 62}, {"type": "table", "img_path": "tZtepJBtHg/tmp/617ed48fbb5b2bd2350ed0c8eff86ba3ed040de60b3d03de3cdaaef6fb3542ee.jpg", "table_caption": ["Table 3: Magnitudes of $\\gamma_{n}$ for common kernels. The magnitudes hold under the assumption that $\\mathcal{X}$ is compact. Here, $B_{\\nu}$ is the modified Bessel function. We take the magnitudes from Theorem 5 of Srinivas et al. (2009) and Remark 2 of Vakili et al. (2021). The notation $\\widetilde O(\\cdot)$ subsumes log-factors. For $\\nu=1/2$ , the Mat\u00e9rn kernel is equivalent to the Laplace kernel. For $\\nu\\to\\infty$ , the Mat\u00e9rn kernel is equivalent to the Gaussian kernel. The functions sampled from a Mat\u00e9rn kernel are $\\lceil\\nu\\rceil-1$ mean square differentiable. The kernel-agnostic bound follows by simple reduction to a linear kernel in $|{\\mathcal{X}}|$ dimensions. "], "table_footnote": [], "page_idx": 63}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 64}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 64}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 65}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: The code is publicly available. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 65}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 66}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: See the extensive discussions in the appendices. Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 66}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 66}, {"type": "text", "text": "", "page_idx": 67}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: All individual experiments require only small compute resources. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 67}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 67}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 67}, {"type": "text", "text": "", "page_idx": 68}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 68}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 68}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 68}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 68}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 69}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 69}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 69}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 69}]