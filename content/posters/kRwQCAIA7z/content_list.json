[{"type": "text", "text": "Dimension-free Private Mean Estimation for Anisotropic Distributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuval Dagan   \nSchool of Computer Science Tel Aviv University   \nydagan@tauex.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Michael I. Jordan Department of EECS and Statistics University of California Berkeley michael_jordan@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Xuelin Yang Department of Statistics University of California Berkeley xuelin@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Lydia Zakynthinou Department of EECS University of California Berkeley lydiazak@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Nikita Zhivotovskiy Department of Statistics   \nUniversity of California Berkeley   \nzhivotovskiy@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present differentially private algorithms for high-dimensional mean estimation. Previous private estimators on distributions over Rd suffer from a curse of dimensionality, as they require $\\Omega(d^{1/2})$ samples to achieve non-trivial error, even in cases where $O(1)$ samples suffice without privacy. This rate is unavoidable when the distribution is isotropic, namely, when the covariance is a multiple of the identity matrix. Yet, real-world data is often highly anisotropic, with signals concentrated on a small number of principal components. We develop estimators that are appropriate for such signals\u2014our estimators are $(\\varepsilon,\\delta)$ -differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions. Given $n$ samples from a distribution with known covariance-proxy $\\Sigma$ and unknown mean $\\mu$ , we present an estimator $\\hat{\\mu}$ that achieves error , $\\|{\\hat{\\mu}}-{\\bar{\\mu}}\\|_{2}\\leq\\alpha$ , as long as $n\\gtrsim\\mathrm{tr}(\\Sigma)/\\alpha^{2}+\\mathrm{tr}(\\Sigma^{1/2})/(\\alpha\\varepsilon)$ . We show that this is the optimal sample complexity for this task up to logarithmic factors. Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from $d^{1/2}$ to $d^{1/4}$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning is increasingly deployed in real-world settings to learn about properties of populations, both large and small. When the data comes from human populations, it is essential that algorithm design allows inferring properties of populations without revealing potentially sensitive information about specific individuals in the population. That sensitive information can be revealed, inadvertently or adversarially, has been demonstrated in numerous ways, including via reconstruction attacks [21, 24], as well as membership-inference attacks [56], often targeting sensitive genomic data [34, 55, 64]. To mitigate the risk of privacy violations in general database theory, Dwork, McSherry, Nissim, and Smith [25] proposed the rigorous guarantee of differential privacy (DP), which has been widely adopted in industry [29, 11, 33, 62, 53, 4] and government [30, 1, 2]. Algorithms that are differentially private are guaranteed to not leak too much information about the individuals in a database. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the machine learning setting, there is a tension between differential privacy and inferential and predictive accuracy. It is an ongoing challenge to capture that tension mathematically, in a way that is applicable to a wide variety of problems and is sufficiently quantitative so as to provide a guide for real users and real systems designers. A particularly salient theoretical challenge is to obtain results that capture dimension-dependence\u2014given that machine learning data are often of high dimensionality and involve significant correlation among dimensions, and given that privacy is difficult to guarantee in high dimensions, particularly so when there are correlations. Indeed, differentially private inference suffers from a curse of dimensionality\u2014the sample size $n$ that is required to obtain a non-trivial DP learner is often polynomial in the dimension $d$ of the data. ", "page_idx": 1}, {"type": "text", "text": "Significant progress has been made in addressing this challenge in recent years by focusing on a relatively simple inferential task, that of high-dimensional mean estimation. Formally, given a data set of $n$ points, $X=(X^{(1)},\\ldots,X^{(n)})\\in\\bar{\\mathbb{R}}^{d\\times n}$ drawn i.i.d. from a multivariate distribution $\\mathcal{P}$ with unknown mean $\\mu\\in\\mathbb{R}^{d}$ , the goal is to learn $\\mu$ . ", "page_idx": 1}, {"type": "text", "text": "Obtaining low-error private mean estimators in the high-dimensional regime is not always possible. For example, consider a Gaussian distribution ${\\mathcal{P}}\\,=\\,{\\overline{{\\mathcal{N}}}}(\\mu,\\sigma^{2}I_{d})$ , where $I_{d}$ is the $d\\times d$ identity matrix. Here, the sample complexity of any private estimator $\\hat{\\mu}$ achieving error $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq\\alpha$ is $n=\\Omega(d\\sigma^{2}/\\alpha^{2}+d\\sigma/(\\alpha\\varepsilon))$ [39], where $\\varepsilon$ is the privacy parameter.1 The first term corresponds to the non-private sample complexity and the second term to the additional samp\u221ales required due to privacy. Although both depend on $d$ , note that for non-trivial error $\\alpha=0.01\\sigma\\sqrt{d}$ and $\\varepsilon=0.1$ , the non\u221a-private term is $O(1)$ , whereas the dimension-dependence persists in the cost of privacy which is $O({\\sqrt{d}})$ . ", "page_idx": 1}, {"type": "text", "text": "In spite of this lower bound, there is still hope for obtaining better dependence on the dimension in certain cases. This is due to the fact that the lower bound instance assumes that the covariance is isotropic: a multiple of the identity matrix. However, real-world data are far from being isotropic. Often, the signal is concentrated in a few directions, while it is significantly weaker in others, as can be revealed via Singular Value Decomposition (SVD). In these cases, there are several examples of non-private estimators for a variety of tasks which exploit the structure of the data to achieve lower sample complexity. Specifically for mean estimation of Gaussian distributions, as in our example above, only $\\bar{n}=\\dot{O}(\\mathrm{tr}(\\bar{\\Sigma})/\\alpha^{2})$ samples are required [48] (this number of samples is sufficient even for robust estimators under the strong contamination model [50]). This bound is instance-adaptive, as the trace of the covariance matrix $\\operatorname{tr}(\\Sigma)$ equals its upper bound, $d\\Vert\\Sigma\\Vert_{2}$ , in the isotropic case, but can be much smaller for anisotropic data. Exploiting the non-isotropic structure of the covariance matrix is also central to the covariance estimation problem with respect to the operator norm (namely, when the error between the true covariance matrix $\\Sigma$ and its estimate $\\hat{\\Sigma}$ is measured in terms of $\\lVert\\hat{\\boldsymbol{\\Sigma}}-\\boldsymbol{\\Sigma}\\rVert_{2})$ [43, 65]. A more recent focus is on overparametrized linear regression [9], where again the highly non-isotropic structure of the covariance matrix allows for inference under certain assumptions on the decay of eigenvalues of the covariance matrix. In all the mentioned results, non-private estimation is possible when $n\\ll d$ , including even infinite-dimensional Hilbert spaces. ", "page_idx": 1}, {"type": "text", "text": "Returning to private estimation, prior work has obtained optimal bounds for learning the mean of high-dimensional (sub)Gaussian distributions in the affine-invariant Mahalanobis distance [18, 39, 3, 46, 13, 47]. These imply an upper bound for learning the mean in Euclidean distance in the order of $n=O(d\\|\\Sigma\\|_{2}/\\alpha^{2}+d\\sqrt{\\|\\Sigma\\|_{2}}/(\\alpha\\varepsilon))$ , which is optimal for isotropic, but loose for anisotropic cases. A folklore estimator, based on [41] achieves $n=\\Omega(\\sqrt{d\\mathrm{tr}(\\Sigma)}/(\\alpha\\varepsilon))$ , while [8] are the first to focus on the anisotropic ca\u221ase and obtain improved bounds for diagonal covariance, achieving error $n=\\Omega(\\mathrm{tr}(\\Sigma^{1/2})/(\\alpha\\bar{\\varepsilon})+\\sqrt{d}/\\varepsilon)$ . Thus, all previous work requires that the sample complexity is at least $\\Omega({\\sqrt{d}})$ , which excludes the high-dimensional scenario we are interested in. We are led to pose the following question.: ", "page_idx": 1}, {"type": "text", "text": "Question 1. Is it possible to obtain good private mean estimators with a sample size that grows slower with the dimension, or is even dimension-independent, when the covariance of the data is far from isotropic? What is the optimal sample complexity in the case of known and unknown covariance? ", "page_idx": 2}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, note that no improved bounds are possible for pure DP, as follows directly from the so-called packing technique [32, 18] and specifically applying [18, Lemma 5.1]: any $\\varepsilon$ -DP algorithm which estimates the mean of a Gaussian distribution up to constant accuracy requires $n=\\Omega\\bar{(}d/\\varepsilon)$ samples. This negative result motivates us to focus on $(\\varepsilon,\\delta)$ -differential privacy. ", "page_idx": 2}, {"type": "text", "text": "In order to make progress, one would like to utilize the fact that when the covariance is far from being isotropic, the data is closer to being low-dimensional. Concretely, let $\\Sigma$ be the covariance matrix of $\\mathcal{P}$ and $\\sigma_{1}^{2}\\,\\geq\\,.\\ldots\\,\\geq\\,\\sigma_{d}^{2}$ its singular values. If the covariance is far from isotropic, there are only few directions with non-trivial variance. For illustration, if $\\sigma_{1}=\\cdot\\cdot\\cdot=\\sigma_{k}=1$ , whereas $\\sigma_{k+1}=\\cdot\\cdot\\cdot=\\sigma_{d}=1/d$ , then the distribution is, in some sense, clo\u221ase to being $k$ -dimensional. Here, we would like our sample complexity to be of order $k$ rather than $\\sqrt{d}$ . ", "page_idx": 2}, {"type": "text", "text": "We start by presenting a result in the case where the covariance matrix is known. Here, the bound depends only on $\\textstyle\\sum_{i=1}^{d}\\sigma_{i}\\,=\\,\\operatorname{tr}(\\Sigma^{1/2})$ , a quantity allowing less contribution from small singular values: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 (Upper bound, known covariance, informal). Set $\\varepsilon,\\delta\\,\\in\\,(0,1)$ , $\\alpha\\,>\\,0$ . Let $\\textit{X}\\sim$ $\\mathcal{N}(\\mu,\\Sigma)^{n}$ with known covariance. There exists an $(\\varepsilon,\\delta)$ -differentially private algorithm which, with probability 0.99, returns an estimate $\\hat{\\mu}$ such that $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq\\alpha,$ , and has sample complexity ", "page_idx": 2}, {"type": "equation", "text": "$$\nn=\\tilde{O}\\left(\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\mathrm{tr}(\\Sigma^{1/2})\\sqrt{\\log(1/\\delta)}}{\\alpha\\varepsilon}+\\frac{\\log(1/\\delta)}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The first term corresponds to the non-private sample complexity, whereas the remaining two terms are due to privacy. The result extends to subgaussian distributions. In the example illustrated above, this bound indeed yields a dimension-independent complexity of $n=\\tilde{O}_{\\delta}(k/\\alpha^{\\stackrel{.}{2}}+k/(\\alpha\\varepsilon))$ . ", "page_idx": 2}, {"type": "text", "text": "We show that the sample complexity of Theorem 1.1 is nearly optimal. Indeed, the first summand is optimal due to [19, Theorem 4], while the last summand is optimal by a lower bound in the univariate case [41]. We show the optimality of the intermediate summand in (1) up to polylogarithmic terms. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2 (Lower bound, informal). Any $(\\varepsilon,\\delta)$ -DP algorithm which estimates the mean $\\mu\\in$ $[-1,1]^{d}$ of a Gaussian up to $\\alpha$ with probability 0.99 has sample complexity $\\begin{array}{r}{n=\\Omega\\left(\\frac{\\mathrm{tr}(\\Sigma^{1/2})}{\\alpha\\varepsilon\\log^{2}(d)}\\right)}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "We now move to the case of unknown covariance. A first approach would be to learn the covariance approximately, namely, find a matrix $A$ such that $A\\preceq\\bar{\\Sigma}\\preceq C A$ , for some $C>1$ , and then use $A$ instead of $\\Sigma$ in our known-covariance estimator. However, learning such a matrix $A$ privately requires sample size $n=\\Theta(d^{3/2})$ [39, 40]. Another approach\u221a would be to learn only the diagonal elements of the covariance [41] this would require $n=O(\\sqrt{d}/\\varepsilon)$ samples. Below, we obtain a sample complexity whose dependence in the dimension is $d^{1/4}$ , together with a dependendence on the diagonal elements of the covariance matrix: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.3 (Upper bound, unknown covariance, informal). Let parameters $\\varepsilon,\\delta\\,\\in\\,(0,1)$ . Let $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{\\mu}},\\ensuremath{\\boldsymbol{\\Sigma}})^{n}$ with unknown covariance $\\Sigma$ . There exists an $(\\varepsilon,\\delta)$ -DP algorithm which, with probability 0.99, returns an estimate $\\hat{\\mu}$ such that $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq\\alpha_{*}$ , and has sample complexity ", "page_idx": 2}, {"type": "equation", "text": "$$\nn=\\tilde{O}\\left(\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}\\sqrt{\\log(1/\\delta)}}{\\alpha\\varepsilon}+\\frac{d^{1/4}\\sqrt{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}}\\log(1/\\delta)}{\\sqrt{\\alpha}\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In general, $\\mathrm{tr}(\\Sigma^{1/2})\\le\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}$ , and if $\\Sigma$ is diagonal, the two quantities coincide. Our theorem is in fact more adaptable to easier cases of covariance structure. As a special case, when the covariance is diagonal and the \u221asingular values exhibit an exponential decay, that is, $\\sigma_{i}=\\sigma_{1}e^{-(i-1)}$ , then $\\begin{array}{r}{n=\\tilde{O}\\left(\\frac{\\mathrm{tr}(\\overline{{\\Sigma}})}{\\alpha^{2}}+\\frac{\\mathrm{tr}(\\Sigma^{1/2})\\sqrt{\\mathrm{log}(1/\\delta)}}{\\alpha\\varepsilon}+\\frac{\\mathrm{log}^{5/3}(d)\\log^{3/2}(1/\\delta)}{\\varepsilon}\\right)}\\end{array}$ samples suffice even under unknown covariance. ", "page_idx": 2}, {"type": "text", "text": "1.2 Techniques ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Known covariance. A folklore $(\\varepsilon,\\delta)$ -DP algorithm, based on techniques for the univariate case developed by [41], is to fliter outliers by privately estimating each individual coordinate of the mean, $\\mu_{i}$ , up to an additive error of $\\tilde{O}(\\Sigma_{i i}^{1/2})$ for all $i$ , clipping any sample point to within that range, and outputting the mean of the modified data set with added spherical Gaussian noise $\\mathcal{N}(0,\\mathrm{tr}(\\Sigma)\\bar{I}_{d}/(\\varepsilon^{2}n^{2}))$ . A standard analysis of this procedure yields a sample complexity of n \u2273 tr\u03b1(2\u03a3)+ $\\begin{array}{r}{n\\gtrsim\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\sqrt{d}}{\\varepsilon}+\\frac{\\sqrt{d\\,\\mathrm{tr}(\\Sigma)}}{\\alpha\\varepsilon}}\\end{array}$ where the dependence on $\\delta$ is omitted for clarity. For constant $\\varepsilon$ , the folklore estimator achieves privacy for free, that is, the error due to privacy is lower than the error of statistical estimation, when $n\\gtrsim d$ . ", "page_idx": 3}, {"type": "text", "text": "An improvement to this simple analysis, proposed recently by Aum\u00fcller et al. [8] for matrices of diagonal covariance, suggests adding noise $N\\left(0,\\mathrm{tr}(\\Sigma^{1/2})\\Sigma^{1/2}/(\\varepsilon^{2}n^{2})\\right)$ instead, which introduces more noise in the directions of larger variance. Slightly simplifying their result and addition\u221aally g.  lTohgiasr ietshtimmica tfoar catocrhsi eivn $d$ , parnivda tchye  froar nfgree eo fa $\\mu$ l,o tnhge iar s $\\begin{array}{r}{n\\gtrsim\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\sqrt{d}}{\\varepsilon}+}\\end{array}$ $\\frac{\\mathrm{tr}(\\Sigma^{1/2})}{\\alpha\\varepsilon}$ $n\\gtrsim\\operatorname*{max}\\{\\|\\pmb{\\sigma}\\|_{1}^{2}/\\|\\pmb{\\sigma}\\|_{2}^{2},\\sqrt{d}\\}$ $\\sigma^{2}$ denotes the vector of singular values of $\\Sigma$ . While this removes the dimension dependence in the third term compared to the na\u00efve sample complexity, the second term still requires $\\Omega({\\sqrt{d}})$ samples. This is due to the first step of the algorithm (inherited from [37]), which performs $d$ independent estimation tasks. In both approaches, the pre-processing step is a form of coarse mean estimation which ensures that the data will not include outliers, and it is the source of sample-inefficiency. ", "page_idx": 3}, {"type": "text", "text": "Thus, in our work, we remove outliers, namely vectors too far away from the true mean in one of the coordinates, using only $n=\\tilde{O}(1/\\varepsilon)$ samples, thus completely removing the dependence on $d$ in the final sample complexity bounds. (Indeed, our estimator achieves privacy for free for $n\\gtrsim\\operatorname*{max}\\{\\|\\pmb{\\sigma}\\|_{1}^{2}/\\|\\pmb{\\sigma}\\|_{2}^{2}\\}.$ .) Next, we generalize the approach of [8] to general covariance, rather than diagonal. Finally, we show that the sample complexity is nearly optimal. Specifically: ", "page_idx": 3}, {"type": "text", "text": "Our pre-processing is realized by using a polynomial-time filtering algorithm of Tsfadia et al. [63]. Given a predicate computed for two data points, so-called FriendlyCore returns a subset $X^{\\prime}$ of the input, such that all pairs of the remaining, unfiltered data points satisfy the predicate. Its sample complexity is $\\tilde{O}(1/\\bar{\\varepsilon})$ for any predicate, hence it has the potential to yield a dimension-independent bound. For our purposes, $X^{\\prime}$ needs to satisfy some sensitivity properties. It follows from our analysis that the flitering should be such that for any two points $X^{(j)},\\bar{X}^{(\\ell)}\\in X^{\\prime}$ , $\\lVert\\Sigma^{-1/4}(X^{(j)}-X^{(\\ell)})\\rVert_{2}^{2}\\leq$ $\\tilde{O}\\left(\\mathrm{tr}(\\Sigma^{1/2})\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "The lower bound for $(\\varepsilon,\\delta)$ -DP is an application of the standard fingerprinting [15, 28, 39, 40] technique for isotropic Gaussians. A straightforward modification of the technique to anisotropic covariance $\\Sigma$ gives a weaker bound than Theorem 1.2. Instead one needs to choose an appropriate set of almost-isotropic coordinates whose size scales with $\\operatorname{tr}(\\Sigma^{1/2})$ , and apply the technique to that set. ", "page_idx": 3}, {"type": "text", "text": "Unknown covariance. Moving to the case of unknown covariance, for illustration, we focus on the simpler, yet fundamental, case where the covariance matrix is diagonal, so that $\\Sigma=\\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{d}^{2})$ . First, the folklore algorithm described in the known-covariance setting, which adds spherical Gaussian noise, does not require knowledge of the covariance but only of its trac\u221ae. The trace can be privately learned with $n=\\tilde{O}(1/\\varepsilon)$ samples. Second, we note that with $n=\\tilde{O}(\\sqrt{d}/\\varepsilon)$ samples, it is possible to learn each $\\sigma_{i}$ up to a multiplicative constant [41]. This allows us to apply the algorithm\u221a with known covariance from Theorem 1.1. However, the first step in this approach still requires $\\Omega({\\sqrt{d}})$ samples. ", "page_idx": 3}, {"type": "text", "text": "Our approach is to combine these two methods. We privately learn the largest $k\\approx\\varepsilon^{2}n^{2}$ variances, and their indices. This is done using the sparse vector technique [23] and can be achieved with $n$ samples. We use the known-covariance algorithm to estimate the mean in these top $k$ coordinates, with the same error bound as in the known-covariance setting. For the mean at the remaining coordinates, we use the algorithm that only requires knowledge of the trace of the covariance. The error of the latter estimate is $\\alpha_{\\mathrm{bot}}\\approx\\sqrt{d}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}/(n\\varepsilon)$ , where $\\pmb{\\sigma}_{\\mathrm{bot}}$ is the vector containing the lowest $d-k$ variances. The first observation is that $\\pmb{\\sigma}$ contains at least $k$ entries as large as $\\|\\sigma_{\\mathrm{bot}}\\|_{\\infty}$ , hence, $\\|\\pmb{\\sigma}\\|_{1}\\geq k\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}$ . Then, by H\u00f6lder\u2019s inequality, $\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}\\leq\\sqrt{\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{1}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}}$ . Substituting $k$ yields $\\alpha_{\\mathrm{bot}}\\approx\\sqrt{d}\\|\\pmb{\\sigma}\\|_{1}/(\\varepsilon^{2}n^{2})$ , which implies the desired sample complexity bound in Theorem 1.3. ", "page_idx": 3}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Differentially private Gaussian mean estimation. Smith [58] proposed estimators for asymptotically normal statistics with optimal convergence rates under a certain range of parameters. The optimal sample complexity for learning the mean of a Gaussian with known covariance in Mahalanobis norm under $(\\varepsilon,\\delta)$ -DP is $n\\gtrsim\\bar{d}/\\alpha^{2}+d/(\\alpha\\varepsilon)+\\log(1/\\delta)/\\varepsilon$ and has been established in a series of works [18, 39, 3, 46], starting from [41] in the univariate setting. Given the covariance matrix $\\Sigma$ , the Mahalanobis distance between the estimate $\\tilde{\\mu}$ and the true mean $\\mu$ is defined as: $\\|\\tilde{\\mu}-\\mu\\|_{\\Sigma}=\\|\\Sigma^{-1/2}(\\tilde{\\mu}-\\mu)\\|_{2}$ . When $\\Sigma=I_{d}$ , the Mahalanobis and Euclidean norms coincide. The Mahalanobis distance yields an affine-invariant accuracy guarantee, and $\\|\\tilde{\\mu}-\\mu\\|_{\\Sigma}\\leq\\alpha$ immediately implies $\\|\\tilde{\\mu}-\\mu\\|_{2}\\leq\\alpha\\sqrt{\\|\\Sigma\\|_{2}}$ . \u221aHowever, the power of the Mahalanobis gua\u221arantee is overshadowed by the fact that even for $\\alpha\\,=\\,{\\sqrt{d}}$ , a large sample size, namely $n\\,=\\,\\Omega({\\sqrt{d}})$ , is required, which excludes the high-dimensional scenario we are interested in.2 Furthermore, confidence sets induced by guarantees in the Euclidean distance have the pleasant property of being more easily constructible. ", "page_idx": 4}, {"type": "text", "text": "Beyond global sensitivity. There are several lines of work within differential privacy which aim to satisfy some form of instance-adaptive accuracy guarantee, as we do. General purpose frameworks which aim to privately estimate a statistic of the data, with error which adapts to \u201cgood\u201d data sets, include propose-test-release [22], smooth-sensitivity [52], and Lipschitz extensions [12, 42]. Our method follows the same high-level structure as propose-test-release. The latter has been combined with robust estimators to yield optimal private learners for several tasks [13, 47]. Even more generally, [6, 35] give a black-box method which transforms robust estimators to private ones via the inverse-sensitivity mechanism [5] (see [59] for a discussion on inverse-sensitivity). As there exist optimal robust estimators for the mean of anisotropic Gaussians [50], this would be a viable approach, but the volumetric analysis of the transformation involves terms which depend on the dimension. Tsfadia et al. [63] propose a flitering method which yields private aggregators whose error adapts to the diameter of the input data set. It is their method that we utilize for our upper bounds. A series of works formalize instance-optimality for private estimation of empirical [5, 37, 20] or population [49, 7] quantities. These are all generally well-suited to our setting but either do not adapt to high dimensions, or a direct application would require $n\\gtrsim\\sqrt{d\\operatorname{tr}(\\Sigma)}/(\\alpha\\varepsilon)$ . ", "page_idx": 4}, {"type": "text", "text": "Nikolov and Tang [51] study instance-optimality specifically for Gaussian noise mechanisms, albeit for data that belong in a bounded convex set. Although this is not the case for Gaussian data, it is worth noting that our error rates match those of [51], which hold for arbitrary distributions over $K$ , when the bounded set is $K=\\mu+\\Sigma^{1/2}\\mathcal{B}^{d}(1)$ . Privately learning $K$ however would require more samples. ", "page_idx": 4}, {"type": "text", "text": "Privately learning nuisance parameters. Karwa and Vadhan [41] learn (a constant multiple of) the variance of a univariate Gaussian using $n=\\tilde{O}\\left(\\log(1/\\delta)/\\varepsilon\\right)$ samples. In high dimensions, privately learning the covariance matrix of a Gaussian in spectral norm requires $n\\gtrsim d^{3/2}$ samples [39, 40], which is more than one needs to learn the mean under known covariance. Brown et al. [13] avoid the bottleneck of private covariance estimation, showing that the sample complexity of Gaussian mean estimation under known covariance with respect to Mahalanobis distance can in fact be matched, even when the covariance is unknown. Their tools also follow the propose-test-release approach and could be modified to fit our setting, but the privacy analysis would still require $n\\gtrsim d$ . Singhal and Steinke [57] learn a subspace in which the majority of the data lie, which could be used as a pre-processing step, followed by projection. However, to recover the set of top $k$ eigenvectors, they require that there exists a large gap between the two consecutive variances, that is, $\\sigma_{k}\\geq\\mathrm{poly}(d)\\sigma_{k+1}$ ", "page_idx": 4}, {"type": "text", "text": "Comparison with [8]. The paper by Aum\u00fcller et al. [8] is the closest work to ours, aiming to find sample-efficient mean estimators with respect to the Euclidean norm in the anisotropic case. Their work focuses on the less general cas\u221ae of diagonal, (almost) known covariance. The sample complexity of their estimator requires $n\\gtrsim{\\sqrt{d}}$ , whereas our estimator for the known covariance case is dimension-independent, and, as we prove, optimal. However, the focus in [8] is on estimators that satisfy the stricter privacy guarantee of $\\rho$ -zCDP, which forces the need for dimension-dependent sample size. This is the key contrast with our dimension-free philosophy. As an interesting distinction, ", "page_idx": 4}, {"type": "text", "text": "Aum\u00fcller et al. [8] provide accuracy guarantees with respect to the $\\ell_{p}$ norm (the upper bounds) for slightly more general classes of so-called well-concentrated distributions, which include subgaussians. It would be interesting to establish optimal private mean estimation bounds with respect to general $\\ell_{p}$ norms. In fact, the optimal non-private sample complexity of Gaussian mean estimation, with matching upper and lower bounds, with respect to general norms has been established only recently, and it depends on the Gaussian mean width of the set induced by the unit dual ball of the norm [19]. ", "page_idx": 5}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We write $[n]=\\{1,\\ldots,n\\}$ , log denotes the natural logarithm, and $B^{d}(c,r)$ denotes the $d_{\\cdot}$ -dimensional Euclidean ball with radius $r$ and center $c$ . We omit $c$ if $c=0$ . ", "page_idx": 5}, {"type": "text", "text": "We introduce differential privacy here. We say that $X,X^{\\prime}$ are neighboring data sets if either $\\exists j\\in[|X|]$ such that $X^{\\prime}=\\mathbf{\\bar{\\boldsymbol{X}}}\\setminus X^{\\odot}$ or $\\exists j\\in[|X^{\\prime}|]$ such that $X=X^{\\prime}\\backslash X^{\\prime(j)}$ .3 Differentially private algorithms have indistinguishable output distributions on neighboring data sets. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.1 $(\\varepsilon,\\delta)$ -indistinguishability). Two distributions $P,Q$ over domain $\\mathcal{W}$ are $(\\varepsilon,\\delta)$ - indistinguishable, denoted by $P\\approx_{\\varepsilon,\\delta}Q$ , if for any measurable subset $W\\subseteq\\mathcal{W}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{w\\sim P}[w\\in W]\\leq e^{\\varepsilon}\\operatorname*{Pr}_{w\\sim Q}[w\\in W]+\\delta\\quad a n d\\quad\\operatorname*{Pr}_{w\\sim Q}[w\\in W]\\leq e^{\\varepsilon}\\operatorname*{Pr}_{w\\sim P}[w\\in W]+\\delta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Definition 2.2 (Differential Privacy [25]). $A$ randomized algorithm $\\mathcal{A}\\colon\\mathcal{X}^{*}\\ \\rightarrow\\ \\mathcal{W}$ is $(\\varepsilon,\\delta)$ - differentially private $i f$ for all neighboring data sets $X,X^{\\prime}$ we have $\\mathcal{A}(X)\\approx_{\\varepsilon,\\delta}\\mathcal{A}(X^{\\prime})$ . We say that algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies pure differential privacy if it satisfies the definition for $\\delta=0$ . ", "page_idx": 5}, {"type": "text", "text": "Differential privacy satisfies several useful properties, such as post-processing and composition [25, 27]. For further details and guarantees of standard DP mechanisms, see Section A. ", "page_idx": 5}, {"type": "text", "text": "Our estimators will use the BasicFilter procedure of Tsfadia et al. [63], whose detailed definition is presented in Section 3 . They provide a framework which allows us to extend an algorithm which is private only for \u201ceasy\u201d pairs of data sets, to an algorithm that is private for any worst-case pair. \u201cEasy\u201d pairs of data sets are modelled with respect to a predicate $f$ between two data points: ", "page_idx": 5}, {"type": "text", "text": "Definition 2.3 ( $f$ -friendly, Def. 1.1 [63]). Let $X$ be a data set over $\\mathcal{X}$ and let $f:\\mathcal{X}^{2}\\to\\{0,1\\}$ be a predicate. We say $X$ is $f$ -friendly if for all $x,y\\in X$ there exists $z\\in\\mathcal{X}$ such that $f(x,z)=$ $f(z,{\\bar{y}})=1$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 2.4 ( $f$ -friendly DP, Def. 1.3 [63]). An algorithm $\\boldsymbol{\\mathcal{A}}$ is called $f$ -friendly $(\\varepsilon,\\delta)$ -DP if for any neighboring data sets $X,X^{\\prime}$ , such that $X\\cup X^{\\prime}$ is $f$ -friendly, $\\mathcal{A}(X)\\approx_{\\varepsilon,\\delta}\\mathcal{A}(X^{\\prime})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.5 (Theorem 4.11 [63]). Let $\\boldsymbol{\\mathcal{A}}$ be an $f$ -friendly $(\\varepsilon,\\delta)$ -DP algorithm. Given data set $X$ , let $\\pmb{v}=\\mathrm{BasicFilter}(X,f,\\alpha=0)$ and $C(X)=\\{X^{(j)}\\}_{\\{j:\\ v_{j}=1\\}}$ . Then $B(X):=A(C(X))$ is $(2(e^{\\varepsilon}-1)\\varepsilon,2e^{\\varepsilon+2(e^{\\varepsilon}-1)}\\delta){-}D P.$ ", "page_idx": 5}, {"type": "text", "text": "We assume data are drawn from subgaussian distributions, which include Gaussians. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.6 (Subgaussian distributions). The random vector $X$ with mean $\\mu$ is subgaussian with $a$ p.s.d. covariance matrix proxy \u03a3 if for any \u03bb and any v \u2208Rd, E e\u03bb\u27e8X\u2212\u00b5,v\u27e9\u2264e\u03bbv \u03a3v/2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.7 (Norm of the subgaussian vector [36, 65]). Let $X=(X^{(1)},\\ldots,X^{(n)})$ be drawn i.i.d. from the subgaussian distribution with mean $\\mu$ and covariance-proxy $\\Sigma$ . With probability at least $1-\\beta$ , $\\begin{array}{r}{\\|\\frac{1}{n}\\sum_{j=1}^{n}X^{(j)}-\\mu\\|_{2}\\le\\sqrt{\\mathrm{tr}(\\Sigma)/n}+\\sqrt{2\\|\\Sigma\\|_{2}\\log(1/\\beta)/n}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "3 Nearly-matching upper and lower bounds under known covariance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Algorithm 1 proceeds in two simple steps. The first step filters out outliers so that all remaining pairs of data points satisfy the re-scaled distance predicate $\\mathrm{dist}_{M,\\lambda}$ and, assuming enough data points remain, the second step releases their empirical mean along with appropriate Gaussian noise. ", "page_idx": 5}, {"type": "text", "text": "We retrieve the folklore result, by taking $M=I_{d},\\lambda\\approx\\sqrt{\\mathrm{tr}(\\Sigma)}$ , which is known (otherwise, can be easily privately estimated as in Section 4). The filtering then guarantees that all pairs of points ", "page_idx": 5}, {"type": "text", "text": "are within distance $\\sqrt{\\mathrm{tr}(\\Sigma)}$ , and adds spherical Gaussian noise with covariance $\\mathrm{tr}(\\Sigma)I_{d}/(\\varepsilon^{2}n^{2})$ . To retrieve the optimal bound, take $M=\\Sigma$ , which splits the privacy budget unevenly among coordinates. Then, $\\lambda\\approx\\sqrt{\\mathrm{tr}(\\Sigma^{1/2})}$ and the Gaussian noise has covariance $\\mathrm{tr}(\\Sigma^{1/2})\\Sigma^{1/2}/(\\varepsilon^{2}n^{2})$ , as in [8]. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Private Re-scaled Averaging: $\\operatorname{Avg}_{M,\\lambda,\\varepsilon,\\delta}(X)$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Require: Data set $X\\;=\\;(X^{(1)},\\ldots,X^{(n)})^{T}\\;\\in\\;\\mathbb{R}^{n\\times d}$ . Privacy parameters: $\\varepsilon,\\delta~>~0$ . Failure   \nprobability $\\beta>0$ . Symmetric invertible matrix $M$ . Parameter $\\lambda$ .   \n1: Let $\\mathrm{dist}_{M,\\lambda}(x,y)=\\mathrm{\\bar{1}}\\{\\|M^{-1/4}(x-y)\\|_{2}\\le\\lambda\\}$ .   \n2: v = Ba $\\mathrm{sicFilter}(X,\\mathrm{dist}_{M,\\lambda},\\alpha=0)$ .   \n3: Let C = {X(j)}{j: vj=1}.   \n4: Compute $\\begin{array}{r}{\\hat{n}_{C}=|C|-\\frac{\\log(1/\\delta)}{\\varepsilon}+z}\\end{array}$ where $\\begin{array}{r}{z\\sim\\mathrm{Lap}(\\frac{1}{\\varepsilon})}\\end{array}$ .   \n5: if $|C|=0$ or $\\hat{n}_{C}\\le0$ then   \n6: return $\\bot$ .   \n7: return $\\begin{array}{r}{\\hat{\\mu}=\\frac{1}{|C|}\\sum_{x\\in C}x+\\eta}\\end{array}$ where $\\begin{array}{r}{\\eta\\sim\\mathcal{N}\\left(0,\\frac{8\\log(1.25/\\delta)\\lambda^{2}}{\\varepsilon^{2}\\hat{n}_{C}^{2}}M^{1/2}\\right)}\\end{array}$   \n8: procedure BasicFilter $(X,f,\\alpha)$ \u25b7Algorithm 4.3 from [63].   \n9: for $j=1,\\dots,n$ do ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Let $\\varepsilon\\,\\in\\,(0,10),\\delta\\,\\in\\,(0,1),\\alpha\\,>\\,0,\\beta\\,\\in\\,(0,1)$ . Algorithm $^{\\,l}$ is $(\\varepsilon,\\delta)$ -differentially private. Let $X$ be a data set of size $n_{i}$ , drawn from a subgaussian distribution with covariance-proxy $\\Sigma$ and mean $\\mu$ . Given $M=\\Sigma,$ , $\\lambda\\geq\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}+2\\sqrt{2\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(\\frac{n}{\\beta})},$ , with probability at least $1-\\beta$ , Algorithm $^{\\,l}$ returns $\\hat{\\mu}$ such that $\\|\\hat{\\mu}-\\mu\\|_{2}\\leq\\alpha$ , as long as ", "page_idx": 6}, {"type": "equation", "text": "$$\nn=\\tilde{\\Omega}\\left(\\frac{\\operatorname{tr}(\\Sigma)+\\|\\Sigma\\|_{2}\\log\\frac{1}{\\beta}}{\\alpha^{2}}+\\frac{\\operatorname{tr}(\\Sigma^{1/2})\\sqrt{\\log\\frac{1}{\\delta}}}{\\alpha\\varepsilon}+\\frac{\\sqrt{\\|\\Sigma\\|_{2}\\log\\frac{1}{\\delta}}\\log\\frac{1}{\\beta}}{\\alpha\\varepsilon}+\\frac{\\log\\frac{1}{\\delta\\beta}}{\\varepsilon}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tilde{\\Omega}$ hides constants and a log factor of the third term multiplied with itself. ", "page_idx": 6}, {"type": "text", "text": "The theorem holds more generally for any symmetric invertible $M$ , and $\\lambda$ satisfying the assumptions.   \nWe sketch the proof of Theorem 3.1 next. All remaining details are in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. We start with the accuracy analysis. First we show that the original dataset $X$ passes through BasicFilter (i.e., $C\\,=\\,X)$ ) with high probability. It suffices to show that each pair $j\\neq$ $k\\,\\in\\,[n]$ , satisfies $\\mathrm{dist}_{M,\\lambda}(X^{(j)},X^{(k)})\\,=\\,1$ with probability $1-\\beta/n^{2}$ . Observe that for $j\\neq k$ $M^{-1/4}(X^{(j)}\\mathrm{~-~}X^{(k)})$ is subgaussian with mean 0 and covariance proxy $2M^{-1/4}\\Sigma M^{-1/4}$ . By Lemma 2.7 and our setting of $\\lambda$ , indeed $\\lVert M^{-1/4}(X^{(j)}-X^{(k)})\\rVert_{2}\\leq\\lambda$ for each pair with probability $1-\\beta/n^{2}$ . We condition on $C=X$ . With high probability by the CDF of the Laplace distribution and since $|C|\\,=\\,n\\,=\\,\\Omega(\\log(1/\\delta\\beta)/\\varepsilon)$ , $\\bar{n_{C}}\\,=\\,\\Omega(n)$ . Thus, the algorithm does not abort, and returns estimate $\\hat{\\mu}$ . It remains to upper bound the total error of $\\hat{\\mu}$ . This is at most the error of the empirical mean plus the error due to noise $\\lVert\\boldsymbol{\\eta}\\rVert_{2}$ . By Lemma 2.7, with high probability, the former is ${\\tilde{O}}({\\sqrt{\\operatorname{tr}(\\Sigma)/n}})$ and the latter ${\\tilde{O}}(\\lambda{\\sqrt{\\operatorname{tr}(M^{1/2})}}/(\\varepsilon n))$ . Substituting $M=\\Sigma$ , and the value for $\\lambda$ , the total error becomes ${\\tilde{O}}({\\sqrt{\\operatorname{tr}(\\Sigma)/n}}+\\operatorname{tr}(\\Sigma^{1/2})/(\\varepsilon n))$ , which yields the stated sample complexity. ", "page_idx": 6}, {"type": "text", "text": "The privacy analysis follows the steps of [63, Claim 3.4]. By Theorem 2.5, it suffices to show that lines 4-7 of Algorithm 1, namely, $\\boldsymbol{\\mathcal{A}}$ , are $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly DP. Consider neighboring inputs $X,X^{\\prime}$ , differing in the $n$ -th data point w.l.o.g., that is $X^{\\prime}=X\\setminus X^{(n)}$ . Since $||X|-|X^{\\prime}||=1$ , by the guarantees of the Laplace mechanism, the r.v.s $\\hat{n}_{X},\\hat{n}_{X^{\\prime}}$ in Line 4 are $(\\varepsilon,0)$ -indistinguishable. Moreover, they both satisfy $\\hat{n}_{X},\\hat{n}_{X}^{\\prime}<|X|$ with probability $1\\!-\\!\\delta/2>1/2$ . Conditioning on this event for the remainder of the sketch, we can fix $\\hat{n}_{X}=\\hat{n}_{X}^{\\prime}=\\hat{n}<|X|$ , for some value $\\hat{\\boldsymbol{n}}$ . If $\\hat{n}\\leq0$ , both runs abort. Otherwise, it suffices to show that Line 7 adds sufficient noise to maintain privacy. By post-processing, since $M$ is not data-dependent, this is equivalent to ensuring that $\\begin{array}{r}{\\mathcal{N}(M^{-1/4}\\frac{1}{|X|}\\sum_{i=1}^{|X|}\\Bar{X^{(i)}},v^{2}\\Bar{I_{d}})\\approx_{\\varepsilon,\\delta}}\\end{array}$ $\\begin{array}{r}{\\mathcal{N}(M^{-1/4}\\frac{1}{|X|-1}\\sum_{i=1}^{|X|-1}X^{(i)},v^{2}I_{d})}\\end{array}$ where $v\\,=\\,(2\\lambda/\\hat{n})(\\sqrt{2\\log(1.25/\\delta)}/\\varepsilon)$ . This is true by the guarantees of the Gaussian mechanism applied to $\\begin{array}{r}{f(X)\\;=\\;M^{-1/4}\\sum_{i=1}^{|X|}X^{(i)}/|X|}\\end{array}$ , whose $\\ell_{2}$ - sensitivity for $\\mathrm{dist}_{\\Sigma,\\lambda}$ -friendly $X,X^{\\prime}$ can be upper bounded by $2\\lambda/|X|\\leq2\\lambda/\\hat{n}$ (since $0<\\hat{n}<|X|$ , by assumption). By composition, $\\boldsymbol{\\mathcal{A}}$ indeed satisfies $\\mathrm{dist}_{M,\\lambda}$ -friendly $(O(\\varepsilon),O(\\delta))\\!\\cdot\\!\\mathrm{I}$ DP. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We show that the sample complexity of Theorem 3.1 is optimal. We briefly explain our lower bound construction here. All remaining details are in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Proof Sketch of Theorem 1.2. Let $\\Sigma=\\mathrm{diag}(\\pmb{\\sigma}^{2})$ . Assume w.l.o.g. that $\\sigma_{1}^{2}\\,\\geq\\,.\\ldots\\,\\geq\\,\\sigma_{d}^{2}$ . Partition the set of coordinates into buckets $S_{k}\\;=\\;\\{i\\;\\in\\;[d]\\;:\\;\\sigma_{i}\\;\\in\\;\\sigma_{1}\\,\\cdot\\,(2^{-k},2^{-k+1}]\\}$ , $\\forall k\\;\\in\\;[\\log(d)]$ and $S_{\\log(d)+1}\\,=\\,[d]\\,\\setminus\\bigcup_{k\\in[\\log(d)]}S_{k}$ . We have that $\\begin{array}{r}{\\sum_{k\\in[\\log(d)+1]}\\sum_{i\\in S_{k}}\\sigma_{i}\\,=\\,\\|\\pmb{\\sigma}\\|_{1}}\\end{array}$ . Consider the bucket $S$ which contributes the most to this sum and let $\\sigma_{S}$ be the maximum variance in this bucket. It must be that |S| \u2265 (log(\u2225d\u03c3)\u2225+11)\u03c3S . The lower bound of [39, Theorem 6.5] for isotropic Gaussians, implies that any $(\\varepsilon,\\delta)$ -private mean estimator which returns, with constant probability, an estimate $\\hat{\\mu}_{S}$ with error $\\alpha$ for the coordinates in $S$ (note that they are all within a factor of 2), requires $n=\\Omega(|S|\\sigma_{S}/(\\alpha\\varepsilon\\log(d)))\\,=\\,\\Omega(\\|\\pmb{\\sigma}\\|_{1}/(\\alpha\\varepsilon\\log^{2}(d)))$ samples. As an estimator for the $d_{\\cdot}$ -dimensional Gaussian mean restricted to $S$ , would give us such a $\\hat{\\mu}_{S}$ , the statement follows. ", "page_idx": 7}, {"type": "text", "text": "4 Handling unknown covariance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we consider the case of unknown covariance. First, recall that $\\Omega(d^{3/2})$ samples are required to privately learn the covariance matrix in spectral norm [40], which is prohibitive. The lower bound instance is an almost-isotropic Gaussian, which means that anisotropic distributions may circumvent it. Still, the superlinear dependence on $d$ implies that this approach will yield suboptimal sample complexity for mean estimation. Avoiding private covariance estimation, Brown et al. [13] propose a \u201ccovariance-aware\u201d private mean estimator which returns the mean with Gaussian noise which scales with the empirical covariance matrix of the data set $\\Sigma_{X}$ , as $\\mathcal{N}(0,\\lambda_{M}^{2}\\Sigma_{X}/(\\varepsilon^{2}n^{2}))$ for appropriate factor $\\lambda_{M}^{2}$ . Since adding data-dependent noise can break privacy, a pre-processing step is required to ensure that no outliers exist in the data set with respect to the empirical covariance, roughly ensuring that $\\lVert\\Sigma_{X}^{-1/2}(X^{(k)}-X^{(j)})\\rVert_{2}\\,\\leq\\,\\lambda_{M}$ , for all $j\\,\\neq\\,k\\,\\in\\,[n]$ . In our case, to maintain the accuracy guarantee of the known-covariance case, the Gaussian noise should be N(0, \u03bb2\u03a31X/ 2/(\u03b52n2)) and all data points should satisfy $\\lVert\\Sigma_{X}^{-1/4}(X^{(k)}-X^{(j)})\\rVert_{2}\\leq\\lambda$ . Note that $n\\geq\\mathrm{tr}(\\Sigma)/\\|\\Sigma\\|_{2}$ samples suffice for the empirical covariance to be close to the true covariance $\\Sigma$ in spectral norm [43], so applying the algorithm from [13] could maintain accuracy while still allowing a dimension-free sample complexity. Unfortunately, we still cannot use this approach because $n\\geq d$ samples are required for the privacy analysis to go though, namely, for neighboring data sets $X,X^{\\prime}$ it holds that $\\bar{\\mathcal{N}}(0,\\Sigma_{X}^{1/2})\\approx_{\\varepsilon,\\delta}\\bar{\\mathcal{N}}(0,\\Sigma_{X^{\\prime}}^{1/2})$ for $\\varepsilon\\approx d/n$ , which forces us to take $n\\geq d/\\varepsilon$ samples. The same is true for the follow-up works of [14, 44] which give polynomial-time versions of this algorithm with slightly better statistical guarantees. ", "page_idx": 7}, {"type": "text", "text": "Luckily, our accuracy guarantee does not require the variance estimate in all directions to be accurate. For example, consider all directions with variance at most $\\|\\Sigma\\|_{2}/d$ . Adding spherical Gaussian noise to these directions maintains a dimension-free error, without requiring tighter estimates for their variance. Thus, on a high level, our approach for mean estimation in the unknown covariance case is to identify and estimate as many of the top variances as our sample size allows, which turns out to be $k\\approx\\varepsilon^{2}n^{\\tilde{2}}$ , while adding spherical Gaussian noise to the remaining ones. ", "page_idx": 7}, {"type": "text", "text": "We sketch the proof of the following theorem. All remaining details are in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. Let parameters $\\varepsilon,\\delta\\;\\in\\;(0,1)$ . Let $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{\\mu}},\\ensuremath{\\boldsymbol{\\Sigma}})^{n}$ with unknown covariance $\\Sigma$ . Algorithm 2 is $(\\varepsilon,\\delta)$ -differentially private and, with probability $1-\\beta$ , returns an estimate $\\hat{\\mu}$ such ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Private Re-scaled Averaging with Unknown Covariance ", "page_idx": 8}, {"type": "text", "text": "Require: Data set $\\boldsymbol{X}\\,=\\,(\\boldsymbol{X}^{(1)},\\ldots,\\boldsymbol{X}^{(2n)})^{T}\\,\\in\\,\\mathbb{R}^{2n\\times d}$ . Privacy parameters: $\\varepsilon,\\delta\\;>\\;0$ . Failure probability $\\beta>0$ . ", "page_idx": 8}, {"type": "text", "text": "1: Require $\\begin{array}{r}{n=\\Omega\\left(\\log^{2}(d)+\\log(\\frac{1}{\\delta\\beta})\\sqrt{\\log(\\frac{1}{\\delta})}\\log(d)/\\varepsilon\\right)}\\end{array}$ .   \n2: Let $k\\leftarrow\\varepsilon^{2}n^{2}/\\left(\\log^{2}(d)\\log(1/\\delta)\\log^{2}(1/\\delta\\beta)+\\log(\\varepsilon n)\\right)$ and $\\ell\\gets\\Theta(\\log(d))$ .   \n3: Split the dataset into two equal halves: $X^{\\mathrm{var}}$ and $X^{\\mathrm{mean}}$ .   \n4: Split $X^{\\mathrm{var}}$ into $\\textstyle m=\\left\\lfloor{\\frac{n}{2\\ell}}\\right\\rfloor$ groups of size $2\\ell$ . Define $X^{(j,r)}$ as the $r$ -th sample in the $j$ -th group.   \n5: for each group $j=1$ to $m$ and each dimension $i=1$ to $d$ do   \n6: Define $\\begin{array}{r}{V_{i}^{(j)}=\\frac{1}{2\\ell}\\sum_{r=1}^{\\ell}(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)})^{2}}\\end{array}$ .   \n7: $\\begin{array}{r}{\\hat{R}\\gets\\mathrm{FindKthLargestVariance}_{\\varepsilon,\\delta}(V,k).}\\end{array}$ .   \n8: $I_{\\mathrm{top}}\\gets\\mathrm{TopVar}_{\\varepsilon,\\delta}(V,\\hat{R}/8,k)$ and $I_{\\mathrm{bot}}\\leftarrow[d]\\setminus I_{\\mathrm{top}}$ .   \n9: for each $i\\in I_{\\mathrm{top}}$ do   \n10: Estimate $\\hat{\\Sigma}_{i i}\\leftarrow\\mathrm{VarianceSum}_{\\varepsilon^{\\prime},\\delta^{\\prime},\\beta^{\\prime}}(V,\\{i\\})$ for \u221ak lo\u03b5g(1/\u03b4), \u03b4\u2032 \u2190 \u03b4k, \u03b2\u2032 \u2190 \u03b2k .   \n11: Compute $\\begin{array}{r}{\\hat{S}_{\\mathrm{bot}}\\leftarrow\\mathrm{VarianceSum}_{\\varepsilon,\\delta,\\beta}(V,I_{\\mathrm{bot}}).}\\end{array}$ .   \n12: $\\begin{array}{r l}&{\\hat{\\mu}_{\\mathrm{top}}\\leftarrow\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}(X^{\\mathrm{mean}}[I_{\\mathrm{top}}]),\\mathrm{where~}M=\\mathrm{diag}(\\{\\hat{\\Sigma}_{i i}\\}_{I_{\\mathrm{top}}}),\\lambda=\\tilde{\\Theta}\\left(\\sqrt{\\sum_{i\\in I_{\\mathrm{top}}}\\hat{\\Sigma}_{i i}^{1/2}}\\right)\\!.}\\\\ &{\\hat{\\mu}_{\\mathrm{bot}}\\leftarrow\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}(X^{\\mathrm{mean}}[I_{\\mathrm{bot}}]),\\mathrm{where~}M=I_{d},\\lambda=\\tilde{\\Theta}(\\sqrt{\\hat{S}_{\\mathrm{bot}}}).}\\end{array}$   \n13:   \n14: return $(\\hat{\\mu}_{\\mathrm{top}},\\hat{\\mu}_{\\mathrm{bot}})$ ", "page_idx": 8}, {"type": "text", "text": "that $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq\\alpha_{*}$ , as long as ", "text_level": 1, "page_idx": 8}, {"type": "equation", "text": "$$\nn=\\tilde{\\Omega}\\left(\\log^{2}(d)+\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}\\sqrt{\\log\\frac{1}{\\delta}}}{\\alpha\\varepsilon}+\\frac{d^{1/4}\\sqrt{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}}\\log^{5/4}(\\frac{1}{\\delta})\\log(d)}{\\sqrt{\\alpha}\\varepsilon}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the symbol \u2126\u02dchides multiplicative logarithmic factors in $1/\\beta$ . ", "page_idx": 8}, {"type": "text", "text": "Next, we describe Algorithm 2 and introduce some of its subroutines along with their guarantees. All omitted proofs are in Appendix D. Our algorithm receives a data set $X^{(1)},\\ldots,X^{(n)}$ , where each $X^{(i)}$ is a $d$ -dimensional vector distributed as $\\mathcal{N}(\\mu,\\Sigma)$ . The algorithm starts by splitting the dataset into $m=\\lfloor n/(2\\ell)\\rfloor$ groups each of size $2\\ell$ , where $\\ell=\\Theta(\\log d)$ . Denote the elements of each group $j$ by $X^{(j,1)},\\ldots,X^{(j,2\\ell)}$ . Within each group $j$ , for each coordinate $i$ , we compute an estimate $V_{i}^{(j)}$ for \u03a3ii: V (j) $\\begin{array}{r}{V_{i}^{(j)}=\\frac{1}{2\\ell}\\sum_{r=1}^{\\ell}(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)})^{2}}\\end{array}$ . For convenience, we define what it means for the V i(j)variables to provide a good estimate of the set of {\u03a3ii}i\u2208[d]. ", "page_idx": 8}, {"type": "text", "text": "Definition 4.2. Given variances $\\Sigma_{11},\\ldots,\\Sigma_{d d}$ and given a set of estimates, $V=\\{V_{i}^{(j)}\\}_{j\\in[m],i\\in[d]},$ we say that $V$ is valid $i f\\,|\\,\\{j:\\forall i\\in[d]$ , $\\Sigma_{i i}/2\\leq V_{i}^{(j)}\\leq2\\Sigma_{i i}\\}|\\geq4m/5.$ . ", "page_idx": 8}, {"type": "text", "text": "Proof sketch of Theorem 4.1. For ease of notation, we assume that $\\Sigma=\\mathrm{diag}(\\pmb{\\sigma}^{2})$ . We start from the accuracy analysis. Assume $n$ satisfies the sample complexity bound of Eq. (4). By Chernoff bound, since $\\ell=\\mathsf{\\bar{\\Theta}}(\\log(d))$ and $m=\\Omega(\\log(1/\\beta))$ , with probability $1-\\beta,V$ is valid. We use the estimates $V_{i}^{(j)}$ as inputs to private procedures: FindKthLargestVariance $(V,k)$ (compute the $k$ -th largest variance up to a multiplicative constant), ${\\mathrm{VarianceSum}}(V,I)$ (compute the sum $\\sum_{i\\in I}\\sigma_{i}^{2}$ up to a multiplicative constant), and ${\\mathrm{TopVar}}(V,R,k)$ (identify the indices of the at-most- $k$ largest variances $\\sigma_{i}^{2}\\geq R)$ . The first two tasks can be implemented by the Stable Histogram algorithm [16] if $m=\\Omega(\\log(1/\\delta)/\\varepsilon)$ . TopVar can be implemented via the Sparse Vector technique [26, 54, 31], if $m=\\Omega(\\sqrt{k\\log(1/\\delta)}\\log(d/\\beta)/\\sqrt{\\varepsilon n})$ . Both are satisfied for the given $m=n/2\\ell$ . With these procedures, the algorithm privately learns the $k$ -th largest variance $R$ , identifies the top $k$ coordinates, $I_{\\mathrm{top}}$ , and learns estimates $\\{\\hat{\\sigma}_{i}\\}_{I_{\\mathrm{top}}}$ that are accurate up to a multiplicative constant. ", "page_idx": 8}, {"type": "text", "text": "Next, we estimate the mean $\\mu$ in the coordinates $I_{\\mathrm{top}}$ separately from $I_{\\mathrm{bot}}:=[d]\\setminus I_{\\mathrm{top}}$ , denoted by $\\mu_{\\mathrm{top}}$ and $\\mu_{\\mathrm{bot}}$ , respectively. Denote vector $\\bar{\\sigma_{\\mathrm{top}}}\\,=\\,(\\{\\sigma_{i}\\}_{i\\in I_{\\mathrm{top}}})$ and ${\\pmb\\sigma}_{\\mathrm{bot}}~=~(\\{{\\bar{\\sigma}}_{i}\\}_{i\\in I_{\\mathrm{bot}}})$ . To estimate $\\mu_{\\mathrm{top}}$ , Algorithm 1, given input vectors $X^{(1)},\\ldots,X^{(n)}$ , restricted to coordinates $I_{\\mathrm{top}}$ , diagonal matrix $M$ , with $M_{i i}=\\hat{\\sigma}_{i}^{2}$ (assume that the rows and columns of $M$ are indexed by $I_{\\mathrm{top.}}$ ), and $\\lambda\\approx\\sqrt{||\\pmb{\\sigma}_{\\mathrm{top}}||_{1}}$ , returns an estimate $\\hat{\\mu}_{\\mathrm{top}}$ with error $\\alpha$ (since the sample complexity of Eq. (4) is larger than the one required by Theorem 3.1). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "To estimate $\\mu_{\\mathrm{bot}}$ , we do not know the variances, so we use the na\u00efve approach. We first call VarianceSum once again, to provide an estimate $\\hat{t}$ of $\\lVert\\pmb{\\sigma}_{\\mathrm{bot}}\\rVert_{2}$ up to a multiplicative constant. Given $\\hat{t}$ , we again call Algorithm 1, now for a $(d-k)$ -dimensional estimation problem. Given input vectors $X^{(1)},\\ldots,X^{(n)}$ , restricted to coordinates $I_{\\mathrm{{bot}}}$ , matri\u221ax $M=I_{d-k}$ , and $\\lambda\\approx\\hat{t}$ , Algorithm 1 returns an estimate $\\hat{\\mu}_{\\mathrm{bot}}$ with error $\\alpha$ as long as $n=\\tilde{\\Omega}(\\sqrt{d}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}/(\\alpha\\varepsilon))$ . By H\u00f6lder\u2019s inequality, $\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}\\leq\\sqrt{\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{1}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}}\\leq\\sqrt{\\|\\pmb{\\sigma}\\|_{1}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}}$ . By the guarantees of TopVar and FindKthLargestVariance, $\\|\\sigma_{\\mathrm{bot}}\\|_{\\infty}$ is smaller than the $k$ -th largest variance of $\\Sigma$ up to a multiplicative constant, which, in turn, must be smaller than $\\|\\pmb{\\sigma}\\|_{1}/k$ . Substituting this above, we obtain that it suffices for the stated sample complexity to additionally satisfy $n=\\tilde{\\Omega}(\\sqrt{d}\\|\\pmb{\\sigma}\\|_{1}/(\\sqrt{k}\\alpha\\varepsilon))$ , which can be confirmed by substituting the definition for $k$ . ", "page_idx": 9}, {"type": "text", "text": "The privacy guarantee follows directly by composition of $O(1)\\;(\\varepsilon,\\delta)$ -DP mechanisms. ", "page_idx": 9}, {"type": "text", "text": "Remark 1. We note that the sample complexity of Algorithm 2 in fact depends on the decay of the diagonal elements of $\\Sigma$ , and can yield improved bounds for easier instances. In particular, the error of the algorithm due to privacy is in the order of $\\|\\pmb{\\sigma}_{I_{\\mathrm{top}}}\\|_{1}/(\\varepsilon n)+\\sqrt{|I_{\\mathrm{bot}}|}\\|\\pmb{\\sigma}_{I_{\\mathrm{bot}}}\\|_{2}/(\\varepsilon n)$ . Thus, if $\\pmb{\\sigma}$ follows an exponential decay, i.e., the $i$ -th largest variance is proportional to $e^{-(i-1)}$ , or all $\\pmb{\\sigma}_{\\mathrm{bot}}$ variances are smaller than $\\|\\pmb{\\sigma}\\|_{1}^{\\bar{\\prime}}/d$ , then it suffices to learn only the top $k=\\log(d)$ variances and the error almost matches that of the known-covariance case, up to additional logarithmic factors in $d$ , $1/\\delta$ . Moreover, identifying easier instances is possible by computing a private histogram over $\\log(d)$ buckets of the form $(2^{-j},2^{-j+1}]\\|\\pmb{\\sigma}\\|_{\\infty}$ , given $n=\\tilde{O}(\\log(d)/\\varepsilon)$ samples [16, 41]. ", "page_idx": 9}, {"type": "text", "text": "Thus, we can determine special cases where the decay of $\\Sigma$ allows us to achieve the optimal rate of Theorem 1.1 even with unknown diagonal covariance. But without further assumptions, our algorithm has sample complexity that depends on $d^{1/4}$ . The question of the optimal sample complexity for mean estimation in the case of unknown covariance, which captures anisotropic distributions, remains open. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present $(\\varepsilon,\\delta)$ -differentially private mean estimators for subgaussian distributions with error $\\alpha$ as measured in Euclidean distance, with high probability, as long as the sample size is $n=$ $\\tilde{\\Theta}\\left(\\mathrm{tr}(\\Sigma)/\\alpha^{2}+\\mathrm{tr}(\\Sigma^{1/2})/(\\alpha\\varepsilon)\\right)$ . The sample complexity is thus dimension-independent when the covariance is highly anisotropic. We show that this is the optimal sample complexity for this task up to logarithmic factors. We also present an algorithm in the more challenging case of unknown covariance, whose sample complexity has improved dependence on the dimension, that is, $d^{1/4}$ . ", "page_idx": 9}, {"type": "text", "text": "In the known covariance case, the dependence on $\\log(1/\\delta)$ could possibly be decoupled from the $\\mathrm{tr}(\\Sigma^{1/2})/(\\alpha\\varepsilon)$ term. This is an artifact of the Gaussian noise added for privacy and can possibly be avoided using mean estimators based on the exponential mechanism, as in the spherical Gaussian case [13, 3, 35], but the volumetric arguments involved in their analysis incur factors dependent on $d$ , which seem hard to overcome. ", "page_idx": 9}, {"type": "text", "text": "A more interesting direction for future work is the case of unknown covariance. We can determine special cases where the decay of $\\Sigma$ allows us to achieve the optimal rate of Theorem 1.1 with unknown diagonal covariance. What is the appropriate norm in which one needs to learn $\\Sigma$ for the current known-covariance approach to be accurate, and how many samples are needed for this task privately? More generally, the optimal sample complexity of mean estimation in the unknown (even diagonal) covariance case for anisotropic distributions (possibly achieved by an algorithm which doesn\u2019t follow the same structure) is an open question. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank NeurIPS reviewers for suggestions on improving the clarity of this manuscript. This work was done while both LZ and YD were postdoctoral fellows in the Simons Institute for the Theory of Computing, funded by FODSI. We also wish to acknowledge funding from the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] John M. Abowd. The US Census Bureau adopts differential privacy. In ACM International Conference on Knowledge Discovery & Data Mining, KDD \u201918, pages 2867\u20132867, 2018.   \n[2] John M. Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck, Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett Moran, William Sexton, Matthew Spence, and Pavel Zhuravlev. The 2020 Census Disclosure Avoidance System TopDown Algorithm. Harvard Data Science Review, Special Issue 2, June 2022.   \n[3] Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of privately learning unbounded high-dimensional Gaussians. In Proceedings of the 32nd International Conference on Algorithmic Learning Theory, ALT \u201921, March 2021.   \n[4] Apple Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning Journal, 1(8), 2017. https://docs-assets.developer.apple.com/ml-research/papers/ learning-with-privacy-at-scale.pdf.   \n[5] Hilal Asi and John C. Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms. In Advances in Neural Information Processing Systems 33, NeuRIPS \u201920, pages 14106\u201314117, Dec 2020.   \n[6] Hilal Asi, Jonathan Ullman, and Lydia Zakynthinou. From robustness to privacy and back. In Proceedings of the 40th International Conference on Machine Learning, ICML \u201923, pages 1121\u20131146. PMLR, Jul 2023.   \n[7] Hilal Asi, John C. Duchi, Saminul Haque, Zewei Li, and Feng Ruan. Universally instanceoptimal mechanisms for private statistical estimation. In Proceedings of 37th Conference on Learning Theory, COLT \u201924, pages 221\u2013259. PMLR, Jul 2024.   \n[8] Martin Aum\u00fcller, Christian Janos Lebeda, Boel Nelson, and Rasmus Pagh. PLAN: Varianceaware private mean estimation, June 2023. https://arxiv.org/abs/2306.08745.   \n[9] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020.   \n[10] Amos Beimel, Hai Brenner, Shiva Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. Machine Learning, 94:401\u2013437, 2014. doi: 10.1007/s10994-013-5404-1.   \n[11] Andrea Bittau, \u00dalfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld. PROCHLO: Strong privacy for analytics in the crowd. In ACM Symposium on Operating Systems Principles, SOSP \u201917, pages 441\u2013459, 2017.   \n[12] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. Differentially private data analysis of social networks via restricted sensitivity. In 4th ACM Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 87\u201396, 2013.   \n[13] Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, and Lydia Zakynthinou. Covariance-aware private mean estimation without private covariance estimation. In Advances in Neural Information Processing Systems 34, NeurIPS \u201921, pages 7950\u20137964, 2021.   \n[14] Gavin Brown, Samuel B. Hopkins, and Adam Smith. Fast, sample-efficient, affine-invariant private mean and covariance estimation for subgaussian distributions. In Proceedings of the 36th Conference on Learning Theory, COLT \u201923, pages 5578\u20135579. PMLR, Jul 2023.   \n[15] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. In ACM Symposium on the Theory of Computing, STOC \u201914, pages 1\u201310, 2014.   \n[16] Mark Bun, Kobbi Nissim, and Uri Stemmer. Simultaneous private learning of multiple concepts. In Proceedings of the 7th ACM Conference on Innovations in Theoretical Computer Science, ITCS \u201916, pages 369\u2013380. ACM, 2016.   \n[17] Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in differential privacy. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201917, pages 1306\u20131325. SIAM, 2017.   \n[18] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In Advances in Neural Information Processing Systems 32, NeurIPS \u201919, pages 156\u2013167, 2019.   \n[19] Jules Depersin and Guillaume Lecu\u00e9. Optimal robust mean and location estimation via convex programs with respect to any pseudo-norms. Probability Theory and Related Fields, 183(3): 997\u20131025, 2022.   \n[20] Travis Dick, Alex Kulesza, Ziteng Sun, and Ananda Theertha Suresh. Subset-based instance optimality in private estimation. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR, 2023.   \n[21] Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Proceedings of the 22nd ACM Symposium on Principles of Database Systems, PODS \u201903, pages 202\u2013210. ACM, 2003.   \n[22] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the 41st ACM Symposium on Theory of Computing, STOC \u201909, pages 371\u2013380. ACM, 2009.   \n[23] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211\u2013407, 2014.   \n[24] Cynthia Dwork and Sergey Yekhanin. New efficient attacks on statistical disclosure control mechanisms. In Annual International Cryptology Conference, pages 469\u2013480. Springer, 2008.   \n[25] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Conference on Theory of Cryptography, TCC \u201906, pages 265\u2013284, 2006.   \n[26] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the 41st ACM Symposium on Theory of Computing, STOC \u201909, pages 381\u2013390. ACM, 2009.   \n[27] Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 51\u201360. IEEE, 2010.   \n[28] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In Proceedings of the 56th IEEE Annual Symposium on Foundations of Computer Science, FOCS \u201915, 2015.   \n[29] \u00dalfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In ACM Conference on Computer and Communications Security, CCS \u201914, 2014.   \n[30] Samuel Haney, Ashwin Machanavajjhala, John M Abowd, Matthew Graham, Mark Kutzbach, and Lars Vilhuber. Utility cost of formal privacy for releasing national employer-employee statistics. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1339\u20131354. ACM, 2017.   \n[31] Moritz Hardt and Guy Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis. In IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 61\u201370, 2014.   \n[32] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC \u201910, 2010.   \n[33] Florian Hartmann and Peter Kairouz. Distributed differential privacy for federated learning, 2023. https://research.google/blog/ distributed-differential-privacy-for-federated-learning/.   \n[34] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS Genetics, 4(8):e1000167, 2008.   \n[35] Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC \u201923, page 497\u2013506, 2023.   \n[36] Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17:1 \u2013 6, 2012.   \n[37] Ziyue Huang, Yuting Liang, and Ke Yi. Instance-optimal mean estimation under differential privacy. In Advances in Neural Information Processing Systems 34, pages 25993\u201326004, 2021.   \n[38] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In Proceedings of the 32nd International Conference on Machine Learning, ICML \u201915, pages 1376\u20131385, 2015.   \n[39] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high dimensional distributions. In Proceedings of the 32nd Annual Conference on Learning Theory, COLT \u201919. JMLR, 2019.   \n[40] Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal. New lower bounds for private estimation and a generalized fingerprinting lemma. In Advances in Neural Information Processing Systems 35, NeurIPS \u201923, pages 24405\u201324418, 2022.   \n[41] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. In Proceedings of the 9th Conference on Innovations in Theoretical Computer Science, ITCS \u201918, pages 44:1\u201344:9, 2018.   \n[42] Shiva P. Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Analyzing graphs with node differential privacy. In 10th IACR Theory of Cryptography Conference, TCC \u201913, pages 457\u2013476. Springer, 2013.   \n[43] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample covariance operators. Bernoulli, pages 110\u2013133, 2017.   \n[44] Rohith Kuditipudi, John C. Duchi, and Saminul Haque. A pretty fast algorithm for adaptive private mean estimation. In Proceedings of the 36th Conference on Learning Theory, COLT \u201923, pages 2511\u20132551. PMLR, Jul 2023.   \n[45] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302\u20131338, 2000.   \n[46] Xiyang Liu, Weihao Kong, Sham Kakade, and Sewoong Oh. Robust and differentially private mean estimation. In Advances in Neural Information Processing Systems 34, NeurIPS \u201921, pages 3887\u20133901, 2021.   \n[47] Xiyang Liu, Weihao Kong, and Sewoong Oh. Differential privacy and robust statistics in high dimensions. In Proceedings of the 35th Annual Conference on Learning Theory, COLT \u201922, pages 1167\u20131246. PMLR, Jul 2022.   \n[48] G\u00e1bor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. Foundations of Computational Mathematics, 19(5):1145\u20131190, 2019.   \n[49] Audra McMillan, Adam Smith, and Jon Ullman. Instance-optimal differentially private estimation, 2022. https://arxiv.org/abs/2210.15819.   \n[50] Arshak Minasyan and Nikita Zhivotovskiy. Statistically optimal robust mean and covariance estimation for anisotropic Gaussians, 2023. https://arxiv.org/abs/2301.09024.   \n[51] Aleksandar Nikolov and Haohua Tang. General Gaussian noise mechanisms and their optimality for unbiased mean estimation. In 15th ACM Conference on Innovations in Theoretical Computer Science, ITCS \u201924, 2024.   \n[52] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the 30th ACM Symposium on Theory of Computing, STOC, STOC \u201907, pages 75\u201384, 2007.   \n[53] Ryan Rogers, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar Kancha, Shraddha Sahay, and Parvez Ahammad. LinkedIn\u2019s audience engagements API: A privacy preserving data analytics system at scale, 2020. https://arxiv.org/abs/2002. 05839.   \n[54] Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC \u201910, pages 765\u2013774. ACM, June 2010.   \n[55] Sriram Sankararaman, Guillaume Obozinski, Michael I. Jordan, and Eran Halperin. Genomic privacy and limits of individual detection in a pool. Nature Genetics, 41(9):965\u2013967, 2009.   \n[56] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy (S&P), Oakland, 2017.   \n[57] Vikrant Singhal and Thomas Steinke. Privately learning subspaces. In Advances in Neural Information Processing Systems 34, NeurIPS \u201921, pages 1312\u20131324, 2021.   \n[58] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing, STOC \u201911, pages 813\u2013822. ACM, 2011.   \n[59] Thomas Steinke. Beyond global sensitivity via inverse sensitivity. DifferentialPrivacy.org, Sept 2023. https://differentialprivacy.org/inverse-sensitivity/.   \n[60] Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of preventing false discovery. In Proceedings of the 28th Annual Conference on Learning Theory, COLT \u201915, 2015.   \n[61] Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. In Proceedings of the 58th IEEE Symposium on Foundations of Computer Science, FOCS \u201917, 2017.   \n[62] David Tastuggine and Ilya Mironov. Introducing Opacus: A highspeed library for training PyTorch models with differential privacy. Facebook AI Blog, 2020. https://ai.facebook.com/blog/ introducing-opacus-a-high-speed-library-for-training-pytorch-modelswith-differential-privacy/.   \n[63] Eliad Tsfadia, Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer. FriendlyCore: Practical differentially private aggregation. In Proceedings of the 39th International Conference on Machine Learning, ICML \u201922, pages 21828\u201321863. PMLR, Jul 2022.   \n[64] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In IEEE Computer Security Foundations Symposium, CSF \u201918, pages 268\u2013282, 2018.   \n[65] Nikita Zhivotovskiy. Dimension-free bounds for sums of independent matrices and simple tensors via the variational principle. Electronic Journal of Probability, 29:1\u201328, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Standard DP mechanisms and properties ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition A.1 (Laplace distribution). For $v\\geq0$ , let $\\operatorname{Lap}(v)$ denote the Laplace distribution over $\\mathbb{R}$ , which has probability density function $\\textstyle p(z)={\\frac{1}{2\\sigma}}e^{-|z|/v}$ . From the CDF of the Laplace distribution, we get that $\\operatorname*{Pr}_{z\\sim\\mathrm{Lap}(v)}[z\\geq v\\log(1/2\\beta)]=\\beta$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.2 (Laplace Mechanism, [25]). Let $f:\\mathcal{X}^{*}\\to\\mathbb{R},$ , data set $X$ over $\\mathcal{X}$ , and privacy parameter $\\varepsilon$ . The Laplace Mechanism returns ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{f}(X)=f(X)+\\operatorname{Lap}(v),\\ w h e r e\\ v=\\Delta_{f}/\\varepsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\Delta_{f}=\\operatorname*{max}_{X\\sim X^{\\prime}}|f(X)-f(X^{\\prime})|$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3 ([25]). The Laplace Mechanism is $\\varepsilon$ -differentially private. ", "page_idx": 14}, {"type": "text", "text": "Definition A.4 (Gaussian Mechanism, [25]). Let $f:\\mathcal{X}^{*}\\to\\mathbb{R}^{d}$ , data set $X$ over $\\mathcal{X}$ , and privacy parameters $\\varepsilon,\\delta$ . The Gaussian Mechanism returns ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{f}(X)=f(X)+\\mathcal{N}(0,v^{2}I_{d}),\\;w h e r e\\;v=\\Delta_{f}\\sqrt{2\\log(1.25/\\delta)}/\\varepsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\Delta_{f}=\\operatorname*{max}_{X\\sim X^{\\prime}}\\|f(X)-f(X^{\\prime})\\|_{2}$ is the global $\\ell_{2}$ -sensitivity of $f$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.5 ([25]). The Gaussian Mechanism is $(\\varepsilon,\\delta)$ -differentially private. ", "page_idx": 14}, {"type": "text", "text": "Differential privacy is maintained under post-processing and degrades mildly under composition. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.6 (Composition, [25, 27, 38]). Let $M$ be an adaptive composition of $M_{1},\\ldots,M_{T}$ , that is, on input $X$ , $M(X):=M_{T}(X,M_{T-1}(X,\\dots,M_{2}(X,M_{1}(X))))$ ). Then ", "page_idx": 14}, {"type": "text", "text": "2. (Advanced composition) Let $\\varepsilon_{t}~>~0$ , $\\delta_{t}\\,\\in\\,[0,1]$ for $t\\,\\in\\,\\{1,\\ldots,T\\}$ , and $\\tilde{\\delta}\\,\\in\\,[0,1]$ . If $M_{1},\\ldots,M_{T}$ are $(\\varepsilon_{1},\\delta_{1}),\\dots,(\\varepsilon_{T},\\delta_{T})$ -differentially private respectively, then $M$ is $(\\Tilde{\\varepsilon}_{\\Tilde{\\delta}},\\Tilde{\\delta}+$ $\\textstyle\\sum_{t=1}^{T}\\delta_{t})$ -differentially private where $\\tilde{\\varepsilon}_{\\tilde{\\delta}}$ is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\varepsilon}_{\\tilde{\\delta}}=\\sum_{\\ell=1}^{k}\\frac{(e^{\\varepsilon_{\\ell}}-1)\\varepsilon_{\\ell}}{e^{\\varepsilon_{\\ell}}+1}+\\sqrt{\\sum_{\\ell=1}^{k}\\varepsilon_{\\ell}^{2}\\log\\left(\\frac{1}{\\tilde{\\delta}}\\right)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Fact A.7 (Fact 2.17 [63] reduced to pure DP). Let $Y\\approx_{\\varepsilon}Y^{\\prime}$ random variables over $\\boldsymbol{\\wp}$ and let the event $E\\subseteq\\mathcal{V}$ be such that $\\operatorname*{Pr}[Y\\in E],\\operatorname*{Pr}[Y^{\\prime}\\in E]\\geq q$ . Then $Y_{|E}\\approx_{\\varepsilon/q}Y_{|E}^{\\prime}$ . ", "page_idx": 14}, {"type": "text", "text": "B Omitted details of Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We state here again the general theorem which holds for any $M$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem B.1. Let $\\varepsilon\\in(0,10),\\delta\\in(0,1),\\alpha>0,\\beta\\in(0,1)$ .4 Algorithm $^{\\,l}$ is $(\\varepsilon,\\delta)$ -differentially private. Let $X$ be a data set of size $n$ , drawn from a subgaussian distribution with covariance-proxy $\\Sigma$ and mean $\\mu$ . Given $M=\\Sigma,$ , $\\lambda\\geq\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}+2\\sqrt{2\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(\\frac{n}{\\beta})},$ , with probability at least $1-\\beta$ , Algorithm $^{\\,l}$ returns $\\hat{\\mu}$ such that $\\|\\hat{\\mu}-\\mu\\|_{2}\\leq\\alpha$ , as long as ", "page_idx": 14}, {"type": "equation", "text": "$$\nn\\geq C\\left(\\frac{\\mathrm{tr}(\\Sigma)+\\|\\Sigma\\|_{2}\\log\\frac{1}{\\beta}}{\\alpha^{2}}+\\lambda\\left(\\sqrt{\\mathrm{tr}(M^{1/2})}+\\sqrt{\\|M^{1/2}\\|_{2}\\log\\frac{1}{\\beta}}\\right)\\frac{\\sqrt{\\log\\frac{1}{\\delta}}}{\\alpha\\varepsilon}+\\frac{\\log\\frac{1}{\\delta\\beta}}{\\varepsilon}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some universal constant $C$ . If $\\Sigma$ is known, choosing $M=\\Sigma$ and substituting $\\lambda,$ , the sample complexity becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{n\\geq C\\Bigg(\\frac{\\operatorname{tr}(\\Sigma)+\\|\\Sigma\\|_{2}\\log\\frac{1}{\\beta}}{\\alpha^{2}}+\\frac{\\operatorname{tr}(\\Sigma^{1/2})\\sqrt{\\log\\frac{1}{\\delta}}}{\\alpha\\varepsilon}}}\\\\ &{}&{\\qquad+\\,\\frac{\\sqrt{\\|\\Sigma\\|_{2}\\log\\frac{1}{\\delta}}\\log\\frac{1}{\\beta}}{\\alpha\\varepsilon}\\log\\left(\\frac{\\|\\Sigma\\|_{2}\\log\\frac{1}{\\delta}\\log\\frac{1}{\\beta}}{\\alpha\\varepsilon}\\right)+\\frac{\\log\\frac{1}{\\delta\\beta}}{\\varepsilon}\\Bigg)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Although the theorem holds for any $M,\\lambda$ , choosing $M=\\Sigma$ gives us the optimal bound.5 ", "page_idx": 15}, {"type": "text", "text": "Remark 2. If $M$ is such that $\\Sigma\\preceq M$ , we may assume without loss of generality that $M$ is invertible. Indeed, if this is not the case, then we know that the distribution of the data is supported on a lower-dimensional subspace along with its mean $\\mu$ . Using $M$ , we can project onto this subspace. In this context, we can refocus our analysis on the scenario where $M$ is an invertible matrix. ", "page_idx": 15}, {"type": "text", "text": "Computational complexity We note that Algorithm 1 has time complexity $O(n^{2})$ , which can be further improved to $O(n\\log n)$ as in [63]. ", "page_idx": 15}, {"type": "text", "text": "The guarantees of Theorem B.1 follow by combining Theorem B.3 and Theorem B.5 below and re-scaling parameters $\\varepsilon,\\delta,\\beta$ with appropriate constants. ", "page_idx": 15}, {"type": "text", "text": "B.1 Accuracy analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We start with the accuracy analysis. We first prove that for subgaussian data sets, all data points pass the filter with high probability. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. Let $X=(X^{(1)},\\ldots,X^{(n)})$ be a data set drawn from a subgaussian distribution with covariance proxy $\\Sigma$ . Let $\\beta\\in(0,1)$ , $M$ invertible matrix and some $\\lambda\\geq\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}+$ $2\\sqrt{2}\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(n/\\beta)$ given as inputs to Algorithm 1. Then the BasicFilter procedure outputs $C=X$ , with probability $1-\\beta$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. It suffices to show that in BasicFilter we have $p_{j}=1$ for $j\\in[n]$ (so that $v_{j}=1$ and thus $C=X)$ . For each $j,k\\in[n]$ , we want to show $\\mathrm{dist}_{M,\\lambda}(X^{(j)},X^{(k)})=1$ with probability at least $1-\\beta/n^{2}$ . For $j=k$ , it is trivial. What is left is to show for $j\\neq k$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|M^{-1/4}(X^{(j)}-X^{(k)})\\|_{2}\\le\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}+2\\sqrt{2\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(n/\\beta)}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\beta/n^{2}$ . First observe that $-X^{(k)}$ is a subgaussian vector independent of $X^{(j)}$ with mean $-\\mu$ and covariance proxy $\\Sigma$ . Hence, $X^{(j)}-X^{(k)}$ is a subgaussian vector with mean 0 and covariance proxy $2\\Sigma$ , and so $M^{-1/4}(X^{(j)}-X^{(k)})$ is subgaussian with mean zero and covariance proxy $2M^{-1/4}\\Sigma M^{-1/4}$ . By Lemma 2.7, with probability $1-\\beta/n^{2}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|M^{-1/4}(X^{(j)}-X^{(k)})\\|_{2}\\le\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}+2\\sqrt{2\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(n/\\beta)}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we union bound all $n(n-1)$ pairs of $j,k\\in[n],j\\neq k$ such that Eq. (7) holds with probability of at least $1-\\beta$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Theorem B.3. Let $\\varepsilon\\;>\\;0,\\delta\\;\\in\\;(0,1),\\alpha\\;>\\;0,\\beta\\;\\in\\;(0,1)$ . Suppose $n\\;\\geq\\;2\\log(1/\\delta\\beta)/\\varepsilon$ . Let $X\\,=\\,(X^{(1)},\\ldots,X^{(n)})$ be a data set drawn from a subgaussian distribution with mean $\\mu$ and covariance proxy $\\Sigma$ . Then, given invertible matrix $M$ and $\\lambda~\\geq~\\sqrt{2\\operatorname{tr}(M^{-1/4}\\Sigma M^{-1/4})}~+$ $2\\sqrt{2}\\|M^{-1/4}\\Sigma M^{-1/4}\\|_{2}\\log(n/\\beta),$ , Algorithm $^{\\,I}$ , with probability $\\textstyle1-{\\frac{7}{2}}\\beta$ , returns $\\hat{\\mu}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\mu}-\\mu\\|_{2}\\le\\displaystyle\\frac{\\sqrt{\\operatorname{tr}(\\Sigma)}}{\\sqrt{n}}+\\frac{\\sqrt{2\\|\\Sigma\\|_{2}\\log(1/\\beta)}}{\\sqrt{n}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\displaystyle\\frac{4\\sqrt{2\\log(1.25/\\delta)}\\lambda}{\\varepsilon n}\\left(\\sqrt{\\operatorname{tr}(M^{1/2})}+\\sqrt{2\\|M^{1/2}\\|_{2}\\log(1/\\beta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\mu_{C},\\mu_{X}$ be the sample mean of $C$ and $X$ , respectively. By the triangle inequality, we decompose it into ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}-\\mu\\|_{2}\\le\\|\\hat{\\mu}-\\mu_{C}\\|_{2}+\\|\\mu_{C}-\\mu_{X}\\|_{2}+\\|\\mu_{X}-\\mu\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma B.2, $C=X$ with probability $1-\\beta$ . Condition on this event for the rest of the proof. Then, $\\begin{array}{r}{\\hat{n}_{C}=n-\\frac{\\log(1/\\delta)}{\\varepsilon}+z}\\end{array}$ log(\u03b51/\u03b4)+ z satisfies n\u02c6C \u22650.5n > 0 with probability 1 \u2212\u03b2/2 because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[z<{\\frac{\\log(1/\\delta)}{\\varepsilon}}-0.5n\\right]\\leq\\operatorname*{Pr}\\left[z<{\\frac{\\log(1/\\delta)}{\\varepsilon}}-{\\frac{\\log(1/\\delta\\beta)}{\\varepsilon}}\\right]=\\operatorname*{Pr}\\left[z<-{\\frac{\\log(1/\\beta)}{\\varepsilon}}\\right]={\\frac{1}{2}}\\beta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Definition A.1 and our assumption that $n\\geq2\\log(1/\\delta\\beta)/\\varepsilon$ . Conditioning on this assumption, we do not abort and with probability $1-\\beta$ , by Lemma 2.7, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu_{C}\\|_{2}=\\|\\eta\\|_{2}\\le\\frac{4\\sqrt{2\\log(1.25/\\delta)}\\lambda}{\\varepsilon n}\\left(\\sqrt{\\mathrm{tr}(M^{1/2})}+\\sqrt{2\\|M^{1/2}\\|_{2}\\log(1/\\beta)}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Again, by Lemma 2.7, with probability $1-\\beta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mu_{X}-\\mu\\|_{2}=\\left\\|\\frac{1}{n}\\sum_{j=1}^{n}X^{(j)}-\\mu\\right\\|_{2}\\leq\\frac{\\sqrt{\\operatorname{tr}(\\Sigma)}}{\\sqrt{n}}+\\frac{\\sqrt{2\\|\\Sigma\\|_{2}\\log(1/\\beta)}}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, since $C=X$ , it holds that $\\mu_{X}=\\mu_{C}$ . Combining these results into Eq. (8), the algorithm does not abort and we retrieve the stated error bound, with probability $\\begin{array}{r}{1-\\frac{7}{2}\\beta}\\end{array}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.2 Privacy analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now move to the privacy analysis. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. In Algorithm 1 $,\\,\\hat{n}_{C}<|C|,$ , with probability $1-\\delta/2$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. It follows that by Definition A.1, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[{\\hat{n}}_{C}\\geq|C|\\right]=\\operatorname*{Pr}\\left[|C|-\\log(1/\\delta)/\\varepsilon+z\\geq|C|\\right]=\\operatorname*{Pr}\\left[z\\geq\\log(1/\\delta)/\\varepsilon\\right]={\\frac{1}{2}}\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that this holds regardless of whether $C$ is $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly or whether $X$ is subgaussian. ", "page_idx": 16}, {"type": "text", "text": "The privacy analysis follows the steps of [63, Claim 3.4]. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.5. Let $\\varepsilon\\in(0,1/2),\\delta\\in(0,1/2)$ . For any input parameters $M,\\lambda,$ , Algorithm $^{\\,I}$ satisfies $(21\\varepsilon,e^{10}\\delta){-}D P.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. It suffices show that lines 4-7 of Algorithm 1 are $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly $(\\varepsilon^{\\prime},\\delta^{\\prime})$ -DP, such that by Theorem 2.5, Algorithm 1 is $(2(e^{\\varepsilon^{\\prime}}-1)\\varepsilon^{\\prime},2e^{\\varepsilon^{\\prime}+2(e^{\\varepsilon^{\\prime}}-1)}\\delta^{\\prime})$ -DP. ", "page_idx": 16}, {"type": "text", "text": "Denote lines 4-7 of Algorithm 1 as algorithm $\\boldsymbol{\\mathcal{A}}$ . Consider neighboring inputs $X,X^{\\prime}$ such that $X\\cup X^{\\prime}$ is $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly. Assume without loss of generality $X^{\\prime}=X\\setminus X^{(j)}$ so that $|X^{\\prime}|=|X|-1$ . Let ${\\mathcal{A}}(X),{\\mathcal{A}}(X^{\\prime})$ represent the outputs of two independent executions of $\\boldsymbol{\\mathcal{A}}$ and let $\\widehat{N}_{X},\\widehat{N}_{X^{\\prime}}$ be the random variable in line 4 of the algorithm. We want to show $\\mathcal{A}(X)\\approx_{\\varepsilon^{\\prime},\\delta^{\\prime}}\\mathcal{A}(X^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "Note that $|X|>0$ . If $\\left|X^{\\prime}\\right|\\;=\\;0$ , then $|X|\\,=\\,1$ and $\\operatorname*{Pr}\\left[A(X^{\\prime})=\\bot\\right]\\,=\\,1$ . We then show that $\\operatorname*{Pr}\\left[A(X)=\\bot\\right]\\geq1-e^{\\varepsilon}\\delta/2$ . This holds since by Definition A.1, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\hat{N}_{X}\\leq0]=\\operatorname*{Pr}\\left[z\\leq\\log(1/\\delta)/\\varepsilon-1\\right]=\\operatorname*{Pr}\\left[z\\leq\\log(1/(e^{\\varepsilon}\\delta))/\\varepsilon\\right]=1-\\frac{e^{\\varepsilon}\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, in this case, $\\mathcal{A}(X)\\approx_{0,e^{\\varepsilon}\\delta/2}\\mathcal{A}(X^{\\prime})$ . That is, if $\\varepsilon\\leq1/2,A$ is $(0,\\delta)$ -DP. ", "page_idx": 17}, {"type": "text", "text": "Now consider $|X^{\\prime}|>0$ . By Lemma B.4, We know $\\mathrm{Pr}[\\hat{N}_{X}<|X|]=\\mathrm{Pr}[\\hat{N}_{X^{\\prime}}<|X^{\\prime}|]=1-\\delta/2$ . Hence, $\\mathrm{Pr}[\\hat{N}_{X^{\\prime}}\\ <\\ |X|]\\,=\\,\\mathrm{Pr}[\\hat{N}_{X^{\\prime}}\\ <\\ |X^{\\prime}|+1]\\,\\ge\\,1-\\delta/2$ . Then what is left is to compare $A(X)|_{\\hat{N}_{X}<|X|},A(X^{\\prime})|_{\\hat{N}_{X^{\\prime}}<|X|}^{\\cdot}$ . ", "page_idx": 17}, {"type": "text", "text": "By Lemma A.3, N\u02c6X \u2248\u03b5,0 N\u02c6X\u2032 as |X| \u2212|X\u2032| = 1. By Fact A.7, N\u02c6X| N\u02c6 <|X| \u2248\u03b5/(1\u2212\u03b4/2),0 $\\hat{N}_{X^{\\prime}}|_{\\hat{N}_{X^{\\prime}}<|X|}$ . In order to perform composition by Lemma A.6, we now show that for each fixed $\\hat{n}<|\\dot{X}|,\\dot{A}(\\dot{X})|_{\\hat{N}_{X}=\\hat{n}}\\approx_{\\varepsilon,\\delta}A(X^{\\prime})|_{\\hat{N}_{X^{\\prime}}=\\hat{n}}$ as follows: ", "page_idx": 17}, {"type": "text", "text": "Choose $\\boldsymbol{\\hat{n}}\\,<\\,|X|$ . If $\\hat{n}\\,\\le\\,0$ , then $\\mathop{A(X)}\\lvert_{\\widehat{N}_{X}=\\widehat{n}}\\,=\\,\\mathop{A(X^{\\prime})}\\lvert_{\\widehat{N}_{X}=\\widehat{n}}\\,=\\,\\bot$ and we are done. If $0~<$ $\\hat{n}<|X|$ , it suffices to show $\\begin{array}{r}{\\mathcal{N}(\\frac{1}{|X|}\\sum_{i=1}^{|X|}X^{(i)},v^{2}M^{1/2})\\approx_{\\varepsilon,\\delta}\\mathcal{N}(\\frac{1}{|X|-1}\\sum_{i=1,i\\neq j}^{|X|}X^{(i)},v^{2}M^{1/2})}\\end{array}$ , where v2 = 8 log(1.25/\u03b4)\u03bb2, which, by post-processing, is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{N}}\\left(M^{-1/4}\\frac{1}{|\\boldsymbol{X}|}\\sum_{i=1}^{|\\boldsymbol{X}|}\\boldsymbol{X}^{(i)},\\boldsymbol{v}^{2}\\boldsymbol{I}_{d}\\right)\\approx_{\\varepsilon,\\delta}\\boldsymbol{\\mathcal{N}}\\left(M^{-1/4}\\frac{1}{|\\boldsymbol{X}|-1}\\sum_{i=1,i\\neq j}^{|\\boldsymbol{X}|}\\boldsymbol{X}^{(i)},\\boldsymbol{v}^{2}\\boldsymbol{I}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Define vector $\\begin{array}{r}{D=\\frac{1}{|X|}\\sum_{i=1}^{|X|}X^{(i)}-\\frac{1}{|X|-1}\\sum_{i=1,i\\neq j}^{|X|}X^{(i)}}\\end{array}$ . As $X\\cup X^{\\prime}$ is $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly, for every $i\\in[|X|]\\setminus\\{j\\}$ , there exists some $Y^{(i)}\\in\\mathbb{R}^{d}$ such that $\\mathrm{dist}_{\\Sigma,\\beta}(X^{(i)},Y^{(i)})=\\mathrm{dist}_{\\Sigma,\\beta}(X^{(j)},Y^{(i)})=$ 1. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|M^{-1/4}D||_{2}=\\frac{1}{|X|(|X|-1)}\\left\\|M^{-1/4}\\left(\\left(\\sum_{i=1,i\\neq j}^{|X|}X^{(i)}\\right)-X^{(j)}(|X|-1)\\right)\\right\\|_{2}}}\\\\ &{\\leq\\frac{1}{|X|(|X|-1)}\\sum_{i=1,i\\neq j}^{|X|}\\left(\\left\\|M^{-1/4}\\left(X^{(i)}-Y^{(i)}\\right)\\right\\|_{2}+\\left\\|M^{-1/4}\\left(Y^{(i)}-X^{(j)}\\right)\\right\\|_{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\overset{{,}}{\\underset{{,}}{\\leq}}\\frac{|X|}{|X|}\\qquad\\underset{{2\\,\\lambda}}{\\geq}\\quad2\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\leq{\\frac{1}{|X|(|X|-1)}}\\sum_{i=1,i\\neq j}^{|X|}2\\lambda={\\frac{2\\lambda}{|X|}}\\leq{\\frac{2\\lambda}{\\hat{n}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(by $\\mathrm{dist}_{\\Sigma,\\beta}$ -friendly assumption and since $0<\\hat{n}<|X|)$ ", "page_idx": 17}, {"type": "text", "text": "We know Equation (9) holds by applying the guarantees of the Gaussian mechanism (Lemma A.5) where we set $\\begin{array}{r}{f(X)=M^{-1/4}\\frac{1}{|X|}\\sum_{i=1}^{|X|}X^{(i)}}\\end{array}$ 4|X1| |iX=|1 X(i) and \u2206f = 2\u03bb/n\u02c6. ", "page_idx": 17}, {"type": "text", "text": "Combining these results, we have $\\boldsymbol{\\mathcal{A}}$ is $\\begin{array}{r}{(\\varepsilon+\\frac{\\varepsilon}{1-\\delta/2},\\delta e^{\\varepsilon/(1-\\delta/2)}+\\frac{\\delta}{2})\\mathrm{-}\\mathrm{DP}}\\end{array}$ in this case. For $\\varepsilon\\,\\leq$ $1/2,\\delta\\le1/2$ , this becomes at most $(3\\varepsilon,2\\delta)$ -DP. ", "page_idx": 17}, {"type": "text", "text": "Therefore, overall, by Theorem 2.5, Algorithm 1 is $(2(e^{\\varepsilon^{\\prime}}-1)\\varepsilon^{\\prime},2e^{\\varepsilon^{\\prime}+2(e^{\\varepsilon^{\\prime}}-1)}\\delta^{\\prime})$ -DP with $\\varepsilon^{\\prime}=$ $3\\varepsilon,\\delta^{\\prime}=2\\delta$ . So for $\\varepsilon\\leq1/2$ , the algorithm is $(21\\varepsilon,e^{10}\\dot{\\delta})$ -DP overall. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Lower bounds ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Dimension-dependent lower bound under pure DP ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The so-called packing lower bound technique [32, 10] implies a lower bound on the order of $d$ for the number of samples required by any pure DP algorithm learning the mean of a Gaussian distribution, even in the anisotropic case we consider in this paper. ", "page_idx": 17}, {"type": "text", "text": "There exist several statements in prior works which establish the lower bound for learning a Gaussian distribution with known covariance in TV distance, which is equivalent to learning the mean in Mahalanobis distance, or to learning the mean in $\\ell_{2}$ norm in the isotropic case [18, Lemma 5.1]. It is trivial to observe that the dependence on the dimension $d$ persists in the anisotropic case, yet we include the proof here for completeness. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. For any $\\alpha<R/2$ , any $\\varepsilon$ -DP algorithm which estimates the mean $\\mu\\in{\\mathcal{B}}^{d}(R)$ of $a$ Gaussian distribution with known covariance $\\Sigma$ , up to accuracy $\\alpha$ in $\\ell_{2}$ norm with probability $9/10$ , requires $\\begin{array}{r}{n\\geq\\frac{d\\log(R/2\\alpha)}{\\varepsilon}}\\end{array}$ samples. ", "page_idx": 18}, {"type": "text", "text": "Proof. Consider a $2\\alpha$ -packing of the $d$ -dimensional $R$ -radius ball, denoted by ${\\mathcal{P}}_{2\\alpha}~\\subset~{\\mathcal{B}}^{d}(R)$ . That is, $\\forall u,v\\in\\mathcal{P}$ , $\\|u-v\\|_{2}>2\\alpha$ , so that the balls with centers $u,v$ and radius $\\alpha$ are disjoint: $B^{d}(u,\\alpha)\\cap B^{d}(v,\\alpha)\\ =\\ \\emptyset$ . We consider the family of Gaussian distributions $\\{\\mathcal{N}(u,\\Sigma)\\}_{u\\in\\mathcal{P}_{2\\alpha}}$ . Suppose $\\boldsymbol{\\mathcal{A}}$ is an $\\varepsilon$ -DP algorithm with the stated accuracy requirement. This implies that $\\forall u\\in\\mathcal{P}_{2\\alpha}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Pr_{A,X\\sim{\\cal N}(u,\\Sigma)^{n}}[A(X)\\in{\\cal B}^{d}(u,\\alpha)]\\geq9/10.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "At the same time, for any pair of samples $X,X_{0}$ of size $n$ , and any measurable set $B\\subset\\mathrm{range}(A)$ , by the privacy guarantee, $\\operatorname*{Pr}_{A}[A(X)\\in B]\\leq e^{\\varepsilon n}\\operatorname*{Pr}_{A}[A(X_{0})\\in B]$ . This implies specifically that for $u_{0},u\\in\\mathcal{P}_{2\\alpha}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\substack{A,X\\sim\\mathcal{N}(u,\\Sigma)^{n}}}[A(X)\\in B]\\leq e^{\\varepsilon n}\\operatorname*{Pr}_{\\substack{A,X_{0}\\sim\\mathcal{N}(u_{0},\\Sigma)^{n}}}[A(X_{0})\\in B].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{1\\geq\\underset{A,X_{0}\\sim\\mathcal{N}(u_{0},\\Sigma)^{n}}{\\operatorname*{Pr}}[A(X_{0})\\in\\underset{u\\in\\mathcal{P}_{2\\alpha}}{\\bigcup}B^{d}(u,\\alpha)]}\\\\ &{\\quad=\\underset{u\\in\\mathcal{P}_{2\\alpha}}{\\sum}\\underset{A,X_{0}\\sim\\mathcal{N}(u_{0},\\Sigma)^{n}}{\\operatorname*{Pr}}[A(X_{0})\\in B^{d}(u,\\alpha)]}&&{\\quad(\\{B^{d}(u,\\alpha)\\}_{u\\in\\mathcal{P}_{2\\alpha}}\\,\\,\\mathrm{disjoint})}\\\\ &{\\quad\\geq\\underset{u\\in\\mathcal{P}_{2\\alpha}}{\\sum}e^{-\\varepsilon n}\\underset{A,X\\sim\\mathcal{N}(u,\\Sigma)^{n}}{\\operatorname*{Pr}}[A(X)\\in B^{d}(u,\\alpha)]}&&{\\quad\\mathrm{(by~Eq.~(11))}}\\\\ &{\\quad\\geq|\\mathcal{P}_{2\\alpha}|e^{-\\varepsilon n}\\cdot\\frac{9}{10}.}&&{\\quad\\mathrm{(by~Eq.~(10))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We conclude that $n\\geq\\frac{\\log|\\mathcal{P}_{2\\alpha}|}{\\varepsilon}$ . Since $\\begin{array}{r}{|\\mathcal{P}_{2\\alpha}|\\geq\\left(\\frac{R}{\\alpha}\\right)^{d}}\\end{array}$ , it follows that $\\begin{array}{r}{n\\geq\\frac{d\\log(R/2\\alpha)}{\\varepsilon}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "This lower bound makes $\\varepsilon$ -DP prohibitive for the regime we consider in our setting. To compare with our upper bounds for $(\\varepsilon,\\delta)$ -DP, suppose that we want to learn $\\mu$ with accuracy $c\\sigma_{1}<R$ , where $c>0$ is a small constant. Then our main result implies that this is achievable with $n\\leq C{\\frac{\\|{\\pmb{\\sigma}}\\|_{1}}{\\varepsilon\\sigma_{1}}}$ samples for some constant $C>0$ , whereas under $\\varepsilon$ -DP, we would need at least $\\begin{array}{r}{n\\geq\\frac{d}{\\varepsilon}\\gg\\frac{\\|\\pmb{\\sigma}\\|_{1}}{\\varepsilon\\sigma_{1}}}\\end{array}$ for the regime we consider in this paper. ", "page_idx": 18}, {"type": "text", "text": "C.2 Lower bound for approximate DP ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The so-called tracing or fingerprinting lower-bound technique [15, 60, 17, 61, 28] is the main technique used to yield lower bounds for mean estimation under $(\\varepsilon,\\delta)$ -DP. Kamath et al. [39, 40] apply it to give lower bounds for the problem of learning a Gaussian in TV distance (which is equivalent to learning the Gaussian in Mahalanobis distance for the known covariance case, or to the isotropic case). ", "page_idx": 18}, {"type": "text", "text": "Theorem C.2 (Theorem 6.5 [39]). If $\\boldsymbol{\\mathcal{A}}:\\mathbb{R}^{d\\times n}\\rightarrow[-R\\sigma,R\\sigma]^{d}$ is $(\\varepsilon,\\delta)$ -DP for $\\begin{array}{r}{\\delta=\\tilde{O}(\\frac{\\sqrt{d}}{R n})}\\end{array}$ , and for every Gaussian distribution with me\u221aan $\\mu\\in[-R\\sigma,R\\sigma]^{d}$ and known covariance matrix $\\sigma^{2}I_{d}$ , with probability 2/3, \u2225A(X) \u2212\u00b5\u2225\u2264\u03b1 \u2264 d\u03c3R/3, then n \u226524\u03b1\u03b5 ldo\u03c3g(dR). ", "page_idx": 18}, {"type": "text", "text": "Following exactly the same steps as the proof of the theorem under the slightly more general case of known covariance $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ gives us a weak lower bound for our setting, on the order of n \u226524\u03b5\u03b1\u03c312 lo2g(dR). ", "page_idx": 18}, {"type": "text", "text": "However, a more careful application of the same theorem directly gives us the following stronger lower bound, which implies that our algorithm for the known covariance case is near-optimal. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.3. If $\\mathcal{A}:\\mathbb{R}^{d\\times n}\\rightarrow\\mathbb{R}^{d}$ is $(\\varepsilon,\\delta)$ -DP for $\\delta=O((n{\\sqrt{\\log(n)}})^{-1})$ , and for every Gaussian distribution with mean $\\mu\\in[-1,1]^{d}$ and known covariance proxy $\\Sigma=\\mathrm{diag}(\\pmb{\\sigma}^{2})$ , with probability $2/3,\\,\\|A(X)-\\mu\\|\\leq\\alpha=O(\\|\\pmb{\\sigma}\\|_{1}/\\log(d))$ , then $\\begin{array}{r}{n=\\Omega\\left(\\frac{||\\pmb{\\sigma}||_{1}}{\\alpha\\varepsilon\\log^{2}(d)}\\right)}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Assume w.l.o.g. that $\\sigma_{1}^{2}\\geq...\\geq\\sigma_{d}^{2}$ . Consider a partition of the set of coordinates $[d]$ into buckets $\\begin{array}{r}{S_{k}=\\{i\\in[d]:\\sigma_{i}\\in\\bar{(}\\frac{\\sigma_{1}}{2^{k}},\\frac{\\sigma_{1}}{2^{k-1}}]\\}}\\end{array}$ , $\\forall k\\in[\\log(d)]$ and $S_{\\log(d)+1}=[d]\\setminus\\bigcup_{k\\in[\\log(d)]}\\bar{S}_{k}$ . We have that $\\begin{array}{r}{\\sum_{k=1}^{\\log(d)+1}\\sum_{i\\in S_{k}}\\sigma_{i}=\\|\\pmb{\\sigma}\\|_{1}}\\end{array}$ . Consider the bucket $S_{m}$ which contribu tes the most to this sum, that is $\\begin{array}{r}{m=\\arg\\operatorname*{max}\\sum_{i\\in S_{m}}\\sigma_{i}}\\end{array}$ . Let $\\sigma_{S_{m}}=\\operatorname*{max}\\{\\sigma_{i}:i\\in S_{m}\\}$ . It must be that ", "page_idx": 19}, {"type": "equation", "text": "$$\n|S_{m}|\\geq\\frac{\\|\\pmb{\\sigma}\\|_{1}}{(\\log(d)+1)\\sigma_{S_{m}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Otherwise, $\\begin{array}{r}{\\|\\pmb{\\sigma}\\|_{1}=\\sum_{k=1}^{\\log(d)+1}\\sum_{i\\in S_{k}}\\sigma_{i}\\leq(\\log(d)+1)|S_{m}|\\sigma_{S_{m}}<\\|\\pmb{\\sigma}\\|_{1},}\\end{array}$ , which is a contradiction. All the variances of the coordinates in $S_{m}$ are within a factor of two from $\\sigma_{S_{m}}$ . We apply Theorem C.2 to the $\\vert S_{m}\\vert$ -dimensional Gaussian with $R\\,=\\,1$ . Consider the Gaussian distribution with mean $\\mu_{S_{m}}\\,\\in\\,[-1,1]^{|S_{m}|}$ and known covariance matrix $\\sigma_{S_{m}}^{2}I_{d}$ . We have that any $(\\varepsilon,\\delta)$ -DP algorithm for $\\begin{array}{r}{\\delta=O\\left(\\frac{1}{n\\sqrt{\\log(n)}}\\right)}\\end{array}$ which returns, with probability $2/3$ , an estimate $\\hat{\\mu}_{S_{m}}$ with error $\\|\\hat{\\mu}_{S_{m}}-$ $\\mu_{S_{m}}\\|_{2}\\le\\alpha\\le\\sqrt{|S_{m}|}\\sigma_{S_{m}}/3$ , requires ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\geq\\frac{|S_{m}|\\sigma_{S_{m}}}{24\\alpha\\varepsilon\\log(d)}\\geq\\frac{\\|\\pmb{\\sigma}\\|_{1}}{48\\alpha\\varepsilon\\log^{2}(d)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "samples. ", "page_idx": 19}, {"type": "text", "text": "Now assume that there exists $(\\varepsilon,\\delta)$ -DP algorithm $\\mathcal{A}:\\mathbb{R}^{d\\times n}\\rightarrow\\mathbb{R}^{d}$ for $\\begin{array}{r}{\\delta=O\\left(\\frac{1}{n\\sqrt{\\log(n)}}\\right)}\\end{array}$ , such that, for every Gaussian distribution with mean $\\mu\\in[-1,1]^{d}$ and known covariance proxy $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ , with probability 2/3, \u2225A(X)\u2212\u00b5\u2225\u2264\u03b1 \u22643(lo\u2225g\u03c3(d\u2225)1+1). Restricting the output ${\\mathcal{A}}(X)$ to the coordinates in $S_{m}$ , would give us a mean estimate for $S_{m}$ with error at most $\\alpha$ . Combined with Eq. (12), this completes the proof of the theorem. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Omitted details of Section 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We state the main theorem in more detail. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.1. Let parameters $\\varepsilon,\\delta\\in(0,1)$ . Let $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{\\mu}},\\ensuremath{\\boldsymbol{\\Sigma}})^{n}$ with unknown covariance $\\Sigma$ . There exists an $(\\varepsilon,\\delta)$ -differentially private algorithm which, with probability $1-\\beta$ , returns an estimate $\\hat{\\mu}$ such that $\\|{\\hat{\\mu}}-\\mu\\|_{2}\\leq\\alpha_{*}$ , as long as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n=\\Omega\\left(\\log^{2}(d)+\\frac{\\log(d)\\log(\\frac{1}{\\delta\\beta})\\sqrt{\\log\\frac{1}{\\delta}}}{\\varepsilon}\\right)\\,,}\\\\ &{\\qquad n=\\Omega\\left(\\frac{\\mathrm{tr}(\\Sigma)+\\|\\Sigma\\|_{2}\\log\\frac{1}{\\beta}}{\\alpha^{2}}\\right),}\\\\ &{\\qquad n=\\tilde{\\Omega}\\left(\\frac{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}\\sqrt{\\log\\frac{1}{\\delta}}}{\\alpha\\varepsilon}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\nn=\\tilde{\\Omega}\\left(\\frac{d^{1/4}\\sqrt{\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}}\\log^{5/4}(\\frac{1}{\\delta})\\log(d)}{\\sqrt{\\alpha}\\varepsilon}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the symbol $\\tilde{\\Omega}$ hides multiplicative logarithmic factors in $1/\\beta$ and the term in parentheses. ", "page_idx": 19}, {"type": "text", "text": "Recall the definition of set $V$ . Within each group $j$ , for each coordinate $i$ , we compute an estimate V i(j)for \u03a3ii as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{i}^{(j)}=\\frac{1}{2\\ell}\\sum_{r=1}^{\\ell}\\left(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)}\\right)^{2}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We show that the variance estimates are valid in the subsequent sections. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2. Let $X^{(1)},\\ldots,X^{(n)}$ be $d$ -dimensional i.i.d. samples from $\\mathcal{N}(\\mu,\\Sigma)$ . Let $\\{V_{i}^{(j)}\\}_{j\\in[m],i\\in[d]}$ be the estimates defined in Eq. (16). Then, there exist universal constants $C,C^{\\prime}>1$ such that if $\\ell\\geq C\\log d$ and $m\\geq C^{\\prime}\\log(1/\\beta)$ , with probability at least $1-\\beta$ , the set $V$ of estimates is valid. ", "page_idx": 20}, {"type": "text", "text": "Next, we use the estimates $V_{i}^{(j)}$ as inputs to multiple procedures. We introduce the following estimation tasks. ", "page_idx": 20}, {"type": "text", "text": "Definition D.3. For a covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$ consider the following: ", "page_idx": 20}, {"type": "text", "text": "1. $k$ -th largest variance: Approximate the $k$ -th largest value among the diagonal of $\\Sigma$ , namely, the $k$ -th largest value among $(\\Sigma_{11},\\ldots,\\Sigma_{d d})$ . ", "page_idx": 20}, {"type": "text", "text": "2. Sum of variances: given a subset $I\\subseteq[d]$ , approximate the sum $\\sum_{i\\in I}\\Sigma_{i i}$ . ", "page_idx": 20}, {"type": "text", "text": "We have the following algorithms for these tasks. The proofs of Lemmas D.4, D.5, and D.6 are in the subsequent subsections. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.4. Let $\\varepsilon,\\delta,\\beta\\in(0,1/2)$ . There exists an algorithm FindKthLargestVaria $\\mathrm{nce}_{\\varepsilon,\\delta}$ , which receives variance estimates $V^{(1)},\\ldots,V^{(m)}\\in\\mathbb{R}^{d}$ and an integer $k\\in[d]$ , and satisfies the following, provided that ", "page_idx": 20}, {"type": "equation", "text": "$$\nm\\geq\\Omega\\left(\\frac{1}{\\varepsilon}\\log\\frac{1}{\\delta\\beta}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Privacy: FindKthLargestVariance\u03b5,\u03b4 is $(\\varepsilon,\\delta){-}D P$ with respect to changing each input vector $V^{(j)}$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 Accuracy: denote the $k$ -th largest entry of $\\{\\Sigma_{11},\\ldots,\\Sigma_{d d}\\}$ by $Q$ and the algorithm\u2019s output by $\\hat{Q}$ . If the estimates $\\left(V^{(1)},\\ldots,V^{(m)}\\right)$ are valid wrt $\\Sigma$ , then there exists a universal constant $C>0$ such that with probability at least $1-\\beta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ/8\\leq\\hat{Q}\\leq8Q\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma D.5. Let $\\varepsilon,\\delta,\\beta\\ \\in\\ (0,1)$ . There exists an algorithm Variance $\\mathrm{Sum}_{\\varepsilon,\\delta}$ , which receives variance estimates $V^{(1)},\\dots,V^{(m)}\\in\\mathbb{R}^{d}$ and a subset $I\\subseteq[d]$ . It has the exact same guarantees as FindKthLargestVariance from Lemma $D.4,$ , except that it provides an estimate for $\\textstyle\\sum_{i\\in I}\\Sigma_{i i}$ instead of an estimate for the $k$ -th largest diagonal entry of $\\Sigma$ . ", "page_idx": 20}, {"type": "text", "text": "Assume for now that the estimates $V_{i}^{(j)}$ are valid. With these procedures at hand, we first compute $R$ such that (by rescaling) $Q/64\\leq R\\leq Q$ , where $Q$ is the $k$ -th largest diagonal entry of $\\Sigma$ . Then, we call a procedure that finds $k$ entries $i$ such that $\\Sigma_{i i}\\geq R$ . Its guarantees are listed below: ", "page_idx": 20}, {"type": "text", "text": "Lemma D.6. Let $\\varepsilon,\\delta,\\beta\\in(0,1)$ . There exists an $(\\varepsilon,\\delta)$ -DP algorithm $\\mathrm{TopVar}_{\\varepsilon,\\delta}(V,R)$ , such that, $i f$ $V$ is valid, ", "page_idx": 20}, {"type": "equation", "text": "$$\nm\\geq\\Omega\\left(\\sqrt{\\frac{k\\log(1/\\delta)}{\\varepsilon n}}\\log\\frac{d}{\\beta}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $|\\{i\\colon\\Sigma_{i i}\\geq R\\}|\\geq k,$ , then the algorithm outputs a set $I_{\\mathrm{top}}$ of size $k$ such that for all $i\\in I_{\\mathrm{top}}$ , $\\Sigma_{i i}\\geq R/4$ . ", "page_idx": 20}, {"type": "text", "text": "At the next step, we would like to find, up to a constant factor, the variances corresponding to these coordinates: the values $\\Sigma_{i i}$ for $i\\in I_{\\mathrm{top}}$ . We use the algorithm VarianceSum $k$ times, providing the sets $\\{i\\}$ for $i\\in I_{\\mathrm{top}}$ . We obtain estimates $\\hat{\\Sigma}_{i i}$ that approximate $\\Sigma_{i i}$ up to a constant factor. ", "page_idx": 20}, {"type": "text", "text": "Next, we estimate the mean $\\mu$ in the coordinates $I_{\\mathrm{top}}$ , denoted $\\mu_{\\mathrm{top}}$ , separately from $I_{\\mathrm{bot}}:=[d]\\backslash I_{\\mathrm{top}}$ , denoted $\\mu_{\\mathrm{bot}}$ : since we approximately know the variances in $I_{\\mathrm{top}}$ , we can obtain a better estimate. ", "page_idx": 20}, {"type": "text", "text": "Both for estimating $\\mu_{\\mathrm{top}}$ , and for estimating $\\mu_{\\mathrm{bot}}$ , we use $\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}$ (Algorithm 1 from Section 3) with appropriate choices of parameters $M,\\lambda$ . Recall that Algorithm 1 satisfies the guarantees of Theorem B.1. ", "page_idx": 21}, {"type": "text", "text": "For estimating $\\mu_{\\mathrm{top}}$ , we use $\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}$ for estimating the mean of a $k$ -dimensional Gaussian, with input vectors restricted to coordinates $I_{\\mathrm{top}},X_{I_{\\mathrm{top}}}^{(1)},\\ldots,X_{I_{\\mathrm{top}}}^{(n)}$ , the $k\\times k$ -dimensional diagonal matrix $M$ , with $M_{i i}\\,=\\,\\hat{\\Sigma}_{i i}$ (we assume that the rows and columns of $M$ are indexed by $I_{\\mathrm{top.}}$ ), and $\\begin{array}{r}{\\lambda=O\\left(\\sqrt{\\sum_{i\\in I_{\\mathrm{top}}}\\hat{\\Sigma}_{i i}^{1/2}\\log\\frac{n}{\\beta}}\\right)}\\end{array}$ . Denote the output by $\\hat{\\mu}_{\\mathrm{top}}$ . Theorem B.1 shows that with probability $1-\\beta,\\,\\|\\hat{\\mu}_{\\mathrm{top}}-\\mu_{\\mathrm{top}}\\|_{2}\\le\\alpha$ , if ", "page_idx": 21}, {"type": "equation", "text": "$$\nn\\geq\\tilde{\\Omega}\\left(\\frac{\\mathrm{tr}(\\Sigma)}{\\alpha^{2}}+\\frac{\\sqrt{\\log(1/\\delta)}\\sum_{i\\in I_{\\mathrm{top}}}\\Sigma_{i i}^{1/2}}{\\alpha\\varepsilon}+\\frac{\\log(1/\\delta)}{\\varepsilon}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\tilde{\\Omega}$ hides multiplicative logarithmic factors in $1/\\beta$ and the second term. ", "page_idx": 21}, {"type": "text", "text": "For estimating $I_{\\mathrm{bot}}$ , we do not know the variances. In order to perform the estimation, we first call the algorithm VarianceSum to provide an estimate $\\hat{S}_{\\mathrm{bot}}$ such that $\\begin{array}{r}{\\frac{1}{C}\\sum_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}\\,\\le\\,\\hat{S}_{\\mathrm{bot}}\\,\\le\\,}\\end{array}$ $C\\sum_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}$ for a constant $C$ . Given that estimate, we again will call $\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}.$ , now for a $(d\\!-\\!k)$ - dimensional estimation problem. We input the samples XI(b1o)t, $X_{I_{\\mathrm{bot}}}^{(1)},\\ldots,X_{I_{\\mathrm{bot}}}^{(n)}$ XI(bno)t, replace the matrix M with the identity of dimension $(d-k)\\times(d-k)$ , and let $\\begin{array}{r}{\\lambda=O\\left(\\sqrt{\\hat{S}_{\\mathrm{bot}}\\log\\frac{n}{\\beta}}\\right)}\\end{array}$ .6 Denote the output by $\\hat{\\mu}_{\\mathrm{bot}}$ . The guarantees of Theorem B.1 provide that with probability $1-\\beta$ , $\\|\\hat{\\mu}_{\\mathrm{bot}}-\\mu_{\\mathrm{bot}}\\|_{2}\\leq\\alpha$ , if, additionally to Eq. (17), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nn\\geq\\tilde{\\Omega}\\left(\\frac{\\sqrt{d\\log\\frac{1}{\\delta}\\sum_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}}}{\\alpha\\varepsilon}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\tilde{\\Omega}$ hides multiplicative logarithmic factors of $1/\\beta$ and of the term in parentheses. As we prove below, combining these guarantees would yield the desired result. Additionally, we note that in order for the proof to go through, we split the sample into two groups. One group is used for estimating the variances and the other group is given as an input to the two invocations of VarianceSum. We provide the formal pseudocode in Algorithm 2. ", "page_idx": 21}, {"type": "text", "text": "Accuracy analysis. We put together the statements of the lemmas above, to establish the overall accuracy guarantee of Algorithm 2. By Lemma D.2, the estimates V i(j) are valid (i.e., at least $4m/5$ of the groups have approximation up to 2 for every coordinate), with probability $1-\\beta$ as long as $\\begin{array}{r}{m\\,=\\,\\Omega(\\log\\frac{1}{\\beta})}\\end{array}$ . Consequently, Lemma D.4 implies that as long as $\\begin{array}{r}{m\\,=\\,\\dot{\\Omega^{\\prime}}(\\frac{1}{\\varepsilon}\\log\\frac{1}{\\delta\\beta})}\\end{array}$ , FindKthLargestVariance outputs an estimate of the $k$ -th largest variance, which is accurate up to a constant factor $C=8$ , with probability $1-\\beta$ . By scaling, we can assume that, $\\hat{R}/8$ , is upper bounded by the $k$ -th largest variance. Under this assumption, and as long as $\\begin{array}{r}{m=\\Omega\\left(\\sqrt{\\frac{k\\log(1/\\delta)}{\\varepsilon n}}\\log\\frac{d}{\\beta}\\right)}\\end{array}$ Lemma D.6 implies that w.p. $1-\\beta$ , the output of TopVar, $I_{\\mathrm{top}}$ , is a set of size $k$ , containing indices of elements whose variances are at least $\\hat{R}/32$ . By Lemma D.5, as long as $\\begin{array}{r}{m=\\Omega\\left(\\frac{1}{\\varepsilon^{\\prime}}\\log\\frac{1}{\\delta^{\\prime}\\beta^{\\prime}}\\right)=}\\end{array}$ $\\Omega\\left(\\frac{\\sqrt{k\\log(1/\\delta)}}{\\varepsilon}\\log\\frac{k}{\\delta\\beta}\\right)$ , the estimates $\\hat{\\Sigma}_{i i}$ to the variances in the indices in $I_{\\mathrm{top}}$ are accurate up to a constant factor, with a failure probability of $\\beta/k$ for each invocation of this lemma, which sums up to a failure probability of $\\beta$ . Similarly, the estimate $\\hat{S}_{\\mathrm{bot}}$ has the same guarantee. If $n$ is large enough to satisfy the requirement of Line 1, then all previous constraints on $m$ are satisfied. ", "page_idx": 21}, {"type": "text", "text": "Lastly, the two estimates from $\\mathrm{Avg}_{M,\\lambda,\\varepsilon,\\delta}$ suffer an approximation of $\\alpha$ , each with a failure probability of $\\beta$ , provided that the conditions on the sample complexity $n$ , that are given in Eq. (17) and Eq. (18), hold. By assumption on the sample complexity (Eq. (13),(14)), the guarantee of Eq. (17) indeed holds. It remains to prove that the guarantee of Eq. (18) holds as well. We analyze the term $\\sqrt{\\Sigma_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}}$ . Denote vector $\\sigma_{\\mathrm{bot}}=(\\{\\sigma_{i}\\}_{i\\in I_{\\mathrm{bot}}})$ where $\\sigma_{i}=\\Sigma_{i i}^{1/2}$ . Then $\\begin{array}{r}{\\sqrt{\\sum_{i\\in I_{\\mathrm{bot}}}\\sum_{i i}}=\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}}\\end{array}$ . By H\u00f6lder\u2019s inequality, ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{2}\\leq\\sqrt{\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{1}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}}\\leq\\sqrt{\\|\\pmb{\\sigma}\\|_{1}\\|\\pmb{\\sigma}_{\\mathrm{bot}}\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the guarantees of TopVar and FindKthLargestVariance, except for a failure probability of $\\bar{O^{(\\beta)}}$ , there exists a universal constant $C>1$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}^{1/2}\\leq C\\hat{R}^{1/2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Further, by assumption, $\\hat{R}$ is up to a constant the $k$ -th largest diagonal element of $\\Sigma$ , hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\nC\\hat{R}^{1/2}\\leq\\frac1k\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting this above, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{i\\in I_{\\mathrm{bot}}}\\Sigma_{i i}}\\le\\frac{1}{\\sqrt{k}}\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, it suffices for the stated sample complexity to additionally satisfy $\\begin{array}{r l r l}{n}&{{}}&{=}\\end{array}$ $\\tilde{\\Omega}\\left(\\frac{\\sqrt{d\\log(1/\\delta)}\\sum_{i=1}^{d}\\Sigma_{i i}^{1/2}}{\\sqrt{k}\\alpha\\varepsilon}\\right)$ Substituting the definition for $k$ , we obtain Eq. (15), which completes the proof. ", "page_idx": 22}, {"type": "text", "text": "Privacy analysis. Notice that the output of the algorithm is obtained by composing multiple differentially private mechanisms. Some of these mechanisms access the estimates $V^{(1)},\\bar{\\b{\\ldots}},V^{[m)}$ instead of the original dataset. Yet, since each input datapoint $X^{(i)}$ influences only one vector $V^{(j)}$ , this implies that any DP guarantees for algorithms that use the $V^{(j)}$ estimates, directly translate to DP guarantees on the original input dataset. ", "page_idx": 22}, {"type": "text", "text": "Notice that the algorithm has $O(1)$ calls to $(\\varepsilon,\\delta)$ -DP mechanisms, and $k$ calls to $(\\varepsilon^{\\prime},\\delta^{\\prime})$ -DP mechanisms: these are the calls to VarianceSum. By Lemma A.6 (advanced composition), the concatenation of all the calls to VarianceSum are together, $(O(\\varepsilon),O(\\delta))$ . By basic composition of the same lemma, composing the resulting composion with the other calls to DP mechanisms, yields an $(O(\\varepsilon),O(\\delta))$ -DP mechanism. ", "page_idx": 22}, {"type": "text", "text": "D.1 The variance estimates are valid: Proof of Lemma D.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The random variable $(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)})/\\sqrt{2\\Sigma_{i i}}$ is standard normal. Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{r=1}^{\\ell}\\frac{(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)})^{2}}{2\\Sigma_{i i}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "follows a chi-squared distribution with $\\ell$ degrees of freedom. We use the following concentration property of a Chi-squared random variable [45, Lemma 1]: if $Z$ is Chi-squared with $\\ell$ degrees of freedom, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathbb{E}[Z]/2\\le Z\\le2\\,\\mathbb{E}[Z]]\\ge1-2e^{-c\\ell}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some constant $c>0$ . Consequently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\frac{\\ell}{2}\\leq\\sum_{r=1}^{\\ell}\\frac{(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)})^{2}}{2\\Sigma_{i i}}\\leq2\\ell\\right]\\geq1-2e^{-c\\ell},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some constant $c>0$ . ", "page_idx": 22}, {"type": "text", "text": "By a union bound over all dimensions $i$ , the probability that all dimensions\u2019 variance estimates fall within the specified bounds in a single group $j$ is at least $1-2d\\exp(-c\\ell)$ . Assuming $\\ell\\geq C\\log d$ ensures this probability is very high (e.g., at least $7/8$ for suitable constant $C$ ). ", "page_idx": 22}, {"type": "text", "text": "Using the Chernoff bound for the binomial distribution, if each group independently satisfies the variance bounds with probability at least $7/8$ , then the probability that at least $4/5$ of the groups satisfy the variance bounds is at least $1-\\beta$ for $m=\\Omega(\\bar{\\log}(1/\\beta))$ . ", "page_idx": 22}, {"type": "text", "text": "D.2 Finding the indices of the largest variances: Proof of Lemma D.6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We propose an algorithm which, receives estimates $V_{i}^{(j)}$ for the variances and a threshold $R$ , and outputs $k$ indices $i\\in[d]$ whose variance is at least $R/\\dot{C}$ for some universal constant $C$ . To do so, we use the sparse vector algorithm, which receives a dataset $D$ , queries $Q_{1}(D),\\dots,Q_{d}(D)$ , a threshold $T$ and a natural number $k$ . It outputs $k$ indices $i$ such that $Q_{i}(D)\\geq T$ (approximately). In order to use the sparse vector to identify the largest variances, our dataset $D$ will be $V$ , the collection of estimates. The query $Q_{i}(V)$ will capture whether the $i$ \u2019th variance is $\\Omega(R)$ . We define the query ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{i}(V)=\\frac{1}{m}\\left|\\left\\{j:V_{i}^{(j)}\\geq R/2\\right\\}\\right|\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the threshold $T=1/2$ . Intuitively, if $Q_{i}(V)\\geq1/2$ this means that at least half of the values of $\\boldsymbol{j},V_{i}^{(j)}\\ge R/2$ , which implies that $\\Sigma_{i i}\\geq\\Omega(R)$ , provided that the estimates $V$ are valid. Otherwise, it implies $\\Sigma_{i i}\\leq R$ . ", "page_idx": 23}, {"type": "text", "text": "We now formally define the sparse vector algorithm [26, 54, 31], and review its guarantees. See [23, Section 3.6] for a detailed analysis of the sparse vector technique. ", "page_idx": 23}, {"type": "text", "text": "Algorithm 3 Sparse(D, {Qi}, T, d, \u03b5, \u03b4), from [23] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Require: Input is a private database $D$ , an adaptively chosen stream of sensitivity $1/n$ queries $Q_{1},\\ldots$ a threshold $T$ , a cutoff point $k$ , and privacy parameters $\\varepsilon,\\delta$ .   \n1: $\\begin{array}{r}{\\hat{T}\\leftarrow T+\\mathrm{Lap}\\left(\\frac{2}{\\varepsilon n}\\right)}\\end{array}$   \n$\\sigma\\leftarrow\\sqrt{\\frac{32k\\ln(1/\\delta)}{\\varepsilon n}}$   \n3: count $\\gets0$   \n4: $I\\leftarrow\\emptyset$   \n5: for each query $i$ do   \n6: $v_{i}\\gets\\mathrm{Lap}(\\sigma)$   \n7: if $Q_{i}(D)+v_{i}\\geq{\\hat{T}}$ then   \n8: $I\\leftarrow I\\cup\\{i\\}$   \n9: count \u2190count + 1   \n10: if count $\\geq k$ then   \n11: return I   \n12: return I ", "page_idx": 23}, {"type": "text", "text": "Lemma D.7 (Sparse guarantees). Sparse (Algorithm 3) is $(\\varepsilon,\\delta)$ -differentially private. Let $\\beta\\in(0,1)$ and define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha=2\\sigma\\left(\\log d+\\log{\\frac{2}{\\beta}}\\right)={\\sqrt{\\frac{128k\\ln(1/\\delta)}{\\varepsilon n}}}\\left(\\log d+\\log{\\frac{2}{\\beta}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any sequence of $d$ queries $Q_{1},\\ldots,Q_{d}$ if there are at least $k$ queries $i$ such that $Q_{i}(D)\\geq T+\\alpha,$ then the following holds with probability $1-\\beta$ : the output of Algorithm 3, $I$ , is a set of size $k$ , and for each $i\\in I$ , $Q_{i}(D)\\geq T-\\alpha$ . ", "page_idx": 23}, {"type": "text", "text": "Next, we formally define the algorithm TopVar, to find the indices of the largest variances. ", "page_idx": 23}, {"type": "text", "text": "Algorithm 4 TopVar\u03b5,\u03b4(V, R, k) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Require: Variance estimates $V=\\{V_{i}^{(j)}\\}_{j\\in[m],i\\in[d]}$ , threshold $R\\in\\mathbb{R}$ , privacy parameters $\\varepsilon,\\delta\\in$ $(0,1)$ , number of indices $k\\in\\mathbb{N}$ .   \n1: Define queries $Q_{i}(D)$ for each $i\\in[d]$ as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{i}(D)=\\frac{1}{m}\\left|\\left\\{j:V_{i}^{(j)}\\geq R/2\\right\\}\\right|\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2: $T\\gets1/2$ ", "page_idx": 23}, {"type": "text", "text": "3: return Sparse(V, {Qi}, T, k, \u03b4) ", "page_idx": 23}, {"type": "text", "text": "The privacy guarantees of TopVar follow directly from the guarantees of the sparse vector. Next, we describe how to derive the accuracy guarantees. Notice that if the $V_{i}^{(j)}$ are valid, then, for any $i$ such that $\\Sigma_{i i}\\geq R$ : for at least $4m/5$ values of $j$ , it holds that $V_{i}^{(j)}\\geq R/2$ , hence, $Q_{i}(D)\\geq4/5$ . Further, for any $i$ such that $\\Sigma_{i i}<R/4$ , for at least $4m/5$ values of $j$ it holds that $V_{i}^{(j)}<R/2$ , hence $Q_{i}(D)\\leq1/5$ . Hence, if we set the threshold at $T=1/2$ , and $\\alpha=1/4$ , then, for any $i$ output by the algorithm, $\\dot{\\Sigma}_{i i}\\geq R/4$ . Further, if there are at least $k$ indices $i$ such that $Q_{i i}\\geq R$ , the algorithm will output $k$ indices. ", "page_idx": 24}, {"type": "text", "text": "D.3 Finding the $k$ -th largest variance: Proof of Lemma D.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We propose an algorithm, Algorithm 5, that receives pre-computed variance estimates i(j)for each group $j$ and coordinate $i$ . The algorithm uses them to compute an estimate for the $k$ -th largest variance for each $V^{(j)}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nM_{j}:=\\mathrm{k\\mathrm{-}t h\\;l a r g e s t\\;o f~}\\left\\{V_{1}^{(j)},\\ldots,V_{d}^{(j)}\\right\\}_{i\\in[d]}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Our algorithm combines all of these estimates in a differentially private manner, using a stable histogram: Algorithm 6. That algorithm splits the real line into buckets, $\\{B_{b}\\}_{b\\in\\mathbb{Z}\\cup\\{-\\infty\\}}$ . It receives the estimates $M_{1},\\ldots,M_{m}\\in\\mathbb{R}$ and outputs the index $b$ of the bucket that contains the largest number of estimates $M_{j}$ (approximately). ", "page_idx": 24}, {"type": "text", "text": "In our application, we would like to estimate the $k$ -th largest variance up to a multiplicative constant factor, hence, we define the buckets as ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{b}={\\binom{[4^{b},4^{b+1})}{\\{0\\}}}\\quad b\\in\\mathbb{Z}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Denote by $b^{*}$ index of the bucket that contains the $k$ -th largest diagonal entry of $\\Sigma$ . If the estimates $V^{(1)},\\ldots,V^{(m)}$ are valid then, by definition of validity (Definition 4.2), it follows that at least $4m/5$ of the estimates $M_{j}$ fall into the union $B_{b^{*}-1}\\cup B_{b^{*}}\\cup B_{b^{*}+1}$ . Under this assumption, Algorithm 6 is guaranteed to output one of $b^{*}-1,b^{*}$ or $b^{*}+1$ , with probability $1-\\delta$ . ", "page_idx": 24}, {"type": "text", "text": "The algorithm for $k$ -th largest variance, Algorithm 5, is presented here: ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\widehat{\\mathbf{Algorithm}}\\left.5\\operatorname{FindKthLargestVariance}_{\\varepsilon,\\delta}(\\{V_{i}^{(j)}\\}_{i\\in[d],j\\in[m]},k)\\right.}\\end{array}$   \nRequire: Pre-computed variance estimates $V_{i}^{(j)}$ for each group $j$ and each coordinate $i$ . Privacy parameters $\\varepsilon,\\delta>0$ . Integer $k\\leq d$ . Number of groups $m$ .   \n1: for $j\\in[m]$ do   \n2: $M_{j}\\gets k$ -th largest value among $\\{V_{1}^{(j)},V_{2}^{(j)},\\cdot\\cdot\\cdot,V_{d}^{(j)}\\}$   \n3: Define bins $\\{B_{b}\\}_{b\\in\\mathbb{Z}\\cup\\{-\\infty\\}}$ by: $B_{b}={\\left\\{\\begin{array}{l l}{[4^{b},4^{b+1})}&{b\\in\\mathbb{Z}}\\\\ {\\{0\\}}&{b=-\\infty}\\end{array}\\right.}$   \n4 $:\\mathit{b}\\gets\\mathrm{StableHistogram}_{\\varepsilon,\\delta}(\\{M_{j}\\}_{j\\in[m]},\\{B_{b}\\})$   \n5: return ${\\hat{M}}=4^{b}$ ", "page_idx": 24}, {"type": "text", "text": "We proceed by defining StableHistogram as introduced in [16] and providing its guarantees, and then we conclude with the proof of Lemma D.4. The presentation of StableHistogram is from [13]. ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{Algorithm\\6\\StableHistogram}_{\\varepsilon,\\delta}(\\{M_{i}\\},\\{B_{b}\\}).}\\end{array}$ , from [16] ", "page_idx": 25}, {"type": "text", "text": "Require: Items $M_{1},\\dots,M_{m}\\in\\mathcal{U}$ . Bins $\\{B_{b}\\}_{b\\in\\mathbb{Z}}$ . Privacy parameters $\\varepsilon,\\delta>0$ .   \n1: for $b\\in\\mathbb{Z}$ do   \n2: $c_{b}\\gets|\\{i:z_{i}\\in B_{b}\\}|$   \n3: for $b$ with $c_{b}>0$ do   \n4: $\\tilde{c}_{b}\\gets c_{b}+\\mathrm{Lap}(2/\\varepsilon)$   \n5: \u03c4 \u21901 + 2 log(1/\u03b4)   \n6: Let $b_{\\mathrm{max}}=\\arg\\operatorname*{max}_{b}\\tilde{c}_{b}$ , with arbitrary tie breaks   \n7: if $\\tilde{c}_{b_{\\mathrm{max}}}\\geq\\tau$ then   \n8: return $b_{\\mathrm{max}}$   \n9: else   \n10: return $\\bot$ ", "page_idx": 25}, {"type": "text", "text": "We use its privacy and accuracy guarantees, proved as Lemma C.1 in [13]: ", "page_idx": 25}, {"type": "text", "text": "Lemma D.8 (Stable Histogram Guarantees). StableHistogr $\\mathrm{\\nablaam}_{\\varepsilon,\\delta}$ (Algorithm 6) is $(\\varepsilon,\\delta)$ - differentially private. Suppose that there exists $b^{*}\\in\\mathbb{Z}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\left\\{M_{1},\\ldots,M_{m}\\right\\}\\cap\\left(B_{b^{*}-1}\\cup B_{b^{*}}\\cup B_{b^{*}+1}\\right)\\right|\\geq3m/4\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "There exists a constant $C>0$ such that, for all $0<\\varepsilon,\\beta,\\delta<1,$ , if ", "page_idx": 25}, {"type": "equation", "text": "$$\nm\\geq\\frac{C}{\\varepsilon}\\log\\frac{1}{\\delta\\beta},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then with probability at least $1-\\beta$ , the algorithm\u2019s output lies in $\\{b-1,b,b+1\\}$ . ", "page_idx": 25}, {"type": "text", "text": "The privacy guarantees of Algorithm 5 follow directly from the privacy guarantees of Algorithm 6. For the accuracy guarantees, notice that if the estimates $V^{(j)}$ are valid then at least $4m/5$ of the values $M_{j}$ fall into the bucket $B_{b^{*}}$ that contains the true value of the $k$ -th largest entry of the diagonal of $\\Sigma$ . Under this assumption, Algorithm 6 is guaranteed to output, with probability $1-\\beta$ , one of $b^{*}-1$ , $b^{*}$ or $b^{*}+1$ . This implies that the output of Algorithm 5 is approximates the target quantity up to a constant, as required. ", "page_idx": 25}, {"type": "text", "text": "D.4 Finding a sum of variances: Proof of Lemma D.5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We propose an algorithm that is similar to Algorithm 5, with a single difference: given each estimate V (j), the algorithm computes ", "page_idx": 25}, {"type": "equation", "text": "$$\nM_{j}=\\sum_{i\\in I}V_{i}^{(j)}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The algorithm is summarized below: ", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\overline{{\\mathrm{Algorithm~7~VarianceSum}_{\\varepsilon,\\delta}(\\{V_{i}^{(j)}\\}_{i\\in[d],j\\in[m]},I)}}}\\end{array}$   \nRequire: Pre-computed variance estimates $V_{i}^{(j)}$ for each group $j$ and each coordinate $i$ . Privacy parameters $\\varepsilon,\\delta>0$ . Subset $I\\subseteq[d]$ . Number of groups $m$ .   \n1: for $j\\in[m]$ do   \n2: $\\begin{array}{r}{M_{j}\\leftarrow\\sum_{i\\in I}V_{i}^{(j)}}\\end{array}$   \n3: Define bins $\\{B_{b}\\}_{b\\in\\mathbb{Z}\\cup\\{-\\infty\\}}$ by: $B_{b}={\\left\\{\\begin{array}{l l}{[4^{b},4^{b+1})}&{b\\in\\mathbb{Z}}\\\\ {\\{0\\}}&{b=-\\infty}\\end{array}\\right.}$   \n4: $b\\gets\\mathrm{StableHistogram}_{\\varepsilon,\\delta}(\\{M_{j}\\}_{j\\in[m]},\\{B_{b}\\})$   \n5: return ${\\hat{M}}=4^{b}$ ", "page_idx": 25}, {"type": "text", "text": "The proof is identical to the proof of Lemma D.4. In order to carry that proof, one has to notice that if $b^{*}$ is the bucket that contains $\\textstyle\\sum_{i\\in I}\\Sigma_{i i}$ and if the estimates $V^{(j)}$ are valid, then at least $4m/5$ of the estimates $M_{j}$ fall within $B_{b^{*}-1}\\,\\bar{\\cup}\\,B_{b^{*}}\\cup B_{b^{*}+1}$ . ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The introduction includes informal theorem statements for each of our main claims. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss limitations of this work in the introduction, in Section 4, and when outlining directions for future work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All theorems in the technical Sections 3, 4, as well as the Appendix include their assumptions in the theorem statement. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We believe that the presented research conforms with the NeurIPS Code of Ethics, as it is theoretical and does not enable harmful consequences. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper is theoretical and contributes to a line of work that aims to design privacy-preserving statistical estimators, which use few samples, even in high-dimensional settings. We believe that in the long-term, the societal impact of this line of work will be positive, in the sense that it will enable the use of privacy-preserving methods, and possibly even mitigate the need for collection of large data sets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not contribute data or models with high risk of misuse. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]