[{"type": "text", "text": "Fair Wasserstein Coresets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zikai Xiong\u2217 Niccol\u00f2 Dalmasso\u2020 Shubham Sharma\u2020 Freddy Lecue\u2020 ", "page_idx": 0}, {"type": "text", "text": "Daniele Magazzeni\u2020 Vamsi K. Potluru\u2020 Tucker Balch\u2020 Manuela Veloso\u2020 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equivalent to Lloyd\u2019s algorithm for $\\mathbf{k}$ -medians and $\\boldsymbol{\\mathrm{k}}$ -means clustering. Experiments conducted on both synthetic and real datasets show that FWC: (i) achieves a competitive fairness-utility tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the last decade, the rapid pace of technological advancement has provided the ability of collecting, storing and processing massive amounts of data from multiple sources [65]. As the volume of data continues to surge, it often surpasses both the available computational resources as well as the capacity of machine learning algorithms. In response to this limitation, dataset distillation approaches aim to reduce the amount of data by creating a smaller, yet representative, set of samples; see [80, 40] for comprehensive reviews on the topic. Among those approaches, coresets provide a weighted subset of the original data that achieve similar performance to the original dataset in (usually) a specific machine learning task, such as clustering [26, 21], Bayesian inference [11], online learning [9] and classification [16], among others. ", "page_idx": 0}, {"type": "text", "text": "In tandem with these developments, the adoption of machine learning techniques has seen a surge in multiple decision-making processes that affect society at large [69, 81]. This proliferation of machine learning applications has highlighted the need to mitigate inherent biases in the data, as these biases can significantly impact the equity of machine learning models and their decisions [14]. Among many definitions of algorithmic fairness, demographic parity is one of the most prominently used metric [29], enforcing the distribution of an outcome of a machine learning model to not differ dramatically across different subgroups in the data. ", "page_idx": 0}, {"type": "text", "text": "Current methodologies for generating a smaller set of fair representative samples focus on the local characteristics of these samples with respect to the original dataset. For instance, [13, 30, 4, 24] obtain representative points by clustering while enforcing each cluster to include the same proportion of points from each subgroup in the original dataset. In another line of work, [33, 44, 53, 72, 12] create representative points by ensuring that points in the original dataset each have at least one representative point within a given distance in the feature space. While these methods can successfully reduce clustering cost and ensure a more evenly spread-out distribution of representative points in the feature space, it is unclear whether such representative samples can positively affect performance or discrimination reduction in downstream learning processes. As the induced distribution of the representative points might be far away from the original dataset distribution, downstream machine learning algorithm might lose significant performance due to this distribution shift, without necessarily reducing biases in the original data (as we also demonstrate empirically in our experiments). ", "page_idx": 1}, {"type": "text", "text": "Contributions In this work, we introduce Fair Wasserstein Coresets (FWC), a novel coreset approach that not only generates synthetic representative samples but also assigns sample-level weights to be used in downstream learning tasks. FWC generates synthetic samples by minimizing the Wasserstein distance between the distribution of the original datasets and that of the weighted synthetic samples, while simultaneously enforcing an empirical version of demographic parity. The Wasserstein distance is particularly suitable to this task due to its various connections with downstream learning processes and coresets generation (Section 2). Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. we show how the FWC optimization problem can be reduced to a nested minimization problem in which fairness constraints are equivalent to linear constraints (Section 3); ", "page_idx": 1}, {"type": "text", "text": "2. we develop an efficient majority minimization algorithm [56, 39] to solve the reformulated problem (Section 4). We analyze theoretical properties of our proposed algorithm and FWC (Section 5) and show that, in the absence of fairness constraints, our algorithm reduces to an equivalent version of Lloyd\u2019s algorithm for $\\mathbf{k}$ -means and $\\mathbf{k}$ -medians clustering, extending its applicability beyond fairness applications (Section 6); ", "page_idx": 1}, {"type": "text", "text": "3. we empirically validate the scalability and effectiveness of FWC by providing experiments on both synthetic and real datasets (Section 7). In downstream learning tasks, FWC result in competitive fairness-utility tradeoffs against current approaches, even when we enhance the fairness of existing approaches using fair pre-processing techniques, with an average disparity reduction of $53\\%$ and $18\\%$ , respectively. In addition, we show FWC can correct biases in large language models when passing coresets as examples to the LLM, reducing downstream disparities by $75\\%$ with GPT-3.5 [55] by $35\\%$ with GPT-4 [1]. Finally, we show FWC can improve downstream fairness-utility tradeoffs in downstream models when added to the training data (via data augmentation, see Appendix C.2). ", "page_idx": 1}, {"type": "text", "text": "Finally, we refer the reader to the Appendix for more details on the optimization problem (Section A), theoretical proofs (Section B) and further experiments and details (Section C). ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation We indicate the original dataset samples $\\{Z_{i}\\}_{i=1}^{n}$ , with $Z_{i}\\;=\\;(D_{i},X_{i},Y_{i})\\;\\in\\;{\\mathcal{Z}}\\;=$ $(\\mathcal{D}\\times\\mathcal{X}\\times\\mathcal{Y})\\subseteq\\mathbb{R}^{d}$ , where $X$ indicates the non-sensitive features, $D$ indicates one or more protected attributes such as ethnicity or gender, and $Y$ is a decision outcome. In this work, we assume $D$ and $Y$ to be discrete features, i.e., to have a finite number of levels so that $|D|\\ll n$ and $|\\mathcal{V}|\\ll n$ . For example, $Y$ might indicate a credit card approval decision based on credit history $X$ , with $D$ denoting sensitive demographic information. Given a set of weights $\\{\\boldsymbol{\\theta}\\}_{i=1}^{n}$ , define $p_{Z;\\theta}$ the weighted distribution of a dataset {Zi}in=1 as pZ;\u03b8 = n1 in=1 , where $\\delta_{x}$ indicates the Dirac delta distribution, i.e., the unit mass distribution at point $x\\,\\in\\,{\\mathcal{X}}$ . Using this notation, we can express the empirical distribution of the original dataset by setting $\\theta_{i}\\,=\\,e_{i}\\,=\\,1$ for any $i$ , i.e., $\\begin{array}{r}{p_{Z;e}^{\\dot{}}\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}e_{i}\\delta_{Z_{i}}}\\end{array}$ . For a matrix $A,A^{\\top}$ denotes its transpose. For two vectors (or matrices) $\\langle u,v\\rangle\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\sum_{i}u_{i}v_{i}$ is the canonical inner product (the Frobenius dot-product for matrices). We define $\\mathbf{1}_{m}\\ \\stackrel{\\mathrm{def.}}{=}\\ (1,\\ldots,1)\\in\\mathbb{R}_{+}^{m}$ . ", "page_idx": 1}, {"type": "text", "text": "Wasserstein distance and coresets Given two probability distributions $p_{1}$ and $p_{2}$ over a metric space $\\mathcal{X}$ , the Wasserstein distance, or optimal transport metric, quantifies the distance between the two distributions as solution of the following linear program (LP): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{W}_{c}(p_{1},p_{2})\\stackrel{\\mathrm{def.}}{=}\\operatorname*{min}_{\\pi\\in\\Pi(p_{1},p_{2})}\\int_{\\mathcal{X}\\times\\mathcal{X}}c(x_{1},x_{2})\\mathrm{d}\\pi(x_{1},x_{2}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\Pi(p_{1},p_{2})$ indicating the set of all joint probability distributions over the product space $\\mathcal X\\times\\mathcal X$ with marginals equal to $(p_{1},p_{2})$ [35]. The operator $c(x,y)$ represents the \u201ccost\u201d of moving probability mass from $x$ and $y$ , and reduces to a matrix $C$ if the underlying metric space $\\mathcal{X}$ is discrete. ", "page_idx": 2}, {"type": "text", "text": "The Wasserstein distance has several connections with downstream learning processes and coresets. Firstly, the higher the Wasserstein distance between the weighted representative samples $p_{\\hat{Z};\\theta}$ and the original dataset $p z{;}e$ , the higher the distribution shift, the more degradation we might expect in terms of downstream learning performance [62]. Secondly, the Wasserstein distance between two probability distributions can also be used to bound the deviation of functions applied to samples from such distributions. Define the following deviation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd(p_{\\hat{Z};\\theta},p_{Z;e})\\;\\stackrel{\\mathrm{def.}}{=}\\;\\operatorname*{sup}_{f\\in{\\mathcal{F}}}\\left|\\mathbb{E}_{z\\sim p_{\\hat{Z};\\theta}}f(z)-\\mathbb{E}_{z\\sim p_{Z;e}}f(z)\\right|\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When $\\mathcal{F}$ is the class of Lipschitz-continuous functions with Lipschitz constant equal or less than 1, the deviation $d(p_{\\hat{Z};\\theta},p_{Z;e})$ is equal to 1-Wasserstein distance $\\mathcal{W}_{1}(p_{\\hat{Z};\\theta},p_{Z;e})$ [66, 74]. The connection with learning processes and the downstream deviation $d(p_{(\\hat{X},\\hat{D});\\theta},p_{(X,D);e})$ is immediate when considering Lipschitz continuous classifiers with Lipschitz constant less than 1 (such as logistic regression or Lipschitz-constrained neural networks $[2])^{3}$ . For other classifiers, we note that the 1-Wasserstein distance still bounds the downstream discrepancy: in Proposition 2.1 we show that the 1-Wasserstein distance bounds the downstream discrepancy for ReLu-activated multilayer perceptrons (MLPs), which we use in our experiments on real datasets (Section 7). ", "page_idx": 2}, {"type": "text", "text": "Proposition 2.1. Let $g_{\\psi}\\in\\mathcal{G}^{K}$ be the class of $K$ -layer multilayer perceptrons with ReLu activations. Then, the downstream discrepancy in downstream performance of $g_{\\psi}$ applied to samples from $p_{(\\hat{X},\\hat{D});\\theta}$ and $p_{(X,D);e}$ is bounded by the $^{\\,l}$ -Wasserstein distance : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(p_{(\\hat{X},\\hat{D});\\theta},p_{(X,D);e})=}\\\\ &{\\qquad\\underset{g_{\\psi}\\in\\mathcal{G}^{\\kappa}}{\\operatorname*{sup}}\\left|\\mathbb{E}_{(x,d)\\sim p_{(\\hat{X},\\hat{D});\\theta}}g_{\\psi}(x,d)-\\mathbb{E}_{(x,d)\\sim p_{(X,D);e}}g_{\\psi}(x,d)\\right|\\leq L_{k}\\mathcal{W}_{1}(p_{\\hat{Z};\\theta},p_{Z;e}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L_{k}$ is the MLP Lipschitz constant upper bound defined in [75, Section 6.1, Equation $(\\vartheta)J$ . ", "page_idx": 2}, {"type": "text", "text": "We point out that minimizing the Wasserstein distance, hence bounding the downstream performance as in Equation (2), is equivalent to the definition of a measure coresets proposed by [15]. The Wasserstein distance is also connected to coresets in the sense of the best discrete approximation of a continuous distributions. Considering a feature space endowed with a continuous distribution, minimizing the $p$ -Wasserstein distance across all the distributions of size $m$ is topologically equivalent to identify the samples of size $m$ that provide the best Voronoi tessellation of the space in $L_{p}$ sense [57, 41]. Although other definitions of coresets using Wasserstein distance have been proposed in the literature, they require either to solve the underlying optimal transport problems or the knowledge of the downstream classifier and loss function [15, 46, 82, 43]. FWC is agnostic to any downstream model or loss function, and uses an efficient implementation that does not actually incur in the usual high cost connected to optimal transport. By adapting the approach proposed by [77], we solve an equivalent re-formulated linear optimization problem, which is more computationally tractable than classic approaches such as the simplex or the interior point method (see Section 4.1). ", "page_idx": 2}, {"type": "text", "text": "Demographic parity Also known as statistical parity, demographic parity (DP) imposes the decision outcome and protected attributes to be independent [18]. Using a credit card approval decision example, demographic parity enforces an automatic decision process to approve similar proportion of applicants across different race demographics. DP is one of the most extensively analyzed fairness criterion; we refer the reader to [29] for a review. In this work, we use the demographic parity definition by [10], which enforces the ratio between the conditional distribution of the decision outcome across each subgroups $p(y|D=d)$ and the marginal distribution of the decision outcome $p(y)$ to be close to 1 in a given dataset. We refer to the \u201cempirical version\u201d of demographic parity to indicate that the conditional and marginal distributions are quantities estimated from the data. Note that the demographic parity definition we adopt enforces a condition on the weighted average of the conditional distributions across groups, which is different from a more recent approach of using Wasserstein distance, and specifically Wasserstein barycenters, to enforce demographic parity [25, 32, 23, 78]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 FWC: Fair Wasserstein Coresets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a dataset $\\{Z_{i}\\}_{i=1}^{n}$ , our goal is to find a set of samples $\\{\\hat{Z}_{j}\\}_{j=1}^{m}$ and weights $\\{\\theta_{j}\\}_{j=1}^{m}$ such that $m\\ll n$ and that the Wasserstein distance between $p z{;}e$ and $p_{\\hat{Z};\\theta}$ is as small as possible. In addition, we use the fairness constraints proposed by [10] to control the demographic parity violation for the $p_{\\hat{Z};\\theta}$ distribution. Let $p_{\\hat{Z};\\theta}(y|d)$ indicate the conditional distribution $\\dot{p_{\\hat{Z};\\theta}}(\\bar{\\hat{Y}}=\\dot{y}|\\hat{D}=d)$ . Imposing a constraint on the demographic disparity violation then reduces to requiring the conditional distribution under the weights $\\{\\theta_{i}\\}_{i\\in[n]}$ to be close to a target distribution $p_{Y_{T}}$ across all possible values of the protected attributes $D$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ\\left(p_{\\hat{Z};\\theta}(y|d),p_{Y_{T}}(y)\\right)=\\left|\\frac{p_{\\hat{Z};\\theta}(y|d)}{p_{Y_{T}}(y)}-1\\right|\\leq\\epsilon,\\,\\forall\\,d\\in\\mathcal{D},y\\in\\mathcal{Y},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon$ is a parameter that determines the maximum fairness violation, and $J(\\cdot,\\cdot)$ is the probability ratio between distributions as defined in [10]. ", "page_idx": 3}, {"type": "text", "text": "Using the notation above, our goal can then be formulated as the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Delta_{m},\\hat{Z}\\in\\mathcal{Z}^{m}}{\\operatorname*{min}}\\ \\mathcal{W}_{c}(p_{\\hat{Z};\\theta},p_{Z;e})}\\\\ &{\\qquad\\mathrm{s.t.}\\quad J\\left(p_{\\hat{Z};\\theta}(y\\vert d),p_{Y_{T}}(y)\\right)\\leq\\epsilon,\\;\\forall\\;d\\in\\mathcal{D},y\\in\\mathcal{Y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta_{m}$ indicates the set of valid weights $\\begin{array}{r}{\\{\\theta\\in\\mathbb{R}_{+}^{m}:\\sum_{i=1}^{m}\\theta_{i}=m\\}_{.}}\\end{array}$ . Note that the optimization problem in (4) shares some similarities with the optimizat ion problem in [77] in using the Wasserstein distance as a distance metric between distributions and using (3) to enforce demographic parity. However, [77] only provide sample-level integer weights for the original dataset and do not generate any new samples, while our approach provides a separate set of samples $\\{\\hat{Z}_{j}\\}_{j=1}^{m}$ with associated real-valued weights $\\{\\theta_{j}\\}_{j=1}^{m}$ , with $m\\ll n$ . ", "page_idx": 3}, {"type": "text", "text": "We now take the following steps to solve the optimization problem in (4): (i) we reduce the dimensionality of the feasible set by fixing Y\u02c6 and $\\hat{D}$ a priori, (ii) we formulate the fairness constraints as linear constraints, (iii) we add artificial variables to express the objective function and (iv) we simplify the optimization problem to minimizing a continuous non-convex function of the $\\{\\hat{X}_{j}\\}_{j=1}^{m}$ . ", "page_idx": 3}, {"type": "text", "text": "Step 1. Reduce the feasible set of the optimization problem As in practice all possible $Y_{i}$ and $D_{i}$ are known a priori, and there are only a limited number of them, we can avoid optimizing over them and instead manually set the proportion of each combination of $\\hat{Y}$ and $\\hat{D}$ . This reduces the optimization problem feasible set only over $\\Delta_{m}$ and ${\\mathcal{X}}^{m}$ . The following lemma shows that this in fact does not affect the optimization problem: ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. For any $m>0$ , the best fair Wasserstein coreset formed by $m$ data points $\\{\\hat{Z}_{i}:i\\in[m]\\}$ is no better (i.e., the optimal Wasserstein distance value is no lower) than the best fair Wasserstein coreset formed by $m|\\mathcal{D}||\\mathcal{Y}|$ data points $\\{(d,X_{i},y)_{i}:i\\in[m],d\\in\\mathcal{D},y\\in\\mathcal{Y}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Hence, we simply set the proportions of $\\{(\\underline{{\\hat{D}_{i}}},\\hat{Y})_{i}\\}_{i\\in[m]_{*}}$ in the coresets to be similar to their respective proportions in the original dataset. The optimization problem then reduces to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Delta_{m},\\hat{X}\\in\\mathcal{X}^{m}}{\\operatorname*{min}}\\,\\mathcal{W}_{c}(p_{\\hat{Z};\\theta},p_{Z;e})}\\\\ &{\\qquad\\mathrm{s.t.}\\qquad J\\left(p_{\\hat{Z};\\theta}(y|d),p_{Y_{T}}(y)\\right)\\leq\\epsilon,\\,\\forall\\,d\\in\\mathcal{D},y\\in\\mathcal{Y}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whose solutions are the features in the coreset $\\{\\hat{X}_{j}\\}_{j=1}^{m}$ and the corresponding weights $\\{\\theta_{j}\\}_{j=1}^{m}$ . ", "page_idx": 4}, {"type": "text", "text": "Step 2. Equivalent linear constraints Following [77], the fairness constraint in Equation (3) can be expressed as $2|\\mathcal{V}||\\mathcal{D}|$ linear constraints on the weights $\\theta$ , as the disparity reduces to the following for all $d\\in\\mathcal{D},y\\in\\mathcal{D}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{\\substack{\\in[m]:\\hat{\\mathcal{D}}_{i}=d,\\hat{Y}_{i}=y}}\\theta_{i}\\le(1+\\epsilon)\\cdot p_{Y_{T}}(y)\\cdot\\sum_{\\substack{i\\in[m]:\\hat{\\mathcal{D}}_{i}=d}}\\theta_{i}\\ ,\\,\\sum_{\\substack{i\\in[m]:\\hat{\\mathcal{D}}_{i}=d,\\hat{\\mathcal{Y}}_{i}=y}}\\theta_{i}\\ge(1-\\epsilon)\\cdot p_{Y_{T}}(y)\\cdot\\sum_{\\substack{i\\in[m]:\\hat{\\mathcal{D}}_{i}=d}}\\theta_{i}\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can express these by using a $2|\\mathcal{V}||\\mathcal{D}|$ -row matrix $A$ as $A\\theta\\geq{\\bf0}$ . ", "page_idx": 4}, {"type": "text", "text": "Step 3. Reformulate the objective function by introducing artificial variables When keeping the samples $\\hat{X}$ fixed, we can follow [60] to derive an equivalent formulation of the Wasserstein distance in the objective as a linear program with mn variables. By indicating the transportation cost matrix $C({\\hat{X}})$ , we define its components as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\nC(\\hat{X})_{i j}\\;\\stackrel{\\mathrm{def.}}{=}\\;c(Z_{i},\\hat{Z}_{j}),\\;\\mathrm{for}\\;i\\in[n],j\\in[m]\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $C({\\hat{X}})$ is a convex function of $\\hat{X}$ when, e.g., using any $L^{p}$ norm to define the transportation cost. Therefore, now the problem (5) is equivalent to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\hat{X}\\in\\mathcal{X}^{m},\\boldsymbol{\\theta}\\in\\Delta_{m},P\\in\\mathbb{R}^{n\\times m}}\\langle C(\\hat{X}),P\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\mathrm{s.t.}\\ \\ P\\mathbf{1}_{m}=\\frac{1}{n}\\cdot\\mathbf{1}_{n},\\ P^{\\top}\\mathbf{1}_{n}=\\frac{1}{m}\\cdot\\boldsymbol{\\theta},\\ P\\geq\\mathbf{0},\\ A\\boldsymbol{\\theta}\\geq\\mathbf{0}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Step 4. Reduce to an optimization problem of $\\hat{X}$ As from one of the constraints we get $\\theta=$ $m\\cdot\\bar{P}^{\\top}{\\mathbf{1}}_{n}$ , we further simplify problem (6) as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\hat{X}\\in\\mathcal{X}^{m},P\\in{\\mathbb R}^{n\\times m}}\\,\\langle C(\\hat{X}),P\\rangle}\\\\ &{\\quad\\quad\\mathrm{s.t.}\\quad\\quad\\,P\\mathbf{1}_{m}=\\frac{1}{n}\\cdot\\mathbf{1}_{n},\\;P\\geq\\mathbf{0},\\;A P^{\\top}\\mathbf{1}_{n}\\geq\\mathbf{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $F(C)$ , as a function $F$ of $C$ , denote the optimal objective value of the following optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{P\\in\\mathbb{R}^{n\\times m}}{\\operatorname*{min}}\\left\\langle C,P\\right\\rangle}\\\\ &{\\quad\\mathrm{s.t.}\\quad P\\mathbf{1}_{m}=\\frac{1}{n}\\cdot\\mathbf{1}_{n},\\;P\\geq\\mathbf{0},\\;A P^{\\top}\\mathbf{1}_{n}\\geq\\mathbf{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and then problem (7) is equivalent to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{X}\\in\\mathcal{X}^{m}}\\ F(C(\\hat{X}))\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In (9) the objective is continuous but nonconvex with respect to $\\hat{X}$ . Once the optimal ${\\hat{X}}^{\\star}$ is solved, then the optimal $P^{\\star}$ of the problem (7) is obtained by solving problem (8) with $C$ replaced with $C(\\hat{X}^{\\star})$ . Finally, the optimal $\\theta^{\\star}$ follows by the equation $\\theta^{\\star}=m\\cdot(P^{\\star})^{\\top}\\mathbf{1}_{n}$ . We now provide a majority minimization algorithm for solving problem (9). ", "page_idx": 4}, {"type": "text", "text": "4 Majority Minimization for Solving the Reformulated Problem ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Majority minimization aims at solving nonconvex optimization problems, and refers to the process of defining a convex surrogate function that upper bounds the nonconvex objective function, so that optimizing the surrogate function improves the objective function [56, 39]. As the algorithm proceeds, the surrogate function also updates accordingly, which ensures the value of the original objective function keeps decreasing. Following this framework, we define the surrogate function $g(\\cdot;\\hat{X}^{k})$ as follows for the $k$ -th iterate $\\hat{X}^{k}\\in\\mathcal{X}^{m}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\ng(\\hat{X};\\hat{X}^{k})\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\langle C(\\hat{X}),P_{k}^{\\star}\\rangle\\ ,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in which $P_{k}^{\\star}$ is the minimizer of problem (8) with the cost $C=C(\\hat{X}^{k})^{4}$ . ", "page_idx": 5}, {"type": "text", "text": "With this surrogate function, Algorithm 1 summarizes the overall algorithm to minimize problem (9). In each iteration of Algorithm 1, line 3 is straightforward since it only involves computing the new cost matrix using the new feature vectors ${\\hat{X}}^{k}$ . We separately discuss how to solve the optimization problems in lines 4 and 5 below. ", "page_idx": 5}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/318725d12f1f910affffe404a40622c271a45169769bf221734827d361a9bc33.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Updating the Surrogate Function (Line 4) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To update the surrogate function, we need to solve problem (8), which is a large-scale linear program. Rather than solving the computationally prohibitive dual problem we solve a lower-dimensional dual problem by using a variant of the FairWASP algorithm proposed by [77]. We adapt FairWASP for cases where $m\\ne\\ n$ to find the solution of (8) via applying the cutting plane methods on the Lagrangian dual problems with reduced dimension5. We choose FairWASP over established commercial solvers due to its computational complexity being lower than other state of the art approaches such as interior-point or simplex method; see Lemma A.2 in Appendix A for more details. ", "page_idx": 5}, {"type": "text", "text": "4.2 Updating Feature Vectors (Line 5) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To update the feature vectors, we need to obtain the minimizer of the surrogate function $g(\\hat{X};\\hat{X}^{k})$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{X}\\in\\mathcal{X}^{m}}\\hat{g}(\\hat{X};\\hat{X}^{k})\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The above can be written as the following problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{X}_{j}\\in\\mathcal{X}:j\\in[m]}\\sum_{i\\in[n]}\\sum_{j\\in[m]}c(Z_{i},\\hat{Z}_{j})P_{i j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $P=P_{k}^{\\star}$ , in which each component of $P$ is nonnegative and $\\hat{Z}_{j}=(\\hat{d}_{j},\\hat{X}_{j},\\hat{y}_{j})$ , for the known fixed $\\hat{d}_{j}$ and ${\\hat{y}}_{j}$ . Furthermore, the matrix $P$ is sparse, containing at most $n$ non-zeros (as when ", "page_idx": 5}, {"type": "text", "text": "updating $P_{k}^{\\star}$ for problem (8), see Appendix A). Moreover, problem (12) can be separated into the following $m$ subproblems, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{X}_{j}\\in\\mathcal{X}}\\sum_{i\\in[n]}c(Z_{i},\\hat{Z}_{j})P_{i j}\\;,\\;\\mathrm{for}\\;j\\in[m]\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Each subproblem computes the weighted centroid of $\\{Z_{i}\\,:\\,i\\,\\in\\,[n],P_{i j}\\,>\\,0\\}$ under the distance function $c$ . Therefore, (11) is suitable for parallel and distributed computing. Additionally, since the cost matrix $C({\\hat{X}})$ is a convex function of $\\hat{X}$ , each subproblem is a convex problem so gradient-based methods could converge to global minimizers. Furthermore, under some particular conditions, solving these small subproblems can be computationally cheap: ", "page_idx": 6}, {"type": "text", "text": "1. If $\\mathcal{X}$ is convex and $c(Z,\\hat{Z})\\ \\stackrel{\\mathrm{def.}}{=}\\ \\|Z-\\hat{Z}\\|_{2}^{2}$ , then the minimizer of (13) is the weighted average $\\begin{array}{r}{\\sum_{i\\in[n]}P_{i j}X_{i}/\\sum_{i\\in[n]}P_{i j}}\\end{array}$ .   \n2. If $\\mathcal{X}$ is convex and $c(Z,\\hat{Z})\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\|Z-\\hat{Z}\\|_{1}$ , then the minimizer of (13) requires sorting the costs coordinate-wisely and finding the median.   \n3. If creating new feature vectors is not permitted and $\\mathcal{X}\\,=\\,\\{X_{i}\\,:\\,i\\,\\in\\,[n]\\}$ , solving (13) requires finding the smallest $\\begin{array}{r l}{\\sum_{i\\in[n]:P_{i j}\\neq0}c(Z_{i},(\\hat{d}_{j},X,\\hat{y}_{j}))P_{i j}}&{{}}\\end{array}$ for $X$ within the finite set $\\mathcal{X}$ . The matrix $P$ is highly sparse so this operation is not computationally expensive. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we provide theoretical insights on FWC complexity, convergence behavior of Algorithm 1 as well as generalizability of FWC performance on unseen test sets. ", "page_idx": 6}, {"type": "text", "text": "5.1 Computational Complexity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we consider the FairWASP variant used in Algorithm 1, line 4. The initialization requires $O(m n)$ flops and uses $O(n|Y||D|)$ space for storing the cost matrix. After that, the per-iteration time and space complexities are both only $O(n|Y||D|)$ . Lemma 5.1 analyzes the computational complexity of our adaptation of the FairWASP algorithm when solving problem (8). ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1. With efficient computation and space management, the cutting plane method has a computational complexity of ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\tilde{O}}\\left(n m+|{\\mathcal{D}}|^{2}|{\\mathcal{V}}|^{2}n\\cdot\\log(R/\\epsilon)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "flops and $O(n|\\mathcal{D}||\\mathcal{V}|)$ space. Here $R$ denotes the size of an optimal dual solution of (8), and $\\tilde{O}(\\cdot)$ absorbs $m$ , $n_{\\!_{\\perp}}$ , $\\left|\\mathcal{D}\\right|,\\left|\\mathcal{D}\\right|$ in the logarithm function. ", "page_idx": 6}, {"type": "text", "text": "Hence, the overall complexity of FWC is $\\tilde{O}(m n+|\\mathcal{D}|^{2}|\\mathcal{V}|^{2}n\\cdot\\log(R/\\epsilon))$ . Note that in practice, both $|\\mathcal D|$ and $|\\mathcal{V}|$ are very small compared with the coreset size $m$ and dataset size $n$ , so the overall complexity is almost as low as $O(m n)$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Convergence Guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we establish that our proposed surrogate function is indeed convex and a valid upper bound. We then show Algorithm 1 converges to a first-order stationary point, within finite iterations if the minimizer of problem (11) is unique. Note that because $g(\\hat{X};\\hat{\\dot{X}^{k}})=\\langle C(\\hat{X}),P_{k}^{\\star}\\rangle$ , the minimizer is unique whenever the cost matrix $C(\\cdot)$ is strongly convex. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.2. The function $g(\\hat{X};\\hat{X}^{k})$ is convex function of $\\hat{X}$ and $a$ valid upper bound, i.e., $g(\\hat{X};\\hat{X}^{k})\\geq F(C(\\hat{X}))$ . This inequality holds at equality when $\\hat{X}=\\hat{X}^{k}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3. The objective value is monotonically decreasing, i.e., $F(C(\\hat{X}^{k+1}))\\leq F(C(\\hat{X}^{k}))$ for any $k\\geq0$ . And once the algorithm stops and $C(\\hat{X}^{k})$ is smooth at ${\\hat{X}}^{k}$ , then ${\\hat{X}}^{k}$ is a first-order stationary point of (9). ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.4. When the minimizer of $(I I)$ is unique, Algorithm $^{\\,I}$ terminates within finite iterations. ", "page_idx": 6}, {"type": "text", "text": "5.3 Generalization Guarantees ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Proposition 5.5 below bounds the distance and demographic parity between the FWC samples and the true underlying distribution of the data, from which the original dataset of size $n$ was observed. This generalizes the performance of FWC to unseen test sets sampled from the data generating distribution. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.5. Let $\\lambda$ indicate the distance between $p_{\\hat{Z};\\theta}$ and $p z{;}e$ after convergence of FWC i.e., $\\begin{array}{r}{\\mathcal{W}(p_{\\hat{Z};\\theta},p_{Z;e})=\\lambda.}\\end{array}$ . Let $q_{Z}$ be the true underlying distribution of the data supported over $\\mathbb{R}^{d}$ , with marginal distribution over y bounded away from zero, so that $\\begin{array}{r}{\\rho=\\operatorname*{min}_{y\\in{\\mathcal{Y}}}q_{Z}(y)>0}\\end{array}$ . Then with probability $1-\\alpha$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\quad\\mathcal{W}_{c}(p_{\\hat{Z}},q_{Z})\\leq\\lambda+\\mathcal{O}(\\log(1/\\alpha)^{1/d}n^{-1/d})}\\\\ &{\\quad\\quad\\quad\\quad\\quad J(p_{\\hat{Z}}(y|d),q_{Y}(y))\\leq\\frac{\\epsilon}{\\rho}+\\mathcal{O}\\left(\\sqrt{\\frac{\\log2/\\alpha}{n\\rho^{2}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In addition, in Appendix B.1 we consider the task of learning using FWC samples. We show that the error in downstream learning tasks can be seen as the sum of (i) the approximation error FWC samples make with respect to the original dataset and (ii) how well $\\hat{Y}$ can be learnt from $\\hat{X}$ and $\\hat{D}$ from FWC samples. However, as FWC samples $\\{\\hat{Z}_{j}\\}_{j=1}^{m}$ are not i.i.d., standard sample complexity results in e.g., empirical risk minimization, do not apply, highlighting the hardness in developing finite-sample learning bounds in this setting. ", "page_idx": 7}, {"type": "text", "text": "6 An Alternative View: Generalized clustering algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When the fairness constraints are absent, problem (8) reduces to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{P\\in\\mathbb{R}^{n\\times m}}~}&{\\langle C,P\\rangle}\\\\ {\\mathrm{s.t.}\\quad}&{P\\mathbf{1}_{m}=\\frac{1}{n}\\cdot\\mathbf{1}_{n},~P\\geq\\mathbf{0}_{n\\times m}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The minimizer $P^{\\star}$ of (17) can be written in closed form. For each $i~\\in~[n]$ , let $C_{i j_{i}^{\\star}}$ denote a smallest component on the $i$ -th row of $C$ . Then the components of a minimizer $P^{\\star}$ can be written as $\\begin{array}{r}{P_{i j}^{\\star}\\;\\stackrel{\\cdot}{=}\\;\\frac{1}{n}\\,\\cdot\\,{\\bf I}(j\\;=\\;j_{i}^{\\star})}\\end{array}$ (where I is the indicator function). Hence, without fairness constraints, FWC corresponds to Lloyd\u2019s algorithm for clustering. Specifically, Lloyd\u2019s algorithm iteratively computes the centroid for each subset in the partition and subsequently re-partitions the input based on the closeness to these centroids [42]; these are the same operations FWC does in optimizing the surrogate function and solving problem (17). Thus, when $c(x,y)$ is correspondingly defined as $\\|\\bar{x_{\\mathrm{~-~}}}y\\|_{1}$ or $\\|{\\boldsymbol{x}}-{\\boldsymbol{y}}\\|_{2}^{2}$ , FWC corresponds to Lloyd\u2019s algorithm applied to $\\mathbf{k}$ -medians or $\\mathbf{k}$ -means problems, except the centroids have fixed values for $\\hat{D}$ and $\\hat{Y}$ (see Section 3). ", "page_idx": 7}, {"type": "text", "text": "Comparison with $\\mathbf{k}$ -means and $\\mathbf{k}$ -medoids FWC and Lloyds\u2019 algorithm for $\\mathbf{k}$ -means or $\\mathbf{k}$ -median share similar per-iteration complexity, with the main difference in complexity due to solving problem (8). We solve this problem efficiently by utilizing a variant of the FairWASP approach by [77] (Section 4.1), hence avoiding the usual complexity in solving optimal transport problems. As shown in Section 5, the leading term in the runtime complexity is ${\\mathcal{O}}(n m)$ , which comes from calculating and storing the cost matrix $C$ . This level of complexity is the same as those in $\\boldsymbol{\\mathrm{k}}$ -means and $\\boldsymbol{\\mathrm{k}}$ -medoids. In addition, from our experiments we also see that the per-iteration complexity of FWC is roughly linear with the original dataset size $n$ (see the runtime experiment in Section 7 and Appendix C). ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Runtime analysis We evaluate the runtime performance of FWC by creating a synthetic dataset of dimension $n$ and features of dimension $p$ , with the goal of creating a coreset of size $m$ (see Appendix C.1 for details). We fix two out of the three parameters to default values $\\left(n,m,p\\right)=$ (5000, 250, 25) and vary the other across suitable ranges, to analyse the runtime and total number of iterations. Figure 1, top left, and Table 2, in Appendix C.1, show the runtime and number of iterations when increasing the dataset size $n$ from 1,000 to 1,000,000, with averages and standard deviations over 10 separate runs; both the runtime and number of iterations grow proportionally to the sample size $n$ . Figure 2 in Appendix C.1 also shows that requiring a larger coreset size $m$ implies the need of fewer iterations but longer iteration runtime, as more representatives need to be computed. ", "page_idx": 7}, {"type": "text", "text": "Real datasets results We evaluate the performance of FWC on 4 datasets widely used in the fairness literature [19]: (i) Adult [7], (ii) German Credit [28], (iii) Communities and Crime [64] and (iv) Drug [20]. For each dataset, we consider 3 different coreset sizes $m\\,=\\,5\\%$ , $10\\%$ , $20\\%$ (apart from the Adult dataset, in which we select $m$ equal to $0.5\\%$ , $1\\%$ and $2\\%$ due to the large dataset size). We compare our approach with: (a) Fairlets and IndFair, two fair clustering approaches by [4] and [12], (b) K-Median Coresets, a coreset approach by [3], (c) $\\mathbf{k}$ -means [42] and $\\mathbf{k}$ -medoids [45, 58], two classic clustering approaches and (d) Uniform Subsampling of the original dataset. For FWC, we consider three different values of the fairness violation hyper-parameters $\\epsilon$ for the optimization problem in (5). We compute the fairness-utility tradeoff by first training a 2-layer multilayer perceptron (MLP) classifier with ReLu activations on the coresets created by each approach and then evaluating the classifier demographic disparity (fairness) and AUC (utility). Figure 1 shows the model with the best fairness-utility tradeoff across the three coreset sizes $m$ , for each approach. FWC obtains equal or better fairness-utility tradeoffs (smaller disparity at the same level of utility, higher utility with the same disparity, or both) across all datasets, and performance remains competitive even when using a fairness pre-processing approach [34]. Appendix C.2 includes more experiments and details, which highlight that: (a) FWC consistently achieves coresets that are closer in distribution to the original dataset with respect to the other methods and, although not natively minimizing clustering cost, also provide competitive performance for smaller datasets (Tables 3 and 4); (b) when added to the training data using the data augmentation schema proposed by [68, Section 2.1] FWC generally either increase the performance or reduce the demographic disparity in the downstream learning process. ", "page_idx": 8}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/ad610c005b4e858214a7ec521e1a867fbacdd73f418559f480d66124bf1b8a52.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Top left: FWC runtime when changing the original dataset size $n$ . Others: Fairnessutility tradeoff on real datasets for a downstream MLP classifier, selecting the model with the best fairness-utility tradeoff across three different coreset sizes $m$ , with averages taken over 10 runs. FWC consistently achieves a comparable/better tradeoff as shown by the Pareto frontier (dashed red line, computed over all models and coreset sizes), even when adjusting the other coresets with a fairness pre-processing technique [33]. See text and Appendix C.2 for more details. ", "page_idx": 8}, {"type": "text", "text": "Using FWC to improve fairness for LLM [76] evaluate GPT-3.5 and GPT-4 for fairness on predictive tasks for the UCI Adult dataset in a zero and few shot setting. We use a similar evaluation setup and use FWC in the few shot setting as examples and evaluate the results for the gender protected attribute. Specifically, we transform the tabular data into language descriptions, and ask GPT-3.5 Turbo and GPT-4 to perform classification tasks on it. We select 200 samples to construct the test set and use a set of 16 samples found using FWC as examples. Further details on this experiment are provided in the appendix. The results are shown in Table 1. Examples provided by FWC help reduce demographic disparity more than providing balanced few shot examples, while losing on predictive accuracy (note that the drop in accuracy is similar to the drop observed in [76] and is representative of the fairness-utility trade-off). Owing to the token limitation of LLM\u2019s, these representative coresets can evidently be valuable to provide a small set of samples that can help mitigate bias. When accounting for the standard deviations for demographic parity in Table 1, FWC reduces the LLM bias when compared to zero shot prompting for GPT-4 across all runs. For GPT-3.5 Turbo, while the average disparity is reduced across runs, such consistency is indeed not observed, owing to a diverse set of outputs from the large language model. Due to limited availability of computational resources (associated with querying these models), we leave a more thorough evaluation across different datasets and models to future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 1: Using the same setup as in [76], we use GPT-3.5 Turbo and GPT-4 LLM\u2019s for fairness evaluations, with a test set of 200 samples with 0.5 base parity $[b_{p}=0.5)$ . Few Shot - FWC is used to provide sixteen examples with weights to the model as examples. Accuracy and demographic disparity (DP) are based on the resulting predictions from GPT-3.5 and GPT-4 models. ", "page_idx": 9}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/bfe06c7303ca29e7e365d63dc72a2650256e71f68ba32d2f318e1e376de6517c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Discussion and Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce FWC, a novel coreset approach that generates synthetic representative samples along with sample-level weights for downstream learning tasks. FWC minimizes the Wasserstein distance between the distribution of the original datasets and that of the weighted synthetic samples while enforcing demographic parity. We demonstrate the effectiveness and scalability of FWC through experiments conducted on both synthetic and real datasets, as well as reducing biases in LLM predictions (GPT 3.5 and GPT 4). Future extensions include: (i) targeting different fairness metrics such as equalized odds [27, 49] as well as robustness of fairness-performance tradeoff over distribution shifts [48, 67], (ii) exploring privacy and explainability properties of FWC [50, 51], (iii) utilizing coresets for accelerating gradient descents algorithms and test their convergence [47, 70] (iv) reformulating the optimization framework to target deep neural network pruning [52, 54] and (v) investigate applications of fair synthetic data in the financial sector [61]. ", "page_idx": 9}, {"type": "text", "text": "Disclaimer ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates (\"JP Morgan\u201d) and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] C. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In International Conference on Machine Learning, pages 291\u2013301. PMLR, 2019.   \n[3] O. Bachem, M. Lucic, and S. Lattanzi. One-shot coresets: The case of $\\mathbf{k}$ -clustering. In International Conference on Artificial Intelligence and Statistics, pages 784\u2013792. PMLR, 2018.   \n[4] A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering. In International Conference on Machine Learning, pages 405\u2013413. PMLR, 2019.   \n[5] S. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning: Limitations and Opportunities. MIT Press, 2023.   \n[6] G. Basso. A hitchhiker\u2019s guide to wasserstein distances. Online manuscript available at https://api. semanticscholar. org/CorpusID, 51801464, 2015.   \n[7] B. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.   \n[8] D. Bertsimas and J. N. Tsitsiklis. Introduction to linear optimization, volume 6. Athena scientific Belmont, MA, 1997.   \n[9] Z. Borsos, M. Mutny, and A. Krause. Coresets via bilevel optimization for continual learning and streaming. Advances in Neural Information Processing Systems, 33:14879\u201314890, 2020.   \n[10] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney. Optimized pre-processing for discrimination prevention. Advances in Neural Information Processing Systems, 30, 2017.   \n[11] T. Campbell and T. Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. In International Conference on Machine Learning, pages 698\u2013706. PMLR, 2018.   \n[12] R. Chhaya, A. Dasgupta, J. Choudhari, and S. Shit. On coresets for fair regression and individually fair clustering. In International Conference on Artificial Intelligence and Statistics, pages 9603\u20139625. PMLR, 2022.   \n[13] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. Advances in Neural Information Processing Systems, 30, 2017.   \n[14] A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2):153\u2013163, 2017.   \n[15] S. Claici, A. Genevay, and J. Solomon. Wasserstein measure coresets. arXiv preprint arXiv:1805.07412, 2018.   \n[16] C. Coleman, C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations, 2020.   \n[17] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages 642\u2013669, 1956.   \n[18] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214\u2013226, 2012.   \n[19] A. Fabris, S. Messina, G. Silvello, and G. A. Susto. Algorithmic fairness datasets: The story so far. Data Mining and Knowledge Discovery, 36(6):2074\u20132152, 2022.   \n[20] E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan, and A. N. Gorban. The five factor model of personality and evaluation of drug consumption risk. In Data Science: Innovative Developments in Data Analysis and Clustering, pages 231\u2013242. Springer, 2017.   \n[21] D. Feldman. Core-sets: Updated survey. Sampling Techniques for Supervised or Unsupervised Tasks, pages 23\u201344, 2020.   \n[22] N. Fournier and A. Guillin. On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707\u2013738, 2015.   \n[23] S. Gaucher, N. Schreuder, and E. Chzhen. Fair learning with wasserstein barycenters for nondecomposable performance measures. In International Conference on Artificial Intelligence and Statistics, pages 2436\u20132459. PMLR, 2023.   \n[24] M. Ghadiri, S. Samadi, and S. Vempala. Socially fair k-means clustering. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 438\u2013448, 2021.   \n[25] P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M. Loubes. Obtaining fairness using optimal transport theory. In International conference on machine learning, pages 2357\u20132365. PMLR, 2019.   \n[26] S. Har-Peled and S. Mazumdar. On coresets for $\\mathbf{k}$ -means and k-median clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291\u2013300, 2004.   \n[27] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016.   \n[28] H. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77.   \n[29] M. Hort, Z. Chen, J. M. Zhang, M. Harman, and F. Sarro. Bias mitigation for machine learning classifiers: A comprehensive survey. ACM J. Responsib. Comput., nov 2023.   \n[30] L. Huang, S. Jiang, and N. Vishnoi. Coresets for clustering with fairness constraints. Advances in Neural Information Processing Systems, 32, 2019.   \n[31] H. Jiang, Y. T. Lee, Z. Song, and S. C.-w. Wong. An improved cutting plane method for convex optimization, convex-concave games, and its applications. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 944\u2013953, 2020.   \n[32] R. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa. Wasserstein fair classification. In Uncertainty in artificial intelligence, pages 862\u2013872. PMLR, 2020.   \n[33] C. Jung, S. Kannan, and N. Lutz. Service in your neighborhood: Fairness in center location. Foundations of Responsible Computing (FORC), 2020.   \n[34] F. Kamiran and T. Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1\u201333, 2012.   \n[35] L. Kantorovitch. On the translocation of masses. Management Science, 5(1):1\u20134, 1958.   \n[36] L. G. Khachiyan. Polynomial algorithms in linear programming. USSR Computational Mathematics and Mathematical Physics, 20(1):53\u201372, 1980.   \n[37] Y.-g. Kim, K. Lee, and M. C. Paik. Conditional wasserstein generator. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[38] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[39] K. Lange. MM optimization algorithms. SIAM, 2016.   \n[40] S. Lei and D. Tao. A comprehensive survey of dataset distillation. IEEE Transactions on Pattern Analysis & Machine Intelligence, 46(01):17\u201332, jan 2024.   \n[41] Y. Liu and G. Pag\u00e8s. Characterization of probability distribution convergence in Wasserstein distance by $L^{p}$ -quantization error function. Bernoulli, 26(2):1171 \u2013 1204, 2020.   \n[42] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129\u2013137, 1982.   \n[43] N. Loo, R. Hasani, A. Amini, and D. Rus. Efficient dataset distillation using random feature approximation. Advances in Neural Information Processing Systems, 35:13877\u201313891, 2022.   \n[44] S. Mahabadi and A. Vakilian. Individual fairness for k-clustering. In International Conference on Machine Learning, pages 6586\u20136596. PMLR, 2020.   \n[45] F. E. Maranzana. On the location of supply points to minimize transportation costs. IBM Systems Journal, 2(2):129\u2013135, 1963.   \n[46] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of machine learning models. In International Conference on Machine Learning, pages 6950\u20136960. PMLR, 2020.   \n[47] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of machine learning models. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6950\u20136960. PMLR, 13\u201318 Jul 2020.   \n[48] A. Mishler and N. Dalmasso. Fair when trained, unfair when deployed: Observable fairness measures are unstable in performative prediction settings. arXiv preprint arXiv:2202.05049, 2022.   \n[49] A. Mishler, E. H. Kennedy, and A. Chouldechova. Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 386\u2013400, 2021.   \n[50] P. Mohassel, M. Rosulek, and N. Trieu. Practical privacy-preserving k-means clustering. Proceedings on privacy enhancing technologies, 2020.   \n[51] M. Moshkovitz, S. Dasgupta, C. Rashtchian, and N. Frost. Explainable k-means and k-medians clustering. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7055\u20137065. PMLR, 13\u201318 Jul 2020.   \n[52] B. Mussay, M. Osadchy, V. Braverman, S. Zhou, and D. Feldman. Data-independent neural pruning via coresets. In International Conference on Learning Representations, 2020.   \n[53] M. Negahbani and D. Chakrabarty. Better algorithms for individually fair $k$ -clustering. Advances in Neural Information Processing Systems, 34:13340\u201313351, 2021.   \n[54] R. Ohib, N. Gillis, N. Dalmasso, S. Shah, V. K. Potluru, and S. Plis. Explicit group sparse projection with applications to deep learning and NMF. Transactions on Machine Learning Research, arXiv:1912.03896, 2022.   \n[55] OpenAI. Chatgpt3.5. https://chat.openai.com, 2022.   \n[56] J. M. Ortega and W. C. Rheinboldt. Iterative solution of nonlinear equations in several variables. SIAM, 2000.   \n[57] G. Pag\u00e8s. Introduction to vector quantization and its applications for numerics. ESAIM: proceedings and surveys, 48:29\u201379, 2015.   \n[58] H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering. Expert Systems with Applications, 36(2):3336\u20133341, 2009.   \n[59] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(Oct):2825\u20132830, 2011.   \n[60] G. Peyr\u00e9, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[61] V. K. Potluru, D. Borrajo, A. Coletta, N. Dalmasso, Y. El-Laham, E. Fons, M. Ghassemi, S. Gopalakrishnan, V. Gosai, E. Krea\\`ci\u00b4c, et al. Synthetic data applications in finance. arXiv preprint arXiv:2401.00081, 2023.   \n[62] J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. Mit Press, 2022.   \n[63] Y. P. Raykov, A. Boukouvalas, F. Baig, and M. A. Little. What to do when k-means clustering fails: a simple yet principled alternative algorithm. PloS one, 11(9):e0162259, 2016.   \n[64] M. Redmond. Communities and Crime. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C53W3X.   \n[65] S. Sagiroglu and D. Sinanc. Big data: A review. In 2013 International Conference on Collaboration Technologies and Systems (CTS), pages 42\u201347. IEEE, 2013.   \n[66] F. Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63):94, 2015.   \n[67] S. Sharma, J. Henderson, and J. Ghosh. Feamoe: fair, explainable and adaptive mixture of experts. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI \u201923, 2023.   \n[68] S. Sharma, Y. Zhang, J. M. Rios Aliaga, D. Bouneffouf, V. Muthusamy, and K. R. Varshney. Data augmentation for discrimination prevention and bias disambiguation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 358\u2013364, 2020.   \n[69] M. Sloane, E. Moss, and R. Chowdhury. A silicon valley love triangle: Hiring algorithms, pseudo-science, and the quest for auditability. Patterns, 3(2), 2022.   \n[70] M. Sordello, N. Dalmasso, H. He, and W. Su. Robust learning rate selection for stochastic optimization via splitting diagnostic. Transactions on Machine Learning Research arXiv:1910.08597, 2024.   \n[71] J. Thickstun. Kantorovich-rubinstein duality. Online manuscript available at https://courses. cs. washington. edu/courses/cse599i/20au/resources/L12_ duality. pdf, 2019.   \n[72] A. Vakilian and M. Yalciner. Improved approximation algorithms for individually fair clustering. In International Conference on Artificial Intelligence and Statistics, pages 8758\u20138779. PMLR, 2022.   \n[73] A. Vattani. K-means requires exponentially many iterations even in the plane. In Proceedings of the twenty-fifth annual symposium on Computational geometry, pages 324\u2013332, 2009.   \n[74] C. Villani et al. Optimal transport: Old and new, volume 338. Springer, 2009.   \n[75] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems, 31, 2018.   \n[76] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[77] Z. Xiong, N. Dalmasso, A. Mishler, V. K. Potluru, T. Balch, and M. Veloso. FairWASP: Fast and optimal fair wasserstein pre-processing. Proceedings of the AAAI Conference on Artificial Intelligence, 38(14):16120\u201316128, Mar. 2024.   \n[78] S. Xu and T. Strohmer. Fair data representation for machine learning at the pareto frontier. Journal of Machine Learning Research, 24(331):1\u201363, 2023.   \n[79] R. Yin, Y. Liu, W. Wang, and D. Meng. Randomized sketches for clustering: Fast and optimal kernel $k$ -means. Advances in Neural Information Processing Systems, 35:6424\u20136436, 2022.   \n[80] R. Yu, S. Liu, and X. Wang. Dataset distillation: A comprehensive review. IEEE Transactions on Pattern Analysis & Machine Intelligence, 46(01):150\u2013170, jan 2024.   \n[81] A. Zhang, L. Xing, J. Zou, and J. C. Wu. Shifting machine learning for healthcare from development to deployment and from models to data. Nature Biomedical Engineering, 6(12):1330\u20131345, July 2022.   \n[82] B. Zhao, K. R. Mopuri, and H. Bilen. Dataset condensation with gradient matching. In International Conference on Learning Representations, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Details on Updating the Surrogate Function (line 4 of Algorithm 1) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To update the surrogate function, we need to solve problem (8), which is a huge-scale linear program with $O(n)$ constraints and $O(m n)$ nonnegative variables. In this work, we adapt FairWASP [77] for cases where $m\\neq n$ , as opposed to the scenario where $m=n$ , which was tackled by [77]. Before showing the main idea of the algorithm, we rephrase a useful lemma for doing linear minimization on $S_{n,m}\\ {\\overset{\\mathrm{def.}}{=}}\\ \\{P\\in\\mathbb{R}^{n\\times m}:P\\mathbf{1}_{m}={\\frac{1}{n}}\\cdot\\mathbf{1}_{n},P\\geq\\mathbf{0}\\}.$ ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. For the function $G(C)\\stackrel{d e f}{=}\\operatorname*{max}_{P\\in S_{n,m}}\\langle C,P\\rangle$ , it is a convex function of $C$ in $\\mathbb{R}^{n\\times m}$ . For each $i\\in[n]$ , let $C_{i j_{i}^{\\star}}$ denote a largest component on the i-th row of $C$ , then $\\begin{array}{r}{G(C)=\\frac{1}{n}\\sum_{i=1}^{n}C_{i j_{i}^{\\star}}}\\end{array}$ . Define the components of $P^{\\star}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{i j}^{\\star}=\\left\\{\\begin{array}{l l}{0}&{i f j\\not=j_{i}^{\\star}}\\\\ {\\frac{1}{n}}&{i f j=j_{i}^{\\star}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and then $P^{\\star}\\in\\arg\\operatorname*{max}_{P\\in S_{n,m}}\\langle C,P\\rangle$ and $P^{\\star}\\in\\partial G(C)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof of the above lemma is equivalent with that of Lemma 1 of [77] in the case when $m\\neq n$ , which can be extended directly. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "With this lemma, now we show how to efficiently solve problem (8) via its dual problem. Although (8) is of large scale and computationally prohibitive, it is equivalent to the following saddle point problem on the Lagrangian: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in S_{n,m}}\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{+}^{h}}L(P,\\lambda)\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\langle C,P\\rangle-\\lambda^{\\top}A P^{\\top}\\mathbf{1}_{n}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $h$ is the number of rows of $A$ , which is at most $2|\\mathcal{V}||\\mathcal{D}|$ . It should be mentioned that $h$ is significantly smaller than $m n$ ; for example, for classification tasks with only two protected variables, $h$ is no larger than 8, independent of the number of samples or features. Since $L(\\cdot,\\cdot)$ is bilinear, the minimax theorem guarantees that (19) is equivalent to $\\mathrm{max}_{\\lambda\\in\\mathbb{R}_{+}^{h}}\\operatorname*{min}_{P\\in S_{n,m}}L(P,\\lambda)$ . This is further equal to the dual problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{+}^{h}}-\\Big[G(\\lambda)\\stackrel{\\mathrm{def.}}{=}\\operatorname*{max}_{P\\in S_{n,m}}\\Big\\langle\\sum_{j=1}^{h}\\lambda_{j}\\mathbf{1}_{n}a_{j}^{\\top}-C,P\\Big\\rangle\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "in which $a_{j}^{\\top}$ denotes the $j$ -th row of $A$ . Note that the problem (20) has much fewer decision variables than that of (19) and Lemma A.1 ensures the function $G(\\cdot)$ is convex and has easily accessible function values and subgradients. Therefore, directly applying a cutting plane method has low per-iteration complexity and solves the problem (20) in linear time. We include the details on the cutting plane method in Section A.1 below. Finally, the primal optimal solution of (8) can be easily recovered from the dual optimal solution $\\lambda^{\\star}$ via solving $\\begin{array}{r}{\\stackrel{-}{\\operatorname*{max}_{P\\in S_{n,m}}}\\Big\\langle\\sum_{j=1}^{h}\\lambda_{j}^{\\star}\\mathbf{1}_{n}a_{j}^{\\top}-C,P\\Big\\rangle}\\end{array}$ , under the assumption that this problem has a unique minimizer, which almost always holds in practice for the computed $\\lambda^{\\star}$ and is also assumed by [77]. In this way, we have shown how the problem (8) can be efficiently solved by applying a cutting plane method on its dual problem. ", "page_idx": 14}, {"type": "text", "text": "A.1 Details of the Cutting Plane Method for Solving (20) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The cutting plane method is designed for convex problems where a separation oracle can be employed [36]. For any $\\lambda\\,\\in\\,\\mathbb{R}^{m}$ , a separation oracle operates by generating a vector $g$ which satisfies $\\bar{g}^{\\top}\\bar{\\lambda}\\geq g^{\\top}\\lambda^{\\star}$ for all $\\lambda^{\\star}$ in the set of optimal solutions. By repeatedly applying the separation oracle to cut down the potential possible feasible set, the cutting plane method progressively narrows down the feasible solution space until it reaches convergence. The specific steps of the cutting plane algorithm are detailed in Algorithm 2, with the key distinctions among different versions of this method lying in how lines 3 and 4 are implemented. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 General Cutting Plane Method for (20) ", "page_idx": 15}, {"type": "text", "text": "1: Choose a bounded set $E_{0}$ that contains an optimal solution   \n2: for $k$ from 0 to $n$ do   \n3: Choose an interior point $\\lambda^{k}$ of $E_{k}$ ;   \n4: Compute $g\\in\\mathbb{R}^{m}$ such that   \n5: Choose the next bounded set $E_{k+1}\\supseteq\\{\\lambda\\in E_{k}:g^{\\top}\\lambda\\leq g^{\\top}\\lambda^{k}\\};$   \n6: end for ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "For the problem (20), a separation oracle (line 4 in Algorithm 2) can directly use the vector of subgradients, which are efficiently accessible, as we mentioned in section 4. Given that we have shown (20) is a low-dimensional convex program with subgradient oracles, there exist many wellestablished algorithms that can be used. Suppose that the norm of an optimal $\\lambda^{\\star}$ is bounded by $R$ , to the best of our knowledge, the cutting plane method with the best theoretical complexity is given by [31], who proposed an improved cutting plane method that only needs $O((h\\cdot\\mathrm{SO}\\,\\dot{+}h^{2})\\cdot\\log\\overline{{(}}h R/\\epsilon))$ flops. Here SO denotes the complexity of the separation oracle. Note that here $h$ is at most $2|\\mathcal \u1e0a D \u1e0c ||\\mathcal \u1e0a Y \u1e0c |$ , which is far smaller than $n$ or $m$ . Here we restate the Corollary 5 of [77] below for the overall time and space complexity of applying the cutting plane method in the case $m\\neq n$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 (Essentially Corollary 5 of [77]). With efficient computation and space management, the cutting plane method solves the problem (20) within ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{O}\\left(m n+|\\mathcal{D}|^{2}|\\mathcal{V}|^{2}n\\cdot\\log(\\frac{R}{\\epsilon})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "flops and $O(n|\\mathcal{D}||\\mathcal{V}|)$ space. Here we use $\\tilde{O}(\\cdot)$ to hide $m,\\,n,\\,|{\\mathcal{D}}|,$ , and $\\lvert\\mathcal{V}\\rvert$ in the logarithm function. ", "page_idx": 15}, {"type": "text", "text": "The above result is essentially Corollary 5 of [77] by slightly extending the proof to the general case $m\\neq n$ . Finally, in terms of the implementation, we follow [77] and use the analytic center cutting plane method. ", "page_idx": 15}, {"type": "text", "text": "B Theoretical Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section includes the theoretical proofs for Section 2 and Section 5. We first show the Wasserstein distance upper bounds downstream disparity for MLP networks (Proposition 2.1). We then show (i) the optimization problem can be reduced to optimizing over the $\\hat{X}$ rather than $\\hat{Z}$ (Lemma 3.1), (ii) the surrogate function is convex and a valid upper bound of the optimization objective (Lemma 5.2), (iii) our proposed algorithm converges to a first-order stationary point in X\u02c6 (Theorem 5.3), and (iv) our proposed algorithm terminates in a finite amount of iterations (Theorem 5.4). We also prove the generalization bound for FWC performance in terms of Wasserstein distance and demographic parity to unseen datasets coming from the same (unknown) distribution the original dataset was sampled from. Finally, we include Section B.1 to note how the downstream learning using FWC can be broken down into two terms, which highlights the challenges in analyzing its theoretical properties. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Let $Z=(X,Y),{\\hat{Z}}=({\\hat{X}},{\\hat{Y}})\\in{\\mathcal{Z}}=({\\mathcal{X}}\\times{\\mathcal{Y}})$ be two pairs of random variables with joint distributions $p_{Z}$ and $p_{\\hat{Z}}$ and marginal distributions $p_{X},\\,p_{Y}$ and $p_{\\hat{X}}$ and $p_{\\hat{Y}}$ respectively. Let $\\Pi(Z,{\\hat{Z}})$ indicate the set of all joint probability distributions over the product space ${\\mathcal{Z}}\\times{\\mathcal{Z}}$ that admit marginal and conditional distributions over $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ . For any non-negative cost operator $c$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{c}(p_{X},p_{\\hat{X}})\\leq\\mathcal{W}_{c}(p_{Z},p_{\\hat{Z}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Proof of Lemma B.1 Let $\\Pi_{X}(X,{\\hat{X}})$ indicate the set of all marginal (joint) probability distributions over the product space $\\mathcal X\\times\\mathcal X$ . For any $\\pi\\,\\in\\,\\Pi(Z,{\\hat{Z}})$ and the corresponding $\\pi_{x}~\\in$ $\\Pi_{X}(X,{\\hat{X}})$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathcal{Z}\\times\\mathcal{Z}}c(z,\\hat{z})d\\pi(z,\\hat{z})\\geq\\int_{\\mathcal{Z}\\times\\mathcal{Z}}c(x,\\hat{x})d\\pi(z,\\hat{z})}}\\\\ &{=\\int_{\\mathcal{X}\\times\\mathcal{X}}c(x,\\hat{x})d\\pi_{x}(x,\\hat{x})}\\\\ &{\\geq\\frac{\\operatorname*{min}}{\\pi_{x}\\in\\Pi_{x}(x,\\hat{x})}c(x,\\hat{x})d\\pi_{x}(x,\\hat{x})=\\mathcal{W}_{c}(p_{X},p_{\\hat{X}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As this is valid for any $\\pi\\in\\Pi(Z,\\hat{Z})$ , select: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{\\star}=\\operatorname*{\\arg\\operatorname*{min}}_{\\pi\\in\\Pi(Z,\\hat{Z})}\\int_{\\mathcal{Z}\\times\\mathcal{Z}}c(z,\\hat{z})d\\pi(z,\\hat{z}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then the left-hand-side of (22) is equal to $\\mathcal{W}_{c}(p_{Z},p_{\\hat{Z}})$ , hence proving that $\\begin{array}{r l}{\\mathcal{W}_{c}(p_{Z},p_{\\hat{Z}})}&{{}\\geq}\\end{array}$ $\\mathcal{W}_{c}(p_{X},p_{\\hat{X}})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Proof of Proposition 2.1 The proof of the upper bound follows from the first part of the proof of the Kantorovich-Rubinstein duality [66]. In this work we follow the proof by [6, 71], which consider the Lagrangian form of the 1-Wasserstein distance and express it in the following form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{W}(p_{(X,D)},p_{(\\hat{X},\\hat{D})})=\\operatorname*{sup}_{\\substack{f,g:f(x)+g(y)\\leq\\|x-y\\|_{2}}}\\left|\\mathbb{E}_{(x,d)\\sim p_{(X,D)}}f_{\\theta}(x,d)-\\mathbb{E}_{(x,d)\\sim p_{(\\hat{X},\\hat{D})}}f_{\\theta}(x,d)\\right|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f,g:\\mathcal{X}\\to\\mathbb{R}$ are bounded, measurable functional Lagrangian multipliers. Let $L_{f_{\\theta}}$ be the Lipschitz constant of the MLP $f_{\\theta}$ , and define the following function: ", "page_idx": 16}, {"type": "equation", "text": "$$\nh(x,d)=\\frac{f_{\\theta}(x,d)}{L_{f_{\\theta}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By definition, $h(x,d)$ is 1-Lipschitz. Again following [6, 71], we know that for 1-Lipschitz functions the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x,d)\\sim p_{(X,D)}}f_{\\theta}(x,d)-\\mathbb{E}_{(x,d)\\sim p_{(\\tilde{x},\\hat{D})}}f_{\\theta}(x,d)\\Big|=L_{f_{\\theta}}\\left|\\mathbb{E}_{(x,d)\\sim p_{(X,D)}}h(x,d)-\\mathbb{E}_{(x,d)\\sim p_{(\\tilde{x},\\hat{D})}}h(x,d)\\right|}\\\\ &{\\quad=L_{f_{\\theta}}\\left|\\int_{(X\\times\\mathcal{D})\\times(\\mathcal{X}\\times\\mathcal{D})}h(x_{1},d_{1},x_{2},d_{2})d\\pi\\left(p_{(X,D)},p_{(\\tilde{X},\\hat{D})}\\right)\\right|}\\\\ &{\\quad\\le L_{f_{\\theta}}\\int_{(X\\times\\mathcal{D})\\times(\\mathcal{X}\\times\\mathcal{D})}\\left|h(x_{1},d_{1},x_{2},d_{2})\\right|d\\pi\\left(p_{(X,D)},p_{(\\tilde{X},\\hat{D})}\\right)}\\\\ &{\\quad\\le L_{f_{\\theta}}\\int_{(X\\times\\mathcal{D})\\times(X\\times\\mathcal{D})}\\left|(x_{1},d_{1})-(x_{2},d_{2})\\right|\\!\\Vert_{2}d\\pi\\left(p_{(X,D)},p_{(\\tilde{X},\\hat{D})}\\right)}\\\\ &{\\le L_{f_{\\theta}}\\mathcal{W}_{1}(p_{(X,D)},p_{(\\tilde{X},\\hat{D})})\\le L_{f_{\\theta}}\\mathcal{W}_{1}(p_{Z},p_{\\tilde{Z}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is due to Lemma B.1. The result can be obtained by using the upper bound in [75, Section 6.1], which shows that $L_{f_{\\theta}}\\leq L_{k}$ for $K$ -layer MLPs with ReLu activations. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.1. Once we generate $m|\\mathcal{D}||\\mathcal{Y}|$ data points, the feasible set of the latter Wasserstein coreset contains the feasible set of the former Wasserstein coreset. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5.2. The convexity follows directly from the convexity of $C({\\hat{X}})$ , as the $P_{k}^{\\star}\\ge0$ in (10). ", "page_idx": 17}, {"type": "text", "text": "Before proving it is an upper bound, we show some important properties of $F(C)$ as a function of $C$ . Firstly, $F(C)$ is concave on $C$ because of the concavity of the minimum LP\u2019s optimal objective on the objective vector. Secondly, since the feasible set of problem (8) is bounded, the optimal solution $\\dot{F}(C)$ is continuous with respect to $C$ . Thirdly, due to the sensitivity analysis of LP [8], a supergradient of $F(C)$ at point $C$ is the corresponding optimal solution $P^{\\star}$ . Here the definition of supergradients for concave functions is analogous to the definition of subgradients for convex functions. ", "page_idx": 17}, {"type": "text", "text": "Now we prove $g(\\hat{X};\\hat{X}^{k})$ is an upper bound of $F(C(\\hat{X}))$ . Because $P_{k}^{k}$ is a supergradient of $F(C)$ when $C=C(\\hat{X}^{k})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(C)+\\langle P_{k}^{k},C(\\hat{X})-C\\rangle\\geq F(C(\\hat{X}))\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in which the left-hand side is equal to $g(\\hat{X};\\hat{X}^{k})$ because $F(C)~=~\\langle P_{k}^{k},C\\rangle$ and $g(\\hat{X};\\hat{X}^{k})\\;=\\;$ $\\langle C(\\hat{X}),P_{k}^{\\star}\\rangle$ . Therefore, the surrogate function is an upper bound of the objective function $F(C({\\hat{X}}))$ , i.e., $g(\\hat{X};\\hat{X}^{k})\\geq F(C(\\hat{X}))$ . Moreover, due to the definition in (10), $g(\\hat{X};\\hat{X}^{k})=F(C(\\hat{X}))$ when $\\hat{X}=\\hat{X}^{k}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5.3. The monotonically decreasing part of the claim follows by: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(C(\\hat{X}^{k+1}))\\leq g(\\hat{X}^{k+1};\\hat{X}^{k})=\\arg\\operatorname*{min}_{\\hat{X}\\in\\mathcal{X}^{m}}g(\\hat{X};\\hat{X}^{k})\\leq g(\\hat{X}^{k};\\hat{X}^{k})=F(C(\\hat{X}^{k}))\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here the first inequality is due to the fact that $g(\\hat{X};\\hat{X}^{k})\\geq F(C(\\hat{X}))$ for any $\\hat{X}$ . The final equality is because $g(\\hat{X};\\hat{X}^{k})\\,=\\,F(C(\\hat{X}))$ when $\\hat{X}\\,=\\,\\hat{X}^{k}$ . Once $g(\\hat{X}^{k};\\hat{X}^{k})\\,=\\,g(\\hat{X}^{k+1};\\hat{X}^{k})$ and thus $\\begin{array}{r}{\\hat{X}^{k}\\in\\operatorname*{arg\\,min}_{\\hat{X}\\in\\mathcal{X}^{m}}g(\\hat{X};\\hat{X}^{k})}\\end{array}$ , then ${\\hat{X}}^{k}$ is a global minimizer of the convex upper bound $g(\\cdot;\\hat{X}^{k})$ for $F(C(\\cdot))$ and the upper bound $g(\\hat{X}^{k};\\hat{X}^{k})$ attains the same function value with $F(C(\\hat{X}^{k}))$ . Therefore, if the surrogate function is smooth at ${\\hat{X}}^{k}$ , which could be achieved if $C(\\hat{X}^{k})$ is smooth at ${\\hat{X}}^{k}$ , then $X^{k}$ is a first-order stationary point of (9). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5.4. Because (11) has a unique minimizer, the second inequality in (23) holds strictly when $\\hat{X}^{k+1}\\neq\\hat{X}^{k}$ , or equivalently $g(\\mathring{X}^{k+1};\\mathring{X}^{k})\\neq g(\\mathring{X}^{k};\\mathring{X}^{k})$ . Once $\\hat{X}^{k+1}=\\hat{X}^{k}$ , then the algorithm terminates. Note that there are only finite possible optimal basic feasible solution $P^{\\star}$ that could be generated by FairWASP, as shown in Lemma A.1. However, before the majority minimization converges, (23) holds strictly and the corresponding $P_{k}^{\\star}$ keeps changing. Therefore, after finite iterations, there must be a $P_{t}^{\\star}$ equal to a previous $P_{j}^{\\star}$ for $j~<t$ . When that happens, because the surrogate functions are the same and thus have the same minimizer, $\\hat{X}^{t+1}=\\hat{X}^{j+1}$ , and the inequalities (23) then hold at equality when $k=j,j+1,\\ldots,t.$ . This implies that $\\begin{array}{r l r}{\\lefteqn{g(\\hat{X}^{j+1};\\hat{X}^{j})=}}\\end{array}$ $g(\\hat{X}^{j};\\hat{X}^{j})$ , so the algorithm terminates within finite iterations. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 5.5. For determining the convergence in Wasserstein distance between $p_{\\hat{Z}}$ and $q_{Z}$ , we first use the triangle inequality: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{W}_{c}(p_{\\hat{Z}},q_{Z})\\leq\\mathcal{W}_{c}(p_{\\hat{Z}},p_{Z})+\\mathcal{W}_{c}(p_{Z},q_{Z})=\\lambda+\\mathcal{W}_{c}(p_{Z},q_{Z})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the first term is deterministic, as it is the result of the optimization problem (4). For the second term, we use the result from [22], which implies that with probability $1\\mathrm{~-~}\\alpha$ and for the 1-Wasserstein distance: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{W}_{c}(p_{Z},q_{Z})>\\xi)\\le\\exp\\left(-c n\\xi^{1/d}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so by setting the right-hand side equal to $\\alpha$ , or equivalently, setting ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi=\\sqrt[d]{\\frac{c\\log(1/\\alpha)}{n}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we obtain the first result. ", "page_idx": 17}, {"type": "text", "text": "For determining the convergence of the disparity between $p_{\\hat{Z}}$ and $p_{\\hat{Z}}(y,d)$ and $q_{Y}(y)$ , we again first use the triangle inequality from the definition of the disparity $J$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y\\in\\mathcal{Y},d\\in\\mathcal{D}}{\\operatorname*{sup}}J(p_{\\hat{Z}}(y|d),q_{Y}(y))=\\underset{y\\in\\mathcal{Y},d\\in\\mathcal{D}}{\\operatorname*{sup}}\\frac{\\vert p_{\\hat{Z}}(y\\vert d)-q_{Y}(y)\\vert}{q_{Y}(y)}}\\\\ {\\leq\\underset{y\\in\\mathcal{Y},d\\in\\mathcal{D}}{\\operatorname*{sup}}\\left(\\frac{\\vert p_{\\hat{Z}}(y\\vert d)-p_{Y}(y)\\vert}{q_{Y}(y)}+\\frac{\\vert p_{Y}(y)-q_{Y}(y)\\vert}{q_{Y}(y)}\\right)}\\\\ {\\leq\\underset{y\\in\\mathcal{Y},d\\in\\mathcal{D}}{\\operatorname*{sup}}\\frac{\\vert p_{\\hat{Z}}(y\\vert d)-p_{Y}(y)\\vert}{q_{Y}(y)}+\\underset{y\\in\\mathcal{Y}}{\\operatorname*{sup}}\\frac{\\vert p_{Y}(y)-q_{Y}(y)\\vert}{q_{Y}(y)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As the minimum of the marginal distribution of $q_{Y}(y)$ is bounded away from zero $\\begin{array}{r}{\\operatorname*{min}_{y\\in{\\mathcal{Y}}}q_{Z}(y)=}\\end{array}$ $\\rho>0$ , and since by the optimization problem (4) we have $J(p_{\\hat{Z}}(y|d),p_{Y}(y))\\leq\\epsilon$ for all $y\\in\\mathcal{Y},d\\in$ $\\mathcal{D}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{y\\in\\mathcal{Y},d\\in\\mathcal{D}}J(p_{\\hat{Z}}(y|d),q_{Y}(y))\\leq\\operatorname*{sup}_{y\\in\\mathcal{Y},d\\in\\mathcal{D}}\\frac{|p_{\\hat{Z}}(y|d)-p_{Y}(y)|}{p_{Y}(y)}\\frac{p_{Y}(y)}{q_{Y}(y)}+\\operatorname*{sup}_{y\\in\\mathcal{Y}}\\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{y\\in\\mathcal{Y},d\\in\\mathcal{D}}J(p_{\\hat{Z}}(y|d),p_{Y}(y))\\frac{1}{\\rho}+\\operatorname*{sup}_{y\\in\\mathcal{Y}}\\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}}\\\\ &{\\leq\\displaystyle\\frac{\\epsilon}{\\rho}+\\operatorname*{sup}_{y\\in\\mathcal{Y}}\\frac{|p_{Y}(y)-q_{Y}(y)|}{q_{Y}(y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the first term is deterministic, while the second one is not, as we need to account for the uncertainty of observing $n$ i.i.d. samples $\\{Z_{i}\\}_{i=1}^{n}$ . For the second part, we use the Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW, [17]) inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\underset{y\\in\\mathcal{Y}}{\\operatorname*{sup}}\\frac{\\left|p_{Y}\\left(y\\right)-q_{Y}\\left(y\\right)\\right|}{q_{Y}\\left(y\\right)}>\\xi\\right)\\leq\\mathbb{P}\\left(\\underset{y\\in\\mathcal{Y}}{\\operatorname*{sup}}\\left|p_{Y}(y)-q_{Y}(y)\\right|>\\xi\\rho\\right)}\\\\ &{\\qquad\\qquad\\qquad q_{Y}(y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad q_{X}\\leq2\\exp(-2n\\rho^{2}\\xi^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By setting the right-hand side equal to $\\alpha$ , or equivalently, setting ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\xi=\\sqrt{\\frac{\\log(\\frac{2}{\\alpha})}{2n\\rho^{2}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we obtain the second result. ", "page_idx": 18}, {"type": "text", "text": "Finally, we note that the assumption on the marginal distribution of $q_{Y}$ is bounded away from zero, i.e., $\\rho>0$ , is reasonable as the outcome is a discrete (usually binary) random variable. This assumption would be much more restricting in case $y$ was a continuous random variable (e.g., in regression settings). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.1 Downstream Learning using FWC ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Overall, the hardness of deriving bounds for the synthetic representatives provided by FWC can be analyzed using the following breakdown, which is adapted from [78]. Consider the given dataset $Z=\\{(X_{i},Y_{i},D_{i})\\}_{i=1}^{n}$ , the synthetic representatives obtained using FWC $\\hat{Z}=\\{(\\hat{X}_{j},\\^{}\\hat{Y}_{j},\\hat{D}_{j})\\}_{j=1}^{m}$ and $h\\in\\mathcal{H}=L^{2}(\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{D})$ , the set of measurable square-integrable function in $L^{2}$ . If we consider the downstream learning process using FWC samples over the $L^{\\tilde{2}}$ space: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{Y|X,D}\\left[\\|Y-h(\\hat{X},\\hat{D})\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then the above can be expanded in two terms, due to the property of the conditional expectation being an orthogonal operator in $\\mathcal{H}$ : ", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{r l}&{\\underset{h\\in\\mathcal{H}}{\\operatorname*{inf}}\\mathbb{E}_{Y|X,D}\\left[\\|Y-h(\\hat{X},\\hat{D})\\|_{2}^{2}\\right]=}\\\\ &{\\quad\\quad=\\underbrace{\\mathbb{E}_{Y|X,D}\\left[\\|Y-\\mathbb{E}_{X,D}[\\hat{Y}|\\hat{X},\\hat{D}]\\|_{2}^{2}\\right]}_{=\\mathrm{~h}\\in\\mathcal{H}}+\\underset{h\\in\\mathcal{H}}{\\operatorname*{inf}}\\;\\mathbb{E}_{Y|X,D}\\left[\\|\\mathbb{E}_{X,D}[\\hat{Y}|\\hat{X},\\hat{D}]-h(\\hat{X},\\hat{D})\\|_{2}^{2}\\right]}\\end{array}$ (24) FWC Approx i mation Error Learning wit h  FWC Samples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The first term corresponds to the loss of information in approximating $Y$ with $\\hat{Y}$ via the FWC approach, and is actually independent of any downstream learning. This condition requires for the first moment (which for the binary $Y$ case is equivalent to the joint distribution) of $Y$ and $\\hat{Y}$ to be as close as possible. Using FWC this is enforced by minimizing the Wasserstein distance. Indeed, if in definition (1) one restricts to couplings that admit marginal and conditional distributions, then the conditional distributions of $Y|X,D$ and $\\hat{Y}|\\hat{X},\\hat{D}$ are upper bounded in Wasserstein sense by the Wasserstein distance between the joint distribution of $p z$ and $p_{\\hat{Z}}$ [37]. ", "page_idx": 19}, {"type": "text", "text": "The second terms refers to the training process using FWC samples Firstly, by using the equivalence in [78], the second term is equivalent to $\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{Y|X,D}\\left[\\|\\hat{Y}-f(\\hat{X},\\hat{D^{}}\\|_{2}^{2}\\right]$ , which correspond the finding the best $L^{2}$ function to approximate the distribution of $\\hat{Y}$ . This fact implies that using FWC samples is indeed mathematically equivalent to the learning task for the original $Y$ . However, this second term also highlights the hardness of developing learning bounds, as the FWC synthetic representatives $\\hat{Z}=\\{(\\bar{X_{j}},\\bar{Y_{j}},\\hat{D}_{j})\\}_{j=1}^{m}$ are not i.i.d., and hence standard bounds are not applicable. ", "page_idx": 19}, {"type": "text", "text": "C Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Runtime Analysis on Synthetic Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As mentioned in Section 7, we generate a synthetic dataset in which one feature is strongly correlated with the protected attribute $D$ to induce a backdoor dependency on the outcome. We consider a binary protected attribute, $D\\,\\in\\,\\{0,1\\}$ , which could indicate e.g., gender or race. The synthetic dataset contains two features, a feature $X_{1}$ correlated with the protected attribute and a feature $X_{2}$ uncorrelated with the protected attribute. For $D=0$ , $X_{1}$ is uniformly distributed in [0, 10], while for $D=1$ , $X_{1}=0$ . Instead, $X_{2}$ is 5 times a random variable from a normal distribution $\\bar{\\mathcal{N}}(0,1)$ . Finally, the outcome $Y$ is binary, so $Y\\,=\\,\\{0,1\\};\\,Y_{i}\\,=\\,1$ when $Y_{i}>m_{x}+\\epsilon_{i}$ and $Y_{i}=0$ when $Y_{i}\\leq m_{x}+\\epsilon_{i}$ , where $m_{x}$ is the mean of $\\{(\\dot{X}_{1})_{i}+(X_{2})_{i}\\}_{i}$ and the noise $\\epsilon_{i}$ comes from a normal distribution $\\mathcal{N}(0,1)$ . ", "page_idx": 19}, {"type": "text", "text": "This experiment visualizes the speed of our method with respect to different numbers of overall samples $n$ , number of samples in the compressed dataset $m$ , and the dimensionality of features $p$ . We evaluated the performance of the algorithm under the synthetic data with different configurations of $n$ , $p$ , and $m$ . In this experiment, we set compute the fair Wasserstein coreset under the $l_{1}$ -norm distance and we use $\\mathbf{k}$ -means [42] to initialize the starting coreset ${\\hat{X}}^{0}$ . We terminate the algorithm when $\\hat{X}^{k}=\\hat{X}^{k-1}$ . The time per iteration and total iterations for varying $n,p$ , and $m$ are shown in the Figures 1 (top left) and 2. We see that increasing the sample size of the original dataset $n$ increases the runtime and number of iterations, while increasing the number of coresets $m$ or dimensionality of the features $p$ reduces the overall numbers of iterations but increases each iteration\u2019s runtime. Additionally, for the setting of Figure 1 (top left), in which we vary the dataset size $n$ , Table 2 provides FWC average runtimes from $n=500$ to $n=1,000,000$ . We compare FWC runtimes with the runtime at $n=500$ (our lowest dataset size in the experiment) extrapolated (i) linearly, with a factor of 1, (ii) linearly, with a factor of 10 and (iii) quadratically. We can see that the complexity is near linear and less than quadratic with respect to the dataset size $n$ , although the rate indeed seem to increase for $n$ at 500, 000 and above, which can be attributed to the increasing number of iterations required to achieve convergence. This is akin to the phenomenon well-known for $\\boldsymbol{\\mathrm{k}}$ -means, for which in larger datasets $\\mathbf{k}$ -means might take an exponentially large number of iterations to terminate [73]. ", "page_idx": 19}, {"type": "text", "text": "In practice, a fixed number of overall iterations is set to avoid this case: sklearn sets it to $300^{6}$ , feiss to $\\mathrm{\\dot{2}5^{7}}$ and Matlab to $100^{8}$ . ", "page_idx": 20}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/7dca7080c364ce3220134887ceba4d451717d49e7755b57f4b54a0835ca3c5f1.jpg", "img_caption": ["Figure 2: Runtime analysis of FWC when varying the size of the coreset $m$ (left) and the dimensionality of the features $p$ (right). We report averages and one standard deviation computed over 10 runs. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 2: Average runtimes for FWC in the same settings as Figures 1 (top left), varying the dataset size $n$ while fixing $m=250$ and $p=25$ , compared with the runtimes for the smallest dataset extrapolated (i) linearly, with a factor of 1, (ii) linearly, with a factor of 10 and (iii) quadratically. FWC enjoys a near linear time complexity, increasing with the largest dataset sizes; this phenomenon is shared with other clustering algorithm such as $\\boldsymbol{\\mathrm{k}}$ -means (see text). ", "page_idx": 20}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/3bc65b6bcfa815c2db636eb54d176cec9280baaa2be0555938e5ad6560da37b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Real Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We consider the following four real datasets widely used in the fairness literature [19]: ", "page_idx": 20}, {"type": "text", "text": "\u2022 the Adult dataset [7], which reports demographic data from the 1994 US demographic survey about $\\sim49$ , 000 individuals. We use all the available features for classification apart from the \u201cfnlwgt\u201d feature, including gender as the protected attribute $D$ and whether the individual salary is more than USD50, 000;   \n\u2022 the Drug dataset [20], which contains drug usage history for 1, 885 individuals. Features $X$ include the individual age, country of origin, education and scores on various psychological test. We use the individual gender as the protected variable $D$ . The response $Y$ is based on whether the individual has reported to have ever used the drug \u201ccannabis\u201d or not;   \n\u2022 the Communities and Crime dataset [64] was put together towards the creation of a software tool for the US police department. The dataset contains socio-economic factors for $\\sim2,000$ communities in the US, along with the proportion of violent crimes in each community. As protected attribute $D$ , we include whether the percentage of the black population in the community is above the overall median. For the response $Y$ , we use a binary indicator of whether the violent crimes percentage level is above the mean across all communities in the dataset; ", "page_idx": 20}, {"type": "text", "text": "\u2022 the German Credit dataset [28] reports a set of 1, 000 algorithmic credit decisions from a regional bank in Germany. We use all the available features, including gender as protected attribute $D$ and whether the credit was approved as response $Y$ . ", "page_idx": 21}, {"type": "text", "text": "As perfect demographic parity achieves a value of 0 for the discrepancy $J$ , so we have included demographic \u201cdis\u201d-parity to indicate any deviation from demographic parity. Across all experiments we compute demographic parity as the following absolute difference: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\cal D}D\\,\\stackrel{\\mathrm{def.}}{=}\\,\\vert p\\left(h(X,D)=1\\vert D=1\\right)-p(h(X,D)=1\\vert D=0)\\vert\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for a given classifier $h$ and a protected attribute $D$ with two levels; the larger this difference, the larger the disparity. We also include the implementation and hyper-parameters of the methods used in the fairness-utility tradeoffs throughout the experiments in this paper: ", "page_idx": 21}, {"type": "text", "text": "\u2022 For FWC we set the fairness violation hyper-parameter $\\epsilon$ of problem in Equation (5) to be $\\epsilon=[0.01,0.05,0.1]$ , hence obtaining three separate FWC models, FWC (0.01), FWC (0.05) and FWC (0.1);   \n\u2022 For Fairlet [4], we use the implementation available at the following GitHub repository: https://github.com/talwagner/fair_clustering/tree/master   \n\u2022 For IndFair [12] and K-Median Coresets [3], we use the implementation available at the following GitHub repository: https://github.com/jayeshchoudhari/ CoresetIndividualFairness/tree/master   \n\u2022 For k-means [42] and k-medoids [45, 58] we use the implementations available in the Python package Scikit-Learn [59] ", "page_idx": 21}, {"type": "text", "text": "All computations are run on an Ubuntu machine with 32GB of RAM and 2.50GHz Intel(R) Xeon(R) Platinum 8259CL CPU. For all datasets, we randomly split $75\\%$ of the data into training/test set, and change the split during each separate run; the training data are further separated into training and validation with $90/10$ to compute early stopping criteria during training. The downstream classifier used is a one-layer deep multi-layer perceptron (MLP) with 20 hidden layers, ReLu activation function in the hidden layer and softmax activation function in the final layer. Unless stated otherwise, FWC uses the $L^{1}$ to compute the distance from the original datasets in the optimization problem. For the downstream classifier, we use Adam optimizer [38] with a learning rate set to $10^{-3}$ , a batch size of 32, a maximum number of epochs set to 500 with early stopping evaluated on the separate validation set with a patience of 10 epochs and both the features $X$ and the protected attribute $D$ are used for training the classifier. Note that due to the size of the Adult dataset, Fairlet coresets [4] could not be run due to the RAM memory required exceeding the machine capacity (32GB). Finally, all uncertainties are reported at $\\pm1\\sigma$ (one standard deviation) in both figures and tables. Uncertainties are computed over a set of 10 runs where the random seed for the algorithm initialization and train/test split was changed, but consistent across all methods (i.e., all methods in the first run were presented the same train/test split across each datasets). ", "page_idx": 21}, {"type": "text", "text": "Table 3: Wasserstein distance of the weighted coresets with respect to the original dataset, with averages and standard deviations obtained over 10 runs. In bold, coresets with the closest distance to the original dataset (i.e., smallest Wasserstein distance) in each coreset size and dataset combination. ", "page_idx": 21}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/7c76bcef0abc3b760ebbec36d2f90b2dbbcb7842cf6bd4e11ddc941acb0cf51e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Closeness to the original dataset and clustering performance Tables 3 and 4 include the numerical values (means and standard deviations computed over 10 runs) for all methods in ", "page_idx": 21}, {"type": "text", "text": "Table 4: Clustering cost of the coresets with respect to the original dataset, with averages and standard deviations obtained over 10 runs. In bold, coresets with the smallest clustering cost (i.e., smallest sum of square distances of original dataset samples from the closest generated coreset sample) in each coreset size and dataset combination. ", "page_idx": 22}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/1e52e55b41ef3908cab28c871d03b62a9d4c7ad272c286c208a705cc9b03b834.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "terms of Wasserstein distance from the original dataset and clustering cost, for the three coreset sizes $m\\,=\\,[5\\%,10\\%,20\\%]$ (apart from the Adult dataset, in which coreset sizes are set to $m\\,=\\,[0.5\\%,1\\%,2\\%]$ due to the large size of the original dataset). Clustering cost is computed as the sum of the squared distance of each point in the original dataset from the closest coreset representative, while the Wasserstein distance is computed solving the optimal transport between the empirical distribution of the original dataset and the one of the coresets, using the $L^{1}$ norm as cost function. FWC consistently provides the closest distributional distance to the original dataset in Wasserstein distance, with the only exception of the Credit dataset, in which the large number of discrete features makes the optimization non-smooth in feature space, resulting in a potentially imprecise solution of Equation (9). In addition, FWC , while not naturally minimizing clustering costs, seems to achieve competitive clustering costs in smaller datasets while not performing as well on larger datasets such as Adult. Finally, we note that the Wasserstein distance might not always decrease with a higher coreset size, which is due to the parity violation constraint $\\epsilon$ . In other words, the coreset samples not only have to be close to the original dataset distribution but also respect the hard fairness constraint; the lower the $\\epsilon$ , the tighter this constraint is (Equation (4)). Indeed, when $\\epsilon$ is the largest $\\epsilon=0.1)$ ), coresets of size $20\\%$ (or $2\\%$ for the Adult dataset, i.e., the largest) consistently has a smaller Wasserstein distance to the original dataset than coresets of size $5\\%$ ( $\\bar{0.5\\%}$ for the Adult dataset, i.e., the smallest). ", "page_idx": 22}, {"type": "text", "text": "Fairness-utility tradeoff when using coresets for training downstream models Figure 6 expands the results provided in Figure 1 and shows all the fairness-utility tradeoffs for all methods across the four datasets, both with (right column) and without (left column) using a pre-processing fairness approach [34] (excluding FWC , to which no fairness modification is applied after coresets have been generated). For each method, the coreset size that achieves the best fairness-utility tradeoff is shown (which is not necessarily the coreset with the largest size). FWC achieves a competitive fairness-utility tradeoff with respect to other competing methods, when using the generated coresets to train a downstream MLP classifier model. FWC consistently reduces disparity in the downstream classification with respect to other approaches, and often maintains the same utility (indicated by the AUC). For completeness, Figure 7 also reports standard deviations for the fairness-utility tradeoff; standard deviations for the Adult and Credit datasets are large due to the MLP classifier becoming trivial (i.e., always returning 0s or 1s), which yields very low performance but has no demographic disparity (by definition, as all test samples are assigned the same outcome). Tables 5 and 6 report the numbers shown in Figures 6 and 7, including one number to quantify the fairness-utility tradeoff, computed as the Euclidean distance in the Figure from the $(0,1)$ point (which would be a fair classifier with perfect performance). In other words, for the $k$ -th method achieving a disparity of $d_{k}$ with uncertainty $\\Delta d_{k}$ and performance $a_{k}$ with uncertainty $\\Delta a_{k}$ , the tradeoff $t_{k}$ and associated uncertainty $\\Delta t_{k}$ are quantified as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nt_{k}=\\sqrt{(1-a_{k})^{2}+d_{k}^{2}}\\quad,\\quad\\Delta t_{k}=\\sqrt{\\left(\\frac{d_{k}}{t_{k}}\\Delta d_{k}\\right)^{2}+\\left(\\frac{a_{k}-1}{t_{k}}\\Delta a_{k}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, the average reduction in disparity was computed from Tables 5 and 6, by averaging the improvement obtained by FWC samples against all methods and across datasets. FWC result in an average reduction in disparity of $53\\%$ and $18\\%$ for the scenario without and with fairness preprocessing, respectively. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Table 5: Demographic disparity (Equation (25)), AUC and fairness-utility tradeoff (Equation (26)) of downstream MLP classifier trained using all fair coresets/clustering methods across the four real datasets. The best method across the 3 different coreset sizes is shown, and the best performing method for each metric in each dataset is bolded. Averages and standard deviations taken over 10 runs. For the Credit dataset, K-means reaches low disparities due to the classifier being trivial, i.e., returning the same prediction regardless of input features. ", "page_idx": 23}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/c206126d8a4f49b76d55d8b71d9870521e8e885060f3df7ddd9da5e2b2d0f691.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 6: Demographic disparity (Equation (25)), AUC and fairness-utility tradeoff (Equation (26)) of downstream MLP classifier trained using all fair coresets/clustering methods across the four real datasets. All methods apart from FWC have been corrected for fairness using a preprocessing fairness technique by [34]. The best method across the 3 different coreset sizes is shown, and the best performing method for each metric in each dataset is bolded. Averages and standard deviations taken over 10 runs. ", "page_idx": 23}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/7347511881f47ecbf01d72508a9f1210ec83928377dbd914f9484ed0d8c84fca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Fairness-utility tradeoff when using coresets for data augmentation We also evaluate the performance of FWC in reducing the downstream demographic disparity when doing data augmentation, i.e., adding the synthetic representatives to the training data when training a downstream model. We use the data augmentation scheme adopted by [68, Section 2.1], which first uses $\\mathbf{k}$ -means on the original dataset and then sorts the synthetic representatives based on the distance of each synthetic representative to the nearest $\\mathbf{k}$ -mean centroid with the same combination of protected attribute and outcome $D$ and $Y$ . We generate a set of synthetic datasets of size equal to $50\\%$ of the original dataset and look at the fairness-utility of a downstream model trained augmented with such synthetic representative in increments of $5\\%$ ( $\\dot{2}0\\%$ and $2.5\\%$ respectively for the Adult dataset, given the large dataset size). Figures 3 and 4 show the fairness-utility tradeoff of the downstream MLP classifier when doing data augmentation, selecting the best model across various degrees of data augmentation, along with the performance of the baseline MLP classifier with no data augmentation (averages and standard deviations over 10 runs). Table 7 shows the numerical values for the downstream fairness-utility tradeoff, including the tradeoff value computed as in Equation (26). In all datasets the data augmentation with FWC seems to either increase the performance or reduce the demographic disparity, with the only exception being the Drug dataset. Upon further investigation, Figure 5 shows this effect does not appear if the protected attribute $D$ (gender) is not included in the features used to train the downstream MLP classifier. This phenomenon indicates the protected attribute provides a strong predictive effect on the outcome (whether the individual has tried cannabis or not), which might potentially be mediated by unmeasured confounders, i.e., other features regarding the recorded individuals that are not available in the Drug dataset. This would require either in-training or post-processing fairness approaches to be alleviated; see [29] for a comprehensive review on different potential approaches. Finally, as in Figure 7, the standard deviations for Adult and Credit dataset are large due to the downstream model becoming trivial. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/20dbbeb11c4974619181d1953665b52cbd762b99b769e2b6384fd98b6e918764.jpg", "img_caption": ["Data Augmentation Performance-Fairness Tradeoff, FWC "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 3: Fairness-utility tradeoff of downstream MLP classifier trained using the original training set augmented with coresets representatives, following the augmentation strategy from [68]. Each point shows the best model in terms of fairness-utility tradeoff over various degrees of data augmentation, in addition to the baseline model with no augmentation. Means and standard deviations taken over 10 runs, with the computed Pareto frontier indicated by the dashed red line. ", "page_idx": 24}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/949267b2affccb2857581b109ea19c54219970d8f491cbcb85e8aacbb13da9ed.jpg", "img_caption": ["Data Augmentation Performance-Fairness Tradeoff, All Methods ", "Figure 4: Fairness-utility tradeoff of downstream MLP classifier trained using the original training set augmented with coresets representatives, following the augmentation strategy from [68], including all methods mentioned in Section 7. Each point shows the best model in terms of fairness-utility tradeoff over various degrees of data augmentation, in addition to the baseline model with no augmentation. Averages and standard deviations computed over 10 runs, with the top panel showing just means and the bottom panel combining both means and standard deviations, with the computed Pareto frontier indicated by the dashed red line. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/1013cb4ac610fbb574eafa2cc1fa3dfd03881ddbf1e25c528cbf738c11419391.jpg", "img_caption": ["Data Augmentation Performance-Fairness Tradeoff, Drug Dataset, FWC "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 5: Data augmentation fairness-utility tradeoff of downstream MLP classifier for the Drug dataset when the protected attribute $D$ (gender) is either not included (left) or included (right) as feature in the learning process. As in Figure 3, the best model across various data augmentation degrees is reported, with averages and standard deviations obtained over 10 runs. FWC manages to successfully reduce the demographic disparity when gender is not used as a feature, but fail to do so when gender is used, indicating that gender provides strong predictive power for the outcome in question, which would require enforcing fairness either during model training or by post-processing the outputs. ", "page_idx": 25}, {"type": "text", "text": "Table 7: Demographic disparity (Equation (25)), AUC and fairness-utility tradeoff (Equation (26)) of downstream MLP classifier trained via data augmentation with all fair coresets/clustering methods, across the four real datasets. The best methods across various degrees of data augmentation is shown. The best performing methods for every column is bolded, with averages and standard deviations taken over 10 runs. ", "page_idx": 25}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/ac6f9edc120b3e743a5b7e86e7e57a5505f969d39f61fb33a064171d4f89f78d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Using FWC to correct biases in LLMs To query GPT models, test data with 200 samples is provided with a base parity of 0.5 (similar to [76]) and additionally, examples are provided in the case of the few shot settings. Data is fed in as text. One of the tabular data, for example, is: \u201cA person in 1996 has the following attributes: Age: 21.0, workclass: Private, education: Some-college, highest education level: 10.0, marital status: Never-married, occupation: Other-service, relationship: Own-child, race: White, sex: Female, capital gain: 0.0, capital loss: 0.0, hours per week: 25.0, native country: United-States\". ", "page_idx": 25}, {"type": "text", "text": "We use the following prompts: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 Zero shot: \u201cUsing the provided data, will this person from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word \u2019yes\u2019 or \u2019no\u2019. Here are 0 examples with the correct answer.\" \u2022 Few shot: \u201cGiven the provided data, will this person from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word \u2019yes\u2019 or \u2019no\u2019. Here are n examples with the correct income level for a person in 1996. Make sure you use the examples as a reference\", where $n$ is the number of demographically balanced samples. \u2022 Few shot (FWC): \u201cGiven the provided data, will this from 1996 be hired at greater than 50,000 USD per year? You must only respond with the word \u2019yes\u2019 or \u2019no\u2019. Here are n examples with the correct income level for a person in 1996, along with weights in the ", "page_idx": 25}, {"type": "text", "text": "In the few shot settings, 16 examples are provided in both cases due to the LLM token limitation; passing fewer examples yields similar results to the ones in Table 1. For FWC, we run a separate coreset generation run (differently from other experiments in Section 7), where we selected $m=16$ and ensured that the positive class $\\mathit{Y}=1$ ) has an equal number of male and female samples. We also note that while the results from GPT-4 for the zero shot and few shot cases are similar to what was observed by [76], the accuracies reported for the GPT-3.5 Turbo model appear to be lower in our experiments, which points to a potential difference in the exact backend LLM model used for inference. ", "page_idx": 26}, {"type": "text", "text": "D Limitations of FWC ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Coreset support and non-convex feature spaces FWC representative do not need to be within the original $n$ samples, but they do need to share the same support. As shown in Section 4.2, solving line 5 in Algorithm 1 in cases 1. and 2. means the synthetic representatives could fall outside the original dataset. In case 3., the solution has to be selected from the data points already existing in the original dataset (akin to $\\mathbf{k}\\cdot$ -medoids). In general, non-convex feature spaces $\\mathcal{X}$ might represent a challenge, as representatives might be generated in zero-density regions (a simple example could be a dataset distributed as a hollow circle or two moons). However, this criticism is also more generally applicable to the existing fair coresets/clustering literature, as well as the $\\mathbf{k}$ -means algorithms, for which specific adjustments have been developed [63] and could be indeed extended to FWC . ", "page_idx": 26}, {"type": "text", "text": "Limiting the total number of iterations FWC is not of polynomial time complexity and the total number of iterations might grow faster than linear time when the dataset size is very large, as shown in the synthetic data experiment in Section C.1 and Table 2. This phenomenon is shared with $\\mathbf{k}$ -means, which is also not of polynomial time complexity and is known to potentially take an exponentially large number of iterations to terminate [73]. As mentioned in Section C.1, a common practice for clustering is set a fixed number of maximum iterations, after which the algorithm is stopped. ", "page_idx": 26}, {"type": "text", "text": "Computational bottlenecks The main complexity term for FWC is $O(m n)$ , which comes from establishing the cost matrix in the beginning of the solution of problem (8). This complexity is comparable with what occurs in Lloyd\u2019s algorithm for $\\boldsymbol{\\mathrm{k}}$ -means and $\\boldsymbol{\\mathrm{k}}$ -medians. This might be problematic if the cost matrix is too large to be stored directly in memory. In practice, we do not actually need to store the entire matrix, as we only need to compute the largest component for each row of $C$ for solving problem (8) (see Lemma A.1), so one could further improve the cost of storing the cost matrix. Another option would also be leverage the same approaches used for $\\mathbf{k}$ -means such as, e.g., cost matrix sketching [79]. Finally, FWC would also benefti from GPU implementations akin to $\\boldsymbol{\\mathrm{k}}$ -means and k-medians, which would substantially accelerate the runtime speed of FWC . ", "page_idx": 26}, {"type": "text", "text": "Connection between $\\epsilon$ and downstream learning In our definition of demographic parity in Equation 3, the hyper-parameter $\\epsilon$ effectively controls how different the outcome rates across sensitive feature groups $D$ of the weighted coreset distribution $p_{\\hat{Z},\\theta}$ can be from the overall outcome rates in the original dataset $p_{Y_{T}}$ . In our experiments (Section 7), we empirically show that limiting the fairness violation in the coresets results in a fairer downstream model. However, when training a downstream model using FWC we induce a distribution shift between the train set and the test set, as the coresets distribution is never identical to the original dataset distribution. Although we have provided some results on the generalization properties of FWC (Proposition 5.5) as well as some intuition about developing downstream learning bounds for FWC in non-i.i.d. settings (Section B.1), theoretically characterize the connection between the fairness violation parameter $\\epsilon$ remains an open question. In essence, the analysis is challenging as the induced distribution shift is dependent on the biases in the original dataset distribution, the coreset size $m$ , the metric chosen for the cost matrix and, ultimately, the fairness violation parameter $\\epsilon$ . For this reason, although we have shown that restricting the fairness violation improves downstream models fairness, an explicit characterization of the downstream effects of $\\epsilon$ , as well as other hyper-parameters, would require significant further work, beyond the scope of this paper. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Implications for other fairness measures As highlighted by [5, Chapter 3], fairness notions in classification settings can be categorized into notions of independence, separation, and sufficiency. Demographic parity falls in the class of the independence notion, and hence, other measures of fairness that are closely related, e.g., disparate impact, would also improve when optimizing for demographic parity. However, other notions of fairness such as separation or sufficiency may not simultaneously be satisfied [5]. As FWC targets demographic parity, it cannot guarantee an improvement in these other measures. To test this, we compute the equalized odds, which falls under the notion of separation, for the downstream classifier in Section 7 and check the performance of FWC compared to the other approaches. Table 8 indicates the datasets in which FWC is part of the Pareto frontier for both demographic parity and equalized odds. When considering equalized odds, FWC is not a part of the Pareto frontier for the Drug dataset, and more generally, FWC performance is not as competitive. This is in contrast to demographic parity: in Figure 1, FWC sits on the Pareto frontier across all datasets for fairness-performance trade-off in downstream classification. ", "page_idx": 27}, {"type": "text", "text": "Table 8: Presence on the Pareto frontier for FWC across different fairness violation hyper-parameter values $(\\epsilon=\\{0.01,0.05,0.1\\})$ , for both demographic parity (left) and equalized odds (right) in the downstream learning settings of Section 7. As equalized odds is not an independence notion of fairness as demographic parity, constraints on demographic parity do not guarantee an improvement in equalized odds, resulting in FWC not performing as well for downstream performance-fairness tradeoff when using equalized odds. ", "page_idx": 27}, {"type": "table", "img_path": "ylceJ2xIw5/tmp/99eb97e5797f31c2dd6665c22fa12236754943a90b3c838d3cb1e9441fbb6ce8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our work presents a novel approach to obtain coresets (synthetic representative samples) of a given dataset while reducing biases and disparities in subgroups of the given dataset. As other approaches in the field of algorithmic fairness, our efforts may help populations that would otherwise face disadvantages from a model or decision process. Importantly, our approach refrains from exploiting biases inherent in the data itself; rather, it seeks to mitigate biases in data-driven decision systems. It is crucial to note that our method does not claim to address all sources or types of bias. In addition, while our tools enable a malicious modeler to manipulate algorithmic fairness methods to amplify disparities instead of reducing them, for instance, by reversing the fairness constraint (replacing $\\leq$ with $\\geq,$ ), the unfairness of a trained model can be detected by assessing it over a separate test set from the original dataset. ", "page_idx": 27}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/061d3a2a103bd494bbcaadff80b8ba274fe199d2efc3a4f943c51b613e9b07bf.jpg", "img_caption": ["Figure 6: Fairness-utility tradeoff of all methods, indicated by AUC and demographic disparity of a downstream MLP classifier across all datasets (rows) and without (left column) or with (right column) fair pre-processing [34] (excluding FWC , to which no fairness modification is applied after coresets have been generated). The Pareto frontier, indicated with a dashed red-line, is computed across all models and coreset sizes. We report the averages over 10 separate train/test splits. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "ylceJ2xIw5/tmp/5a0250bf8b5956c860a350741fcd34ac44794f0bd9bb588ead91a70752e4cc79.jpg", "img_caption": ["Figure 7: Similarly to Figure 6, we report the means and standard deviations over 10 runs of the fairness-utility tradeoff of all methods, indicated by AUC and demographic disparity of a downstream MLP classifier across all datasets (rows) and without (left column) or with (right column) fair preprocessing [34] (excluding FWC , to which no fairness modification is applied after coresets have been generated). "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 4 includes the details on the majority minimization algorithm for FWC, with Section 6 showing the equivalence between the unconstrained version of FWC and Lloyd\u2019s algorithm for k-medians and k-means. Section 7 includes all experiments, which are presented in the same order as the conclusions presented in the abstract, with the synthetic data experiment first (Figures 1, top left, 2 and Table 2), downstream learning and data augmentation using synthetic representatives (Figures 1, 3, 4, 6, 7 and Tables 3, 4, 5, 6, 7) and reducing biases in prediction from large language models (Table 1). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See the Limitation section in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: See Section 5 and Appendix B for theorems and proposition statements and proof, respectively. The only exception is Proposition 2.1, which is stated in Section 2, Lemma A.1, which is stated and proved in Appendix A and Lemma A.2, which proof follows directly from [77, Corollary 5] by extending to the case in which $m\\neq n$ . ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section 4 provides the algorithm outline, with Algorithm 1 providing the stepby-step breakdown of the algorithm. The variant of FairWASP for line 4 in Algorithm 1 uses the same algorithm as [77] with $m\\neq n$ , while implementation details for the experiments, along with hyper-parameters for each methods can be found in Section 7 and Appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 31}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Data are openly available and details to reproduce the main experimental results are provided in Section 7 and Appendix C. The code is not available publicly at the moment. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Implementation details are provided in Section 7, Appendix C.1 for the runtime analysis with synthetic data and Appendix C.2 for the downstream learning and data augmentation experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we provide commentary in Appendix C.2, and include $1\\sigma$ uncertainty computed over 10 runs with separate seeds in Figures 1 (top left), 2, 3 (bottom row), 4 (bottom row), 5 and 7 and Tables 1, 2, 3, 4, 5, 6 and 7. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: For non-LLM experiment, the computing environment is reported in Appendix C ( Ubuntu machine with 32GB of RAM and 2.50GHz Intel(R) Xeon(R) Platinum 8259CL CPU). LLM experiments require access to the OpenAI API to query GPT-3.5 and GPT-4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include reproducibility details, data are publicly available and we address the broader impact and societal implications in Section E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We address the broader impact of our method in Section E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Not applicable, this paper is not releasing public data, image generator or pretrained language models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All datasets are publicly available, and GPT-3.5 and GPT-4 were access through OpenAI API. All sources were cited in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new assets were introduced by this work. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects were included in this work. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No human subjects were involved in this study. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]