[{"type": "text", "text": "Few-Shot Task Learning through Inverse Generative Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aviv Netanyahu1, Yilun $\\mathbf{D}\\mathbf{u}^{1,2}$ , Antonia Bronars1, Jyothish Pari1, Joshua Tenenbaum1, Tianmin $\\mathbf{Shu}^{3}$ , and Pulkit Agrawal1 ", "page_idx": 0}, {"type": "text", "text": "1Massachusetts Institute of Technology 2Harvard University 3Johns Hopkins University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains \u2013 object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability to learn concepts about a novel task, such as the goal and motion plans, from a few demonstrations is a crucial building block for intelligent agents \u2013 it allows an agent to learn to perform new tasks from other agents (including humans) from little data. Humans, even from a young age, can learn various new tasks from little data and generalize what they learned to perform these tasks in new situations [1]. ", "page_idx": 0}, {"type": "text", "text": "In machine learning and robotics, this class of problems is referred to as Few-Shot Learning [2]. Despite being a widely studied problem, it remains unclear how we can enable machine learning models to learn concepts of a novel task from only a few demonstrations and generalize the concepts to new situations, just like humans do. Common approaches learn policies either directly, which often suffer from covariate shift [3], or via rewards [4\u20136], which are largely limited to previously seen behavior [7]. In a different vein, other work has relied on pretraining on task families and assumes that task learning corresponds to learning similar tasks to ones already seen in the task family [8, 9]. ", "page_idx": 0}, {"type": "text", "text": "Inspired by the success of generative modeling in few-shot visual concept learning [10\u201312], where concepts are latent representations, in this work, we investigate whether and how few-shot task concept learning can benefit from generative modeling as well. Learning concepts from sequential demonstrations rather than images is by nature more challenging due to sequential data often not ", "page_idx": 0}, {"type": "image", "img_path": "atIE6Npr5A/tmp/db47e648cf01d16f1794b8c1cb750bc56fc9b3b95a6bc716cc38ab11a8d696d0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Few-shot concept learning. Given paired task demonstration $\\tau$ (e.g., \u2018walk\u2019) and concept $c$ (a latent representation of the task), we train a generative model $g_{\\theta}$ to generate behavior from a concept. Then, given demonstrations of a new behavior $\\tilde{\\tau}$ (e.g., \u2018jumping jacks\u2019) without its concept label, we aim to learn its concept representation by optimizing concept $\\tilde{c}$ as input to frozen $g_{\\theta}$ . ", "page_idx": 1}, {"type": "image", "img_path": "atIE6Npr5A/tmp/d848b9b6e2ead7b3c65f0b565cb13b128b7715b6832e67c0c6da8ec3c76e272f.jpg", "img_caption": ["Figure 2: Experiment Domains. We extensively evaluate our approach for various domains. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "satisfying the i.i.d. assumption in machine learning [13]. In particular, we assume access to a large pretraining dataset of paired behaviors and task representations to learn a conditional generative model that synthesizes trajectories conditioned on task descriptions. We hypothesize that by learning a generative model conditioned on explicit representations of behavior, we can acquire strong priors about the nature of behaviors in these domains, enabling us to more effectively learn new behavior that is not within the pretraining distribution, given a limited number of demonstrations, and further generate the learned behavior in new settings. ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). In our approach, we first pretrain a large conditional generative model which synthesizes different trajectories conditioned on different task descriptions. To learn new tasks from a limited number of demonstrations, we then formulate few-shot task learning as an inverse generative modeling problem, where we find the latent task description, which we refer to as a concept, which maximizes the likelihood of generating the demonstrations. This approach allows us to leverage the powerful task priors learned by the generative model to learn the shared concepts between demonstrations without finetuning the model (Figure 1). We demonstrate this approach in various domains: object rearrangement, where concepts are relations between objects, goal-oriented navigation, where concepts are target attributes, motion capture, where concepts are human actions, autonomous driving, where concepts are driving scenarios, and real-world table-top manipulation where concepts are manipulation tasks (Figure 2). ", "page_idx": 1}, {"type": "text", "text": "New concepts are either (1) compositions of training concepts (e.g., multiple desired relations between objects that define a new object rearrangement concept) or (2) new concepts that are not explicit compositions in the natural language symbolic space of training concepts (e.g., a new human motion \u2018jumping jacks\u2019 is not an explicit composition of training concepts \u2018walk\u2019, \u2018golf\u2019 etc.) Thanks to generative models\u2019 compositional properties that enable compositional concept learning [14], in addition to being able to learn a single concept from demonstrations directly, FTL-IGM learns compositions of concepts from demonstrations that, when combined, describe the new concept. ", "page_idx": 1}, {"type": "text", "text": "We show that our approach generates diverse trajectories encapsulating the learned concept. We achieve this due to two properties of generative models. First, these models have shown strong interpolation abilities [15, 16], which allow generating the new concept on new initial states they were not demonstrated from. Second, these models have compositional properties that enable compositional trajectory generation [17], which allow composing learned concepts with training concepts to synthesize novel behavior that was not demonstrated (e.g., \u2018jumping jacks\u2019 and \u2018walk\u2019), see Figure 3. We further demonstrate that our approach addresses a unique challenge introduced in learning task concepts: we utilize plans generated by learned concepts in a closed-loop fashion. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are (1) formulating the problem of task learning from few demonstrations as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), (2) adapting a method for efficient concept learning to this problem based on the new formulation, and (3) a systematic evaluation revealing the ability of our method to learn new concepts across a diverse set of domains. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Learning from few demonstrations. Our problem setting is closely related to learning from few demonstrations. There has been much work on learning to generate agent behavior given few demonstrations. There are several common approaches to this problem. First, behavior cloning (BC) is a supervised learning method to learn a policy from demonstrations that predicts actions from states. Similar to our framework, goal-conditioned BC can predict states from task representations and states [18]. Finetuning these models to learn new behaviors requires labeled demonstrations of the new task. We assume unlabeled demonstrations. BC often suffers from covariate shift [3] and fails to generate the demonstrated behavior in novel scenarios. This can be mitigated by assuming access to a human in the loop [19]. Second, the inverse reinforcement learning (IRL) framework learns a policy that maximizes the return of an explicitly [5, 6, 20] or implicitly [21] learned reward function. These works learn a reward for a single task or for a set of tasks (e.g., goal-conditioned IRL [22, 23] and multi-task IRL [24]). While IRL is more data efficient than BC, it is computationally costly due to learning a policy every iteration via an inner reinforcement learning (RL) loop. Additionally, it requires access to taking actions in the environment during training and when faced with a new task, we have to retrain the reward again. A third approach is inverse planning [25\u201327], which can robustly infer concepts such as goals and beliefs even in unseen scenarios. However, it assumes access to a planner, knowledge about environment dynamics, and the task/goal space. Finally, in-context learning approaches [8, 28, 29] learn actions in a supervised manner by representing the task with demonstrations. This allows few-shot generalization without further training. ", "page_idx": 2}, {"type": "text", "text": "In contrast, we do not learn an action-generating policy directly or via a reward function. For concept learning, we do not assume having access to any given planner, world model, actions, rewards, or prior over the task space. Instead, we learn concepts (task representations) from demonstrations via a pretrained generative model that takes a concept as input and directly produces state sequences. We then input the learned concept into the generative model to produce behavior similar yet diverse to the demonstrated one. We further demonstrate how to use these state sequences with a planner to take actions and achieve the desired behavior. The idea of concept learning via generative models has been explored for computer vision applications [11, 12]. We build on this work and show how to extend it to learn agent task concepts. Our work also differs from prior works on learning trajectory representations [30\u201334]. These works focus on learning plans over trajectory embeddings, whereas we learn a task representation from demonstrations on which we condition to generate behavior. ", "page_idx": 2}, {"type": "text", "text": "Generative Models in Decision Making. There has been work on generative modeling for decisionmaking, including generative models for single-agent behaviors, such as implicit BC [35], Diffuser [36, 17], Diffusion Policy [37], Decision Transformer [38\u201340], and for multi-agent motion prediction such as Jiang et al. [41]. The success of diffusion policy in predicting sequences of future actions has led to 3D extensions [42], and combined with ongoing robotic data collection efforts [43] and advanced vision and language models, has led to vision-language-action generative models [44\u201346]. In this work, we utilize a conditional generative model for the inverse problem, i.e., learning concepts from demonstrations. ", "page_idx": 2}, {"type": "text", "text": "Composable representations. There has been work on obtaining composable data representations. $\\beta$ -VAE [47] learns unsupervised disentangled representations for images. MONet [48] and IODINE [49] decompose visual scenes via segmentation masks and COMET [50] and [12] via energy functions. There is also work on composing representations to generate data with composed concepts. Generative models can be composed together to generate visual concepts [14, 51\u201355] and robotic skills [17]. The generative process can also be altered to generate compositions of visual [56\u201359] and molecular [60] concepts. We aim to obtain task concepts and generate them in composition with other task concepts. ", "page_idx": 2}, {"type": "text", "text": "3 Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by recent success in large generative models, we propose a generative formulation for learning specific behavior given a small set of demonstrations, which we term Few-Shot Task Learning through ", "page_idx": 2}, {"type": "image", "img_path": "atIE6Npr5A/tmp/578614368debbe101975f276c8cf70b162538d37008329fd19889ecce44c74a1.jpg", "img_caption": ["Figure 3: Diverse learned concept generation. We generate versions of the new behavior conditioned on the learned concept and (1) new initial states and (2) composed with other concepts. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Inverse Generative Modeling (FTL-IGM). In our formulation, we assume access to a large pretraining dataset $D_{\\mathrm{pretrain}}=\\{(\\tau_{i},c_{i})\\}_{i=1}^{N}$ of state-based sequences $\\tau_{i}=\\{s_{0},s_{1},...\\}\\subseteq\\mathcal{T}$ of states from state space $S$ annotated with meta-data \u201cconcepts\u201d $c_{i}\\in\\mathcal{C}\\subseteq\\mathbb{R}^{n}$ describing trajectories. This assumption is often not prohibitive in practice. There is typically a vast amount of existing data collected from the internet or prior exploration in an environment, which may only need to be weakly annotated to characterize the trajectory, e.g., the goal state. Given $D_{\\mathrm{pretrain}}$ , we learn a conditional generative model $\\mathcal{G}_{\\theta}:\\mathcal{C}\\times\\mathcal{S}\\rightarrow\\mathcal{T}$ conditioned on concepts and initial states, which learns to generate future trajectories. We train the parameters of $g_{\\theta}$ to maximize likelihood arg $\\begin{array}{r}{\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\tau,c\\sim D_{\\mathrm{pretrain}}}[\\log\\mathcal{G}_{\\theta}(\\tau|c,s_{0})]}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Then, given an unlabeled demonstration dataset $D_{\\mathrm{new}}\\sim\\mathcal{D}$ , we formulate learning a new concept $\\tilde{c}$ that is used to sample trajectories from $\\mathcal{D}$ as inverting the generative model. In particular, we learn new concept $\\tilde{c}$ so that our frozen conditional generative model $\\mathcal{G}_{\\theta}$ maximizes the likelihood of trajectories in $D_{\\mathrm{new}}$ , corresponding to ar $;\\mathrm{max}_{\\tilde{c}}\\,\\mathbb{E}_{\\tau\\sim D_{\\mathrm{new}}}[\\log\\mathcal{G}_{\\theta}(\\tau|\\tilde{c},s_{0})]$ . We find that this design choice enables us to leverage the priors learned by $g_{\\theta}$ from $D_{\\mathrm{pretrain}}$ to effectively learn concepts from $D_{\\mathrm{new}}$ given very few demonstrations, even if the demonstrated $D_{\\mathrm{new}}$ deviates from the concept labels $c$ seen in $D_{\\mathrm{pretrain}}$ . For evaluation in closed loop, we further assume access to a planner that given two states plans which action to take in the environment, sometimes via access to simulation in the environment. We use this planner sequentially to make decisions in the environment. ", "page_idx": 3}, {"type": "text", "text": "The key difference between our approach to few-shot adaptation from demonstrations and prior approaches is the assumption and usage of a large pretraining dataset of paired behaviors and concepts $D_{\\mathrm{pretrain}}$ combined with an invertible generative model. We learn new concepts solely from demonstrations without finetuning model weights or taking actions in the environment by relying on the pretrained concept space. ", "page_idx": 3}, {"type": "text", "text": "4 Few-Shot Concept Learning Based on FTL-IGM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We adapt a few-shot concept learning method to task concepts based on the FTL-IGM framework. During training we learn a generative model $g_{\\theta}$ from training $\\{(\\tau_{i},c_{i})\\}_{i}$ pairs. We then freeze $g_{\\theta}$ , and given demonstrations of a new task $\\{\\widetilde{\\tau}\\}_{i}$ , optimize a concept $\\tilde{c}$ to produce the new behavior via $\\mathcal{G}_{\\theta}$ . We then generate a diverse set of behaviors via ${\\mathcal{G}}_{\\theta}$ , either for the learned concept $\\tilde{c}$ conditioned on new initial states or for compositions of $\\tilde{c}$ with other concepts. ", "page_idx": 3}, {"type": "text", "text": "4.1 Training a diffusion model to generate behavior ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A diffu\u221asion model is a generative model that given a forward noise adding process $q(x_{t}|x_{t-1}):=$ $\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\tilde{\\mathbf{I}})$ starting from data $x_{0}$ according to a variance schedule $\\beta_{1},...,\\beta_{T}$ , learns the reverse process $p_{\\theta}(x_{t-1}|x_{t}):=\\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t))$ . Ho et al. [61] simplify the training objective to estimate noise $\\mathbb{E}_{t\\sim\\mathcal{U}\\left\\{1,T\\right\\},x_{0},\\epsilon\\sim\\mathcal{N}\\left(0,\\mathbf{I}\\right)}\\left[\\vert\\vert\\epsilon-\\epsilon_{\\theta}(x_{t},t)\\vert\\vert^{2}\\right]$ where $x_{t}$ is produce\u221ad by adding noise $\\epsilon$ to data $x_{0}$ by the forward noising process at diffusion step $t$ , $q(x_{t}|x_{0}):=\\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha}_{t}}x_{0},(1-$ $\\bar{\\alpha}_{t})\\mathbf{I}$ ) where $\\bar{\\alpha}_{t}\\;:=\\;\\Pi_{s=1}^{t}(1\\,-\\,\\beta_{s})$ . Dhariwal and Nichol [62] enable conditioned generation by guiding the re\u221averse noising process with classifier gradients. The noise prediction becomes $\\overleftarrow{\\epsilon}=\\epsilon_{\\theta}(x_{t},t)-\\omega\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla_{x_{t}}\\overleftarrow{\\log}p_{\\phi}(y|x_{t},t)$ where classifier $p_{\\phi}(y|x_{t},t)$ is trained on noisy images, and $\\omega$ is the guidance scale. Ho and Salimans [63] introduce classifier-free guidance that achieves the same objective without the need for training a separate classifier. This is done by learning a conditional and unconditional model by removing the conditioning information with dropout during training. The noise prediction is then $\\hat{\\epsilon}=\\epsilon_{\\theta}(x_{t},t)+\\omega(\\epsilon_{\\theta}(x_{t},y,t)-\\epsilon_{\\theta}(x_{t},t))$ . Ramesh et al. [64] and Nichol et al. [65] demonstrate how this idea can be used to generate images conditioned on a class. Diffusion models have recently shown success as generative models for decision making [36, 17]. Specifically, Ajay et al. [17] used a conditional classifier-free guidance diffusion model [63] to generate trajectories of future states to reach given an input observation. We adopt this objective and learn a denoising model $\\epsilon_{\\theta}$ conditioned on latent concepts and initial observed states to estimate noise of a future state trajectory: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\tau,c)\\sim D_{\\mathrm{pertain}},\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}),t\\sim\\mathcal{U}\\{1,T\\},\\gamma\\sim\\mathrm{Bern}(p)}[||\\epsilon-\\epsilon_{\\theta}(x_{t}(\\tau),(1-\\gamma)c+\\gamma c_{\\emptyset},s_{0},t)||^{2}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p$ is the probability of removing conditioning information which is then replaced by dummy condition $c_{\\varnothing}$ , and $s_{0}$ is the initial state corresponding to trajectory $\\tau$ . $x_{t}(\\tau)$ is obtained from $x_{0}=\\tau$ by the forward noising process. We then extend this approach for the inverse problem, namely, learning a concept from demonstrations. ", "page_idx": 4}, {"type": "text", "text": "4.2 Few-shot concept learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Gal et al. [11] use a frozen generative model to learn visual concept representations from few images depicting the concept by optimizing the model\u2019s input $v_{*}=\\arg\\operatorname*{min}_{v}\\!\\mathbb{E}_{x_{0},v,\\epsilon\\sim\\!N(0,1),t}\\big[||\\epsilon-$ $\\epsilon_{\\theta}(x_{t},c_{\\theta}(v),t)||_{2}^{2}]$ , where $c_{\\theta}$ and $\\epsilon_{\\theta}$ are fixed. Liu et al. [12] extend this and learn visual concept compositions with a pretrained diffusion model in an unsupervised manner. Namely, from a set of images that depict various concepts, for each image $x^{i}$ they learn a set of weights $\\bar{\\omega}_{k}^{i}$ and a shared set of visual concepts for all images $c_{k}$ , $\\begin{array}{r}{\\hat{\\epsilon}=\\epsilon(x_{t}^{i},t)+\\sum_{k=1}^{K}\\omega_{k}^{i}\\big(\\epsilon(x_{t}^{i},c_{k},t)-\\epsilon(x_{t}^{i},t)\\big)}\\end{array}$ . We extend these formulations to inferring multiple concepts, whos e composition describes a single task concept, from few demonstrations of a task. ", "page_idx": 4}, {"type": "text", "text": "Given a trained diffusion model $\\epsilon_{\\theta}$ and demonstrations of a new concept $\\{\\widetilde{\\tau}\\}_{i}$ from $D_{\\mathrm{new}}$ , we learn concepts $\\{\\tilde{c}_{1},...,\\tilde{c}_{K}\\}$ for $K\\geq1$ and their weights $\\{\\omega_{1},...,\\omega_{K}\\}$ that best describe the demonstrations. Starting from uniformly sampled concept embeddings $\\tilde{c}_{k}\\sim\\mathcal{U}([0,1]^{n})$ , we freeze $\\epsilon_{\\theta}$ , and optimize $\\tilde{c}_{k}$ and $\\omega_{k}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}[||\\epsilon-(\\epsilon_{\\theta}(x_{t}(\\tilde{\\tau}),c_{\\theta},s_{0},t)+\\sum_{k=1}^{K}\\omega_{k}(\\epsilon_{\\theta}(x_{t}(\\tilde{\\tau}),\\tilde{c}_{k},s_{0},t)-\\epsilon_{\\theta}(x_{t}(\\tilde{\\tau}),c_{\\theta},s_{0},t)))||^{2}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We find that this compositional approach enables us to effectively represent and learn new demonstrations, even when demonstrations are substantially different than those seen in training tasks. ", "page_idx": 4}, {"type": "text", "text": "4.3 Generating the learned concept ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After learning concepts $\\tilde{c}_{k}$ , whose composition describes the new task $\\tilde{c}$ , we evaluate the behavior it generates by initializing $x_{T}(\\tau)\\sim\\mathcal{N}(0,\\alpha\\mathbf{I})$ , and compute $x_{t}\\sim\\mathcal N(\\mu_{t-1},\\alpha\\Sigma_{t-1})$ iteratively as a function of the estimated denoising function $\\hat{\\epsilon}(\\epsilon_{\\theta})$ , where $\\mu$ and $\\Sigma$ are the mean and variance that define the reverse process, and $\\alpha\\in[0,1)$ is a scaling factor that leads to lower temperature samples, until generating $x_{0}=\\tau$ representing the trajectory of the agent. The denoising function is constructed by fixed or learned weights as defined in Eq. 2 and by any number of concepts $\\geq1$ . The applications of the generation procedure can be summarized as: ", "page_idx": 4}, {"type": "text", "text": "Learned concept and demonstrated initial states. We apply our learned concept to a set of demonstrated initial states. In domains where the initial state and concept jointly determine optimal behavior, the generated trajectory corresponds to optimal actions to execute (e.g. goal-oriented navigation). In contrast to other domains where the initial state is irrelevant for a task due to the randomness in sampling $x_{T}(\\tau)$ (e.g. motion capture), generated trajectories correspond to diverse plausible behaviors exhibiting the learned concept. ", "page_idx": 4}, {"type": "text", "text": "Learned concept and novel initial states. We further apply the generation procedure from our learned concept on novel initial states, to generate trajectories of new behaviors exhibiting our conditioned concept. Prior methods may suffer from covariate shift in this setting [21]. We empirically show that our method is less prone to this problem. ", "page_idx": 4}, {"type": "text", "text": "Learned concept composed with other concepts. Finally, we modify the generation procedure of our newly learned concept to generate trajectories that simultaneously exhibit other concepts. To generate a trajectory with an added another concept, we add another term to the sum in Eq. 2 where the learned concept is composed with a training concept $c_{k}$ and its weight $\\omega_{k}\\colon\\omega_{k}\\bigl(\\epsilon_{\\theta}\\bigl(x_{t}(\\tau),c_{k},s_{0},t\\bigr)-$ $\\epsilon_{\\theta}(x_{t}(\\tau),c_{\\theta},s_{0},t))$ . This modified generation procedure constructs trajectories which exhibits behavior that has a composition of the learned concept and the other specified concepts [14]. ", "page_idx": 4}, {"type": "image", "img_path": "atIE6Npr5A/tmp/e6bf1af3eddde17ca6f2bb196cea97ad45ef47197cf3576cd9fa9267cdf54c4a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Object rearrangement. Training concepts are single pairwise relations (\u2018A right of/above $\\mathbf{B}^{\\bullet}$ ), and new concepts are either compositions of training concepts (\u2018A right of/above $\\mathbf{B}^{\\,\\bullet}\\ \\wedge$ \u2018B right of/above C\u2019) or new relations (\u2018A diagonal to B\u2019, \u2018A, B, C on circle circumference of radius r\u2019). ", "page_idx": 5}, {"type": "image", "img_path": "atIE6Npr5A/tmp/d01405e5cb05e63a2019df1e044d885a2938e35e594d906c13f175bf5991a609.jpg", "img_caption": ["Figure 5: Object rearrangement new concept qualitative evaluation. Learning the new concept \u2018square diagonal to triangle\u2019 and composing it with the training concept \u2018circle right of square\u2019. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Similarly to Ajay et al. [17], in environments where an inverse dynamics model is provided, we generate trajectories in a closed loop. We execute actions calculated by the inverse dynamics given the predicted plan by the model and then repeatedly replan given new observations. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We demonstrate results in four domains where concept representations are T5 [66] embeddings of task descriptions in natural language for training, and empty string embeddings for the dummy condition. During few-shot concept learning, we are provided with three to five demonstrations of a composition of training concepts or of a novel concept that is not an explicit composition of training tasks in natural language symbolic space. We ask a model to learn the concept from these demonstrations. ", "page_idx": 5}, {"type": "text", "text": "5.1 Task-Concepts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Learning concepts describing goals that are spatial relations between objects. Object rearrangement is a common task in robotics [67\u201369] and embodied artificial intelligence (AI) [70, 71], serving as a foundation for a broader range of tasks such as housekeeping and manufacturing. Here, we use a 2D object rearrangement domain to evaluate the ability of our method to learn task specification concepts. Given a concept representing a relation between objects, we generate a single state describing that relation. The concept in a training example describes the relation (either \u2018right of\u2019 or \u2018above\u2019) between only one pair of objects (out of three objects) in the environment. Then, a model must learn compositions of these pairwise relations and new concepts such as \u2018diagonal\u2019 and \u2018circle\u2019 (see Figure 4). The results in Figures 5 and 6 demonstrate that our method learns unseen compositions of training concepts and new concepts. They further demonstrate how our method composes new concepts with learned concepts. For additional qualitative results, please refer to Appendix A. ", "page_idx": 5}, {"type": "text", "text": "While successful in most cases, there are also a few failure examples. The accuracy for the new \u2018circle\u2019 concept is low (0.44) compared to the mean over task types in Figure 6 Object Rearrangement New Concept $(0.82\\pm0.09)$ . This is most likely due to this concept lying far out of the training distribution. The task \u2018square right of circle $\\wedge$ triangle above circle\u2019 has low accuracy for 2 concepts (0.32) compared to the mean in Table 2 Object Rearrangement Training Composition $(0.75\\pm0.11)$ . This may arise from the combined concept-weight optimization process \u2013 as there is no explicit regularization on weights, they may converge to 0 or diverge. In Figure 12, we show that concept components may or may not capture new concept relations. ", "page_idx": 5}, {"type": "text", "text": "Learning concepts describing goals based on attributes of target objects. We test our method in a goal-oriented navigation domain adapted from the AGENT dataset [72], where an agent navigates to one of two potential targets. Conditioned on a concept representing the attributes of the desired target object and initial state, we generate a state-based trajectory describing an agent navigating to the target. Each object has a color and a shape out of four possible colors and four shapes. During training, we provide 16 target-distractor combinations that include all colors and shapes (but not all combinations), and a concept is conditioned on one of the target\u2019s attributes (e.g., color). We introduce new concepts defined by both target attributes, including (1) unseen color-shape target combinations and (2) new target-distractor combinations. Figure 7 shows an example. In training, we see bowl and red object targets. A new concept includes a novel composition as the target \u2013 red bowl. The new concept distractor objects (green bowl and red sphere) were introduced during training, but they were not paired with a red bowl as the target. As Figure 13 shows, our method successfully learns concepts where targets are new compositions of target attributes in settings with new target-distractor pairs and generalizes to new initial object states. We further evaluate our model and baselines in closed loop (Figure 6) by making an additional assumption that a planner is provided. The planner produces an action given a current state and a future desired state predicted by a model. ", "page_idx": 5}, {"type": "image", "img_path": "atIE6Npr5A/tmp/67a11abd830001ee50bfec11dfc30e954e8c47ef791af5316ee4a370100428f4.jpg", "img_caption": ["Figure 6: Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning. Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix B and for baselines implementation in Appendix C. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Learning concepts describing human motion. Unlike prior work on learning to compose human poses from motion capture (MoCap) data [e.g., 73, 74], here we focus on the inverse problem \u2013 learning new actions from MoCap data. In particular, we use the CMU Graphics Lab Motion Capture Database (http://mocap.cs.cmu.edu/). We train on various human actions in the database and few-shot learn three novel concepts (see Appendix B.3 for details). Learning tasks from few demonstrations is especially beneficial in this domain since describing motion concepts in words could be hard. In Table 5.1, we ask five human volunteers to select generated behaviors that depict training and new concepts. We demonstrate quantitatively that our method generates human motion that captures the desired behavior across training and new behaviors. We qualitatively demonstrate how our method generates learned new concepts (\u2018jumping jacks\u2019 and \u2018breaststroke\u2019) from new initial states and composes \u2018jumping jacks\u2019 with training concepts \u2018walk\u2019, \u2018jump\u2019, and \u2018march\u2019. ", "page_idx": 6}, {"type": "text", "text": "Results are best viewed on our website website. We compare motion generated by our method to various baselines on new initial states for \u2018jumping jacks\u2019 and \u2018breaststroke\u2019. Our method is noisy yet captures the widest range of motion. While other methods often produce smoother trajectories, they mostly capture local (VAE) or degenerate (BC, In-Context, Language) motion. ", "page_idx": 6}, {"type": "text", "text": "Learning concepts describing driving scenarios. In an Autonomous Driving domain [75], an agent acts in a challenging multi-agent environment to complete a driving task. We train on several driving scenarios (\u2018highway\u2019, \u2018exit\u2019, \u2018merge\u2019, and \u2018intersection\u2019) and learn a new driving scenario (\u2018roundabout\u2019) from several demonstrations (see Figure 8 and further details in Appendix B.4). We evaluate this scenario in closed loop on new initial states, assuming access to a planner that can simulate taking actions in the environment. Over two evaluation metrics (crash and task completion rate), our method achieves overall best results (Figure 9). ", "page_idx": 6}, {"type": "text", "text": "Learning concepts describing real-world table-top manipulation tasks. We evaluate our method\u2019s capability to learn a novel concept for real-world table-top manipulation with a Franka Research 3 robot. Training concepts include \u2018pick green circle and place on book\u2019, \u2018pick green circle and place on elevated white surface\u2019, \u2018push green circle to orange triangle\u2019 and \u2018push green circle to orange triangle around purple bowl\u2019. The new scenario includes pushing the green circle to the orange triangle on a book (Figure 10). We evaluate training pushing in closed loop and and achieve success ", "page_idx": 6}, {"type": "text", "text": "Figure 7: Goal-oriented navigation. In training, targets are defined by a single attribute (color or shape), and in new concepts, targets are defined by a novel combination of color and shape attributes. To make the setting more challenging, distractor objects in new concept demonstrations have a combination of attributes that are within the training distribution. ", "page_idx": 7}, {"type": "table", "img_path": "atIE6Npr5A/tmp/bd9e601fcf95ff303e897054938ca2ae660c269255e62aca33f794d90dc4de1f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: MoCap human experiment. For training concepts and new concepts on new initial states, we report (top) percentage of time each method is the most successful at depicting a concept and (bottom) percentage of time each method depicts a concept. Mean and standard deviation are calculated over the number of scenarios in each setting and human subjects. ", "page_idx": 7}, {"type": "text", "text": "$90\\%$ accuracy). We evaluate the new concept, elevated pushing, against a baseline that conditions on the training \u2018push green circle to orange triangle\u2019 representation in the new scenario setup where the objects are placed on a book. Learning the new concept succeeds $55\\%$ accuracy) while using the training representation fails $15\\%$ accuracy) and the robot often pushes the book instead of the object. Details are in Appendix B.5 and qualitative results are on our website. ", "page_idx": 7}, {"type": "text", "text": "5.2 Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Goal conditioned behavior cloning. We compare our method with goal-conditioned behavior cloning (BC), which, given a condition and a state, outputs the next state in a sequence. It is trained on our paired pretraining dataset and learns concepts by optimizing the input condition to maximize the likelihood of new concept demonstration transitions. We test its ability to compose new learned concepts with training concepts naively by adding conditions that are then inputted into the model. We observe that even though goal-conditioned BC has access to the pretrained dataset and conditions, and while it may learn new concepts, it suffers from covariate shift on new initial states and lacks the ability to generalize to novel compositions of the learned concept with training concepts (Figures 6, 9, 13). To achieve these generalization capacities, we need a model that can process interpolated (initial states) and composed (concepts) conditions, such as our generative model. ", "page_idx": 7}, {"type": "text", "text": "Learning the concept space with a VAE. We compare our method with VAE [76] that does not utilize the concepts in the paired pretraining data but rather learns the concept space by reconstructing pretraining data trajectories through their encoded representation $z$ . Trajectories are generated via a decoder conditioned on an initial state and $z$ with added noise. $z$ is obtained by encoding a trajectory for training evaluation and by fixing the decoder and optimizing $z$ to generate a given demonstration when learning a new concept. We find that the VAE model learns a latent space that captures training and new concepts but does not enable generalization to new initial states (Figures 6, 9, 13). This highlights the importance of concept representations in the pretrained data. ", "page_idx": 7}, {"type": "image", "img_path": "atIE6Npr5A/tmp/c3e1d93edeb47deebffa45cdb67b0e4571504993fe3e64d2312496bdcbc3bfaa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 8: Autonomous driving. A controlled agent (green) completes various driving objectives as fast as possible while sharing the road with other vehicles (blue) and avoiding collisions. Training concepts: \u2018highway\u2019, \u2018exit\u2019, \u2018merge\u2019, and \u2018intersection\u2019, new concept: \u2018roundabout\u2019. ", "page_idx": 7}, {"type": "image", "img_path": "atIE6Npr5A/tmp/d6df952794d378544d73575dd387069a35f6ed587c70bb4a28587c0b1012dfc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 9: Driving crash (left) and success (right) rates. Crash rate (lower is better) and task completion rate (higher is better) averaged over training tasks. We report standard errors over training tasks and accuracy over 50 trajectories generated from the learned new concept. VAE has a high completion rate yet a high crash rate. In-context has a low crash rate yet $0\\%$ success rate \u2013 typically, the controlled vehicle reaches the roundabout\u2019s center but does not complete the crossing. Overall, our method learns to complete the roundabout crossing with competitive crash and success rates. ", "page_idx": 8}, {"type": "image", "img_path": "atIE6Npr5A/tmp/99513633a5859bca15f0f62f6d14921cbb4e0c2313ea64e5cc2e5ab371240270.jpg", "img_caption": ["Figure 10: Table-top manipulation. Training concepts: pick-and-place onto elevated surfaces and table-top pushing. New concept: pushing on an elevated surface. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In-Context learning. We compare our approach with training a method to in-context learn from demonstrations. Specifically, we compare our approach to Xu et al. [28] using the pretrained dataset for few-shot behavior generation without further training. We sequentially predict states from demonstrations of a concept and window of current states and show that our method adapts better to new concepts (Figures 6, 9, 13). This emphasizes the need to learn explicit concept representations. ", "page_idx": 8}, {"type": "text", "text": "Conditioning on Language Descriptions of New Concepts. There has been work on generating actions from language instructions [77\u201379]. We demonstrate that in our setup, merely providing new concept language instructions embedded with T5 (as in our pretraining dataset) is insufficient, and generalization is better when learning concepts from few demonstrations. For AGENT, training compositions on new initial states has an average accuracy and standard error of $0.63\\pm0.07$ , lower than ours $(0.73\\pm0.07)$ . For Object Rearrangement, training compositions $(0.2\\pm0.07)$ , new concept $(0.2\\pm0.05)$ , and new and training concept compositions $(0.13\\pm0.04)$ accuracies are significantly lower than ours $(0.9\\pm0.04\\$ , $0.82\\pm0.09$ and $0.8\\pm0.02)$ ). For MoCap, we demonstrate qualitatively that instead of capturing new human actions, the agent transitions into walking. Results are best viewed on our website. ", "page_idx": 8}, {"type": "text", "text": "5.3 Learning two concepts yields higher accuracy than one concept ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When learning weights together with concepts, we check the effect of the number of learned concepts and weights. We report results in Table 2 for Object Rearrangement, AGENT, and Driving, and find that, on average, learning two concepts improves concept learning. We demonstrate qualitatively for MoCap that learning two conditions is preferable. In \u2018jumping jacks\u2019, we observe that the motion lacks raising and lowering both arms and in \u2018breaststroke\u2019, it lacks complete arm and upper torso movement. Results are best viewed on our website. ", "page_idx": 8}, {"type": "text", "text": "5.4 How are learned new concepts related to training concepts? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "New concepts that are compositions of training concepts. We analyze what the learned two concepts in Object Rearrangement and AGENT learn for novel concept compositions (e.g., \u2018red bowl\u2019). For each concept (e.g., \u2018red\u2019 and \u2018bowl\u2019), we generate two sets of 50 samples from the learned components. Table 3 shows accuracy for these sets over the concepts. In some cases (most notably the \u2018line\u2019 concept in Object Rearrangement, \u2018circle right of triangle and triangle right of square\u2019), each ", "page_idx": 8}, {"type": "table", "img_path": "atIE6Npr5A/tmp/bdbaedce290f582c1ec77444f00e9317b88c1b76dbcd703d07e4b09d58e14cc3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 2: Ablation on the number of learned concepts. We test the effect of the number of learned concepts and weights in FTL-IGM on the generation accuracy of new learned concepts. On average, learning two concept components and their weights is preferable to learning one concept component and its weight. We report average accuracy and standard error over task types for Object Rearrangement and AGENT. Driving includes a single new concept and we report accuracy only. ", "page_idx": 9}, {"type": "text", "text": "learned component captures a single composed concept. In other cases, a single learned component captures both concepts (Figure 12). ", "page_idx": 9}, {"type": "text", "text": "New concepts that are not explicit compositions of training concepts in natural language symbolic space. In Figure 11 we visualize t-SNE [80] embeddings for T5 training concept representations and learned concept component representations. We note that learned components are relatively close to training concepts, maintaining the model\u2019s input distribution, yet capture concepts that are not explicit compositions of training concepts. ", "page_idx": 9}, {"type": "image", "img_path": "atIE6Npr5A/tmp/e1ea188e660a959d9ec52b0f3d647f7d8d1ddee1d9d4ae05a29819cec4bf0127.jpg", "img_caption": ["Figure 11: t-SNE embeddings of new concepts that are not explicit compositions of training concepts. See interactive version for detailed labels on our website. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we formulate the problem of new task concept learning as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). We adapt a method for concept learning based on this new formulation and evaluate task concept learning against baselines in four domains. Our extensive experimental results show that, unlike the baselines, FTL-IGM successfully learns novel concepts from a few examples and generalizes the learned concepts to unfamiliar scenarios. It also composes learned concepts to form unseen behavior thanks to the compositionality of the generative model. These results demonstrate the efficacy, sample efficiency, and generalizability of FTL-IGM. ", "page_idx": 9}, {"type": "text", "text": "However, our work has several limitations. First, while our framework is general for any parameterized generative model, our implementation with a diffusion model incurs high inference time. We note that there is still space for improvement in the MoCap generation quality and in the compatibility rate of demonstrations generated by composing learned and training concepts. In addition, we assume that learned concepts lie within the landscape of training concepts to learn them from a few demonstrations without retraining the model. We have approached the question of what new concepts can be represented by compositions of concepts in this landscape empirically, leaving a theoretical analysis as future work. We are hopeful that with the continued progress in the field of generative AI, more powerful pretrained models will become available. Combined with our framework, this will unlock a stronger ability to learn and generalize various task concepts in complex domains. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was also partly sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19- 2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. We acknowledge support from ONR MURI under N00014-22-1-2740 and ARO MURI under W911NF-23-1-0277. Yilun is supported in part by an NSF Graduate Research Fellowship. We would like to thank Abhishek Bhandwaldar for help with the AGENT environment, and Anurag Ajay, Lucy Chai, Andi Peng, Felix Yanwei Wang, Anthony Simeonov and Zhang-Wei Hong for helpful discussions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brenden M Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. arXiv preprint arXiv:1901.04587, 2019. 1   \n[2] Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning. arXiv preprint arXiv:2203.04291, 2022. 1   \n[3] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227\u2013244, 2000. 1, 3   \n[4] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, volume 8, pages 1433\u20131438, 2008. 1   \n[5] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, volume 1, page 2, 2000. 3   \n[6] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations, 2018. 1, 3   \n[7] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pages 1259\u20131277. PMLR, 2020. 1   \n[8] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in Neural Information Processing Systems, 30, 2017. 1, 3, 25   \n[9] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. In Conference on Robot Learning, pages 357\u2013368. PMLR, 2017. 1   \n[10] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015. 1   \n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2023. 3, 5   \n[12] Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compositional concepts discovery with text-to-image generative models. In International Conference on Computer Vision, 2023. 1, 3, 5   \n[13] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. 2   \n[14] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022. 2, 3, 5   \n[15] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021. 2   \n[16] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, volume 35, pages 36479\u201336494, 2022. 2   \n[17] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In International Conference on Learning Representations, 2023. 2, 3, 5, 6, 24, 25   \n[18] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. Advances in Neural Information Processing Systems, 32, 2019. 3   \n[19] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 627\u2013635, 2011. 3   \n[20] Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. In International Conference on Autonomous Agents and Multiagent Systems, 2021. 3   \n[21] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in Neural Information Processing Systems, 29, 2016. 3, 5   \n[22] Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Edward Grefenstette. Learning to understand goal specifications by modelling reward. In International Conference on Learning Representations, 2019. 3   \n[23] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. In International Conference on Learning Representations, 2019. 3   \n[24] Adam Gleave and Oliver Habryka. Multi-task maximum entropy inverse reinforcement learning. In Proceedings of the 1st Workshop on Goal Specifications for Reinforcement Learning, 2018. 3   \n[25] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. Cognition, 113(3):329\u2013349, 2009. 3   \n[26] Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour, 1(4):0064, 2017.   \n[27] Tan Zhi-Xuan, Jordyn Mann, Tom Silver, Josh Tenenbaum, and Vikash Mansinghka. Online bayesian goal inference for boundedly rational planning agents. Advances in Neural Information Processing Systems, 33:19238\u201319250, 2020. 3   \n[28] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In International Conference on Machine Learning, pages 24631\u201324645. PMLR, 2022. 3, 9, 25   \n[29] Ahmed Touati, J\u00e9r\u00e9my Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? arXiv preprint arXiv:2209.14935, 2022. 3   \n[30] Sherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, Aaron Van Den Oord, and Oriol Vinyals. Vector quantized models for planning. In International Conference on Machine Learning, pages 8302\u20138313. PMLR, 2021. 3   \n[31] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with goalconditioned policies. Advances in Neural Information Processing Systems, 32, 2019.   \n[32] John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In International Conference on Machine Learning, pages 1009\u20131018. PMLR, 2018.   \n[33] Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.   \n[34] Divyansh Garg, Skanda Vaidyanath, Kuno Kim, Jiaming Song, and Stefano Ermon. Lisa: Learning interpretable skill abstractions from language. Advances in Neural Information Processing Systems, 35:21711\u201321724, 2022. 3   \n[35] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158\u2013168. PMLR, 2022. 3   \n[36] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. 3, 5   \n[37] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems, 2023. 3   \n[38] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in Neural Information Processing Systems, 2021. 3   \n[39] Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rockt\u00e4schel, Edward Grefenstette, and Yuandong Tian. Efficient planning in a compact latent action space. International Conference on Learning Representations, 2023.   \n[40] Shyam Sudhakaran and Sebastian Risi. Skill decision transformer. arXiv preprint arXiv:2301.13573, 2023. 3   \n[41] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9644\u20139653, 2023. 3   \n[42] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy. arXiv preprint arXiv:2403.03954, 2024. 3   \n[43] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 3   \n[44] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, and Xin Yan. Yilun du, yining hong, and chuang gan. 3d-vla: A 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 7(8), 2024. 3   \n[45] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024.   \n[46] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 3   \n[47] Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. International Conference on Learning Representations, 3, 2017. 3   \n[48] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019. 3   \n[49] Klaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pages 2424\u20132433. PMLR, 2019. 3   \n[50] Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional energy concepts. Advances in Neural Information Processing Systems, 34: 15608\u201315620, 2021. 3   \n[51] Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba. Learning to compose visual relations. Advances in Neural Information Processing Systems, 34:23166\u201323178, 2021. 3   \n[52] Weili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-space energy-based models. Advances in Neural Information Processing Systems, 34:13497\u201313510, 2021.   \n[53] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International Conference on Machine Learning, pages 8489\u20138510. PMLR, 2023.   \n[54] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for (score-based) text-controlled generative models. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation and inference with energy based models. arXiv preprint arXiv:2004.06030, 2020. 3   \n[56] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 3   \n[57] Changhao Shi, Haomiao Ni, Kai Li, Shaobo Han, Mingfu Liang, and Martin Renqiang Min. Exploring compositional visual generation with latent classifier guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 853\u2013862, 2023.   \n[58] Yuren Cong, Martin Renqiang Min, Li Erran Li, Bodo Rosenhahn, and Michael Ying Yang. Attribute-centric compositional text-to-image generation. arXiv preprint arXiv:2301.01413, 2023.   \n[59] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. 3   \n[60] Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, and Tommi Jaakkola. Compositional sculpting of iterative generative processes. Advances in Neural Information Processing Systems, 36:12665\u201312702, 2023. 3   \n[61] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851, 2020. 4   \n[62] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021. 4   \n[63] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4, 5   \n[64] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 4   \n[65] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 4   \n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 6   \n[67] Ankit Shah, Pritish Kamath, Julie A Shah, and Shen Li. Bayesian inference of temporal task specifications from demonstrations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3804\u20133813. Curran Associates, Inc., 2018. 6   \n[68] Fujian Yan, Dali Wang, and Hongsheng He. Robotic understanding of spatial relationships using neural-logic learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 8358\u20138365. IEEE, 2020.   \n[69] Ryan Rowe, Shivam Singhal, Daqing Yi, Tapomayukh Bhattacharjee, and Siddhartha S Srinivasa. Desk organization: Effect of multimodal inputs on spatial relational learning. In 2019 28th IEEE International Conference on Robot and Human Interactive Communication, pages 1\u20138. IEEE, 2019. 6   \n[70] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890, 2020. 6   \n[71] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pages 477\u2013490. PMLR, 2022. 6   \n[72] Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin Smith, Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua Tenenbaum, and Tomer Ullman. Agent: A benchmark for core psychological reasoning. In International Conference on Machine Learning, 2021. 6, 21, 22   \n[73] Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, and Juan Carlos Niebles. Imitation learning for human pose prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7124\u20137133, 2019. 7   \n[74] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. International Conference on Learning Representations, 2023. 7   \n[75] Edouard Leurent. An environment for autonomous driving decision-making. https://github. com/eleurent/highway-env, 2018. 7, 23   \n[76] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 8, 25   \n[77] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):7327\u20137334, 2022. 9   \n[78] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.   \n[79] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. 9   \n[80] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008. 10   \n[81] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3749\u20133761, 2022. 22   \n[82] Julian Tanke, Chintan Zaveri, and Juergen Gall. Intention-based Long-Term Human Motion Anticipation. International Conference on 3D Vision, 2021. 23   \n[83] Edouard Leurent. rl-agents: Implementations of reinforcement learning algorithms. https: //github.com/eleurent/rl-agents, 2018. 23   \n[84] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016. 25   \n[85] Dean Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In D.S. Touretzky, editor, Proceedings of Neural Information Processing Systems, pages 305 \u2013 313. Morgan Kaufmann, December 1989. 25   \n[86] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. 25 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Object Rearrangement. We display additional states generated by our model for various new concepts. We generate learned concepts that are compositions of training concepts (Figure 14), new concepts that are not explicit compositions of training concepts (Figure 15), and a new concept composed with a training concept (Figure 16). We analyze the learned concepts qualitatively (Figure 12) and quantitatively (Table 3). ", "page_idx": 16}, {"type": "text", "text": "Goal-Oriented Navigation. We provide accuracy for open loop evaluation in Figure 13. We further analyze the learned concepts quantitatively (Table 3). ", "page_idx": 16}, {"type": "image", "img_path": "atIE6Npr5A/tmp/d48a98688587ee179fcde4f6603f8392561e3e1281ce0cc31f2b618b9c9c9126.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 12: Object rearrangement new compositions analysis qualitative evaluation. \u2018circle right of triangle $\\wedge$ triangle right of square\u2019 (top) each learned component corresponds to a single composed concept: \u2018circle right of triangle\u2019 (component 1) and \u2018triangle right of square\u2019 (component 2). In \u2018square right of circle $\\wedge$ triangle above circle\u2019 (bottom), learned component 2 corresponds to both composed concepts and component 1 to none. ", "page_idx": 16}, {"type": "image", "img_path": "atIE6Npr5A/tmp/0e5a7c3479f585bc83ece0a512b2fe4be86199c1c2118d5a3fc8a5e6458d99ee.jpg", "img_caption": ["Figure 13: AGENT open loop evaluation. We plot the average and standard error over task types. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Data Generation and Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Object Rearrangement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Training. The training dataset of $\\sim11\\mathrm{k}$ samples consists of concepts \u2018A right of $\\mathbf{B}^{\\,\\cdot}$ and $\\mathbf{\\lambda}_{\\mathrm{A}}$ above $\\mathbf{B}^{\\,\\bullet}$ , where A and $\\mathbf{B}$ are one of three objects: circle, triangle, or square. Altogether, there are 12 possible concepts (two relations and three objects where order is important). In the data generation process, A at center position $(x_{A},y_{A})\\in[0,\\bar{5}]^{2}$ with radius $r_{A}\\in[0.3,1]$ and angle $\\theta_{A}\\in[0,2\\pi]$ is considered \u2018right of\u2019 $\\mathbf{B}$ at center position $\\left(x_{B},y_{B}\\right)$ with radius $r_{B}$ if $x_{A}>x_{B}$ and $|y_{A}-y_{B}|\\leq r_{A}$ . Similarly, A is considered \u2018above\u2019 $\\mathbf{B}$ if $y_{A}>y_{B}$ and $|x_{A}-x_{B}|\\leq r_{A}$ . We further verify that training objects do not overlap. ", "page_idx": 16}, {"type": "text", "text": "New Tasks. The new scenarios include ", "page_idx": 16}, {"type": "text", "text": "\u2022 Five novel compositions of training concepts (training composition in Figure 6). \u2018triangle right of square $\\wedge$ circle above square\u2019, \u2018square right of triangle $\\wedge$ circle above triangle\u2019, \u2018circle right of square $\\wedge$ triangle above square\u2019, \u2018square right of circle $\\wedge$ triangle above circle\u2019, \u2018line\u2019: \u2018circle right of triangle $\\wedge$ triangle right of square\u2019. There are five demonstrations of each novel composition. ", "page_idx": 16}, {"type": "image", "img_path": "atIE6Npr5A/tmp/f684bf216553fa244166dc1aa6fc4b6095ab746978bec53ec5959e847705dc90.jpg", "img_caption": ["Figure 14: New concepts that are training concept compositions. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on learned concepts that are training concept compositions. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "atIE6Npr5A/tmp/762e0212abd1671f6e86ee19252a2a46496d1d05699304fd0a67c5eade167ff8.jpg", "img_caption": ["Figure 15: New concepts that are not explicit training concept compositions. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on learned concepts that are not explicit training concept compositions. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "atIE6Npr5A/tmp/283e6c7b6e49a879743dd91a38f2f8e7d102cbc454c018eec7a0a0da6fddcf6c.jpg", "img_caption": ["Figure 16: New concept composed with training concepts. We display successful (green frames) and unsuccessful (red frames) states generated by our model conditioned on a learned concept that is not an explicit training concept composition (\u2018square diagonal to triangle\u2019) in composition with various training concepts. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "atIE6Npr5A/tmp/4cd813f4808d8d007f06824c0a22fb7ac138543d2bde89c82a3202ade0a114ac.jpg", "table_caption": ["Table 3: Analysis of learned concepts. For new concepts that are explicit training concept compositions, we evaluate what each learned component captures. For both concepts 1 and 2, we report accuracy with respect to the concept when trajectories are generated solely from one learned component. e.g. in AGENT (bottom), $42\\bar{\\%}$ of the trajectories generated by component 1 and $48\\%$ of the trajectories generated by component 2 target the red object. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 Five new concepts (new concept in Figure 6). \u2018circle\u2019: circle, triangle, and square lie in equal intervals on the circumference of a circle with radius 1.67 and center $\\in[0,\\bar{5}]^{2}$ , \u2018square diagonal to triangle\u2019, \u2018triangle diagonal to square\u2019, \u2018circle diagonal to triangle\u2019, \u2018triangle diagonal to circle\u2019. A is considered diagonal to B if their centers lie on $f(x)=x$ and if $_\\mathrm{A}$ is \u2018above\u2019 and \u2018right of\u2019 B. There are five demonstrations of each novel concept. \u2022 Composing a new concept, \u2018square diagonal to triangle\u2019, with five training concepts (new and training concept composition in Figure 6): \u2018circle right of square\u2019, \u2018circle above square\u2019, \u2018circle above triangle\u2019, \u2018circle right of triangle\u2019, \u2018triangle above circle\u2019. When we learn weights for the new concept, the training concept is weighted by $\\omega=1$ . Otherwise, both training and learned concepts are weighted by fixed $\\omega$ . ", "page_idx": 20}, {"type": "text", "text": "We verify that objects do not overlap and that no training relations unrelated to the specified new concepts exist in the demonstrations between objects. ", "page_idx": 20}, {"type": "text", "text": "State space. The state space is a 21-tuple describing three shapes (circle, triangle, and square), each represented by a 7-tuple: their center 2D position, size, angle, and one-hot shape type. ", "page_idx": 20}, {"type": "text", "text": "Evaluation data and metrics. For training, we generate 50 states conditioned on uniformly sampled training concept embeddings for single relations. For each concept composition and new concept, we report accuracy on 50 generated states conditioned on the learned concept. For new concepts composed with training concepts, we report accuracy based on 50 generated states for each training concept. Results in Figure 6 are reported for best learned $\\omega$ and fixed $\\omega\\in\\{1.2,1.4,1.6,1.8\\}$ \u2013 we learn two concepts with classifier-free guidance weight $\\omega=1.2$ . We report accuracy based on a relaxed version of the data generation process: A is considered \u2018right of\u2019 B if $x_{A}>x_{B}$ and $|y_{A}\\!-\\!y_{B}|\\leq2{\\cdot}\\operatorname*{max}\\{r_{A},r_{B}\\}$ , A is considered \u2018above\u2019 B if $y_{A}>y_{B}$ and $|x_{A}\\!-\\!x_{B}|\\leq2\\!\\cdot\\!\\operatorname*{max}\\{r_{A},r_{B}\\}$ . A, B and C lie on a \u2018circle\u2019 if their centers form a circle of radius $r$ where $|r-1.67|<0.3$ and A is \u2018diagonal\u2019 to $\\mathbf{B}$ if $x_{A}>x_{B}$ , $y_{A}>y_{B}$ and they lie within a $2\\cdot\\operatorname*{max}\\{r_{A},r_{B}\\}$ margin of $f(x)=x$ . ", "page_idx": 20}, {"type": "text", "text": "B.2 Goal-Oriented Navigation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Training. To collect demonstrations, we follow the data generation process in the AGENT benchmark environment [72], which provides a planner for navigation given the desired target. In the provided environment, the agent\u2019s initial position $a_{t=0}^{p}=\\bar{(0,0.102,-3.806)}$ , color (yellow) and shape (cone) are fixed. There are two objects: the target and a distractor. Each object has a color $\\!)_{1}^{c},o_{2}^{c}\\in\\{\\mathrm{red},\\mathrm{yellow},\\mathrm{purple},\\mathrm{green}\\}$ , a shape $o_{1}^{s},o_{2}^{s}\\in\\{\\mathrm{cone,sphere,bowl,cube}\\}$ and a position $\\sigma_{1}^{\\bar{p}}\\;\\in\\;[0,\\bar{1}.66]\\times\\{0.102\\}\\times[-4.355,-3.257],$ $^{-}\\!o_{2}^{p}\\phantom{^{-}}\\in[-1.66,0]\\times\\{0.102\\}\\times[-4.355,-3.257]$ . ", "page_idx": 20}, {"type": "text", "text": "The training dataset of $\\sim900$ samples consists of concepts \u2018go to red object\u2019 and \u2018go to yellow object\u2019 where $o_{1}^{c}\\,\\in\\,\\{\\mathrm{red},\\mathrm{yellow}\\}$ , $\\begin{array}{r}{o_{2}^{c}\\,\\in\\,\\{\\mathrm{red},\\mathrm{yellow}\\}\\,\\setminus^{\\bullet}\\{o_{1}^{c}\\}}\\end{array}$ and $o_{1}^{s},o_{2}^{s}\\,\\in\\,\\{\\mathrm{cone,sphere}\\},$ , and concepts \u2018go to bowl\u2019 and \u2018go to cube\u2019 where $o_{1}^{s}\\in\\{\\mathrm{bowl},\\mathrm{cube}\\}$ , $o_{2}^{s}\\in\\{\\mathrm{bowl},\\mathrm{cube}\\}\\setminus\\{o_{1}^{c}\\}$ and $o_{1}^{c},o_{2}^{c}\\in\\{\\mathrm{purple},\\mathrm{green}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 Five novel compositions of training target color and shape attributes (training composition in Figure 6). \u2018go to red bowl\u2019 $(o_{i}^{c},o_{j}^{c}\\,=\\,\\mathrm{red},o_{i}^{s}\\,=\\,\\mathrm{bow})$ l, $o_{j}^{s}\\;\\in\\;\\{\\mathrm{cone,sphere}\\}$ or $o_{i}^{s},o_{j}^{s}\\,=\\,\\mathrm{bowl},o_{i}^{c}\\,=\\,\\mathrm{red}$ , $o_{j}^{c}\\;\\in\\;\\{\\mathrm{purple,\\dot{green}}\\}$ where $i\\in\\{1,2\\}$ and $j\\,=\\,\\{1,2\\}\\setminus i)$ , \u2018go to yellow bowl\u2019 $(o_{i}^{c},o_{j}^{c}\\;=\\;\\mathrm{yellow},o_{i}^{s}\\;=$ bowl, $o_{j}^{s}~\\in~\\{\\mathrm{cone,sphere}\\}$ or $o_{i}^{s},o_{j}^{s}\\;=\\;$ bowl, $o_{i}^{c}=$ yellow, $o_{j}^{c}\\,\\in\\,\\{\\mathrm{purple,green}\\}$ ), \u2018go to red cube\u2019 $(o_{i}^{c},o_{j}^{c}\\,=\\,\\mathrm{red},o_{i}^{s}\\,=\\,\\mathrm{cube},$ $o_{j}^{s}\\ \\in\\ \\{\\mathrm{cone,sphere}\\}$ or $o_{i}^{s},o_{j}^{s}\\;=\\;\\mathrm{cube},o_{i}^{c}\\;=$ red, $o_{j}^{c}\\;\\in\\;\\{\\mathrm{purple,green}\\}$ ), \u2018go to yellow cube\u2019 $(o_{i}^{c},o_{j}^{c}\\ =$ yellow, $o_{i}^{s}\\;=\\;$ cube, $o_{j}^{s}\\;\\in\\;\\{\\mathrm{cone,sphere}\\}$ or $o_{i}^{s},o_{j}^{s}\\,=\\,\\mathrm{cube},o_{i}^{c}\\,=$ yellow, $o_{j}^{c}~\\in~\\{\\mathrm{purple,green}\\}$ ), \u2018go to purple cone\u2019 $(o_{i}^{c},o_{j}^{c}\\ =\\ \\mathrm{purple},o_{i}^{s}\\ =\\ \\$ cone, $o_{j}^{s}\\;\\in\\;\\{\\mathrm{bowl},\\mathrm{cube}\\}\\;$ or $o_{i}^{s},o_{j}^{s}\\,=\\,\\mathrm{cone},o_{i}^{c}\\,=$ purple, $o_{j}^{c}\\;\\in\\;\\{\\mathrm{red},\\mathrm{yellow}\\}\\rangle$ ). Note that in each scenario, the distractor object either has the same color or shape as the target, and combined with its other attribute (shape or color), it is within the training distribution (i.e., red and yellow cones and spheres, and purple and green bowls and cubes). There are five demonstrations of each novel composition. \u2022 Conditioning each novel composition concept on novel initial states sampled from the novel concept distribution (training composition new initial state in Figure 6). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "State space. The initial state space we condition on is a 52-tuple based on the state space in Shu et al. [72] describing the agent and two objects. The agent is represented by its 3D position, quaternion $\\in\\mathbb{R}^{4}$ , velocity $\\in\\mathbb{R}^{3}$ , angular velocity $\\in\\mathbb{R}^{3}$ , one-hot type (representing the agent and two objects), rgba color, and one-hot shape (representing the four possible shapes). Similarly, each object is represented by its 3D position, type, color, and shape. The demonstration state space is a 13-tuple representing the first 13 dimensions of the initial state space (agent position, quaternion, velocity, and angular velocity). Demonstrations $i\\in[N]$ have horizons $H_{i}\\leq150$ and are padded to length 150 using the final state. During training, we sample the demonstrations to generate subtrajectories of length 128. ", "page_idx": 21}, {"type": "text", "text": "Evaluation data and metrics. For training, we generate 50 trajectories conditioned on uniformly sampled training concepts and initial states. For each new concept, we generate five trajectories conditioned on the learned concept and five initial states from the new concept demonstrations. To evaluate each concept composition on new initial states, we generate trajectories conditioned on the learned concept and 50 initial states sampled from the new concept distribution. Results in Figure 6 are reported for best learned $\\omega$ and fixed $\\omega\\in\\{1.2,1.4,1.6,1.\\bar{8}\\}-\\mathrm{we}$ learn two concepts with classifier-free guidance weight $\\omega=1.6$ . We report accuracy based on whether the agent in the generated trajectory has made progress towards the desired target $(|a_{t=128}^{p}-o_{\\mathrm{target}}^{p}|<|a_{t=0}^{p}-o_{\\mathrm{target}}^{p}|)$ . Accuracy for closed loop evaluation (Figure 6) is reported based on optimally reaching target otarget before a distractor odistractor $(|a_{t=128}^{p}-o_{\\mathrm{target}}^{\\overline{{{p}}}}|<0.365\\wedge\\forall t<128:\\,|\\dot{a}_{t}^{p}-o_{\\mathrm{distractor}}^{\\overline{{{p}}}}|\\geq\\overline{{0}}.3\\bar{65})$ . ", "page_idx": 21}, {"type": "text", "text": "Closed loop evaluation. The action space $\\mathcal{A}\\subseteq\\mathbb{R}^{2}$ represents forces applied to the agent. We take actions $a_{t}=\\tau_{t+5}-s_{t}$ where $s_{t}$ is an observation in the environment, and $\\tau_{t+5}$ is a future step in the open loop plan generated by $\\mathcal{G}$ conditioned on observation $s_{t}$ and learned concept c\u02dc. We execute actions in the environment until reaching the target or a maximum number of steps and report the evaluation metric described above. Closed loop evaluation is done in pybullet simulation [81] for efficiency. ", "page_idx": 21}, {"type": "text", "text": "B.3 MoCap ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Training. All human actions in the CMU Graphics Lab Motion Capture Database (http://mocap.cs.cmu.edu/) are included in the training set except three new scenarios. We further discard videos with less than 128 frames. The training set includes 2210 demonstrations. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Three new concepts. \u2018jumping jacks\u2019 (3 demonstrations), \u2018chop wood\u2019 (2), and \u2018breaststroke\u2019 (3).   \n\u2022 Conditioning each novel concept on novel initial states \u2013 the last states in the new concept demonstrations.   \n\u2022 Composing new learned concept \u2018jumping jacks\u2019 weighted by learned concept weights with three training concepts (\u2018walk\u2019, \u2018jump\u2019 and \u2018march\u2019) weighted by $\\omega=1$ , and conditioned on a training initial state from the training concepts. ", "page_idx": 22}, {"type": "text", "text": "State space. The initial state space is a 42-tuple representing the 3D position of 14 joints. The original dataset contains 31 joints. Our version is adapted with Tanke et al. [82] to reduce the number of joints to 14 and to remove rotation and translation. The demonstration state space is the same with horizons $H_{i}$ , $i\\in[N]$ . We subsample the original trajectories every 4 steps and further sample trajectories to generate subtrajectories of unified length 32 on which we train. ", "page_idx": 22}, {"type": "text", "text": "Evaluation data and metrics. For each new concept, we generate trajectories conditioned on the learned concept and an initial state from the new concept demonstrations. For evaluating each new concept on new initial states, we generate trajectories conditioned on the learned concept and novel initial states from the new concept demonstrations that were not used as initial states during concept learning, specifically the last state in each demonstration. For new concepts composed with training concepts, we generate trajectories, each conditioned on the learned concept, a uniformly sampled training concept, and training initial state. Results are reported for best learned $\\omega$ and fixed $\\bar{\\omega}\\in\\bar{\\{1.2,1.4,1.6,1.8\\}}\\bar{-}\\mathrm{v}$ e learn two concepts with classifier-free guidance weight $\\omega=1.8$ . ", "page_idx": 22}, {"type": "text", "text": "Human experiment. We show five humans (aged 23-28, four male) videos of training and new concepts with their natural language labels and ask whether each video depicts the concept and which depicts it best. The participants gave their consent, and the study was approved by an Institutional Review Board. We show three training concepts: \u2018march\u2019, \u2018run\u2019, and \u2018walk\u2019, and three new concepts: \u2018jumping jacks\u2019, \u2018chop wood\u2019, and \u2018breaststroke\u2019 on new initial states. ", "page_idx": 22}, {"type": "text", "text": "B.4 Autonomous Driving ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Training. We use the following scenarios from the HighwayEnv driving simulation environment [75]. The training scenarios include four driving scenarios: \u2018highway\u2019, \u2018exit\u2019, \u2018merge\u2019, and \u2018intersection\u2019. In \u2018highway\u2019, the objective is driving on a highway at high speed on the rightmost lanes while avoiding collisions. The highway has four lanes and 50 vehicles. The initial lane and position of all vehicles are sampled, as well as non-controlled vehicle speeds. An episode ends if the controlled vehicle crashes or a time limit is reached. In \u2018exit\u2019 the objective is to take a highway exit while driving on a four-lane highway with an exit lane and 20 vehicles. The controlled vehicle is rewarded for exiting at high speed and driving on the rightmost lanes while avoiding collisions. The initial position of all vehicles is sampled. An episode ends if the controlled vehicle crashes or a time limit is reached. In \u2018merge\u2019, the objective is driving on a highway with three lanes and a merging lane and three vehicles, one of them merging. The controlled vehicle is rewarded for driving at high speed while avoiding collisions and allowing another vehicle to merge into the highway. The lane, position, and speed are sampled for non-controlled vehicles. An episode ends if the controlled vehicle passes the merging lane or crashes. In \u2018intersection\u2019, the objective is making a left turn at an intersection with four two-way roads and 10 vehicles. The controlled vehicle is rewarded for crossing the intersection at high speed while staying on the road and avoiding collisions. The controlled vehicle\u2019s position is sampled, as well as the lane, position, and speed of other vehicles. An episode ends if the controlled vehicle completes crossing the intersection or crashes. Expert demonstrations were collected using a deterministic tree search planner provided in [83]. ", "page_idx": 22}, {"type": "text", "text": "New Task. The new scenario includes: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 A novel driving scenario: \u2018roundabout\u2019. In this scenario, the objective is to take the second exit at a roundabout with four exits and four vehicles. The position and speed of noncontrolled vehicles are sampled. An episode ends if the controlled vehicle crashes or a time limit is reached. The novel concept is conditioned on novel initial states sampled from the new concept distribution. There are five demonstrations of this new concept. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "State space. The initial state space we condition on is in $\\mathbb{R}^{5\\times7}$ , the controlled vehicle and the four closest vehicles. Each vehicle is represented by a 7-tuple, including whether it is present on the road, its x and y positions, ${\\bf X}$ and y velocities, and cosine and sine heading directions. The demonstration state space observations of the controlled vehicle in $\\mathbb{R}^{7}$ for horizons $H_{i}$ , $i\\in[N]$ . We sample trajectories to generate subtrajectories of length 8, which we train on. ", "page_idx": 23}, {"type": "text", "text": "Evaluation data and metrics. For evaluating the new concept on new initial states, we evaluate in closed loop on 50 initial states sampled from the new concept distribution. Task return, crash, and success rates are calculated based on the rewards described above. Note that the \u2018highway\u2019 scenario doesn\u2019t have a success score as there is no final state to reach in this scenario. Results are reported for best learned $\\omega$ and fixed $\\omega\\in\\{1.2,1.4,1.6,1.8\\}$ \u2013 for training, we report $\\omega=1.8$ and during new concept learning, we learn two concepts and their corresponding classifier-free guidance weights. ", "page_idx": 23}, {"type": "text", "text": "Closed Loop Evaluation. The action space is discrete: move to the left lane, stay idle, move to the right lane, drive faster, drive slower. We assume access to a planner $\\mathcal{P}$ that given two states plans which action to take in the environment via access to simulation in the environment, $a_{t}=\\mathcal{P}(\\tau_{t+1},s_{t})$ . The planner simulates the possible actions from observed state $s_{t}$ and randomly selects an action out of the ones closest to the model predicted next state $\\tau_{t+1}$ that does not result in the controlled vehicle crashing. We execute actions until reaching a maximum number of steps or until the controlled vehicle crashes. ", "page_idx": 23}, {"type": "text", "text": "B.5 Table-Top Manipulation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Training. We collect 214 expert demonstrations with a Franka Research 3 robot via teleop with a Spacemouse for four table-top manipulation tasks: \u2018pick green circle and place on book\u2019 (29 demonstrations), \u2018pick green circle and place on elevated white surface\u2019 (30), \u2018push green circle to orange triangle\u2019 (124), and \u2018push green circle to orange triangle around purple bowl\u2019 (31). Examples of these tasks are best viewed on our website. ", "page_idx": 23}, {"type": "text", "text": "New task. The new scenario includes pushing the green circle to the orange triangle on a book. We provide ten demonstrations of this task. ", "page_idx": 23}, {"type": "text", "text": "State space. The initial state space we condition on includes an overhead RGB image of the scene (Figure 10) and the robot\u2019s end effector pose, a 7-tuple of its 3D position and quaternion, and gripper state in $\\mathbb{R}$ . We verify that most states are fully observable. The demonstration state space includes observations of the end effector pose and gripper state in $\\mathbb{R}^{8}$ for horizons $H_{i}$ , $i\\in[N]$ . We sample trajectories to generate subtrajectories of length 32, which we train on. ", "page_idx": 23}, {"type": "text", "text": "Evaluation data and metrics. We evaluate 20 episodes for training \u2018push green circle to orange triangle\u2019 and new concept \u2018push green circle to orange triangle on book\u2019 on new initial states sampled from the concept distributions. An episode is successful if the green circle touches the orange triangle before a maximum horizon is reached. Results for training are reported for $\\omega=1.8$ and during new concept learning, we learn two concepts and their corresponding classifier-free guidance weights. ", "page_idx": 23}, {"type": "text", "text": "Closed loop evaluation. The action space is an 8-tuple of the end effector pose and gripper state, equivalent to the predictions of the model. Given predicted action $\\tau_{t+1}$ and end effector pose $s_{t}$ , we assume access to a planner that linearly interpolates between the current and next predicted pose. We make predictions and roll out the 32 predicted states in closed loop until it succeeds or executes a maximum number of steps $H=160$ . ", "page_idx": 23}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Diffusion model. We represent the noise model $\\epsilon_{\\theta}$ with an MLP for the object rearrangement domain and with a temporal U-Net for the AGENT, MoCap and Driving domains as implemented in Ajay et al. [17]. For Manipulation, we use a temporal U-Net where images are processed by a pretrained resnet18 [84] that is finetuned with the model during training. We use the same hyperparameters as in Ajay et al. [17] except for the probability of removing conditioning information, $p$ , which we set to 0.1. In Table 4 we demonstrate the effect of choosing different classifier-free guidance weights $\\omega$ . We are overall better or comparable to the baselines. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "atIE6Npr5A/tmp/3bafdb00b0f51df95af47ca92c3cd10d4ffbd0a9fdda7d6f2800a1bcb4922074.jpg", "table_caption": ["Table 4: Classifier-free guidance weight choice effect. For Object Rearrangement and GoalOriented Navigation we report the accuracy and standard error of the mean for new concepts from new initial states, and for Driving, the success and crash rates. We compare the four baselines with our approach as reported in Figures 6, 9 and 13 and Section 5.2. We report results for our approach with all $\\omega$ in our hyperparameter search and mark the reported $\\omega$ in Figures 6, 9 and 13 in bold font. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "BC. We implement behavior cloning (BC) [85]. The BC model is deterministic. In the Object Rearrangement domain, we only condition on concept $c$ . To add stochasticity, when evaluating BC, we average results over 50 different seeds. We use an MLP with one hidden layer of size 512, ReLU activations, and AdamW [86] with learning rate $6\\cdot10^{-4}$ . In AGENT, at each step, the input to the model is the fully observable initial state (52-dim) and the condition, and the model learns to predict the next partially observable agent state (13-dim). Composing concepts is implemented by adding conditions. ", "page_idx": 24}, {"type": "text", "text": "VAE. We implement a conditional variational autoencoder model (CVAE) $[76]^{2}$ . We use an MLP for the architecture with AdamW [86] and learning rate $6\\cdot10^{-4}$ . We sample 50 trajectories per concept and average the results. Composing concepts is implemented by adding conditions. ", "page_idx": 24}, {"type": "text", "text": "In-Context. For the In-Context learning baseline [8, 28], we use a transformer encoder-decoder architecture with 4 layers and 4 heads, where multiple demonstrations are passed into the encoder with a zero vector to separate consecutive demonstration trajectories. During training, there are five prompt demonstrations, during training evaluation, we provide one demonstration, and for new concepts, we provide two to five demonstrations, depending on the number of demonstrations provided in each domain. The decoder is passed a window of the previous states and predicts the next state. In AGENT and Driving, we use window size $K=1$ , and in MoCap $K=2$ . We simplify the learning objective to negative log-likelihood loss and convert the data to 20 bins for AGENT and Driving and 16 bins for MoCap. We further simplify the AGENT data by sampling the trajectories every eight states. We use learning rate $10^{-4}$ in all domains. We do not evaluate on Object Rearrangement since the horizon is one. ", "page_idx": 24}, {"type": "text", "text": "Language. The language instructions are as follows. For Object Rearrangement, training compositions are training concept descriptions composed with the word \u2018and\u2019. New concepts are described as \u2018circle triangle and square in a circle of radius $1.67^{\\circ}$ and \u2018A right and above diagonally to $\\mathbf{B}^{\\bullet}$ where A, $\\mathbf{B}\\in\\{$ \u2018circle\u2019, \u2018triangle\u2019, \u2018square\u2019}. New concepts composed with training concepts are composed with the word \u2018and\u2019. AGENT concepts are described based on new concept attributes, e.g. \u2018go to red bowl\u2019. In MoCap, descriptions are human actions such as \u2018jumping jacks\u2019. ", "page_idx": 25}, {"type": "text", "text": "Compute. We run all simulated experiments on a single NVIDIA RTX A4000 machine. We evaluate our method on real-world table-top manipulation tasks using a Franka Research 3 robot with an overhead Realsense D435I RGB camera and an NVIDIA RTX 4090 machine. Concept learning can take approximately one to two hours. Since a new concept only has to be learned once to generate behavior, in the domains we presented, it is reasonable to learn a concept offilne, and therefore, it is not prohibitive. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe our formulation in Section 3, the adapted method in Section 4 and empirical evaluation in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss limitations in Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Described in Section 5 and Appendix B and C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We plan to release our code in the near future. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss implementation details in Appendix B and C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide errors in Figures 6, 9 and 13. Plots without errors display accuracy over a fixed set. Similarly for Tables 5.1, 2 and 3. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Described in Appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss limitations in Section 6 and obtain an IRB for a small-scale human experiment. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: In this paper, we demonstrate that effectively learning behavior (or concept) representations from a few demonstrations is possible given a pretraining dataset, dependent crucially on the quality of training and new concept demonstrations. While such an effective few-shot learning system may have many societal implications, none are specific to this work that we feel a need to highlight. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no high risks we feel the need to highlight. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We add citations and links throughout the paper to assets used in this work. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not release any new assets. In the future, we plan to release our code and add documentation. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We conduct a small scale human experiment described in Section 5, Table 5.1 and Appendix B.3. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: IRB approval was obtained. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 31}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]